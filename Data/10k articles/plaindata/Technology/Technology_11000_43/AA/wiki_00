{"id": "51159625", "url": "https://en.wikipedia.org/wiki?curid=51159625", "title": "AV1", "text": "AV1\n\nAOMedia Video 1 (AV1) is an open, royalty-free video coding format designed for video transmissions over the Internet. It is being developed by the Alliance for Open Media (AOMedia), a consortium of firms from the semiconductor industry, video on demand providers, and web browser developers, founded in 2015. The AV1 bitstream specification includes a reference video codec.\n\nAV1 is meant to succeed its predecessor VP9 and compete with HEVC/H.265 from the Moving Picture Experts Group. It is the primary contender for standardization by the video standard working group NetVC of the Internet Engineering Task Force (IETF). The group has put together a list of criteria to be met by the new video standard.\n\nAV1 is intended to be able to be used together with the audio format Opus in a future version of the WebM container format for HTML5 web video and WebRTC.\n\nThe first official announcement of the project came with the press release on the formation of the Alliance on 1 September 2015.\nThe increased usage of its predecessor VP9 is attributed to confidence in the Alliance and development of AV1 as well as the pricey and complicated licensing situation of HEVC (High Efficiency Video Coding).\n\nThe roots of the project precede the Alliance, however.\nIndividual contributors started experimental technology platforms years before: Xiph's/Mozilla's Daala already published code in 2010, VP10 was announced on 12 September 2014, and Cisco's Thor was published on 11 August 2015.\nThe first version 0.1.0 of the AV1 reference codec was published on 7 April 2016.\n\nSoft feature freeze was at the end of October 2017, but a few significant features were decided to continue developing beyond this. The bitstream format was projected to be frozen in January 2018; however, this was delayed due to unresolved critical bugs as well as last changes to transformations, syntax, the prediction of motion vectors, and the completion of legal analysis. The Alliance announced the release of the AV1 bitstream specification on 28 March 2018, along with a reference, software-based encoder and decoder. On 25 June 2018, a validated version 1.0.0 of the specification was released.\n\nMartin Smole from AOM member Bitmovin says that the computational efficiency of the reference encoder is the greatest remaining challenge after the bitstream format freeze. While still working on the format, the encoder was not targeted for productive use and didn't receive any speed optimizations. Therefore, it works orders of magnitude slower than e.g. existing HEVC encoders, and development is planned to shift its focus towards maturing the reference encoder after the freeze.\n\nAV1 aims to be a video format for the web that is both state of the art and royalty free. The mission of the Alliance for Open Media is the same as that of the WebM project.\n\nTo fulfill the goal of being royalty free, the development process is such that no feature is adopted before it has been independently double checked that it does not infringe on patents of competing companies.\nThis contrasts with its main competitor HEVC, for which a review of the intellectual property rights (IPR review) was not part of the standardization process. The latter reviewing practice is stipulated in the ITU-T's definition of an open standard.\n\nThe possible existence of yet unknown patents has been a recurring concern in the field of royalty-free multimedia formats; the concern has been raised regarding AV1, and previously VP9, Theora and IVC. The problem of unforeseen patents is not unique to royalty-free formats, but it uniquely threatens their \"status\" as royalty-free. In contrast, IPR avoidance has not traditionally been a priority in MPEG's business model for royalty-bearing formats (although the MPEG chairman argues it has to change).\n\nUnder patent rules adopted from the World Wide Web Consortium (W3C), technology contributors license their AV1-connected patents to anyone, anywhere, anytime based on reciprocity, i.e. as long as the user does not engage in patent litigation. As a defensive condition, anyone engaging in patent litigation loses the right to the patents of \"all\" patent holders.\n\nThe performance goals include \"a step up from VP9 and HEVC\" in efficiency for a low increase in complexity. NETVC's efficiency goal is 25% improvement over HEVC. The primary complexity concern is for software decoding, since hardware support will take time to reach users. However, for WebRTC, live encoding performance is also relevant, which is Cisco's agenda: Cisco is a manufacturer of videoconferencing equipment, and their Thor contributions aim at \"reasonable compression at only moderate complexity\".\n\nFeature wise, it is specifically designed for real-time applications (especially WebRTC) and higher resolutions (wider color gamuts, higher frame rates, UHD) than typical usage scenarios of the current generation (H.264) of video formats where it is expected to achieve its biggest efficiency gains. It is therefore planned to support the color space from ITU-R Recommendation BT.2020 and up to 12 bits of precision per color component. AV1 is primarily intended for lossy encoding, although lossless compression is supported as well.\n\nAV1-based containers have also been proposed as a replacement for JPEG, similar to Better Portable Graphics and High Efficiency Image File Format which wrap HEVC.\n\nAV1 is a traditional block-based frequency transform format featuring new techniques, several of which were developed in experimental formats that have been testing technology for a next-generation format after HEVC and VP9. Based on Google's experimental VP9 evolution project \"VP10\", AV1 incorporates additional techniques developed in Xiph's/Mozilla's Daala and Cisco's Thor.\nThe Alliance published a reference implementation written in C and assembly language (codice_1, codice_2) as free software under the terms of the BSD 2-Clause License. Development happens in public and is open for contributions, regardless of AOM membership.\n\nThere is another open source encoder, namely \"rav1e\", which – unlike aomenc – aims to be the simplest and fastest conforming encoder at the expense of efficiency.\n\nThe development process is such that coding tools are added to the reference codebase as \"experiments\", controlled by flags that enable or disable them at build time, for review by other group members as well as specialized teams that help with and ensure hardware friendliness and compliance with intellectual property rights (TAPAS). Once the feature gains some support in the community, the experiment can be enabled by default, and ultimately have its flag removed when all of the reviews are passed. Experiment names are lowercased in the \"configure\" script and uppercased in conditional compilation flags.\n\nTo transform the error remaining after prediction to the frequency domain, AV1 uses square and 2:1/1:2 rectangular DCTs, as well as an asymmetric DST for blocks where the top and/or left edge is expected to have lower error thanks to prediction from nearby pixels.\n\nIt can combine two one-dimensional transforms in order to use different transforms for the horizontal and the vertical dimension (codice_3).\n\nPrediction can happen for bigger units (≤128×128), and they can be subpartitioned in more ways. \"T-shaped\" partitioning schemes for coding units are introduced, a feature developed for VP10, as well as horizontal or vertical splits into four stripes of 4:1 aspect ratio. Two separate predictions can now be used on spatially different parts of a block using a smooth, oblique transition line (\"wedge-partitioned prediction\"). This enables more accurate separation of objects without the traditional staircase lines along the boundaries of square blocks.\n\nMore encoder parallelism is possible thanks to configurable prediction dependency between tile rows.\n\nAV1 performs internal processing in higher precision (10 or 12 bits per sample), which leads to compression improvement due to smaller rounding errors in reference imagery.\n\nPredictions can be combined in more advanced ways (than a uniform average) in a block (\"compound prediction\"), including smooth and sharp transition gradients in different directions (\"wedge-partitioned prediction\") as well as implicit masks that are based on the difference between the two predictors. This allows combination of either two inter predictions or an inter and an intra prediction to be used in the same block.\n\nA frame can reference 6 instead of 3 of the 8 available frame buffers for temporal (inter) prediction.\n\nThe \"Warped Motion\" (codice_4) and \"Global Motion\" (codice_5) tools in AV1 aim to reduce redundant information in motion vectors by recognizing patterns arising from camera motion. They implement ideas that were tried to be exploited in preceding formats like e.g. MPEG-4 ASP, albeit with a novel approach that works in three dimensions. There can be a set of warping parameters for a whole frame offered in the bitstream, or blocks can use a set of implicit local parameters that get computed based on surrounding blocks.\n\nFor intra prediction, there are 56 (instead of 8) angles for directional prediction and weighted filters for per-pixel extrapolation. The \"TrueMotion\" predictor got replaced with a Paeth predictor which looks at the difference from the known pixel in the above left corner to the pixel directly above and directly left of the new one and then chooses the one that lies in direction of the smaller gradient as predictor. A palette predictor is available for blocks with very few colors like in some computer screen content. Correlations between the luminosity and the color information can now be exploited with a predictor for chroma blocks that is based on samples from the luma plane (codice_6). In order to reduce discontinuities along borders of inter-predicted blocks, predictors can be overlapped and blended with those of neighbouring blocks (\"overlapped block motion compensation\").\nAV1 has new optimized quantization matrices.\n\nFor the in-loop filtering step, the integration of Thor's constrained low-pass filter and Daala's directional deringing filter has been fruitful: The combined \"Constrained Directional Enhancement Filter\" (codice_7) exceeds the results of using the original filters separately or together.\nIt is an edge-directed conditional replacement filter that smoothes blocks with configurable (signaled) strength roughly along the direction of the dominant edge to eliminate ringing artifacts.\n\nThere is also the \"loop restoration filter\" (codice_8) to remove blur artifacts due to block processing.\n\n\"Film grain synthesis\" (codice_9) improves coding of noisy signals using a parametric video coding approach.\nDue to the randomness inherent to film grain noise, this signal component is traditionally either very expensive to code or prone to get damaged or lost, possibly leaving serious coding artefacts as residue. This tool circumvents these problems using analysis and synthesis, replacing parts of the signal with a visually similar synthetic texture, based solely on subjective visual impression instead of objective similarity. It removes the grain component from the signal, analyzes its non-random characteristics, and instead transmits only descriptive parameters to the decoder, which adds back a synthetic, pseudorandom noise signal that's shaped after the original component.\n\nDaala's entropy coder (codice_10), a non-binary arithmetic coder, was selected for replacing VP9's binary entropy coder. The use of \"non-binary\" arithmetic coding helps evade patents, but also adds bit-level parallelism to an otherwise serial process, reducing clock rate demands on hardware implementations. This is to say that the effectiveness of modern binary arithmetic coding like CABAC is being approached using a greater alphabet than binary, hence greater speed, as in Huffman code (but not as simple and fast as Huffman code).\nAV1 also gained the ability to adapt the symbol probabilities in the arithmetic coder per coded symbol instead of per frame (codice_11).\n\n\"This list is no longer complete. It is being rewritten in prose.\"\n\n\"Only explained experiments are listed.\"\n\n\"Daala Transforms\" were the major innovation behind the daala codec. They implement \"lapped\" discrete cosine and sine transforms that its authors describe as \"better in every way\" than the codice_12 set of transforms that prevailed in AV1. Both the codice_12 and codice_14 experiments have merged high and low bitdepth code paths (unlike VP9), but codice_14 achieved full embedding of smaller transforms within larger, as well as using fewer multiplies, which could have further reduced the cost of hardware implementations. The Daala transforms were kept as optional in the experimental codebase until late January 2018, but changing hardware blocks at a late stage was a general concern for delaying hardware availability.\n\nThe encoding complexity of Daala's Perceptual Vector Quantization was too much within the already complex framework of AV1. The \"Rate Distortion\" codice_16 heuristic aims to speed up the encoder by a sizable factor, PVQ or not, but PVQ was ultimately dropped.\n\nANS was the other non-binary arithmetic coder, developed in parallel with Daala's entropy coder. Of the two, Daala EC was the more hardware friendly, but ANS was the fastest to decode in software.\n\nA first comparison from the beginning of June 2016 found AV1 roughly on par with HEVC, as did one using code from late January 2017.\n\nIn April 2017, using the 8 enabled experimental features at the time (of 77 total), Bitmovin was able to demonstrate favorable objective metrics, as well as visual results, compared to HEVC on the \"Sintel\" and \"Tears of Steel\" animated films. A follow-up comparison by Jan Ozer of \"Streaming Media Magazine\" confirmed this, and concluded that \"AV1 is at least as good as HEVC now\".\n\nOzer noted that his and Bitmovin's results contradicted a comparison by Fraunhofer Institute for Telecommunications from late 2016 that had found AV1 38.4% less efficient than HEVC, underperforming even H.264/AVC, and justified this discrepancy by having used encoding parameters endorsed by each encoder vendor, as well as having more features in the newer AV1 encoder.\n\nTests from Netflix showed that, based on measurements with PSNR and VMAF at 720p, AV1 could be about 25% more efficient than VP9 (libvpx), at the expense of a 4–10 fold increase in encoding complexity. Similar conclusions with respect to quality were drawn from a test conducted by Moscow State University researchers, where VP9 was found to require 31% and HEVC 22% more bitrate than AV1 for the same level of quality. The researchers found that the used AV1 encoder was operating at a speed “2500–3500 times lower than competitors”, while admitting that it has not been optimized yet.\n\nIn a comparison of AV1 against H.264 (x264) and VP9 (libvpx), Facebook showed about 45–50% bitrate savings over H.264 and about 40% over VP9 when using a constant quality encoding mode.\n\nAOMedia provides a list of test results on their website.\n\nAV1 defines three profiles for decoders which are Main, High, and Professional. The Main profile allows for a bit depth of 8- or 10-bits per sample with 4:0:0 (greyscale) and 4:2:0 chroma sampling. The High profile allows for a bit depth of 8- or 10-bits per sample with 4:4:4 chroma sampling. The Professional profile allows for a bit depth of 12-bits per sample with 4:0:0, 4:2:0, and 4:4:4 chroma sampling, and 8, 10 or 12 bits per sample with 4:2:2 chroma sampling.\nAV1 defines levels for decoders with maximum variables for levels ranging from 2.0 to 7.3. Example resolutions would be 426×240@30fps for level 2.0, 854×480@30fps for level 3.0, 1920×1080@30fps for level 4.0, 3840×2160@60fps for level 5.1, 3840×2160@120fps for level 5.2 and 5.3, and 7680×4320@120fps for level 6.2. Level 7 has not been defined yet.\n\nLike its predecessor VP9, AV1 can be used inside WebM container files alongside the Opus audio format. These formats are well supported among web browsers, with the exception of Safari (only has Opus support) and the discontinued Internet Explorer (prior to Edge) (see VP9 in HTML5 video).\n\nFrom November 2017 onwards, nightly builds of the Firefox web browser contained preliminary support for AV1. Upon its release on 9 February 2018, version 3.0.0 of the VLC media player shipped with an experimental AV1 decoder. Firefox 64 is planned for first official release. It is available in Firefox 63 and Google Chrome 70, but it is deactivated in standard configuration of Firefox 63. \n\nIt is expected that Alliance members have interest in adopting the format, in respective ways, once the bitstream is frozen. The member companies represent several industries, including browser vendors (Apple, Google, Mozilla, Microsoft), content distributors (Apple, Amazon, Facebook, Google, Hulu, Netflix) and hardware designers (AMD, Apple, Arm, Broadcom, Intel, Nvidia).\nVideo streaming service YouTube declared intent to transition to the new format as fast as possible, starting with highest resolutions within six months after the finalization of the bitstream format.\nNetflix \"expects to be an early adopter of AV1\".\n\nAccording to Mukund Srinivasan, chief business officer of AOM member Ittiam, early hardware support will be dominated by software running on non-CPU hardware (such as GPGPU, DSP or shader programs, as is the case with some VP9 hardware implementations), as fixed-function hardware will take 12–18 months after bitstream freeze until chips are available, plus 6 months for products based on those chips to hit the market. The bitstream was finally frozen on 28 March 2018, meaning chips could be available sometime between March and August 2019. According to the above forecast, products based on chips could then be on the market at the end of 2019 or the beginning of 2020.\n\nMozilla researchers Nathan Egge and Michael Bebenita claimed in an interview in April 2018 that the web browser Mozilla Firefox would have AV1 support enabled by default by the end of 2018.\n\nAV1 support has been added to the MP4 container.\n\nYouTube released a beta launch playlist of AV1 videos at high bit-rates to test decoder performance and show their commitment to AV1.\n\n\nThe AV1 Image File Format (AVIF) is a specification for storing images or image sequences compressed with AV1 in the HEIF file format. AVIF files conform to the HEIC specification and support features like High Dynamic Range (HDR) and wide color gamut (WCG).\n\n"}
{"id": "9931", "url": "https://en.wikipedia.org/wiki?curid=9931", "title": "Amplifier", "text": "Amplifier\n\nAn amplifier, electronic amplifier or (informally) amp is an electronic device that can increase the power of a signal (a time-varying voltage or current). It is a two-port electronic circuit that uses electric power from a power supply to increase the amplitude of a signal applied to its input terminals, producing a proportionally greater amplitude signal at its output. The amount of amplification provided by an amplifier is measured by its gain: the ratio of output voltage, current, or power to input. An amplifier is a circuit that has a power gain greater than one.\n\nAn amplifier can either be a separate piece of equipment or an electrical circuit contained within another device. Amplification is fundamental to modern electronics, and amplifiers are widely used in almost all electronic equipment. Amplifiers can be categorized in different ways. One is by the frequency of the electronic signal being amplified. For example, audio amplifiers amplify signals in the audio (sound) range of less than 20 kHz, RF amplifiers amplify frequencies in the radio frequency range between 20 kHz and 300 GHz, and servo amplifiers and instrumentation amplifiers may work with very low frequencies down to direct current. Amplifiers can also be categorized by their physical placement in the signal chain; a preamplifier may precede other signal processing stages, for example. The first practical electrical device which could amplify was the triode vacuum tube, invented in 1906 by Lee De Forest, which led to the first amplifiers around 1912. Today most amplifiers use transistors.\n\nThe first practical device that could amplify was the triode vacuum tube, invented in 1906 by Lee De Forest, which led to the first amplifiers around 1912. Vacuum tubes were used in almost all amplifiers until the 1960s–1970s when the transistor, invented in 1947, replaced them. Today, most amplifiers use transistors, but vacuum tubes continue to be used in some applications.\nThe development of audio communication technology in form of the telephone, first patented in 1876, created the need to increase the amplitude of electrical signals to extend the transmission of signals over increasingly long distances. In telegraphy, this problem had been solved with intermediate devices at stations that replenished the dissipated energy by operating a signal recorder and transmitter back-to-back, forming a relay, so that a local energy source at each intermediate station powered the next leg of transmission.\nFor duplex transmission, i.e. sending and receiving in both directions, bi-directional relay repeaters were developed starting with the work of C. F. Varley for telegraphic transmission. Duplex transmission was essential for telephony and the problem was not satisfactorily solved until 1904, when H. E. Shreeve of the American Telephone and Telegraph Company improved existing attempts at constructing a telephone repeater consisting of back-to-back carbon-granule transmitter and electrodynamic receiver pairs. The Shreeve repeater was first tested on a line between Boston and Amesbury, MA, and more refined devices remained in service for some time. After the turn of the century it was found that negative resistance mercury lamps could amplify, and were also tried in repeaters, with little success.\n\nThe development of thermionic valves starting around 1902, provided an entirely electronic method of amplifying signals. The first practical version of such devices was the Audion triode, invented in 1906 by Lee De Forest, which led to the first amplifiers around 1912. Since the only previous device which was widely used to strengthen a signal was the relay used in telegraph systems, the amplifying vacuum tube was first called an \"electron relay\". The terms \"amplifier\" and \"amplification\", derived from the Latin \"amplificare\", (\"to enlarge or expand\"), were first used for this new capability around 1915 when triodes became widespread.\n\nThe amplifying vacuum tube revolutionized electrical technology, creating the new field of electronics, the technology of active electrical devices. It made possible long distance telephone lines, public address systems, radio broadcasting, talking motion pictures, practical audio recording, radar, television, and the first computers. For 50 years virtually all consumer electronic devices used vacuum tubes. Early tube amplifiers often had positive feedback (regeneration), which could increase gain but also make the amplifier unstable and prone to oscillation. Much of the mathematical theory of amplifiers was developed at Bell Telephone Laboratories during the 1920s to 1940s. Distortion levels in early amplifiers were high, usually around 5%, until 1934, when Harold Black developed negative feedback; this allowed the distortion levels to be greatly reduced, at the cost of lower gain. Other advances in the theory of amplification were made by Harry Nyquist and Hendrik Wade Bode.\n\nThe vacuum tube was virtually the only amplifying device, other than specialized power devices such as the magnetic amplifier and amplidyne, for 40 years. Power control circuitry used magnetic amplifiers until the latter half of the twentieth century when power semiconductor devices became more economical, with higher operating speeds. The old Shreeve electroacoustic carbon repeaters were used in adjustable amplifiers in telephone subscriber sets for the hearing impaired until the transistor provided smaller and higher quality amplifiers in the 1950s. \n\nThe replacement of bulky electron tubes with transistors during the 1960s and 1970s created another revolution in electronics, making possible a large class of portable electronic devices, such as the transistor radio developed in 1954. Today, use of vacuum tubes is limited for some high power applications, such as radio transmitters.\n\nBeginning in the 1970s, more and more transistors were connected on a single chip thereby creating higher scales of integration (small-scale, medium-scale, large-scale, etc.) in integrated circuits. Many amplifiers commercially available today are based on integrated circuits.\n\nFor special purposes, other active elements have been used. For example, in the early days of the satellite communication, parametric amplifiers were used. The core circuit was a diode whose capacitance was changed by an RF signal created locally. Under certain conditions, this RF signal provided energy that was modulated by the extremely weak satellite signal received at the earth station.\n\nAdvances in digital electronics since the late 20th century provided new alternatives to the traditional linear-gain amplifiers by using digital switching to vary the pulse-shape of fixed amplitude signals, resulting in devices such as the Class-D amplifier.\n\nIn principle, an amplifier is an electrical two-port network that produces a signal at the output port that is a replica of the signal applied to the input port, but increased in magnitude.\n\nThe input port can be idealized as either being a voltage input, which takes no current, with the output proportional to the voltage across the port; or a current input, with no voltage across it, in which the output is proportional to the current through the port. The output port can be idealized as being either a dependent voltage source, with zero source resistance and its output voltage dependent on the input; or a dependent current source, with infinite source resistance and the output current dependent on the input. Combinations of these choices lead to four types of ideal amplifiers. In idealized form they are represented by each of the four types of dependent source used in linear analysis, as shown in the figure, namely:\n\nEach type of amplifier in its ideal form has an ideal input and output resistance that is the same as that of the corresponding dependent source:\n\nIn real amplifiers the ideal impedances are not possible to achieve, but these ideal elements can be used to construct equivalent circuits of real amplifiers by adding impedances (resistance, capacitance and inductance) to the input and output. For any particular circuit, a small-signal analysis is often used to find the actual impedance. A small-signal AC test current \"I\" is applied to the input or output node, all external sources are set to AC zero, and the corresponding alternating voltage \"V\" across the test current source determines the impedance seen at that node as \"R = V / I\".\n\nAmplifiers designed to attach to a transmission line at input and output, especially RF amplifiers, do not fit into this classification approach. Rather than dealing with voltage or current individually, they ideally couple with an input or output impedance matched to the transmission line impedance, that is, match \"ratios\" of voltage to current. Many real RF amplifiers come close to this ideal. Although, for a given appropriate source and load impedance, RF amplifiers can be characterized as amplifying voltage or current, they fundamentally are amplifying power.\n\nAmplifier properties are given by parameters that include: \n\nAmplifiers are described according to the properties of their inputs, their outputs, and how they relate. All amplifiers have gain, a multiplication factor that relates the magnitude of some property of the output signal to a property of the input signal. The gain may be specified as the ratio of output voltage to input voltage (voltage gain), output power to input power (power gain), or some combination of current, voltage, and power. In many cases the property of the output that varies is dependent on the same property of the input, making the gain unitless (though often expressed in decibels (dB)).\n\nMost amplifiers are designed to be linear. That is, they provide constant gain for any normal input level and output signal. If an amplifier's gain is not linear, the output signal can become distorted. There are, however, cases where variable gain is useful. Certain signal processing applications use exponential gain amplifiers.\n\nAmplifiers are usually designed to function well in a specific application, for example: radio and television transmitters and receivers, high-fidelity (\"hi-fi\") stereo equipment, microcomputers and other digital equipment, and guitar and other instrument amplifiers. Every amplifier includes at least one active device, such as a vacuum tube or transistor.\n\nNegative feedback is a technique used in most modern amplifiers to improve bandwidth and distortion and control gain. In a negative feedback amplifier part of the output is fed back and added to the input in opposite phase, subtracting from the input. The main effect is to reduce the overall gain of the system. However, any unwanted signals introduced by the amplifier, such as distortion are also fed back. Since they are not part of the original input, they are added to the input in opposite phase, subtracting them from the input. In this way, negative feedback also reduces nonlinearity, distortion and other errors introduced by the amplifier. Large amounts of negative feedback can reduce errors to the point that the response of the amplifier itself becomes almost irrelevant as long as it has a large gain, and the output performance of the system (the \"closed loop performance\") is defined entirely by the components in the feedback loop. This technique is particularly used with operational amplifiers (op-amps).\n\nNon-feedback amplifiers can only achieve about 1% distortion for audio-frequency signals. With negative feedback, distortion can typically be reduced to 0.001%. Noise, even crossover distortion, can be practically eliminated. Negative feedback also compensates for changing temperatures, and degrading or nonlinear components in the gain stage, but any change or nonlinearity in the components in the feedback loop will affect the output. Indeed, the ability of the feedback loop to define the output is used to make active filter circuits. \n\nAnother advantage of negative feedback is that it extends the bandwidth of the amplifier. The concept of feedback is used in operational amplifiers to precisely define gain, bandwidth, and other parameters entirely based on the components in the feedback loop.\n\nNegative feedback can be applied at each stage of an amplifier to stabilize the operating point of active devices against minor changes in power-supply voltage or device characteristics.\n\nSome feedback, positive or negative, is unavoidable and often undesirable—introduced, for example, by parasitic elements, such as inherent capacitance between input and output of devices such as transistors, and capacitive coupling of external wiring. Excessive frequency-dependent positive feedback can produce parasitic oscillation and turn an amplifier into an oscillator.\n\nAll amplifiers include some form of active device: this is the device that does the actual amplification. The active device can be a vacuum tube, discrete solid state component, such as a single transistor, or part of an integrated circuit, as in an op-amp).\n\nTransistor amplifiers (or solid state amplifiers) are the most common type of amplifier in use today. A transistor is used as the active element. The gain of the amplifier is determined by the properties of the transistor itself as well as the circuit it is contained within.\n\nCommon active devices in transistor amplifiers include bipolar junction transistors (BJTs) and metal oxide semiconductor field-effect transistors (MOSFETs).\n\nApplications are numerous, some common examples are audio amplifiers in a home stereo or public address system, RF high power generation for semiconductor equipment, to RF and microwave applications such as radio transmitters.\n\nTransistor-based amplification can be realized using various configurations: for example a bipolar junction transistor can realize common base, common collector or common emitter amplification; a MOSFET can realize common gate, common source or common drain amplification. Each configuration has different characteristics.\n\nVacuum-tube amplifiers (also known as tube amplifiers or valve amplifiers) use a vacuum tube as the active device. While semiconductor amplifiers have largely displaced valve amplifiers for low-power applications, valve amplifiers can be much more cost effective in high power applications such as radar, countermeasures equipment, and communications equipment. Many microwave amplifiers are specially designed valve amplifiers, such as the klystron, gyrotron, traveling wave tube, and crossed-field amplifier, and these microwave valves provide much greater single-device power output at microwave frequencies than solid-state devices. Vacuum tubes remain in use in some high end audio equipment, as well as in musical instrument amplifiers, due to a preference for \"tube sound\".\n\nMagnetic amplifiers are devices somewhat similar to a transformer where one winding is used to control the saturation of a magnetic core and hence alter the impedance of the other winding.\n\nThey have largely fallen out of use due to development in semiconductor amplifiers but are still useful in HVDC control, and in nuclear power control circuitry due to not being affected by radioactivity.\n\nNegative resistances can be used as amplifiers, such as the tunnel diode amplifier.\n\nA power amplifier is an amplifier designed primarily to increase the power available to a load. In practice, amplifier power gain depends on the source and load impedances, as well as the inherent voltage and current gain. A radio frequency (RF) amplifier design typically optimizes impedances for power transfer, while audio and instrumentation amplifier designs normally optimize input and output impedance for least loading and highest signal integrity. An amplifier that is said to have a gain of 20 dB might have a voltage gain of 20 dB and an available power gain of much more than 20 dB (power ratio of 100)—yet actually deliver a much lower power gain if, for example, the input is from a 600 Ω microphone and the output connects to a 47 kΩ input socket for a power amplifier. In general the power amplifier is the last 'amplifier' or actual circuit in a signal chain (the output stage) and is the amplifier stage that requires attention to power efficiency. Efficiency considerations lead to the various classes of power amplifier based on the biasing of the output transistors or tubes: see power amplifier classes below.\n\nAudio power amplifiers are typically used to drive loudspeakers. They will often have two output channels and deliver equal power to each. An RF power amplifier is found in radio transmitter final stages. A Servo motor controller: amplifies a control voltage to adjust the speed of a motor, or the position of a motorized system.\n\nAn operational amplifier is an amplifier circuit which typically has very high open loop gain and differential inputs. Op amps have become very widely used as standardized \"gain blocks\" in circuits due to their versatility; their gain, bandwidth and other characteristics can be controlled by feedback through an external circuit. Though the term today commonly applies to integrated circuits, the original operational amplifier design used valves, and later designs used discrete transistor circuits.\n\nA fully differential amplifier is similar to the operational amplifier, but also has differential outputs. These are usually constructed using BJTs or FETs.\n\nThese use balanced transmission lines to separate individual single stage amplifiers, the outputs of which are summed by the same transmission line. The transmission line is a balanced type with the input at one end and on one side only of the balanced transmission line and the output at the opposite end is also the opposite side of the balanced transmission line. The gain of each stage adds linearly to the output rather than multiplies one on the other as in a cascade configuration. This allows a higher bandwidth to be achieved than could otherwise be realised even with the same gain stage elements.\n\nThese nonlinear amplifiers have much higher efficiencies than linear amps, and are used where the power saving justifies the extra complexity. Class-D amplifiers are the main example of this type of amplification.\n\nVideo amplifiers are designed to process video signals and have varying bandwidths depending on whether the video signal is for SDTV, EDTV, HDTV 720p or 1080i/p etc.. The specification of the bandwidth itself depends on what kind of filter is used—and at which point ( or for example) the bandwidth is measured. Certain requirements for step response and overshoot are necessary for an acceptable TV image.\n\nTraveling wave tube amplifiers (TWTAs) are used for high power amplification at low microwave frequencies. They typically can amplify across a broad spectrum of frequencies; however, they are usually not as tunable as klystrons.\n\nKlystrons are specialized linear-beam vacuum-devices, designed to provide high power, widely tunable amplification of millimetre and sub-millimetre waves. Klystrons are designed for large scale operations and despite having a narrower bandwidth than TWTAs, they have the advantage of coherently amplifying a reference signal so its output may be precisely controlled in amplitude, frequency and phase.\n\nSolid-state devices are used such as GaAs FETs, IMPATT diodes, and others, especially at lower microwave frequencies and power levels on the order of watts.\n\nThe maser is a non-electronic microwave amplifier.\n\nInstrument amplifiers are a range of audio power amplifiers used to increase the sound level of musical instruments, for example guitars, during performances.\n\nOne set of classifications for amplifiers is based on which device terminal is common to both the input and the output circuit. In the case of bipolar junction transistors, the three classes are common emitter, common base, and common collector. For field-effect transistors, the corresponding configurations are common source, common gate, and common drain; for vacuum tubes, common cathode, common grid, and common plate.\n\nThe common emitter (or common source, common cathode, etc.) is most often configured to provide amplification of a voltage applied between base and emitter, and the output signal taken between collector and emitter is inverted, relative to the input. The common collector arrangement applies the input voltage between base and collector, and to take the output voltage between emitter and collector. This causes negative feedback, and the output voltage tends to follow the input voltage. This arrangement is also used as the input presents a high impedance and does not load the signal source, though the voltage amplification is less than one. The common-collector circuit is, therefore, better known as an emitter follower, source follower, or cathode follower.\n\nAn amplifier whose output exhibits no feedback to its input side is described as 'unilateral'. The input impedance of a unilateral amplifier is independent of load, and output impedance is independent of signal source impedance.\n\nAn amplifier that uses feedback to connect part of the output back to the input is a \"bilateral\" amplifier. Bilateral amplifier input impedance depends on the load, and output impedance on the signal source impedance.\nAll amplifiers are bilateral to some degree; however they may often be modeled as unilateral under operating conditions where feedback is small enough to neglect for most purposes, simplifying analysis (see the common base article for an example).\n\nAnother way to classify amplifiers is by the phase relationship of the input signal to the output signal. An 'inverting' amplifier produces an output 180 degrees out of phase with the input signal (that is, a polarity inversion or mirror image of the input as seen on an oscilloscope). A 'non-inverting' amplifier maintains the phase of the input signal waveforms. An emitter follower is a type of non-inverting amplifier, indicating that the signal at the emitter of a transistor is following (that is, matching with unity gain but perhaps an offset) the input signal. Voltage follower is also non inverting type of amplifier having unity gain.\n\nThis description can apply to a single stage of an amplifier, or to a complete amplifier system.\n\nOther amplifiers may be classified by their function or output characteristics. These functional descriptions usually apply to complete amplifier systems or sub-systems and rarely to individual stages.\n\nAmplifiers are sometimes classified by the coupling method of the signal at the input, output, or between stages. Different types of these include:\n\nDepending on the frequency range and other properties amplifiers are designed according to different principles.\n\nFrequency ranges down to DC are only used when this property is needed. Amplifiers for direct current signals are vulnerable to minor variations in the properties of components with time. Special methods, such as chopper stabilized amplifiers are used to prevent objectionable drift in the amplifier's properties for DC. \"DC-blocking\" capacitors can be added to remove DC and sub-sonic frequencies from audio amplifiers.\n\nDepending on the frequency range specified different design principles must be used. Up to the MHz range only \"discrete\" properties need be considered; e.g., a terminal has an input impedance.\n\nAs soon as any connection within the circuit gets longer than perhaps 1% of the wavelength of the highest specified frequency (e.g., at 100 MHz the wavelength is 3 m, so the critical connection length is approx. 3 cm) design properties radically change. For example, a specified length and width of a PCB trace can be used as a selective or impedance-matching entity.\nAbove a few hundred MHz, it gets difficult to use discrete elements, especially inductors. In most cases, PCB traces of very closely defined shapes are used instead (stripline techniques).\n\nThe frequency range handled by an amplifier might be specified in terms of bandwidth (normally implying a response that is 3 dB down when the frequency reaches the specified bandwidth), or by specifying a frequency response that is within a certain number of decibels between a lower and an upper frequency (e.g. \"20 Hz to 20 kHz plus or minus 1 dB\").\n\nPower amplifier circuits (output stages) are classified as A, B, AB and C for analog designs—and class D and E for switching designs. The power amplifier classes are based on the proportion of each input cycle (conduction angle) during which an amplifying device passes current. The image of the conduction angle derives from amplifying a sinusoidal signal. If the device is always on, the conducting angle is 360°. If it is on for only half of each cycle, the angle is 180°. The angle of flow is closely related to the amplifier power efficiency.\n\nThe practical amplifier circuit to the right could be the basis for a moderate-power audio amplifier. It features a typical (though substantially simplified) design as found in modern amplifiers, with a class-AB push–pull output stage, and uses some overall negative feedback. Bipolar transistors are shown, but this design would also be realizable with FETs or valves.\n\nThe input signal is coupled through capacitor C1 to the base of transistor Q1. The capacitor allows the AC signal to pass, but blocks the DC bias voltage established by resistors R1 and R2 so that any preceding circuit is not affected by it. Q1 and Q2 form a differential amplifier (an amplifier that multiplies the difference between two inputs by some constant), in an arrangement known as a long-tailed pair. This arrangement is used to conveniently allow the use of negative feedback, which is fed from the output to Q2 via R7 and R8.\n\nThe negative feedback into the difference amplifier allows the amplifier to compare the input to the actual output. The amplified signal from Q1 is directly fed to the second stage, Q3, which is a common emitter stage that provides further amplification of the signal and the DC bias for the output stages, Q4 and Q5. R6 provides the load for Q3 (a better design would probably use some form of active load here, such as a constant-current sink). So far, all of the amplifier is operating in class A. The output pair are arranged in class-AB push–pull, also called a complementary pair. They provide the majority of the current amplification (while consuming low quiescent current) and directly drive the load, connected via DC-blocking capacitor C2. The diodes D1 and D2 provide a small amount of constant voltage bias for the output pair, just biasing them into the conducting state so that crossover distortion is minimized. That is, the diodes push the output stage firmly into class-AB mode (assuming that the base-emitter drop of the output transistors is reduced by heat dissipation).\n\nThis design is simple, but a good basis for a practical design because it automatically stabilises its operating point, since feedback internally operates from DC up through the audio range and beyond. Further circuit elements would probably be found in a real design that would roll-off the frequency response above the needed range to prevent the possibility of unwanted oscillation. Also, the use of fixed diode bias as shown here can cause problems if the diodes are not both electrically and thermally matched to the output transistors if the output transistors turn on too much, they can easily overheat and destroy themselves, as the full current from the power supply is not limited at this stage.\n\nA common solution to help stabilise the output devices is to include some emitter resistors, typically one ohm or so. Calculating the values of the circuit's resistors and capacitors is done based on the components employed and the intended use of the amp.\n\nAny real amplifier is an imperfect realization of an ideal amplifier. An important limitation of a real amplifier is that the output it generates is ultimately limited by the power available from the power supply. An amplifier saturates and clips the output if the input signal becomes too large for the amplifier to reproduce or exceeds operational limits for the device. The power supply may influence the output, so must be considered in the design. The power output from an amplifier cannot exceed its input power.\n\nThe amplifier circuit has an \"open loop\" performance. This is described by various parameters (gain, slew rate, output impedance, distortion, bandwidth, signal-to-noise ratio, etc.). Many modern amplifiers use negative feedback techniques to hold the gain at the desired value and reduce distortion. Negative loop feedback has the intended effect of lowering the output impedance and thereby increasing electrical damping of loudspeaker motion at and near the resonance frequency of the speaker.\n\nWhen assessing rated amplifier power output, it is useful to consider the applied load, the signal type (e.g., speech or music), required power output duration (i.e., short-time or continuous), and required dynamic range (e.g., recorded or live audio). In high-powered audio applications that require long cables to the load (e.g., cinemas and shopping centres) it may be more efficient to connect to the load at line output voltage, with matching transformers at source and loads. This avoids long runs of heavy speaker cables.\n\nTo prevent instability or overheating requires care to ensure solid state amplifiers are adequately loaded. Most have a rated minimum load impedance.\n\nAll amplifiers generate heat through electrical losses. The amplifier must dissipate this heat via convection or forced air cooling. Heat can damage or reduce electronic component service life. Designers and installers must also consider heating effects on adjacent equipment.\n\nDifferent power supply types result in many different methods of bias. Bias is a technique by which active devices are set to operate in a particular region, or by which the DC component of the output signal is set to the midpoint between the maximum voltages available from the power supply. Most amplifiers use several devices at each stage; they are typically matched in specifications except for polarity. Matched inverted polarity devices are called complementary pairs. Class-A amplifiers generally use only one device, unless the power supply is set to provide both positive and negative voltages, in which case a dual device symmetrical design may be used. Class-C amplifiers, by definition, use a single polarity supply.\n\nAmplifiers often have multiple stages in cascade to increase gain. Each stage of these designs may be a different type of amp to suit the needs of that stage. For instance, the first stage might be a class-A stage, feeding a class-AB push–pull second stage, which then drives a class-G final output stage, taking advantage of the strengths of each type, while minimizing their weaknesses.\n\n"}
{"id": "49044242", "url": "https://en.wikipedia.org/wiki?curid=49044242", "title": "Ashes of the Singularity", "text": "Ashes of the Singularity\n\nAshes of the Singularity is a real-time strategy video game developed by Oxide Games and Stardock Entertainment. The game was released for Microsoft Windows on March 31, 2016.\n\nOn November 10, 2016, Ashes of the Singularity: Escalation was released. It is a standalone expansion that adds to the base game with more units, maps, and structures, as well as several interface tweaks. The total player count was also increased from 8 to 16 players. The expansion was later merged into the base game on February 16, 2017, after it became apparent that the separate games divided the player community.\n\n\"Ashes of the Singularity\" is a real-time strategy video game. Its main distinction is its ability to handle thousands of individual units engaging in combat simultaneously, far greater than most other games of its kind, across large maps and without abstraction. This is achieved through a newly developed engine called \"Nitrous\" designed to fully leverage modern 64-bit multi-core processors, reflected in the relatively high system requirements (which include a quad-core processor). To allow players to effectively control such large numbers of units, groups of individual units can be combined into \"meta-units\" which operate in a cohesive manner, upon which complex strategies can be developed.\n\n\"Ashes of the Singularity\" was the first video game released with DirectX 12 support. It is also one of the first to support Vulkan. An in-development version of the game was released commercially via Steam Early Access on October 22, 2015. The full version of the game was released on Windows on March 31, 2016.\n\nBecause of the game's early DirectX 12 support and extensive use of parallel computation, it is commonly used as a benchmark. Controversy erupted when Nvidia GPUs were found to perform poorly relative to their AMD counterparts on early beta versions of \"Ashes\"; this was due to the game's use of asynchronous compute and shading features which are implemented in hardware on AMD Graphics Core Next GPUs but had to be performed in software on Nvidia GPUs.\n\n\"Ashes of the Singularity\" received an \"average\" reception from critics according to aggregate review website Metacritic.\n\nGiving it a good score of 7.7, IGN wrote: \"This is a warzone where the shrewd general looking at the bigger picture will triumph over the fast-thinking ace with lightning hotkeys.\"\n\nPC Gamer gave it a score of 75/100. GameSpot gave it a mixed review.\n"}
{"id": "859981", "url": "https://en.wikipedia.org/wiki?curid=859981", "title": "Biochip", "text": "Biochip\n\nIn molecular biology, biochips are essentially miniaturized laboratories that can perform hundreds or thousands of simultaneous biochemical reactions. Biochips enable researchers to quickly screen large numbers of biological analytes for a variety of purposes, from disease diagnosis to detection of bioterrorism agents. Digital microfluidic biochips have become one of the most promising technologies in many biomedical fields. In a digital microfluidic biochip, a group of (adjacent) cells in the microfluidic array can be configured to work as storage, functional operations, as well as for transporting fluid droplets dynamically.\n\nThe development started with early work on the underlying sensor technology. One of the first portable, chemistry-based sensors was the glass pH electrode, invented in 1922 by Hughes. In subsequent years. For example, a K sensor was produced by incorporating valinomycin into a thin membrane.\n\nIn 1953, Watson and Crick announced their discovery of the now familiar double helix structure of DNA molecules and set the stage for genetics research that continues to the present day. The development of sequencing techniques in 1977 by Gilbert and Sanger (working separately) enabled researchers to directly read the genetic codes that provide instructions for protein synthesis. This research showed how hybridization of complementary single oligonucleotide strands could be used as a basis for DNA sensing. Two additional developments enabled the technology used in modern DNA-based. First, in 1983 Kary Mullis invented the polymerase chain reaction (PCR) technique, a method for amplifying DNA concentrations. This discovery made possible the detection of extremely small quantities of DNA in samples. Secondly in 1986 Hood and co-workers devised a method to label DNA molecules with fluorescent tags instead of radiolabels, thus enabling hybridization experiments to be observed optically.\nFigure 1 shows the make up of a typical biochip platform. The actual sensing component (or \"chip\") is just one piece of a complete analysis system. Transduction must be done to translate the actual sensing event (DNA binding, oxidation/reduction, \"etc.\") into a format understandable by a computer (voltage, light intensity, mass, \"etc.\"), which then enables additional analysis and processing to produce a final, human-readable output. The multiple technologies needed to make a successful biochip—from sensing chemistry, to microarraying, to signal processing—require a true multidisciplinary approach, making the barrier to entry steep. One of the first commercial biochips was introduced by Affymetrix. Their \"GeneChip\" products contain thousands of individual DNA sensors for use in sensing defects, or single nucleotide polymorphisms (SNPs), in genes such as p53 (a tumor suppressor) and BRCA1 and BRCA2 (related to breast cancer). The chips are produced using microlithography techniques traditionally used to fabricate integrated circuits (see below).\n\nThe microarray—the dense, two-dimensional grid of biosensors—is the critical component of a biochip platform. Typically, the sensors are deposited on a flat substrate, which may either be passive (\"e.g.\" silicon or glass) or active, the latter\nconsisting of integrated electronics or micromechanical devices that perform or assist signal transduction. Surface chemistry is used to covalently bind the sensor molecules to the substrate medium. The fabrication of microarrays is non-trivial and is a major economic and technological hurdle that may\nultimately decide the success of future biochip platforms. The primary manufacturing challenge is the process of placing each sensor at a specific position (typically on a Cartesian grid) on the substrate. Various means exist to achieve the placement, but typically robotic micro-pipetting or micro-printing systems are used to place tiny spots of sensor material on the chip surface. Because each sensor is unique, only a few spots can be placed at a time. The low-throughput nature of this\nprocess results in high manufacturing costs.\n\nFodor and colleagues developed a unique fabrication process (later used by Affymetrix) in which a series of microlithography steps is used to combinatorially synthesize hundreds of thousands of unique, single-stranded DNA sensors on a substrate one nucleotide at a time. One lithography step is needed per base type; thus, a total of four steps is required per nucleotide level. Although this technique is very powerful in that many sensors can be created simultaneously, it is currently only feasible for creating short DNA strands (15–25 nucleotides). Reliability and cost factors limit the number of photolithography steps that can be done. Furthermore, light-directed combinatorial synthesis techniques are not currently possible for proteins or other sensing molecules.\n\nAs noted above, most microarrays consist of a Cartesian grid of sensors. This approach is used chiefly to map or \"encode\" the coordinate of each sensor to its function. Sensors in these arrays typically use a universal signalling technique (\"e.g.\" fluorescence), thus making coordinates their only identifying feature. These arrays must be made using a serial process (\"i.e.\" requiring multiple, sequential steps) to ensure that each sensor is placed at the correct position.\n\n\"Random\" fabrication, in which the sensors are placed at arbitrary positions on the chip, is an alternative to the serial method. The tedious and expensive positioning process is\nnot required, enabling the use of parallelized self-assembly techniques. In this approach, large batches of identical sensors can be produced; sensors from each batch are then combined and assembled into an array. A non-coordinate based encoding scheme must be used to identify each sensor. As the figure shows, such a design was first demonstrated (and later commercialized by Illumina) using functionalized beads placed randomly in the wells of an etched fiber optic cable. Each bead was uniquely encoded with a fluorescent signature. However, this encoding scheme is limited in the number of unique dye combinations that can be used and successfully differentiated.\n\nMicroarrays are not limited to DNA analysis; protein microarrays, antibody microarray, chemical compound microarray can also be produced using biochips. Randox Laboratories Ltd. launched Evidence, the first protein Biochip Array Technology analyzer in 2003. In protein Biochip Array Technology, the biochip replaces the ELISA plate or cuvette as the reaction platform. The biochip is used to simultaneously analyze a panel of related tests in a single sample, producing a patient profile. The patient profile can be used in disease screening, diagnosis, monitoring disease progression or monitoring treatment. Performing multiple analyses simultaneously, described as multiplexing, allows a significant reduction in processing time and the amount of patient sample required. Biochip Array Technology is a novel application of a familiar methodology, using sandwich, competitive and antibody-capture immunoassays. The difference from conventional immunoassays is that, the capture ligands are covalently attached to the surface of the biochip in an ordered array rather than in solution.\n\nIn sandwich assays an enzyme-labelled antibody is used; in competitive assays an enzyme-labelled antigen is used. On antibody-antigen binding a chemiluminescence reaction produces light. Detection is by a charge-coupled device (CCD) camera. The CCD camera is a sensitive and high-resolution sensor able to accurately detect and quantify very low levels of light. The test regions are located using a grid pattern then the chemiluminescence signals are analysed by imaging software to rapidly and simultaneously quantify the individual analytes.\n\nBiochips are also used in the field of microphysiometry e.g. in skin-on-a-chip applications.\n\nFor details about other array technologies, see Antibody microarray.\n\n"}
{"id": "18652174", "url": "https://en.wikipedia.org/wiki?curid=18652174", "title": "Cab Secure Radio", "text": "Cab Secure Radio\n\nCab Secure Radio (CSR) was an in-cab analogue radiotelephone system formerly used on parts of the British railway network. Its main function was to provide a secure speech link between the train driver and the signaller which could not be overheard by other train drivers. In areas where CSR was used it had to be the primary method of communication between driver and signaller, always being used in preference to the \"signal post telephone\". CSR was replaced by the GSM-R digital system, forming the initial phase of rollout of ERTMS throughout the UK.\n\nCSR was first introduced in the Glasgow area in 1986 to enable driver-only operation of trains. It was later used in the London and Liverpool areas.\n\nThe driver initialised the CSR with an area code followed by the identification number of the signal in front of the train. The radio then automatically sent the stock number of the train (e.g., 455112), to the signalling system. The signaller then allocated a train reporting number (e.g., 2M34) to the train. This reporting number was used by the signalling equipment to track the progress of the train as it moves through the rail network.\n\nEach signalling area (or signal panel within a larger signalbox) used a different radio channel corresponding with an area code. As the train passed from the jurisdiction of one signaller to another, the CSR in the driver's cab changed channel automatically. Lineside signs were provided to inform the driver of where the area code changes. Occasionally the radio failed to change channel or lost the signal completely, in which case the driver could enter the code manually. Lineside signs indicated the change of the CSR radio channel area.\n\nCSR cab equipment was either the Stornophone 6000 or a Siemens model.\n\n"}
{"id": "57195775", "url": "https://en.wikipedia.org/wiki?curid=57195775", "title": "Clarke–Riley diffusion flame", "text": "Clarke–Riley diffusion flame\n\nIn combustion, Clarke–Riley diffusion flame is a diffusion flame that develops inside a naturally convected boundary layer on a hot fuel surface with quiescent oxidizer environment, first studied and experimentally verified by John Frederick Clarke and Norman Riley in 1976. This problem is an extension of Emmons problem.\n\n"}
{"id": "296058", "url": "https://en.wikipedia.org/wiki?curid=296058", "title": "Coherer", "text": "Coherer\n\nThe coherer was a primitive form of radio signal detector used in the first radio receivers during the wireless telegraphy era at the beginning of the 20th century. Its use in radio was based on the 1890 findings of French physicist Edouard Branly and adapted by other physicists and inventors over the next ten years. The device consists of a tube or capsule containing two electrodes spaced a small distance apart with loose metal filings in the space between. When a radio frequency signal is applied to the device, the metal particles would cling together or \"cohere\", reducing the initial high resistance of the device, thereby allowing a much greater direct current to flow through it. In a receiver, the current would activate a bell, or a Morse paper tape recorder to make a record of the received signal. The metal filings in the coherer remained conductive after the signal (pulse) ended so that the coherer had to be \"decohered\" by tapping it with a clapper actuated by an electromagnet, each time a signal was received, thereby restoring the coherer to its original state. Coherers remained in widespread use until about 1907, when they were replaced by more sensitive electrolytic and crystal detectors.\n\nThe behavior of particles or metal filings in the presence of electricity or electric sparks was noticed in many experiments well before Edouard Branly's 1890 paper and even before there was proof of the theory of electromagnetism. In 1835 Swedish scientist Peter Samuel Munk noticed a change of resistance in a mixture of metal filings in the presence of spark discharge from a Leyden jar. In 1850 Pierre Guitard found that when dusty air was electrified, the particles would tend to collect in the form of strings. The idea that particles could react to electricity was used in English engineer Samuel Alfred Varley's 1866 lightning bridge, a lightning arrester attached to telegraph lines consisting of a piece of wood with two metal spikes extending into a chamber. The space was filled with powdered carbon that would not allow the low voltage telegraph signals to pass through but it would conduct and ground a high voltage lightning strike. In 1879 the Welsh scientist David Edward Hughes found that loose contacts between a carbon rod and two carbon blocks as well as the metallic granules in a microphone he was developing responded to sparks generated in a nearby apparatus. Temistocle Calzecchi-Onesti in Italy began studying the anomalous change in the resistance of thin metallic films and metal particles at Fermo/Monterubbiano. He found that copper filings between two brass plates would cling together, becoming conductive, when he applied a voltage to them. He also found that other types of metal filings would have the same reaction to electric sparks occurring at a distance, a phenomenon that he thought could be used for detecting lightning strikes. Calzecchi-Onesti's papers were published in il Nuovo Cimento in 1884, 1885 and 1886.\n\nIn 1890, French physicist Edouard Branly published \"On the Changes in Resistance of Bodies under Different Electrical Conditions\" in a French Journal where he described his thorough investigation of the effect of minute electrical charges on metal and many types of metal filings. In one type of circuit, filings were placed in a tube of glass or ebonite, held between two metal plates. When an electric discharge was produced in the neighbourhood of the circuit, a large deviation was seen on the attached galvanometer needle. He noted the filings in the tube would react to the electric discharge even when the tube was placed in another room 20 yards away. Branly went on to devise many types of these devices based on \"imperfect\" metal contacts. Branly's filings tube came to light in 1892 in Great Britain when it was described by Dr. Dawson Turner at a meeting of the British Association in Edinburgh. The Scottish electrical engineer and astronomer George Forbes suggested that Branly's filings tube might be reacting in the presence of Hertzian waves, a type of air-borne electromagnetic radiation proven to exist by German physicist Heinrich Hertz (later called radio waves).\n\nIn 1893 physicist W.B. Croft exhibited Branly's experiments at a meeting of the Physical Society in London. It was unclear to Croft and others whether the filings in the Branly tube were reacting to sparks or the light from the sparks. George Minchin noticed the Branly tube might be reacting to Hertzian waves the same way his solar cell did and wrote the paper \"The Action of Electromagnetic Radiation on Films containing Metallic Powders\". These papers were read by English physicist Oliver Lodge who saw this as a way to build a much improved Hertzian wave detector. On 1 June 1894, a few months after the death of Heinrich Hertz, Oliver Lodge delivered a memorial lecture on Hertz where he demonstrated the properties of \"Hertzian waves\" (radio), including transmitting them over a short distance, using an improved version of Branly's filings tube, which Lodge had named the \"coherer\", as a detector. In May 1895, after reading about Lodge's demonstrations, the Russian physicist Alexander Popov built a \"Hertzian wave\" (radio wave) based lightning detector using a coherer. That same year, Italian inventor Guglielmo Marconi demonstrated a wireless telegraphy system using Hertzian waves (radio), based on a coherer.\n\nThe coherer was replaced in receivers by the simpler and more sensitive electrolytic and crystal detectors around 1907, and became obsolete.\n\nOne minor use of the coherer in modern times was by Japanese tin-plate toy manufacturer Matsudaya Toy Co. who beginning 1957 used a spark-gap transmitter and coherer-based receiver in a range of radio-controlled (RC) toys, called Radicon (abbreviation for Radio-Controlled) toys. Several different types using the same RC system were commercially sold, including a Radicon Boat (very rare), Radicon Oldsmobile Car (rare) and a Radicon Bus (the most popular).\n\nUnlike modern AM radio stations that transmit a continuous radio frequency, whose amplitude (power) is modulated by an audio signal, the first radio transmitters transmitted information by wireless telegraphy (radiotelegraphy), the transmitter was turned on and off (on-off keying) to produce different length pulses of unmodulated carrier wave signal, \"dots\" and \"dashes\", that spelled out text messages in Morse code. As a result, early radio receiving apparatus merely had to detect the presence or absence of the radio signal, not convert it to audio. The device that did this was called a detector. The coherer was the most successful of many detector devices that were tried in the early days of radio.\n\nThe basis for the operation of the coherer is that metal particles cohere (cling together) and conduct electricity much better after being subjected to radio frequency electricity. The radio signal from the antenna was applied directly across the coherer's electrodes. When the radio signal from a \"dot\" or \"dash\" came in, the coherer would become conductive. The coherer's electrodes were also attached to a DC circuit powered by a battery that created a \"click\" sound in earphones or a telegraph sounder, or a mark on a paper tape, to record the signal. Unfortunately, the reduction in the coherer's electrical resistance persisted after the radio signal was removed. This was a problem because the coherer had to be ready immediately to receive the next \"dot\" or \"dash\". Therefore, a decoherer mechanism was added to tap the coherer, mechanically disturbing the particles to reset it to the high resistance state.\n\nCoherence of particles by radio waves is an obscure phenomenon that is not well understood even today. Recent experiments with particle coherers seem to have confirmed the hypothesis that the particles cohere by a micro-weld phenomenon caused by radio frequency electricity flowing across the small contact area between particles. The underlying principle of so-called \"imperfect contact\" coherers is also not well understood, but may involve a kind of tunneling of charge carriers across an imperfect junction between conductors.\n\nThe coherer used in practical receivers was a glass tube, sometimes evacuated, which was about half filled with sharply cut metal filings, often part silver and part nickel. Silver electrodes made contact with the metal particles on both ends. In some coherers, the electrodes were slanted so the width of the gap occupied by the filings could be varied by rotating the tube about its long axis, thus adjusting its sensitivity to the prevailing conditions.\n\nIn operation, the coherer is included in two separate electrical circuits. One is the antenna-ground circuit shown in the untuned receiver circuit diagram below. The other is the battery-sounder relay circuit including battery \"B1\" and relay \"R\" in the diagram. A radio signal from the antenna-ground circuit \"turns on\" the coherer, enabling current flow in the battery-sounder circuit, activating the sounder, \"S\". The coils, \"L\", act as RF chokes to prevent the RF signal power from leaking away through the relay circuit.\n\nOne electrode, \"A\", of the coherer, (\"C\", in the left diagram) is connected to the antenna and the other electrode, \"B\", to ground. A series combination of a battery, \"B1\", and a relay, \"R\", is also attached to the two electrodes. When the signal from a spark gap transmitter is received, the filings tend to cling to each other, reducing the resistance of the coherer. When the coherer conducts better, battery \"B1\" supplies enough current through the coherer to activate relay \"R\", which connects battery \"B2\" to the telegraph sounder \"S\", giving an audible click. In some applications, a pair of headphones replaced the telegraph sounder, being much more sensitive to weak signals, or a Morse recorder which recorded the dots and dashes of the signal on paper tape.\n\nThe problem of the filings continuing to cling together and conduct after the removal of the signal was solved by tapping or shaking the coherer after the arrival of each signal, shaking the filings and raising the resistance of the coherer to the original value. This apparatus was called a decoherer. This process was referred to as 'decohering' the device and was subject to much innovation during the life of the popular use of this component. Tesla, for example, invented a coherer in which the tube rotated continually along its axis.\n\nIn later practical receivers the decoherer was a clapper similar to an electric bell, operated by an electromagnet powered by the coherer current itself. When the radio wave turned on the coherer, the DC current from the battery flowed through the electromagnet, pulling the arm over to give the coherer a tap. This returned the coherer to the nonconductive state, turning off the electromagnet current, and the arm sprang back. If the radio signal was still present, the coherer would immediately turn on again, pulling the clapper over to give it another tap, which would turn it off again. The result was a constant \"trembling\" of the clapper during the period that the radio signal was on, during the \"dots\" and \"dashes\" of the Morse code signal.\n\nThere are several variations of what is known as the imperfect junction coherer. The principle of operation (microwelding) suggested above for the filings coherer may be less likely to apply to this type because there is no need for decohering. An iron and mercury variation on this device was used by Marconi for the first transatlantic radio message. An earlier form was invented by Jagdish Chandra Bose in 1899. The device consisted of a small metallic cup containing a pool of mercury covered by a very thin insulating film of oil; above the surface of the oil, a small iron disc is suspended. By means of an adjusting screw the lower edge of the disc is made to touch the oil-covered mercury with a pressure small enough not to puncture the film of oil. Its principle of operation is not well understood. The action of detection occurs when the radio frequency signal somehow breaks down the insulating film of oil, allowing the device to conduct, operating the receiving sounder wired in series. This form of coherer is self-restoring and needs no decohering.\n\nIn 1899, Bose announced the development of an \"\"iron-mercury-iron coherer with telephone detector\" in a paper presented at the Royal Society, London. He also later received , \"Detector for electrical disturbances\"\" (1904), for a specific electromagnetic receiver.\n\nCoherers have difficulty discriminating between the impulsive signals of spark-gap transmitters, and other impulsive electrical noise:\n\nCoherers were also finicky to adjust and not very sensitive. Another problem was that, because of the cumbersome mechanical \"decohering\" mechanism, the coherer was limited to a receiving speed of 12 - 15 words per minute of Morse code, while telegraph operators could send at rates of 50 WPM, and paper tape machines at 100 WPM.\n\nMore important for the future, the coherer could not detect AM (radio) transmissions. As a simple switch that registered the presence or absence of radio waves, the coherer could detect the on-off keying of wireless telegraphy transmitters, but it could not demodulate (rectify) the waveforms of AM radiotelephone signals, which began to be experimented with in the first years of the 20th century. This problem was solved by the rectification capability of Reginald Fessenden's hot wire barretter and electrolytic detector. These were replaced by the crystal detector around 1906, and then around 1912 by vacuum tube technologies such as John Ambrose Fleming's thermionic diode and Lee De Forest's Audion (triode) tube.\n\n\n\n"}
{"id": "36108052", "url": "https://en.wikipedia.org/wiki?curid=36108052", "title": "Contextualization (computer science)", "text": "Contextualization (computer science)\n\nIn computer science, contextualization is the process of identifying the data relevant to an entity (e.g., a person or a city) based on the entity's contextual information.\n\nContext or contextual information is any information about any entity that can be used to effectively reduce the amount of reasoning required (via filtering, aggregation, and inference) for decision making within the scope of a specific application. Contextualisation is then the process of identifying the data relevant to an entity based on the entity's contextual information. Contextualisation excludes irrelevant data from consideration and has the potential to reduce data from several aspects including volume, velocity, and variety in large-scale data intensive applications (Yavari et al.).\n\nData Processing, Contextualisation has the potential to reduce the amount of data based on the interests from applications/services/users. \nContextualisation can improve the scalability and efficiency of data process, query, delivery by excluding irrelevant data. As an example, ConTaaS facilitates contextualisation of the data for IoT applications and could improve the processing for large-scale IoT applications from various Big Data aspects including volume, velocity, and variety.\n\n"}
{"id": "41821661", "url": "https://en.wikipedia.org/wiki?curid=41821661", "title": "Dehalogenimonas lykanthroporepellens", "text": "Dehalogenimonas lykanthroporepellens\n\nDehalogenimonas lykanthroporepellens is an anaerobic, Gram-negative bacteria in the phylum Chloroflexi isolated from a Superfund site in Baton Rouge, Louisiana. It is useful in bioremediation for its ability to reductively dehalogenate chlorinated alkanes.\n\n\"Dehalogenimonas lykanthroporepellens\" cells are Gram-negative, non-motile, irregular cocci that are 0.3-0.6 μm in diameter. There is no evidence of pathogenicity. They are mesophiles that can grow in a temperature range of 20-34 °C with their optimum temperature range being 28-34 °C. They grow best in pH 7-7.5 (pH range 6-8, although it was isolated from groundwater of pH 5.1). Growth has been observed in salt concentrations from 0.1-2% NaCl with optimum growth at ≤1%. GC-content reported in characterization of \"D\". \"lykanthroporellens\" is 53.8% as determined by HPLC; however, as determined by genomic analysis, the GC-content is 55.04%. \"D\". \"lyankanthroporepellens\" does not form spores. Resistance to the antibiotics ampicillin and vancomycin has been observed.\n\n\"D\". \"lykanthroporepellens\" is strictly anaerobic and uses hydrogen as an electron donor. It has been cultured in an anaerobic basal medium at 30 °C in the dark. It is able to reductively dehalogenate aliphatic alkanes (non-aromatic alkanes) such as 1,2,3-trichloropropane (reduces it to allyl chloride which abiotically transforms in the presence of water to allyl alcohol).\n\nTwo strains (BL-DC-9 and BL-DC-8) were isolated from a Superfund site in Baton Rouge, Louisiana in 2009 by Moe, Yan, Nobre, Costa, and Rainey—researchers at Louisiana State University and the University of Coimbra (Coimbra, Portugal). A Superfund site is an abandoned site that contains hazardous waste. This site was contaminated with chlorinated solvents.\n\nThe genus name \"Dehalogenimonas\" reflects its ability to dehalogenate chlorinated alkanes. The species name \"lykanthroporepellens\" comes from \"lykanthropos\" meaning werewolf and \"re-pellens\" meaning repelling. The species name refers to the garlic smell of the bacteria when cultured. Some folklore states that garlic can be used to repel creatures like werewolves and vampires.\n\nThere are six classes within the phylum Chloroflexi: Chloroflexi, Anaerolinea, Caldilinea, Dehalococcoidia (previously known informally as Dehalococcoidetes), Ktedonobacteria, and Thermomicrobia. \"D.\" \"lykanthroporepellens\" is in the class Dehalococcoidia. Chloroflexi consists of the green non-sulfur bacteria which are anoxygenic phototrophs (do not produce oxygen during photosynthesis) that use either H or HS as an electron donor. However \"D\". \"lykanthroporepellens\" uses polychlorinated aliphatic alkanes as the electron acceptor. Chloroflexi are the deepest branching (oldest) anoxygenic phototrophs on the tree of life.\n\nMany of the species in Chloroflexi are thermophilic however \"Dehalogenimonas\" \"lykanthroporepellens\" is a mesophile. The \"Oscillochloris\" (Class Chloroflexi) are also mesophilic. Despite this relationship, \"D\". \"lykanthroporepellens\" is more closely related to the \"Dehalococcoides\" (class Dehalococcoidia) with 90% 16S rRNA gene sequence similarity. \"D\". \"lykanthroporepellens\" also differ from other species in the phylum Chloroflexi in that they are not filamentous.\n\n\"Dehalogenimonas\" \"lykanthroporepellens\" is a chemotrophic organism that uses H as an electron donor and polychlorinated aliphatic alkanes as an electron acceptor. These molecules include 1,2,3-trichloropropane, 1,2-dichloropropane, 1,1,2,2-tetrachloroethane, 1,1,2-trichloroethane, and 1,2-dichloroethane. However, there are several chlorinated alkanes that it cannot reduce, such as 1-chloropropane and 2-chloropropane. It uses these compounds as electron acceptors in dihaloelimination reactions. In dihaloelimination the electron donor (H in this case) is used to remove two halogens from adjacent carbons that are double bonded. 1,2,3-trichloropropane is reduced to allyl chloride by \"D\". \"lykanthroporepellens\", and further transformed abiotically to allyl alcohol in the presence of water (other abiotic reactions can occur). The carbon source has not been determined for this species but other organisms within Chloroflexi use CO as a carbon source.\n\nAlthough two strains of \"D\". \"lykanthroporepellens\" have been isolated and characterized, only the type strain BL-DC-9 has had the genome sequenced. Therefore, when referring to \"D\". \"lykanthroporepellens\" in this section, all information is only verified for BL-DC-9. \"D\". \"lykanthroporepellens\" has a circular chromosome consisting of 1,686,510 bp and a G-C content, based on genomic analysis, of 55.04%. The genome was sequenced using both Illumina and 454 sequencing platforms, more specifically an Illumina shotgun library, a 454 draft library, and a paired end 454 library. Illumina sequence data was assembled and combined with assembled 454 data. Initial assembly contained 64 contigs (a set of overlapping DNA) in 1 scaffold (a set of overlapping contigs with known gap lengths). Genes were annotated using a combination of automated and manual curation. 1,771 genes were predicted, in which 1,720 were protein-coding genes and 51 were RNAs. Putative function was designated to nearly 70% of the protein-coding genes.\n\nInterest in \"D\". \"lykanthroporepellens\" stems from its ability to degrade polychlorinated aliphatic alkanes into nonhazardous products. The catalysis of reductive dehalogenation of chlorinated compounds is dependent on the presence and expression of genes coding for reductive dehalogenase enzymes. These genes are organized in \"rdhAB\" operons, which encode the RdhA protein (reductive dehalogenase) and the RdhB protein (membrane anchor). \"D\". \"lykanthroporepellens\" was shown to have several \"rdhA\" and \"rdhB\" genes in the chromosome.\n\nFurthermore, \"D\". \"lykanthroporepellens\" has a prophage region containing 45 hypothetic proteins, which accounts for roughly 4% of the chromosome. An additional ~4.3% of the genome of \"D\". \"lykanthroporepellens\" is made up of insertion sequence elements, which encode for 74 full or truncated transposases. Thus, horizontal gene transfer appears to be a potential mechanism for the adaptation of \"D\". \"lykanthroporepellens\" to its ecological niche.\n\nPolychlorinated aliphatic C2 and C3 alkanes (ethanes and propanes with at least two chlorine substituents) are industrially important chemical intermediates globally produced on a massive scale. Due to spills and past inappropriate disposal methods, these chlorinated compounds are prevalent groundwater and soil contaminants throughout the US and around the world. Bioremediation approaches that rely on the action of anaerobic, reductively-dehalogenating bacteria, such as \"D\". \"lykanthroporepellens\", have shown great promise for clean-up of chlorinated solvent-contaminated soil and groundwater. Using qPCR (quantitative real-time polymerase chain reaction), 16S rRNA gene sequences for \"Dehalogenimonas\" strains have been found to be at concentrations as high as 10 copies/ml of groundwater contaminated with high concentrations of chlorinated solvents and comprise up to nearly 19% of the total bacterial 16S rRNA gene copies. \nThe characterization of \"D\". \"lykanthroporepellens\" has aided in remediation plans through better understanding of the overall process of reductive dehalogenation of chlorinated compounds present in groundwater and the diversity of organisms involved. Due to its close relationship to \"Dehalococcoides\" spp., \"D\". \"lykanthroporepellens\" was found to be amplified by primers that at one time were believed to be specific to targeting \"Dehalococcoides\" spp. Differentiation between the presence of \"Dehalococcoides\" spp. and \"D\". \"lykanthroporepellens\" is important for remediation planning because \"D\". \"lykanthroporepellens\" dehalogenates polychlorinated alkanes, but is unable to dehalogenate chlorinated ethenes like \"Dehalococcoides\" spp. Furthermore, \"D\". \"lykanthroporepellens\" was the first pure culture isolated which could dehalogenate 1,2,3-trichloropropane (1,2,3-TCP) under anaerobic conditions. \"D\". \"lykanthroporepellens\" has also been shown to dehalogenate 1,2-dichloroethane (1,2-DCA), 1,2-dichloropropane (1,2-DCP), and 1,1,2-trichloroethane (1,1,2-TCA) present in mixtures and at concentrations as high as 8.7, 4.0, and 3.8 mM respectively. These findings are important because a large number of contaminated sites contain mixtures of various chlorinated solvents and/or high concentrations.\n"}
{"id": "23170404", "url": "https://en.wikipedia.org/wiki?curid=23170404", "title": "Drum pump", "text": "Drum pump\n\nDrum pump refers to pumps that are used to empty barrels, tanks, IBCs and drums. Many liquids used on manufacturing and processing plants are delivered in 100 or 200 litre barrels and are too heavy to tip to empty the liquids inside. Drum pumps consist of a vertical shaft inside a narrow tube which fits inside the drum opening. The pump motor is attached to the vertical shaft at the top of the tube outside and above the drum and the pumping element is located at the end of the shaft inside the drum. This configuration allows the drum to be emptied without tipping and so reduces the risk of spills and operator injury.\n\nPrimary uses of drum pump are in 55 gallon barrels but they are also used in 5, 15, 30, 55, 275 and 330 gallon containers. Drum pumps come in many forms from simple rotary style to a variety of other styles like lever, siphon, double diaphragm, electric, air, piston, etc.\n\nAlso referred to as Container Pumps, Barrel Pumps and Tote Pumps.\n"}
{"id": "33913133", "url": "https://en.wikipedia.org/wiki?curid=33913133", "title": "E-marketing collateral", "text": "E-marketing collateral\n\nSimilar to marketing collateral definition, e-marketing collateral is the collection of basic internet tactical activities that supports the marketing of a product or a service on the internet. These basic tactics are intended to support, facilitate and ease the internet presence of a website, product or a service.\n\nThese activities enforced themselves as givens nowadays, common e-marketing collateral include:\n\n"}
{"id": "8394754", "url": "https://en.wikipedia.org/wiki?curid=8394754", "title": "Edison Pioneers", "text": "Edison Pioneers\n\nThe Edison Pioneers was an organization composed former employees of Thomas Edison who had worked with the inventor in his early years. Membership was limited to people who had worked closely with Edison before 1885. \nOn February 11, 1918, the Edison Pioneers met for the first time, on the 71st birthday of Edison. There were 37 people at the first meeting.\nEdison himself was not present; it was announced he was \"engaged in important government service\".\nIt was suspected he was working on a military project since World War I was still in progress. The organization had 100 members although in later years descendants of Edison Pioneers were also allowed membership.\n\nMembers of the Edison Pioneers:\n\n\n"}
{"id": "16138478", "url": "https://en.wikipedia.org/wiki?curid=16138478", "title": "Enzymatic biofuel cell", "text": "Enzymatic biofuel cell\n\nAn enzymatic biofuel cell is a specific type of fuel cell that uses enzymes as a catalyst to oxidize its fuel, rather than precious metals. Enzymatic biofuel cells, while currently confined to research facilities, are widely prized for the promise they hold in terms of their relatively inexpensive components and fuels, as well as a potential power source for bionic implants.\n\n Enzymatic biofuel cells work on the same general principles as all fuel cells: use a catalyst to separate electrons from a parent molecule and force it to go around an electrolyte barrier through a wire to generate an electric current. What makes the enzymatic biofuel cell distinct from more conventional fuel cells are the catalysts they use and the fuels that they accept. Whereas most fuel cells use metals like platinum and nickel as catalysts, the enzymatic biofuel cell uses enzymes derived from living cells (although not within living cells; fuel cells that use whole cells to catalyze fuel are called microbial fuel cells). This offers a couple of advantages for enzymatic biofuel cells: Enzymes are relatively easy to mass-produce and so benefit from economies of scale, whereas precious metals must be mined and so have an inelastic supply. Enzymes are also specifically designed to process organic compounds such as sugars and alcohols, which are extremely common in nature. Most organic compounds cannot be used as fuel by fuel cells with metal catalysts because the carbon monoxide formed by the interaction of the carbon molecules with oxygen during the fuel cell’s functioning will quickly “poison” the precious metals that the cell relies on, rendering it useless. Because sugars and other biofuels can be grown and harvested on a massive scale, the fuel for enzymatic biofuel cells is extremely cheap and can be found in nearly any part of the world, thus making it an extraordinarily attractive option from a logistics standpoint, and even more so for those concerned with the adoption of renewable energy sources.\n\nEnzymatic biofuel cells also have operating requirements not shared by traditional fuel cells. What is most significant is that the enzymes that allow the fuel cell to operate must be “immobilized” near the anode and cathode in order to work properly; if not immobilized, the enzymes will diffuse into the cell’s fuel and most of the liberated electrons will not reach the electrodes, compromising its effectiveness. Even with immobilization, a means must also be provided for electrons to be transferred to and from the electrodes. This can be done either directly from the enzyme to the electrode (“direct electron transfer”) or with the aid of other chemicals that transfer electrons from the enzyme to the electrode (“mediated electron transfer”). The former technique is possible only with certain types of enzymes whose activation sites are close to the enzyme’s surface, but doing so presents fewer toxicity risks for fuel cells intended to be used inside the human body. Finally, completely processing the complex fuels used in enzymatic biofuel cells requires a series of different enzymes for each step of the ‘metabolism’ process; producing some of the required enzymes and maintaining them at the required levels can pose problems.\n\nEarly work with biofuel cells, which began in the early 20th century, was purely of the microbial variety. Research on using enzymes directly for oxidation in biofuel cells began in the early 1960s, with the first enzymatic biofuel cell being produced in 1964. This research began as a product of NASA’s interest in finding ways to recycle human waste into usable energy on board spacecraft, as well as a component of the quest for an artificial heart, specifically as a power source that could be put directly into the human body. These two applications – use of animal or vegetable products as fuel and development of a power source that can be directly implanted into the human body without external refueling – remain the primary goals for developing these biofuel cells. Initial results, however, were disappointing. While the early cells did successfully produce electricity, there was difficulty in transporting the electrons liberated from the glucose fuel to the fuel cell’s electrode and further difficulties in keeping the system stable enough to produce electricity at all due to the enzymes’ tendency to move away from where they needed to be in order for the fuel cell to function. These difficulties led to an abandonment by biofuel cell researchers of the enzyme-catalyst model for nearly three decades in favor of the more conventional metal catalysts (principally platinum), which are used in most fuel cells. Research on the subject did not begin again until the 1980s after it was realized that the metallic-catalyst method was not going to be able to deliver the qualities desired in a biofuel cell, and since then work on enzymatic biofuel cells has revolved around the resolution of the various problems that plagued earlier efforts at producing a successful enzymatic biofuel cell.\n\nHowever, many of these problems were resolved in 1998. In that year, it was announced that researchers had managed to completely oxidize methanol using a series (or “cascade”) of enzymes in a biofuel cell. Previous to this time, the enzyme catalysts had failed to completely oxidize the cell’s fuel, delivering far lower amounts of energy than what was expected given what was known about the energy capacity of the fuel. While methanol is now far less relevant in this field as a fuel, the demonstrated method of using a series of enzymes to completely oxidize the cell’s fuel gave researchers a way forward, and much work is now devoted to using similar methods to achieve complete oxidation of more complicated compounds, such as glucose. In addition, and perhaps what is more important, 1998 was the year in which enzyme “immobilization” was successfully demonstrated, which increased the usable life of the methanol fuel cell from just eight hours to over a week. Immobilization also provided researchers with the ability to put earlier discoveries into practice, in particular the discovery of enzymes that can be used to directly transfer electrons from the enzyme to the electrode. This process had been understood since the 1980s but depended heavily on placing the enzyme as close to the electrode as possible, which meant that it was unusable until after immobilization techniques were devised. \nIn addition, developers of enzymatic biofuel cells have applied some of the advances in nanotechnology to their designs, including the use of carbon nanotubes to immobilize enzymes directly. Other research has gone into exploiting some of the strengths of the enzymatic design to dramatically miniaturize the fuel cells, a process that must occur if these cells are ever to be used with implantable devices. One research team took advantage of the extreme selectivity of the enzymes to completely remove the barrier between anode and cathode, which is an absolute requirement in fuel cells not of the enzymatic type. This allowed the team to produce a fuel cell that produces 1.1 microwatts operating at over half a volt in a space of just 0.01 cubic millimeters.\n\nWhile enzymatic biofuel cells are not currently in use outside of the laboratory, as the technology has advanced over the past decade non-academic organizations have shown an increasing amount of interest in practical applications for the devices. In 2007, Sony announced that it had developed an enzymatic biofuel cell that can be linked in sequence and used to power an mp3 player, and in 2010 an engineer employed by the US Army announced that the Defense Department was planning to conduct field trials of its own \"bio-batteries\" in the following year. In explaining their pursuit of the technology, both organizations emphasized the extraordinary abundance (and extraordinarily low expense) of fuel for these cells, a key advantage of the technology that is likely to become even more attractive if the price of portable energy sources goes up, or if they can be successfully integrated into electronic human implants.\n\nWith respect to fuel cells, enzymes have several advantages to their incorporation. An important enzymatic property to consider is the driving force or potential necessary for successful reaction catalysis. Many enzymes operate at potentials close to their substrates which is most suitable for fuel cell applications.\n\nFurthermore, the protein matrix surrounding the active site provides many vital functions; selectivity for the substrate, internal electron coupling, acidic/basic properties and the ability to bind to other proteins (or the electrode). Enzymes are more stable in the absence of proteases and enzymes from thermophilic organisms and thus offer a wider range of temperatures. Usual operating conditions is generally between 20-50 °C and pH 4.0 to 8.0.\n\nA drawback with the use of enzymes is size; given the large size of enzymes, they yield a low current density per unit electrode area due to the limited space. Since it is not possible to reduce enzyme size, it has been argued that these types of cells will be lower in activity. One solution has been to use three-dimensional electrodes or immobilization on conducting carbon supports which provide high surface area. These electrodes are extended into three-dimensional space which greatly increases the surface area for enzymes to bind thus increasing the current.\n\nAs per the definition of biofuel cells, enzymes are used as electrocatalysts at both the cathode and anode. In hydrogenase-based biofuel cells, hydrogenases are present at the anode for H2 oxidation in which molecular hydrogen is split into electrons and protons. In the case of H2/O2 biofuel cells, the cathode is coated with oxidase enzymes which then convert the protons into water.\n\nIn recent years, research on hydrogenases has grown significantly due to scientific and technological interest in hydrogen. The bidirectional or reversible reaction catalyzed by hydrogenase is a solution to the challenge in the development of technologies for the capture and storage of renewable energy as fuel with use on demand. This can be demonstrated through the chemical storage of electricity obtained from a renewable source (e.g. solar, wind, hydrothermal) as H during periods of low energy demands. When energy is desired, H can be oxidized to produce electricity which is very efficient.\n\nThe use of hydrogen in energy converting devices has gained interest due to being a clean energy carrier and potential transportation fuel.\n\nIn addition to the advantages previously mentioned associated with incorporating enzymes in fuel cells, hydrogenase is a very efficient catalyst for H consumption forming electrons and protons. Platinum is typically the catalyst for this reaction however, the activity of hydrogenases are comparable without the issue of catalyst poisoning by HS and CO. In the case of H/O fuel cells, there is no production of greenhouse gases where the product is water.\n\nWith regards to structural advantages, hydrogenase is highly selective for its substrate. The lack of need for a membrane simplifies the biofuel cell design to be small and compact, given that hydrogenase does not react with oxygen (an inhibitor) and the cathode enzymes (typically laccase) does not react with the fuel. The electrodes are preferably made from carbon which is abundant, renewable and can be modified in many ways or adsorb enzymes with high affinity. The hydrogenase is attached to a surface which also extends the lifetime of the enzyme.\n\nThere are several difficulties to consider associated with the incorporation of hydrogenase in biofuel cells. These factors must be taken into account to produce an efficient fuel cell.\n\nSince the hydrogenase-based biofuel cell hosts a redox reaction, hydrogenase must be immobilized on the electrode in such a way that it can exchange electrons directly with the electrode to facilitate the transfer of electrons. This proves to be a challenge in that the active site of hydrogenase is buried in the center of the enzyme where the FeS clusters are used as an electron relay to exchange electrons with its natural redox partner.\n\nPossible solutions for greater efficiency of electron delivery include the immobilization of hydrogenase with the most exposed FeS cluster close enough to the electrode or the use of a redox mediator to carry out the electron transfer. Direct electron transfer is also possible through the adsorption of the enzyme on graphite electrodes or covalent attachment to the electrode. Another solution includes the entrapment of hydrogenase in a conductive polymer.\n\nImmediate comparison of the size of hydrogenase with standard inorganic molecular catalysts reveal that hydrogenase is very bulky. It is approximately 5 nm in diameter compared to 1-5 nm for Pt catalysts. This limits the possible electrode coverage by capping the maximum current density.\n\nSince altering the size of hydrogenase is not a possibility, to increase the density of enzyme present on the electrode to maintain fuel cell activity, a porous electrode can be used instead of one that is planar. This increases the electroactive area allowing more enzyme to be loaded onto the electrode. An alternative is to form films with graphite particles adsorbed with hydrogenase inside a polymer matrix. The graphite particles then can collect and transport electrons to the electrode surface.\n\nIn a biofuel cell, hydrogenase is exposed to two oxidizing threats. O inactivates most hydrogenases with the exception of [NiFe] through diffusion of O to the active site followed by destructive modification of the active site. O is the fuel at the cathode and therefore must be physically separated or else the hydrogenase enzymes at the anode would be inactivated. Secondly, there is a positive potential imposed on hydrogenase at the anode by the enzyme on the cathode. This further enhances the inactivation of hydrogenase by O causing even [NiFe] which was previously O2-tolerant, to be affected.\n\nTo avoid inactivation by O, a proton exchange membrane can be used to separate the anode and cathode compartments such that O is unable to diffuse to and destructively modify the active site of hydrogenase.\n\nThere are many ways to adsorb hydrogenases onto carbon electrodes that have been modified with polymers. An example is a study done by Morozov et al. where they inserted NiFe hydrogenase into polypyrrole films and to provide proper contact to the electrode, there were redox mediators entrapped into the film. This was successful because the hydrogenase density was high in the films and the redox mediator helped to connect all enzyme molecules for catalysis which was about the same power output as hydrogenase in solution.\n\nCarbon nanotubes can also be used for a support for hydrogenase on the electrode due to their ability to assemble in large porous and conductive networks. These hybrids have been prepared using [FeFe] and [NiFe] hydrogenases. The [NiFe] hydrogenase isolated from \"A. aeolicus\" (thermophilic bacteria) was able to oxidize H with direct electron transfer without a redox mediator with a 10-fold higher catalytic current with stationary CNT-coated electrodes than with bare electrodes.\n\nAnother way of coupling hydrogenase to the nanotubes was to covalently bind them to avoid a time delay. Hydrogenase isolated from D. gigas (jumbo squid) was coupled to multiwalled carbon nanotube (MWCNT) networks and produced a current ~30 times higher than the graphite-hydrogenase anode. A slight drawback to this method is that the ratio of hydrogenase covering the surface of the nanotube network leaves hydrogenase to cover only the scarce defective spots in the network. It is also found that some adsorption procedures tend to damage the enzymes whereas covalently coupling them stabilized the enzyme and allows it to remain stable for longer. The catalytic activity of hydrogenase-MWCNT electrodes provided stability for over a month whereas the hydrogenase-graphite electrodes only lasted about a week.\n\nA fully enzymatic hydrogen fuel cell was constructed by the Armstrong group who used the cell to power a watch. The fuel cell consisted of a graphite anode with hydrogenase isolated from R. metallidurans and a graphite cathode modified with fungal laccase. The electrodes were placed in a single chamber with a mixture of 3% H gas in air and there was no membrane due to the tolerance of the hydrogenase to oxygen. The fuel cell produced a voltage of 950mV and generated 5.2 uW/cm of electricity. Although this system was very functional, it was still not at optimum output due to the low accessible H levels, the lower catalytic activity of the oxygen tolerant hydrogenases and the lower density of catalysts on the flat electrodes.\n\nThis system was then later improved by adding a MWCNT network to increase the electrode area.\n\n"}
{"id": "55704798", "url": "https://en.wikipedia.org/wiki?curid=55704798", "title": "Europa Imaging System", "text": "Europa Imaging System\n\nThe Europa Imaging System (EIS) is a visible spectrum wide and narrow angle camera on board the planned \"Europa Clipper\" mission that will map most of Europa at resolution, and will provide images of selected surface areas at up to 0.5 m resolution.\n\nEIS will provide comprehensive data sets, including cartographic and three-dimensional geologic maps, regional and high-resolution digital topography, geographic information system data products, color and photometric data products, a geodetic control network tied to radar altimetry, and a database of plume-search observations. \n\nEIS combines a narrow-angle camera (NAC) and a wide-angle camera (WAC) designed to address the reconnaissance goals. Both cameras operate on the visible spectrum (390 to 700 nm) and make use of push broom scanners for obtaining images with spectroscopic sensors.\n\nThe Principal investigator is Elizabeth Turtle.\n\nThe objectives of the EIS instrument are: \n\n\nThe NAC provides very high-resolution, stereo reconnaissance, generating 2 km wide swaths at 0.5 m pixel scale from 50 km altitude. NAC observations also include: near-global (>95%) mapping of Europa at ≥50 m pixel scale; regional and high-resolution stereo imaging at <1 m/pixel, and high phase angle observations for plume searches. The NAC will also perform high-phase-angle observations to search for potential plumes, even\nwhen the spacecraft is distant from Europa. \n\nThe WAC will acquire stereo reconnaissance, generating digital topographic models with 32 m spatial scale and 4 m vertical precision from 50 km altitude.\n"}
{"id": "46621887", "url": "https://en.wikipedia.org/wiki?curid=46621887", "title": "Faceware Technologies", "text": "Faceware Technologies\n\nFaceware Technologies is an American company that designs facial animation and motion capture technology. The company was established under Image Metrics and became its own company at the beginning of 2012.\n\nFaceware produces software used to capture an actor’s performance and transfer it onto an animated character, as well as hardware needed to capture the performances. The software line includes Faceware Analyzer, Faceware Retargeter, and Faceware Live.\n\nFaceware software is used by film studios and video game developers including Rockstar Games, Bungie, Cloud Imperium Games, and 2K in games such as Grand Theft Auto V, Destiny, Star Citizen, and .\n\nThrough its application in the video game industry, Faceware won the Develop Award while it was still part of Image Metrics for Technical Innovation in 2008. It won the Develop Award again for Creative Contribution: Visuals in 2014. Faceware received Best of Show recognition at the Game Developers Conference 2011 in San Francisco as well as Computer Graphics World's Silver Edge Award at SIGGRAPH 2014 and 2016. Finally, Faceware won the XDS Gary Award in 2016 for its contributions to the Faceware-EA presentation at the 2016 XDS Summit.\n\nImage Metrics, founded in 2000, is a provider of facial animation and motion capture technology within the video game and entertainment industries. In 2008, Image Metrics offered a beta version of its facial animation technology to visual effects and film studios. The technology captured an actor’s performance on video, analyzed it, and mapped it onto a CG model. The release of the beta allowed studios to incorporate the facial animation technology into internal pipelines rather than going to the Image Metrics studio as they had in the past. The first studio to beta test Image Metric’s software in 2009 was the visual effects studio Double Negative out of London.\n\nIn 2010, Image Metrics launched the facial animation technology platform Faceware. Faceware focused on increasing creative control, efficiency and production speed for animators. The software could be integrated into any pipeline or used with any game engine. Image Metrics provided training to learn the Faceware platform. The first studio to sign on as a Faceware customer was Bungie, which incorporated the software into its in-house production. Image Metrics acquired FacePro in 2010, a company that provided automated lip synchronization which could be altered for accurate results, and Image Metrics integrated the acquired technology into its facial animation software. Also in 2010, Image Metrics bought Character-FX, a character animation company. Character-FX produced tools for use in Autodesk’s Maya and 3DS Max which aide in the creation of character facial rigs using an automated weighting transfer system that rapidly shifts facial features on a character to create lifelike movement.\n\nImage Metrics raised $8 million in funding and went public through a reverse merger in 2010 with International Cellular Industries. Image Metrics became wholly owned by International Cellular industries, which changed its name and took on facial animation technology as its sole line of business. Faceware 3.0 was announced in March 2011. The upgrade included auto-pose, a shared pose database, and curve refinement. Image Metrics led a workshop and presentation about Faceware 3.0 at the CTN Animation Expo 2011 titled \"Faceware: Creating an Immersive Experience through Facial Animation.\" Faceware’s technology was displayed at Edinburgh Interactive in August 2011 to show its ability to add player facial animation from webcam or Kinect sensor into a game in real time.\n\nImage Metrics sold the Faceware software to its spinoff company, Faceware Technologies, in January 2012. Following the spinoff, Faceware Technologies focused on producing and distributing its technology to professional animators. The technology was tested through Universities, including the University of Portsmouth.\n\nFaceware launched its 3D facial animation tools, software packages Faceware Analyzer and Faceware Retargeter with the Head-Mounted Camera System (HMCS). Analyzer tracks and processes live footage of an actor and Retargeter transfers that movement onto the face of a computer-generated character. The Head-Mounted Camera System is not required to use the software. Six actors can be captured simultaneously.\n\nFaceware Live was shown for the first time at SIGGRAPH 2013. It was created to enable the real-time capture and retargeting of facial movements. The live capture of facial performance can use any video source to track and translate facial expressions into a set of animation values and transfer the captured data onto a 3D animated character in real time. In 2014, Faceware released Faceware Live 2.0. The update included the option to stream multiple characters simultaneously, instant calibration, improved facial tracking, consistent calibration, and support for high-frame-rate cameras.\n\nIn 2015, Faceware launched a plugin for Unreal Engine 4 called Faceware Live. The company co-developed the plugin with Australia-based Opaque Multimedia. It makes motion capture of expressions and other facial movements possible with any video camera through Faceware's markerless 3D facial motion capture software.\n\nIn 2016, Faceware announced the launch of Faceware Interactive, which is focused on the development of software and hardware that can be used in the creation of digital characters with whom real people can interact.\n\nFaceware Technologies partnered with Binari Sonori in 2014 to develop a video-based localization service. Also in 2014, Faceware Technologies entered a global partnership with Vicon, a company focused on motion capture. The partnership would focus on developing new technology to expand into full-body motion capture data. The first step of the integration was to make the Faceware software compatible with Vicon’s head rig, Cara, to allow data acquired from Cara to be processed and transferred into Faceware products.\n\nFaceware Technologies has two main aspects of facial animation software.\n\nFaceware Analyzer is a stand-alone single-camera facial tracking software that converts videos of facial motion into files that can be used for Faceware Retargeter. The Lite version of the software can automatically track facial movements which can be applied to 3D models with Faceware Retargeter. The Pro version can perform shot specific custom calibrations, import and export actor data, auto indicate tracking regions, and has server and local licensing options. The data captured by Faceware Analyzer is then processed in Faceware Retargeter.\n\nFaceware Retargeter 4.0 was announced in 2014. Faceware Retargeter uses facial tracking data created in Analyzer to create facial animation in a pose-based workflow. The upgrade has a plug-in for Autodesk animation tools, advanced character expression sets, visual tracking data, shared pose thumbnails, and batch processing. The Lite version of the Retargeter software transfers actor’s performances onto animated characters and reduces and smooths key frames. The Pro version includes custom poses, intelligent pose suggestions, shared pose libraries, and the ability to backup and restore jobs.\n\nFaceware Live aims to create natural looking faces and facial expressions in real-time. Any video source can be used with the software’s one-button calibration. The captured video is transferred onto a 3D animated character. This process combines image processing and data streaming to translate facial expressions into a set of animation values.\n\nFaceware has hardware options that can be rented or purchased. Available hardware is the entry level GoPro Headcam Kit and the Professional Headcam System. The Indie Facial Mo-cap package includes hardware, a camera and headmount, and the tools to use it.\n\nFaceware software is used by companies such as Activision-Blizzard, Bethesda, Ubisoft, Electronic Arts, Sony, Cloud Imperium Games, and Microsoft. Rockstar Games used the software in games such as Grand Theft Auto V and Red Dead Redemption and Bungie used Faceware in games including Destiny and . Faceware has also been used in other games like XCOM2, Dying Light: The Following, Hitman, EA Sports UFC 2, Fragments for Microsoft’s HoloLens, DOOM, Mirror’s Edge Catalyst, Kingsglaive, F1 2016, ReCore, , Mafia III, , Killzone:Shadow Fall, NBA 2K10-2K17, Sleeping Dogs, Crysis 2 and 3, Star Citizen, and in movies like The Curious Case of Benjamin Button and Robert Zemeckis's The Walk.\n\n"}
{"id": "549892", "url": "https://en.wikipedia.org/wiki?curid=549892", "title": "GRASS GIS", "text": "GRASS GIS\n\nGeographic Resources Analysis Support System (commonly termed GRASS GIS) is a geographic information system (GIS) software suite used for geospatial data management and analysis, image processing, producing graphics and maps, spatial and temporal modeling, and visualizing. It can handle raster, topological vector, image processing, and graphic data.\n\nGRASS GIS contains over 350 modules to render maps and images on monitor and paper; manipulate raster and vector data including vector networks; process multispectral image data; and create, manage, and store spatial data.\n\nIt is licensed and released as free and open-source software under the GNU General Public License (GPL). It runs on multiple operating systems, including , Windows and Linux. Users can interface with the software features through a graphical user interface (GUI) or by \"plugging into\" GRASS via other software such as QGIS. They can also interface with the modules directly through a bespoke shell that the application launches or by calling individual modules directly from a standard shell. The latest stable release version (LTS) is GRASS GIS 7, which is available since 2015\n\nThe GRASS Development Team is a multinational group consisting of developers at many locations. GRASS is one of the eight initial Software Projects of the Open Source Geospatial Foundation.\n\nGRASS supports raster and vector data in two and three dimensions. The vector data model is topological, meaning that areas are defined by boundaries and centroids; boundaries cannot overlap within one layer. In contrast, OpenGIS Simple Features, defines vectors more freely, much as a non-georeferenced vector illustration program does.\n\nGRASS is designed as an environment in which tools that perform specific GIS computations are executed. Unlike GUI-based application software, the GRASS user is presented with a Unix shell containing a modified environment that supports execution of GRASS commands, termed modules. The environment has a state that includes parameters such as the geographic region covered and the map projection in use. All GRASS modules read this state and additionally are given specific parameters (such as input and output maps, or values to use in a computation) when executed. Most GRASS modules and abilities can be operated via a graphical user interface (provided by a GRASS module), as an alternative to manipulating geographic data in a shell.\n\nThe GRASS distribution includes over 350 core modules. Over 100 add-on modules created by users are offered on its website. The libraries and core modules are written in C. Other modules are written in C, C++, Python, Unix shell, Tcl, or other scripting languages. The modules are designed under the Unix philosophy and hence can be combined using Python or shell scripting to build more complex or specialized modules, by users, without knowledge of C programming.\n\nThere is cooperation between the GRASS and Quantum GIS (QGIS) projects. Recent versions of QGIS can be executed within the GRASS environment, allowing QGIS to be used as a user-friendly graphical interface to GRASS that more closely resembles other graphical GIS software than does the shell-based GRASS interface.\n\nAnother project exists to re-implement GRASS in Java as \"JGRASS\".\n\nGRASS has been under continuous development since 1982 and has involved a large number of federal US agencies, universities, and private companies. The core components of GRASS and the management of integration of efforts into its releases was originally directed by the U.S. Army - Construction Engineering Research Laboratory (USA-CERL), a branch of the U.S. Army Corps of Engineers, in Champaign, Illinois. USA-CERL completed its last release of GRASS as version 4.1 in 1992, and provided five updates and patches to this release through 1995. USA-CERL also wrote the core components of the GRASS 5.0 floating point version.\n\nThe development of GRASS was started by the USA-CERL to meet the need of the United States military for software for land management and environmental planning. A key motive was the National Environmental Policy Act. The development platform was Unix running on VAX hardware. During 1982 through 1995, USA-CERL led the development of GRASS, with the involvement of many others, including universities and other federal agencies. USA-CERL officially ceased its involvement in GRASS after release 4.1 (1995), though development had been limited to minor patches since 1993. A group formed at Baylor University to take over the software, releasing GRASS 4.2. Around this time, a port of the software to Linux was made. In 1998, Markus Neteler, the current project leader, announced the release of GRASS 4.2.1, which offered major improvements including a new graphical user interface. In October 1999, the license of the originally public domain software GRASS software was changed to the GNU GPL in version 5.0.\n\nSince then, GRASS has evolved into a powerful software suite with a wide range of applications in many different areas of scientific research and engineering. For example, it is used to estimate potential solar photovoltaic yield with r.sun. As of 2015, GRASS is used in academic and commercial settings around the world, and in many government agencies including NASA, NOAA, USDA, DLR, CSIRO, the National Park Service, the U.S. Census Bureau, USGS, and many environmental consulting companies.\n\n, the latest stable release version (LTS) is GRASS GIS 7. It was released in 2015, replacing the old stable branch (6.4) which was released in 2011. Version 7 added many new features, including large data support, a fast topological 2D/3D vector engine, powerful vector network analysis, a full temporal framework and many other features and improvements.\n\n, GRASS development is split into two branches: stable and developmental. The stable branch is recommended for most users, while the 7.1 branch operates as a testbed for new features.\n\n\n\n"}
{"id": "9420612", "url": "https://en.wikipedia.org/wiki?curid=9420612", "title": "Guideline tensioner", "text": "Guideline tensioner\n\nA guideline tensioner is a hydropneumatic device used on an offshore drilling rig that keeps a positive pulling force on the guidelines from the platform to a template on the seabed.\n\nThe guidelines act as a guidance for equipment and tools that must be lowered to the template. If there was no tensioner and the platform moved, the guidelines would become slack and could be broken. For this reason a number of guideline tensioners are mounted between the platform and riser. Each of these guideline tensioners consists of a hydraulic cylinder with sheaves at both sides. The cylinder is connected to one or more high pressure gas bottles via a medium separator. A wire rope is rigged in the cylinder; one end is connected to the fixed part of the tensioner, the other end to the template.\n\n"}
{"id": "15286841", "url": "https://en.wikipedia.org/wiki?curid=15286841", "title": "Harris Ranch", "text": "Harris Ranch\n\nHarris Ranch, or the Harris Cattle Ranch, feedlot is California's largest beef producer and the largest ranch on the West Coast of the United States, producing 150,000,000 pounds of beef per year in 2010. It is located alongside Interstate 5 at its intersection with California State Route 198 east of Coalinga, in the San Joaquin Valley of central California. The ranch is owned by Harris Farms.\n\nFounded by Jack Harris in 1937, the Harris Ranch Beef Company (now operated by Jack Harris' son John) was originally a cotton and grain operation. In the 1970s the ranch opened a burger stand near Interstate 5.\n\nThe farm also operates an inn and restaurant, raises fruit and vegetable crops, and breeds thoroughbred horses. Overall, the operation has more than 400 employees. Approximately are devoted to garlic, broccoli, pomegranates, and tomatoes, among 35 types of fruits and vegetables.\n\nDuring the War on Terror, volunteers from the San Joaquin Valley, especially Bakersfield, supplied with beef from Harris Ranch, have volunteered to serve steaks to servicemembers who are OCONUS. \nIn January 2012, an arsonist destroyed fourteen cattle trucks on the ranch. The Animal Liberation Front claimed responsibility.\n\nAt over and with a population of over 100,000 cattle, and hundreds harvested daily, the ranch is the largest on the West Coast. It is also among the largest (when including density) in the United States. A vertically integrated operation, it owns a fleet of trucks that take cattle from several ranches with which it deals, and does its own finishing, slaughtering, and packaging.\n\nThe ranch supplies the hamburger meat for the In-N-Out Burger chain, and also distributes beef and prepared meals through grocery stores and restaurants nationwide.\n\nHarris Ranch was one of the first to build a brand around itself as a specialty niche product, and is credited as a forerunner of companies like Niman Ranch and Dakota Beef.\n\nThe restaurant was targeted to local farmers when it opened in 1977, but later became popular as a halfway stop on the busy highway connecting San Francisco and Los Angeles. </ref> A 153-room luxury inn was added in 1987. It was built in hacienda-style. The restaurant evolved into a \"farm to fork\" concept in the late 2000s, featuring not only beef but wine and other products made locally by the ranch. As of 2008 the restaurant was the 57th busiest in the United States and sixth busiest in California based on gross receipts. The site was chosen as one of the first battery swapping Tesla stations.\n\nThe ranch is known to travelers for the \"ripe, tangy odor of cow manure\", described alternately as a \"horrible stench\" and \"a good, honest, American smell\". This smell inspired food writer Michael Pollan to conduct the research on factory farming that led to his sustainability book, \"The Omnivore's Dilemma\". The owner of Harris Ranch, in turn, threatened to withhold a $500,000 donation to California Polytechnic State University, San Luis Obispo if it sponsored a speech there by Pollan. In reference to the large number of cattle processed at its facilities, some critics have nicknamed the ranch \"Cowschwitz\", comparing the slaughtering of cows to the slaughtering of Jews during the Holocaust at the Auschwitz concentration camp. Animal behavior expert Temple Grandin described the nickname as a matter of public misperception, saying that the company \"does a great job\" of keeping its animals.\n\n\n"}
{"id": "491658", "url": "https://en.wikipedia.org/wiki?curid=491658", "title": "Heavy equipment", "text": "Heavy equipment\n\nHeavy equipment refers to heavy-duty vehicles, specially designed for executing construction tasks, most frequently ones involving earthwork operations. They are also known as heavy machines, heavy trucks, construction equipment, engineering equipment, heavy vehicles, or heavy hydraulics. They usually comprise five equipment systems: implement, traction, structure, power train, control and information. Heavy equipment functions through the mechanical advantage of a simple machine, the ratio between input force applied and force exerted is multiplied. Some equipment uses hydraulic drives as a primary source of motion.\n\nThe use of heavy equipment has a long history; the ancient Roman engineer Vitruvius (1st century BCE) gave descriptions of heavy equipment and cranes in ancient Rome in his treatise \"De architectura\". The pile driver was invented around 1500. The first tunnelling shield was patented by Marc Isambard Brunel in 1818.\n\nUntil the 19th century and into the early 20th century heavy machines were drawn under human or animal power. With the advent of portable steam-powered engines the drawn machine precursors were reconfigured with the new engines, such as the combine harvester. The design of a core tractor evolved around the new steam power source into a new machine core traction engine, that can be configured as the steam tractor and the steamroller. During the 20th century, internal-combustion engines became the major power source of heavy equipment. Kerosene and ethanol engines were used, but today diesel engines are dominant. Mechanical transmission was in many cases replaced by hydraulic machinery. The early 20th century also saw new electric-powered machines such as the forklift. Caterpillar Inc. is a present-day brand from these days, starting out as the Holt Manufacturing Company. The first mass-produced heavy machine was the Fordson tractor in 1917.\n\nThe first commercial continuous track vehicle was the 1901 Lombard Steam Log Hauler. The use of tracks became popular for tanks during World War I, and later for civilian machinery like the bulldozer. The largest engineering vehicles and mobile land machines are bucket-wheel excavators, built since the 1920s.\n\n\"Until almost the twentieth century, one simple tool constituted the primary earthmoving machine: the hand shovel - moved with animal and human powered, sleds, barges, and wagons. This tool was the principal method by which material was either sidecast or elevated to load a conveyance, usually a wheelbarrow, or a cart or wagon drawn by a draft animal. In antiquity, an equivalent of the hand shovel or hoe and head basket—and masses of men—were used to move earth to build civil works. Builders have long used the inclined plane, levers, and pulleys to place solid building materials, but these labor-saving devices did not lend themselves to earthmoving, which required digging, raising, moving, and placing loose materials. The two elements required for mechanized earthmoving, then as now, were an independent power source and off-road mobility, neither of which could be provided by the technology of that time.\"\n\nContainer cranes were used from the 1950s and onwards, and made containerization possible.\n\nNowadays such is the importance of this machinery, some transport companies have developed specific equipment to transport heavy construction equipment to and from sites.\n\nThese subdivisions, in this order, are the standard heavy equipment categorization.\n\nTrack-type\n\nGrader\nSkidSteer\n\nExcavator\n\nBackhoe\n\nTimber\n\nPipelayer\n\nScraper\nMining\nArticulated\n\nCompactor\n\nLoader\n\nTrack Loader\n\nMaterial Handler\n\nPaving\n\nUnderground\n\nHydromatic Tool\n\nHighway\n\nHeavy equipment requires specialized tires for various construction applications. While many types of equipment have continuous tracks applicable to more severe service requirements, tires are used where greater speed or mobility is required. An understanding of what equipment will be used for during the life of the tires is required for proper selection. Tire selection can have a significant impact on production and unit cost. There are three types of off-the-road tires, \"transport\" for earthmoving machines, \"work\" for slow moving earthmoving machines, and \"load and carry\" for transporting as well as digging. Off-highway tires have six categories of service C compactor, E earthmover, G grader, L loader, LS log-skidder and ML mining and logging. Within these service categories are various tread types designed for use on hard-packed surface, soft surface and rock. Tires are a large expense on any construction project, careful consideration should be given to prevent excessive wear or damage.\n\nA heavy equipment operator drives and operates heavy equipment used in engineering and construction projects. Typically only skilled workers may operate heavy equipment, and there is specialized training for learning to use heavy equipment.\n\nMuch publication about heavy equipment operators focuses on improving safety for such workers. The field of occupational medicine researches and makes recommendations about safety for these and other workers in safety-sensitive positions.\n\nDue to the small profit margins on construction projects it is important to maintain accurate records concerning equipment utilization, repairs and maintenance. The two main categories of equipment costs are ownership cost and operating cost.\n\nTo classify as an ownership cost an expense must have been incurred regardless of if the equipment is used or not. These costs are as follows:\nDepreciation can be calculated several ways, the simplest is the straight-line method. The annual depreciation is constant, reducing the equipment value annually. The following are simple equations paraphrased from the Peurifoy & Schexnayder text:\nFor an expense to be classified as an operating cost, it must be incurred through use of the equipment. These costs are as follows:\nThe biggest distinction from a cost standpoint is if a repair is classified as a \"major repair\" or a \"minor repair\". A major repair can change the depreciable equipment value due to an extension in service life, while a minor repair is normal maintenance. How a firm chooses to cost major and minor repairs vary from firm to firm depending on the costing strategies being used. Some firms will charge only major repairs to the equipment while minor repairs are costed to a project. Another common costing strategy is to cost all repairs to the equipment and only frequently replaced wear items are excluded from the equipment cost. Many firms keep their costing structure closely guarded as it can impact the bidding strategies of their competition. In a company with multiple semi-independent divisions, the equipment department often wants to classify all repairs as \"minor\" and charge the work to a job - therefore improving their 'profit' from the equipment.\n\nDie-cast metal promotional scale models of heavy equipment are often produced for each vehicle to give to prospective customers. These are typically in . The popular manufacturers of these models are Conrad and NZG in Germany, even for US vehicles.\n\nThe largest 10 construction equipment manufacturers in 2015 \n\nOther manufacturers include:\n\n\n"}
{"id": "55262818", "url": "https://en.wikipedia.org/wiki?curid=55262818", "title": "IAWTV Awards", "text": "IAWTV Awards\n\nThe IAWTV Awards is an annual event hosted by the International Academy of Web Television, currently based in Los Angeles, that honors web series creators and talent in over a dozen categories, voted on by the IAWTV membership. \n\nThe IAWTV Awards are hosted annually by the International Academy of Web Television (IAWTV), which was founded in 2008 and is devoted to the advancement of the arts and sciences of web television production.\n\nIn 2010, before the IAWTV Awards existed, the IAWTV hosted the 2nd annual Streamy Awards. The poor reception of the event, and the surrounding controversy, resulted in a two year hiatus for the Streamy Awards, and the subsequent creation of the IAWTV Awards. The two awards ceremonies are both still running annually, though as completely separate entities.\n\nSince its inception in 2011, the IAWTV has held award ceremonies every year (with the exception of 2016), presenting awards to web series creators and talent in over a dozen categories, covering several genres. Notable IAWTV winners include Felicia Day, Julia Stiles and Milo Ventimiglia, as well as the critically acclaimed web series \"The Guild\", \"Blue\", \"Anyone But Me\", \"Husbands\", \"Leap Year\", and \"Whatever, Linda\".\n\nBetween 2012 and 2015, the IAWTV Awards were held in Las Vegas. The 2017 ceremony took place in Los Angeles.\n\nFor the full list of winners from each ceremony, visit the official IAWTV website for archives.\n"}
{"id": "23838445", "url": "https://en.wikipedia.org/wiki?curid=23838445", "title": "Industrial safety system", "text": "Industrial safety system\n\nAn industrial safety system is a countermeasure crucial in any hazardous plants such as oil and gas plants and nuclear plants. They are used to protect human, industrial plant, and the environment in case of the process going beyond the allowed control margins.\n\nAs the name suggests, these systems are not intended for controlling the process itself but rather protection. Process control is performed by means of process control systems (PCS) and is interlocked by the safety systems so that immediate actions are taken should the process control systems fail.\n\nProcess control and safety systems are usually merged under one system, called \"Integrated Control and Safety System\" (ICSS).\nIndustrial safety systems typically use dedicated systems that are SIL 2 certified at minimum; whereas control systems can start with SIL 1. SIL applies to both hardware and software requirements such as cards, processors redundancy and voting functions.\n\nThere are 2 main types of industrial safety systems in process industry:\nA third system also exists which acts as a barrier and contains the spray out of hot oil & gases from flanges, valves & pipe joints. These systems are popularly known as safety spray shields and flange guards.The use of spray guards is mandated by OSHA.\n\nThese systems may also be redefined in terms of ESD/EDP levels as:\n\nThe safety shutdown system (SSS) shall shut down the facilities to a safe state in case of an emergency situation, thus protecting personnel, the environment and the asset. The safety shutdown system shall manage all inputs and outputs relative to emergency shutdown (ESD) functions (environment and personnel protection). This system might also be fed by signals from the main fire and gas system.\n\nThe main objectives of the fire and gas system are to protect personnel, environment, and plant (including equipment and structures).\nThe FGS shall achieve these objectives by:\n\nDue to closing ESD valves in a process, there may be some trapped flammable fluids, and these must be released in order to avoid any undesired consequences (such as pressure increase in vessels and piping). For this, emergency depressurization (EDP) systems are used in conjunction with the ESD systems to release (to a safe location and in a safe manner) such trapped fluids.\n\nPressure safety valves or PSVs are usually used as a final safety solution when all previous systems fail to prevent any further pressure accumulation and protect vessels from rupture due to overpressure by their designed action.\n\n"}
{"id": "36674345", "url": "https://en.wikipedia.org/wiki?curid=36674345", "title": "Information technology", "text": "Information technology\n\nInformation technology (IT) is the use of computers to store, retrieve, transmit, and manipulate data, or information, often in the context of a business or other enterprise. IT is considered to be a subset of information and communications technology (ICT).\n\nHumans have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC, but the term \"information technology\" in its modern sense first appeared in a 1958 article published in the \"Harvard Business Review\"; authors Harold J. Leavitt and Thomas L. Whisler commented that \"the new technology does not yet have a single established name. We shall call it information technology (IT).\" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.\n\nThe term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce.\n\nBased on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present). This article focuses on the most recent period (electronic), which began in about 1940.\n\nDevices have been used to aid computation for thousands of years, probably initially in the form of a tally stick. The Antikythera mechanism, dating from about the beginning of the first century BC, is generally considered to be the earliest known mechanical analog computer, and the earliest known geared mechanism. Comparable geared devices did not emerge in Europe until the 16th century, and it was not until 1645 that the first mechanical calculator capable of performing the four basic arithmetical operations was developed.\n\nElectronic computers, using either relays or valves, began to appear in the early 1940s. The electromechanical Zuse Z3, completed in 1941, was the world's first programmable computer, and by modern standards one of the first machines that could be considered a complete computing machine. Colossus, developed during the Second World War to decrypt German messages, was the first electronic digital computer. Although it was programmable, it was not general-purpose, being designed to perform only a single task. It also lacked the ability to store its program in memory; programming was carried out using plugs and switches to alter the internal wiring. The first recognisably modern electronic digital stored-program computer was the Manchester Baby, which ran its first program on 21 June 1948.\n\nThe development of transistors in the late 1940s at Bell Laboratories allowed a new generation of computers to be designed with greatly reduced power consumption. The first commercially available stored-program computer, the Ferranti Mark I, contained 4050 valves and had a power consumption of 25 kilowatts. By comparison the first transistorised computer, developed at the University of Manchester and operational by November 1953, consumed only 150 watts in its final version.\n\nEarly electronic computers such as Colossus made use of punched tape, a long strip of paper on which data was represented by a series of holes, a technology now obsolete. Electronic data storage, which is used in modern computers, dates from World War II, when a form of delay line memory was developed to remove the clutter from radar signals, the first practical application of which was the mercury delay line. The first random-access digital storage device was the Williams tube, based on a standard cathode ray tube, but the information stored in it and delay line memory was volatile in that it had to be continuously refreshed, and thus was lost once power was removed. The earliest form of non-volatile computer storage was the magnetic drum, invented in 1932 and used in the Ferranti Mark 1, the world's first commercially available general-purpose electronic computer.\n\nIBM introduced the first hard disk drive in 1956, as a component of their 305 RAMAC computer system. Most digital data today is still stored magnetically on hard disks, or optically on media such as CD-ROMs. Until 2002 most information was stored on analog devices, but that year digital storage capacity exceeded analog for the first time. As of 2007 almost 94% of the data stored worldwide was held digitally: 52% on hard disks, 28% on optical devices and 11% on digital magnetic tape. It has been estimated that the worldwide capacity to store information on electronic devices grew from less than 3 exabytes in 1986 to 295 exabytes in 2007, doubling roughly every 3 years.\n\nDatabase management systems emerged in the 1960s to address the problem of storing and retrieving large amounts of data accurately and quickly. One of the earliest such systems was IBM's Information Management System (IMS), which is still widely deployed more than 50 years later. IMS stores data hierarchically, but in the 1970s Ted Codd proposed an alternative relational storage model based on set theory and predicate logic and the familiar concepts of tables, rows and columns. The first commercially available relational database management system (RDBMS) was available from Oracle in 1981.\n\nAll database management systems consist of a number of components that together allow the data they store to be accessed simultaneously by many users while maintaining its integrity. A characteristic of all databases is that the structure of the data they contain is defined and stored separately from the data itself, in a database schema.\n\nThe extensible markup language (XML) has become a popular format for data representation in recent years. Although XML data can be stored in normal file systems, it is commonly held in relational databases to take advantage of their \"robust implementation verified by years of both theoretical and practical effort\". As an evolution of the Standard Generalized Markup Language (SGML), XML's text-based structure offers the advantage of being both machine and human-readable.\n\nThe relational database model introduced a programming-language independent Structured Query Language (SQL), based on relational algebra.\n\nThe terms \"data\" and \"information\" are not synonymous. Anything stored is data, but it only becomes information when it is organized and presented meaningfully. Most of the world's digital data is unstructured, and stored in a variety of different physical formats even within a single organization. Data warehouses began to be developed in the 1980s to integrate these disparate stores. They typically contain data extracted from various sources, including external sources such as the Internet, organized in such a way as to facilitate decision support systems (DSS).\n\nData transmission has three aspects: transmission, propagation, and reception. It can be broadly categorized as broadcasting, in which information is transmitted unidirectionally downstream, or telecommunications, with bidirectional upstream and downstream channels.\n\nXML has been increasingly employed as a means of data interchange since the early 2000s, particularly for machine-oriented interactions such as those involved in web-oriented protocols such as SOAP, describing \"data-in-transit rather than ... data-at-rest\". One of the challenges of such usage is converting data from relational databases into XML Document Object Model (DOM) structures.\n\nHilbert and Lopez identify the exponential pace of technological change (a kind of Moore's law): machines' application-specific capacity to compute information per capita roughly doubled every 14 months between 1986 and 2007; the per capita capacity of the world's general-purpose computers doubled every 18 months during the same two decades; the global telecommunication capacity per capita doubled every 34 months; the world's storage capacity per capita required roughly 40 months to double (every 3 years); and per capita broadcast information has doubled every 12.3 years.\n\nMassive amounts of data are stored worldwide every day, but unless it can be analysed and presented effectively it essentially resides in what have been called data tombs: \"data archives that are seldom visited\". To address that issue, the field of data mining – \"the process of discovering interesting patterns and knowledge from large amounts of data\" – emerged in the late 1980s.\n\nIn an academic context, the Association for Computing Machinery defines IT as \"undergraduate degree programs that prepare students to meet the computer technology needs of business, government, healthcare, schools, and other kinds of organizations ... IT specialists assume responsibility for selecting hardware and software products appropriate for an organization, integrating those products with organizational needs and infrastructure, and installing, customizing, and maintaining those applications for the organization’s computer users.\"\n\nCompanies in the information technology field are often discussed as a group as the \"tech sector\" or the \"tech industry\".\n\nIn a business context, the Information Technology Association of America has defined information technology as \"the study, design, development, application, implementation, support or management of computer-based information systems\". The responsibilities of those working in the field include network administration, software development and installation, and the planning and management of an organization's technology life cycle, by which hardware and software are maintained, upgraded and replaced.\n\nThe business value of information technology lies in the automation of business processes, provision of information for decision making, connecting businesses with their customers, and the provision of productivity tools to increase efficiency.\n\nThe field of information ethics was established by mathematician Norbert Wiener in the 1940s. Some of the ethical issues associated with the use of information technology include:\n\n\n\n"}
{"id": "23463958", "url": "https://en.wikipedia.org/wiki?curid=23463958", "title": "Kofler bench", "text": "Kofler bench\n\nA Kofler bench or Kofler hot-stage microscope is a metal strip with a temperature gradient (range room temperature to 300°C). Any substance can be placed on a section of the strip revealing its thermal behaviour at the temperature at that point.\n\nThis melting-point apparatus for use with a microscope was developed by the Austrian pharmacognosist Ludwig Kofler (30 November 1891 Dornbirn - 23 August 1951 Innsbruck) and his wife Adelheid Kofler. In 1936, the Koflers and Mayrhofer published their \"Mikroskopische Methoden in der Mikrochemie\" [Kofler, L., A. Kofler and Mayrhofer, A. (1936)], Kofler and Kofler published their \"Thermomikromethoden\" [Kofler L., and A. Kofler (1954)] in 1954.\n\nKofler, his wife Adelheid, and their colleague, Maria Kuhnert-Brandstätter, investigated numerous organic molecules, and published some 250 papers describing their work.\n\nThermomicroscopy, incepted by Ludwig and Adelheid Kofler and developed further by Maria Kuhnert-Brandstätter (1919-) and Walter C. McCrone is a technique for studying the phases of solid drug substances.\n\n\n"}
{"id": "1906143", "url": "https://en.wikipedia.org/wiki?curid=1906143", "title": "List of communication satellite companies", "text": "List of communication satellite companies\n\nThis is a list of all companies currently operating at least one commercial communication satellite or currently has one on order.\n\nThe World Teleport Association publishes lists of companies based on revenues from all customized communications sources and includes operators of teleports and satellite fleets. Top satellite operators in 2016 were:\n\n\n\n\n\n"}
{"id": "7659846", "url": "https://en.wikipedia.org/wiki?curid=7659846", "title": "List of fictional gynoids", "text": "List of fictional gynoids\n\nThis list of fictional gynoids is sorted by media genre and alphabetised by character name or media title. Gynoids are humanoid robots that are gendered feminine. They appear widely in science fiction film and art. They are also known as female androids, female robots or fembots, although some media have used other terms such as robotess, cyberdoll, \"skin-job\", or Replicant. Although there are a variety of gynoids across genres, this list excludes female cyborgs (e.g. Seven of Nine in \"Star Trek: Voyager\"), non-humanoid robots (e.g. EVE from \"Wall-E\"), virtual female characters (Dot Matrix and women from the cartoon \"ReBoot\", Simone from \"Simone\" (2002 film), Samantha from \"Her\"), holograms (Hatsune Miku in concert, Cortana from \"Halo\"), non-robotic haunted dolls, and general Artificial intelligence network systems (SAL 9000, GLaDOS from \"Portal\") Gynoids for Japanese manga and anime are grouped separately.\n\n\n\n\n\n\n\n"}
{"id": "8425040", "url": "https://en.wikipedia.org/wiki?curid=8425040", "title": "List of network protocol stacks", "text": "List of network protocol stacks\n\nThis is a list of protocol stack architectures. A \"protocol stack\" is a suite of complementary communications protocols in a computer network or a computer bus system.\n\n"}
{"id": "8828258", "url": "https://en.wikipedia.org/wiki?curid=8828258", "title": "MOS-controlled thyristor", "text": "MOS-controlled thyristor\n\nAn MOS-controlled thyristor (MCT) is a voltage-controlled fully controllable thyristor. It was invented by V.A.K. Temple. MCTs are similar in operation to GTO thyristors, but have voltage controlled insulated gates. They have two MOSFETs of opposite conductivity types in their equivalent circuits. One is responsible for turn-on and the other for turn-off. A thyristor with only one MOSFET in its equivalent circuit, which can only be turned on (like normal SCRs), is called an MOS-gated thyristor.\n\nPositive voltage on the gate terminal with respect to the cathode turns the thyristor to the on state.\n\nNegative voltage on the gate terminal with respect to the anode, which is close to cathode voltage during the on state, turns the thyristor to the off state.\n\nThese devices proved unwanted current crowding when fabricated, consequently small SOA (safe operating area), so Fairchild withdrew them after brief commercialization.\n\n"}
{"id": "5137427", "url": "https://en.wikipedia.org/wiki?curid=5137427", "title": "Malaysian Electronic Payment System", "text": "Malaysian Electronic Payment System\n\nThe Malaysian Electronic Payment System (MEPS) is an interbank network service provider in Malaysia. In August 2017, MEPS has merged with Malaysian Electronic Clearing Corporation Sdn Bhd (MyClear) to form Payments Network Malaysia Sdn Bhd (PayNet).\n\nWith the result of the merger, PayNet is now the holding company for the PayNet Group which comprises two main subsidiaries, namely Malaysian Electronic Payment System Sdn Bhd (MEPS) and MEPS Currency Management Sdn Bhd (MCM). The PayNet Group is Malaysia's premier payments network and central infrastructure for financial markets.\n\nBank Negara Malaysia (BNM) is PayNet's single largest shareholder, with eleven Malaysian's financial institutions namely, Malayan Banking Berhad, RHB Bank Berhad, Public Bank Berhad, CIMB Bank Berhad, AmBank (M) Berhad, Hong Leong Bank Berhad, Affin Bank Berhad, Alliance Bank Malaysia Berhad, Bank Islam Malaysia Berhad, Bank Muamalat Malaysia Berhad and Bank Kerjasama Rakyat Malaysia Berhad, as joint shareholders.\n\nMEPS plays the integral role in the implementation of smart cards for automated teller machine (ATM) cards, which are an upgrade to chip-based cards from previous magnetic-stripe cards issued to all banks' customers.\n\nThe card is also known as \"Bankcard\", a card with multiple functions. There are three main functions that can be used, namely ATM (with various combinations of banking transactions), e-debit (online purchase payment) transactions at participating merchants and MEPS Cash (a stored-value card that can be used to pay at participating merchants).\n\nMEPS is a member of the Asian Payment Network (APN).\n\nIn brief, MEPS’ role encompasses:\n\nMEPS provide the following services through its network to all participating banks:\n\nListed below is the participating banks. However, some participating banks provide only selected few of the services offered by MEPS as mentioned above.\n\n\n\nMEPS is accredited with the following:\n\n\n\np://www.bernama.com/bernama/v5/newsindex.php?id=574202 OCBC Customers Can Make ATM Withdrawals Via MEPS Network for FREE]\n"}
{"id": "45030952", "url": "https://en.wikipedia.org/wiki?curid=45030952", "title": "Mileševa printing house", "text": "Mileševa printing house\n\nThe Mileševa printing house () was a printing house established in 1544 in the Mileševa monastery near Prijepolje, Ottoman Empire (modern day Serbia). Three srbulje books were printed in this printing house. Two in 1544 and 1545 and one in 1557.\n\nActivities connected with printing in Mileševa began when in 1518 or at the beginning of 1519 when Teodor Ljubavić, who was a hieromonk of Mileševa, travelled to Venice to learn printing skills. Ljubavić worked at Goražde printing house between 1521 and 1523. Between 1533 and 1535 Božidar Vuković visited Mileševa and concluded an agreement with Mileševa to distribute his books.\n\nIn 1543 Todor Ljubojević, a monk in Mileševa and son of Božidar, was sent to Venice to join his brother Đurađ and to buy a printing press for the monastery. He was accompanied by Mileševa monk Sava and by Mardarije who was a hegumen of the Banja Monastery near Priboj. At that time Banja Monastery was a seat of the metropolitan bishop while Mileševa was the richest monastery of Dabar eparchy. That is why those two monasteries were given the task to finance and organize establishing of the printing house in Mileševa and why Mardarije travelled to Venice together with monks from Mileševa.\n\nMileševa printing house was operational in period 1544—1557. Three books were printed in it, Psalter (Псалтир, 1544), Breviary (Требник, 1545) and another Psalter (1557). Psalter of 1544 was edited and prepared by Mardarije and Teodor Ljubavić, based on 1519-20 Psalter of Božidar Vuković.\n\nPsalter was edited and prepared by Mardarije and Teodor Ljubavić, based on 1519-20 Psalter of Božidar Vuković. Trebnik was printed by deak Damjan and Milan from Obna (region around river Kolubara).\n\nThe third book was printed after a pause of twelve years. Because it was printed with different types, some sources say that two printing houses existed in Mileševa, both of them founded by the order of hegumen of Mileševa, Danilo.\n\n\n"}
{"id": "54034", "url": "https://en.wikipedia.org/wiki?curid=54034", "title": "Misznay–Schardin effect", "text": "Misznay–Schardin effect\n\nThe Misznay–Schardin effect (alternative spelling Misnay–Schardin), or platter effect, is a characteristic of the detonation of a broad sheet of explosive. The explosive blast expands directly away from and perpendicular to the surface of an explosive. Unlike the blast from a rounded explosive charge, which expands in all directions, the blast produced by an explosive sheet expands primarily perpendicular to its plane, in both directions. However, if one side is backed by a heavy or fixed mass, most of the blast (that is, most of the rapidly expanding gas and its kinetic energy) will be sent in the direction away from the mass.\n\nThis effect was studied and experimented with by explosive experts József Misznay (alternative spelling Misnay), a Hungarian, and Hubert Schardin, a German, who initially sought to develop a more effective anti-tank (AT) mine for Nazi Germany. Some sources claim World War II ended before their design became usable, but they and others continued their work. Misznay designed two weapons: The 43 M was an AT mine, the LŐTAK was a side-attack mine. The Hungarian army used these weapons in 1944–1945.\n\nThe AT2 and M18 Claymore mines rely on this effect.\n\n"}
{"id": "2873221", "url": "https://en.wikipedia.org/wiki?curid=2873221", "title": "National highways of Japan", "text": "National highways of Japan\n\nJapan has a nationwide system of distinct from the expressways. The Ministry of Land, Infrastructure, Transport and Tourism and other government agencies administer the national highways. Beginning in 1952, Japan classified these as Class 1 or Class 2. Class 1 highways had one- or two-digit numbers, while Class 2 highways had three-digit numbers. For example, Routes 1 and 58 were Class 1 highways while 507 (the one with the highest number) was a Class 2 highway.\n\nA 1964 amendment to the governing law resulted in a unification of the classes, which took effect in April of the following year. Highways numbered since that time have had three-digit numbers, so the numbers 58–100, which had so far been unused, remained unused. However, when Okinawa Prefecture was reverted to Japanese control in 1972, Route 58, with its southern endpoint in Okinawa's capital city of Naha, was established. The numbers from 59 to 100 remain unused. Some other numbers have been vacated by the joining or changing of routes: 109 (joined with 108), 110 (renumbered as 48), 111 (renumbered as 45), 214–216 (joined to form 57).\n\n\"Initially established as \"Class 1 highways\", except Route 58\"\n\n1 2 3 4 5 \n6 7 8 9 10 \n11 12 13 14 15 \n16 17 18 19 20 \n21 22 23 24 25 \n26 27 28 29 30 \n31 32 33 34 35 \n36 37 38 39 40 \n41 42 43 44 45 \n46 47 48 49 50 \n51 52 53 54 55 \n56 57 58\n\n101 102 103 104 105 \n106 107 108 112 113 \n114 115 116 117 118 \n119 120 121 122 123 \n124 125 126 127 128 \n129 130 131 132 133 \n134 135 136 137 138 \n139 140 141 142 143\n144 145 146 147 148 \n149 150 151 152 153 \n154 155 156 157 158 \n159 160 161 162 163 \n164 165 166 167 168 \n169 170 171 172 173 \n174 175 176 177 178 \n179 180 181 182 183 \n184 185 186 187 188 \n189 190 191 192 193 \n194 195 196 197 198 \n199\n\n200\n201 202 203 204 205 \n206 207 208 209 210 \n211 212 213 217 218 \n219 220 221 222 223 \n224 225 226 227 228 \n229 230 231 232 233 \n234 235 236 237 238 \n239 240 241 242 243 \n244 245 246 247 248 \n249 250 251 252 253 \n254 255 256 257 258 \n259 260 261 262 263 \n264 265 266 267 268 \n269 270 271 272 273 \n274 275 276 277 278 \n279 280 281 282 283 \n284 285 286 287 288 \n289 290 291 292 293 \n294 295 296 297 298 \n299\n\n300\n301 302 303 304 305 \n306 307 308 309 310 \n311 312 313 314 315 \n316 317 318 319 320 \n321 322 323 324 325 \n326 327 328 329 330 \n331 332 333 334 335 \n336 337 338 339 340\n341 342 343 344 345 \n346 347 348 349 350 \n351 352 353 354 355 \n356 357 358 359 360 \n361 362 363 364 365 \n366 367 368 369 370 \n371 372 373 374 375 \n376 377 378 379 380 \n381 382 383 384 385 \n386 387 388 389 390 \n391 392 393 394 395 \n396 397 398 399\n\n400\n401 402 403 404 405 \n406 407 408 409 410 \n411 412 413 414 415 \n416 417 418 419 420\n421 422 423 424 425 \n426 427 428 429 430 \n431 432 433 434 435 \n436 437 438 439 440 \n441 442 443 444 445 \n446 447 448 449 450 \n451 452 453 454 455 \n456 457 458 459 460 \n461 462 463 464 465 \n466 467 468 469 470 \n471 472 473 474 475 \n476 477 478 479 480 \n481 482 483 484 485\n486 487 488 489 490 \n491 492 493 494 495\n496 497 498 499\n\n500\n501 502 503 504 505\n506 507\n"}
{"id": "24622480", "url": "https://en.wikipedia.org/wiki?curid=24622480", "title": "PICMG 2.9", "text": "PICMG 2.9\n\nPICMG 2.9 is a specification by PICMG that defines an implementation of a system management bus in a CompactPCI system. This system management bus uses an I2C hardware layer, and is based on the Intelligent Platform Management Interface (IPMI) and Intelligent Platform Management Bus (IPMB) specifications.\n\nAdopted : 2/2/2000\n\nCurrent Revision : 1.0\n\nECN001 (Engineering Change Number) was adopted 5/20/2002\n"}
{"id": "39515416", "url": "https://en.wikipedia.org/wiki?curid=39515416", "title": "Polymorphic Systems (computers)", "text": "Polymorphic Systems (computers)\n\nPolyMorphic Systems was a manufacturer of microcomputer boards and systems based on the S-100 bus. Their products included the Poly-88 and the System 8813. The company was incorporated in California in 1976 as Interactive Products Corporation d/b/a PolyMorphic Systems. It was initially based in Goleta, then Santa Barbara, California.\n\nPolyMorphic Systems' first products were several interface boards based on the then-popular S-100 bus. These were compatible with other microcomputers such as the Altair 8800 and IMSAI 8080. The first was an A/D and D/A converter board. This was followed by a video terminal interface (VTI) card which became the primary display device for their systems. Later board-level products included CPU, RAM, and disk controller cards.\n\nWith the release of their CPU card, PolyMorphic began selling complete systems. Their first was the Poly-88, housed in a 5-slot S100 chassis, with additional side-mounted S-100 connectors for the purpose of joining chassis together. This unit earned the nickname \"orange toaster\" due to its orange metal cover, and the fact that the S-100 cards generated noticeable heat. The Poly-88 was available in kit form, or assembled. It was originally called the Micro-Altair, but after objections from MITS, manufacturers of the Altair, the name was changed.\n\nThe Poly-88 board set consisted of the following:\n\n\nThe Poly-88 ROM contained a boot loader program, capable of reading programs from the cassette tape interface. Available programs included games, utilities, a BASIC interpreter, and an 8080 assembler.\n\nPolyMorphic's disk-based system was the System 8813. It consisted of a larger chassis holding one, two, or three 5-inch minifloppy disk drives from Shugart Associates. The drives used single-sided, single-density storage on hard-sectored diskettes. Storage capacity was approximately 90K bytes per diskette.\n\nSystem 8813 hardware included the standard CPU and VTI cards; a RAM card, typically with at least 32K of memory; and a disk controller card, to interface with the minifloppy drives. Later, a Z80 based disk controller supported double sided, double-density minifloppy drives, and full-size (8-inch) floppy drives.\n\nThe Poly disk operating system was called Exec. The three disk drives were distinguished by numbers enclosed in angle brackets such as <1>, rather than the drive letter convention (A:) used by CP/M and later MS-DOS. File names were case-sensitive and could contain up to 31 characters including a two-character extension. For example, a text file might be named Notes.TX.\n\nVarious file extensions had predefined meanings: .GO for executable files, .BS for BASIC programs, .OV for overlays. Overlays were used extensively to provide more code space for the operating system. If a file named INITIAL.TX was present when the system booted, commands listed in that file were executed automatically, similar to the AUTOEXEC.BAT file of an MS-DOS system.\n\nLater versions of Exec supported subdirectories. The naming syntax continued to use angle brackets. For example, a file in a second-level subdirectory on drive 2 might be named <2<Projects<Dan<Accounts.TX. Unlike MS-DOS and Unix, no explicit \"make directory\" command was needed. When a program tried to create a file within a subdirectory, that subdirectory would be automatically created (if it didn't already exist).\n\nSystem 8813 software included an 8080 macro assembler and a BASIC interpreter for program development. Poly BASIC used BCD arithmetic for high precision in financial applications. A word-processing system, named WordMaster, consisted of a text editor and separate formatter program. Stuart Woods wrote his second novel, \"Run Before the Wind\", using WordMaster on a PolyMorphic 8813 system.\n\nWith the introduction of double-sided, double density minifloppy drives, the storage capacity of a single floppy became approximately 360K bytes (the same as the original IBM-PC floppy drive capacity). This made it feasible to store Exec, applications and data on a single floppy. The System 8810 was functionally identical to the 8813, but in a smaller chassis, with 5 slots and only one minifloppy drive.\n\nThe 88/MS (Mass Storage) was a cabinet housing dual, 8-inch (full size) floppy drives. It was available with either single- or double-sided disk drives, both using double-density recording on hard-sectored media. The 88/MS could be added onto either an 8813 or 8810 system. The largest Poly configurations would contain three mini-floppy drives and four full-size drives, with drive numbers from 1 to 7.\n\nThe 88/HD was a subsystem with an 18 MB SASI hard drive, housed in an 8810 chassis. Software called Volume Manager partitioned the available space into several logical disk drives, similar to the FDISK partitioning command used by other operating systems.\n\nThe TwinSystem was marketed as \"Get more work done on a computer built for two.\" The System 8813 TwinSystem had an additional RAM card, video card, and keyboard. Bank switching between the RAM cards allowed the CPU to keep two applications in memory simultaneously. However, the TTL-level keyboard interface limited the distance between the two user stations to a few feet.\n\nThe dominant operating system for microcomputers in this era was CP/M. Unmodified Poly systems were unable to run CP/M, for several reasons:\n\n\nLate in the system's lifetime, hardware modifications were introduced to solve the memory map issues, and a version of CP/M was released for the 8813.\n\n"}
{"id": "22855713", "url": "https://en.wikipedia.org/wiki?curid=22855713", "title": "Prontor-Compur", "text": "Prontor-Compur\n\nA Prontor-Compur connection (also known as a PC connector, PC terminal, or PC socket) is a standard 3.5 mm (1/8\") electrical connector (as defined in ISO 519) used in photography to synchronize the shutter to the flash.\n\n\"Prontor\" has its origins in the Italian word \"pronto\", meaning \"ready\" (and was a leaf shutter made by ). \"Compur\" is derived from the word \"compound\" (the \"\" was a long-lived series of leaf shutters made by ).\n\nThe term is derived from brands of widely marketed photographic leaf shutters manufactured from the early 1950s by two distinct, but now defunct German companies. (which made the \"Prontor-S\" and \"Prontor SV\" models, amongst others) and (the \"Synchro-Compur\" model, successor to the \"Compound\" model).\n\nBoth companies' brands, \"Prontor\" (from 1953) and \"Compur\" (from 1951), shared a common 1/8\"-inch coaxial connector for shutter/flash synchronization. This convergence of design is not as coincidental as it might first appear, owing to the fact that the Zeiss organisation held a significant shareholding in both of these companies prior to the introduction of the shared connector. By the 1950s, Gauthier were manufacturing up to 10,000 \"Prontor\" shutters daily.\n\nThe Gauthier company's essence lives on as , which is a wholly owned subsidiary of . The Deckel company went bankrupt in 1994.\n\n"}
{"id": "17071121", "url": "https://en.wikipedia.org/wiki?curid=17071121", "title": "Pudding basin", "text": "Pudding basin\n\nA pudding basin is a bowl or vessel that is specifically used to steam puddings. Typically made of glazed earthenware or tempered glass, this kitchen vessel may also be used as a mixing bowl.\n\nAvailable in various sizes and designs (the most famous of which being Cornishware striped earthenware design), pudding basins have been manufactured specifically for the steaming of puddings since the growth of manufacturers in the 17th century.\n\nPudding basins are often associated with popular historic British dishes such as Christmas Pudding, Syrup Sponge Pudding or Steak & Kidney Pudding.\n"}
{"id": "69656", "url": "https://en.wikipedia.org/wiki?curid=69656", "title": "Reactive armour", "text": "Reactive armour\n\nReactive armour is a type of vehicle armour that reacts in some way to the impact of a weapon to reduce the damage done to the vehicle being protected. It is most effective in protecting against shaped charges and specially hardened kinetic energy penetrators. The most common type is \"explosive reactive armour\" (ERA), but variants include \"self-limiting explosive reactive armour\" (SLERA), \"non-energetic reactive armour\" (NERA), \"non-explosive reactive armour\" (NxRA), and electric reactive armour. NERA and NxRA modules can withstand multiple hits, unlike ERA and SLERA, but a second hit in exactly the same location may potentially penetrate any of those.\n\nEssentially all anti-tank munitions work by piercing the armour and killing the crew inside, disabling vital mechanical systems, or both. Reactive armour can be defeated with multiple hits in the same place, as by tandem-charge weapons, which fire two or more shaped charges in rapid succession. Without tandem charges, hitting the same spot twice is much more difficult.\n\nThe idea of counterexplosion (\"kontrvzryv\" in Russian) in armour was first proposed by the Scientific Research Institute of Steel (NII Stali) in 1949 in the USSR by academician Bogdan Vjacheslavovich Voitsekhovsky\n(1922–1999). The first pre-production models were produced during the 1960s. However, insufficient theoretical analysis during one of the tests resulted in all of the prototype elements being blown up. For a number of reasons, including the accident, as well as a belief that Soviet tanks had sufficient armour, the research was ended. No more research was conducted until 1974 when the Ministry of the Defensive Industry announced a contest to find the best tank protection project.\n\nA West German researcher, Manfred Held carried out similar work with the IDF in 1967–69. Reactive armour created on the basis of the joint research was first installed on Israeli tanks during the 1982 Lebanon war and was judged very effective.\n\nAn element of explosive reactive armour consists of a sheet or slab of high explosive sandwiched between two plates, typically metal, called the reactive or dynamic elements. On attack by a penetrating weapon, the explosive detonates, forcibly driving the metal plates apart to damage the penetrator. Against a shaped charge, the projected plates disrupt the metallic jet penetrator, effectively providing a greater path-length of material to be penetrated. Against a kinetic energy penetrator, the projected plates serve to deflect and break up the rod.\n\nThe disruption is attributed to two mechanisms. First, the moving plates change the effective velocity and angle of impact of the shaped charge jet, reducing the angle of incidence and increasing the effective jet velocity versus the plate element. Second, since the plates are angled compared to the usual impact direction of shaped charge warheads, as the plates move outwards the impact point on the plate moves over time, requiring the jet to cut through fresh plate material. This second effect significantly increases the effective plate thickness during the impact.\n\nTo be effective against kinetic energy projectiles, ERA (Explosive Reactive Armour) must use much thicker and heavier plates and a correspondingly thicker explosive layer. Such \"heavy ERA,\" such as the Soviet-developed Kontakt-5, can break apart a penetrating rod that is longer than the ERA is deep, again significantly reducing penetration capability.\n\nAn important aspect of ERA is the brisance, or detonation speed of its explosive element. A more brisant explosive and greater plate velocity will result in more plate material being fed into the path of the oncoming jet, greatly increasing the plate's effective thickness. This effect is especially pronounced in the rear plate receding away from the jet, which triples in effective thickness with double the velocity.\n\nERA also counters explosively forged projectiles, as produced by a shaped charge. The counter-explosion must disrupt the incoming projectile so that its momentum is distributed in all directions rather than towards the target, greatly diminishing its effectiveness.\n\nExplosive reactive armour has been valued by the Soviet Union and its now-independent component states since the 1980s, and almost every tank in the eastern-European military inventory today has either been manufactured to use ERA or had ERA tiles added to it, including even the T-55 and T-62 tanks built forty to fifty years ago, but still used today by reserve units. The U.S. Army uses reactive armour on its Abrams tanks as part of the TUSK (Tank Urban Survivability Kit) package and on Bradley vehicles and the Israelis use it frequently on their American built M60 tanks.\n\nERA tiles are used as add-on (or \"appliqué\") armour to the portions of an armoured fighting vehicle that are most likely to be hit, typically the front (glacis) of the hull and the front and sides of the turret. Their use requires that a vehicle be fairly heavily armoured to protect itself and its crew from the exploding ERA.\n\nA further complication to the use of ERA is the inherent danger to anyone near the tank when a plate detonates, disregarding that a high explosive anti-tank (HEAT) warhead explosion would already cause great danger to anyone near the tank. Although ERA plates are intended only to bulge following detonation, the combined energy of the ERA explosive, coupled with the kinetic or explosive energy of the projectile, will frequently cause explosive fragmentation of the plate. The explosion of an ERA plate creates a significant amount of shrapnel, and bystanders are in grave danger of fatal injury. Thus, infantry must operate some distance from vehicles protected by ERA in combined arms operations.\n\nNERA and NxRA operate similarly to explosive reactive armour, but without the explosive liner. Two metal plates sandwich an inert liner, such as rubber. When struck by a shaped charge's metal jet, some of the impact energy is dissipated into the inert liner layer, and the resulting high pressure causes a localized bending or bulging of the plates in the area of the impact. As the plates bulge, the point of jet impact shifts with the plate bulging, increasing the effective thickness of the armour. This is almost the same as the second mechanism that explosive reactive armour uses, but it uses energy from the shaped charge jet rather than from explosives.\n\nSince the inner liner is non-explosive, the bulging is less energetic than on explosive reactive armour, and thus offers less protection than a similarly-sized ERA. However, NERA and NxRA are lighter, safe to handle, safer for nearby infantry, can theoretically be placed on any part of the vehicle, and can be packaged in multiple spaced layers if needed. A key advantage of this kind of armour is that it cannot be defeated via tandem warhead shaped charges, which employ a small forward warhead to detonate ERA before the main warhead fires.\n\nA new technology called \"electric reactive armour\" (also termed \"electromagnetic reactive armour\", or colloquially as \"electric armour\") is in development. This armour is made up of two or more conductive plates separated by an air gap or by an insulating material, creating a high-power capacitor. In operation, a high-voltage power source charges the armour. When an incoming body penetrates the plates, it closes the circuit to discharge the capacitor, dumping a great deal of energy into the penetrator, which may vaporize it or even turn it into a plasma, significantly diffusing the attack. It is not public knowledge whether this is supposed to function against both kinetic energy penetrators and shaped charge jets, or only the latter. This technology has not yet been introduced on any known operational platform. \n\nAnother electromagnetic alternative to ERA uses layers of plates of electromagnetic metal with silicone spacers on alternate sides. The damage to the exterior of the armour passes electricity into the plates causing them to magnetically move together. As the process is completed at the speed of electricity the plates are moving when struck by the projectile causing the projectile energy to be deflected whilst the energy is also dissipated in parting the magnetically attracted plates.\n\nSome active protection systems, like the German AMAP-ADS or the American Iron Curtain, have explosive modules that destroy projectiles with a directed energy explosion. This type of explosive module is similar in concept to the classic ERA, but without the metal plates and with directed energy explosives. The composition materials of the explosive modules are not public. The designer of the AMAP-ADS claims that his AMAP-ADS systems are effective against EFP and APFSDS.\n\n\nFor analyzing reactive plate velocities, the Gurney equations are commonly used.\n\n\n"}
{"id": "41524596", "url": "https://en.wikipedia.org/wiki?curid=41524596", "title": "Sandra Carpenter", "text": "Sandra Carpenter\n\nSandra Mitchell Carpenter (1934–2003) was an American corporate executive, engineer, and information technology professional. During the 1980s, Carpenter served as corporate vice president for information management systems at Hilton Hotels. She was one of the first women to serve as a chief information officer (CIO) at a company with more than $1 billion in revenue. During her tenure at Hilton, CIO magazine named the company to their list of the top ten travel service IT Innovators. Carpenter also received placement on CIO Magazine's list of the top 100 CIOs.\n\nCarpenter was born in Des Moines, Iowa on February 4, 1934. She grew up in several Midwestern cities and then in Short Hills, New Jersey. In 1952, Carpenter graduated from the Beard School in Orange, NJ (now Morristown-Beard School). She then completed her bachelor's degree in literature at Smith College in Northampton, MA in 1956. Carpenter co-chaired a reception of the Smith College Clubs of Los Angeles and Pasadena to launch the $125 million fundraising campaign for the school in 1988.\n\nIn the 1960s, Carpenter joined IBM as a systems engineer, instructor, and sales representative. She was the first woman to take on all three of these roles at the company. Carpenter later served as director of information systems at Quanex Corp., an industrial company in Houston, Texas, and chief information officer at Rosenbluth Travel in Philadelphia during the 1990s.\n\nRosenbluth, the fifth largest travel agency in the U.S. at the time, had a corporate client list that included Walmart, DuPont, Nike, Inc., and Chevron Corporation. (After its 2003 acquisition by American Express, Rosenblauth's operations now work as the American Express Business Travel division.) Carpenter led the automation of Rosenblauth's booking system, which had previously relied on manual entry. In a 1992 profile of her work written by \"CIO\" magazine, Carpenter outlined five critical elements for IT innovation at Rosenblauth:\n\n\nCarpenter founded the first woman's shelter in the Detroit, Michigan suburbs. She helped organize the Michigan chapter of the National Organization for Women (NOW) and served as the founding president of NOW's Oakland County chapter. in 1975, the Wayne County chapter of NOW awarded Carpenter their Feminist of the Year Award.\n\nAfter her college studies, Sandra Carpenter married Nick de Kuyper and moved to Europe to live with de Kuyper. They had one son, John. They divorced in 1961. In 1964, Sandra Carpenter married widower Robert Carpenter. She adopted his four children: Robert, John, Kristin and Charles. Sandra Carpenter and Robert Carpenter divorced in 1980.\n"}
{"id": "877070", "url": "https://en.wikipedia.org/wiki?curid=877070", "title": "Sha-Mail", "text": "Sha-Mail\n\nSha-Mail () is a kind of mailing service of J-Phone (now Softbank). This term is made from Sha, which is the front part of a Japanese word \"Shashin\" meaning photograph, and Mail. Sha-Mail is used to send pictures and/or email through mobile handsets.\n\nThe term came into usage after Softbank (then Vodafone) introduced the service in November, 2000. \n\nIn current usage, sha-mail refers not only to Softbank, but has come to apply to all the mobile phone carriers, as well as to mail with accompanying picture sent via other hand held devices and PCs. The term has come to mean not only mail with pictures attached, but is also sometimes used to refer to the pictures themselves that have been taken with camera phones and other such devices. It can be abbreviated to sha-me ()\n"}
{"id": "7187296", "url": "https://en.wikipedia.org/wiki?curid=7187296", "title": "Space Integrated GPS/INS", "text": "Space Integrated GPS/INS\n\nSpace Integrated GPS/INS (SIGI) is a strapdown Inertial Navigation Unit (INU) developed and built by Honeywell International to control and stabilize spacecrafts during flight.\n\nSIGI has integrated global positioning and inertial navigation technology to provide three navigation solutions : \"Pure inertial, GPS-only and blended GPS/INS\".\n\nSIGI have been employed on the International Space Station, the Japanese H-II Transfer Vehicle (HTV) the Boeing X-37 and X-40.\n\nSIGI is also proposed as the primary navigation system for Orion, which is scheduled to replace the Space Shuttle.\n\n\n"}
{"id": "17297866", "url": "https://en.wikipedia.org/wiki?curid=17297866", "title": "Spatial organization", "text": "Spatial organization\n\nSpatial organization can be observed when components of an abiotic or biological group are arranged non-randomly in space. Abiotic patterns, such as the ripple formations in sand dunes or the oscillating wave patterns of the Belousov-Zhabotinsky reaction emerge after thousands of particles interact millions of times. On the other hand, individuals in biological groups may be arranged non-randomly due to selfish behavior, dominance interactions, or cooperative behavior. W. D. Hamilton (1971) proposed that in a non-related \"herd\" of animals, spatial organization is likely a result of the selfish interests of individuals trying to acquire food or avoid predation. On the other hand, spatial arrangements have also been observed among highly related members of eusocial groups, suggesting that the arrangement of individuals may provide some advantage for the group.\n\nIndividuals in a social insect colony can be spatially organized, or arranged non-randomly inside the nest. These miniature territories, or spatial fidelity zones have been described in honey bees (\"Apis mellifera\"), ants (\"Odontomachus brunneus\"; \"Temnothorax albipennis\"; \"Pheidole dentata\"), and paper wasps (\"Polistes dominulus\", \"Ropalidia revolutionalis\"). While residing in these zones, workers perform the task appropriate to the area they reside. For example, individuals that remain in the center of an ant nest are more likely to feed larvae, whereas individuals found at the periphery of the nest are more likely to forage. E. O. Wilson proposed that by remaining in small, non-random areas inside the nest, the distance an individual moves between tasks may be minimized, and overall colony efficiency would increase.\n\nThere are a variety of ways in which individuals can divide space inside a nest. According to the “foraging-for-work” hypothesis, adult workers begin performing tasks in the area of the nest where they emerged, and gradually move towards the periphery of the nest as demands to perform particular tasks change. This hypothesis is based on two observations: \"(1) that there is spatial structure in the layout of tasks in social insect colonies and (2) that workers first become adults in or around the center of the nest\". Individuals can remain in an area for an extended period of time, as long as tasks need to be performed there. Over time, an individual’s zone may shift as tasks are accomplished and workers search for other areas where tasks need to be performed. Honey bees, for example, begin their adult life caring for brood located in the area near where they emerged (i.e. nurse bees). Eventually, workers move away from the brood rearing area and begin to perform other tasks, such as food storage, guarding, or foraging.\n\nSpace inside the nest may also be divided as a result of dominance interactions. For example, in paper wasp colonies, a single inseminated queen may found (initiate) a colony after waking up from hibernation (overwintering). However, it is common in many species that multiple inseminated females join these foundresses instead of founding their own nest. When multiple inseminated females found a colony together, the colony grows quickly, yet only one individual will become the primary egg-layer. Through a series of dominance interactions, the most aggressive wasp will emerge as the dominant individual and will become the primary egg-layer for the group (the prime role for ensuring your genes are passed on to subsequent generations), whereas the remaining subordinate wasps will perform other tasks, such as nest construction or foraging. There is evidence that these dominance interactions affect the spatial zones individuals occupy as well. In paper wasps (\"Ropalidia revolutionalis\"), as well as in the ant species \"Odontomachus brunneus\", dominant individuals are more likely to reside in the central areas of the nest, where they take care of brood, while the subordinate individuals are pushed towards the edge, where they are more likely to forage. It is unknown whether division of space or establishment of dominance occurs first and if the other is a result of it.\n\nThere is also evidence that foragers, which are the insects that leave the nest to collect the valuable resources for the developing colony, can divide space outside the nest. Makino & Sakai showed that bumble bee foragers maintain foraging zones in flower patches, which means that bees consistently return to the same areas within a patch and there is little overlap between individuals. These zones can expand and contract when neighboring foragers are removed or introduced, respectively. By dividing foraging patches into miniature ‘foraging territories’, individuals can maximize the number of flowers visited with minimal interruptions or competition between foragers. These ‘foraging territories’ divided among individuals from the same colony are the result of self-organization among the foragers; that is, there is no lead forager dictating where the bees will forage. Instead, the maintenance of these foraging zones is due to simple rules followed by each individual forager. Studies to determine these “rules” are an important area of research in computer science, basic biology, behavioral ecology, and mathematic modeling.\n\nThe self-organization observed in foraging territories is a microcosm for the self-organization seen in the entire colony. Spatial organization observed across social insect colonies can be considered an emergent property of a self-organized complex system. It is self-organized because there is no leader dictating where each individual will reside, nor which task an individual will perform once they get there. Instead, zones may be a by-product of division of labor, whereby individuals end up in a particular location for a period of time based on the task they perform, or dominance interactions, whereby dominant individuals are granted access to the most desirable places inside the nest. Spatial patterns exhibited by individuals of social insect colonies are not obvious, because it is difficult to observe and differentiate among individuals inside a nest cavity or flying across a foraging patch. However, when careful attention is given to the individual worker, the spatial organization of workers in the nest becomes apparent.\n\n"}
{"id": "11523749", "url": "https://en.wikipedia.org/wiki?curid=11523749", "title": "Spinplasmonics", "text": "Spinplasmonics\n\nSpinplasmonics is a field of nanotechnology combining spintronics and plasmonics. The field was pioneered by Professor Abdulhakem Elezzabi at the University of Alberta in Canada. In a simple spinplasmonic device, light waves couple to electron spin states in a metallic structure. The most elementary spinplasmonic device consists of a bilayer structure made from magnetic and nonmagnetic metals. It is the nanometer scale interface between such metals that gives rise to an electron spin phenomenon. The plasmonic current is generated by optical excitation and its properties are manipulated by applying a weak magnetic field. Electrons with a specific spin state can cross the interfacial barrier, but those with a different spin state are impeded. Essentially, switching operations are performed with the electrons spin and then sent out as a light signal.\n\nSpinplasmonic devices potentially have the advantages of high speed, miniaturization, low power consumption, and multifunctionality. On a lengthscale that is less than a single magnetic domain size, the interaction between atomic spins realigns the magnetic moments. Unlike semiconductor-based devices, smaller spinplasmonics devices are expected to be more efficient in transporting the spin-polarized electron current.\n\n\n"}
{"id": "48592395", "url": "https://en.wikipedia.org/wiki?curid=48592395", "title": "Stirrup pump", "text": "Stirrup pump\n\nA stirrup pump is a portable water pump used to extinguish or control small fires. It is operated by hand. The operator places a foot on a stirrup-like bracket at the bottom of the pump to hold the pump steady.\n\n"}
{"id": "3095091", "url": "https://en.wikipedia.org/wiki?curid=3095091", "title": "Sydney Accord", "text": "Sydney Accord\n\nThe Sydney Accord is an international mutual recognition agreement for qualifications in the fields of engineering technology.\n\nThe Sydney Accord is an agreement between the bodies responsible for accrediting engineering technologist qualification programs in each of the signatory countries. \nIt recognizes the substantial equivalency of programs accredited by those bodies, and recommends that graduates of accredited programs in any of the signatory countries be recognized by the other countries as having met the academic requirements for entry to the practice of engineering technologist. The Sydney Accord was signed in 2001.\n\nThe Sydney Accord covers engineering technologist qualifications.\n\nThe scope of the Sydney Accord only covers the \"academic\" requirement for an engineering technologist qualification. Engineering technologist titles do not transfer directly between signatory countries that don't have reciprocating agreements, because the signatory countries reserve the right to scrutinize foreign titles and compare them to their own licensing criteria. However, this does not mean the titles are not respected by employers within those signatory countries.\n\nThe engineering technologist may be hired within a country by an employer where a formal license is not required. The \"industrial exemption\" clause negates formal engineering registration within the United States for those who meet the criteria.\n\nForeign titles may be utilized as a foundation for recognition of professional licensing. The titles can be supplemented with additional experience and/or training to meet the local definition of formal registration. This serves to underline that a foreign technologist covered under the accord does not arrive in a fellow signatory country without merit. The Sydney Accord is therefore not a hollow agreement without advantages.\n\nThe Canadian Council of Technicians and Technologists (CCTT) and the United Kingdom's Institution of Incorporated Engineers (IIE) signed a reciprocating agreement of recognition for engineering technologist. In 2006, the IIE merged with the Institute of Electrical Engineers (IEE) to form the Institution of Engineering and Technology (IET). The CCTT also signed a reciprocating agreement with the National Institute for Certification in Engineering Technologies (NICET). NICET is a United States organization sponsored by the National Society of Professional Engineers (NSPE). The formal recognition of the CCTT as a common link between NICET and the IET has not been realized.\n\nThe signatory countries/territories of the Sydney Accord are: \n\nCanada has signed the Sydney Accord with the title of \"Applied Science\" and \"Engineering Technologist\".\n\nThe Canadian signatory body is the Canadian Council of Technicians and Technologists (CCTT). This is different from the Canadian signatory of the Washington Accord, Engineers Canada.\n\nIn the case of all other Sydney Accord members, the same organization has signed both the Sydney Accord and the Washington Accord.\n\nHong Kong originally signed the Sydney Accord with the title of \"Science Technologist\" and later abbreviated the title to \"Technologist.\"\n\nThe United States applied for recognition with the Sydney Accord in 2007 and was granted that status in 2009. Despite this achievement the United States still has significant confusion in defining a unified technologist registration for professionals. Part of the reason for this is that the engineering technology profession is not well defined as a separate profession (distinct from professional engineering) in the United States. This is because the NSPE has opposed legal registration of technologist by the United States government through a licensing program. The loss of government oversight has led to competing ideologies from societies with different perspectives on what represents the qualities of a technologist.\n\nSome legitimate societies and organizations that have established technology programs do not have clear representation in the accord. The Society of Manufacturing Engineers (SME), and the Society of Broadcast Engineers (SBE) are two organizations that have engineering technology and technician certifications that are respected and recognized but operate independently from the accord. In addition to these societies there are legitimate accreditations that are unacknowledged. They are the Distance Education and Training Council (DETC), the National Association of Industrial Technology (NAIT), the Accrediting Commission of Career Schools and Colleges of Technology (ACCSCT), or other non-ABET/TAC institutions that are exclusively regionally accredited. It is unclear if these organizations or societies will eventually be represented by a formal avenue of recognition in the Sydney Accord.\n\nAs an international representative of the accord the UK offer a registration program for individuals from any country. However, since the standards for technologist are higher in UK (B.Sc. and B.Eng. in Engineering) only 25 UK (emigrating) registrants up to this date applied for registration as \"Technologist\" indicating the failure of the Accord within UK. In the UK the term \"engineer\", \"professional engineer\" or \"engineering\" have no meaning in law so anyone can call themselves a professional engineer, or technologist without restrictions. However the titles \"Chartered Engineer\", \"Incorporated Engineer\" and \"Engineering Technician\" awarded by The Engineering Council (UK) are protected by law. United States' graduates may apply for a peer review by the Engineering Council UK if they belong to one of the organizations or societies that are not explicitly mentioned as a member of the accord. Individuals that graduate from a regionally accredited technology program are likely to receive acceptance through professional engineering registration as an Incorporated Engineer.\n\nU.S. professional registration is a state concern. While the profession of engineering technologist is not specifically recognised, many states provide engineering technologists with a pathway towards \"Professional Engineer (PE)\" licensing that bypasses national engineering requirements. This is opposed by the national regulatory and representative bodies for professional engineers, the National Society of Professional Engineers (NSPE). Notably, the Washington Accord does not apply to American PEs who have obtained this status through a technologist route.\n\n\n"}
{"id": "56256054", "url": "https://en.wikipedia.org/wiki?curid=56256054", "title": "Testbed aircraft", "text": "Testbed aircraft\n\nA testbed aircraft is an aeroplane, helicopter or other kind of aircraft intended for flight research or testing the aircraft concepts or on-board equipment. These could be specially designed or modified from serial production aircraft.\n\nFor example, in development of new aircraft engines, these are fitted to a testbed aircraft for flight testing, before certification. For this adaptation it is required, among other changes, that new instrumentation wiring and equipment, fuel system and piping, as well as structural modifications of wing. AlliedSignal, Honeywell Aerospace and Pratt & Whitney all used Boeing jetliners as flying testbed aircraft. A large number of aircraft testbeds have been produced and tested since 1941 in the USSR and Russia by the Gromov Flight Research Institute.\n\n"}
{"id": "52721848", "url": "https://en.wikipedia.org/wiki?curid=52721848", "title": "TopstepTrader", "text": "TopstepTrader\n\nTopstepTrader is a financial technology firm based in Chicago, IL that evaluates day traders’ performance in real-time simulated accounts. Traders who pass the company’s evaluation earn a funded trading account and trade futures contracts in the financial markets using the firm’s capital.\n\nTopstepTrader currently operates out of their headquarters in the West Loop Gate neighborhood of Chicago at 130 S. Jefferson St. Employees work out of the 10,000+ sq. ft. loft space in downtown Chicago.\n\nTopstepTrader was started on the floor of the Chicago Board of Trade by current CEO and former Dow futures contracts floor trader, Michael Patak, and became an LLC (Limited Liability Company) in July, 2012. According to the Omaha World Herald, the name TopstepTrader was chosen because the best traders within each trading pit stand on the top step, where they have the best view and could theoretically receive the best market prices.\n\nTopstepTrader’s funding process consists of three steps. The first step is an evaluation of the trader's profitability, the second step proves the trader's risk management, and the last step is the funded trading account.\n\nTopstepTrader produces a daily Squawk Radio trading broadcast through an online chat platform.\n\nTopstepTrader has been awarded 101 Best and Brightest Companies to Work For in 2016, 2016 Chicago Innovation Awards Finalist, and 2016 FIA (Futures Industry Association) Innovator. Founder and CEO Michael Patak was a 2015 semi-finalist and 2016 finalist for the Ernst & Young Entrepreneur of the Year Award. In 2017, Inc. Magazine ranked TopstepTrader No. 1,261 on its 36th annual Inc. 5000.\n\nTopstepTrader has appeared in news sources including Forbes, CNBC, Bloomberg, Fox Business, MarketWatch, and Built in Chicago.\n"}
{"id": "8885355", "url": "https://en.wikipedia.org/wiki?curid=8885355", "title": "Total Hi Def", "text": "Total Hi Def\n\nTotal Hi Def Disc, also called Total HD or THD, was a planned optical disc format that included both of the rival high-definition optical disc formats, Blu-ray Disc and HD DVD. It was officially announced January 8, 2007 at the Warner Bros. press conference held at CES 2007. One side was to contain a single or dual-layer Blu-ray Disc, and the other was to contain a single or dual-layer HD DVD.\n\nHowever, in November 2007, Warner Brothers put development of the Total HD discs on hold for an indefinite amount of time after several delays. Warner announced one of the reasons for doing so as a lack of interest in it from other major studios. Finally, on January 4, 2008, Warner Bros. Pictures announced that they were discontinuing HD DVD support, putting an end to THD's development.\n\n"}
{"id": "1599001", "url": "https://en.wikipedia.org/wiki?curid=1599001", "title": "Tucker Telephone", "text": "Tucker Telephone\n\nThe Tucker Telephone is a torture device designed using parts from an old-fashioned crank telephone. The electric generator of the telephone is wired in sequence to two dry cell batteries so that the instrument can be used to administer electric shocks to another person. The Tucker Telephone was invented by Dr. A. E. Rollins, the resident physician at the Tucker State Prison Farm, Arkansas, in the 1960s.\n\nAt the Tucker State Prison Farm, an inmate would be taken to the \"hospital room\" where he was most likely restrained to an examining table and two wires would be applied to the prisoner. The ground wire was wrapped around the big toe and the \"hot wire\" (the wire that administers the current of electricity) would be applied to the genitals. The crank on the phone would then be turned, and an electric current would shoot into the prisoner's body. Continuing with the telephone euphemisms, 'long-distance calls' referred to several such charges, just before the point of losing consciousness. Often the victim would experience detrimental effects, mainly permanent organ damage and mental health problems. Its use was substantiated until 1968.\n\nThere are scattered reports from American Vietnam War veterans that field phones were occasionally converted into Tucker Telephones which were used by platoon commanders to torture Viet Cong prisoners.\n\nA version of the device is used on a prisoner in the Robert Redford film \"Brubaker.\"\n\nA 1974 report by Seth B. Goldsmith, SCD noted \"The Tucker telephone not only shocked the penises of the allegedly uncooperative and incorrigible prison-farm inmates of the Arkansas penal system, but it shocked the consciousness of the nation and awakened it to the atrocious conditions inside prisons.\"\n\n"}
{"id": "10266841", "url": "https://en.wikipedia.org/wiki?curid=10266841", "title": "Wet bottom furnace", "text": "Wet bottom furnace\n\nA Wet-bottom furnace or wet-bottom boiler is boiler that contains a wet bottom furnace. It is a kind of boiler used for pulverised fuel firing.\nIn wet bottom boiler the bottom ash is kept in a molten state and tapped off as a liquid. Wet bottom boiler slag is the molten condition ash as it is drawn from the bottom of the slag-tap or cyclone furnaces.\nAn advantage is the fact that the end product in this process has a higher value compared to that of a dry bottom boiler. Wet bottom boilers are preferred for low volatile coals that produce a lot of ash. But it has higher investment costs and higher maintenance costs, so it is built less often.\n\n\nIf the ash fusion temperature is less than the furnace temperature then that type of furnace is called a wet bottom furnace.\n\n"}
{"id": "56972556", "url": "https://en.wikipedia.org/wiki?curid=56972556", "title": "William James (engineer)", "text": "William James (engineer)\n\nWilliam James (born 15 November 1854; died 16 February 1889) was a British engineer, who worked in India.\n\nWilliam James was son of Edward James, J.P., of Greenbank House, Plymouth. He was educated at the nearby private school of Dr. P. Holmes, of Mannamead. From 1872, he served a pupilage of three years to S. W. Jenkin, during which he was engaged in connection with the parliamentary and working plans and sections of several branch lines for the Cornwall Minerals Railway, the Fal Valley Railway and the Truro and Penzance Railway. For one year from 1875 to 1876, he was a student at the School of Practical Engineering at Crystal Palace under W. J. Wilson. In 1876 he went to India, however, failing to obtain employment as an engineer, he became a pupil of H. Whymper, of the Murree Brewery, and was occupied for fifteen months on the water supply for the Murree Brewery Company in the Punjab.\n\nSubsequently, he proceeded to Calcutta, where he was engaged by Mitchell and Co., engineers and contractors, of which he became a partner in 1881. He was first employed in building bungalows, machine-shops, engine-sheds for the Northern Bengal State Railway at Saidpore, and afterwards, during the year 1879, was in charge of the construction of 10 miles of the Darjeeling Steam Tramway. He was also concerned in the making of a road through the Teesta Valley and on the Bengal Central Railway, the Calcutta drainage works, the Dacca and Mymensing Railway, and the Patna and Bankipore Tramway. \n\nDuring the last two years of his life he was the resident partner in the firm of Walsh, Lovett, Mitchell and Co., contractors for the Tansa water-works, for bringing a large supply of good water to the City of Bombay. During this time he had in the working season from six thousand to ten thousand men under his charge, but the anxiety arising from his responsibilities acting on a frame already much weakened by jungle fever, was too much for him, and he died somewhat suddenly from heat apoplexy at Vasind on 16 February 1889, in his thirty-fifth year.\n"}
