{"id": "12148968", "url": "https://en.wikipedia.org/wiki?curid=12148968", "title": "Aerodynamic Park", "text": "Aerodynamic Park\n\nThe Aerodynamic Park () of the Humboldt University in Berlin-Adlershof was once part of the German Johannisthal Air Field.\n\nLaboratories, motor test beds, wind tunnels and hangars, erected in the 20s and 30s, are historical landmarks of the Aerodynamic Park today. On the grassy areas of the park is the site-specific sound art \"Air Borne\" (Stefan Krüskemper with the collaboration of Karlheinz Essl) located, in a loose spatial relation to the monuments of the German Experimental Institute for Aviation and the exceptional university buildings (Architects: Volker Staab with Alfred Nieuwenhuizen, Georg Augustin and Ute Frank, a.o.). The park is the green center of the Campus Berlin-Adlershof.\n"}
{"id": "10215849", "url": "https://en.wikipedia.org/wiki?curid=10215849", "title": "Atomistix ToolKit", "text": "Atomistix ToolKit\n\nAtomistix ToolKit (ATK) is a commercial software for atomic-scale modeling and simulation of nanosystems. The software was originally developed by Atomistix A/S, and was later acquired by QuantumWise following the Atomistix bankruptcy.\n\nAtomistix ToolKit is a further development of TranSIESTA-C, which in turn in based on the technology, models, and algorithms developed in the academic codes TranSIESTA, Physical Review B 65, 165401 (2002).</ref> and McDCal, employing localized basis sets as developed in SIESTA.\n\nAtomistix ToolKit combines density functional theory with non-equilibrium Green's functions for first principles electronic structure and transport calculations of \n\nThe key features are\n\n\n"}
{"id": "46462085", "url": "https://en.wikipedia.org/wiki?curid=46462085", "title": "Bowers Group", "text": "Bowers Group\n\nBowers Group founded in 1915 manufacture Test equipment and Measuring instruments. The group includes Baty International, Bowers Export, Bowers UK, Moore & Wright and CV Instruments.\n\nOriginally known as 'Bowers Precision Engineers', established as a toolmaker in Bradford in 1915. In 1994 it developed a 3-point internal bore micrometer. In 1979 'Bowers Internal Gauge Company Limited' was found by Roger Bowers. The company moved to Bradford, West Yorkshire in 1989. In 2000, Bowers Metrological Group was founded. In 2009, Bowers assisted the Nuclear Advanced Manufacturing Research Centre accurately measure internal screw threads. Baty International was acquired in 2010.\n"}
{"id": "2036147", "url": "https://en.wikipedia.org/wiki?curid=2036147", "title": "Chip-scale package", "text": "Chip-scale package\n\nA chip scale package or chip-scale package (CSP) is a type of integrated circuit package. \n\nOriginally, CSP was the acronym for \"chip-size packaging.\" Since only a few packages are chip size, the meaning of the acronym was adapted to \"chip-scale packaging\". According to IPC's standard J-STD-012, \"Implementation of Flip Chip and Chip Scale Technology\", in order to qualify as chip scale, the package must have an area no greater than 1.2 times that of the die and it must be a single-die, direct surface mountable package. Another criterion that is often applied to qualify these packages as CSPs is their ball pitch should be no more than 1 mm. \n\nThe concept was first proposed by Junichi Kasai of Fujitsu and Gen Murakami of Hitachi Cable in 1993. The first concept demonstration however came from Mitsubishi Electric.\n\nThe die may be mounted on an interposer upon which pads or balls are formed, like with flip chip ball grid array (BGA) packaging, or the pads may be etched or printed directly onto the silicon wafer, resulting in a package very close to the size of the silicon die: such a package is called a wafer-level package (WLP) or a wafer-level chip-scale package (WL-CSP). WL-CSP had been in development since 1990s, and several companies begun volume production in early 2000, such as Advanced Semiconductor Engineering (ASE).\n\nChip scale packages can be classified into the following groups:\n\n"}
{"id": "44846612", "url": "https://en.wikipedia.org/wiki?curid=44846612", "title": "Classical nucleation theory", "text": "Classical nucleation theory\n\nNucleation is the first step in the formation of either a new thermodynamic phase or a new structure with lower free energy via self-assembly or self-organisation. Nucleation is typically defined to be the process that determines how long we have to wait before the new phase or self-organised structure appears. Classical nucleation theory (CNT) is the most common theoretical model used to understand why nucleation may take hours, years, or never happen at all.\n\nThis is the standard simple theory for nucleation of a new thermodynamic phase, such as a liquid or a crystal. It should be borne in mind that it is approximate. The basic CNT nucleation of a new phase provides an approximate but physically reasonable prediction for the rate at which nuclei of a new phase form, via nucleation on a set of identical nucleation sites. This rate, \"R\" is the number of, for example, water droplets nucleating in a uniform volume of air supersaturated with water vapour, per unit time. So if a 100 droplets\nnucleate in a volume of 0.1m in 1s, then the rate \"R\"=1000/s. The description here follows modern\nstandard CNT. The prediction for the rate \"R\" is\n\nwhere\n\nThis expression for the rate can be thought of as a product of two factors: The first, formula_6, is the number of nucleation sites multiplied by the probability that a nucleus of critical size has grown around it. It can be interpreted as the average, instantaneous number of nuclei at the top of the nucleation barrier. Free energies and probabilities are closely related in general, by definition. The probability of a nucleus forming at a site is proportional to formula_7. So if formula_2 is large and positive the probability of forming a nucleus is very low and nucleation will be slow. Then the average number will be much less than one, i.e., it is likely that at any given time none of the sites has a nucleus.\n\nThe second factor in the expression for the rate is the dynamic part, formula_9. Here, formula_10 expresses the rate of incoming matter and formula_5 is the probability that a nucleus of critical size (at the maximum of the energy barrier) will continue to grow and not dissolve. The Zeldovich factor is derived by assuming that the nuclei near the top of the barrier are effectively diffusing along the radial axis. By statistical fluctuations, a nucleus at the top of the barrier can grow diffusively into a larger nucleus that will grow into a new phase, or it can lose molecules and shrink back to nothing. The probability that a given nucleus goes forward is formula_5.\n\nTo see how this works in practice we can look at an example. Sanz and coworkers\nhave used computer simulation to estimate all the quantities in the above equation, for the nucleation of ice in liquid water. They did this for a simple but approximate model of water called TIP4P/2005. At a supercooling of 19.5 °C, i.e., 19.5 °C below the freezing point of water in their model, they estimate a free energy barrier to nucleation of ice of formula_13. They also estimate a rate of addition of water molecules to an ice nucleus near the top of the barrier of \"j\" = 10/s and a Zeldovich factor \"Z\" = 10 (note that this factor is dimensionless because it is basically a probability). The number of water molecules in 1 m of water is approximately 10. Putting all these numbers into the formula we get a nucleation rate of approximately 10/s. This means that on average we would have to wait 10s (10 years) to see a single ice nucleus forming in 1 m of water at -20 °C!\n\nThis is a rate of homogeneous nucleation estimated for a model of water, not real water—in experiments we cannot grow nuclei of water and so cannot directly determine the values of the barrier \"ΔG*\", or the dynamic parameters such as \"j\", for real water. However, it may be that indeed the homogeneous nucleation of ice at temperatures near -20 °C and above is \"extremely\" slow and so that whenever we see water freezing temperatures of -20 °C and above this is due to heterogeneous nucleation, i.e., the ice nucleates in contact with a surface.\n\nHomogeneous nucleation is much rarer than heterogeneous nucleation. However, homogeneous nucleation is simpler and easier to understand than heterogeneous nucleation, so the easiest way to understand heterogeneous nucleation is to start with homogeneous nucleation. So we will outline the CNT calculation for the homogeneous nucleation barrier formula_2.\n\nTo understand if nucleation is fast or slow, formula_15 needs to be calculated. The classical theory assumes that even for a microscopic nucleus of the new phase, we can write the free energy of a droplet formula_16 as the sum of a bulk term that is proportional to the volume of the nucleus, and a surface term, that is proportional to its surface area\n\nThe first term is the volume term, and as we are assuming that the nucleus is spherical, this is the volume of a sphere of radius formula_18.\nformula_19 is the difference in free energy per unit volume between the thermodynamic phase nucleation is occurring in, and the phase that is nucleating. For example, if water is nucleating in supersaturated air, then formula_19 is the free energy per unit volume of the supersaturated air minus that of water at the same pressure. As nucleation only occurs when the air is supersaturated, formula_19 is always negative. The second term comes from the interface at surface of the nucleus, which is why it is proportional to the surface area of a sphere. formula_22 is the surface tension of the interface between the nucleus and its surroundings, which is always positive.\n\nFor small formula_18 the second surface term dominates and formula_24. The free energy is the sum of an formula_25 and formula_26 terms. Now the formula_26 terms varies more rapidly with formula_18 than the formula_25 term, so\nas small formula_18 the formula_25 term dominates and the free energy is positive while for large formula_18, the formula_26 term dominates and the free energy is negative. This shown in the figure to the right. Thus at some intermediate value of formula_18, the free energy goes through a maximum, and so the probability of formation of a nucleus goes through a minimum.\nThere is a least-probable nucleus occurs, i.e., the one with the highest value of formula_35\nwhere\n\nAddition of new molecules to nuclei larger than this critical radius decreases the free energy, so these nuclei are more probable. The rate at which nucleation occurs is then limited by, i.e., determined by the probability, of forming the critical nucleus. This is just the exponential of minus the free energy of the critical nucleus formula_2, which is\nThis is the free energy barrier needed in the CNT expression for formula_39 above.\n\nFrom an experimental standpoint, this theory grants tuning of the critical radius through the dependence of formula_16 on temperature. The variable formula_41, described above, can be expressed as\n\nwhere formula_43 is the melting point and formula_44 is the enthalpy of formation for the material. Furthermore, the critical radius can be expressed as\n\nrevealing a dependence of reaction temperature. Thus as you increase the temperature near formula_43, the critical radius will increase.\n\nHeterogeneous nucleation is the second of the two nucleations as discussed in the classical nucleation theory. Unlike homogeneous nucleation, heterogeneous nucleation occurs when the nucleus is on the surface. As previously stated, heterogeneous nucleation is more common than homogeneous nucleation. This is due to the different factors within a single system. These factors may include a lower level of supersaturation or being more kinetically favorable than homogeneous nucleation. Heterogeneous nucleation is more kinetically favorable than homogeneous nucleation because the nucleation barrier for the heterogeneous nucleation, ΔG*, is much lower at the surface in contrast to the nucleation barrier for homogeneous nucleation. This is because the nucleation barrier comes from the positive term in the free energy ΔG, which is the surface term. For homogeneous nucleation the nucleus is approximated by a sphere and so has a surface term containing the surface area of a sphere and so has a free energy equal to the surface area of a sphere, 4πr, times the surface tension σ. However, because heterogeneous nucleation is the nucleation of a nucleus on a surface, it is important to take into account of more factors that go into a heterogeneous nucleation.\n\nAs shown in a diagram on the lefthand side, the factors are the size of the droplet, the contact angle at which the particle is making contact on the surface, and the interactions at three different phases(the interaction between the liquid interface and the solid surface, the interaction between the solid surface and the air, and the interaction between the liquid interface and the air). A schematic on the righthand side shows that the macroscopic droplets are not complete spheres. Therefore, the area of the interface between the droplet and the surrounding fluid is less than formula_47. This geometrical factor reduces the interfacial area and so the interfacial free energy, which also reduces the nucleation barrier. Note that this simple theory treats the microscopic nucleus just as if it is a macroscopic droplet.\n\nIn the schematic to the right the contact angle between the droplet surface and the surface decreases from left to right (A to C). In this schematic, the surface area of the droplet decreases as the contact angle decreases. This geometrical effect reduces the barrier which will increase the rate of nucleation as opposed to having a surface with a larger contact angle. Also, if instead of the surface being flat it curves like a fluid, then this also reduces the interfacial area and the nucleation barrier. There are expressions for this reduction for simple surface geometries. In practice, this means we expect nucleation to be fastest on any imperfections in the surface such that the nucleus forms a small contact angle on its surface.\n\nThe classical nucleation theory makes a number of assumptions, for example it treats a microscopic nucleus as if it is a macroscopic droplet with a well defined surface whose free energy is estimated using an equilibrium property: the interfacial tension σ. For a nucleus that may be only of order ten molecules across it is not always clear that we can treat something so small as a volume plus a surface. Also nucleation is an inherently out of thermodynamic equilibrium phenomenon so it is not always obvious that its rate can estimated using equilibrium properties.\n\nFor simple model systems, modern computers are powerful enough to calculate numerically exact nucleation rates. One such example is the nucleation of the crystal phase in the model of hard spheres. This is a simple model of some colloids consisting of perfectly hard spheres in thermal motion. The agreement of CNT with the calculated rates for this system confirms that the classical theory is a very reasonable approximate theory. For the simple models CNT works quite well, however it is unclear if it describes complex (e.g. molecular) systems equally well. Jones \"et al.\" computationally explored the nucleation of small Water cluster using classical water model. It was found that CNT could describe the nucleation of clusters of 8-50 water molecules well, but failed to describe smaller clusters. Corrections to CNT, obtained from higher accuracy methods such as quantum chemical calculations, can provide necessary interactions for accurate nucleation rates.\nHowever, the CNT fails in describing experimental results of vapour to liquid nucleation even for model substances like Argon by several orders of magnitude.\n"}
{"id": "8940918", "url": "https://en.wikipedia.org/wiki?curid=8940918", "title": "Computer Measurement Group", "text": "Computer Measurement Group\n\nThe Computer Measurement Group (CMG), founded in 1974, is a worldwide non-profit organization of data processing professionals whose work involves measuring and managing the performance of computing systems. In this context, \"performance\" is understood to mean the \"response time\" of software applications of interest, and the overall \"capacity\" (or \"throughput\") characteristics of the system, or of some part of the system.\n\nCMG members are primarily concerned with evaluating and maximizing the performance of existing computer systems and networks, and with capacity management, in which planned enhancements to existing systems or the designs of new systems are evaluated to find the necessary resources required to provide adequate performance at a reasonable cost.\n\nCMG's purpose is to promote the exchange of technical information among Information Technology (IT) professionals through regional groups, technical publications, and an annual conference. In common with other user groups devoted to a broad range of products or technologies (for example SHARE or DECUS), CMG provides education, networking, and leadership opportunities for its members.\n\nThe association's activities provide:\n\n\nWith over thirty regional and international groups, CMG's wide reaching structure emphasizes an extensive information and peer network. Regional groups hold local educational meetings, typically three or four times a year, and many publish informational newsletters. Regional meetings may span a half-day, a full day (the most common), or occasionally two days. International CMG groups also hold their own annual conferences and publish their own conference proceedings.\n\nIn the US, Regional CMG groups cover the following areas: Boston, Connecticut, Florida, Greater Atlanta, Kansas City, Midwest, Minneapolis, National Capital Area, New York, Northern California, Northwest, Ohio Valley, Philadelphia, Rocky Mountain, St Louis, Salt Lake City, Southern, Southern California, and South West.\n\nInternational CMG groups exist in Australia, Austria and Eastern Europe (CMG AE), Canada, Central Europe (CECMG), Italy, China, the Netherlands South Africa, India and the United Kingdom (UKCMG)\n\nCMG allows members to exchange information about the measurement, management and performance of Information technology systems. Topics of particular concern among CMG members include:\n\nAt its annual conference, CMG presents several awards recognizing outstanding contributions to the field of computer measurement and performance evaluation:\n\n\nCMG produces a number of both print and electronic publications. Currently there are four unique publications, the CMG Journal, the CMG Proceedings, the CMG Bulletin and MeasureIT. Membership in CMG is required to obtain copies of the publications with the exception of MeasureIT. In addition, some libraries and Universities have copies of the CMG Journal available for reference use.\n\nThe free electronic newsletter, MeasureIT, is written by computer and performance professionals and is distributed around the second Tuesday of every month. Anyone may subscribe to MeasureIT by visiting the CMG MeasureIT homepage.\n\nCMG holds an annual conference for performance professionals from around the world. CMG'11 was held in Washington, D.C., United States, 5–9 December 2011.\nIn 2013 the annual conference was renamed Performance and Capacity 2013 by CMG Conference \nand held in La Jolla, CA, United States, 4–8 November 2013.\n\nA. A. Michelson Award recipients:\n\n"}
{"id": "7135697", "url": "https://en.wikipedia.org/wiki?curid=7135697", "title": "Copernicus Programme", "text": "Copernicus Programme\n\nCopernicus is European Union's Earth observation programme coordinated and managed by the European Commission in partnership with the European Space Agency (ESA), the EU Member States and EU Agencies. Copernicus Programme has been established by the Regulation (EU) No 377/2014 in 2014, building on the previous EU's Earth monitoring initiative GMES (est. by Regulation (EU) No 911/2010).\n\nIt aims at achieving a global, continuous, autonomous, high quality, wide range Earth observation capacity. Providing accurate, timely and easily accessible information to, among other things, improve the management of the environment, understand and mitigate the effects of climate change, and ensure civil security. \n\nThe objective is to use vast amount of global data from satellites and from ground-based, airborne and seaborne measurement systems to produce timely and quality information, services and knowledge, and to provide autonomous and independent access to information in the domains of environment and security on a global level in order to help service providers, public authorities and other international organizations improve the quality of life for the citizens of Europe. In other words, it pulls together all the information obtained by the Copernicus environmental satellites, air and ground stations and sensors to provide a comprehensive picture of the \"health\" of Earth. \n\nOne of the crucial benefits of the Copernicus Programme is that the data and information produced in the framework of Copernicus are made available on a full, open and free-of-charge basis (subject to appropriate conditions and limitations) to all its users and public in general, thus allowing many new downstream services to be developed.\n\nThe value-adding services offered by Copernicus cover six main interacting themes: atmosphere, marine, land, climate, emergency and security.\n\nCopernicus builds upon three components:\n\nIts cost during 1998 to 2020 are estimated at 6.7 billion euros with around €4.3bn spent in the period 2014 to 2020 and shared between the EU (66%) and ESA (33%) with benefits of the data to the EU economy estimated at roughly 30 billion euros through 2030. ESA as a main partner has performed much of the design and oversees and co-funds the development of Sentinel mission 1, 2, 3, 4, 5 and 6 with each sentinel mission consisting of at least 2 satellites and some like sentinel 1 consisting of 4 satellites. They will also provide the instruments for MTG and MetOp-SG weather satellites of EUMETSAT where ESA and EUMETSAT will also coordinate the delivery of data from upwards of 30 satellites that form the contributing satellite missions to Copernicus.\n\nOver a few decades European and national institutions have made substantial R&D efforts in the field of Earth observation. These efforts have resulted in tremendous achievements but the services and products developed during this period had limitations that were inherent to R&D activities (e.g. lack of service continuity on the long-term). The idea for a global and continuous European earth observation system was developed under the name of Global Monitoring for Environment and Security (GMES) which was later re-branded into Copernicus after the EU became directly involved in financing and development. It follows and greatly expands on the work of the previous 2.3 billion euros European Envisat program which operated from 2002 to 2012.\n\nCopernicus moved from R&D to operational services following a phased approach:\n\n\nESA is currently developing seven missions under the Sentinel programme. The Sentinel missions include radar and super-spectral imaging for land, ocean and atmospheric monitoring. Each Sentinel mission is based on a constellation of two satellites to fulfill and revisit the coverage requirements for each mission, providing robust datasets for all Copernicus services. The Sentinel missions will have the following objectives:\n\nBefore the Sentinel missions provide data to Copernicus, numerous existing or planned space missions provide or will provide data useful to the provision of Copernicus services. (These missions are often referred to as \"GMES Contributing Missions (GCMs)\".)\n\nData provided by non-European satellite missions (e.g. LANDSAT, GOSAT, RADARSAT-2) can also be used by Copernicus.\n\n\nGMES In-Situ Coordination (GISC). GISC is a FP7 funded initiative, will last for three years (January 2010 – December 2012) and is coordinated by the European Environment Agency (EEA).\n\n\"In-situ\" data are all data from sources other than Earth observation satellites. Consequently, all ground-based, air-borne, and ship/buoy-based observations and measurements that are needed to implement and operate the Copernicus services are part of the in-situ component. In-situ data are indispensable; they are assimilated into forecasting models, provide calibration and validation of space-based information, and contribute to analysis or filling gaps not available from space sources.\n\nGISC objectives will be achieved by:\n\nGISC is undertaken with reference to other initiatives, such as INSPIRE (Infrastructure for Spatial Information in Europe) and SEIS (Shared Environmental Information System) as well as existing coordination and data exchange networks. The coordinated access to data will retain the capacity to link directly data providers and the service providers because it is based on the principles of SEIS and INSPIRE. The implementation of INSPIRE is embedded in the synergies and meta-data standards that are used in GISC. Data and information will aim to be managed as close as possible to its source in order to achieve a distributed system, by involving countries and existing capacities that maintain and operate the required observation infrastructure.\n\nCopernicus services are dedicated to the monitoring and forecasting of the Earth's subsystems. They contribute directly to the monitoring of climate change. Copernicus services also address emergency management (e.g. in case of natural disaster, technological accidents or humanitarian crises) and security-related issues (e.g. maritime surveillance, border control).\n\nCopernicus services address six main thematic areas:\n\nThe development of the pre-operational version of the services has been realised by a series of projects launched by the European Commission and partly funded through the EU's 7th Framework Programme (FP7). These projects were geoland2 (land), MyOcean (marine), SAFER (emergency response), MACC and its successor MACC II (atmosphere) and G-MOSAIC (security). Most of these projects also contributed to the monitoring of Climate Change.\n\n\"The information provided by the Copernicus services can be used by end users for a wide range of applications in a variety of areas. These include urban area management, sustainable development and nature protection, regional and local planning, agriculture, forestry and fisheries, health, civil protection, infrastructure, transport and mobility, as well as tourism.\"\n\nCopernicus is the European Union's contribution to the Global Earth Observation System of Systems (GEOSS) thus delivering geospatial information globally.\n\nSome Copernicus services make use of OpenStreetMap data in their maps production.\n\nOther initiatives will also facilitate the development and functioning of Copernicus services:\nCopernicus is one of three related initiatives that are the subject of the GIGAS (\"GEOSS, INSPIRE and GMES an Action in Support\") harmonization and analysis project under the auspices of the EU 7th Framework Programme.\n\n\n"}
{"id": "5034846", "url": "https://en.wikipedia.org/wiki?curid=5034846", "title": "Duluth pack", "text": "Duluth pack\n\nA Duluth pack is traditional portage pack used in canoe travel, particularly in the Boundary Waters region of northern Minnesota and the Quetico Provincial Park of Ontario. A specialized type of backpack, Duluth packs are made of heavy canvas, leather, and/or cordura nylon, and are nearly square in order to fit easily in the bottom of a canoe.\n\nOriginally known as the Poirier pack or Poirier pack-sack (for its inventor, Camille Poirier), as its primary manufacturer was located in Duluth, Minnesota, the pack style later became known as the \"Duluth pack.\" \n\nA Duluth pack is a specialized type of backpack made of heavy canvas, leather, and/or cordura nylon. The packs are a traditional portage pack which are nearly square in order to fit easily in the bottom of a canoe. The simplest and most traditional Duluth pack consists of a single large envelope which is closed by straps and roller buckles. The pack is carried by two shoulder straps, and sometimes a tumpline worn over the top of the head.\n\nTheir key attributes make them well adapted to wilderness canoe camping where travel is largely by water (where the packs and gear do not need to be carried) punctuated by portages where the packs and gear need to be carried over land: \n\n\nConversely, they lack many features and attributes of long distance backpacking type backpacks, and so are generally not used for such.\n\nThe Duluth pack has its roots in a French-Canadian named Camille Poirier, who made his way west to Duluth, Minnesota. Arriving in 1870 with his \"little stock of leather and tools,\" he began a small shoe store and quickly made a go of it in this booming frontier town on the shores of Lake Superior. Out of his small shoe shop on the waterfront in Duluth, Camille began building a new style canoe pack with a tumpline, sternum strap, and umbrella holder. The patented the \"C. Poirier Pack Sack\" strap design in 1882. In 1911, Poirier sold his business to Duluth Tent & Awning Company, which later became the company Duluth Pack.\n\n"}
{"id": "1504755", "url": "https://en.wikipedia.org/wiki?curid=1504755", "title": "Electronic flight instrument system", "text": "Electronic flight instrument system\n\nAn electronic flight instrument system (EFIS) is a flight deck instrument display system that displays flight data electronically rather than electromechanically. An EFIS normally consists of a primary flight display (PFD), multi-function display (MFD), and an engine indicating and crew alerting system (EICAS) display. Early EFIS models used cathode ray tube (CRT) displays, but liquid crystal displays (LCD) are now more common. The complex electromechanical attitude director indicator (ADI) and horizontal situation indicator (HSI) were the first candidates for replacement by EFIS. Now, however, few flight deck instruments cannot be replaced by an electronic display.\n\nEFIS installations vary greatly. A light aircraft might be equipped with one display unit that displays flight and navigation data. A large, commercial aircraft is likely to have six or more display units. Typical EFIS displays and controls can be seen at this B737 technical information web site. The equivalent electromechanical instruments are also shown here.\n\nEFIS installation follows the sequence:\nA basic EFIS might have all these facilities in the one unit.\n\nOn the flight deck, the display units are the most obvious parts of an EFIS system, and are the features that lead to the term \"glass cockpit\". The display unit that replaces the ADI is called the primary flight display (PFD). If a separate display replaces the HSI, it is called the navigation display. The PFD displays all information critical to flight, including calibrated airspeed, altitude, heading, attitude, vertical speed and yaw. The PFD is designed to improve a pilot's situational awareness by integrating this information into a single display instead of six different analog instruments, reducing the amount of time necessary to monitor the instruments. PFDs also increase situational awareness by alerting the aircrew to unusual or potentially hazardous conditions — for example, low airspeed, high rate of descent — by changing the color or shape of the display or by providing audio alerts.\n\nThe names Electronic Attitude Director Indicator and Electronic Horizontal Situation Indicator are used by some manufacturers. However, a simulated ADI is only the centerpiece of the PFD. Additional information is both superimposed on and arranged around this graphic.\n\nMulti-function displays can render a separate navigation display unnecessary. Another option is to use one large screen to show both the PFD and navigation display.\n\nThe PFD and navigation display (and multi-function display, where fitted) are often physically identical. The information displayed is determined by the system interfaces where the display units are fitted. Thus, spares holding is simplified: the one display unit can be fitted in any position.\n\nLCD units generate less heat than CRTs; an advantage in a congested instrument panel. They are also lighter, and occupy a lower volume.\n\nThe MFD (multi-function display) displays navigational and weather information from multiple systems. MFDs are most frequently designed as \"chart-centric\", where the aircrew can overlay different information over a map or chart. Examples of MFD overlay information include the aircraft's current route plan, weather information from either on-board radar or lightning detection sensors or ground-based sensors, e.g., NEXRAD, restricted airspace and aircraft traffic. The MFD can also be used to view other non-overlay type of data (e.g., current route plan) and calculated overlay-type data, e.g., the glide radius of the aircraft, given current location over terrain, winds, and aircraft speed and altitude.\n\nMFDs can also display information about aircraft systems, such as fuel and electrical systems (see EICAS, below). As with the PFD, the MFD can change the color or shape of the data to alert the aircrew to hazardous situations.\n\nEICAS (Engine Indications and Crew Alerting System) displays information about the aircraft's systems, including its fuel, electrical and propulsion systems (engines). EICAS displays are often designed to mimic traditional round gauges while also supplying digital readouts of the parameters.\n\nEICAS improves situational awareness by allowing the aircrew to view complex information in a graphical format and also by alerting the crew to unusual or hazardous situations. For example, if an engine begins to lose oil pressure, the EICAS might sound an alert, switch the display to the page with the oil system information and outline the low oil pressure data with a red box. Unlike traditional round gauges, many levels of warnings and alarms can be set. Proper care must be taken when designing EICAS to ensure that the aircrew are always provided with the most important information and not overloaded with warnings or alarms.\n\nECAM is a similar system used by Airbus, which in addition to providing EICAS functions also recommend remedial action.\n\nEFIS provides pilots with controls that select display range and mode (for example, map or compass rose) and enter data (such as selected heading).\n\nWhere other equipment uses pilot inputs, data buses broadcast the pilot's selections so that the pilot need only enter the selection once. For example, the pilot selects the desired level-off altitude on a control unit. The EFIS repeats this selected altitude on the PFD, and by comparing it with the actual altitude (from the air data computer) generates an altitude error display. This same altitude selection is used by the automatic flight control system to level off, and by the altitude alerting system to provide appropriate warnings.\n\nThe EFIS visual display is produced by the symbol generator. This receives data inputs from the pilot, signals from sensors, and EFIS format selections made by the pilot. The symbol generator can go by other names, such as display processing computer, display electronics unit, etc.\n\nThe symbol generator does more than generate symbols. It has (at the least) monitoring facilities, a graphics generator and a display driver. Inputs from sensors and controls arrive via data buses, and are checked for validity. The required computations are performed, and the graphics generator and display driver produce the inputs to the display units.\n\nLike personal computers, flight instrument systems need power-on-self-test facilities and continuous self-monitoring. Flight instrument systems, however, need additional monitoring capabilities:\n\nTraditional (electromechanical) displays are equipped with synchro mechanisms that transmit the pitch, roll, and heading shown on the captain and first officer's instruments to an instrument comparator. The comparator warns of excessive differences between the Captain and First Officer displays. Even a fault as far \"downstream\" as a jam in, say, the roll mechanism of an ADI triggers a comparator warning. The instrument comparator thus provides both comparator monitoring and display monitoring.\n\nWith EFIS, the comparator function is simple: Is roll data (bank angle) from sensor 1 the same as roll data from sensor 2? If not, display a warning caption (such as CHECK ROLL) on both PFDs. Comparison monitors give warnings for airspeed, pitch, roll, and altitude indications. More advanced EFIS systems have more comparator monitors.\n\nIn this technique, each symbol generator contains two display monitoring channels. One channel, the internal, samples the output from its own symbol generator to the display unit and computes, for example, what roll attitude should produce that indication. This computed roll attitude is then compared with the roll attitude input to the symbol generator from the INS or AHRS. Any difference has probably been introduced by faulty processing, and triggers a warning on the relevant display.\n\nThe external monitoring channel carries out the same check on the symbol generator on the other side of the flight deck: the Captain's symbol generator checks the First Officer's, the First Officer's checks the Captain's. Whichever symbol generator detects a fault, puts up a warning on its own display.\n\nThe external monitoring channel also checks sensor inputs (to the symbol generator) for reasonableness. A spurious input, such as a radio height greater than the radio altimeter's maximum, results in a warning.\n\nAt various stages of a flight, a pilot needs different combinations of data. Ideally, the avionics only show the data in use—but an electromechanical instrument must be in view all the time. To improve display clarity, ADIs and HSIs use intricate mechanisms to remove superfluous indications temporarily—e.g., removing the glide slope scale when the pilot doesn't need it.\n\nUnder normal conditions, an EFIS might not display some indications, e.g., engine vibration. Only when some parameter exceeds its limits does the system display the reading. In similar fashion, EFIS is programmed to show the glideslope scale and pointer only during an ILS approach.\n\nIn the case of an input failure, an electromechanical instrument adds yet another indicator—typically, a bar drops across the erroneous data. EFIS, on the other hand, removes invalid data from the display and substitutes an appropriate warning.\n\nA de-clutter mode activates automatically when circumstances require the pilot's attention for a specific item. For example, if the aircraft pitches up or down beyond a specified limit—usually 30 to 60 degrees—the attitude indicator de-clutters other items from sight until the pilot brings the pitch to an acceptable level. This helps the pilot focus on the most important tasks.\n\nTraditional instruments have long used color, but lack the ability to change a color to indicate some change in condition. The electronic display technology of EFIS has no such restriction and uses color widely. For example, as an aircraft approaches the glide slope, a blue caption can indicate glide slope is armed, and capture might change the color to green. Typical EFIS systems color code the navigation needles to reflect the type of navigation. Green needles indicate ground based navigation, such as VORs, Localizers and ILS systems. Magenta needles indicate GPS navigation.\n\nEFIS provides versatility by avoiding some physical limitations of traditional instruments. A pilot can switch the same display that shows a course deviation indicator to show the planned track provided by an area navigation or flight management system. Pilots can choose to superimpose the weather radar picture on the displayed route.\n\nThe flexibility afforded by software modifications minimises the costs of responding to new aircraft regulations and equipment. Software updates can update an EFIS system to extend its capabilities. Updates introduced in the 1990s included the ground proximity warning system, and traffic collision avoidance system.\n\nA degree of redundancy is available even with the simple two-screen EFIS installation. Should the PFD fail, transfer switching repositions its vital information to the screen normally occupied by the navigation display.\n\nIn the late 1980s, EFIS became standard equipment on most Boeing and Airbus airliners, and many business aircraft adopted EFIS in the 1990s.\n\nRecent advances in computing power and reductions in the cost of liquid-crystal displays and navigational sensors (such as GPS and attitude and heading reference system) have brought EFIS to general aviation aircraft. Notable examples are the Garmin G1000 and Chelton Flight Systems EFIS-SV.\n\nSeveral EFIS manufacturers have focused on the experimental aircraft market, producing EFIS and EICAS systems for as little as US$1,000-2000. The low cost is possible because of steep drops in the price of sensors and displays, and equipment for experimental aircraft doesn't require expensive Federal Aviation Administration certification. This latter point restricts their use to experimental aircraft and certain other aircraft categories, depending on local regulations. Uncertified EFIS systems are also found in Sport Pilot category aircraft, including factory built, microlight, and ultralight aircraft. These systems can be fitted to certified aircraft in some cases as secondary or backup systems depending on local aviation rules.\n\n\n"}
{"id": "12257271", "url": "https://en.wikipedia.org/wiki?curid=12257271", "title": "Elevator:2010", "text": "Elevator:2010\n\nElevator:2010 was an inducement prize contest with the purpose of developing space elevator and space elevator-related technologies. Elevator:2010 organized annual competitions for climbers, ribbons and power-beaming systems, and was operated by a partnership between Spaceward Foundation and the NASA Centennial Challenges.\n\nOn March 23, 2005 NASA's Centennial Challenges program announced a partnership with the Spaceward Foundation regarding Elevator:2010, to raise the amounts of monetary prizes and to get more teams involved in the competitions. The partnership was not renewed after its initial 5-year term.\n\nThere were two (out of an intended seven) competitions of the NASA Centennial Challenges which fell under the Elevator:2010 banner: The Tether Challenge and the Beam Power Challenge. There were also the two original competitions.\n\nThis competition presented the challenge of constructing super-strong tethers, a crucial component of a space elevator. The 2005 contest was to award US$50,000 to the team which constructed the strongest tether, with contests in future years requiring that each winner outperform that of the previous year by 50%. No competing tether surpassed the commercial off-the-shelf baseline and the prize was increased to $200,000 in 2006.\n\nOf the four teams competing, three were disqualified for not following length rules—one of these cases by a fraction of a millimeter. Ultimately, the 'House Tether' won against the remaining team. The 'House Tether' is composed of Zylon fiber and M77 adhesive. It was stronger than the machine used to test the tether itself: it began to fail at , forcing the test to be called off.\n\nThe Beam Power Challenge was a competition to build a wirelessly-powered ribbon-climbing robot. The contest involves having the robot raise a specified payload to a specific height within a limited period of time. The first competition in 2005 would have awarded , US$20,000, and US$10,000 to the three best-performing teams meeting the minimum benchmark of . However no team met the minimum standard in 2005.\n\nIn 2006 the prize for first place increased to $150,000 with the goal of climbing 50 meters in under 1 minute. It was held October 20–21, 2006 at the Las Cruces International Airport at the Wirefly X PRIZE Cup. 13 teams entered the competition. Only one team, University of Saskatchewan, was able to climb the tether in under 1 minute, reaching the top in .\n\nThe Challenge had $500,000 in prize money for the 2007 competition.\n\nAt the 2009 Challenge, on November 6, 2009, LaserMotive successfully used lasers to drive a device up a cable suspended from a helicopter. Energy is transmitted to the climber using a high-power infrared beam. LaserMotive's entry, which was the only climber to top the cable, reached an average speed of and earned a $900,000 prize. This marked both a performance record, and the first award of a cash prize at the Challenge.\n\nLaserMotive won the prize for the Level 1 power beaming prize in 2009 with the achievement of climber speed over a sub-kilometer climb. The Level 2 power beaming prize, for a climb, remains available for future competitions.\n\nAfter LaserMotive claimed the prize for the Level 1 power beaming prize in 2009, the Space Elevator games being conducted by Elevator:2010 planned to offer a prize purse for future competitions of , for both the Power Beaming (Climber) Competition and the Tether Strength Competition.\n\nThe Japan Space Elevator Association conducted climbing competitions in August 2013.\n\n"}
{"id": "47530099", "url": "https://en.wikipedia.org/wiki?curid=47530099", "title": "Elizabeth Riddle Graves", "text": "Elizabeth Riddle Graves\n\nElizabeth Riddle Graves (25 January 1916 – 6 January 1972) was a pioneer in the physics of neutrons and the detection and measurement of fast neutrons. During World War II, she worked in the Metallurgical Laboratory and at the Los Alamos Laboratory, becoming a group leader there after the war.\n\nElizabeth Riddle was born in Nashville, Tennessee, on 23 January 1916 to James Marion Riddle from South Carolina and Georgia Clymetra Boykin from Arkansas. She had two brothers, James Marion Riddle Jr. and John Burwell Boykin Riddle. Around 1921, the Riddle family moved to Chicago, Illinois.\n\nRiddle entered the University of Chicago, where she was known as \"Diz\". She earned her Bachelor of Science degree in physics in 1936, and developed a keen interest in the physics of neutrons, particularly the detection and measurement of fast neutrons. She earned her PhD in 1940, writing her thesis on the \"Energy Released from Be 9 (d, α) Li 7 and the Production of Li 7\" under the supervision of Samuel K. Allison.\n\nWhile there, she met and married Alvin C. Graves, a fellow physics major. Jobs were hard to find during the Great Depression. Alvin remained at the University of Chicago as a research fellow and an assistant professor until 1939, when he moved to the University of Texas, but Elizabeth was unable to secure a job there as well due to its anti-nepotism rules, which tended to discriminate against women.\n\nIn 1942 Alvin was invited back to the University of Chicago by Arthur H. Compton to join the Manhattan Project's Metallurgical Laboratory.\n\nIn 1943 they joined the Manhattan Project's Los Alamos Laboratory in New Mexico. With Texas in mind, Alvin made it a condition of his going there that Elizabeth also be given a job. As it turned out, she almost certainly would have been recruited anyway. She was one of the few scientists who knew about fast neutron scattering, which was crucial to nuclear weapon design, and who knew how to operate the Cockcroft–Walton accelerator that had been brought from the University of Illinois.\n\nAt the time of the Trinity nuclear test in 1945, Elizabeth was seven months pregnant with her first child. They therefore requested that they be assigned to a post far from the blast. They listened to Allison's countdown to the explosion on the radio, and using Geiger counters, they monitored the test's radioactive fallout, which took until the afternoon to reach them. Elizabeth finished an experiment while in labor, timing her contractions with a stopwatch. The child was a healthy daughter, Marilyn Edith. Alvin and Elizabeth had two more children, Alvin Palmer and Elizabeth Anne.\n\nThey remained in Los Alamos after the war. Elizabeth became a group leader in the experimental physics division in 1950, and researched neutron interactions with matter and material. She died of cancer at Bataan Memorial Hospital in Albuquerque, New Mexico, on 6 January 1972, and was buried at Guaje Pines Cemetery, Los Alamos, Los Alamos County, New Mexico.\n\n\n\n\n"}
{"id": "27902882", "url": "https://en.wikipedia.org/wiki?curid=27902882", "title": "Enumeral", "text": "Enumeral\n\nEnumeral was a Cambridge, Massachusetts-based biotechnology company which developed monoclonal antibody immunotherapies through an 'immunoprofiling' platform that allowed it to scan the human immune microenvironment and identify and validate potential drug candidates. The company filed for Chapter 11 bankruptcy and arranged to sell its assets to Xoma Corporation in January 2018.\n\nEnumeral was founded in 2009 to bring together various immunoprofiling technologies from Harvard University, Massachusetts Institute of Technology, the Whitehead Institute for Biomedical Research and Massachusetts General Hospital. The company's Scientific Founder was Christopher Love, Associate Professor of Chemical Engineering at MIT; the Executive Chairman is John Rydzewskand and its CEO is Arthur Tinkelenberg. In 2014 the company was taken public through a reverse takeover into a shell called Cerulean Group. Its stock is traded OTC in the US, with trading in the OTCQB marketplace tier commencing on 4 August 2014. The stock code is ENUM.\n\nBy 2015, Enumeral had completed pre-clinical development of PD-1 inhibitors and sought partners to enter clinical development with. In December 2016, Enumeral completed raising to fund long-term development plans, but by May 2017, the company announced it only had sufficient cash on hand to fund operations through June 2017. Shortly thereafter, in June, the company dismissed its R&D research staff. In August 2017, Enumeral was kicked out of its headquarters in Cambridge due to non-payment of rent and other fees. The Cambridge headquarters was also the home of Celgene and Unum Therapeutics. By the time the company lost its headquarters, Arthur Tinkelenberg was being described as the company's \"former president and chief executive officer\". In January 2018, Enumeral struck a deal with Xoma Corporation, located in the San Francisco Bay Area, to sell its assets for , while concurrently filing for Chapter 11 bankruptcy.\n\nEnumeral's platform consists of various proprietary cellular libraries derived from target-specific immunized sources or from human patient donors. The platform has three main parts:\n\n\n\n\nAn early commercial interest of Enumeral has focused on PD-1, currently targeted by two FDA-approved monoclonal antibody drugs - Keytruda, from Merck & Co., and Opdivo, from Bristol-Myers Squibb. In 2015 Enumeral reported that it had used its platform to raise anti-PD-1 antibodies that did not compete with Keytruda or Opdivo for binding to PD-1, nor did they appear to compete with PD-1's ligand, PD-L1. These potentially allosteric antibodies also produced more interferon gamma and showed dose-dependent increases in T cell CD25 expression. Further, Enumeral's antibodies caused higher T cell activation in \"ex vivo\" human assays than the currently marketed anti-PD-1 antibodies and, in that same setting, in combination with one of the marketed antibodies, could elicit an additive effect on T cell activation. Enumeral expects to take an anti-PD-1 antibody into clinical testing in 2016.\n\nIn December 2014 Enumeral announced a collaboration with Merck & Co. in which the two companies would use the Enumeral platform to interrogate the tumor microenvironment in colorectal cancer tissues obtained directly from patients, with the aim of identifying functional cellular responses to Merck-developed immuno-oncology products. In September 2015 Enumeral announced that the Merck collaboration had achieved its first milestone, enabling Enumeral to receive a milestone payment from Merck.\n"}
{"id": "43751582", "url": "https://en.wikipedia.org/wiki?curid=43751582", "title": "Equivalent input", "text": "Equivalent input\n\nEquivalent input (also input-referred or input-related), is a method of referring to the signal or noise level at the output of a system as if it were an input to the same system. This is accomplished by removing all signal changes (e.g. amplifier gain, transducer sensitivity, etc.) to get the units to match the input.\n\nA microphone converts acoustical energy to electrical energy. Microphones have some level of electrical noise at their output. This noise may have contributions from random diaphragm movement, thermal noise, or a dozen other sources, but those can all be thought of as an imaginary acoustic noise source injecting sound into the (now noiseless) microphone. The units on this noise are no longer volts, but units of sound pressure (pascals or dBSPL), which can be directly compared to the desired sound pressure inputs.\n\nA device which uses a microphone may be susceptible to electromagnetic interference which causes sonic artifacts. The problem is not in the microphone, but the interference level can be \"related\" back to the input to compare to the level of typical inputs to see how audible the artifact is.\n"}
{"id": "1663116", "url": "https://en.wikipedia.org/wiki?curid=1663116", "title": "Feather duster", "text": "Feather duster\n\nA feather duster is an implement used for cleaning. It consists typically of a wooden-dowel handle and feathers from either the male or female ostrich bird that are wound onto the handle by a wrapped wire. Dusters vary in size but are most often between in total length. Some dusters have a retractable casing instead of a dowel handle. These dusters are typically used by rack-jobbers and truck drivers who need to dust store shelves, and like to retract the feathers into the handle to avoid damage.\n\nFeather dusters are effective in dusting tight areas, or areas where there are a lot of odds and ends to dust around. The individual feathers are able to penetrate through the knick-knacks and pull the dust out of the area without disturbing items. On large open surfaces or walls, or in trying to get spider webs in the ceiling, either a feather duster or other dusters like lambswool or synthetic dusters will work.\n\nIn 1870, the original idea for the feather duster was conceived in a broom factory in Jones County, Iowa, U.S.A. farmer brought a bundle of turkey feathers into the factory asking if they could be used to assembly a brush. E.E. Hoag used these feathers to invent the first feather duster. Using a short broom stick and splitting the feathers with a pocket knife, the duster was found to be too stiff for use. In 1874, the Hoag Duster Company was founded, which became a pioneer of feather dusters in the U.S. state of Iowa.\n\nIn 1874, Susan Hibbard of Geneva Lake, Wisconsin, U.S. used discarded turkey feathers to invent a feather duster. Hibbard filed a patent (U.S. patent #177,939) on November 13, 1874 which was issued on May 30, 1876. After a hard fought legal battle against her husband, George Hibbard, and the National Feather Duster Company in December 1881, the United States Court of Appeals for the Seventh Circuit in Chicago ruled in favor of Susan Hibbard giving her priority of invention of the feather duster.\n\nSouth African ostrich feather dusters were developed in Johannesburg, South Africa by missionary, broom factory manager, Harry S. Beckner in 1903. He felt that the Ostrich feathers made a convenient tool for cleaning up the machines at the broom factory. His first ostrich feather dusters were wound on broom handles using the foot powered kick winders and the same wire used to attach broom straw. \nThe Chicago Feather Duster Company was established in 1875. They received a patent for the head of the duster on September 17, 1907 along with a patent for the cuff on December 22, 1906.\n\nThe first ostrich feather duster company in the United States was formed in 1913 by Harry S. Beckner and his brother George Beckner in Athol, Massachusetts and has survived till this day as the Beckner Feather Duster Company under the care of George Beckner's great granddaughter, Margret Fish Rempher. Today the largest manufacturer of Ostrich Feather Dusters is Klein Karoo International (Feathers) which is located in Oudtshoorn, South Africa.\n\nThe feather duster was considered a status symbol in the late 19th and early 20th centuries.\n\nThere are several types of feathers used in feather dusters, but ostrich feathers are most often used. Feathers for dusters are the ostrich family's most valuable industrial commodity.\n\nBlack ostrich feathers come from the male ostrich and are very soft with feathers that are more \"stringy\" in nature. Gray ostrich feathers are more stark than the black feathers and are often sold at grocery stores. Floss ostrich feathers are the most soft and delicate feathers and come from underneath the bird's wings. Floss ostrich feathers are more expensive. Chick feathers are more pointed and stark than ostrich feathers, are usually sold at very low prices.\n\nThroughout China, feather dusters are normally made of chicken feathers. They are attached to a length of bamboo, and unlike Western dusters, are commonly present along most of the stick rather than just the end.\n\nOstrich feather dusters are effective by means of the tiny barbules on the feathers themselves that act as fingers to collect the dust. They are often used around electronics because of their resistance to static electricity.\n"}
{"id": "2601638", "url": "https://en.wikipedia.org/wiki?curid=2601638", "title": "Fire blanket", "text": "Fire blanket\n\nA fire blanket is a safety device designed to extinguish incipient (starting) fires. It consists of a sheet of a fire retardant material which is placed over a fire in order to smother it.\n\nSmall fire blankets, such as for use in kitchens and around the home are usually made of fiberglass and sometimes kevlar, and are folded into a quick-release contraption for ease of storage.\n\nFire blankets, along with fire extinguishers, are fire safety items that can be useful in case of a fire. These nonflammable blankets are helpful in temperatures up to 900 degrees and are useful in smothering fires by not allowing any oxygen to the fire. Due to its simplicity, a fire blanket may be more helpful for someone who is inexperienced with fire extinguishers.\n\nLarger fire blankets, for use in laboratory and industrial situations, are often made of wool (sometimes treated with a flame retardant fluid). These blankets are usually mounted in vertical quick-release container so that they can be easily pulled out and wrapped round a person whose clothes are on fire.\n\nSome older fire blankets were made of woven asbestos fibers and are not NFPA rated. This can pose a hazard during the decommissioning of old equipment.\n\nAfter initial investigation in 2013, and later in 2014, the Netherlands Food and Consumer Product Safety Authority issued a statement that fire blankets should never be used to extinguish an oil/fat fire such as a chip pan fire, even if the icons or text on the blanket indicates the blanket may be used in such a case.\nFor a fire to burn, all three elements of the fire triangle must be present: heat, fuel and oxygen. The fire blanket is used to cut off the oxygen supply to the fire, thereby putting it out. The fire blanket must be sealed closely to a solid surface around the fire. Fire blankets usually have two pull down tails visible from outside the packaging. The user should place one hand on each tag and pull down simultaneously removing the blanket from the bag. The tails are located near the top of the fire blanket which allows the top lip of the fire blanket to fold back over the users' hands, protecting them from heat and direct contact burns.\n\nThe Fire Industry Association (\"FIA\") publish a \"Code of Practice for the Commissioning and Maintenance of Fire Blankets Manufactured to BS EN 1869\".\n\nThe FIA's code of practice recommends that the responsible person ensures that such fire blankets are subject to annual maintenance by a competent service provider.\n\nIt also recommends that consideration should be given to the replacement of fire blankets after seven years from the date of commissioning (or as otherwise specified by the fire blanket's manufacturer).\n"}
{"id": "8929820", "url": "https://en.wikipedia.org/wiki?curid=8929820", "title": "First International Computer", "text": "First International Computer\n\nFirst International Computer, Inc. (FIC; ) is a Taiwanese computer and components manufacturer, that designs and manufactures computer products and electronic components for other electronics equipment manufacturers worldwide. The company's products include motherboards, embedded computing systems, graphics cards, PCs, and notebook computers. Founded in 1980 by Dr. Ming-Jen Chien, in Taipei, Taiwan, FIC is publicly listed on the Taiwan Stock Exchange (TSE 3701).\nFirst International Computer product line-up includes: desktop PCs, notebook PCs, graphics cards embedded systems, mobile phones, and computer memory. They also manufacture Intel and AMD based motherboards for computer hobbyists and PC Manufacturers to build PCs from.\n\nFIC-s Apollo VP3 based motherboard PA-2012 released in November 1997 was the first Socket 7 design supporting AGP.\n\n\n"}
{"id": "58741519", "url": "https://en.wikipedia.org/wiki?curid=58741519", "title": "Francesca Bria", "text": "Francesca Bria\n\nFrancesca Bria is an Italian information technologist and advisor on digital strategy, technology and information policy. She is the Chief Technology and Digital Innovation Officer for the City of Barcelona and she is the founder of the Decode Project, a EU-wide effort to reclaim data sovereignty of citizens.\n\nBria has a background in social science and innovation economics and an MSc in E-business and Innovation from the University College of London, Birkbeck. She was a Senior Project Lead in the Innovation Lab. She was EU Coordinator of the D-CENT project on direct democracy and social digital currencies and principle investigator of the DSI project on digital social innovation in Europe.\n\nBria is a member of the Internet of Things Council and an advisor for the European Commission on Future Internet and Smart Cities policy. She is also a member of the EC Expert Group on Open Innovation (OISPG) and a member of the European Research Cluster on the Internet of Things (IERC).\n\nBria has been advising the City of Rome and the Region of Lazio on innovation policy, open technology, and open cities. She is also active in various grassroots movements advocating for open access, knowledge commons and open, decentralised privacy aware technologies.\n\nIn 2018, Bria was named in Europe's Top 50 Women In Tech, 50 women from across Europe who are putting technology at the heart of their businesses assembled by \"Forbes.\"\n"}
{"id": "12600", "url": "https://en.wikipedia.org/wiki?curid=12600", "title": "Grid network", "text": "Grid network\n\nA grid network is a computer network consisting of a number of (computer) systems connected in a grid topology.\n\nIn a regular grid topology, each node in the network is connected with two neighbors along one or more dimensions. If the network is one-dimensional, and the chain of nodes is connected to form a circular loop, the resulting topology is known as a ring. Network systems such as FDDI use two counter-rotating token-passing rings to achieve high reliability and performance. In general, when an \"n\"-dimensional grid network is connected circularly in more than one dimension, the resulting network topology is a torus, and the network is called \"toroidal\". When the number of nodes along each dimension of a toroidal network is 2, the resulting network is called\na hypercube.\n\nA parallel computing cluster or multi-core processor is often connected in regular interconnection network such as a\nde Bruijn graph,\na hypercube graph,\na hypertree network,\na fat tree network,\na torus, or cube-connected cycles.\n\nNote that a grid network is not the same as a grid computer (or computational grid) (even though the nodes in a grid network are usually computers, and grid computing obviously requires some kind of computer network or \"universal coding\" to interconnect the computers).\n\n"}
{"id": "46186824", "url": "https://en.wikipedia.org/wiki?curid=46186824", "title": "Hard infrastructure", "text": "Hard infrastructure\n\nHard infrastructure is the physical infrastructure of roads, bridges etc., as opposed to the soft infrastructure of human capital and the institutions that cultivate infrastructure. This article delineates both the fixed assets, and the control systems, software required to operate, manage and monitor the systems, as well as any accessory buildings, plants, or vehicles that are an essential part of the system. Also included are fleets of vehicles operating according to schedules such as public transit buses and garbage collection, as well as basic energy or communications facilities that are not usually part of a physical network, such as oil refineries, radio, and television broadcasting facilities.\n\nHard infrastructure in general usually has the following attributes:\n\nThese are physical assets that provide services. The people employed in the hard infrastructure sector generally maintain, monitor, and operate the assets, but do not offer services to the clients or users of the infrastructure. Interactions between workers and clients are generally limited to administrative tasks concerning ordering, scheduling, or billing of services.\n\nThese are large networks constructed over generations, and are not often replaced as a whole system. The network provides services to a geographically defined area, and has a long life because its service capacity is maintained by continual refurbishment or replacement of components as they wear out.\n\nThe system or network tends to evolve over time as it is continuously modified, improved, enlarged, and as various components are rebuilt, decommissioned or adapted to other uses. The system components are interdependent and not usually capable of subdivision or separate disposal, and consequently are not readily disposable within the commercial marketplace. The system interdependency may limit a component life to a lesser period than the expected life of the component itself.\n\nThe systems tend to be natural monopolies, insofar that economies of scale means that multiple agencies providing a service are less efficient than would be the case if a single agency provided the service. This is because the assets have a high initial cost and a value that is difficult to determine. Once most of the system is built, the marginal cost of servicing additional clients or users tends to be relatively inexpensive, and may be negligible if there is no need to increase the peak capacity or the geographical extent of the network.\n\nIn public economics theory, infrastructure assets such as highways and railways tend to be public goods, in that they carry a high degree of non-excludability, where no household can be excluded from using it, and non-rivalry, where no household can reduce another from enjoying it. These properties lead to externality, free ridership, and spillover effects that distort perfect competition and market efficiency. Hence, government becomes the best actor to supply the public goods.\n\nIn 1990, Grübler discussed the history and importance of transportation infrastructures like canals, railroads, highways, airways and pipelines.\n\n\nThe OECD classifies coal mines, oil wells and natural gas wells as part of the mining sector, and power generation as part of the industrial sector of the economy, not part of infrastructure.\n\n\nOECD lists communications under its economic infrastructure Common Reporting Standard codes.\n\n\n\n\n"}
{"id": "1681276", "url": "https://en.wikipedia.org/wiki?curid=1681276", "title": "Historical sizes of railroads", "text": "Historical sizes of railroads\n\nBy 1948, there were 14 railroads in North America with more than 1000 locomotives in service each. Twelve were located in the US and two were in Canada. The total number of steam locomotives and the number of route miles for each railroad in 1948 are given by Bruce (1952). \n\nBruce (1952) also reports that by the end of 1949 a total of approximately 29,000 steam locomotives were left on Class I railroads in the United States.\n\n\n"}
{"id": "1055835", "url": "https://en.wikipedia.org/wiki?curid=1055835", "title": "Hydrogen fuel", "text": "Hydrogen fuel\n\nHydrogen fuel is a zero-emission fuel when burned with oxygen. It can be used in electrochemical cells or internal combustion engines to power vehicles or electric devices. It has begun to be used in commercial fuel cell vehicles such as passenger cars, and has been used in fuel cell buses for many years. It is also used as a fuel for the propulsion of spacecraft.\n\nHydrogen is found in the first group and first period in the periodic table, i.e. it is the first element on the periodic table, making it the lightest element. Since hydrogen gas is so light, it rises in the atmosphere and is therefore rarely found in its pure form, H. In a flame of pure hydrogen gas, burning in air, the hydrogen (H) reacts with oxygen (O) to form water (HO) and releases energy.\n\nIf carried out in atmospheric air instead of pure oxygen, as is usually the case, hydrogen combustion may yield small amounts of nitrogen oxides, along with the water vapor.\n\nThe energy released enables hydrogen to act as a fuel. In an electrochemical cell, that energy can be used with relatively high efficiency. If it is used simply for heat, the usual thermodynamics limits on the thermal efficiency apply.\n\nHydrogen is usually considered an energy carrier, like electricity, as it must be produced from a primary energy source such as solar energy, biomass, electricity (e.g. in the form of solar PV or via wind turbines), or hydrocarbons such as natural gas or coal. Conventional hydrogen production using natural gas induces significant environmental impacts; as with the use of any hydrocarbon, carbon dioxide is emitted.\n\nBecause pure hydrogen does not occur naturally on Earth in large quantities, it usually requires a primary energy input to produce on an industrial scale. Common production methods include electrolysis and steam-methane reforming. In electrolysis, electricity is run through water to separate the hydrogen and oxygen atoms. This method can use wind, solar, geothermal, hydro, fossil fuels, biomass, nuclear, and many other energy sources. Obtaining hydrogen from this process is being studied as a viable way to produce it domestically at a low cost. Steam-methane reforming, the current leading technology for producing hydrogen in large quantities, extracts hydrogen from methane. However, this reaction releases fossil carbon dioxide and carbon monoxide into the atmosphere which are greenhouse gases exogenous to the natural carbon cycle, and thus contribute to global warming which is rapidly heating the Earth's oceans and atmosphere.\n\nHydrogen is locked up in enormous quantities in water, hydrocarbons, and other organic matter. One of the challenges of using hydrogen as a fuel comes from being able to efficiently extract hydrogen from these compounds. Currently, steam reforming, which combines high-temperature steam with natural gas, accounts for the majority of the hydrogen produced. This method of hydrogen production occurs at temperatures between 700-1100°C, and has a resultant efficiency of between 60-75%. Hydrogen can also be produced from water through electrolysis, which is less carbon intensive if the electricity used to drive the reaction does not come from fossil-fuel power plants but rather renewable or nuclear energy instead. The efficiency of water electrolysis is between about 70-80%, with a goal set to reach 82-86% efficiency by 2030 using proton exchange membrane (PEM) electrolyzers. Once produced, hydrogen can be used in much the same way as natural gas - it can be delivered to fuel cells to generate electricity and heat, used in a combined cycle gas turbine to produce larger quantities of centrally produced electricity or burned to run a combustion engine; all methods producing no carbon or methane emissions. In each case hydrogen is combined with oxygen to form water. The heat in a hydrogen flame is a radiant emission from the newly formed water molecules. The water molecules are in an excited state on initial formation and then transition to a ground state; the transition releasing thermal radiation. When burning in air, the temperature is roughly 2000 °C (the same as natural gas). Historically, carbon has been the most practical carrier of energy, as hydrogen and carbon combined are more volumetrically dense, although hydrogen itself has three times the energy density per weight as methane or gasoline. Although hydrogen is the smallest element and thus has a slightly higher propensity to leak from venerable natural gas pipes such as those made from iron, leakage from plastic (polythylene PE100) pipes is expected to be very low at about 0.001%. \n\nThe reason steam methane reforming has traditionally been favoured over electrolysis is because whereas methane reforming directly uses natural gas, electrolysis requires electricity. As the cost of producing electricity (via wind turbines and solar PV) falls below the cost of natural gas, electrolysis becomes cheaper than SMR.\n\nHydrogen fuel can provide motive power for liquid-propellant rockets, cars, boats and airplanes, portable fuel cell applications or stationary fuel cell applications, which can power an electric motor. The problems of using hydrogen fuel in cars arise from the fact that hydrogen is difficult to store in either a high pressure tank or a cryogenic tank.\n\nCombustion engines in commercial vehicles have been converted to run on a hydrogen-diesel mix in the UK, where up to 70% of emissions have been reduced during normal driving conditions. This eliminates range anxiety as the vehicles can fill up on diesel. Minor modifications are needed to the engines, as well as the addition of hydrogen tanks at a compression of 350 bars. Trials are now underway to test the efficiency of the 100% conversion of a Volvo FH16 heavy-duty truck to use only hydrogen. The range is expected to be 300km/17kg; which means an efficiency better than a standard diesel engine (where the embodied energy of 1 gallon of gasoline is equal to 1 kilogram of hydrogen). At a low cost price for hydrogen (€5/kg), significant fuel savings could be made via such a conversion in Europe or the UK. A lower price would be needed to compete with gasoline in the US, as gasoline is not exposed to high taxes at the pump. \n\nFuel cell engines are two to three times more efficient than combustion engines, meaning that much greater fuel economy is available using hydrogen in a fuel cell.\n\n\n"}
{"id": "45699734", "url": "https://en.wikipedia.org/wiki?curid=45699734", "title": "ISmartAlarm", "text": "ISmartAlarm\n\niSmartAlarm is a do-it-yourself (DIY) smart home security system controlled with a user’s smartphone. The system and devices are designed and manufactured by iSmart Alarm, Inc, a start-up based in Sunnyvale, California.\nThe system uses a hub connected to a home’s router to allow users control of home security and home automation devices, including multiple wireless devices. Users can arm and disarm their system, and receive a push notifications, phone call, email, and text message if the system is triggered. The iSmartAlarm system is to be used as a self-monitored solution with no monthly fees and no contracts, as opposed to traditional monitored systems such as ADT or Vivint. iSmartAlarm is currently a closed ecosystem, only operating with its own devices. Announcements have been made for future integrations with Google Thread Group, HomeKit, and Z-Wave device support. On August 10, 2017, iSmart Alarm, Inc. announced the integration with Amazon Alexa. \n\niSmart Alarm, Inc. first announced the development of the iSmartAlarm Home Security System at the annual Consumer Electronics Show in 2013. Launching with an Indiegogo crowdfunding campaign, the campaign was successful in raising $226,074 in pre-sales (against an original goal of $50,000). After completion of the crowdfunding campaign, the iSmartAlarm began shipping to contributors on June 28, 2013. iSmartAlarm also launched a new home security video camera on Indiegogo September 30, 2014.\n\nAn IFTTT Channel was launched for the iSmartAlarm on March 18, 2015.\n\niSmartAlarm currently manufactures and supports the following devices.\n\nThe iSmartAlarm Home Security System has received the Editor’s Choice Award from \"PCMag\", cited as “Best DIY Home Security Kit” by \"CNET\" in their review of best smart home devices of 2015, and positive marks and reviews from \"SecurityGem,\", \"TUAW\", and \"MacWorld\". iSmartAlarm has also won prestigious design and program awards from Red Dot, the Mark of Excellence Home Technology App of the Year from the Consumer Electronics Association (CEA) TechHome Division in 2013, a finalist for the Residential Security Product of the Year, and featured on Coldwell Banker’s “25 Smart Home Technologies that Matter Most to Home Buyers.” \n\n"}
{"id": "47903929", "url": "https://en.wikipedia.org/wiki?curid=47903929", "title": "International Council on Clean Transportation", "text": "International Council on Clean Transportation\n\nThe International Council on Clean Transportation (ICCT) is an independent nonprofit organization incorporated under Section 501(c)(3) of the US tax code. It provides technical and scientific analysis to environmental regulators. It is funded by the ClimateWorks Foundation, the William and Flora Hewlett Foundation, the Energy Foundation, and the David and Lucile Packard Foundation.\n\nThe ICCT commissioned researchers at West Virginia University to test Volkswagen diesel car emissions in 2013. In May 2014, ICCT alerted the US EPA and the California Air Resources Board that the models displayed much higher levels of nitrogen oxide emissions than permitted by law. In September 2015, the EPA said Volkswagen could be liable for up to $18 billion in penalties for using software on almost 500,000 VW and Audi diesel cars sold between 2009 and 2015 that circumvented emissions regulations, unleashing a controversy that led to multiple regulatory probes worldwide.\n\nIn 2015 an ADAC study (ordered by ICCT) of 32 Euro6 cars showed that few complied with on-road emission limits. In 2016 ICCT measured 19 new cars and found that real emissions were 40% higher than they were approved with, primarily due to the lax methods of NEDC-testing.\n\n\n"}
{"id": "52654682", "url": "https://en.wikipedia.org/wiki?curid=52654682", "title": "List of Namco video game compilations", "text": "List of Namco video game compilations\n\nOver the years, video game developer Namco has released various compilation-versions of their classic video games.\n\n\"Namco Classic Collection Vol. 1\" and \"Vol. 2\" are two arcade machines, with both volumes including three Namco arcade games and updated variants of each game.\n\nTwo compilations of Namco arcade games for Windows (specifically Windows 95) that were published by Microsoft.\n\n\"Arcade Classics\" is a compilation for the Phillips CD-i that was released in Europe, but unlike the majority of Namco compilations, it was not released in North America. This compilation contains ports of \"Galaxian\", \"Ms. Pac-Man\" and \"Galaga\".\n\n\"Xevious 3D/G+\" is a compilation of four Xevious games for PlayStation that was also released on the PlayStation Store.\n\"Pac-Man Super ABC\" is a compilation of Pac-Man games developed by Two Bit Score and released into arcades in 1999. It includes \"Pac-Man\", \"Ms. Pac-Man\", \"Pac-Attack\" (not to be confused with the puzzle game on consoles with the same name), \"Ms. Pac-Attack\", \"Pac-Man Plus\", \"Ms. Pac-Man After Dark\", \"Ultra Pac-Man\", and \"Piranha\".\nOn anniversary years of the release of either \"Pac-Man\" or \"Ms. Pac-Man\", a compilation of Namco arcade games would be released into the arcades.\n\"20 Year Reunion: Ms. Pac-Man/ Galaga - Class of 1981\" was released in 2001, and was made to celebrate the 20th anniversary of \"Ms. Pac-Man\" and \"Galaga\", both of which are playable. Additionally, the original \"Pac-Man\" is playable by performing a special code using the joystick.\n\"Pac-Man 25th Anniversary\" was released in 2005 to celebrate the 25th anniversary of \"Pac-Man\", and also featured \"Ms. Pac-Man\" and \"Galaga\". Two versions of this machine were produced; one for arcades and another for homes, without the coin slot.\n\"Pac-Man's Arcade Party\" was released in 2010 to celebrate the 30th anniversary of \"Pac-Man\". The cabinet includes \"Pac-Man\", \"Pac-Mania\", \"Galaxian\", \"Galaga\", \"Galaga '88\", \"Dig Dug\", \"Xevious\", \"Mappy\", \"Rally-X\", \"Bosconian\", \"Rolling Thunder\" and \"Dragon Spirit\". Much like \"Pac-Man 25th Anniversary\", a home version was also produced, with \"Ms. Pac-Man\" as a bonus game.\n\n\"Pac-Man Collection\" is a compilation of four Pac-Man games for Game Boy Advance that was also released on the Wii U Virtual Console, and included \"Pac-Man\", \"Pac-Attack\", \"Pac-Mania\" and \"Pac-Man Arrangement\".\n\n\"Namco Vintage\" was a compilation containing \"Galaga\", \"Dig Dug\", and \"Pole Position\" that was a downloadable game on the \"Xbox Live Arcade\" disc for Xbox (not to be confused with Xbox Live Arcade for Xbox 360).\n\nA series of \"Plug It In & Play TV Games\" featuring Namco arcade games has had almost annual releases with either \"Pac-Man\" or \"Ms. Pac-Man\" being the main game included.\n\n\"Namco All-Stars: Pac-Man and Dig Dug\" was a compilation of ports for Windows, and featured the original \"Pac-Man\" and \"Dig Dug\", and also featured versions of both games that added enhanced graphics and sound. The enhanced graphics for \"Pac-Man\" were from \"Pac-Man Championship Edition\" and the enhanced graphics for \"Dig Dug\" were from \"\".\n\n\"Namco Games Portal\" was an iOS application released in 2010 that included several Bandai Namco-developed iOS games as well as games that could be purchased through the app. The games that were free to play were \"Letter Labyrinth\", \"Time Crisis 2nd Strike\", \"Pac-Man Lite\", \"Galaga Remix Lite\", \"Dig Dug Remix Lite\" and \"\".\n\nTo celebrate Galaga's 30th anniversary, \"Galaga 30th Collection\" is an iOS application that is downloadable for free and comes with \"Galaxian\", with its three arcade sequels available to buy as in-app purchases.\n\n\"Pac-Man & Galaga Dimensions\" is a video game compilation for Nintendo 3DS which includes two new games, \"Pac-Man Tilt\" and \"Galaga 3D Impact\", as well as \"Pac-Man Championship Edition\", \"Galaga Legions\", the original \"Pac-Man\" and the original \"Galaga\".\n\n\"Namco Arcade\" was an application for iOS and Android that was downloadable for free and allowed players to play each game for free once a day, with either the option to purchase the game in-app, or to utilize its \"Play Coin\" feature. The app featured \"Pac-Man\", \"Galaga\", \"The Tower of Druaga\", \"Rolling Thunder\", \"Dragon Buster\", \"Motos\", \"Phozon\", \"Xevious\", \"Pac-Land\" and \"StarBlade\". The app was later delisted from the App Store on March 31, 2016.\n\n\"Pac-Man Games\" was an iOS application by Namco Bandai Games that contained timed \"S\" (Score Attack) versions of six different Namco games along with social network features, with the games being \"Pac-Man S\", \"Dig Dug S\", \"Galaga S\", \"Rally-X S\", \"Gator Panic S\", and \"Pac-Chain S\".\n\n\"Pac-Man Museum\" is a downloadable compilation of \"Pac-Man\" games for Xbox Live Arcade, PlayStation Network (PS3), and Windows PC (through Steam).\n\n\"Pac-Man Championship Edition 2 + Arcade Game Series\" is a retail disc containing three of the Arcade Game Series games (\"Pac-Man\", \"Galaga\", and \"Dig Dug\") compiled with \"Pac-Man Championship Edition 2\", it was released for PlayStation 4 and Xbox One on November 1, 2016 in North America.\n\n\"Pac-Man Pocket Player\" is a dedicated handheld console developed by My Arcade that includes \"Pac-Man\", \"Pac-Mania\", and \"Pac-Attack\" (as \"Pac-Panic\"). It was be released by Bandai Namco during July 2018. All of the games included are the Sega Genesis versions of the games including \"Pac-Man\" which is a homebrew port.\n\n\"Pac-Man's Pixel Bash\" is an arcade cabinet that released with both a coin-op version and a version for homes that includes a compilation of 31 and 32 Namco arcade games respectively. The games included are:\n denotes that the game is only included in the home version of the arcade cabinet.\n\n\"Disk NG\" is a series of two compilations for the MSX that contain MSX ports of Namco arcade games with also an exclusive game in each.\n\n\"Namco Gallery\" is a Japan-only series of game compilations for Game Boy containing Game Boy versions of console and arcade games from Namco. It is split into three volumes, and they are all enhanced when played on a Super Game Boy.\n\n\"Namco History\" is a series of four Japan-only compilations of 1980s Namco arcade games released for Windows in the late 1990s.\n\"Namco Anthology\" is a two disc series of game compilations for PlayStation that have only been released in Japan in 1998. They are similar to the Namco Museum series except that the Anthology collections include games that have been released on consoles originally. Each disc includes four games and along with each of the games, there were also updated versions of each of the games. Both \"Namco Anthology\" titles were released onto the Japanese PlayStation Store as PSOne Classics on December 18th, 2013.\n\nThe remake of \"Pac-Attack\" was later included as a bonus game in \"Pac-Man World 2\".\n\nNamco Collection for Windows is a two-volume series of Namco arcade compilations that were only released in Japan. It was released there less than a year after the final volume of \"Namco History\" (another series of Namco arcade compilations for Windows that was only released in Japan). None of the games in either volume were included in any of the \"Namco History\" volumes, making this series seem like its successor. It is currently unknown if the second volume was ever released.\n\n\"Gunvari Collection + Time Crisis\" is a video game compilation for PlayStation 2 that contains all three of the original \"Point Blank\" games as well as the first \"Time Crisis\", all using the Guncon 2.\n\n\"NamCollection\" is a video game compilation for PlayStation 2 that contains five PlayStation games, the compilation celebrated Namco's 50th anniversary but it's not to be confused with \"\".\n\nTwo of the \"Let's! TV Play Classic\" devices includes Namco games, each one contains two classic games and two new games using the classic sprites.\n\n"}
{"id": "9490488", "url": "https://en.wikipedia.org/wiki?curid=9490488", "title": "List of lens designs", "text": "List of lens designs\n\nThis list covers optical lens designs grouped by tasks or overall type. The field of optical lens designing has many variables including the function the lens or group of lenses have to perform, the limits of optical glass because of the index of refraction and dispersion properties, and design constraints including realistic lens element center and edge thicknesses, minimum and maximum air-spaces between lenses, maximum constraints on entrance and exit angles, and even cost. Some lenses listed are overall types with sub-designs (\"as noted\"). \n\nSimple lenses are lenses consisting of a single element. Lenses in this section may overlap with lens designs in other sections, for example the Wollaston landscape lens is a single element and also a camera lens design.\n\n\nThere are many compound designs of achromatic lenses, designed to reduce color-related distortion (Chromatic aberration):\n\n\nCamera lenses use a wide variety of designs because of the need to balance and trade off different requirements: angle of view (i.e. focal length in relation to the film or sensor size), maximum aperture, resolution, distortion, color correction, back focal distance, and cost.\n\n\nAn eyepiece, a type of (usually a compound) lens that attaches to optical devices such as telescopes and microscopes, comes in many designs:\n\n"}
{"id": "43210266", "url": "https://en.wikipedia.org/wiki?curid=43210266", "title": "MOS (brand)", "text": "MOS (brand)\n\nMOS (Magnetic Organization System) is an American brand of organizational tools that use magnetism to manage cables and other magnetic items. The brand was formed through crowdfunding on Kickstarter in August 2012. MOS has launched every subsequent product through the Kickstarter platform since. In January 2014 MOS released Spring, a line of phone and audio cables, as well as a miniaturized version of the original Magnetic Organization System called MOS Menos. Later in June 2014 MOS started another Kickstarter campaign for the Reach project which made its goal of $50,000 in the first day of the Kickstarter campaign. The Reach campaign was funded at 284 percent of the initial goal on July 26, 2014. MOS is a brand of Sewell Development Corp.\n\n"}
{"id": "29100672", "url": "https://en.wikipedia.org/wiki?curid=29100672", "title": "Macro BIM", "text": "Macro BIM\n\nMacro BIM (Building Information Model) is a building information model, assembled of higher level building elements, used for macro level analysis including visualization, spatial validation, cost modeling/estimating, phasing/sequencing, energy performance, and risk.\nMacro models are intended to be built quickly, facilitating rapid analysis of multiple concepts or ideas prior to launching into a more detailed in depth study of a preferred concept using \"Micro BIM\" applications. \nMacro BIM authoring applications often utilize parametric variables and properties as well as inferencing capabilities to quickly build enough relevant data to facilitate analysis.\n\n"}
{"id": "14440975", "url": "https://en.wikipedia.org/wiki?curid=14440975", "title": "Mediastinoscope", "text": "Mediastinoscope\n\nA mediastinoscope is a thin, tube-like instrument used to examine the tissues and lymph nodes in the area between the lungs (mediastinum) in a procedure known as mediastinoscopy. These tissues include the heart and its large blood vessels, trachea, esophagus, and bronchi. The mediastinoscope has a light and a lens for viewing and may also have a tool to remove tissue. It is inserted into the chest through a cut above the breastbone.\n\n"}
{"id": "25292015", "url": "https://en.wikipedia.org/wiki?curid=25292015", "title": "Medical technology assessment", "text": "Medical technology assessment\n\nMedical technology assessment (MTA) is the objective evaluation of a medical technology regarding its safety and performance, its (future) impact on clinical and non-clinical patient outcomes as well as its interactive effects on economical, organizational, social, juridical and ethical aspects of healthcare. Medical technologies are assessed both in absolute terms and in comparison to other (combinations of) medical technologies, procedures, treatments or ‘doing-nothing’.\n\nThe aim of MTA is to provide objective, high-quality information that relevant stakeholders use for decision-making about for example development, pricing, market access and reimbursement of new medical technologies. As such, MTA is similar to health technology assessment (HTA), except that HTA has a wider scope and may include assessments of for example organizational or financial interventions.\n\nThe classical approach of MTA is to evaluate technologies after they enter the marketplace. Yet, a growing number of researchers and policy-makers argue that new technologies should be evaluated before they diffuse into routine clinical practice. MTA of biomedical innovations in a very early stage of development could improve health outcomes, minimise wrong investment and prevent social and ethical conflicts.\n\nOne particular method within the area of early MTA is constructive technology assessment (CTA). CTA is particularly appropriate for the early assessment of dynamic technologies that are implemented under uncertain circumstances. CTA is based on the idea that during the course of technology development, choices are constantly being made about the form, the function, and the use of that technology. Especially in early stages, technologies are not always stable, nor are its specifications and neither is its use, as both technology and environment will mutually influence each other. In recent years, CTA has developed from assessing the (clinical) impact of a new technology to a much broader approach, including the analysis of design, development, and implementation of that new technology.\n\nIn the Netherlands, the department Health Technology and Services Research (HTSR) of the University of Twente and the institute for Medical Technology Assessment (iMTA) of the Erasmus University Rotterdam perform early MTA and CTA in collaboration with technology users (patients, healthcare professionals), technology developers (academic and industrial), technology investors (venture capitalists, government, etc.) technology procurers (hospitals, patients, etc.) and decision-makers in healthcare (patients, policy-makers etc.) By performing excellent scientific research, that is valuable and relevant for society, HTSR and iMTA aim to support decisions about early development and implementation of health care technology in order to achieve high quality healthcare for individual patients. Examples of the research of HTSR include the early economic evaluation of neuromuscular electrical stimulation in the treatment of shoulder pain and early phase technology assessment of nanotechnology in oncology. Examples of the work if iMTA include the development of the widely used cost-effectiveness acceptability curves (CEACs), the introduction of the friction cost method, the valuation if informal care with the CarerQoL instrument and the estimation of indirect medical costs.\n\n"}
{"id": "3847842", "url": "https://en.wikipedia.org/wiki?curid=3847842", "title": "Medium Atomic Demolition Munition", "text": "Medium Atomic Demolition Munition\n\nMedium Atomic Demolition Munition (MADM) was a tactical nuclear weapon developed by the United States during the Cold War. They were designed to be used as nuclear land mines and for other tactical purposes, with a relatively low explosive yield from a W45 warhead, between 1 and 15 kilotons. Each MADM weighed less than 400 lb (181 kg) total. They were deployed between 1965 and 1986.\n\n\n"}
{"id": "471174", "url": "https://en.wikipedia.org/wiki?curid=471174", "title": "Moderate Resolution Imaging Spectroradiometer", "text": "Moderate Resolution Imaging Spectroradiometer\n\nThe Moderate Resolution Imaging Spectroradiometer (MODIS) is a payload imaging sensor built by Santa Barbara Remote Sensing that was launched into Earth orbit by NASA in 1999 on board the Terra (EOS AM) Satellite, and in 2002 on board the Aqua (EOS PM) satellite. The instruments capture data in 36 spectral bands ranging in wavelength from 0.4 µm to 14.4 µm and at varying spatial resolutions (2 bands at 250 m, 5 bands at 500 m and 29 bands at 1 km). Together the instruments image the entire Earth every 1 to 2 days. They are designed to provide measurements in large-scale global dynamics including changes in Earth's cloud cover, radiation budget and processes occurring in the oceans, on land, and in the lower atmosphere. MODIS utilizes four on-board calibrators in addition to the space view in order to provide in-flight calibration: solar diffuser (SD), solar diffuser stability monitor (SDSM), spectral radiometric calibration assembly (SRCA), and a v-groove black body. MODIS has used the marine optical buoy for vicarious calibration. MODIS is succeeded by the VIIRS instrument on board the Suomi NPP satellite launched in 2011 and future Joint Polar Satellite System (JPSS) satellites.\n\nThe MODIS characterization support team (MCST) is dedicated to the production of high-quality MODIS calibrated product which is a precursor to every geophysical science product. A detailed description of the MCST mission statement and other details can be found at MCST Web.\n\nWith its low spatial resolution but high temporal resolution, MODIS data is useful to track changes in the landscape over time. Examples of such applications are the monitoring of vegetation health by means of time-series analyses with vegetation indices, long term land cover changes (e.g. to monitor deforestation rates), global snow cover trends, water inundation from pluvial, riverine, or sea level rise flooding in coastal areas, change of water levels of major lakes such as the Aral Sea, and the detection and mapping of wildland fires in the United States. The United States Forest Service's Remote Sensing Applications Center analyzes MODIS imagery on a continuous basis to provide information for the management and suppression of wildfires.\n\nThe following MODIS Level 3 (L3) datasets are available from NASA, as processed by the Collection 5 software.\n\nRaw MODIS data stream can be received in real-time using a tracking antenna, thanks to the instrument's direct broadcast capability.\n\nAlternatively, the scientific data is made available to the public via several World Wide Web sites and FTP archives, such as:\n\nMost of the data is available in the HDF-EOS format — a variant of Hierarchical Data Format prescribed for the data derived from Earth Observing System missions.\n\n\n"}
{"id": "28203264", "url": "https://en.wikipedia.org/wiki?curid=28203264", "title": "Muzeum Inżynierii Miejskiej w Krakowie", "text": "Muzeum Inżynierii Miejskiej w Krakowie\n\nThe Museum of Municipal Engineering in Kraków or the Muzeum Inżynierii Miejskiej w Krakowie is a municipal museum in Kraków, Poland; located at ul. św. Wawrzyńca 15 street in the centre of historical Kazimierz district. It was established in 1998 by the city, for the purpose of documenting and popularizing the history of the city engineering, transport as well as technological progress. It consists of several buildings housing early trams, buses and motorcycles, radios, industrial machinery and early means of production, as well as many educational aids and displays. The museum is very popular with school children, but also with adults.\n"}
{"id": "16948367", "url": "https://en.wikipedia.org/wiki?curid=16948367", "title": "NFPA 72", "text": "NFPA 72\n\nThe NFPA 72 (\"National Fire Alarm Code\") is a standard published by the National Fire Protection Association.\n\nThe NFPA 72 specifies \"the application, installation, location, performance, inspection, testing, and maintenance of fire alarm systems, fire warning equipment, and emergency warning equipment, and their components.\" [§ 1.1.1]. Federal, state, and local municipalities across the United States have adopted the NFPA 72 as a standard in the enforcement of fire code regulation. Municipalities often adopt revisions of the code after years of review and amendments, making many local fire codes specific to their governing authorities.\n\nThe NFPA 72 2007 edition is sectioned as follows:\nThe NFPA 72 2013 edition (current edition) is sectioned as follows:\n\n"}
{"id": "2847982", "url": "https://en.wikipedia.org/wiki?curid=2847982", "title": "National Internet Exchange of India", "text": "National Internet Exchange of India\n\nThe National Internet Exchange of India (NIXI) is a government non-profit company established in 2003 to provide neutral Internet Exchange Point services in India. It was established under section 8 of the Companies Act 2013, with the Internet Service Providers Association of India (ISPAI) to become the operational meeting point of Internet service providers (ISPs) in India. It was registered on 19 July 2003. Its main purpose is to facilitate the handing over of domestic Internet traffic between the peering ISP members, rather than using servers in the United States or elsewhere. This enables more efficient use of international bandwidth and saves foreign exchange. It also improves the Quality of Services for the customers of member ISPs, by being able to avoid multiple international hops and thus lowering delays and better latency. Utilising servers routed through, and administered by India also reduces the chances of Indian data being intercepted unlawfully by the NSA and the GCHQ. NIXI is managed and operated on a neutral basis and currently has seven operational NOC located in Delhi (Noida), Mumbai (Vashi), Chennai, Kolkata, Bangalore, Hyderabad and Ahmedabad.\n\nSince 2005, NIXI has also created INRegistry (.in domain) as its autonomous body for maintenance of .IN domain.\n\nSince December 2012, NIXI also manages the National Internet Registry of the country delegation Internet Protocol addresses (IPv4 and IPv6) and Autonomous System numbers to its Affiliates.\n\n\n"}
{"id": "49866832", "url": "https://en.wikipedia.org/wiki?curid=49866832", "title": "National Medals of Appreciation and Memorial", "text": "National Medals of Appreciation and Memorial\n\nThe National Medals of Appreciation and Memorial is an honor bestowed by the President of Iran to individuals who have made important contributions to the advancement of knowledge in various fields of science or have lost their lives defending the country. The presidential Committee on the National Medals is responsible for selecting award recipients and is administered by the Presidential Office.\n\nThe National Medals of Appreciation and Memorial were established on November 21, 2010, by an act of the Cabinet of Iran. The National Medal of Appreciation is to honor scientists who have shown significant contributions to the development of the country. The National Medal of Memorial is to honor those who lost their lives defending the country. The National Medals of Appreciation and Memorial are classified as Golden and Silver Medals.\n\n\nThe awards ceremony is organized by the Office of President of Iran. It is presided by the sitting President of Iran.\n\n"}
{"id": "22456358", "url": "https://en.wikipedia.org/wiki?curid=22456358", "title": "Parchment paper", "text": "Parchment paper\n\nParchment paper and bakery release paper are cellulose-based papers that are used in baking as a disposable non-stick surface. Both are also called bakery paper or baking paper. They should not be confused with waxed paper, also known as wax paper or rarely as butter paper.\n\nModern parchment paper is made by running sheets of paper pulp through a bath of sulfuric acid (a method similar to how tracing paper is made) or sometimes zinc chloride. This process partially dissolves or gelatinizes the paper. This treatment forms a sulfurized cross-linked material with high density, stability, and heat resistance, and low surface energy—thereby imparting good non-stick or release properties. The treated paper has an appearance similar to that of traditional parchment, and because of its stability is sometimes used for legal purposes where traditional parchment was used.\n\nThe stickless properties can be also achieved by employing a coated paper, for which a suitable release agent—a coating with a low surface energy and capability to withstand the temperatures involved in the baking or roasting process—is deposited onto the paper's surface; silicone (cured with a suitable catalyst) is frequently used.\n\nA common use is to eliminate the need to grease sheet pans and the like, allowing very rapid turn-around of batches of baked goods. Parchment paper is also used to cook \"en papillote\", a technique where food is steamed or cooked within closed pouches made from parchment paper.\n\nBakery paper can be used in most applications that call for wax paper as a non-stick surface. The reverse is not true, as using wax paper will cause smoke in the oven and affect taste.\n\n"}
{"id": "46418096", "url": "https://en.wikipedia.org/wiki?curid=46418096", "title": "Pristine (company)", "text": "Pristine (company)\n\nPristine is a VC funded startup that develops software for hands-free smartglasses and smart mobile devices, enabling video collaboration and remote support in industrial and manufacturing environments, field service management and healthcare. Pristine is based in Austin, Texas.\n\nPristine was founded by Kyle Samani and Patrick Kolencherry May 2013, shortly after Google announced the Google Glass program. It raised initial funding through angel investors and began piloting in a major academic medical center. In the months following, Pristine raised over $5 million in venture capital investment from S3 Ventures, Capital Factory, Healthfundr, and others.\n\nPristine took second place at HATCH Pitch 2013, a start up pitch competition that was held at the George R. Brown Convention Center. At the 2013 DEMO conference, Pristine CEO Kyle Samani demonstrated on stage how an emergency room surgeon would use Google Glass to request support from another physician.\n\nUniversity of California, Irvine participated in a smartglasses pilot in October 2013, and announced in February the following year that they would roll out the technology to outpatient programs and wound care.\n\nPristine launched the first Google Glass pilot in an emergency room at Rhode Island Hospital in April 2014. It resulted in a peer-reviewed study published in \"JAMA Dermatology\" on the use of smartglasses in a healthcare environment.\n\nIn May 2014, the CEO spoke at TedXYouth@Austin about rural healthcare and how telemedicine could help those affected by the 2010 Haiti earthquake.\n\nIn December 2014, Pristine CTO Patrick Kolencharry spoke at AWS re:Invent conference keynote about how Pristine used AWS and Docker to build a high-availability platform on AWS.\n\nIn April 2015, Pristine attended HIMSS15 conference in Chicago and showcased a smartglasses technology with the University of Pittsburgh Medical Center with Intel.\n\nPristine is one of the ten official partners of Google’s Glass at Work program. The company also has formal partnerships with Vuzix, as well as Epson.\n\nPristine develops software for smartglasses to enable hands free video collaboration, and supports mobile-to-mobile capabilities on web browsers, Android and iOS platforms. Built on WebRTC, MongoDB, Redis, AngularJS, the technology supports secure two-way audio and video, messaging, annotations, and high resolution snapshots.\n\n"}
{"id": "11520063", "url": "https://en.wikipedia.org/wiki?curid=11520063", "title": "Reich Ministry of Public Enlightenment and Propaganda", "text": "Reich Ministry of Public Enlightenment and Propaganda\n\nThe Reich Ministry of Public Enlightenment and Propaganda (, RMVP or \"Propagandaministerium\") was a Nazi government agency to enforce Nazi ideology.\nFounded on 14 March 1933, a few months after the Nazi seizure of power by Adolf Hitler's government, it was headed by Reich Minister Joseph Goebbels. The role of the new ministry, which set up its offices in the 18th-century Ordenspalais across from the Reich Chancellery, was to centralise Nazi control of all aspects of German cultural and intellectual life. An unstated goal was to present to other nations the impression that the Nazi Party had the full and enthusiastic backing of the entire population. Censorship in Germany was vital to the Nazi's retention of political control. It was responsible for controlling the German news media, literature, visual arts, filmmaking, theatre, music, and broadcasting.\n\nAs the central office of Nazi propaganda, it comprehensively supervised and regulated the culture and mass media of Nazi Germany. A major focus of the propaganda was Hitler himself, who was glorified as a heroic and infallible leader and became the focus of a cult of personality. Much of this was spontaneous, but some was stage-managed as part of Goebbels' propaganda work. An example of the latter would be the 1934 Nuremberg Rally. Hitler was the focus and his moves were carefully choreographed. The rally was the subject of the film \"Triumph of the Will\", one of several Nazi propaganda films directed by Leni Riefenstahl. It won the Gold Medal at the 1935 Venice Film Festival. Goebbels and his ministry were involved in both the rally and the film production.\n\nThe ministry was organized into seven departments.\n\n\n"}
{"id": "50044340", "url": "https://en.wikipedia.org/wiki?curid=50044340", "title": "Riovic", "text": "Riovic\n\nRiovic is a financial technology company that owns and operates a technology platform that enables private investors to invest in insurance risks.\n\nIn September 2015 the company officially launched a money transfer service called RiovicPay to help people access cross-border money remittance services on holidays and after hours. The service was rebranded to AfricanRemit. It then released the its on-demand platform for financial services, labeled the \"Uber of Finance\" in November 2015. The platform provided financial advisers, crowd investing and Peer-to-peer insurance.\n\nIn 2016 the company was labelled the Lloyd's of FinTech and successfully launched this revised model to eliminate insurance companies in the insurance value by directly connecting brokers and consumers with risk capital. Also labelled the \"Uber of Insurance\", Riovic pioneered \"private investor backed insurance\" where a private investor (or group of private investors) essentially steps into the financial shoes of the insurer, accepting a stream of certain cash flows in exchange for an uncertain future liability.\n\nRiovic has a collaboration with New Zealand's PeerCover and was admitted into South African Rand Merchant Investment Holdings' (RMIH) incubator for next generation financial services companies (similar to Level39), which provides the company with working space. It is also a member of Facebook's FBStart accelerator programme.\n\nIn April 2016, it got nominated as the only insurtech company in the African FinTech Awards and also go listed on the African FinTech 100 list.\n"}
{"id": "34737565", "url": "https://en.wikipedia.org/wiki?curid=34737565", "title": "STANAG 3910", "text": "STANAG 3910\n\nSTANAG 3910 \"High Speed Data Transmission Under STANAG 3838 or Fibre Optic Equivalent Control\" is a protocol defined in a NATO Standardization Agreement for the transfer of data, principally intended for use in avionic systems. STANAG 3910 allows a 1 Mb/s STANAG 3838 / MIL-STD-1553B / MoD Def Stan 00-18 Pt 2 (3838/1553B) data bus to be augmented with a 20 Mb/s high-speed (HS) bus, which is referred to in the standard as the HS channel: the 3838/1553B bus in an implementation of STANAG 3910 is then referred to as the low-speed (LS) channel. Either or both channels may be multiply redundant, and may use either electrical or optical media. Where the channels use redundant media, these are individually referred to as buses by the standard.\n\nThe original STANAG 3910, i.e. the NATO standard, reached, at least, draft version 1.8, before work on it was abandoned in the early 1990s in favour of its publication through non-military standardization organizations: the foreword to Rev. 1.7 of the STANAG from March 1990 stated \"The main body of this document is identical to the proposed Rev 1.7 of prEN 3910\". Following this, several provisional, green-paper versions, prEN 3910 P1 & P2, were produced by working-group C2-GT9 of the \"Association Europeene des Constructeurs de Materiel Aerospatial\" (AECMA) (now ASD-STAN), before its development also ceased in 1996-7 (following the withdrawal of the French delegation, who held the chair of AECMA C2-GT9 at the time). As a result, the standard remains (as of Aug. 2013) in green paper form: the latest draft version is prEN3910-001 Issue P1, the front sheet of which states, 'This \"Aerospace Series\" Prestandard has been drawn up under the responsibility of AECMA (The European Association of Aerospace Industries). It is published on green paper for the needs of AECMA-Members.' However, despite this disclaimer, the document is offered for sale by ASD-STAN, currently (August 2013) at €382.64.\n\nThe incomplete nature of the standardization process (as of Aug. 2013) has not prevented at least two versions of STANAG 3910 being implemented: one for the Eurofighter Typhoon and one for the Dassault Rafale. The Eurofighter version, known as EFABus, is standardized by an internal Eurofighter document (SP-J-402-E-1039). The standardization documentation for the Dassault version is unknown.\n\nThe EFABus version of STANAG 3910 is known to use an electrical low speed (3838/1553B) control channel and a fibre optic HS channel. The version specified for the Dassault Rafale uses electrical media for both channels.\n\nThere are a number of manufacturers of avionic equipment that supply both flight and ground (e.g. test) equipment to this protocol standard.\n\nThe (draft) standard contains annexes, known as slash-sheets, that specify a number of different media types for the high-speed and low-speed channels, implementations identifying a specific slash-sheet with the relevant specifications.\n\nVersions of STANAG 3910 using optical media for the HS channel component require an additional passive component, in the form of an optical star coupler either reflective or transmissive, to interconnect the remote terminals. This limits the number of remote terminals that may be connected to the HS media, through the effect of the optical star on the optical power (determined by the number of \"ways\" of the star). Therefore, it may not be possible for all the (up to) 31 RTs (and 1 BC) that may be connected to the LS channel to have HS channel connections.\n\nThe optical media types include 200 and 100 μm diameter core (280, 240, or 140 μm clading) Step-index profile (depressed cladding) optical fibre. These are much larger-core fibres than are commonly used in short-haul commercial applications, which are more normally 50/125 or 62.5/125 μm. This is, in part at least, to reduce the problems associated with contamination of the optical connectors – a given size of particle between the end faces of the fibre in a connector or misalignment of such a connector has significantly less effect on the larger fibre – which is seen as a significant issue in avionic applications, especially where contaminating environments, high vibration, and wide temperature ranges can apply.\n\nThe major difference between the transmissive and reflective star coupled fibre networks is that two fibres are needed with the transmissive star coupler to connect a line replaceable item (LRI), but with the reflective star, and a \"Y\" coupler internal to the LRI, only a single fibre is required: a \"Y\" coupler, is a three-port optical device that connects the simplex transmitter and simplex receiver to a single fibre that carries the optical signals transmitted and received by the LRI in opposite directions (half duplex). However, while the use of the reflective star reduces the cabling in the aircraft, and thus weight, the excess losses involved in the use of the \"Y\" couplers and reflective star coupler makes meeting the power budget requirements, given a transmitter power and receiver sensitivity, more difficult.\nWhilst it is explicitly stated that the LS buses may be a fibre optic equivalent to STANAG 3838, e.g. MIL-STD-1773, there are no known implementations of this approach.\n\nVersions using an electrical HS channel require an additional active component, in the form of a \"central repeater\", with multi-tap collector and distributor lines (which use directional couplers to connect to the LRIs) and a buffer memory, to allow for small differences in data rates.\n\nThe standard and the electrical media slash sheet it contains specify a 100-ohm characteristic impedance cable for both collector and distributor lines. A maximum cable length is not given for either, and neither are limits on the numbers of directional couplers and thus RTs. However, the losses in the directional couplers, etc., especially for the RT furthest from the central repeater, and the limitations on dynamic range between the furthest (and most attenuated) and nearest (and least attenuated) RT, will limit the number of RTs operating to the standard that may be connected to the HS media.\n\nSince STANAG 3910 uses a 3838/1553B LS channel for control, the logical architectures that are supported are very similar to those described for 3838/1553B. Essentially, there is a bus controller (BC) and up to 31 individually addressed (0-30) remote terminals (RTs) connected to the bus. The BC then commands the RTs to receive or transmit the data, either as RT to RT, RT to BC, BC to RT, RT to RTs (broadcast), or BC to RTs (broadcast) transfers.\n\nWith electrical media HS buses, the physical architecture is like that with 3838/1553B, save that the central repeater has to be at one end of each of the collector and distributor lines: the RT's connections to these lines work preferentially in one physical direction along the bus - hence directional couplers.\n\nThe use of optical media for the HS buses, e.g. in EFABus, has a significant effect on the physical architectures: it is not practical to implement linier T coupled bus architectures, where the bus is run around the platform (e.g. the aircraft), and each line replaceable item (LRI) connects, though a stub, at the nearest convenient point in its path. Rather, each LRI has an optical physical media connection to a common star coupler, which passively connects it to all the other LRIs connected to the same star. In the case of a reflective star, the bus connection from the RT will be a single fibre cable, over which the RT both transmits and receives (half duplex). With a transmissive star, each RT is connected through two fibres, one for it to transmit and one for it to receive data over.\n\nTransfers over the HS channel are initiated via the 3838/1553B LS channel, in an analogous way to the setup of 3838/1553B data transfers. 3838/1553B BC-RT transfers are sent to a specific subaddress of the receiving and transmitting RTs by the STANAG 3910 bus controller (BC). Despite this being a subaddress on the LS side of the RT, and thus exactly the same as any other 3838/1553B RT's subaddress, this subaddress is known as the \"HS subaddress\". The 3838/1553B BC-RT transfers each carry a single data word, known as an HS action word. Each HS action word identifies the HS message to be transmitted or received, analogous to the command words used to initiate 3838/1553B RT transfers. As with 3838/1553B transfers, there can be HS transfers from BC to RT, RT to BC, RT to RT, BC to RTs (broadcast) and RT to RTs (broadcast).\n\nAccording to the standard, the HS actions words comprise the following:\n\nAs a 3838/1553B data word, the HS action word is preceded by the 3 bit-time data word sync field and followed by the single bit parity bit. As part of a 3838/1553B BC-RT transfer, it is preceded by a 3838/1553B command word, and should normally, i.e. if not broadcast, invalid, or illegal, elicit a 3838/1553B status word from the receiving RT.\n\nIn the case of an RT to RT HS transfer, the BC sends an HS action word to the receiving HS RT, instructing it to receive the HS message with a specified block count value at the specified subaddress. The receiving RT will then reply on the LS channel with an LS status word indicating it received the HS action word. The BC will then, after an intermessage gap on the LS channel, send another HS action word to the transmitting HS RT, instructing it to transmit the message, normally with the same block count value, and from one of its subaddresses. The transmitting RT will then reply on the LS channel with an LS status word indicating it received the HS action word and completing the HS control format. The HS RT transmitting an HS message will then begin its transmission within a maximum time measured from the parity (last) bit of the transmit HS action word. This initialization time is specified in the slash sheets, though all those in the current, draft standard are 24 to 32 µS. If the receiving HS RT does not receive the start of the HS message within a specified (in the slash sheet) time, which should be sufficient for the duration of the HS control format and the initialization time of the transmitter, it is required to timeout.\n\nAccording to the standard, HS messages comprise the following:\n\nWhile the WC fields [sic] are required to contain the actual lengths of the following info fields in words, if the receiving RT implements a feature called \"word count checking\", then the length of the info field may be less than 32 times the block count value in the HS action word by up to 31 words. In effect, the last block of an HS message may vary in length from 1 to 32 words. If the receiving terminal does not implement word count checking then the length of the info field shall be the block count multiplied by 32. The standard does not indicate how the transmitting terminal is meant to know whether the receiving RT implements this feature or not; hence it may be assumed to be part of the system's design.\n\nThere are also, analogous to the 3838/1553B status words, HS status words. These are also 3838/1553B data words sent over the LS channel, from the HS subaddress to which the HS action words are sent. The status words are therefore, unlike with 3838/1553B statuses, not transmitted automatically by the RTs, and require the STANAG 3910 BC to cause their transmission over the LS channel from the same HS subaddress the action words are sent to.\n\nThe HS subaddress, to which the HS action words are sent, and from which HS status words and HS ???? words are transmitted, is not specified by the standard, other than it \"shall not be equal to 00000 or 11111 [binary] and shall not be used for any other function\". It may then, be selected for the specific implementation, i.e. a value that is not otherwise in use.\n\nIt also is possible to have \"normal\" 3838/1553B transfers that take place over the LS channel alone, and which may use any of the other 3910/1553B subaddresses. These transfers may happen in parallel with the HS channel transfers or be in between them. It is, however, common practice not to use the LS channel other than for control of the HS, and for LS mode commands, etc., e.g. during BC handover.\n\nThe duration of an HS control format initiating an HS RT to HS RT transfer over the HS channel comprises a pair of 3838/1553B BC-RT transfers, including command words, data words (the HS action words themselves), LS status responses, LS RT response times, and an inter message gap (which is limited by, but is not necessarily the same as the 3838/1553B specified minimum intermessage gap of 4 μs). As a consequence, the duration of such a HS control format can be relatively long in comparison to the duration of the HS transfer that follows. This overhead is then compounded where the BC initiates an RT to BC transfer on the LS channel to, e.g., obtain the HS status word from the receiver. It is technically possible to begin the setup of the next HS transfer while the previous one is in progress, and thus achieve the minimum permitted HS interframe gap of 4 μs. However, it is common practice to wait for one HS transfer to end before beginning the LS channel transfers to set up the next, as predicting the timing of the end of a transmission is complicated by the possible variations in transmitter bit rates. Thus, while the theoretical throughput approaches 21 (20 + 1) Mbps, the actual throughput will be significantly less than 20 Mbps.\n\nThere is also an extended version of EFABus, known as EFABus Express (EfEx). This was designed for tranche 2 of the Eurofighter Typhoon to reduce the time needed to set up the HS transfers by allowing them to be set up over the HS channel. This version is fully compatible with MIL-STD-1553 / STANAG 3838 and the mixed EFABus (STANAG 3910).\n\nSince the setup of HS transactions over an EfEx channel occurs between the HS transfers themselves, like the implementations of STANAG 3910 that wait for the preceding HS transfer to complete before initiating the next, the maximum bandwidth is necessarily less than 20 Mbps; though it is higher than that of this type of STANAG 3910 channel, because the HS control formats on the HS channel require less time than those on the LS channel. However, where a STANAG 3910 channel implementation performs the setup of an HS transfer in parallel with the preceding one, an implementation of STANAG 3910 could provide a very slightly higher throughput than an EfEX implementation, even allowing for the longest possible transmission of the HS message at the lowest possible data transmission rate. Also, assuming that the RTs met the requirements of the standard for a minimum 4 μs interframe gap time, this should have meant modifying only the BC to predict the end times of the HS messages, and initiate the HS control just before this; rather than modifying both the BC and multiple RTs to send and receive HS control formats on the HS channel.\n\nAnother proposed development of MIL-STD-1553 is known as MIL-STD-1553E or E-1553. This uses technologies similar to those used in ADSL to transmit very much higher bandwidths, in multiple channels, over the same media as the existing data bus, but in such a way that they do not interfere with the operation of the normal 1553B data transfers or RTs that should not be involved in them. MIL-STD-1553E is, therefore, an attractive option for upgrading existing aircraft, etc., that use 1553B, because it should not involve any modification to the wiring or any RTs that are not required to take part in these high-speed transfers.\n\nHowever, whilst there has been some research into its use, there do not appear to be any existing or impending implementations of it on production aircraft, either as new build or upgrades. This may be related to the susceptibility of these additional high-speed transmissions to the specific routeing of the 1553 bus cables, and the exact placement of the couplers, BC, and RTs on different aircraft of a fleet, which may make it difficult to specify, in advance of an upgrade, precisely what additional capacity might be provided.\n\n"}
{"id": "1416993", "url": "https://en.wikipedia.org/wiki?curid=1416993", "title": "Social computing", "text": "Social computing\n\nSocial computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing. \n\nSocial computing begins with the observation that humans—and human behavior—are profoundly social. From birth, humans orient to one another, and as they grow, they develop abilities for interacting with each other. This ranges from expression and gesture to spoken and written language. As a consequence, people are remarkably sensitive to the behavior of those around them and make countless decisions that are shaped by their social context. Whether it's wrapping up a talk when the audience starts fidgeting, choosing the crowded restaurant over the nearly deserted one, or crossing the street against the light because everyone else is doing so, social information provides a basis for inferences, planning, and coordinating activity.\n\nThe premise of social computing is that it is possible to design digital systems that support useful functionality by making socially produced information available to their users. This information may be provided directly, as when systems show the number of users who have rated a review as helpful or not. Or the information may be provided after being filtered and aggregated, as is done when systems recommend a product based on what else people with similar purchase history have purchased. Alternatively, the information may be provided indirectly, as is the case with Google's page rank algorithms which orders search results based on the number of pages that (recursively) point to them. In all of these cases, information that is produced by a group of people is used to provide or enhance the functioning of a system. Social computing is concerned with systems of this sort and the mechanisms and principles that underlie them.\n\nSocial computing can be defined as follows:\n\nMore recent definitions, however, have foregone the restrictions regarding anonymity of information, acknowledging the continued spread and increasing pervasiveness of social computing. As an example, Hemmatazad, N. (2014) defined social computing as \"the use of computational devices to facilitate or augment the social interactions of their users, or to evaluate those interactions in an effort to obtain new information.\"\n\nPLATO may be the earliest example of social computing in a live production environment with initially hundreds and soon thousands of users, on the PLATO computer system based in the University of Illinois at Urbana Champaign in 1973, when social software applications for multi-user chat rooms, group message forums, and instant messaging appeared all within that year. In 1974, email was made available as well as the world's first online newspaper called NewsReport, which supported content submitted by the user community as well as written by editors and reporters.\n\nSocial computing has to do with supporting \"computations\" that are carried out by groups of people, an idea that has been popularized in James Surowiecki's book, The Wisdom of Crowds. Examples of social computing in this sense include collaborative filtering, online auctions, prediction markets, reputation systems, computational social choice, tagging, and verification games. The social information processing page focuses on this sense of social computing.\n\nThe idea to engage users using websites to interact was first brought forth by Web 2.0 and was an advancement from Web 1.0 where according to Cormode, G. and Krishnamurthy, B. (2008): \"content creators were few in Web 1.0 with the vast majority of users simply acting as consumers of content.\"\n\nWeb 2.0 provided functionalities that allowed for low cost web-hosting services and introduced features with browser windows that used basic information structure and expanded it to as many devices as possible using HTTP.\n\nBy 2006, Of particular interest in the realm of social computing is social software for enterprise. Sometimes referred to as \"Enterprise 2.0\", a term derived from Web 2.0, this generally refers to the use of social computing in corporate intranets and in other medium- and large-scale business environments. It consisted of a class of tools that allowed for networking and social changes to businesses at the time. It was a layering of the business tools on Web 2.0 and brought forth several applications and collaborative software with specific uses.\n\nFinanceElectronic negotiation, which first came up in 1969 and was adapted over time to suit financial markets networking needs, represents an important and desirable coordination mechanism for electronic markets. Negotiation between agents (software agents as well as humans) allows cooperative and competitive sharing of information to determine a proper price. Recent research and practice has also shown that electronic negotiation is beneficial for the coordination of complex interactions among organizations. Electronic negotiation has recently emerged as a very dynamic, interdisciplinary research area covering aspects from disciplines such as Economics, Information Systems, Computer Science, Communication Theory, Sociology and Psychology.Social computing has become more widely known because of its relationship to a number of recent trends. These include the growing popularity of social software and Web 3.0, increased academic interest in social network analysis, the rise of open source as a viable method of production, and a growing conviction that all of this can have a profound impact on daily life. A February 13, 2006 paper by market research company Forrester Research suggested that:\nSocially intelligent computing is a new term that refers to the recent efforts of individuals to understand the ways in which systems of people and computers will prove useful as intermediaries between people and tools used by people. These systems result in new behaviors that occur as a result of the complex interaction between humans and computers and can be explained by several different areas of science. The Foundations of Social Computing are deeply vested in the understanding of social psychology and cyberpsychology. Social psychology covers topics such as decision making, persuasion, group behavior, personal attraction, and factors that promote health and well-being. Cognitive sciences also play a huge role in understanding Social computing and human behavior on networking elements driven by personal needs/means. Sociology is also a factor since overall environments decide how individuals choose to interact.\n\nThere are multiple areas of social computing that have been able to expand the threshold of knowledge in this discipline. Each area has been able to have a focus and goal behind it that provides us with a deeper understanding of the social behavior between users that interact using some variation of social computing.\n\nSocial software can be any computational system that supports social interactions among groups of people. The following are examples of such systems.\n\nSocial media has become an outlet that is one of the most widely used ways of interacting through computers. Though there are many different platforms that can be used for social media, they all serve the same primary purpose of creating a social interaction through computers, mobile devices, etc. Social media has evolved into not just an interaction through text, but through pictures, videos, GIFs, and many other forms of multimedia. This has provided users an enhanced way to interact with other users while being able to more widely express and share during computational interaction. Within the last couple decades, social media has blown up and created many famous applications within the social computing arena.\n\nThrough social networking, people are able to use platforms to build or enhance social networks/relations among people. These are people who commonly share similar backgrounds, interests, or participate in the same activities. For more details see social networking service.\n\nA wiki provides computing users a chance to collaborate to come together with a common goal and provide content to the public; both novice and expert users. Through the collaboration and efforts of many, a wiki page has no limit for the number of improvements that can be made.\n\nA blog, in social computing aspects, is more a way for people to follow a particular user, group, or company and comment on the progress toward the particular ideal being covered in the blog. This allows users to interact using the content that is provided by page admin as the main subject.\n\nFive of the best blogging platforms include Tumblr, Wordpress, Squarespace, Blogger, and Posterous. These sites enable users, whether it be a person, company, or organization, to express certain ideas, thoughts, and/or opinions on either a single or variety of subjects. There are also a new technology called webloging which are sites that hosts blogs such as Myspace and Xanga. Both blogs and weblogging are very similar in that they act as a form of social computing where they help form social relations through one another such as gaining followers, trending using hashtags, or commenting on a post providing an opinion on a blog.\n\nAccording to a study conducted by Rachael Kwai Fun IP and Christian Wagner, some features of weblogs that attract users and support blogs and weblogs as an important aspect of social computing in forming and strengthening relationships are: content management tools, community building tools, time structuring, search by category, commentary, and the ability to secure closed blogs.\n\nBlogs are also highly used in social computing concepts in order to understand human behaviors amongst online communities through a concept called social network analysis. Social network analysis (SNA) is \"a discipline of social science that seeks to explain social phenomena through a structural interpretation of human interaction both as a theory and a methodology\". There are certain links that occur in blogs, weblogs in this case, where they have different functions that portray different types of information such as Permalink, Blogrolls, Comments, and Trackbacks.\n\nOnline gaming is the social behavior of using an online game while interacting with other users. Online gaming can be done using a multitude of different platforms; common ones include personal computers, Xbox, PlayStation, and many more gaming consoles that can be stationary or mobile.\n\nOnline dating has created a community of websites like OkCupid, eHarmony, and Match.com. These platforms provide users with a way to interact with others that have goals relating to creating new relationships. The interaction between users in sites like these will differ based on the platform but the goal is simple; create relationships through online social interaction.\n\nGroups of people interact with these social computing systems in a variety of ways, all of which may be described as socially intelligent computing.\n\nCrowdsourcing is currently a branch of social computing that has brought computing tasks to a new level when it comes to completion speed. This has also given users a way to earn an income through things like Amazon Mechanical Turk.\n\nThe Dark social media is the social media tools used to collaborate between individuals where contents are supposed to be only available to the participants. However, unlike mobile phone calls or messaging where information is sent from one user, transmitted through a medium and stored on each user devices, with the medium having no storage permission of the actual content of the data, more and more communication methods include a centralized server where all the contents are received, stored, and then transmitted. Some examples of these new mechanisms include Google Doc, Facebook Messages or Snapchat. All of the information passes through these channels has largely been unaccounted for by users themselves and the data analytics. However, in addition to their respective users private companies (Facebook, Twitter, Snapchat) that provided these services do have complete control over such data. The number of images, links, referrals and information pass through digital is supposed to be completely unaccounted for in the marketing scheme of things.\n\nCollective intelligence is considered an area of social computing because of the group collaboration aspect. Becoming a growing area in computer science, collective intelligence provides users with a way to gain knowledge through collective efforts in a social interactive environment.\n\nRecent research has begun to look at interactions between humans and their computers in groups. This line of research focuses on the interaction as the primary unit of analysis by drawing from fields such as psychology, social psychology, and sociology.\n\nSince 2007, research in social computing has become more popular for researchers and professionals in multiple fields dealing with technology, business and politics. A study performed by affiliates of Washington State University used a Latent semantic analysis on academic papers containing the term \"social computing\" to find that topics in social computing converge into the three major themes of Knowledge Discovery, Knowledge Sharing and Content Management. Social computing continues to shift the direction of research in Information Sciences as a whole, extending social aspects into technological and corporate domains. Companies and industries such as Google, Cisco and Fox have invested in such endeavors. Possible questions to be answered through social computing research include how to form stable communities, how these communities evolve, how knowledge is created and processed, how people are motivated to participate, etc.\n\nCurrently, research in the areas of social computing is being done by many well known labs owned by Microsoft and Massachusetts Institute of Technology. The team at Microsoft has taken off with a mission statement of \"To research and develop software that contributes to compelling and effective social interactions.\" They take a main focus on user-centered design processes. They also add rapid prototyping combined with rigorous science to bring forth complete projects and research that can impact the social computing field. Current projects being worked on by the Microsoft team include Hotmap, SNARF, Slam, and Wallop to name a few. MIT, however, has a goal of creating software that shapes our cities and more in depth:\"More specifically, (1) we create micro-institutions in physical space, (2) we design social processes that allow others to replicate and evolve those micro-institutions, and (3) we write software that enables those social processes. We use this process to create more robust, decentralized, human-scale systems in our cities. We are particularly focused on reinventing our current systems for learning, agriculture, and transportation.\"The current research projects at the MIT social computing lab include The Dog Programming Language, Wildflower Montessori, and You Are Here. A broad overview of what to expect from newly started Wildflower Montessori is as follows:\"Wildflower Montessori School is a pilot Lab School and the first in a new network of learning centers. Its aim is to be an experiment in a new learning environment, blurring the boundaries between coffee shops and schools, between home-schooling and institutional schooling, between tactile, multisensory methods and abstract thinking. Wildflower will serve as a research platform to test new ideas in advancing the Montessori Method in the context of modern fluencies, as well as to test how to direct the organic growth of a social system that fosters the growth and connection of such schools.\"\n\nComputational social science can be defined as the interdisciplinary investigation of the social universe on many scales, ranging from individual actors to the largest grouping through the medium of computation.\nComputer science is the study of the principles and use of computers to study experimentation and theories.\n\n\n Introduction to Computational Social Science: Principles and Applications . textbook by Claudio Cioffi-Revilla\nPublished at December 31,2013.page 2,3\n\n"}
{"id": "31762179", "url": "https://en.wikipedia.org/wiki?curid=31762179", "title": "Solar Spark Lighter", "text": "Solar Spark Lighter\n\nA Solar Spark Lighter or Sunlighter is a pocket-sized stainless steel parabolic mirror, shaped to concentrate sunlight on a small prong holding combustible material at the focal point. A revival of an old gadget marketed as a cigarette lighter by RadioShack in the 1980s, it is a useful hiking and camping accessory as its functioning is not affected by having been soaked by rain or falling in rivers or the sea. To operate it clearly needs sunlight and a small piece of flammable material. Once a glowing spark has been achieved, careful blowing will produce a blaze. Its simplicity, small size and light weight make it a useful item in a survival kit.\n\n\n"}
{"id": "733291", "url": "https://en.wikipedia.org/wiki?curid=733291", "title": "Sundial Bridge at Turtle Bay", "text": "Sundial Bridge at Turtle Bay\n\nThe Sundial Bridge (also known as the Sundial Bridge at Turtle Bay) is a cantilever spar cable-stayed bridge for bicycles and pedestrians that spans the Sacramento River in Redding, California, United States and forms a large sundial. It was designed by Santiago Calatrava and completed in 2004 at a cost of US$23,500,000. The bridge has become iconic for Redding.\n\nThe Sundial Bridge provides pedestrian access to the north and south areas of Turtle Bay Exploration Park, a complex containing environmental, art and history museums and the McConnell Arboretum and Gardens. It also forms the gateway to the Sacramento River Trail, a trail completed in 2010 that extends along both sides of the river and connects the bridge to the Shasta Dam. Drift boats of fishermen are often seen passing beneath the bridge as they fish for salmon, steelhead and rainbow trout. In the distance, Mount Shasta is barely visible. Shasta Bally is visible to the West looking upstream the Sacramento.\n\nThe support tower of the bridge forms a single mast that points due north at a cantilevered angle, allowing it to serve as the gnomon of a sundial; it has been billed as the world's largest sundial, although Taipei 101 and the associated sundial design of its adjoining park are much larger. The Sundial Bridge gnomon's shadow is cast upon a large dial to the north of the bridge, although the shadow cast by the tower is exactly accurate on only one day in a year – the summer solstice, June 20 or 21. The time is given as Pacific Daylight Time. The tip of the shadow moves at approximately one foot per minute so that the Earth's rotation about its axis can be seen with the naked eye.\n\nThe Sundial Bridge is a cantilever spar cable-stayed bridge, similar to Calatrava's earlier design of the \"Puente del Alamillo\" in Seville, Spain (1992). This type of bridge does not balance the forces by using a symmetrical arrangement of cable forces on each side of its support tower; instead, it uses a cantilever tower, set at a 42-degree angle and loaded by cable stays on only one side. This design requires that the spar resist bending and torsional forces and that its foundation resists overturning. While this leads to a less structurally efficient structure, the architectural statement is dramatic. The bridge is in length and crosses the river without touching the water, a design criterion that helps protect the salmon spawning grounds beneath the bridge. The cable stays are not centered on the walkway but instead divide the bridge into a major and minor path.\n\nThe cable for the bridge totals and was made in England. The dial of the sundial and a small plaza beneath the support tower are decorated with broken white tile from Spain. The bridge's deck is surfaced with translucent structural glass from Quebec, which is illuminated from beneath and glows aquamarine at night. The steel support structure of the bridge was made in Vancouver, Washington and transported in sections by truck to Redding.\nPlans for the Sundial Bridge began in the 1990s, when the city of Redding budgeted $3 million for a pedestrian bridge across the river. However, costs escalated after Calatrava's design was chosen in 1996, and the project became a controversial one within Redding, supported by a small group of doctors, lawyers, and other professionals but opposed by other residents who thought it would be too expensive and who favored a more \"folksy\" covered bridge design. The bridge was completed in 2004, three years later than originally planned, at a cost of $23.5 million, with funding from the Redding-based McConnell Foundation. The expense was justified on the basis that it would increase tourism in the Redding area, which also features Shasta Dam as another architectural marvel, and it has been successful in that goal.\n\nIn the fiscal year following its grand opening, Turtle Bay Exploration Park, adjacent to the bridge, saw a 42-percent increase in its visitation. As of 2011, Redding's city manager stated that the bridge \"continues to generate millions of dollars worth of commerce and tourism each year\".\n\nThe bridge is the cover image of a general physics textbook by Serway and Jewett, demonstrating the bridge resisting forces of wind and gravity.\n\nIn 2009, Nor-Cal Think Pink, a non-profit organization dedicated to raising awareness of the importance of early detection of breast cancer, received approval from the City of Redding to illuminate the Sundial Bridge in pink for its Think Pink Day. The event now takes place annually.\n\n\n"}
{"id": "45326188", "url": "https://en.wikipedia.org/wiki?curid=45326188", "title": "The Fly Stop", "text": "The Fly Stop\n\nTheFlyStop is a retail fly shop and e-commerce business specializing in fly fishing, hunting and sporting goods. Founded in Pennsylvania in 2006, it is the largest online seller of fishing flies in the United States.\n\nThe companies beginnings started out of a truck where Kory Van Tassel spent downtime tying flies one dozen at a time as he waited for customers he was guiding on the rivers of central and western PA. While building the initial customer base he worked for another successful store, the Neshannock Creek Fly Shop, which sat on prime Blue Ribbon trout fishing. He would take long junkets up to New York for steelhead guiding and after fishing a client all day would set up a vice tying late into the night in cheap motels trying to keep up, before getting up at 4 A.M. to drive to the next spot and set up for the day. Eventually the demand grew to the point he could no longer keep up on his own and began to search for additional tiers to fulfill demand. At the same time he realized that he could only reach a limited number of customers within a small radius of his region invested in a website, www.theflystop.com, to try and expand the market he could reach.\n\nAfter seeing some great initial success he relocated the business to Colorado, continuing to service the East Coast through dedicated customer service and long hours to keep up in both time zones. The business expanded slower than expected in Colorado and after a series of unfortunate business dealings migrated back to its roots in Pennsylvania.\n\nIn 2012 Matthew Austin was brought in as a temporary consultant to the company and was later invited to join as Director of Marketing and E-commerce. This key hire would serve as a turning point in the direction of the company and led to the technologization of many of the companies infrastructure and fulfillment processes, re-branding, expansion of core business arena and growth into additional markets in California. In 2015 he was appointed to a role of President of Business and Lead Strategist.\n\nTheir flagship store is located at 9275 Trade Place, Suite E San Diego, California 92126 and their fleet of guide boats operates out of Dana Landing.\n\nTheir fishing guide service is known worldwide for two of the sports most interesting target species, Mako Sharks and Carp. Captain Conway Bowman pioneered fishing for Mako's on the fly also is host to several ESPN, Sportman Channel, World Fishing network, and the target of several fly fishing film documentaries.\n\nAdditionally the company is a national host to the Carpthrow Down held in Santa Ysabel, CA which attracts anglers from around the country to compete for various prizes from orvis, patagonia and others\n\nThe Carp Throwdown is an annual carp fishing event held in Southern California at Lake Henshaw. It is one of the premier fly fishing events in the country Annually sponsored by Orvis, Patagonia, Scott Fly Rods, Abel Reels, Simms Fishing Products and Reisa Fly Fishing it features divisions for both team boat and individual walk and wade. Started in 2012 by Conway Bowman and Al Quatracchi it was acquired by Theflystop.com in 2014. It was featured in the film Carpland by RA Beattie in 2014 which preceded to win 2015 Movie of the Year by the Drake Magazine.\n\nAdditionally the Flying Mako tournament has run in San Diego several years but is less popular than their Carp Event.\n\nThe Fly Stop is a dedicated benefactor of local and national environmental, fishing, hunting, and Women Oriented causes including Wounded Warriors, Coast Keepers, Trout Unlimited, Pheasants Forever, National Wild Turkey Federation, Walk For a Cure, Ruffed Grouse Society, Surfrider Foundation, Casting for Recovery, Stop Pebble Mine, Save Bristol Bay and several educational institutions and youth programs.\n\nThe company services customers in over 150 countries, the American Armed Forces Abroad, and all 50 States and 16 territories. They disperse over 200 parcels daily through a newly expanded shipping and receiving center.\n"}
{"id": "39664778", "url": "https://en.wikipedia.org/wiki?curid=39664778", "title": "USHUS (sonar)", "text": "USHUS (sonar)\n\nUSHUS is an integrated sonar system developed by the Naval Physical and Oceanographic Laboratory (NPOL) of the Defence Research and Development Organisation (DRDO), India, for use in submarines of the Indian Navy. It is primarily designed to be used in Sindhughosh class submarines, though it is reported to be fitted in the \"Arihant\"-class nuclear-powered ballistic missile submarines as well. USHUS is reported to be superior to its Russian equivalents.\n\nUSHUS is used for detecting and tracking enemy submarines, surface vessels, and torpedoes and can be used for underwater communication and avoiding obstacles. The sonar can work in both active and passive mode, and is capable of interception and underwater communication. It can detect both surface ships and submarines at a range of a few kilometers. The team developing the sonar was awarded the \"Agni Award for self-reliance\" by the Indian Prime Minister in May 2007.\n\nThe production of the sonar is done by Bharat Electronics (BEL) at its Bangalore unit, after transfer of technology from NPOL. NPOL continues to provide technical consultancy and support. The Indian Ministry of Defence signed a contract worth Rs 167 crore with BEL for the delivery of the sonars for four Kilo-class submarines between 2003 and 2007. Initially one sonar system was installed and integrated in Russia and other system was installed on board an Indian submarine.\n\nA CAG audit report filed in December 2012 criticised the program for the delays, and mentioned that by then, only three submarines were upgraded with the sonar, and two of those had completed sea trials. The report also stated that due to delays in implementing the upgrade, a large portion of the sonar's technical life had expired. In 2005, two submarines were outfitted, of which one completed sea trial in January 2011. In 2008, the third submarine was upgraded and it completed the trial in December 2011.\n\nIndian Nuclear Submarine Project starting with INS Arihant (ATV-1) include advanced USHUS sonar system.\n\nBy April 2013, five Sindhughosh class submarines of the navy were upgraded to include the USHUS system. They are, in order of their upgrade, \"Sindhuvir\", \"Sindhuratna\", \"Sindugosh\", \"Sinduvijay\" and \"Sindhurakshak\". INS \"Sindhukirti\" was upgraded in India at Hindustan Shipyard. The remaining submarines of the class are expect to follow.\n\n"}
{"id": "8172483", "url": "https://en.wikipedia.org/wiki?curid=8172483", "title": "Underbalanced drilling", "text": "Underbalanced drilling\n\nUnderbalanced drilling, or UBD, is a procedure used to drill oil and gas wells where the pressure in the wellbore is kept lower than the static pressure then the formation being drilled. As the well is being drilled, formation fluid flows into the wellbore and up to the surface. This is the opposite of the usual situation, where the wellbore is kept at a pressure above the formation to prevent formation fluid entering the well. In such a conventional \"overbalanced\" well, the invasion of fluid is considered a kick, and if the well is not shut-in it can lead to a blowout, a dangerous situation. In underbalanced drilling, however, there is a \"rotating head\" at the surface - essentially a seal that diverts produced fluids to a separator while allowing the drill string to continue rotating.\n\nIf the formation pressure is relatively high, using a lower density mud will reduce the well bore pressure below the pore pressure of the formation. Sometimes an inert gas is injected into the drilling mud to reduce its equivalent density and hence its hydrostatic pressure throughout the well depth. This gas is commonly nitrogen, as it is non-combustible and readily available, but air, reduced oxygen air, processed flue gas and natural gas have all been used in this fashion.\n\nCoiled tubing drilling (CTD) allows for continuous drilling and pumping and therefore underbalanced drilling can be utilized which can increase the rate of penetration (ROP).\n\nThere are several kinds of underbalanced drilling. The most common are listed below.\n\n\nUnder-balanced wells have several advantages over conventional drilling including:\n\n\nUnderbalanced drilling is usually more expensive than conventional drilling (when drilling a deviated well which requires directional drilling tools), and has safety issues of its own. Technically the well is always in a blowout condition unless a heavier fluid is displaced into the well. Air drilling requires a faster up hole volume as the cuttings will fall faster down the annulus when the compressors are taken off the hole compared to having a higher viscosity fluid in the hole. Because air is compressible mud pulse telemetry measurement while drilling (MWD) tools which require an incompressible fluid can not work. Common technologies used to eliminate this problem are either electromagnetic MWD tools or wireline MWD tools. Downhole mechanics are usually more violent also because the volume of fluid going through a downhole motor or downhole hammer is greater than an equivalent fluid when drilling balanced or over balanced because of the need of higher up hole velocities. Corrosion is also a problem, but can be largely avoided using a coating oil or rust inhibitors.\n\nReferences\n\nNas, Steve, Chapter 12 Underbalanced Drilling, from Petroleum Engineering Handbook, Volume II, Editor Robert Mitchell, 2007, pages II-519 to 569. Handbook available from Society of Petroleum Engineers. \n"}
{"id": "12296170", "url": "https://en.wikipedia.org/wiki?curid=12296170", "title": "W29 (nuclear warhead)", "text": "W29 (nuclear warhead)\n\nW29 would have been the operational designation of the TX-29, which was development of the Mk-15 and was proposed for the Javajo, Redstone, and Snark missiles. The TX-29 was terminated in mid-1955 in favor of the W-39 development of the TX-15-X3.\nAs this warhead was never tested its yield is unknown. However it is known that the warhead would have been 52 or 35 wide by 145 inches long and would have weighed in at 3,500 pounds.\n\n"}
{"id": "10631826", "url": "https://en.wikipedia.org/wiki?curid=10631826", "title": "Wagon fort", "text": "Wagon fort\n\nA wagon fort is a mobile fortification made of wagons arranged into a rectangle, a circle or other shape and possibly joined with each other, an improvised military camp. It is also known as a laager (from Afrikaans) ().\n\nAmmianus Marcellinus, a Roman army officer and historian of the 4th century, describes a Roman army approaching \"ad carraginem\" as they approach a Gothic camp. Historians interpret this as a wagon-fort.\nNotable historical examples include Hussites, which called it \"vozová hradba\" (\"wagon wall\"), known under the German word \"Wagenburg\" (\"wagon castle\"), \"tabors\" in the armies of the Polish-Lithuanian Commonwealth and Cossacks, the \"Laager\" of the settlers in South Africa.\n\nSimilar, ad hoc, defensive formations were used in the United States, and were called corrals.\nThese were traditionally used by 19th century American settlers travelling to the West in convoys of Conestoga wagons. When faced with attack, such as by hostile Native American tribes, the travellers would rapidly form a circle out of their wagons, bringing the draft animals (sometimes horses, but more commonly oxen) and women and children to the center of the circle. The armed men would then man the perimeter, the circled wagons serving to break up the enemy charge, to create a certain amount of concealment from observation and shelter from enemy firearms fire. They would also slow down and separate any warriors who attempted to get past the wagons into the circle, making them easier to dispatch, although they never formed a perfect barricade as a true wall would. This tactic was popularly known as \"circling the wagons\", and survives into the modern day as an idiom describing a person or group preparing to defend themselves from attack or criticism.\n\nOne of the earliest examples of using conjoined wagons as fortification is described in the Chinese historical record \"Book of Han\". During the 119 BC Battle of Mobei of the Han–Xiongnu War, the famous Han general Wei Qing used armored wagons known as \"Wu Gang Wagon\" (武剛車) in ring formations to neutralise the Xiongnu's cavalry charges, before launching a counteroffensive which overran the nomads.\n\nIn the 15th century, during the Hussite Wars, the Hussites developed tactics of using the tabors, called \"vozová hradba\" in Czech or \"Wagenburg\" by the Germans, as mobile fortifications. When the Hussite army faced a numerically superior opponent, the Bohemians usually formed a square of the armed wagons, joined them with iron chains, and defended the resulting fortification against charges of the enemy. Such a camp was easy to establish and practically invulnerable to enemy cavalry. The etymology of the word \"tabor\" may come from the Hussite fortress and modern day Czech city of Tábor, which itself is a name derived from biblical Jezreel mountain Tavor (in Hebrew תבור).\n\nThe crew of each wagon consisted of 18 to 21 soldiers: 4 to 8 crossbowmen, 2 handgunners, 6 to 8 soldiers equipped with pikes or flails, 2 shield carriers and 2 drivers. The wagons would normally form a square, and inside the square would usually be the cavalry. There were two principal stages of the battle using the wagon fort: defensive and counterattack. The defensive part would be a pounding of the enemy with artillery. The Hussite artillery was a primitive form of a howitzer, called in Czech a \"houfnice\", from which the English word howitzer comes. Also, they called their guns the Czech word \"píšťala\" (hand cannon), meaning that they were shaped like a pipe or a fife, from which the English word pistol is possibly derived. When the enemy would come close to the wagon fort, crossbowmen and hand-gunners would come from inside the wagons and inflict more casualties on the enemy at close range. There would even be stones stored in a pouch inside the wagons for throwing whenever the soldiers were out of ammunition. After this huge barrage, the enemy would be demoralized. The armies of the anti-Hussite crusaders were usually heavily armored knights, and Hussite tactics were to disable the knight's horses so that the dismounted (and slow) knights would be easier targets for the ranged men. Once the commander saw it fit, the second stage of battle would begin. Men with swords, flails, and polearms would come out and attack the weary enemy. Together with the infantry, the cavalry in the square would come out and attack. At this point, the enemy would be eliminated or very nearly so.\n\nThe wagon fort's effect on Czech history was lost, but the Czechs would continue to use the wagon forts in later conflicts. After the Hussite Wars, foreign powers such as the Hungarians and Poles who had confronted the destructive forces of Hussites, hired thousands of Czech mercenaries (such as into the Black Army of Hungary). At the Battle of Varna in 1444, it is said that 600 Bohemian handgunners (men armed with early shoulder arms) defended a wagon fortification. The Germans would also use wagons for fortification. They would use much cheaper materials than the Hussites, and they would have different wagons for the infantry and the artillery. The Russians also used a type of movable fortress, called a \"guliai-gorod\" in the 16th century.\n\nAnother use of this tactic would be very similar to the infantry squares used by Wellington at the Battle of Waterloo and the South African laager. The wagon forts would form into squares that would support each other. Whenever an enemy charged between two forts, marksmen from both of them would easily exploit the advantage and kill many of the enemy. The wagon fort was later used by the crusading anti-Hussite armies at the Battle of Tachov (1427). However, the anti-Hussite German forces, being inexperienced at this type of strategy, were defeated. The Hussite wagon fort would meet its demise at the Battle of Lipany (1434), where the Utraquist faction of Hussites defeated the Taborite faction by getting the Taborites inside a wagon fort on a hill to charge at them by at first attacking, then retreating. The Utraquists would reunite with the Catholic Church afterwards. Thus ended the wagon fort's effect on Czech history. The first victory against the wagon fort at the Battle of Tachov showed that the best ways to defeat it were to prevent it from being erected in the first place or to get the men inside of it to charge out of it by means of a feint retreat. Thus, the fortification would lose its prime advantage.\n\nA \"laager\", \"lager\", \"leaguer\" or \"laer\" (Afrikaans, from Dutch \"leger\" (camp or army); or ). The word is South African in origin, and originally referred to a formation used by travelers whereby they would draw wagons into a circle and place cattle and horses on the inside to protect them from raiders or nocturnal animals. Laager were extensively used by the Voortrekkers of the Great Trek during the 1830s. The laager was put to the ultimate test on 16 December 1838, when an army of 10 000 Zulu Impis besieged and were defeated by approximately 350 Voortrekkers in the aptly named Battle of Blood River. In 19th century America, the same approach was used by pioneers who would \"circle the wagons\" in case of attack.\n\n\n"}
{"id": "22368429", "url": "https://en.wikipedia.org/wiki?curid=22368429", "title": "Worldwide Responsible Accredited Production", "text": "Worldwide Responsible Accredited Production\n\nWorldwide Responsible Accredited Production (WRAP), formerly Worldwide Responsible Apparel Production, is a not-for-profit 501(c)(6) organization dedicated to promoting safe, lawful, humane and ethical manufacturing around the world through certification and education. The WRAP certification program mainly focuses on the apparel, footwear and sewn products sectors.\n\nCreated by a working group of the American Apparel & Footwear Association in 1997, WRAP became a standalone entity in 2000. While it was founded by clothing industry companies, WRAP's charter requires that a majority of its board members come from outside the apparel industry.\n\nThe WRAP Principles are based on generally accepted international workplace standards, local laws and workplace regulations which encompass human resources management, health and safety, environmental practices, and legal compliance including import/export and customs compliance and security standards:\n\n\n1. Application\n\nA production facility submits basic information to WRAP and pays a registration fee of US$1195.\n\n2. Self-Assessment\n\nFacilities complete a self-assessment of their facility to show that they\nhave been utilizing socially-compliant practices for a minimum of 90 days (for new facilities; facilities seeking re-certification are expected to have been compliant throughout their preceding certification period).\n\n3. Monitoring\n\nAfter submitting their self-assessment, the facility selects a WRAP-accredited monitoring organization to audit the facility against WRAP's 12 Principles. The audit must be successfully passed within 6 months of paying the registration fee to avoid having to re-register.\n\n4. Evaluation\n\nWRAP will review the monitor's audit report and decide whether or not to certify the facility. If WRAP decides not to issue a certification, the facility will be notified of the corrections that need to be made and the monitoring firm will conduct an additional inspection. If the facility does not satisfactorily implement the recommendations within the original six-month period, it must renew its application and pay the registration fee again.\n\n5. Certification\n\nThe certificate issued to a facility is determined by WRAP and depends on the extent to which the audit indicates full compliance and management commitment to the WRAP Principles.\n\nPLATINUM (Valid for 2 years)\n\nA facility must maintain a Gold certification for at least 3 consecutive years before it can be considered for Platinum certification. All Platinum-certified facilities must successfully pass each audit with no corrective actions and maintain continuous certification in order to remain Platinum (a facility whose certification lapses for any reason must hold a Gold certification for at least 3 consecutive years before it can regain Platinum status).\n\nGOLD (Valid for 1 year)\n\nGold certifications are awarded to facilities that demonstrate full compliance with WRAP's 12 Principles during an audit.\n\n\nSILVER (Valid for 6 months)\n\nA facility may request a Silver level certification if an audit finds that it is in substantial compliance with the WRAP principles, but has minor non-compliances in policies, procedures or training that need to be addressed. Facilities seeking certification through this route must request a Silver certificate in writing from a WRAP office at the conclusion of their first formal audit as a certificate will not be automatically issued. Important points to note in this regard are:\n\n•Facilities may not have any \"red flag\" non-compliances such as child labor, egregious health & safety or environmental issues, prison labor, forced or involuntary labor, or harassment or abuse of employees.\n\n•Facilities must demonstrate that their employees are paid at least the minimum wage and any required overtime compensation.\n\nAlternatively, the WRAP Review Board may issue a Silver certificate if any of these criteria are met:\n\nThe facility is a first-time applicant and has demonstrated difficulty in achieving full compliance or has shown non-material non-compliances in one of these areas:\n\n•working hours\n\n•training and communications with employees\n\n•payment of regular wages and overtime premiums\n\n•any other factors that would bar the facility from being granted a Gold certificate\n\nThe facility is applying for a re-certification, but non-material non-compliances are found during the audit\n\nSpecial Notices for Silver Certifications:\n\nAll Silver-certified facilities wishing to renew their certification must reapply prior to the expiration of their certificate, pay a reduced registration fee of US$895, and demonstrate improvement toward achieving Gold certification at the subsequent audit.\n\nA facility may be awarded no more than 3 consecutive Silver certificates. If a facility fails to achieve full compliance within this time period, its certification will be revoked, however the facility may reapply for certification using its original registration number following a waiting period of 6 months.\n\nFacilities that have two successive \"clean\" audits will be eligible for a Gold certificate.\n\nWRAP administers a comprehensive training program that has become one of the leaders in the industry. All of WRAP's accredited monitors must undergo an intensive 5-day Lead Auditor Course and attend regular refresher training sessions in order to maintain accreditation. Additionally, WRAP offers a 2-day Internal Auditor Course for factory personnel charged with maintaining compliance in a facility. WRAP has also been a leader with its Fire Safety Awareness Course that focuses on the prevention of fires via the 5-step Risk Assessment Process\n\nIn February 2018, WRAP became the official Corporate Social Responsibility partner of the American Apparel and Footwear Association.\n\n\n"}
