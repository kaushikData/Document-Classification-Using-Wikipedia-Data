{"id": "56367582", "url": "https://en.wikipedia.org/wiki?curid=56367582", "title": "ACS Mersin", "text": "ACS Mersin\n\nACS Mersin is the name of a glass factory in Mersin, Turkey. ACS stands for \"Anadolu Cam Sanayii\" (\"Anatolian Glass Industry\")\n\nThe factory is at in Yenitaşkent neighborhood to the north of the Turkish state highway which connects Mersin to Tarsus. Its distance to Mersin is about .\n\nThe factory was put into operation in 1969. In 1975, it was acquired by Şişecam Group of Companies. In 1988 NNPB (narrow neck press and blow) technology was successfully used for the first time in Turkey at ACS. Current annual glass production is 260 822 metric tons. The number of employees is 461. But after the planned instauration the annual production will rise to 366685 metric tons and the number of employees will increase to 483.\n"}
{"id": "49018159", "url": "https://en.wikipedia.org/wiki?curid=49018159", "title": "AstroLabs", "text": "AstroLabs\n\nAstroLabs is a Dubai-based Google for Entrepreneurs (GFE) tech startup company co-working space and a member of the GFE Tech Hub Network. \n\nAstroLabs is the first tech hub that is partnered with Google in Dubai, United Arab Emirates (UAE). Network members have access to over twenty hubs in various cities in the world, and can also meet with local entrepreneurs. The work space is located in Dubai’s Jumeirah Lakes Towers neighborhood. AstroLabs was founded in 2012 by Louis Lebbos & Muhammed Mekki, co-founders of Namshi.com. The company launched in March 2013. \n\nIn 2016, AstroLabs hosted the Women in STEM (WiSTEM) Hackathon, among 328 other public community events.\n\nAstroLabs has inspired the formation of technology-based businesses in the area.\n\nThe company's name is based upon the astrolabe, a tool for navigation that was used by mariners in the Middle Ages.\n\n\n"}
{"id": "22251489", "url": "https://en.wikipedia.org/wiki?curid=22251489", "title": "CA/Browser Forum", "text": "CA/Browser Forum\n\nThe Certification Authority Browser Forum, also known as CA/Browser Forum, is a voluntary consortium of certification authorities, vendors of Internet browser software, operating systems, and other PKI-enabled applications that promulgates industry guidelines governing the issuance and management of X.509 v.3 digital certificates that chain to a trust anchor embedded in such applications. Its guidelines cover certificates used for the SSL/TLS protocol and code signing, as well as system and network security of certificate authorities.\n\n, the CA/Browser Forum includes over forty certificate authority members and the following six Internet browser software vendors: Microsoft (Microsoft Edge and Internet Explorer), Apple (Safari), Mozilla (Firefox), Google (Chrome), Opera, and Qihoo 360 (360 Secure Browser).\n\nThe CA/Browser Forum maintains “Guidelines For The Issuance And Management Of Extended Validation (EV) Certificates”. The EV SSL standard improves security for Internet transactions and creates a more intuitive method of displaying secure sites to Internet users. In order for certificate authorities to issue EV SSL Certificates, they must be audited for compliance with the Forum's EV Guidelines in accordance with either WebTrust or ETSI audit criteria.\n\nThe CA/Browser Forum adopted the \"Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates\" in 2011. These Guidelines, which are binding on members of the CA/Browser Forum, took effect July 1, 2012. These guidelines cover all CA-issued certificates. Certificates are now classified as \"DV\" (Domain Validated), \"OV\" (Organization Validated), \"IV\" (Individual Validated), and \"EV\" (Extended Validation), and a method is defined within the specification to distinguish the types of certificates.\n\nIn 2005, Melih Abdulhayoglu of the Comodo Group organized and arranged the first meeting of CA/Browser Forum. The first meeting was held in New York City. This was followed by a meeting in November 2005 in Kanata, Ontario, and a meeting in December, 2005, in Scottsdale, Arizona with the main objective to enable secure connections between users and websites.\n\nIn addition to CA/Browser Forum members, representatives of the Information Security Committee of the American Bar Association Section of Science & Technology, Law and the Canadian Institute of Chartered Accountants participated in developing the standards for issuing and managing Extended Validation SSL certificates.\n\nVersion 1.0 of the EV Guidelines was adopted on 7 June 2007.\n\nVersion 1.1 was adopted by the CA/Browser Forum on 10 April 2008.\n\nVersion 1.2 was adopted by the CA/Browser Forum on 1 Oct 2009.\n\nIt is a great step forward in establishing verified identity for websites considers MSDN in its blog post. Also, Microsoft's vision is that the backbone of an Internet identity system is composed of Extended Validation SSL Certificates intimately integrated with the users' browsing experience.\n\nThe tougher certificates, coupled with browser developments, could help fight phishing, which threatens the multibillion-dollar online retail market.\n\nIn November 2011, the CA/Browser Forum adopted version 1.0 of the \"Baseline Requirements for the Issuance and Management of Trusted Certificates.\"\n\nIn February 2013 a new industry group, the Certificate Authority Security Council (CASC), was formed with a mission that includes promoting CA/Browser Forum standards. Membership requires adherence to CA/Browser Forum standards. The CASC's founding members consist of the seven largest certificate authorities: Comodo, Symantec, Trend Micro, DigiCert, Entrust, GlobalSign and GoDaddy.\n\n"}
{"id": "1461772", "url": "https://en.wikipedia.org/wiki?curid=1461772", "title": "Chart recorder", "text": "Chart recorder\n\nA chart recorder is an electromechanical device that records an electrical or mechanical input trend onto a piece of paper (the chart). Chart recorders may record several inputs using different color pens and may record onto strip charts or circular charts. Chart recorders may be entirely mechanical with clockwork mechanisms, electro-mechanical with an electrical clockwork mechanism for driving the chart (with mechanical or pressure inputs), or entirely electronic with no mechanical components at all (a virtual chart recorder). \n\nChart recorders are built in three primary formats. Strip chart recorders have a long strip of paper that is ejected out of the recorder. Circular chart recorders have a rotating disc of paper that must be replaced more often, but are more compact and amenable to being enclosed behind glass. Roll chart recorders are similar to strip chart recorders except that the recorded data is stored on a round roll, and the unit is usually fully enclosed.\n\nChart recorders pre-dated electronic data loggers which have replaced them in many applications.\n\nCharles Babbage incorporated a chart recorder into the dynamometer car that he built in 1838 or 1839. Here is how he described it:\n\"A roll of paper a thousand feet in length was slowly unwinding itself upon the long table ... About a dozen pens connected with a bridge crossing the middle of the table were each marking its own independent curve gradually or by jumps ...\" The paper advance was geared to the wheels of the railroad carriage, while pens recorded time, the drawbar pull of the locomotive, and numerous other variables.\n\nPart of Samuel Morse's telegraph system was an automatic recorder of the dots and dashes of the code, inscribed on a paper tape by a pen moved by an electromagnet, with a clockwork mechanism advancing the paper. In 1848-1850 a system of such registers was used by John Locke to improve the precision of astronomical observations of stars, providing timing precision much greater than previous methods. This method was adopted by astronomers in other countries as well. William Thomson, 1st Baron Kelvin's syphon recorder of 1858 was a sensitive instrument that provided a permanent record of telegraph signals through long underwater telegraph cables. These recorders came to be referred to as pen registers, although this term later became part of law enforcement jargon referring to the use of such a register to record dialed telephone numbers.\n\nA patent for a 'Pressure Indicator and Recorder' was issued to William Henry Bristol, on September 18, 1888. Bristol went on to form the Bristol Manufacturing Company in 1889. The Bristol Company was acquired by Emerson Electric Company in March 2006, and continues to manufacture a number of different electro-mechanical chart recorders, as well as other instrumentation, measurement, and control products.\n\nThe first chart recorder for environmental monitoring was designed by American inventor J.C. Stevens while working for Leupold & Stevens in Portland, Oregon and was issued a patent for this design in 1915. Chart recorders are still used in applications where instant visual feedback is required or where users do not have the need, opportunity or technical ability to download and view data on a computer or where no electrical power is available (such as in hazardous zones on an oil rig or in remote ecological studies). However, dataloggers' decreasing cost and power requirements allow them to increasingly replace chart recorders, even in situations where battery power is the only option.\n\nThe paper chart is driven past the pen at a steady rate by a clockwork or electrical drive mechanism. One common method is to use a miniature synchronous motor which turns at a constant speed related to the power frequency; a gear-train is used to propel the paper. Industrial strip-chart recorders may have two-speed gear trains that allow a higher speed to be used for initial adjustments of a process or to follow process upsets. Medical and scientific recorders allow a wide range of accurately-controlled speeds to be set.\n\nAn \"X-Y\" recorder drives the chart depending on the value of another process signal. For example, a universal testing machine may plot the tension force on a specimen against its length. Depending on the particular recorder, either the paper chart is moved or else the pen carriage has two axes of motion. Examples of an x-y recorder date back to the 18th century in the form of the steam indicator diagrams used to record pressure and volume in steam engines.\n\nMany mechanisms have been adopted for marking paper. In the telegraphic siphon recorder of 1858 a fine capillary tube is connected to an ink reservoir and is deflected by the process signal. In modern strip chart recorders, a disposable cartridge combining both a fiber-tipped pen and ink reservoir has been used. Other types of recorder use a heated stylus and thermally sensitive paper, an impact printer using a ribbon and an electrically operated hammer, an electric signal acting through a stylus onto electro-sensitive paper, or an electric spark that makes a visible spot on aluminized paper. One form of sensitive and high-speed recorder used beams of ultraviolet light reflected off mirror galvanometers, directed at light-sensitive paper. \n\nThe earliest instruments derived power to move the pen directly from the sensed process signal, which limited their sensitivity and speed of response. Friction between the marking device and paper would reduce the accuracy of the measurements. Instruments with pneumatic, mechanical, or electromechanical amplifiers decoupled pen movement from process measurement, greatly increasing the sensitivity of the instrument and the flexibility of the recorder. Directly-driven pens often moved in the arc of a circle, making the scale difficult to read; pre-printed charts have curvilinear scales printed on them that compensated for the path of the marking pen.\n\nMany types of chart recorders use a galvanometer to drive the marking device. A light coil of wire suspended in the magnetic field of a permanent magnet deflects in proportion to the current through it; instead of the pointer and scale of a direct-reading meter, the recorder deflects a pen or other marking device. The writing mechanism may be a heated needle writing on heat-sensitive paper or a simple hollow ink-fed pen. If the pen is continuously pressed against the paper, the galvanometer must be strong enough to move the pen against the friction of the paper. To lessen the strain on the galvanometer the pen might instead only intermittently be pressed against the writing medium, to make an impression, and then move while pressure is released.\n\nWhere greater sensitivity and speed of response is required a mirror galvanometer, might be used instead, to deflected a beam of light which can be recorded photographically.\n\nAnalog chart recorders using a galvanometer movement to directly drive the pen have limited sensitivity. In a potentiometric type of recorder, the direct drive of the marking pen is replaced with a servomechanism where energy to move the pen is supplied by an amplifier. The motor-operated pen is arranged to move the sliding contact of a potentiometer to feed back the pen position to an error amplifier. The amplifier drives the motor in such a direct as to reduce the error between desired and actual pen position to zero. With a suitable signal processing amplifier, such instruments can record a wide range of process signals. However, the inertia of the servo system limits the speed of response, making these instruments most useful for signals changing over the span of a second or more.\n\nA modern chart recorder is an embedded computer system with an analog to digital converter, a microcontroller, and a hard-copy printing device; such instruments allow great flexibility in signal processing, variable chart speed on process upsets, and can also communicate their measurements to remote points.\n\nOne of the first digital units was designed by William (Bill) C. McElroy jr working for Dohrman Instrument Company in Santa Clara, California. Up until this unit, most chart recorders were rack mounted and had one speed and one sensitivity range. Mr. McElroy's design was an instant loading paper roll 'table-top' unit using an Integrated Chopper Circuit for signal conversion. The unit had plug in circuit boards, plug in single or multi-range modules and plug in single or multi-speed modules. The recorder's sensitivity was 1 microvolt to 100 volts full-scale, which at the time was an industry first. Mr. McElroy also aided in the design and build of the Gas Chromatograph used for analysing dirt and rock samples from the 1969 moon landing. \n\n"}
{"id": "25133010", "url": "https://en.wikipedia.org/wiki?curid=25133010", "title": "Comparison of wiki hosting services", "text": "Comparison of wiki hosting services\n\nThis comparison of wiki hosting services details notable online services which host wiki-style editable web pages. General characteristics of cost, presence of advertising, licensing, and Alexa rank are compared, as are technical differences in editing, features, wiki engine, multilingual support and syntax support.\n\nAn alternative to this page can be found on MediaWiki's site, .\n\nThe following tables compare general information for several of the more than 100 wiki hosting services that have been created. The Alexa traffic rankings are a snapshot that will vary over time. For wiki hosting services that allow hosted wikis to have separate domain names, the Alexa count might not be complete, as such wikis may have their own Alexa rankings. \n\n"}
{"id": "35117434", "url": "https://en.wikipedia.org/wiki?curid=35117434", "title": "Compile and go system", "text": "Compile and go system\n\nIn computer programming, a compile and go system, compile, load, and go system, assemble and go system, or load and go system\nis a programming language processor in which the compilation, assembly, or link steps are not separated from program execution. The intermediate forms of the program are generally kept in primary memory, and not saved to the file system.\n\nExamples of compile-and-go systems are WATFOR, PL/C, and Dartmouth BASIC.\n\nAn example of a load-and-go system is the OS/360 loader, which performed many of the functions of the Linkage Editor, but placed the linked program in memory rather than creating an executable on disk.\n\nCompile and go systems differ from interpreters, which either directly execute source code or execute an intermediate representation.\n\nAdvantages of compile-and-go systems are:\n\nDisadvantages of compile-and-go loaders are:\n\nCompile-and-go systems were popular in academic environments, where student programs were small, compiled many times, usually executed quickly and, once debugged, seldom needed to be re-executed.\n\n"}
{"id": "3779965", "url": "https://en.wikipedia.org/wiki?curid=3779965", "title": "Critical technical practice", "text": "Critical technical practice\n\nCritical technical practice is critical theory based approach towards technological design proposed by Phil Agre where critical and cultural theories are brought to bear in the work of designers and engineers. One of the goals of critical technical practice is to increase awareness and critical reflection on the hidden assumptions, ideologies and values underlying technology design.\n\nA different fork of Critical Technical Practice from Agre’s root (Agre, 1997) was initiated at the former Centre for Cultural Studies between 2007-2017, Goldsmiths, University of London as a way to examine, the live techno-social aspects of contemporary digital culture. This stream of Critical Technical Practice (CTP) at its most broad use can be described as the formation of thought and action that incorporates art as a method of enquiry. This is a compacted intellectual form, that makes the space between the technical, theory and practice ambiguous. A typical digital culture class would make/explore things, attempting to explain the phenomena caught in the lens of a project or proposition. \nAn example of a class may clarify this approach. We might propose that the class create a simple (DOS) denial of service attack on a test remote server by learning to code computers for the first time. It is empowering for students to find out how quickly they can code. The class would learn how to do this from T.J. Connor’s book Violent Python. After the group had reached a self-satisfied tingle of radicalism the group would be asked to look up the author and would find that Connor is a top grade US Military expert. The group would then look at the books distribution and market penetration and be encouraged to question the affective logics, politics and culture of the book, reflect on why the workshop had been constructed the way it was, and their own learning in different registers of technicality, politics of information and personal critic and empowerment was achieved. \nCritical Technical Practice then is not necessarily a reduction of phenomena to literature or a system of logics, but can instead be thought of as knowledge incorporated into a thing that the class created, look at or pointed too, through revealing a certain type of gaze. A prerequisite of Critical Practice is that incorporates this form of gaze is thinking through the formation of oneself as a thinker, actor in the world. Enquiring into one's pre-existence helps understand the structuring of potential that has informed what one has become, what one could easily recognise, and what one could easily achieve. This is not a summation of limit but an acknowledgement of the hard work needed to escape a pre-existence, as it may relate to the four pillars of repression, class, gender, sexuality and race. The situated knowledge of family and friends, their relation to making things, to popular culture, to oral histories, to struggles with money or law, reading, writing and speaking, all inform this process. \nTo this end, CTP is partially related to a schizoanalysis of Foucault's question “What are we today?” (Foucault 1984: pp. 42ff.) The class is always encouraged to unfold what conditions, constrain, control, resist, govern, determine this moment and not another? What patterns of recognition are we privileging and why does it blind us to others? How does language restrict us at the very moment we are able to say something? How can this engagement be born anew in every instance? \n\nPeople whose work contributes to the critical technical practice agenda include \nPhil Agre,\nPaul Dourish,\nNatalie Jeremijenko,\nMichael Mateas,\nSimon Penny,\nWarren Sack,\nGarnet Hertz,\nYoHa, (Matsuko Yokokoji, Graham Harwood)\nand \nPhoebe Sengers.\n\n\n"}
{"id": "1414830", "url": "https://en.wikipedia.org/wiki?curid=1414830", "title": "Dart (rocketry)", "text": "Dart (rocketry)\n\nA dart is a free flying top of a sounding rocket, and contains the payload. Its form is very aerodynamically designed. After the launch stage burned out the dart is detached and continues to rise only with its own inertia.\n\nSome sounding rockets are available both with or without dart. The version without dart is able to transport more payload, but reaches lesser height.\n\n"}
{"id": "33122928", "url": "https://en.wikipedia.org/wiki?curid=33122928", "title": "David Recordon", "text": "David Recordon\n\nDavid Recordon (born September 4, 1986, in Portland, Oregon), aka \"daveman692\", is an open standards advocate currently residing in San Francisco. On March 19, 2015 he was appointed to the newly created role of Director of White House Information Technology by U.S. President Barack Obama.\n\nRecordon was formerly employed as an Open Platforms Tech Lead at blogging company Six Apart and more recently as a Facebook Engineering Director.\n\nRecordon is best known for his involvement in development and popularization of OpenID. Recordon is an active proponent of OAuth and microformats.\n\nIn 2007, Recordon became the youngest recipient of the Google-O'Reilly Open Source Award.\n"}
{"id": "27687109", "url": "https://en.wikipedia.org/wiki?curid=27687109", "title": "Deepwater Nautilus", "text": "Deepwater Nautilus\n\nDeepwater Nautilus is an ultra-deepwater, semi-submersible offshore drilling rig.\n\nBuilt in 2000 in South Korea, she is owned by Transocean, registered in Panama, and currently leased through August 2017 to Royal Dutch Shell at the rate of $531,000 per day for drilling operations in the Gulf of Mexico.\n\n\"Deepwater Nautilus\" is a fifth-generation, RBS-8M design, ultra-deepwater, column-stabilized, semi-submersible mobile offshore drilling unit, designed to drill subsea wells for oil exploration and production.\n\nShe was designed by Reading & Bates RBS-8M and built by Hyundai Heavy Industries in 2000 at the Ulsan shipyard in South Korea. \"Deepwater Nautilus\" can operate at water depths up to and has drilling depth down to .\n\nIn 2000, \"Deepwater Nautilus\" set the world water-depth record for an offshore drilling rig operating in moored configuration at at the Alaminos Canyon block 557 in the United State sector of the Gulf of Mexico.\n\nIn 2002, \"Deepwater Nautilus\" discovered oil at the Shell-operated Great White oil field in Alaminos Canyon block 813.\n\nOn March 6, 2002 she drilled a well in water depth of at the Great White field (Alaminos Canyon block 857).\n\nIn the same year, the new record was set at while drilling at the Alaminos Canyon block 813.\n\nThis record was surpassed in 2003 by waterdepth of at the Alaminos Canyon block 857.\n\nIn 2004, the water-depth of was achieved in Lloyd block 399.\n\nIn March 2009, \"Deepwater Nautilus\" discovered oil at the Appomattox prospect in Mississippi Canyon blocks 391 and 392.\n\nIn 2004, at the time of Hurricane Ivan \"Deepwater Nautilus\" broke free from its location.\n\nIn 2005, as a result of Hurricane Katrina \"Deepwater Nautilus\" had drifted off location. All personnel had been safely evacuated before the approach of the storm.\n\nThe rig's mooring system revealed significant damage and the rig lost approximately of marine riser and a portion of the subsea well control system.\n\nLess than a month later, \"Deepwater Nautilus\" broke free during Hurricane Rita.\n"}
{"id": "231224", "url": "https://en.wikipedia.org/wiki?curid=231224", "title": "Detasheet", "text": "Detasheet\n\nDetasheet is a flexible rubberized explosive, somewhat similar to plastic explosives, originally manufactured by DuPont. Its ingredients are PETN with nitrocellulose and a binder.\n\nIt was manufactured in thin flexible sheets with a rubbery texture, technically known as a rubberized explosive and is generally colored either reddish/orange (commercial) or green (military). In use, it is typically cut to shape for precision engineering charges.\n\nCompared to other explosives detasheet is very stable. It is detonated with a blasting cap or primercord but not by small-arms fire, heat, water, pressure, or concussion. Detasheet is relatively expensive compared to other explosives.\n\nDetasheet C, the last and most common version produced, was made up of 63% PETN, 8% nitrocellulose, and 29% acetyl tributyl citrate (ATBC), an organic plasticizer.\n\nDuPont ceased manufacturing explosives in the 1990s and the trademark is now owned by Ensign-Bickford Aerospace and Defense Company, which now sells a very similar explosive sheet called Primasheet.\n\nA military variant of Detasheet, called Deta Flex, was manufactured in a single thickness (0.25 inch (6.25 mm)) and olive green colored. Deta Flex contains a higher percentage of PETN (70%).\n\nA version of Deta Flex is manufactured for Department of Energy research purposes, colored blue and manufactured in various thicknesses. LX-02-1 contains 73.5% PETN, 17.6% butyl rubber, 6.9% ATBC, and 2.0% Cab-o-sil.\n\n"}
{"id": "9505421", "url": "https://en.wikipedia.org/wiki?curid=9505421", "title": "Dishmaker", "text": "Dishmaker\n\nDishmaker is a machine which creates cups, bowls, and plates from acrylic material and can recycle them into their raw material when they are done being used. \n\nIt was designed by Leonardo Bonanni and made by The MIT Media Lab’s Counter Intelligence Group.\n\nDishmaker uses the shape-memory property of acrylic to make the dishes.\n\n"}
{"id": "14775368", "url": "https://en.wikipedia.org/wiki?curid=14775368", "title": "Elastration", "text": "Elastration\n\nElastration (a portmanteau of \"elastic\" and \"castration\") is a bloodless method of male castration and docking commonly used for livestock. Elastration is simply banding the body part (scrotum or tail) until it drops off. This method is favored for its simplicity, low cost, and minimal training requirements.\n\nElastration is the most common method used to castrate sheep and goats, but is also common in cattle.\n\nElastration involves restraining the animal, without the need for anesthesia or sedation (unlike most other castration methods), in a position that provides access to the genitals. Special \"elastrator pliers\" are then used to place a tight latex (rubber) \"elastrator ring\" gently around the base of the scrotum. This cuts the blood supply to the scrotum and testicles, which will totally decay and slough off within a few weeks. Care must be taken during the procedure to ensure that both testicles are fully descended and properly located inside the scrotum, and that the animal's nipples are not included within the ring. Elastration is normally limited to castrations done during the first few weeks of life, and it cannot be used for species where the scrotum does not have a narrow base, such as pigs or horses. It is commonly recommended to not use this method on goats until they are 8 weeks or older. This is due to possible complications that could occur later in life like Urinary Calculi. \"Goats banding from day 1–30 are most at risk.\" There are those who feel that this method is inhumane and choose to use other methods. These methods would include what some call the \"Emasculatome\", \"Burdizzo\", or \"Richey Nipper\". The Burdizzo and Richey Nipper are names of tools used for the process of the emasculatome.\n\nSome European countries have banned the practice due to their belief that the procedure is inhumane. There is some evidence that elastration is more painful if carried out on older animals, although much of the immediate pain of application can be prevented by injection of local anaesthesia into the scrotal neck and testicles. Practitioners usually try to elastrate as soon as possible, once the testicles have descended, to reduce the amount of dead tissue, infection, and accompanying complications. However, with some animals such as goats, castrating too early increases the frequency of kidney stones and urinary problems due to reduced size of the urethra, so elastration may be postponed. If bull calves are castrated within the first one or two days the testes may sometimes be small and soft enough to be drawn up through the ring, and they continue to develop above the scrotum – surgical castration then becomes necessary.\n\nImproper use of banding on cats can result in death and charges of cruelty. \n\nThe same tool and rings are also used to dock the tails of many breeds of sheep, to prevent dung building up on the tails (which can lead to fly strike). This is usually done at the same time as castration of the ram lambs.\n\nIt is also called sheep marking in Australia.\n\n"}
{"id": "33825382", "url": "https://en.wikipedia.org/wiki?curid=33825382", "title": "Electra (radio)", "text": "Electra (radio)\n\nElectra, formally called the Electra Proximity Link Payload, is a telecommunications package that acts as a communications relay and navigation aid for Mars spacecraft and rovers. The use of such a relay increases the amount of data that can be returned by two to three orders of magnitude.\n\nThe ultimate goal of Electra is to achieve a higher level of system integration, thus allowing significant mass, power, and size reductions, at lower cost, for a broad class of spacecraft.\n\nThe \"Mars Global Surveyor\", \"Mars Odyssey\" and \"Mars Express\" orbiters carry the first generation of UHF relay payloads. Building on this initial experience, NASA developed a next-generation relay payload, the Electra Proximity Link Payload, which flew for the first time on the 2005 \"Mars Reconnaissance Orbiter\".\n\nUsing Mars orbiters as radio relays to increase data return from rovers and other landers reduces the mass and power the surface spacecraft need for communications. A special feature is that it can actively adjust the data rate during a communication session – slower when the orbiter is near the horizon from the surface robot's perspective, faster when it is overhead. To build the relay network cost-effectively, NASA includes a relay communications payload on each of its science orbiters. Mars missions launched after 2005 make use of Electra UHF transceiver to provide for any navigation, command, and data-return needs these missions may have. The arriving spacecraft can receive these signals and determine its distance and speed in relation to Mars. This communication allows much more precise navigation.\n\nWhen NASA's landers and rovers land safely on Mars, Electra can provide precise Doppler data which, when combined with \"Mars Reconnaissance Orbiter\" position information, can accurately determine the location of the lander or rover on the surface of Mars. Electra can also provide UHF coverage to Mars landers and rovers on the surface using its nadir-pointed (pointed straight down at the surface) antenna. This coverage would be important to landed crafts on Mars that might not have sufficient radio power to communicate directly with Earth by themselves.\n\n\n\n\n\n\n"}
{"id": "15399117", "url": "https://en.wikipedia.org/wiki?curid=15399117", "title": "Electro-mechanical modeling", "text": "Electro-mechanical modeling\n\nThe purpose of electro-mechanical modeling is to model and simulate an electro-mechanical system, such that its physical parameters can be examined before the actual system is built. Parameter estimation and physical realization of the overall system is the major design objective of Electro-Mechanical modeling. Theory driven mathematical model can be used or applied to other system to judge the performance of the joint system as a whole. This is a well known & proven technique for designing large control system for industrial as well as academic multi-disciplinary complex system. This technique is also being employed in MEMS technology recently.\n\nThe modeling of purely mechanical systems is mainly based on the Lagrangian which is a function of the generalized coordinates and the associated velocities. If all forces are derivable from a potential, then the time behavior of the dynamical systems is completely determined. For simple mechanical systems, the Lagrangian is defined as the difference of the kinetic energy and the potential energy.\n\nThere exists a similar approach for electrical system. By means of the electrical co-energy and well defined power quantities, the equations of motions are uniquely defined. The currents of the inductors and the voltage drops across the capacitors play the role of the generalized coordinates. All constraints, for instance caused by the Kirchhoff laws, are eliminated from the considerations. After that, a suitable transfer function is to be derived from the system parameters which eventually governs the behavior of the system.\n\nIn consequence, we have quantities (kinetic and potential energy, generalized forces) which determine the mechanical part and quantities (co-energy, powers) for the description of the electrical part. This offers a combination of the mechanical and electrical parts by means of an energy approach. As a result, an extended Lagrangian format is produced.\n\n\n"}
{"id": "33637460", "url": "https://en.wikipedia.org/wiki?curid=33637460", "title": "Electromigrated nanogaps", "text": "Electromigrated nanogaps\n\nElectromigrated Nanogaps are gaps formed in metallic bridges formed by the process of electromigration.\n\nA nanosized contact formed by electromigration acts like a waveguide for electrons. The nanocontact essentially acts like a one-dimensional wire with a conductance of formula_1. The current in a wire is the velocity of the electrons multiplied by the charge and number per unit length, formula_2 or formula_3. This gives a conductance of formula_4. In nano scale bridges the conductance falls in discrete steps of multiples of the quantum conductance formula_1.\n\n"}
{"id": "9663", "url": "https://en.wikipedia.org/wiki?curid=9663", "title": "Electronics", "text": "Electronics\n\nElectronics comprises the physics, engineering, technology and applications that deal with the emission, flow and control of electrons in vacuum and matter. The identification of the electron in 1897, along with the invention of the vacuum tube, which could amplify and rectify small electrical signals, inaugurated the field of electronics and the electron age.\n\nElectronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, integrated circuits, optoelectronics, and sensors, associated passive electrical components, and interconnection technologies. Commonly, electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit.\n\nThe nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible. Electronics is widely used in information processing, telecommunication, and signal processing. The ability of electronic devices to act as switches makes digital information-processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.\n\nElectrical and electromechanical science and technology deals with the generation, distribution, switching, storage, and conversion of electrical energy to and from other energy forms (using wires, motors, generators, batteries, switches, relays, transformers, resistors, and other passive components). This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical amplification of weak radio signals and audio signals possible with a non-mechanical device. Until 1950 this field was called \"radio technology\" because its principal application was the design and theory of radio transmitters, receivers, and vacuum tubes.\n\nElectronics has branches as follows:\n\n\nAn electronic component is any physical entity in an electronic system used to affect the electrons or their associated fields in a manner consistent with the intended function of the electronic system. Components are generally intended to be connected together, usually by being soldered to a printed circuit board (PCB), to create an electronic circuit with a particular function (for example an amplifier, radio receiver, or oscillator). Components may be packaged singly, or in more complex groups as integrated circuits. Some common electronic components are capacitors, inductors, resistors, diodes, transistors, etc. Components are often categorized as active (e.g. transistors and thyristors) or passive (e.g. resistors, diodes, inductors and capacitors).\n\nVacuum tubes (Thermionic valves) were among the earliest electronic components. They were almost solely responsible for the electronics revolution of the first half of the twentieth century. They allowed for vastly more complicated systems and gave us radio, television, phonographs, radar, long-distance telephony and much more. They played a leading role in the field of microwave and high power transmission as well as television receivers until the middle of the 1980s. Since that time, solid-state devices have all but completely taken over. Vacuum tubes are still used in some specialist applications such as high power RF amplifiers, cathode ray tubes, specialist audio equipment, guitar amplifiers and some microwave devices.\n\nIn April 1955, the IBM 608 was the first IBM product to use transistor circuits without any vacuum tubes and is believed to be the first all-transistorized calculator to be manufactured for the commercial market. The 608 contained more than 3,000 germanium transistors. Thomas J. Watson Jr. ordered all future IBM products to use transistors in their design. From that time on transistors were almost exclusively used for computer logic and peripherals.\n\nCircuits and components can be divided into two groups: analog and digital. A particular device may consist of circuitry that has one or the other or a mix of the two types.\n\nMost analog electronic appliances, such as radio receivers, are constructed from combinations of a few types of basic circuits. Analog circuits use a continuous range of voltage or current as opposed to discrete levels as in digital circuits.\n\nThe number of different analog circuits so far devised is huge, especially because a 'circuit' can be defined as anything from a single component, to systems containing thousands of components.\n\nAnalog circuits are sometimes called linear circuits although many non-linear effects are used in analog circuits such as mixers, modulators, etc. Good examples of analog circuits include vacuum tube and transistor amplifiers, operational amplifiers and oscillators.\n\nOne rarely finds modern circuits that are entirely analog. These days analog circuitry may use digital or even microprocessor techniques to improve performance. This type of circuit is usually called \"mixed signal\" rather than analog or digital.\n\nSometimes it may be difficult to differentiate between analog and digital circuits as they have elements of both linear and non-linear operation. An example is the comparator which takes in a continuous range of voltage but only outputs one of two levels as in a digital circuit. Similarly, an overdriven transistor amplifier can take on the characteristics of a controlled switch having essentially two levels of output. In fact, many digital circuits are actually implemented as variations of analog circuits similar to this example – after all, all aspects of the real physical world are essentially analog, so digital effects are only realized by constraining analog behavior.\n\nDigital circuits are electric circuits based on a number of discrete voltage levels. Digital circuits are the most common physical representation of Boolean algebra, and are the basis of all digital computers. To most engineers, the terms \"digital circuit\", \"digital system\" and \"logic\" are interchangeable in the context of digital circuits.\nMost digital circuits use a binary system with two voltage levels labeled \"0\" and \"1\". Often logic \"0\" will be a lower voltage and referred to as \"Low\" while logic \"1\" is referred to as \"High\". However, some systems use the reverse definition (\"0\" is \"High\") or are current based. Quite often the logic designer may reverse these definitions from one circuit to the next as he sees fit to facilitate his design. The definition of the levels as \"0\" or \"1\" is arbitrary.\n\nTernary (with three states) logic has been studied, and some prototype computers made.\n\nComputers, electronic clocks, and programmable logic controllers (used to control industrial processes) are constructed of digital circuits. Digital signal processors are another example.\n\nBuilding blocks:\n\nHighly integrated devices:\n\nHeat generated by electronic circuitry must be dissipated to prevent immediate failure and improve long term reliability. Heat dissipation is mostly achieved by passive conduction/convection. Means to achieve greater dissipation include heat sinks and fans for air cooling, and other forms of computer cooling such as water cooling. These techniques use convection, conduction, and radiation of heat energy.\n\nElectronic noise is defined as unwanted disturbances superposed on a useful signal that tend to obscure its information content. Noise is not the same as signal distortion caused by a circuit. Noise is associated with all electronic circuits. Noise may be electromagnetically or thermally generated, which can be decreased by lowering the operating temperature of the circuit. Other types of noise, such as shot noise cannot be removed as they are due to limitations in physical properties.\n\nMathematical methods are integral to the study of electronics. To become proficient in electronics it is also necessary to become proficient in the mathematics of circuit analysis.\n\nCircuit analysis is the study of methods of solving generally linear systems for unknown variables such as the voltage at a certain node or the current through a certain branch of a network. A common analytical tool for this is the SPICE circuit simulator.\n\nAlso important to electronics is the study and understanding of electromagnetic field theory.\n\nDue to the complex nature of electronics theory, laboratory experimentation is an important part of the development of electronic devices. These experiments are used to test or verify the engineer’s design and detect errors. Historically, electronics labs have consisted of electronics devices and equipment located in a physical space, although in more recent years the trend has been towards electronics lab simulation software, such as CircuitLogix, Multisim, and PSpice.\n\nToday's electronics engineers have the ability to design circuits using premanufactured building blocks such as power supplies, semiconductors (i.e. semiconductor devices, such as transistors), and integrated circuits. Electronic design automation software programs include schematic capture programs and printed circuit board design programs. Popular names in the EDA software world are NI Multisim, Cadence (ORCAD), EAGLE PCB and Schematic, Mentor (PADS PCB and LOGIC Schematic), Altium (Protel), LabCentre Electronics (Proteus), gEDA, KiCad and many others.\n\nMany different methods of connecting components have been used over the years. For instance, early electronics often used point to point wiring with components attached to wooden breadboards to construct circuits. Cordwood construction and wire wrap were other methods used. Most modern day electronics now use printed circuit boards made of materials such as FR4, or the cheaper (and less hard-wearing) Synthetic Resin Bonded Paper (SRBP, also known as Paxoline/Paxolin (trade marks) and FR2) – characterised by its brown colour. Health and environmental concerns associated with electronics assembly have gained increased attention in recent years, especially for products destined to the European Union, with its Restriction of Hazardous Substances Directive (RoHS) and Waste Electrical and Electronic Equipment Directive (WEEE), which went into force in July 2006.\n\nElectronic systems design deals with the multi-disciplinary design issues of complex electronic devices, such as mobile phones and computers. The subject covers a broad spectrum, from the design and development of an electronic system (new product development) to assuring its proper function, service life and disposal. Electronic systems design is therefore the process of defining and developing complex electronic devices to satisfy specified requirements of the user.\n\n\n"}
{"id": "1515736", "url": "https://en.wikipedia.org/wiki?curid=1515736", "title": "Eudiometer", "text": "Eudiometer\n\nA eudiometer is a laboratory device that measures the change in volume of a gas mixture following a physical or chemical change.\n\nDepending on the reaction being measured, the device can take a variety of forms. In general, it is similar to a graduated cylinder, and is most commonly found in two sizes: 50 mL and 100 mL. It is closed at the top end with the bottom end immersed in water or mercury. The liquid traps a sample of gas in the cylinder, and the graduation allows the volume of the gas to be measured. For some reactions, two platinum wires (chosen for their non-reactivity) are placed in the sealed end so an electric spark can be created between them. The electric spark can initiate a reaction in the gas mixture and the graduation on the cylinder can be read to determine the change in volume resulting from the reaction. The use of the device is quite similar to the original barometer, except that the gas inside displaces some of the liquid that is used.\n\nIn 1772, Joseph Priestley began experimenting with different “airs” using his own redesigned pneumatic trough in which mercury instead of water would trap gases that were usually soluble in water. From these experiments Priestley is credited with discovering many new gases such as oxygen, hydrogen chloride, and ammonia. He also discovered a way to find the purity or “goodness” of air using “nitrous air test”. This test was conducted by mixing nitrous gas with a test sample of another gas and trapping it in the pneumatic trough, essentially the greater the decrease in volume, the more pure the sample of gas was (key principle in eudiometry). Henry Cavendish later used a similar device to determine the fraction of oxygen in the Earth's atmosphere.\n\nIt is from these experiments that Marsilio Landriani became inspired to create a more useful tool in measuring the “healthiness” of air. In 1775, Landriani invented the first eudiometer and used it to conduct his own experiments. From these experiments Landriani theorized about the temperament of certain gases and the atmosphere and later published a paper called \"Ricerche fisiche intorno alla salubrità dell'aria\" (Physical researches on the salubrity of air). Though many of his findings were incorrect, his invention was the building block for the modern day eudiometer.\n\nAlthough the eudiometer's invention is usually credited to Marsilio Landriani, it was truly pioneered by Count Alessandro Volta (1745–1827), an Italian physicist who is well known for his contributions to the electric battery and electricity. Aside from its laboratory function, the eudiometer is also known for its part in the \"Volta pistol\". Volta invented this instrument in 1777 for the purpose of testing the \"goodness\" of air, analyzing the flammability of gases, or to demonstrate the chemical effects of electricity. Volta's Pistol had a long glass tube that was closed at the top, like a eudiometer. Two electrodes were fed through the tube and produced a spark gap inside the tube. Volta's initial use of this instrument concerned the study of swamp gases in particular. Volta's pistol was filled with oxygen and another gas. The homogeneous mixture was taped shut with a cork. A spark could be introduced into the gas chamber by electrodes, and possibly catalyze a reaction by static electricity, using Volta's electrophorus. If the gases were flammable, they would explode, and increase the pressure within the gas chamber. This pressure would be too great and eventually cause the cork to become airborne. Volta's pistol was made with either glass or brass, however due to the electricity the glass was vulnerable to exploding. Volta's extensive studies on measuring and creating high levels of electric currents caused the electrical unit, the volt, to be named after him.\n\nThe name \"eudiometer\" comes from the Greek \"eúdios\" meaning clear or mild, which is the combination of the prefix \"eu-\" meaning \"good\", and \"-dios\" meaning \"heavenly\" or \"of Zeus\" (the god of the sky and atmosphere), with the suffix \"-meter\" meaning \"measure\". Because the eudiometer was originally used to measure the amount of oxygen in the air, which was thought to be greater in \"nice\" weather, the root \"eudio-\" appropriately describes the apparatus.\n\nApplications of a eudiometer include the analysis of gases and the determination of volume differences in chemical reactions. The eudiometer is filled with water, inverted so that its open end is facing the ground (while holding the open end so that no water escapes), and then submersed in a basin of water. A chemical reaction is taking place through which gas is created. One reactant is typically at the bottom of the eudiometer (which flows downward when the eudiometer is inverted) and the other reactant is suspended on the rim of the eudiometer, typically by means of a platinum or copper wire (due to their low reactivity). When the gas created by the chemical reaction is released, it should rise into the eudiometer so that the experimenter may accurately read the volume of the gas produced at any given time. Normally a person would read the volume when the reaction is completed. This procedure is followed in many experiments, including an experiment in which one experimentally determines the Ideal gas law constant R.\n\nThe eudiometer is similar in structure to the meteorological barometer. Similarly, a eudiometer uses water to release gas into the eudiometer tube, converting the gas into a visible, measurable amount. A correct measurement of the pressure when performing these experiments is crucial for the calculations involved in the \"PV\"=\"nRT\" equation, because the pressure could change the density of the gas.\n\n\n"}
{"id": "5340627", "url": "https://en.wikipedia.org/wiki?curid=5340627", "title": "Fougasse (weapon)", "text": "Fougasse (weapon)\n\nA fougasse is an improvised mortar constructed by making a hollow in the ground or rock and filling it with explosives (originally, black powder) and projectiles. The fougasse was used by Samuel Zimmermann at Augsburg in the sixteenth century, referred to by Vauban in the seventeenth century, and well known to military engineers by the mid-eighteenth century. This technique was used in several European wars, the American Revolution, and the American Civil War. The term is still used to describe such devices.\n\nThe normal method of firing was to use a burning torch or slow match to ignite a \"saucisson\" (French for \"sausage\", a cloth or leather tube waterproofed with pitch and filled with black powder) leading to the main charge. This had numerous disadvantages; the firer was obvious to the attacking enemy, and had to race to get clear after lighting the fuse. The black powder was also very susceptible to moisture, and might not work at all. In 1573 Samuel Zimmermann devised an improved method which incorporated a snaphance (or later, flintlock mechanism) into the charge and connected its trigger to the surface with a wire. This was more resistant to moisture, better concealed, and enabled the firer to be further away. It also enabled the fougasse to be tripwire activated, turning it into an anti-personnel fragmentation mine.\n\nIn a letter to his sister, Colonel Hugh Robert Hibbert described such a weapon employed during the Crimean War:\n\nThere are several variants according to the material projected by the explosion.\n\nThe most common type in early use was the stone fougasse, which was simply filled with large rocks, bricks, etc. When fired, it would scatter a hail of fast-moving stones across the area to its front. Large stone fougasses might hold several tons of rubble and as much as a hundredweight of powder.\n\nThe shell fougasse was loaded with early black powder mortar shells (essentially a large version of an early black powder hand grenade) or incendiary \"carcasses\". When fired, the powder charge would throw out the shells and ignite their fuses, so the projectiles would be scattered across the target area and then begin exploding, filling the area with fragmentation or flame from all directions in an effect similar to a cluster bomb.\n\nA flame fougasse was a similar weapon in which the projectile was a flammable liquid, typically a mixture of petrol (gasoline) and oil. The flame fougasse was developed by the British Petroleum Warfare Department in response to the threat of German invasion during World War II.\n\nIn Britain, during WWII, the flame fougasse was usually constructed from a 40-gallon drum dug into the roadside and camouflaged. It would be placed at a location such as a corner, steep incline or roadblock where vehicles would be obliged to slow down. Ammonal provided the propellant charge which, when triggered, caused the weapon to shoot a flame wide and long. Initially a mixture of 40% petrol and 60% gas oil was used; this was later replaced by an adhesive gel of tar, lime and petrol known as 5B.\n\nThe November 1944 issue of the US War Department Intelligence Bulletin refers to 'Fougasse flame throwers' used in the Russian defence at Stalingrad being the basis of a German version found in Italy that were buried with a fixed direction discharge tube and integrated with conventional landmines and barbed wire in defense works. The German weapon, the Abwehrflammenwerfer 42 had an 8-gallon fuel tank and the seven in the installation were wired back to a control point and could be fired individually or together.\n\nThe flame fougasse has remained in army field manuals as a battlefield expedient. Such expedients are constructed from available fuel containers combined with standard explosive charges or hand grenades triggered electrically or by lengths of detonating cord. Some designs use lengths of detonating cord or blasting caps to rupture the fuel container as well as detonate the main charge. Weapons of this sort were widely used in the Korean and Vietnam wars as well as other conflicts.\n\n\n\n"}
{"id": "16376341", "url": "https://en.wikipedia.org/wiki?curid=16376341", "title": "Gas diffusion electrode", "text": "Gas diffusion electrode\n\nGas diffusion electrodes (GDE) are electrodes with a conjunction of a solid, liquid and gaseous interface, and an electrical conducting catalyst supporting an electrochemical reaction between the liquid and the gaseous phase.\n\nGDEs are used in fuel cells, where oxygen and hydrogen react at the gas diffusion electrodes, to form water, while converting the chemical bond energy into electrical energy. Usually the catalyst is fixed in a porous foil, so that the liquid and the gas can interact. Besides these wetting characteristics, the gas diffusion electrode must, of course, offer an optimal electric conductivity, in order to enable an electron transport with low ohmic resistance.\n\nAn important prerequisite for the operation of gas diffusion electrodes is that both the liquid and the gaseous phase coexist in the pore system of the electrodes which can be demonstrated with the Young-Laplace equation:\n\nThe gas pressure p is in relation with the liquid in the pore system over the pore radius r, the surface tension γ of the liquid and the contact angle Θ. This equation is to be taken as a guide for determination because there are too many unknown, or difficult to achieve, parameters. When the surface tension is considered, the difference in surface tension of the solid and the liquid have to be taken into account. But the surface tension of catalysts such as platinum on carbon or silver are hardly measurable. The contact angle on a flat surface can be determined with a microscope. A single pore, however, cannot be examined so it is necessary to determine the pore system of an entire electrode. Thus in order to create an electrode area for liquid and gas, the path can be chosen to create different pore radius r, or to create different wetting angles Θ.\n\nIn this image of a sintered electrode it can be seen that three different grain sizes were used. The different layers were:\nMost of the electrodes that were manufactured from 1950 to 1970 with the sintered method were for use in fuel cells. This type of production was dropped for economic reasons because the electrodes were thick and heavy, with a common thickness of 2 mm, while the individual layers had to be very thin and without defects. The sales price was too high and the electrodes could not be produced continuously.\n\nThe principle of gas diffusion is illustrated in this diagram. The so-called gas distribution layer is located in the middle of the electrode. With only a small gas pressure, the electrolyte is displaced from this pore system. A small flow resistance ensures that the gas can freely flow inside the electrode. At a slightly higher gas pressure the electrolyte in the pore system is restricted to the work layer. The surface layer itself has such fine pores that, even when the pressure peaks, gas cannot flow through the electrode into the electrolyte. Such electrodes were produced by scattering and subsequent sintering or hot pressing. To produce multi-layered electrodes a fine-grained material was scattered in a mold and smoothed. Then, the other materials were applied in multiple layers and put under pressure. The production was not only error-prone but also time consuming and difficult to automate.\n\nSince about 1970, PTFEs are used to produce an electrode having both hydrophilic and hydrophobic properties while chemically stable and which can be used as binders. This means that, in places with a high proportion of PTFE, no electrolyte can penetrate the pore system and vice versa. In that case the catalyst itself should be non-hydrophobic.\n\nThere are two technical variations to produce PTFE catalyst-mixtures:\n\nThe dispersion route is chosen mainly for electrodes with polymer electrolytes, as successfully introduced in the PEM fuel cell and in PEM or HCL membrane electrolysis. When used in liquid electrolyte, a dry process is more appropriate. \n\nAlso, in the dispersion route (through evaporation of water and sintering of the PTFEs at 340°C) the mechanical pressing is skipped and the produced electrodes are very porous. With fast drying methods, cracks can form in the electrodes which can be penetrated by the liquid electrolyte. For applications with liquid electrolytes, such as the zinc-air battery or the alkaline fuel cell, the dry mixture method is used.\n\nLast, but not least, the right choice of catalyst is important too. In acidic electrolytes the catalysts are usually precious metals like platinum, ruthenium, iridium and rhodium. In alkaline electrolytes, like zinc-air batteries and alkaline fuel cells, it is usual to use less expensive catalysts like carbon, manganese, silver, nickel foam or nickel mesh.\n\nAt first solid electrodes were used in the Grove cell, Francis Thomas Bacon was the first to use gas diffusion electrodes for the Bacon fuel cell, converting hydrogen and oxygen at high temperature into electricity. Over the years, gas diffusion electrodes have been adapted for various other processes like: \n\n\nIn recent years the use of gas diffusion electrodes for electrochemical carbon dioxide reduction is a strongly growing research topic.\n\nGDE is produced at all levels. It is not only used for research and development firms but for larger companies as well in the production of a Membrane Electrode Assembly (MEA) that is in most cases used in a fuel cell or battery apparatus. Companies that specialize in high volume production of GDE include Johnson Matthey, Gore and Gaskatel. However, there are many companies which produce custom or low quantity GDE, allowing different shapes, catalysts and loadings to be evaluated as well, which include FuelCellStore, FuelCellsEtc, and many others.\n\n"}
{"id": "1794502", "url": "https://en.wikipedia.org/wiki?curid=1794502", "title": "Hair rig", "text": "Hair rig\n\nThe hair rig is a piece of fishing tackle which allows a bait to be presented without sitting directly on the hook. It is mainly associated with boilies, but also works effectively with many other baits. The hair rig became popular in the 1970s. It has been experimented with by English anglers, and has revolutionised carp fishing.\n\nAt the beginning, natural wire (from a ponytail) was used to link the bait to the hook. This is why it was called the 'hair rig'. This material was very discreet but it was a little fragile. The bait also didn't act in a sufficiently natural manner. In tests, the carp only took the free offerings and left the hook bait. It was also a problem because anglers sometimes lost their baits while they were casting. These problems have been solved with the appearance of braided thread, a new material that allows threads to be as discreet as natural ones but more resistant.\n\nKnotless knot\n\n"}
{"id": "7341793", "url": "https://en.wikipedia.org/wiki?curid=7341793", "title": "Hearst Transcontinental Prize", "text": "Hearst Transcontinental Prize\n\nThe Hearst prize was a $50,000 (approximately $ today) aviation prize offered by publisher William Randolph Hearst in 1910 to the first aviator to fly coast to coast across the United States, in either direction, in fewer than 30 days from start to finish. The prize expired in November 1911 without a winner.\n\n"}
{"id": "8155923", "url": "https://en.wikipedia.org/wiki?curid=8155923", "title": "IMETS", "text": "IMETS\n\nDeveloped by Northrop Grumman, the Integrated Meteorological System (AN/TMQ-40 IMETS) is the meteorological component of the Intelligence and Electronic Warfare (IEW) an element of the Army Battle Command System (ABCS). IMETS provides Army commanders at all echelons with an automated weather system to receive, process, and disseminate weather observations, forecasts, and weather and environmental effects decision aids to all Battlefield Operating Systems (BOS). IMETS is a mobile, tactical, automated weather data receiving, processing and dissemination system. The IMETS is an Army-furnished and maintained system operated by US Air Force battlefield weather team personnel. It uses US Air Force and Army developed software to provide a total weather system to support the Army. The Integrated Weather Effects Decision Aid (IWEDA) software, originally developed by the U.S. Army Research Laboratory (ARL) in 1992, has been fielded on IMETS since 1997 to provide tactical weather support to the U.S. Army.\n\nIMETS is a heavy Humvee mounted tactical system which provides automation and communications support to staff weather teams assigned to echelons from brigade through Echelons Above Corps (EAC). IMETS receives weather information from polar-orbiting civilian and defense meteorological satellites, Air Force Global Weather Center, artillery meteorological teams, remote sensors and civilian forecast centers. IMETS processes and collates forecasts, observations, and climatological data to produce timely and accurate weather products tailored to the specific Warfighter’s needs.\n\nIMETS provides automation and communications support to USAF Weather Teams assigned to Army G2/G3 sections at echelons Brigade through EAC. IMETS receives, processes, and collates forecasts, observations, and climatological data to produce weather forecasts and timely and accurate products to meet Commanders' requirements. IMETS produces displays and disseminates, over Army ABCS, weather forecasts and tactical decision aids that compare the impact of current, projected, or hypothesized weather conditions on friendly and enemy capabilities.\n\nThe IMETS currently fielded is in two configurations. They are the vehicle-mounted configuration (VMC), IMETS-Heavy, and the laptop version, the IMETS-Light. The IMETS-Light is the most common version; US Army uses IMETS-Light for aviation brigades and brigade combat teams. Both configurations have identical intelligence processing capabilities. The IMETS-Light has recently passed the Milestone C review and its production and fielding have official authorization. Both configurations of IMETS operate with ABCS Version 6.X-complaint software.\n\nIMETS training is accomplished at Staff Weather Officer (SWO) Course at Fort Huachuca, Arizona. SWO training is four weeks long, IMETS portion being half of that. This is somewhat ironic, as IMETS has not been used much since the wars in Afghanistan and Iraq began.\n\nThe following units use IMETS:\n\nAs of 2018, My Weather Impacts Decision Aid (MyWIDA), a decision-support software, was developed by the U.S. Army Research Laboratory (ARL). Designed to improve compatibility of technology with environmental factors through weather forecasting, MyWIDA is an updated version of the Integrated Weather Effects Decision Aid (IWEDA), which the Army fielded in 1995.\n\nMyWIDA uses weather forecast data to evaluate environmental impacts on military technology, aiding decision-makers in selecting appropriate technological tools under forecasted weather events. An example is surface winds greater than 25 knots, which prohibit launching of unmanned aerial vehicles (UAVs) in a military scenario. The user would be alerted of this physical limitation through the MyWIDA software.\n\nThe system includes red-amber-green ratings (unfavorable-marginal-favorable), which account for a combination of weather parameters that affect a system or technology. These ratings determine limits beyond which it is not feasible for the technology function due to safety considerations, decreased system effectiveness or violation of manufacturer's operating limits.\n\nMyWida was preceded by Integrated Weather Effects Decision Aid (IWEDA), an automated software decision aid developed by ARL in 1992. \n\nIWEDA was fielded on the Integrated Meteorological System (IMETS) in 1997 to provide tactical weather support to the U.S. IWEDA software was certified and accredited for the Army in 2006. As of 2011, the web-based MYWIDA was under development for eventual replacement of IWEDA.\n\nIWEDA was designed to address the adverse effects of the environment and climate (i.e. wind, precipitation, storms and temperature) on military operations and weapon systems. IWEDA produced a graphic display of weather impacts on 70 weapon systems, including 16 threat systems. Impacts were displayed graphically on a user interface, called the Weather Effects Matrix (WEM), which color coded the impacts on the system of interest (i.e. green “favorable,” amber “marginal,” and red “unfavorable”). Although intended for the Army, applications were also integrated into the Air Force and Navy systems.\n\nOver the years, observations have identified varying needs for IWEDA’s improvement. These included (1) a need to derive complete mission impact due to poor weather conditions, rather than simply presenting a “worst-case scenario” for specific weapon systems; and (2) a need to improve representation of IWEDA’s “stoplight” color scheme (green, yellow and red) by providing more color-coded values.\n\nThis article contains some information that originally came from GlobalSecurity.org, in the public domain from http://www.globalsecurity.org/space/systems/imets.htm\n\nThis article contains some information that originally came from Military Intelligence Bulletin, in the public domain from http://www.findarticles.com/p/articles/mi_m0IBS/is_4_28/ai_94538577\n"}
{"id": "17545148", "url": "https://en.wikipedia.org/wiki?curid=17545148", "title": "Instruments used in general medicine", "text": "Instruments used in general medicine\n"}
{"id": "54075994", "url": "https://en.wikipedia.org/wiki?curid=54075994", "title": "LandlordInvest", "text": "LandlordInvest\n\nLandlordInvest is a peer-to-peer lending platform which enables people to invest in residential buy to let mortgages and bridging loans. The platform’s target audience is buy-to-let and bridging loan borrowers with a near perfect credit score, that are having difficulties with raising finance from traditional lenders due to a one off adverse credit event in the last five years. It was the first residential property-backed Innovative Finance ISA made available to UK savers.\n\nLandlordInvest was founded in 2014. The platform was officially launched in December 2016 when it received authorisation by the FCA. In early 2017 LandlordInvest received an ISA manager approval from HMRC, ahead of similar platforms such as Funding Circle, Zopa, and Ratesetter, becoming one of a few companies to offer tax-free returns to peer-to-peer investors. In May 2017 the platform launched a secondary market which allows investors to sell off their property loan parts. Property Wire Awards listed LandlordInvest as one of three crowdfunding platforms of the year 2017, alongside The House Crowd and Homegrown. In July 2017 LandlordInvest received an undisclosed sum of seed funding from Alan Gabbay, director O&H Properties which currently has a £1bn property portfolio in London. Later in the same month the platform passed the £1m lending milestone and amassed 700 investors since it launched in December 2016. By the end of July LandlordInvest completed its largest loan to date, a £740,741 bridging loan, bringing its cumulative lending to £2m. LandlordInvest published its loan book for the first time on 5 December 2017, marking its one-year anniversary. As of June 2018, the P2P platform has lent £5,209,745 across 25 bridging and buy-to-let loans.\n\nLandlordInvest is an online marketplace which matches landlords looking for financing with investors. The platform provides secured borrowing to landlords seeking between £30,000 and £750,000. The minimum investment amount is £100 and the platform’s bridging loans carry a maximum term of 18 months, while the buy-to-let loans have terms of up to five years. Investors lend money to eligible borrowers and receive money from the interest charged on loans, while LandlordInvest covers the costs of running the platform by charging borrowers an arrangement fee on each funded loan and by deducting a servicing fee from monthly interest payments to investors.\n\nThe platform is holding out the prospect of returns of up to 12% per annum. In its first year of operation, LandlordInvest facilitated more than £2.7m of lending through its platform, with an average loan amount of £210,535, an average LTV of 63.7%, and an average annual gross return to investors of 11.1%.\n\nIn March 2018 the platform's secondary market reached £1m of sales, with a total of 1,393 loan sales completed and an average transaction value of £730.\n\nLandlordInvest reached over £2m in subscriptions for its Innovative Finance ISA after having raised £1.65m in subscriptions in the 2017-18 tax year, an increase of 292.5% compared to the £419,835 of subscriptions made in the previous tax year. LandlordInvest's Innovative Finance ISA offers returns of up to 12.3% per annum. \n\n"}
{"id": "44543594", "url": "https://en.wikipedia.org/wiki?curid=44543594", "title": "List of AR platform calibers", "text": "List of AR platform calibers\n\nThe AR platform has become widely popular for makers of military and sporting rifles. Although the designations \"AR-10\" and \"AR-15\" are trademarks of Colt's Manufacturing Company, variants of both are made by many manufacturers.\n\n\n\n\n\n450 marlin \n\nSome companies have created AR pattern rifles that depart from the standard AR-15 and AR-10 dimensions in order to accommodate other types of ammunition that would not fit into the those standards.\n\nExamples include:\n\nA variety of manufacturers have introduced semiautomatic shotguns whose overall designs are heavily influenced by the AR pattern rifle.\n"}
{"id": "2542716", "url": "https://en.wikipedia.org/wiki?curid=2542716", "title": "List of robotic dogs", "text": "List of robotic dogs\n\nRobotic dogs are robots designed to resemble dogs in appearance and behavior, usually incorporating canine characteristics such as barking or tail-wagging. In addition, many such \"dogs\" have appeared as toys and in fiction.\n\n\n\"Joinmax Digital Robot Dog JM-DOG-001]\", offered as a semi-assembled kit (no soldering required) at $331, it offers a 15 servo-based impressive freedom of motion. Control is possible through a serial connection to the included controller board, or through simple commands sequences stored in memory.\n\n\n\n\n"}
{"id": "24200863", "url": "https://en.wikipedia.org/wiki?curid=24200863", "title": "Local information systems", "text": "Local information systems\n\nA Local information system (LIS) is a form of information system built with business intelligence tools, designed primarily to support geographic reporting. They overlap with some capabilities of geographic information systems (GIS), although their primary function is the reporting of statistical data rather than the analysis of geospatial data. LIS also tend to offer some common knowledge management functionality for storage and retrieval of unstructured data such as documents. They deliver functionality to load, store, analyse and present statistical data that has a strong geographic reference. In most cases the data is structured as indicators and is linked to discrete geographic areas, for example population figures for US counties or numbers claiming unemployment benefit across wards in England. The ability to present this data using data visualization tools like charts and maps is also a core feature of these systems.\n\nThe term \"LIS\" has emerged since 2004, primarily in the UK public sector. To date it is not widely used elsewhere although other terms like Community Information Systems apply to solutions, primarily in North America, that have a great deal of overlap. Another widely used and largely synonymous term is Data Observatory. Data Observatory is a more widely used term internationally particularly within the area of public health where sites which often include this type of statistical reporting application are often termed a health observatory.\n\nThe primary application for LIS is to provide a place-focused evidence base that is easily accessible to a wide range of users including data experts, managers, policy makers, front-line staff and citizens. They provide a wide range of statistics and reports allowing users to review the current evidence base and build a picture of localities and neighbourhoods for their area of interest. LIS are commonly used by partnerships where they need to come together to provide joined-up services for a common area. The ability to have a common evidence base and a platform to share sensitive and non-sensitive data is critical in this situation. LIS enables partners to publish a wide range of indicators in the form of defined outputs which combine locally and nationally available data into more meaningful intelligence aimed at specific user groups\n\nIn the UK, like many other countries, there has been a rapid growth in the availability of small area statistics. National Neighbourhood Statistics projects across the UK, set up as a result of the PAT 18 report, have opened up access to a wide range of government small area based statistics. This has been accompanied by a gradual shift across the public sector, a shift that remains very much on-going, towards the recognition that policy and decisions should be influenced to a greater degree by evidence. There also continues to be a growing acceptance that some services can be more effectively delivered by targeting resources at specific areas of need − the idea of high demand 'hotspots'. This relies on having reliable and detailed data about the needs of customers, in this case citizens, and where they live, work and take leisure time. \n\nIn England in particular this has led to a rapid increase in the number of Local Information Systems particularly within local Authorities and Local Strategic Partnerships. This development has been actively supported by the Department of Communities and Local Government (CLG) under their ‘Neighbourhood Renewal’ agenda. A national research project was funded to identify examples and disseminate best practice – this reported publicly in 2004 and led to a more formal report being published in 2006. CLG’s role as a catalyst in this area is further re-enforced through its provision of Neighbourhood Renewal Funds (NRF) - this funding was used by a number of authorities to pump-prime their initial LIS developments.\n\nAn initiative is currently on-going through the CLG Information Management Programme to coordinate all LIS activity across local government and partnerships. This has led to regular national LIS meetings and a dedicated LIS forum. To date it is estimated that approximately 50 per cent of top tier authorities in England now have some form of LIS. In some cases these have been built as bespoke solutions, in other cases they are based on off-the-shelf products. Elsewhere within the UK this figure is lower although an initiative has been launched in Scotland in 2009 resulting in a Scottish LIS Toolkit to complement the English version.\n\nThe range of data managed within a LIS can be wide and classified in many different ways. Most common is some form of domain specific classification where indicators are grouped into top level categories like ‘Demography’, ‘Health and Welfare’, ‘Crime and Community Safety’, ‘Education and Children's Services’, ‘Environment’ and ‘Economy’. There may also be cross-cutting themes such as ‘Performance’ and ‘Social Disadvantage’. In the UK key government data sources include ONS Neighbourhood Statistics, CLG, Dept for Work and Pensions, NOMIS, Audit Commission and several areas of NHS information services. However the real value of LIS is their ability to combine national data with local data available from a wide range of internal business systems including those of partners. This local data is often not provided to central government and, even when it is, it tends to be in a form that limits its value.\n\n\n"}
{"id": "34403294", "url": "https://en.wikipedia.org/wiki?curid=34403294", "title": "Matthew Buckland", "text": "Matthew Buckland\n\nMatthew Buckland is a South African Internet entrepreneur and businessman who founded and exited digital agency and publisher Creative Spark, acquired in 2015 by UK firm \"M&C Saatchi PLC\" (), the holding group of M&C Saatchi. Buckland is also the founder of Burn Media, a suite of technology publishing brands which includes Memeburn, Ventureburn.com, Gearburn.com and others.\n\nBuckland previously headed the online division of the \"Mail & Guardian\", thereafter he started 20fourlabs at Naspers' news24.com, the largest South African online news publisher, owned by Naspers. While at the \"Mail & Guardian\" Buckland founded Thought Leader.\n\nIn 2015 he was selected as a \"Master of Digital\", sitting down with actor Idris Elba for a \"Q&A Session\".\n\nBuckland lives in Cape Town with his wife and two daughters.\n\nBuckland studied journalism at Rhodes University in the Eastern Cape, South Africa. He worked in London for the BBC's then-commercial web arm, beeb.com as a developer, and then a web development producer. He then later worked as the Internet editor for prime-time TV show, Carte Blanche, before becoming Managing Director of Mail & Guardian's online division. He is the eldest son of Andrew Buckland and Janet Buckland, a prominent South African acting family.\n\nIn 2010 Buckland founded Creative Spark and Burn Media, which he funded himself after being unable to raise venture capital for it. Five years later, in 2015, he sold a majority share in his company to the FTSE-listed agency, M&C Saatchi. The deal included the sale of both the digital agency and the publishing arm, which owns titles such as Memeburn and Ventureburn. He later retook control of the publishing arm when exiting his former company in 2018. Ventureburn is a site that reports on the startup, entrepreneur and investor ecosystems in South Africa, Kenya and Nigeria.\n\nHe was the Master of Ceremonies at the Silicon Cape Initiative launch event and was elected to its inaugural board. The Silicon Cape Initiative is a South African-based organization which aims to turn the Western Cape into a high-tech startup hub. Buckland credits this organisation for inspiring him to move out of the corporate world into entrepreneurship.\n\nIn 2012 Buckland was recognised in the Power of 40: 40 Entrepreneurs Under 40 \"Doing Interesting Things\"\n\nIn 2013 and 2014 he was a member of the Cape Town chapter of Entrepreneurs Organisation.\n\nBuckland’s industry accolades include:\n\n"}
{"id": "16454056", "url": "https://en.wikipedia.org/wiki?curid=16454056", "title": "Net explosive quantity", "text": "Net explosive quantity\n\nThe net explosive quantity (NEQ), also known as net explosive content (NEC) or net explosive weight (NEW), of a shipment of munitions, fireworks or similar products is the total mass of the contained explosive substances, without the packaging, casings, bullets etc. It also includes the mass of the TNT-equivalent of all contained energetic substances.\n\nThe NEQ is often stated on shipment containers for safety purposes.\n"}
{"id": "58585652", "url": "https://en.wikipedia.org/wiki?curid=58585652", "title": "OneClass", "text": "OneClass\n\nOneClass is an online educational technology platform where university students share notes and access resources.\n\nIn September 2010, the precursor to OneClass, called \"Notesolution\", was founded by four undergraduate students from the University of Toronto as a way to provide students with additional learning resources in the form of course notes. \n\nOne year later in October 2011, Notesolution acquired StudyMonkey, a competitor with a similar note-sharing platform.\n\nNotesolution raised $400,000 in 2012 and participated in FounderFuel’s Accelerator Program in Montreal. In 2013 OneClass raised $1.6M from a Series A led by SAIF Partners, Real Ventures, and several other angel investors totaling $2.3M in funding over three rounds.\n\nIn 2018, OneClass co-founders were named in Forbes 30 Under 30 list for leaders in education.\n\nAs of 2018, OneClass has over 11 million pages of study documents, and has been used by more than 2.2 million students in the United States, Canada, Australia, and New Zealand.\n\nOneClass uses a freemium model where users can contribute learning material by uploading notes and study documents in exchange for site currency, or “credits.” Users can use credits to unlock other documents on the site for viewing or downloading, or use the credits to purchase gift cards. Alternatively, users can choose to pay for a subscription. Additionally, OneClass offers an app where users can ask questions and get help from other students and tutors.\n\nOpinions are mixed about the concept of students sharing study content such as lecture notes and exam study guides. Some students, such as University of Saskatchewan Students' Union VP academic Kelsey Topola, say that notes are students' intellectual property, and they can share them. There are others such as Professor Camille Hernandez-Ramdwar of Ryerson University who points out that students who post their notes may not have the correct interpretation of the material, which can result in a problem for other students.\n\nIn November and December of 2012, Red Bull and OneClass (Notesolution) provided Canadian students with Exam Survival Kits that included more than 20,000 cans of Red Bull. \n\n"}
{"id": "23398732", "url": "https://en.wikipedia.org/wiki?curid=23398732", "title": "PC power management", "text": "PC power management\n\nPC power management refers to the mechanism for controlling the power use of personal computer hardware. This is typically through the use of software that puts the hardware into the lowest power demand state available. It is an aspect of Green computing.\n\nA typical office PC might use on the order of 90 watts when active (approximately 50 watts for the base unit, and 40 watts for a typical LCD screen); and three to four watts when ‘asleep’. Up to 10% of a modern office’s electricity demand might be due to PCs and monitors.\n\nWhile some PCs allow low power settings, there are many situations, especially in a networked environment, where processes running on the computer will prevent the low power settings from taking effect. This can have a dramatic effect on energy use that is invisible to the user. The monitor may have gone into standby mode, and the PC may appear to be idle, but operational testing has shown that on any given day an average of over 50% of an organisation's computers would fail to go to sleep, and over time this happened to over 90% of the machines.\n\nThe Windows power management system is based upon an idle timer. If the computer is idle for longer than the preset timeout then the PC may be configured to sleep or hibernate. The user may configure the timeout using the Control Panel. Windows uses a combination of user activity and CPU activity to determine when the computer is idle.\n\nApplications can temporarily inhibit this timer by using the \"SetThreadExecutionState\" API. There are legitimate reasons why this may be necessary such as burning a DVD or playing a video. However, in many cases applications can unnecessarily prevent power management from working. This is commonly known as Windows 'Insomnia' and can be a significant barrier to successfully implementing power management.\n\nCommon causes of 'insomnia' include:\n\n\nOperating systems have built-in settings to control power use. Microsoft Windows supports predefined power plans and custom sleep and hibernation settings through a Control Panel Power Options applet. Apple's OS X includes idle and sleep configuration settings through the Energy Saver System Preferences applet. Likewise, Linux distributions include a variety of power management settings and tools.\n\nThere is a significant market in third-party PC power management software offering features beyond those present in the Windows operating system. Products are targeted at enterprise environments offering Active Directory integration and per-user/per-machine settings with the more advanced offering multiple power plans, scheduled power plans, anti-insomnia features and enterprise power usage reporting. Notable vendors include 1E NightWatchman, Data Synergy PowerMAN (Software), Faronics Power Save, Verdiem SURVEYOR. and EnviProt Auto Shutdown Manager\n\nUsing this type of energy management tool on an organisation's network has been demonstrated to save on average 200 kg of CO emissions per PC per year, and generate $36 per PC per year in energy savings. An organisation such as a large office, with 4,000 employees, would be able to make CO emissions savings of over 800 tonnes and about $140,000 per year in energy savings. A large corporation with 50,000 employees could save 10,000 tonnes of CO and about $1.7 million per year in energy costs.\n\n"}
{"id": "14922237", "url": "https://en.wikipedia.org/wiki?curid=14922237", "title": "PLEDM", "text": "PLEDM\n\nPLEDM \"(Phase-state Low Electron (hole)-number Drive Memory)\" is a type of memory chip, developed in 1999 at Hitachi's laboratory at the University of Cambridge. PLEDM is based on PLEDTR technology.\n\n\n"}
{"id": "20452948", "url": "https://en.wikipedia.org/wiki?curid=20452948", "title": "PicoSAT", "text": "PicoSAT\n\nPicoSAT, launched on September 30, 2001, is a real time tracking satellite. The name \"PICO\" combines the first letters of all four of its experiments (see below). PICOSat series are designed for a minimum of one year of on-orbit operations.\n\nThe name Picosat was coined by Peter P. Vekinis and was used to describe a constellation of amateur radio satellites, called the Picosat System, first analog, then digital, that would offer instant emergency communications, worldwide, using cheap amateur radio transceivers. The details were presented at AMSAT's conference in Orlando, Florida, in 1995 and in Tucson, Arizona, in 1996.\n\nTethered Picosats, Picosat 5, Picosat 6, Picosat 7, and Picosat 8 are hectogram mass satellites that were ejected from OPAL (2000-004C). The primary builders were by engineering students at Santa Clara University in California. They used off-the-shelf components and miniature batteries, for technology tests. The Tethered Picosats were a pair of Picosats tethered together by a short wire, was ejected on February 8, 2000, from an OPAL Launch System. Picosats 7 and 8 on launched on 11 February, and Picosats 5 and 6 launched on 12 February. Alternate common names were given by the investigators: Picosats 7 and 8 are the Thelma and Louise pair and Picosats 5 and 6 are the JAK and Stensat pair. The Tethered Picosats were functional for a short time after ejection, communicating with each other by microwatt radio transmitters. There was no indication if the Picosats (5, 6, 7,and 8) were operational at the time of ejection into orbit. USSPACECOM's Picosat numbers extending to eight is erroneous. There were only six Picosats on board the OPAL, with possibly one or two still on the ground, with tests to communicate with the orbiters. The tests were managed by the Defense Advanced Research Project Agency (DARPA).\n\nThe current Picosat 9 is a British-built (US DOD-funded) microsatellite (67 kg) to test electronic components/systems in space conditions. Oboard this model carries four test payloads: Polymer Battery Experiment (PBEX), Ionospheric Occultation Experiment (IOX), Coherent Electromagnetic Radio Tomography (CERTO) and On Orbit Mission Control (OOMC) an ultra-quiet platform (OPPEX). PICOSat flies in an 800 km circular orbit with a 67 degree inclination. PICOSat uses a gravity gradient boom for stabilization. The body mounted solar panels produce an average on orbit power of 22 W. The Ultra-Quiet Platform (UQP), developed by the US Air Force Research Lab, aims to provide a 10:1 reduction in vibration isolation over a 100 Hz bandwidth between the spacecraft bus and a science payload.\n\n\n\nInternational designation numbers with USSPACECOM Catalog numbers are in parentheses:\n\n\n"}
{"id": "52020456", "url": "https://en.wikipedia.org/wiki?curid=52020456", "title": "Privatization (computer programming)", "text": "Privatization (computer programming)\n\nPrivatization is a technique used in shared-memory programming to enable parallelism, by removing dependencies that occur across different threads in a parallel program. Dependencies within threads occur due to the presence of variables that are written and/or read by different threads at the same time during execution. This basic principle of this technique is making private copies of a variable shared by multiple threads, hence making each thread capable to operate on its local copy of this variable rather than a shared one.\n\nFor a variable to be shared or private, is specified by the parallel algorithm. Hence, declaring a variable as shared while the algorithm specifies otherwise or vice versa is one of most common errors that might appear in shared-memory programming.\n\nTraditional compilers used in parallelization were able to perform privatization on scalar elements only. In order to exploit parallelism that occurs across loops within a parallel program (loop-level parallelism), the need rose for compilers that are able to perform array variable privatization as well. Hence, most of today's compilers are capable of performing array privatization with more features and functions to enhance the performance of the parallel program in general. An example of such compiler is the Polaris parallelizing compiler \n\n\"A Shared-Memory Multiprocessor\" is defined as \"A computer system composed of multiple independent processors that execute different instruction streams\". The Shared memory programming model is the major and most widely used model in the design space of Shared-Memory Multiprocessors. This programming model starts with the identification of parallelism within a piece of code (recognition of parallel tasks) and then mapping these tasks into threads.\n\nThe next step is Determining the Scope of Variables used in a parallel program, which is one of the key steps and main concerns within this model.\n\nAs mentioned before, after the programming model identifies all parallel tasks within our parallel program, the next step would be grouping these tasks into bigger tasks, as there typically would be more tasks than the available processors within our system. Typically, the number of execution threads that the tasks are assigned to, is chosen to be less than or equal to the number of processors, with each thread assigned to a unique processor.\n\nRight after this step, the use of variables within our identified parallel tasks need to be analyzed. This step determines whether each variable within the parallel task should be shared-by-all or private-to-each thread (Determining the scope of the variable). It is important to note that this particular step is unique to shared-memory programming, as opposed to the other major programming model (message-passing), in which all variables are private since each process has its own address space.\n\nAccording to their behavior, the variables are then categorized into:\n\nRead-Only: when a variable is only read by all the parallel tasks.\n\nRead/Write Non-conflicting: when a variable is read, written, or both by only one task. In the case when the variable is a matrix, different elements are read/written by different parallel tasks.\n\n\"Read/Write Conflicting\": when a variable is written by a task and may be read by another. If the variable is a matrix or array, different elements are read/written by different parallel tasks.\n\nAs it appears from their definition, Read/Write Conflicting variables result in dependencies across different execution threads and hence prevent the parallelization of the program. The two major techniques used to remove these dependencies are Privatization and Reduction. In \"Reduction,\" each thread is provided with a copy of the R/W conflicting variable to operate on it and produce a partial result, which will then be combined with other threads' copies to produce a global result. Another technique similar to Privatization is called Expansion, in which a scalar variable is expanded into an array, which makes each thread access a different array element when trying to access the variable. If the variable to be expanded is already a matrix, then expansion adds another dimension to the matrix.\n\nAs it was previously mentioned, dependencies or conflicts between different threads during execution stops parallelization, and these conflicts appear when we have read/write conflicting variables. One technique to remove this conflict is Privatization. As the name suggests, the basic principle involves making (Private) copies for each thread to operate on, rather than the shared copy that causes the conflict. Hence, changing the type of the variable from Read/Write conflicting to Read/Write Non conflicting.\n\nThe actual local (private) copies of the read/write conflicting variable are created during compile time, by compiling the variable into several variables stored at different memory locations. The architecture of shared-memory multiprocessors aids in this process as threads can address any memory location by default.\n\nThere are two situations in which a variable can be described as \"Privatizable\":\n\nThe first situation is when the variable is (written) before it's being (read) by the parallel task during the original program's sequential order. In this case, if the task wrote to its (private) copy rather than the shared one, the conflict/dependency will be removed. The reason for this is that the program's sequential order will ensures that the value read by the task will be the one written by the same task; removing any conflicts that might occur due to other threads accessing the same variable. This is more explained in the following Example 1.\n\nIn the second situation, the read happens before the write for the variable within the parallel task, but the difference here is that the value the parallel task is trying to read is a value previously know from the program execution. Taking this note into consideration, if each parallel task wrote to its own private copy of the variable, any conflicts or dependencies will be solved during execution, as they would all read a value known ahead of time and them write their correct values on their own copies. This case is more explained in the following Example 2.\n\nDue to the fact that read/write conflicting variables are the only reason to hold parallelization, there is no actual need to declare (read-only) and (read/write non-conflicting) variables as private. However, doing so, won’t affect the correctness of the program, but will add unnecessary memory overhead.\n\nThere are cases when a certain variable can’t be privatized nor reduced to remove the read/write conflict. In these cases, the read/write conflicting variable need to be updated by different tasks at different points of time. An example of this case is explained in Example 3.\n\nOne way to solve this problem, is changing the scope of parallelism to explore a different parallel region. This technique might produce good results, as it's often that after reanalyzing the code, some read/write conflicting variables may change to read/write non-conflicting. If the variable still causes conflicts among different threads, the last resort will be declaring it as shared and protecting its access by the critical section, and providing synchronization if accesses to the variable need to happen in order to ensure the correctness..\n\nAs it appears from the previous variables’ definitions, the Read/Write Conflicting variables can either be scalars or arrays. Hence, Privatization can be applied on both types of variables.\n\nWhen applied to scalar variables, the additional space and overhead introduced by making the extra private copies per threads is relatively small. However, applying privatization on array elements is much more complex.\n\nWhen dealing with arrays, the compiler tries to analyze the behavior of each array element separately and check for the order it's (read) and/or (written). If each element is being (written to) before it's being (read) in the same loop iteration, then this array can be privatized to remove conflicts and enable privatization. To be able to privatize such array, the compiler needs to further analyze the array to combine its accesses into sections. Moreover, the compiler should have extra functions, to be able to manipulate and deal with the array elements. For example, some array expressions may have symbolic terms, hence, to be able to privatize such array, the compiler needs to have some advanced symbolic manipulation functions.\n\nThere are several situations in which it is appropriate to privatize a variable. The first case is when each thread will write to the variable before reading from it. In this case, it does not matter what the other threads are doing because the current thread always writes to the variable before reading (using) it. See the simple example below in which the variable \"x\" is used as a \"temp\" variable to help swaps 3 different pair of variables. Because variable x is always written to before being used, variable x can be privatized.\nThe block above is the sequential code. Notice that without privatizing the variable \"x\", the code could not be parallelized. The code below shows what is possible by parallelizing \"x\". The code can be split up and run on 3 different threads. Each thread has its own copy of \"x\". \nAnother case in which privatization is possible is when a variables value is known before it is used – even if it is not redefined by the same thread. The example below demonstrates this. The variable \"x\" is redefined in the middle of each thread, however, the value that it is redefined as is known when the program was written. By making \"x\" private and defining it at the beginning of each thread, the code can be run in parallel. The example below first shows the sequential code and then how it can parallelized with the help of privatization.\n\nNotice that in order to make the sequential code above parallel, a few extra lines of code had to be added so that \"x\" could be privatized. Because of this, this example may not actually see much of a speed up. However, in larger/longer examples this technique could help improve performance a lot.\n\nOne example when privatization fails is when a value is written in one task and read in another and the value is not known ahead of time. Take this example of summing an array. The sum is a shared variable and is read/written in each iteration of the loop. In sequential code, this works fine. However, if you try to parallelize the loops (each loop a different thread), then the wrong sum would be calculated. In this case, privatization does not work. You cannot privatize the sum because they each rely on each other. While there are techniques to still parallelize this code, simple privatization does not work.\n\nOpenMP is a programming language that supports multiprocessor programming with shared memory. Because of this, read/write conflicting variables will undoubtedly occur. In these cases, Privatization can sometimes be used to allow parallel execution of the code. The example below shows 2 different examples of code. The first example is the original code written sequentially. This example includes a dependence which would normally prevent the code to be run in parallel. The second example, shows the code parallelized and the privatization technique used to remove the dependence.\nFor each iteration of the loop above, x is assigned and then read. Because x is only a single variable, the loop cannot be executed in parallel because it would constantly be overwritten and b(i) would not always be assigned the correct value.\nFor the parallel code, x is declared as private which means each thread gets its own copy and the dependence is therefore removed.\n\nNormally, when a variable is read/write conflicting, the solution will be declaring it as shared and protecting access to it by the critical section, and providing synchronization when needed. Due to the fact that adding more elements into the critical section decreases the overall system performance, this technique is avoided as much as possible.\n\nThus, the variable is checked if it can be reduced first. If it can’t be reduced, the variable is checked for Privatization, taken into consideration whether it's more tolerable to add the extra private copies or serialize access to the variable through the critical section.\n\nWhen compared with Reduction, privatization requires one task instead of two separate tasks in the case of privatization. This task, in an abstract form, is basically analyzing the code to identify the privatizable variables. On the other hand, the two tasks required by Reduction are: identifying the reduction variable, and then parallelizing the reduction operator. By observing each of the two techniques, it's easy tell what type of overhead each one adds to the parallel program; reduction increases the computation overhead while privatization increases the memory consumed by the program.\n\nWhen compared to Expansion, the difference here is that expansion produces more memory overhead. The reason for this is that the memory space needed for privatization is proportional to the number of processors, while in expansion, it's proportional to the number of loop iterations. As it was previously mentioned, the number of tasks is typically higher than the number of processors available, which makes memory required here much larger than what is required for privatization.\n\nAs previously mentioned, changing the scope of parallelism to explore a different parallel region is also one of the techniques used to enable parallelization. Changing the scope in which the parallel tasks are identified might sometimes greatly change the behavior of the variables. Hence, reanalyzing the code and performing this technique may often change read/write conflicting variables into non-conflicting.\n\n"}
{"id": "358237", "url": "https://en.wikipedia.org/wiki?curid=358237", "title": "Roof garden", "text": "Roof garden\n\nA roof garden is a garden on the roof of a building. Besides the decorative benefit, roof plantings may provide food, temperature control, hydrological benefits, architectural enhancement, habitats or corridors for wildlife, recreational opportunities, and in large scale it may even have ecological benefits. The practice of cultivating food on the rooftop of buildings is sometimes referred to as rooftop farming. Rooftop farming is usually done using green roof, hydroponics, aeroponics or air-dynaponics systems or container gardens.\nHumans have grown plants atop structures since the ziggurats of ancient Mesopotamia (4th millennium BC–600 BC) had plantings of trees and shrubs on aboveground terraces. An example in Roman times was the Villa of the Mysteries in Pompeii, which had an elevated terrace where plants were grown. A roof garden has also been discovered around an audience hall in Roman-Byzantine Caesarea. The medieval Egyptian city of Fustat had a number of high-rise buildings that Nasir Khusraw in the early 11th century described as rising up to 14 stories, with roof gardens on the top story complete with ox-drawn water wheels for irrigating them.\n\nAmong the Seven Wonders of the Ancient World, The Hanging Gardens are often depicted as tall structures holding vegetation; even immense trees.\n\nRoof gardens are most often found in urban environments. Plants have the ability to reduce the overall heat absorption of the building which then reduces energy consumption. \"The primary cause of heat build-up in cities is insolation, the absorption of solar radiation by roads and buildings in the city and the storage of this heat in the building material and its subsequent re-radiation. Plant surfaces however, as a result of transpiration, do not rise more than 4–5 °C above the ambient and are sometimes cooler.\" This then translates into a cooling of the environment between 3.6 and 11.3 degrees Celsius (6.5 and 20.3 °F), depending on the area on earth (in hotter areas, the environmental temperature will cool more). The study was performed by the University of Cardiff.\n\nA study at the National Research Council of Canada showed the differences between roofs with gardens and roofs without gardens against temperature. The study shows temperature effects on different layers of each roof at different times of the day. Roof gardens are obviously very beneficial in reducing the effects of temperature against roofs without gardens. “If widely adopted, rooftop gardens could reduce the urban heat island, which would decrease smog episodes, problems associated with heat stress and further lower energy consumption.”\n\nAside from rooftop gardens providing resistance to thermal radiation, rooftop gardens are also beneficial in reducing rain run off. A roof garden can delay run off; reduce the rate and volume of run off. “As cities grow, permeable substrates are replaced by impervious structures such as buildings and paved roads. Storm water run-off and combined sewage overflow events are now major problems for many cities in North America. A key solution is to reduce peak flow by delaying (e.g., control flow drain on roofs) or retaining run-off (e.g., rain detention basins). Rooftop gardens can delay peak flow and retain the run-off for later use by the plants.”\n\n“In an accessible rooftop garden, space becomes available for localized small-scale urban agriculture, a source of local food production. An urban garden can supplement the diets of the community it feeds with fresh produce and provide a tangible tie to food production.” At Trent University, there is currently a working rooftop garden which provides food to the student café and local citizens.\n\nAvailable gardening areas in cities are often seriously lacking, which is likely the key impetus for many roof gardens. The garden may be on the roof of an autonomous building which takes care of its own water and waste. Hydroponics and other alternative methods can expand the possibilities of roof top gardening by reducing, for example, the need for soil or its tremendous weight. Plantings in containers are used extensively in roof top gardens. Planting in containers prevents added stress to the roof's waterproofing. One high-profile example of a building with a roof garden is Chicago City Hall.\n\nFor those who live in small apartments with little space, square foot gardening, or (when even less space is available) green walls (vertical gardening) can be a solution. These use much less space than traditional gardening (square foot gardening uses 20% of the space of conventional rows; ten times more produce can be generated from vertical gardens). These also encourage environmentally responsible practices, eliminating tilling, reducing or eliminating pesticides, and weeding, and encouraging the recycling of wastes through composting.\n\nBecoming green is a high priority for urban planners. The environmental and aesthetic benefits to cities is the prime motivation. It was calculated that the temperature in Tokyo could be lowered by 0.11–0.84 °C if 50% of all available rooftop space were planted with greenery. This would lead to a savings of approximately 100 million yen \n\nSingapore is active in green urban development. \"Roof gardens present possibilities for carrying the notions of nature and open space further in tall building development.\" When surveyed, 80% of Singapore residents voted for more roof gardens to be implemented in the city's plans. Recreational reasons, such as leisure and relaxation, beautifying the environment, and greenery and nature, received the most votes. Planting roof gardens on tops of building is a way to make cities more efficient.\n\nA roof garden can be distinguished from a green roof, although the two terms are often used interchangeably. The term roof garden is well suited to roof spaces that incorporate recreation, entertaining, and provide additional outdoor living space for the building's residents. It may include planters, plants, dining and lounging furniture, outdoor structures such as pergolas and sheds, and automated irrigation and lighting systems.\n\nAlthough they may provide aesthetic and recreational benefits a green roof is not necessarily designed for this purpose. A green roof may not provide any recreational space and be constructed with an emphasis towards improving the insulation or improving the overall energy efficiency and reducing the cooling and heating costs within a building.\n\nGreen roofs may be extensive or intensive. The terms are used to describe the type of planting required. The panels that comprise a green roof are generally no more than a few inches up to a foot in depth, since weight is an important factor when covering an entire roof surface. The plants that go into a green roof are usually sedum or other shallow-rooted plants that will tolerate the hot, dry, windy conditions that prevail on most rooftop gardens. With a green roof, \"the plants layer can shield off as much as 87% of solar radiation while a bare roof receives 100% direct exposure\".\n\nThe planters on a roof garden may be designed for a variety of functions and vary greatly in depth to satisfy aesthetic and recreational purposes. These planters can hold a range of ornamental plants: anything from trees, shrubs, vines, or an assortment of flowers. As aesthetics and recreation are the priority they may not provide the environmental and energy benefits of a green roof.\n\nThe related idea of a living machine is based on the most basic cycle of gardening: using wastes (organic waste and sewage), appropriately broken down, usually in some specialized container, on the soil, and harvesting food which, when processed, generates biodegradable waste, and when eaten, generates sewage. In most of the world, this kind of very tight closed loop gardening is used, despite certain health risks if necessary precautions are not taken. Composting human or pet waste should achieve thermophilic conditions and age for at least a year before being used.\n\n\n"}
{"id": "6432985", "url": "https://en.wikipedia.org/wiki?curid=6432985", "title": "Sanex", "text": "Sanex\n\nSanex is a brand of personal care products owned by Colgate-Palmolive. It is sold in European countries (including United Kingdom, Netherlands, Belgium, France, Spain, Portugal, Denmark, Greece, Poland, Norway and Croatia) and South Africa.\n\nIn 2011, the brand was acquired from Unilever for £580 million.\n\nProduct lines include deodorant, shower gel and liquid hand soap.\n\n"}
{"id": "37809153", "url": "https://en.wikipedia.org/wiki?curid=37809153", "title": "Sevcon", "text": "Sevcon\n\nSevcon was an electrical engineering company based in Gateshead in the UK, that manufactured controls for electric vehicles.\n\nIt was founded in 1961 as Sevcon Engineering Ltd. The company was formed to develop technology originated by Horace William Heyman (13 March 1912 - 4 September 1998), who had been Managing Director of Smith Electric Vehicles from 1949 until 1964. It was the largest manufacturer of electric vehicles in Europe.\n\nHis technology improved battery-life for frequent start-stop runs of electric vehicles at low speeds. There were always large electrical losses on starting an electric vehicle because of electrical resistances heating up. The technology allowed a 20% reduction in battery size for equivalent power, or increased battery length or power for a standard-sized battery.\n\nAn electric vehicle would have had resistances to drop the voltage on the motor when starting, but this created heating. His technology employed semiconductor-controlled rectifiers, with a semiconductor oscillator, which provided pulses of electrical power to power the motor.\n\nIt won the Queen's Award to Industry in April 1970, and the Queen's Awards for Export in April 1975. A new factory opened in June 1970 at Gateshead.\n\nIt opened its first subsidiary in 1968 in Paris. In 1969 it was bought by Tech/Ops of the USA.\n\nIn 1988 it was listed on the USA stock exchange, then joined the NASDAQ Capital Market in 2009.\n\nIn September 2017, following a period of rapid expansion, the company was purchased and merged with Borg Warner.\n\nThe head office is situated opposite the automotive centre of Gateshead College. It outsources much of the manufacture of its components to Key Tronic of the USA. A UK site manufactures film capacitors.\n\nIts competitors include Danaher's Motion division, and General Electric. In 2010 it spent around £3 million in research and development at Gateshead.\n\nIt makes microprocessor controllers for electric vehicles.\n\n"}
{"id": "4093822", "url": "https://en.wikipedia.org/wiki?curid=4093822", "title": "System in package", "text": "System in package\n\nA system in package (SiP) or system-in-a-package is a number of integrated circuits enclosed in a single chip carrier package. The SiP performs all or most of the functions of an electronic system, and is typically used inside a mobile phone, digital music player, etc. Dies containing integrated circuits may be stacked vertically on a substrate. They are internally connected by fine wires that are bonded to the package. Alternatively, with a flip chip technology, solder bumps are used to join stacked chips together. Systems-in-package are like systems-on-chip (SoC) but less tightly integrated and not on a single semiconductor die. \n\nSiP dies can be stacked vertically or tiled horizontally, unlike less dense multi-chip modules, which place dies horizontally on a carrier. SiP connects the dies with standard off-chip wire bonds or solder bumps, unlike slightly denser three-dimensional integrated circuits which connect stacked silicon dies with conductors running through the die.\n\nMany different 3-D packaging techniques have been developed for stacking many fairly standard chip dies into a compact area.\n\nAn example SiP can contain several chips—such as a specialized processor, DRAM, flash memory—combined with passive components—resistors and capacitors—all mounted on the same substrate. This means that a complete functional unit can be built in a multi-chip package, so that few external components need to be added to make it work. This is particularly valuable in space constrained environments like MP3 players and mobile phones as it reduces the complexity of the printed circuit board and overall design. Despite its benefits, this technique decreases the yield of fabrication since any defective chip in the package will result in a non-functional packaged integrated circuit, even if all other modules in that same package are functional.\n\nSiP technology is primarily being driven by market trends in wearables, mobile devices and the internet of things. As the internet of things becomes more of a reality and less of a vision, there is innovation going on at the system on a chip and SiP level so that microelectromechanical (MEMS) sensors can be integrated on a separate die and control the connectivity. \n\nSiP solutions may require multiple packaging technologies, such as flip chip, wire bonding, wafer-level packaging and more. \n\n\n\n"}
{"id": "306125", "url": "https://en.wikipedia.org/wiki?curid=306125", "title": "Telautograph", "text": "Telautograph\n\nThe telautograph, an analog precursor to the modern fax machine, transmits electrical impulses recorded by potentiometers at the sending station to servomechanisms attached to a pen at the receiving station, thus reproducing at the receiving station a drawing or signature made by the sender. It was the first such device to transmit drawings to a stationary sheet of paper; previous inventions in Europe had used rotating drums to make such transmissions.\n\nThe telautograph's invention is attributed to Elisha Gray, who patented it on July 31, 1888. Gray's patent stated that the telautograph would allow \"one to transmit his own handwriting to a distant point over a two-wire circuit.\" It was the first facsimile machine in which the stylus was controlled by horizontal and vertical bars. The telautograph was first publicly exhibited at the 1893 World's Columbian Exposition held in Chicago.\n\nWhile the patent schema's geometry implies vertical and horizontal coordinates, systems used in the 20th Century (and presumably before) had a different coordinate scheme, based on transmitting two angles.\n\nIn an 1888 interview in \"The Manufacturer & Builder\" (Vol. 24: No. 4: pages 85–86) Gray made this statement:\n\nBy my invention you can sit down in your office in Chicago, take a pencil in your hand, write a message to me, and as your pencil moves, a pencil here in my laboratory moves simultaneously, and forms the same letters and words in the same way. What you write in Chicago is instantly reproduced here in fac-simile. You may write in any language, use a code or cipher, no matter, a fac-simile is produced here. If you want to draw a picture it is the same, the picture is reproduced here. The artist of your newspaper can, by this device, telegraph his pictures of a railway wreck or other occurrences just as a reporter telegraphs his description in words.\n\nBy the end of the 19th century, the telautograph was modified by Foster Ritchie. Calling it the telewriter, Ritchie's version of the telautograph could be operated using a telephone line for simultaneous copying and speaking.\n\nThe telautograph became very popular for the transmission of signatures over a distance, and in banks and large hospitals to ensure that doctors' orders and patient information were transmitted quickly and accurately.\nTeleautograph systems were installed in a number of major railroad stations to relay hand-written reports of train movements from the interlocking tower to various parts of the station. The teleautograph network in Grand Central Terminal included a public display in the main concourse into the 1960s; a similar setup in Chicago Union Station remained in operation into the 1970s.\nA Telautograph was used in 1911 to warn workers on the 10th floor about the Triangle Shirtwaist Factory fire that had broken out two floors below.\nAn example of a Telautograph machine writing script can be seen in the 1956 movie Earth vs the Flying Saucers as the output device for the mechanical translator.\n\nTelautograph Corporation changed its name several times. In 1971, it was acquired by Arden/Mayfair. In 1993, Danka Industries purchased the company and renamed it \"Danka/Omnifax\". In 1999, Xerox corporation purchased the company and called it the \"Omnifax division\", which has since been absorbed by the corporation.\n\n\n\"Patent images in TIFF format\"\n"}
{"id": "5092381", "url": "https://en.wikipedia.org/wiki?curid=5092381", "title": "The Winter Market", "text": "The Winter Market\n\n\"The Winter Market\" is a science fiction short story written by William Gibson and published as part of his \"Burning Chrome\" short story collection. The story was commissioned in 1985 by \"Vancouver Magazine\", who stipulated that Gibson – who at the time was \"unquestionably the leading Vancouver author on the international literary scene\" – set it in the city (thereby making it unique among the author's works).\n\nThe market of the title was modelled on that of Granville Island, though in a state of bohemian decay. As the author commented in a 2007 blog post: \"Vancouver's Granville Island, centered around Granville Island Market (produce and food fair) is a very successful (and pleasant) retrofit of an under-bridge urban island that previously was heavily industrial. When the story was written, the retrofit was recent, and I dirtied it up for requisite punky near-future effect.\"\n\nThe story primarily concerns human relationships and their tenuous and problematic qualities by deploying the concept of technological immortality, in which one's consciousness is separated from the body and \"uploaded\" into a supercomputer, where it continues to think and function on its own. Characters in the story are marked by a distinct failure to connect, while they express typical genre concerns regarding this type of theoretical mind transfer; whether or not the online consciousness really is the same individual, and whether or not it was moral to allow this to happen. In this particular tale, Lise's original body is defective and failing, partially due to a congenital disease, and partially due to drug abuse. Hence, the act of leaving behind the original physical form is potentially one of escape into an untainted existence. However, the story undercuts this simplistic reading by convincingly evoking Lise's humanity and her longing for a \"normal\" relationship to her body.\n\nAccording to the analysis of critic Pramod Nayar, the story \"depicts the body as a vehicle for experiencing dreams edited into Hollywood thrillers\".\n\nCritic David Seed saw the character of Rubin as a \"thinly disguised\" incarnation of performance artist Mark Pauline of Survival Research Laboratories.\n\nThe story was critically well-received, garnering nominations for the Hugo Award for Best Novelette, the Nebula Award for Best Novelette, the \"short-form, English\" Prix Aurora award, and the British Science Fiction Association award for best short story. It also finished highly in several science fiction magazines' annual readers polls in 1987, coming fourth in the \"Locus\" novelette category, third in the \"Interzone\" fiction category, and joint second in the \"Science Fiction Chronicle\" novelette category.\n\n"}
{"id": "36960047", "url": "https://en.wikipedia.org/wiki?curid=36960047", "title": "Thin-wall injection molding", "text": "Thin-wall injection molding\n\nThin wall injection molding is a specialized form of conventional injection molding that focuses on mass-producing plastic parts that are thin and light so that material cost savings can be made and cycle times can be as short as possible. Shorter cycle times means higher productivity and lower costs per part.\n\nThe definition of thin wall is really about the size of the part compared to its wall thickness. For any particular plastic part, as the wall thickness reduces the harder it is to manufacture using the injection molding process. The size of a part puts a limit on how thin the wall thickness can be. For packaging containers thin wall means wall thicknesses that are less than 0.025 inch (0.62mm) with a flow length to wall thickness greater than 200.\n\nThe trend towards thin wall molding continues to increase in many plastic industries as plastic material and energy costs continue to rise and delivery lead times are squeezed.\nThe following industries make use of thin wall molding: \n\n\n\nPlastic resins suitable for thin-wall molding should have high-flow properties, particularly low melt viscosity. In addition, they need to be robust enough to avoid degradation from the heat generated by high shear rates (high injection speeds)\n\nSome plastic manufacturers make plastics specifically for thin wall applications that have excellent flow properties inside the mold cavity. For example, plastic manufacturer Sabic has a polypropylene food contact grade plastic which is specifically designed for thin wall margarine containers and lids.\n\nAnother plastic manufacturer, Bayer, makes a blend of Polycarbonate (PC) and Acrylonitrile butadiene styrene (ABS) specifically designed to make thin wall mobile housings.\n\nCompared to conventional injection molding, thin wall molding requires molding machines that are designed and built to withstand higher stresses and injection pressures. The molding machines computer control should also be precise in order to make quality parts. For this reason these molding machines are more expensive than general purpose machines. \n\nThin-wall-capable machines usually also have accumulator-assisted clamps to accommodate fast cycle times.\n\nRegular maintenance schedules must be completed so that the machine and part quality does not suffer. These machines usually work 24/7 so they need to be well maintained.\n\nAs with the injection molding machines, injection molds need to be robust enough to withstand high stresses and pressures. Heavy mold construction with through hardened tool steels will ensure a long lasting mold.\n\nThe mold must also have a well designed cooling system so that heat can be quickly extracted from the hot plastic part allowing fast cycle times. To achieve this, cooling channels need to be designed close to the molding surface. \nCleaning the mould on a daily basis is also a critical requirement to maintain the part quality.\n\nIn countries where manual labour is expensive, robots are commonly used to remove the plastic parts from the mold and order them into equal stacks. These robots are fixed to the molding machine and need to be fast and reliable.\n\nThe range of process parameters, which are employed for thin wall molded parts, is considerably narrower than that of conventional injection molding because thin parts are difficult for the injection unit of the machine to fill compared to thicker parts. Even with optimally designed parts and molds, it is still more difficult to produce parts with thin walls.\n\nConsistent injection speeds and pressures are required to maintain the quality of the parts produced from thin wall molding. A properly trained molding technician who understands and operates the machine within the confines of the narrow processing window at which the molded product's production cost-effectiveness is optimized will ensure that the process produces quality parts during the production.\n"}
{"id": "42195952", "url": "https://en.wikipedia.org/wiki?curid=42195952", "title": "Trinity: A Graphic History of the First Atomic Bomb", "text": "Trinity: A Graphic History of the First Atomic Bomb\n\nTrinity: A Graphic History of the First Atomic Bomb is the debut graphic novel written and illustrated by Jonathan Fetter-Vorm. It provides an account of the Manhattan Project and the atomic bombings of Hiroshima and Nagasaki, as well as mentioning the chain of events after. The title arises from the code-name, Trinity, given to the test site for the first nuclear weapon.\n\nThe book is written as a \"work of history\", although Fetter-Vorm writes at the end of the book \"for the most part, the dialog from the principal characters in this book is taken from written records. When that was impossible, I introduced language that hews closely to what I have learned of these characters over the course of my research...\" He goes on to provide a bibliography of the works he consulted in creating the book.\n\n\"Trinity\" begins with a conversation between a soldier named Private Daniels and J. Robert Oppenheimer as they are entering Los Alamos National Laboratory in New Mexico. Oppenheimer asks Daniels if he has ever heard of Prometheus, and upon receiving a \"No, sir\", recounts the myth.\n\nThe novel then goes back in time to 1898, when the Marie Curie and her husband discover radioactivity. The story presents a brief timeline that continues up into the 1930s, when James Chadwick discovers neutrons and the discovery of nuclear fission. The novel takes a shift, and begins to discuss the political environment. Leo Szilard becomes troubled as he sees the dangers that these nuclear weapons could produce, and travels with Eugene Wigner to talk with Albert Einstein about the possibility of Nazi scientists creating a bomb. News soon reaches the United States president, who authorizes the precursor to the Manhattan Project. After the bombing of Pearl Harbor, the United States founds covertly Los Alamos National Laboratory and other cities to create nuclear weapons. The scientists and their families are sworn to secrecy. Soon, the scientists at Los Alamos discover how to create a nuclear chain reaction, leading to the development of the first atomic weapons: Little Boy and Fat Man.\n\nSoon, the day of the Trinity test arrives, and President Truman receives notice at the Potsdam Conference in Potsdam, Germany. Although it has ended in Europe, the war continues in the Pacific, with Japan refusing to surrender even after the aerial raid of Tokyo. The author briefly mentions history of warfare weapons, up to the new atomic bomb. Eventually, Truman authorizes the bombing of Hiroshima. The crew from Enola Gay is seen on their way to the skies over Hiroshima, and then drop the bomb. The book describes in detail what happens:\n\"The effect was like this: The heat and the light hit before the sound. Now, in a world without sight or sound, a wave of air traveling at more than 800 miles per hour sweeps outwards in all directions. In its wake comes the earth-trembling roar of the atmosphere aflame. The blast is so hot that everything flammable within a few hundred yards of ground zero vaporizes in a flash of smoke. Then suddenly the air pressure spikes. Your eyes and your lungs bulge, swell, and burst. Your eardrums explode. In a few seconds the air pressure settles back to normal, and the wind slow...and then picks up speed in the opposite direction, sucking everything inward to the churning heart of the explosion.\"\nThe news of the bombing becomes worldwide. The Japanese still refuse to surrender. The US prepares to drop a bomb on the city of Kokura, but cannot because of the weather; at the same time, there is not enough fuel to carry the bomb back to the US base in Tinian. As a result, the flight crew ends up dropping the bomb on Nagasaki.\n\nThe book cuts out to a scene of two children on their way home from school when the bomb drops and incinerates them. One of the boys is still alive, and walking around searching for water, as the view pans out to the carnage and death that resulted from the bombing. Japan ends up surrendering.\n\nIn the aftermath, the survivors begin to be afflicted by a mysterious disease, called Disease X, which turns out to be radiation poisoning. The world now sees the possible results of nuclear warfare, and the doctrine of mutually assured destruction arises. Nuclear weapons begin to proliferate. The public now prepares itself as it enters into the new Atomic Age.\n\nThe book was published to generally positive reviews. Ray Olson, writing in \"Booklist\", praised the artwork and design, saying that \"the page layouts are attractively busy and varied, never crowded and hard to read, while the text proceeds stepwise down each page, never courting confusion by running in circles or zigzagging...\", and finishing by calling the book \"exemplary\".\n\nOne reviewer, however, writing for \"Publishers Weekly\", found the text confusing to follow and derided the marketing, along with book itself, because of \"flat illustrations, heavy use of captions, and stiff, static panels of talking heads\".\n\n\n"}
{"id": "23984627", "url": "https://en.wikipedia.org/wiki?curid=23984627", "title": "Unified Wine &amp; Grape Symposium", "text": "Unified Wine &amp; Grape Symposium\n\n"}
{"id": "23065805", "url": "https://en.wikipedia.org/wiki?curid=23065805", "title": "Video banking", "text": "Video banking\n\nVideo banking is a term used for performing banking transactions or professional banking consultations via a remote video connection. Video banking can be performed via purpose built banking transaction machines (similar to an Automated teller machine), or via a videoconference enabled bank branch.\n\nToday, video banking has many forms, each with its own benefits and limitations.\n\nVideo banking can be conducted in a traditional banking branch. This form of video banking replaces or partially displaces the traditional banking tellers to a location outside of the main banking branch area. Via the video and audio link, the tellers are able to service the banking customer. The customer in the branch uses a purpose built machine to process viable medias such as cheques, cash, or coins.\n\nVideo banking can provide professional banking services to bank customers during nontraditional banking hours at convenient times such as in after hours banking branch vestibules that could be open up to 24 hours a day. This gives bank customers the benefit of personal teller service during hours when bank branches are not typically open.\n\nIn addition, following Check 21, \"cut off\" times are typically later for Personal Teller Machines as physical checks do not need to be gathered and collected to be delivered to a separate check processing location. Substitute checks, or digital images of the original check, can be utilized to process the transaction. This can result in the typical 3PM business day \"cut off\" for a branch being extended well into the evening. This could greatly benefit customers by making their funds available even sooner than visiting a branch teller window.\n\nVideo banking can provide professional banking services in nontraditional banking locations such as after hours banking branch vestibules, grocery stores, office buildings, factories, or educational campuses.\n\nVideo banking can enable banks to expand real-time availability of high-value banking consultative services in branches that might not otherwise have access to the banking expertise.\n\nVideo Banking has now been taken to another level of convenience. IndusInd Bank from India, has recently launched an innovative functionality called Video Branch. Video Branch empowers the customer to do a Virtual yet Face- to-Face banking with his bank branch Manager or a Central video branch. This concept has brought banking virtually in the hands of the customer. The customer can engage with the bank for services and financial transactions by ‘meeting’ his bank Anytime, Anywhere.\n\nCustomer can simply connect to the bank through an App on Android and Apple devices as well as through an application on the laptop/Desktop. So whether the customer is at his home or office or even travelling he will be able to set up a Video conference and experience instant banking services. This facility is available to IndusInd Bank customers only\n\nAlthough video banking has many different forms, they all have similar basic components.\n\nAlthough termed video banking, the video connection is always accompanied by an audio link which ensures the customer and bank representative can communicate clearly with one another. The communication link for that video and audio typically requires a high-speed data connection for applications where the tellers are not in the same physical location. Various technologies are employed by the vendors of video banking, but recent advances in audio and video compression make the use of these technologies much more affordable. For an in depth discussion on videoconferencing technologies see wiki videoconferencing article.\n\nOther than the deployment location, one of the major differences between video banking and videoconferencing is the ability to conduct banking transactions and exchange viable medias such as checks, cash, and coins. Purpose built machines, such as a Personal Teller Machine (PTM), enable both the video / audio link to the customer plus the ability to accept and dispense viable medias. The system typically allows the bank teller to manipulate the PTM machine to accept or dispense the cash and checks.\n\nPurpose-built transaction equipment is currently available, but in the future these video banking systems will likely leverage existing automated teller machines which will be modified to enable the audio and video communication.\n\nDepending on the type of video banking solution deployed there are numerous types of services that can be offered. In conjunction with transaction hardware video banking can include all of the following types of services.\n\n\nWith all types of video banking the following services are enabled:\n"}
{"id": "41856", "url": "https://en.wikipedia.org/wiki?curid=41856", "title": "Voice frequency primary patch bay", "text": "Voice frequency primary patch bay\n\nIn telecommunication, a voice frequency primary patch bay (VF) is a patching facility that provides the first appearance of local-user VF circuits in the technical control facility (TCF). \n\nThe VF primary patch bay provides patching, monitoring, and testing for all VF circuits. Signals will have various levels and signaling schemes depending on the user terminal equipment.\n"}
{"id": "2711593", "url": "https://en.wikipedia.org/wiki?curid=2711593", "title": "Well control", "text": "Well control\n\nWell control is the technique used in oil and gas operations such as drilling, well workover, and well completions for maintaining the fluid column hydrostatic pressure and formation pressure to prevent influx of formation fluids into the wellbore. This technique involves the estimation of formation fluid pressures, the strength of the subsurface formations and the use of casing and mud density to offset those pressures in a predictable fashion. Understanding of pressure and pressure relationships are very important in well control.\n\nFluid is any substance that flows; e.g. oil, water, gas, and ice are all examples of fluids. Under extreme pressure and temperature almost anything will become fluid.\nFluid exerts pressure and this pressure is as a result of the density and the height of the fluid column. Most oil companies usually represent density measurement in pounds per gallon (ppg) or kilograms per cubic meter (kg/m) and pressure measurement in pounds per square inch (psi) or bar or pascal (Pa). Pressure increases as the density of the fluid increases.\nTo find out the amount of pressure a fluid of a known density exerts for each unit of length, the pressure gradient is used.\nA pressure gradient is defined as the pressure increase per unit of the depth due to its density and it is usually measured in pounds per square inch per foot or bars per meter.It is expressed mathematically as;\n\"pressure gradient = fluid density × conversion factor\".\nThe conversion factor used to convert density to pressure is 0.052 in English system and 0.0981 in Metric system.\n\nHydro means water, or fluid, that exerts pressure and static means not moving or at rest. Therefore, hydrostatic pressure is the total fluid pressure created by the weight of a column of fluid, acting on any given point in a well. In oil and gas operations, it is represented mathematically as;\n\"Hydrostatic pressure = pressure gradient × true vertical depth\" or \"Hydrostatic pressure = fluid density × conversion factor × true vertical depth\" .\n\nThe figure (not shown) shows two wells, well X and Y. Well X has measured depth of 9800 ft and a true vertical depth of 9800 ft while well Y has measured depth of 10380 ft and its true vertical depth is 9800 ft.To calculate the hydrostatic pressure of the bottomhole, the true vertical depth is used because gravity acts (pulls) vertically down the hole. The figure also illustrates the difference between true vertical depth (TVD) and measured depth (MD).\n\n Formation pressure is the pressure of the fluid within the pore spaces of the formation rock. This pressure can be affected by the weight of the overburden (rock layers) above the formation, which exerts pressure on both the grains and pore fluids. Grains are solid or rock material, and pores are spaces between grains. If pore\nfluids are free to move, or escape, the grains lose some of their support and move closer together. This process is called consolidation.\nDepending on the magnitude of the pore pressure, it can be described as being normal, abnormal or subnormal.\nNormal pore pressure or formation pressure is equal to the hydrostatic pressure of formation fluid extending from the surface to the surface formation being considered. In other words, if the formation was opened up and allowed to fill a column whose length is equal to the depth of the formation, then the pressure at the bottom of the column will be equal to the formation pressure and the pressure at surface is equal to zero.\nNormal pore pressure is not a constant. Its magnitude varies with the concentration of dissolved salts, type of fluid, gases present and temperature gradient.\n\nWhen a normally pressured formation is raised toward the surface while prevented from losing pore fluid in the process, it will change from normal pressure (at a greater depth) to abnormal pressure (at a shallower depth). \nWhen this happens, and then one drill into the formation, mud weights of up to 20 ppg (2397 kg/m ³) may be required for control. This process accounts for many of the shallow, abnormally pressured zones in the world. In areas where faulting is present, salt layers or domes are predicted, or excessive geothermal gradients are known, drilling operations may encounter abnormal pressure. \nAbnormal pore pressure is defined as any pore pressure that is greater than the hydrostatic pressure of the formation fluid occupying the pore space. It is sometimes called overpressure or geopressure. An abnormally pressured formation can often be predicted using well history, surface geology, downhole logs or geophysical surveys.\nSubnormal pore pressure is defined as any formation pressure that is less than the corresponding fluid hydrostatic pressure at a given depth. Subnormally pressured formations have pressure gradients lower than fresh water or less than 0.433 psi/ft (0.0979 bar/m). Naturally occurring subnormal pressure can be developed when the overburden has been stripped away, leaving the formation exposed at the surface.\nDepletion of original pore fluids through evaporation, capillary action and dilution produces hydrostatic gradients below 0.433 psi/ft (0.0979 bar/m). Subnormal pressures may also be induced through depletion of formation fluids.\nIf Formation Pressure < Hydrostatic pressure then it is under pressured. \nIf Formation Pressure > Hydrostatic pressure then it is over pressured .\n\nFracture pressure is the amount of pressure it takes to permanently deform the rock structure of a formation. Overcoming formation pressure is usually not sufficient to cause fracturing. If pore fluid is free to move, a slow rate of entry into the formation will not cause fractures. If pore fluid cannot move out of the way, fracturing and permanent deformation of the formation can occur. Fracture pressure can be expressed as a gradient (psi/ft), a fluid density equivalent (ppg), or by calculated total pressure at the formation (psi). Fracture gradients normally increase with depth due to increasing overburden pressure. Deep, highly compacted formations can require very high fracture pressures to overcome the existing formation pressure and resisting rock structure. Loosely compacted formations, such as those found offshore in deep water, can fracture at low gradients (a situation exacerbated by the fact that some of total \"overburden\" up the surface is sea water rather than the heavier rock that would be present in an otherwise-comparable land well). Fracture pressures at any given depth can vary widely because of the geology of the area.\n\nBottom hole pressure is used to represent the sum of all the pressures being exerted at the bottom of the hole. Pressure is imposed on the walls of the hole. The hydrostatic fluid column accounts for most of the pressure, but pressure to move fluid up the annulus also acts on the walls. In larger diameters, this annular pressure is small, rarely exceeding 200 psi (13.79 bar). In smaller diameters it can be 400 psi (27.58 bar) or higher. Backpressure or pressure held on the choke also increases bottomhole pressure, which can be estimated by adding up all the known pressures acting in, or on, the annular (casing) side. Bottomhole pressure can be estimated during the following activities;\n\nIf no fluid is moving, the well is static. The bottomhole pressure (BHP) is equal to the hydrostatic pressure (HP) on the annular side. If shut in on a kick, bottomhole pressure is equal to the hydrostatic pressure in the annulus plus the casing (wellhead or surface pressure) pressure.\n\nDuring circulation, the bottomhole pressure is equal to the hydrostatic pressure on the annular side plus the annular pressure loss (APL).\n\nDuring circulating with a rotating head the bottomhole pressure is equal to the hydrostatic pressure on the annular side, plus the annular pressure loss, plus the rotating head backpressure.\n\nBottomhole pressure is equal to hydrostatic pressure on the annular side, plus annular pressure loss, plus choke (casing) pressure. For subsea, add choke line pressure loss.\n\nAn accurate evaluation of a casing cement job as well as of the formation is extremely important during the drilling of a well and for subsequent work. The Information resulting from Formation Integrity Tests (FIT) is used throughout the life of the well and also for nearby wells. Casing depths, well control options, formation fracture pressures and limiting fluid weights may be based on this information. To determine the strength and integrity of a formation, a Leak Off Test (LOT) or a Formation Integrity Test (FIT) may be performed. This test is first: a method of checking the cement seal between casing and the formation, and second: determining the pressure and/or fluid weight the test zone below the casing can sustain. Whichever test is performed, some general points should be observed. The fluid in the well should be circulated clean to ensure it is of a known and consistent density. If mud is used for the test, it should be properly conditioned and gel strengths minimized. The pump used should be a high-pressure, low-volume test or cementing pump. Rig pumps can be used if the rig has electric drives on the mud pumps, and they can be slowly rolled over. If the rig pump must be used and the pump cannot be easily controlled at low rates, then the leak-off technique must be modified. It is a good idea to make a graph of the pressure versus time or volume for all leak-off tests.\n\nThe main reasons for performing formation integrity test (FIT) are:\n\nIt is often helpful to visualize the well as a U-tube as in Figure beside. Column Y of the tube represents the annulus and column X represents the pipe (string) in the well. The bottom of the U-tube represents the bottom of the well. In most cases, there are fluids creating hydrostatic pressures in both the pipe and annulus. Atmospheric pressure can be omitted, since it works the same on both columns. If the fluid in both the pipe and annulus are of the same density, hydrostatic pressures would be equal and the fluid would be static on both sides of the tube. If the fluid in the annulus is heavier, it will exert more pressure downward and will flow into the string, displacing some of the lighter fluid out of the string causing a flow at surface. The fluid level will fall in the annulus, equalizing pressures. When there is a difference in the hydrostatic pressures, the fluid will try to reach balance point. This is called U-tubing, and it explains why there is often flow from the pipe when making connections. This is often evident when drilling fast because the effective density in the annulus is increased by cuttings.\n\nThe Equivalent Circulating Density (ECD) is defined as the increase in density due to friction and it is normally expressed in pounds per gallon. Equivalent Circulating Density (when forward circulating) is defined as the apparent fluid density which results from adding annular friction to the actual fluid density in the well.\n\nformula_1 or ECD = MW +( p/1.4223*TVD(M)\n\nWhere;\nECD = Equivalent circulating density (ppg),\nPa = Difference between annular pressure at surface & annular pressure at depth TVD (psi),\nTVD = True vertical depth (ft),\nMW = Mud weight (ppg)\n\nWhen the drilling mud is under static condition(no circulation),Pressure at any point is only due to drilling mud weight and is given by:-\n\nPressure under static condition= 0.052*Mud weight(in ppg)*TVD(in feet)\n\nDuring circulation, pressure applied is due to drilling mud weight and also due to the pressure applied by the mud pumps to circulate the drilling fluid.\n\nPressure under circulating condition= Pressure under static condition+ Pressure due to pumping at that point or pressure loss in the system\n\nIf we convert pressure under circulating condition in annulus to its density equivalent it will be called ECD\n\nDividing the above equation by 0.052*TVD on both sides :-\n\nECD= (Pressure under static condition + Annular pressure loss)/(0.052*TVD)\n\nECD=MW+ Annular pressure loss/(0.052*TVD) using (Pressure under static condition=0.052*TVD*MW)\n\nDuring trips (up/down) drill string acts as a large piston, when moving down it increases the pressure below the drill string and forces the drilling fluid into the formation which is termed as surge. Similarly while moving up there is low pressure zone created below the drill string which sucks the formation fluid into the wellbore which is called swab.\n\nThe total pressure acting on the wellbore is affected by pipe movement upwards or downwards.Tripping pipe into and out of a well is one other common operation during completions and workovers. Unfortunately, statistics indicate that most kicks occur during trips. Therefore, understanding the basic concepts of tripping is a major concern in completion/workover operations.\nDownward movement of tubing(tripping in) creates a pressure that is exerted on the bottom of a well. As the tubing is being run into a well, the fluid in the well must move upward to exit the volume being entered by the tubing.The combination of the downward movement of the tubing and the upward movement of the fluid (or piston effect) results in an increase in pressure at any given point in the well. This increase in pressure is commonly called Surge pressure.\nUpward movement of the tubing(tripping out) also affects the pressure which is imposed at the bottom of the well. When pulling pipe from the well, fluid must move downward and replace the volume which was occupied by the tubing. The net effect of the upward movement of the tubing and the downward movement of the fluid creates a decrease in bottomhole pressure. This decrease in pressure is referred to as Swab pressure.\nBoth surge and swab pressures are affected by the following parameters:\n\nThe faster pipe is tripped, the higher the surge and swab pressure effects will be. Also, the greater the fluid density, viscosity and gel strength, the greater the surge and swab tendency. Finally, the downhole tools such as packers and scrapers,which have small annular clearance, also increase surge and swab pressure effects.\nDetermination of actual surge and swab pressures can be accomplished with the use of WORKPRO and DRILPRO calculator programs or hydraulics manuals.\n\nIn well control,it is defined as the difference between the formation pressure and the bottomhole hydrostatic pressure. These are classified as overbalanced, underbalanced and balanced.\n\nIt means the hydrostatic pressure exerted on the bottom of the hole is greater than the formation pressure. i.e. HP > FP\n\nIt means the hydrostatic pressure exerted on the bottom of the hole is less than the formation pressure. i.e. HP < FP\n\nIt means the hydrostatic pressure exerted on the bottom of the hole is equal to the formation pressure. i.e. HP = FP\n\nCuttings are rock fragments chipped, scraped or crushed away from a formation by the action of the bit. The size, shape and amount of cuttings depend largely on formation type, weight on the bit, bit dullness and the pressure differential (formation versus fluid hydrostatic pressures).\nThe size of the cuttings usually decreases as the bit dulls during drilling if weight on bit, formation type and the pressure differential, remain constant. However, if the pressure differential changes (formation pressure increase),even a dull bit could cut more effectively, and the size, shape and amount of cuttings could increase.\n\nKick is defined as an undesirable influx of formation fluid into the wellbore. If left unchecked, a kick can develop into blowout (an uncontrolled influx of formation fluid in to the wellbore).The result of failing to control a kick leads to loss operation time, loss of well and quite possibly, the loss of the rig and lives of personnel.\n\nOnce the hydrostatic pressure is less than the formation pore pressure, formation fluid can flow into the well. This can happen when one or a combination of the following occurs;\n\nWhen tripping out of the hole, the volume of the steel pipe being removed results in a corresponding decrease in wellbore fluid. Whenever the fluid level in the hole decreases, the hydrostatic pressure exerted by the fluid also decreases and if the decrease in hydrostatic pressure falls below the formation pore pressure, the well may flow. Therefore, the hole must be filled to maintain sufficient hydrostatic pressure to control formation pressure.\nDuring tripping, the pipe could be dry or wet depending on the conditions. The API7G illustrates the methodology for calculating accurate pipe displacement and gives correct charts and tables.\nTo calculate the volume to fill the well when tripping dry pipe out is given as;\n\n\"Barrel to fill=pipe displacement(bbl/ft) × length pulled (ft)\"\n\nTo calculate the volume to fill the well when tripping wet pipe out is given as;\n\n\"Barrel to fill=( pipe displacement(bbls/ft) + pipe capacity(bbls/ft) )×length pulled(ft)\"\n\nIn some wells, monitoring fill –up volumes on trips can be complicated by loss through perforations.The wells may stand full of fluid initially, but over a period of time the fluid seeps in to the reservoir.In such wells, the fill up volume will always exceed the calculated or theoretical volume of the steel removed from the well.\nIn some fields, wells have low reservoir pressures and will not support a full column of fluid.In these wells filling the hole with fluid is essentially impossible unless sort of bridging agent is used to temporarily bridge off the subnormally pressured zone.The common practice is to pump the theoretical fill up volume while pulling out of the well.\n\nThe mud in the wellbore must exert enough hydrostatic pressure to equal the formation pore pressure. If the fluid’s hydrostatic pressure is less than formation pressure the well can flow.The most common reason for insufficient fluid density is drilling into unexpected abnormally pressured formations. This situation usually arises when unpredicted geological conditions are encountered. Such as drilling across a fault that abruptly changes the formation being drilled.\nMishandling mud at the surface accounts for many instances of insufficient fluid weight. Such as opening wrong valve on the pump suction manifold and allowing a tank of light weight fluid to be pumped; bumping the water valve so more is added than intended; washing off shale shakers; or clean-up operations. All of these can affect mud weight.\n\nSwabbing is as a result of the upward movement of pipe in a well and results in a decrease in bottomhole pressure. In some cases, the bottomhole pressure reduction can be large enough to cause the well to go underbalanced and allow formation fluids to enter the wellbore. The initial swabbing action compounded by the reduction in hydrostatic pressure(from formation fluids entering the well) can lead to a significant reduction in bottomhole pressure and a larger influx of formation fluids. Therefore, early detection of swabbing on trips is critical to minimizing the size of a kick.\nMany wellbore conditions increase the likelihood of swabbing on a trip. Swabbing (piston) action is enhanced when pipe is pulled too fast. Poor fluid properties, such as high viscosity and gel strengths, also increase the chances of swabbing a well in. Additionally, large outside diameter (OD) tools (packers, scrapers, fishing tools, etc.) enhance the piston effect.\nThese conditions need to be recognized in order to decrease the likelihood of swabbing a well in during completion/workover operations. As mentioned earlier, there are several computer and calculator programs that can estimate surge and swab pressures. Swabbing is detected by closely monitoring hole fill-up volumes during trips. For example, if three barrels of steel (tubing) are removed from the well and it takes only two barrels of fluid to fill the hole, then a one barrel kick has probably been swabbed into the wellbore. Special attention should be paid to hole fill-up volumes since statistics indicate that most kicks occur on trips.\n\nAnother cause of kick during completion/workover operations is lost circulation. Loss of\ncirculation leads to a drop of both the fluid level and hydrostatic pressure in a well. If the\nhydrostatic pressure falls below the reservoir pressure, the well kicks. Three main causes of lost circulation are:\n\nIn case of drilling a wildcat or exploratory well(often the formation pressures are not known accurately) the bit suddenly penetrates into an abnormal pressure formation resulting the hydrostatic pressure of mud become less than the formation pressure and cause a kick.\n\nWhen the gas is circulated to the surface, it expands and reduces the hydrostatic pressure sufficient to allow a kick. Although the mud density is reduced considerably at the surface, the hydrostatic pressure is not reduced significantly since the gas expansion occurs near surface and not at the bottom.\n\nThe fourth cause of kick is poor well planning. The mud and casing programs have a great bearing on well control. These programs must be flexible enough to allow progressively deeper casing strings to be set; otherwise a situation may arise where it is not possible to control kicks or lost circulation. Well control is an important part of well planning.\n\n During drilling operations, kicks are usually killed using the Driller’s, Engineer’s or a combination of both called Concurrent Method while forward circulating. The selection of which to use will depend upon the amount and type of kick fluids that have entered the well, the rig's equipment capabilities, the minimum fracture pressure in the open hole, and the drilling and operating companies well control policies.\nFor workover or completion operations, other methods are often used. . Bullheading is a common way to kill a well during workovers and completions operations but is not often used for drilling operations. Reverse circulation is another kill method used for workovers that is not used for drilling.\n\nThe aim of oil operations is to complete all tasks in a safe and efficient manner without detrimental effects to the environment. This aim can only be achieved if control of the well is maintained at all times. The understanding of pressure and pressure relationships is important in preventing blowouts. Blowouts are prevented by experienced personnel that are able to detect when the well is kicking and take proper and prompt actions to shut-in the well.\n\n"}
{"id": "2887461", "url": "https://en.wikipedia.org/wiki?curid=2887461", "title": "Wishbone (computer bus)", "text": "Wishbone (computer bus)\n\nThe Wishbone Bus is an open source hardware computer bus intended to let the parts of an integrated circuit communicate with each other. The aim is to allow the connection of differing cores to each other inside of a chip. The Wishbone Bus is used by many designs in the OpenCores project.\n\nA large number of open-source CPU designs and auxiliary computer peripherals have been released with Wishbone interfaces. Many can be found at OpenCores, a foundation that attempts to make open-source hardware designs available.\n\nWishbone is intended as a \"logic bus\". It does not specify electrical information or the bus topology. Instead, the specification is written in terms of \"signals\", clock cycles, and high and low levels.\n\nThis ambiguity is intentional. Wishbone is made to let designers combine several designs written in Verilog, VHDL or some other logic-description language for electronic design automation. Wishbone provides a standard way for designers to combine these hardware logic designs (called \"cores\").\nWishbone is defined to have 8, 16, 32, and 64-bit buses. All signals are synchronous to a single clock but some slave responses must be generated combinatorially for maximum performance. Wishbone permits addition of a \"tag bus\" to describe the data. But reset, simple addressed reads and writes, movement of blocks of data, and indivisible bus cycles all work without tags.\nWishbone is open source, which makes it easy for engineers and hobbyists to share public domain designs for hardware logic on the Internet. To prevent preemption of its technologies by aggressive patenting, the Wishbone specification includes examples of prior art, to prove its concepts are in the public domain.\n\nA device does not \"conform\" to the Wishbone specification unless it includes a \"data sheet\" that describes what it does, bus width, utilization, etc. Promoting reuse of a design requires the data sheet. Making a design reusable in turn makes it easier to share with others.\n\nThe Simple Bus Architecture is a simplified version of the Wishbone specification.\n\nWishbone adapts well to common topologies such as point-to-point, many-to-many (i.e. the classic bus system), hierarchical, or even switched fabrics such as crossbar switches. In the more exotic topologies, Wishbone requires a bus controller or arbiter, but devices still maintain the same interface.\n\nWishbone Control Signals Compared to Other SOC Bus Standards:\n\n\n"}
{"id": "9269786", "url": "https://en.wikipedia.org/wiki?curid=9269786", "title": "World Habitat Awards", "text": "World Habitat Awards\n\nThe World Habitat Awards were established in 1985 by the Building and Social Housing Foundation as part of its contribution to the United Nations' International Year of Shelter for the Homeless in 1987.\n\nTwo awards are given annually to projects from the Global South as well as the North that provide practical, innovative and sustainable solutions to current housing needs, which are capable of being transferred or adapted for use elsewhere. \n\nThe World Habitat Awards celebrate projects & approaches that:\n"}
