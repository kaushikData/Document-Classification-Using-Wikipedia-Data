{"id": "18402617", "url": "https://en.wikipedia.org/wiki?curid=18402617", "title": "Artificial psychology", "text": "Artificial psychology\n\nArtificial psychology is a theoretical discipline proposed by Dan Curtis (b. 1963). The theory considers the situation when an artificial intelligence approaches the level of complexity where the intelligence meets two conditions:\n\nCondition I\n\nCondition II\n\nWhen both conditions are met, then, according to this theory, the possibility exists that the intelligence will reach irrational conclusions based on real or created information. At this point, the criteria is met for intervention which will not necessarily be resolved by simple re-coding of processes due to extraordinarily complex nature of the codebase itself; but rather a discussion with the intelligence in a format which more closely resembles classical (human) psychology.\n\nIf the intelligence cannot be reprogrammed by directly inputting new code, but requires the intelligence to reprogram itself through a process of analysis and decision based on information provided by a human, in order for it to overcome behavior which is inconsistent with the machines purpose or ability to function normally, then artificial psychology is by definition, what is required.\n\nThe level of complexity that is required before these thresholds are met is currently a subject of extensive debate. The theory of artificial psychology does not address the specifics of what those levels may be, but only that the level is sufficiently complex that the intelligence cannot simply be recoded by a software developer, and therefore dysfunctionality must be addressed through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious.\n\nAs of 2015, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can even be tested, and therefore, artificial psychology remains a largely theoretical discipline.\n"}
{"id": "58841118", "url": "https://en.wikipedia.org/wiki?curid=58841118", "title": "Automated Guideway Transit System project (Philippines)", "text": "Automated Guideway Transit System project (Philippines)\n\nThe Philippine government has commenced a project to develop a locally-designed and manufactured Automated Guideway Transit System (AGTS) through its Department of Science and Technology (DOST). Two prototype lines has been set up by the DOST, one within the University of the Philippines Diliman campus and another in Bicutan in Taguig.\n\nThe Philippine government through the Department of Science and Technology (DOST) has made efforts to develop a locally designed Automated Guideway Transit System (AGTS) to curb heavy traffic and air pollution in Metro Manila. The DOST's AGTS was conceptualized to be similar to that of a monorail system but at one-fifth of its cost. AGTS lines are intended to be set up in Metro Manila to augment or serve as a \"feeder\" to the existing Manila Metro Rail Transit System. A prototype system was developed by the DOST with the University of the Philippines Diliman as a means to demonstrate AGT technology as an efficient mode of transport for the country.\n\nThe project is part of the DOST's broader High-Impact Technology Solutions (HITS) project. The leading implementing agency is the Metals Industry Research and Development Center (MIRDC) Other child agencies of the DOST involved are the Philippine Council for Industry, Energy and Emerging Technology Research and Development (PCIEERD) which funded the project under the Makina at Teknolohiya para sa Bayan (MakiBayan) Program of the MIRDC, and Project Management and Engineering Design Services Office (PMEDSO).\n\nThe UP National Center for Transportation Studies, College of Engineering, and the National Institute of Geological Sciences are also involved in the project as consultants to the DOST.\n\nThe project’s lead implementing agency in DOST— the Metals Industry Research and Development Center (MIRDC)—constructed the train’s main mechanical frameworks or “rolling stocks”, and subcontracted local companies and Fil-Asia Automotive to construct the guideway and the coaches, respectively, based on the design team’s specifications.\n\nA prototype elevated line, which ran from CP Garcia Street to Jacinto Street near the College of Fine Arts was first established within the University of the Philippines Diliman campus in Quezon City. The first test run for the AGTS was made at this line on December 14, 2012.\n\nThe Metals Industry Research and Development Center constructed the rolling stocks or main mechanical frameworks used for the DOST's AGTS. Construction of the coaches were also subcontracted to Fil-Asia Automotive.\n\nAt least two rolling stock prototypes has been developed as part of the AGTS project of the DOST, the AGT 30 and the AGT 120. The former was used in the University of the Philippines line while the latter was used in the Bicutan line.\n\nThe AGT 30, can carry 30 passenger per coach and has two passenger sections with safety features. It also has a communication and automated fare collection system. The prototype has two coaches with full airconditioning and has a suspension system, solid tires for guide wheels and extra wide automatic sliding doors. The full load top speed of each coach is . The AGT 120 has a larger capacity, with its capacity of 120 passenger per coach. The target maximum speed of each coach is .\n\nDOST's AGTS was initially conceptualized as a monorail in 2011 but modifications made to the prototype has revised the project. The current design of the AGTS makes use of a railroad track with two rails instead of only one rail used in monorails.Miescor Builders, a subcontractor of the MIRDC, was responsible for the guideway.\n\n"}
{"id": "574185", "url": "https://en.wikipedia.org/wiki?curid=574185", "title": "Bertha Benz", "text": "Bertha Benz\n\nBertha Benz (, born Bertha Ringer, 3 May 1849 – 5 May 1944) was a German automotive pioneer. She was the business partner and wife of automobile inventor Karl Benz. On 5 August 1888, she was the first person to drive an automobile over a long distance. In doing so, she brought the Benz Patent-Motorwagen worldwide attention and got the company its first sales.\n\nBertha Ringer was born in 1849 to a wealthy family in Pforzheim, Grand Duchy of Baden.\n\nTwo years before her marriage to Karl Benz, she used part of her dowry to invest in his failing iron construction company. As an unmarried woman, she was able to do so; after she married Benz, according to German law, Bertha lost her legal power to act as an investor. On 20 July 1872, Bertha Ringer married Karl Benz. As he moved on to a new manufacturing venture, Benz & Cie, he continued to use her dowry as financial support. He finished his work on his first horseless carriage in December 1885. Although Bertha financed the development process, and would hold patent rights under modern law, as a married woman she was not allowed to apply for the patent.\n\nTogether they had five children: Eugen (1873–1958), Richard (1874–1955), Clara (1877–1968), Thilde (1882–1974), and Ellen (1890–1973).\n\nIn 1886, Benz presented the Patent-Motorwagen automobile to the world. Within the decade, 25 vehicles had been built. With cutting-edge bicycle constructions, the Model I was the original Patent Motor Car and the world's first automobile.\n\nThe Model II was converted to a four-wheeler for test purposes, making it the only one of this model.\n\nThe first Patent Motor Car sold in small production runs was the Model III. It had powered rear wheels with a ringed steel and solid rubber, steerable front wheel. Various options from which to choose were provided for customers, such as seat arrangements and a folding top.\n\nOn 5 August 1888, 39-year-old Bertha Benz drove from Mannheim to Pforzheim with her sons Richard and Eugen, thirteen and fifteen years old respectively, in a Model III, without telling her husband and without permission of the authorities, thus becoming the first person to drive an automobile a significant distance, though illegally. Before this historic trip, motorized drives were merely very short trials, returning to the point of origin, made with assistance of mechanics. Following wagon tracks, this pioneering tour covered a one-way distance of about .\nAlthough the ostensible purpose of the trip was to visit her mother, Bertha Benz had other motives — to prove to her husband, who had failed to adequately consider marketing his invention, that the automobile in which they both had heavily invested would become a financial success once it was shown to be useful to the general public; and to give her husband the confidence that his constructions had a future.\n\nShe left Mannheim around dawn, solving numerous problems along the way. Bertha demonstrated her significant technical capabilities on this journey. With no fuel tank and only a 4.5-litre supply of petrol in the carburetor, she had to find ligroin, the petroleum solvent needed for the car to run. It was only available at apothecary shops, so she stopped in Wiesloch at the city pharmacy to purchase the fuel. At the time, petrol and other fuels could only be bought from chemists, and so this is how the chemist in Wiesloch became the first fuel station in the world.\n\nShe cleaned a blocked fuel line with her hat pin and used her garter as insulation material. A blacksmith had to help mend a chain at one point. When the wooden brakes began to fail, Benz visited a cobbler to install leather, making the world's first pair of brake pads. An evaporative cooling system was employed to cool the engine, making water supply a big worry along the trip. The trio added water to their supply every time they stopped. The car's two gears were not enough to surmount uphill inclines and Eugen and Richard often had to push the vehicle up steep roads. Benz reached Pforzheim somewhat after dusk, notifying her husband of her successful journey by telegram. She drove back to Mannheim several days later.\nThe novel trip received a great deal of publicity, as she had sought. The drive was a key event in the technical development of the automobile. The pioneering couple introduced several improvements after Bertha's experiences. She reported everything that had happened along the way and made important suggestions, such as the introduction of an additional gear for climbing hills and brake linings to improve brake-power. Her trip proved to the burgeoning automotive industry that test drives were essential to their business.\n\nIn 1944, on her 95th birthday, Bertha Benz was honoured with the title \"Honourable Senator\", by the Technical University of Karlsruhe. This is her husband's alma mater and they had awarded an honorary doctorate degree to him in his lifetime. Two days later, Bertha Benz died in her villa in Ladenburg, where the workshop of Karl Benz had been built after they had moved there in 1906 and he established a solely family-held business, Benz and Sons. Karl Benz later wrote the following in his memoirs: \"Only one person remained with me in the small ship of life when it seemed destined to sink. That was my wife. Bravely and resolutely she set the new sails of hope.\"\n\nIn 2008, the Bertha Benz Memorial Route was officially approved as a route of the industrial heritage of mankind, because it follows Bertha Benz's path during the world's first long-distance journey by automobile in 1888. Now it is possible to follow the 194 km of signs indicating her route from Mannheim via Heidelberg to Pforzheim (Black Forest) and back.\n\nThe \"Bertha Benz Challenge\", embedded in the framework of the ceremony of \"Automobile Summer 2011\", the big official German event and birthday party commemorating the invention of the automobile by Karl Benz over , took place on Bertha Benz Memorial Route on 10 and 11 September 2011. It was a globally visible signal for new automobile breakthroughs, and was only open for sustainable mobility — future-oriented vehicles with alternative drive systems, \"i.e.\", hybrid and electric, hydrogen and fuel cell vehicles, and other extremely economical vehicles. The motto is \"Bertha Benz Challenge – Sustainable Mobility on the World's Oldest Automobile Road!\".\n\nOn 25 January 2011 Deutsche Welle (DW-TV) broadcast worldwide in its series, \"Made in Germany\", a TV documentary on the invention of the automobile by Karl Benz, highlighting the very important role of his wife, Bertha Benz. The report is not only on the history of the automobile, but takes a look at its future as well, shown by the Bertha Benz Challenge on 10 and 11 September 2011.\n\nThe documentary \"The Car is Born\", produced by Ulli Kampelmann, centered on the first road trip by Bertha Benz.\n\nIn 2011, a television movie about the life of Karl and Bertha Benz was made, titled \"Karl & Bertha\", which premiered on 11 May and was aired by Das Erste on 23 May. A trailer of the movie and a \"making of\" special were released on YouTube.\n\nIn 2018, a short video depicting Bertha's first trip was published by Mercedes-Benz USA.\n\n"}
{"id": "25205346", "url": "https://en.wikipedia.org/wiki?curid=25205346", "title": "Black hole starship", "text": "Black hole starship\n\nA black hole starship is a theoretical idea for enabling interstellar travel by propelling a starship by using a black hole as the energy source. The concept was first discussed in science fiction, notably in the book \"Imperial Earth\" by Arthur C. Clarke, and in the work of Charles Sheffield, in which energy extracted from a Kerr-Newman black hole is described as powering the rocket engines in the story \"Killing Vector\" (1978).\n\nIn a more detailed analysis, a proposal to create an artificial black hole and using a parabolic reflector to reflect its Hawking radiation was discussed in 2009 by Louis Crane and Shawn Westmoreland. Their conclusion was that it was on the edge of possibility, but that quantum gravity effects that are presently unknown will either make it easier, or make it impossible. Similar concepts were also sketched out by Bolonkin.\n\nAlthough beyond current technological capabilities, a black hole starship offers some advantages compared to other possible methods. For example, in nuclear fusion or fission, only a small proportion of the mass is converted into energy, so enormous quantities of material would be needed. Thus, a nuclear starship would greatly deplete Earth of fissile and fusile material. One possibility is antimatter, but the manufacturing of antimatter is hugely energy-inefficient, and antimatter is difficult to contain. The Crane and Westmoreland paper states:\n\nAccording to the authors, a black hole to be used in space travel needs to meet five criteria:\n\n\nBlack holes seem to have a sweet spot in terms of size, power and lifespan which is almost ideal. A black hole weighing 606,000 metric tons (6.06 × 10 kg) would have a Schwarzschild radius of 0.9 attometers (0.9 × 10 m, or 9 × 10 m), a power output of 160 petawatts (160 × 10 W, or 1.6 × 10 W), and a 3.5-year lifespan. With such a power output, the black hole could accelerate to 10% the speed of light in 20 days, assuming 100% conversion of energy into kinetic energy. Assuming only 10% conversion into kinetic energy would only take 10 times longer to accelerate to 0.1\"c\" (10% of the speed of light).\n\nGetting the black hole to act as a power source and engine also requires a way to convert the Hawking radiation into energy and thrust. One potential method involves placing the hole at the focal point of a parabolic reflector attached to the ship, creating forward thrust. A slightly easier, but less efficient method would involve simply absorbing all the gamma radiation heading towards the fore of the ship to push it onwards, and let the rest shoot out the back. This would, however, generate an enormous amount of heat as radiation is absorbed by the dish.\n\nIt is not clear that a starship powered by Hawking radiation can be made feasible within the laws of known physics. In the standard black hole thermodynamic model, the average energy of emitted quanta increases as size decreases, and extremely small black holes emit the majority of their energy in particles other than photons. In the Journal of the British Interplanetary Society, Jeffrey S. Lee of Icarus Interstellar states a typical quantum of radiation from a one-attometer black hole would be too energetic to be reflected. Lee further argues absorption (for example, by pair production from emitted gamma rays) may also be infeasible: A titanium \"Dyson cap\", optimized at 1 cm thickness and a radius around 33 km (to avoid melting), would absorb almost half the incident energy, but the maximum spaceship velocity over the black hole lifetime would be less than 0.0001\"c\" (about 30 km/s), according to Lee's calculations.\n\nGovind Menon of Troy University suggests exploring the use of a rotating (Kerr-Newmann) black hole instead: \"With non-rotating black holes, this is a very difficult thing...we typically look for energy almost exclusively from rotating black holes. Schwarzschild black holes do not radiate in an astrophysical, gamma ray burst point of view. It is not clear if Hawking radiation alone can power starships.\"\n\n\n"}
{"id": "2351828", "url": "https://en.wikipedia.org/wiki?curid=2351828", "title": "Broadcast Wave Format", "text": "Broadcast Wave Format\n\nBroadcast Wave Format (BWF) is an extension of the popular Microsoft WAV audio format and is the recording format of most file-based non-linear digital recorders used for motion picture, radio and television production. It was first specified by the European Broadcasting Union in 1997, and updated in 2001 and 2003.\n\nThe purpose of this file format is the addition of metadata to facilitate the seamless exchange of sound data between different computer platforms and applications. It specifies the format of metadata, allowing audio processing elements to identify themselves, document their activities, and supports timecode to enable synchronization with other recordings. This metadata is stored as extension chunks in a standard digital audio WAV file.\n\nFiles conforming to the Broadcast Wave specification have names ending with the file extension .WAV.\n\nIn addition to the common WAVE chunks, the following extension chunks can appear in a Broadcast Wave file:\n\nSince the only difference between a BWF and a \"normal\" WAV is the extended information in the file header (Bext-Chunk, Coding-History, etc...), a BWF does not require a special player for playback.\n\nUnfortunately, this compatibility also preserves the filesize limitation that WAV files have (4 GB of audio data per data chunk).\nIn order to be able to store audio which would exceed this limit, 2 different chunks exist allowing the audio material to be spread across several files: \"cont\" & \"link\" (see list above)\n\nSince there is no official naming convention for these subsequent files, and it is still desirable to see at a glance which ones belong to a continuous piece of audio, a lot of programs apply a numbering scheme to the file suffix: \".wav, .w01, .w02, ..., .wNN\".\n\nEach of those segments is a regular Wave/BWF file, but players that are aware of the continue/link chunk will treat all segments as one single, long piece of audio when opening the first segment \".wav\".\n\nAs an extension, RF64 is a BWF-compatible multichannel file format enabling file sizes to exceed 4 GB that has been specified in 2006.\n\nThe axml chunk allows users to incorporate data compliant with the XML format with the audio; the chunk may contain data fragments from one or more schema.\n\nOn August 2012, the European Broadcasting Union published a specification for embedding International Standard Recording Code (ISRC) in the axml chunk of the Broadcast Wave Format.\n\nBWF is specified for use in MXF by SMPTE standard 382.\n\nBWF is specified for use in AES31.\n\n\n"}
{"id": "14415076", "url": "https://en.wikipedia.org/wiki?curid=14415076", "title": "CEATEC", "text": "CEATEC\n\nCombined Exhibition of Advanced Technologies (also known as CEATEC) is an annual trade show in Japan. It is regarded as the Japanese equivalent of Consumer Electronics Show. It is Japan's largest IT and electronics exhibition and conference.\n\nThe show is held every year in October in Makuhari Messe since the first CEATEC in 2000. The most recent exhibition, the ninth iteration of CEATEC, was held between September 30 to October 4, 2008. CEATEC Japan 2008, attracted nearly 197,000 visitors, with some 804 exhibitors and 138 seminars. Attendance was down from the almost 206,000 people and 895 companies in 2007 as it apparently felt the crunch of the financial crisis of 2007–2010.\n\nSony shown off the PSX\nSony and Panasonic announced devices that could play as well as record shows in Blu-ray Disc format.\n\nNTT DoCoMo will show their latest technologies,\nwhile both Sharp and SEGA mentioned about attending in their Twitter posts.\n\n\n"}
{"id": "2900082", "url": "https://en.wikipedia.org/wiki?curid=2900082", "title": "CEITEC", "text": "CEITEC\n\nThe Centro Nacional de Tecnologia Electrônica Avançada S.A (CEITEC - National Center for Advanced Electronic Technology) is a Brazilian technology center specialized in project development and fabrication in microelectronics, i.e. integrated circuits, or \"chips\". This center is one of the agents for the Brazilian Microelectronics Program (PNM - \"Programa Nacional de Microelectrônica\").\n\nThe main objective of CEITEC is to provide an incentive to the production of semiconductor components and microelectronics education. This project is part of the technology and industrial development policy of the federal Brazilian government. The initiative is a result of a partnership between governmental entities, universities, research centers and companies and will be operating in Porto Alegre, state of Rio Grande do Sul in a 13,650 m² facility divided into:\n\n\nChip do Boi is CEITEC S.A.’s first commercial product and is the first chip designed in Brazil by Brazilian engineers in a company that was home grown and home funded to reach volume production at a world-class semiconductor manufacturing facility. This IC device is used for cattle tracking – essentially an electronic cow ID. Embedded in an ear ring, the chip can be read within a distance of 50 cm. The chip was designed in entirely in Brazil and is competitively priced. This device is being manufactured at X-Fab Silicon Foundries in Germany.This company has been chosen by CEITEC to produce the Chip do Boi design at its facilities because X-Fab CMOS 0.6 micron technology is the same that will soon be available in CEITEC’s own factory that will be the first of its kind in Latin America to manufacture RF analog/digital products. It can produce up to 100 million chips per year using 6-inch wafers.\n\nCEITEC’s Chip do Boi is an advanced LF-RFID device designed for use in Brazil’s cattle industry as part of a leading-edge system to track livestock. Forecasted domestic demand for the chip is as high as 1.5 million units for 2012 with an expected minimum growth rate of 10 percent a year over the next decade.\n\n\nThe chip contains all vehicle data, including chassis and national registration numbers, taxes and fines not paid, etc.\n\n\nThe chip is embedded in an electronic tag. It ensures the traceability and safety of blood products.\n\nThe Design House worked on projects funded by FINEP.\n\n\n\n"}
{"id": "12549121", "url": "https://en.wikipedia.org/wiki?curid=12549121", "title": "CMD640", "text": "CMD640\n\nCMD640, the California Micro Devices Technology Inc product 0640, is an IDE interface chip for the PCI and VLB buses. CMD640 had some sort of hardware acceleration: WDMA and Read-Ahead (prefetch) support.\n\nCMD Technology Inc was acquired by Silicon Image Inc. in 2001.\n\nThe original CMD640 has a data corruption bug. The data corruption bug is similar to the bug affecting the contemporaneous PC Tech RZ1000 chipset. Both chipsets were used on a number of motherboards, including those from Intel.\n\nМodern operating systems have a workaround for this bug by prohibiting aggressive acceleration mode and losing about 10% of the performance.\n\n"}
{"id": "10425639", "url": "https://en.wikipedia.org/wiki?curid=10425639", "title": "CMD Group", "text": "CMD Group\n\nCMD Group, formerly Reed Construction Data and Construction Market Data, is a provider of business information for the North American construction industry. CMD is owned by Warburg Pincus (51%) and Reed Business Information (49%). Its historical roots lie in Construction Market Data, founded in 1982 to publish construction leads and market data. In 2000, London-based Reed Elsevier purchased this original CMD Group, transitioning the company to Reed Construction Data.\n\nIn October 2014 private equity firm Warburg Pincus in New York purchased a majority stake in the company, and Reed Construction Data changed its name to CMD. The Norcross, Ga.-based provider of North American construction data said the new name is a nod to the company’s original name: Construction Market Data. The new brand includes an updated logo and website.\n\nThe company tracks data on hundreds of thousands of projects per year, providing coverage of construction projects in both the United States and Canada. The company provides monthly analysis and data for all aspects of the construction industry. CMD also provides a detailed view of construction activity, including historical data, current-year projections and a five-year forecast. Their research helps customers forecast to find those market segments experiencing the greatest growth and plan tactical marketing strategies.\n\nIn October 2009, Reed Construction Data (which is now Construction Market Data a division of ConstructCONNECT) filed suit in federal court against McGraw-Hill Construction, charging that the company's Dodge Report had unlawfully accessed confidential and trade secret information from Reed since 2002 by using a series of fake companies to pose as Reed customers.\nThe lawsuit, filed in the U.S. District Court for the Southern District of New York, seeks an unspecified amount in lost profits and punitive damages, trial by jury, and injunctive relief as a result of Dodge’s misuse of proprietary construction project information, and that Dodge allegedly manipulated the information to create misleading comparisons between Dodge’s and Reed’s products and services in an effort to mislead the marketplace.\n\nIn 2016, CMD Group became a part of ConstructConnect as part of a merger with iSqFt, BidClerk, Construction Data.\n\n"}
{"id": "3734574", "url": "https://en.wikipedia.org/wiki?curid=3734574", "title": "Cold roll laminator", "text": "Cold roll laminator\n\nCold roll laminators use a plastic film which is coated with an adhesive and glossy backing which does not adhere to the glue. When the glossy backing is removed, the adhesive is exposed, which then sticks directly onto the item which needs to be laminated. This method, apart from having the obvious benefit of not requiring expensive equipment, is also suitable for those items which would be damaged by heat. Cold laminators range from simple two roller, hand crank machines up to large and complex motor driven machines with high precision rollers, adjustable roller pressure and other advanced features.\n\nCold lamination increased in popularity with the rise of wide format inkjet printers, which often used inks and papers incompatible with hot lamination. A large percentage of cold laminate for use in the print industry is PVC, although a wide range of other materials are available. Cold laminating processes are also used outside of the print industry, for example coating sheet glass or stainless steel with protective films.\n\nCold roll laminators are also used for laying down adhesive films in the sign making industry, for example mounting a large print onto a board. A practiced operator can apply a large adhesive sheet in a fraction of the time it takes to do so by hand.\n\n"}
{"id": "3648410", "url": "https://en.wikipedia.org/wiki?curid=3648410", "title": "Emmett Leith", "text": "Emmett Leith\n\nEmmett Norman Leith (March 12, 1927 in Detroit, Michigan – December 23, 2005 in Ann Arbor, Michigan) was a professor of electrical engineering at the University of Michigan and, with Juris Upatnieks of the University of Michigan, the co-inventor of three-dimensional holography.\n\nLeith received his B.S. in physics from Wayne State University in 1949 and his M.S. in physics in 1952. He received his Ph.D. in electrical engineering from Wayne State in 1978. Much of Leith's holographic work was an outgrowth of his research on synthetic aperture radar (SAR) performed while a member of the Radar Laboratory of the University of Michigan's Willow Run Laboratory beginning in 1952. Leith joined the University of Michigan as a research assistant and was promoted to graduate research assistant in 1955, research associate in 1956, research engineer in 1960, associate professor in 1965, and full professor in 1968.\n\nProfessor Leith and his coworker Juris Upatnieks at the University of Michigan displayed the world's first three-dimensional hologram at a conference of the Optical Society of America in 1964.\n\nHe received the 1960 IEEE Morris N. Liebmann Memorial Award and the Stuart Ballantine Medal in 1969. In 1975 he was awarded the William F. Meggers Award by the Optical Society. In 1979, President Jimmy Carter awarded Leith with the National Medal of Science for his research. He was awarded the 1985 Frederic Ives Medal by the OSA.\n\n"}
{"id": "52023575", "url": "https://en.wikipedia.org/wiki?curid=52023575", "title": "Episerver", "text": "Episerver\n\nEpiserver is a global software company offering web content management (WCM) (or CMS), digital commerce, and digital marketing, through the Episerver Digital Experience Cloud software platform. Episerver also offers personalization through Peerius, a company acquired in August 2016.\n\nEpiserver was founded in 1994 in Stockholm, Sweden, by Mikael Runhem. Then known as Elektropost Stockholm AB, the company focused on internet-based electronic mail. Elektropost Stockholm AB expanded to provide technology for building websites and introduced the first version of the EPiServer CMS platform in 1997. \n\nIn 2002, the first version of EPiServer CMS that was based on Microsoft’s .NET Framework, EPiServer CMS 4.0, was launched. In 2006, Mikael Runhem changed the company’s name to EPiServer AB. In 2007, the second .NET-based version of EPiServer CMS was launched, version 5.\n\nIn 2010, the company was owned by a group of investors, including Amadeus Capital, Martin Bjäringer, Monterro Holdings, Northzone Ventures, Mikael Runhem and family and employees. In November 2010, this group sold the company to IK Investment Partners.\n\nIn December 2014, IK Investment Partners sold Episerver to Accel-KKR, a technology-focused private equity investment firm. Accel-KKR had also recently purchased Ektron, a Nashua, New Hampshire-based CMS company. In January 2015, Episerver and Ektron merged. Two former executives from KANA Software, CEO Mark Duffell and CMO James Norwood, were appointed President and CEO and Executive Vice President Strategy and CMO of Episerver, respectively. Martin Henricson, CEO of the former Episerver business assumed the role of Executive Chairman for the merged entity.\n\nThe company combined its software into the Episerver Digital Experience Cloud, and in June 2015, Episerver launched the first major release of its platform following the Ektron merger. In November 2015, the company rebranded itself, changing EPiServer to Episerver, and including the shortened name “Epi”.\n\nIn August 2016, Episerver acquired Peerius, a London UK-based commerce personalization company. The brand now is fully integrated as EpiPersonalization into the Episerver Platform. Former Peerius CEO Roger Brown left the company.\n\n\n"}
{"id": "56776732", "url": "https://en.wikipedia.org/wiki?curid=56776732", "title": "Fiber optic display", "text": "Fiber optic display\n\nA fiber optic display is a light-emitting display that uses fiber optics to display images, text, and notification lights. Fiber-optic displays can either be static or dynamic, with the typical lighting source being halogen light bulbs.\n\nStatic fiber optic displays have been commonly used for some types of traffic signals. One common use for static fiber optic displays are lane control lights, which display either a green downward-pointing arrow or a red X to indicate the open/closed status of road lanes.\n\nDynamic fiber optic displays typically display alphanumeric text, and utilize electromechanical shutters to open or close the ends of the fiber strands to display an alphanumeric pixel. These type of displays were commonly used as variable-message signs on highways. Compared to eggcrate displays, dynamic fiber optic displays offered lower energy consumption due to requiring fewer bulbs, and offered improved nighttime legibility. For daytime legibility, they were sometimes combined with flip-disc displays to be reflective in daylight and emissive at night.\n"}
{"id": "36535684", "url": "https://en.wikipedia.org/wiki?curid=36535684", "title": "Glossary of engineering", "text": "Glossary of engineering\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\n\nThis glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.\n\n \n\n \n\nwhere formula_1 is the infinitesimal amount of heat absorbed by the system from the reservoir and formula_2 is the temperature of the external reservoir (surroundings) at a particular instant in time. In the special case of a reversible process, the equality holds. The reversible case is used to introduce the entropy state function. This is because in a cyclic process the variation of a state function is zero. In words, the Clausius statement states that it is impossible to construct a device whose sole effect is the transfer of heat from a cool reservoir to a hot reservoir. Equivalently, heat spontaneously flows from a hot body to a cooler one, not the other way around. The generalized \"inequality of Clausius\" \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n"}
{"id": "903583", "url": "https://en.wikipedia.org/wiki?curid=903583", "title": "Grand Alliance (HDTV)", "text": "Grand Alliance (HDTV)\n\nThe Grand Alliance (GA) was a consortium created in 1993 at the behest of the Federal Communications Commission (FCC) to develop the American digital television (SDTV, EDTV) and HDTV specification, with the aim of pooling the best work from different companies. It consisted of AT&T Corporation, General Instrument Corporation, Massachusetts Institute of Technology, Philips Consumer Electronics, David Sarnoff Research Center, Thomson Consumer Electronics, and Zenith Electronics Corporation. The Grand Alliance DTV system is the basis for the ATSC standard.\n\nRecognizing that earlier proposed systems demonstrated particular strengths in the FCC's Advisory Committee on Advanced Television Service (ACATS) testing and evaluation process, the Grand Alliance system was proposed to combine the\nadvantages of all of the previously proposed terrestrial digital HDTV systems. At the time of its inception, the Grand Alliance HDTV system was specified to include:\n\nAudio and transmission systems had not been decided at the time of the GA agreement. Five channel audio was specified, but a decision among the Dolby AC-3, multi-channel MUSICAM audio, and MIT \"AC\" systems had not yet been made. Candidate transmission approaches included QAM, Spectrally-Shaped QAM, 6 VSB (with trellis code) and 4/2 VSB. COFDM had been proposed by third parties, but was rejected as not being mature, and not offering fringe-area coverage equivalent to analog transmission. A thorough analysis of service area, interference characteristics, transmission robustness and system attributes would be performed to determine the \"best approach.\"\n\nIn the end, 1080, 720 and 480-line resolutions were implemented at various aspect ratios and frame rates, with progressive and interlaced scanning (the so-called \"18 formats\"), together with 8-VSB modulation and Dolby AC-3 audio. However, the selection of transmission and audio systems was not without controversy. The choice of 8-VSB was later criticised by several groups as being inferior to COFDM under conditions of multipath interference. Improvements in receiver designs would later render this apparently moot. With MUSICAM originally faltering during GA testing, the GA issued a statement finding the MPEG-2 audio system to be \"essentially equivalent\" to Dolby, but only after the Dolby selection had been made. Later, a story emerged that MIT had entered into an agreement with Dolby whereupon the university would be awarded a large sum if the MUSICAM system was rejected. Following a five-year lawsuit for breach of contract, MIT and its GA representative received a total of $30 million from Dolby, after the litigants reached a last-minute out-of-court settlement. Dolby also offered an incentive for Zenith to switch their vote (which they did), however it is unknown whether they accepted the offer.\n\n\n\n\n\n"}
{"id": "46779", "url": "https://en.wikipedia.org/wiki?curid=46779", "title": "Human-powered transport", "text": "Human-powered transport\n\nHuman-powered transport is the transport of person(s) and/or goods using human muscle power. Like animal-powered transport, human-powered transport has existed since time immemorial in the form of walking, running and swimming. Modern technology has allowed machines to enhance human-power.\n\nAlthough motorization has increased speed and load capacity, many forms of human-powered transport remain popular for reasons of lower cost, leisure, physical exercise and environmentalism. Human-powered transport is sometimes the only type available, especially in underdeveloped or inaccessible regions.\n\nIn the 1989 Race Across America, one team (Team Strawberry) used an experimental device comprising a rear wheel hub, a sensor, and a handlebar mounted processor, to measure each cyclist's power output.\n\nIn lab experiments an average \"in-shape\" cyclist can produce about 3 watts/kg for more than an hour (e.g., around 200 watts for a rider), with top amateurs producing 5 watts/kg and elite athletes achieving 6 watts/kg for similar lengths of time. Elite track sprint cyclists are able to attain an instantaneous maximum output of around 2,000 watts, or in excess of 25 watts/kg; elite road cyclists may produce 1,600 to 1,700 watts as an instantaneous maximum in their burst to the finish line at the end of a five-hour-long road race.\n\n\nSkateboards have the advantage of being so small and light that users can easily carry them when not skating.\n\nThe most efficient human-powered land vehicle is the bicycle. Compared to the much more common upright bicycle, the recumbent bicycle may be faster on level ground or down hills due to better aerodynamics while having similar power transfer efficiency.\n\nVelomobiles are increasingly popular in colder and/or wetter countries due to the protection they offer against the environment. Freight bicycles are used to transport cargo. Cycle rickshaws can be used as taxicabs.\n\nIn 2016, AeroVelo cyclist Todd Reichert achieved the human-powered speed record of with a velomobile at Battle Mountain, Nevada.\n\nDutch cyclist Fred Rompelberg set a speed record at the Bonneville Salt Flats in Utah on October 3, 1995 while cycling in the wake of a motor dragster pace-car. The wake of the pace-car reduced the aerodynamic drag against which Rompelberg pedalled to almost zero.\n\nGreg Kolodziejzyk set two world records recognized by both the International Human Powered Vehicle Association and Guinness (TM) World Records on July 17, 2006 on a race track in Eureka, California. The first record is for the most distance traveled in 24 hours by human power , and the second for the worlds fastest time trial (23 hours, 2 minutes).\nBoth records were broken on August 6, 2010 by Christian von Ascheberg who drove in 19 hours, 27 minutes and managed to go in 24 hours with his Milan SL Velomobile. In the same race he also raised the 12-hour record to , which is an average of .\nIn 1969, artists in a small Northern California town began the Kinetic sculpture race which has grown to a , three-day all terrain, human-powered sculpture race and county wide event. It is held every year on the last weekend in May.\n\nThe \"Pedaliante\" flew short distances fully under human power in 1936, but the distances were not significant enough to win the prize of the Italian competition for which it was built. The flights were deemed to be a result of the pilot's significant strength and endurance, and not attainable by a typical human. Additional attempts were made in 1937 and 1938 using a catapult system, launching the plane to a height of . With the catapult launch, the plane successfully traveled the distance outlined by the competition, but was declined the prize due to the takeoff method.\n\nThe first officially authenticated regularly feasible take-off and landing of a human-powered aircraft (one capable of powered takeoffs, unlike a glider) was made on 9 November 1961 by Derek Piggott in Southampton University's Man Powered Aircraft (SUMPAC).\n\nPerhaps the best-known human-powered plane is the Gossamer Albatross, which flew across the English Channel in 1979.\n\nThe current distance and duration record recognised by the FAI, a straight distance of in 3 hours and 54 minutes, was achieved on 23 April 1988 from Heraklion on Crete to Santorini in a MIT Daedalus 88 piloted by Kanellos Kanellopoulos.\n\nThe current speed record is held by the Monarch B, built by a team at MIT in 1983, which won a Kremer Prize of £20,000 for sustaining a speed of over over a triangular course.\n\nThe first officially observed human-powered helicopter to have left the ground was the Da Vinci III in 1989. It was designed and built by students at Cal Poly San Luis Obispo in California, USA. It flew for 7.1 seconds and reached a height of . The second was the Yuri I in 1994, designed and built by students at Nihon University in Japan. It flew for 19.46 seconds and reached an altitude of . On 13 June 2013, the AeroVelo Atlas was the first to complete a flight that lasted 64 seconds and reached an altitude of 3.3 meters, thus winning the Sikorsky Prize.\n\nFrench inventors have built man-powered airships and balloons. Solar balloons and solar airships are new types of balloons and airships. Because lift is supplied through buoyancy, human power can be devoted to thrust.\n\nHuman-powered watercraft include prehistoric, historic and well-known traditional and sporting craft such as canoes, rowing boats and galleys. The term \"human-powered boat\" is often used for more modern craft using propellers and water wheels for propulsion. These can be more efficient than paddles or oars and especially allow the use of the leg muscles which are generally stronger than arm muscles, even for non-athletes. Competitive rowing boats use sliding seats to engage the legs for propulsion with an oar for this reason, but require considerable skill to use efficiently. In addition, there is little skill required for forward propulsion while looking forwards and craft such as pedalos are popular at resorts.\n\nHydrofoils have less water resistance at the highest speeds attainable by humans and are thus usually faster than displacement boats on short courses. The world speed record on water was set 27 October 1991 by MIT professor Mark Drela who pedalled a human-powered hydrofoil, \"Decavitator\", to 18.5 knots (21.3 mph)(9.53 meters/second) over a 100-meter course in Boston, Massachusetts, US.\n\nIn 1989, the first human-powered International Submarine Race (ISR) was held in Florida with 17 craft. Since then nine more races have been held. The races themselves have been moved from the waters of Florida to the David Taylor Model Basin at the Carderock Division of the Naval Surface Warfare Center in Bethesda, Maryland, and are held biennially. At the 9th ISR in 2007 (in which 23 submarines participated) several new records were set: A single-person craft, Omer5 achieved a record speed of 8.035 knots breaking the Omer team's previous record of 7.19 knots set by Omer 4 in 2004. Also Omer 6 snatched up a record for non-propeller driven craft with a speed of 4.642 knots.\n\n\n\n\n"}
{"id": "33673358", "url": "https://en.wikipedia.org/wiki?curid=33673358", "title": "Industrial architecture", "text": "Industrial architecture\n\nIndustrial architecture is the design and construction of buildings serving industry. Such buildings rose in importance with the Industrial Revolution, and were some of the pioneering structures of modern architecture.\n\n\n"}
{"id": "23128375", "url": "https://en.wikipedia.org/wiki?curid=23128375", "title": "International Commission of Agricultural and Biosystems Engineering", "text": "International Commission of Agricultural and Biosystems Engineering\n\nInternational Commission of Agricultural and Biosystems Engineering – CIGR (Commission Internationale du Genie Rural) founded in 1930 in Liège, Belgium, as an international, non-governmental, non-profit organization regrouping, as a networking system, regional and national societies of agricultural and biological engineering as well as private and public companies and individuals all over the world, is the largest and highest international institution in the field. Until 2008 the organisation was known as International Commission of Agricultural Engineering. Its membership includes American Society of Agricultural and Biological Engineers (ASABE), Asian Association for Agricultural Engineering (AAAE), European Society of Agricultural Engineers (EurAgEng), Latin American and Caribbean Association of Agricultural Engineering (ALIA), South and East African Society of Agricultural Engineering (SEASAE), Euro Asian Association of Agricultural Engineers (EAAAE), Association of Agricultural Engineers of South-Eastern Europe (AAESEE), and many national societies.\n\nThe main missions of CIGR are to\n\nThe structure of CIGR is divided by seven technical Sections and various working groups. Each technical Section is charged with promoting and developing its respective field of science and technology as it relates to agricultural engineering. The CIGR Working Groups are appointed by the Executive Board to carry out studies on specific subjects of international importance and interest.\n\n\n\nThe title of Fellow is the highest honour in CIGR. The title of Fellow is conferred to individuals who have made sustained, outstanding contributions world-wide, and that continue to improve the outcomes of the Agricultural and Bio-systems Engineering profession.\n\n\n"}
{"id": "41318005", "url": "https://en.wikipedia.org/wiki?curid=41318005", "title": "Jill Dann", "text": "Jill Dann\n\nJill Dann is a Chartered Fellow of the British Computer Society. She is an established British author of many books related to emotional intelligence.\n\nDann has also written a book on Neuro-Linguistic Programming. She is a founding Director of Consultation Limited started in 1987 providing consultancy to the IT industry.\nShe is a Chartered Fellow of the BCS and Information Systems Examination Board Examiner in the Certificate in Project Management of IT Systems since 1992.\nDann is also author of seven Hodder books including two \"Teach Yourself: Workbooks\". The first two have been best sellers in each of their series and many are now translated into Spanish, Simplified Chinese, French, Thai, Greek and Portuguese. Her first book is in its third edition published last year, 30 March 2012. Her latest book in print is one of the TY: Workbook titles and is co-written by her husband Derek Dann formally launched on 25 October 2012.\nDann has served on the BCSWomen Committee.\nBCS Women is a specialist group for networking and sharing knowledge amongst women computing professionals.\n\n\n"}
{"id": "41525099", "url": "https://en.wikipedia.org/wiki?curid=41525099", "title": "Katherine C. Kelly", "text": "Katherine C. Kelly\n\nKatherine Crowley Kelly (1924-2011) was a civil rights activist who advocated for women's rights and LGBT rights issues. Kelly served as a delegate to the Electoral College in 1996 and as a delegate to five Democratic National Conventions. In 2011, U.S. Congressman Alcee Hastings commemorated her life's work on the floor of the U.S. House of Representatives.\n\nKelly was born in New Rochelle, New York on June 20, 1924, and she grew up in South Orange, New Jersey. In 1942, Kelly graduated from the Beard School in Orange, New Jersey (now Morristown-Beard School). She then completed her bachelor's degree at Wellesley College in Wellesley, Massachusetts in 1946.\n\nDuring the 1980s, Kelly helped lead Americans for Democratic Action. She later served as the Vice President of the Florida Women's Political Caucus and as the Development Director of Women Leaders Online. Kelly also served as the legislative director for Florida's chapter of the National Organization for Women (FLNOW) and as a member of the Board of Directors of NARAL Pro-Choice America. In 2001, FLNOW awarded her their Outstanding Feminist of the Year Award.\n\nKelly served as the representative from Palm Beach County to the Florida State Democratic Committee for 26 years. She began serving in 1985 after committee member Christine Mitchell resigned from the post to spend more time with her family. At the time, Kelly had served on the executive committee for the Democratic Party in Palm Beach County for 10 years. Recognizing the impact of her work in public policy, the Democratic Women's Club of Palm Beach County awarded Kelly their Woman of the Year Award in 2008.\n\nKelly served as a founding member of the Society of Woman Engineers. Following her graduation from college, she worked for Henry L. Crowley & Company, Inc., the family business which manufactured radio electronics components. She rose to the position of Vice President.\n\nKelly married her husband Edward in 1961.\n"}
{"id": "37023189", "url": "https://en.wikipedia.org/wiki?curid=37023189", "title": "List of home automation software", "text": "List of home automation software\n\nHome automation software is software that facilitates control of common appliances found in a home, office, or sometimes a commercial setting, such as lights, HVAC equipment, access control, sprinklers, and other devices. Software typically provides for scheduling tasks, such as turning sprinklers on at the appropriate time, event handling, such as turning lights on when motion is detected. Typically the application will support multiple interfaces to the outside world, such as XMPP, email, Z-Wave, and X10.\n\nThe software will typically provide a user interface which is often based on a client-server model, such as a web UI or a smartphone app, or some combination thereof. More advanced applications will often allow users to write scripts in a programming language to handle more complex tasks. there are many competing home automation standards for both hardware and software.\n\nThis is a list of software across multiple platforms which is designed to perform home automation.\n\nThis is a list of platforms that require custom, closed hardware for home automation.\n"}
{"id": "21000205", "url": "https://en.wikipedia.org/wiki?curid=21000205", "title": "List of video services using H.264/MPEG-4 AVC", "text": "List of video services using H.264/MPEG-4 AVC\n\nThis page provides a list of specific service providers who are, or soon will be, providing video in the H.264/MPEG-4 AVC format. In some cases the list may include announced plans for services that have not yet actually been deployed.\n\n\n\n\n\n"}
{"id": "1845134", "url": "https://en.wikipedia.org/wiki?curid=1845134", "title": "List of years in home video", "text": "List of years in home video\n\nThis page indexes the individual year in home video pages. Some years are annotated with a significant event as a reference point.\n\n\n\n\n\n"}
{"id": "1564929", "url": "https://en.wikipedia.org/wiki?curid=1564929", "title": "Marin Computer Center", "text": "Marin Computer Center\n\nOpened in 1977 in Marin County, CA, the Marin Computer Center was the world's first public access microcomputer center. The non-profit company was co-created by David Fox (later to become one of Lucasfilm Games' founding members) and Annie Fox an author.\n\nMCC (as it was known) initially featured the Atari 2600, an Equinox 100, 9 Processor Technology Sol 20 computers (S-100 bus systems), the Radio Shack Model I and the Commodore PET. In addition to providing computer access to the public it had classes on the programming language BASIC. Later it added Apple II and Atari 800 computers, for a total of about 40 systems.\n\nThe Foxes left MCC in 1981, turning it over to new management, and later to the teens and young adults who helped run it.\n\n\n"}
{"id": "6159905", "url": "https://en.wikipedia.org/wiki?curid=6159905", "title": "Murano glass", "text": "Murano glass\n\nMurano glass is made on the Venetian island of Murano, which has been a glassmaking center for over 700 years. It is also sometimes referred to as Venetian glass. Today Murano is known for its art glass, but it has a long history of innovations in glassmaking in addition to its artistic fame—and was Europe's first major glassmaking center. During the 1400s, Murano glassmakers created \"cristallo\"—which was almost transparent and considered the finest glass in the world. Murano glassmakers also developed a white-colored glass (milk glass called \"lattimo\") that looked like porcelain. They later became Europe's finest makers of mirrors.\n\nOriginally, Venice was controlled by the Eastern Roman Empire, but it eventually became an independent city state. It flourished as a trading center and seaport. Its connections with the Middle East helped its glassmakers gain additional skills, as glassmaking was more advanced in countries such as Syria and Egypt. Although Venetian glassmaking existed as far back as the 8th Century, it became concentrated in Murano by law beginning 1291. Since glass factories often caught fire, this removed much of the possibility of a major fire disaster for the city. Murano glassmakers developed secret recipes and methods for making glass, and the concentration of Venice's glassmaking on the island of Murano enabled better control of those secrets.\n\nMurano became Europe's elite glassmaking center, peaking in popularity in the 15th and 16th centuries. Venice's dominance in trade along the Mediterranean Sea created a wealthy merchant class that was a strong connoisseur of the arts. This helped establish demand for art glass and more innovations. The spread of glassmaking talent in Europe eventually diminished the importance of Venice and its Murano glassmakers. A defeat by Napoleon Bonaparte in 1797, and occupation, caused more hardship for Murano's glassmaking industry. Murano glassmaking began a revival in the 1920s. Today, Murano and Venice are tourist attractions, and Murano is home to numerous glass factories and a few individual artists' studios. Its \"Museo del Vetro\" (Glass Museum) in the Palazzo Giustinian contains displays on the history of glassmaking as well as glass samples ranging from Egyptian times through the present day.\n\nThe Venetian city state grew during the decline of the Roman Empire as people fled barbarian invasions to the safety of islands in a lagoon. Small communities grew in the lagoon, and Venice became the most prominent. The city of Venice became a highly successful trading port, and by the 11th century dominated trade between Europe, North Africa, and the Middle East. It also had a strong navy. European Crusaders passed through Venice on their way to the Holy Land. Treasures of many kinds were bought and sold in Venice: spices, precious metals, gemstones, ivory, silks—and glass. Successful trade bred a wealthy merchant class in addition to the nobles, and the wealthy were responsible for Venice's famous art and architecture.\n\nGlass began being produced in Venice around 450, as glassmakers from Aquileia fled to the islands to escape barbarian invaders. The earliest archaeological evidence of a glass factory in the area is from the Venetian lagoon island of Torcello, which dates from the 7th to 8th century. The original Venetian glassmakers were joined by glassmakers from Byzantium (a.k.a. Constantinople) and the Middle East—which enriched their glassmaking knowledge. Glass was made in the Middle East long before it was made in Europe. Early products were beads, glass for mosaics, jewelry, small mirrors, and window glass.\n\nVenetian glassmaking grew in importance to the city's economy. Around 1271, the local glassmakers' guild made rules to help preserve glassmaking secrets. Trade secrets were forbidden to be divulged outside of Venice. If a glassworker left the city without permission, he would be ordered to return. If he failed to return, his family would be imprisoned. If he still did not return, an assassin would be sent to kill him. Additional rules were made regarding ingredients used for making glass and the type of wood used as fuel for the furnaces.\n\nA November 8, 1291, law ordered most of Venice's glassmaking industry to be on the \"island of Murano\". Murano is actually a cluster of islands linked by short bridges, and located about north of Venice in the Venetian lagoon. The furnaces used to make molten glass were a fire hazard, especially to cities with wooden structures nearby. The removal of the threat of a disaster from fire was not the only reason for moving the glassmaking industry to Murano. The glassmaking, \"and the glassmakers\", were kept in Murano. The Venetian government did what it believed was necessary to prevent the spread of Venetian glassmaking expertise to potential competitors. Glassmakers could not leave the island without permission from the government. Leaving without permission, or revealing trade secrets, was punishable by death. Another advantage for the Murano location was that imports and exports could easily be monitored.\n\nMurano in the 1200s was a summer resort where the aristocrats of Venice built villas with orchards and gardens. It took about an hour to row a boat from Venice to Murano. Although the glassmakers could not leave the island, the nobles had no constraints. Despite their travel restrictions, the glassmakers lived on a beautiful island, were under the direct rule of Venice's Council of Ten, and had extra privileges. They did not work during the hot summer, as furnace repair and maintenance was conducted at that time. During the 1300s, the annual summer vacation lasted five months. In the 1400s, the summer vacation was shortened to three and one half months. Sometimes, the Murano glassworkers believed they were not working enough. Glassmakers also enjoyed heightened status. On December 22, 1376, it was announced that if a glassmaker's daughter married a nobleman, there was no forfeiture of social class—and their children were noble.\n\nThe glassmakers of Murano are known for many innovations and refinements to glassmaking. Among them are Murano beads, \"cristallo\", \"lattimo\", chandeliers, and mirrors. Additional refinements or creations are goldstone, multicolored glass (millefiori), and imitation gemstones made of glass. In addition to guarding their secret processes and glass recipes, Venetian/Murano glassmakers strived for beauty with their glass.\n\nAventurine glass, also known as goldstone glass, is translucent brownish with metallic (copper) specs. It was developed by Venetian glassmakers in the early 15th century. It is first cited in historical documents in 1626. The name aventurine is used because it was discovered accidentally.\n\nGlass beads (a.k.a. Murano beads) were made by the Venetians beginning in the 1200s. The beads were used as rosary beads and jewelry. They were also popular in Africa. Christopher Columbus noted that the people of the New World (Native Americans) were \"delighted\" with the beads as gifts, and beads became popular with American Indians.\n\n\"Calcedonio\" is a marbled glass that looked like the semiprecious stone chalcedony. This type of glass was created during the 1400s by Angelo Barovier, who is considered Murano's greatest glassmaker. Barovier was an expert glassblower, revived enameling, and also worked with colored glass. His family had been involved with glassmaking since at least 1331, and the family continued in the business after his death. He died in 1460.\n\nDuring the 1700s, Giuseppe Briati was famous for his work with ornamented mirrors and chandeliers. Briati's chandelier style was called \"ciocche\"—literally bouquet of flowers. Briati's typical chandelier was large with multiple arms decorated with garlands, flowers and leaves. One of the common uses of the huge Murano chandeliers was interior lighting for theatres and important rooms in palaces. Briati was born in Murano in 1686, and his family's business was glassmaking. He was allowed to work in a Bohemian glass factory, where he learned the secrets of working with Bohemian crystal—which was becoming more popular than Murano \"cristallo\". In 1739, the Council of Ten allowed him to move his furnace from Murano to Venice because his work had caused such jealousy that he and his workers feared for their lives. (His father had been stabbed to death in 1701.) Briati retired in 1762, and his nephew became manager of the glass works. Briati died in Venice in 1772, and is buried in Murano.\n\n\"Cristallo\" is a soda glass, created during the 15th century by Murano's Angelo Barovier. The oldest reference to \"cristallo\" is dated May 24, 1453. At the time, \"cristallo\" was considered Europe's clearest glass, and is one of the main reasons Murano became \"the most important glass center\". It looked like quartz, which was said to have magical qualities and often used in religious objects. \"Cristallo\" became very popular. This type of glass was fragile and difficult to cut, but it could be enameled and engraved. Manganese was a key ingredient in the secret formula used to make \"cristallo\". An easy modification to \"cristallo\" made in Murano was to produce a frosted or crackle version.\n\nThe \"filigrana\" (a.k.a. filigree) style was developed in Murano in the 1500s. By embedding glass canes (usually white but not always) in colorless glass, the glassware has a striped appearance. \"Vetro a fili\" has straight white stripes, \"vetro a retortoli\" has twisted or spiral patterns, and \"vetro a reticello\" has two sets of lines twisted in opposite directions. Francesco Zeno has been mentioned as the inventor of \"vetro a retortoli\".\n\n\"Lattimo\", or milk glass, began being made in Murano during the 15th century, and Angelo Barovier is credited with its re-discovery and development. This glass is opaque white, and was meant to resemble enameled porcelain. It was often decorated with enamel showing sacred scenes or views of Venice.\n\n\"Millefiori\" glass is a variation of the murrine technique made from colored canes in clear glass, and is often arranged in flower-like patterns. The Italian word \"millefiori\" means thousand flowers. This technique was perfected in Alexandria, Egypt, and began being used in Murano in the 15th century.\n\nSmall mirrors were made in Murano beginning in the 1500s, and mirror makers had their own guild beginning in 1569. Murano mirrors were known for the artwork on the frame that held the mirror in addition to their quality. By the 1600s, Murano mirrors were in great demand. However, by the end of the century, English-made mirrors had the best quality. Only one glass house in Murano was still making mirrors by 1772.\n\nMurrine technique begins with the layering of colored liquid glass, heated to , which is then stretched into long rods called canes. When cooled, these canes are then sliced in cross-sections, which reveals the layered pattern. Ercole Barovier, a descendent of Murano's greatest glassmaker Angelo Barovier, won numerous awards during the 1940s and 1950s for his innovations using the murrine technique.\n\n\"Sommerso\" (\"submerged\" in Italian), is a form of artistic Murano glass that has layers of contrasting colors (typically two), which are formed by dipping colored glass into another molten glass and then blowing the combination into a desired shape. The outermost layer, or casing, is often clear. \"Sommerso\" was developed in Murano during the late 1930s. Flavio Poli was known for using this technique, and it was made popular by Seguso Vetri d'Arte and the Mandruzzato family in the 1950s. This process is a popular technique for vases, and is sometimes used for sculptures.\n\nThe 16th century was the golden age for glassmaking in Murano. Major trading partners included the Spanish Indies, Italy, Spain, Turkey, and the German-speaking states. At least 28 glassmaking furnaces were in Murano in 1581. Numerous leaders and dignitaries visited Murano during this century, including the queen of France, dukes, princes, generals, cardinals, archbishops, and ambassadors. Collectors of Murano glass included Henry VIII of England, Pope Clement VII, King Ferdinand of Hungary, Francis I of France, and Phillip II of Spain.\n\nEventually, the dominance of \"cristallo\" came to an end. In 1673, English glass merchant George Ravenscroft created a clear glass he called \"crystalline\"—but it was not stable. Three years later, he improved this glass by adding lead oxide, and lead glass (a.k.a. crystal) was created. Ravenscroft, who had lived for many years in Venice, made lead crystal that was less breakable than \"cristallo\". In 1674, Bohemian glassmaker Louis Le Vasseur made crystal that was similar to Ravenscroft's. In 1678, Johan Friedrich Kunkel von Lowenstein produced a \"cristallo\"-like glass in Potsdam. The Bohemian and Prussian-style glass was later modified by the addition of lime and chalk. This new glass is attributed to Bohemian glassmaker Michael Müller in 1683. The Bohemian glass was not suitable to the Murano-style artwork on the glass. However, this harder glass was produced as a thicker glass suitable for engraving and grinding. The Bohemian and English glass eventually became more popular than \"cristallo\" made in Murano. By the 1700s, Murano glass was traded mostly with Italian states and the Turkish empire. Small quantities were traded with England, Flanders, the Netherlands and Spain.\n\nNapoleon conquered Venice during May 1797, and the Venetian Republic came to an end. The fall of the Venetian Republic caused hard times for glassmaking in Murano, and some of the Murano methods became lost. Controlled by France and Austria, Venetian glassmaking became unprofitable because of tariffs and taxes—and glassmakers that survived were reduced to making mostly beads. Napoleon closed the Venetian glass factories in 1807, although simple glassware and beadmaking continued. In the 1830s, outsiders tried to revive the industry. However, it was not until Venice became part of Italy in 1866 that Murano glassmaking could experience a revival. Around that time, local leaders such as Abbot Vincenzo Zanetti (founder of the Murano Glass Museum), along with the Murano factory owners, began reinventing the earlier Murano techniques for making glass. Antonio Salviati, a Venetian lawyer who gave up his profession in 1859 in order to devote his time to glassmaking, also had an important role in the revival of glassmaking in Murano.\n\nFrom its beginning until the fall of the Venetian Republic, Murano glass was mostly a very high quality soda lime glass (using today's terminology) that had extra attention focused on its appearance. Glass from that time typically contained 65 to 70 percent silica. A flux, usually soda (sodium oxide as 10 to 20 percent of the glass composition) was added to enable the silica to melt at a lower temperature. A stabilizer, usually lime (calcium oxide as about 10 percent of the glass) was also added for durability and to prevent solubility in water. Small quantities of other ingredients were added to the glass, mostly to affect appearance. Sand is a common source for silica. For certain types of glass, the Murano glassmakers used quartz as their source for silica. Quartz pebbles were crushed into a fine powder. Two sources for sand were Creta and Sicily. Quartz pebbles were selected from the Ticino and Adige rivers in Northern Italy. Their source for soda was what they called \"allume catina\"—plant ash found in the eastern Mediterranean countries of the Middle East. Beginning in the 16th century, \"allume catina\" was also imported from Mediterranean coastal regions of Spain and France.\nThe mixing and melting of the batch of ingredients was a two-stage process. First, nearly equal amounts of silica and flux were continuously stirred in a special furnace. The furnace was called a \"calchera\" furnace, and the mix was called \"fritta\". In the second stage, the \"fritta\" was mixed with selected recycled waste glass (cullet) and melted in another furnace. Depending on the type and color of glass, other additives were used. Lead and tin were added for white opaque glass (\"latimo\"). Cobalt was used for blue glass. Copper and iron were used for green and for various shades of green, blue, and yellow. Manganese was used to remove colors. Although natural gas is the furnace fuel of choice for glassmaking today, the fuel mandated in Murano during the 13th century was alder and willow wood. During this second stage, the surface of the molten glass was skimmed to remove undesirable chemicals that affected the appearance of the glass. Additional techniques were used as glassmaking evolved. To improve clarity, molten glass was put in water and then re-melted. Another technique was to purify the flux by boiling and filtering.\n\nThe Venetian glassmakers had a set of tools that changed little for hundreds of years. A \"ferro sbuso\", also called a \"canna da soffio\", is the blowpipe essential for extracting molten glass and beginning the shaping process. A \"borselle\" is a tong-like tool of various sizes used to shape glass that has not hardened. A \"borselle puntata\" is a similar tool, only it has a pattern that an be imprinted on the glass. A \"pontello\" is an iron rod that holds the glass while work is done on the edge of the glass. A \"tagianti\" is a large scissors used to cut glass before it has hardened. A \"scagno\" is the workbench used by the glassmaker. \"Good tools are nice, but good hands are better,\" is an old Murano saying that reinforces the idea that the glassmakers of Murano rely on their skills instead of any advantage caused by special tools.\n\nSome of Murano's historical glass factories remain well known brands today, including De Biasi, Gabbiani, Venini, Salviati, Barovier & Toso, Pauly, Berengo Studio, Seguso, Formia International, Simone Cenedese, Alessandro Mandruzzato, Vetreria Ducale, Estevan Rossetto 1950 and others. The oldest glass factory is Antica Vetreria Fratelli Toso, founded in 1854.\n\nOverall, the industry has been shrinking as demand has waned. Imitation works (recognizable by experts but not by the typical tourist) from Asia and Eastern Europe take an estimated 40 to 45 percent of the market for Murano glass, and public tastes have changed while the designs in Murano have largely stayed the same. To fight the imitation problem, a group of companies and concerned individuals created a trademark in 1994 that certifies that the product was made on Murano. By 2012, about 50 companies were using the Artistic Glass Murano® trademark of origin.\n\nGlassmaking is a difficult and uncomfortable profession, as glassmakers must work with a product heated to extremely high temperatures. Unlike 500 years ago, children of glassmakers do not enjoy any special privileges, extra wealth, or marriage into nobility. Today, it is difficult to recruit young glassmakers. Foreign imitations, and difficulty attracting young workers, caused the number of professional glassmakers in Murano to decrease from about 6,000 in 1990 to fewer than 1,000 by 2012.\n\n\n\n\n"}
{"id": "8327305", "url": "https://en.wikipedia.org/wiki?curid=8327305", "title": "Nanoelectronics", "text": "Nanoelectronics\n\nNanoelectronics refer to the use of nanotechnology in electronic components. The term covers a diverse set of devices and materials, with the common characteristic that they are so small that inter-atomic interactions and quantum mechanical properties need to be studied extensively. Some of these candidates include: hybrid molecular/semiconductor electronics, one-dimensional nanotubes/nanowires (e.g. Silicon nanowires or Carbon nanotubes) or advanced molecular electronics. Recent silicon CMOS technology generations, such as the 22 nanometer node, are already within this regime. Nanoelectronics are sometimes considered as disruptive technology because present candidates are significantly different from traditional transistors.\n\nIn 1965 Gordon Moore observed that silicon transistors were undergoing a continual process of scaling downward, an observation which was later codified as Moore's law. Since his observation transistor minimum feature sizes have decreased from 10 micrometers to the 28-22 nm range in 2011. The field of nanoelectronics aims to enable the continued realization of this law by using new methods and materials to build electronic devices with feature sizes on the nanoscale.\n\nThe volume of an object decreases as the third power of its linear dimensions, but the surface area only decreases as its second power. This somewhat subtle and unavoidable principle has huge ramifications. For example, the power of a drill (or any other machine) is proportional to the volume, while the friction of the drill's bearings and gears is proportional to their surface area. For a normal-sized drill, the power of the device is enough to handily overcome any friction. However, scaling its length down by a factor of 1000, for example, decreases its power by 1000 (a factor of a billion) while reducing the friction by only 1000 (a factor of only a million). Proportionally it has 1000 times less power per unit friction than the original drill. If the original friction-to-power ratio was, say, 1%, that implies the smaller drill will have 10 times as much friction as power; the drill is useless.\n\nFor this reason, while super-miniature electronic integrated circuits are fully functional, the same technology cannot be used to make working mechanical devices beyond the scales where frictional forces start to exceed the available power. So even though you may see microphotographs of delicately etched silicon gears, such devices are currently little more than curiosities with limited real world applications, for example, in moving mirrors and shutters. Surface tension increases in much the same way, thus magnifying the tendency for very small objects to stick together. This could possibly make any kind of \"micro factory\" impractical: even if robotic arms and hands could be scaled down, anything they pick up will tend to be impossible to put down. The above being said, molecular evolution has resulted in working cilia, flagella, muscle fibers and rotary motors in aqueous environments, all on the nanoscale. These machines exploit the increased frictional forces found at the micro or nanoscale. Unlike a paddle or a propeller which depends on normal frictional forces (the frictional forces perpendicular to the surface) to achieve propulsion, cilia develop motion from the exaggerated drag or laminar forces (frictional forces parallel to the surface) present at micro and nano dimensions. To build meaningful \"machines\" at the nanoscale, the relevant forces need to be considered. We are faced with the development and design of intrinsically pertinent machines rather than the simple reproductions of macroscopic ones.\n\nAll scaling issues therefore need to be assessed thoroughly when evaluating nanotechnology for practical applications.\n\nFor example, electron transistors, which involve transistor operation based on a single electron. Nanoelectromechanical systems also fall under this category.\nNanofabrication can be used to construct ultradense parallel arrays of nanowires, as an alternative to synthesizing nanowires individually.\n\nBesides being small and allowing more transistors to be packed into a single chip, the uniform and symmetrical structure of nanowires and/or nanotubes allows a higher electron mobility (faster electron movement in the material), a higher dielectric constant (faster frequency), and a symmetrical electron/hole characteristic.\n\nAlso, nanoparticles can be used as quantum dots.\n\nSingle molecule devices are another possibility. These schemes would make heavy use of molecular self-assembly, designing the device components to construct a larger structure or even a complete system on their own. This can be very useful for reconfigurable computing, and may even completely replace present FPGA technology.\n\nMolecular electronics is a new technology which is still in its infancy, but also brings hope for truly atomic scale electronic systems in the future. One of the more promising applications of molecular electronics was proposed by the IBM researcher Ari Aviram and the theoretical chemist Mark Ratner in their 1974 and 1988 papers \"Molecules for Memory, Logic and Amplification\", (see Unimolecular rectifier).\n\nThis is one of many possible ways in which a molecular level diode / transistor might be synthesized by organic chemistry.\nA model system was proposed with a spiro carbon structure giving a molecular diode about half a nanometre across which could be connected by polythiophene molecular wires. Theoretical calculations showed the design to be sound in principle and there is still hope that such a system can be made to work.\n\nNanoionics studies the transport of ions rather than electrons in nanoscale systems.\n\nNanophotonics studies the behavior of light on the nanoscale, and has the goal of developing devices that take advantage of this behavior.\n\nCurrent high-technology production processes are based on traditional top down strategies, where nanotechnology has already been introduced silently. The critical length scale of integrated circuits is already at the nanoscale (50 nm and below) regarding the gate length of transistors in CPUs or DRAM devices.\n\nNanoelectronics holds the promise of making computer processors more powerful than are possible with conventional semiconductor fabrication techniques. A number of approaches are currently being researched, including new forms of nanolithography, as well as the use of nanomaterials such as nanowires or small molecules in place of traditional CMOS components. Field effect transistors have been made using both semiconducting carbon nanotubes and with heterostructured semiconductor nanowires (SiNWs).\n\nIn 1999, the CMOS transistor developed at the Laboratory for Electronics and Information Technology in Grenoble, France, tested the limits of the principles of the MOSFET transistor with a diameter of 18 nm (approximately 70 atoms placed side by side). This was almost one tenth the size of the smallest industrial transistor in 2003 (130 nm in 2003, 90 nm in 2004, 65 nm in 2005 and 45 nm in 2007). It enabled the theoretical integration of seven billion junctions on a €1 coin. However, the CMOS transistor, which was created in 1999, was not a simple research experiment to study how CMOS technology functions, but rather a demonstration of how this technology functions now that we ourselves are getting ever closer to working on a molecular scale. Today it would be impossible to master the coordinated assembly of a large number of these transistors on a circuit and it would also be impossible to create this on an industrial level.\n\nElectronic memory designs in the past have largely relied on the formation of transistors. However, research into crossbar switch based electronics have offered an alternative using reconfigurable interconnections between vertical and horizontal wiring arrays to create ultra high density memories. Two leaders in this area are Nantero which has developed a carbon nanotube based crossbar memory called Nano-RAM and Hewlett-Packard which has proposed the use of memristor material as a future replacement of Flash memory.\n\nAn example of such novel devices is based on spintronics.The dependence of the resistance of a material (due to the spin of the electrons) on an external field is called magnetoresistance. This effect can be significantly amplified (GMR - Giant Magneto-Resistance) for nanosized objects, for example when two ferromagnetic layers are separated by a nonmagnetic layer, which is several nanometers thick (e.g. Co-Cu-Co). The GMR effect has led to a strong increase in the data storage density of hard disks and made the gigabyte range possible. The so-called tunneling magnetoresistance (TMR) is very similar to GMR and based on the spin dependent tunneling of electrons through adjacent ferromagnetic layers. Both GMR and TMR effects can be used to create a non-volatile main memory for computers, such as the so-called magnetic random access memory or MRAM.\n\nIn the modern communication technology traditional analog electrical devices are increasingly replaced by optical or optoelectronic devices due to their enormous bandwidth and capacity, respectively. Two promising examples are\nphotonic crystals and quantum dots. Photonic crystals are materials with a periodic variation in the refractive index with a lattice constant that is half the wavelength of the light used. They offer a selectable band gap for the propagation of a certain wavelength, thus they resemble a semiconductor, but for light or photons instead of electrons. Quantum dots are nanoscaled objects, which can be used, among many other things, for the construction of lasers. The advantage of a quantum dot laser over the traditional semiconductor laser is that their emitted wavelength depends on the diameter of the dot. Quantum dot lasers are cheaper and offer a higher beam quality than conventional laser diodes.\n\nThe production of displays with low energy consumption might be accomplished using carbon nanotubes (CNT) and/or Silicon nanowires. Such nanostructures are electrically conductive and due to their small diameter of several nanometers, they can be used as field emitters with extremely high efficiency for field emission displays (FED). The principle of operation resembles that of the cathode ray tube, but on a much smaller length scale.\n\nEntirely new approaches for computing exploit the laws of quantum mechanics for novel quantum computers, which enable the use of fast quantum algorithms. The Quantum computer has quantum bit memory space termed \"Qubit\" for several computations at the same time. This facility may improve the performance of the older systems.\n\nNanoradios have been developed structured around carbon nanotubes.\n\nResearch is ongoing to use nanowires and other nanostructured materials with the hope to create cheaper and more efficient solar cells than are possible with conventional planar silicon solar cells. It is believed that the invention of more efficient solar energy would have a great effect on satisfying global energy needs.\n\nThere is also research into energy production for devices that would operate \"in vivo\", called bio-nano generators. A bio-nano generator is a nanoscale electrochemical device, like a fuel cell or galvanic cell, but drawing power from blood glucose in a living body, much the same as how the body generates energy from food. To achieve the effect, an enzyme is used that is capable of stripping glucose of its electrons, freeing them for use in electrical devices. The average person's body could, theoretically, generate 100 watts of electricity (about 2000 food calories per day) using a bio-nano generator. However, this estimate is only true if all food was converted to electricity, and the human body needs some energy consistently, so possible power generated is likely much lower. The electricity generated by such a device could power devices embedded in the body (such as pacemakers), or sugar-fed nanorobots. Much of the research done on bio-nano generators is still experimental, with Panasonic's Nanotechnology Research Laboratory among those at the forefront.\n\nThere is great interest in constructing nanoelectronic devices that could detect the concentrations of biomolecules in real time for use as medical diagnostics, thus falling into the category of nanomedicine.\nA parallel line of research seeks to create nanoelectronic devices which could interact with single cells for use in basic biological research.\nThese devices are called nanosensors. Such miniaturization on nanoelectronics towards in vivo proteomic sensing should enable new approaches for health monitoring, surveillance, and defense technology.\n\n https://openlibrary.org/works/OL15759799W/Bits_on_Chips/\n\n"}
{"id": "51694499", "url": "https://en.wikipedia.org/wiki?curid=51694499", "title": "National Sustainable Agriculture Coalition (NSAC)", "text": "National Sustainable Agriculture Coalition (NSAC)\n\nThe National Sustainable Agriculture Coalition (NSAC) is an alliance of over 116 member groups that work collectively with NSAC's Washington, D.C.-based staff to promote and enhance sustainable food and farm policy at the federal level. NSAC aims to promote healthy rural and urban communities, support small-medium family farms, and protect natural resources. NSAC's definition of a sustainable agricultural system is one that produces readily available, affordable, nutritious food; provides farmers and laborers a high standard of living; and promotes the health of the environment and of communities. \n\nNSAC’s advocacy begins with input from sustainable and organic farmers, ranchers, and member organizations that work closely with producers. With this information in mind, NSAC develops policy priorities through its member-based issue committees. While NSAC staff conduct direct advocacy and education on policy issues to legislators and federal agencies on Capitol Hill, member organizations lead outreach and implementation work on the local, state, and regional levels.\nNSAC has been involved in the creation of federal programs, including: Conservation Stewardship Program (CSP), Wetlands Reserve Program (WRP), and Cooperative Conservation Partnership Initiative (CCPI) as part of the National Resource Conservation Service (NRCS); Value-Added Producer Grants Program (VAPG) as part of USDA Office of Rural Development; Beginning Farmer and Rancher Development Program (BFRDP) as part of USDA’s National Institute of Food and Agriculture (NIFA); Farmers Market and Local Food Promotion Program (FMLFPP) as part of USDA Agriculture Marketing Service; National Organic Certification Cost Share Program (NOCCSP) as part of the National Organic Program (NOP); and Sustainable Agriculture Research and Education Program (SARE). \n\nNSAC was formed in 2009 after the respective governing bodies of Sustainable Agriculture Coalition (SAC) and National Campaign for Sustainable Agriculture (NCSA) voted to merge. SAC was originally a midwest-based organization that formed as a direct result of the mid 1980's farm crisis. In 2003, SAC members voted to form a national coalition in order to expand their representation in Washington, D.C. NCSA formed in 1994 with a mission of bringing together diverse voices to influence federal food policy. Their community ties and more than 100 member organizations brought a grassroots voice to the NCSA - SAC merger. As an equal product of both of these organizations, NSAC aims to equally prioritize supporting, building, developing and engaging the grassroots of sustainable agriculture with researching, developing and advocating federal policies.\n\nNSAC’s member organizations represent the interests of almost all 50 states and the District of Columbia and are supported by a core staff in Washington, D.C. NSAC staff work with represented and participating members on issue-based councils and committees, which meet regularly via conference call and at biannual meetings. \n\nNSAC is overseen by three distinct governing councils: the Organizational Council, Grassroots Council, and Policy Council. The Organizational Council oversees development, membership, planning, and execution of NSAC mission. All potential NSAC members must first be approved by the Organizational Council before joining the coalition. The Grassroots Council coordinates NSAC’s advocacy efforts among member organizations, including media messaging and action alerts pertaining to campaigns. Within the Grassroots Council, the Diversity Committee works to ensure NSAC actively practices its commitment to equity and raising social justice concerns in relation to NSAC’s policy priorities and advocacy strategies. The Policy Council sets NSAC's yearly policy priorities after taking into consideration recommendations from each of NSAC’s issue committees.\n\nUnlike the three governing councils, NSAC Issue Committees focus on research and development of specific federal farm and food policies in accordance with the annual priorities set by the Policy Council. Issue Committees meet monthly to discuss policy strategies and coordinate action across states and regions. There are currently five Policy Issue Committees that work on diverse, but often overlapping, topics: \n\nPotential NSAC member organizations can apply to be either Represented Members or Participating Members. Represented Members receive formal representation by NSAC before government agencies and Congress. These member groups serve on the NSAC Issue Committees as well as the Policy Council and Organizational Council. Participating Members can serve on NSAC Issue Committees and attend NSAC events, but do not receive formal representation by NSAC on the Hill.\n\n\n"}
{"id": "46729756", "url": "https://en.wikipedia.org/wiki?curid=46729756", "title": "Norm Houghton (historian)", "text": "Norm Houghton (historian)\n\nNorman Houghton (born 1948) is a historian and archivist in Geelong, Victoria, who has published over 30 books, many focusing on timber tramways and sawmills of the Otway and Wombat Forests of Western Victoria, Australia. Most of his works have been self-published, while he has provided numerous articles to the newsletter and journal of the Light Railway Research Society of Australia\n\nHoughton grew up in Colac (in southern Victoria ) and attended Monash University, graduating in history. His interest in railway and forest history of Victoria's Otway Ranges was nurtured from an early age and resulted in his documentation and mapping of more than 300 sawmills and 160 kilometres of timber tramlines, which were built in the area from the 1850s to the mid 20th century.\n\nHoughton worked at Sovereign Hill Historical Gold Mining Village and the Gold Museum in Ballarat and undertook assessment of the archives of the Queensland Railways, before operationally establishing the Geelong Heritage Centre as its foundation Director in 1979, where he held the role of archivist with the Geelong Historical Records Centre for many decades, and was instrumental in assisting the Geelong Historical Society to collate and compile records which formed the basis of the Heritage Centre archival collection.\n\nHoughton's primary research on sawmills and timber tramways has been used as the basis for comprehensive assessments of the value of forest heritage sites, for example by the Victorian Department of Conservation and Natural Resources, for conservation management of heritage places, and also to further the investigation of domestic and spatial arrangements of isolated bush settlements.\nHis publications include histories of the sawmills and tramways of the Wombat and Otway Forests and have been described as \"...part of his substantial legacy ... of the lives led by timber-getters, road-makers, railway workers, farmers, and others in the communities that battled with the high rainfall, heavily timbered, and steep landscapes of this unique part of Victoria.\" \n\nHis research has been acknowledged by the Australian Forest History Society, while Gregg Borschmann, in the People's Forest Oral History Project, noted that he had:\n\nHoughton's contribution to forestry history has been recognized in a number of recent surveys of Australian and Victorian forest history, particularly in undertaking the primary field work which has relocated the isolated bush settlements, mill sites and tramway networks, and in the compilation of oral histories. He provided a large proportion of the entries to the first annotated bibliography of forest history, was a co-founder of, and subsequently contributed to most of the national conferences on Australian forest history since its inception. He is also credited as a major contributor to the reinvigoration of heritage protection and tourism in forest areas through his publications and promotion of timber tramway trails.\n\nHoughton is club historian for The Geelong Club for which he has written several histories.\n\nMost of Houghton's research has been self-published, and distributed through the Light Railway Research Society of Australia, while he has also been a regular contributor to the LRRSA Newsletter, their Journal \"Light Railways\" and the newsletter of the Australian Forest History Society.\n"}
{"id": "57024890", "url": "https://en.wikipedia.org/wiki?curid=57024890", "title": "Panaglide", "text": "Panaglide\n\nPanaglide is a brand of camera stabilizer mounts for motion picture cameras made by Panavision.\n\nThe Panaglide steadies the camera operator's movement, allowing for a smooth shot.\n\nThe Panaglide was used in such films as \"Halloween\" and Terrence Malick's \"Days of Heaven\", which was the first film to use the Panaglide.\n\n"}
{"id": "48138738", "url": "https://en.wikipedia.org/wiki?curid=48138738", "title": "Panel PC", "text": "Panel PC\n\nPanel PC, typically attached with an LCD, is incorporated into the same enclosure as the motherboard and other electronic components. These are typically panel mounted and often incorporate touch screens for user interaction.\n\n"}
{"id": "2609259", "url": "https://en.wikipedia.org/wiki?curid=2609259", "title": "Panel beater", "text": "Panel beater\n\nPanel beater or panelbeater is a term used in some Commonwealth countries to describe a person who repairs vehicle bodies back to their factory state after having been damaged (e.g., after being involved in a collision). In the United States and Canada, the same job is done by an auto body mechanic.\n\nPanel beaters repair body work using skills such as planishing and metalworking techniques, welding, use of putty fillers, and other skills. Accident repair may require the panel beater to repair or replace parts of a vehicle. These parts may be made from various metals including steels and alloys, many different plastics, fibreglass and others.\n\nThe common panel beater will work on everyday vehicles, cars, vans, 4WDs for example. Specialised areas include repairs to motorcycles, trucks and even aircraft. Some panel beaters also work exclusively on vehicle restorations, and do not repair smash work at all. Others may specialise in body customisation such as is seen on hot rods etc.\n\nTraining to become a panel beater is done by completing a trade apprenticeship. For the most part these apprenticeships are around three \nyears long, but can be completed earlier. These usually consist of three years on the job training mixed with schooling at a trade school or TAFE. The fourth year is usually on the job training alone.\n\nSpecial equipment examples:\n\n\nSpecial equipment for restoration, advanced panel repair or panel fabrication from scratch include: \n"}
{"id": "49188762", "url": "https://en.wikipedia.org/wiki?curid=49188762", "title": "Peer-to-peer ridesharing", "text": "Peer-to-peer ridesharing\n\nPeer-to-peer ridesharing can be divided along the spectrum from commercial, for-fee transportation network companies (TNC) to for-profit ridesharing services to informal nonprofit peer-to-peer carpooling arrangements. The term \"transportation network company\" comes from a 2013 California Public Utilities Commission ruling that decided to make the TNC revenue model legal.\n\nEssentially all modern peer-to-peer ridesharing schemes rely on web application and mobile app technology.\n\nIn the early 2010s, several transportation network companies were introduced. These were advertised as ridesharing, but dispatch drivers in a fashion similar to a taxi service, where they do not share a destination with passengers. The first such service to appear on the market was the San Francisco–based company Sidecar (launched in 2011). Transportation experts have called these services \"ridesourcing\" to clarify that drivers do not share a destination with their passengers; the app simply outsources rides to commercial drivers. Despite multiple efforts to re-name the category, it still is commonly referred to as, \"ridesharing.\"\n\nReal-time ridesharing (also known as instant ridesharing, dynamic ridesharing, ad-hoc ridesharing, on-demand ridesharing, and dynamic carpooling) is a service that arranges one-time shared rides on very short notice. This type of carpooling generally makes use of three recent technological advances:\n\nThese elements are coordinated through a network service, which can instantaneously handle the driver payments and match rides using an optimization algorithm.\n\nLike carpooling, real-time ridesharing is promoted as a way to better utilize the empty seats in most passenger cars, thus lowering fuel usage and transport costs. It can serve areas not covered by a public transit system and act as a transit feeder service. Ridesharing is also capable of serving one-time trips, not only recurrent commute trips or scheduled trips. \n\nReal-time ridesharing is especially suitable for daily commuting compared to driving alone. Because such trips tend to happen at peak travel times, when traffic jams cause cars to pollute an 80% more, additional benefits for the urban environment and climate change mitigation are expected by a reduction in the number of cars riding daily by the cities with a single occupant, and their related CO and NO emissions.\n\nA 2010 survey at the University of California, Berkeley found 20% of respondents willing to use real-time ridesharing at least once a week; and real-time ridesharing was more popular among current drive-alone commuters (30%) than transit or non-motorized commuters. The top obstacles to using real-time ridesharing were short trip lengths and the added time of ride logistics.\n\nEarly real-time ridesharing projects began in the 1990s, but they faced obstacles such as the need to develop a user network and a convenient means of communication. Gradually the means of arranging the ride shifted from telephone to internet, email, and smartphone; and user networks were developed around major employers and universities. As of 2006, the goal of taxi-like responsiveness still generally eluded the industry; \"next day\" responsiveness was considered the state of the art. More recently taxi-sharing systems that accept taxi passengers’ real-time ride requests via smartphones have been proposed and studied.\n\nA number of technology companies based in San Francisco premiered apps for real-time ridesharing (as well as ride-hailing where the driver does not share a destination with passengers) around 2012. However, in the fall of 2012, the California Public Utilities Commission issued a cease and desist letter to rideshare companies Lyft, Uber, Wingz, and Sidecar, and fined each $20,000. In 2013 an agreement was reached reversing those actions, creating a new category of service called \"Transportation Network Companies\" to cover both real-time and scheduled ride-sharing companies. Transportation Network Companies have faced regulatory opposition in many other cities, including Los Angeles, Chicago, New York City, and Washington, D.C.\n\nTwo dynamic ridesharing pilots in Norway received government funds from Transnova in 2011. One pilot in Bergen had 31 passengers in private cars during one day. Thirty-nine users acted as drivers or passengers between June 30 and September 15 with four ridesharing episodes or more. The phone apps that was used was Avego Driver and HentMEG.no cell client, a prototype developed for the NPRA of Norway. The other pilot is run by the company Sharepool.\n\nSome more advanced real-time ridesharing features have been proposed but not implemented. For example, longer trips might be facilitated using \"multihop\" matches in which passengers change cars to reach their final destination.\n\n"}
{"id": "2466815", "url": "https://en.wikipedia.org/wiki?curid=2466815", "title": "Plug compatible", "text": "Plug compatible\n\nPlug compatible refers to \"hardware that is designed to perform exactly like another vendor's product.\"\n\nThe term PCM can refer to\n\nThe term PCM was originally applied to manufacturers who made replacements for IBM peripherals. Later this term was used to refer to IBM-compatible computers.\n\nBefore the rise of the PCM peripheral industry, computing systems were either\n\nMany peripherals were originally designed to be used with a specific central processing unit (CPU)\n\nThe term PCM was originally applied to manufacturers who made replacements for IBM peripherals.\n\nThe first example of plug compatible IBM subsystems were tape drives and controls offered by Telex beginning 1965. Memorex in 1968 was first to enter the IBM plug-compatible disk followed shortly thereafter by a number of suppliers such as \nCDC, Itel, and Storage Technology Corporation. This was boosted by the world's largest user of computing equipment in both directions.\n\nUltimately plug-compatible products were offered for most peripherals and system main memory.\n\nA plug-compatible machine is one that has been designed to be backward compatible with a prior machine. In particular, a new computer system that is plug-compatible has not only the same connectors and protocol interfaces to peripherals, but also binary code compatibility—it runs the same software as the old system. A plug compatible manufacturer or PCM is a company that makes such products.\n\nOne recurring theme in plug-compatible systems is the ability to be bug compatible as well. That is, if the forerunner system had software or interface problems, then the successor must have (or simulate) the same problems. Otherwise, the new system may generate unpredictable results, defeating the full compatibility objective. Thus, it is important for customers to understand the difference between a \"bug\" and a \"feature\", where the latter is defined as an intentional modification to the previous system (e.g. higher speed, lighter weight, smaller package, better operator controls, etc.).\n\nThe original example of PCM mainframes was the Amdahl 470 mainframe computer which was plug-compatible with the IBM System 360 and 370, costing millions of dollars to develop. An IBM customer could literally remove the 360 or 370 on Friday, install the Amdahl 470, attach the same connectors from the peripherals to the channel interfaces, and have the new mainframe up and running the same software on Sunday night. Unfortunately, system status indicators for operators of the new system were very different, which introduced a learning curve for operators and service technicians.\n\nSimilar systems were available from Comparex, Fujitsu and Hitachi. Not all were large systems.\n\nMost of these system vendors eventually left the PCM market.\n\nThe term may also be used to define replacement criteria for other components available from multiple sources. For example, a plug-compatible cooling fan may need to have not only the same physical size and shape, but also similar capability, run from the same voltage, use similar power, attach with a standard electrical connector, and have similar mounting arrangements. Some non-conforming units may be re-packaged or modified to meet plug-compatible requirements, as where an adapter plate is provided for mounting, or a different tool and instructions are supplied for installation, and these modifications would be reflected in the bill of materials for such components. Similar issues arise for computer system interfaces when competitors wish to offer an easy upgrade path.\n\nIn general, plug-compatible systems are designed where industry or de facto standards have rigorously defined the environment, and there is a large installed population of machines that can benefit from third-party enhancements. \"Plug compatible\" does not mean identical replacement. However, nothing prevents a company from developing follow-on products that are backwards compatible with its own early products.\n\n"}
{"id": "15149616", "url": "https://en.wikipedia.org/wiki?curid=15149616", "title": "Presentation folder", "text": "Presentation folder\n\nA presentation folder is a kind of folder that holds loose papers or documents together for organization and protection. Presentation folders usually consist of a sheet of heavy paper stock or other thin, but stiff, material which is folded in half with pockets in order to keep paper documents. Presentation folders function much like that of a file folder for organizational purposes. They can be either printed or plain and can be used, amongst other things, as a tool for business presentations to customers to aid in the sales process.\n\nPresentation folders come in many different styles to suit a variety of purposes. Most all are produced by a company to provide marketing for a product (business) and/or service, but they can fulfil other functions. A few examples would be a company producing a new product and wanted to show their customers all the benefits of that product in an organized fashion, or folders used to organise documents for distribution to delegates at a conference.\n\nSome types of presentation folders:\n\nExamples\n\n\n"}
{"id": "57619067", "url": "https://en.wikipedia.org/wiki?curid=57619067", "title": "Printed electronic circuit", "text": "Printed electronic circuit\n\nA printed electronic circuit (PEC) was an ancestor of the hybrid integrated circuit (IC). PECs were common in tube (valve) equipment from the 1940s through the 1970s.\n\nCouplate was the Centralab trademark, whilst Sprague called them BulPlates. Aerovox just used the generic PEC.\n\nPECs contained only resistors and capacitors arranged in circuits to simplify construction of tube equipment. Also their voltage ratings were suitable for tubes. Later hybrid ICs contained at least transistors and often monolithic integrated circuits. Their voltage ratings were suitable for the transistors they contained.\n"}
{"id": "45047140", "url": "https://en.wikipedia.org/wiki?curid=45047140", "title": "QoS Class Identifier", "text": "QoS Class Identifier\n\nQoS Class Identifier (QCI) is a mechanism used in 3GPP Long Term Evolution (LTE) networks to ensure bearer traffic is allocated appropriate Quality of Service (QoS). Different bearer traffic requires different QoS and therefore different QCI values. QCI value 9 is typically used for the default bearer of a UE/PDN for non privileged subscribers.\n\nTo ensure that bearer traffic in LTE networks is appropriately handled, a mechanism is needed to classify the different types of bearers into different classes, with each class having appropriate QoS parameters for the traffic type. Examples of the QoS parameters include Guaranteed Bit Rate (GBR) or non-Guaranteed Bit Rate (non-GBR), Priority Handling, Packet Delay Budget and Packet Error Loss rate. This overall mechanism is called QCI.\n\nThe QoS concept as used in LTE networks is class-based, where each bearer type is assigned one QoS Class Identifier (QCI) by the network. The QCI is a scalar that is used within the access network (namely the eNodeB) as a reference to node specific parameters that control packet forwarding treatment, for example scheduling weight, admission thresholds and link-layer protocol configuration.\n\nThe QCI is also mapped to transport network layer parameters in the relevant Evolved Packet Core (EPC) core network nodes (for example, the PDN Gateway (P-GW), Mobility Management Entity (MME) and Policy and Charging Rules Function (PCRF)), by preconfigured QCI to Differentiated Services Code Point (DSCP) mapping. \n\nAccording to 3GPP TS 23.203, 9 QCI values in Rel-8 (13 QCIs Rel-12, 15 QCIs Rel-14) are standardized and associated with QCI characteristics in terms of packet forwarding treatment that the bearer traffic receives edge-to-edge between the UE and the P-GW. Scheduling priority, resource type, packet delay budget and packet error loss rate are the set of characteristics defined by the 3GPP standard and they should be understood as guidelines for the pre-configuration of node specific parameters to ensure that applications/services mapped to a given QCI receive the same level of QoS in multi-vendor environments as well as in roaming scenarios. The QCI characteristics are not signalled on any interface.\n\nThe following table illustrates the standardized characteristics as defined in the 3GPP TS 23.203 standard \"Policy and Charging Control Architecture\".\n\nEvery QCI (GBR and Non-GBR) is associated with a Priority level. Priority level 0.5 is the highest Priority level. If congestion is encountered, the lowest Priority level traffic would be the first to be discarded.\n\nQCI-65, QCI-66, QCI-69 and QCI-70 were introduced in 3GPP TS 23.203 Rel-12.\n\nQCI-75 and QCI-79 were introduced in 3GPP TS 23.203 Rel-14.\n\n"}
{"id": "24887696", "url": "https://en.wikipedia.org/wiki?curid=24887696", "title": "Quantapoint", "text": "Quantapoint\n\nQuantapoint, Inc. is a technology and services company that develops and uses patented 3D laser scanning hardware and software. Quantapoint creates a Digital Facility using 3D laser scanning and then provides visualization, analysis, quality control, decision support and documentation services for buildings, museums, refineries, chemical plants, nuclear and fossil-fuel power plants, offshore platforms and other structures.\n\nQuantapoint was founded as \"KT\", Inc (or \"K2T\") in Pittsburgh, Pennsylvania in 1991 by Eric Hoffman, Pradeep Khosla, Takeo Kanade and other Carnegie Mellon University faculty members. \"KT\" focused on creating custom robotics and 3D range-finding imaging systems to help them navigate complex environments. The most notable are the laser range-finding system created in 1992 for the DANTE walking robot that explored Mount Erebus in Antarctica as part of a NASA sponsored competition and the 360-degree phase-based 3D laser scanner named SceneModeler created in 1997.\n\nThe company name was changed to Quantapoint in 1999 to reflect the focus on 3D laser scanning hardware, software and services.\n\nInitially, Quantapoint focused on using 3D laser scanning to \"digitize\" buildings and create 2D drawings, 3D models and/or other animations or visualizations for renovations, additions or historic preservation. Notable projects include the Museum of Modern Art, the Theban Mapping Project in the Valley of the Kings, Monticello and the Guggenheim Museum.\n\nSince 2002, Quantapoint has served the chemical, petroleum and power industries both globally and within the United States. Quantapoint has also worked with the United States General Services Administration (GSA).\n\nQuantapoint has received several patents and awards for the 3D laser scanner hardware and 3D laser scanning data software that it has developed.\n\nQuantapoint uses both its own 3D laser scanner hardware, the SceneModeler 5 and SceneModeler 9, and the Photon from Faro Systems. Quantapoint has a fleet of more than twenty (20) 3D laser scanners.\n\nThe Quantapoint Digital Facility consists of the following:\n\nQuantapoint provides the following software for using the 3D laser scanning data in the Digital Facility:\n\nQuantapoint has alliances or software development relationships to integrate laser data with software from Autodesk, AVEVA, Bentley Systems and Intergraph.\n\nQuantapoint has locations within the United States (Pittsburgh, Houston, Los Angeles), England, Scotland and Nigeria. Quantapoint also uses representatives in various countries, such as Mexico, Venezuela, South Africa, Malaysia and Brazil.\n\nQuantapoint has received the following awards for its laser scanning technology and services:\n\nQuantapoint has published or been included in the articles from the following magazines:\n\nQuantapoint has been issued the following patents in the United States and has filed for similar patents in the EU, Canada and Japan:\n"}
{"id": "11112511", "url": "https://en.wikipedia.org/wiki?curid=11112511", "title": "Radio acoustic sounding system", "text": "Radio acoustic sounding system\n\nA radio acoustic sounding system (RASS) is a system for measuring the atmospheric lapse rate using backscattering of radio waves from an acoustic wave front to measure the speed of sound at various heights above the ground. This is possible because the compression and rarefaction of air by an acoustic wave changes the dielectric properties, producing partial reflection of the transmitted radar signal.\nFrom the speed of sound, the temperature of the air in the planetary boundary layer can be computed.\nThe maximum altitude range of RASS systems is typically 750 meters, although observations have been reported up to 1.2 km in moist air.\n\nThe principle of operation behind RASS is as follows: Bragg scattering occurs when acoustic energy (i.e., sound) is transmitted into the vertical beam of a radar such that the wavelength of the acoustic signal matches the half-wavelength of the radar. As the frequency of the acoustic signal is varied, strongly enhanced scattering of the radar signal occurs when the Bragg match takes place. \n\nWhen this occurs, the Doppler shift of the radar signal produced by the Bragg scattering can be determined, as well as the atmospheric vertical velocity. Thus, the speed of sound as a function of altitude can be measured, from which virtual temperature (TV) profiles can be calculated with appropriate corrections for vertical air motion. The virtual temperature of an air parcel is the temperature that dry air would have if its pressure and density were equal to\nthose of a sample of moist air. As a rule of thumb, an atmospheric vertical velocity of 1 m/s can alter a TV observation by 1.6°C.\n\nRASS can be added to a radar wind profiler or to a sodar system. In the former case, the necessary acoustic subsystems must be added to the radar wind profiler to generate the sound signals and to perform signal processing. When RASS is added to a radar profiler, three or four vertically pointing acoustic sources (equivalent to high quality stereo loud speakers) are placed around the radar wind profiler's antenna, and electronic subsystems are added that include the acoustic power amplifier and the signal generating circuit boards. The acoustic sources are used only to transmit sound into the vertical beam of the radar, and are usually encased in noise suppression enclosures to minimize nuisance effects that may bother nearby neighbors or others in the vicinity of the instrument.\n\nWhen RASS is added to a sodar, the necessary radar subsystems are added to transmit and receive the radar signals and to process the radar reflectivity information. Since the wind data are obtained by the sodar, the radar only needs to sample along the vertical axis. The sodar transducers are used to transmit the acoustic signals that produce the Bragg scattering of the radar signals, which allows the speed of sound to be measured by the radar.\n\nThe vertical resolution of RASS data is determined by the pulse length(s) used by the radar. RASS sampling is usually performed with a 60 to 100 meter pulse length. Because of atmospheric attenuation of the acoustic signals at the RASS frequencies used by boundary layer radar wind profilers, the altitude range that can be sampled is usually 0.1 to 1.5 km, depending on atmospheric conditions (e.g., high wind velocities tend to limit RASS altitude coverage to a few hundred meters because the acoustic signals are blown out of the radar beam).\n"}
{"id": "34268389", "url": "https://en.wikipedia.org/wiki?curid=34268389", "title": "Roger &amp; Gallet", "text": "Roger &amp; Gallet\n\nRoger & Gallet (also Roger et Gallet) is a French perfume company founded by merchant Charles Armand Roger and banker Charles Martial Gallet in 1862. They purchased a Parisian perfumery business founded in 1806 by Jean Marie Joseph Farina, grand-grand-nephew of Giovanni Maria Farina, the creator of Eau de Cologne. Roger & Gallet then won a legal dispute over the right to use the Farina family name, and the company now owns the rights to \"Eau de Cologne extra vieille\", in contrast to the \"Original Eau de Cologne\" from Cologne. \n\nRoger & Gallet specialized in toilet soap which was produced in a large factory near Paris. The company made a name for itself with its luxurious bath soaps, which in 1879 first appeared in their signature round shape, with the crinkled silk paper and seal, still in use today. Later, they proved successful with the newly synthesized fragrance of violet, for which they held the French rights, producing perfumes such as \"Vera Violetta\".\n\nIn 1975, Roger & Gallet was acquired by the Sanofi Group, who in turn sold the brand to Gucci in 1999. L'Oreal acquired Roger & Gallet from Gucci, a subsidiary at that time of PPR, in 2008.\n"}
{"id": "29083", "url": "https://en.wikipedia.org/wiki?curid=29083", "title": "Segway PT", "text": "Segway PT\n\nThe Segway PT (originally Segway HT) is a two-wheeled, self-balancing personal transporter by Segway Inc. Invented by Dean Kamen and brought to market in 2001. \"HT\" is an initialism for 'human transporter' and \"PT\" for 'personal transporter'.\n\nThe Segway PT (referred to at the time as the Segway HT) was developed from the self-balancing iBOT wheelchair which was initially developed at University of Plymouth, in conjunction with BAE Systems and Sumitomo Precision Products. Segway's first patent was filed in 1994 and granted in 1997 followed by others including one submitted in June 1999 and granted in October 2001.\n\nThe invention, development, and financing of the Segway was the subject of a book and a leak of information prior to publication of the book and the launch of the product led to excited speculation about the device and its importance. John Doerr speculated that it would be more important than the Internet. \"South Park\" devoted an episode to making fun of the hype before the product was released. Steve Jobs was quoted as saying that it was \"as big a deal as the PC\", (but later retracted that saying that it \"sucked\", presumably referring to \"the design\" but commenting about the boutique price, asking, \"You're \"sure\" your market is upscale consumers for transportation?\") The device was unveiled 3 December 2001, following months of public speculation, in Bryant Park, New York City, on the ABC News morning program \"Good Morning America\" with the first units delivered to customers in early 2002.\n\nThe original Segway models were activated for three different speed settings: , with faster turning and . Steering of early versions was controlled using a twist grip that varied the speeds of the two motors. The range of the p-Series was on a fully charged nickel metal hydride (NiMH) battery with a recharge time of 4–6 hours. In September 2003, the Segway PT was recalled, because if users ignored repeated low battery warnings on the PTs, it could ultimately lead them to fall. With a software patch to version 12.0, the PT would automatically slow down and stop in response to detecting low battery power.\n\nIn August 2006 Segway discontinued all previous models and introduced the i2 and x2 products which were steered by leaning the handlebars to the right or left, had a maximum speed of from a pair of Brushless DC electric motor with regenerative braking and a range of up to , depending on terrain, riding style and state of the batteries. Recharging took 8–10 hours. The i2 and x2 also introduced the wireless InfoKey which could show mileage and a trip odometer, as well as put the Segway into Security mode, which locked the wheels and set off an alarm if it was moved, and could also be used to turn on the PT from up to away.\n\nVersions of the product prior to 2011 included (in order of release):\n\nIn March 2014, Segway announced third generation designs, including the i2 SE and x2 SE sport, new LeanSteer frame and powerbase designs, with integrated lighting.\n\nNinebot Inc., a Beijing-based transportation robotics startup and a Segway rival, acquired Segway in April 2015 having raised $80M from Xiaomi and Sequoia Capital. \n\nIn June 2016 the company launched the Segway miniPRO, a smaller self-balancing scooter.\n\n the following self-balancing scooters are available from Segway (For other Segway products see Segway Inc.):\n\n\n\nThe dynamics of the Segway PT are similar to a classic control problem, the inverted pendulum. It uses brushless DC electric motors in each wheel powered by lithium-ion batteries with balance achieved using tilt sensors, and gyroscopic sensors developed by BAE Systems' Advanced Technology Centre. The wheels are driven forward or backward as needed to return its pitch to upright.\n\nIn 2011 the Segway i2 was being marketed to the emergency medical services community. The special police forces trained to protect the public during the 2008 Summer Olympics used the Segway for mobility.\n\nThe Segway miniPro is also available to be used as the mobility section of a robot.\n\nDisability Rights Advocates for Technology promoted the use of the Segway PT on sidewalks as an Americans with Disabilities Act of 1990 (ADA) issue. Segway Inc. cannot however market these devices in the US as medical devices (as per agreement with Johnson & Johnson with regard to the iBOT, a self-balancing wheelchair) and they have not been approved by the Food and Drug Administration as a medical device.\n\nThe maximum speed of the Segway PT is . The product is capable of covering on a fully charged lithium-ion battery, depending on terrain, riding style, and the condition of the batteries. The U.S. Consumer Product Safety Commission does not have Segway-specific recommendations but does say that bicycle helmets are adequate for \"low-speed, motor-assisted\" scooters.\n\n"}
{"id": "33482931", "url": "https://en.wikipedia.org/wiki?curid=33482931", "title": "Smart traffic light", "text": "Smart traffic light\n\nSmart traffic lights or Intelligent traffic lights are a vehicle traffic control system that combines traditional traffic lights with an array of sensors and artificial intelligence to intelligently route vehicle and pedestrian traffic.\n\nA technology for smart traffic signals has been developed at Carnegie Mellon University and is being used in a pilot project in Pittsburgh in an effort to reduce vehicle emissions in the city. Unlike other dynamic control signals that adjust the timing and phasing of lights according to limits that are set in controller programming, this system combines existing technology with artificial intelligence.\n\nThe signals communicate with each other and adapt to changing traffic conditions to reduce the amount of time that cars spend idling. Using fiber optic video receivers similar to those already employed in dynamic control systems, the new technology monitors vehicle numbers and makes changes in real time to avoid congestion wherever possible. Initial results from the pilot study are encouraging: the amount of time that motorists spent idling at lights was reduced by 40% and travel times across the city were reduced by 25% .\n\nCompanies involved in developing smart traffic management systems include BMW and Siemens, who unveiled their system of networked lights in 2010. This system works with the anti-idling technology that many cars are equipped with, to warn them of impending light changes. This should help cars that feature anti-idling systems to use them more intelligently, and the information that networks receive from the cars should help them to adjust light cycling times to make them more efficient.\n\nA new patent appearing March 1st, 2016 by John F. Hart Jr. is for a \"Smart\" traffic control system that \"sees\" traffic approaching the intersections and reacts according to what is needed to keep the flow of vehicles at the most efficient rate. By anticipating the needs of the approaching vehicles, as opposed to reacting to them after they arrive and stop, this system has the potential to save motorist time while cutting down harmful emissions.\n\nRomanian and US research teams believe that the time spent by motorists waiting for lights to change could be reduced by over 28% with the introduction of smart traffic lights and that CO emissions could be cut by as much as 6.5%.\n\nA major use of Smart traffic lights could be as part of public transport systems. The signals can be set up to sense the approach of buses or trams and change the signals in their favour, thus improving the speed and efficiency of sustainable transport modes.\n\nThe main stumbling block to the widespread introduction of such systems is the fact that most vehicles on the road are unable to communicate with the computer systems that town and city authorities use to control traffic lights. However, the trial in Harris County, Texas, referred to above, uses a simple system based on signals received from drivers' cell phones and it has found that even if only a few drivers have their phone switched on, the system is still able to produce reliable data on traffic density. \nResults from simulations, where an algorithm was applied to control traffic lights from driver mobile phone positions, showed that a 30% of drivers having the system would be enough to benefit also drivers that are not using the system (reduced travel times).\nThis means that the adoption of smart traffic lights around the world could be started as soon as a reasonable minority of vehicles were fitted with the technology to communicate with the computers that control the signals rather than having to wait until the majority of cars had such technology.\n\nMeanwhile, in the United Kingdom, lights that change to red when sensing that an approaching motorist is traveling too fast are being trialled in Swindon to see if they are more effective at reducing the number of accidents on the road than the speed cameras that preceded them and which were removed following a council decision in 2008. These lights are more focused on encouraging motorists to obey the law but if they prove to be a success then they could pave the way for more sophisticated systems to be introduced in the UK.\n\nIn addition to the findings of the Romanian and US researchers mentioned above, scientists in Dresden, Germany came to the conclusion that smart traffic lights could handle their task more efficiently without human interface.\n\n"}
{"id": "33573510", "url": "https://en.wikipedia.org/wiki?curid=33573510", "title": "Special input/output", "text": "Special input/output\n\nSpecial input/output (Special I/O or SIO) are inputs and/or outputs of a microcontroller designated to perform specialized functions or have specialized features. \n\nSpecialized functions can include:\n\nSome kinds of special I/O functions can sometimes be emulated with general-purpose input/output and bit banging software.\n\n"}
{"id": "10679172", "url": "https://en.wikipedia.org/wiki?curid=10679172", "title": "Suspended athletic courts", "text": "Suspended athletic courts\n\nMany people have started to use suspended athletic courts to cover old athletic courts like tennis courts and basketball courts. The surfaces have been approved for play by the USTA (United States Tennis Association). Most outdoor athletic courts in the U.S.A. are made of asphalt, blacktop or concrete where the stress to the player’s body is maximized because of the surface's hardness.\n\nWhen a suspended court surface is installed over a hard surface, such as concrete, it can reduce the stress as well as hide cracks and reduce the impact of birdbaths or puddles on the court. Orthopedic specialists agree that these suspended courts are the healthiest and safest outdoors court sports surfaces for tennis and basketball.\n\nMost of the tile products are injection molded Injection molding of polypropylene plastic and measure between 25 cm and 12\" square, with thickness heights ranging from 1/2\" to 3/4\".\n\nThese suspended athletic court are available in many colors and can be configured to fit any size space. They have become particularly popular with homeowners wanting small or full size backyard multi-sport gamecourts. The most popular court sports played are tennis, basketball, volleyball and Futsal.\n\nMany municipalities are also recognizing the benefits of resurfacing tennis and basketball courts with modular sports tile because it needs little to no maintenance and typically lasts 25 years or more.\n\nWhile there are many companies that manufacture suspended courts, some of the better known brands include:\nhttp://pro-bounce.co.uk/hexacourt/\nThese courts are often referred to as game courts.\n"}
{"id": "24718638", "url": "https://en.wikipedia.org/wiki?curid=24718638", "title": "Sustainable Harvest International", "text": "Sustainable Harvest International\n\nSustainable Harvest International (SHI) is a non-profit organization, based in the United States, that addresses the tropical deforestation crisis in Central America and provides farmers with sustainable alternatives to slash-and-burn agriculture. SHI is dedicated to working with farming communities in Honduras, Belize, Nicaragua, and Panama to overcome poverty and restore tropical forests in these nations.\n\nSHI was founded in 1997 by Florence Reed, who discovered first-hand the consequences of tropical deforestation while serving as a Peace Corps volunteer in Panamá in the early 1990s.\n\nFollowing completion of her Peace Corps service, Reed sought to bring about significant and permanent change throughout Central America. She met with a group of villages in Honduras that desired to implement sustainable techniques. Reed then promoted the project to an interested group of people that included university professors, small business owners, and non-profit executives. In May 1997, this group formed a Board of Directors and Sustainable Harvest International was incorporated as a nonprofit, 501(c)(3) organization.\n\nSince its inception, SHI has expanded its reach beyond a single program in Honduras, and now includes programs in Panamá, Belize and Nicaragua. SHI has also established La Fundacion Cosecha Sostenible Honduras (the Sustainable Harvest Honduras Foundation) as an independent affiliate which, following a transition period, has become responsible for its own management and funding.\n\nA primary objective of SHI is to facilitate implementation of a program that allows poor farmers to take responsibility for reversing environmental degradation and achieving economic viability within their own neighborhoods and countries.\n\nSHI launched the Smaller World Program with the aim of increasing cultural understanding and empowering the global community to create a more equitable and sustainable world. The program creates direct connections between groups and individuals who support SHI's work and the communities it serves.\n\n"}
{"id": "13179109", "url": "https://en.wikipedia.org/wiki?curid=13179109", "title": "TREX search engine", "text": "TREX search engine\n\nTREX is a search engine in the SAP NetWeaver integrated technology platform produced by SAP SE using columnar storage. The TREX engine is a standalone component that can be used in a range of system environments but is used primarily as an integral part of SAP products such as Enterprise Portal, Knowledge Warehouse, and Business Intelligence (BI, formerly SAP Business Information Warehouse). In SAP NetWeaver BI, the TREX engine powers the BI Accelerator, which is a plug-in appliance for enhancing the performance of online analytical processing. The name \"TREX\" stands for Text Retrieval and information EXtraction, but it is not a registered trademark of SAP and is not used in marketing collateral.\n\nTREX supports various kinds of text search, including exact search, boolean search, wildcard search, linguistic search (grammatical variants are normalized for the index search) and fuzzy search (input strings that differ by a few letters from an index term are normalized for the index search). Result sets are ranked using term frequency-inverse document frequency (tf-idf) weighting, and results can include snippets with the search terms highlighted.\n\nTREX supports text mining and classification using a vector space model. Groups of documents can be classified using query based classification, example based classification, or a combination of these plus keyword management.\n\nTREX supports structured data search not only for document metadata but also for mass business data and data in SAP Business Objects. Indexes for structured data are implemented compactly using data compression and the data can be aggregated in linear time, to enable large volumes of data to be processed entirely in memory.\n\nRecent developments include:\n\nThe first code for the engine was written in 1998 and TREX became an SAP component in 2000. The SAP NetWeaver BI Accelerator was first rolled out in 2005. As of Q1 2013, the current release of TREX is SAP NW 7.1.\n\nA security vulnerability in TREX was first identified and fixed in 2015 (see SAP Security Note 2234226). The vulnerability occurred due to lack of authentication in TREXnet, an internal communication protocol. The aforementioned patch fixed the problem by removing some critical functionality.\n\nLater on, ERPScan head of threat intelligence Mathieu Geli continued to look into the vulnerability and found that the vulnerability was still exploitable . Moreover, in case of successful attack, the vulnerability would allow a remote attacker to get full control over the server without authorization . The vulnerability has been finally patched via SAP Security Note 2419592.\n\n"}
{"id": "31826819", "url": "https://en.wikipedia.org/wiki?curid=31826819", "title": "Thai ceramics", "text": "Thai ceramics\n\nThai ceramics refers to ceramic art and pottery designed or produced as a form of Thai art. The tradition of Thai ceramics dates back to the third millennium BCE. Much of Thai pottery and ceramics in the later centuries was influenced by Chinese ceramics, but has always remained distinct by mixing indigenous styles with preferences for unique shapes, colors and decorative motifs. Thai pottery and ceramics were an essential part of the trade between Thai and its neighbors during feudalistic times, throughout many dynasties.\n\nThai ceramics show a continuous development through different clay types and methods of manufacturing since the prehistoric period and are one of the most common Thai art forms. The first type of Thai ceramics ever recorded was the Ban Chiang, dating back to about 3600 BCE. Sukhothai ware, the most famous style of Thai ceramics, is exported to many countries around the world today.\n\nMedieval Thai wares were especially influenced by Chinese celadons, and later by blue and white porcelain. \n\nThe earliest trace of Thai ceramics ever recorded is the Ban Chiang, said to date back to about 3600 BCE and found in what is the present day Udon Thani Province, Thailand. The ceramics were earthenware. Common forms of excavated artifacts were cylinders and round vases. The early pots were undecorated while the later ones were carved with geometric patterns and swirling designs. Each of the pieces was also found to have axial perforations which showed that people at that time had knowledge of using tools. \n\nThe second important prehistoric Thai ceramics is the Ban Kao which was in Kanchanburi Province. Unlike Ban Chiang, Ban Kao's wares were thinner and had a glossy surface finish. What is interesting is that there are a wide range of forms and shapes, some of which are similar to bronze wares of Han China. After the prehistoric period the kingdom that emerged at about 1st century CE was the Mons. They made considerable ceramics uses in relation to religious symbols in the form of figurines. Ceramics were also used as a building decorations. \n\nFollowing the Mons were the Khmers who appeared in about the 9th century CE Little is known about Khmer ceramics because archaeological research has focused on their great achievements in stone and bronze sculpture. The ceramics of Khmer era are quite interesting. Many of the designs include parts from animal and have a dark brown glaze finish. \n\nThe best known of all traditional Thai ceramics are those from Sukhothai and Sawankhalok. Sukhothai wares were generally treated with a creamy white slip and decorated in black with an opaque or greenish glaze. The most famous Sukhothai kiln is the Si Satchanalai. Examples of the wares can be found in many leading museums of the world. Sawankhalok products tend to be more finely made than the Sukhothai ones. These products are incised and often include animal shapes. Some of the original examples can be found in many private collections and museums today. Ceramics based on these styles are still made at present and widely exported, particularly to the Philippines and Indonesia.\n\nOne of the most famous examples of Thai pottery are from the Sukhothai period from the kilns of S(r)i Satchanalai, which is around Sawankalok in north-central Thailand. This period started in the 13th century CE and continued until the 16th century. The art reached its apex in the 14th century. Examples of Si Satchanalai can be found in many leading museums of the world.\n\nSukothai traded with these precious ceramics with its neighbours. The transport was often by ship across the oceans. A number of Si Satchanalai ceramics in excellent condition have been excavated in ship wrecks in the Gulf of Thailand, the Andaman Sea and other waters.\n\nBangkok, the capital of Thailand, was founded in 1782 and is represented by the Bencharong and Lai Nam Thong wares. It would seem that Bencharong ceramics first made their appearance during the final phases of the Ayutthaya period in the 18th century, while the Lai Nam Thong wares developed during the 19th century. Bencharong, meaning five colours in Thai, is a hand painted enamel over glazed ceramic. Bencharong was originally made in China and exclusively designed by Thai artists for Thai royals during the 18th – 19th centuries. Lai Nam Thong is an exclusive version of the Bencharong using gold embellishment instead of gold enamel. Both of these wares can be found in private collections of well-to-do citizens.\n\nThe Southeast Asian Ceramics Museum was opened in 2005 in Bangkok.\n\n\n\n"}
{"id": "6291603", "url": "https://en.wikipedia.org/wiki?curid=6291603", "title": "Twinfield", "text": "Twinfield\n\nTwinfield is an online financial accounting software package for small and medium enterprises, produced and maintained by Twinfield International, based in the Netherlands. It is used by over 15,000 subscribers in 20 countries.\n\nThe software is based on Software as a service (SaaS), so accounts can be accessed and analysed online, and the software is maintained by the provider, not the subscriber.\n\nIn 2005 Twinfield International received an Export Connection Award from Dutch Ministry of Economic Affairs for international focus and innovation.\n"}
{"id": "2964073", "url": "https://en.wikipedia.org/wiki?curid=2964073", "title": "William Samuel Henson", "text": "William Samuel Henson\n\nWilliam Samuel Henson (3 May 1812 – 22 March 1888) was a pre-Wright brothers aviation engineer and inventor.\n\nHenson was born in Nottingham, England on 3 May 1812. Henson was involved in lace-making in Chard and he obtained a patent on improved lace-making machines in 1835. Henson is best known as an early pioneer in aviation, but patented many other inventions, some of which are in wide use today. \n\nIn 1849 William Henson and his wife, Sarah, left England and moved to the United States, joining his father and settling in Newark, New Jersey. Henson never did any further aviation research while in the United States and worked as a machinist, civil engineer and inventor. He had 7 children, only 4 of whom lived to adulthood.\n\nHenson died on 22 March 1888 in Newark, New Jersey. He and most of his family were buried in Rosedale Cemetery in Orange, New Jersey.\n\nStarting c. 1838, Henson became interested in aviation. In April 1841 he patented an improved lightweight steam engine, and with fellow lacemaking-engineer John Stringfellow in c. 1842 he designed a large passenger-carrying steam-powered monoplane, with a wing span of 150 feet, which he named the \"Henson Aerial Steam Carriage\". He received a patent on it in 1843 along with Stringfellow.\n\nHenson, Stringfellow, Frederick Marriott, and D.E. Colombine, incorporated as the Aerial Transit Company in 1843 in England, with the intention of raising money to construct the flying machine. Henson built a scale model of his design, which made one tentative steam-powered \"hop\" as it lifted, or bounced, off its guide wire. Attempts were made to fly the small model, and a larger model with a 20-foot wing span, between 1844 and 1847, without success.\n\nHenson grew discouraged, married and emigrated in 1849 to the United States, while Stringfellow continued to experiment with aviation.\n\nHenson appeared as a character in a fictional newspaper story by Edgar Allan Poe, which recounted a supposed trans-Atlantic balloon trip, in which Henson was one of the passengers on the balloon.\n\nHenson and Stringfellow are frequently mentioned in books on the history of aviation. The Royal Aeronautical Society holds annual \"Henson-Stringfellow\" lectures; as of 2008 they have held 52. A glacier in Antarctica is named after him due to his work in aviation (: 64'06'S, 60'11'W).\n\nHenson and Stringfellow were referred to in the 1965 film \"The Flight of the Phoenix\".\n\nThe Aerial's wings were rectangular, and were formed by wooden spars covered with fabric, and braced, internally and externally, with wires. The Aerial Steam Carriage was to be powered by two contra-rotating six-bladed propellers mounted in the rear in a push-type system. The design follows earlier \"birdlike\" gliders, and the ideas of George Cayley, and Henson corresponded with Cayley in an attempt to obtain funding after the efforts to obtain the support of Parliament and sell stock failed. The Aerial Transit Company never built the largest version of the Aerial Steam Carriage because of the failed attempts with the medium-sized model. Henson, Stringfellow, Marriott and Colombine dissolved the company around 1848.\n\nThe Aerial Transit Company's publicist, Frederick Marriott, commissioned prints in 1843 depicting the Aerial Steam Carriage over the pyramids of Egypt, in India, and over London, England, and other places, which drew considerable interest from the public. The prints have appeared on several stamps of various countries. Marriott later became an aviation pioneer in California.\n\nHenson obtained a number of patents in widely varying areas. Major patents include:\n\nHenson invented the modern form of the razor, the 'T' shaped safety razor, and patented it in 1847: \"the cutting blade of which is at right angles with the handle, and resembles somewhat the form of a common hoe.\" While a major improvement on the previous form of safety razor, an additional improvement was needed to make safety razors common. In 1901, Gillette combined Henson's T-shaped safety razor with disposable blades, and produced the modern razor.\n\nHenson published a pamphlet on astronomy in 1871 suggesting that the solar system formed from cold dust and gas, and discussed how it could condense into meteors and comets, and further condense into planets, moons and the sun, in the process heating up. He also discusses how this would lead to the planets orbiting in the ecliptic and rotating in the same general plane.\n\nHenson created inventions in other areas as well. Among them were ice-making machines (1870), fabric waterproofing, and cistern-cleaning. He patented and submitted a proposal for an improved low-recoil breech-loading cannon design to the US Navy in 1861; it was rejected as impractical.\n\n\n\n\n"}
{"id": "56613687", "url": "https://en.wikipedia.org/wiki?curid=56613687", "title": "Young Woman Engineer of the Year Award", "text": "Young Woman Engineer of the Year Award\n\nThe Young Woman Engineer of the Year Awards are presented at the Institution of Engineering and Technology, London, England. Part of the IET Achievement Medals collection, the award was launched in 1978, and was originally known as the Girl Technician of the Year, until renamed in 1988. The award was first sponsored by the Caroline Haslett Memorial Trust, which was formed in 1945. It is now funded by the Institution of Engineering and Technology.\n\n\nMary George CBE was the Director and Secretary of the Electrical Association for Women. The prize is given annually to a young woman apprentice.\n\nThe winners so far have included:\n\nThe Women's Engineering Society Prize is awarded to a young woman engineer who demonstrates exceptional talent within engineering alongside a commitment to improving diversity within engineering.\n\nThe winners so far have included:\n"}
