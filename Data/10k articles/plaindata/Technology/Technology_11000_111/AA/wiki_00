{"id": "39146265", "url": "https://en.wikipedia.org/wiki?curid=39146265", "title": "Anis Ebeid", "text": "Anis Ebeid\n\nAnis Ebeid (1909–1988) is an Egyptian translator, known for Arabic subtitling of American movies. He was a pioneer in Arabic movie subtitling in the Middle East.\n\nHe graduated from Engineering college, and then travelled to Paris to study for the master's degree in Engineering. He was the first in the world to insert subtitles on 16-mm film. He kept the record for 40 years as the sole vendor of this service till 1944. The movie \"Romeo and Juliet\" was his first Arabic subtitled movie.\n\nIn 1940, he established \"Anis Ebeid Films\", a subtitling company based in Cairo, now a major subtitling service provider and film distribution agency in the Middle East.\n"}
{"id": "41417601", "url": "https://en.wikipedia.org/wiki?curid=41417601", "title": "Astronics Max-Viz", "text": "Astronics Max-Viz\n\nAstronics Max-Viz is an American company founded in Portland, Oregon on May 31, 2001 as Max-Viz, Inc. to design, manufacture and certify Enhanced Vision Systems (\"EVS\") primarily for use in the aerospace industry. Max-Viz EVS devices present real-time images of the external environment on aircraft cockpit monitors to improve pilot situational awareness under circumstances where visibility is impaired by weather or darkness. The company objective is to help the pilot see clearly and fly safely by providing visual information about where they are, where they are going and what is in their way. The Max-Viz EVS captures and enhances thermal infrared signals and can be combined with visible light as well as other electromagnetic energy sources. The company's systems are designed to be integrated with a variety of displays already in the aircraft cockpit.\n\nThe company came into being as a result of two inventions that allowed for possibility of a small lightweight design for EVS that could be deployed onto small private and commercial aircraft and helicopters.\n\nThe first invention was an uncooled focal plane array licensed from Honeywell (called a microbolometer) that could detect thermal infrared radiation, did not require a cryogenic cooling system (as conventional systems required in the 1990s) and could convert the electromagnetic thermal impulses into signals that could be displayed on a monitor. The second invention was a patented system to fuse video images from multiple sources developed by Dr. J. Richard Kerr (one of the original founders of Max-Viz). This invention enabled signals from a visible light source to be combined with the signals from an infrared source (like the Honeywell focal plane array mentioned above) and present a picture which Astronics describe as \"turning night into day\".\nThe Max-Viz start-up was financed with initial investments from a number of Portland area angel investors, strategic investments from FLIR Systems and, later, with venture capital funding from OVP Venture Partners, Alexander Hutton Venture Partners, Montlake Capital, and Highway 12 Ventures. Max-Viz licensed the microbolometer design from Honeywell for use in aviation and initially licensed the fusion patent (developed by Dr. Kerr) from \"FLIR Systems\". In 2012, the company was acquired by Astronics Corporation (NASDAQ: ATRO) as a wholly owned subsidiary.\n\nBy May 2011, the company had sold more than 1,000 EVS systems using 50+ aviation certifications on more than 200 different aircraft models including business jets, helicopters and various piston driven aircraft. Among those companies that offer the Max-Viz EVS at the factory on selected aircraft models are AgustaWestland, Beechcraft, Bell Helicopter, \"Cessna\", Cirrus Aircraft and Eurocopter. Max-Viz continues as a standalone business unit of Astronics Corporation and focuses its resources on core competencies involving packaging & design, image fusion and certification/installation of EVS systems.\n\nThe company's first product was the Max-Viz 1000 and was certified in 2003. It contained a single microbolometer to detect thermal radiation and was made of an exotic compound called Vanadium Oxide tuned to detect infrared waves in the 8 to 12 micron bandwidth. The sensor had a 53-degree field of view (slightly wider than normal 30 degree human vision) and produced a video signal that could be fed to a monitor using a standard RS-170, the standard NTSC signal used in analog displays. The Max-Viz 1000 system weighed about 5 pounds and required 10 watts during normal usage at 28 VDC. This product was produced from 2003 to 2008. The company's second product was the Max-Viz 2500 and was certified in 2005. This product contained two sensors; a long wave microbolometer like the one used in the Max-Viz 1000 and a short wave sensor that could pick up the incandescent lights from the airport environment. This system had a 30-degree field of view (equal to the field of view of human vision) and weighed 10 pounds. This product was produced from 2005 to 2010.\n\nThe Max-Viz 1500 was introduced in 2008 was an enhancement of the Max-Viz 1000. It offers an optical dual field of view (30° and 53°) and advanced image processing to avoid blooming and washout. Both the Max-Viz 1000 and Max-Viz 1500 use the same housing making it easier for existing Max-Viz 1000 customers to upgrade and preserving the investment in FAA certifications.\nThe Max-Viz 600 was also introduced in 2008. This product was specifically designed for the General Aviation community and is housed in ultra-lightweight high impact plastic weighing less than 1.2 pounds. The system is dual wavelength (infrared and visible light sensors) and uses a patented process to fuse the images.\n"}
{"id": "30863105", "url": "https://en.wikipedia.org/wiki?curid=30863105", "title": "Asymmetric multiprocessing", "text": "Asymmetric multiprocessing\n\nIn an asymmetric multiprocessing system (AMP), not all CPUs are treated equally; for example, a system might allow (either at the hardware or operating system level) only one CPU to execute operating system code or might allow only one CPU to perform I/O operations. Other AMP systems would allow any CPU to execute operating system code and perform I/O operations, so that they were symmetric with regard to processor roles, but attached some or all peripherals to particular CPUs, so that they were asymmetric with respect to the peripheral attachment. Asymmetric multiprocessing was the only method for handling multiple CPUs before symmetric multiprocessing (SMP) was available. It has also been used to provide less expensive options on systems where SMP was available. Additionally, AMP is used in applications that are dedicated, such as embedded systems, when individual processors can be dedicated to specific tasks at design time. \n\nMultiprocessing is the use of more than one CPU in a computer system. The CPU is the arithmetic and logic engine that executes user applications. With multiple CPUs, more than one set of program instructions can be executed at the same time. All of the CPUs have the same user-mode instruction set, so a running job can be rescheduled from one CPU to another.\n\nFor the room-size computers of the 1960s and 1970s, a cost-effective way to increase compute power was to add a second CPU. Since these computers were already close to the fastest available (near the peak of the price:performance ratio), two standard-speed CPUs were much less expensive than a CPU that ran twice as fast. Also, adding a second CPU was less expensive than a second complete computer, which would need its own peripherals, thus requiring much more floor space and an increased operations staff.\n\nNotable early AMP offerings by computer manufacturers were the Burroughs B5000, the DECsystem-1055, and the IBM System/360 model 65MP. There were also dual-CPU machines built at universities.\n\nThe problem with adding a second CPU to a computer system was that the operating system had been developed for single-CPU systems, and extending it to handle multiple CPUs efficiently and reliably took a long time. To fill this gap, operating systems intended for single CPUs were initially extended to provide minimal support for a second CPU. In this minimal support, the operating system ran on the “boot” processor, with the other only allowed to run user programs. In the case of the Burroughs B5000, the second processor's hardware was not capable of running \"control state\" code.\n\nOther systems allowed the operating system to run on all processors, but either attached all the peripherals to one processor or attached particular peripherals to particular processors.\n\nAn option on the Burroughs B5000 was “Processor B”. This second processor, unlike “Processor A” had no connection to the peripherals, though the two processors shared main memory, and Processor B could not run in Control State. The operating system ran only on Processor A. When there was a user job to be executed, it might be run on Processor B, but when that job tried to access the operating system the processor halted and signaled Processor A. The requested operating system service was then run on Processor A.\n\nOn the B5500, either Processor A or Processor B could be designated as Processor 1 by a switch on the engineer's panel, with the other processor being Processor 2; both processors shared main memory and had hardware access to the I/O processors hence the peripherals, but only Processor 1 could respond to peripheral interrupts. When a job on Processor 2 required an operating system service it would be rescheduled on Processor 1, which was responsible for both initiating I/O processor activity and responding to interrupts indicating completion. In practice, this meant that while user jobs could run on either Processor 1 or Processor 2 and could access intrinsic library routines that didn't require kernel support, the operating system would schedule them on the latter whenever possible.\n\nControl Data Corporation offered two configurations of its CDC 6000 series that featured two central processors. The CDC 6500 was a CDC 6400 with two central processors. The CDC 6700 was a CDC 6600 with the CDC 6400 central processor added to it.\n\nThese systems were organized quite differently from the other multiprocessors in this article. The operating system ran on the peripheral processors, while the user's application ran on the CPUs. Thus, the terms ASMP and SMP do not properly apply to these multiprocessors.\n\nDigital Equipment Corporation (DEC) offered a dual-processor version of its DECsystem-1050 which used two KA10 processors. This offering was extended to later processors in the PDP-10 line.\n\nDigital Equipment Corporation developed, but never released, a multiprocessor PDP-11, the PDP-11/74, running a multiprocessor version of RSX-11M. In that system, either processor could run operating system code, and could perform I/O, but not all peripherals were accessible to all processors; most peripherals were attached to one or the other of the CPUs, so that a processor to which a peripheral wasn't attached would, when it needed to perform an I/O operation on that peripheral, request the processor to which the peripheral was attached to perform the operation.\n\nDEC's first multi-processor VAX system, the VAX-11/782, was an asymmetric dual-processor system; only the first processor had access to the I/O devices.\n\nThe Univac 1108-II and its successors had up to three CPUs. These computers ran the UNIVAC EXEC 8 operating system, but it is not clear from the surviving documentation where that operating system was on the path from asymmetric to symmetric multiprocessing.\n\nTwo options were available for the IBM System/370 Model 168 for attaching a second processor. One was the IBM 3062 Attached Processing Unit, in which the second processor had no access to the channels, and was therefore similar to the B5000's Processor B or the second processor on a VAX-11/782. The other option offered a complete second CPU, and was thus more like the System/360 model 65MP.\n\n\n\n"}
{"id": "10893891", "url": "https://en.wikipedia.org/wiki?curid=10893891", "title": "Bauma (trade fair)", "text": "Bauma (trade fair)\n\nThe bauma (International Trade Fair for Construction Machinery, Building Material Machines, Mining Machines, Construction Vehicles and Construction Equipment) is the world's largest trade fair in the construction industry. The trade fair, which can be visited by anyone, is held every three years on the grounds of the Neue Messe München and lasts for seven days. The next fair will be held from 8 to 14 April 2019.\n\nThe Bauma 2013 was held from 15 to 21 April 2013, with approximately 3,420 exhibitors from 57 countries (2010: 3,150 exhibitors from 53 countries, 2007: 3,002 exhibitors from 48 countries)and attended by over 530,000 visitors (2010: approximately 415,000, 2007: 500,000). The 200,000 foreign visitors came from 200 different countries. The exhibition area was 570,000 m (2010: 555,000 m, 2007: 540,000 m), of which 180,000 m was within the hall areas. Partner region was the land of Indonesia. The last fair was held from 11 to 17 April 2016.\n\nBauma is the world’s largest trade fair based on the measured surface space. The exhibition area is spread throughout all the halls of Neue Messe München and the enlarged outdoor area for this exhibition.\n\nIn addition to the manufacturers of classical construction equipment (excavators, cranes, road rollers, etc.), the fair provides a large selection of manufacturers of tools (such as saws, drills, cutters), site equipment, formwork, scaffolding, formwork accessories, construction vehicles and machinery, such as equipment for the building material industry and mining.\n\nAn important branch of the fair is the \"bauma China\", which takes place every two years each November in Shanghai, China, serving the Southeast Asian market. Another international trade fair modeled after the Bauma is also the \"bC India - A Bauma Conexpo show\" - which was held from the 8 to 11th February, 2011 in Mumbai, India, for the first time. Bauma Conexpo India 2016 is being held for the fourth time in India from December 12 to 15, 2016 at the HUDA Ground in Gurgaon/Delhi. The last event was held in Greater Noida in December 2014, was attended by 635 companies from 25 countries and 26,018 specialist visitors.\n\nThe first exhibition took place in 1954, and every year after that until 1967, on a portion of the former airport Oberwiesenfeld, now the location of Olympiapark, Munich. The Bauma is organized by Messe München since 1966 and since 1998, Bauma is located at the Neue Messe München exhibition center in Riem, where the fair was the first major exhibition on the site.\n\n"}
{"id": "313830", "url": "https://en.wikipedia.org/wiki?curid=313830", "title": "Bulldozer", "text": "Bulldozer\n\nA bulldozer is a crawler (continuous tracked tractor) equipped with a substantial metal plate (known as a blade) used to push large quantities of soil, sand, rubble, or other such material during construction or conversion work and typically equipped at the rear with a claw-like device (known as a ripper) to loosen densely compacted materials.\n\nBulldozers can be found on a wide range of sites, mines and quarries, military bases, heavy industry factories, engineering projects and farms.\n\nThe term \"bulldozer\" correctly refers only to a tractor (usually tracked) fitted with a dozer blade.\n\nTypically, bulldozers are large and powerful tracked heavy equipment. The tracks give them excellent ground holding capability and mobility through very rough terrain. Wide tracks help distribute the bulldozer's weight over a large area (decreasing ground pressure), thus preventing it from sinking in sandy or muddy ground. Extra wide tracks are known as \"swamp tracks\" or \"LGP\" (low ground pressure) tracks. Bulldozers have transmission systems designed to take advantage of the track system and provide excellent tractive force.\n\nBecause of these attributes, bulldozers are often used in road building, construction, mining, forestry, land clearing, infrastructure development, and any other projects requiring highly mobile, powerful, and stable earth-moving equipment.\n\nAnother type of bulldozer is the wheeled bulldozer, which generally has four wheels driven by a 4-wheel-drive system and has a hydraulic, articulated steering system. The blade is mounted forward of the articulation joint, and is hydraulically actuated.\n\nThe bulldozer's primary tools are the blade and the ripper.\n\nThe word \"bulldozer\" is sometimes used inaccurately for other similar construction vehicles such as a large front loader.\n\nThe bulldozer blade is a heavy metal plate on the front of the tractor, used to push objects, and shove sand, soil, debris, and sometimes snow. Dozer blades usually come in three varieties:\n\nBlades can be fitted straight across the frame, or at an angle, sometimes using additional 'tilt cylinders' to vary the angle while moving. The bottom edge of the blade can be sharpened, e.g. to cut tree stumps.\n\nSometimes a bulldozer is used to push another piece of earth moving equipment known as a \"scraper\". The towed Fresno Scraper, invented in 1883 by James Porteous, was the first design to enable this to be done economically, removing the soil from the \"cut\" and depositing it elsewhere on shallow ground (\"fill\"). Many dozer blades have a reinforced center section with this purpose in mind, and are called \"bull blades\".\n\nIn military use, dozer blades are fixed on combat engineering vehicles and can optionally be fitted on other vehicles, such as artillery tractors such as the Type 73 or M8 Tractor. Dozer blades can also be mounted on main battle tanks, where it can be used to clear antitank obstacles, mines, and dig improvised shelters. Combat applications for dozer blades include clearing battlefield obstacles and preparing fire positions.\n\nThe ripper is the long claw-like device on the back of the bulldozer. Rippers can come as a single (single shank/giant ripper) or in groups of two or more (multi shank rippers). Usually, a single shank is preferred for heavy ripping. The ripper shank is fitted with a replaceable tungsten steel alloy tip, referred to as a 'boot'.\nRipping rock breaks the ground surface rock or pavement into small rubble easy to handle and transport, which can then be removed so grading can take place. With agricultural ripping, a farmer breaks up rocky or very hard earth (such as podzol hardpan), which is otherwise unploughable, in order to farm it. For example, much of the best land in the California wine country consists of old lava flows. The grower shatters the lava with heavy bulldozers so surface crops or trees can be planted.\nSome bulldozers are equipped with a less common rear attachment referred to as a stumpbuster, which is a single spike that protrudes horizontally and can be raised to get it (mostly) out of the way. A stumpbuster is used to split a tree stump. A bulldozer with a stumpbuster is used for landclearing operations, and is often equipped with a brush-rake blade.\n\nBulldozers have been further modified over time to evolve into new machines which can work in ways that the original bulldozer cannot.\n\nOne example is that loader tractors were created by removing the blade and substituting a large volume bucket and hydraulic arms which can raise and lower the bucket, thus making it useful for scooping up earth and loading it into trucks, these are often known as a Drott, trackscavator or track loader.\n\nOther modifications to the original bulldozer include making it smaller to let it operate in small work areas where movement is limited, such as in mining.\n\nSome lightweight form of bulldozer are commonly used in snow removal and as a tool for preparing winter sports areas for ski and snowboard sports.\n\nA very small light bulldozer is sometimes called a \"calfdozer\".\n\nIn an angledozer (image here) the blade can be pushed forward at one end to make it easier to push material away to the side.\n\nNevertheless, the original earthmoving bulldozers are still irreplaceable as their tasks are concentrated in deforestation, earthmoving, ground levelling, and road carving. Heavy bulldozers are mainly employed to level the terrain to prepare it for construction. The construction, however, is mainly done by small bulldozers and loader tractors.\n\nBulldozers employed for combat engineering roles are often fitted with armor to protect the driver from firearms and debris, enabling bulldozers to operate in combat zones. The most widely documented use is the Israeli Defence Forces' (IDF) militarized Caterpillar D9, for earth moving, clearing terrain obstacles, opening routes, detonating explosive charges. The extensive use of armored bulldozers during the Second Intifada drew controversy and criticism from human rights organizations while military experts saw it as a key factor in reducing IDF casualties.\n\nSome forces' engineer doctrines differentiate between a Low-Mobility Armoured Dozer (LMAD) and a High-Mobility Armoured Dozer (HMAD). The former is dependent on a flatbed to move it to its employment site, whereas the latter has a more robust engine and drive system designed to give it road mobility with a moderate range and speed. HMADs however normally lack the full cross-country mobility characteristics of a dozer-blade equipped tank or armoured personnel carrier.\n\nSome bulldozers have been fitted with armor by non-government civilian operators to prevent bystanders or police from interfering with the work performed by the bulldozer, as in the case of strikes or demolition of condemned buildings. This has also been done by civilians with a dispute with the authorities, such as Marvin Heemeyer, who outfitted his Komatsu D355A bulldozer with homemade composite armor to then demolish government buildings.\n\nThe first bulldozers were adapted from Holt farm tractors that were used to plow fields. The versatility of tractors in soft ground for logging and road building contributed to the development of the armoured tank in World War I.\nIn 1923, a young farmer named James Cummings and a draftsman named J. Earl McLeod made the first designs for the bulldozer. A replica is on display at the city park in Morrowville, Kansas where the two built the first bulldozer. On December 18, 1923, Cummings and McLeod filed U.S. patent #1,522,378 that was later issued on January 6, 1925 for an \"Attachment for Tractors.\"\n\nBy the 1920s, tracked vehicles became common, particularly the Caterpillar 60. Rubber-tired vehicles came into use in the 1940s. To dig canals, raise earth dams, and do other earth-moving jobs, these tractors were equipped with a large thick metal plate in front. (The blade got its curved shape later). In some early models the driver sat on top in the open without a cabin. There are three main types of bulldozer blades: a U-blade for pushing and carrying dirt relatively long distances, a straight blade for \"knocking down\" and spreading piles of dirt, and a brush rake for removing brush and roots. These attachments (home-built or built by small equipment manufacturers of attachments for wheeled and crawler tractors and trucks) appeared by 1929.\n\nWidespread acceptance of the bull-grader does not seem to appear before the mid-1930s. The addition of power down-force provided by hydraulic cylinders instead of just the weight of the blade made them the preferred excavation machine for large and small contractors alike by the 1940s, by which time the term \"bulldozer\" referred to the entire machine and not just the attachment.\n\nOver the years, bulldozers got bigger and more powerful in response to the demand for equipment suited for ever larger earthworks. Firms like Caterpillar, Komatsu, Case, Euclid, Allis Chalmers, Liebherr, LiuGong, Terex, Fiat-Allis, John Deere, BEML, XGMA and International Harvester manufactured large tracked-type earthmoving machines. R.G. LeTourneau and Caterpillar manufactured large rubber tired bulldozers.\n\nBulldozers grew more sophisticated as time passed. Improvements include drivetrains analogous to (in automobiles) an automatic transmission instead of a manual transmission such as the early Euclid C-6 and TC-12 or Model C Tournadozer, blade movement controlled by hydraulic cylinders or electric motors instead of early models' cable winch/brake, and automatic grade control. Hydraulic cylinders enabled the application of down force, more precise manipulation of the blade and automated controls.\n\nIn the very snowy winter of 1946–47 in the United Kingdom, in at least one case a remote cut-off village running out of food was supplied by a bulldozer towing a big sledge carrying necessary supplies.\n\nA more recent innovation is the outfitting of bulldozers with GPS technology, such as manufactured by Topcon Positioning Systems, Inc., Trimble Inc, or Leica Geosystems for precise grade control and (potentially) \"stakeless\" construction. As a response to the many, -and often varying claims about these systems, The Kellogg Report published in 2010 a detailed comparison of all the manufacturers' systems, evaluating more than 200 features for dozers alone.\n\nThe best known maker of bulldozers is Caterpillar. Komatsu, Liebherr, Case and John Deere are present-day competitors. Although these machines began as modified farm tractors, they became the mainstay for big civil construction projects, and found their way into use by military construction units worldwide. The best-known model, the Caterpillar D9, was also used to clear mines and demolish enemy structures.\n\nIndustry statistics based on 2010 production published by Off-Highway Research showed Shantui was the largest producer of bulldozers, making over 10,000 units that year or 2 in 5 crawler-type dozers made in the world. The next largest producer by number of units is Caterpillar Inc., which produced 6,400 units.\n\nKomatsu introduced the D575A in 1981, the D757A-2 in 1991, and the D575A-3 in 2002 that the company touts as the biggest bulldozer in the world.\n\n\nThese appeared as early as 1929, but were known as \"bull grader\" blades, and the term \"bulldozer blade\" did not appear to come into widespread use until the mid-1930s. \"Bulldozer\" now refers to the whole machine, not just the attachment. In contemporary usage, \"bulldozer\" is sometimes shortened to \"dozer\", and the verb \"bulldozing\" to \"dozing\", thus making a homophone with the pre-existing verb \"dozing\".\n\n"}
{"id": "19998165", "url": "https://en.wikipedia.org/wiki?curid=19998165", "title": "Chartered Institution of Civil Engineering Surveyors", "text": "Chartered Institution of Civil Engineering Surveyors\n\nThe Chartered Institution of Civil Engineering Surveyors or ICES is a professional association in the field of civil engineering surveying. ICES is now recognised as the leading chartered professional body for civil engineering surveyors. ICES members consist mainly of commercial managers and geospatial engineers working and studying within civil engineering surveying. The institution began in 1969 as the Association of Surveyors in Civil Engineering and became a registered educational charity in 1992. In 1992, ICES became the first associated institution of the Institution of Civil Engineers (ICE) and together formed two joint boards to provide and disseminate surveying knowledge and expertise; the Geospatial Engineering Board and Commercial Management Board. ICES also has reciprocal membership agreements in place with the ICE.\n\nICES is an internationally recognised qualifying body established to regulate, educate and train surveyors working within civil engineering. The headquarters are in the United Kingdom, with 11 regions consisting of volunteer-led committee members. ICES regions include the UK, Ireland, Hong Kong and the United Arab Emirates. ICES also has memoranda of understandings agreements with many international surveying institutions and is a member of the International Federation of Surveyors (FIG).\n\nApplicants for membership must demonstrate that they have fulfilled the institution's competency requirements and the general competencies plus at least one specialism must be met with and signed off by a sponsor.\n\nMembers may use designations after their names such as:\n"}
{"id": "24271731", "url": "https://en.wikipedia.org/wiki?curid=24271731", "title": "Chemical WorkBench", "text": "Chemical WorkBench\n\nChemical WorkBench is a proprietary simulation software tool aimed at the reactor scale kinetic modeling of homogeneous gas-phase and heterogeneous processes and kinetic mechanism development. It can be effectively used for the modeling, optimization, and design of a wide range of industrially and environmentally important chemistry-loaded processes. Chemical WorkBench is a modeling environment based on advanced scientific approaches, complementary databases, and accurate solution methods. Chemical WorkBench is developed and distributed by Kintech Lab.\n\nChemical WorkBench has an extensive library of physicochemical models:\n\nChemical WorkBench can be used by researchers and engineers working in the following fields:\n\n\n\n1. http://www.softscout.com/software/Science-and-Laboratory/Scientific-Modeling-and-Simulation/Chemical-Workbench.html \n2. \n\n3. \n"}
{"id": "31134807", "url": "https://en.wikipedia.org/wiki?curid=31134807", "title": "Christine Poon", "text": "Christine Poon\n\nChristine Ann Poon (born c. 1952 in Brentwood, Missouri) is an American business executive. She is the former vice chairman of the Johnson & Johnson Board of Directors and worldwide chairman of J&J's Pharmaceuticals Group. \n\nPoon was raised as one of seven children; her mother was a nurse, and her father a physician. In a 2013 Distinguished Lecture at West Virginia University, she related how early experiences being Chinese in a predominantly white Ohio suburb shaped her character and desire to \"...not be the stereotype you saw, but be who I wanted to be.\" Poon credits her brother for encouraging her to pursue a career at the interface of science and business, which led to her enrollment for an MBA (1982) and eventual employment at BMS in the Strategic Planning function.\n\nPoon worked at Bristol Myers Squibb from 1985 to 2000, advancing to the position of President of the international medicines division. She credited an early role transition from a \"line\" to a \"staff\" function - Global Marketing - which exposed her to a more international vision of the pharmaceutical world and prepared her for leadership. She then joined Johnson & Johnson for 8 years, from 2000 to 2008. Major projects during her time included drugs to treat pain, schizophrenia, and HIV.\n\nAfter her stint with J&J, Poon served as dean of Ohio State University's Fisher College of Business from April 2009 to November 2014. She is also member of the board of directors of Prudential Financial, Regeneron, and Sherwin-Williams, and member of the supervisory board of Royal Philips Electronics.\n\n\n"}
{"id": "5246698", "url": "https://en.wikipedia.org/wiki?curid=5246698", "title": "Commotion (animation)", "text": "Commotion (animation)\n\nCommotion is a visual effects application, originally released by Puffin Designs. Puffin Designs was founded by Scott Squires (Visual Effects Supervisor at Industrial Light and Magic) and Forest Key to market Commotion.\n\nCommotion set a high standard for a rotoscoping application, introducing rotosplines and offering features like motion tracking and motion blurring for masks.\nIt was the first desktop application to allow real-time playback of full quality video clips from RAM.\n\nPuffin Designs was later acquired by Pinnacle in 2000. After another release, Pinnacle let the program languish. Pinnacle has since been acquired by Avid, who shows no signs of reviving the product.\n\nThe last release was version 4.1.\n\nApple Computer briefly bundled a limited version of the program with Final Cut Pro.\n\n"}
{"id": "14664694", "url": "https://en.wikipedia.org/wiki?curid=14664694", "title": "Compact Model Coalition", "text": "Compact Model Coalition\n\nThe Compact Model Coalition (formerly the Compact Model Council) is a working group in the Electronic Design Automation industry formed to choose, maintain and promote the use of standard semiconductor device models. Commercial and industrial analog simulators (such as SPICE) need to add device models as technology advances (see Moore's law) and earlier models become inaccurate. Before this group was formed, new transistor models were largely proprietary, which severely limited the choice of simulators that could be used.\n\nIt was formed in August, 1996, for the purpose developing and standardizing the use and implementation of SPICE models and the model interfaces. In May 2013, the Silicon Integration Initiative (Si2) and TechAmerica announced the transfer of the Compact Model Council to Si2 and a renaming to Compact Model Coalition.\n\nNew models are submitted to the Coalition, where their technical merits are discussed, and then potential standard models are voted on.\n\nSome of the models supported by the Compact Modeling Coalition include:\n\n\n"}
{"id": "38673321", "url": "https://en.wikipedia.org/wiki?curid=38673321", "title": "Construction 3D printing", "text": "Construction 3D printing\n\nConstruction 3D Printing (c3Dp) or 3D Construction Printing (3DCP) refers to various technologies that use 3D printing as a core method to fabricate buildings or construction components. Alternative terms are also in use, such as Large scale Additive Manufacturing (LSAM), or Freeform construction (FC), also to refer to sub-groups, such as '3D Concrete', used to refer to concrete extrusion technologies. \n\nThere are a variety of 3D printing methods used at construction scale, these include the following main methods: extrusion (concrete/cement, wax, foam, polymers), powder bonding (polymer bond, reactive bond, sintering) and additive welding. 3D printing at a construction scale will have a wide variety of applications within the private, commercial, industrial and public sectors. Potential advantages of these technologies include faster construction, lower labor costs, increased complexity and/or accuracy, greater integration of function and less waste produced. \n\nA number of different approaches have been demonstrated to date which include on-site and off-site fabrication of buildings and construction components, using industrial robots, gantry systems and tethered autonomous vehicles. Demonstrations of construction 3D printing technologies to date have included fabrication of housing, construction components (cladding and structural panels and columns), bridges and civil infrastructure, artificial reefs, follies and sculptures.\n\nThe technology has seen a significant increase in popularity in recent years with many new companies, including some backed up by prominent names from the construction industry and academia (Purdue University). This led to several important milestones, such as the first 3D printed building (Winsun), the first 3D printed bridge (D-Shape), the first 3D printed part in a public building (XtreeE), the first living 3D printed building in Europe and CIS (Specavia), the first 3D printed building in Europe fully approved by the authorities (COBOD International), among many others.\n\nRobotic bricklaying was conceptualized and explored in the 1950s and related technology development around automated construction began in the 1960s, with pumped concrete and isocyanate foams. Development of automated fabrication of entire buildings using slip forming techniques and robotic assembly of components, akin to 3D printing, were pioneered in Japan to address the dangers of building high rise buildings by Shimizu and Hitachi in the 1980s and 1990s. Many of these early approaches to on-site automation foundered because of the construction 'bubble', their inability to respond to novel architectures and the problem of feeding and preparing materials to the site in built up areas.\n\nEarly construction 3D printing development and research have been under way since 1995. Two methods were invented, one by Joseph Pegna which was focused on a sand/cement forming technique which utilized steam to selectively bond the material in layers or solid parts, though this technique was never demonstrated.\n\nThe second technique, Contour Crafting by Behrohk Khoshnevis, initially began as a novel ceramic extrusion and shaping method, as an alternative to the emerging polymer and metal 3D printing techniques, and was patented in 1995. Khoshnevis realized that this technique could exceed these techniques where \"current methods are limited to fabrication of part dimensions that are generally less than one meter is each dimension\". Around 2000, Khoshnevis's team at USC Vertibi began to focus on construction scale 3D printing of cementitious and ceramic pastes, encompassing and exploring automated integration of modular reinforcement, built-in plumbing and electrical services, within one continuous build process. This technology has only been tested at lab scale to date and controversially and allegedly formed the basis for recent efforts in China.\n\nIn 2003, Rupert Soar secured funding and formed the freeform construction group at Loughborough University, UK, to explore the potential for up-scaling existing 3D printing techniques for construction applications. Early work identified the challenge of reaching any realistic break-even for the technology at the scale of construction and highlighted that there could be ways into the application by massively increasing the value proposition of integrated design (many functions, one component). In 2005, the group secured funding to build a large-scale construction 3D printing machine using 'off the shelf' components (concrete pumping, spray concrete, gantry system) to explore how complex such components could be and realistically meet the demands for construction.\n\nIn 2005 Enrico Dini, Italy, patented the D-Shape technology, employing a massively scaled powder jetting/bonding technique over an area approximately 6m x 6m x 3m. This technique although originally developed with epoxy resin bonding system was later adapted to use inorganic bonding agents. This technology has been used commercially for a range of projects in construction and other sectors including for [artificial reefs]. \n\nOne of the most recent developments has been the printing of a bridge, the first of this kind in the world, in collaboration with IaaC and Acciona. \n\nIn 2008 3D Concrete Printing began at Loughborough University, UK, headed by Richard Buswell and colleagues to extend the groups prior research and look to commercial applications moving from a gantry based technology to an industrial robot, which they succeeded in licensing the technology to Skanska in 2014.\n\nOn January 18, 2015 the company gained further press coverage with the unveiling of 2 further buildings, a mansion style villa and a 5 storey tower, using 3D printed components. Detailed photographic inspection indicates that the buildings were fabricated with both precast and 3D printed components. The buildings stand as the first complete structures of their kind fabricated using construction 3D printing technologies. In May 2016 a new 'office building' was opened in Dubai. The 250-square-metre space (2,700 square foot) is what Dubai's Museum of the Future project is calling the world's first 3D-printed office building. In 2017 an ambitious project to build a 3D printed skyscraper in the United Arab Emirates was announced. Cazza construction would help to build the structure. At present there are no specific details, such as the buildings height or exact location.\n\nFreeFAB Wax™, invented by James B Gardiner and Steven Janssen at Laing O'Rourke (construction company). The patented technology has been in development since March 2013. The technique uses construction scale 3D printing to print high volumes of engineered wax (up to 400L/hr) to fabricate a 'fast and dirty' 3D printed mould for precast concrete, glass fibre reinforced concrete (GRC) and other sprayable/cast-able materials. The mould casting surface is then 5 axis milled removing approximately 5mm of wax to create a high quality mould (approximately 20 micron surface roughness). After the component has cured, the mould is then either crushed or melted-off and the wax filtered and re-used, significantly reducing waste compared to conventional mould technologies. The benefits of the technology are fast mould fabrication speeds, increased production efficiencies, reduced labour and virtual elimination of waste by re-use of materials for bespoke moulds compared to conventional mould technologies.\n\nThe system was originally demonstrated in 2014 using an industrial robot. The system was later adapted to integrate with a 5 axis high speed gantry to achieve the high speed and surface milling tolerances required for the system. The first industrialized system is installed at a Laing O'Rourke factory in the United Kingdom and is due to start industrial production for a prominent London project in late 2016.\n\nMX3D Metal founded by Loris Jaarman and team has developed two 6 axis robotic 3D printing systems, the first uses a thermoplastic which is extruded, notably this system allows the fabrication of freeform non-planar beads. The second is a system that relies on additive welding (essentially spot welding on previous spot welds) the additive welding technology has been developed by various groups in the past, however the MX3D metal system is the most accomplished to date. MX3D are currently working toward the fabrication and installation of a metal bridge in Amsterdam.\n\nBetAbram is a simple gantry based concrete extrusion 3D printer developed in Slovenia. This system is available commercially, offering 3 models (P3, P2 and P1) to consumers since 2013. The largest P1 can print objects up to 16m x 9m x 2.5m.\nTotal Custom concrete 3D printer developed by Rudenko is a concrete deposition technology mounted in a gantry configuration, the system has a similar output to Winsun and other concrete 3D printing technologies, however it uses a lightweight truss type gantry. The technology has been used to fabricate a backyard scale version of a castle and a hotel room in the Philippines\n\nThe world's first serial production of construction printers was launched by SPECAVIA company, based in Yaroslavl (Russia). In May 2015, the company introduced the first model of a construction 3d printer and announced the start of sales. As of the beginning of 2018 the group of companies \"AMT-SPEСAVIA\" produces 7 models of portal construction printers: from small format (for printing small architectural forms) to large scale (for printing buildings up to 3 floors) printers. Today, the construction 3D printers of Russian production under the \"AMT\" trademark are operating in several countries, including, in August 2017 the first construction printer was delivered to Europe - for 3DPrinthuset (Denmark). This printer was used Copenhagen for the construction of the first 3D printed building in the EU (office-hotel of 50 m2).\n\nXtreeE has developed a multi-component printing system, mounted on top of a 6-axis robotic arm. The project has started in July 2015, and boasts collaboration and investments from strong names in the construction industry, such as Saint Gobain, Vinci and LafargeHolcim.\n\n3DPrinthuset, a successful Danish 3DPrinting startup, has also branched into construction with its sister company COBOD International, which made its own gantry-based printer in October 2017. With the collaboration of strong names in the scandinavian region, such as NCC and Force Technology , the company's spin-off has quickly gained traction by constructing the first 3DPrinted house in Europe. The Building on Demand (BOD) project, as the structure is called, is a small office hotel in Copenhagen, Nordhavn area, with walls and part of the foundation fully printed, while the rest of the construction is made in traditional construction. As of November 2017, the building is in the final phase of applying fixtures and roofing, while all the 3DPrinted parts have been fully completed.\n\nArchitect James Bruce Gardiner pioneered architectural design for Construction 3D Printing with two projects. The first Freefab Tower 2004 and the second Villa Roccia 2009-2010. FreeFAB Tower was based on the original concept to combine a hybrid form of construction 3D printing with modular construction. This was the first architectural design for a building focused on the use of Construction 3D Printing. Influences can be seen in various designs used by Winsun, including articles on the Winsun's original press release and office of the future The FreeFAB Tower project also depicts the first speculative use of multi-axis robotic arms in construction 3D printing, the use of such machines within construction has grown steadily in recent years with projects by MX3D and Branch Technology \n\nThe Villa Roccia 2009-2010 took this pioneering work a step further with the a design for a Villa at Porto Rotondo, Sardinia, Italy in collaboration with D-Shape. The design for the Villa focused on the development of a site specific architectural language influenced by the rock formations on the site and along the coast of Sardinia, while also taking into account the use of a panellised prefabricated 3D printing process. The project went through prototyping and didn't proceed to full construction. \nFrancios Roche (R&Sie) developed the exhibition project and monograph 'I heard about' in 2005 which explored the use of a highly speculative self propelling snake like autonomous 3D printing apparatus and generative design system to create high rise residential towers. The project although impossible to put into practice with current or contemporary technology demonstrated a deep exploration of the future of design and construction. The exhibition showcased large scale CNC milling of foam and rendering to create the freeform building envelopes envisaged.\n\nDutch architect Janjaap Ruijssenaars's performative architecture 3D-printed building was planned to be built by a partnership of Dutch companies. The house was planned to be built in the end of 2014, but this deadline wasn't met. The companies have said that they are still committed to the project. \n\nThe Building On Demand, or BOD, a small office hotel 3D printed by 3D Printhuset (now COBOD International) and designed by architect Ana Goidea, has incorporated curved walls and a rippling effects on their surface, to showcase the design freedom that 3D printing allows in the horizontal plane.\n\nThe 3D Print Canal House was the first full-scale construction project of its kind to get off the ground. In just a short space of time, the Kamermaker has been further developed to increase its production speed by 300%. However, progress has not been swift enough to claim the title of 'World's First 3D Printed House'.\n\nThe first residential building in Europe and the CIS, constructed using the 3D printing construction technology, was the home in Yaroslavl (Russia) with the area of 298,5 sq. m. The walls of the building were printed by the company SPECAVIA in December 2015. 600 elements of the walls were printed in the shop and assembled at the construction site. After completing the roof structure and interior decoration, the company presented a fully finished 3D building in October 2017. The peculiarity of this project is that for the first time in the world the entire technological cycle of construction has been passed: design, obtaining a building permit, registration of the building, connection of all engineering systems. An important feature of 3D house in Yaroslavl, that also distinguishes this project from other implemented ones - this is not a presentation structure, but rather a full-fledged residential building. Today it is home of a real ordinary family.\n\nDutch and Chinese demonstration projects are slowly constructing 3D-printed buildings in China, Dubai and the Netherlands. Using the effort to educate the public to the possibilities of the new plant-based building technology and to spur greater innovation in 3D printing of residential buildings. A small concrete house was 3D-printed in 2017.\n\nThe Building on Demand (BOD), the first 3D printed house in Europe, is a project led by COBOD International (formerly known as 3DPrinthuset, now its sister company) for a small 3D printed office hotel in Copenhagen, Nordhavn area. The building is also the first 3D printed permanent building, with all permits in place and fully approved by the authorities. As of 2018, the building stands fully completed and furbished .\n\nIn Spain, the first pedestrian bridge printed in 3D in the world (3DBRIDGE) was inaugurated 14th of December of 2016 in the urban park of Castilla-La Mancha in Alcobendas, Madrid. The 3DBUILD technology used was developed by ACCIONA, who was in charge of the structural design, material development and manufacturing of 3D printed elements. The bridge has a total length of 12 meters and a width of 1.75 meters and is printed in micro-reinforced concrete. Architectural design was done by Institute of Advanced Architecture of Catalonia (IAAC).\n\nThe 3D printer used to build the footbridge was manufactured by D-Shape. The 3D printed bridge reflects the complexities of nature’s forms and was developed through parametric design and computational design, which allows to optimize the distribution of materials and allows to maximize the structural performance, being able to dispose the material only where it is needed, with total freedom of forms. The 3D printed footbridge of Alcobendas represented a milestone for the construction sector at international level, as large scale 3D printing technology has been applied in this project for the first time in the field of civil engineering in a public space.\n\nIn August 2018 in Palekh (old town in Russia) was the world's first application of additive technology for сreating the fountain. The fountain “Snop” (Sheaf) was originally created in the middle of the 20th century by famous sculptor Nikolai Dydykin. Nowadays, during the restoration of the fountain, it was changed from a rectangular to a round shape. The backlight system has also been updated. The restored fountain is 26 meters in diameter and 2.2 meters deep. The parapet of the 3D fountain with internal communication channels was printed by the AMT construction printer produced by AMT-SPETSAVIA group.\n\nThe printing of buildings has been proposed as a particularly useful technology for constructing off-Earth habitats, such as habitats on the Moon or Mars. , the European Space Agency was working with London-based Foster + Partners to examine the potential of printing lunar bases using regular 3D printing technology. The architectural firm proposed a building-construction 3D-printer technology in January 2013 that would use lunar regolith raw materials to produce lunar building structures while using enclosed inflatable habitats for housing the human occupants inside the hardshell printed lunar structures. Overall, these habitats would require only ten percent of the structure mass to be transported from Earth, while using local lunar materials for the other 90 percent of the structure mass.\n\nThe dome-shaped structures would be a weight-bearing catenary form, with structural support provided by a closed-cell structure, reminiscent of bird bones. In this conception, \"printed\" lunar soil will provide both \"radiation and temperature insulation\" for the Lunar occupants.\nThe building technology mixes lunar material with magnesium oxide which will turn the \"moonstuff into a pulp that can be sprayed to form the block\" when a binding salt is applied that \"converts [this] material into a stone-like solid.\" A type of sulfur concrete is also envisioned.\n\nTests of 3D printing of an architectural structure with simulated lunar material have been completed, using a large vacuum chamber in a terrestrial lab. The technique involves injecting the binding liquid under the surface of the regolith with a 3D printer nozzle, which in tests trapped -scale droplets under the surface via capillary forces. The printer used was the D-Shape.\n\nA variety of lunar infrastructure elements have been conceived for 3D structural printing, including landing pads, blast protection walls, roads, hangars and fuel storage. In early 2014, NASA funded a small study at the University of Southern California to further develop \n\nthe \"Contour Crafting\" 3D printing technique. Potential applications of this technology include constructing lunar structures of a material that could consist of up to 90-percent lunar material with only ten percent of the material requiring transport from Earth.\n\nNASA is also looking at a different technique that would involve the sintering of lunar dust using low-power (1500 watt) microwave energy. The lunar material would be bound by heating to , somewhat below the melting point, in order to fuse the nanoparticle dust into a solid block that is ceramic-like, and would not require the transport of a binder material from Earth as required by the Foster+Partners, Contour Crafting, and D-shape approaches to extraterrestrial building printing. One specific proposed plan for building a lunar base using this technique would be called SinterHab, and would utilize the JPL six-legged ATHLETE robot to autonomously or telerobotically build lunar structures.\n\nLarge-scale, cement-based 3D printing disposes the need for conventional molding by precisely placing, or solidifying, specific volumes of material in sequential layers by a computer controlled positioning process. This 3D printing approach consist of three general stages: data preparation, concrete preparation and component printing.\n\nFor path and data generation, a variety of methods are implemented for the generation of robotic building paths. A general approach is to slice a 3D shape into flat thin layers with a constant thickness which can be stacked up onto each other. In this method, each layer consists of a contour line and a filling pattern which can be implemented as honeycomb structures or space-filling curves. Another method is the tangential continuity method which produces 3-dimensional building paths with locally varying thicknesses. This method results in creating constant contact surfaces between two layers, therefore, the geometrical gaps between two layers which often limits the 3D printing process will be avoided.\n\nThe material preparation stage includes mixing and placing the concrete into the container. Once the fresh concrete has been placed into the container, it can be conveyed through the pump–pipe– nozzle system to print out self-compacting concrete filaments, which can build layer-by-layer structural components. In the additive processes, pumpability and the stability of the extrusion is important for the applications of mortars. These properties will all vary depending on the concrete mix design, the delivery system, and the deposition device. General specifications of wet concrete 3D printing are categorized into four main characteristics:\n\n\nIn the printing step, a control system is required to execute the printing process. These systems can be generally split into two categories: gantry systems and robotic arm systems. The gantry system drives a manipulator mounted onto an overhead to locate the print nozzle in XYZ cartesian coordinates while robotic arms offer additional degrees of freedom to the nozzle, allowing more accurate printing workflows such as printing with tangential continuity method. Moreover, multiple 3D printing robotic arms can be programmed to run simultaneously resulting in decreased construction time. Finally, automated post-processing procedures can also be applied in scenarios which require the removal of support structures or any surface finishing.\n\nClaims have been made by Behrokh Khoshnevis since 2006 for 3D printing a house in a day, with further claims to notionally complete the building in approximately 20 hours of \"printer\" time. By January 2013, working versions of 3D-printing building technology were printing of building material per hour, with a follow-on generation of printers proposed to be capable of per hour, sufficient to complete a building in a week. \n\nThe Chinese company WinSun has built several houses using large 3D printers using a mixture of quick drying cement and recycled raw materials. Ten demonstration houses were said by Winsun to have been built in 24 hours, each costing US $5000 (structure not including, footings, services, doors/windows and fitout). However, construction 3D printing pioneer Dr. Behrokh Khoshnevis claims this was faked and that WinSun stole his intellectual property.\n\nThere are several research project dealing with 3D Construction printing, such as the 3D concrete printing (3DCP) project at the Eindhoven University of Technology, or the various projects at the Institute for Advanced Architecture of Catalonia (Pylos , Mataerial, and Minibuilders). The list of research projects is expanding even more in the last couple of years, thanks to a growing interest in the field.\nThe majority of the projects have been focused on researching the physical aspects behind the technology, such as the printing technology, material technology, and the various issues \n\nrelated to them. COBOD International (formerly known as 3DPrinthuset, now its sister company) has recently led a research oriented towards exploring the current state of the technology worldwide, by visiting more than 35 different 3D Construction printing related projects. For each project, a research report has been issued, and the gathered data has been used to unify all the various technologies into a first attempt at a common standardized categorization and terminology. \n\nThe researchers at Purdue University have pioneered a 3D printing process known as Direct-ink-Writing for fabrication of architectured cement-based materials for the first time . They demonstrated using 3D-printing, bio-inspired designs of cement-based materials is feasible and novel performance characteristics such as flaw-tolerance and compliance can be achieved. \n\nAlong with the research, 3DPrinthuset (now known as COBOD International) has organized two international conferences on 3D Construction printing (February and November 2017 respectively), aimed at bringing together the strongest names in this emerging industry to discuss the potentials and challenges that lie ahead. The conferences were the first of this kind, and have brought together names such as D-Shape, Contour Crafting, Cybe Construction, Eindhoven's 3DCP research, Winsun, and many more. Along the 3D Construction printing specialists, there has also been a strong presence from the traditional construction industry key players for the first time, with names such as Sika AG, Vinci , Royal BAM Group, NCC, among others. A general idea emerged that the 3D Construction printing field needs a more unified platform where ideas, applications, issues and challenges can be shared and discussed.\n\nAlthough its the first steps have been made nearly three decades ago, 3D Construction printing has struggled to reach out for years. The first technologies to achieve some media attention were Contour Crafting and D-Shape, with a few sporadic articles in 2008–2012 and a 2012 TV report. D-Shape has also been featured in an independent documentary dedicated to its creator Enrico Dini, called \"The man who prints houses\".\n\nOne important break-through has been seen with the announcement of the first 3D printed building, using a prefabricated 3D printed components made by Winsun, which claimed to be able to print 10 houses in a day with its technology. Although the claims were still to be confirmed, the story has created a wide traction and a growing interest in the field. In a matter of months, many new companies began to emerge. This led to many new endeavors that reached the media, such as, in 2017, the first pedestrian 3d printed bridge and the first cyclist 3d printed bridge, plus an early structural element made with 3d printing in 2016, among many others.\n\nRecently, COBOD International, formerly known as 3DPrinthuset (its sister company) has gained wide media attention with their first permanent 3D printed building, the first of its kind in Europe. The project set an important precedent for being the first 3D printed building with a building permit and documentation in place, and a full approval from the city authorities, a crucial milestone for a wider acceptance in the construction field. The story gained extensive coverage, both on national and international media, appearing on TV in Denmark, Russia, Poland, Lithuania, among many others.\n\n\n"}
{"id": "1270187", "url": "https://en.wikipedia.org/wiki?curid=1270187", "title": "Current transformer", "text": "Current transformer\n\nA current transformer (CT) is a type of transformer that is used to measure alternating current (AC). It produces a current in its secondary which is proportional to the current in its primary.\n\nCurrent transformers, along with voltage or potential transformers, are instrument transformers. Instrument transformers scale the large values of voltage or current to small, standardized values that are easy to handle for instruments and protective relays. The instrument transformers isolate measurement or protection circuits from the high voltage of the primary system. A current transformer provides a secondary current that is accurately proportional to the current flowing in its primary. The current transformer presents a negligible load to the primary circuit. \n\nCurrent transformers are the current-sensing units of the power system and are used at generating stations, electrical substations, and in industrial and commercial electric power distribution.\n\nLike any transformer, a current transformer has a primary winding, a core and a secondary winding, although some transformers, including current transformers, use an air core. In principle, the only difference between a current transformer and a voltage transformer (normal type) is that the former is fed with a 'constant' current while the latter is fed with a 'constant' voltage, where 'constant' has the strict circuit theory meaning.\n\nThe alternating current in the primary produces an alternating magnetic field in the core, which then induces an alternating current in the secondary. The primary circuit is largely unaffected by the insertion of the CT. Accurate current transformers need close coupling between the primary and secondary to ensure that the secondary current is proportional to the primary current over a wide current range. The current in the secondary is the current in the primary (assuming a single turn primary) divided by the number of turns of the secondary. In the illustration on the right, 'I' is the current in the primary, 'B' is the magnetic field, 'N' is the number of turns on the secondary, and 'A' is an AC ammeter.\n\nCurrent transformers typically consist of a silicon steel ring core wound with many turns of copper wire as shown in the illustration to the right. The conductor carrying the primary current is passed through the ring. The CT's primary, therefore, consists of a single 'turn'. The primary 'winding' may be a permanent part of the current transformer, i.e. a heavy copper bar to carry current through the core. Window-type current transformers are also common, which can have circuit cables run through the middle of an opening in the core to provide a single-turn primary winding. To assist accuracy, the primary conductor should be centered in the aperture.\n\nCTs are specified by their current ratio from primary to secondary. The rated secondary current is normally standardized at 1 or 5 amperes. For example, a 4000:5 CT secondary winding will supply an output current of 5 amperes when the primary winding current is 4000 amperes. This ratio can also be used to find the impedance or voltage on one side of the transformer, given the appropriate value at the other side. For the 4000:5 CT, the secondary impedance can be found as , and the secondary voltage can be found as . In some cases, the secondary impedance is \"referred\" to the primary side, and is found as . Referring the impedance is done simply by multiplying initial secondary impedance value by the current ratio. The secondary winding of a CT can have taps to provide a range of ratios, five taps being common.\n\nCurrent transformer shapes and sizes vary depending on the end user or switch gear manufacturer. Low-voltage single ratio metering current transformers are either a ring type or plastic molded case.\n\nSplit-core current transformers either have a two-part core or a core with a removable section. This allows the transformer to be placed around a conductor without having to disconnect it first. Split-core current transformers are typically used in low current measuring instruments, often portable, battery-operated, and hand-held (see illustration lower right).\n\nCurrent transformers are used extensively for measuring current and monitoring the operation of the power grid. Along with voltage leads, revenue-grade CTs drive the electrical utility's watt-hour meter on many larger commercial and industrial supplies.\n\nHigh-voltage current transformers are mounted on porcelain or polymer insulators to isolate them from ground. Some CT configurations slip around the bushing of a high-voltage transformer or circuit breaker, which automatically centers the conductor inside the CT window.\n\nCurrent transformers can be mounted on the low voltage or high voltage leads of a power transformer. Sometimes a section of a bus bar can be removed to replace a current transformer.\n\nOften, multiple CTs are installed as a \"stack\" for various uses. For example, protection devices and revenue metering may use separate CTs to provide isolation between metering and protection circuits and allows current transformers with different characteristics (accuracy, overload performance) to be used for the devices.\n\nThe burden (load) impedance should not exceed the specified maximum value to avoid the secondary voltage exceeding the limits for the current transformer. The primary current rating of a current transformer should not be exceeded or the core may enter its non linear region and ultimately saturate. This would occur near the end of the first half of each half (positive and negative) of the AC sine wave in the primary and would compromise the accuracy.\n\nCurrent transformers are often used to monitor high currents or currents at high voltages. Technical standards and design practices are used to ensure the safety of installations using current transformers.\n\nThe secondary of a current transformer should not be disconnected from its burden while current is in the primary, as the secondary will attempt to continue driving current into an effective infinite impedance up to its insulation break-down voltage and thus compromise operator safety. For certain current transformers, this voltage may reach several kilovolts and may cause arcing. Exceeding the secondary voltage may also degrade the accuracy of the transformer or destroy it. Energizing a current transformer with an open circuit secondary is equivalent to energizing a voltage transformer (normal type) with a short circuit secondary. In the first case the secondary tries to produce an infinite voltage and in the second case the secondary tries to produce an infinite current. Both scenarios can be dangerous and damage the transformer. \n\nThe accuracy of a CT is affected by a number of factors including:\n\nAccuracy classes for various types of measurement and at standard loads in the secondary circuit (burdens) are defined in IEC 61869-1 as classes 0.1, 0.2s, 0.2, 0.5, 0.5s, 1 and 3. The class designation is an approximate measure of the CT's accuracy. The ratio (primary to secondary current) error of a Class 1 CT is 1% at rated current; the ratio error of a Class 0.5 CT is 0.5% or less. Errors in phase are also important, especially in power measuring circuits. Each class has an allowable maximum phase error for a specified load impedance. \n\nCurrent transformers used for protective relaying also have accuracy requirements at overload currents in excess of the normal rating to ensure accurate performance of relays during system faults. A CT with a rating of 2.5L400 specifies with an output from its secondary winding of twenty times its rated secondary current (usually ) and 400 V (IZ drop) its output accuracy will be within 2.5 percent.\n\nThe secondary load of a current transformer is termed the \"burden\" to distinguish it from the primary load.\n\nThe burden in a CT metering electrical network is largely resistive impedance presented to its secondary winding. Typical burden ratings for IEC CTs are 1.5 VA, 3 VA, 5 VA, 10 VA, 15 VA, 20 VA, 30 VA, 45 VA and 60 VA. ANSI/IEEE burden ratings are B-0.1, B-0.2, B-0.5, B-1.0, B-2.0 and B-4.0. This means a CT with a burden rating of B-0.2 will maintain its stated accuracy with up to 0.2 Ω on the secondary circuit. These specification diagrams show accuracy parallelograms on a grid incorporating magnitude and phase angle error scales at the CT's rated burden. Items that contribute to the burden of a current measurement circuit are switch-blocks, meters and intermediate conductors. The most common cause of excess burden impedance is the conductor between the meter and the CT. When substation meters are located far from the meter cabinets, the excessive length of cable creates a large resistance. This problem can be reduced by using thicker cables and CTs with lower secondary currents (1 A), both of which will produce less voltage drop between the CT and its metering devices. \n\nThe knee-point voltage of a current transformer is the magnitude of the secondary voltage above which the output current ceases to linearly follow the input current within declared accuracy. In testing, if a voltage is applied across the secondary terminals the magnetizing current will increase in proportion to the applied voltage, until the knee point is reached. The knee point is defined as the voltage at which a 10% increase in applied voltage increases the magnetizing current by 50%. For voltages greater than the knee point, the magnetizing current increases considerably even for small increments in voltage across the secondary terminals. The knee-point voltage is less applicable for metering current transformers as their accuracy is generally much higher but constrained within a very small range of the current transformer rating, typically 1.2 to 1.5 times rated current. However, the concept of knee point voltage is very pertinent to protection current transformers, since they are necessarily exposed to fault currents of 20 to 30 times rated current.\n\nIdeally, the primary and secondary currents of a current transformer should be in phase. In practice, this is impossible, but, at normal power frequencies, phase shifts of a few tenths of a degree are achievable, while simpler CTs may have phase shifts up to six degrees. For current measurement, phase shift is immaterial as ammeters only display the magnitude of the current. However, in wattmeters, energy meters, and power factor meters, phase shift produces errors. For power and energy measurement, the errors are considered to be negligible at unity power factor but become more significant as the power factor approaches zero. At zero power-factor, any indicated power is entirely due to the current transformer's phase error. The introduction of electronic power and energy meters has allowed current phase error to be calibrated out.\n\nSpecially constructed \"wideband current transformers\" are also used (usually with an oscilloscope) to measure waveforms of high frequency or pulsed currents within pulsed power systems. Unlike CTs used for power circuitry, wideband CTs are rated in output volts per ampere of primary current.\n\nIf the burden resistance is much less than inductive impedance of the secondary winding at the measurement frequency then the current in the secondary tracks the primary current and the transformer provides a current output that is proportional to the measured current. On the other hand, if that condition is not true, then the transformer is inductive and gives a differential output. The Rogowski coil uses this effect and requires an external integrator in order to provide a voltage output that is proportional to the measured current.\n\nUltimately, depending on client requirements, there are two main standards to which current transformers are designed. IEC 61869-1 (in the past IEC 60044-1) & IEEE C57.13 (ANSI), although the Canadian and Australian standards are also recognised.\n\nCurrent transformers are used for protection, measurement and control in high-voltage electrical substations and the electrical grid. Current transformers may be installed inside switchgear or in apparatus bushings, but very often free-standing outdoor current transformers are used. In a switchyard, \"live tank\" current transformers have a substantial part of their enclosure energized at the line voltage and must be mounted on insulators. \"Dead tank\" current transformers isolate the measured circuit from the enclosure. Live tank CTs are useful because the primary conductor is short, which gives better stability and a higher short-circuit current rating. The primary of the winding can be evenly distributed around the magnetic core, which gives better performance for overloads and transients. Since the major insulation of a live-tank current transformer is not exposed to the heat of the primary conductors, insulation life and thermal stability is improved.\n\nA high-voltage current transformer may contain several cores, each with a secondary winding, for different purposes (such as metering circuits, control, or protection). A neutral current transformer is used as earth fault protection to measure any fault current flowing through the neutral line from the wye neutral point of a transformer.\n\n\n"}
{"id": "2568485", "url": "https://en.wikipedia.org/wiki?curid=2568485", "title": "Cycloconverter", "text": "Cycloconverter\n\nA cycloconverter (CCV) or a cycloinverter converts a constant voltage, constant frequency AC waveform to another AC waveform of a lower frequency by synthesizing the output waveform from segments of the AC supply without an intermediate DC link ( and ). There are two main types of CCVs, circulating current type or blocking mode type, most commercial high power products being of the blocking mode type.\n\nWhereas phase-controlled SCR switching devices can be used throughout the range of CCVs, low cost, low-power TRIAC-based CCVs are inherently reserved for resistive load applications. The amplitude and frequency of converters' output voltage are both variable. The output to input frequency ratio of a three-phase CCV must be less than about one-third for circulating current mode CCVs or one-half for blocking mode CCVs. Output waveform quality improves as the \"pulse number\" of switching-device bridges in phase-shifted configuration increases in CCV's input. In general, CCVs can be with 1-phase/1-phase, 3-phase/1-phase and 3-phase/3-phase input/output configurations, most applications however being 3-phase/3-phase.\n\nThe competitive power rating span of standardized CCVs ranges from few megawatts up to many tens of megawatts. CCVs are used for driving mine hoists, rolling mill main motors, ball mills for ore processing, cement kilns, ship propulsion systems, slip power recovery wound-rotor induction motors (i.e., Scherbius drives) and aircraft 400 Hz power generation. The variable-frequency output of a cycloconverter can be reduced essentially to zero. This means that very large motors can be started on full load at very slow revolutions, and brought gradually up to full speed. This is invaluable with, for example, ball mills, allowing starting with a full load rather than the alternative of having to start the mill with an empty barrel then progressively load it to full capacity. A fully loaded \"hard start\" for such equipment would essentially be applying full power to a stalled motor. Variable speed and reversing are essential to processes such as hot-rolling steel mills. Previously, SCR-controlled DC motors were used, needing regular brush/commutator servicing and delivering lower efficiency. Cycloconverter-driven synchronous motors need less maintenance and give greater reliability and efficiency. Single-phase bridge CCVs have also been used extensively in electric traction applications to for example produce 25 Hz power in the U.S. and 16 2/3 Hz power in Europe.\n\nWhereas phase-controlled converters including CCVs are gradually being replaced by faster PWM self-controlled converters based on IGBT, GTO, IGCT and other switching devices, these older classical converters are still used at the higher end of the power rating range of these applications.\n\nCCV operation creates current and voltage \"harmonics\" on the CCV's input and output. AC line harmonics are created on CCV's input accordance to the equation,\nwhere\n\n"}
{"id": "15337298", "url": "https://en.wikipedia.org/wiki?curid=15337298", "title": "Electronic Payments Network", "text": "Electronic Payments Network\n\nThe Electronic Payments Network (EPN) is an electronic clearing house that provides functions similar to those provided by Federal Reserve banks. The Electronic Payments Network is the only private-sector operator in the ACH Network in the United States. The EPN is operated by The Clearing House Payments Company.\n\n\n"}
{"id": "10392041", "url": "https://en.wikipedia.org/wiki?curid=10392041", "title": "Electronic oscillation", "text": "Electronic oscillation\n\nElectronic oscillation is a repeating cyclical variation in voltage or current in an electrical circuit, resulting in a periodic waveform. The frequency of the oscillation in hertz is the number of times the cycle repeats per second. \n\nThe recurrence may be in the form of a varying voltage or a varying current. The waveform may be sinusoidal or some other shape when its magnitude is plotted against time. Electronic oscillation may be intentionally caused, as in devices designed as oscillators, or it may be the result of unintentional positive feedback from the output of an electronic device to its input. The latter appears often in feedback amplifiers (such as operational amplifiers) that do not have sufficient gain or phase margins. In this case, the oscillation often interferes with or compromises the amplifier's intended function, and is known as parasitic oscillation.\n"}
{"id": "28377258", "url": "https://en.wikipedia.org/wiki?curid=28377258", "title": "Flood opening", "text": "Flood opening\n\nA flood opening or flood vent (also styled floodvent) is an orifice in an enclosed structure intended to allow the free passage of water between the interior and exterior.\n\nIn the United States, flood openings are used to provide for the automatic equalization of hydrostatic pressure on either side of a wall. Building codes usually require the installation of flood openings in the walls of structures located in A-type flood zones recognized by the Federal Emergency Management Agency. Various agencies in the United States define necessary characteristics for flood openings. The NFIP Regulations and Building Codes require that any residential building constructed in Flood Zone Type A have the lowest floor, including basements, elevated to or above the Base Flood Elevation (BFE). Enclosed areas are permitted under elevated buildings provided that they meet certain use restrictions and construction requirements such as the installation of flood vents to allow for the automatic entry and exit of flood waters. This wet floodproofing technique is required for residential buildings.\n\nMost regulatory authorities in the United States that offer requirements for flood openings define two major classes of opening: engineered, and non-engineered. The requirements for non-engineered openings are typically stricter, defining necessary characteristics for aspects ranging from overall size of each opening, to allowable screening or other coverage options, to number and placement of openings. Engineered openings ignore many of these requirements, depending on the particular regulatory authority. To qualify as an engineered opening, testing and/or certification by a qualified agency (varying from regulator to regulator, and indicated below where appropriate) is required.\n\nNon-Engineered Opening WARNING: \"Any louvers, blades, screens, and faceplates or other covers and devices should be selected or specified so as to minimize the likelihood of blockage by small debris and sediment. Where experience has shown that a particular device or type of device has been blocked or clogged by flood debris or sediment, use of such devices should be avoided.\" -ASCE 24-14 Pg. 45 (commentary)\n\nThe American Society of Civil Engineers (ASCE) requirements apply to any structure that is not dry flood-proofed and which is in the mapped flood zone. It calls for openings in load-bearing foundation walls located below the mapped flood elevation. Where non-engineered openings are used, each opening must be at least three inches in diameter, and have no screen or other cover that interferes with the transition of water between interior and exterior. The total net open area of all flood openings in the structure must be equal to or greater than one square inch, per square foot of footprint of the enclosed area—though no fewer than two openings, total, which must be located on different walls. Openings must be placed such that the bottom of each opening is no more than one foot above the adjacent ground level.\n\nIn lieu of these requirements, engineered openings must conform to a performance standard: during a flood with a rate of rise/fall of five feet per hour, the difference between interior and exterior flood water levels in an enclosure using the engineered openings must not be greater than one foot.\n\nThe International Building Code refers to the American Society of Civil Engineer requirements for both non-engineered and engineered flood openings.\n\nThe International Residential Code requirements vary mildly from revision to revision, but require that entry and exit of floodwater be provided for in accordance with the requirements of the ASCE. These requirements apply for both non-engineered and engineered flood openings.\n\nWhile the Federal Emergency Management Agency does not have de jure authority over the building code, it maintains crucial influence over flood opening standards through its administration of the National Flood Insurance Program (NFIP). By controlling the standards for nearly all flood insurance in the United States, the NFIP exerts exceptional de facto authority over many aspects of floodplain construction. The FEMA (and, thus, NFIP) requirements for non-engineered openings are similar to requirements from the American Society of Civil Engineers. Unlike the ASCE, FEMA requires the placement of openings such that the bottom of each opening is no more than one foot above the higher of the adjacent ground level, or the interior foundation slab height.\n\nFor engineered openings, FEMA offers two subclassifications: individual certification openings, and openings with International Code Council Evaluation Service (ICC-ES) Evaluation Reports. Individual certification openings are offered for use when, \"[f]or architectural or other reasons, building designers or owners may prefer to use unique or individually designed openings or devices\". In such cases, an architect or engineer may provide certification including the professional's signature and applied seal. The certification must include a \"statement certifying that the openings are designed to automatically equalize hydrostatic flood loads on exterior walls by allowing the automatic entry and exit of floodwaters in accordance with the...design requirements\"; \"[d]escription of the range of flood characteristics tested or computed for which the certification is valid, such as rates of rise and fall of floodwaters\"; and \"[d]escription of the installation requirements and limitations that, if not followed, will void the certification\". The nature of the \"live seal\" requirement means that each structure containing an individual certification opening must have a separate certification, even if the opening is identical to that used in another structure.\n\nThe alternative subclassification is an opening that carries certification through the ICC-ES. According to FEMA, \"Evaluation Reports are issued only after the ICC-ES performs technical evaluations of documentation submitted by a manufacturer, including technical design reports, certifications, and testing that demonstrate code compliance and performance.\" The report must include a statement concerning the purpose of the opening tested; a description of the characteristics tested; and a description of installation requirements. FEMA allows a copy of the report to be used as a blanket certification of any project including an ICC-ES certified opening, in contrast to the requirements of an individual certification opening.\n\nAC364-1006-R1 documents the ICC-ES's testing standards for flood openings, including specifications for a dual-chambered testing tank. While the requirements for the opening itself are based on ASCE 24, the substance of the test adds new layers of performance expectation. Under these requirements, the opening must activate before water level is one foot above the bottom of the opening, under conditions of 50 and 300 gallons per minute flooding, at a minimum of five foot per hour rate of rise. Additionally, water levels on the testing tank's \"interior\" and \"exterior\" portions must at no point differ more than one foot. To gauge performance against waterborne debris, leaves and grass clippings are added to both chambers of the tank.\n\n"}
{"id": "39934465", "url": "https://en.wikipedia.org/wiki?curid=39934465", "title": "Funnel analysis", "text": "Funnel analysis\n\nFunnel analysis involves using a series of events that lead towards a defined goal, like from user engagement in a mobile app to a sale in an eCommerce platform or advertisement to purchase in online advertising. The funnel analyses \"are an effective way to calculate conversion rates on specific user behaviors\".\nThis can be in the form of a sale, registration, or other intended action from an audience. The origin of the term funnel analysis comes from the nature of a funnel where individuals will enter the funnel, yet only a small number of them will perform the intended goals.\n\nFor more emphasis, it makes sense why a funnel in analytics is called a funnel. An actual funnel, like the ones from a kitchen or garage, gets narrower along its length, allowing less volume to pass through it. An analytics funnel represents a very similar idea, just in regards to users on an eCommerce platform, application or online game.\n\nAn example of how a company would use funnel analytics is by focusing on drawing actionable insights from funnels. Funnel analysis can be used to determine conversion and user fallout rates in a given funnel. An analysis to determine the steps that lead to a desired goal in order to improve future interactions in the same funnel can be done for further success. To illustrate further, looking at how many users actually make it to the end of the funnel, for example to make a purchase or register, compared with how many do not.\n\nBy continuously monitoring and analyzing funnels, it is possible to assess if changes to an application or platform are having a positive effect on conversion. For instance, one might find that only 10% of users who come to a platform and enter the registration funnel actually reach the goal of completing registration. Using the funnel analytics process, it is then possible to tweak settings or features within the funnel in order to see what makes that number improve. Or when creating a marketing campaign, there is a chance to analyze how well the campaign is working by monitoring a funnel that brings users from the initial event all the way to purchasing a product.\n\nFunnel analysis helps determine the point in which users are dropping off. The next step is to understand why they’re dropping off, in order to reduce drop off rates and in turn increase overall conversion.\n\n\n\n"}
{"id": "42244620", "url": "https://en.wikipedia.org/wiki?curid=42244620", "title": "Gas volume corrector", "text": "Gas volume corrector\n\nGas volume corrector - device for calculating, summing and determining increments of gas volume, measured by gas meter if it were operating base conditions. For this purpose, uses as input the gas volume, measured by the gas meter and other parameters such as: gas pressure and temperature. It is used for the settlement of trade wholesale gas.\n\nThere are two types of gas volume correctors:\n\nType 1- gas volume corrector with specific types of transducers for pressure and temperature or temperature only. This type of gas volume corrector is powered by battery.\n\nType 2 - a device that converts separate transmitters with external temperature and pressure, or temperature only and for separate calculator, which may be approved separately. This type of gas volume corrector is powered by mains.\n\nEuropean Standard EN 12405-1:2005+A2\n"}
{"id": "482371", "url": "https://en.wikipedia.org/wiki?curid=482371", "title": "Glass cockpit", "text": "Glass cockpit\n\nA glass cockpit is an aircraft cockpit that features electronic (digital) flight instrument displays, typically large LCD screens, rather than the traditional style of analog dials and gauges. While a traditional cockpit (nicknamed a \"steam cockpit\" within aviation circles) relies on numerous mechanical gauges to display information, a glass cockpit uses several displays driven by flight management systems, that can be adjusted (multi-function display) to display flight information as needed. This simplifies aircraft operation and navigation and allows pilots to focus only on the most pertinent information. They are also popular with airline companies as they usually eliminate the need for a flight engineer, saving costs. In recent years the technology has also become widely available in small aircraft.\n\nAs aircraft displays have modernized, the sensors that feed them have modernized as well. Traditional gyroscopic flight instruments have been replaced by electronic attitude and heading reference systems (AHRS) and air data computers (ADCs), improving reliability and reducing cost and maintenance. GPS receivers are usually integrated into glass cockpits.\n\nEarly glass cockpits, found in the McDonnell Douglas MD-80/90, Boeing 737 Classic, 757 and 767-200/-300, ATR 42, ATR 72 and in the Airbus A300-600 and A310, used Electronic Flight Instrument Systems (EFIS) to display attitude and navigational information only, with traditional mechanical gauges retained for airspeed, altitude, vertical speed, and engine performance. Later glass cockpits, found in the Boeing 737NG, 747-400, 767-400, 777, A320 and later Airbuses, Ilyushin Il-96 and Tupolev Tu-204 have completely replaced the mechanical gauges and warning lights in previous generations of aircraft. While glass cockpit-equipped aircraft throughout the late 20th century still retained analog altimeters, attitude, and airspeed indicators as standby instruments in case the EFIS displays failed, more modern aircraft have been increasingly been using digital standby instruments as well, such as the Integrated standby instrument system.\n\nGlass cockpits originated in military aircraft in the late 1960s and early 1970s; an early example is the Mark II avionics of the F-111D (first ordered in 1967, delivered from 1970–73), which featured a multi-function display.\n\nPrior to the 1970s, air transport operations were not considered sufficiently demanding to require advanced equipment like electronic flight displays. Also, computer technology was not at a level where sufficiently light and powerful circuits were available. The increasing complexity of transport aircraft, the advent of digital systems and the growing air traffic congestion around airports began to change that.\n\nThe Boeing 2707 was one of the earliest commercial aircraft designed with a glass cockpit. Most cockpit instruments were still analog, but CRT displays were to be used for the Attitude indicator and HSI. However, the 2707 was cancelled in 1971 after insurmountable technical difficulties and ultimately the end of project funding by the US government.\n\nThe average transport aircraft in the mid-1970s had more than one hundred cockpit instruments and controls, and the primary flight instruments were already crowded with indicators, crossbars, and symbols, and the growing number of cockpit elements were competing for cockpit space and pilot attention. As a result, NASA conducted research on displays that could process the raw aircraft system and flight data into an integrated, easily understood picture of the flight situation, culminating in a series of flights demonstrating a full glass cockpit system.\n\nThe success of the NASA-led glass cockpit work is reflected in the total acceptance of electronic flight displays beginning with the introduction of the MD-80 in 1979. Airlines and their passengers alike have benefited. The safety and efficiency of flights have been increased with improved pilot understanding of the aircraft's situation relative to its environment (or \"situational awareness\").\n\nBy the end of the 1990s, liquid-crystal display (LCD) panels were increasingly favored among aircraft manufacturers because of their efficiency, reliability and legibility. Earlier LCD panels suffered from poor legibility at some viewing angles and poor response times, making them unsuitable for aviation. Modern aircraft such as the Boeing 737 Next Generation, 777, 717, 747-400ER, 747-8F 767-400ER, 747-8, and 787, Airbus A320 family (later versions), A330 (later versions), A340-500/600, A340-300 (later versions), A380 and A350 are fitted with glass cockpits consisting of LCD units.\n\nThe glass cockpit has become standard equipment in airliners, business jets, and military aircraft. It was fitted into NASA's Space Shuttle orbiters \"Atlantis\", \"Columbia\", \"Discovery\", and \"Endeavour\", and the current Russian Soyuz TMA model spacecraft that was launched in 2002. By the end of the century glass cockpits began appearing in general aviation aircraft as well. In 2003, Cirrus Design's SR20 and SR22 became the first light aircraft equipped with glass cockpits, which they made standard on all Cirrus aircraft. By 2005, even basic trainers like the Piper Cherokee and Cessna 172 were shipping with glass cockpits as options (which nearly all customers chose), as well as many modern aircraft such as the Diamond DA42 twin-engine travel and training aircraft. The Lockheed Martin F-35 Lightning II features a \"panoramic cockpit display\" touchscreen that replaces most of the switches and toggles found in an aircraft cockpit. The civilian Cirrus Vision SF50 now has the same, which they call a \"Perspective Touch\" glass cockpit.\n\nUnlike the previous era of glass cockpits—where designers merely copied the look and feel of conventional electromechanical instruments onto cathode ray tubes—the new displays represent a true departure. They look and behave very similarly to other computers, with windows and data that can be manipulated with point-and-click devices. They also add terrain, approach charts, weather, vertical displays, and 3D navigation images.\n\nThe improved concepts enable aircraft makers to customize cockpits to a greater degree than previously. All of the manufacturers involved have chosen to do so in one way or another—such as using a trackball, thumb pad or joystick as a pilot-input device in a computer-style environment. Many of the modifications offered by the aircraft manufacturers improve situational awareness and customize the human-machine interface to increase safety.\n\nModern glass cockpits might include Synthetic Vision (SVS) or Enhanced Vision systems (EVS). Synthetic Vision systems display a realistic 3D depiction of the outside world (similar to a flight simulator), based on a database of terrain and geophysical features in conjunction with the attitude and position information gathered from the aircraft navigational systems. Enhanced Vision systems add real-time information from external sensors, such as an infrared camera.\n\nAll new airliners such as the Airbus A380, Boeing 787 and private jets such as Bombardier Global Express and Learjet use glass cockpits.\n\nMany modern general aviation aircraft are available with glass cockpits. Systems such as the Garmin G1000 are now available on many new GA aircraft, including the classic Cessna 172. Many small aircraft can also be modified post-production to replace analogue instruments.\n\nGlass cockpits are also popular as a retrofit for older private jets and turboprops such as Dassault Falcons, Raytheon Hawkers, Bombardier Challengers, Cessna Citations, Gulfstreams, King Airs, Learjets, Astras, and many others. Aviation service companies work closely with equipment manufacturers to address the needs of the owners of these aircraft.\n\nToday, smartphones and tablets use mini-applications, or \"apps,\" to remotely control complex devices, by WiFi radio interface. They demonstrate how the \"glass cockpit\" idea is being applied to consumer devices. Applications include toy-grade UAVs which use the display and touch screen of a tablet or smartphone to employ every aspect of the \"glass cockpit\" for instrument display, and fly-by-wire for aircraft control.\n\nThe glass cockpit idea made news in 1980s trade magazines, like Aviation Week & Space Technology, when NASA announced that it would be replacing most of the electro-mechanical flight instruments in the space shuttles with glass cockpit components. The articles mentioned how glass cockpit components had the added benefit of being a few hundred pounds lighter than the original flight instruments and support systems used in the space shuttles.\n\nAs aircraft operation depends on glass cockpit systems, flight crews must be trained to deal with possible failures. The Airbus A320 family has seen fifty incidents where several flight displays were lost.\n\nOn 25 January 2008 United Airlines Flight 731 experienced a serious glass-cockpit blackout, losing half of the ECAM displays as well as all radios, transponders, TCAS, and attitude indicators. The pilots were able to land at Newark Airport without radio contact in good weather and daylight conditions.\n\nAirbus has offered an optional fix, which the US NTSB has suggested to the US FAA as mandatory, but the FAA has yet to make it a requirement. A preliminary NTSB factsheet is available. Due to the possibility of a blackout, glass cockpit aircraft also have an integrated standby instrument system that includes (at a minimum) an artificial horizon, altimeter and airspeed indicator. It is electronically separate from the main instruments and can run for several hours on a backup battery.\n\nIn 2010, the NTSB published a study done on 8,000 general aviation light aircraft. The study found that, although aircraft equipped with glass cockpits had a lower overall accident rate, they also had a larger chance of being involved in a fatal accident. The NTSB Chairman said in response to the study:\n\n"}
{"id": "19513970", "url": "https://en.wikipedia.org/wiki?curid=19513970", "title": "HCMOS", "text": "HCMOS\n\nHCMOS (\"high-speed CMOS\") is the set of specifications for electrical ratings and characteristics, forming the 74HC00 family, a part of the 7400 series of integrated circuits.\n\nThe 74HC00 family followed, and improved upon, the 74C00 series (which provided an alternative CMOS logic family to the 4000 series but retained the part number scheme and pinouts of the standard 7400 series (especially the 74LS00 series) .\nSome specifications include:\n\nHCMOS also stands for \"high-density CMOS\". The term was used to describe microprocessors, and other complex integrated circuits, which use a smaller manufacturing processes, producing more transistors per area. The Freescale 68HC11 is an example of a popular HCMOS microcontroller.\n\n\n"}
{"id": "33445579", "url": "https://en.wikipedia.org/wiki?curid=33445579", "title": "Hermann Hammesfahr", "text": "Hermann Hammesfahr\n\nHermann Hammesfahr (February 20, 1845 – November 23, 1914) was a Prussian-American inventor who invented a type of fiberglass cloth in which glass was interwoven with silk. He was awarded the patent by the United States Patent Office in 1880.This was the earliest fiberglass of any kind that is known to have been patented or produced. \n\nHammesfahr was born in Evangelisch, Flachsberg, Wald Solingen, Rhineland, Prussia and died in Brooklyn, Kings County, New York—the son of Carl Wilhelm Hammesfahr (1811–1878) and Caroline Wilhelmine Remschied (1806–1878). The Hammesfahr family is an old Solingen steel family tracing its roots back to the medieval armorer's guild.\n\nThis patent and a number of associated patents also provided the practical foundation for the development and production of fiber optics (Hammesfahr has been called “the grandfather of fiber optics”) and fiberglass. All were purchased by the Libbey Glass Company in Toledo, Ohio.\n\nAn immigrant from Prussia, he was known for his keen aesthetic achievements and technical innovations, which have proven recently to be historically seminal, until recently, Hermann Hammesfahr and his contributions have been obscured, buried in scientific journals and first-hand historical accounts—and in many cases erroneously attributed to others, such as Edward Drummond Libbey himself, owner of the Libbey Glassworks.\n\nLibbey Glass purchased the fiberglass patent among others from Hammesfahr with the intention of displaying the fabric in a spectacular manner at the 1892 Chicago World's Fair. At first, the company wove lamp shades from the glass fabric. Then it caught the eye of actress Georgia Cayven who requested a dress be made from it. Hammesfahr designed a dress and the Libby Glass company showed it off at the 1892 World's Fair. The dress was a sensation. Edison’s electric lightbulb also debuted at the Fair, as well as the Ferris Wheel; however contemporary accounts indicate it was the Glass Dress that attracted the attention of women across America.\n\nMany onlookers were impressed by the dress at the 1892 World's Fair and a copy was purchased for $30,000 by Princess Eulalie of Spain. However, the dress reportedly impractical and no one else is known to have bought another dress. Hammesfahr's granddaughter modeled a glass dress her mother had made at the 1904 St. Louis World's Fair and reported that the dress scratched and was fragile. Despite this, the dresses, neckties, and other attire made from the glass fabric succeeded in publicizing the flexible glass thread technology. Hammesfahr's creation inspired new, innovative uses for glass fabric. It could withstand corrosive chemicals so chemists and druggist used it to filter solid particles out of liquid. Tangled glass fibers, glass wool, made a great insulator and was used by industry to surround steam pipes. Glass fabric was even used as bandages.\n"}
{"id": "6949369", "url": "https://en.wikipedia.org/wiki?curid=6949369", "title": "History of spaceflight", "text": "History of spaceflight\n\nSpaceflight began in the 20th century following theoretical and practical breakthroughs by Konstantin Tsiolkovsky and Robert H. Goddard. The Soviet Union took the lead in the post-war Space Race, launching the first satellite, the first man and the first woman into orbit. The United States caught up with, and then passed, their Soviet rivals during the mid-1960s, landing the first man on the Moon in 1969. In the same period, France, the United Kingdom, Japan and China were concurrently developing more limited launch capabilities.\n\nFollowing the end of the Space Race, spaceflight has been characterised by greater international co-operation, cheaper access to low Earth orbit and an expansion of commercial ventures. Interplanetary probes have visited all of the planets in the Solar System, and humans have remained in orbit for long periods aboard space stations such as \"Mir\" and the ISS. Most recently, China has emerged as the third nation with the capability to launch independent manned missions, whilst operators in the commercial sector have developed re-usable booster systems and craft launched from airborne platforms.\n\nAt the beginning of the 20th century, there was a burst of scientific investigation into interplanetary travel, inspired by fiction by writers such as Jules Verne (\"From the Earth to the Moon\", \"Around the Moon\") and H.G. Wells (\"The First Men in the Moon\", \"The War of the Worlds\").\n\nThe first realistic proposal of spaceflight goes back to Konstantin Tsiolkovsky. His most famous work, \"\" (\"Issledovanie mirovikh prostranstv reaktivnimi priborami\", or \"The Exploration of Cosmic Space by Means of Reaction Devices\"), was published in 1903, but this theoretical work was not widely influential outside Russia.\n\nSpaceflight became an engineering possibility with the work of Robert H. Goddard's publication in 1919 of his paper \"A Method of Reaching Extreme Altitudes\", where his application of the de Laval nozzle to liquid fuel rockets gave sufficient power for interplanetary travel to become possible. This paper was highly influential on Hermann Oberth and Wernher Von Braun, later key players in spaceflight.\n\nIn 1929, the Slovene officer Hermann Noordung was the first to imagine a complete space station in his book \"The Problem of Space Travel\".\n\nThe first rocket to reach space was a German V-2 rocket, on a vertical test flight in June 1944. After the war ended, the research and development branch of the (British) Ordinance Office organised \"Operation Backfire\" which, in October 1945, assembled enough V-2 missiles and supporting components to enable the launch of three (possibly four, depending on source consulted) of them from a site near Cuxhaven in northern Germany. Although these launches were inclined and the rockets didn't achieve the altitude necessary to be regarded as sub-orbital spaceflight, the Backfire report remains the most extensive technical documentation of the rocket, including all support procedures, tailored vehicles and fuel composition.\n\nSubsequently, the British Interplanetary Society proposed an enlarged man-carrying version of the V-2 called \"Megaroc\". The plan, written in 1946, envisaged a three-year development programme culminating in the launch of test pilot Eric Brown on a sub-orbital mission in 1949.\n\nThe decision by the Ministry of Supply under Attlee's government to concentrate on research into nuclear power generation and sub-sonic passenger jet aircraft over supersonic atmospheric flight and spaceflight delayed the introduction of both of the latter (although only by a year in the case of supersonic flight, as the data from the Miles M.52 was handed to Bell Aircraft.\n\nOver a decade after the \"Megaroc\" proposal, true orbital space flight, both unmanned and manned, was developed by the Soviet Union and the United States during the Cold War, in a competition dubbed the \"Space Race\".\n\nThe race began in 1957, when both the US and the USSR made statements announcing they planned to launch artificial satellites during the 18 month long International Geophysical Year of July 1957 to December 1958. On July 29, 1957, the US announced a planned launch of the Vanguard by the spring of 1958, and on July 31, the USSR announced it would launch a satellite in the fall of 1957.\n\nOn October 4, 1957, the Soviet Union launched Sputnik 1, the first artificial satellite of Earth in the history of mankind.\n\nOn November 3, 1957, the Soviet Union launched the second satellite, Sputnik 2, and the first to carry a living animal, a dog named Laika. Sputnik 3 was launched on May 15, 1958, and carried a large array of instruments for geophysical research and provided data on pressure and composition of the upper atmosphere, concentration of charged particles, photons in cosmic rays, heavy nuclei in cosmic rays, magnetic and electrostatic fields, and meteoric particles.\n\nAfter a series of failures with the program, the US succeeded with Explorer 1, which became the first US satellite in space, on February 1, 1958. This carried scientific instrumentation and detected the theorized Van Allen radiation belt.\n\nThe US public shock over Sputnik 1 became known as the Sputnik crisis. On July 29, 1958, the US Congress passed legislation turning the National Advisory Committee for Aeronautics (NACA) into the National Aeronautics and Space Administration (NASA) with responsibility for the nation's civilian space programs. In 1959, NASA began Project Mercury to launch single-man capsules into Earth orbit and chose a corps of seven astronauts introduced as the \"Mercury Seven\".\n\nOn April 12, 1961, the USSR opened the era of manned spaceflight, with the flight of the first \"cosmonaut\" (Russian name for space travelers), Yuri Gagarin. Gagarin's flight, part of the Soviet Vostok space exploration program, took 108 minutes and consisted of a single orbit of the Earth.\n\nOn August 7, 1961, Gherman Titov, another Soviet cosmonaut, became the second man in orbit during his Vostok 2 mission.\n\nBy June 16, 1962, the Union launched a total of six Vostok cosmonauts, two pairs of them flying concurrently, and accumulating a total of 260 cosmonaut-orbits and just over sixteen cosmonaut-days in space.\n\nOn May 5, 1961, the US launched its first suborbital Mercury astronaut, Alan Shepard, in the Freedom 7 capsule. The US public was becoming increasingly shocked and alarmed at the widening lead obtained by the USSR, so President John F. Kennedy announced on May 25 a plan to land a man on the moon by 1970, launching the three-man Apollo program.\n\nOn February 20, 1962, the US succeeded in launching the third manned orbital spaceflight in history, with John Glenn, the first US orbital astronaut, making three orbits during his Friendship 7 mission. By May 16, 1963, the US launched a total of six Project Mercury astronauts, logging a cumulative 34 Earth orbits, and 51 hours in space.\n\nThe first woman in space was former civilian parachutist Valentina Tereshkova, who entered orbit on June 16, 1963, aboard the Soviet mission Vostok 6. The chief Soviet spacecraft designer, Sergey Korolyov, conceived of the idea to recruit a female cosmonaut corps and launch two women concurrently on Vostok 5/6. However, his plan was changed to launch a male first in Vostok 5, followed shortly afterward by Tereshkova. Khrushchev personally spoke to Tereshkova by radio during her flight.\n\nOn November 3, 1963, Tereshkova married fellow cosmonaut Andrian Nikolayev, who had previously flown on Vostok 3. On June 8, 1964, she gave birth to the first child conceived by two space travelers. The couple divorced in 1982, and Tereshkova went on to become a prominent member of the Communist Party of the Soviet Union.\n\nThe second woman to fly to space was aviator Svetlana Savitskaya, aboard Soyuz T-7 on August 18, 1982.\n\nSally Ride became the first American woman in space when she flew aboard Space Shuttle mission STS-7 on June 18, 1983. Women space travelers went on to become commonplace during the 1980s.\n\nHelen Sharman became the first European woman in space aboard the Soyuz TM-12 on May 18, 1991.\n\nKhrushchev pressured Korolyov to quickly produce greater space achievements in competition with the announced Gemini and Apollo plans. Rather than allowing him to develop his plans for a crewed Soyuz spacecraft, he was forced to make modifications to squeeze two or three men into the Vostok capsule, calling the result Voskhod. Only two of these were launched. Voskhod 1 was the first spacecraft with a crew of three, who could not wear space suits because of size and weight constrictions. Alexei Leonov made the first spacewalk when he left the Voskhod 2 on March 8, 1965. He was almost lost in space when he had extreme difficulty fitting his inflated space suit back into the cabin through an airlock, and a landing error forced him and his crewmate to be lost in dangerous woods for hours before being found by the recovery crew.\n\nThe start of manned Gemini missions was delayed a year later than NASA had planned, but ten largely successful missions were launched in 1965 and 1966, allowing the US to overtake the Soviet lead by achieving space rendezvous (Gemini 6A) and docking (Gemini 8) of two vehicles, long duration flights of eight days (Gemini 5) and fourteen days (Gemini 7), and demonstrating the use of extra-vehicular activity to do useful work outside a spacecraft (Gemini 12).\n\nThe USSR made no manned flights during this period but continued to develop its Soyuz craft and secretly accepted Kennedy's implicit lunar challenge, designing Soyuz variants for lunar orbit and landing. They also attempted to develop the N1, a large, manned moon-capable launch vehicle similar to the US Saturn V.\n\nAs both nations rushed to get their new spacecraft flying with men, the intensity of the competition caught up to them in early 1967, when they suffered their first crew fatalities. On January 27, the entire crew of Apollo 1, \"Gus\" Grissom, Ed White, and Roger Chaffee, were killed by suffocation in a fire that swept through their cabin during a ground test approximately one month before their planned launch. On April 24, the single pilot of Soyuz 1, Vladimir Komarov, was killed in a crash when his landing parachutes tangled, after a mission cut short by electrical and control system problems. Both accidents were determined to be caused by design defects in the spacecraft, which were corrected before manned flights resumed.\n\nThe US succeeded in achieving President Kennedy's goal on July 20, 1969, with the landing of Apollo 11. Neil Armstrong and Buzz Aldrin became the first men to set foot on the Moon. Six such successful landings were achieved through 1972, with one failure on Apollo 13.\n\nThe N1 rocket suffered four catastrophic unmanned launch failures between 1969 and 1972, and the Soviet government officially discontinued its manned lunar program on June 24, 1974, when Valentin Glushko succeeded Korolyov as General Spacecraft Designer.\n\nBoth nations went on to fly relatively small, non-permanent manned space laboratories Salyut and Skylab, using their Soyuz and Apollo craft as shuttles. The US launched only one Skylab, but the USSR launched a total of seven \"Salyuts\", three of which were secretly Almaz military manned reconnaissance stations, which carried \"defensive\" cannons. Manned reconnaissance stations were found to be a bad idea since unmanned satellites could do the job much more cost-effectively. The United States Air Force had planned a manned reconnaissance station, the Manned Orbital Laboratory, which was cancelled in 1969. The Soviets cancelled Almaz in 1978.\n\nIn a season of detente, the two competitors declared an end to the race and shook hands (literally) on July 17, 1975, with the Apollo-Soyuz Test Project, where the two craft docked, and the crews exchanged visits.\n\nThe National Aeronautics and Space Administration (NASA ) is an independent agency of the executive branch of the United States federal government responsible for the civilian space program, as well as aeronautics and aerospace research.\n\nProject Mercury was the first human spaceflight program of the United States, running from 1958 through 1963. Its goal was to put a man into Earth orbit and return him safely, ideally before the Soviet Union. John Glenn became the first American to orbit the earth on February 20, 1962 aboard the Mercury-Atlas 6.\n\nProject Gemini was NASA's second manned spaceflight program. The program ran from 1961 to 1966. The program pioneered the orbital maneuvers required for space rendezvous. Ed White became the first American to make an extravehicular activity (EVA, or \"space walk\"), on June 3, 1965, during Gemini 4. Gemini 6A and 7 accomplished the first space rendezvous on December 15, 1965. Gemini 8 achieved the first space docking with an unmanned Agena Target Vehicle on March 16, 1966. Gemini 8 was also the first US spacecraft to experience in-space critical failure endangering the lives of the crew.\n\nThe Apollo program was the third manned spaceflight program carried out by NASA. The programs goal was to orbit and land manned vehicles on the moon. The program ran from 1969 to 1972. Apollo 8 was the first manned spaceflight to leave earth orbit and orbit the Earth's moon on December 21, 1968. Neil Armstrong and Buzz Aldrin became the first men to set foot on the moon during the Apollo 11 mission on July 20, 1969.\n\nThe Skylab programs goal was to create the first space station of NASA. The program marked the last launch of the Saturn V rocket on May 19, 1973. Many experiments were performed on board, including unprecedented solar studies. The longest manned mission of the program was Skylab 4 which lasted 84 days, from November 16, 1973 to February 8, 1974. The total mission duration was 2249 days, with Skylab finally falling from orbit over Australia on July 11, 1979.\n\nAlthough its pace slowed, space exploration continued after the end of the Space Race. The United States launched the first reusable spacecraft, the Space Shuttle, on the 20th anniversary of Gagarin's flight, April 12, 1981. On November 15, 1988, the Soviet Union duplicated this with an unmanned flight of the only \"Buran\"-class shuttle to fly, its first and only reusable spacecraft. It was never used again after the first flight; instead the Soviet Union continued to develop space stations using the Soyuz craft as the crew shuttle.\n\nSally Ride became the first American woman in space in 1983. Eileen Collins was the first female Shuttle pilot, and with Shuttle mission STS-93 in July 1999 she became the first woman to command a US spacecraft.\n\nThe United States continued missions to the ISS and other goals with the high-cost shuttle system, which was retired in 2011.\n\nThe Soviet space program was the rocketry and space exploration program conducted by the Soviet Union from the 1930s till its collapse in 1991.\n\nThe Sputnik 1 became the first artificial Earth satellite on 4 October 1957. The satellite transmitted a radio signal, but had no sensors otherwise. Studying the Sputnik 1 allowed scientists to calculate the drag from the upper atmosphere by measuring position and speed of the satellite. Sputnik 1 broadcast for 21 days until its batteries depleted on 4 October 1957, and the satellite finally fell from orbit on 4 January 1958.\n\nThe Luna programme was a series of unmanned robotic satellite launches with the goal of studying the moon.The program ran from 1959 to 1976 and consisted of 15 successful missions, the program achieved many first achievements and collected data on the Moon's chemical composition, gravity, temperature, and radiation. Luna 2 became the first man made object to make contact with the moons surface in September 1959. Luna 3 returned the first photographs of the far side of the moon in October 1959.\n\nThe Vostok Programme the first Soviet spaceflight project to put the Soviet citizens into low Earth orbit and return them safely. The programme carried out six manned spaceflights between 1961 and 1963. The program was the first program to put humans into space, with Yuri Gagarin becoming the first man in space on April 12, 1961 aboard the Vostok 1. Gherman Titov Became the first person to stay in orbit for a full day on August 7, 1961 aboard the vostok 2. Valentina Tereshkova became the first woman in space on June 16, 1963 aboard the vostok 6.\n\nThe Voskhod programme began in 1964 and consisted of two manned flights before the program was canceled by the Soyuz programme in 1966. Voskhod 1 launched on October 12, 1964 and was the first manned spaceflight with a multi-crewed vehicle. Alexey Leonov performed the first spacewalk aboard Voskhod 2 on March 18, 1965.\n\nThe Salyut programme was the first space station program undertaken by the Soviet Union. The goal was to carry out long-term research into the problems of living in space and a variety of astronomical, biological and Earth-resources experiments. The program ran from 1971 to 1986. Salyut 1, the first station in the program, became the world's first crewed space station.\n\nThe Soyuz programme was initiated by the soviet space program in the 1960s and continues as the responsibility of roscosmos to this day. The program currently consists of 140 completed flights, and since the retirement of the US Space Shuttle has been the only craft to transport humans. The programs original goal was part of a program to put a cosmonaut on the moon, and later became crucial to the construction of the Mir space station.\n\nMir was a space station that operated in low Earth orbit from 1986 to 2001, operated by the Soviet Union and later by Russia. \"Mir\" was the first modular space station and was assembled in orbit from 1986 to 1996. The station served as a microgravity research laboratory in which crews conducted experiments in biology, human biology, physics, astronomy, meteorology and spacecraft systems with a goal of developing technologies required for permanent occupation of space. The record for longest human endurance on a single tour in space is held by Valeriy Polyakov, who left Earth on January 8, 1994, and stayed aboard the \"Mir\" space station for a total of 437 days, 17 hours, 58 minutes, and 16 seconds, returning March 22, 1995. Sergei Krikalyov holds the current record for combined total time in space: 803 days, 9 hours, and 39 seconds. \"Mir\" was continuously occupied for 3,644 days, eight days short of 10 years, between the launch of Soyuz TM-8 on September 5, 1989 and the landing of Soyuz TM-29 on August 28, 1999. This record was held until surpassed by the International Space Station (ISS) in 2010. The ISS has been continuously occupied for days.\n\nThe Buran Programme was a Soviet and later Russian reusable spacecraft project that began in 1974 at the Central Aerohydrodynamic Institute in Moscow and was formally suspended in 1993. The Buran programme was started by the Soviet Union as a response to the United States Space Shuttle program. Officially, the Buran orbital vehicle was designed for the delivery to orbit and return to Earth of spacecraft, cosmonauts, and supplies.\n\nRecent space exploration has proceeded, to some extent in worldwide cooperation, the high point of which was the construction and operation of the International Space Station (ISS). At the same time, the international space race between smaller space powers since the end of the 20th century can be considered the foundation and expansion of markets of commercial rocket launches and space tourism.\n\nThe United States continued other space exploration, including major participation with the ISS with its own modules. It also planned a set of unmanned Mars probes, military satellites, and more. The Constellation space program, began by President George W. Bush in 2004, aimed to launch a next-generation multifunction Orion spacecraft by 2018. A subsequent return to the Moon by 2020 was to be followed by manned flights to Mars, but the program was canceled in 2010 in favor of encouraging commercial US manned launch capabilities.\n\nRussia, a successor to the Soviet Union, has high potential but smaller funding. Its own space programs, some of a military nature, perform several functions. They offer a wide commercial launch service while continuing to support the ISS with several of their own modules. They also operate manned and cargo spacecraft which continued after the US Shuttle program ended. They are developing a new multi-function PPTS manned spacecraft for use in 2018 and have plans to perform manned moon missions as well. The program aims to put a man on the moon in the 2020s, becoming the second country to do so.\n\nThe European Space Agency has taken the lead in commercial unmanned launches since the introduction of the Ariane 4 in 1988 but is in competition with NASA, Russia, Sea Launch (private), China, India, and others. The ESA-designed manned shuttle Hermes and space station Columbus were under development in the late 1980s in Europe; however, these projects were canceled, and Europe did not become the third major \"space power\".\n\nThe European Space Agency has launched various satellites, has utilized the manned Spacelab module aboard US shuttles, and has sent probes to comets and Mars. It also participates in ISS with its own module and the unmanned cargo spacecraft ATV.\n\nCurrently ESA has a program for development of an independent multi-function manned spacecraft CSTS scheduled for completion in 2018. Further goals include an ambitious plan called the Aurora Programme, which intends to send a human mission to Mars soon after 2030. A set of various landmark missions to reach this goal are currently under consideration. The ESA has a multi-lateral partnership and plans for spacecraft and further missions with foreign participation and co-funding.\nESA is also developing Galileo program which seeks to give independence to the EU from the American GPS.\n\nSince 1956 the Chinese have had a space program which was aided early on from 1957-1960 by the Soviets. They were provided missile technology experts and missiles to study from. In 1965 plans were made to launch a human into space by 1979, and in 1967 the plans were made for a 4-human spacecraft. \"East is Red\" was launched on April 24, 1970 and was the first satellite to be launched by the Chinese. In 1974 the plan for human spaceflight was scrapped when policy makers decided that applications satellites were more important and competing with the USA and USSR wasn't as important. In late 1986, the 863 Project was started which had a focus on military applications, but also had a goal for human spaceflight.\n\nDespite possessing less funding than ESA or NASA, the People's Republic of China has achieved manned space flight and operates a commercial satellite launch service. There are plans for a Chinese space station and a program to send unmanned probes to Mars.\n\nChina's first attempt at a manned spacecraft, Shuguang, was abandoned after years of development, but on October 15, 2003, China became the third nation to develop an indigenous human spaceflight capability when Yang Liwei entered orbit aboard Shenzhou 5.\n\nThe US Pentagon released a report in 2006, detailing concerns about China's growing presence in space, including its capability for military action. In 2007 China tested a ballistic missile designed to destroy satellites in orbit, which was followed by a US demonstration of a similar capability in 2008.\n\nJapan's space agency, the Japan Aerospace Exploration Agency, is a major space player in Asia. While not maintaining a commercial launch service, Japan has deployed a module in the ISS and operates an unmanned cargo spacecraft, the H-II Transfer Vehicle.\n\nJAXA has plans to launch a Mars fly-by probe. Their lunar probe, SELENE, is touted as the most sophisticated lunar exploration mission in the post-Apollo era. Japan's Hayabusa probe was mankind's first sample return from an asteroid. IKAROS was the first operational solar sail.\n\nAlthough Japan developed the HOPE-X, Kankoh-maru, and Fuji manned capsule spacecraft, none of them have been launched. Japan's current ambition is to deploy a new manned spacecraft by 2025 and to establish a Moon base by 2030.\n\nThe National Space Organization (NSPO; formerly known as the National Space Program Office) and the National Chung-Shan Institute of Science and Technology are the national civilian space agencies of the democratic industrialized developed country of Taiwan under the auspices of the Ministry of Science and Technology (Taiwan). The National Chung-Shan Institute of Science and Technology is involved in designing and building Taiwanese nuclear weapons, hypersonic missiles, spacecraft and rockets for launching satellites while the National Space Organization is involved in space exploration, satellite construction, and satellite development as well as related technologies and infrastructure (including the FORMOSAT series of Earth observation satellites similar to NASA along with DARPA {In-Q-Tel} such as Google Earth {Keyhole, Inc} or so forth) and related research in astronautics, quantum physics, materials science with microgravity, aerospace engineering, remote sensing, astrophysics, atmospheric science, information science, design and construction of indigenous Taiwanese satellites and spacecraft, launching satellites and space probes into low Earth orbit. Additionally, a state of the art manned spaceflight program is currently in development in Taiwan and is designed to compete directly with the manned programs of China, United States and Russia. Active research is currently undergoing in the development and deployment of space-based weapons for the defense of national security in Taiwan.\n\nIndian Space Research Organisation, India's national space agency, maintains an active space program. It operates a small commercial launch service and launched a successful unmanned lunar mission dubbed Chandrayaan-1 in October 2007. India has plans for a further unmanned mission to the Moon Chandrayaan 2 by end of 2016 or early 2017. India has successfully launched an interplanetary mission, Mars Orbiter Mission, in 2013 which reached Mars in September 2014, hence becoming the first country in the world to do a Mars mission in its maiden attempt. The ISRO is currently developing a small shuttle system.\n\nCosmonauts and astronauts from other nations have flown in space, beginning with the flight of Vladimir Remek, a Czech, on a Soviet spacecraft on March 2, 1978. , a total of 536 people from 38 countries have gone into space according to the FAI guideline.\n\nIndia, Taiwan and Japan are increasingly capable of competing in space research and activity. These nations, along with China, form the main players in the Asian space race.\nIran announced plans to begin a manned space program in 2021.\n\n\n"}
{"id": "8806323", "url": "https://en.wikipedia.org/wiki?curid=8806323", "title": "Home medical equipment", "text": "Home medical equipment\n\nThis article discusses the definitions and types of home medical equipment (HME), also known as durable medical equipment (DME), and durable medical equipment prosthetics and orthotics (DMEPOS).\n\nHome medical equipment is a category of devices used for patients whose care is being managed from a home or other private facility managed by a nonprofessional caregiver or family member. It is often referred to as \"durable\" medical equipment (DME) as it is intended to withstand repeated use by non-professionals or the patient, and is appropriate for use in the home.\n\nMedical supplies of an expendable nature, such as bandages, rubber gloves and irrigating kits are not considered by Medicare to be DME.\n\nWithin the US medical and insurance industries, the following acronyms are used to describe home medical equipment:\n\n\nThe following are representative examples of home medical equipment\n\n\n\nFor most home medical equipment to be reimbursed by insurance, a patient must have a doctor's prescription for the equipment needed. Some equipment, such as oxygen, is FDA regulated and must be prescribed by a physician before purchase whether insurance reimbursed or otherwise.\n\nThe physician may recommend a supplier for the home medical equipment, or the patient will have to research this on their own. HME / DMEPOS suppliers are located throughout the country and some specialty shops can also be found on the internet.\n\nThere is no established typical size for HME / DMEPOS suppliers. Supply companies include very large organizations such as Walgreens, Lincare, and Apria to smaller local companies operated by sole proprietors or families. A new evolution in the home medical equipment arena is the advent of internet retailers who have lower operating costs so they often sell equipment for lower prices than local \"brick and mortar\", but lack the ability to offer in-home setup, equipment training and customer service. In all cases, however, there are strict rules and laws governing HME / DMEPOS suppliers that participate in Medicare and Medicaid programs. In addition to rules outlined the National Supplier Clearinghouse, of division of CMS (centers for Medicare and Medicaid), all Medicare DME suppliers must obtain and maintain accreditation by one of many approved accrediting bodies.\n\nOnce a patient or caregiver selects an appropriate HME / DMEPOS supplier, he/she presents the supplier with the prescription and patient's insurance information. HME / DMEPOS suppliers maintain an inventory of products and equipment, so fulfillment of the prescription is rapid, much like a Pharmacy.\n\nThe HME / DMEPOS supplier is obligated to perform certain functions when providing home medical equipment. These include:\n\n\nAll HME / DMEPOS suppliers are required to comply with Health Insurance Portability and Accountability Act (HIPAA) to protect patients' confidentiality and records.\n\nHome medical equipment is typically covered by patient's healthcare insurance, including Medicare (Part B). In order to properly code home medical equipment for billing, the Healthcare Common Procedure Coding System HCPCS is utilized.\nAs of 2014, under the Medicare Prescription Drug, Improvement, and Modernization Act of 2003, providers of HME/DMEPOS will be required to become third-party accredited to standards regulated by the Centers for Medicare and Medicaid Services (CMS) in order to continue eligibility under Medicare Part B. This effort aims to standardize and improve the quality of service to patients provided by home medical equipment suppliers.\n\n"}
{"id": "35029495", "url": "https://en.wikipedia.org/wiki?curid=35029495", "title": "Human HGF plasmid DNA therapy", "text": "Human HGF plasmid DNA therapy\n\nHuman HGF plasmid DNA therapy of cardiomyocytes is being examined as a potential treatment for coronary artery disease (a major cause of myocardial infarction (MI)), as well as treatment for the damage that occurs to the heart after MI. After MI, the myocardium suffers from reperfusion injury which leads to death of cardiomyocytes and detrimental remodelling of the heart, consequently reducing proper cardiac function. Transfection of cardiac myocytes with human HGF reduces ischemic reperfusion injury after MI. The benefits of HGF therapy include preventing improper remodelling of the heart and ameliorating heart dysfunction post-MI.\n\nHuman hepatocyte growth factor (HGF) is an 80kD pleiotropic protein that is endogenously produced by a variety of cell types from the mesenchymal cell lineage (such as cardiomyocytes and neurons). It is produced and proteolytically cleaved to its active state in response to cellular injury or during apoptosis. HGF binds to c-met receptors found on mesenchymal cell types to produce its many different effects such as increased cellular motility, morphogenesis, proliferation and differentiation. Research has shown that HGF has potent angiogenic, anti-fibrotic, and anti-apoptotic properties. It has also been shown to act as a chemoattractant for adult mesenchymal stem cells via c-met receptor binding.\n\nAnimal research has demonstrated that administration of HGF cDNA plasmids into ischemic cardiac tissue can increase cardiac function (improved left ventricular ejection fraction and fractional shortening compared to control subjects) after induced MI or ischemia. Transfection with HGF plasmids in damaged cardiac tissue also promotes angiogenesis (increased capillary density compared to control subjects), as well as decreasing detrimental remodelling of the tissue at the site of injury (decreased fibrotic deposition). The increased production of HGF by transfected cardiomyocytes during injury has also shown to be a powerful chemo-attractant of adult mesenchymal stem cells via HGF/c-Met binding. The mitogenic and morphogenic properties of HGF induce recruited stem cells to take on cardiomyocyte phenotypes, potentially helping in the healing of ischemic tissue.\nThe benefits of HGF in experimental models have led to its investigation in clinical trials. A phase I clinical trial entailed injecting an adenovirus vector with the human HGF (Ad-hHGF) gene into the coronary vessels localized to ischemic tissue. Results demonstrate that it is in fact safe to administer the Ad-hHGF vector into patients with coronary artery disease in hopes of re-vascularizing damaged tissue in patients for which coronary artery bypass surgery (CABG) or percutaneous coronary intervention (PCI) are not available or possible. Despite the trial’s limitations (\"i.e.\" no assessment of left ventricular function and sample size was quite small), upon follow up assessments at 12 months, none of the patients receiving the treatment had been readmitted to hospital for MI, angina or aggravated heart failure.\n"}
{"id": "30882491", "url": "https://en.wikipedia.org/wiki?curid=30882491", "title": "International Society for Music Information Retrieval", "text": "International Society for Music Information Retrieval\n\nThe International Society for Music Information Retrieval (ISMIR) is an international forum for research on the organization of music-related data. It started as an informal group steered by an \"ad hoc\" committee in 2000 which established a yearly symposium - whence \"ISMIR\", which meant International Symposium on Music Information Retrieval. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.\n\nGiven the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.\n\nAs the term Music Information Retrieval (MIR) indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science, electrical engineering and many others.\n\nSince its inception in 2000, ISMIR has been the world’s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.\n\n\nThe official webpage provides a more up-to-date information on past and future conferences and provides access to all past websites and to the cumulative database of all papers, posters and tutorials presented at these conferences. An overview of all papers published at ISMIR can be found at DBLP.\n\nThe following list gives an overview of the main research areas and topics that are within the scope of \nMusic Information Retrieval.\n\n\n\n\n\n\n\nThe \"Music Information Retrieval Evaluation eXchange\" (MIREX) is an annual evaluation campaign for MIR algorithms, coupled to the ISMIR conference. Since it started in 2005, MIREX has fostered advancements both in specific areas of MIR and in the general understanding of how MIR systems and algorithms are to be evaluated. MIREX is to the MIR community what the Text Retrieval Conference (TREC) is to the text information retrieval community: A set of community-defined formal evaluations through which a wide variety of state-of-the-art systems, algorithms and techniques are evaluated under controlled conditions. MIREX is managed by the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the University of Illinois at Urbana-Champaign (UIUC).\n\n\n\n"}
{"id": "53512425", "url": "https://en.wikipedia.org/wiki?curid=53512425", "title": "Jussi Pekka Kasurinen", "text": "Jussi Pekka Kasurinen\n\nJussi Pekka Kasurinen is a Finnish non-fiction writer and a post-doctoral scholar, born in Savonlinna, Finland, in 1982.\n\nHe graduated as a master of science (in engineering) from Lappeenranta University of Technology (LUT) in 2007 and finished his PhD in 2011, also in LUT.\n\nJussi Kasurinen has received the best paper award in the Computer Systems and Technologies conference in 2016 with his paper \"Games as Software – Similarities and Differences between the Implementation Projects\".\n\nKasurinen has also utilized his knowledge gained in academia to popularize science as a writer. He has written several books and articles in IT magazines, such as Skrolli. Kasurinen uses his full name when writing books so that he would not be confused with another Jussi Kasurinen who is also a writer.\n\n\nKasurinen is credited as actor in Iron Sky: Coming Race and producer in the 2016 short film Infirmity.\n\nKasurinen is a member of Finnish Software Measurement Association (FiSMA), working as a board member and chairman of the research forum.\n\nKasurinen has Bacon-Erdos-index of 8.\n"}
{"id": "28777173", "url": "https://en.wikipedia.org/wiki?curid=28777173", "title": "Kindle Banking Systems", "text": "Kindle Banking Systems\n\nKindle Banking Systems was a banking solutions company that originated from a bespoke banking systems project in 1979 for the Irish merchant bank Ansbacher Bank. Its main product was a modular banking system known as Bankmaster. The system originally ran on the ICL hardware platform - the relationship with the UK-based computer manufacturer facilitated Kindle establishing a significant customer base in Asia and Africa.\n\nKindle is regarded as a company of note, as it was the first high-tech Irish company which achieved the status of world leader in its field.\n\nKindle's main product Bankmaster, was a modular System25 Assembler-based banking system and started development in 1979 as a bespoke banking systems project in the Irish merchant bank Ansbacher Bank.\n\nThe Company was originally called MA Systems, then Triple A Systems before finally settling on Kindle Banking Systems. The driving force behind the design and development of Bankmaster was Cian Kinsella. Led by MD and founder Tony Kilduff, the company had an impressive sales record developing many lasting customer relationships in the UK, Africa, India, Asia and Latin America.\n\nBy 1987, according to the RBS Market Report, there were 170 sites operating Bankmaster worldwide. Standard Chartered Bank was the largest user of Bankmaster - the company formed a spin-off joint venture with Kindle specifically to support its own deployments. State Bank of India deployed the branch platform, Branchpower to 2000 branches throughout India. The system allowed branches to operate in an off-line mode - this made it popular with retail banks in countries where the communications infrastructure was poor. The Bank of England and eight other central banks (mainly in Africa) used the treasury modules of Bankmaster.\n\nIn 1991, Kindle was acquired by a rival company; ACT, and was renamed as ACT Kindle. ACT was subsequently acquired by Misys in 1995 - the Kindle brand was retained until 2001 at which point it was subsumed into the Misys branding. During the later part of the 1990s sales of Bankmaster declined - whilst the Bankmaster solution set was considered to be functionally rich, the core technology had been allowed to become obsolete.\n\nBetween 1994 and 2000 the company made a significant investment in developing a new generation retail banking system, called Bankmaster-RS. It was developed in cooperation with the Colombian bank Coomeva. Bankmaster-RS struggled to deliver the planned functionality and was never as widely adopted as its predecessor.\n"}
{"id": "10175196", "url": "https://en.wikipedia.org/wiki?curid=10175196", "title": "LIXI", "text": "LIXI\n\nLIXI is an Australian, member-based not-for-profit company that develops data message transaction standards for the Australian mortgage processing industry, and promotes improvements in efficiency in mortgage processing. Owned by the members of the initiative, LIXI represents participants in the residential mortgage lending industry. Founded in 2001, LIXI has developed and released standards for such transactions as mortgage applications, property valuations, broker commissions and several others.\n\nWith the purpose being the exchange of transaction data in the lending chain, the initial LIXI efforts focused on the creation and promulgation of data standards based on XML. This effort had the intended result, with the organisation winning an Innovation Award from MIS magazine in 2005. LIXI also collaborates with the Australian research institution NICTA. This effort is in two primary areas, one is the creation of reference implementations of the data standards and the second is in the area of mapping data standards on to business processes.\n\nMembers of LIXI include all the major and mid-tier banks in Australia, as well as the mortgage insurers, valuation panel managers and large valuations firms, the major software providers to the lending industry, and a number of legal, conveyancing and outsource service providers:\n\n"}
{"id": "46901976", "url": "https://en.wikipedia.org/wiki?curid=46901976", "title": "Lactoscope", "text": "Lactoscope\n\nThe lactoscope is an instrument for estimating the amount of cream in milk, based on its relative opacity. The higher the opacity, the greater the amount of cream present. Credit for the development of this instrument is given to Alfred Donné in 1843. The instrument was also used to measure the fat content of milk, but it gave inaccurate results.\n\n"}
{"id": "3302451", "url": "https://en.wikipedia.org/wiki?curid=3302451", "title": "List of Palm OS devices", "text": "List of Palm OS devices\n\nThis is a list of Palm OS devices, and companies that make, or have made, them.\n\nemia\nFIZA\nWizz\ne2\ne3\ne4\nS.A.F.E\nGage\n\nFossil, made Wrist PDAs that use the Palm OS operating system.(Discontinued)\n\n\n\n\n\nPDA with integrated GPS.\n\n\nSmartphones with Palm OS \n\n\n\nThe inventors of the Palm formed a new company called Handspring in June 1998, operating until 2003 when it merged with Palm, Inc.'s hardware division.\n\nSmartphones (except 90)\n\nVisors introduced color cases and the Springboard Expansion slot.\n\n\nIBM's Workpad series was nearly identical to PDAs manufactured by Palm. The main difference were color and logo on the casing.\n\n\n\nSmartphones\n\n\nChinese PDAs\n\n\n\nThe Zire series, replaced by the \"Z\" series in 2005, are the lower end Palm models. Some have color screens (some 160x160, some 320x320), some B&W (at 160x160). \n\n\nThe Tungsten series are the high end Palm models, with ARM/RISC processors (except T|W), high-resolution color screens, and SD memory cards.\n\n\nThe Treo series are combo Cell Phones/PDA models, originally developed by Handspring.\n\n\nSmartphones, later sold to Kyocera\n\nSmartphones\n\n\nSony developed and marketed the CLIÉ multimedia PDA from 2000 to 2005.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDA with integrated barcode reader\n\n\nA PDA designed for handheld gaming. It was held sideways (landscape), had an analog joystick and extra gaming buttons, and used Bluetooth for multiplayer gaming as well as standard PDA functions. It also introduced a dedicated video chip, and dual SD card slots.\n\n\nA PalmIII \"clone\" with loudspeaker and a CF card slot supporting memory expansion, modems, network and barcode scanner cards.\n\n\nTwo models (candybar and slider) were demonstrated at PalmSource Euro Dev Con 2005 running PalmOS Cobalt 6.1.1 \nA few were sold onsite. Oswin never produced more. These were the only PalmOS cobalt devices to be seen in the wild.\nThe codename for the candybar version was Zircon A108\n\n\n\n"}
{"id": "43161284", "url": "https://en.wikipedia.org/wiki?curid=43161284", "title": "List of common EMC test standards", "text": "List of common EMC test standards\n\nThe following list outlines a number of EMC standards which are known at the time of writing to be either available or have been made available for public comment. These standards attempt to standardize product EMC performance, with respect to conducted or radiated radio interference from electrical or electronic equipment, imposition of other types of disturbance on the mains supply by such equipment, and the sensitivity of such equipment to received interference. \n\nThe legal status of these standards varies according to the jurisdiction. Standards called up by the European Union's EMC Directive effectively have the force of law in the EU. \n\nCISPR standards cover product emission and immunity requirements as well as defining test methods and equipment.\n\nCISPR is the acronym of Comité International Spécial des Perturbations Radio, or the International Special Committee for Radio Protection.\n\nThe IEC standards on EMC are mostly part of the IEC 61000 family. Below are some examples.\n\nThe following are ISO standards on automotive EMC issues.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4331300", "url": "https://en.wikipedia.org/wiki?curid=4331300", "title": "List of light sources", "text": "List of light sources\n\nThis is a list of sources of light, including both natural and artificial processes that emit light. This article focuses on sources that produce wavelengths from about 390 to 700 nanometers, called visible light.\n\n\nIncandescence is the emission of light from a hot body as a result of its temperature.\n\n\n\n\n\n\nLuminescence is emission of light by a substance not resulting from heat.\n\nIn gemology, aventurescence (sometimes called aventurization) is an optical reflectance effect seen in certain gems.\n\nBioluminescence is light resulting from biochemical reaction by a living organism.\n\nCathodoluminescence is light resulting from a luminescent material being struck by electrons.\n\nChemiluminescence is light resulting from a chemical reaction.\n\nCryoluminescence is the emission of light when an object is cooled.\n\nCrystalloluminescence is light produced during crystallization.\n\nElectrochemiluminescence is light resulting from electrochemical reaction.\n\nElectroluminescence is light resulting from an electric current being passed through a substance.\n\nMechanoluminescence is light resulting from a mechanical action on a solid.\n\nTriboluminescence, a type of mechanoluminescence, is light generated when bonds in a material are broken when that material is scratched, crushed, or rubbed.\n\nFractoluminescence, a type of mechanoluminescence, is light generated when bonds in certain crystals are broken by fractures.\n\nPiezoluminescence, a type of mechanoluminescence, is light produced by the action of pressure on certain solids.\n\nSonoluminescence, a type of mechanoluminescence, is light resulting from imploding bubbles in a liquid when excited by sound.\n\nPhotoluminescence is light resulting from absorption of photons.\n\nFluorescence, a type of photoluminescence, is the emission of light by a substance that has absorbed light or other electromagnetic radiation.\n\nUnlike fluorescence, a phosphorescent material does not immediately re-emit the radiation it absorbs.\n\nRadioluminescence is light resulting from bombardment by ionizing radiation.\n\nThermoluminescence is light from the re-emission of absorbed energy when a substance is heated.\n\n\n"}
{"id": "24855546", "url": "https://en.wikipedia.org/wiki?curid=24855546", "title": "Lyric Hearing", "text": "Lyric Hearing\n\nLyric Hearing is an extended wear hearing aid developed by InSound Medical, founded by medical device inventor entrepreneur Adnan Shennib and Thar Casey. The device rests inside the ear canal and uses the ears anatomy to funnel sound to the ear drum. The device is placed into the ear canal by a certified professional and no surgery or anesthesia is required when inserting the device. Although individual replacement needs can vary, Lyric is designed to be worn for up to 4 months at a time without needing to remove it or replace batteries.\n\nThe design and placement of the Lyric Hearing device reduces background noise and feedback for natural sound quality. Wearers of the device can use it while talking on the phone, exercising, or sleeping and do not need to fear the device falling out. Lyric Hearing is also water resistant, so wearers can shower with the device or do other water surface based activities, although the company does not recommend swimming or diving.\n\nThe Lyric Hearing device is not a Cochlear Implant. The device is removed every few months and is replaced with a new one. Wearers can adjust the volume of the device using a magnet and a removal tool is also provided. \"Business Week\" reports that this hearing aid received a 2009 Medical Design Excellence Award.\n\nLyric was sold in 2010 to Sonova, the parent company of Phonak, a hearing aid (daily wear) manufacturer. Phonak launched a new version of the Lyric (Lyric 2) in 2012 that is smaller in order to fit more ear canals.\n\n\n"}
{"id": "41217487", "url": "https://en.wikipedia.org/wiki?curid=41217487", "title": "MOSFET Gate Driver", "text": "MOSFET Gate Driver\n\nMOSFET Gate Driver is a specialized circuit that is used to drive the gate of power MOSFETs effectively and efficiently in high-speed switching applications. The addition of high MOSFET Gate drivers are the last step if the turn-on is to fully enhance the conducting channel of the MOSFET technology.\nThe Gate Driver works under the same principle as the MOSFET transistor. It provides an output current that provides a charge to the semiconductor by a control electrode. It is also simple to drive and has resistive nature for power uses.\n"}
{"id": "47144806", "url": "https://en.wikipedia.org/wiki?curid=47144806", "title": "Macrogen", "text": "Macrogen\n\nMacrogen, Inc. is a South Korea public biotechnology company. The company's headquarters are located in Seoul. The company was founded in 1997 by Jeong-sun Seo, a professor at Seoul National University. It was venture capital-backed until its initial public offering (IPO) and subsequent listing on the KOSDAQ market in 2000, making it the first Korean biotechnology firm to raise funds through an IPO. The company announced plans in 2010 to map the \"Korean genome\" based on the notion that United States-backed genome mapping efforts up to that time were representative of the \"Caucasian genome\" rather than the \"Human genome\" as claimed. By 2015, the company was described by one news outlet as \"a global leader in personalized genomic medicine\".\n\nMacrogen's business model focuses on providing research services to other firms engaged in biotechnology research and development. , 50% of the firm's revenue was derived from DNA sequencing services. Prior to its 2000 IPO, the company aimed to be the low-cost leader in providing research services; after the IPO, Macrogen turned to DNA sequencing technology development as a way to distinguish itself from competitors. , the company had four main divisions based on geography: Korea, USA, Europe and Japan.\n\nConsistent with the post-IPO competitive technology strategy was Macrogen's being among the first customers to purchase a HiSeq X Ten System from Illumina in January 2014, purported to be the first sequencing system to enable cracking the $1,000 genome boundary.\n\nIn 2015, the company underwent a corporate identity revision, which included a revised corporate logo and a revised corporate slogan: \"Humanizing Genomics.\"\n\nAs of 2014, HyungTae Kim held the role of chief executive officer (CEO) at Macrogen. By early 2015, however, the CEO role had been filled by Hyon-yong Chong.\n\nWith respect to the company's Board, founder Jeong-sun Seo remained Chairman as of 2015.\n\nAccording to KOSDAQ statistics, over 90% of the company's outstanding stock is held by individual investors.\n\nIn late 2014, Macrogen licensed Clarity LIMS from GenoLogics, a system \"built specifically for the needs of clinical or research genomics and mass spec laboratories\". Initial integration with service offerings was to be through the USA-based \"Macrogen Clinical Laboratory\".\n\nIn 2014 and 2015, Macrogen won competitive bidding rounds to provide genome analysis services to the University College London.\n"}
{"id": "11136953", "url": "https://en.wikipedia.org/wiki?curid=11136953", "title": "Manufacturing Technologies Association", "text": "Manufacturing Technologies Association\n\nThe Manufacturing Technologies Association (MTA) is a UK trade association representing the manufacturing technologies industry. The MTA sits at the core of the engineering based manufacturing sector and as an association works tirelessly to ensure member companies are as commercially successful as possible.\n\nThe association's key activities include:\n- Representing engineering based manufacturing and supporting the advanced engineering sector through lobbying, media contact and networking.\n- Providing relevant and specific industry intelligence \n- Encouraging talent through funding and support for workplace training and education initiatives in schools, colleges and universities.\n- Delivering the UK's only major exhibition focused on manufacturing technologies – MACH (owned and organised by the MTA)\n\nMembership of the MTA is open to companies involved with the manufacturing technologies sector and end users of such technology. \n\nThe MTA represents UK companies, their associates and affiliates who drive UK manufacturing technology, innovation and quality.\n\nThe website contains a member & product directory by geographic position. It also provides information and statistics on the economic health of the British manufacturing markets.\n\nMTA is the owner and organiser of the UK's premier manufacturing event - the MACH exhibition, a week-long event held biennially in April at the NEC.\n\nThe types of manufacturing technologies covered by the organisation, and the exhibition, include:\n\nThe original Machine Tools Trades Association (MTTA) was formed in 1919, becoming the Machine Tools Technologies Association. Since its formation the association has evolved into a trade body with a global support network, promoting internationally competitive trade in manufacturing technologies.\n\nThe MTA is based in London, with close contact with the British Government through the Department of Business Innovation and Skills (BIS) (United Kingdom), CBI and UK Trade & Investment. \n\nThe association's offices are located on \"Bayswater Road\" (A402) next to Lancaster Gate tube station.\n\nThe association also provides a secretariat service to the Federation of British Hand Tool Manufacturers (FBHTM), and the MTA Tooling Group, formerly the British Hard Metal and Engineers' Cutting Tools Association (BHECTA).\n\n\n"}
{"id": "432912", "url": "https://en.wikipedia.org/wiki?curid=432912", "title": "Mascara", "text": "Mascara\n\nMascara is a cosmetic commonly used to enhance the eyelashes. It may darken, thicken, lengthen, and/or define the eyelashes. Normally in one of three forms—liquid, cake, or cream—the modern mascara product has various formulas; however, most contain the same basic components of pigments, oils, waxes, and preservatives.\n\nThe \"Collins English Dictionary\" defines \"mascara\" as, \"a cosmetic substance for darkening, lengthening, curling, coloring, and thickening the eyelashes, applied with a brush or rod.\" The \"Oxford English Dictionary\" (\"OED\") adds that mascara is occasionally used on the eyebrows as well.\n\nThe \"OED\" also references \"mascaro\" from works published in the late 15th century. In 1886, the \"Peck & Snyder Catalogue\" advertises, “Mascaro or Water Cosmetique… For darkening the eyebrow and moustaches without greasing them and making them prominent.” In 1890, the \"Century Dictionary\" defined mascara as “a kind of paint used for the eyebrows and eyelashes by actors.” And in 1894, N. Lynn advises in \"Lynn’s Practical Hints for Making-up\", “to darken eyelashes, paint with mascara, or black paint, with a small brush.\n\nThe source of the word “mascara” is unclear. The Spanish word \"máscara\" meaning ‘mask’ or ‘stain’, and the Italian word \"maschera\" meaning ‘mask’ are possible origins. A related Catalan word describes soot or a black smear, and the Portuguese word \"máscara\" means ‘mask’ and \"mascarra\" means dark stain or smut). There is even strong support for a possible source from the Arabic word \"maskharah\" or ‘buffoon’. The Hebrew word משקרות (MaSQROTh) as relating to women's eyes is found in Isaiah 3:16.\n\nLatin treatises sometimes used the word mascara when referring to witches.\n\nAesthetic adornment is a cultural universal and mascara can be documented in ancient Egypt. Records from around 4000 BC refer to a substance called kohl that was used to darken eyelashes, eyelids, and eyebrows. Kohl was used to mask the eyes, believed to ward off evil spirits and protect the soul, by both men and women. Often composed of galena; malachite; and charcoal or soot, crocodile stool; honey; and water was added to keep the kohl from running. Through Egypt’s influence, kohl usage persisted in the subsequent Babylonian, Greek and Roman empires. Following the fall of the Roman Empire, kohl fell into disuse on the European continent, where it had been considered solely a cosmetic; conversely, it continued to be widely used in the Middle East for religious purposes.\nMakeup was considered unsightly and uncouth in Western culture until the Victorian era. During the Victorian era, social opinion shifted radically towards the promotion of cosmetics, and women were known to spend a majority of their day occupied with beauty regimens. Great efforts were made to create the illusion of long, dark eyelashes. Attempting this, Victorian women made a type of mascara in their own homes. They would heat a mixture of ash or lampblack and elderberry juice on a plate and apply the heated mixture to their eyelashes.\n\nThe product that people would recognize as mascara today did not develop until the 19th century. A chemist named Eugène Rimmel developed a cosmetic using the newly invented petroleum jelly. The name Rimmel became synonymous with the substance and still translates to “mascara” in the Portuguese, Spanish, Greek, Turkish, Romanian, and Persian languages today.\n\nAcross the Atlantic Ocean and at roughly the same time, in 1915, Thomas Lyle Williams created a remarkably similar substance for his sister Maybel. In 1917 he started a mail-order business from the product that grew to become the company Maybelline.\n\nThe mascara developed by these two men consisted of petroleum jelly and coal in a set ratio. It was undeniably messy, and a better alternative was soon developed. A dampened brush was rubbed against a cake containing soap and black dye in equal proportions and applied to the lashes. Still it was extremely messy. No significant improvement occurred until 1957 with an innovation by Helena Rubinstein.\n\nThe events leading to Rubinstein’s improvement began in Paris in the early 20th century. There, at the fashion capital of the world, mascara was quickly gaining popularity and common usage. Elizabeth Arden and Helena Rubinstein, two giants in the American beauty industry, watched and kept abreast of its development. After the First World War, American consumers became eager for new products. Sensing an opportunity, both Rubinstein and Arden launched their own brands of cosmetics that included mascara. Through the efforts of these two rivals and public temperament, mascara finally gained respectability and favor in American society.\n\nThe invention of the photograph and motion picture launched mascara’s popularity and usage further forward in America. Motion pictures especially advertised a new standard of beauty and sex appeal. Famous actresses of the classic cinema era, such as Theda Bara, Pola Negri, Clara Bow, Greta Garbo, Marlene Dietrich, Bette Davis, and Jean Harlow, depended heavily upon mascara for their glamorized appearances, which the average woman sought to mimic.\n\nIn 1933, a woman known on court records as Mrs. Brown consented to have her eyelashes permanently dyed. Unfortunately, the product, Lash Lure, used \"para\"-phenylenediamine, a chemical extremely toxic to the body, as the dyeing agent. At the time, cosmetics were unregulated by the Federal Drug Administration, and the dangers of paraphenylenediamine were unknown. Within hours of the treatment, Mrs. Brown began experiencing severe symptoms of stinging and burning eyes. By the next morning, Mrs. Brown’s eyes had developed ulcers which oozed and had swollen shut. Use of Lash Lure resulted in blindness in Mrs. Brown and fifteen other women and also caused the death of another. It was only after the Lash Lure incident and several others like it, documented in Ruth deForest Lamb’s book entitled \"American Chamber of Horrors\", that Congress granted the FDA the right to regulate cosmetics in 1938.\n\nYears later in 1957, Rubinstein created a formula that evolved mascara from a hard cake into a lotion-based cream. She packaged the new mascara in a tube to be sold with a brush. For use, the cream was squeezed onto the brush and applied to lashes. Although still messy, it was a step towards the modern mascara product.\n\nSoon, a grooved rod was patented. This device picked up the same amount of mascara for each use. Then the grooved rod was altered to the brush similar to the ones used today. The change in applicator led mascara to be even easier to use, and its popularity increased.\n\nMascara is now trending towards multi-functional usage, with many mascaras including lash-boosting serums, botanicals, and pro-vitamin-enriched formulas. Korean technology is at the forefront of the development, and a number of brands use tubing formulas to coat the lash. \n\nAll formulations contain pigments, oils, and waxes.\n\nThe pigmentation for black mascara is similar to that used by the Egyptians and Victorian women. Black and brown mascaras typically are colored by use of iron oxides. Some mascaras contain ultramarine blue.\n\nMascara is composed of a base mixture of pigments, waxes, and oils with varying supporting components. Mascara pigments most commonly include iron oxides and titanium dioxide which provide mascara with its desired color. Titanium dioxide (TiO) accounts for over 65% of inorganic pigments sales volume. TiO gives the pigment a white color while different iron oxides provide a variety of colors such as red, yellow, brown, and black. The particle size of opaque pigments ranges from 0.2-0.3 µm.\n\nAmong the many oils used, linseed oil, castor oil, eucalyptus oil, lanolin, and oil of turpentine are found frequently. Sesame oil is also commonly used. Waxes usually found in mascara are paraffin wax, carnauba wax, and beeswax.\n\nSome mascaras are created with the use of fish scales.\n\nThe desired effects of the mascara account for most variations of ingredients. Water-resistant mascaras require hydrophobic ingredients, like dodecane. Non water-resistant mascaras have base ingredients that are water-soluble. Mascaras designed to lengthen or curl the eyelashes often contain nylon or rayon microfibers. Additionally, ceresin, gum tragacanth, and methyl cellulose are regular ingredients and serve as stiffeners.\n\nMascara is often used on a daily basis around the world. In 2016 alone, U.S. consumers spent 335.6 million USD on the top 10 leading mascara brands.\n\nIn one method of production, referred to as anhydrous, all waxes, oils, and pigments are mixed, heated, and agitated simultaneously. The alternative method, termed emulsion, starts by combining water and thickeners. Separately, waxes and emulsifiers are combined. Pigment is then added individually to both mixtures. Finally all is combined in a homogenizer, which acts as a high-speed agitator in order to thoroughly mix the oils, water, waxes, and emulsifiers—ingredients that naturally repulse each other.\n\nMascara has a shelf life of two to four months.\n\nMercury compounds can be used as a preservative in eye make-up and so are occasionally found in mascara .\n\nIt is more common to develop a stye, or commoner still, swollen eyelids. Styes and swollen eyelids are better classified as allergic reactions. The allergic reactions can be stimulated by any of the components of mascara but is usually attributed to methylparaben, aluminum powder, ceteareth-20, butylparaben, or benzyl alcohol.\n\nThe use of mascara may help to mimic neoteny, a youthful or childlike appearance which is believed to be correlated to beauty in women. In various cultures, the typical characteristics of children—soft cheeks, a round face, soft skin, large eyes, upturned nose, and a short chin—are often seen as ideal facial characteristics for women. Mascara pulls away the eyelash from the rim of the eye creating a doe-like illusion of larger, more open eyes. Wide eyes may be associated to youth.\n"}
{"id": "46840724", "url": "https://en.wikipedia.org/wiki?curid=46840724", "title": "Micromotor", "text": "Micromotor\n\nMicromotors are very small particles (measured in microns) that can move themselves. The term is often used interchangeably with \"nanomotor,\" despite the implicit size difference. These micromotors actually propel themselves in a specific direction autonomously when placed in a chemical solution. There are many different micromotor types operating under a host of mechanisms. Easily the most important examples are biological motors such as bacteria and any other self-propelled cells. Synthetically, researchers have exploited oxidation-reduction reactions to produce chemical gradients, local fluid flows, or streams of bubbles that then propel these micromotors through chemical media. \n\nMicromotors may have applications in medicine since they have been shown to be able to deliver materials to living cells within an organism. They also have been shown to be effective in degrading certain chemical and biological warfare agents.\n\nJanus sphere micromotors usually consist of a titanium dioxide surface layer and a strong reducing agent inner layer. The interaction of the two layer under irradiation of UV light produces bubbles as a result of a reduction reaction. Micromotors usually have a size of about 30μm with a small 2μm opening on the outer layer. This leads to the exposure of the inner core, which is typically the fuel source for the propulsion mechanism. The diameter of the hole controls the rate and speed of the reaction.\n\nNano particle incorporation into micromotors has been recently studied and observed further. Specifically, gold nanoparticles have been introduced to the traditional titanium dioxide outer layer of most micromotors. The size of these gold nanoparticles typically is distributed from anywhere around 3 nm to 30 nm. Since these gold nanoparticles are layered on top of the inner core (usually a reducing agent, such as magnesium), there is enhanced macrogalvanic corrosion observed. Technically, this is where the cathode and anode are in contact with each other, creating a circuit. The cathode, as a result of the circuit, is corroded. The depletion of this inner core leads to the reduction of the chemical environment as a fuel source. For example, in a TiO/Au/Mg micromotor in a seawater environment, the magnesium inner core would experience corrosion and reduce water to begin a chain of reactions that results in hydrogen gas as a fuel source. The reduction reaction is as follows: formula_1\n\nResearchers hope that micromotors will be used in medicine to deliver medication and do other precise small-scale interventions. A study has shown that micromotors could deliver gold particles to the stomach layer of living mice.\n\nMicromotors are capable of photocatalytic degradation with the appropriate composition. Specifically, micromotors with a titanium dioxide/gold nanoparticle outer layer and magnesium inner core are currently being examined and studied for their degradation efficacy against chemical and biological warfare agents (CBWA). These new TiO/Au/Mg micromotors produce no reagents or toxic byproducts from the propulsion and degradation mechanisms. However, they are very effective against CBWAs and present a complete and rapid degradation of certain CBWAs. There has been recent research of TiO/Au/Mg micromotors and their use and degradation efficacy against biological warfare agents, such as Bacillus anthracis, and chemical warfare agents, such as organaphosphate nerve agents- a class of acetylcholinesterase inhibitors. Therefore, application of these micromotors is a possibility for defense and environmental applications.\n\nThese new micromotors are composed of a photoactive titanium dioxide outer/surface layer that has gold nanoparticles as well. Under UV irradiation, the adsorbed water produces strongly oxidizing hydroxyl radicals. Also, adsorbed molecular O reacts with electrons producing superoxide anions. Those superoxide anions also produce to the production of peroxide radicals, hydroxyl radicals, and hydroxyl anions. Transformation into carbon dioxide and water, otherwise known as mineralization, of CWAs has been observed as a result of the radicals and anions. Also, the gold nanoparticles effectively shift the Fermi level of titanium dioxide, enhancing the distribution of the electron charge. Therefore, the lifetime of the radicals and anions is extended, so the implementation of gold nanoparticles has greatly improved photocatalytic efficiency.\n"}
{"id": "6531559", "url": "https://en.wikipedia.org/wiki?curid=6531559", "title": "Multiverse Network", "text": "Multiverse Network\n\nThe Multiverse Network, Inc. was an American startup company creating a network and platform for Massively Multiplayer Online Games (MMOGs) and 3D virtual worlds. Multiverse's stated aim was to lower the barrier of entry for development teams by providing a low-cost software platform for online game and virtual world development.\n\nIn 2009, the company extended its development platform to support Flash and built a series of real-time multiplayer games to demo the technology. As part of the worldwide marketing effort behind James Cameron's AVATAR film, Multiverse built two Flash-based games, one with McDonald's and another with Coca-Cola Zero. Both games allow players to explore Pandora, where much of the film takes place.\n\nIn late 2011, Multiverse closed from lack of profits, releasing the source code to The Multiverse Foundation, a non-profit group of volunteers who are presently updating the Platform.\n\nMultiverse provided technology known as MMOG middleware (Multiverse used the term platform). It included the client software Multiverse World Browser (for Microsoft Windows only), a server suite, development tools, sample assets, documentation, and a developer community. The goal was to provide consumers/users with a single client program that let them visit all of the virtual worlds built on the Multiverse Platform. From the consumer point of view, this enabled a de facto network of virtual worlds.\n\nLike RealmForge, the Multiverse World Browser was written in C#, and based on the Axiom Engine. The Multiverse server suite was written in Java and used a publish/subscribe messaging system to provide reliability and scalability. The server also provided a plug-in API. The Windows-based tools used the COLLADA data interchange format, to enable artists to import 3D assets from popular tools such as Maya, 3D Studio Max, and Google SketchUp.\n\nMultiverse provided its technology platform cost-free for development and deployment. Income came through revenue-sharing; Multiverse took a share of any payments made by consumers/users to the world developer. If a developer provided a world for free (or free for a period of time), Multiverse did not charge anything. When a developer started charging consumers/users, Multiverse took a share (10 percent), and also handled the financial transaction processing. Development teams hosted their own servers and retained 100 percent of their world's IP.\n\nJames Cameron joined the company's board of advisors, and Red Herring magazine selected it as one of the \"Red Herring 100\" privately held companies that play a leading role in innovating the technology business.\n\nIn December, 2006, Multiverse announced that it had optioned the rights to develop an MMOG based on \"Firefly\", the science fiction television series . In 2008, a Buffy and Titanic MMO were announced . None of them ever came to fruition.\n\nAfter closing shop, the Multiverse Network released its code as open source under the MIT License. It is now managed by the Multiverse Foundation , a worldwide non-profit organization.\n\n\n"}
{"id": "1950346", "url": "https://en.wikipedia.org/wiki?curid=1950346", "title": "Navarro Networks", "text": "Navarro Networks\n\nNavarro Networks was a developer of Ethernet-based ASIC components based in Plano, Texas, in the United States. They produced a high-end network processor for Ethernet and other applications.\n\nNavarro Networks was founded in 2000. Their CEO was Mark Bluhm, who was formerly a Vice President at Cyrix. A group of nine employees left the Cyrix division of Via in March 2000 to staff the company.\n\nThey were acquired by Cisco Systems on May 1, 2002 in a stock transaction worth up to $85M. They joined the Internet Systems Business Unit to enhance Cisco's internal ASIC capability in Ethernet switching platforms.\n\n"}
{"id": "7168628", "url": "https://en.wikipedia.org/wiki?curid=7168628", "title": "Network Centric Operations Industry Consortium", "text": "Network Centric Operations Industry Consortium\n\nThe Network Centric Operations Industry Consortium (NCOIC) is an international not-for-profit, chartered in the United States, whose goal is to facilitate the adoption of cross-domain interoperability standards. Formed in September 2004, the organization is composed of more than 50 members and advisors representing business, government organizations and academic institutions in 12 countries.\n\nNCO is the application of the fundamental tenets of network-centric warfare to aspects of national security, especially industry support for the missions of both the United States Department of Defense and the Department of Homeland Security (DHS). NCOIC does not only subscribe to the military use of this theory, but also works to apply NCO and interoperability across nations and industries, including emergency response, health care, aerospace, information technology cyber security & cloud computing, energy and financial services.\n\nNCOIC’s technical teams have developed resources to further the use of network-centric systems and interoperability in both the public and private sectors. These resources – including processes, tools, frameworks, patterns, principles and databases—are available free of charge on the NCOIC website. They are aimed at helping an organization lower engineering costs, speed program implementation, increase capability and reduce risk. The consortium also provides training and services such as interoperability demonstrations, acquisition strategies, evaluations and verification.\n\nNCOIC focuses on four interdependent areas in identifying solutions that will enable cross-domain interoperability: business, culture, governance and technical. The interaction, influence and impact of factors—such as financial objectives, business goals, laws and regulations, and cultural considerations – are all taken into account when planning and/or implementing technology change.\n\n\"Systems, Capabilities, Operations, Programs, & Enterprises (SCOPE) Model\"\n• The SCOPE interoperability assessment model is designed to characterize interoperability-relevant aspects or capabilities of a system or set of systems over a network in terms of a set of dimensions and values along those dimensions.\n\n\"NCOIC Interoperability Framework (NIF)\"\n• The NIF is a development framework that helps system architects and system engineers to embed interoperability elements throughout the life cycle of programs, beginning with requirements. Whenever possible, those resources are based upon standards.\n\nNet Centric Patterns\"\n• NCOIC Net Centric Patterns contain prescriptive recommendations on approaches and standards in specific interoperability domains.\n\n\"Network Centric Analysis Tool (NCAT)\"\n• NCAT is a collaborative, web-enabled questionnaire-based tool developed to assist NCOIC teams and member companies to enhance the likelihood and reduce the time and effort of member companies developing interoperable systems consistent with customers’ policies and guidelines, reference models and architectures. It is also available in an excel format.\n\"NCOIC QuadTrangle\"\n• The QuadTrangle™ developed by the Network Centric Operations Industry Consortium shows the four, interdependent areas that must be considered when developing a reliable and trusted interoperable environment: business, culture, governance and technical.\n"}
{"id": "14598720", "url": "https://en.wikipedia.org/wiki?curid=14598720", "title": "Nutating disc engine", "text": "Nutating disc engine\n\nA nutating disc engine (also sometimes called a disc engine) is an internal combustion engine comprising fundamentally of one moving part and a direct drive onto the crankshaft. Initially patented in 1993, it differs from earlier internal combustion engines in a number of ways and utilizes a circular rocking or wobbling \"nutating motion\", drawing heavily from similar steam-powered engines developed in the 19th century, and similar to the motion of the non-rotating portion of a swash plate on a swash plate engine.\n\nIn its basic configuration the core of the engine is a nutating non-rotating disc, with the center of its hub mounted in the middle of a Z-shaped shaft. The two ends of the shaft rotate, while the disc \"nutates\", (performs a wobbling motion without rotating around its axis). The motion of the disc circumference prescribes a portion of a sphere. A portion of the area of the disc is used for intake and compression, a portion is used to seal against a center casing, and the remaining portion is used for expansion and exhaust. The compressed air is admitted to an external accumulator, and then into an external combustion chamber before it is admitted to the power side of the disc. The external combustion chamber enables the engine to use diesel fuel in small engine sizes, giving it unique capabilities for unmanned aerial vehicle propulsion and other applications. One significant benefit of the nutating engine is the overlap of the power strokes.\n\nPower is transmitted directly to the output shaft, (the crankshaft), completely eliminating the need for complicated linkages essential in a conventional piston engine (to convert the piston's linear motion to rotating output motion). Since the disc does not rotate, the seal velocities are lower than in an equivalent IC piston engine. The total seal length is rather long, however, which may negate this advantage.\n\nThe disc wobbles inside a housing and, in its simplest version, half of the single disc (one lobe) performs the intake/compression function while the other lobe performs the power/exhaust function. The disc lobes can be configured to have equal compression and expansion volumes, or to have the compression volume greater than or less than\nthe expansion volume. This means that the engine can be self supercharged (see supercharger), or operate as a Miller cycle / Atkinson cycle.\n\nU.S. patent number 5,251,594 was granted to Leonard Meyer of Illinois in 1993 for a \"nutating internal combustion disc engine\". The Meyer Nutating Engine is a new type of internal combustion engine with higher power density than conventional reciprocating piston engines and which can operate on a variety of fuels, including gasoline, heavy fuels and hydrogen. The patent made reference to various 20th-century nutating engines in the United States, but no reference at all to the original Dakeyne engine, described below, in its prior art. The similarity to its 166-year-old hydraulic predecessor is strikingly evident, the main change being that the disc is not entirely flat but slightly convex.\n\nThe details of operation as well as the potential of the Meyer nutating disk engine has been published by Professor T. Alexander (publishes as T. Korakianitis) and co-workers.\n\nA single prototype has been run briefly under its own power, with a power- to-weight ratio equal to those of typical current four-stroke engines. It is claimed by the authors of the developer/US Army Research Laboratory/NASA technical evaluation report that a production version of the new engine (for UAV applications) might provide a power-to-weight ratio of 1.6 hp/lb or 2.7 kW/kg. This is slightly better than current automotive production engines but nowhere near the Graupner G58 or the Desert Air DA 150.\n\nA company called McMasters, previously headed by successful American entrepreneur Harold McMaster, is also developing a nutating motor burning a mixture of pure hydrogen and pure oxygen that, it claims, will give 200 hp but weigh only one-tenth that of gasoline/air production automotive engines with the same output. So far the McMasters company claims to have spent $10 million on its development. Plans are also being made to develop a version \"the size of a coffee can\" that can be built directly into wheel hubs, eliminating the traditional drive train entirely. This concept was first attempted in the British Leyland Mini Moke but was, at that time, severely hampered by lack of reliable synchronization – which is now more commonplace because of ubiquitous miniaturized embedded modern day computer chips. A gasoline-powered version is also planned by McMasters, which is claimed to give substantially cleaner operation than traditional engines.\n\nIn the 1820s the mill owners Edward & James Dakeyne of Darley Dale, Derbyshire designed and had constructed a hydraulic engine (a water engine) known as \"The Romping Lion\" to make use of the high-pressure water available near their mill.\n\nThe Dakeyne brothers had previously also invented \"The Equalinium\", a machine for the preparation of flax for spinning, and their father Daniel Dakeyne (1733–1819) was granted a patent for this device in 1794. It is often said that Edward and James did not take out the patent themselves because they were minors at the time, but in fact they were 23 and 21.\n\nLittle is known of their engine other than from the somewhat unclear description accompanying the patent, which was granted in 1830. Its main castings were made at the Morley Park foundry near Heage, and it weighed 7 tons and generated 35 horsepower at a head of 96 feet of water.\n\nStephen Glover, in his gazetteer of Derbyshire, was enthusiastic about the prospects for the disc engine, foreseeing its use in all manner of applications, domestic as well as industrial, not only as a prime mover but also as a pump. He stated that John Dakeyne had also commissioned a disc engine to drive the bellows of an organ in the family's residence, Knabb House.\n\nFrank Nixon in his book \"The Industrial Archaeology of Derbyshire\" (1969) commented that \"The most striking characteristic of this ingenious machine is perhaps the difficulty experienced by those trying to describe it; the patentees & Stephen Glover only succeeded in producing descriptions of monumental incomprehensibility\".\n\nA larger model was constructed to drain lead mines at Alport near Youlgreave and many steam versions were subsequently built by other people.\n\nThe first people to develop steam-powered disc engines based on the Dakeynes' design were George Davies and Henry Taylor who patented their engine in 1836. It was fitted with valves to control the admission of steam and also differed from the Dakeynes' version in that the axis of the engine was horizontal and the casing of the engine rotated around the disc, the opposite of the original. More patents followed over the next eight years, mainly introducing expansive working and improving the engine's sealing.\n\nIn 1836 Davies and Taylor granted manufacturing rights for the engine to Fardon and Gossage, owners of a salt works. At the same time Davies was working on a canal tug with a disc engine driving a paddle wheel at the stern. By 1838 a 5 hp engine was in use at the salt works pumping brine.\n\nIn 1839 Davies, Taylor, Fardon and Gossage conveyed manufacturing rights to the engine to the Birmingham Patent Disc Engine company. As Superintendent of the Company, Henry Davies was responsible for all design and manufacture, while Gossage was a director. In February 1841 the Board reported that 26 engines had been completed, further engines totalling 260 horsepower were in progress, and a total of 500 horsepower were on order. They could make engines ranging from 5 to 30 horsepower and were currently making engines for a railway carriage. An article in a French journal of 1841 reported that a 12 hp engine had been in use for six months as a winding engine at Corbyn's Hall Mine, Dudley, which could lift a load of 1 ton 180 ft in 1 minute. The disc engines cost from £96 for an 8 hp machine to £300 for a 30 hp model.\n\nRansomes of Ipswich (who were later to become the well known agricultural engineers, Ransomes and Sims) exhibited a portable steam engine at the Royal Liverpool Show in 1841, powered by a 5 hp BPDE disc engine.\n\nBy 1840 a canal boat, \"The Experiment\", powered by a Davies engine, was being used for propellor testing, and in 1842 Davies installed a disc engine and disc pump in a canal barge which he demonstrated by draining mile of the Stourbridge canal. The same year, a 5 hp engine was fitted in one of HMS \"Geyser's\" pinnaces. However, trials on the Thames and for the Directors of the Grand Junction canal failed to convince either the Admiralty or the canal owners.\n\nNevertheless, there was a growing interest in using steam power on the canals, and the small beam of canal boats very much favoured disc engines. Davies saw his opportunity and built an iron-hulled canal tug with a 16 hp BPDE engine in 1843. To minimise wash he fitted four propellors spaced along a shaft the length of the boat and enclosed in a tube below the waterline. There were two of these propulsion units side by side for a total of 8 propellors. It worked well enough to convince the Directors of the Birmingham and Liverpool Junction Canal to order six tugs which could tow as many as sixteen barges a day at a reasonable speed. In use, a train of six to eight barges left Ellesmere Port and Wolverhampton each day, carrying an average of 100 tons. Unfortunately nobody had considered how the barge train was to transit through the canal locks and shallows. Each such obstruction meant that the train had to be uncoupled and the barges individually manhandled or towed by horse through the obstruction before the train was reassembled on the other side. This negated the benefits of the tug and train and in 1845 the canal's Directors removed the tugs from service.\n\nIn 1844 the BPDE collapsed. The workshop equipment, various completed engines and quantities of work in progress were offered for sale. During legal proceedings in 1851 following the bankruptcy of two of the BPDE's principal investors, it was said that the disc engine had not made a profit and that to have relied on it as a realisable asset \"was absurd\".\n\nA competitor to Davies and Taylor was former locomotive engineer George Daniell Bishopp, who had Donkin & Co build his first engine in 1840, and a patent was granted in 1845. The partners Barnard William Farey and Bryan Donkin Jr. patented improvements to the basic design; Donkin had worked with Bishopp on his original engine, while Farey was an employee of Donkins.\n\nBishopp's engine met with some scepticism from the trade press when it was launched on the market. But Bishopp had opted to revert to the Dakeynes' original design which had a yoke which took most of the dynamic forces and greatly reduced the load on the bearings and seals. In the event that there was any leakage, the seals were adjustable. In addition, Bishopp had his engines produced by companies with recognised engineering capabilities rather than carrying out his own manufacturing; as well as Donkin's, some of his first engines were built by Joseph Whitworth & Co of Manchester. Another engineering company with a very good reputation was G. Rennie and Son of London who were so convinced of the engine's potential that in 1849 they employed Bishopp as their foreman of works with specific responsibility for the disc engine.\n\nBy 1849 a number of Bishopp engines had been sold, and one was used with great success to run the printing presses of the \"Times\" newspaper, while another produced by G. Rennie and Son was used to power the iron gunboat HMS \"Minx\". The \"Times\" engine had been built by Whitworth and had been shown at the Great Exhibition of 1851 where it ran smoothly and quietly and impressed all who saw it.\n\nIn 1853 a disc engine 13 inches in diameter was purchased from Rennie to propel a 55 foot Russian gunboat, which it did at a speed of .\n\nAt the time the advantages of the disc engine were listed in 1855 by \"The Mechanics' Magazine\" as:\n\nDisc engines ultimately fell into disuse due to the competition offered by modern high speed steam engines which were small and light and could offer features such as compounding. Additionally, conventional engines did not require the same precision manufacture as disc engines and steam leakage was not a problem.\n\nThe nutating disc meter which uses the same geometry and concept as the Dakeynes' original engine is probably the most widely used flowmeter in the world, and it is claimed that more than half the water meters installed in domestic premises in the US and Europe are of this type. Used for 150 years, it is essentially a Dakeyne Disc Engine and was most probably developed by Farey and Donkin who mentioned a \"fluid measurement meter\" in their 1850 disc engine patent granted in 1850. By 1859 they were being manufactured by the Buffalo Meter Company of Buffalo, New York.\n\n\n\n\n"}
{"id": "41615840", "url": "https://en.wikipedia.org/wiki?curid=41615840", "title": "Paymentwall", "text": "Paymentwall\n\nPaymentwall Inc. is a San Francisco-based international payments platform that allows both private individuals and businesses to sell goods and services over the Internet.\n\nHonor Gunday and Vladimir Kovalyov started Paymentwall in April 2010 to help game companies on Facebook monetize their global traffic. On July 1, 2011, Facebook announced that they would no longer allow any outside monetization providers, at which point the company pivoted to providing services to non-Facebook games and online dating websites.\n\nIn 2011, the company entered into partnership with Alipay, a third-party mobile and online payment platform, established in Hangzhou, China in February 2004 by Alibaba Group.\n\nPaymentwall launched a payment system for Smart TVs in November 2014.\n\nPaymentwall is headquartered in San Francisco, with offices in Istanbul, Berlin, Kyiv, Manila, Amsterdam, Las Vegas, Hanoi, Beijing, London, Lisbon, Seoul, Sofia, Poznan, Moscow and Novosibirsk. \n\nIn November 2015, the company unveiled its intergalactic-themed development lab in Kyiv.\n"}
{"id": "46843228", "url": "https://en.wikipedia.org/wiki?curid=46843228", "title": "Pharmacyclics", "text": "Pharmacyclics\n\nPharmacyclics LLC is an American biopharmaceutical company based in Sunnyvale, California. It is primarily focused on the development of cancer therapies. In 2017, Xynomic Pharmaceuticals acquired all global rights to Abexinostat, Pharmacyclics' primary commercial product.\n\nIn March 2015, Chicago-based biopharmaceutical firm AbbVie announced it would acquire Pharmacyclics, as well as its lead anti-cancer compound ibrutinib (Imbruvica) for $21 billion. As part of the deal, AbbVie will pay $261.25 per share in a mixture of both cash and AbbVie equity. The merger is expected to close in mid-2015.\n\nDuggan will receive over $3.55 billion from the sale of Pharmacyclics to AbbVie in \"one of the biggest paydays ever from the buyout of a publicly held company.\"\n\nAs CEO and Chairman of Pharmacyclics since 2008, Robert Duggan has opted not to receive compensation from the company.\n"}
{"id": "56625300", "url": "https://en.wikipedia.org/wiki?curid=56625300", "title": "S. George Bankoff", "text": "S. George Bankoff\n\nSeymour George Bankoff (October 7, 1921 – July 14, 2011) was an American chemical engineer.\n\nBankoff was born on October 7, 1921 and raised in Brooklyn. He received bachelor's and master's degrees in mineral dressing at Columbia University. Bankoff then worked for the Manhattan Project between stints at DuPont. In 1948, he began teaching at Rose Polytechnic Institute and concurrently earned a Ph.D from Purdue University. Bankoff joined the Northwestern University faculty in 1959, where he was appointed the Walter P. Murphy Professor of Chemical and Mechanical Engineering. In 1966, Bankoff was named a Guggenheim fellow. Over the course of his career, Bankoff was also granted fellowship in the American Institute of Chemical Engineers and the American Society of Mechanical Engineers, as well as a membership in the National Academy of Engineering. He died on July 14, 2011 at Evanston Hospital, aged 89.\n"}
{"id": "337004", "url": "https://en.wikipedia.org/wiki?curid=337004", "title": "Shell (projectile)", "text": "Shell (projectile)\n\nA shell is a payload-carrying projectile that, as opposed to shot, contains an explosive or other filling, though modern usage sometimes includes large solid projectiles properly termed shot. Solid shot may contain a pyrotechnic compound if a tracer or spotting charge is used. Originally, it was called a \"bombshell\", but \"shell\" has come to be unambiguous in a military context.\n\nAll explosive- and incendiary-filled projectiles, particularly for mortars, were originally called \"grenades\", derived from the pomegranate, so called because the many-seeded fruit suggested the powder-filled, fragmenting bomb, or from the similarity of shape. Words cognate with \"grenade\" are still used for an artillery or mortar projectile in some European languages.\n\nShells are usually large-caliber projectiles fired by artillery, combat vehicles (including tanks), and warships.\n\nShells usually have the shape of a cylinder topped by an ogive-shaped nose for good aerodynamic performance, possibly with a tapering base (boat-tail); but some specialized types are quite different.\n\nSolid cannonballs (\"shot\") did not need a fuse, but hollow munitions (\"shells\") filled with something such as gunpowder to fragment the ball, needed a fuse, either impact (percussion) or time. Percussion fuses with a spherical projectile presented a challenge because there was no way of ensuring that the impact mechanism contacted the target. Therefore, shells needed a time fuse that was ignited before or during firing and burned until the shell reached its target.\n\nThe earliest record of shells being used in combat was by the Republic of Venice at Jadra in 1376. Shells with fuses were used at the 1421 siege of St Boniface in Corsica. These were two hollowed hemispheres of stone or bronze held together by an iron hoop.\n\nWritten evidence for early explosive shells in China appears in the early Ming Dynasty (1368–1644) Chinese military manual \"Huolongjing\", compiled by Jiao Yu (fl. 14th to early 15th century) and Liu Bowen (1311–1375) sometime before the latter's death, a preface added by Jiao in 1412. As described in their book, these hollow, gunpowder-packed shells were made of cast iron. At least since the 16th Century grenades made of ceramics or glass were in use in Central Europe. A hoard of several hundred ceramic grenades were discovered during building works in front of a bastion of the Bavarian City of Ingolstadt, Germany dated to the 17th Century. Lots of the grenades contained their original blackpowder loads and igniters. Most probably the grenades were intentionally dumped in the moat of the bastion before the year 1723.\n\nAn early problem was that there was no means of precisely measuring the time to detonation — reliable fuses did not yet exist and the burning time of the powder fuse was subject to considerable trial and error. Early powder burning fuses had to be loaded fuse down to be ignited by firing or a portfire put down the barrel to light the fuse. Other shells were wrapped in bitumen cloth, which would ignite during the firing and in turn ignite a powder fuse. Nevertheless, shells came into regular use in the 16th Century, for example a 1543 English mortar shell was filled with 'wildfire'.\nBy the 18th Century, it was known that the fuse toward the muzzle could be lit by the flash through the windage between the shell and the barrel. At about this time, shells began to be employed for horizontal fire from howitzers with a small propelling charge and, in 1779, experiments demonstrated that they could be used from guns with heavier charges.\n\nThe use of exploding shells from field artillery became relatively commonplace from early in the 19th century. Until the mid 19th century, shells remained as simple exploding spheres that used gunpowder, set off by a slow burning fuse. They were usually made of cast iron, but bronze, lead, brass and even glass shell casings were experimented with. The word \"bomb\" encompassed them at the time, as heard in the lyrics of \"The Star-Spangled Banner\" (\"the bombs bursting in air\"), although today that sense of \"bomb\" is obsolete. Typically, the thickness of the metal body was about a sixth of their diameter and they were about two thirds the weight of solid shot of the same caliber.\n\nTo ensure that shells were loaded with their fuses toward the muzzle, they were attached to wooden bottoms called \"sabots\". In 1819, a committee of British artillery officers recognized that they were essential stores and in 1830 Britain standardized sabot thickness as a half inch. The sabot was also intended to reduce jamming during loading. Despite the use of exploding shell, the use of smoothbore cannons firing spherical projectiles of shot remained the dominant artillery method until the 1850s.\n\nThe mid 19th century saw a revolution in artillery, with the introduction of the first practical rifled breech loading weapons. The new methods resulted in the reshaping of the spherical shell into its modern recognizable cylindro-conoidal form. This shape greatly improved the in-flight stability of the projectile and meant that the primitive time fuzes could be replaced with the percussion fuze situated in the nose of the shell. The new shape also meant that further, armor-piercing designs could be used.\n\nDuring the 20th Century, shells became increasingly streamlined. In World War I, ogives were typically two circular radius head (crh) - the curve was a segment of a circle having a radius of twice the shell caliber. After that war, ogive shapes became more complex and elongated. From the 1960s, higher quality steels were introduced by some countries for their HE shells, this enabled thinner shell walls with less weight of metal and hence a greater weight of explosive. Ogives were further elongated to improve their ballistic performance.\n\nAdvances in metallurgy in the industrial era allowed for the construction of rifled breech-loading guns that could fire at a much greater muzzle velocity. After the British artillery was shown up in the Crimean War as having barely changed since the Napoleonic Wars, the industrialist William Armstrong was awarded a contract by the government to design a new piece of artillery. Production started in 1855 at the Elswick Ordnance Company and the Royal Arsenal at Woolwich.\n\nThe piece was rifled, which allowed for a much more accurate and powerful action. Although rifling had been tried on small arms since the 15th century, the necessary machinery to accurately rifle artillery only became available in the mid-19th century. Martin von Wahrendorff and Joseph Whitworth independently produced rifled cannon in the 1840s, but it was Armstrong's gun that was first to see widespread use during the Crimean War. The cast iron shell of the Armstrong gun was similar in shape to a Minié ball and had a thin lead coating which made it fractionally larger than the gun's bore and which engaged with the gun's rifling grooves to impart spin to the shell. This spin, together with the elimination of windage as a result of the tight fit, enabled the gun to achieve greater range and accuracy than existing smooth-bore muzzle-loaders with a smaller powder charge.\n\nThe gun was also a breech-loader. Although attempts at breech-loading mechanisms had been made since medieval times, the essential engineering problem was that the mechanism couldn't withstand the explosive charge. It was only with the advances in metallurgy and precision engineering capabilities during the Industrial Revolution that Armstrong was able to construct a viable solution. Another innovative feature was what Armstrong called its \"grip\", which was essentially a squeeze bore; the 6 inches of the bore at the muzzle end was of slightly smaller diameter, which centered the shell before it left the barrel and at the same time slightly swaged down its lead coating, reducing its diameter and slightly improving its ballistic qualities.\n\nRifled guns were also developed elsewhere - by Major Giovanni Cavalli and Baron Martin von Wahrendorff in Sweden, Krupp in Germany and the Wiard gun in the United States. However, rifled barrels required some means of engaging the shell with the rifling. Lead coated shells were used with the Armstrong gun, but were not satisfactory so studded projectiles were adopted. However, these did not seal the gap between shell and barrel. Wads at the shell base were also tried without success.\n\nIn 1878, the British adopted a copper 'gas-check' at the base of their studded projectiles and in 1879 tried a rotating gas check to replace the studs, leading to the 1881 automatic gas-check. This was soon followed by the Vavaseur copper driving band as part of the projectile. The driving band rotated the projectile, centered it in the bore and prevented gas escaping forwards. A driving band has to be soft but tough enough to prevent stripping by rotational and engraving stresses. Copper is generally most suitable but cupronickel or gilding metal were also used.\n\nAlthough an early percussion fuze appeared in 1650 that used a flint to create sparks to ignite the powder, the shell had to fall in a particular way for this to work and this did not work with spherical projectiles. An additional problem was finding a suitably stable ‘percussion powder’. Progress was not possible until the discovery of mercury fulminate in 1800, leading to priming mixtures for small arms patented by the Rev Alexander Forsyth, and the copper percussion cap in 1818.\n\nThe percussion fuze was adopted by Britain in 1842. Many designs were jointly examined by the army and navy, but were unsatisfactory, probably because of the safety and arming features. However, in 1846 the design by Quartermaster Freeburn of the Royal Artillery was adopted by the army. It was a wooden fuze some 6 inches long and used shear wire to hold blocks between the fuze magazine and a burning match. The match was ignited by propellant flash and the shear wire broke on impact. A British naval percussion fuze made of metal did not appear until 1861.\n\n\nGunpowder was used as the only form of explosive up until the end of the 19th century. Guns using black powder ammunition would have their view obscured by a huge cloud of smoke and concealed shooters were given away by a cloud of smoke over the firing position. Guncotton, a nitrocellulose-based material, was discovered by Swiss chemist Christian Friedrich Schönbein in 1846. He promoted its use as a blasting explosive and sold manufacturing rights to the Austrian Empire. Guncotton was more powerful than gunpowder, but at the same time was somewhat more unstable. John Taylor obtained an English patent for guncotton; and John Hall & Sons began manufacture in Faversham. British interest waned after an explosion destroyed the Faversham factory in 1847. Austrian Baron Wilhelm Lenk von Wolfsberg built two guncotton plants producing artillery propellant, but it was dangerous under field conditions, and guns that could fire thousands of rounds using gunpowder would reach their service life after only a few hundred shots with the more powerful guncotton.\n\nSmall arms could not withstand the pressures generated by guncotton. After one of the Austrian factories blew up in 1862, Thomas Prentice & Company began manufacturing guncotton in Stowmarket in 1863; and British War Office chemist Sir Frederick Abel began thorough research at Waltham Abbey Royal Gunpowder Mills leading to a manufacturing process that eliminated the impurities in nitrocellulose making it safer to produce and a stable product safer to handle. Abel patented this process in 1865, when the second Austrian guncotton factory exploded. After the Stowmarket factory exploded in 1871, Waltham Abbey began production of guncotton for torpedo and mine warheads.\nIn 1884, Paul Vieille invented a smokeless powder called Poudre B (short for \"poudre blanche\"—white powder, as distinguished from black powder) made from 68.2% insoluble nitrocellulose, 29.8% soluble nitrocellusose gelatinized with ether and 2% paraffin. This was adopted for the Lebel rifle. Vieille's powder revolutionized the effectiveness of small guns, because it gave off almost no smoke and was three times more powerful than black powder. Higher muzzle velocity meant a flatter trajectory and less wind drift and bullet drop, making 1000 meter shots practicable. Other European countries swiftly followed and started using their own versions of Poudre B, the first being Germany and Austria which introduced new weapons in 1888. Subsequently, Poudre B was modified several times with various compounds being added and removed. Krupp began adding diphenylamine as a stabilizer in 1888.\n\nBritain conducted trials on all the various types of propellant brought to their attention, but were dissatisfied with them all and sought something superior to all existing types. In 1889, Sir Frederick Abel, James Dewar and Dr W Kellner patented (Nos 5614 and 11,664 in the names of Abel and Dewar) a new formulation that was manufactured at the Royal Gunpowder Factory at Waltham Abbey. It entered British service in 1891 as Cordite Mark 1. Its main composition was 58% Nitro-glycerine, 37% Guncotton and 3% mineral jelly. A modified version, Cordite MD, entered service in 1901, this increased guncotton to 65% and reduced nitro-glycerine to 30%, this change reduced the combustion temperature and hence erosion and barrel wear. Cordite could be made to burn slower which reduced maximum pressure in the chamber (hence lighter breeches, etc.), but longer high pressure, significant improvements over gunpowder. Cordite could be made in any desired shape or size. The creation of cordite led to a lengthy court battle between Nobel, Maxim, and another inventor over alleged British patent infringement.\n\nA variety of fillings have been used in shells throughout history. An incendiary shell was invented by Valturio in 1460. The carcass shell was first used by the French under Louis XIV in 1672. Initially in the shape of an oblong in an iron frame (with poor ballistic properties) it evolved into a spherical shell. Their use continued well into the 19th century.\n\nA modern version of the incendiary shell was developed in 1857 by the British and was known as \"Martin's shell\" after its inventor. The shell was filled with molten iron and was intended to break up on impact with an enemy ship, splashing molten iron on the target. It was used by the Royal Navy between 1860 and 1869, replacing Heated shot as an anti-ship, incendiary projectile.\n\nTwo patterns of incendiary shell were used by the British in World War 1, one designed for use against Zeppelins.\n\nSimilar to incendiary shells were star shells, designed for illumination rather than arson. Sometimes called lightballs they were in use from the 17th Century onwards. The British adopted parachute lightballs in 1866 for 10, 8 and 5 inch calibers. The 10-inch wasn't officially declared obsolete until 1920.\n\nSmoke balls also date back to the 17th Century, British ones contained a mix of saltpetre, coal, pitch, tar, resin, sawdust, crude antimony and sulphur. They produced a 'noisome smoke in abundance that is impossible to bear'. In 19th century British service, they were made of concentric paper with a thickness about 1/15th of the total diameter and filled with powder, saltpeter, pitch, coal and tallow. They were used to 'suffocate or expel the enemy in casemates, mines or between decks; for concealing operations; and as signals.\n\nDuring the First World War, shrapnel shells and explosive shells inflicted terrible casualties on infantry, accounting for nearly 70% of all war casualties and leading to the adoption of steel helmets on both sides. Shells filled with poison gas were used from 1917 onwards. Frequent problems with shells led to many military disasters when shells failed to explode, most notably during the 1916 Battle of the Somme.\n\nArtillery shells are differentiated by how the shell is loaded, propelled and the type of breech mechanism:\n\nWith this style of ammunition, there are three main components the fuzed projectile, the casing to hold the propellants and primer, and the single propellant charge. With a fixed round everything is included in one ready to use package and in British ordnance, terms are called fixed quick firing. Often guns which use fixed ammunition use sliding-block or sliding-wedge breeches and the case provides obturation which seals the breech of the gun and prevents propellant gasses from escaping. Sliding block breeches can be horizontal or vertical. Advantages of fixed ammunition are simplicity, safety, moisture resistance and speed of loading. Disadvantages are eventually a fixed round becomes too long or too heavy to load by a gun crew. Another issue is the inability to vary propellant charges to achieve different velocities and ranges. Lastly, there's the issue of resource usage since a fixed round uses a case, which can be an issue in a prolonged war if there are metal shortages.\n\nWith this style of ammunition there are three main components: The fuzed projectile, the casing to hold the propellants and primer, and the bagged propellant charges. With a separate loading cased charge round the casing, bagged propellant charges and projectile are usually separated into two or more parts. In British ordnance terms, this type of ammunition is called separate quick firing. Often guns which use separate loading cased charge ammunition use sliding-block or sliding-wedge breeches and during World War I and World War II Germany predominantly used fixed or separate loading cased charges and sliding block breeches even for their largest guns. A variant of separate loading cased charge ammunition is semi-fixed ammunition. With semi-fixed ammunition the round comes as a complete package but the projectile and its case can be separated. The case holds a set number of bagged charges and the gun crew can add or subtract propellant to change range and velocity. The round is then reassembled and fired. Advantages include easier handling for large rounds, while range and velocity can be varied by using more or fewer propellant charges. Disadvantages include more complexity, slower loading, less safety, less moisture resistance and the metal cases can still be a resource issue.\n\nWith this style of ammunition there are three main components the fuzed projectile, the bagged charges and the primer. Like separate loading cased charge ammunition, the number of propellant charges can be varied. However, this style of ammunition does not use a cartridge case and it achieves obturation through a screw breech instead of a sliding block. Sometimes when reading about artillery the term separate loading ammunition will be used without clarification of whether a cartridge case is used or not, in which case refer to the type of breech used. Heavy artillery pieces and Naval artillery tend to use bagged charges and projectiles because the weight and size of the projectiles and propelling charges can be more than a gun crew can manage. Advantages include easier handling for large rounds, decreased metal usage, while range and velocity can be varied by using more or fewer propellant charges. Disadvantages include more complexity, slower loading, less safety and less moisture resistance.\n\nExtended range shells are sometimes used. These special shell designs may be Rocket Assisted Projectiles (RAP) or base bleed to increase range. The first has a small rocket motor built into its base to provide additional thrust. The second has a pyrotechnic device in its base that bleeds gas to fill the partial vacuum created behind the shell and hence reduce base-drag. These shell designs usually have reduced HE filling to remain within the permitted weight for the projectile, and hence less lethality.\n\nThe caliber of a shell is its diameter. Depending on the historical period and national preferences, this may be specified in millimeters, centimeters, or inches. The length of gun barrels for large cartridges and shells (naval) is frequently quoted in terms of the ratio of the barrel length to the bore size, also called caliber. For example, the 16\"/50 caliber Mark 7 gun is 50 calibers long, that is, 16\"×50=800\"=66.7 feet long. Some guns, mainly British, were specified by the weight of their shells (see below).\n\nExplosive rounds as small as 12.7 x 82 and 13 x 64 millimeter have been used on aircraft and armored vehicles, but their small explosive yield have led some nations to limit their explosive rounds to 20 mm or larger. International Law precludes the use of explosive ammunition for use against individual persons, but not against vehicles and aircraft. The largest shells ever fired were those from the German super-railway guns, Gustav and Dora, which were 800 mm (31.5 in) in caliber. Very large shells have been replaced by rockets, missiles, and bombs, and today the largest shells in common use are 155 mm (6.1 in).\n\nGun calibers have standardized around a few common sizes, especially in the larger range, mainly due to the uniformity required for efficient military logistics. Shells of 105 and 155 mm for artillery and 105mm and 120 mm for tank guns in NATO. Artillery shells of 122, 130 and 152 mm, and tank gun ammunition of 100, 115, or 125 mm caliber remain in use in Eastern Europe, Western Asia, Northern Africa, and Eastern Asia. Most common calibers have been in use for many years, since it is logistically complex to change the caliber of all guns and ammunition stores.\n\nThe weight of shells increases by and large with caliber. A typical 155 mm (6.1 in) shell weighs about 50 kg, a common 203 mm (8 in) shell about 100 kg, a concrete demolition 203 mm (8 in) shell 146 kg, a 280 mm (11 in) battleship shell about 300 kg, and a 460 mm (18 in) battleship shell over 1,500 kg. The Schwerer Gustav supergun fired 4.8 and 7.1 tonne shells.\n\nDuring the 19th century, the British adopted a particular form of designating artillery. Field guns were designated by nominal standard projectile weight, while howitzers were designated by barrel caliber. British guns and their ammunition were designated in pounds, e.g., as \"two-pounder\" shortened to \"2-pr\" or \"2-pdr\". Usually, this referred to the actual weight of the standard projectile (shot, shrapnel or HE), but, confusingly, this was not always the case.\n\nSome were named after the weights of obsolete projectile types of the same caliber, or even obsolete types that were considered to have been functionally equivalent. Also, projectiles fired from the same gun, but of non-standard weight, took their name from the gun. Thus, conversion from \"pounds\" to an actual barrel diameter requires consulting a historical reference. A mixture of designations were in use for land artillery from the First World War (such as the BL 60-pounder gun, RML 2.5 inch Mountain Gun, 4 inch gun, 4.5 inch howitzer) through to the end of World War II (5.5 inch medium gun, 25-pounder gun-howitzer, 17-pounder tank gun), but the majority of naval guns were by caliber. After World War II, guns were designated by caliber.\n\nThere are many different types of shells. The principal ones include:\n\nWith the introduction of the first ironclads in the 1850s and 1860s, it became clear that shells had to be designed to effectively pierce the ship armor. A series of British tests in 1863 demonstrated that the way forward lay with high-velocity lighter shells. The first pointed armor-piercing shell was introduced by Major Palliser in 1863. Approved in 1867, Palliser shot and shell was an improvement over the ordinary elongated shot of the time. Palliser shot was made of cast iron, the head being chilled in casting to harden it, using composite molds with a metal, water cooled portion for the head.\n\nBritain also deployed Palliser shells in the 1870s-1880s. In the shell, the cavity was slightly larger than in the shot and was filled with 1.5% gunpowder instead of being empty, to provide a small explosive effect after penetrating armor plating. The shell was correspondingly slightly longer than the shot to compensate for the lighter cavity. The powder filling was ignited by the shock of impact and hence did not require a fuze. However, ship armor rapidly improved during the 1880s and 1890s, and it was realised that explosive shells with steel had advantages including better fragmentation and resistance to the stresses of firing. These were cast and forged steel.\n\nAP shells containing an explosive filling were initially distinguished from their non-HE counterparts by being called a \"shell\" as opposed to \"shot\". By the time of the Second World War, AP shells with a bursting charge were sometimes distinguished by appending the suffix \"HE\". At the beginning of the war, APHE was common in anti-tank shells of 75 mm caliber and larger due to the similarity with the much larger naval armor piercing shells already in common use. As the war progressed, ordnance design evolved so that the bursting charges in APHE became ever smaller to non-existent, especially in smaller caliber shells, e.g. Panzergranate 39 with only 0.2% HE filling.\n\n\nAlthough smokeless powders were used as a propellant, they could not be used as the substance for the explosive warhead, because shock sensitivity sometimes caused detonation in the artillery barrel at the time of firing. Picric acid was the first high-explosive nitrated organic compound widely considered suitable to withstand the shock of firing in conventional artillery. In 1885, based on research of Hermann Sprengel, French chemist Eugène Turpin patented the use of pressed and cast picric acid in blasting charges and artillery shells. In 1887, the French government adopted a mixture of picric acid and guncotton under the name \"Melinite\". In 1888, Britain started manufacturing a very similar mixture in Lydd, Kent, under the name \"Lyddite\".\n\nJapan followed with an \"improved\" formula known as \"shimose powder\". In 1889, a similar material, a mixture of ammonium cresylate with trinitrocresol, or an ammonium salt of trinitrocresol, started to be manufactured under the name \"ecrasite\" in Austria-Hungary. By 1894, Russia was manufacturing artillery shells filled with picric acid. Ammonium picrate (known as \"Dunnite\" or explosive D) was used by the United States beginning in 1906. Germany began filling artillery shells with TNT in 1902. Toluene was less readily available than phenol, and TNT is less powerful than picric acid, but the improved safety of munitions manufacturing and storage caused the replacement of picric acid by TNT for most military purposes between the World Wars. However, pure TNT was expensive to produce and most nations made some use of mixtures using cruder TNT and ammonium nitrate, some with other compounds included. These fills included Ammonal, Schneiderite and Amatol. The latter was still in wide use in World War II.\n\nThe percentage of shell weight taken up by its explosive fill increased steadily throughout the 20th Century. Less than 10% was usual in the first few decades; by World War II, leading designs were around 15%. However, British researchers in that war identified 25% as being the optimal design for anti-personnel purposes, based on the recognition that far smaller fragments than hitherto would give a better effect. This guideline was achieved by the 1960s with the 155 mm L15 shell, developed as part of the German-British FH-70 program. The key requirement for increasing the HE content without increasing shell weight was to reduce the thickness of shell walls, which required improvements in high tensile steel.\nThe most common shell type is high explosive, commonly referred to simply as HE. They have a strong steel case, a bursting charge, and a fuse. The fuse detonates the bursting charge which shatters the case and scatters hot, sharp case pieces (\"fragments\", \"splinters\") at high velocity. Most of the damage to soft targets, such as unprotected personnel, is caused by shell pieces rather than by the blast. The term \"shrapnel\" is sometimes used to describe the shell pieces, but shrapnel shells functioned very differently and are long obsolete. The speed of fragments is limited by Gurney equations. Depending on the type of fuse used the HE shell can be set to burst on the ground (percussion), in the air above the ground, which is called called air burst (time or proximity), or after penetrating a short distance into the ground (percussion with delay, either to transmit more ground shock to covered positions, or to reduce the spread of fragments). Projectiles with enhanced fragmentation are called high-explosive fragmentation (HE-FRAG).\n\nRDX and TNT mixtures are the standard chemicals used, notably Composition B and Cyclotol. The introduction of 'insensitive munition' requirements, agreements and regulations in the 1990s caused modern western designs to use various types of plastic bonded explosives (PBX) based on RDX.\n\nCommon shells designated in the early (i.e. 1800s) British explosive shells were filled with \"low explosives\" such as \"P mixture\" (gunpowder) and usually with a fuze in the nose. Common shells on bursting (non-detonating) tended to break into relatively large fragments which continued along the shell's trajectory rather than laterally. They had some incendiary effect.\n\nIn the late 19th century \"double common shells\" were developed, lengthened so as to approach twice the standard shell weight, to carry more powder and hence increase explosive effect. They suffered from instability in flight and low velocity and were not widely used.\n\nAs at 1914, common shells 6 inch and up were of cast steel, smaller shells were of forged steel for service and cast iron for practice. They were replaced by \"common lyddite\" shells in the late 1890s but some stocks remained as late as 1914. In British service common shells were typically painted black with a red band behind the nose to indicate the shell was filled.\n\nCommon pointed shells, or CP were a type of common shell used in naval service from the 1890s - 1910s which had a solid nose and a percussion fuze in the base rather than the common shell's nose fuze. The ogival two C.R.H. solid pointed nose was considered suitable for attacking shipping but was not armor-piercing - the main function was still explosive. They were of cast or forged (three- and six-pounder) steel and contained a gunpowder bursting charge slightly smaller than that of a common shell, a trade off for the longer heavier nose.\n\nIn British service common pointed shells were typically painted black, except 12-pounder shells specific for QF guns which were painted lead colour to distinguish them from 12-pounder shells usable with both BL and QF guns. A red ring behind the nose indicated the shell was filled.\n\nBy World War II they were superseded in Royal Navy service by common pointed capped (CPC) and semi-armor piercing (SAP), filled with TNT.\n\nCommon lyddite shells were British explosive shells filled with Lyddite were initially designated \"common lyddite\" and beginning in 1896 were the first British generation of modern \"high explosive\" shells. Lyddite is picric acid fused at 280 °F and allowed to solidify, producing a much denser dark-yellow form which is not affected by moisture and is easier to detonate than the liquid form. Its French equivalent was \"melinite\", Japanese equivalent was \"shimose\". Common lyddite shells \"detonated\" and fragmented into small pieces in all directions, with no incendiary effect. For maximum destructive effect the explosion needed to be delayed until the shell had penetrated its target.\n\nEarly shells had walls of the same thickness for the whole length, later shells had walls thicker at the base and thinning towards the nose. This was found to give greater strength and provide more space for explosive. Later shells had 4 c.r. heads, more pointed and hence streamlined than earlier 2 c.r.h. designs.\n\nProper detonation of a lyddite shell would show black to grey smoke, or white from the steam of a water detonation. Yellow smoke indicated simple explosion rather than detonation, and failure to reliably detonate was a problem with lyddite, especially in its earlier usage. To improve the detonation \"exploders\" with a small quantity of picric powder or even of TNT (in smaller shells, 3 pdr, 12 pdr - 4.7 inch) was loaded between the fuze and the main lyddite filling or in a thin tube running through most of the shell's length.\n\nLyddite presented a major safety problem because it reacted dangerously with metal bases. This required that the interior of shells had to be varnished, the exterior had to be painted with leadless paint and the fuze-hole had to be made of a leadless alloy. Fuzes containing any lead could not be used with it.\n\nWhen World War I began Britain was replacing lyddite with modern \"high explosive\" (HE) such as TNT. After World War I the term \"common lyddite\" was dropped, and remaining stocks of lyddite-filled shells were referred to as HE (high explosive) shell filled lyddite. Hence \"common\" faded from use, replaced by \"HE\" as the explosive shell designation.\n\nCommon lyddite shells in British service were painted yellow, with a red ring behind the nose to indicate the shell had been filled.\n\nThe mine shell is a particular form of HE shell developed for use in small caliber weapons such as 20 mm to 30 mm cannon. Small HE shells of conventional design can contain only a limited amount of explosive. By using a thin-walled steel casing of high tensile strength, a larger explosive charge can be used. Most commonly the explosive charge also was a more expensive but higher-detonation-energy type.\n\nThe \"mine shell\" concept was invented by the Germans in the Second World War primarily for use in aircraft guns intended to be fired at opposing aircraft. Mine shells produced relatively little damage due to fragments, but a much more powerful blast. The aluminium structures and skins of Second World War aircraft were readily damaged by this greater level of blast.\n\nShrapnel shells are an anti-personnel munition which delivered large numbers of bullets at ranges far greater than rifles or machine guns could attain - up to 6,500 yards by 1914. A typical shrapnel shell as used in World War I was streamlined, 75 mm (3 in) in diameter and contained approximately 300 lead–antimony balls (bullets), each around 1/2 inch in diameter. Shrapnel used the principle that the bullets encountered much less air resistance if they travelled most of their journey packed together in a single streamlined shell than they would if they travelled individually, and could hence attain a far greater range.\n\nThe gunner set the shell's time fuze so that it was timed to burst as it was angling down towards the ground just before it reached its target (ideally about 150 yards before, and 60–100 feet above the ground). The fuze then ignited a small \"bursting charge\" in the base of the shell which fired the balls forward out of the front of the shell case, adding 200–250 ft/second to the existing velocity of 750–1200 ft/second. The shell body dropped to the ground mostly intact and the bullets continued in an expanding cone shape before striking the ground over an area approximately 250 yards × 30 yards in the case of the US 3 inch shell. The effect was of a large shotgun blast just in front of and above the target, and was deadly against troops in the open. A trained gun team could fire 20 such shells per minute, with a total of 6,000 balls, which compared very favorably with rifles and machine-guns.\n\nHowever, shrapnel's relatively flat trajectory (it depended mainly on the shell's velocity for its lethality, and was lethal only in the forward direction) meant that it could not strike trained troops who avoided open spaces and instead used dead ground (dips), shelters, trenches, buildings, and trees for cover. It was of no use in destroying buildings or shelters. Hence, it was replaced during World War I by the high-explosive shell, which exploded its fragments in all directions (and thus more difficult to avoid) and could be fired by high-angle weapons, such as howitzers.\n\nCluster shells are a type of carrier shell or cargo munition. Like cluster bombs, an artillery shell may be used to scatter smaller sub-munitions, including anti-personnel grenades, anti-tank top-attack munitions, and landmines. These are generally far more lethal against both armor and infantry than simple high-explosive shells, since the multiple munitions create a larger kill zone and increase the chance of achieving the direct hit necessary to kill armor. Most modern armies make significant use of cluster munitions in their artillery batteries.\n\nHowever, in operational use, sub-munitions have demonstrated a far higher malfunction rate than previously claimed, including those that have self-destruct mechanisms. This problem, the \"dirty battlefield\", led to the Ottawa Treaty.\n\nArtillery-scattered mines allow for the quick deployment of minefields into the path of the enemy without placing engineering units at risk, but artillery delivery may lead to an irregular and unpredictable minefield with more unexploded ordnance than if mines were individually placed.\n\nSignatories of the Ottawa Treaty have renounced the use of cluster munitions of all types where the carrier contains more than ten sub-munitions.\n\nChemical shells contain just a small explosive charge to burst the shell, and a larger quantity of a chemical agent such as a poison gas. Signatories of the Chemical Weapons Convention have renounced such shells.\n\nNot all shells are designed to kill or destroy. The following types are designed to achieve particular non-lethal effects. They are not completely harmless: smoke and illumination shells can accidentally start fires, and impact by the discarded carrier of all three types can wound or kill personnel, or cause minor damage to property.\n\nThe smoke shell is designed to create a smoke screen. The main types are bursting (those filled with white phosphorus WP and a small HE bursting charge are best known) and base ejection (delivering three or four smoke canisters, or material impregnated with white phosphorus). Base ejection shells are a type of carrier shell or cargo munition.\n\nBase ejection smoke is usually white, however, colored smoke has been used for marking purposes. The original canisters were non-burning, being filled with a compound that created smoke when it reacted with atmospheric moisture, modern ones use red phosphorus because of its multi-spectral properties. However, other compounds have been used; in World War II, Germany used oleum (fuming sulfuric acid) and pumice.\n\nModern illuminating shells are a type of carrier shell or cargo munition. Those used in World War I were shrapnel pattern shells ejecting small burning 'pots'.\n\nA modern illumination shell has a time fuze that ejects a flare 'package' through the base of the carrier shell at a standard height above ground (typically about 600 metres), from where it slowly falls beneath a non-flammable parachute, illuminating the area below. The ejection process also initiates a pyrotechnic flare emitting white or 'black' infrared light.\nTypically illumination flares burn for about 60 seconds. These are also known as \"starshell\" or \"star shell\". Infrared illumination is a more recent development used to enhance the performance of night vision devices. Both white and black light illuminating shells may be used to provide continuous illumination over an area for a period of time, and may use several dispersed aimpoints to illuminate a large area. Alternatively firing single illuminating shells may be coordinated with the adjustment of HE shell fire onto a target.\n\nColored flare shells have also been used for target marking and other signaling purposes.\nThe carrier shell is simply a hollow carrier equipped with a fuze that ejects the contents at a calculated time. They are often filled with propaganda leaflets (see external links), but can be filled with anything that meets the weight restrictions and is able to withstand the shock of firing. Famously, on Christmas Day 1899 during the siege of Ladysmith, the Boers fired into Ladysmith a carrier shell without a fuze, which contained a Christmas pudding, two Union Flags and the message \"compliments of the season\". The shell is still kept in the museum at Ladysmith.\n\nA proof shot is not used in combat but to confirm that a new gun barrel can withstand operational stresses. The proof shot is heavier than a normal shot or shell, and an oversize propelling charge is used, subjecting the barrel to greater than normal stress. The proof shot is inert (no explosive or functioning filling) and is often a solid unit, although water, sand or iron powder filled versions may be used for testing the gun mounting. Although the proof shot resembles a functioning shell (of whatever sort), so that it behaves as a real shell in the barrel, it is not aerodynamic as its job is over once it has left the muzzle of the gun. Consequently, it travels a much shorter distance and is usually stopped by an earth bank for safety measures.\n\nThe gun, operated remotely for safety in case it fails, fires the proof shot, and is then inspected for damage. If the barrel passes the examination, \"proof marks\" are added to the barrel. The gun can be expected to handle normal ammunition, which subjects it to less stress than the proof shot, without being damaged.\n\nGuided or \"smart\" ammunition have been developed in recent years, but have yet to supplant unguided munitions in all applications.\nThe fuze of a shell has to keep the shell safe from accidental functioning during storage, due to (possibly) rough handling, fire, etc. It also has to survive the violent launch through the barrel, then reliably function at the appropriate moment. To do this it has a number of arming mechanisms which are successively enabled under the influence of the firing sequence.\n\nSometimes, one or more of these arming mechanisms fail, resulting in a projectile that is unable to detonate. More worrying (and potentially far more hazardous) are fully armed shells on which the fuze fails to initiate the HE firing. This may be due to a shallow trajectory of fire, low-velocity firing or soft impact conditions. Whatever the reason for failure, such a shell is called a \"blind\" or \"unexploded ordnance (UXO)\" (the older term, \"dud\", is discouraged because it implies that the shell \"cannot\" detonate.) Blind shells often litter old battlefields; depending on the impact velocity, they may be buried some distance into the earth, all the while remaining potentially hazardous. For example, antitank ammunition with a piezoelectric fuze can be detonated by relatively light impact to the piezoelectric element, and others, depending on the type of fuze used, can be detonated by even a small movement. The battlefields of the First World War still claim casualties today from leftover munitions. Modern electrical and mechanical fuzes are highly reliable: if they do not arm correctly, they keep the initiation train out of line or (if electrical in nature) discharge any stored electrical energy.\n\n\n\n"}
{"id": "42844729", "url": "https://en.wikipedia.org/wiki?curid=42844729", "title": "Smart Cell", "text": "Smart Cell\n\nSmart Cells are innovative ubiquitous radio access nodes that provide wireless connectivity across multiple spectrum ranges and technologies. As of January 2014, Macrocells, Small Cells, and Wi-Fi connections were the primary means of data connectivity. For these types of cells, the spectrum utilized is static and is based on the antenna installed. \"A Smart Cell may transmit multiple frequencies and technologies which are controlled by the software and not the hardware\" (antenna).\n\nSmart Cells are currently in the research and development stage but support software-defined networks which are proliferating the current mobile network structure, led by AT&T's adoption. According to John Donovan (AT&T), \"This is the way we will build our network.\".\n\nSmart Cells are expected to lower capital and operational costs due to reduced equipment and manual manipulations needed to modify cell site coverage. \"The term Smart Cell is also used to identify other technologically enhanced cell sites where technology has reduced the need to manually manipulate radio access equipment or add additional carriers at a radio access node.\" Similar to Software-defined networking, Smart Cells may replace the existing static technologies with a dynamic, more efficient solution.\n\n \n"}
{"id": "13636664", "url": "https://en.wikipedia.org/wiki?curid=13636664", "title": "Svea 123", "text": "Svea 123\n\nThe Swedish-made Svea 123 is a small liquid-fuel (naphtha, commonly referred to as \"white gas\" or \"Coleman fuel\") pressurized-burner camping stove that traces its origins to designs first pioneered in the late 19th century.\n\nSvea stoves were first made by C.R. Nybergs Lödlampfabrik, which also manufactured blowtorches as well as other machinery and equipment. Founded by Carl Nyberg, the firm later became one of the largest industries in Sundbyberg, Sweden. In 1922, the business was taken over by Max Sievert, an early associate of Nyberg's, and renamed Sieverts Lödlampfabrik (later known as Sievert AB). The Svea 123, introduced in 1955, is considered to be the first compact backpacking white gas stove and one of the most popular camping stoves ever made. Its distinctive \"roaring\" sound has been likened to that of a jet engine at takeoff. In 1969, the Svea brand was acquired by Optimus, another Swedish manufacturer of portable stoves, which has continued production of the Svea 123 to the present day. Because of its simple design and reputation for dependable performance, even under extreme conditions, the Svea 123 enjoys a devoted following.\n\nThe popularity of portable camping stoves such as the Svea coincided with the increase during the 1950s and 1960s in the awareness of the environmental impact of backpacking, particularly in heavily-traveled areas, and the rise of the leave no trace ethic in the 1970s and 1980s. At the same time, scarcity of fuel in over-used camping areas as well as regulatory requirements (open-fire bans) also contributed to the need for a substitute for open campfires for \"wilderness\" area cooking. Eventually stoves that were lighter in weight than the Svea, as well as those of other designs that were capable of burning a wider variety of fuels (useful when camping in other parts of the world where white gas is difficult to find) knocked it from its perch as one of the most popular backpacking stoves after nearly 50 years of production. However, the rugged and durable Svea 123—often described by long-time users as \"bomb-proof\"—still remains popular and continues in wide use.\n\nMade of solid brass, the Svea 123 weighs about , measures and will burn for over an hour on full tank (about 4 ounces) of fuel. Later models (designated the Svea 123R and also sold as the Optimus Climber) were made with a built-in cleaning needle to keep the burner jet from clogging by pushing soot or other impurities outward; early Sievert models without the self-cleaning needle came with a small wire pricker that is used to clean the burner jet manually by pushing the soot inwards. These older models are distinguishable by their downwardly-angled spindle and control valve, to which the adjusting key is attached. The spindle on a Svea 123R with the self-cleaning needle is at a right angle to the stem. Other differences between older and newer models include the vaporizer on older models, which is smooth, while newer models of both the Svea 123 and the 123R are finned and have a stronger joint configuration at the base. The pressure-relief valve in the filler cap has also been redesigned several times to improve both reliability as well as re-seating of the valve after it has opened. The pentagonal hole in the pressure relief valve is designed as a vent, not a key socket. The valve disassembles easily with a pliers. A brass windscreen attaches directly to the stove, and has built-in pot supports that fold inward for storage. The aluminum lid comes with a detachable handle and can also be used as a small cook-pot.\n\nTo light the stove, the fuel tank must first be pre-heated and pressurized by lighting a small amount of fuel poured into the primer pan or spirit cup (a small well) on top of the tank at the base of the vaporizer (the vertical stem connecting the fuel tank to the burner). Alternatively, the primer pan can be filled directly from the fuel tank by opening the control valve and warming the fuel tank by holding it in one’s hands. This will increase the pressure in the fuel tank and force a small amount of fuel to trickle out of the burner jet and into the primer pan. The control valve must then be closed before lighting the priming fuel so as to allow pressure to build up in the tank when the exterior fuel begins to heat the tank and the fuel within. The tank can also be pressurized by an optional pump that may be attached to the filler cap, but this is generally not necessary except in extreme cold. Fuel from the tank is fed by a cotton wick inside the tank to the base of the vaporizer. The heat and pressure created by the priming flame vaporizes the fuel inside the vaporizer. When the priming flame is nearly burnt out, the control valve is opened by turning the adjusting key. This allows the vaporized fuel to flow under pressure through the burner jet (a small opening at the base of the burner), where it mixes with oxygen and burns with a blue flame. Adjusting the flow of the vaporized fuel that is forced through the burner jet controls the flame size and heat output. The control valve (a spindle) is threaded in the vaporizer's housing, and as it is opened (by turning the adjusting key) it opens like a faucet (counter-clockwise to open and clockwise to close) and the vaporized fuel flows through the burner jet. Closing the spindle closes the fuel supply. A small plate on the top of the burner (a flame spreader) spreads the flame outwards. The heat generated in the burner and vaporizer maintains the internal pressure in the fuel tank.\n\nLike most gasoline stoves, the Svea 123 uses an inverted bell-shaped burner topped with a flame spreader (sometimes called a \"target burner\" or \"plate burner\" design). As the vaporized fuel exits the jet, it shoots upward and strikes the bottom of the flame spreader, where it mixes with air that is drawn into the burner housing by convection and, more importantly at higher flow rates, by Bernoulli effect entrainment. The air/fuel mixture flows around the bottom of the flame spreader where it burns with a blue flame. In this type of burner design, flame efficiency depends on how fast the vaporized fuel strikes the flame spreader and on how well the air and vaporized fuel mixes beneath the flame spreader (i.e., the amount of turbulence in the air/fuel mixing zone). As a result, this design works best at high fuel output levels; efficient combustion is indicated by a blue flame. As the fuel output is decreased (i.e., when the control valve is turned down) the velocity with which the fuel exits the jet is also decreased, and fuel mixing (i.e., the amount of turbulence in the air/fuel mixing zone) is likewise decreased. At very low fuel output levels, the fuel will no longer exit with sufficient velocity to fully strike the flame spreader and combustion will consequently be very inefficient, usually indicated by a yellow flame.\n\nThe cleaning needle on the Svea 123R model has two small toothed cogs that face each other, which cleans the burner jet from both the inside and outside when using the stove. The cleaning needle moves upward and downward when the spindle is turned; when the spindle is fully opened, the needle clears the burner jet's opening. As the spindle is closed, the needle retracts into the burner housing. In this way, any soot that may clog the burner jet is expelled.\n\nBecause the Svea 123 is made of brass and has only one moving part – the control valve (the later Svea 123R model has an additional moving part, the internal self-cleaning needle) – the Svea has a well-established record of reliability and can withstand years of heavy use with only minimal maintenance. Some users have reported operational problems with the self-cleaning needle on the 123R, such as that the stove may not simmer as well as the earlier Sievert models, but reports from years of field use of the Svea on the Appalachian Trail indicate that it has the lowest record of clogging among stoves used on the trail.\n\nSome common but nonrecommended practices can adversely affect the Svea's performance and reliability. For example, when using a wind screen or shield other than the built-in wind screen (such as the flexible aluminum foil windscreens used with stoves made by Mountain Safety Research), care should be taken not to wrap the windscreen too tightly around the stove because this may cause the stove to overheat and the fuel tank to over-pressurize. This in turn will cause the pressure-relief valve in the filler cap to open and the over-pressurized gas vapor to escape, which may catch fire and result in a dangerous \"flareup\" or large fireball. In addition, while the Svea is capable of burning unleaded automobile gasoline, only naphtha or Coleman fuel is recommended: Coleman fuel contains rust inhibitors and is specially refined for use in camping stoves, while automotive fuel contains additives that vaporize when burned and leave gumlike deposits behind that causes clogging. The stove should also not be allowed to run dry because doing so will burn or char the cotton wick inside the fuel tank, which will inhibit the wick's ability to draw fuel to the vaporizing tube.\n\nThe Svea 123 shares a number of design features with several other small portable stoves. \n\nIn the 1930s, Sieverts Lödlampfabrik (maker of the original Svea 123) produced the Campus No. 3 stove. Much like the later-produced Svea, it was a self-pressurizing stove with an integrated windscreen and an aluminum lid that doubled as a cook-pot, but was slightly narrower and taller (80 mm x 150 mm) than the Svea. Because of its small size, the Campus No. 3 was advertised as a \"boon to Hikers, Cyclists, and Travellers generally.\" Also in the 1930s, Optimus introduced the No. 6 stove, which was nearly identical to the Svea 123 in size, weight, capacity, operation and design. Optimus dropped the No. 6 in the 1940s and did not produce a similar model until its acquisition of the Svea line in 1969.\n\nThe Juwel 33 and 34 (made by Gustav Barthel of Dresden which, like Sieverts, was a maker of blowtorches and stoves) is a World War II-era German military field stove of similar size, design and operation to the Svea. The Arara 37, another German-made stove, is similar, as is the Czech-made Meva Type 2140. More recently manufactured stoves with the same design features as the Svea include the Russian-made Примус Туристский ПТ-2 \"Огонёк\" (Primus Tourist PT-2 \"Little Flame\"), the German-made Enders \"Biwak\" (\"Bivouac\") No. 2650, and the Juwel 84, which is essentially a larger and more recent version of the World War II-era Juwel 34. The Lion G102 stove, manufactured by the Jaeil Metal Co., Ltd. of South Korea during the 1990s, is nearly identical to the Svea, but with curved brass pot supports attached to the top of the windscreen instead of the straight metal supports used on the Svea. \nThe Swedish-made Primus 71 and the similar Optimus 80 have a larger fuel tank and are slightly taller and heavier than the Svea. (After Optimus acquired the rights to the Primus name for liquid-fueled stoves in 1962, the Primus 71 and Optimus 80 were identical, except for the name and markings.) However, instead of the integrated windscreen on the Svea, the Primus 71 and Optimus 80 stoves fit inside a sheet-metal case for transport that when opened serves as the stove's windscreen and pot support. The type of fuel used and the method of operation of the Primus 71 is the same as the Svea. The Primus 70 is similar, but with a cylindrical aluminum container instead of a sheet-metal box. The Radius 42, another Swedish-made stove, dates from the 1920s and is slightly smaller than the Primus 71, but is otherwise the same general design.\n\nTaiwanese-made knock-offs of the Svea 123 include the Fire-Lite and Trav-ler 77, and the Pak-Cook 235 marketed in the United States under the Stansport and Texsport names.\n\n\n"}
{"id": "58014258", "url": "https://en.wikipedia.org/wiki?curid=58014258", "title": "WOWCube", "text": "WOWCube\n\nWOWCube is a game console or an electronic puzzle shaped as 2x2x2 Rubik's Cube. Serving as an example of a tangible user interface,\nit consists of eight identical elements working as a whole. It allows launching specially designed games.\n\nThe concept of the puzzle was proposed by Savva Osipov in 2016. The first prototype based on Arduino was developed in 2017. The Russian patent was obtained by Ilya and Savva Osipov in 2017. An early prototype was presented on June 8, 2017 at a scientific conference \"CALL\" in UC Berkeley, and in May 2018, the puzzle was presented to the general public at the Maker Faire exhibition in Santa Clara, California.\n\nWowCube is a cube which consists of eight self-contained modules, which have 24 subdisplays collectively and rotate like elements of Rubik’s Cube. Magnetic connectors ensure continuous data exchange between the self-contained modules and the streamlined gaming process. Several games have been designed for WowCube, including puzzles, brain-teasers, Scrabble, platformers, and mazes. The gaming experience provided by WowCube is based on the mixed reality approach, which combines twisting and shaking the cube in reality with digital actions visualized on 24 squared subdisplays.\n\nThe console combines the properties of physical gadgets, such dynamic twiddling toys (e.g., Fidget spinner), and digital gaming consoles in one unit.\n\nThe unit consists of eight identical sections which exchange data via groups of magnetic contacts. The contacts, along with acting as communicating devices, ensure the stability of the entire structure, which allows the player to observe the gaming process on common faces comprised by displays of four adjacent sections.\n\nEach section contains: \n\nIn the current version, the size of the unit is 3″ x 3″ x 3″ (76х76х76mm) and its weight is 14 oz (400g). The frame is made of an ABS polymer.\n\n\nThe operation system used by the WowCube console is FreeRTOS. The P-code is interpreted by the abstract machine of the Pawn language, which allows executing precompiled game logics both on the console, and in its software emulation. The WowCube SDK contains a software emulator, which allows launching the designed games on a PC without an actual console. All components of the WowCube software are used under open licenses, such as MIT License, BSD Licenses, and Apache License.\n\nThe WowCube SDK includes a software emulator which allows developing software programs with no actual WowCube console available. Currently, the supported operating systems are Windows and MacOS. The emulator is based on the following free open-source software\n\n\n"}
{"id": "1789206", "url": "https://en.wikipedia.org/wiki?curid=1789206", "title": "Wattmeter", "text": "Wattmeter\n\nThe wattmeter is an instrument for measuring the electric power (or the supply rate of electrical energy) in watts of any given circuit. Electromagnetic wattmeters are used for measurement of utility frequency and audio frequency power; other types are required for radio frequency measurements.\n\nThe traditional analog wattmeter is an electrodynamic instrument. The device consists of a pair of fixed coils, known as \"current coils\", and a movable coil known as the \"potential coil\".\n\nThe current coils are connected in series with the circuit, while the potential coil is connected in parallel. Also, on analog wattmeters, the potential coil carries a needle that moves over a scale to indicate the measurement. A current flowing through the current coil generates an electromagnetic field around the coil. The strength of this field is proportional to the line current and in phase with it. The potential coil has, as a general rule, a high-value resistor connected in series with it to reduce the current that flows through it.\n\nThe result of this arrangement is that on a DC circuit, the deflection of the needle is proportional to \"both\" the current (\"I\") \"and\" the voltage (\"V\"), thus conforming to the equation \"P\"=\"VI\". \n\nFor AC power, current and voltage may not be in phase, owing to the delaying effects of circuit inductance or capacitance. On an AC circuit the deflection is proportional to the average instantaneous product of voltage and current, thus measuring true power, \"P\"=\"VI\" cos \"φ\". Here, cos\"φ\" represents the power factor which shows that the power transmitted may be less than the apparent power obtained by multiplying the readings of a voltmeter and ammeter in the same circuit.\n\nThe two circuits of a wattmeter can be damaged by excessive current. The ammeter and voltmeter are both vulnerable to overheating — in case of an overload, their pointers will be driven off scale — but in the wattmeter, either or even both the current and potential circuits can overheat \"without\" the pointer approaching the end of the scale. This is because the position of the pointer depends on the power factor, voltage and current. Thus, a circuit with a low power factor will give a low reading on the wattmeter, even when both of its circuits are loaded to the maximum safety limit. Therefore, a wattmeter is rated not only in watts, but also in volts and amperes.\n\nA typical wattmeter in educational labs has two voltage coils (pressure coils) and a current coil. The two pressure coils can be connected in series or parallel to change the ranges of the wattmeter. The pressure coil can also be tapped to change the meter's range. If the pressure coil has range of 300 volts, the half of it can be used so that the range becomes 150 volts.\n\nElectronic wattmeters are used for direct, small power measurements or for power measurements at frequencies beyond the range of electrodynamometer-type instruments.\n\nA modern digital wattmeter samples the voltage and current thousands of times a second. For each sample, the voltage is multiplied by the current at the same instant; the average over at least one cycle is the real power. The real power divided by the apparent volt-amperes (VA) is the power factor. A computer circuit uses the sampled values to calculate RMS voltage, RMS current, VA, power (watts), power factor, and kilowatt-hours. The readings may be displayed on the device, retained to provide a log and calculate averages, or transmitted to other equipment for further use. Wattmeters vary considerably in correctly calculating energy consumption, especially when real power is much lower than VA (highly reactive loads, e.g. electric motors). Simple meters may be calibrated to meet specified accuracy only for sinusoidal waveforms. Waveforms for switched-mode power supplies as used for much electronic equipment may be very far from sinusoidal, leading to unknown and possibly large errors at any power. This may not be specified in the meter's manual.\n\nThere are limitations to measuring power with inexpensive wattmeters, or indeed with any meters not designed for low-power measurements. This particularly affects low power (e.g. under 10 watts), as used in standby; readings may be so inaccurate as to be useless (although they do confirm that standby power is low, rather than high). The difficulty is largely due to difficulty in accurate measurement of the alternating current, rather than voltage, and the relatively little need for low-power measurements. The specification for the meter should specify the reading error for different situations. For a typical plug-in meter the error in wattage is stated as ±5% of measured value ±10 W (e.g., a measured value of 100W may be wrong by 5% of 100 W plus 10 W, i.e., ±15 W, or 85–115 W); and the error in kW·h is stated as ±5% of measured value ±0.1 kW·h. If a laptop computer in sleep mode consumes 5 W, the meter may read anything from 0 to 15.25 W, without taking into account errors due to non-sinusoidal waveform. In practice accuracy can be improved by connecting a fixed load such as an incandescent light bulb, adding the device in standby, and using the difference in power consumption. This moves the measurement out of the problematic low-power zone.\n\nInstruments with moving coils can be calibrated for direct current or power frequency currents up to a few hundred hertz. At radio frequencies a common method is a rectifier circuit arranged to respond to current in a transmission line; the system is calibrated for the known circuit impedance. Diode detectors are either directly connected to the source, or used with a sampling system that diverts only a portion of the RF power through the detector. Thermistors and thermocouples are used to measure heat produced by RF power and can be calibrated either directly or by comparison with a known reference source of power. A bolometer power sensor converts incident radio frequency power to heat. The sensor element is maintained at a constant temperature by a small direct current. The reduction in current required to maintain temperature is related to the incident RF power. Instruments of this type are used throughout the RF spectrum and can even measure visible light power. For high-power measurements, a calorimeter directly measures heat produced by RF power.\n\nAn instrument which measures electrical energy in watt hours (electricity meter or energy analyser) is essentially a wattmeter which accumulates or averages readings. Digital electronic instruments measure many parameters and can be used where a wattmeter is needed: volts, current,in amperes, apparent instantaneous power, actual power, power factor, energy in [k]W·h over a period of time, and cost of electricity consumed.\n\n\n"}
