{"id": "1356772", "url": "https://en.wikipedia.org/wiki?curid=1356772", "title": "Acacia Research", "text": "Acacia Research\n\nAcacia Research Corporation is an American company based in Newport Beach, California, that works to license patents and perform relevant legal action such as dealing with patent infringement.\n\nAcacia is sometimes referred to as a non-practicing entity because the company derives revenue from licenses and lawsuits rather than from building products. According to the company, it has returned over $705,000,000 to patent owners. The corporation creates a subsidiary company for each set of patents it enforces. Currently, 95% of the company's business involves partnering with inventors and patent holders. It currently has 180 known subsidiaries and 257 active cases as of September 2014, four of which are not through a subsidiary.\n\n"}
{"id": "33429894", "url": "https://en.wikipedia.org/wiki?curid=33429894", "title": "Adolf Zethelius", "text": "Adolf Zethelius\n\nAdolf Zethelius (born \"Erik Adolf Zethelius\"; 19 February 17817 March 7, 1864) was a Swedish silversmith and industrialist and owner of the Swedish ironworks Surahammars bruk and Nyby bruk.\n\nThe orb, sceptre and anointing horn which are part of the Norwegian Royal Regalia were commissioned by King Charles John and made by Zethelius in Stockholm for the King's coronation as King of Norway in 1818. Works by Zethelius are also exhibited at Nationalmuseum and Hallwyl Palace in Stockholm.\n\nZethelius acquired the manor house Nyby, near Torshälla, from his father-in-law Eric Nordewall, and in 1829 founded the ironworks Nyby bruk on the site.\n\nZethelius had the manor house of Surahammars herrgård constructed 1856–1858.\n"}
{"id": "2233425", "url": "https://en.wikipedia.org/wiki?curid=2233425", "title": "Aldrin", "text": "Aldrin\n\nAldrin is an organochlorine insecticide that was widely used until the 1990s, when it was banned in most countries. It is a colourless solid. Before the ban, it was heavily used as a pesticide to treat seed and soil. Aldrin and related \"cyclodiene\" pesticides (a term for pesticides derived from Hexachlorocyclopentadiene) became notorious as persistent organic pollutants.\n\nAldrin is produced by combining hexachlorocyclopentadiene with norbornadiene in a Diels-Alder reaction to give the adduct.\n\nSimilarly, an isomer of aldrin, known as isodrin, is produced by reaction of hexachloronobornadiene with cyclopentadiene.\n\nAldrin is named after the German chemist Kurt Alder, one of the coinventors of this kind of reaction. An estimated 270 million kilograms of aldrin and related cyclodiene pesticides were produced between 1946 and 1976.\n\nIn soil, on plant surfaces, or in the digestive tracts of insects, aldrin oxidizes to the epoxide dieldrin, which is more strongly insecticidal.\n\nLike related polychlorinated pesticides, aldrin is highly lipophilic. Its solubility in water is only 0.027 mg/L, which exacerbates its persistence in the environment. It was banned by the Stockholm Convention on Persistent Organic Pollutants. In the U.S., aldrin was cancelled in 1974. The substance is banned from use for plant protection by the EU.\n\nAldrin has rat of 39 to 60 mg/kg (oral in rats). For fish however, it is extremely toxic, with an LC50 of 0.006 – 0.01 for trout and bluegill.\n\nIn the US, aldrin is considered a potential occupational carcinogen by the Occupational Safety and Health Administration and the National Institute for Occupational Safety and Health; these agencies have set an occupational exposure limit for dermal exposures at 0.25 mg/m over an eight-hour time-weighted average.\nFurther, an IDLH limit has been set at 25 mg/m, based on acute toxicity data in humans to which subjects reacted with convulsions within 20 minutes of exposure.\n\nIt is classified as an extremely hazardous substance in the United States as defined in Section 302 of the U.S. Emergency Planning and Community Right-to-Know Act (42 U.S.C. 11002), and is subject to strict reporting requirements by facilities which produce, store, or use it in significant quantities.\n"}
{"id": "42572559", "url": "https://en.wikipedia.org/wiki?curid=42572559", "title": "Beer tower", "text": "Beer tower\n\nA beer tower, also known as a portable beer tap, a tabletop beer dispenser, a triton dispenser or a giraffe, is a beer dispensing device, usually found in bars, pubs and restaurants. The idea behind beer towers is that several patrons in a group can serve themselves the amount of beer they want without having to order individually.\n\nThe device comes in a variety of sizes, most often double to triple the size of standard beer pitchers that hold around of beer.\n\nEarly versions came in the shape of a four-foot tall plastic cylinder attached to a beer tap at the bottom. Current models sometimes have several dispensing valves, to allow for simultaneous dispense. \n\nChilling of the beer is usually achieved with ice chambers or with freezable ice packs, which may be internal (traditional) or external (for increased hygiene when handling). \n\nThe actual dispense mechanism is usually in the form of a simplified beer tap or a bespoke patented pouring mechanism..\n\nThe ancestor of the modern beer tower was called a portable keg tap or a picnic tap, since the device was mostly used at outdoor events, such as picnics. They were significantly larger than modern beer towers. The method involved manually pumping air into the beer container; the resulting pressure within the container would force the beer out the tap. There was also a variant with lager beer barrels that stood upright, allowing gravity to force the beer out the tap, which is the principle of the modern day system.\n\nThe beer tower is popular in bars and clubs throughout Thailand, Malaysia, Singapore, and the Philippines. Since 2012, the beer tower has appeared in Australian bars after first being used in Queensland.\n\nIn March 2011, Boston licensing officials forbade bars from serving beer in beer towers. The officials required licensed alcohol serving businesses to request permission to use these beverage dispensers.\n\nSelf-service tabletop beer dispensers have been in use in India at least since 2017. \n\n"}
{"id": "46266097", "url": "https://en.wikipedia.org/wiki?curid=46266097", "title": "Bus encoding", "text": "Bus encoding\n\nBus encoding refers to converting/encoding a piece of data to another form before launching on the bus. While bus encoding can be used to serve various purposes like reducing the number of pins, compressing the data to be transmitted, reducing cross-talk between bit lines, etc., it is one of the popular techniques used in system design to reduce dynamic power consumed by the system bus. Bus encoding aims to reduce the Hamming distance between 2 consecutive values on the bus. Since the activity is directly proportional to the Hamming distance, bus encoding proves to be effective in reducing the overall activity factor thereby reducing the dynamic power consumption in the system.\n\nIn the context of this article, a system can refer to anything where data is transferred from one element to another over bus (viz. System on a Chip (SoC), a computer system, an embedded system on board, etc.).\n\nPower consumption in electronic systems is a matter of concern today for the below reasons:\n\nThe dynamic power dissipated by an electronic circuit is directly proportional to the activity factor and the load capacitance as seen by the output of the logic gate. In case of a bus, the load capacitance is usually high since bus needs to be connected to multiple modules and routed longer and the activity factor is also high. Due to higher value of load capacitance and activity factor, in a typical system, bus power consumption can contribute up to 50% of the total power consumption. Bus encoding aims to reduce this power by reducing the amount of activity (number of toggles) in the bus lines. While the kind of bus encoding to be used for a particular system can be best determined when the target application and environmental constraints about the system are known apriori, described below are some bus encoding techniques which can help reduce bus power for most systems. Hence bus encoding is important for any electronic system design.\n\nFollowing are some of the implementations to use bus encoding for reducing dynamic power consumption in different scenarios:\n\nIn case of SoC designs, bus encoding schemes can be best implemented in RTL by instantiating dedicated encoders and decoders over the bus. Another way it could be implemented is by passing hint to the synthesis tool either as a trace of the simulation or by using synthesis pragma to define the type of encoding needed.\n\nOn board, a small low power IC can be deployed in between the master and slave modules on the bus to implement the encoding and decoding functions.\n\nThe bus encoding/decoding function must be a bijection. This essentially requires encoding function to possess below behavior:\n\n\n"}
{"id": "7545098", "url": "https://en.wikipedia.org/wiki?curid=7545098", "title": "CMX Systems", "text": "CMX Systems\n\nCMX Editing Systems (also known as CMX Systems) was a company founded jointly by CBS and Memorex; with help from many individuals such as Ronald Lee Martin, who later became a head of Universal Studios; that developed some of the very first computerized systems for linear and non-linear editing of videotape for post production. The company's name, CMX, stood for CBS, Memorex, and eXperimental.\n\nHeadquartered in Sunnyvale, California, the company pioneered in integrating computers with videotape editing, starting in 1971 with the CMX 600, the first non-linear video editing system. The 600 was designed primarily for off-line editing, by creating both a rough cut edit of a video program, along with an edit decision list, or EDL. It stored its video & audio content on disk pack drives supplied by Memorex for instant random access of the video content. The 600 was paired with the CMX-200, which took the edit decision list created by the 600, and automatically controlled several VTRs to auto-assemble the final program. The 600 was controlled using a Digital PDP-11 minicomputer, and the 200 used a Teletype Model 33 terminal to input EDLs from the 600.\n\nCMX also developed the CMX-300 in 1972, a system used for online editing (and CMX's first online product). It was a computer-controlled linear editing system, with support for up to four VTRs, and also included and controlled a simple video mixer for wipes and fades. The edits were input to the 300 (and displayed) using a Digital VT-05 terminal.\n\nCMX would later develop more advanced systems such as the 340 in 1976, and the CMX Edge, which could be used for both on and off-line editing.\n\nCMX was sold to John Herbert Orr's company Orrox in 1974, and then moved its headquarters to Santa Clara, California. It was then later purchased by Chyron, and remained under its ownership until 1998, when Chyron announced that it would discontinue all CMX products.\n\nDuring the mid-1980s, CMX hardware comprised 90% of all video editing systems used for post-production video editing.\n\nThe CMX keyboard control style was used as a basis of several other editing systems, including Grass Valley, Calaway and Strassner Editing Systems.\n\n\n\nSee External Links below.\n\n"}
{"id": "11424228", "url": "https://en.wikipedia.org/wiki?curid=11424228", "title": "CarIFS", "text": "CarIFS\n\nCaribbean Integrated Financial Services Inc. (CarIFS) is a Barbados-based ABM-network provider.\nUsing the name CarIFS (for short), the company's offering allows customers of various financial institutions in Barbados to have 24-hour access to cash from their Bank Accounts via any CarIFS affiliated Automated Banking Machine (ABM) in the country. The Manager of Carifs is currently David Robinson.\n\nThe network links over 104 automated banking machines and 35 hundred point of sale terminals throughout Barbados. From 2006 the network recorded a total of 3.5 million transactions.\n\n\n"}
{"id": "22793110", "url": "https://en.wikipedia.org/wiki?curid=22793110", "title": "Caswell-Massey", "text": "Caswell-Massey\n\nCaswell-Massey, founded in 1752, is the first fragrance and personal care product company in America. Originally, Caswell Massey started as an apothecary shop in Newport, Rhode Island, by a Scottish-born doctor named William Hunter. The main product categories include fine-fragrance, soap, bath & body products, men's shaving products and toiletries, other assorted apothecary-style personal care accessories. Its products are preferred favorites of notable historical figures such as: JFK, George Washington, Cole Porter, Alla Nazimova, John Denver, and The Rolling Stones.\n\nThe company is regarded as the fourth-oldest continuously operating company in America and the oldest American consumer brand in operation. The current motto of is \"America's Original\".\n\nDr. William Hunter established \"Dr. Hunters Dispensary\" in Newport in 1752. Caswell Massey began as an apothecary shop selling medical supplies. Hunter gave the first lectures on anatomy and surgery in the Colonies in 1755. During that time, he also invented orange soda to help his customers take the medicines sold in his apothecary shop.\n\nNewport, Rhode Island, at the time, was a destination for the social elite to buy European-style luxuries. While selling medical products, Dr. Hunter also began selling cosmetic, personal care, and hygiene products and developed a business in medicinal essential oils, such as lavender and verbena. He also imported fragrances from Europe, and blended 20 of his own different colognes, numbered One through Twenty; Dr. Hunter's Cologne Number Six was often purchased by George Washington and was given as a gift by the President to the Marquis de Lafayette, and thereby became very popular among many politicians and intellectuals in early American society after Independence. During the same time, White Rose perfume was introduced for women, and became a favorite of Dolly Madison, who was rumored to bathe in the perfume. The shop was also known for its very high-quality castille soap, which was purchased by Lewis and Clark for their Western expedition.\n\nFor approximately the first three-quarters of a century the apothecary shop changed owners in the traditional way; each retiring pharmacist handing over the keys to his apprentice. Dr. William Hunter was followed by his son, also William Hunter; then followed by Charles Feke, who in turn was followed by Rowland Hazard in 1822. Hazard took Philip Caswell into partnership and the name became Hazard & Caswell. In 1833 following Rowland Hazard’s death the company became Caswell & Hazard. In the same year, the first Caswell-Hazard branch opened in New York City.\n\nA fragrance called Jockey Club was introduced in 1840. The company during that time also continued to expand its line to include other apothecary products including cucumber night cream, oatmeal soaps, goats milk soaps, and other items. In the 1850s, Hazard formed a partnership with Phillip Caswell, and later in 1867, was joined by both John R. Caswell (Phillips Brother) and J. Hazard (Rowland's Brother or son?).\n\nPhillip Caswell resigned from the partnership in 1872, and sold his shares to Rowland Hazard; John Caswell set up shops and continued the business in parallel to the Hazard business during this time, and a lawsuit was undertaken in 1876 over the rights to be the successor of both the 'Caswell' name and the 'Established 1780 AD' trademark, as well as the books of formulations, Caswell was triumphant in the suit, proving in the court records that in fact, they had sold the physical shops and the shop fixtures and stock in labels, but not the rights to the name or the formulations.\n\nIt is unclear, but it appears that the Hazards were given the exclusive right to claim the 'Established 1780 AD' mark; but in fact, Caswell also succeeded on this point, as it was later established that the business was in fact established by Dr. Hunter in 1752 and not by Feke in 1780 - that had merely been the aforementioned ruse that was used to prevent the firm from being confiscated under disloyalty to the new American government. The company took its present name, Caswell-Massey, when then-owner John Rose Caswell formed a partnership with New York businessman William Massey in 1876. With rightful ownership of the original formulations of Dr. Hunter and a new partnership with William Massey, a Canadian doctor, the firm rightly claimed the 'Established 1752' trademark and provenance for their brand. In that year the company operated two stores, one in Newport and one in New York City. John R. Caswell and William Massey continued to operate in New York City and Newport, RI, until 1906 when the business was incorporated as the Caswell-Massey Company under the guidance of George C. Lyon and John C. Knight of the Hall & Lyon Company of Providence, Rhode Island; the business continued to grow to ten stores, but was reduced to two stores by 1915.\n\nDuring the early to mid-1900s, the apothecary business expanded its service to provide 'custom' perfumes and fragrances as private stock for its most notable customers. Customers who used this service were interviewed on their tastes in food, music, and lifestyle, and given five fragrances to try. After a period of time, their favorite fragrance was selected and five variants of that fragrance were created. After a final period of wearing these, the customer's signature fragrance was refined and bottled and could be ordered on demand. This was an expensive service at the time, costing customers up to $200 per ounce. \n\nCustomers to the Caswell-Massey stores during this time included many notable New Yorkers, as well as Broadway stars such as Alla Nazimova, and New York elite such as the Astors and Vanderbilts; George Gershwin, Judy Garland, Katharine Hepburn and Greta Garbo. In 1926 a store was opened on Lexington (the 48th in New York City), in what was then the Barclay Hotel, later InterContinental New York Barclay Hotel. ( It remained the company’s flagship store until 2010 when the Barclay's Hotel was closed for renovations). \n\nIn 1936, Ralph Taylor and his younger brother Milton Taylor bought the company. Ralph had been working for Caswell-Massey for twenty years since 1916 when a 13-year-old Ralph Taylor was hired to sweep the shop and clean bottles in the basement. Unlike the previous owners, the Taylors retained the name Caswell-Massey, which had acquired a worldwide reputation during the 60 years under Caswell as one of the most important fragrances and soap brands in the world.\n\nIn 1941, Caswell-Massey introduced Tricorn cologne and expanded its notoriety among the New York elite. The company's cold cream was a popular product for many Broadway stars, and luminaries such as Cole Porter who wore the new Tricorn cologne.\n\nDuring the second half of the century, clients included United States President John F. Kennedy, who wore Caswell Massey's Jockey Club cologne; his wife, Jacqueline Onassis (who bought avocado oil), and pop culture figures who shopped from the company's famously hand-illustrated direct mail catalog and who frequented the company's Lexington Avenue Store. Patrons at the Lexington & 48th street store included Debbie Harry, Joni Mitchell, John Denver, and the Rolling Stones. In the 1980s, the company expanded across the United States and added several overseas stores. \n\nCaswell-Massey was managed by the Taylor family from 1936–1989. Milton's older son Adam joined the company in 1974 and soon rose to be president, and younger son Joshua joined in 1979. When they started working at Caswell-Massey, the company had only one store. However, the new younger Taylor blood injected fresh energy into the enterprise, and over the next ten years they added 20 more retail stores and about a dozen franchise stores, spread throughout the US and Canada.\nAlso during that time frame, Josh oversaw and overhauled the extensive product lines and assortments offered by the company, and introduced or re packaged 600 Caswell-Massey branded products, and together with the retail store expansion the company grew ten times in sales.\n\nJosh created and introduced Greenbriar cologne in 1984.\n\nThe Taylors sold the company to private owners in 1988; at the time, the company had over 28 stores in the US; the company continued with modest growth under new owners for several years before taking a downturn due to rising competition from mall competitors such as Crabtree & Evelyn (founded in 1972) and Bath & Body Works (founded in 1990) which offered similar products, albeit with less history. In 1999, after struggling for several years, Anne Robinson lead a buyout of the company. Under Anne Robinson's leadership, the company experienced a brief turnaround and celebrated its 250th anniversary in 2002. Modest growth and international business continued for the company. Anne Robinson departed the company in 2003. After trading hands between various investors, the company was purchased by its current ownership in 2007.\n\nAs of 2017, the company remains privately owned and operated in America, headquartered in Edison, New Jersey, and New York City. The company re-branded in 2017 which included the launch of a new website and new catalog. Updated formulas of their \"Heritage\" men’s colognes and their \"Centuries\" collection have been re-released. Caswell also re-launched its essential oil business in collaboration with MKG Bio Alchemy, and is planning 2018 releases of new men's and women's fragrances in collaboration with the world-renowned New York Botanical Gardens and the International Flavors and Fragrances, Inc. Caswell Massey plans to re-release some of its private stock fragrances, such as \"Marem\": a fragrance originally created for silent film star Alla Nazimova in 1914, and \"Supernatural Number 6\": the fragrance favorite of George Washington.\n\nCaswell Massey continues to formulate and produce original, unique products that maintains a loyal client base. All of its fragrance and personal care items are still made in America, many using the original formulations. \n\n\n"}
{"id": "52615946", "url": "https://en.wikipedia.org/wiki?curid=52615946", "title": "Civic Technology Companies", "text": "Civic Technology Companies\n\nCivic technology companies are platforms, products, and services that facilitate civic engagement. Civic technology encompasses any type of technology that enables greater participation in government affairs, or \"assists government in delivering citizen services and strengthening ties with the public\". The phrase can essentially be used to describe any company that is concerned with improving the quality, access, and efficiency of government services within the political system through technological means.\n\n\n\n"}
{"id": "42975490", "url": "https://en.wikipedia.org/wiki?curid=42975490", "title": "Clipix", "text": "Clipix\n\nClipix is a privately funded company offering an online bookmarking, file sharing, and organizational tool. The utility is available in 13 languages, and as of 2014 had users in 154 countries. Clipix allows users to save online content and other documents to private or public \"clipboards.\" By dragging a “clip” button onto the browser bar, users click and clip online items for later reference. Clipix users can also add digital files they want to access, including Excel spreadsheets, Microsoft Word documents, PDF files, and email messages. To organize content further, Clipix also offers multiboards, which are created by dragging several clipboards into one \"Main Category\" clipboard.\nSyncboards are synchronized clipboards that multiple users can access and add to, in order to work collaboratively. The company’s \"Price Drop Alert\" allows users to name the price they’d like to pay for any item that is sold online. Once a \"Price Drop Alert\" is set, Clipix notifies the user when an item has been discounted to his or her preferred price. The Clipix app for iPad, iPhone, and Android synchronizes with the Clipix web interface.\n\nClipix is often compared to Pinterest, a similar tool.\n\n"}
{"id": "34856965", "url": "https://en.wikipedia.org/wiki?curid=34856965", "title": "Council of Europe Convention on the Counterfeiting of Medical Products", "text": "Council of Europe Convention on the Counterfeiting of Medical Products\n\nThe Council of Europe Convention on the counterfeiting of medical products and similar crimes involving threats to public health (or MEDICRIME convention) is a multilateral convention of the Council of Europe aiming at prevention of counterfeiting medical products. Its purpose is threefold (article 1):\n\nAs of July 2016, the treaty has been ratified by Albania, Armenia, Guinea, Hungary, Moldova, Spain, and Ukraine. Having been ratified by the requisite five states (at least three of which had to be Council of Europe states), the convention is entered into force on 1 January 2016.\n\n\n"}
{"id": "4249694", "url": "https://en.wikipedia.org/wiki?curid=4249694", "title": "Coupling (electronics)", "text": "Coupling (electronics)\n\nIn electronics and telecommunication, coupling is the desirable or undesirable transfer of energy from one medium, such as a metallic wire or an optical fiber, to another medium.\n\nCoupling is also the transfer of electrical energy from one circuit segment to another. For example, energy is transferred from a power source to an electrical load by means of conductive coupling, which may be either resistive or hard-wire.  An AC potential may be transferred from one circuit segment to another having a DC potential by use of a capacitor.  Electrical energy may be transferred from one circuit segment to another segment with different impedance by use of a transformer. This is known as impedance matching. These are examples of electrostatic and electrodynamic inductive coupling.\n\nElectrical conduction:\n\nElectromagnetic induction:\n\nElectromagnetic radiation:\n\nOther kinds of energy coupling:\n\n"}
{"id": "37753134", "url": "https://en.wikipedia.org/wiki?curid=37753134", "title": "CuproBraze", "text": "CuproBraze\n\nCuproBraze is a copper-alloy heat exchanger technology for harsh temperature and pressure environments such as those in the latest generations of cleaner diesel engines mandated by global environmental regulations. The technology, developed by the International Copper Association (ICA), is licensed free of charge to heat exchanger manufacturers around the world.\n\nApplications for CuproBraze include charge air coolers, radiators, oil coolers, climate control systems, and heat transfer cores. CuproBraze is particularly suited for charge air coolers and radiators in capital intensive industries where machinery must operate for long periods of time under harsh conditions without premature failures. For these reasons, CuproBraze is being specified for off-road vehicles, trucks, buses, industrial engines, generators, locomotives, and military equipment. The technology is also amenable for light trucks, SUVs and passenger cars with special needs.\n\nCuproBraze is replacing soldered copper/brass plate fin, soldered copper brass serpentine fin, and brazed aluminium serpentine fin in demanding applications.\n\nAluminium heat exchangers are viable and economical for cars, light trucks, and other light-duty applications. However, they are not amenable for environments characterized by high operating temperatures, humidity, vibration, salty corrosive air, and air pollution. In these environments, the additional tensile strength, durability, and corrosion resistance that CuproBraze provides are useful.\n\nThe CuproBraze technology uses brazing instead of soldering to join copper and brass radiator components. The heat exchangers are made with anneal-resistant copper and brass alloys. The tubes are fabricated from brass strip and coated with a brazing filler material in form of a powder based paste or an amorphous brazing foil is laid between the tube and fin. There is another method of coating the tube in-line on the tube mill. This is done using the twin wire-arc spray process where the wire is the braze alloy, deposited on the tube as it is being manufactured at 200-400 fpm. This saves one process step of coating the tube later. The coated tubes, along with copper fins, headers and side supports made of brass, are fitted together into a core assembly which is brazed in a furnace.\n\nThe technology enables brazed serpentine fins to be used in copper-brass heat exchanger designs. They are stronger, lighter, more durable, and have tougher joints.\n\nCuproBraze radiators have important performance advantages over radiators made with other materials. These include better thermal performance, heat transfer, size, strength, durability, emissions, corrosion resistance, repairability, and antimicrobial benefits.\n\nThe ability to withstand elevated temperatures is essential in high-heat applications. Aluminium alloys are challenged at higher temperatures due to their lower melting points. The yield strength of aluminium is compromised above 200 °C. Problems with fatigue cracking are exacerbated at elevated temperatures. CuproBraze heat exchangers are capable of operating at temperatures of 290 °C and above. Special anneal-resistant copper and brass strip ensure that radiator cores maintain their strength without softening, despite exposures to high brazing temperatures.\n\nCooling efficiency is a measure of heat rejection from a given space by a heat exchanger. The overall thermal efficiency of a heat exchanger core depends on many factors, such as thermal conductivity of fins and tubes; strength and weight of the fins and tubes; spacing, size, thickness and shape of fins and tubes; velocity of the air passing through the core; and other factors.\n\nThe main performance criterion for heat exchangers is cooling efficiency. Heat exchanger cores made from copper and brass can reject more heat per unit volume than any other material. This is why copper-brass heat exchangers generally have a greater cooling efficiency than alternate materials. Brazed copper-brass heat exchangers are also more rugged than soldered copper-brass and alternate materials, including brazed aluminium serpentine.\n\nAir pressure drop is a good evaluator of heat exchanger design. A heat exchanger core with a smaller air pressure drop from the front to the back of the core (i.e. from the windward to the leeward side in a wind tunnel test) is more efficient. Air pressure drops typically are 24% less for CuproBraze versus aluminium heat exchangers. This advantage, responsible for a 6% increase in heat rejection, contributes to CuproBraze’s overall greater efficiency.\n\nSince copper’s thermal conductivity is higher than aluminium’s, copper has a higher capacity to dissipate heat. By using thinner material gauges in combination with higher fin density, heat dissipation capacity with CuproBraze can be increased with air pressure drops still at reasonable levels.\n\nDue to its high heat transfer efficiency, CuproBraze offers a significant amount of cooling capacity in a small size. This is because the same heat rejection level can be achieved with a smaller-sized core. Hence, a significant reduction in frontal area and volume is achievable with CuproBraze versus other materials.\n\nThree new alloys were developed to enhance the strength and durability of CuproBraze heat exchangers: 1) an anneal-resistant fin material that maintains its strength after brazing; 2) an anneal-resistant tube alloy that retains its fine grain structure after brazing and provides ductility and fatigue strength in the brazed heat exchanger core; and 3) the brazing alloy. Brazing at 650 °C creates a joint that is stronger than a soldered joint and comparable in strength to a welded joint. Unlike welding, brazing does not melt the base metals. Therefore, brazing is more amenable to joining dissimilar alloys.\n\nCuproBraze has more strength at elevated temperatures than soldered copper-brass or aluminium. Due to the lower thermal expansion of copper versus aluminium, there is less thermal stress during the manufacturing of CuproBraze and in its use as a heat exchanger. CuproBraze heat exchangers have stronger tube-to-header joints than other materials. These braze joints are the most critical in heat exchangers and must be leak-free. CuproBraze also has higher tolerances to internal pressures because its thin-gauge high strength materials provide stronger support for the tubes. The material is also less sensitive to bad coolants than aluminium heat exchangers.\n\nTest results demonstrate a much longer fatigue life for CuproBraze joints compared to similar soldered copper-brass or brazed aluminium joints. Stronger joints allow for the use of thinner fins and new radiator and cooler designs.\n\nThe copper fins are not easily bent when dirty radiators are washed with high pressure water. Anticorrosive coatings further improve strength and resistance against humidity, sand erosion, and stone impingement on copper fins.\n\nFor further information, see: CuproBraze: Durability and reliability (Technology Series): and CupropBraze durability (design criteria series).\n\nNew legislation in Europe, Japan and the U.S. call for strong reductions in NO and particulate emissions from diesel engines used in trucks, buses, power plants, and other heavy equipment. These goals can in part be accomplished by using cleaner-performing turbocharged diesel engines and charge air coolers. Turbocharging enables better power outputs. Charge air coolers allow power to be produced with more efficiency by reducing the temperature of the air charge entering the engine, thereby increasing its density.\n\nThe charge air cooler, located between the turbocharger and the engine air inlet manifold, is an air-to-air heat exchanger. It reduces the inlet air temperatures of turbocharged diesel engines from 200 °C to 45 °C while increasing inlet air densities to increase engine efficiencies. Even higher inlet temperatures (246 °C or higher) and boost pressures may be necessary to comply with the emissions standards in the future.\n\nPresent-day charge air cooler systems, based on aluminium alloys, experience durability problems at temperatures and pressures necessary to meet the U.S. Tier 4i standards for stationary and mobile engines. Published reports estimate that the average life of an aluminium charge air cooler is currently only about 3,500 hours. Aluminium is near its upper technological limit to accommodate higher temperatures and thermal stress levels because the tensile strength of the metal declines rapidly at 150 °C and repetitive thermal cycling between 150 °C and 200 °C substantially weakens it. Thermal cycling creates weak spots in aluminium tubes, which in turn causes charge air coolers to fail prematurely. A potential option is to install stainless steel precoolers in aluminium charge air coolers, but limited space and the complexity of this solution is a hampering factor for this option.\n\nA CuproBraze charge air cooler can operate at temperatures as high as 290 °C without creep, fatigue, or other metallurgical problems.\n\nExterior corrosion resistance in a heat exchanger is especially important in coastal areas, humid areas, polluted areas, and in mining operations. Corrosion mechanisms of copper and aluminium alloys are different. CuproBraze tube contains 85% copper which provides high resistance against dezincification and stress corrosion cracking. The copper alloys tend to corrode uniformly over entire surfaces at known rates. This predictability of copper corrosion is important for proper maintenance management. Aluminium, on the other hand, is more likely to corrode locally by pitting, resulting eventually in holes.\n\nIn accelerated corrosion tests, such as SWAAT for salt spray and marine conditions, CuproBraze performed better than aluminium.\n\nThe corrosion resistance of CuproBraze is generally better than soft soldered heat exchangers. This is because the materials in CuproBraze heat exchangers are of equal nobility, so galvanic differences are minimized. On soft soldered heat exchangers, the solder is less noble than fin and tube materials and can suffer from galvanic attack in corrosive environments.\n\nCuproBraze can be easily repaired. This advantage of the technology is especially important in remote areas where spare parts may be limited. CuproBraze can be repaired with lead-free soft solder (for example 97% tin, 3% copper) or with common silver-containing brazing alloys.\n\nBiofouling is often a problem in HVAC systems that operate in warm, dark, and humid environments. The antimicrobial properties of CuproBraze alloys eliminate foul odors, thereby improving indoor air quality. CuproBraze is being investigated in mobile air conditioner units as a solution to bad odors from fungus and bacteria in aluminium-based heat exchange systems.\n\nRussian OEMs, such as Kamaz and Ural Automotive Plant, are using CuproBraze radiators and charge air coolers in heavy-duty trucks for off-highway and on-highway applications. Other manufacturers include UAZ and GAZ (Russia) and MAZ (Belarus). The Finnish Radiator Manufacturing Company, also known as FinnRadiator, produces 95% of its radiators and charge air-coolers with CuproBraze for OEM manufacturers of off-road construction equipment. Nakamura Jico Co., Ltd. (Japan) manufactures CuproBraze heat exchangers for construction equipment, locomotives and on-highway trucks. Young Touchstone supplies CuproBraze radiators to MotivePower’s diesel-powered commuter train locomotives in North America. Siemens AG Transportation Systems plans to use the technology for its Asia Runner locomotive for South Vietnam and other Asian markets. Bombardier Transportation heat exchangers cool transformer oil in electric-powered locomotives. These huge oil coolers have been used successfully in coal trains for South African Railways. Kohler Power Systems Americas, one of the largest users of diesel engines for power generation, adopted CuproBraze for diesel engine turbocharger air-to-air cooling in its “gen sets.”\n\n\n"}
{"id": "31334601", "url": "https://en.wikipedia.org/wiki?curid=31334601", "title": "Disney Second Screen", "text": "Disney Second Screen\n\nDisney Second Screen was an interactive onscreen film feature accessible via a computer or iPad app download that provided additional content to a user as he or she views a film released by the Walt Disney Studios Home Entertainment. The movie linked with the viewer's device through an audio cue, a manual sync, or with a visual sync indicator. As the film plays on a viewer's television, interactive elements such as trivia, photo galleries, and animated flipbooks appeared on the iPad or computer screen. Disney Second Screen was available to use on an iPad or computer with Flash. As posted on disneystudioshelp.com, \"Second Screen is no longer available. All Second Screen apps are now closed. The decision was difficult to make, but we feel it’s best to focus our energy and resources into making new digital experiences for our fans and community. We greatly appreciate all of the amazing engagement, passion and commitment to our apps. Thank you!\"\n\nDisney Second Screen was released by Walt Disney Studios Home Entertainment in the \"Bambi\" Diamond Edition Blu-ray on March 1, 2011. The feature is included or planned to be included with the following home media releases:\n\n\nSpecial features for the Bambi Diamond Edition release include a Thumper flipbook, games, art galleries, videos, and trivia. The \"\" DVD and Blu-ray includes an interactive reel of images and a 3-D environment that can be explored using the touchscreen or the mouse. Similarly, the application on \"The Lion King\" Diamond Edition allows you to look at storyboards, concept art, and interactive games while the film plays. For the release of \"\", Disney Second Screen allows you to look at concept art, time-lapse photography, and makeup tests. The \"Real Steel\" Second Screen includes stories and opinions from the filmmaker that expand the story. On the \"Lady and the Tramp\" Diamond Edition release, the Second Screen application includes storyboards, activities, and behind-the-scenes features. For \"Iron Man 3\", the second screen app features the voice of actor Paul Bettany as JARVIS, who recorded more than 20 hours of original audio content specifically for the app.\n\n"}
{"id": "39308049", "url": "https://en.wikipedia.org/wiki?curid=39308049", "title": "ENERPOS", "text": "ENERPOS\n\nENERPOS (which stands for Positive Energy in French) is the first educational net-zero energy building in the tropics and one of the 13 Net ZEBs in the tropics thanks to its bioclimatic design. ENERPOS is located on Reunion Island, a French territory in the Indian Ocean. Building an energy efficient building in such a climate is particularly challenging. However, the energy expectations with regard to ENERPOS have been reached, even largely exceeded. \nENERPOS is not only an energy-efficient building, but also displays various passive methods to reduce energy consumption while providing a comfortable environment for its users. Classes are hosted for both undergraduate diploma and degree courses as well as for the Department of Construction and Energy at the Graduate Engineering School of Reunion Island.\n\nENERPOS is a university building located in Saint Pierre on the French island of La Reunion.\nThis island, whose climate is hot and humid, is located in the Indian Ocean, to the east of Madagascar. This area is also often struck by tropical cyclones, generating building difficulties.\n\nOver 800,000 inhabitants of Reunion Island rely on a limited supply of energy. In addition to that, electricity production on Reunion Island is one of the most polluting on earth, mainly generated from fossil fuels such as coal and fuel. The electricity produced is expensive and is one of the most polluting in the world with 820 gCO2/kWhel (close to eight times more than in mainland France). Unfortunately, the ever-increasing demand for energy, due to a significant demographic growth, sometimes exceeds the amount of energy available. As a consequence, shortages can occur certain times during austral summers when air conditioning is widely used.\n\nAs is the case in other parts of the world, the electricity consumption of buildings represents a significant percentage of the energy used on Reunion Island. Buildings in French Tropical zones, and especially on Reunion Island, have often been awkwardly designed, contrary to former Creole vernacular architecture, importing metropolitan building concepts. Indeed, it has been common practice to build as cheaply as possible for decades, without any consideration for the environment and the climate, making it uncomfortable to live in without air conditioning. Moreover, air conditioning and lighting are usually oversized by designers, this leads to wasteful energy consumption.\nThe good news is that considerable improvements can be made in reducing energy consumption in the field of building in Reunion island.\n\nIn accordance with the general context of energy supply and consumption on Reunion Island, this building was expected to be net zero energy and to export at least as much energy as its consumption to the polluting electricity grid of the island. To achieve this goal, the first issue to be addressed is the energy consumption of the building and how to build without using air conditioning, a considerable consumer, while providing a comfortable environment for the users. Air conditioning is only used in one third of the total surface area for computers and servers, the remaining two-thirds are cooled and ventilated naturally. Passive methods are used, requiring people to be active instead of being passive in an active building (François Garde, Ph.D., P.E. and ASHRAE member). At the beginning of the design of ENERPOS in 2005, the main aim was to demonstrate that the overall consumption of the building could be reduced by three times compared to that of a standard building. The result is that only one-seventh of the annual energy consumption of a standard building is used (14kWh/m² instead of 100kWh/m² in a year). To compensate for this, photovoltaic panels are implemented over the rooftops, enabling a production of 71,118kWh in 2010 compared to an overall consumption of 9,824kWh, making this building one of the 13 Net ZEBs in a tropical climate. The city of Saint Pierre is usually sunny all year and receives a heavy amount of solar radiation with up to 1,200W/m² in summer. Consequently, the building consumes 7 times less energy than its production, the extra production of electricity being released into the grid.\n\nENERPOS meets the requirements of the following two performance labels, HQE and PERENE. PERENE is a local label in Reunion Island guiding those wishing to abide by it on how to build in harmony with the corresponding climate (four different classifications are defined depending mainly on the altitude), Saint Pierre being in the hottest zone of the island.\n\nHuman comfort depends on five criteria, two of which concern one’s clothing and metabolism, and three others which focus on air temperature, air humidity and airflow speed. The faster the airflow is on someone the cooler he feels. Moreover, natural ventilation brings sound fresh air into the building when well-designed. Thus, natural ventilation tends to increase comfort as well as health in tropical climates and aims to suppress air conditioning.\n\nThe building has been orientated in order to prevent strong East-South-East trade winds from entering the rooms in winter and still benefit from thermal breezes in summer. The main façades are then orientated North and South, reducing the heat gain on the Western and Southern façades (which are more exposed to solar radiation) at the same time.\n\nSuccessful ventilation has been obtained by creating a window to wall ratio of 30% using louvres on both opposite sides of the rooms. Louvres are not only useful to regulate the airflow, but also to protect against cyclones and break-ins. In addition to that, the surroundings of the building have been well-thought out to prevent the ground from heating before entering the rooms:\n\n- Planting native plants and trees in the patio and around the building to create a microclimate which is as fresh as possible.\n\n- Placing car parks under the building instead of next to it for the same reason. Furthermore, it increases soil permeability so that tropical rains do not cause floods but penetrate the ground.\n\nFinally, large ceiling fans are installed in every room, even those using air conditioning. Ceiling fans ensure that even in the absence of the necessary breezes, the airflow needed to feel comfortable in the room is provided for the users. This solution significantly reduces the amount of energy consumed. The overall consumption of the ceiling fans and the split system (the latter used to cool the technical rooms) is only 3.7kWh/m².yr compared to a classic air conditioning system in a standard building consuming 80kWh/m².yr.\n\nNow that the airflow parameter has been dealt with, the next issue to be addressed is air temperature. Most of the heat gained in the rooms is due to solar radiation coming through the glazing.\n\nBarely no glazing has been placed on the Western and Eastern small façades because they are the most likely to receive solar radiation. The two main façades, including the louvres, are protected against direct sunrays thanks to vertical solar shading composed of inclined wooden strips. In order to be as efficient as possible, solar shading has been simulated with a 3-D software. This glazing protection has two main effects:\n\n- To prevent glare on the desks, which can be very annoying for the students and the employees working in ENERPOS.\n\n- To decrease indoor temperature.\n\nWith regard to the envelope, the walls are made of concrete; the roofing is insulated with a 10 cm-layer of polystyrene and a ventilated BIPV (Building Integrated Photovoltaic) over-roof; the solar shading systems are made of wooden strips; the east and west gables are insulated with mineral wood and a wooden cladding. The paint used is completely organic and the wooden components have not undergone any specific treatment. No insulation is required on the main facades as they are very efficient in terms of S-value due to the solar shading.\n\nDay lighting has also been simulated to ensure a Useful Daylight Index (UDI) of at least 90% in most places. Two classrooms facing the sea on the first floor of the building do not have any artificial lighting. With the exception of those two classrooms, all classrooms and offices are lit by low energy consumption lights producing an artificial lighting density inferior to that of a standard building, 7W/m² in the classrooms with low energy T-5 lights and 3.7W/m² in the offices with personal LED desks lamps. These lighting densities are high enough to make these workplaces comfortable to work in and yet reduce both the energy consumption and the thermal heat gain produced by the lights in so far as is possible.\nThere are multiple switches to control the lights and ceiling fans by the rows of tables in the classrooms since some people could feel hot or do not have enough light while others are comfortable. This avoids wasting electricity.\n\nA Building Management System is used to control the active systems. In the event of people forgetting to turn off the light when leaving a room, a timer will turn off lights automatically after two hours.\n\nThis is similar to the lighting issue as computing systems can affect both energy consumption and thermal heat gain greatly. \nThe main solution adopted in the offices is to use laptops rather than desktop computers since they usually consume less electricity.\nAs for the computer rooms, they are only equipped with screens, mice and keyboards and all central units are located in the air-conditioned technical room. The thermal loads from the computers are thus kept outside the computing rooms.\n\nAs previously explained is, the city of Saint Pierre receives a huge amount of solar radiation throughout the year. This natural source of energy has consequently been exploited. The target being to make ENERPOS a positive energy building thanks to solar panels, the PV production must be at least equal to the energy consumption of the building.\n\nIn fact, since the photovoltaic panels have been used as an over-roof, the overall surface of the panels has been oversized compared to the energy needs in order to protect the whole roof from direct sunrays. The low energy consumption of ENERPOS is more than balanced by 365m² of these integrated solar panels, generating a total production of 71,118kWh over one year. The resulting surplus of energy, not being consumed by the building but released into the grid instead, is then up to 61kWh in a year.\n\nFurthermore, all of the costs and risks of this installation are provided by the manufacturer, as agreed in the contract, and not by the owner of ENERPOS (that is to say the University of Reunion Island). In exchange, the University of Reunion Island rents the photovoltaic production to the manufacturer who receives the benefit of the electricity fed into the grid for 15 years. After that period, the owner of the building becomes the owner of the PV panels.\n\nThe energy consumption of a building not only relies on the way it has been built, but also mainly on the occupant behaviour. This idea is all the more true for buildings being constructed based on passive designs. Indeed, making ENERPOS a passive building implies that people need to be active to use it to its full capacity. \nFor example, before turning on the ceiling fans of a classroom, the students have to open the louvres first. It seems to be common sense but as a matter of fact, people do not think about that most of the time. That is why signs are displayed in the rooms explaining how to use a classroom properly to avoid wasting energy. The purpose is to educate students (and teachers) about the way to behave and to make them realize the environmental issues at stake on Reunion Island.\n\nSince ENERPOS is a pioneer project on Reunion Island and in the tropics, it is essential to analyze the consumption distribution, the performance and how people feel in this building. Therefore, a post occupancy evaluation has been carried out for three hot seasons to assess the comfort level in ENERPOS. Students and teachers were asked to fill in a questionnaire about how they feel in the building while environmental parameters, such as temperatures, humidity and air velocity, were collected. \nThe main conclusion is that out of 700 students surveyed, the vast majority feel comfortable in ENERPOS in the hot season without any air conditioning. The ultimate objective of this project has then been met.\n\nAmong the annual 14.4kWh/m² consumption of ENERPOS, the energy end uses are as follows:\n\n- Plug loads 46%\n\n- Air conditioning and mechanical ventilation 15%\n\n- Interior lighting 14%\n\n- Ceiling fans 11%\n\n- Exterior lighting 7%\n\n- Elevator 7%\n\nThe contribution of the plug loads to the overall energy consumption is abnormally high compared to a standard building because the air conditioning and lighting parts have been well reduced.\n\nTo conclude, the ENERPOS building shows that it is possible to build an educational Net ZEB in the tropics while providing a comfortable environment for people to work and study in. Moreover, the lessons learnt about the ENERPOS project can be applied to green building and Net ZEB projects in hot climates.\n\n\n\n"}
{"id": "11262719", "url": "https://en.wikipedia.org/wiki?curid=11262719", "title": "Electronic Commerce Regulations 2002", "text": "Electronic Commerce Regulations 2002\n\nThe Electronic Commerce (EC Directive) Regulations 2002, SI 2002/2013, incorporates the EU Electronic Commerce Directive 2000/31/EC into the law of the United Kingdom. They apply to contracts concluded by electronic means over distance whereby the buyer is a consumer. This subordinate legislation provides for rights of the consumer and provisions for which the seller is obliged to fulfill.\n\nA Consumer is a \"natural person who is acting for the purposes other than those of his trade, business or profession.\" The definition is slightly broader than that for the purposes of the Unfair Contract Terms Act 1977 as the subjective requirement of \"the person not regarding themselves as acting in the course of a business\", therefore one may be a consumer if using a company account or using business details for tax purposes.\n\nBefore the contract is formed, the seller must state in a \"clear comprehensible and unambiguous manner\" the technical step involved to place an order (contractual offer). Terms and conditions under which the contract is concluded must be made available to the consumer in a means capable of reproduction and storage. This does not apply to email, although the Consumer Protection (Distance Selling) Regulations 2000 may apply.\n\nThese rights can be expressly exempted, although these regulations applied to the exemption clause, as did the Unfair Contract Terms Act 1977 and the Unfair Terms in Consumer Contracts Regulations 1999.\n\nInformation that must be provided to the consumer.\n\nMore information must be given under the Consumer Protection (Distance Selling) Regulations 2000.\n\nLiability of breach of these conditions gives rise to an action for Breach of statutory duty.\n\nA court order may be given for access to terms and conditions of which the \"consumer\" has already consented.\n\nWhere the \"consumer\" has not been informed in the correct manner of the procedure to amend errors in orders and they have made errors, the contract can be rescinded.\n\nreg 12 provides that a contractual offer occurs when the order is sent. Richards construes reg 11(2) as providing that the acknowledgement screen will constitute a contractual acceptance.\n\n\"Instantaneous Communication\" here is in line with that discussed by Lord Denning in \"Entores Ltd v Miles Far East Corporation\" and so communication is effect when received or when it can reasonably be deemed to have been received.\n\n\n"}
{"id": "5361228", "url": "https://en.wikipedia.org/wiki?curid=5361228", "title": "Event data recorder", "text": "Event data recorder\n\nAn event data recorder or EDR, sometimes referred to informally as an automotive \"black box\" (by analogy with the common nickname for flight recorders), is a device installed in some automobiles to record information related to vehicle crashes or accidents. In the USA EDRs must meet federal standards, as described within the U.S. Code of Federal Regulations.\n\nIn modern diesel trucks, EDRs are triggered by electronically sensed problems in the engine (often called faults), or a sudden change in wheel speed. One or more of these conditions may occur because of an accident. Information from these devices can be collected after a crash and analyzed to help determine what the vehicles were doing before, during and after the crash or event. The term generally refers to a simple, tamper-proof, read-write memory device. \n\nSome EDRs continuously record data, overwriting the previous few minutes until a crash stops them, and others are activated by crash-like events (such as sudden changes in velocity) and may continue to record until the accident is over, or until the recording time is expired. EDRs may record a wide range of data elements, potentially including whether the brakes were applied, the speed at the time of impact, the steering angle, and whether seat belt circuits were shown as \"Buckled\" or \"Unbuckled\" at the time of the crash. Current EDRs store the information internally on an EEPROM until recovered from the module. Some vehicles have communications systems (such as GM's OnStar system) that may transmit some data, such as an alert that the airbags have been deployed, to a remote location.\n\nMost EDRs in automobiles and light trucks are part of the restraint system control module, which senses impact accelerations and determines what restraints (airbags and/or seatbelt tensioners) to deploy. After the deployment (or non-deployment) decisions are made, and if there is still power available, the data are written to memory. The data downloaded from older EDRs usually contain 6 to 8 pages of information, though many newer systems include many more data elements and require more pages, depending on the make/model/year of the vehicle being evaluated. Depending on the type of EDR, it may contain either a deployment file or a non-deployment file or sometimes both, depending on the circumstances of the collisions and the time interval between them, among other things. \n\nIt is also possible that no data can be recovered from a data recorder. One situation where this might occur is a catastrophic loss of electrical power early in a collision event. In this situation, the power reserve in the restraint system control module capacitors may be completely spent by the deployment of the air bags, leaving insufficient power to write data to the EEPROM. There are other circumstances where a module may fail to record a data file as well.\n\nMost EDRs in heavy trucks are part of the engine electronic control module (ECM), which controls fuel injection timing and other functions in modern heavy-duty diesel engines. The EDR functions are different for different engine manufacturers, but most recognize engine events such as sudden stops, low oil pressure, or coolant loss. Detroit Diesel, Caterpillar Inc., Mercedes-Benz, Mack Trucks, and Cummins engines are among those that may contain this function. When a fault-related event occurs, the data is written to memory. When an event triggered by a reduction in wheel speed is sensed, the data that is written to memory can include almost two minutes of data about vehicle speed, brake application, clutch application, and cruise control status. The data can be downloaded later using the computer software and cables for the specific engine involved. These software tools often allow monitoring of the driver hours of service, fuel economy, idle time, average travel speeds, and other information related to the maintenance and operation of the vehicle.\n\nSome EDRs only keep track of the car's speed along its length and not the speed going sideways. Analysts generally look at the momentum, energy, and crush damage, and then compare their speed estimates to the number coming out of the EDR to create a complete view of the accident.\n\nThere are many different patents related to various types of EDR features.\n\nThe Eaton Vehicle Onboard Radar (VORAD) Collision Warning System is used by many commercial trucking firms to aid drivers and improve safety. The system includes forward and side radar sensors to detect the presence, proximity and movements of vehicles around the truck and then alert the truck driver. When sensors determine that the truck is closing on a vehicle ahead too quickly or that a nearby vehicle is potentially hazardous, the VORAD system gives the driver both a visual and audible warning. The VORAD system also monitors various parameters of the truck including vehicle speed and turn rate plus the status of vehicle systems and controls. \nThe monitored data is captured and recorded by the VORAD system. This monitored data can be extracted and analyzed in the event of an accident. The recorded data can be used by accident investigators and forensic engineers to show the movement and speed of the host vehicle plus the position and speeds of other vehicles prior to the incident. In accident reconstruction, the VORAD system is a step above the EDR systems in that VORAD monitors other vehicles relative to the host vehicle, while EDR’s only record data about the host vehicle.\n\nEvent data recorders were introduced to American open-wheel championship CART in the 1993 season, and the Formula One World Championship in 1997. This allowed to study crashes that allow to develop new car rules and track safety measures that reduce damages.\n\nUsage of the device in road vehicles once varied widely from manufacturer to manufacturer, but EDRs are now mandated in all new vehicles. As of 2003, there were at least 40 million vehicles equipped with the devices. In the UK many police and emergency service vehicles are fitted with a more accurate and detailed version that is produced by one of several independent companies. Both the Metropolitan police and the City of London police are long-term users of EDRs and have used the data recovered after an incident to convict both police officers and members of the public.\n\nDownloading an airbag module in most vehicles is best accomplished by connecting the appropriate scanning tool to the Diagnostic Link Connector (DLC) usually found under the vehicle's dashboard near the driver's knees. Alternately, some modules can be downloaded \"on the bench\" after removal from the vehicle.\n\nOver 88% of model year 2016 and newer vehicles are supported by the Bosch CDR tool, enabling the retrieval of Event Data Recorder (EDR) data from a vehicle that has been involved in a crash. This tool is made up of hardware and software which provides the ability to “image”, “download”, or “retrieve” EDR data that may be stored in the control modules of passenger cars, light trucks and SUVs. The software component is a single, standalone program designed to run in a Windows environment. The hardware part of the Tool is a collection of components including cables and adapters which, with proper training and minimal difficulty, are used to “retrieve” data from supported vehicles.\n\nAnother 11% of model year 2016 and newer vehicles are supported by other EDR tools. The limited need to cover less commonly supported vehicles may make the initial investment in software and equipment unnecessary for many in the accident reconstruction or related industries.\n\nFrom 1998 to 2001, the National Highway Traffic Safety Administration (NHTSA) sponsored a working group specifically tasked with the study of EDRs. After years of evaluation, NHTSA released a formal Notice of Proposed Rulemaking in 2004. This notice declared NHTSA’s intent to standardize EDRs. It was not until August 2006 that NHTSA released its final ruling (49 CFR Part 563). The ruling was lengthy (207 pages), consisting of not only definitions and mandatory EDR standards, but also acted as a formal reply to the dozens of petitions received by NHTSA after the 2004 notice.\n\nSince there was already an overwhelming trend for voluntary EDR installation, the ruling did not require manufacturers to install EDRs in vehicles produced for North America. Based on its analysis, NHTSA estimated that by 2010, over 85% of vehicles would already have EDRs installed in them, but warned that if the trend did not continue, the agency would revisit their decision and possibly make installation a requirement.\n\nThe mandate did, however, provide a minimum standard for the type of data that EDRs would be required to record: at least 15 types of crash data. Some of the required crash data include pre-crash speed, engine throttle, brake use, measured changes in forward velocity (Delta-V), driver safety belt use, airbag warning lamp status and airbag deployment times.\n\nIn addition to the required data, NHTSA also set standards for 30 other types of data if EDRs were voluntarily configured to record them. For example, if a manufacturer configured an EDR to record engine RPMs or ABS activity, then the EDR would have to record 5 seconds of those pre-crash data in half-second increments.\n\nBesides the requirement that all data be able to survive a 30 MPH barrier crash and be measured with defined precision, NHTSA also required that all manufacturers make their EDR data publicly available. As of October 2009, only General Motors, Ford and Daimler Chrysler had released their EDR data to be publicly read.\nIn the August 2006 ruling, NHTSA set a time table for all vehicle manufacturers to be in compliance with the new EDR standards. The compliance date was originally set for all vehicles manufactured after September 1, 2010. But in 2008, NHTSA pushed the date back to September 1, 2012. In 2014, it was working on another rule update to give vehicle manufacturers until September 1, 2014, but that rule was never issued.\n\nDespite alerts and warnings in their vehicle owner's manual, many drivers are not aware of their vehicle's recording capability. Civil liberty and privacy groups have raised concerns about the implications of data recorders 'spying' on car users, particularly as the issue of 'who owns the data' has not yet been fully resolved, and there has been some controversy over the use of recorded data as evidence in court cases and for insurance claims against the driver of a crashed vehicle. But the use of EDR data in civil and criminal court cases is on the rise as they become more accepted as a source of reliable empirical evidence in accident reconstruction.\n\nFourteen states have statutes specific to EDRs. Generally, these state statutes restrict access to the EDR or limit the use of recovered EDR information.\n\nThe federal Driver Privacy Act of 2015 was enacted on December 4, 2015. It stated that the owner or lessee of a motor vehicle is the owner of the data collected by the EDR. In order to access that data, an investigator would need to (1) be authorized by a court or judicial or administrative authority, subject to the standards for admission into evidence; (2) obtain the written, electronic or recorded audio consent of the vehicle owner or lessee; (3) be conducting an investigation or inspection authorized by federal law; (4) demonstrate it is necessary to facilitate medical care in response to a car accident; aor (5) be conducting traffic safety research, so long as the personal information of the owner/lessee is not disclosed.\n\nThere have been a number of trial cases in the US and Canada involving EDRs. Drivers have been convicted and exonerated as a result of EDR evidence.\n\nExamples include:\n\n\nAlthough EDR evidence can be valuable in the litigation of traffic-related accidents and incidents, the primary purpose of an EDR is to improve driver safety and not to provide data for accident reconstruction, and courts should consider the limitations of EDR data in determining the cause of traffic accidents.\n\nOn 12 April 2007, N.J. Governor Jon Corzine was seriously injured in an automobile accident. According to the superintendent of state police, an Event Data Recorder in the SUV he was traveling in recorded he was traveling at about 91 MPH five seconds before the crash. The speed limit on the road is 65 MPH. The Governor was not the driver of the vehicle.\n\nOn November 2, 2011, Mass. Lt. Governor Tim Murray crashed a government-owned vehicle on a stretch of Interstate 190. Initially, police investigating did not issue any citations. \n\nMurray initially claimed he simply lost control on the ice, wasn’t speeding, was wearing a seat belt and braked. But those claims were all later disproven when the Crown Victoria black box data recorder information was released. The data revealed the car was traveling 108 miles per hour, accelerated, and the Lt. Governor was not wearing a seat belt at the time the vehicle collided with a rock ledge and flipped over. Murray was unhurt in the accident.\n\nA Video Event Data Recorder (VEDR) is a device that records video in a vehicle to create a record of accidents and for evaluating driver and vehicle performance.\n\n\n"}
{"id": "43091601", "url": "https://en.wikipedia.org/wiki?curid=43091601", "title": "Falcon 9 first-stage landing tests", "text": "Falcon 9 first-stage landing tests\n\nThe Falcon 9 first-stage landing tests were a series of controlled-descent flight tests conducted by SpaceX between 2013 and 2016. Since 2017, the first stage of Falcon 9 missions has been routinely landed if the rocket performance allowed it, and if SpaceX chose to recover the stage. \n\nThe program's objective was to reliably execute controlled re-entry, descent and landing (EDL) of the Falcon 9 first stage into Earth's atmosphere after the stage completes the boost phase of an orbital spaceflight. The first tests aimed to touch down vertically in the ocean at zero velocity. Later tests attempted to land the rocket precisely on an autonomous spaceport drone ship (a barge commissioned by SpaceX to provide a stable landing surface at sea) or at Landing Zone 1 (LZ-1), a concrete pad at Cape Canaveral. The first ground landing at LZ-1 succeeded in December 2015, and the first landing at sea on a drone ship in April 2016. The second landed booster, B1021, was the first to fly again in March 2017, and was recovered a second time.\n\nThe first landing test occurred in September 2013 on the sixth flight of a Falcon 9 and maiden launch of the v1.1 rocket version. From 2013 to 2016, sixteen test flights were conducted, six of which achieved a soft landing and recovery of the booster:\n\nSince the January 2017 return to flight, SpaceX has stopped referring to landing attempts as \"experimental\", indicating that they have become a routine procedure (see Iridium-1 and CRS-10 press kits of 2017, compared with CRS-9 and JCSAT-16 of 2016). , 14 routine landings have been performed ( success) and three missions were launched in expendable configuration, not attempting to land.\n\nThe first-stage descent tests were part of the larger SpaceX reusable launch system development program, which included a large amount of new technology development activities and earlier low-altitude test flights at the SpaceX facility in McGregor, Texas in preparation for the high-altitude high-velocity testing of landing test phase of the program. The overall objective of the program is to privately develop reusable rockets using vertical-landing technology so as to substantially reduce the cost of space access.\n\nTraditionally, the first stages of orbital carrier rockets have been discarded in the ocean once the ascent was complete. Achieving routine recovery and reuse of the launch vehicles could dramatically reduce the cost of access to space.\n\nSpaceX first announced in March 2013 that it would instrument and equip subsequent Falcon 9 first stages as controlled-descent test vehicles, able to propulsively decelerate towards a soft touchdown over the water surface. The company expected to begin these flight tests in 2013, with an attempt to return the vehicle to the launch site for a powered landing no earlier than mid-2014.\n\nIn the event, SpaceX did perform their first controlled-descent test flight in 2013 but continued the over-water testing well into 2015. Following analysis of telemetry data from the first controlled descent in September 2013, SpaceX announced that a large amount of new technology passed their real-life test objectives, and that coupled with the technology advancements made on the Grasshopper low-altitude landing demonstrator, they were now ready to test the full EDL process to recover the first stage. The rocket was \"able to successfully transition from vacuum through hypersonic, through supersonic, through transonic, and light the engines all the way and control the stage all the way through [the atmosphere]\".\n\nThis second EDL test took place during the third cargo resupply mission for NASA in April 2014. SpaceX attached landing legs to the first stage, decelerated the stage through atmospheric re-entry and attempted a simulated landing over water, following the separation of the second stage carrying the Dragon capsule to the ISS. The first stage was slowed down sufficiently to perform a soft touchdown over the Atlantic Ocean.\nSpaceX announced in February 2014 that they intended to continue over-water tests of the first stage until mastering precision control of the vehicle from hypersonic speed all the way through subsonic regimes.\n\nSubsequent tests, starting with the CRS-5 mission in January 2015, attempted to land the first stage on an autonomous spaceport drone ship stationed off the Florida coastline or in the Pacific Ocean depending on launch site.\nThe ships were used for six landing attempts, two of which succeeded in April and May 2016. Meanwhile, the at Cape Canaveral occurred on December 21, 2015, and succeeded flawlessly.\n\nThe post-mission Falcon 9 test plan for the earliest flight tests called for the first stage to perform a retro-propulsion burn in the upper atmosphere to slow it down and put it on a descent ballistic trajectory to its target landing location, followed by a second burn in the lower atmosphere before the first stage reached the water. SpaceX announced in March 2013 that it intended to conduct such tests on Falcon 9 v1.1 launch vehicles and would \"continue doing such tests until they can do a return to the launch site and a powered landing\". The company said it expected several failures before it could land the vehicle successfully.\n\nIn detailed information disclosed in the Falcon 9 flight 6 launch license for the CASSIOPE mission, SpaceX said it would fire three of the nine Merlin 1D engines initially to slow the horizontal velocity of the rocket and begin the attempt at a controlled descent. Then, shortly before hitting the ocean, one engine would be relighted in an attempt to reduce the stage's speed so that it could be recovered. , SpaceX said the experiment had approximately a ten percent chance of success.\n\nSpaceX did not perform controlled-descent tests on all Falcon 9 v1.1 flights, as payloads going to GTO did not leave enough fuel margin.\nIn September 2013, SpaceX announced that the CRS-3 mission of April 2014 (fourth flight of Falcon 9 v1.1) would be the second test of the descent test profile.\n\nWhereas the early tests restarted the engines only twice, by the fourth flight test, in September 2014, SpaceX was reigniting the engines three times to accomplish its EDL test objectives (although only three of the nine engines were used): a boost-back burn, a reentry burn, and a landing burn. The boost-back burn limits downrange translation of the used stage; the reentry burn (from approximately altitude) is used to control the descent and deceleration profile at atmospheric interface; and the landing burn completes the deceleration from terminal velocity to zero at the landing surface.\n\nThe first propulsive reentry, descent, and ocean-surface touchdown test occurred on September 29, 2013, on Falcon 9 flight 6, the maiden launch of the Falcon 9 rocket, version v1.1. After the three-minute boost phase and separation of the second stage with the CASSIOPE and nanosat payloads, the rocket's first stage was reoriented backwards and three of the nine Merlin 1D engines were reignited at high altitude to initiate a deceleration and controlled descent trajectory to the surface of the ocean. The first phase of the test \"worked well and the first stage re-entered safely\". However, the stage began to roll because of aerodynamic forces during the atmospheric descent and the roll rate exceeded the capabilities of the first stage attitude control system (ACS) to null it out. The fuel in the tanks \"centrifuged\" to the outside of the tank and the single engine involved in the low-altitude deceleration maneuver shut down. SpaceX was able to retrieve some first-stage debris from the ocean. The company did not expect to recover the first stage on this flight, nor on the first several powered-descent tests, as predicted in their March 2013 announcement.\n\nThis first experimental descent was considered successful, achieving substantial test milestones and collecting a great deal of engineering data, despite losing the stage into the ocean. SpaceX tested a large amount of new technology on this flight, and, combining those results with the advances made on the Grasshopper demonstrator, the company now believed it had \"all the pieces of the puzzle\".\n\nThe second test of controlled-descent hardware and software on the first stage occurred on April 8, 2014, and became the first successful controlled ocean soft touchdown of a liquid-rocket-engine orbital first stage.\nThe first stage included landing legs for the first time which were extended to simulate a landing upon touchdown, and the test utilized more powerful gaseous Nitrogen control thrusters to control the aerodynamic-induced rotation that had occurred on the first test flight. The first stage successfully approached the water surface with no spin and at zero vertical velocity, as designed.\n\nDuring the second test, the first stage was traveling at a velocity of at an altitude of at the time of the high-altitude turn-around maneuver, followed by ignition of three of the nine main engines for the initial deceleration and placement onto its descent trajectory. The \"first stage executed a good re-entry burn and was able to stabilize itself on the way down. ... [The] landing in [the] Atlantic [ocean] was good! ... Flight computers continued transmitting [telemetry data] for 8 seconds after reaching the water\" and stopped only after the first stage went horizontal.\n\nThe major modifications for the second first stage controlled-descent test flight included changes to both the reentry burn and the landing burn as well as adding increased attitude control system (ACS) capabilities.\n\nSpaceX had projected a low probability of stage recovery following the flight test due to complexity of the test sequence and the large number of steps that would need to be carried out perfectly. The company was careful to label the entire flight test as \"an experiment\". In a press conference at the National Press Club on April 25, Elon Musk said that the first stage achieved a soft touchdown on the ocean but due to rough seas, the stage was destroyed.\n\nThe third test flight of a returned first stage was July 14, 2014, on Falcon 9 flight 10. Whereas the previous test reached a target landing area some hundreds of kilometers off the Florida coast, this flight aimed for a boost-back trajectory that would attempt the ocean touchdown much nearer the coast, and closer to the original launch location at Cape Canaveral. Following the third controlled-descent test flight, SpaceX expressed confidence in their ability to successfully land in the future on a \"floating launch pad or back at the launch site and refly the rocket with no required refurbishment.\"\n\nFollowing the first stage loft of the second stage and payload on its orbital trajectory, SpaceX conducted a successful flight test on the spent first stage. The first stage successfully decelerated from hypersonic speed in the upper atmosphere, made a successful reentry, landing burn, and deployment of its landing legs, and touched down on the ocean surface. The first stage was not recovered for analysis as the hull integrity was breached, either upon touchdown or on the subsequent \"tip over and body slam\".\nResults of the post-landing analysis showed that the hull integrity was lost as the -tall first stage fell horizontally, as planned, onto the ocean surface following the landing.\n\nThe fourth test flight of a returned first stage, with a planned ocean touchdown, occurred on Falcon 9 flight 13 which was launched on September 21, 2014. and the first stage flew a profile approaching a zero-velocity at zero-altitude simulated landing on the sea surface. SpaceX made no attempt to recover the first stage, since earlier tests had confirmed that the 14-story tall first stage would not survive the tip-over event into the sea. The booster did run out of liquid oxygen.\n\nOne month later, detailed thermal imaging infrared sensor data and video were released of the controlled-descent test. The data was collected by NASA in a joint arrangement with SpaceX as part of research on retropropulsive deceleration technologies in order to develop new approaches to Mars atmospheric entry. A key problem with propulsive techniques is handling the fluid flow problems and attitude control of the descent vehicle during the supersonic retropropulsion phase of the entry and deceleration. All phases of the night-time flight test on the first stage were successfully imaged except for the final landing burn, as that occurred below the clouds where the IR data was not visible.\nThe research team is particularly interested in the altitude range of the SpaceX \"reentry burn\" on the Falcon 9 Earth-entry tests as this is the \"powered flight through the Mars-relevant retropulsion regime\" that models Mars entry and descent conditions.\n\nSpaceX had planned to make the sixth controlled-descent test flight and second landing attempt on their drone ship no earlier than February 11, 2015. Landing a returning rocket at sea would have been a \"potentially historic rocket launch and landing\", as such a feat \"was unheard of\" five years earlier.\n\nAccording to regulatory paperwork filed in 2014, SpaceX plans had called for the sixth test flight to occur on a late January 2015 launch attempt. However, after the completion of the fifth test flight, and with some damage being incurred by the drone ship in the botched landing, it was not clear whether the sixth test would still be feasible only a few weeks later.\nThis issue was resolved within days of the ship's return to Jacksonville, and by January 15, SpaceX was unambiguous about its plans to attempt a landing of the first stage following the boost phase of the Deep Space Climate Observatory mission.\n\nHowever, in a statement by SpaceX, the drone ship was in conditions \"with waves reaching up to three stories in height crashing over the decks\". Additionally, one of the four thrusters that keep the barge in a constant position had malfunctioned, making station-keeping difficult. For these reasons, the post-launch flight test did not involve the barge, but instead attempted a soft touchdown over water.\n\nThe test was successful, and the first stage of the Falcon 9 landed \"nicely vertical\" with an accuracy of 10 meters from the target location in the ocean.\n\nTherefore, this test represented the fifth ocean touchdown, and the sixth overall Falcon 9 first stage controlled-descent test.\n\n, SpaceX has attempted 25 landings of a first stage on a solid surface, 20 of which have succeeded.\n\nIn July 2014, SpaceX announced that the fifth and sixth controlled-descent test flights would attempt to land on a solid surface, merging the lessons from the high-altitude envelope expansion of the first four controlled-descent flights over water with the low-altitude lessons of the F9R Dev testing in Texas. At that time, the \"solid surface\" was not further described, and was later revealed to be a seafaring barge dubbed an autonomous spaceport drone ship.\n\nMany of the test objectives were achieved on the first attempt, including bringing the stage to the specific location of the floating platform and collecting a large amount of test data with the first use of grid fin control surfaces for more precise reentry positioning. However the touchdown on the corner of the barge was a hard landing and most of the rocket body fell into the ocean and sank; SpaceX published a short clip of the crash. It would take four more attempts to achieve the first barge landing at sea on . Meanwhile, ground landing succeeded on the first attempt with on December 21, 2015.\n\nIn October 2014, SpaceX clarified that the \"solid surface\" would be a floating platform constructed from a barge in Louisiana, and confirmed that they would attempt to land the first stage of the fourteenth Falcon 9 flight on the platform.\nFor the landing to succeed, the -wide span of the rocket landing legs must not only land within the -wide barge deck, but would need to also deal with ocean swell and GPS errors.\nIn late November, SpaceX revealed that the landing barge would be capable of autonomous operation and would not need to be anchored or moored; it was hence called an autonomous spaceport drone ship. three of these ships had been built, two of which were operational.\n\nThis fifth controlled-descent test flight was anticipated by the specialized press as a historic core return attempt. It incorporated for the first time in an orbital mission the grid fin aerodynamic control surfaces that had previously been tested only during a low-altitude, low-speed test with the F9R Dev1 prototype vehicle in early 2014.\nThe addition of grid fins, with continuation of the control authority obtained from gimbaling the engines as on previous test flights, was projected to improve the landing accuracy to , a thousand-fold improvement over the four previous test flights which landed within of their target coordinates. Prior to the flight, SpaceX projected that the likelihood of success on the first try was 50 percent or less.\n\nThe first test flight for this new hardware occurred on January 10, 2015, on the CRS-5 mission for NASA. The controlled-descent flight started approximately three minutes after launch, following the second stage separation event,\nwhen the first stage was approximately high and moving at a velocity of .\n\nThe SpaceX webcast indicated that the boostback burn and reentry burns for the descending first stage occurred, and that the descending rocket then went \"below the horizon,\" as expected, which eliminated the live telemetry signal, so that the retropropulsive landing attempt was not shown live. Shortly thereafter, SpaceX released information that the rocket did get to the drone ship as planned, but \"landed hard ... Ship itself is fine. Some of the support equipment on the deck will need to be replaced.\"\nMusk later elaborated that the rocket's flight-control surfaces had exhausted their supply of hydraulic fluid prior to impact.\nMusk posted photos of the impact while talking to John Carmack on Twitter. SpaceX later released a video of the impact on Vine.\n\nA seventh test flight of the first stage controlled-descent profile occurred on April 14, 2015, on Falcon 9 flight 17, which carried CRS-6 to the International Space Station. This was SpaceX's second attempt to land on a floating platform. The first stage was fitted with grid fins and landing legs to facilitate the post-mission test.\n\nAn early report from Elon Musk suggested that the first stage made a hard landing on the drone ship.\nMusk later clarified that the bipropellant valve was stuck, and therefore the control system could not react rapidly enough for a successful landing.\nOn April 15, SpaceX released a video of the terminal phase of the descent, the landing, the tip over, and the resulting deflagration as the stage broke up on the deck of the ASDS.\n\nThe first attempt to land the first stage of Falcon 9 on a ground pad near the launch site occurred on flight 20, the maiden flight of the Falcon 9 Full Thrust version, on the evening of December 21, 2015. The landing was successful and the first stage was recovered.\nThis was the first time in history that a rocket first stage returned to Earth after propelling an orbital launch mission and achieved a controlled vertical landing.\n\nSpaceX applied to the Federal Aviation Administration (FAA) US regulatory authority to perform its eighth booster controlled-descent test culminating with a landing attempt at the \"Landing Zone 1\" facility (formerly Launch Complex 13) that SpaceX had recently built at Cape Canaveral Air Force Station.\nThe FAA cleared SpaceX to attempt this landing after assessing that it would inflict minimal damage on the environment.\nAdditionally, NASA planned to close the NASA Causeway near the launch and landing site and significantly increase the size of exclusion zones during the launch and landing attempt.\nBoth options to attempt landing on the ground pad or on the drone ship at sea remained open until the day of the launch. The final decision to return the booster to Cape Canaveral was made based on a number of factors, including weather at the potential landing sites.\n\nFlight 20 took off at 20:29 EST on December 21, 2015 (01:29 UTC on December 22, 2015). About 9 minutes and 45 seconds later, the first stage landed vertically on the pad.\n\nSpaceX does not plan to fly the Falcon 9 flight 20 first stage again. Rather, the rocket was inspected and moved back to the launch pad a few miles north to perform a static fire test. After the hot fire test, the vehicle was evaluated in detail by SpaceX to assess capabilities for reflight of the launch vehicle design after future landings.\n\nOn December 31, SpaceX announced that no damage had been found on the stage and that it was ready to fire again.\nOn January 15, 2016, SpaceX conducted the static fire test on the recovered booster and reported a good overall outcome, except for some thrust fluctuations in one of the outer engines (engine 9). Elon Musk reported that this may have been due to debris ingestion.\n\nThis booster has been on display outside of SpaceX headquarters in Hawthorne, California since August 20, 2016.\n\nFlight 21, the final launch of a Falcon 9 v1.1, carried the Jason 3 payload. At one point this was the first possible opportunity for an attempt to land the first stage on land,\nbut the launches were reordered following the loss of Falcon 9 flight 19 in June 2015. Jason-3 was successfully launched on January 17, 2016, and while the first stage managed to slow down towards a soft landing, the lockout collet on one of the landing legs did not latch correctly, which caused the rocket to fall over and explode after touching down.\nElon Musk noted that ice buildup on the collet from the high-humidity launch conditions may have led to the failure of the latch.\n\nOn March 4, 2016 the Falcon 9 flight 22 launched the heavy SES-9 communications satellite,\nthe rocket's largest payload yet targeting a highly-energetic geosynchronous transfer orbit (GTO). Consequently, the Falcon 9 first stage followed a ballistic trajectory after separation and re-entered the atmosphere at high velocity with very little fuel to mitigate potential aerodynamic damage.\n\nTherefore, SpaceX did not expect to successfully land its Falcon 9 booster on its sea barge, the \"Of Course I Still Love You,\" positioned in the Atlantic Ocean. Elon Musk confirmed in a tweet that the landing attempt had failed.\n\nOn April 8, 2016, Falcon 9 flight 23, the third flight of the full-thrust version, delivered the SpaceX CRS-8 cargo on its way to the International Space Station while the first stage conducted a boostback and re-entry maneuver over the Atlantic Ocean. Nine minutes after liftoff, the booster landed vertically on the drone ship \"Of Course I Still Love You\", from the Florida coastline, achieving a long-sought-after milestone for the SpaceX reusability development program.\n\nThis stage, serial number B1021, was refurbished and flown again in March 2017 for the SES-10 mission, setting another milestone in the development of reusable rockets.\n\nOn May 6, 2016, Falcon 9 flight 24 delivered the JCSAT-14 satellite on a geostationary transfer orbit (GTO) while the first stage conducted a re-entry burn under ballistic conditions without prior boostback. Following the controlled descent through the atmosphere, the booster executed a short landing burn as it approached the drone ship \"Of Course I Still Love You\", and succeeded in landing vertically. This second landing at sea was more difficult than the previous one because the booster at separation was traveling about compared to on the CRS-8 launch to low Earth orbit. Pursuing their experiments to test the limits of the flight envelope, SpaceX opted for a shorter landing burn with three engines instead of the single-engine burns seen in earlier attempts; this approach consumes less fuel by leaving the stage in free fall as long as possible and decelerating more sharply, thereby minimizing the amount of energy expended to counter gravity. Elon Musk indicated this first stage may not be flown again and will instead be used as a life leader for ground tests to confirm future first stage rockets are good.\n\nOn May 27, 2016, Falcon 9 flight 25 delivered THAICOM 8 to a supersynchronous transfer orbit; despite high re-entry speed, the first stage again landed successfully on the SpaceX drone ship. The landing crushed a \"crush core\" in one leg, leading to a notable tilt to the stage as it stood on the drone ship.\n\nOn June 15, 2016, Falcon 9 flight 26 successfully delivered the Eutelsat 117W B and ABS 2A satellites into GTO. The first stage conducted a re-entry burn and successfully deployed its grid fins, before attempting a landing on the barge. The landing failed in its final moments due to low thrust on one of the first stage engines, caused by the exhaustion of its liquid oxygen fuel supply. That caused the engines to shut down early while the first stage was just above the drone's deck, causing a landing failure.\n\nIn the early hours of July 18, 2016, Falcon 9 flight 27, carrying the Dragon spacecraft for the CRS-9 mission was followed by a successful landing of the first stage at Landing Zone 1, Cape Canaveral.\n\nOn August 14, 2016, the Falcon 9 flight 28 successfully propelled the Japanese JCSAT-16 telecommunications satellite to a geosynchronous transfer orbit. The first stage re-entered the atmosphere and landed vertically on the \"Of Course I Still Love You\" drone ship that was located in the Atlantic Ocean.\n\nSpaceX continued to return a number of first stages in both ground and sea landings to clarify the procedures needed to re-use flown boosters. The company had hoped to begin offering pre-flown Falcon 9 rocket stages commercially by the end of 2016, but the first re-used booster eventually took off on March 30, 2017 with the SES-10 mission. The booster performed well and was recovered a second time.\n\nIn January 2016 Musk evaluated the likelihood of success to approximately 70 percent for landing attempts in 2016, hopefully rising to 90 percent in 2017; he also cautioned that the company expected \"a few more RUDs\", referring to the term \"Rapid Unscheduled Disassembly\", a humorous euphemism for destruction of the vehicle. Musk's prediction were close to the actual numbers, as five out of eight flown boosters () were recovered in 2016, and 14 out of 14 () in 2017. Three GTO missions for heavy payloads were flown in an expendable configuration, not equipped for landing. Five boosters were flown a second time in 2017, marking the beginning of routine reuse of boosters.\n\n\n"}
{"id": "681283", "url": "https://en.wikipedia.org/wiki?curid=681283", "title": "Feeding tube", "text": "Feeding tube\n\nA feeding tube is a medical device used to provide nutrition to people who cannot obtain nutrition by mouth, are unable to swallow safely, or need nutritional supplementation. The state of being fed by a feeding tube is called gavage, enteral feeding or tube feeding. Placement may be temporary for the treatment of acute conditions or lifelong in the case of chronic disabilities. \nA variety of feeding tubes are used in medical practice. They are usually made of polyurethane or silicone. The diameter of a feeding tube is measured in French units (each French unit equals ⅓ mm). They are classified by the site of insertion and intended use.\nThere are dozens of conditions that may require tube feeding. The more common conditions that necessitate feeding tubes include prematurity, failure to thrive (or malnutrition), neurologic and neuromuscular disorders, inability to swallow, anatomical and post-surgical malformations of the mouth and esophagus, cancer, Sanfilippo syndrome, and digestive disorders.\n\nFeeding tubes are used widely in children with excellent success for a wide variety of conditions. Some children use them temporarily until they are able to eat on their own, while other children require them longterm. Some children only use feeding tubes to supplement their oral diet, while others rely on them exclusively.\n\nPeople with advanced dementia who get feeding assistance rather than feeding tubes have better outcomes. Feeding tubes do not increase life expectancy for such people, or protect them from aspiration pneumonia. Feeding tubes can also increase the risk of pressure ulcers, require pharmacological or physical restraints, and lead to distress. In the final stages of dementia, assisted feeding may still be preferred over a feeding tube to bring benefits of palliative care and human interaction even when nutritional goals are not being met.\n\nFeeding tubes are often used in the intensive care unit (ICU) to provide nutrition to people who are critically ill while their medical conditions are addressed; as of 2016 there was no consensus as to whether nasogastric or gastric tubes led to better outcomes.\n\nThere is at least moderate evidence for feeding tubes improving outcomes for chronic malnutrition in people with cancers of the head and neck that obstruct the esophagus and would limit oral intake, people with advanced gastroparesis, and ALS. For long term use, gastric tubes appear to have better outcomes than nasogastric tubes.\n\nPeople who have surgery on their throat or stomach often have a feeding tube while recovering from surgery; a tube leading through the nose and down to the middle part of the small intestine is used, or a tube is directly placed through the abdomen to the small intestine. As of 2017 it appeared that people with a tube through the nose were able to start eating normally sooner.\n\nMedical nutrition companies make flavored products for drinking and unflavored for tube feeding. These are regulated as medical foods, which are defined in section 5(b) of the Orphan Drug Act (21 U.S.C. 360ee (b) (3)) as \"a food which is formulated to be consumed or administered enterally under the supervision of a physician and which is intended for the specific dietary management of a disease or condition for which distinctive nutritional requirements, based on recognized scientific principles, are established by medical evaluation.\"\n\nNasogastric and nasojejeunal tubes are meant to convey liquid food to the stomach or intestines. When inserted incorrectly, the tip may rest in the respiratory system instead of the stomach or intestines; in this case, the liquid food will enter the lungs, resulting in pneumonia and can, in rare cases, lead to death.\n\nComplications associated with gastrostomy tubes (inserted through the abdomen and into the stomach or intestines) include leakage of gastric contents (containing hydrochloric acid) around the tube into the abdominal (peritoneal) cavity resulting in peritonitis, a serious complication which will cause death if it is not properly treated. Septic shock is another possible complication. Minor leakage may cause irritation of the skin around the gastrostomy site or stoma. Barrier creams, to protect the skin from the corrosive acid, are used to manage this.\n\nA phenomenon called \"tube dependency\" has been discussed in the medical literature, in which a child refuses to eat after being on a feeding tube, but it is not recognized as a disorder in the ICD or DSM and its epidemiology is unknown.\n\nThe most common types of tubes include those placed through the nose, including nasogastric, nasoduodenal, and nasojejunal tubes, and those placed directly into the abdomen, such as a gastrostomy, gastrojejunostomy, or jejunostomy feeding tube.\n\nA nasogastric feeding tube or NG-tube is passed through the nares (nostril), down the esophagus and into the stomach. This type of feeding tube is generally used for short term feeding, usually less than a month, though some infants and children may use an NG-tube longterm. Individuals who need tube feeding for a longer period of time are typically transitioned to a more permanent gastric feeding tube. The primary advantage of the NG-tube is that it is temporary and relatively non-invasive to place, meaning it can be removed or replaced at any time without surgery. NG-tubes can have complications, particularly related to accidental removal of the tube and nasal irritation.\n\nA nasojejunal or NJ-tube is similar to an NG-tube except that it is threaded through the stomach and into the jejunum, the middle section of the small intestine. In some cases, a nasoduodenal or ND-tube may be placed into the duodenum, the first part of the small intestine. These types of tubes are used for individuals who are unable to tolerate feeding into the stomach, due to dysfunction of the stomach, impaired gastric motility, severe reflux or vomiting. These types of tubes must be placed in a hospital setting.\n\nA gastric feeding tube (G-tube or \"button\") is a tube inserted through a small incision in the abdomen into the stomach and is used for long-term enteral nutrition. One type is the percutaneous endoscopic gastrostomy (PEG) tube which is placed endoscopically. The position of the endoscope can be visualized on the outside of the person's abdomen because it contains a powerful light source. A needle is inserted through the abdomen, visualized within the stomach by the endoscope, and a suture passed through the needle is grasped by the endoscope and pulled up through the esophagus. The suture is then tied to the end of the PEG tube that will be external, and pulled back down through the esophagus, stomach, and out through the abdominal wall. The insertion takes about 20 minutes. The tube is kept within the stomach either by a balloon on its tip (which can be deflated) or by a retention dome which is wider than the tract of the tube. G-tubes may also be placed surgically, using either an open or laparoscopic technique.\n\nGastric feeding tubes are suitable for long-term use, though they sometimes need to be replaced if used long term. The G-tube can be useful where there is difficulty with swallowing because of neurologic or anatomic disorders (stroke, esophageal atresia, tracheoesophageal fistula, radiotherapy for head and neck cancer), and to decrease the risk of aspiration pneumonia. However, in people with advanced dementia or adult failure to thrive it does not decrease the risk of pneumonia. There is moderate quality evidence suggesting that the risk of aspiration pneumonia may be reduced by inserting the feeding tube into the duodenum or the jejunum (post-pyloric feeding), when compared to inserting the feeding tube into the stomach. People with dementia may attempt to remove the PEG, which causes complications.\n\nA G-tube may instead be used for gastric drainage as a longer term solution to the condition where blockage in the proximal small intestine causes bile and acid to accumulate in the stomach, typically leading to periodic vomiting, or if the vagus nerve is damaged. Where such conditions are only short term, as in a hospital setting, a nasal tube connected to suction is usually used. A blockage lower in the intestinal tract may be addressed with a surgical procedure known as a colostomy, and either type of blockage may be corrected with a bowel resection under appropriate circumstances. If such correction is not possible or practical, nutrition may be supplied by parenteral nutrition.\n\nA gastrojejunostomy or GJ feeding tube is a combination device that includes access to both the stomach and the jejunum, or middle part of the small intestine. Typical tubes are placed in a G-tube site or stoma, with a narrower long tube continuing through the stomach and into the small intestine. The GJ-tube is used widely in individuals with severe gastric motility, high risk of aspiration, or an inability to feed into the stomach. It allows the stomach to be continually vented or drained while simultaneously feeding into the small intestine. GJ-tubes are typically placed by an interventional radiologist in a hospital setting. The primary complication of a GJ-tube is migration of the long portion of the tube out of the intestine and back into the stomach.\n\nA jejunostomy feeding tube (J-tube) is a tube surgically or endoscopically inserted through the abdomen and into the jejunum (the second part of the small intestine).\n\n\n"}
{"id": "13956504", "url": "https://en.wikipedia.org/wiki?curid=13956504", "title": "Glenn Ricart", "text": "Glenn Ricart\n\nGlenn Ricart is a computer scientist. He started using one of the original Internet (ARPANET) nodes in 1969.\n\nRicart set up what was probably the first Internet Exchange point, the FIX (Federal Internet Exchange) in College Park, Maryland which interconnected the original federal TCP/IP networks and was extended to form MAE-East.\n\nRicart led the team that wrote the code for the first implementation of TCP/IP for the IBM PC. He then secured financial support from IBM for writing the code, and, in addition to its free availability to the education community, arranged for IBM to sell it as IBM's entry into the field (the product was called PC/IP by IBM).\nHe led the team that developed the OSPF reference implementation at the University of Maryland, including Louis Mamakos and Mike Petry.\nHe also led the team that provided and operated the routers for the first NSFNet backbone.\n\nThe Ricart-Agrawala Algorithm was the result of his dissertation work at the University of Maryland. \n\nFrom 1971 to 1982, he was a lead software engineer at the National Institutes of Health, developing the first e-mail program for the TOPS-10 (PDP-10) operating system in 1973. From 1982-1993, he headed academic computing at the University of Maryland. In 1984, it became the first campus to adopt TCP/IP campus-wide and use it to connect all academic minicomputers and mainframes.\n\nIn 1985 to 1989, he was instrumental in bringing the Internet to South America, helping to bring the first BITNET and Internet connections to Brazil in partnership with CNPq, Argentina via the University of Buenos Aires, and Chile by first connecting REUNA. He instructed the first networking workshop for Latin and South America (ESLARED) and several succeeding workshops.\n\nFrom 1993 to 1995, Ricart was a Program Manager at DARPA for operating systems, middleware, and end-system security. From 1995 to 1999, he was chief technology officer at Novell, helping to move that company from the proprietary Xerox Network Systems protocol to also embrace TCP/IP. In 1999, he co-founded CenterBeam, a start-up based on remote system management driven by directory services.\n\nFrom 2003 to 2009, he was the founding managing director of the PricewaterhouseCoopers Center for Advanced Research based in San Jose, California.\n\nIn October 2009 Ricart was named president and CEO of National LambdaRail, the high-speed networking platform owned by the U.S. research and education community.\nOn September 7, 2010 he announced his resignation from National LambdaRail due to strategic differences with the board.\n\nIn 2010 he presented the Bernard Price Memorial Lecture in South Africa.\n\nIn 2013, he was inducted into the Internet Hall of Fame.\n\nHe previously served on the boards of the Internet Society, BITNET, CACI, First USA Financial Services, Santa Cruz Operation, and NASULGC. He is the CTO of Artkick - a service that displays artwork on televisions.\n\nRicart received his B.S. degree in engineering from Case Institute of Technology in 1971, and his M.S. in Computing and Information Sciences from Case Western Reserve University in 1973. He received his doctorate in the University of Maryland Department of Computer Science, which is part of the University of Maryland College of Computer, Mathematical, and Natural Sciences in 1980, mentored by Ashok Agrawala.\n"}
{"id": "10455584", "url": "https://en.wikipedia.org/wiki?curid=10455584", "title": "Go-box", "text": "Go-box\n\nGo-box is a name used for a number of electronic devices.\n\nThe \"Go-Box\" is often a box, crate, carry-case, modified briefcase or similar construction containing electronic equipment pre-setup and ready to function. The box can then be taken into the field or placed at a remote site with minimal effort. These are often used by radio amateurs (or \"Hams\") for emergency communications, experimental work, or field communications. This has also led to similar equipment being used in the Emergency Services, utility companies, military, and government agencies.\n\nA search of the YouTube website can reveal a number of ideas for these devices mostly built by people at home.\n\nTerms created after the use of \"go-box\" include the \"go-bag\" which is an 'essentials' bag of items needed for evacuations or quick departures, i.e. medicines, clothes, torch, Broadcast radio receiver, batteries, etc.\n\nIn Austria it is a radio transmitter used in trucks as part of the Videomaut toll collection system.\n\nOne use of the term in the United States it is a device which is supposed to change traffic signals from red to green. U.S. Fire trucks have a similar device, called an Opticon, that uses an infrared beam. Two residents of Miami, Florida, were arrested for selling fake go-boxes online. Several hundred were sold, prices ranging from $69 to $150. In reality, the boxes contained nothing more than strobe lights.\n\n"}
{"id": "47160658", "url": "https://en.wikipedia.org/wiki?curid=47160658", "title": "High flyer (fishing)", "text": "High flyer (fishing)\n\nHigh Flyers, also known as Long Line High Flyers, are vertical poles used by commercial fishermen that serve to locate the beginning and end of a long fishing line, commonly used in tuna, swordfish, as well as other fishing. Often constructed with a vertical Marine Grade aluminum pole which resists corrosion, they have a tapered fluted buoy to provide buoyancy, and the poles are about 6–9 feet (2–3 meters) high above the water. The counter-weight at the bottom is often a PVC pipe filled with concrete. The High Flyer is tipped with an aluminum radar reflector that warns ships from a distance of a line between two High Flyers.\n"}
{"id": "2869783", "url": "https://en.wikipedia.org/wiki?curid=2869783", "title": "History of the Panama Canal", "text": "History of the Panama Canal\n\nThe idea of the Panama canal dates back to 1513, when Vasco Núñez de Balboa first crossed the isthmus. The narrow land bridge between North and South America houses the Panama Canal, a water passage between the Atlantic and Pacific Oceans. The earliest European colonists recognized this potential, and several proposals for a canal were made.\n\nBy the late nineteenth century, technological advances and commercial pressure allowed construction to begin in earnest. Noted canal engineer Ferdinand de Lesseps led an initial attempt by France to build a sea-level canal. Beset by cost overruns due to the severe underestimation of the difficulties in excavating the rugged Panama land, heavy personnel losses in Panama due to tropical diseases, and political corruption in France surrounding the financing of the massive project, the project succeeded in only partially completing the canal. \n\nInterest in a U.S.-led canal effort picked up as soon as France abandoned the project. Initially, the Panama site was politically unfavorable in the U.S. for a variety of reasons, including the taint of the failed French efforts and the Colombian government's unfriendly attitude towards the U.S. continuing the project. The U.S. first sought to construct a completely new canal through Nicaragua instead.\n\nFrench engineer and financier Philippe-Jean Bunau-Varilla played a key role in changing U.S. attitudes. Bunau-Varilla had a large stake in the failed French canal company, and stood to make money on his investment only if the Panama Canal was completed. Extensive lobbying of U.S. lawmakers coupled with his support of a nascent independence movement among the Panamanian people led to a simultaneous revolution in Panama and the negotiation of the Hay–Bunau-Varilla Treaty which secured both independence for Panama and the right for the U.S. to lead a renewed effort to construct the canal. Colombia's response to the Panamanian independence movement was tempered by U.S. military presence; the move is often cited as a classic example of the era of gunboat diplomacy.\n\nU.S. success hinged on two factors. First was converting the original French sea-level plan to a more realistic lock-controlled canal. The second was controlling disease which decimated workers and management alike under the original French attempt. Initial chief engineer John Frank Stevens built much of the infrastructure necessary for later construction; slow progress on the canal itself led to his replacement by George Washington Goethals. Goethals oversaw the bulk of the excavation of the canal, including appointing Major David du Bose Gaillard to oversee the most daunting project, the Culebra Cut through the roughest terrain on the route. Almost as important as the engineering advances was the healthcare advances made during the construction, led by William C. Gorgas, an expert in controlling tropical diseases such as yellow fever and malaria. Gorgas was one of the first to recognize the role of mosquitoes in the spread of these diseases, and by focusing on controlling the mosquitoes greatly improved worker conditions.\n\nOn 7 January 1914 the French crane boat \"Alexandre La Valley\" became the first to make the traverse, and on 1 April 1914 the construction was officially completed with the hand-over of the project from the construction company to the Canal Zone government. The outbreak of World War I caused the cancellation of any official \"grand opening\" celebration, and the canal officially opened to commercial traffic on 15 August 1914 with the transit of the \"SS Ancon\".\n\nDuring World War II, the canal proved a vital part of the U.S. military strategy, allowing ships to transfer easily between the Atlantic and Pacific. Politically, the Canal remained a territory of the United States until 1977, when the Torrijos–Carter Treaties began the process of transferring territorial control of the Panama Canal Zone to Panama, a process completed on 31 December 1999.\n\nThe Panama Canal continues to be a viable commercial venture and a vital link in world shipping, and continues to be periodically updated and maintained. The Panama Canal expansion project started construction in 2007 and began commercial operation on 26 June 2016. The new locks allow transit of larger Post-Panamax and New Panamax ships, which have a greater cargo capacity than the original locks could accommodate.\n\nThe idea of a canal across Central America was revived during the early 19th century. In 1819, the Spanish government authorized the construction of a canal and the creation of a company to build it.\n\nAlthough the project stalled for some time, a number of surveys were made between 1850 and 1875. They indicated that the two most-favorable routes were across Panama (then part of Colombia) and Nicaragua, with a third route across the Isthmus of Tehuantepec in Mexico another option. The Nicaraguan route was surveyed.\n\nAfter the 1869 completion of the Suez Canal, France thought that an apparently similar project to connect the Atlantic and Pacific Oceans could be carried out with little difficulty. In 1876 an international company, La Société internationale du Canal interocéanique, was created to undertake its construction; two years later, it obtained a concession from the Colombian government (since Panama was a Colombian province) to dig a canal across the isthmus.\n\nFerdinand de Lesseps, who was in charge of the Suez Canal construction, headed the project. His enthusiastic leadership and his reputation as the man who had built the Suez Canal persuaded speculators and ordinary citizens to invest nearly $400 million in the project.\n\nHowever, despite his previous success de Lesseps was not an engineer. The Suez Canal, essentially a ditch dug through a flat, sandy desert, presented few challenges. Although Central America's mountainous spine has a low point in Panama, it is still above sea level at its lowest crossing point. The sea-level canal proposed by de Lesseps would require a great deal of excavation through a variety of rock, rather than Suez' sand.\n\nLess-obvious barriers were the rivers crossing the canal, particularly the Chagres (which flows strongly during the rainy season). Since the water would be a hazard to shipping if it drained into the canal, a sea-level canal would require the river's diversion.\n\nThe most serious problem was tropical disease, particularly malaria and yellow fever, whose methods of transmission were unknown at the time. The legs of hospital beds were placed in cans of water to keep insects from crawling up them, but the stagnant water was an ideal breeding place for mosquitoes (carriers of the diseases).\n\nThe project was plagued by a lack of engineering expertise. In May 1879, an international engineering congress led by de Lesseps convened in Paris. Of its 136 delegates, only 42 were engineers; the others were speculators, politicians and friends of de Lesseps.\n\nHe was convinced that a sea-level canal, dug through the mountainous spine of Central America, could be completed at least as easily as the Suez Canal. The engineering congress estimated the project's cost at $214 million; on February 14, 1880, an engineering commission revised the estimate to $168.6 million. De Lesseps reduced this estimate twice, with no apparent justification: on February 20 to $131.6 million and on March 1 to $120 million. The congress estimated seven or eight years as the time required to complete the canal; de Lesseps reduced this estimate to six years (the Suez Canal required ten).\n\nThe proposed sea-level canal would have a uniform depth of , a bottom width of and a width at water level of about ; the excavation estimate was . A dam was proposed at Gamboa to control flooding of the Chagres River, with channels to drain water away from the canal. However, the Gamboa dam was later found impracticable and the Chagres River problem was left unsolved.\n\nConstruction of the canal began on January 1, 1881, with digging at Culebra beginning on January 22. A large labor force was assembled, numbering about 40,000 in 1888 (nine-tenths of whom were afro-Caribbean workers from the West Indies). Although the project attracted good, well-paid French engineers, retaining them was difficult due to disease. The death toll from 1881 to 1889 was estimated at over 22,000, of whom as many as 5,000 were French citizens.\n\nBy 1885 it had become clear to many that a sea-level canal was impractical, and an elevated canal with locks was preferable; de Lesseps resisted, and a lock canal plan was not adopted until October 1887. By this time increasing mortality rates, as well as financial and engineering problems coupled with frequent floods and mudslides, indicated that the project was in serious trouble. Work continued under the new plan until May 15, 1889, when the company went bankrupt and the project was suspended. After eight years the canal was about two-fifths completed, and about $234.8 million had been spent.\nThe company's collapse was a scandal in France, and the antisemitic Edouard Drumont exploited the role of two Jewish speculators in the affair. One hundred and four legislators were found to have been involved in the corruption, and Jean Jaurès was commissioned by the French parliament to conduct an inquiry which was completed in 1893.\n\nIt soon became clear that the only way to recoup expenses for the stockholders was to continue the project. A new concession was obtained from Colombia, and in 1894 the Compagnie Nouvelle du Canal de Panama was created to finish the canal. To comply with the terms of the contract, work began immediately on the Culebra excavation while a team of engineers began a comprehensive study of the project. They eventually settled on a plan for a two-level, lock-based canal.\n\nThe new effort never gained traction, mainly because of US speculation that a canal through Nicaragua would render one through Panama useless. The most men employed on the new project was 3,600 (in 1896), primarily to comply with the terms of the concession and to maintain the existing excavation and equipment in saleable condition. The company had already begun looking for a buyer, with an asking price of $109 million.\n\nIn the US, a congressional Isthmian Canal Commission was established in 1899 to examine possibilities for a Central American canal and recommend a route. In November 1901, the commission reported that a US canal should be built through Nicaragua unless the French were willing to sell their holdings for $40 million. The recommendation became law on June 28, 1902, and the New Panama Canal Company was compelled to sell at that price.\n\nAlthough the French effort was, to a large extent, doomed to failure from the beginning due to disease and a lack of understanding of the engineering difficulties, it was not entirely futile. The old and new companies excavated of material, of which was taken from the Culebra Cut. The old company dredged a channel from Panama Bay to the port at Balboa, and the channel dredged on the Atlantic side (known as the French canal) was useful for bringing in sand and stone for the locks and spillway concrete at Gatún.\n\nDetailed surveys and studies (particularly those carried out by the new canal company) and machinery, including railroad equipment and vehicles, aided the later American effort. The French lowered the summit of the Culebra Cut along the canal route by five meters (17 ft), from . An estimated of excavation, valued at about $25.4 million, and equipment and surveys valued at about $17.4 million were usable by the Americans.\n\nThe 1848 discovery of gold in California and the rush of would-be miners stimulated US interest in building a canal between the oceans. In 1887, a United States Army Corps of Engineers regiment surveyed canal possibilities in Nicaragua. Two years later, the Maritime Canal Company was asked to begin a canal in the area and chose Nicaragua. The company lost money in the panic of 1893, and its work in Nicaragua ceased. In 1897 and 1899, the United States Congress charged a canal commission with researching possible construction; Nicaragua was chosen as the location both times.\n\nAlthough the Nicaraguan canal proposal was made redundant by the American takeover of the French Panama Canal project, increases in shipping volume and ship sizes have revived interest in the project. A canal across Nicaragua accommodating post-Panamax ships or a rail link carrying containers between ports on either coast have been proposed.\n\nTheodore Roosevelt believed that a US-controlled canal across Central America was a vital strategic interest of the country. This idea gained wide circulation after the destruction of the USS \"Maine\" in Cuba on February 15, 1898. Reversing a Walker Commission decision in favor of a Nicaraguan canal, Roosevelt encouraged the acquisition of the French Panama Canal effort. George S. Morison was the only commission member who argued for the Panama location. The purchase of the French-held land for $40 million was authorized by the June 28, 1902 Spooner Act. Since Panama was then part of Colombia, Roosevelt began negotiating with that country to obtain the necessary rights. In early 1903 the Hay–Herrán Treaty was signed by both nations, but the Senate of Colombia failed to ratify the treaty.\n\nRoosevelt implied to Panamanian rebels that if they revolted, the US Navy would assist their fight for independence. Panama declared its independence on November 3, 1903, and the USS \"Nashville\" impeded Colombian interference. The victorious Panamanians gave the United States control of the Panama Canal Zone on February 23, 1904, for $10 million in accordance with the November 18, 1903 Hay–Bunau-Varilla Treaty.\n\nThe United States took control of the French property connected to the canal on May 4, 1904, when Lieutenant Jatara Oneel of the United States Army was presented with the keys during a small ceremony. The new Panama Canal Zone Control was overseen by the Isthmian Canal Commission (ICC) during construction.\n\nThe first step taken by the US government was to place all the canal workers under the new administration. The operation was maintained at minimum strength to comply with the canal concession and keep the machinery in working order. The US inherited a small workforce and an assortment of buildings, infrastructure and equipment, much of which had been neglected for fifteen years in the humid jungle environment. There were no facilities in place for a large workforce, and the infrastructure was crumbling.\n\nCataloguing assets was a large job; it took many weeks to card-index available equipment. About 2,150 buildings had been acquired, many of which were uninhabitable; housing was an early problem, and the Panama Railway was in a state of decay. However, much equipment (such as locomotives, dredges and other floating equipment) was still serviceable.\nAlthough chief engineer John Findley Wallace was pressured to resume construction, red tape from Washington stifled his efforts to obtain heavy equipment and caused friction between Wallace and the ICC. He and chief sanitary officer William C. Gorgas were frustrated by delay, and Wallace resigned in 1905. He was replaced by John Frank Stevens, who arrived on July 26, 1905. Stevens quickly realized that serious investment in infrastructure was necessary and determined to upgrade the railway, improve sanitation in Panama City and Colón, renovate the old French buildings and build hundreds of new ones for housing. He then began the difficult task of recruiting the large labor force required for construction. Stevens' approach was to press ahead first and obtain approval later. He improved drilling and dirt-removal equipment at the Culebra Cut for greater efficiency, revising the inadequate provisions in place for soil disposal.\n\nNo decision had been made about whether the canal should be a lock or a sea-level one; the ongoing excavation would be useful in either case. In late 1905, President Roosevelt sent a team of engineers to Panama to investigate the relative merits of both types in cost and time. Although the engineers voted eight to five in favor of a sea-level canal, Stevens and the ICC opposed the plan; Stevens' report to Roosevelt was instrumental in convincing the president of the merits of a lock canal and Congress concurred. In November 1906 Roosevelt visited Panama to inspect the canal's progress, the first trip outside the United States by a sitting president.\n\nWhether contract employees or government workers would build the canal was controversial. Bids for the canal's construction were opened in January 1907, and Knoxville, Tennessee-based contractor William J. Oliver was the low bidder. Stevens disliked Oliver, and vehemently opposed his choice. Although Roosevelt initially favored the use of a contractor, he eventually decided that army engineers should carry out the work and appointed Major George Washington Goethals as chief engineer (under Stevens' direction) in February 1907. Stevens, frustrated by government inaction and the army involvement, resigned and was replaced by Goethals.\n\nThe US relied on a stratified workforce to build the canal. High-level engineering jobs, clerical positions, skilled labor and jobs in supporting industries were generally reserved for white Americans, with manual labor primarily by cheap immigrant labor. These jobs were initially filled by Europeans, primarily from Spain, Italy and Greece, many of whom were radical and militant due to political turmoil in Europe. The US then decided to recruit primarily from the British and French West Indies, and these workers provided most of the manual labor on the canal.\n\nThe Canal Zone originally had minimal facilities for entertainment and relaxation for the canal workers apart from saloons; as a result, alcohol abuse was a great problem. The inhospitable conditions resulted in many American workers returning home each year.\n\nA program of improvements was implemented. Clubhouses were built, managed by the YMCA, with billiard, assembly and reading rooms, bowling alleys, darkrooms for camera clubs, gymnastic equipment, ice cream parlors, soda fountains and a circulating library. Member dues were ten dollars a year, with the remaining upkeep (about $7,000 at the larger clubhouses) paid by the ICC. The commission built baseball fields and arranged rail transportation to games; a competitive league soon developed. Semimonthly Saturday-night dances were held at the Hotel Tivoli, which had a spacious ballroom.\n\nThese measures influenced life in the Canal Zone; alcohol abuse fell, with saloon business declining by 60 percent. The number of workers leaving the project each year dropped significantly.\n\nThe work done thus far was preparation, rather than construction. By the time Goethals took over, the construction infrastructure had been created or overhauled and expanded from the French effort and he was soon able to begin construction in earnest.\n\nGoethals divided the project into three divisions: Atlantic, Central and Pacific. The Atlantic Division, under Major William L. Sibert, was responsible for construction of the breakwater at the entrance to Limon Bay, the Gatún locks and their approach channel, and the Gatun Dam. The Pacific Division (under Sydney B. Williamson, the only civilian division head) was responsible for the Pacific entrance to the canal, including a breakwater in Panama Bay, the approach channel, and the Miraflores and Pedro Miguel locks and their associated dams. The Central Division, under Major David du Bose Gaillard, was responsible for everything in between. It had arguably the project's greatest challenge: excavating the Culebra Cut (known as the Gaillard Cut from 1915 to 2000), which involved cutting through the continental divide down to 12 meters (40 ft) above sea level.\n\nBy August 1907, 765,000 m³ (1,000,000 cubic yards) per month was being excavated; this set a record for the rainy season; soon afterwards this doubled, before increasing again. At the peak of production, 2,300,000 m³ (3,000,000 cubic yards) were being excavated per month (the equivalent of digging a Channel Tunnel every 3½ months).\n\nOne of the greatest barriers to a canal was the continental divide, which originally rose to above sea level at its highest point. The effort to cut through this barrier of rock was one of the greatest challenges faced by the project.\n\nGoethals arrived at the canal with Major David du Bose Gaillard of the US Army Corps of Engineers. Gaillard was placed in charge of the canal's Central Division, which stretched from the Pedro Miguel locks to the Gatun Dam, and dedicated himself to getting the Culebra Cut (as it was then known) excavated.\n\nThe scale of the work was massive. Six thousand men worked in the cut, drilling holes in which a total of of dynamite were placed to break up the rock (which was then removed by as many as 160 trains per day). Landslides were frequent, due to the oxidation and weakening of the rock's underlying iron strata. Although the scale of the job and the frequent, unpredictable slides generated chaos, Gaillard provided quiet, clear-sighted leadership.\n\nOn May 20, 1913, Bucyrus steam shovels made a passage through the Culebra Cut at the level of the canal bottom. The French effort had reduced the summit to over a relatively narrow width; the Americans had lowered this to above sea level over a greater width, and had excavated over of material. About of this material in addition to the planned excavation, in the form of landslides. Dry excavation ended on September 10, 1913; a January slide had added of earth, but it was decided that this loose material would be removed by dredging when the cut was flooded.\n\nTwo artificial lakes are key parts of the canal: Gatun and Miraflores Lakes. Four dams were constructed to create them. Two small dams at Miraflores impound Miraflores Lake, and a dam at Pedro Miguel encloses the south end of the Culebra Cut (essentially an arm of Lake Gatun). The Gatun Dam is the main dam blocking the original course of the Chagres River, creating Gatun Lake.\n\nThe Miraflores dams are an earth dam connecting the Miraflores Locks in the west and a concrete spillway dam east of the locks. The concrete dam has eight floodgates, similar to those on the Gatun spillway. The earthen, Pedro Miguel dam extends from a hill in the west to the lock. Its face is protected by rock riprap at the water level. The largest and most challenging of the dams is the Gatun Dam. This earthen dam, thick at the base and long along the top, was the largest of its kind in the world when the canal opened.\n\nThe original lock canal plan called for a two-step set of locks at Sosa Hill and a long Sosa Lake extending to Pedro Miguel. In late 1907, it was decided to move the Sosa Hill locks further inland to Miraflores, mostly because the new site provided a more stable construction foundation. The resulting small lake Miraflores became a fresh water supply for Panama City.\n\nBuilding the locks began with the first concrete laid at Gatun on August 24, 1909. The Gatun locks are built into a cutting into a hill bordering the lake, requiring the excavation of of material (mostly rock). The locks were made of of concrete, with an extensive system of electric railways and aerial lifts transporting concrete to the lock-construction sites.\n\nThe Pacific-side locks were finished first: the single flight at Pedro Miguel in 1911, and Miraflores in May 1913. The seagoing tugboat \"Gatun\", an Atlantic-entrance tug used to haul barges, traversed the Gatun locks on September 26, 1913. The trip was successful, although the valves were controlled manually; the central control board was not yet ready.\n\nOn October 10, 1913, the dike at Gamboa which had kept the Culebra Cut isolated from Gatun Lake was demolished; the detonation was made telegraphically by President Woodrow Wilson in Washington. On January 7, 1914, the \"Alexandre La Valley\", an old French crane boat, became the first ship to make a complete transit of the Panama Canal under its own steam after working its way across during the final stages of construction.\n\nAs construction wound down, the canal team began to disperse. Thousands of workers were laid off, and entire towns were disassembled or demolished. Chief sanitary officer William C. Gorgas, who left to fight pneumonia in the South African gold mines, became surgeon general of the Army. On April 1, 1914 the Isthmian Canal Commission disbanded, and the zone was governed by a Canal Zone Governor; the first governor was George Washington Goethals.\n\nAlthough a large celebration was planned for the canal's opening, the outbreak of World War I forced the cancellation of the main festivities and it became a modest local affair. The Panama Railway steamship , piloted by Captain John A. Constantine (the canal's first pilot), made the first official transit on August 15, 1914. With no international dignitaries in attendance, Goethals followed the \"Ancon\" progress by railroad.\n\nThe canal was a technological marvel and an important strategic and economic asset to the US. It changed world shipping patterns, removing the need for ships to navigate the Drake Passage and Cape Horn. The canal saves a total of about on a sea trip from New York to San Francisco.\n\nIts anticipated military significance of the canal was proven during World War II, when the canal helped restore the devastated United States Pacific Fleet. Some of the largest ships the United States had to send through the canal were aircraft carriers, particularly \"Essex\" class; they were so large that although the locks could accommodate them, the lampposts along the canal had to be removed.\n\nThe Panama Canal cost the United States about $375 million, including $10 million paid to Panama and $40 million paid to the French company. Although it was the most expensive construction project in US history to that time, it cost about $23 million less than the 1907 estimate despite landslides and an increase in the canal's width. An additional $12 million was spent on fortifications.\nA total of over 75,000 people worked on the project; at the peak of construction, there were 40,000 workers. According to hospital records, 5,609 workers died from disease and accidents during the American construction era.\n\nA total of of material was excavated in the American effort, including the approach channels at the canal ends. Adding the work by the French, the total excavation was about (over 25 times the volume excavated in the Channel Tunnel project).\n\nOf the three presidents whose terms spanned the construction period, Theodore Roosevelt is most associated with the canal and Woodrow Wilson presided over its opening. However, William Howard Taft may have given the canal its greatest impetus for the longest time. Taft visited Panama five times as Roosevelt's secretary of war and twice as president. He hired John Stevens and later recommended Goethals as Stevens' replacement. Taft became president in 1909, when the canal was half finished, and was in office for most of the remainder of the work. However, Goethals later wrote: \"The real builder of the Panama Canal was Theodore Roosevelt\".\n\nThe following words by Roosevelt are displayed in the rotunda of the canal's administration building in Balboa:\n\nIt is not the critic who counts, not the man who points out how the strong man stumbled, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena; whose face is marred by dust and sweat and blood; who strives valiantly, who errs and comes short again and again; who knows the great enthusiasms, the great devotions, and spends himself in a worthy cause; who, at the best, knows in the end the triumph of high achievement; and who, at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who know neither victory nor defeat.\n\nDavid du Bose Gaillard died of a brain tumor in Baltimore on December 5, 1913, at age 54. Promoted to colonel only a month earlier, Gaillard never saw the opening of the canal whose creation he directed. The Culebra Cut (as it was originally known) was renamed the Gaillard Cut on April 27, 1915, in his honor. A plaque commemorating Gaillard's work stood over the cut for many years; in 1998 it was moved to the administration building, near a memorial to Goethals.\n\nAs the situation in Europe deteriorated during the late 1930s, the US again became concerned about its ability to move warships between the oceans. The largest US battleships already had problems with the canal locks, and there were concerns about the locks being incapacitated by bombing.\n\nThese concerns led Congress to pass a resolution on May 1, 1936, authorizing a study of improving the canal's defenses against attack and expanding its capacity to handle large vessels. A special engineering section was created on July 3, 1937, to carry out the study.\nThe section reported to Congress on February 24, 1939, recommending work to protect the existing locks and the construction of a new set of locks capable of carrying larger vessels than the existing locks could accommodate. On August 11, Congress authorized the work.\n\nThree new locks were planned, at Gatún, Pedro Miguel and Miraflores, parallel to the existing locks with new approach channels. The new locks would add a traffic lane to the canal, with each chamber long, wide and deep. They would be east of the existing Gatún locks and west of the Pedro Miguel and Miraflores locks.\n\nThe first excavations for the new approach channels at Miraflores began on July 1, 1940, following the passage by Congress of an appropriations bill on June 24, 1940. The first dry excavation at Gatún began on February 19, 1941. Considerable material was excavated before the project was abandoned, and the approach channels can still be seen paralleling the original channels at Gatún and Miraflores.\n\nIn 2006, the Autoridad del Canal de Panamá (the Panama Canal Authority, or ACP) proposed a plan creating a third lane of locks using part of the abandoned 1940s approach canals. Following a referendum, work began in 2007 and the expanded canal began commercial operations on June 26, 2016. After a two-year delay, the new locks allow the transit of Panamax ships (which have a greater cargo capacity than the original locks can handle). The first ship to cross the canal through the third set of locks was a Panamax container ship, the Chinese-owned \"Cosco Shipping Panama\". The cost of the expansion was estimated at $5.25 billion.\n\nAfter construction, the canal and the Canal Zone surrounding it were administered by the United States. On September 7, 1977, US President Jimmy Carter signed the Torrijos-Carter Treaty setting in motion the process of transferring control of the canal to Panama. The treaty became effective on October 1, 1979, providing for a 20-year period in which Panama would have increasing responsibility for canal operations before complete US withdrawal on December 31, 1999. Since then, the canal has been administered by the Panama Canal Authority (Autoridad de Canal de Panama, or ACP).\n\nThe treaty was controversial in the US, and its passage was difficult. The controversy was largely generated by contracts to manage two ports, at either end of the canal, which were awarded by Panama to Hong Kong-based conglomerate Hutchison Whampoa. According to US Republicans, the company has close ties to the Chinese government and the Chinese military. However, the United States Department of State said that it found no evidence of connections between Hutchison Whampoa and Beijing. Some Americans were wary of placing the strategic waterway under the protection of Panamanian security forces.\nAlthough concerns existed in the US and the shipping industry about the canal after the transfer, Panama has exercised good stewardship. According to ACP figures, canal income increased from $769 million in 2000 (the first year under Panamanian control) to $1.4 billion in 2006. Traffic through the canal increased from 230 million tons in 2000 to nearly 300 million tons in 2006. The number of accidents has decreased from an average of 28 per year in the late 1990s to 12 in 2005. Transit time through the canal averages about 30 hours, about the same as during the late 1990s. Canal expenses have increased less than revenue, from $427 million in 2000 to $497 million in 2006. On October 22, 2006, Panamanian citizens approved a referendum to expand the canal.\n\nFormer US Ambassador to Panama Linda Watt, who served from 2002 to 2005, said that the canal operation in Panamanian hands has been \"outstanding\". \"The international shipping community is quite pleased\", Watt added.\n\n\n\n\n\n"}
{"id": "15426416", "url": "https://en.wikipedia.org/wiki?curid=15426416", "title": "IEEE 1547", "text": "IEEE 1547\n\nIEEE 1547 (\"Standard for Interconnecting Distributed Resources with Electric Power Systems\") is a standard of the Institute of Electrical and Electronics Engineers meant to provide a set of criteria and requirements for the interconnection of distributed generation resources into the power grid.\n\n\"This document provides a uniform standard for interconnection of distributed resources with EPSs [Electric Power Systems]. It provides requirements relevant to the performance, operation, testing, safety, and maintenance of the interconnection.\"\n\nWith the increased adoption of distributed resources in the present and future, a set of standards regarding their usage in the grid becomes increasingly important for the overall reliability, safety, and cost. Furthermore, the lack of a concrete national standard was seen as a roadblock to the implementation of new distributed generation projects. The standard is intended to be universally adoptable, technology-neutral, and cover distributed resources as large 10 MVA.\n\nIn early 1999, the IEEE approved the undertaking of P1547. With the support of the United States Department of Energy, the project was initiated on a fast-track basis, which would halve the usual development time. The draft standard was revised 10 times before P1547/Draft 11 was approved with a 91% vote in February 2003. It was then approved by the IEEE Standards Board on June 12, 2003, and received an ANSI designation on October 20, 2003.\n\nIEEE 1547-2003 has a tight underfrequency protection setting of 59.3 Hz which poses a risk for grid stability. In case of an underfrequency situation, \ne.g. after a major loss of generation, the situation may get worse when a multitude of distributed energy resources (DER) disconnect simultaneously. IEEE 1547-2003 demands also an obligatory overfrequency disconnection at 60.5 Hz. With a rising share of distributed generation there is a possibility of triggering a non-linear oscillator in the multi GW range within the transmission grid. In Europe, this problem with similar standards has already been addressed by ENTSO-E.\n\n\"Interconnection services shall be offered based upon the standards developed by the Institute of Electrical and Electronics Engineers: IEEE Standard 1547 for Interconnecting Distributed Resources With Electric Power Systems, as they may be amended from time to time.\"\n\nThe Energy Policy Act of 2005 established IEEE 1547 as the national standard for the interconnection of distributed generation resources in the United States of America.\n\nCurrently, there are six complementary standards designed to expand upon or clarify the initial standard, two of which are published, and the other four still in the draft phase.\n\n\n"}
{"id": "29850583", "url": "https://en.wikipedia.org/wiki?curid=29850583", "title": "Impella", "text": "Impella\n\nImpella is a family of medical devices used for temporary ventricular support device in people with depressed heart function. Some versions of the device can provide right heart support during other forms of mechanical circulatory support including ECMO and Centrimag.\n\nThe device is approved for use in high-risk percutaneous coronary intervention (PCI) and cardiogenic shock following a heart attack or an open heart surgery procedure. It is placed into the heart through a peripheral artery. Once in place, it pumps blood for the left or right side of the heart, pumping blood into the ascending aorta or pulmonary artery, respectively.\n\nIt results in similar results to veno-arterial extracorporeal life support and TandemHeart.\n\nThe Impella pump is in 7 sets of practice guidelines:\n\nImpella heart pumps are percutaneous microaxial pumps that act as mechanical circulatory support (MCS) devices in patients in need of hemodynamic support. The pumps are mounted on a 9 Fr support catheter. The pumps are inserted by a physician typically through femoral artery access, although axillary and subclavian artery approaches are not uncommon.\n\nThe Impella heart pumps are designed to provide hemodynamic support when the patient's heart is unable to produce sufficient cardiac output. The Impella family of heart pumps can supply 1.0 L/min-5.0 L/min of blood flow, depending on the size and power setting of the individual pump. The physiological consequences of left-sided support are threefold. First, it unloads the left ventricle by reducing left ventricular end-diastolic volume and pressure, which decreases ventricular wall stress. Subsequently, left ventricular work and myocardial oxygen demand decrease. Second, Impella pump flow increases mean arterial pressure, diastolic pressure, and cardiac output. This increases cardiac power output and cardiac index. The combined effects of the Impella on wall stress and perfusion pressure (especially diastolic pressure) augment coronary perfusion. Lastly, augmented cardiac output and forward flow from the left ventricle has secondary benefits related to right ventricular function. Impella support decreases pulmonary capillary wedge pressure and reduces right ventricular afterload.\n\nIn June 2008, the Impella 2.5 heart pump received FDA 510(k) clearance for partial circulatory support for periods of up to 6 hours during cardiac procedures not requiring cardiopulmonary bypass. In March 2015, the Impella 2.5 heart pump received FDA Premarket Approval (PMA) for elective and urgent high-risk PCI procedures, and in December 2016, the FDA PMA was expanded to include the Impella CP heart pump. In April 2009, the Impella 5.0 and Impella LD heart pumps received FDA 510(k) clearance for circulatory support for periods of up to 6 hours during cardiac procedures not requiring cardiopulmonary bypass. In July 2010, the Automated Impella Controller received FDA 510(k) clearance for intended use by trained healthcare professionals in healthcare facilities and medical transport (ie, ambulance, helicopter, or fixed-wing aircraft) environments.\n\nIn January 2015, the Impella RP was granted a humanitarian device exemption (HDE) to provide circulatory assistance for people who develop right heart failure.\n\n"}
{"id": "28426979", "url": "https://en.wikipedia.org/wiki?curid=28426979", "title": "Inerter (mechanical networks)", "text": "Inerter (mechanical networks)\n\nIn the study of mechanical networks in control theory, an inerter is a two-terminal device in which the forces applied at the terminals are equal, opposite, and proportional to relative acceleration between the nodes. Under the name of J-damper the concept has been used in Formula 1 racing car suspension systems.\n\nIt can be constructed with a flywheel mounted on a rack and pinion. It has a similar effect to increasing the inertia of the sprung object.\n\nMalcolm C. Smith, a control engineering professor at the University of Cambridge, first introduced inerters in a 2002 paper. Smith extended the analogy between electrical and mechanical networks (the mobility analogy). He observed that the analogy was incomplete, since it was missing a mechanical device playing the same role as an electrical capacitor. It was found that it is possible to construct such a device using gears and flywheels.\n\nThe generated strength satisfies the equation\nfor a suitable constant \"b\".\n\nA linear inerter can be constructed by meshing a flywheel with a rack gear. The pivot of the flywheel forms one terminal of the device, and the rack gear forms the other.\n\nA rotational inerter can be constructed by meshing a flywheel with the ring gear of a differential. The side gears of the differential form the two terminals.\n\nShortly after its discovery, the inerter principle was used under the name of J-damper in the suspension systems of Formula 1 racing cars. When tuned to the natural oscillation frequencies of the tires, the inerter reduced the mechanical load on the suspension. McLaren Mercedes began using a J-damper in early 2005, and Renault shortly thereafter.\n\nJ-dampers were at the center of the 2007 Formula One espionage controversy which arose when Phil Mackereth left McLaren for Renault.\n\n"}
{"id": "229694", "url": "https://en.wikipedia.org/wiki?curid=229694", "title": "Intercooler", "text": "Intercooler\n\nAn intercooler is any mechanical device used to cool a fluid, including liquids or gases, between stages of a multi-stage compression process, typically a heat exchanger that removes waste heat in a gas compressor. They are used in many ways, including air compressors, air conditioners, refrigeration, and gas turbines, and automotive engines. Here they are widely known as an air-to-air or air-to-liquid cooler for forced induction (turbocharged or supercharged) internal combustion engines to improve their volumetric efficiency, which they do by increasing intake air density through nearly constant pressure cooling.\n\nIntercoolers are utilized to remove the waste heat from the first stage of two-stage air compressors. Two-stage air compressors are manufactured because of their inherent efficiency. The cooling action of the intercooler is principally responsible for this higher efficiency, bringing it closer to Carnot efficiency. Removing the heat-of-compression from the discharge of the first stage has the effect of densifying the air charge. This, in turn, allows the second stage to produce more work from its fixed compression ratio. Adding an intercooler to the setup requires additional investments. \nIntercoolers increase the efficiency of the induction system by reducing induction air heat created by the supercharger or turbocharger and promoting more thorough combustion. This removes the heat of compression (i.e., the temperature rise) that occurs in any gas when its pressure is raised (i.e. its unit mass per unit volume - density - is increased).\n\nA decrease in intake air charge temperature sustains use of a more dense intake charge into the engine, as a result of forced induction. The lowering of the intake charge air temperature also eliminates the danger of pre-detonation (knock) of the fuel/air charge prior to timed spark ignition. This preserves the benefits of more fuel/air burn per engine cycle, increasing the output of the engine.\n\nIntercoolers also eliminate the need for using the wasteful method of lowering intake charge temperature by the injection of excess fuel into the cylinders' air induction chambers, to cool the intake air charge, prior to its flowing into the cylinders. This wasteful practice (before intercoolers were used) nearly eliminated the gain in engine efficiency from forced induction, but was necessitated by the greater need to prevent at all costs the engine damage that pre-detonation engine knocking causes.\n\nThe \"inter\" prefix in the device name originates from its use as a cooler in between compression cycles. Typically in automobiles the intercooler is placed between the turbocharger (or supercharger) and the engine (the piston compression produces the next compression cycle). Aircraft engines are sometimes built with charge air coolers that were installed between multiple stages of forced induction, thus the designation of \"inter\". In a vehicle fitted with two-stage turbocharging, it is possible to have both an intercooler (between the two turbocharger units) and an aftercooler (between the second-stage turbo and the engine). The JCB Dieselmax land speed record-holding car is an example of such a system. In general, an intercooler or aftercooler is said to be a charge-air cooler.\n\nIntercoolers can vary dramatically in size, shape and design, depending on the performance and space requirements of the entire supercharger system. Common spatial designs are front mounted intercoolers (FMIC), top mounted intercoolers (TMIC) and hybrid mount intercoolers (HMIC). Each type can be cooled with an air-to-air system, air-to-liquid system, or a combination of both.\n\nTurbochargers and superchargers are engineered to force more air mass (and thus more oxygen molecules) into an engine's intake manifold and combustion chamber. Intercooling is a method used to compensate for heating caused by supercharging, a natural byproduct of the semi-adiabatic compression process. Increased air pressure can result in an excessively hot intake charge, significantly reducing the performance gains of supercharging due to decreased density. Increased intake charge temperature can also increase the cylinder combustion temperature, causing detonation, excessive wear, or heat damage to an engine block or pistons.\n\nPassing a compressed and heated intake charge through an intercooler reduces its temperature (due to heat rejection) and pressure (due to flow restriction of fins). If the device is properly engineered, the relative decrease in temperature is greater than the relative loss in pressure, resulting in a net increase in density. This increases system performance by recovering some losses of the inefficient compression process by rejecting heat to the atmosphere. Additional cooling can be provided by externally spraying a fine mist onto the intercooler surface, or even into the intake air itself, to further reduce intake charge temperature through evaporative cooling.\n\nIntercoolers that exchange their heat directly with the atmosphere are designed to be mounted in areas of an automobile with maximum air flow. These types are mainly mounted in front mounted systems (FMIC). Cars such as the Nissan Skyline, Saab, Volvo 200 Series Turbo, Volvo 700 Series (and 900 series) turbo, Dodge SRT-4, 1st gen Mazda MX-6, Mitsubishi Lancer Evolution and Chevrolet Cobalt SS all use front mounted intercooler(s) mounted near the front bumper, in line with the car's radiator.\n\nMany other turbo-charged cars, particularly where the aesthetics of the car are not to be compromised by top mount scoops, such as the Toyota Supra (JZA80 only), Nissan 300ZX Twin Turbo, Nissan Silvia (S13/14/14a/15), Nissan 180sx, Mitsubishi 3000gt, Saab 900, Volkswagen, Fiat Turbo diesels, Audi TT, and Turbo Mitsubishi Eclipse use side-mounted air-to-air intercoolers (SMIC), which are mounted in the front corner of the bumper or in front of one of the wheels. Side-mounted intercoolers are generally smaller, mainly due to space constraints, and sometimes two are used to gain the performance of a larger, single intercooler. Cars such as the Subaru Impreza WRX, MINI Cooper S, Toyota Celica GT-Four, Nissan Pulsar GTI-R, Acura RDX, Mazdaspeed3, Mazdaspeed6, and the PSA Peugeot Citroën turbo diesels, use air-to-air top mounted intercoolers (TMIC) located on top of the engine. Air is directed through the intercooler through the use of a hood scoop. In the case of the PSA cars, the air flows through the grille above the front bumper, then through under-hood ducting. Top mounted intercoolers sometimes suffer from heat diffusion due to proximity with the engine, warming them and reducing their overall efficiency. Some World Rally Championship cars use a reverse-induction system design whereby air is forced through ducts in the front bumper to a horizontally mounted intercooler.\n\nBecause FMIC systems require open bumper design for optimal performance, the entire system is vulnerable to debris. Some engineers choose other mount locations due to this reliability concern. FMICs can be located in front of or behind the radiator, depending on the heat dissipation needs of the engine.\n\nAs well as allowing a greater mass of air to be admitted to an engine, intercoolers have a key role in controlling the internal temperatures in a turbocharged engine. When fitted with a turbo (as with any form of supercharging), the engine's specific power is increased, leading to higher combustion and exhaust temperatures. The exhaust gases passing through the turbine section of the turbocharger are usually around 450 °C (840 °F), but can be as high as 1000 °C (1830 °F) under extreme conditions. This heat passes through the turbocharger unit and contributes to the heating of the air being compressed in the compressor section of the turbo. If left uncooled, this hot air enters the engine, further increasing internal temperatures. This leads to a build-up of heat that will eventually stabilise, but this may be at temperatures in excess of the engine's design limits- 'hot spots' at the piston crown or exhaust valve can cause warping or cracking of these components. High air charge temperatures will also increase the possibility of pre-ignition or detonation. Detonation causes damaging pressure spikes in the engine's cylinders, which can quickly damage an engine. These effects are especially found in modified or tuned engines running at very high specific power outputs. An efficient intercooler removes heat from the air in the induction system, preventing the cyclic heat build-up via the turbocharger, allowing higher power outputs to be achieved without damage.\n\nCompression by the turbocharger causes the intake air to heat up and heat is added due to compressor inefficiencies (adiabatic efficiency). This is actually the greater cause of the increase in air temperature in an air charge. The extra power obtained from forced induction is due to the extra air available to burn more fuel in each cylinder. This sometimes requires a lower compression ratio be used, to allow a wider mapping of ignition timing advance before detonation occurs (for a given fuel's octane rating). On the other hand, a lower compression ratio generally lowers combustion efficiency and costs power.\n\nSome high performance tuning companies measure the temperature before and after the intercooler to ensure the output temperature is as close to ambient as possible (without additional cooling; water/liquid gas spray kits).\n\nAir-to-liquid intercoolers, also known as Charge Air Coolers, are heat exchangers that transfer intake charge heat to an intermediate fluid, usually water, which finally rejects heat to the air. These systems use radiators in other locations, usually due to space constraints, to reject unwanted heat, similar to an automotive radiator cooling system. Air-to-liquid intercoolers are usually heavier than their air-to-air counterparts due to additional components making up the system (water circulation pump, radiator, fluid, and plumbing). The Toyota Celica GT-Four had this system from 1988 to 1989, 1994 to 1999, also in the Carlos Sainz Rally Championship Version from 1990 to 1993. The 1989-1993 Subaru Legacy with the 2.0 L DOHC flat-4 engine also used a top installed air-to-water intercooler on the GT and RS models sold in Japan, Europe, and Australia.\n\nA big advantage of the air-to-liquid setup is the lower overall pipe and intercooler length, which offers faster response (lowers turbo lag) , giving peak boost faster than most front-mount intercooler setups. Some setups have reservoirs that can hold ice, producing intake temperatures lower than ambient air, giving a big advantage (but of course, ice would need constant replacement).\n\nFord had adopted the technology when they decided to use forced induction (via Supercharger) on their Mustang Cobra and Ford Lightning truck platforms. It uses a water/glycol mixture intercooler inside the intake manifold, just under the supercharger, and has a long heat exchanger front mounted, all powered by a Bosch pump made for Ford. Ford still uses this technology today with their Shelby GT500. The 2005-2007 Chevrolet Cobalt SS Supercharged also utilizes a similar setup.\n\nAir-to-liquid intercoolers are by far the most common form of intercooler found on marine engines, given that a limitless supply of cooling water is available and most engines are located in closed compartments where obtaining a good flow of cooling air for an air-to-air unit would be difficult. Marine intercoolers take the form of a tubular heat exchanger with the air passing through a series of tubes and cooling water circulating around the tubes within the unit's casing. The source of water for the intercooler depends on the exact cooling system fitted to the engine. Most marine engines have fresh water circulating within them which is passed through a heat exchanger cooled by sea water. In such a system, the intercooler will be attached to the sea water circuit and placed before the engine's own heat exchanger to ensure a supply of cool water.\n\nA charge air cooler is used to cool engine air after it has passed through a turbocharger, but before it enters the engine. The idea is to return the air to a lower temperature, for the optimum power for the combustion process within the engine.\n\nCharge air coolers range in size depending on the engine. The smallest are most often referred to as intercoolers and are attached to automobile engines or truck engines. The largest are reserved for use on huge marine diesel engines, and can weigh over 2 tonnes (see picture).\n\nMarine diesel engine charge-air coolers are still manufactured in Europe, despite the very largest engines mostly being built in the Far East. Vestas aircoil A/S and GEA are the oldest makers still in business.\n\nThe first marine diesel engine charge air cooler was built by Vestas aircoil A/S in 1956.\n\nThere is some confusion in terminology between aftercooler, intercooler, and charge-air cooler. In the past, aircraft engines would run turbochargers in stages, where the first stage compressor would feed the inlet of the second stage compressor that would further compress the air before it enters the engine. Due to the extremely high pressures that would develop, an air cooler was positioned between the first and second stage compressors. That cooler was the \"Intercooler\".\n\nAnother cooler would be positioned after the second stage, which was the final compressor stage, and that was the \"aftercooler\". An aftercooler was the cooler whose outlet fed the engine.\n\nA charge-air cooler is simply an all-encompassing term, meaning that it cools the turbo's air charge before it is routed into the engine. Usually a charge-air cooler means an air-to-air cooler where the heat is rejected using ambient air flowing through the heat exchanger, much like the engine's coolant radiator. While the multi-stage turbocharger systems are still in use in some tractor pull classes, selected high-performance diesels, and are also being used on newer late model commercial diesels, the term intercooler and aftercooler are used synonymously today. The term intercooler is widely used to mean in-between the Turbocharger and the engine. Both terms, intercooler or aftercooler, are correct, but this is the origin of the two terms that are used interchangeably by all levels of experts.\n\nAn intercooler, or \"Charge-Air Cooler\", is an air-to-air or air-to-liquid heat exchange device used on turbocharged and supercharged (forced induction) internal combustion engines to improve their volumetric efficiency by increasing intake air-charge density through isochoric cooling. A decrease in air intake temperature provides a denser intake charge to the engine and allows more air and fuel to be combusted per engine cycle, increasing the output of the engine.\n\nThe inter prefix in the device name originates from historic compressor designs. In the past, aircraft engines were built with Charge-Air Coolers that were installed between multiple stages of supercharging, thus the designation of inter. Modern automobile designs are technically designated aftercoolers because of their placement at the end of supercharging chain. This term is now considered archaic in modern automobile terminology since most forced induction vehicles have single-stage superchargers or turbochargers. In a vehicle fitted with two-stage turbocharging, it is possible to have both an intercooler (between the two turbocharger units) and an aftercooler (between the second-stage turbo and the engine). In general, an intercooler or aftercooler is said to be a Charge-Air Cooler. Text taken from Av-Tekk Charge-Air Coolers website\n"}
{"id": "32261703", "url": "https://en.wikipedia.org/wiki?curid=32261703", "title": "Ip.access", "text": "Ip.access\n\nip.access Limited is a multinational corporation that designs, manufactures, and markets small cells (picocell and femtocell) technologies and infrastructure equipment for GSM, GPRS, EDGE, 3G and LTE. The firm has headquarters in Cambourne, England, the company also maintains operations and offices in Bellevue, United States, and Gurgaon and Pune, India.\n\nip.access combines IP and cellular technologies together to provide 2G, 3G and LTE coverage and for mobile networks .Using satellite backhaul, its products provide coverage to commercial passenger aircraft, ships, and users in remote rural areas.\n\nThe firm is a member of 3GPP, the Cambridge Network, European Telecommunications Standards Institute (ETSI), Small Cell Forum, and an associate member of the Network Vendors Interoperability Testing (NVIOT) Forum.\n\nip.access was founded in December 1999 as a wholly owned subsidiary of TTP Group PLC aimed at developing technologies that would allow multiple radio access technologies to communicate over the Internet. To accommodate its growing staff, in 2006 ip.access relocated to new offices in Cambourne Business Park, Cambridge, where it remains.\n\nIn October 2000, TTP Group spun off its communications division (TTP Communications, or TTPCom) in an initial public offering on the London Stock Exchange, and ip.access joined the spin-off as a wholly owned subsidiary of the TTPCom group.\n\nIn March 2006, the company secured an £8.5 million round of funding from Intel Capital, Scottish Equity Partners, and Rothschild & Cie Banque. As part of its June 2006 acquisition of TTP Communications, Motorola also gained a stake in ip.access. In 2007, after signing an OEM agreement with ip.access, ADC (now part of Tyco Electronics) made a minority interest investment in the company. Followed by, both Cisco Systems and Qualcomm making strategic financial investments in the company in 2008.\n\nIn July 2007, the firm became a founding member of the Femto Forum, renamed Small Cell Forum in February 2012. ip.access was named in The Sunday Times Fast Tech Track 100 in both 2007 and 2008. The company was also cited as the number one picocell vendor by global market intelligence company, ABI Research in 2008.\n\nIn 2009, ip.access was named in the Deloitte Technology Fast 500 EMEA. In April 2009, the company announced its Oyster 3G product would support femtocell standards published by 3GPP and the Broadband Forum. In March 2010, the company took part in the first Plugfest, organized by ETSI as part of its Plugtests program, held to demonstrate the effectiveness of the 3GPP femtocell standards in supporting interoperability between femtocell access points and network equipment from different vendors.\n\nIn June 2011, the market research and analysis firm Infonetics named ip.access along with its partner Cisco Systems, as the leading supplier of 3G femtocells. In August 2011, ip.access announced it had made more than 500,000 installations of its 3G technologies. In February 2013, ip.access announced it had become the first 3G small cell provider to ship one million residential units. In the same month, ip.access and iDirect completed successful interopability test of 3G small cells over IP Satellite.\n\nIn February 2014, ip.access launched a new range of small cells called presenceCell, which unlike traditional small cells, do not rely on providing indoor coverage and capacity to deliver a return on investment. Rather, the ultra-compact base stations are designed to capture anonymous user location and phone identity information from smartphones, which can be analysed and packaged as a service for a variety of businesses.\n\nThe telecommunications firms AT&T uses Oyster 3G as the core femtocell technology for its 3G MicroCell product. Cisco Systems, has jointly developed a femtocell solution with ip.access in compliance with the Broadband Forum's TR-069 technical specification.\n\nIn 2002, ip.access introduced the world’s first IP basestation controller for indoor GSM networks. nanoGSM uses 2G picocells that leverage the standard GSM air interface, full IP-based BSC, and an OMC-R management system that delivers voice, messaging and data to both 2G and 3G handsets at an indoor range of up to 200m.\n\nnano3G is an end-to-end Femtocell system with access points for Enterprise, E-class [E8, E16 and E24] and Small Medium Business, S-class [S8], access controller and element management system, providing carrier-class coverage to commercial and consumer users.\n\nLaunched at the 2007 3GSM World Congress in Barcelona, Spain, the Oyster 3G is ip.access' core 3G femtocell technology used by system integrators and OEM customers to integrate WCDMA femtocells into home gateways, set-top boxes, and other devices. ip.access' Oyster 3G is the core technology of AT&T's 3G MicroCell\n\nnanoLTE [E-40, E-100] is an Enterprise grade platform that brings LTE capacity both in-doors and in public spaces, while also offering the option of providing extra 3G infill and Circuit Switch Fall Back (CSFB) capacity.\n\nLaunched in 2014, the presenceCell is a new range of small cell, designed to capture precise user location data via their smart phone, which can be analysed and packaged as a service for a variety of businesses.\nIn addition to the presenceCell, ip.access also provides the back-end processing and management system that delivers the Presence data anonymously and securely to vertical application providers. The company’s Network Orchestration System serves as the infrastructure management solution and also supports the GSMA’s OneAPI standard, which allows third parties to provide value-added services through web friendly message interfaces. The presenceCell was commercially deployed by Vodafone Turkey in 2015.\n\nAmong ip.access' major customers are AT&T, Bharti Airtel, Blue Ocean Wireless, Bouygues Telecom,Jersey Telecom Monaco Telecom, SFR, SPIE SA, T-Mobile, Tele2, Telefónica O2 Czech Republic,Telia Sonera, Vivacom and Vodafone\n\nThe company's technology partners include AeroMobile, Altobridge, Blue Ocean Wireless, Cisco Systems, Private Mobile Networks, Qualcomm, Quortus, Setcom, and TriaGnoSys.\n\nCorporate, product, and personnel awards won by ip.access include the following:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1904144", "url": "https://en.wikipedia.org/wiki?curid=1904144", "title": "Kate Gleason", "text": "Kate Gleason\n\nCatherine Anselm Gleason (November 25, 1865 – January 9, 1933) was an American engineer and businesswoman known both for being an accomplished woman in the predominantly male field of engineering and for her philanthropy.\n\nCatherine Anselm Gleason was born the first of four children of William and Ellen McDermott Gleason of Rochester, New York, émigrés from Ireland. Her father was the owner of a machine tool company, later named Gleason Works, which later became one of the most important makers of gear-cutting machine tools in the world. When she was 11, her stepbrother Tom died of typhoid fever, causing hardship at her father's company because Tom had been an important helper. At the age of 12 she began working for her father to fill the void left by Tom's death. In 1884, she was the first woman to be admitted to study engineering at Cornell University in Ithaca, NY , although she was unable to complete her studies at Cornell due to her required presence in the factory. She continued her studies upon returning to Rochester at the Mechanics Institute, later renamed Rochester Institute of Technology. She was actively involved as the treasurer as well as saleswoman for Gleason Works. In 1893, she toured Europe to expand the company's business, one of the first times an American manufacturer tried to globalize their business. Today, international sales make up almost 3/4 of the company's business. \n\nFred H. Colvin described Kate Gleason in his memoirs as \"a kind of Madame Curie of machine tools […] Kate spent her youth learning her father's business from the ground up, both in the shop and in the field, so that when she branched out for herself about 1895 as a saleswoman for her father's gear-cutting machines, she knew as much as any man in the business.\"\n\nEllen Gleason was a friend of fellow Rochesterian Susan B. Anthony, and Kate Gleason later cited Anthony as a source of advice. Gleason undertook several efforts supporting Women's Suffrage after Anthony's death.\n\nDue to conflicts with her family she left Gleason Works in 1913 and found work at the Ingle Machining Company.She Joined Ingle Machine Company on the 1st of January 1914. She was appointed the receiver of bankruptcy for the company, the first women to ever do so. Under her guidance she restored the company and repaid their outstanding debts. The company was returned to the stockholders before the end of 1915.After her endeavors with Ingle Machine Company she turned her attention to East Rochester where she helped to finance and build 8 factories for various companies. In 1914, she was the first woman elected to full membership in the American Society of Mechanical Engineers and represented the society at the World Power Conference in Germany. In 1918, she was appointed the president of First National Bank of East Rochester while the previous President was enlisted in World War I. During this period she took charge of a problem loan and used it to finish the housing complexes left from the previous loan holder. She used this to further her humanitarian efforts in Rochester, starting eight companies, including a construction company that built houses for the middle class. After this success she took it upon herself to experiment with concrete to build cheap fireproof houses at an affordable cost using a pouring method she developed. After this success she described her methods in an article she wrote for a trade magazine, Concrete, in 1921 titled \"How Women Builds Houses to Sell at a Profit for $4000\". Later, she left Rochester for business opportunities in South Carolina and California. In the 1920s she rebuilt a castle in Septmonts France for herself. As well as helping the surrounding towns to recover from the damage left from the world war. During this time period she also toured California to study adobe buildings. In 1924, she was asked by Berkeley, California to help them rebuild after a fire. In the late 1920s she began to build more poured concrete buildings in Sausalito, California, but she ran into some more issues and the project was not as successful as her buildings in Rochester. Then at her winter home in Beaufort,South Carolina she had plans to make a community of garden apartments for artists and writers but only 10 of these homes were completed at the time of her death.\n\nGleason was a supporter of women's suffrage. According to an account of 1912 National America Woman Suffrage Association Convention mentions Gleason as having promised $1,200 to the suffrage movement, one of its largest pledges. Many of Kate Gleason's personal writings testify to her and her father's contributions to women's suffrage.\nGleason viewed marriage as a hindrance to her professional life and she never married nor had children.\n\nShe died January 9, 1933 of pneumonia and is interred in Riverside Cemetery in Rochester. She left much of her $1.4 million estate to institutions in the Rochester area, including libraries, parks, and the Rochester Institute of Technology. The Kate Gleason College of Engineering at RIT is named in her honor, and her bust stands proudly in the hallway. Kate Gleason Hall is an RIT residence hall. Gleason Works is still in operation today and retains a strong connection with RIT. In 2010, RIT press published a collection of Gleason's letters.\n\nIn 1884 at the age of 19 Kate Gleason enrolled into the Cornell Mechanical Arts program, She was the first female student to attend for engineering.However she ended up leaving before the end of her first academic year to help her father. Her father had hired a man to replace her in the business but the firm started struggling financially and her father could no longer afford to pay Kate’s replacement and he called her home to help again at Gleason Works. She never was able to complete the requirements for a degree but through training and self-learning she earned the title of engineer and is recognized for her accomplishments. She did get some further education at Sibley College of Engraving and The Mechanics Institute which is now Rochester Institute of Technology\n\n\n"}
{"id": "26855026", "url": "https://en.wikipedia.org/wiki?curid=26855026", "title": "Laterite", "text": "Laterite\n\nLaterite is a soil and rock type rich in iron and aluminium, and is commonly considered to have formed in hot and wet tropical areas. Nearly all laterites are of rusty-red coloration, because of high iron oxide content. They develop by intensive and prolonged weathering of the underlying parent rock. Tropical weathering (laterization) is a prolonged process of chemical weathering which produces a wide variety in the thickness, grade, chemistry and ore mineralogy of the resulting soils. The majority of the land area containing laterites is between the tropics of Cancer and Capricorn.\n\nLaterite has commonly been referred to as a soil type as well as being a rock type. This and further variation in the modes of conceptualizing about laterite (e.g. also as a complete weathering profile or theory about weathering) has led to calls for the term to be abandoned altogether. At least a few researchers specializing in regolith development have considered that hopeless confusion has evolved around the name. There is no likelihood, however, that the name will ever be abandoned; for material that looks highly similar to the Indian laterite occurs abundantly worldwide, and it is reasonable to call such material laterite.\n\nHistorically, laterite was cut into brick-like shapes and used in monument-building. After 1000 CE, construction at Angkor Wat and other southeast Asian sites changed to rectangular temple enclosures made of laterite, brick and stone. Since the mid-1970s, some trial sections of bituminous-surfaced, low-volume roads have used laterite in place of stone as a base course. Thick laterite layers are porous and slightly permeable, so the layers can function as aquifers in rural areas. Locally available laterites have been used in an acid solution, followed by precipitation to remove phosphorus and heavy metals at sewage-treatment facilities.\n\nLaterites are a source of aluminium ore; the ore exists largely in clay minerals and the hydroxides, gibbsite, boehmite, and diaspore, which resembles the composition of bauxite. In Northern Ireland they once provided a major source of iron and aluminium ores. Laterite ores also were the early major source of nickel.\n\nFrancis Buchanan-Hamilton first described and named a laterite formation in southern India in 1807. He named it laterite from the Latin word \"later\", which means a brick; this highly compacted and cemented soil can easily be cut into brick-shaped blocks for building. The word laterite has been used for variably cemented, sesquioxide-rich soil horizons. A sesquioxide is an oxide with three atoms of oxygen and two metal atoms. It has also been used for any reddish soil at or near the Earth's surface.\n\nLaterite covers are thick in the stable areas of the Western Ethiopian Shield, on cratons of the South American Plate, and on the Australian Shield. In Madhya Pradesh, India, the laterite which caps the plateau is thick. Laterites can be either soft and easily broken into smaller pieces, or firm and physically resistant. Basement rocks are buried under the thick weathered layer and rarely exposed. Lateritic soils form the uppermost part of the laterite cover.\nVery good water holding capacity:\n- Because the particles are so small, the water is trapped between them.\n- After rain, the water moves into the soil slowly.\n- Palms are less likely to suffer from drought because the rain water is held in the soil.\n- However, flooding after heavy rains is more likely.\n- Nutrient leaching is not likely because the water moves down very slowly.\n- Nutrients can be washed away from the soil surface easily because the water stays on top of the soil and doesn’t move inside.\nLarge surface of soil particles:\n- Small clay particles have a large surface area compared to sand particles.\n- Nutrients stick to clay soils more strongly.\n- Most clay soils are quite fertile and oil palms need relatively small amounts of fertiliser.\nHeavy structure:\n- Because of the tiny particles, the soil sticks together very easily (see Figure 3).\n- Digging holes or other soil management activities are difficult and should be carried out only on dry soils.\n- Soil compaction happens easily, especially when the soil is wet. Once compacted, the soil becomes very hard and the oil palm roots cannot grow well. Therefore, it is important to be careful with cattle grazing and with allowing machines such as trucks and excavators into the plantation, especially after rain.\n\nTropical weathering (laterization) is a prolonged process of chemical weathering which produces a wide variety in the thickness, grade, chemistry and ore mineralogy of the resulting soils. The initial products of weathering are essentially kaolinized rocks called saprolites. A period of active laterization extended from about the mid-Tertiary to the mid-Quaternary periods (35 to 1.5 million years ago). Statistical analyses show that the transition in the mean and variance levels of O during the middle of the Pleistocene was abrupt. It seems this abrupt change was global and mainly represents an increase in ice mass; at about the same time an abrupt decrease in sea surface temperatures occurred; these two changes indicate a sudden global cooling. The rate of laterization would have decreased with the abrupt cooling of the earth. Weathering in tropical climates continues to this day, at a reduced rate.\n\nLaterites are formed from the leaching of parent sedimentary rocks (sandstones, clays, limestones); metamorphic rocks (schists, gneisses, migmatites); igneous rocks (granites, basalts, gabbros, peridotites); and mineralised proto-ores; which leaves the more insoluble ions, predominantly iron and aluminium. The mechanism of leaching involves acid dissolving the host mineral lattice, followed by hydrolysis and precipitation of insoluble oxides and sulfates of iron, aluminium and silica under the high temperature conditions of a humid sub-tropical monsoon climate.\n\nAn essential feature for the formation of laterite is the repetition of wet and dry seasons. Rocks are leached by percolating rain water during the wet season; the resulting solution containing the leached ions is brought to the surface by capillary action during the dry season. These ions form soluble salt compounds which dry on the surface; these salts are washed away during the next wet season. Laterite formation is favoured in low topographical reliefs of gentle crests and plateaus which prevents erosion of the surface cover. The reaction zone where rocks are in contact with water—from the lowest to highest water table levels—is progressively depleted of the easily leached ions of sodium, potassium, calcium and magnesium. A solution of these ions can have the correct pH to preferentially dissolve silicon oxide rather than the aluminium oxides and iron oxides. \nThe mineralogical and chemical compositions of laterites are dependent on their parent rocks. Laterites consist mainly of quartz, zircon, and oxides of titanium, iron, tin, aluminium and manganese, which remain during the course of weathering. Quartz is the most abundant relic mineral from the parent rock.\n\nLaterites vary significantly according to their location, climate and depth. The main host minerals for nickel and cobalt can be either iron oxides, clay minerals or manganese oxides. Iron oxides are derived from mafic igneous rocks and other iron-rich rocks; bauxites are derived from granitic igneous rock and other iron-poor rocks. Nickel laterites occur in zones of the earth which experienced prolonged tropical weathering of ultramafic rocks containing the ferro-magnesian minerals olivine, pyroxene, and amphibole.\n\nYves Tardy, from the \"French Institut National Polytechnique de Toulouse\" and the \"Centre National de la Recherche Scientifique\", calculated that laterites cover about one-third of the Earth's continental land area. Lateritic soils are the subsoils of the equatorial forests, of the savannas of the humid tropical regions, and of the Sahelian steppes. They cover most of the land area between the tropics of Cancer and Capricorn; areas not covered within these latitudes include the extreme western portion of South America, the southwestern portion of Africa, the desert regions of north-central Africa, the Arabian peninsula and the interior of Australia.\n\nSome of the oldest and most highly deformed ultramafic rocks which underwent laterization are found in the complex Precambrian shields in Brazil and Australia. Smaller highly deformed Alpine-type intrusives have formed laterite profiles in Guatemala, Colombia, Central Europe, India and Burma. Large thrust sheets of Mesozoic island arcs and continental collision zones underwent laterization in New Caledonia, Cuba, Indonesia and the Philippines. Laterites reflect past weathering conditions; laterites which are found in present-day non-tropical areas are products of former geological epochs, when that area was near the equator. Present-day laterite occurring outside the humid tropics are considered to be indicators of climatic change, continental drift or a combination of both.\n\nLaterite soils have a high clay content, which mean they have higher Cation Exchange Capacity and water-holding capacity than sandy soils. It is because the particles are so small, the water is trapped between them.After rain, the water moves into the soil slowly.Palms are less likely to suffer from drought because the rain water is held in the soil.\nHowever, if the structure of lateritic soils becomes degraded, a hard crust can form on the surface, which hinders water infiltration, the emergence of seedlings, and leads to increased runoff. It is possible to rehabilitate such soils, using a system called the 'bio-reclamation of degraded lands'. This involves using indigenous water-harvesting methods (such as planting pits and trenches), applying animal and plant residues, and planting high-value fruit trees and indigenous vegetable crops that are tolerant of drought conditions. They are good for oil palm, tea, coffee and cashew cultivation. The International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) has employed this system to rehabilitate degraded laterite soils in Niger and increase smallholder farmers' incomes.\n\nWhen moist, laterites can easily be cut with a spade into regular-sized blocks. Laterite is mined while it is below the water table, so it is wet and soft. Upon exposure to air it gradually hardens as the moisture between the flat clay particles evaporates and the larger iron salts lock into a rigid lattice structure and become resistant to atmospheric conditions. The art of quarrying laterite material into masonry is suspected to have been introduced from the Indian subcontinent.\n\nAfter 1000 CE Angkorian construction changed from circular or irregular earthen walls to rectangular temple enclosures of laterite, brick and stone structures. Geographic surveys show areas which have laterite stone alignments which may be foundations of temple sites that have not survived. The Khmer people constructed the Angkor monuments—which are widely distributed in Cambodia and Thailand—between the 9th and 13th centuries. The stone materials used were sandstone and laterite; brick had been used in monuments constructed in the 9th and 10th centuries. Two types of laterite can be identified; both types consist of the minerals kaolinite, quartz, hematite and goethite. Differences in the amounts of minor elements arsenic, antimony, vanadium and strontium were measured between the two laterites.\n\nAngkor Wat—located in present-day Cambodia—is the largest religious structure built by Suryavarman II, who ruled the Khmer Empire from 1112 to 1152. It is a World Heritage site. The sandstone used for the building of Angkor Wat is Mesozoic sandstone quarried in the Phnom Kulen Mountains, about away from the temple. The foundations and internal parts of the temple contain laterite blocks behind the sandstone surface. The masonry was laid without joint mortar.\n\nThe French surfaced roads in the Cambodia, Thailand and Viet Nam area with crushed laterite, stone or gravel. Kenya, during the mid-1970s, and Malawi, during the mid-1980s, constructed trial sections of bituminous-surfaced low-volume roads using laterite in place of stone as a base course. The laterite did not conform with any accepted specifications but performed equally well when compared with adjoining sections of road using stone or other stabilized material as a base. In 1984 US$40,000 per was saved in Malawi by using laterite in this way.\n\nBedrock in tropical zones is often granite, gneiss, schist or sandstone; the thick laterite layer is porous and slightly permeable so the layer can function as an aquifer in rural areas. One example is the Southwestern Laterite (Cabook) Aquifer in Sri Lanka. This aquifer is on the southwest border of Sri Lanka, with the narrow Shallow Aquifers on Coastal Sands between it and the ocean. It has considerable water-holding capacity, depending on the depth of the formation. The aquifer in this laterite recharges rapidly with the rains of April–May which follow the dry season of February–March, and continues to fill with the monsoon rains. The water table recedes slowly and is recharged several times during the rest of the year. In some high-density suburban areas the water table could recede to below ground level during a prolonged dry period of more than 65 days. The Cabook Aquifer laterites support relatively shallow aquifers that are accessible to dug wells.\n\nIn Northern Ireland, phosphorus enrichment of lakes due to agriculture is a significant problem. Locally available laterite—a low-grade bauxite rich in iron and aluminium—is used in acid solution, followed by precipitation to remove phosphorus and heavy metals at several sewage treatment facilities. Calcium-, iron- and aluminium-rich solid media are recommended for phosphorus removal. A study, using both laboratory tests and pilot-scale constructed wetlands, reports the effectiveness of granular laterite in removing phosphorus and heavy metals from landfill leachate. Initial laboratory studies show that laterite is capable of 99% removal of phosphorus from solution. A pilot-scale experimental facility containing laterite achieved 96% removal of phosphorus. This removal is greater than reported in other systems. Initial removals of aluminium and iron by pilot-scale facilities have been up to 85% and 98% respectively. Percolating columns of laterite removed enough cadmium, chromium and lead to undetectable concentrations. There is a possible application of this low-cost, low-technology, visually unobtrusive, efficient system for rural areas with dispersed point sources of pollution.\n\nOres are concentrated in metalliferous laterites; aluminium is found in bauxites, iron and manganese are found in iron-rich hard crusts, nickel and copper are found in disintegrated rocks, and gold is found in mottled clays.\n\nBauxite ore is the main source for aluminium. Bauxite is a variety of laterite (residual sedimentary rock), so it has no precise chemical formula. It is composed mainly of hydrated alumina minerals such as gibbsite [Al(OH) or AlO . 3HO)] in newer tropical deposits; in older subtropical, temperate deposits the major minerals are boehmite [γ-AlO(OH) or AlO.HO] and some diaspore [α-AlO(OH) or AlO.HO]. The average chemical composition of bauxite, by weight, is 45 to 60% AlO and 20 to 30% FeO. The remaining weight consists of silicas (quartz, chalcedony and kaolinite), carbonates (calcite, magnesite and dolomite), titanium dioxide and water. Bauxites of economical interest must be low in kaolinite. Formation of lateritic bauxites occurs worldwide in the 145- to 2-million-year-old Cretaceous and Tertiary coastal plains. The bauxites form elongate belts, sometimes hundreds of kilometers long, parallel to Lower Tertiary shorelines in India and South America; their distribution is not related to a particular mineralogical composition of the parent rock. Many high-level bauxites are formed in coastal plains which were subsequently uplifted to their present altitude.\n\nThe basaltic laterites of Northern Ireland were formed by extensive chemical weathering of basalts during a period of volcanic activity. They reach a maximum thickness of and once provided a major source of iron and aluminium ore. Percolating waters caused degradation of the parent basalt and preferential precipitation by acidic water through the lattice left the iron and aluminium ores. Primary olivine, plagioclase feldspar and augite were successively broken down and replaced by a mineral assemblage consisting of hematite, gibbsite, goethite, anatase, halloysite and kaolinite.\n\nLaterite ores were the major source of early nickel. Rich laterite deposits in New Caledonia were mined starting the end of the 19th century to produce white metal. The discovery of sulfide deposits of Sudbury, Ontario, Canada, during the early part of the 20th century shifted the focus to sulfides for nickel extraction. About 70% of the Earth's land-based nickel resources are contained in laterites; they currently account for about 40% of the world nickel production. In 1950 laterite-source nickel was less than 10% of total production, in 2003 it accounted for 42%, and by 2012 the share of laterite-source nickel was expected to be 51%. The four main areas in the world with the largest nickel laterite resources are New Caledonia, with 21%; Australia, with 20%; the Philippines, with 17%; and Indonesia, with 12%.\n\n"}
{"id": "58665743", "url": "https://en.wikipedia.org/wiki?curid=58665743", "title": "Latifa Elouadrhiri", "text": "Latifa Elouadrhiri\n\nLatifa Elouadrhiri is a Moroccan experimental physicist and researcher at Thomas Jefferson National Accelerator Facility studying elementary particle physics and nuclear physics. \nShe has worked significantly with the CLAS collaboration in Jefferson Lab's Hall B, performing 3D imaging of nucleons.\nAdditionally, she is the spokesperson of the Deeply Virtual Compton Scattering (DVCS) experiment, studying Generalized Parton Distributions\n\nElouadrhiri is a fellow of the American Physical Society, and has served as a member of the Department of Energy (DOE) Nuclear Science Advisory Committee (NSAC) and the DOE Nuclear Physics (NP) Committee of Visitors (COV). \n\nElouadrhiri obtained a physics B.Sc. from Mohammed V University in 1984, continuing to complete a M.Sc. in Theoretical Physics in 1987 entitled \"Poincare and Lorentz group (Group Theory)\". She continued to complete a Diplôme des Etudes Approfondies at the University of Blaise Pascal, Clermont-Ferrand in particle physics in 1987 entitled \"Development of Drift Chamber tracking Program\". Following this, she completed her PhD in 1991 in physics, also at the University of Blaise Pascal, Clermont-Ferrand in experimental hadronic physics with electromagnetic probes, producing a thesis entitled \"Measurement of the Nucleon Axial Form Factor from Low Energy Pion Electroproduction (Saclay nuclear physics experiment)\" \n\nIn 1994 Elouadrhiri chose to come to Jefferson Laboratory for a joint appointment with CNU, joining the lab as a staff scientist in 2001 as one of the first female staff scientists in the physics division. \nHere, she led the measurement of the first observation of electron beam asymmetries in polarized, exclusive Deeply Virtual Compton Scattering. \nFrom 2006 to 2015, Elouadrhiri was the Project Manager for the Hall B 12 GeV Upgrade, including all detectors, magnets and infrastructure for the CLAS12 system. \n\nIn 2015, Elouadrhiri was interviewed for an article in the Chicago Tribune about the impacts of women in science and engineering . \n\nIn 2018, she was personally recognized by the US Embassy in Rabat for her contributions to nuclear physics on both the official embassy Twitter and Facebook pages.\n\n\n"}
{"id": "25920920", "url": "https://en.wikipedia.org/wiki?curid=25920920", "title": "List of spatial analysis software", "text": "List of spatial analysis software\n\nSpatial analysis software is software written to enable and facilitate spatial analysis. Currently, there are several packages, both free software and proprietary software, which cover most of the spatial data infrastructure stack.\n\n\nFurther resources may be found in the following links:\n"}
{"id": "35334708", "url": "https://en.wikipedia.org/wiki?curid=35334708", "title": "Medical device reporting", "text": "Medical device reporting\n\nMedical device reporting (MDR) is the procedure for the Food and Drug Administration to get significant medical device adverse events information from manufacturers, importers and user facilities, so these issues can be detected and corrected quickly, and the same lot of that product may be recalled. Consumers and health professionals report any adverse event caused by the device to MedWatch program for reporting significant adverse events or product problems with medical products.\n\nLegislation requiring device user facility reporting was enacted by Congress to increase the amount of information the Food and Drug Administration (FDA) and device manufacturers receive about problems with medical devices. Although manufacturers and importers of medical devices have been required since 1984 to report to FDA all device-related deaths, serious injuries, and certain malfunctions, numerous studies have shown widespread underreporting. A 1986 General Accounting Office (GAO) study showed that less than one percent of device problems occurring in hospitals are reported to the FDA, and that the more serious the problem with a device, the less likely it was to be reported. A GAO follow-up study in 1989 concluded that despite full implementation of the Medical Device Reporting (MDR) regulation, serious shortcomings still existed.\n\nUnder the Safe Medical Devices Act of 1990 (SMDA), device user facilities must report device-related deaths to the FDA and the manufacturer, if known. The facilities must also report device-related serious injuries to the manufacturer, or to the FDA if the manufacturer is not known. In addition, SMDA also required that device user facilities submit to FDA, on a semiannual basis, a summary of all reports submitted during that time period. The device user facility reporting section of SMDA became effective on November 28, 1991.\n\nTo implement SMDA, FDA published a tentative final rule in the Federal Register on November 26, 1991, inviting comments; over 300 comments were received. On June 16, 1992, the President signed into law the Medical Devices Amendments of 1992 (Public Law 102-300) making minor changes. A final rule published in the Federal Register on December 11, 1995, addresses the comments received and the mandated changes.\n\nThe Food and Drug Administration Modernization Act (FDAMA) made four changes that affected MDR, effective 2/19/98:\n"}
{"id": "2021939", "url": "https://en.wikipedia.org/wiki?curid=2021939", "title": "Microstrip antenna", "text": "Microstrip antenna\n\nIn telecommunication, a microstrip antenna (also known as a printed antenna) usually means an antenna fabricated using microstrip techniques on a printed circuit board (PCB). It is a kind of Internal Antenna.They are mostly used at microwave frequencies. An individual microstrip antenna consists of a patch of metal foil of various shapes (a patch antenna) on the surface of a PCB(printed circuit board), with a metal foil ground plane on the other side of the board. Most microstrip antennas consist of multiple patches in a two-dimensional array. The antenna is usually connected to the transmitter or receiver through foil microstrip transmission lines. The radio frequency current is applied (or in receiving antennas the received signal is produced) between the antenna and ground plane. Microstrip antennas have become very popular in recent decades due to their thin planar profile which can be incorporated into the surfaces of consumer products, aircraft and missiles; their ease of fabrication using printed circuit techniques; the ease of integrating the antenna on the same board with the rest of the circuit, and the possibility of adding active devices such as microwave integrated circuits to the antenna itself to make active antennas.\n\nThe most common type of microstrip antenna is the patch antenna. Antennas using patches as constitutive elements in an array are also possible. A patch antenna is a narrowband, wide-beam antenna fabricated by etching the antenna element pattern in metal trace bonded to an insulating dielectric substrate, such as a printed circuit board, with a continuous metal layer bonded to the opposite side of the substrate which forms a ground plane. Common microstrip antenna shapes are square, rectangular, circular and elliptical, but any continuous shape is possible. Some patch antennas do not use a dielectric substrate and instead are made of a metal patch mounted above a ground plane using dielectric spacers; the resulting structure is less rugged but has a wider bandwidth. Because such antennas have a very low profile, are mechanically rugged and can be shaped to conform to the curving skin of a vehicle, they are often mounted on the exterior of aircraft and spacecraft, or are incorporated into mobile radio communications devices.\nIt is used in telecommunication\n\nMicrostrip antennas are relatively inexpensive to manufacture and design because of the simple 2-dimensional physical geometry. They are usually employed at UHF and higher frequencies because the size of the antenna is directly tied to the wavelength at the resonant frequency. A single patch antenna provides a maximum directive gain of around 6-9 dBi. It is relatively easy to print an array of patches on a single (large) substrate using lithographic techniques. Patch arrays can provide much higher gains than a single patch at little additional cost; matching and phase adjustment can be performed with printed microstrip feed structures, again in the same operations that form the radiating patches. The ability to create high gain arrays in a low-profile antenna is one reason that patch arrays are common on airplanes and in other military applications.\n\nSuch an array of patch antennas is an easy way to make a phased array of antennas with dynamic beamforming ability.\n\nAn advantage inherent to patch antennas is the ability to have polarization diversity. Patch antennas can easily be designed to have vertical, horizontal, right hand circular (RHCP) or left hand circular (LHCP) polarizations, using multiple feed points, or a single feedpoint with asymmetric patch structures. This unique property allows patch antennas to be used in many types of communications links that may have varied requirements.\n\nThe most commonly employed microstrip antenna is a rectangular patch which looks like a truncated microstrip transmission line. It is approximately of one-half wavelength long. When air is used as the dielectric substrate, the length of the rectangular microstrip antenna is approximately one-half of a free-space wavelength. As the antenna is loaded with a dielectric as its substrate, the length of the antenna decreases as the relative dielectric constant of the substrate increases. The resonant length of the antenna is slightly shorter because of the extended electric \"fringing fields\" which increase the electrical length of the antenna slightly. An early model of the microstrip antenna is a section of microstrip transmission line with equivalent loads on either end to represent the radiation loss.\n\nThe dielectric loading of a microstrip antenna affects both its radiation pattern and impedance bandwidth. As the dielectric constant of the substrate increases, the antenna bandwidth decreases which increases the Q factor of the antenna and therefore decreases the impedance bandwidth. This relationship did not immediately follow when using the transmission line model of the antenna, but is apparent when using the cavity model which was introduced in the late 1970s by Lo et al. The radiation from a rectangular microstrip antenna may be understood as a pair of equivalent slots. These slots act as an array and have the highest directivity when the antenna has an air dielectric and decreases as the antenna is loaded by material with increasing relative dielectric constant.\n\nThe half-wave rectangular microstrip antenna has a virtual shorting plane along its center. This may be replaced with a physical shorting plane to create a quarter-wavelength microstrip antenna. This is sometimes called a half-patch. The antenna only has a single radiation edge (equivalent slot) which lowers the directivity/gain of the antenna. The impedance bandwidth is slightly lower than a half-wavelength full patch as the coupling between radiating edges has been eliminated.\n\nAnother type of patch antenna is the planar inverted-F antenna (PIFA).\nThe PIFA is common in cellular phones (mobile phones) with built-in antennas.\nThe antenna is resonant at a quarter-wavelength (thus reducing the required space needed on the phone), and also typically has good SAR properties.\nThis antenna resembles an inverted F, which explains the PIFA name. The PIFA is popular because it has a low profile and an omnidirectional pattern.\nThese antennas are derived from a quarter-wave half-patch antenna. The shorting plane of the half-patch is reduced in length which decreases the resonance frequency.\nOften PIFA antennas have multiple branches to resonate at the various cellular bands. On some phones, grounded parasitic elements are used to enhance the radiation bandwidth characteristics. \n\nThe folded inverted conformal antenna (FICA) has some advantages with respect to the PIFA, because it allows a better volume reuse.\n\n"}
{"id": "51294095", "url": "https://en.wikipedia.org/wiki?curid=51294095", "title": "Milivoje Kostic", "text": "Milivoje Kostic\n\nMilivoje Kostic (also, Milivoje M. Kostic; in Serbian Cyrillic: Миливоје Костић; born 20 March 1952 in Bioska, Užice municipality, Serbia), is a Serbian-American thermodynamicist and professor emeritus of mechanical engineering at Northern Illinois University, Licensed Professional Engineer (PE) in Illinois, and Editor-in-Chief of the Thermodynamics section of the journal \"Entropy\". He is an expert in energy fundamentals and applications, including nanotechnology, with emphasis on efficiency, efficient energy use and energy conservation, and environment and sustainability.\n\nMilivoje Kostic was born and raised in Serbia (ex-Yugoslavia at the time). He completed his \"Dipl-Ing\" (Diploma Engineer) degree in Mechanical Engineering at the University of Belgrade in 1975, with the distinction of having the highest GPA in the mechanical engineering program history at the time. Then he worked as a researcher in thermal engineering and combustion at Vinca Institute for Nuclear Sciences, which then hosted the headquarters of the International Center for Heat and Mass Transfer (ICHMT), and later taught at the University of Belgrade. In meantime, he spent three summers as an exchange visitor in England, West Germany, and the former Soviet Union. Kostic came to the University of Illinois at Chicago in 1981 as a Fulbright grantee, where he received his Ph.D. in mechanical engineering in 1984. He subsequently worked several years in industry before emigrated to the United States in 1986. After working for 26 years at Northern Illinois University, he retired in 2014 to focus on his fundamental research, and became Professor Emeritus in 2015.\n\nKostic was appointed the Editor-in-Chief of the Thermodynamics Section of \"Entropy\" journal, after serving as a Guest Editor of two special issues on Entropy and the Second Law of Thermodynamics.\n\nKostic has also worked in industry and has authored a number of patents and professional publications, including invited articles in professional encyclopedias.\n\nProfessor Kostic was appointed as NASA faculty fellow, and Fermi and Argonne National Laboratories faculty researcher. He has a number of professional awards and recognitions, is a frequent keynote plenary speaker at international conferences and at different educational and public institutions, as well as member of several professional societies and scientific advisory boards. \"Entransy concept and controversies\"\n\nKostic is interested in the fundamental laws of nature, thermodynamics and heat transfer fundamentals and applications, and especially the Second law of thermodynamics and entropy. He has developed a collaboration with Tsinghua and other Chinese universities. Recently, Kostic wrote about Entransy concept and controversies \n, and is editing an \"Entropy\" special Issue, Nature of heat and entropy.\n\n"}
{"id": "21575789", "url": "https://en.wikipedia.org/wiki?curid=21575789", "title": "Orion (system-on-a-chip)", "text": "Orion (system-on-a-chip)\n\nOrion is a system-on-a-chip manufactured by Marvell Technology Group and used in network-attached storage. Based on the ARMv5TE architecture, it has on-chip support for Ethernet, SATA and USB, and is used in hardware made by Hewlett-Packard and D-Link among others. It is supported by the Lenny release of Debian GNU/Linux.\n"}
{"id": "42868663", "url": "https://en.wikipedia.org/wiki?curid=42868663", "title": "Piston-cylinder apparatus", "text": "Piston-cylinder apparatus\n\nThe piston-cylinder apparatus is a solid media device, used in Geosciences and Material Sciences, for generating simultaneously high pressure (up to 6 GPa) and temperature (up to 1700 °C). Modifications of the normal set-up can push these limits to even higher pressures and temperatures. A particular type of piston-cylinder, called Griggs apparatus, is also able to add a deviatoric stress on the sample.\nThe principle of the instrument is to generate pressure by compressing a sample assembly, which includes a resistance furnace, inside a pressure vessel. Controlled high temperature is generated by applying a regulated voltage to the furnace and monitoring the temperature with a thermocouple. The pressure vessel is a cylinder that is closed at one end by a rigid plate with a small hole for the thermocouple to pass through. A piston is advanced into the cylinder at the other hand.\n\nSir Charles Parsons was the first to attack the problem of generating high pressure simultaneously with high temperature. His pressure apparatus consisted of piston-cylinder devices that used internal electrical resistance heating. He used a solid pressure transmitting material, which also served as thermal and electrical insulation. His cylindrical chambers ranged in diameter from 1 to 15 cm. The maximum pressure at the temperature he reported was of the order of 15000 atm (corresponding to ~1.5 GPa) at 3000 °C.\nLoring L. Coes, Jr., of the Norton Co., was the first person to develop a piston-cylinder device with capabilities substantially beyond those of the Parsons device. He did not personally publish a description of this equipment until 1962. The key feature of this device is the use of a hot, molded alumina liner or cylinder. The apparatus is double ended, pressure being generated by pushing a tungsten carbide piston into each end of the alumina cylinder. Because the alumina cylinder is electrically insulating, heating is accomplished, very simply, by passing an electric current from one piston through a sample heating tube and out through the opposite piston. The apparatus was used at pressures as high as 45000 atm (corresponding to ~4.5 GPa) simultaneously with a temperature of 800 °C. Temperature was measured by means of a thermocouple located in a well. At these temperature and pressure conditions, only one run is obtained in this device, the pistons and the alumina cylinder both being expendable. Even at 30000 atm (corresponding to ~3.0 GPa) the alumina cylinder is only useful for a few runs, as is also the case for the tungsten carbide pistons. The expense of using such a device is great.\nNowadays both the piston and the cylinder are constructed of cemented tungsten carbide and electrical insulation is provided in a different manner than in the device of Coes. In particular, the basis for the modern piston-cylinder apparatus is given by the design described by Boyd and England in 1960, which has been the first machine that allowed experiments under upper mantle conditions to be routinely carried out in a laboratory.\n\nThe piston-cylinder apparatus is based on the same simple relationship of other high-pressure devices (e.g. Multi-anvil press and Diamond Anvil Cell):\n\nformula_1\n\nwhere P is the pressure, F the applied force and A the area.\nIt achieves high pressures using the principle of pressure amplification: converting a small load on a large piston to a relatively large load on a small piston. The uniaxial pressure is then distributed (quasi-hydrostatically) over the sample through deformation of the assembly materials.\n\nThe main components of the piston-cylinder apparatus (Fig. 1) are the pressure generating system, the pressure vessel, and the assembly parts within the vessel. There are two types of piston-cylinder apparatus: non end-loaded and end-loaded, which involve, respectively, one or two hydraulic rams. In the end-loaded type the second hydraulic ram is used to vertically load and strengthen the pressure vessel. The non end-loaded type is smaller, more compact and cheaper, and is operable only to approximately 4 GPa.\nPressure is applied to the sample by pressing a piston into the sample volume of the pressure vessel. The sample assembly consists of a solid pressure medium, a resistance heater and a small central volume for the sample. Three common configurations are used: formula_2”, formula_3” and 1”, which are the diameters of the piston and thus the sample assembly. According to the pressure amplification concept, the choice of the piston depends on the pressure you need to achieve.\nDuring the experiment, water circulates around the pressure vessel, the bridge and the upper plates to cool the system.\n\nThe purposes of the sample assembly (Fig. 2) are to transmit hydrostatic pressure to the sample from the compressing piston, to provide controlled heating of the sample and to provide, via the capsule, a suitable volatile and oxygen fugacity environment for the experiment. Therefore, it includes a component for each of these purposes.\nThe outer cylinder is a pressure transmitting, electrically insulating cylinder made from NaCl, talc, BaCO, KBr, CaF, or even borosilicate glass. The next components are, in order, an electrically insulating borosilicate glass cylinder and a graphite cylinder, which acts as the “furnace”. To locate the sample exactly in the centre of the furnace and to grip the thermocouple, a support rod usually made of crushable ceramics is used. The final component is a conductive steel base plug, located at the top of the sample assembly.\nThe final part of the assembly is the thermocouple itself, whose wires are insulated from one another and from the material of the assembly by a tube made of mullite.\n\nThe sample capsule must contain the sample and prevent reaction between the sample and the other materials of the sample assembly and not, itself, react with the sample. It must also be weak so as not to interfere with pressure transmission during the run. For this purpose the materials most used are: Au, Pt, AgPd alloys, Ni and graphite.\nSample volumes are typically 200 mm, which translates to ~500 mg of starting material, but with larger assemblies the volume can be up to 750 mm.\n\nThe nominal pressure in an experiment can be calculated from the amplification of the oil pressure through the reduction in area over which it is applied, but every component has a characteristic yield stress, consequently the nominal pressure is different from the effective one. Thus, it must be adjusted taking into account the friction:\n\n\"P = P + P\"\n\nIn order to determine the effective pressure, calibration experiments can be done using either static or dynamic methods, and usually make use of known phase transitions or reactions, melting curves or measured water solubility in melts.\nSince frictional effects also depend on whether the press is in compression or in decompression, it’s good practice to perform the experiments in the same way as the calibration runs.\n\nTemperature can be measured using a thermocouple within an accuracy of ± 1 °C. The accuracy of the temperature is influenced by both random and systematic errors, and is smaller at higher temperature and pressure conditions. Such errors can arise from temperature gradients, differential pressures in the assembly, contamination during the experiment and the effect of pressure on thermocouple electromotive force. These errors can be cushioned choosing the appropriate thermocouple type for the experimental conditions. Temperature gradients, on the other hand, can be minimised using a tapered furnace.\n\nThe main advantages of the piston-cylinder press are the relatively large volume of the assembly, fast heating and quenching rates, and the stability of the equipment over long run durations.\nThese aspects, together with the ease and safety of procedure make this device suitable for geochemical studies and \"in-situ\" measurements of the physical properties of materials.\nSome applications, especially in Geosciences, are: synthesis of high-pressure and temperature materials, hot pressing and investigation of partial melting of rocks.\n"}
{"id": "33012035", "url": "https://en.wikipedia.org/wiki?curid=33012035", "title": "Portuguese inventions", "text": "Portuguese inventions\n\nThe Portuguese inventions are the inventions created by people born in Portugal (continent or overseas) or whose nationality is Portuguese. These inventions were created mainly during the age of Portuguese Discoveries, but as well, during modernity.\n\nRelying on trade secret explains, in part, the difficulty often experienced by researchers in documenting Portuguese inventions, as many are not described in patent documents, or other technical documents. On the other hand, there are cases, like some types of swords, where the inventions themselves or the underlying documents were lost, having been destroyed, for example, during the French invasions. There are as well documentation and objects of Portuguese origin in private collections or museums outside of Portugal.\n\nThe creation of new inventions in Portugal took its peak during the Age of Discovery. These inventions consisted mainly in the improvement of devices and techniques of ocean navigation and coastal cartography, such as the mariner's astrolabe and the chart of latitudes. On the field of military applications, the construction of cannons and new types of swords, like the carracks black sword.\n\nMore recently, the technical domain varies from computers to medicine. Such examples might be Via Verde, an automatic system for collecting tolls for vehicles, the Multibanco, an automatic teller machine network with a multitude of functions ranging from bank transfers to the payment of tickets for shows, or in the field of medicine, a treatment of epilepsy, the drug Zebinix by Bial Laboratories.\n\n"}
{"id": "3982626", "url": "https://en.wikipedia.org/wiki?curid=3982626", "title": "RETMA tube designation", "text": "RETMA tube designation\n\nThe Radio Electronics Television Manufacturers' Association was formed in 1953, as a result of mergers with other trade standards organisations, such as the RMA. It was principally responsible for the standardised nomenclature for American vacuum tubes - however the standard itself had already been in use for a long time before 1953; for example, the 6L6 was introduced in July 1936.\n\nAmerican made tubes bear a RETMA designation to allow for easy cross-referencing. The RETMA tube designation does not incorporate the purpose of each tube in the designation. The Anglo-European Mullard–Philips tube designation does include tube use information in the designation.\n\n\n\n\n"}
{"id": "59104195", "url": "https://en.wikipedia.org/wiki?curid=59104195", "title": "Resident engineer", "text": "Resident engineer\n\nA resident engineer is a construction occupation. It often describes an engineer employed to work from site for the client or the design engineer. The duties include supervision of and issuing of instructions to the contractor and to report regularly to the designer and/or client. The role was common historically and was also defined in the Institution of Civil Engineers Conditions of Contract and FIDIC contracts. It is not a defined role in the commonly used and more modern NEC Engineering and Construction Contract in which similar roles are named as the client's \"project manager\" and \"supervisor\".\n"}
{"id": "56451303", "url": "https://en.wikipedia.org/wiki?curid=56451303", "title": "Rhetoric of technology", "text": "Rhetoric of technology\n\nThe rhetoric of technology is both an object and field of study. It refers to the ways in which makers and consumers of technology talk about and make decisions regarding technology and also the influence that technology has on discourse. Studies of the rhetoric of technology are interdisciplinary. Scholars in communication, media ecology, and science studies research the rhetoric of technology. Technical communication scholars are also concerned with the rhetoric of technology.\n\nThe phrase \"rhetoric of technology\" gained prominence with rhetoricians in the 1970s, and the study developed in conjunction with interest in the rhetoric of science. However, scholars have worked to maintain a distinction between the two fields. Rhetoric of technology criticism addresses several issues related to technology and employs many concepts, including several from the canon of classical rhetoric, for example \"ethos\", but the field has also adopted contemporary approaches, such as new materialism.\n\nWhile the definition and scope of rhetoric is contested, scholars in the discipline, or rhetoricians, study the capacity of symbols to create change and influence perspectives. Often, rhetoricians study discourse and texts, but they also study objects. Technology is both techniques and objects that embody and enact techniques. Thus, rhetoric of technology scholars may look at texts and discourse associated with technology or techniques and technological objects. Concerns of rhetoric of technology include the influence of technology on public deliberation, conceptions of the self, and how new technologies come to be developed and adopted.\n\nScholars who study rhetoric of technology have argued that it should be treated as distinct from the rhetoric of science. Charles Bazerman offers three distinctions between rhetoric of technology and rhetoric of science. First, unlike science, technology has always been intertwined with other clearly rhetorical endeavors, such as commerce and finance. Second, while the discourse of science often works towards specialization, the discourse of technology is pervasive. We may not all conduct experiments, but we all interact with and operate technology. Lastly, the products of technology are mostly material while the products of science are mostly symbols. Carolyn Miller argues that rhetoric of science must be treated as distinct from rhetoric of technology because technology and science differ in motivation, criteria of judgment, and values.\n\nScholars started to suggest the importance of studying rhetoric of technology in the 1970s alongside a growing interest in rhetoric of science. Thomas W. Benson and Gerard A. Hauser referred to the \"rhetoric of technology\" in a book review published in 1973 and noted the shared concern of rhetoric and technology with technique. In 1978 Carolyn Miller wrote “Technology as a Form of Consciousness: A Study of Contemporary Ethos.” In addition to explaining why rhetoric of technology should be treated separately from science, Miller argues that technology gives rise to a particular \"ethos,\" or personal character, because it generates particular actions and a particular consciousness. She concludes that technological consciousness assumes an objective perspective that erodes \"ethos\". Within technological consciousness, actions are right or wrong regardless of cultural possibilities because assessments are assumed to be objective.\n\nIn a series of oral histories collected by the Association for the Rhetoric of Science, Technology, and Medicine, scholars familiar with both fields recognized that there has been more research addressing the rhetoric of science than comparable work on technology. Carolyn Miller suggested that one reason for the relative lack of rhetoric of technology scholarship is that it is harder to find relevant texts to analyze. Miller noted that many of the primary texts dealing with technology are private company documents.\n\nIn 1980 Langdon Winner wrote the article “Do Artifacts Have Politics?” Winner suggests that objects reflect and enact ideological perspectives. Winner's idea shares much in common with a materialist approach to rhetoric of technology. Materialist approaches to rhetoric of technology are related to materialism and treat technological objects the same as they may treat a text by critiquing how objects persuade and influence.\n\nJeremy Packer and Stephen B. Wiley provide an overview of new materialist approaches to rhetoric in their book \"Communication Matters: Materialist Approaches to Media\". Packer and Wiley conceptualize materiality as a “corrective” to the concept of communication as intangible. They suggest that the corrective is in response to a “poststructuralist impasse” in the field of communication, rhetoric, and media studies. They outline two approaches to materiality: 1) to equate materiality with physicality and approach infrastructure, body, space, and technology as fields of communication and 2) to analyze the materiality of communication itself, the physiological, mechanical, or digital media of communication. Packer and Wiley also identify key themes that are explored in materialist approaches: economy, discourse, technology, space, and bodies.\n\nIn her 1978 article “Technology as a Form of Consciousness: A Study of Contemporary Ethos,” Carolyn Miller argued that technology was fundamentally changing how individuals judge personal character. She suggested that the ultimate result of a technological consciousness would be to erode a concern with \"ethos\" as technology would prevail in presenting actions and decision as objectively right or wrong. If all decisions are objective, then there is no longer a need for judgments of character. In a 1992 article, Steven B. Katz employed Miller's \"technological consciousness\" to help explain the rhetoric used by members of the Nazi regime to enact the Holocaust. In 2004, Miller revisited the relationship between \"ethos\" and technology in an exploration of the impact of human-computer interaction. She explores two modes of human-computer interaction: expert systems versus intelligent agents. She argues that in both modes of interaction subjectivity blurs between the human-user and the computer, creating a hybrid (borrowing from Bruno Latour) or cyborg (borrowing from Donna Haraway). Expert systems are designed to mimic human experts; they typically draw from a deep database of information. Intelligent agent computer systems, in contrast, learn from interacting with an environment. The merger between human and computer destabilizes \"ethos\" according to Miller. As the intelligent agent model of computing has grown in popularity, Miller suggests that there has been a shift from a logos-centric to a pathos-centric \"ethos\". Neither, she contends, provide ethics, or \"arete\", which is a gap that rhetoric should fill.\n\nAs invention is important to the development of new technology, invention is also important to rhetoric. Along with arrangement, delivery, style, and memory, invention is one of the five canons of rhetoric, or the five key elements of a competent speech according to classic rhetorical theory. Therefore, some rhetoricians argue that the development of new technology is fundamentally rhetorical. John A. Lynch and William J. Kinsella describe how both technology and rhetoric are both concerned with creating something new from available resources and know-how.\n\nKairos is an ancient Greek word that captures the idea that there is a right time for action. Carolyn Miller has suggested that the \"right time\" is also a central concern for technology. She uses the Japanese \"Fifth Generation\" computer project as a case study. Miller contends that within technological discourse \"kairos\" is both a powerful theme and useful tool of analysis. She supports her claim with an analysis of technological forecasting, which was a central feature of the Japanese computer project, and concludes that technologists employ \"kairos\" to justify investment in particular technologies.\n\n"}
{"id": "54967090", "url": "https://en.wikipedia.org/wiki?curid=54967090", "title": "Seyed Zia Hashemi", "text": "Seyed Zia Hashemi\n\nSeyed Zia Hashemi (, born 1968 in Yasuj) is an Iranian Doctorate, politician and former acting Minister of Science, a position he held from 20 August 2017 until 29 October 2017. He is also Vice President of Social and Cultural Affair at Ministry of Science, Research and Technology, a position he held since 2013. He is head of the Islamic Republic News Agency since 1 January 2018.\nSeyed Zia Hashemi was born and raised in Yasuj. He was from a traditional family and his father was a respected teacher. After graduation from high school, he moved to Tehran and started studying sociology at University of Tehran. After getting his PhD, he joined the Faculty of Social Sciences at University of Tehran and he is still teaching there.\n"}
{"id": "55846595", "url": "https://en.wikipedia.org/wiki?curid=55846595", "title": "Shadow board", "text": "Shadow board\n\nA shadow board is a device for organizing a set of tools; the board defines where particular tools should be placed when they are not in use. Shadow boards have the outlines of a work station's tools marked on them, allowing operators to identify quickly which tools are in use or missing. The boards are commonly located near the work station where the tools are used. Shadow boards are often used in the manufacturing environment to improve a facility's lean six sigma capabilities.\n\nShadow boards reduce time spent looking for tools and also reduce losses. They improve work station safety because tools are replaced safely after use, rather than becoming potential hazards.\n"}
{"id": "2530489", "url": "https://en.wikipedia.org/wiki?curid=2530489", "title": "Side dish", "text": "Side dish\n\nA side dish, sometimes referred to as a side order, side item, or simply a side, is a food item that accompanies the entrée or main course at a meal.\n\nSide dishes such as salad, potatoes and bread are commonly used with main courses throughout many countries of the western world. New side orders introduced within the past decade, such as rice and couscous, have grown to be quite popular throughout Europe, especially at formal occasions (with couscous appearing more commonly at dinner parties with Middle Eastern dishes).\n\nWhen used as an adjective qualifying the name of a dish, the term \"side\" usually refers to a smaller portion served as a side dish, rather than a larger, main dish-sized serving. For example, a \"side salad\" usually served in a small bowl or salad plate, in contrast to a large dinner-plate-sized entrée salad.\n\nA typical American meal with a meat-based main dish might include one vegetable side dish, sometimes in the form of a salad, and one starch side dish, such as bread, potatoes, rice, or pasta.\n\nSome common side dishes include:\nSome restaurants offer a limited selection of side dishes which are included with the price of the entrée as a combination meal. In contrast, sometimes side dishes are ordered separately from an a la carte menu. The term may or may not imply that the dish can only be ordered with other food.\n\nFrench fries are a common side dish served at fast-food restaurants and other American cuisine restaurants. In response to criticism about the high fat and calorie content of French fries, some fast-food chains have recently begun to offer other side dishes, such as salads, as substitutes for the standard French fries with their combination meals.\n\nThe related phrase on the side may be synonymous with \"side dish\" – as in \"French fries on the side\" – or may refer to a complimentary sauce or condiment served in a separate dish. For example, a diner may request a salad be served with its dressing \"on the side\".\n\n\n\n"}
{"id": "440462", "url": "https://en.wikipedia.org/wiki?curid=440462", "title": "Smoke grenade", "text": "Smoke grenade\n\nA smoke grenade is a canister-type grenade used as a signaling device, target or landing zone marking device, or as a screening device for unit movements.\n\nSmoke grenades generally emit a far larger amount of smoke than smoke bombs, which are a type of fireworks typically started with an external fuse rather than a pin and are more complex. Smoke grenades often cost around compared to smoke bombs, which can often cost just a few cents.\nTypical design consists of a sheet steel cylinder with four emission holes on top and one on the bottom to allow smoke release when the grenade is ignited. The filler consists of 250 to 350 grams of colored smoke composition (mostly potassium chlorate, lactose, and a dye) in virtually any color. White smoke grenades typically use hexachloroethane-zinc and granular aluminum. The reaction is exothermic and though they remain intact, smoke grenade casings will often remain scalding hot for some time even after the grenade is no longer emitting smoke. Although modern smoke grenades are designed not to directly emit fire or sparks, they remain a fire hazard and are capable of igniting dry vegetation or flammable substances if used injudiciously.\n\nAnother type of smoke grenade is the bursting variation. These are filled with white phosphorus (WP), a pyrophoric agent which is spread quickly into a cloud by an internal bursting charge. White phosphorus burns with a brilliant yellow flame, while producing copious amounts of white smoke (phosphorus pentoxide). This type of smoke grenade is favored for its ability to produce a very dense and nearly instantaneous cloud of white concealment smoke as compared to the more common solid-filler grenades which expel a slower stream of smoke over a period of roughly 1 minute. For this reason they are favored for use in onboard grenade launching attachments on armored vehicles, which require extremely fast concealment in the event they are targeted by laser guided anti-tank missiles and need to rapidly retreat.\n\nSmoke grenades are used for several purposes. The primary use is the creation of smoke screens for concealment and the signalling of aircraft.\n\nIf movement (such as flanking maneuvers or retreat) is necessary, smoke grenades can be thrown prior to movement in order to provide a wall of visual distraction that reduces the accuracy of enemy fire and temporarily deceives them as to the force's location. The most common color for concealment smoke is white or grey. With the advent of infrared imaging goggles which can spot troop movements through smoke screens, the newest smoke compositions now include a \"multi-spectrum\" component to make them IR impermeable.\n\nSmoke grenades can also be used to signal aircraft. Since locating a target from above (especially in thick forest canopy) can be nearly impossible, even with good radio contact, colored smoke grenades are often used to allow aircraft to spot them. Colored signalling smoke grenades are widely used in CASEVAC and close air support situations where quickly locating friendly ground forces is of paramount importance. Common colors are red, yellow, green and purple, and all use very brightly colored dyes to increase the likelihood of being spotted from above.\n\nSmoke grenades are functionally identical to many forms of chemical grenades (such as CS gas riot control grenades) and incendiary grenades (such as thermite grenades) which use a fuse to ignite a solid filler inside a steel canister, which then slowly propels the combustion products out through holes in the canister as the contents burn. However, the smoke grenade class is restricted to signalling and concealment under the laws of war, and thus they are not considered weapons; since the vast majority are non-explosive, they remain legal for civilian use and ownership in most countries.\n\nSince the basic design of a smoke grenade (a metal canister containing a substance that burns and expels smoke when ignited) is so simple, improvised devices are ubiquitous across the world. Protestors, football spectators, and airsoft enthusiasts often create their own smoke grenades using common materials.\n"}
{"id": "46589273", "url": "https://en.wikipedia.org/wiki?curid=46589273", "title": "Sonic Pi", "text": "Sonic Pi\n\nSonic Pi is a live coding environment based on Ruby, originally designed to support both computing and music lessons in schools, developed by Sam Aaron in the University of Cambridge Computer Laboratory in collaboration with Raspberry Pi Foundation. Thanks to its use of the Supercollider synthesis engine and accurate timing model, it is also used for live coding and other forms of algorithmic music performance and production, including at algoraves. Its research and development has been supported by Nesta, via the \"Sonic PI: Live & Coding\" project.\n"}
{"id": "7736904", "url": "https://en.wikipedia.org/wiki?curid=7736904", "title": "V-Cord", "text": "V-Cord\n\nV-Cord is an analog recording videocassette format developed and released by Sanyo. V-Cord (later referred to as V-Cord I) was released in 1974, and could record 60 minutes on a cassette. V-Cord II, released in 1976, could record 120 minutes on a V-Cord II cassette.\n\nThe V-Cord II machines were the first consumer VCRs to offer two recording speeds.\n\nThe original V-Cord cassette had a large hub and was wound with standard-thickness magnetic tape; V-Cord II used a small hub wound with thin tape, the same thickness later used for VHS-120 and Beta L-750. The cassettes were rectangular; unlike subsequent formats VHS and Betamax, which loaded with the tape facing front on the long side of the cassette, the V-Cord cartridge was loaded sideways with the narrow side serving as the \"front\" and the tape coming out the \"side\".\n\nThe tape was held in place in the machine by a notch halfway down the right side of the tape, similar to what holds an 8-track tape into its player.\n\nThe earliest machines recorded only in black and white and had no rewind mechanism, like the Cartrivision format of a few years earlier; an external rewinder was used after recording or playing a tape. External rewinders were later used with the VHS and Beta formats, although the machines could rewind tapes; external rewinders were considerably faster than the rewind function.\n\nThe system had two recording modes: standard mode (STD), and a long-play mode (LP) which sacrificed recording quality for extra capacity. In STD mode both recording and playback heads are used, writing both fields of each interlaced video frame. In long-play mode only a single head is used to record a single field from each video frame, with each field being read twice on playback, in a \"skip field\" technique. The heads scanned the tape in a helical scan fashion \n\nTape was moved forward at 2.91 inches per second in STD mode, and 1.45 inches per second in LP mode; this gave a recording time of one-hour in standard mode and two hours in long-play mode. Horizontal resolution in color was quoted as 250 lines in advertising materials, stretching to 300 lines in black-and-white, with a video signal-to-noise ratio of 45 dB. Audio response was specified as 80 to 10,000 Hz at -6 dB in STD mode, dropping to 80 to 8,000 Hz in LP mode.\n\nThe tape was a half-inch cobalt doped tape with a 550 oersted coercivity. The cassette measured 4 \" by 6 \" by 1\". Two cassette types were available, a V60 and a V120 whose names matched their recording capacity in LP mode. The cassettes are similar in appearance to eight-track cartridges.\n\nConventional VHS and Beta formats recorded in a helical scan format, resulting in angled tracks running from the lower edge of the tape to the upper edge some distance down. Unlike these formats, the V-Cord format was closer to the 2-inch quadruplex videotape format used from the inception of video in the late 1950s until 2-inch helical IVC videotape format was introduced twenty years later, in that its tracks ran nearly perpendicular to tape travel.\n\nA portable four-head video recorder, the Sanyo VTC-7100, used a similar format of cassette, but produced incompatible recordings.\n\nToshiba KV-4000, KV-4100, and KV-4200; and Sanyo VTC-7300, VTC-8000, and VTC-8200 V-Cord I/II VCR (1976)\n\n\n"}
{"id": "35364792", "url": "https://en.wikipedia.org/wiki?curid=35364792", "title": "Wishtel", "text": "Wishtel\n\nWishTel is an Indian manufacturing company that produce various IT products, founded in Mumbai by Milind Shah. The manufacturing facilities are currently in Maharashtra and Gujarat.\n\nWishTel is an Indian IT products manufacturing company. On 23 March 2012, it announced the launch of a range of low-cost tablet devices by the name of IRA. The company claims to bring cheap and efficient computing devices in the fields of education where the Indian Government has often made efforts with the OLPC initiative and the Aakash (tablet) project.\n\nThe company was also the second lowest bidder for the Aakash project where they lost to DataWind. The CEO of the company Mr. Milind Shah has also claimed that WishTel would be bidding for the Aakash 2 project which was announced by Kapil Sibal who is currently the minister of communications and information technology for India.\n\nThe idea of producing these tablet computers is partly inspired by the fact that there is a big gap between technology and education, and partly by the introduction of another series of low-cost tablets - Aakash tablets - in 2010. \n\nWishtel designed its tablets according to the required specifications from Indian Institute of Technology Rajasthan under the National Mission on Education, and are being funded by the Ministry of Human Resource.\n\nThe seven-inch tablets would be the first such devices to support 23 Indian regional languages. They operate on Android (operating system), and additional applications can be downloaded from the Android Market. \n\nThese tablets will be distributed by Wishtel’s 350 distributors nationwide at Rs. 4,000 (for Ira) and Rs. 5,500 (for Ira Thing).\n\n"}
{"id": "18718527", "url": "https://en.wikipedia.org/wiki?curid=18718527", "title": "Xelerated", "text": "Xelerated\n\nXelerated Inc., founded in 2000, was a fabless semiconductor company specializing in ASSP-based Carrier Ethernet chipsets. Xelerated carried a line of programmable Network Processing Units (NPUs) and Ethernet network switches for the Metro Ethernet, access network and high-end enterprise markets.\n\nXelerated's design relies upon a patented dataflow architecture. Like an ASIC, it is optimized in relation to the flow of packets instead of the flow of instructions through a RISC core. As a result, processing speed is predictable rather than variable. Because Xelerated's NPUs and Ethernet switches were programmable, carriers could reprogram them to adapt to new standards as they develop.\n\nThe company had offices in Santa Clara, California, Stockholm, Tel Aviv and Beijing.\n\nIn January 2012, Xelerated was acquired by Marvell Technology Group for an undisclosed sum.\n\nIn May 2015, the staff was informed about pending layoffs. In December 2015, office property from the Stockholm office was auctioned off.\n"}
