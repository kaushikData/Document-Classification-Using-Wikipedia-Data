{"id": "52642349", "url": "https://en.wikipedia.org/wiki?curid=52642349", "title": "AIVA", "text": "AIVA\n\nAIVA (Artificial Intelligence Virtual Artist) is a electronic composer recognized by the SACEM.\n\nCreated in February 2016, AIVA specializes in Classical and Symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM).\nBy reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of detecting regularities in music and on this base composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures.\n\nAIVA was presented at TED by Pierre Barreau\n\nAIVA is a published composer; its first studio album “Genesis” was released in November 2016. Second album \"Among the Stars\" in 2018.\n\n\nAvignon Symphonic Orchestra [ORAP] also performed Aiva's compositions in April 2017.\n\nThis is the preview of the score Op. n°3 for piano solo \"A little chamber music\", composed by AIVA.\n\n"}
{"id": "14449615", "url": "https://en.wikipedia.org/wiki?curid=14449615", "title": "Acidulant", "text": "Acidulant\n\nAcidulants are chemical compounds that confer a tart, sour, or acidic flavor to foods. They differ from acidity regulators, which are food additives intended to modify the stability of food or enzymes within it. Typical acidulants are acetic acid (e.g. in pickles) and citric acid. Many beverages, such as colas, contain phosphoric acid. Sour candies often are formulated with malic acid.\n\n\n"}
{"id": "6170821", "url": "https://en.wikipedia.org/wiki?curid=6170821", "title": "Arrid", "text": "Arrid\n\nArrid is a type of antiperspirant and deodorant originally introduced in 1935 by Carter Products and was acquired by Church & Dwight in 2001. The active ingredient is up to 20% aluminium zirconium tetrachlorohydrex gly.\n\nArrid deodorants and antiperspirants come in 5 different forms: solids, clear gels, sprays, roll-ons and cream.\n\nDuring the 1940s and '50s, its famous slogan was \"Don't be \"half-safe\"—use Arrid to be sure\", which gave rise to \"Half-Safe\", the name of the amphibious vehicle which Ben Carlin used to circumnavigate the world in the mid 20th century.\n\nIn 1993, Arrid was backed by a $14 million television campaign, $1.4 million in radio and $3 million in print support.\n\nIn the 2000s, one of its most popular slogans was \"Stress stinks! Arrid works!\"\n\nEach of Arrid's deodorants contain a different active ingredient. In their solid and clear gel deodorant, the active ingredient is aluminum zirconium tetrachlorohydrex gly. The active ingredient in their spray deodorant is aluminum chlorohydrate and the active ingredient in their cream is aluminum sesquichlorohydrate.\n\n"}
{"id": "31957510", "url": "https://en.wikipedia.org/wiki?curid=31957510", "title": "Automated mining", "text": "Automated mining\n\nAutomated mining involves the removal of human labor from the mining process. The mining industry is in the transition towards automation. It can still require a large amount of human capital, particularly in the developing world where labor costs are low so there is less incentive for increasing efficiency. There are two types of automated mining- process and software automation, and the application of robotic technology to mining vehicles and equipment.\n\nIn order to gain more control over their operations, mining companies may implement mining automation software or processes. Reports generated by mine automation software allow administrators to identify productivity bottlenecks, increase accountability, and better understand return on investment.\n\nAddressing concerns about how to improve productivity and safety in the mine site, some mine companies are turning to equipment automation consisting of robotic hardware and software technologies that convert vehicles or equipment into autonomous mining units.\n\nMine equipment automation comes in four different forms: remote control, teleoperation, driver assist, and full automation.\n\nRemote control mining equipment usually refers to mining vehicles such as excavators or bulldozers that are controlled with a handheld remote control. An operator stands in line-of-sight and uses the remote control to perform the normal vehicle functions. Because visibility and feel of the machine are heavily reduced, vehicle productivity is generally reduced as well using remote control. Remote control technology is generally used to enable mining equipment to operate in dangerous conditions such as unstable terrain, blast areas or in high risk areas of falling debris, or underground mining. Remote control technology is generally the least expensive way to automate mining equipment making it an ideal entry point for companies looking to test the viability of robotic technology in their mine.\n\nTeleoperated mining equipment refers to mining vehicles that are controlled by an operator at a remote location with the use of cameras, sensors, and possibly additional positioning software. Teleoperation allows an operator to further remove themselves from the mining location and control a vehicle from a more protected environment. Joysticks or other handheld controls are still used to control the vehicle's functions, and operators have greater access to vehicle telemetry and positioning data through the teleoperation software. With the operator removed from the cab, teleoperated mining vehicles may also experience reduced productivity; however, the operator has a better vantage point than remote control from on-vehicle cameras and sensors and is further removed from potentially dangerous conditions.\n\n\"Driver assist\" refers to partly automated control of mining machines. Only some of the functions are automated and operator intervention is needed. Common functions include both spotting assist and collision avoidance systems.\n\n\"Full automation\" can refer to the autonomous control of one or more mining vehicles. Robotic components manage all critical vehicle functions including ignition, steering, transmission, acceleration, braking, and implement control (i.e. blade control, dump bed control, excavator bucket and boom, etc.) without the need for operator intervention. Fully autonomous mining systems experience the most productivity gains as software controls one or more mining vehicles allowing operators to take on the role of mining facilitators, troubleshooting errors and monitoring efficiency.\n\nThe benefits of mining equipment automation technologies are varied but may include: improved safety, better fuel efficiency, increased productivity, reduced unscheduled maintenance, improved working conditions, better vehicle utilization, and reduced driver fatigue and attrition. Automation technologies are an efficient way to mitigate the effects of widespread labor shortages for positions such as haul truck driver. In the face of falling commodity prices, many mining companies are looking for ways to dramatically reduce overhead costs while still maintaining site safety and integrity; automation may be the answer.\n\nCritics of vehicle automation often focus on the potential for robotic technology to eliminate jobs while proponents counter that while some jobs will become obsolete (normally the dirty, dangerous, or monotonous jobs), others will be created. Communities supporting underprivileged workers that rely on entry level mining positions are worried about and are calling for social responsibility as mining companies transition to automation technologies that promise to increase productivity in the face of falling commodity prices. Risk averse mining companies are also reluctant to commit large amounts of capital to an unproven technology, preferring more often to enter the automation scene at lower, more inexpensive levels such as remote control.\n\nRio Tinto Group embarked on their Mine of the Future initiative in 2008. From a control center in Perth, Rio Tinto employees operate autonomous mining equipment in Australia's remote but mineral rich Pilbara region. The autonomous mining vehicles reduce the footprint of the mining giant while improving productivity and vehicle utilization. As of June 2014, Rio Tinto's autonomous mining fleet reached the milestone of 200 million tons hauled. Rio Tinto also operate a number of autonomous blast hole drill rigs.\n\nLocated near Salt Lake City, Utah, the Bingham Canyon Mine (Kennecott Utah Copper/Rio Tinto) is one of the largest open pit mine in the world and one of the world's largest copper producers. In April 2013, the mine experienced a catastrophic landslide that halted much of the mine's operations. As part of the cleanup efforts and to improve safety, mine administrators turned to remote control excavator, dozers and teleremote blast hole drills to perform work on the highly unstable terrain areas. Robotic technology helped Kennecott to reduce the steeper, more dangerous areas of the slide to allow manned vehicles access for cleanup efforts.\n\nGerman company «EEP Elektro-Elektronik Pranjic» delivered and put into operation more than 60 sets of advanced automatic control for underground coal mining for the period ~ 2006-2016. For the first time completely deserted coal mining technology has been used by the Chinese concern «China National Coal Group Corp. (CME)» at the mine «Tang Shan Gou» (longwall mining, shearers, three lava, depth 200 m), and at the mine «Nan Liang» (one plow, depth 100 m). Both coal mines have coal layer thickness 1-1.7 m. \nMonitoring the harvesting is carried out by means of video cameras (in real time with signal transmission over optical fiber). Typically, an underground staff is required to monitor the production process and for carrying out repairs. Automation has improved the safety and economic performance.\n\nBHP have deployed a number of autonomous mining components as part of their Next Generation Mining program. This includes autonomous drills and autonomous trucks in the Pilbara region.\n\n"}
{"id": "26500190", "url": "https://en.wikipedia.org/wiki?curid=26500190", "title": "BPAY", "text": "BPAY\n\nBPAY is an electronic bill payment system in Australia which enables payments to be made through a financial institution's online, mobile or telephone banking facility to organisations which are registered BPAY billers.\n\nBPAY is a registered trading name of BPAY Pty Ltd, a wholly owned subsidiary of Cardlink Services Limited. Cardlink is owned equally by the four major Australian banks: Australia and New Zealand Banking Group, Commonwealth Bank, National Australia Bank and Westpac.\n\nBusinesses or other organisations which choose to participate in the BPAY system would register as billers with the BPAY operating company through its bank. The operating company would allocate a biller number to the business, which would be printed with the BPAY logo on their bills, as well as an indication of what reference number a customer should use when making a payment. \n\nTo make a payment, a customer would visit his or her financial institution's online, mobile or telephone banking facility, where he or she would enter the biller code, reference number and payment amount as well as an indication of the account to be debited, which may also be a credit card. Some billers do not accept credit cards (or accept payment from a limited list of credit cards) in payment through BPAY. The financial institution may limit the accounts which may be used for BPAY payments, and save the biller references information for use by the customer for future payments. \n\nThe customer does not need to register for the service. The customer would usually not be required to pay a fee for the service, but the biller would usually pay a fee to its bank, and the credit card company if a card was used in payment.\n\nAfter a customer has made a payment using BPAY, the financial institution where the payment was made would send by electronic funds transfer the payment details to the biller's bank (if different) and the biller's bank would credit the biller's designated bank account with the payment amount, and the biller will be advised of the customer reference number and payment amount, for automatic or manual entry into its accounting system. Transactions completed before a cut-off time set by the financial institution will normally be processed and paid on the same day, otherwise it will be processed on the next business day. The remitting bank may charge the biller's bank an interchange fee.\n\nThe BPAY system was launched on 18 November 1997 as an electronic bill payment system for bill payments by phone. It was the world's first single bill payment service adopted across the banking sector. It soon offered bill payments over the Internet through financial institutions' online banking sites.\n\nIn 2002 BPAY View was introduced, which delivers bills and statements electronically through Australian financial institution's internet banking sites.\n\nAs of January 2015, BPAY payments can be made through more than 156 participating Australian banks, credit unions and financial institutions. More than 45,000 businesses accept payments using BPAY and each month approximately 30 million bills to the value of $24 billion are paid using BPAY.\n\n"}
{"id": "19541428", "url": "https://en.wikipedia.org/wiki?curid=19541428", "title": "Barley", "text": "Barley\n\nBarley (\"Hordeum vulgare\"), a member of the grass family, is a major cereal grain grown in temperate climates globally. It was one of the first cultivated grains, particularly in Eurasia as early as 10,000 years ago. Barley has been used as animal fodder, as a source of fermentable material for beer and certain distilled beverages, and as a component of various health foods. It is used in soups and stews, and in barley bread of various cultures. Barley grains are commonly made into malt in a traditional and ancient method of preparation.\n\nIn 2016, barley was ranked fourth among grains in quantity produced (141 million tonnes) behind maize, rice and wheat.\n\nThe Old English word for 'barley' was \"bære\", which traces back to Proto-Indo-European and is cognate to the Latin word \"farina\" \"flour\". The direct ancestor of modern English \"barley\" in Old English was the derived adjective \"bærlic,\" meaning \"of barley\". The first citation of the form \"bærlic\" in the \"Oxford English Dictionary\" dates to around 966 CE, in the compound word \"bærlic-croft\". \nThe underived word \"bære\" survives in the north of Scotland as \"bere\", and refers to a specific strain of six-row barley grown there. The word barn, which originally meant \"barley-house\", is also rooted in these words.\n\nBarley is a member of the grass family. It is a self-pollinating, diploid species with 14 chromosomes. The wild ancestor of domesticated barley, \"Hordeum vulgare\" subsp. \"spontaneum\", is abundant in grasslands and woodlands throughout the Fertile Crescent area of Western Asia and northeast Africa, and is abundant in disturbed habitats, roadsides and orchards. Outside this region, the wild barley is less common and is usually found in disturbed habitats. However, in a study of genome-wide diversity markers, Tibet was found to be an additional center of domestication of cultivated barley.\n\nWild barley (\"H. spontaneum\") is the ancestor of domestic barley (\"H. vulgare\") and harbours distinctive genes, alleles and regulators with potential for resistance to abiotic or biotic stresses to cultivated barley and adaptation to climatic changes. Wild barley has a brittle spike; upon maturity, the spikelets separate, facilitating seed dispersal. Domesticated barley has nonshattering spikes, making it much easier to harvest the mature ears. The nonshattering condition is caused by a mutation in one of two tightly linked genes known as Bt and Bt; many cultivars possess both mutations. The nonshattering condition is recessive, so varieties of barley that exhibit this condition are homozygous for the mutant allele.\n\nEach plant gets a set of genes from both parents, so two copies of each gene are in every plant. If one gene copy is a nonworking mutant, but the other gene copy works, the mutation has no effect. Only when the plant is homozygous with both copies of the gene as nonworking mutants does the mutation show its effect by exhibiting the nonshattering condition.\n\nDomestication in barley is followed by the change of key phenotypic traits at the genetic level. Little is known about the genetic variation among domesticated and wild genes in the chromosomal regions.\n\nSpikelets are arranged in triplets which alternate along the rachis. In wild barley (and other Old World species of \"Hordeum\"), only the central spikelet is fertile, while the other two are reduced. This condition is retained in certain cultivars known as two-row barleys. A pair of mutations (one dominant, the other recessive) result in fertile lateral spikelets to produce six-row barleys. Recent genetic studies have revealed that a mutation in one gene, \"vrs1\", is responsible for the transition from two-row to six-row barley.\n\nTwo-row barley has a lower protein content than six-row barley, thus a more fermentable sugar content. High-protein barley is best suited for animal feed. Malting barley is usually lower protein\n(\"low grain nitrogen\", usually produced without a late fertilizer application) which shows more uniform germination, needs shorter steeping, and has less protein in the extract that can make beer cloudy. Two-row barley is traditionally used in English ale-style beers. Six-row barley is common in some American lager-style beers, especially when adjuncts such as corn and rice are used, whereas two-row malted summer barley is preferred for traditional German beers.\n\nHulless or \"naked\" barley (\"Hordeum vulgare \"L. var.\" nudum\" Hook. f.) is a form of domesticated barley with an easier-to-remove hull. Naked barley is an ancient food crop, but a new industry has developed around uses of selected hulless barley to increase the digestible energy of the grain, especially for swine and poultry. Hulless barley has been investigated for several potential new applications as whole grain, and for its value-added products. These include bran and flour for multiple food applications.\nIn traditional classifications of barley, these morphological differences have led to different forms of barley being classified as different species. Under these classifications, two-row barley with shattering spikes (wild barley) is classified as \"Hordeum spontaneum\" K. Koch. Two-row barley with nonshattering spikes is classified as \"H. distichum\" L., six-row barley with nonshattering spikes as \"H. vulgare\" L. (or \"H. hexastichum\" L.), and six-row with shattering spikes as \"H. agriocrithon\" Åberg.\n\nBecause these differences were driven by single-gene mutations, coupled with cytological and molecular evidence, most recent classifications treat these forms as a single species, \"H. vulgare\" L.\n\n\n\n\"H. vulgare\" contains the phenolics caffeic acid and p-coumaric acid, the ferulic acid 8,5'-diferulic acid, the flavonoids catechin-7-O-glucoside, saponarin, catechin, procyanidin B3, procyanidin C2, and prodelphinidin B3, and the alkaloid hordenine.\n\nBarley was one of the first domesticated grains in the Fertile Crescent, an area of relatively abundant water in Western Asia, and near the Nile river of northeast Africa. The grain appeared in the same time as einkorn and emmer wheat. Wild barley (\"H. vulgare\" ssp. \"spontaneum\") ranges from North Africa and Crete in the west, to Tibet in the east. According to some scholars, the earliest evidence of wild barley in an archaeological context comes from the Epipaleolithic at Ohalo II at the southern end of the Sea of Galilee. The remains were dated to about 8500 BCE. Other scholars have written that the earliest evidence comes from Jarmo in Kurdistan (present day Iraq). Scholars believe domesticated barley (\"hordeum vulgare\") originally spread from Central Asia to India, Persia, Mesopotamia, Syria and Egypt. Some of the earliest domesticated barley occurs at aceramic (\"pre-pottery\") Neolithic sites, in the Near East such as the Pre-Pottery Neolithic B layers of Tell Abu Hureyra, in Syria. By 4200 BCE domesticated barley occurs as far as in Eastern Finland and had reached Greece and Italy around the 4th c. BCE. Barley has been grown in the Korean Peninsula since the Early Mumun Pottery Period (\"circa\" 1500–850 BCE) along with other crops such as millet, wheat, and legumes.\n\nBarley (known as in both Vedic and Classical Sanskrit) is mentioned many times in Rigveda and other Indian scriptures as one of the principal grains in ancient India. Traces of Barley cultivation have also been found in post-Neolithic Bronze Age Harappan civilization 5700–3300 years before present.\n\nIn the Pulitzer Prize-winning book \"Guns, Germs, and Steel\", Jared Diamond proposed that the availability of barley, along with other domesticable crops and animals, in southwestern Eurasia significantly contributed to the broad historical patterns that human history has followed over approximately the last 13,000 years; \"i.e.\", why Eurasian civilizations, as a whole, have survived and conquered others.\n\nBarley beer was probably one of the first alcoholic drinks developed by Neolithic humans. Barley later on was used as currency. The ancient Sumerian word for barley was \"akiti\". In ancient Mesopotamia, a stalk of barley was the primary symbol of the goddess Shala. Alongside emmer wheat, barley was a staple cereal of ancient Egypt, where it was used to make bread and beer. The general name for barley is \"jt\" (hypothetically pronounced \"eat\"); \"šma\" (hypothetically pronounced \"SHE-ma\") refers to Upper Egyptian barley and is a symbol of Upper Egypt. According to Deuteronomy , barley is one of the \"Seven Species\" of crops that characterize the fertility of the Promised Land of Canaan, and it has a prominent role in the Israelite sacrifices described in the Pentateuch (see e.g. Numbers ). A religious importance extended into the Middle Ages in Europe, and saw barley's use in justice, via alphitomancy and the corsned.\n\nRations of barley for workers appear in Linear B tablets in Mycenaean contexts at Knossos and at Mycenaean Pylos. In mainland Greece, the \"ritual\" significance of barley possibly dates back to the earliest stages of the Eleusinian Mysteries. The preparatory \"kykeon\" or mixed drink of the initiates, prepared from barley and herbs, referred in the Homeric hymn to Demeter, whose name some scholars believe meant \"Barley-mother\". The practice was to dry the barley groats and roast them before preparing the porridge, according to Pliny the Elder's \"Natural History\" (xviii.72). This produces malt that soon ferments and becomes slightly alcoholic.\n\nPliny also noted barley was a special food of gladiators known as \"hordearii\", \"barley-eaters\". However, by Roman times, he added that wheat had replaced barley as a staple.\n\nTibetan barley has been a staple food in Tibetan cuisine since the fifth century CE. This grain, along with a cool climate that permitted storage, produced a civilization that was able to raise great armies. It is made into a flour product called \"tsampa\" that is still a staple in Tibet. The flour is roasted and mixed with butter and butter tea to form a stiff dough that is eaten in small balls.\n\nIn medieval Europe, bread made from barley and rye was peasant food, while wheat products were consumed by the upper classes. Potatoes largely replaced barley in Eastern Europe in the 19th century.\n\nThe genome of barley was sequenced in 2012, due to the efforts of the International Barley Genome Sequencing Consortium and the UK Barley Sequencing Consortium.\n\nThe genome is composed of seven pairs of nuclear chromosomes (recommended designations: 1H, 2H, 3H, 4H, 5H, 6H and 7H), and one mitochondrial and one chloroplastic chromosome, with a total of 5000 Mbp.\n\nAbundant biological information is already freely available in several barley databases.\n\nThe wild barley (\"H. vulgare\" ssp.\" spontaneum\") found currently in the Fertile Crescent might not be the progenitor of the barley cultivated in Eritrea and Ethiopia, indicating that separate domestication may have occurred in eastern Africa.\n\nIn 2016, world production of barley was 141 million tonnes, led by the European Union producing 41% of the world total. Russia, Germany, France, and Ukraine were major producers.\n\nBarley is a widely adaptable crop. It is currently popular in temperate areas where it is grown as a summer crop and tropical areas where it is sown as a winter crop. Its germination time is one to three days. Barley grows under cool conditions, but is not particularly winter hardy.\n\nBarley is more tolerant of soil salinity than wheat, which might explain the increase of barley cultivation in Mesopotamia from the second millennium BCE onwards. Barley is not as cold tolerant as the winter wheats (\"Triticum aestivum\"), fall rye (\"Secale cereale\") or winter triticale (× \"Triticosecale\" Wittm. ex A. Camus.), but may be sown as a winter crop in warmer areas of Australia and Great Britain.\n\nBarley has a short growing season and is also relatively drought tolerant.\n\nThis plant is known or likely to be susceptible to barley mild mosaic bymovirus, as well as bacterial blight. It can be susceptible to many diseases, but plant breeders have been working hard to incorporate resistance. The devastation caused by any one disease will depend upon the susceptibility of the variety being grown and the environmental conditions during disease development. Serious diseases of barley include powdery mildew caused by \"Blumeria graminis\" f.sp. \"hordei\", leaf scald caused by \"Rhynchosporium secalis\", barley rust caused by \"Puccinia hordei\", crown rust caused by \"Puccinia coronata\", and various diseases caused by \"Cochliobolus sativus\". Barley is also susceptible to head blight.\n\nIn a 100-g serving, raw barley provides 352 Calories and is a rich source (20% or more of the Daily Value, DV) of essential nutrients, including protein, dietary fiber, the B vitamins, niacin (31% DV) and vitamin B (20% DV), and several dietary minerals (table). Highest nutrient contents are for manganese (63% DV) and phosphorus (32% DV) (table). Raw barley is 78% carbohydrates, 1% fat, 10% protein, and 10% water (table).\n\nHulled barley (or covered barley) is eaten after removing the inedible, fibrous, outer hull. Once removed, it is called dehulled barley (or pot barley or scotch barley). Considered a whole grain, dehulled barley still has its bran and germ, making it a nutritious and popular health food. Pearl barley (or pearled barley) is dehulled barley which has been steam processed further to remove the bran. It may be polished, a process known as \"pearling\". Dehulled or pearl barley may be processed into a variety of barley products, including flour, flakes similar to oatmeal, and grits.\n\nBarley meal, a wholemeal barley flour lighter than wheat meal but darker in colour, is used in porridge and gruel in Scotland. Barley meal gruel is known as \"sawiq\" in the Arab world. With a long history of cultivation in the Middle East, barley is used in a wide range of traditional Arabic, Assyrian, Israelite, Kurdish, and Persian foodstuffs including kashkak, kashk and murri. Barley soup is traditionally eaten during Ramadan in Saudi Arabia. \"Cholent\" or \"hamin\" (in Hebrew) is a traditional Jewish stew often eaten on Sabbath, in a variety of recipes by both Mizrachi and Ashkenazi Jews, with barley cited throughout the Hebrew Bible in multiple references. In Eastern and Central Europe, barley is also used in soups and stews such as ričet. In Africa, where it is a traditional food plant, it has the potential to improve nutrition, boost food security, foster rural development and support sustainable landcare.\n\nThe six-row variety \"bere\" is cultivated in Orkney, Shetland, Caithness and the Western Isles in the Scottish Highlands and islands. When milled into \"beremeal\" it is used locally in bread, biscuits, and the traditional beremeal bannock.\n\nAccording to Health Canada and the US Food and Drug Administration, consuming at least 3 grams per day of barley beta-glucan or 0.75 grams per serving of soluble fiber can lower levels of blood cholesterol, a risk factor for cardiovascular diseases.\n\nEating whole-grain barley, as well as other grains with lots of fiber, improves regulation of blood sugar (i.e., reduces blood glucose response to a meal). Consuming breakfast cereals containing barley over weeks to months also improved cholesterol levels and glucose regulation.\n\nLike wheat, rye, and their hybrids and derivatives, barley contains gluten, which makes it an unsuitable grain for consumption by people with gluten-related disorders, such as celiac disease, non-celiac gluten sensitivity and wheat allergy sufferers, among others. Nevertheless, some wheat allergy patients can tolerate barley or rye.\n\nBarley is a key ingredient in beer and whisky production. Two-row barley is traditionally used in German and English beers. Six-row barley was traditionally used in US beers, but both varieties are in common usage now. Distilled from green beer, whiskey has been made primarily from barley in Ireland and Scotland, while other countries have used more diverse sources of alcohol, such as the more common corn, rye and wheat in the USA. In the US, a grain type may be identified on a whisky label if that type of grain constitutes 51% or more of the ingredients and certain other conditions are satisfied. About 25% of the United States' production of barley is used for malting, for which barley is the best-suited grain.\n\nBarley wine is a style of strong beer from the English brewing tradition. Another alcoholic drink known by the same name, enjoyed in the 18th century, was prepared by boiling barley in water, then mixing the barley water with white wine and other ingredients, such as borage, lemon and sugar. In the 19th century, a different barley wine was made prepared from recipes of ancient Greek origin.\n\nNonalcoholic drinks such as barley water and roasted barley tea have been made by boiling barley in water. In Italy, barley is also sometimes used as coffee substitute, \"caffè d'orzo\" (coffee of barley). This drink is obtained from ground, roasted barley and it is prepared as an espresso (it can be prepared using percolators, filter machines or cafetieres). It became widely used during the Fascist period and WWII, as Italy was affected by embargo and struggled to import coffee. It was also a cheaper option for poor families (often grown and roasted at home) in the period. Afterwards, it was promoted and sold as a coffee substitute for children. Nowadays, it is experiencing a revival and it can be considered some Italians' favourite alternative to coffee when, for health reasons, caffeine drinks are not recommended.\n\nHalf of the United States' barley production is used as livestock feed. Barley is an important feed grain in many areas of the world not typically suited for maize production, especially in northern climates—for example, northern and eastern Europe. Barley is the principal feed grain in Canada, Europe, and in the northern United States. A finishing diet of barley is one of the defining characteristics of western Canadian beef used in marketing campaigns.\n\nAs of 2014, an enzymatic process can be used to make a high-protein fish feed from barley, which is suitable for carnivorous fish such as trout and salmon.\n\nBarley straw, in England, is placed in mesh bags and floated in fish ponds or water gardens to help prevent algal growth without harming pond plants and animals. Barley straw has not been approved by the EPA for use as a pesticide and its effectiveness as an algae regulator in ponds has produced mixed results, with either more efficacy against phytoplankton algae versus mat-forming algae, or no significant change, during university testing in the US and the UK.\n\nBarley grains were used for measurement in England, there being three or four barleycorns to the inch and four or five poppy seeds to the barleycorn. The statute definition of an inch was three barleycorns, although by the 19th century, this had been superseded by standard inch measures. This unit still persists in the shoe sizes used in Britain and the USA.\n\nAs modern studies show, the actual length of a kernel of barley varies from as short as to as long as depending on the cultivar. Older sources claimed the average length of a grain of barley being .\n\nThe barleycorn was known as \"arpa\" in Turkish, and the feudal system in Ottoman Empire employed the term \"arpalik\", or \"barley-money\", to refer to a second allowance made to officials to offset the costs of fodder for their horses.\n\nA new stabilized variegated variety of \"H. vulgare\", billed as \"H. vulgare\" varigate, has been introduced for cultivation as an ornamental and pot plant for pet cats to nibble.\n\nIn English folklore, the figure of John Barleycorn in the folksong of the same name is a personification of barley, and of the alcoholic beverages made from it: beer and whisky. In the song, John Barleycorn is represented as suffering attacks, death, and indignities that correspond to the various stages of barley cultivation, such as reaping and malting.\n\n"}
{"id": "2520434", "url": "https://en.wikipedia.org/wiki?curid=2520434", "title": "Beneficial organism", "text": "Beneficial organism\n\nIn agriculture and gardening, a beneficial organism is any organism that benefits the growing process, including insects, arachnids, other animals, plants, bacteria, fungi, viruses, and nematodes. Benefits include pest control, pollination, and maintenance of soil health. The opposite of beneficial organisms are pests, which are organisms deemed detrimental to the growing process.There are many different types of beneficial organisms as well as beneficial microorganisms. Beneficial organisms include but are not limited to: Birds, Bears, Nematodes, Insects, Arachnids, and fungi. The ways that birds and bears are considered beneficial is mainly because they consume seeds from plant and spread them through feces. Birds also prey on certain insects that eat plants and hinder them from growing these insects are known as non beneficial organisms. Nematodes are considered beneficial because they will help compost and provide nutrients for the soil the plants are growing in. Insects and arachnids help the growing process because they prey on non beneficial organisms that consume plants for food. Fungi help the growing process by using long threads of mycelium that can reach very long distances away from the tree or plant and bring water and nutrients back to the tree or plant roots.\n\nThe distinction between \"beneficial\" and \"pest\" is arbitrary, subjectively determined by examining the effect of a particular organism in a specific growing situation.There are many different types of beneficial organisms as well as beneficial microorganisms. Beneficial organisms include but are not limited to: Birds, Bears, Nematodes, Insects, Arachnids, and fungi. The ways that birds and bears are considered beneficial is mainly because they consume seeds from plant and spread them through feces. Birds also prey on certain insects that eat plants and hinder them from growing these insects are known as non beneficial organisms. Nematodes are considered beneficial because they will help compost and provide nutrients for the soil the plants are growing in. Insects and arachnids help the growing process because they prey on non beneficial organisms that consume plants for food. Fungi help the growing process by using long threads of mycelium that can reach very long distances away from the tree or plant and bring water and nutrients back to the tree or plant roots.\n\nWith beneficial organisms there is a flip side to these helpful organisms and that's the non beneficial organisms. These organisms hinder or stop the growing process or prey on beneficial organisms. Examples of these are Aphids, Assassin Bugs, and Japanese beetles. Aphids are attracted by pollen which is bad for plants because the aphids feed on the plants after they are located from spreading their pollen. Assassin Bugs are non beneficial because they feed on many beneficial insects by stabbing them with a horn on their head repeated times living up to its name \"Assassin bug\". Japanese beetles are especially a pest to gardeners and plants because the larva feed on the stems and roots while full grown beetles feed on leaves and flowers killing the plant.\n\nBeneficial insects can include predators (such as ladybugs) of pest insects, and pollinators (such as bees, which are an integral part of the growth cycle of many crops). Increasingly certain species of insects are managed and used to intervene where natural pollination or biological control is insufficient, usually due to human disturbance of the balance of nature.\n\nCertain microscopic nematodes (worms) are beneficial in destroying and controlling populations of larvae that are damaging or deadly to crops and other plants. They are commonly used in organic gardening for their ability to kill various kinds of harmful larvae (fungus gnats, flea larvae, spidermites, weevils, grubs, rootworms, cutworms, etc.)\n\nBirds and other animals may, by their actions, improve conditions in various growing situations, and in such cases are also beneficials. Birds assist in the spread of seeds by ingesting the fruits and berries of plants, then depositing the seeds in their droppings. Other animals, such as raccoons, bears, etc. provide similar benefits.\n\nPlants that perform positive functions can also be considered beneficials (companion planting is one technique based on principle of beneficial plants).\n\nIn agriculture, controversy surrounds the concept of beneficial insects. Much of this has to do with the effect of agrichemicals, like insecticides, herbicides and large quantities of synthetic fertilizers, on what are considered beneficials. Citing the reduction or elimination of various organisms as a side effect of agrichemical-based farming, some argue that critical damage is being done to the ecosystem, to the point where conventional agriculture is unsustainable. For example, if bee populations are reduced by insecticides aimed at other pests, pollination is inhibited and crops don't appear. If soil microorganisms are killed off, natural soil regeneration is inhibited, and reliance on mechanical and chemical inputs to keep the soil viable is increased. The longer term impact of these conditions has not been determined. Commercial ventures currently exist to provide pollinators and biological pest control.\n\n\n"}
{"id": "25120027", "url": "https://en.wikipedia.org/wiki?curid=25120027", "title": "CCIR System B", "text": "CCIR System B\n\nCCIR System B was the 625-line analog broadcast television system which at its peak was the system used in most countries. It is being replaced across Western Europe, part of Asia and Africa by digital broadcasting.\n\nThe system was developed for VHF band (part of RF band lower than 300 MHz.) Some of the most important specs are listed below.\nA frame is the total picture. The frame rate is the number of pictures displayed in one second. But each frame is actually scanned twice interleaving odd and even lines. Each scan is known as a field (odd and even fields.) So field rate is twice the frame rate. In each frame there are 625 lines (or 312.5 lines in a field.) So line rate (line frequency) is 625 times the frame frequency or 625•25=15625 Hz.\n\nThe video bandwidth is 5.0 MHz. The video signal modulates the carrier by Amplitude Modulation. But a portion of the lower side band is suppressed. This technique is known as vestigial side band modulation (AC3). The polarity of modulation is negative, meaning that an increase in the instantaneous brightness of the video signal results in a decrease in RF power and vice versa. Specifically, the sync pulses (being \"blacker than black\") result in maximum power from the transmitter.\n\nThe primary audio signal is modulated by Frequency modulation with a preemphasis time constant of τ = 50 μs. The deviation for a 1.0 kHz. AF signal is 50 kHz.\n\nThe separation between the primary audio FM subcarrier and the video carrier is 5.5 MHz.\n\nThe total RF bandwidth of System B (as originally designed with its single FM audio subcarrier) was 6.5 MHz, allowing System B to be transmitted in the 7.0 MHz wide channels specified for television in the VHF bands with an ample 500 kHz guard zone between channels.\n\nIn specs, sometimes, other parameters such as vestigial sideband characteristics and gamma of display device are also given.\n\nSystem B has variously been used with both the PAL or SECAM colour systems. It could have been used with a 625-line variant of the NTSC color system, but apart from possible technical tests in the 1950s, this has never been done officially.\n\nWhen used with PAL, the colour subcarrier is 4.43361875 MHz and the sidebands of the PAL signal have to be truncated on the high-frequency side at +570 kHz (matching the rolloff of the luminance signal at +5.0 MHz). On the low-frequency side, the full 1.3 MHz sideband is radiated. ( This behaviour would cause massive U/V crosstalk in the NTSC system, but delay-line PAL hides such artefacts. )\n\nWhen used with SECAM, the 'R' lines' carrier is at 4.40625 MHz deviating from +350±18 kHz to -506±25 kHz. The 'B' lines' carrier is at 4.250 MHz deviating +506±25 kHz to -350±18 kHz.\n\nNeither colour encoding system has any effect on the bandwidth of system B as a whole.\n\nEnhancements have been made to the specification of System B's audio capabilities over the years. The introduction of Zweiton in the 1970s allowed for stereo sound or twin monophonic audio tracks (possibly in different languages for instance). This was implemented by adding a second FM audio subcarrier at +5.74 MHz. Alternatively, starting in the late 1980s and early 1990s it became possible to replace the second audio FM subcarrier with a digital signal carrying NICAM sound. Either of these extensions to audio capability have eaten into the guard band between channels. Zweiton uses an extra 150 kHz. The alternative NICAM system uses an extra 500 kHz, and needs to be spaced further from the primary audio subcarrier, thus System B with NICAM has only 150 kHz guard zones between channels.\n\nSystem B was the first internationally accepted 625-line broadcasting standard in the world. The European 41-68 MHz Band I television allocation was agreed at the 1947 ITU (International Telecommunications Union) conference in 1947, and the first European channel plan (i.e. the use of channels E2 - E4) was agreed in 1952 at the ITU conference in Stockholm. The extension to VHF Band III (i.e. Channels E5 - E12) was also agreed in the 1950s.\n\nSince then, the System B specification has been used with slightly different broadcast frequencies in many other countries.\n\n† Channel 1 was allocated, but never used.\n\n§ Not used in the former East Germany\n\nTransmitters were operational on the above channels in 1959. During the 1960s, channels 1 to 3 were deleted and channels E3 to E12 adopted, bringing East Germany into line with the channel allocations used in the West.\n\nItalian channel-spacings were erratic. System B is no longer in use in Italy, the switchover to DVB-T having been completed 4 July 2012.\n\nNote: Band I is no longer used for television in Italy.\n\nNote: Unusually for Europe, Band III is used for DVB-T in Italy. At digital switchover time, Italy took the opportunity to discontinue their erratic System B frequencies, and the digital channels (known as Ch5 through Ch12) are regularly-spaced every 7.0 MHz from 177.5 MHz (and identical to Germany's Band III DVB-T bandplan).\n\nAustralia were unique in the world by their use of Band II for television broadcasting.\n\n‡ Channels 3, 4 and 5 were scheduled to be cleared during 1993-96 to make way for FM radio stations in Band II. This clearance action took much longer than was anticipated, and as a result, many stations on channel 3 still remain, along with a few on 4 and 5.\n\n♦ New channel allocations from 1993.\n\n‡ Channels 10 and 11 were shifted up in frequency by 1 MHz to make room for channel 9A. The frequencies of existing stations did not change; only new ones used the new allocations. Digital multiplexes on channels 10 and 11 are using the new channel boundaries.\n\nAustralia are nearly unique in the world for their use of 7 MHz channel-spacing (and therefore System B) on UHF.\n\n† Added in the 1980s\n\n‡ Added in the 1990s\n\nNote: the Band III frequencies are the same as Australia's.\n\nWhen the UHF bands came into use in the early 1960s, two variants of System B began to be used on those frequencies.\n\nIn most countries, the channels on the UHF bands are 8 MHz wide, but in most system B countries transmissions on the UHF channels still use system B specifications, the only difference being that the guard band between the channels is 1.0 MHz wider than for System B. That system for the UHF bands is known as System G and all RF specifications given above (apart from the guard band width) also apply to system G. Exceptions to this would seem to be Australia, Brunei and Tanzania where the UHF channels are 7 MHz wide, and system B is used on UHF just as it is on VHF.\n\nA few countries (Belgium, several of the Balkan states and Malta) use another variant of system B on UHF which is known as System H. System H is similar to system G but the lower (vestigial) side band is 500 kHz wider. This makes much better use of the 8.0 MHz channels of the UHF bands (though whether any system B/H televisions actually made use of the extra bandwidth is not known).\n\n\n"}
{"id": "13336906", "url": "https://en.wikipedia.org/wiki?curid=13336906", "title": "CSSIP", "text": "CSSIP\n\nCooperative Research Centre for Sensor Signal and Information Processing (CSSIP) was an organisation established under the Cooperative Research Centres Program of the Australian Government. It operated from 1992 to 2006, performing research, development, and education within several Information and Communications Technology areas:\n\n\nCSSIP's education arm was assigned to NICTA in mid-2005.\n\n\n"}
{"id": "28168060", "url": "https://en.wikipedia.org/wiki?curid=28168060", "title": "Cargo control room", "text": "Cargo control room\n\nThe cargo control room, CCR, or cargo office of a tankship is where the person in charge (PIC) can monitor and control the loading and unloading of the ship's liquid cargo. Prevalent on automated vessels, the CCR may be in its own room, or located on the ship's bridge. Among other things, the equipment in the CCR may allow the person in charge to control cargo and stripping pumps, control and monitor valve positions, and monitor cargo tank liquid levels.\n\nCargo control rooms began to appear on U.S.-flag tankers in the mid-1960s. Prior to this time, valves were operated manually on deck by reach rods and liquid levels were monitored by a roving watch consisting of the mate and seamen on watch. The use of computers in the cargo control room began in the 1980s. As technology developed, computerized systems began to centralize tasks such as cargo control \"per se\", tank level monitoring, and real-time computation of hull stress information in the cargo control room.\n\nThe design and layout of an individual cargo control room is determined by the ship's design, owner's requirements and the capabilities of the shipyard in which the ship is built. Modern cargo control rooms offer some or all of these components: main cargo pump and stripping pump control, valve control, tank level monitoring, and auxiliary functions.\n\nMain cargo pumps and stripping pumps are used to discharge cargo from the ship. From the cargo control room, the person in charge of the discharge can typically turn pumps on and off, set pump speeds, and monitor pipeline pressures on the suction- and discharge-sides of pumps.\n\nBy actuating cargo valves, the person in charge can control where cargo is pumped from, where it is pumped to, and in systems that use throttle valves, can control the relative flow rates of cargo through the valves. Modern cargo control rooms allow the person in charge to remotely control some or all of the valves in the cargo system and monitor the state of all valves. Valve indicators are typically laid out on a \"mimic panel\" which displays the cargo system piping, valves and pumps in a schematic diagram.\nTank level monitoring is another key functionality often provided in modern cargo control rooms. One aspect of tank level monitoring is overfill alarms, which sound throughout the ship when cargo levels exceed the ship's design specifications. Many systems allow the person in charge to monitor tank levels at all tank levels. Tank level monitoring allows the person in charge to take early action to avoid oil spills, especially when loading the ship. Tank level information is often sent to computers that calculate hull stresses such as shear forces and bending moments.\n\nVarious other functions are available in some cargo control rooms. Many offer the person in charge additional monitoring and control systems, the ability to monitor inert gas systems, and tank pressures. Modern cargo control rooms typically allow the person in charge to control ballast pumps and valves, and monitor oil content of ballast water by the use of oily water separators. In cases where ships carry specialty products, specialized monitoring systems are available in the cargo control room.\n\n\n\n"}
{"id": "80230", "url": "https://en.wikipedia.org/wiki?curid=80230", "title": "Clothes hanger", "text": "Clothes hanger\n\nA clothes hanger, coat hanger, or coathanger, is a device in the shape of:\n\nThere are three basic types of clothes hangers. The first is the wire hanger, which has a simple loop of wire, most often steel, in a flattened triangle shape that continues into a hook at the top. The second is the wooden hanger, which consists of a flat piece of wood cut into a boomerang-like shape with the edges sanded down to prevent damage to the clothing, and a hook, usually of metal, protruding from the point. Some wooden hangers have a rounded bar from tip to tip, forming a flattened triangle. This bar is designed to hang the trousers belonging to the jacket. The third kind and most used in today's world are plastic coat hangers, which mostly mimic the shape of either a wire or wooden hanger. Plastic coat hangers are also produced in smaller sizes to accommodate the shapes of children's clothes.\n\nSome hangers have clips along the bottom for suspending skirts. Dedicated skirt and trousers hangers may not use the triangular shape at all, instead using just a rod with clips. Other hangers have little rings coming from the top two bars to hang straps from tank-tops on. Specialized pant hanger racks may accommodate many pairs of trousers. Foldable clothes hangers that are designed to be inserted through the collar area for ease of use and the reduction of stretching are an old, yet potentially useful variation on traditional clothes hangers. They have been patented over 200 times in the U.S. alone, as in U.S. Patent 0586456, awarded in 1897 to George E. Hideout.\n\nSome historians believe President Thomas Jefferson invented a forerunner of the wooden clothes hanger. However, today's most-used hanger, the shoulder-shaped wire hanger, was inspired by a coat hook that was invented in 1869 by O. A. North of New Britain, Connecticut. An employee of the Timberlake Wire and Novelty Company, Albert J. Parkhouse of Jackson, Michigan has also been credited with the invention. The story goes that one morning in 1903, Parkhouse arrived to work to find that all coat hooks were taken. Annoyed, he took a piece of wire, bent it into the shape we would recognize today, and hung his coat. Also credited is Christopher Cann in 1876 as an engineering student at Boston University.\n\nIn 1906 Meyer May, a men's clothier of Grand Rapids, Michigan, became the first retailer to display his wares on his wishbone-inspired hangers. Some of these original hangers can be seen at the Frank Lloyd Wright designed Meyer May House in Grand Rapids.\n\nIn 1932 Schuyler C. Hulett patented an improved design, which used cardboard tubes mounted on the upper and lower parts of the wire to prevent wrinkles, and in 1935 Elmer D. Rogers added a tube on the lower bar, which is still used.\n\nHangers can be made in wood, wire, plastic, rarely from rubber substance and other materials. Some are padded with fine materials, such as satin, for expensive clothes, lingerie and fancy dresses. The soft, plush padding is intended to protect garments from shoulder dents that wire hangers may make. A caped hanger is an inexpensive wire clothing hanger covered in paper. Caped hangers are used by dry cleaners to protect garments after cleaning. Used wire hangers may be recycled, or returned to the dry cleaner.\n\nA wire clothes hanger was also a featured prop in an iconic central scene in the 1981 movie \"Mommie Dearest\", in which Joan Crawford, played by Faye Dunaway with cold cream all over her face, enters the room of her daughter, Christina, at night while she sleeps, to admire the beautiful clothes hanging nicely in her closet. She then becomes enraged upon discovering that Christina has used a wire hanger, instead of the expensive padded hangers Joan provided and instructed her to use. Fearing that her dresses are ruined, Joan wakes her daughter in terror and gives her a thrashing. Joan's fierce cry of \"No wire hangers, ever!\" quickly worked its way into pop culture since wire hangers sometimes bring distortion in the clothes and tears holes through the fabric when not used properly. \n\nWire clothes hangers play a prominent part in the 2008 movie \"\". During a key scene in this \"Romantic thriller\" directed by James Nguyen, four terrified characters defend themselves against bloodthirsty hawks and vultures by waving wire hangers over their heads in the parking lot of a San Francisco Bay Area Motel 6.\n\nUnfolded wire clothes hangers, because of their purported use in performing illegal or self-induced abortions (by inserting one in the uterus), have been used for pro-choice protests.\n\nWire is versatile, and wire clothes hangers are often used as cheap sources of semi-tough wire, more available than baling wire for all sorts of home projects. The use of wire clothes hangers for use as makeshift welding rod has been common for nearly 100 years. Similarly, many similar do-it-yourself and children's projects use wire hangers as holders of various types, from keeping a brake caliper from hanging by the brake line during auto repair work, to securing a gate on a bird cage. The much-loved 'Advent Crown' made for children's TV programme Blue Peter was made from four wire coathangers. Coathangers can be used to make dowsing rods. After sanding, wire hangers also find uses as conducting wire for uses as varied as hot wiring cars to games to testing hand steadiness. They are commonly used to gain forcible entry into older automobiles whose locks and entry systems are not protected from such methods. There is a long history of using wire coat hangers as replacement car radio antennas. Clothes hangers are also commonly used as an implement for roasting marshmallows or hot dogs at camp-outs.\n\n\"Collecticus\" magazine reported in October 2007 that clothes hangers have now become collectible, especially those with a famous company or event advertised across the front. For example, a 1950 Butlins hanger sold for £10.10 in October 2006 within \"Collecticus\".\n\nIn 1995, while performing surgery in an airliner at 35,000 feet, orthopedic surgeon Angus Wallace and his fellow doctor Tom Wong used an unfolded coathanger, sterilised with brandy, as a trocar to stiffen a catheter for use as a chest tube to relieve a passenger's pneumothorax.\n\nStraightened-out wire clothes hangers have been used to perform unsafe and/or illegal abortions (frequently self-administered) by inserting them into the uterus in an attempt to extract the embryo or fetus, a method that entails serious risks to the life and health of the woman and the baby undergoing the procedure.\n\n"}
{"id": "42122318", "url": "https://en.wikipedia.org/wiki?curid=42122318", "title": "Copper zinc antimony sulfide", "text": "Copper zinc antimony sulfide\n\nCopper zinc antimony sulfide is a relatively new class of semiconductor, derived from copper antimony sulfide (CAS). Copper antimony sulfide, a famatinite class of compound, is being explored for photovoltaic applications. Other sulfosalt group compounds such as chalcostibite and tetrahedrite also exhibit photovoltaic properties. These materials show potential for applications such as thin film solar cells, photoelectrochemical hydrogen production, thermoelectricity production, and topological insulators.\n"}
{"id": "39199983", "url": "https://en.wikipedia.org/wiki?curid=39199983", "title": "Cumming Corporation", "text": "Cumming Corporation\n\nCumming Corporation is a privately held international project management and cost consulting firm with more than 620 employees, 37 offices and a focus on serving the Education, Healthcare, Themed Entertainment, and Hospitality sectors. In 2017, the firm generated an estimated $117 million in professional fees on more than $4 billion in construction.\n\nIn 1996, Finlay Cumming established Cumming LLC in Southern California. Between 2002 and 2006, Cumming opened new offices in Northern California, Colorado, Nevada, Florida, Arizona, Texas, and Georgia. Later in 2006, Cumming was named to Engineering News-Record’s listing of the Top 100 CM-for-Fee firm in the United States, and continues to receive the honor, ranking in the top 40 every year since.\n\nIn 2007, the firm officially incorporated as Cumming Corporation and a year later in 2008, Cumming acquired Construction Controls Group (CCG), a Los Angeles-based program and project/construction management business. Shortly after the acquisition of CCG, Cumming acquired Southern Management Group (SMG), a South Carolina-based firm with more than 20 years of experience in program and project/construction management. Later in 2008, after several years working abroad, Cumming established Cumming International and opened its first overseas office in Abu Dhabi. \nCumming currently has 37 offices across the U.S. and internationally with more than 620 employees. According to Engineering News-Record (ENR), Cumming has ranked among the top 40 construction management firms on its annual listing of the Top 100 CM-for-Fee Firms in the United States for the past six years.\n"}
{"id": "35378820", "url": "https://en.wikipedia.org/wiki?curid=35378820", "title": "Draught excluder", "text": "Draught excluder\n\nA draught excluder is used to eliminate cold draught and slow heat loss. It is placed in the bottom crack of doors and windows.\n\nTubular sand-filled fabric draught excluders are commonly referred to as \"door snakes\" in Australia. Jenny Agutter told \"The Guardian\" that the hotel in the Outback where they stayed while making \"Walkabout\" used them to keep venomous snakes out of the guests' rooms. \n"}
{"id": "11716020", "url": "https://en.wikipedia.org/wiki?curid=11716020", "title": "Electrode array", "text": "Electrode array\n\nAn electrode array is a configuration of electrodes used for measuring either an electric current or voltage. Some electrode arrays can operate in a bidirectional fashion, in that they can also be used to provide a stimulating pattern of electric current or voltage.\n\nCommon arrays include:\n\nResistivity measurement of bulk materials is a frequent application of electrode arrays. The figure shows a Wenner array, one of the possible ways of achieving this. Injecting the current through electrodes separate from those being used for measurement of potential has the advantage of eliminating any inaccuracies caused by the injecting circuit resistance, particularly the contact resistance between the probe and the surface, which can be high. Assuming the material is homogenous, the resistivity is given by:\n\nElectrode arrays are widely used to measure resistivity in geophysics applications. It is also used in the semiconductor industry to measure the bulk resistivity of silicon wafers, which in turn can be taken as a measure of the doping that has been applied to the wafer, before further manufacturing processes are undertaken.\n\n\n"}
{"id": "46460495", "url": "https://en.wikipedia.org/wiki?curid=46460495", "title": "Empathy in online communities", "text": "Empathy in online communities\n\nEmpathy has been studied in the context of online communities as it pertains to enablers of interpersonal communication, anonymity, as well as barriers to online relationships, such as ambiguity, cyberbullying and Internet trolling.\nIt has been found that on online health support communities members tend to exhibit higher levels of empathic concern.\n\nA number of studies have explored the importance of empathy in offline settings. For example, one study found that mindfulness and acceptance-based behavioral approaches may have potential for increasing empathy in interpersonal relationships. Other work has explored the link between fiction and empathy, suggesting that the experience-taking quality of fiction may increase empathy among readers. There is also evidence that individuals tend to more readily feel empathy for those that they view as similar to themselves.\n\nIn online contexts, several researchers have pointed out that there are some key differences in how users interact online that may affect levels of empathy. For example, communication in online forum communities interact asynchronously, and are generally text-based rather than verbal communications. Establishment of trust in online communities may also operate differently in online environments. Furthermore, communications related interactions with others online might facilitate empathy while video or online gaming might negatively affect empathy.\n\n\n"}
{"id": "21647836", "url": "https://en.wikipedia.org/wiki?curid=21647836", "title": "Federal Engineer of the Year Award", "text": "Federal Engineer of the Year Award\n\nThe Federal Engineer of the Year Award is an annual award sponsored by the National Society of Professional Engineers and the Professional Engineers in Government advocacy group of the NSPE. \n\nThe 2009 Awards were the 30th annual award. The award recognizes technical excellence, publications, leadership, and community service. Each major subgroup of the federal government that employs 50 or more professional engineers selects and nominates an agency winner. From these a list of the top ten are selected and announced. The ultimate winner of the Federal Engineer of the Year is announced at a luncheon award ceremony during National Engineers Week.\n\n"}
{"id": "1919621", "url": "https://en.wikipedia.org/wiki?curid=1919621", "title": "Gas explosion", "text": "Gas explosion\n\nA gas explosion is an explosion resulting from mixing a gas, typically from a gas leak, with air in the presence of an ignition source. In household accidents, the principal explosive gases are those used for heating or cooking purposes such as natural gas, methane, propane, butane. In industrial explosions many other gases, like hydrogen, as well as evaporated (gaseous) gasoline (American English)/petrol (British English) or ethanol play an important role. Industrial gas explosions can be prevented with the use of intrinsic safety barriers to prevent ignition.\n\nWhether a mixture of air and gas is combustible depends on the air-to-fuel ratio. For each fuel, ignition occurs only within the explosive range (i.e. the Lower and upper explosive limits). For example, for methane and gasoline vapor, the explosive range is 5-15% and 1.4-7.6% gas to air, respectively.\n\nKline Construction, who are one of South Jersey Gas Company's contractors, was performing work in the area that was apart of a gas main and renewal project. Workers from Kline had punctured a natural main gas line nearby causing a gas leak into the basement of the building. With the buildings heat source being natural gas there was also a gas heater with a pilot light in the basement. As the gas built up, the heater's pilot light became the ignition source of this explosion. South Jersey Gas Company is a natural gas company that serves Salem County, NJ and also has offices in Atlantic County, NJ and Gloucester County, NJ. A year later the as company was fined $300,000.00 USD by the New Jersey Board of Public Utilities after their investigation found multiple safety violations. No-one was injured in the blast or fire. However, the neighboring homes of the building suffered smoke and structual damage as a result. \n\n"}
{"id": "43516586", "url": "https://en.wikipedia.org/wiki?curid=43516586", "title": "Hot melt coating", "text": "Hot melt coating\n\nHot melt coating is the application of a layer to a substrate by pre-melting the desired material and then allowing or forcing the material to cool, solidifying the layer. The process is widely used in industry, particularly for pressure-sensitive adhesives on thin substrates- self-adhesive labels.\n\nCoating is the application of a layer to a substrate. In general, in order for a thin uniform layer to be applied, the application must be done at low or moderate viscosities. In many cases this low viscosity is achieved by dissolving or dispersing the desired coating in a liquid such as water or a solvent, but this then necessitates the subsequent removal of the liquid by, for instance, a drying process. \nHot-melt coating achieves this low to moderate viscosity by melting the desired material before applying it to the substrate. The substrate and coated layer are then cooled, generally by passing over a chilled roller. There is no liquid to remove, so the process is, in principle, much faster than water- or solvent-based equivalents.\n\nThe process has been used so extensively for the application of adhesive layers by slot-die coating that the use of the term ‘hot melt’ often implies slot-die coating, but hot melt methods have also been used for other coating processes, such as metering rod or roller coating.\n\nIf the desired substrate is too temperature-sensitive to allow direct hot-melt coating, the product may be made by coating a transfer belt which is then brought into contact with the substrate after cooling: if the relative adhesion is correct then the coating will transfer to the new substrate from the transfer belt.\n\n\n"}
{"id": "173305", "url": "https://en.wikipedia.org/wiki?curid=173305", "title": "Isobutane", "text": "Isobutane\n\nIsobutane, also known as \"i\"-butane,2-methylpropane or methylpropane, is a chemical compound with molecular formula HC(CH). It is an isomer of butane. It is the simplest alkane with a tertiary carbon. Isobutane is used as a precursor molecule in the petrochemical industry, for example in the synthesis of isooctane.\n\nIsobutane is obtained by isomerization of butane.\n\nIsobutane is the principal feedstock in alkylation units of refineries. Using isobutane, gasoline-grade \"blendstocks\" are generated with high branching for good combustion characteristics. Typical products from isobutane are 2,4-dimethylpentane and especially 2,2,4-trimethylpentane.\nIn the Chevron Phillips slurry process for making high-density polyethylene, isobutane is used as a diluent. As the slurried polyethylene is removed, isobutane is \"flashed\" off, and condensed, and recycled back into the loop reactor for this purpose.\n\nIsobutane is oxidized to tert-butyl hydroperoxide, which is subsequently reacted with propylene to yield propylene oxide. The tert-butanol that results as a by-product is typically used to make gasoline additives such as methyl tert-butyl ether (MTBE).\n\nIsobutane is also used as a propellant for aerosol cans and foam products.\n\nIsobutane is used as part of blended fuels, especially common in fuel canisters used for camping.\n\nIsobutane is used as a refrigerant. The use in refrigerators started in 1993 when Greenpeace presented the Greenfreeze project with the German company Foron. In this regard, blends of pure, dry \"isobutane\" (R-600a) (that is, isobutane mixtures) have negligible ozone depletion potential and very low global warming potential (having a value of 3.3 times the GWP of carbon dioxide) and can serve as a functional replacement for R-12, R-22, R-134a, and other chlorofluorocarbon or hydrofluorocarbon refrigerants in conventional stationary refrigeration and air conditioning systems.\n\nAs a refrigerant, isobutane poses an explosion risk in addition to the hazards associated with non-flammable CFC refrigerants. \nSubstitution of this refrigerant for motor vehicle air conditioning systems not originally designed for isobutane is widely prohibited or discouraged.\n\nVendors and advocates of hydrocarbon refrigerants argue against such bans on the grounds that there have been very few such incidents relative to the number of vehicle air conditioning systems filled with hydrocarbons.\n\nThe traditional name isobutane was still retained in the 1993 IUPAC recommendations, but is no longer recommended according to the 2013 recommendations. Since the longest continuous chain in isobutane contains only three carbon atoms, the preferred IUPAC name is 2-methylpropane but the locant (2-) is typically omitted in general nomenclature as redundant; C2 is the only position on a propane chain where a methyl substituent can be located without altering the main chain and forming the constitutional isomer \"n\"-butane.\n\n"}
{"id": "44749469", "url": "https://en.wikipedia.org/wiki?curid=44749469", "title": "JanusVR", "text": "JanusVR\n\nJanusVR is a corporation based in San Mateo, California, and Toronto, Ontario, that develops immersive web browsing software. It was founded by James McCrae and Karan Singh in December 2014. Named after Janus, the Roman God of passages, JanusVR portrays web content in multi-dimensional spaces interconnected by portals. \n\nThe founders of JanusVR come from the Dynamic Graphics Project, Computer Science at the University of Toronto.\nDevelopment of JanusVR began in the middle of 2013, \nwith early progress documented on the Oculus VR Rift Forum, and subsequently on the janusVR subreddit.\nIn August 2015 JanusVR joined the Boost.VC accelerator program, and raised a Seed Series round with \nLerer Hippeau Ventures as the lead investor in January 2016.\n\nThe JanusVR platform comprises a suite of software that make it simple to create, share and experience spatially rich internet content. \nThe suite includes: \n\n\n\n\n\n\n\n"}
{"id": "56148930", "url": "https://en.wikipedia.org/wiki?curid=56148930", "title": "Kenzo Tsujimoto", "text": "Kenzo Tsujimoto\n\na Japanese businessman who founded the video game companies Irem and Capcom.\n"}
{"id": "31650902", "url": "https://en.wikipedia.org/wiki?curid=31650902", "title": "Leah Rosenfeld", "text": "Leah Rosenfeld\n\nLeah Rosenfeld (October 25, 1908 - November 12, 2006) was a railroad telegraph operator and station agent whose 1968 lawsuit against the Southern Pacific Railroad and the state of California helped to end job and wage discrimination against women and ensure equal opportunities for women in the railroad industry.\n\nAt age 16 in 1924, Rosenfeld had worked as a clerk and paralegal in a law firm. In October 1944, then 36 years old, she began her career as a railroad telegrapher and station agent with the Southern Pacific Railroad after completing courses in telegraphy and clerical work. The railroads began to hire increasing numbers of women during World War II to replace the men drafted into military service; Rosenfeld took the job to help support her growing family, then consisting of nine children.\n\nAfter her divorce in 1953, she became the sole support of 6 of her 12 children and worked in a number of one-operator stations, mostly in desert areas around the Salton Sea from Mecca, California, to Yuma, Arizona. For a time, the family lived in a refrigerator car that Southern Pacific converted into housing with added screen porches. In 1955, a position opened in Saugus, California, for an agent/telegrapher. With ten years' seniority, Rosenfeld applied for the position. The Southern Pacific Railroad denied her promotion from operator to station agent, citing the state of California's \"women's protective laws\" which barred women from lifting more than 25 pounds or working more than eight hours per day, both of which were required of station agents. However, railroad workers were covered by the federal Railroad Labor Act, which did not distinguish between male and female workers.\n\nRosenfeld then protested to her union, the Order of Railroad Telegraphers, noting that she had already performed the duties of a station agent in her previous employment, but earning a lower wage. However, the union did not support her claim.\n\nThe railroad continued to reject her claims for promotion, citing the California state law that barred women from performing the duties of station agents. However, In 1964, the passage of the federal Civil Rights Act of 1964 and its Title VII made it illegal for employers to discriminate against women in hiring practices, and created the Equal Employment Opportunity Commission (EEOC) to redress employment grievances. When the Southern Pacific Railroad gave the position of agent/telegrapher at Thermal, California, to a man with less seniority in March 1966, she took her case to the EEOC, which advised her to file suit against the State of California. On August 30, 1968, she filed suit against the State of California, the Southern Pacific Railroad, and her union, then known as the Transportation Communications International Union. On November 25, 1968, the suit against the Southern Pacific Railroad was settled and the California women's protective laws were declared unconstitutional. A subsequent appeal filed by the railroad in 1971 was also decided in Rosenfeld's favor. \n\nThe outcome of the suit was a victory for Rosenfeld, who received her promotion and pay increase, and benefited all women railroad workers. As a result, the railroad industry began hiring women in all positions in 1971 at the same pay rate as men.\n\nRosenfeld was able to receive her promotion and pay adjustment shortly before she retired from the Southern Pacific in 1974. She spent the rest of her life in Mariposa, California, where she was interviewed by railroad historian Shirley Burman and photographed by railroad photographer Richard Steinheimer in 1987. In her retirement, she helped to found the local animal shelter. She died in Mariposa in 2006.\n\n"}
{"id": "6198052", "url": "https://en.wikipedia.org/wiki?curid=6198052", "title": "Light tube", "text": "Light tube\n\nLight tubes (also known as light pipes or tubular skylights) are physical structures used for transmitting or distributing natural or artificial light for the purpose of illumination, and are examples of optical waveguides.\n\nIn their application to daylighting, they are also often called tubular daylighting devices, sun pipes, sun scopes, or daylight pipes. Light pipes may be divided into two broad categories: hollow structures that contain the light with reflective surfaces, and transparent solids that contain the light by total internal reflection. The principles governing the flow of light through these devices are those of nonimaging optics.\n\nManufacturing custom designed Infrared light pipes, hollow waveguides and homogenizers is non-trivial. This is because these are tubes lined with a highly polished infrared reflective coating of Laser Gold, which can be applied thick enough to permit these tubes to be used in highly corrosive atmospheres. Laser Black can be applied to certain parts of light pipes to absorb IR light (see photonics). This is done to limit IR light to only certain areas of the pipe.\n\nWhile most light pipes are produced with a round cross-section, light pipes are not limited to this geometry. Square and hexagonal cross-sections are used in special applications. Hexagonal pipes tend to produce the most homogenized type of IR Light. The pipes do not need to be straight. Bends in the pipe have little effect on efficiency.\n\nAlso known as a \"tubular skylight\" or \"tubular daylighting device\", this is the oldest and most widespread type of light tube used for daylighting. The concept was originally developed by the ancient Egyptians. The first commercial reflector systems were patented and marketed in the 1850s by Paul Emile Chappuis in London, utilising various forms of angled mirror designs. Chappuis Ltd's reflectors were in continuous production until the factory was destroyed in 1943. The concept was rediscovered and patented in 1986 by Solatube International of Australia. This system has been marketed for widespread residential and commercial use. Other daylighting products are on the market under various generic names, such as \"SunScope\", \"solar pipe\", \"light pipe\", \"light tube\" and \"tubular skylight\".\n\nA tube lined with highly reflective material leads the light rays through a building, starting from an entrance-point located on its roof or one of its outer walls. A light tube is not intended for imaging (in contrast to a periscope, for example), thus image distortions pose no problem and are in many ways encouraged due to the reduction of \"directional\" light.\n\nThe entrance point usually comprises a dome (cupola), which has the function of collecting and reflecting as much sunlight as possible into the tube. Many units also have directional \"collectors\", \"reflectors\" or even Fresnel lens devices that assist in collecting additional directional light down the tube.\n\nA set-up in which a laser cut acrylic panel is arranged to redirect sunlight into a horizontally or vertically orientated mirrored pipe, combined with a light spreading system with a triangular arrangement of laser cut panels that spread the light into the room, was developed at the Queensland University of Technology in Brisbane. In 2003, Veronica Garcia Hansen, Ken Yeang, and Ian Edmonds were awarded the Far East Economic Review Innovation Award in bronze for this development.\n\nLight transmission efficiency is greatest if the tube is short and straight. In longer, angled, or flexible tubes, part of the light intensity is lost. To minimize losses, a high reflectivity of the tube lining is crucial; manufacturers claim reflectivities of their materials, in the visible range, of up to almost 99.5 percent.\n\nAt the end point (the point of use), a diffuser spreads the light into the room.\n\nTo further optimize the use of solar light, a heliostat can be installed which tracks the movement of the sun, thereby directing sunlight into the light tube at all times of the day as far as the surroundings´ limitations allow, possibly with additional mirrors or other reflective elements that influence the light path. The heliostat can be set to capture moonlight at night.\n\nCorning Inc. makes Fibrance Light-Diffusing Fiber. Fibrance works by shining a laser through a light diffusing fiber optic cable. The cable gives off a lighted glow.\n\nOptical fibers are used in fiberscopes for imaging applications.\n\nOptical fibers can also be used for daylighting. A solar lighting system based on plastic optical fibers was in development at Oak Ridge National Laboratory in 2004. The system was installed at the American Museum of Science and Energy, Tennessee, USA, in 2005, and brought to market the same year by the company Sunlight Direct. However, this system was taken off the market in 2009.\n\nOptical fibers are also used in the Bjork system sold by Parans Solar Lighting AB. The optic fibers in this system are made of PMMA (PolyMethylMethAcrylate) and sheathed with Megolon, a halogen-free thermoplastic resin. A system such as this, however, is quite expensive.\n\nA similar system, but using optical fibers of glass, had earlier been under study in Japan.\n\nIn view of the usually small diameter of the fibers, an efficient daylighting set-up requires a parabolic collector to track the sun and concentrate its light.\nOptical fibers intended for \"light transport\" need to propagate as much light as possible within the core; in contrast, optical fibers intended for \"light distribution\" are designed to let part of the light leak through their cladding.\n\nA prism light guide was developed in 1981 by Lorne Whitehead, a physics professor at the University of British Columbia and has been used in solar lighting for both transport and distribution of light. A large solar pipe based on the same principle has been set up in a narrow courtyard of a 14-floor building of a Washington, D.C. law firm in 2001, and a similar proposal has been made for London. A further system has been installed in Berlin.\n\nThe 3M company developed a system based on optical lighting film and developed the 3M light pipe, which is a light guide designed to distribute light uniformly over its length, with a thin film incorporating microscopic prisms, which has been marketed in connection with artificial light sources, e.g. sulfur lamps.\n\nIn contrast to an optical fiber which has a solid core, a prism light guide leads the light through air and is therefore referred to as hollow light guide.\n\nThe project ARTHELIO, partially funded by the European Commission, was an investigation in years 1998 to 2000 into a system for adaptive mixing of solar and artificial light, and which includes a sulfur lamp, a heliostat, and hollow light guides for light transport and distribution.\n\nDisney has experimented in using 3D printing to print internal light guides for illuminated toys.\n\nIn a system developed by Fluorosolar and the University of Technology, Sydney, two fluorescent polymer layers in a flat panel capture short wave sunlight, particularly ultraviolet light, generating red and green light, respectively, which is guided into the interior of a building. There, the red and green light is mixed with artificial blue light to yield white light, without infrared or ultraviolet. This system, which collects light without requiring mobile parts such as a heliostat or a parabolic collector, is intended to transfer light to any place within a building.  By capturing ultraviolet the system can be especially effective on bright but overcast days; this since ultraviolet is diminished less by cloud cover than are the visible components of sunlight.\n\nSolar light pipes, compared to conventional skylights and other windows, offer better heat insulation properties and more flexibility for use in inner rooms, but less visual contact with the external environment.\n\nIn the context of seasonal affective disorder, it may be worth consideration that an additional installation of light tubes increases the amount of natural daily light exposure. It could thus possibly contribute to residents´ or employees´ well-being while avoiding over-illumination effects.\n\nCompared to artificial lights, light tubes have the advantage of providing natural light and of saving energy. The transmitted light varies over the day; should this not be desired, light tubes can be combined with artificial light in a hybrid set-up.\n\nSome artificial light sources are marketed which have a spectrum similar to that of sunlight, at least in the human visible spectrum range, as well as low flicker. Their spectrum can be made to vary dynamically such as to mimick the changes of natural light over the day. Manufacturers and vendors of such light sources claim that their products can provide the same or similar health effects as natural light. When considered as alternatives to solar light pipes, such products may have lower installation costs but do consume energy during use; therefore they may well be more wasteful in terms of overall energy resources and costs.\n\nOn a more practical note, light tubes do not require electric installations or insulation, and are thus especially useful for indoor wet areas such as bathrooms and pools. From a more artistic point of view, recent developments, especially those pertaining to transparent light tubes, open new and interesting possibilities for architectural design.\n\nLight tubes have been used in schools, warehouses, retail environments, homes, government buildings, museums, hotels and restaurants.\n\nDue to the relatively small size and high light output of sun pipes, they have an ideal application to security oriented situations, such as prisons, police cells and other locations where restricted access is required. Being of a narrow diameter, and not largely affected by internal security grilles, this provides daylight to areas without providing electrical connections or escape access, and without allowing objects to be passed into a secure area.\n\nMolded plastic light tubes are commonly used in the electronics industry to direct illumination from LEDs on a circuit board to indicator symbols or buttons. These light tubes typically take on a highly complex shape that uses either gentle curving bends as in an optic fiber or have sharp prismatic folds which reflect off the angled corners. Multiple light tubes are often molded from a single piece of plastic, permitting easy device assembly since the long thin light tubes are all part of a single rigid component that snaps into place.\n\nLight tube indicators make electronics cheaper to manufacture since the old way would be to mount a tiny lamp into a small socket directly behind the spot to be illuminated. This often requires extensive hand-labor for installation and wiring. Light tubes permit all lights to be mounted on a single flat circuit board, but the illumination can be directed up and away from the board by several inches, wherever it is required.\n\n\n\n"}
{"id": "2541783", "url": "https://en.wikipedia.org/wiki?curid=2541783", "title": "List of cyclists", "text": "List of cyclists\n\nThis is an incomplete list of professional racing cyclists, sorted alphabetically by decade in which they won their first major race.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50223751", "url": "https://en.wikipedia.org/wiki?curid=50223751", "title": "MCDRAM", "text": "MCDRAM\n\nMulti-Channel DRAM or MCDRAM (pronounced \"em cee dee ram\") is a 3D-stacked DRAM that is used in the Intel Xeon Phi processor codenamed Knights Landing. It is a version of Hybrid Memory Cube developed in partnership with Micron, and a competitor to High Bandwidth Memory. \n\nThe many cores in the Xeon Phi processors, along with their associated vector processing units, enable them to consume many more gigabytes per second than traditional DRAM DIMMs can supply. The \"Multi-channel\" part of the MCDRAM full name reflects the cores having many more channels available to access the MCDRAM than processors have to access their attached DIMMs.\n\nThis high channel count leads to MCDRAM's high bandwidth, up to 400+ GB/s, although the latencies are similar to a DIMM access.\n\nIts physical placement on the processor imposes some limits on capacity - up to 16 GB at launch, although speculated to go higher in the future.\n\nThe memory can be partitioned at boot time, with some used as cache for more distant DDR, and the remainder mapped into the physical address space.\n\nThe application can request pages of virtual memory to be assigned to either the distant DDR directly, to the portion of DDR that is cached by the MCDRAM, or to the portion of the MCDRAM that is not being used as cache. One way to do this is via thecodice_1 API.\nWhen used as cache, the latency of a miss accessing both the MCDRAM and DDR is slightly higher than going directly to DDR, and so applications may need to be tuned \n\nto avoid excessive cache misses.\n\n"}
{"id": "18433462", "url": "https://en.wikipedia.org/wiki?curid=18433462", "title": "Martifer", "text": "Martifer\n\nMartifer is a family group based in Oliveira de Frades, Portugal, with over 3,000 employees, focusing its activity on the metal construction and renewable energy areas.\n\nThe company launched its first operations in 1990 in the metal structure industry. In 2004, it entered the renewables business, leveraging know-how from the metal construction operation to develop the energy equipment division.\n\nMartifer is market leader in Iberia for metal construction, and aims to become the top player in other specific markets, namely Europe and Angola.\n\nIn renewable energies, Martifer would like to become established as an integrated producer of turn-key solutions for the wind and solar segments. Furthermore, within renewable energies, Martifer operates as a promoter of electricity generation projects, with a portfolio of holdings in projects at different phases of development.\n\nMartifer SGPS, SA is the Group’s holding company and has been listed on the Euronext Lisbon since June 2007. In 2008, its core activity operating revenues reached EUR 650 million. The main shareholder structure is made up of the founding partners, through I’M SGPS, S.A., and the Mota-Engil Group, both control approximately 80% of the company.\n\nIn June 2009, Martifer and Hirschfeld Wind Energy Solutions, a part of Hirschfeld Industries in Texas, United States, announced a Joint venture for the manufacture of wind towers and related components in the United States. The project cost was estimated to cost $40 million US dollars to build a new facility in San Angelo, Texas. Production was originally expected to reach a capacity of 400 towers a year by 2013. However, the operation was unable to compete effectively in the very difficult pricing environment for wind towers in the 2009-2011 time frame and the investment failed to reach its objectives. In July 18, 2012, Hirschfeld announced that it had acquired the remaining 50 percent of the joint venture from Martifer for a small cash payment, and as of 2014, had converted the plant to build steel bridge structures. The last wind tower was produced there in 2012.\n\nThe company was founded by two brothers, Carlos Martins and Jorge Martins in Oliveira de Frades, Central Portugal. The enterprise has started with 18 employees. Today the group has worldwide 3,000 employees. Subsidiaries are in Spain (1999), Poland (2003), Romania (2005), Czech Republic (2005), Slovakia (2005), Germany (2005), Brazil (2006) and Angola (2006).\n\n"}
{"id": "41155509", "url": "https://en.wikipedia.org/wiki?curid=41155509", "title": "Ministry of Science, Technology and Environment (Nepal)", "text": "Ministry of Science, Technology and Environment (Nepal)\n\nMinistry of Science, Technology and Environment (1976) is the government body for sustainable and broad based economic growth contributing to employment generation and poverty reduction in Nepal.\n\nTo establish science, technology and environment as key pillars for achieving sustainable and broad based economic growth contributing to employment generation and poverty reduction.\n\nTo create policy environment and institutional strength for promoting scientific research, innovation and capacity building to achieve sustainable practices and technologies, to minimize risks on life support systems, thus contributing to sustainable development.\n"}
{"id": "31783262", "url": "https://en.wikipedia.org/wiki?curid=31783262", "title": "Mobile marketing research", "text": "Mobile marketing research\n\nMobile Marketing Research describes a method of data collection means by using functions of mobile phones, smart phones and PDAs. It makes use of strengths from mobile communication and applies these strengths to research purposes.\n\nDue to sociocultural changes towards digitization in everyday life, reachability and lifestyle at the turn of the 21st century, it became more and more difficult for the marketing research industry to address respondents that are both willing to participate in surveys and reachable via traditional media. Consumers were no longer clearly classified which made it harder to make obvious assumptions about consumer behavior. Researchers had to adopt their approaches to keep up with technology as well as coping with these changes in order to find out who customers are and what they want.\n\nMobile phones are practical, versatile and have already become an indispensable device for communication which is carried along almost around the clock. Smartphones in particular offer many different kinds of applications and fulfill further technical requirements. There are several mobile applications that provide mobile market research through smartphones. This is the basis to allow conducting empirical research studies. Besides, the number of mobile phones, smart phones and PDAs is still growing: already in 2006, the number of worldwide mobile phone user exceeded the number of land line users. According to a study of PEW \"(Internet & American Life Projects)\", which examined more than 1000 internet leader, -analysts and -executives, mobile devices will become the most often used instrument to connect to internet by the year 2020.\nAccording to Pew Research’s Internet & American Life study in January 2014, 58% of adults in the US had a smartphone, up from 56% in May 2013. In other parts of the world, like Central and Latin America, many people never had a PC but they now have a smartphone. For instance in Chile, more than 85% of the participants of an online panel had a smartphone in 2014, whereas only around 60% had a desktop. For many people the mobile phone is already a kind of \"life-support tool”. Therefore, conducting surveys via mobile phones could possibly help to counteract against the generally declining motivation of respondents, as many potential participants enjoy using this technical device. Particularly the very technology-savvy young target is difficult to reach by traditional media as compared to more modern media such as cell phones. Therefore, this medium is very appropriate to address this target group. Even if some respondents declare that they still prefer to answer web surveys through PCs, the proportions of panelists declaring that they prefer to answer through smartphones are clearly non-negligeable anymore across many different countries \n\nAccording to Maxl, four different kinds of methods are distinguished which can be used as required. These are listed in the following figure.\n\nPush studies without contexts (A) are conducted independently from time and location. This can be CATI- or CACI-studies as well as surveys by SMS or MMS. In cases of these research methods, the feedback impulse actively comes from the researcher.\nWith contextual push studies (B) the researcher prompts the respondent to give appropriate feedback once it is recognized that he/she is located in a particular environment or is in a certain situation.\nPull studies are characterized that participants call in the questionnaire themselves. In many cases short notes draw attention to a survey or an evaluation. Such communications may be placed in certain contexts (C) (e.g. on receipts, advertisements or product packaging) in order to encourage participation. Non-context-sensitive pull studies (D) are not relevant for marketing research since they provide only general feedback, which has no relation to a fixed object of research. Therefore, they are hardly controlled and evaluated.\n\nOn the technical level, three different possibilities of data collection are distinguished according to Pferdekämper/Batinic. The Short Message Service (SMS) outlines one possibility, which can be used as a basis to conduct interviews. This is very suitable for ad hoc surveys, if particular key questions have to be answered.\nFor Java applications, the participants receive a link to a WAP page where an application is to be downloaded. The survey software is immediately installed on the mobile phone and the survey can be filled out straight away.\nFurthermore, surveys can be accessed via the mobile Internet. The webpage is also called in via WAP push. Questionnaires are created and designed in different formats. This allows disrupting to complete the questionnaire in order to further process on a different technical device with internet access.\n\nProbably most important point to be discussed is the representativity of the sampling regarding the quality of data.\nBased on the total population, those people who don’t own a mobile phone, cannot become part of the sampling. Mobile phone users have to be able to be reached as well as to be willing to take part in the survey. But it is by far not enough to just look at the users of mobile phones. Equally important is to also know the number of people who use the SMS service and how many of these people own a web-enabled phone. Only if all these aspects are being noticed, a final conclusion regarding the sampling can be drawn. For these reasons a mobile survey does not meet the standards of the statistical representativity.\nAs mentioned in the beginning, the restricted possibilities to access WAP-Surveys are another problem. The number of those who use the internet via their mobile phone is rather small: According to IFCom only 19% of all mobile phone users do so. But the data acquisition through mobile phones holds more difficulties. On one hand there are high costs for Incentives and an intensive recruitment by the institutes. On the other hand, the participants of the survey have to pay the costs for the internet usage via mobile phone. However, more and more respondents do not pay the internet access based on the time spent on Internet and more and more places proposes free Wi-Fi, making this issue less and less relevant for respondents. Additionally, there are also technical difficulties that may occur. The high number of different and not-compatible software or the low transmission rate of data are just a few to name. In order to improve the respondents' experience in answering the web surveys on mobile devices, optimizing the layout of the survey for smaller screen is crucial: non-optimized layouts lead to lower data quality.\nRestrictions concerning the reveal and passing on of mobile phone numbers (because of data protection and the lack of Anonymity) are setting limits to the research in this field of study, too.\n\nStatistical methods in the mobile market research profit from the fact that they do not depend on place or time. This means that surveys via mobile phone can be done anywhere and at any time and are therefore much more advantageous compared to surveys via land line phones. Considering that a high number of participants of the main target groups can not be reached at home at most hours of the day, the chance to get hold of them by mobile phone is much higher.\nEspecially with regard to people under the age of 25 as well as business people, who are often on the way and cannot get hold of easily, the chances of the mobile market research are promising. As most people always carry their mobile phone with them, an immediate transmission of personal impressions of current events, is possible at any time. Thus, the so-called ‘magic moment’ can be captured, which is often very helpful. Surveys concerning product placement and efficiency of sales promotion measures at the point of sale turned out to be perfectly realizable this way. Looking at the topic “Access possibilities to WAP-Surveys” by mobile phone, as well as at the topic “Getting hold of potential participants”, there are some developments that should be named. The trend towards the increasing distribution of web-enabled mobile phones, and the improved representativity that comes along with it, does still continue: from 2007 to 2008, \"Nielsen Mobile\" noticed an increase of the mobile internet usage.\nWith decreasing costs and an improving technology, a further upward tendency can be expected. In addition, the mobile internet usage increases with the rapidly developing distribution of smart phones. According to IM (Mediawork Initiative), already in 2013 more people will log in with their smart phone, than with a PC. Mobile statistics show a high and very fast response rate. As a result of the independency from time and place, answers can be submitted immediately. This also means that results can almost be transmitted in real time. A study from Globalpark shows that approximately 35% of the participants answered a mobile survey within 2 hours. Through the novelty of the form of survey and its playful design, mobile research has a motivating effect on the participants. This applies especially to young, technologically interested male persons. But also iPhone users are considered to be very communicative. As the devices are easy to handle, the users are more likely and more motivated to take part in surveys. A study examining how to best reach mobile respondents published in the Journal Social Science Computer Review underlines the importance of social factors. Especially their subjective belief of how they are seen by significant others, or their intention of how they would like to be seen by significant others seem to play a major role in their mobile phone behavior.\n"}
{"id": "3067617", "url": "https://en.wikipedia.org/wiki?curid=3067617", "title": "Mobile weapons laboratory", "text": "Mobile weapons laboratory\n\nMobile weapons laboratories are bioreactors and other processing equipment to manufacture and process biological weapons that can be moved from location to location either by train or vehicle.\n\nIn the run up to the 2003 Invasion of Iraq, the main rationale for the Iraq War was Hussein's Iraq failure to transparently and verifiably cease Weapons of Mass Destruction (WMD - nuclear, biological and chemical weapons) programs, and to destroy all materials relating thereto, as mandated in United Nations Resolution 1441. In February 2003 the then Secretary of State Colin Powell gave a presentation before the United Nations showing a computer generated view of what the laboratories looked like. He said Iraq had as many as 18 mobile facilities for making anthrax and botulinum toxin. \"They can produce enough dry, biological agent in a single month to kill thousands upon thousands of people.\" Powell based the assertion on accounts of at least four Iraqi defectors, including a chemical engineer who supervised one of the facilities and been present during production runs of a biological agent.\n\nIn the CIA briefing days before the 2003 United Nations security council presentation Colin Powell knew that all information included in the report had to be solid. \"Powell and I were both suspicious because there were no pictures of the mobile labs,\" Wilkerson, Powell's chief of staff said. Powell demanded multiple sources and the two CIA men present George Tenet, then the CIA director and John E. McLaughlin, then the CIA deputy director claimed to have multiple eye witness accounts and supporting evidence. Wilkerson claims that the two said, \"This is it, Mr. Secretary. You can't doubt this one\"\n\nThe information behind the mobile vehicles had come from the multiple informants but the main and most important one was known as Curveball. Curveball was an Iraqi refugee in Germany.\nHe claimed that after he had graduated at the top of his chemical engineering class at Baghdad University in 1994, he worked for \"Dr. Germ,\" the pseudonym of British-trained microbiologist Rihab Rashid Taha. He led a team that built mobile labs to create biological WMD Curveball was never actually interviewed by American intelligence and in May 2004, over a year after the invasion of Iraq, the CIA concluded formally that Curveball's information was fabricated. Furthermore, on June 26, 2006, the Washington Post reported that \"the CIA acknowledged that Curveball was a con artist who drove a taxi in Iraq and spun his engineering knowledge into a fantastic but plausible tale about secret bioweapons factories on wheels.\"\n\nWith information about the mobile labs the Bush administration then went and asked Ahmed Chalabi's Iraqi National Congress (INC) if they knew anything about this \"threat\". The INC provided an Iraqi defector, Mohammad Harith, who claimed that while working for the Iraqi government he had purchased seven Renault refrigerated trucks to be converted into mobile biological weapons laboratories. The INC used James Woolsey, former director of the CIA, to directly contact Deputy Assistant Defense Secretary Linton Wells, of the Defense Intelligence Agency (DIA), with info about Mohammad Harith's account to avoid any scrutiny by the CIA. Harith's was met by a DIA debriefer who concluded that it \"seemed accurate, but much of it appeared embellished\" and he apparently \"had been coached on what information to provide.\" However, the line about Harith being coached was removed and one that he passed a lie detector added and as such became official evidence of mobile bio-labs even being used by Bush in his January 2003 State of the Union message. Later Mohammad Harith like curveball evidence was labeled with a fabricator notice.\n\nA third source, reporting through Defense HUMINT channels and another asylum seeker, claimed that in June 2001 that Iraq had mobile biological weapons laboratories however after the war in Oct 2003 the source recanted his testimony.\n\nA fourth source existed but all information and details regarding the report are still classified.\n\nAll the sources depended on the Curveball's account and were seen as supportive to it. When Tenet called Powell in late summer 2003, seven months after the U.N. speech, he admitted that all of the CIA's claims Powell used in his speech about Iraqi weapons were wrong. \"They had hung on for a long time, but finally Tenet called Powell to say, 'We don't have that one, either,' \" Wilkerson recalled. \"The mobile labs were the last thing to go.\"\n\nMay 13, 2003 it was reported that a second suspected mobile weapons lab had been found in Iraq on April 19, 2003.\n\nMay 27, 2003 a fact finding mission to Iraq sent its report to Washington unanimously declaring that the trailers had nothing to do with biological weapons. The report was 'shelved'.\n\nMay 28, 2003 the Central Intelligence Agency released a report on the supposed mobile weapons labs, stating:-\n\nDespite the lack of confirmatory samples, we nevertheless are confident that this trailer is a mobile BW production plant. \n\nMay 29, 2003 President George W Bush declared that they had found the weapons of mass destruction that had been claimed were in Iraq, these were in the form of mobile labs for manufacturing biological weapons.\n\nWe found the weapons of mass destruction. We found biological laboratories. You remember when Colin Powell stood up in front of the world, and he said, Iraq has got laboratories, mobile labs to build biological weapons. They're illegal. They're against the United Nations resolutions, and we've so far discovered two. And we'll find more weapons as time goes on. But for those who say we haven't found the banned manufacturing devices or banned weapons, they're wrong, we found them. \n\nMay 29, 2003 \"We have already found two trailers that both our and the American security services believe were used for the manufacture of chemical and biological weapons.\" Tony Blair, Flying into Kuwait for morale boosting trip.\n\nMay 29, 2003 \"My personal view is we're going to find them, just as we found these two mobile laboratories\" Town Hall Meeting with Secretary of Defense Donald Rumsfeld, Infinity-CBS Radio.\n\nJune 2, 2003 In the UK, Susan Watts broadcasts on the influential BBC2 Newsnight report which includes an anonymous experts (Dr David Kelly ) opinion on the Mobile Weapons labs being for biological weapons. Dr Kelly is now only 40% certain the trailers are labs.\n\nBut our source, who is in an excellent position to know, and spoke of being 90% confident these claims are correct on the day the Pentagon showed the trucks to the world, now put that confidence level at just 40%. \n\nJune 3, 2003 \"But let's remember what we've already found. Secretary Powell on February 5th talked about a mobile, biological weapons capability. That has now been found and this is a weapons laboratory trailers capable of making a lot of agent that -- dry agent, dry biological agent that can kill a lot of people. So we are finding these pieces that were described.\" Condoleezza Rice, Capital Report, CNBC.\n\nJune 3, 2003 \"We know that these trailers look exactly like what was described to us by multiple sources as the capabilities for building or for making biological agents. We know that we have from multiple sources who told us that then and sources who have confirmed it now. Now the Iraqis were not stupid about this. They were able to conceal a lot. They've been able to scrub things down. But I think when the whole picture comes out, we will see that this was an active program.\" Condoleezza Rice, Capital Report, CNBC.\n\nJune 5, 2003 \"We recently found two mobile biological weapons facilities which were capable of producing biological agents\" President G W Bush Talks to Troops in Qatar, White House.\n\nJune 5, 2003 Dr. David Kelly one of Britains foremost experts on Biological Weapons visited Iraq to examine the trailers and take photographs.\n\nJune 7, 2003 Judith Miller reports that some scientists had doubts about the trailers in her piece - \"Some experts doubt trailers were germ lab\", Judith Miller and William J. Broad, New York Times.\n\nJune 8, 2003 The UK's Observer newspaper picks up on the story with their piece \"Blow to Blair over 'mobile labs' - Saddam's trucks were for balloons, not germs \" Placing more pressure on Prime Minister Tony Blair over the lack of Weapons of Mass Destruction found in Iraq.\n\nJune 8, 2003 \"Already, we've discovered, uh, uh, trailers, uh, that look remarkably similar to what Colin Powell described in his February 5th speech, biological weapons production facilities.\" Condoleezza Rice, This Week with George Stephanopoulos, ABC.\n\nJune 8, 2003 \"We are confident that we -- I believe that we will find them. I think that we have already found important clues like the biological weapons laboratories that look surprisingly like what Colin Powell described in his speech.\" Condoleezza Rice, Meet the Press, NBC.\n\nJune 8, 2003 \"We have uncovered the mobile vans and we are continuing to search.\" Colin Powell Remarks at Stakeout Following Fox News Interview, Fox News.\n\nJune 8, 2003 \"And I think the mobile labs are what I think is a good indication of the kind of thing they are doing.\" Colin Powell Remarks at Stakeout Following Fox News Interview, Fox News.\n\nJune 15, 2003 It was revealed that the trailers discovered were for the production of hydrogen to fill artillery balloons, as the Iraqis had insisted all along. The artillery balloons were used to get detailed weather data to be used to accurately direct artillery shelling. A British scientist and biological weapons expert was quoted \"They are not mobile germ warfare laboratories. You could not use them for making biological weapons. They do not even look like them. They are exactly what the Iraqis said they were - facilities for the production of hydrogen gas to fill balloons.\" It was confirmed later that this expert was Dr David Kelly \n\nJune 20, 2003 MP Paul Flynn: \"To ask the Prime Minister what assessment has been made of the function of the two vehicles suspected of being biological weapons laboratories that were discovered in Iraq.\" The UK Prime Minister Tony Blair: \"Investigations into their role are continuing.\"\n\nJune 23, 2003:\n\nJuly 17/18, 2003: Dr. David Kelly, a key source for many of the newspaper articles doubting the Mobile weapons labs, is found dead. An inquiry into his death, The Hutton Inquiry, found his death to be suicide.\n\nSeptember 8, 2003: \n\nSeptember 14, 2003: \n\nJanuary 22, 2004: \n\nThe Pentagon produced a secret report in 2003 entitled \"Final Technical Engineering Exploitation Report on Iraqi Suspected Biological Weapons-Associated Trailers\" that found that the trailers were impractical for biological agent production and almost certainly designed and built for the generation of hydrogen.\n\n"}
{"id": "12158680", "url": "https://en.wikipedia.org/wiki?curid=12158680", "title": "Nordic Ware", "text": "Nordic Ware\n\nNordic Ware (also known as Northland Aluminum Products, Inc.) is a company based in the Minneapolis suburb of St. Louis Park, Minnesota, United States, notable for introducing the Bundt cake pan in the early 1950s.\n\nIt was founded in 1946 by Henry David Dalquist (May 25, 1918 – January 2, 2005), who trademarked the name Bundt in 1950, his wife Dorothy, his brother Mark, and their friend Donald Nygren. Nordic Ware remains family-owned and operated, and David Dalquist (son of founders Henry David and Dorothy Dalquist) is the current company President.\n\nIn addition to the Bundt cake pan, Nordic Ware is also a pioneer in the field of microwave cookware. They introduced products such as the patented Micro-Go-Round, better known as the automated food rotator.\n\nMore than 70 million Bundt pans have been sold by Nordic Ware across North America. To mark the 60th anniversary of the pan the company designated November 15 as 'National Bundt Day'. The company also runs a competition every year 'Bundts Across America', celebrating the best Bundt cake creations as well as held cooking classes twice a month, held on Tuesday evenings from 6:30 to 8:30 p.m. Each session caters only 24 people.\n\nNordic Ware is one of the few remaining American cookware companies that produce their products almost entirely in the United States and also offers their products to 50 countries. Only three of their items are ineligible for international shipment - Baker's Joy Cooking Spray, Bundt Cake Mixes, and Kettle Smoker.\n\nAt their headquarters and manufacturing plant in Saint Louis Park, the Nordic Ware branding is painted on the Peavey–Haglin Experimental Concrete Grain Elevator near the interchange of Minnesota State Highway 100 and Minnesota State Highway 7. The grain elevator was the first reinforced concrete circular grain elevator in the United States, and possibly in the world. Prior to Nordic Ware, the grain elevator carried the sign for 'Lumber Stores Inc' until Nordic Ware purchased the land as it expanded and invested $40,000 in a restoration project of it.\n\n"}
{"id": "30071033", "url": "https://en.wikipedia.org/wiki?curid=30071033", "title": "Pipeline pre-commissioning", "text": "Pipeline pre-commissioning\n\nPipeline pre-commissioning is the process of proving the ability of a pipeline and piping systems to contain product without leaking. This product may be liquid, gaseous or multiphase hydrocarbons, water, steam, CO, N, petrol, aviation fuel etc.\n\nPre-commissioning is the series of processes carried out on the pipeline before the final product is introduced. The process during which the pipeline is made \"live\" i.e. the product is put in the pipeline, is called pipeline commissioning or start-up.\n\nDespite being seen as an offshoot, or minor part of the business for the larger oil service companies, the pipeline pre-commissioning industry possesses quite a large portfolio of services including, but not limited to the following services:\n\nPipeline Cleaning – this is carried out by pushing pigs or gel pigs through the pipeline to remove any debris buildup or corrosion.\n\nPipeline Gauging – this is carried out to prove the dimensional quality of the internal diameter of the pipeline.\n\nPipeline Filling (Flooding) – which can be carried out by propelling pigs through the pipeline with water or free flooding with water (normally for smaller or unpiggable pipelines).\n\nHydrotesting – this is a process by which the pipeline in question is pressure tested to a predefined pressure above the operating design pressure of the pipeline.\n\nDewatering – this involves pushing pigs through the pipeline propelled by a gas to remove the water prior to start-up.\n\nOther services include vacuum drying, degassing, pneumatic testing, barrier testing, leak testing, decommissioning to mention but a few.\n\nOn the pipeline process pre-commissioning side, there are various services such as chemical cleaning, helium leak detection, bolting, hot oil flushing, pipe freezing, foam inerting etc...\n\nOther services include valve testing, umbilical testing, hot tapping, leak metering, riser annulus testing.\n\nSome of the main service companies in the business are given below.\n\n"}
{"id": "239068", "url": "https://en.wikipedia.org/wiki?curid=239068", "title": "Plumber", "text": "Plumber\n\nA plumber is a tradesperson who specializes in installing and maintaining systems used for potable (drinking) water, sewage and drainage in plumbing systems. The term dates from ancient times and is related to the Latin word for lead, \"plumbum\".\n\nThe word \"plumber\" dates from the Roman Empire. The Latin for lead is \"\". Roman roofs used lead in conduits and drain pipes and some were also covered with lead, lead was also used for piping and for making baths. In medieval times anyone who worked with lead was referred to as a plumber as can be seen from an extract of workmen fixing a roof in Westminster Palace and were referred to as plumbers \"To Gilbert de Westminster, plumber, working about the roof of the pantry of the little hall, covering it with lead, and about various defects in the roof of the little hall\". Thus a person with expertise in working with lead was first known as a \"Plumbarius\" which was later shortened to plumber.\n\nYears of training and/or experience are needed to become a skilled plumber; some jurisdictions also require that plumbers be licensed.\n\nSome needed skills, interests, and values.\nProtecting health and welfare of the nation is the top priority of a plumber along with,\n\nEach state and locality may have its own licensing and taxing schemes for plumbers. There is no federal law establishing licenses for plumbers.\n\nIn Canada, licensing requirements differ by province, however the provinces have pooled resources to develop an Interprovincial Program Guide that developed and maintains apprenticeship training standards across all provinces. The result is what is known as the Interprovincial Standards Red Seal Program.\n\nNational Vocational Qualifications (NVQ) remained the main form of plumbing qualification until they were superseded in 2008 by the Qualification and Credit Framework (QCF) and then again into the National qualifications frameworks in the United Kingdom in 2015. The terms NVQ and SVQ (Scottish Vocational Qualification) are still widely used.\n\nPlumbers in the United Kingdom are required to pass Level 2 and Level 3 vocational requirements of the City and Guilds of London Institute. There are several regulatory bodies in the United Kingdom providing accredited plumbing qualifications, including City and Guilds of London Institute and Pearson PLC . \n\nPlumbers in Australia have licensing requirements that differ from state to state but it is generally accepted a 4-year apprenticeship with a further minimum experience of 2 years (6 years total) and a further curricular requirement as a benchmark for licensing. Licensed plumbers are also expected to maintain minimum relevant training requirements to maintain their plumbing license\n\nThere are many types of dangers to a plumber. Some of them are strains and sprains, cuts and lacerations, bruises and contusions, fractures, burns and scalds, foreign bodies in the eye, and hernias.\n\nOn a construction site there are many dangers. Without protective measures, a ditch can collapse on a plumber who is at the bottom of one. A plumber can also fall down a hole.\n\nWhen a person has a blockage in their sewage system they often try to fix it themselves by adding an acid or a base such as Drano in an attempt to dissolve or dislodge the problem. These chemicals can get into the plumbers eyes when the sewage is splashed during the repair. The plumbers skin during the repair does come into contact with the sewage water. The owner of the toilet might not report to the plumber they have already tried Drano a highly caustic base .\n\nPlumbers risk infections when dealing with human waste when repairing sewage systems. Microbes can be excreted in the feces or vomit of the sufferer onto the toilet or sewage pipes. Human waste can contain infectious diseases such as cholera, typhoid, hepatitis, polio, cryptosporidiosis, ascariasis, and schistosomiasis.\n\nThe term \"White House Plumbers\" was a popular name given to the covert White House Special Investigations Unit established on July 24, 1971 during the presidency of Richard Nixon. Their job was to plug intelligence \"leaks\" in the U.S. Government relating to the Vietnam War (i.e. the Pentagon Papers); hence the term \"plumbers\".\n\n\n"}
{"id": "846817", "url": "https://en.wikipedia.org/wiki?curid=846817", "title": "Pouch laminator", "text": "Pouch laminator\n\nA pouch laminator uses a lamination pouch that is usually sealed on one side. The inside of the lamination pouch is coated with a heat-activated film that adheres to the product being laminated as it runs through the laminator. The substrate side of the board contains a heat-activated adhesive that bonds the print to the substrate. This can be any of a number of board products or another sheet of laminate. The pouch containing the print, laminate, and substrate is passed through a set of heated rollers under pressure, ensuring that all adhesive layers bond to one another.\n\nPouch laminators are designed for moderate use in the office or home. For continuous, large-volume lamination projects, a roll laminator performs more efficiently.\n\nPouches can be bought with different thicknesses in micrometres. Standard home or office machines normally use 80–250 micrometre pouches, depending on the quality of the machine. The thicker the pouch, the higher the cost. Pouches can also measured in mil, which equals one thousandth of an inch. The most common pouch thicknesses are 3, 5, 7 and 10 mil (76,127,178 and 254 μm).\n\nCertain pouches such as butterfly pouches can be used with a pouch laminator to form ID cards. Butterfly pouches are available with magnetic stripes embedded.\n\nMany pouch laminators require the use of a carrier. A carrier holds the pouch as it is run through the laminator. This helps prevent the hot glue, some of which leaks from the sides of the pouches during the process, from gumming up the rollers. The carrier prevents the rollers from getting sticky, which helps to prevent the lamination pouch from wrapping around the rollers inside the laminator.\n\nMany newer laminators claim that they can be used without a carrier. However the use of carriers will extend the laminator's life.\n\n"}
{"id": "12374239", "url": "https://en.wikipedia.org/wiki?curid=12374239", "title": "Property management system", "text": "Property management system\n\nProperty Management Systems also known as PMS or Hotel Operating System (Hotel OS), under business terms may be used in real estate, manufacturing, logistics, intellectual property, government or hospitality accommodation management. They are computerized systems that facilitate the management of properties, personal property, equipment, including maintenance, legalities and personnel all through a single piece of software. They replaced old-fashioned, paper-based methods that tended to be both cumbersome and inefficient. They are often deployed as client/server configurations. Today, most next generation property management systems favour a software-as-a-service (SaaS) model sustained by web and cloud technologies.\n\nThe first property management systems in the hospitality industry appeared on the market in the 1980s. In hotels a property management system, also known as a PMS, is a comprehensive software application used to cover objectives like coordinating the operational functions of front office, sales and planning, reporting etc. The system automates hotel operations like guest bookings, guest details, online reservations, posting of charges, point of sale, telephone, accounts receivable, sales and marketing, events, food and beverage costing, materials management, HR and payroll, maintenance management, quality management and other amenities. Hotel property management systems may have integrated or interface with third-party solutions like central reservation systems and revenue or yield management systems, online booking engine, back office, point of sale, door-locking, housekeeping optimization, pay-TV, energy management, payment card authorization and channel management systems.\n\nWith the advancement of cloud computing property management systems for hotels expand their functionality towards new service areas like guest-facing features. These include online check-in, room service, in-room controls, guest-staff communication, virtual concierge and more. These new functionalities are mainly used by guests on their own mobile devices or such provided by the hotel in lobbies and/or rooms.\n\nA good PMS should give accurate and timely information on the basic key performance indicators of a hotel business such as average daily rate, RevPAR or occupancy rate and help the food and beverage management control the stocks in the store room and help deciding what to buy, how much and how often.\n\nProperty management systems are used in local government authorities, since these authorities hold and manage large property estates ranging from schools, leisure centres, social housing and parks not to mention investment properties such as shops and industrial estates - even pubs. All of these are necessary income earners for a local authority, so the efficiency gained through an automated, computerized system is essential.\n\nProperty management systems are used to manage, control and account for personal property.\nProperty is defined as the equipment, tooling and physical capital assets that are acquired and used to build, repair and maintain end item deliverables. Property Management involves the processes, systems and manpower required to manage the life cycle of all acquired property as defined above including Acquisition, Control, Accountability, Maintenance, Utilization, and disposition.\n\nProperty Management systems allow local property managers and maintenance personnel manage the day-to-day operations of their properties. Property maintenance for commercial properties includes major focus areas such as risk management, maintenance, communication, and tenant satisfaction. Usually a certain agreed percentage of the rent payment will be deducted on each rent payment collected by the property management as their service fee. There are also other ways of charging the property owners using the service but percentage collection is the common one.\n\n"}
{"id": "1009291", "url": "https://en.wikipedia.org/wiki?curid=1009291", "title": "Pyranometer", "text": "Pyranometer\n\nA pyranometer is a type of actinometer used for measuring solar irradiance on a planar surface and it is designed to measure the solar radiation flux density (W/m) from the hemisphere above within a wavelength range 0.3 μm to 3 μm. The name pyranometer stems from the Greek words \"πῦρ\" (pyr), meaning \"fire\", and \"ἄνω\" (ano), meaning \"above, sky\".\n\nA typical pyranometer does not require any power to operate. However, recent technical development includes use of electronics in pyranometers, which do require (low) external power.\"\n\nThe solar radiation spectrum that reaches earth's surface extends its wavelength approximately from 300 nm to 2800 nm.\nDepending on the type of pyranometer used, irradiance measurements with different degrees of spectral sensitivity will be obtained.\n\nTo make a measurement of irradiance, it is required by definition that the response to “beam” radiation varies with the cosine of the angle of incidence. This ensures a full response when the solar radiation hits the sensor perpendicularly (normal to the surface, sun at zenith, 0° angle of incidence), zero response when the sun is at the horizon (90° angle of incidence, 90° zenith angle), and 0.5 at a 60° angle of incidence. It follows that a pyranometer should have a so-called “directional response” or “cosine response” that is as close as possible to the ideal cosine characteristic.\n\nFollowing the classifications and definitions noted in the ISO 9060, three types of pyranometers can be recognized and grouped in two different technologies: thermopile technology and silicon semiconductor technology.\n\nThe light sensitivity, known as 'spectral response', depends on the type of pyranometer. The figure here above shows the spectral responses of the three types of pyranometer in relation to the Solar Radiation Spectrum. The Solar Radiation Spectrum represents the spectrum of sunlight that reaches the Earth’s surface at sea level, at midday with A.M. (air mass) = 1.5.\nThe latitude and altitude influence this spectrum. The spectrum is influenced also by aerosol and pollution.\n\nA thermopile pyranometer is a sensor based on thermopiles designed to measure the broadband of the solar radiation flux density from a 180° field of view angle. A thermopile pyranometer thus usually measures 300 to 2800 nm with a largely flat spectral sensitivity (see the Spectral Response graph) The first generation of thermopile pyranometers had the active part of the sensor equally divided in black and white sectors. Irradiation was calculated from the differential measure between the temperature of the black sectors, exposed to the sun, and the temperature of the white sectors, sectors not exposed to the sun or better said in the shades.\n\nIn all thermopile technology, irradiation is proportional to the difference between the temperature of the sun exposed area and the temperature of the shadow area.\n\nIn order to attain the proper directional and spectral characteristics, a thermopile pyranometer is constructed with the following main components:\n\nIn the modern thermopile pyranometers the active (hot) junctions of the thermopile are located beneath the black coating surface and are heated by the radiation absorbed from the black coating. The passive (cold) junctions of the thermopile are fully protected from solar radiation and in thermal contact with the pyranometer housing, which serves as a heat-sink. This prevents any alteration from yellowing or decay when measuring the temperature in the shade, thus impairing the measure of the solar irradiance.\n\nThe thermopile generates a small voltage in proportion to the temperature difference between the black coating surface and the instrument housing. This is of the order of 10 µ • VW/m. Typically, on a sunny day the output is around 10 mV. Each pyranometer has a unique sensitivity, unless otherwise equipped with electronics for signal calibration.\n\nThermopile pyranometers are frequently used in meteorology, climatology, climate change research, building engineering physics and in photovoltaic systems.\n\nThey are usually installed horizontally in meteorological stations and typically mounted in the 'plane of array' (with the sensor surface parallel to the solar panel) when used for monitoring of photovoltaic systems.\n\nThe solar energy industry, in a new standard, IEC 61724-1:2017, has defined what type of pyranometers should be used depending on the size and category of solar power plant.\n\nAlso known as a silicon pyranometer in the ISO 9060, a photodiode-based pyranometer can detect the portion of the solar spectrum between 400 nm and 900 nm, with the most performant detecting between 350 nm and 1100 nm. The photodiode converts the aforementioned solar spectrum frequencies into current at high speed, thanks to the photoelectric effect. The conversion is influenced by the temperature with a raise in current produced by the raise in temperature (about 0,1% • °C)\n\nA photodiode-based pyranometer is composed by a housing dome, a photodiode, and a diffuser or optical filters. The photodiode has a small surface area and acts as a sensor. The current generated by the photodiode is proportional to irradiance; an output circuit, such as a Transimpedance amplifier, generates a voltage directly proportional to the photocurrent. The output is usually on the order of millivolts, the same order of magnitude of thermopile-type pyranometers.\n\nPhotodiode-based pyranometers are implemented where the quantity of irradiation of the visible solar spectrum, or of certain portions such as UV, IR or PAR (Photosynthetically active radiation), needs to be calculated. This is done by using diodes with specific spectral responses. \nPhotodiode-based pyranometers are the core of luxmeter used in photography, cinema and lighting technique. Sometimes they are also installed close to modules of photovoltaic systems.\n\nBuilt around the 2000s concurrently with the spread of photovoltaic systems, the photovoltaic pyranometer is a derivation of the photodiode pyranometer. It answered the need for a single reference photovoltaic cell when measuring the power of cell and photovoltaic modules. Specifically, each cell and module is tested through flash tests by their respective manufacturers, and thermopile pyranometers do not possess the adequate speed of response nor the same spectral response of a cell. This would create obvious mismatch when measuring power, which would need to be quantified. In the technical documents, this pyranometer is also known as \"reference PV cell\", \"irradiance sensor\", \"solarimeter\", \"solar sensor\", as bibliographies are more recent than the ISO 9060.\n\nThe active part of the sensor is composed of a photovoltaic cell working in near short-circuit condition. As such, the generated current is directly proportionate to the solar radiation hitting the cell in a range between 350 nm and 1150 nm. When invested by a luminous radiation in the mentioned range, it produces current as a consequence of the photovoltaic effect. Its sensitivity is not flat, but it is same as that of Silicon photovoltaic cell. See the Spectral Response graph.\n\nA photovoltaic pyranometer is essentially assembled with the following parts:\n\nSilicon sensors such as the photodiode and the photovoltaic cell vary the output in function of temperature. In the more recent models, the electronics compensate the signal with the temperature, therefore removing the influence of temperature out of the values of solar irradiance. Inside several models, the case houses a board for the amplification and conditioning of the signal.\n\nPhotovoltaic pyranometers are used in solar simulators and alongside photovoltaic systems for the calculation of photovoltaic module effective power and system performance. Because the spectral response of a photovoltaic pyranometer is similar to that of a photovoltaic module, it may also be used for preliminary diagnosis of malfunction in photovoltaic systems.\n\nBoth thermopile-type and photovoltaic pyranometers are manufactured according to standards.\n\nThermopile Pyranometers follow the ISO 9060 standard, which is also adopted by the World Meteorological Organization (WMO). This standard discriminates three classes. Rather confusingly, the best is confusingly called \"secondary standard\" (\"i.e.,\" calibrated by direct comparison with the single primary instrument). The second best is defined as \"first class\" and the last one \"second class\". Differences in classes are due to a certain number of properties in the sensors: response time, thermal offsets, temperature dependence, directional error, non-stability, non-linearity, spectral selectivity and tilt response. These are all defined in ISO 9060. For a sensor to be classified in a certain category, it needs to fulfill all the minimum requirements for these properties.\n\nThe calibration is typically done having the World Radiometric Reference (WRR) as absolute reference. This is maintained by PMOD in Davos, Switzerland. In addition to the World Radiometric Reference there are private laboratories such as ISO-Cal North America who have acquired accreditation for these unique calibrations. For the \"secondary standard\" pyranometer, calibration is done following ASTM G167, ISO 9847 or ISO 9846. First and second class pyranometers are usually calibrated according to ASTM E824, and ISO 9847.\n\nPhotovoltaic pyranometers are standardized and calibrated under IEC 60904-4 for primary reference samples and under IEC 60904-2 for secondary reference samples and the instruments intended for sale.\n\nIn both standards, their respective traceability chain starts with the primary standard known as the Group of Cavity Radiometer by the World Radiometric Reference (WRR).\n\nThe natural output value of these pyranometers do not usually exceed tens of millivolt (mV). It is considered a ‘weak’ signal, and as such, rather vulnerable to electromagnetic interferences, especially where the cable runs across decametrical distances or lies in photovoltaic systems. Thus, these sensors are frequently equipped with signal conditioning electronics, which allows them to amplify its natural output value by 100 or 1000 times.\n\nAnother solution implies greater immunities to noises, like a 4-20 mA current loop or Modbus over RS-485 output, suitable for ambiances with electromagnetic interferences typical of medium-large scale photovoltaic power stations. The equipped electronics often concur to normalize the signal output into a predetermined value and easy integration in the system's SCADA.\n\nAdditionally, information on the sensor can be stored in the electronics of the sensor, like calibration history, serial number. Some pyranometers also offer extra information like case temperature and sensor tilt angle.\n\n\n"}
{"id": "3487107", "url": "https://en.wikipedia.org/wiki?curid=3487107", "title": "Real-time polymerase chain reaction", "text": "Real-time polymerase chain reaction\n\nA real-time polymerase chain reaction (Real-Time PCR), also known as quantitative polymerase chain reaction (qPCR), is a laboratory technique of molecular biology based on the polymerase chain reaction (PCR). It monitors the amplification of a targeted DNA molecule during the PCR, i.e. in real-time, and not at its end, as in conventional PCR. Real-time PCR can be used quantitatively (quantitative real-time PCR), and semi-quantitatively, i.e. above/below a certain amount of DNA molecules (semi quantitative real-time PCR).\n\nTwo common methods for the detection of PCR products in real-time PCR are: (1) non-specific fluorescent dyes that intercalate with any double-stranded DNA, and (2) sequence-specific DNA probes consisting of oligonucleotides that are labelled with a fluorescent reporter which permits detection only after hybridization of the probe with its complementary sequence.\n\nThe Minimum Information for Publication of Quantitative Real-Time PCR Experiments (MIQE) guidelines propose that the abbreviation \"qPCR\" be used for quantitative real-time PCR and that \"RT-qPCR\" be used for reverse transcription–qPCR. The acronym \"RT-PCR\" commonly denotes reverse transcription polymerase chain reaction and not real-time PCR, but not all authors adhere to this convention.\n\nCells in all organisms regulate gene expression by turnover of gene transcripts (single stranded RNA): The amount of an expressed gene in a cell can be measured by the number of copies of an RNA transcript of that gene present in a sample. In order to robustly detect and quantify gene expression from small amounts of RNA, amplification of the gene transcript is necessary. The polymerase chain reaction (PCR) is a common method for amplifying DNA; for RNA-based PCR the RNA sample is first reverse-transcribed to complementary DNA (cDNA) with reverse transcriptase.\n\nIn order to amplify small amounts of DNA, the same methodology is used as in conventional PCR using a DNA template, at least one pair of specific primers, deoxyribonucleotides, a suitable buffer solution and a thermo-stable DNA polymerase. A substance marked with a fluorophore is added to this mixture in a thermal cycler that contains sensors for measuring the fluorescence of the fluorophore after it has been excited at the required wavelength allowing the generation rate to be measured for one or more specific products. \nThis allows the rate of generation of the amplified product to be measured at each PCR cycle. The data thus generated can be analysed by computer software to calculate \"relative gene expression\" (or \"mRNA copy number\") in several samples. Quantitative PCR can also be applied to the detection and quantification of DNA in samples to determine the presence and abundance of a particular DNA sequence in these samples. This measurement is made after each amplification cycle, and this is the reason why this method is called real time PCR (that is, immediate or simultaneous PCR). In the case of RNA quantitation, the template is complementary DNA (cDNA), which is obtained by reverse transcription of ribonucleic acid (RNA). In this instance the technique used is quantitative RT-PCR or Q-RT-PCR.\n\nQuantitative PCR and DNA microarray are modern methodologies for studying gene expression. Older methods were used to measure mRNA abundance: Differential display, RNase protection assay and Northern blot. Northern blotting is often used to estimate the expression level of a gene by visualizing the abundance of its mRNA transcript in a sample. In this method, purified RNA is separated by agarose gel electrophoresis, transferred to a solid matrix (such as a nylon membrane), and probed with a specific DNA or RNA probe that is complementary to the gene of interest. Although this technique is still used to assess gene expression, it requires relatively large amounts of RNA and provides only qualitative or semi quantitative information of mRNA levels. Estimation errors arising from variations in the quantification method can be the result of DNA integrity, enzyme efficiency and many other factors. For this reason a number of standardization systems (often called normalization methods) have been developed. Some have been developed for quantifying total gene expression, but the most common are aimed at quantifying the specific gene being studied in relation to another gene called a normalizing gene, which is selected for its almost constant level of expression. These genes are often selected from housekeeping genes as their functions related to basic cellular survival normally imply constitutive gene expression. This enables researchers to report a ratio for the expression of the genes of interest divided by the expression of the selected normalizer, thereby allowing comparison of the former without actually knowing its absolute level of expression.\n\nThe most commonly used normalizing genes are those that code for the following molecules: tubulin, glyceraldehyde-3-phosphate dehydrogenase, albumin, cyclophilin, and ribosomal RNAs.\n\nReal-time PCR is carried out in a thermal cycler with the capacity to illuminate each sample with a beam of light of at least one specified wavelength and detect the fluorescence emitted by the excited fluorophore. The thermal cycler is also able to rapidly heat and chill samples, thereby taking advantage of the physicochemical properties of the nucleic acids and DNA polymerase.\n\nThe PCR process generally consists of a series of temperature changes that are repeated 25 – 50 times. These cycles normally consist of three stages: the first, at around 95 °C, allows the separation of the nucleic acid's double chain; the second, at a temperature of around 50-60 °C, allows the binding of the primers with the DNA template; the third, at between 68 - 72 °C, facilitates the polymerization carried out by the DNA polymerase. Due to the small size of the fragments the last step is usually omitted in this type of PCR as the enzyme is able to increase their number during the change between the alignment stage and the denaturing stage. In addition, in four step PCR the fluorescence is measured during short temperature phase lasting only a few seconds in each cycle, with a temperature of, for example, 80 °C, in order to reduce the signal caused by the presence of primer dimers when a non-specific dye is used. The temperatures and the timings used for each cycle depend on a wide variety of parameters, such as: the enzyme used to synthesize the DNA, the concentration of divalent ions and deoxyribonucleotides (dNTPs) in the reaction and the bonding temperature of the primers.\n\nReal-time PCR technique can be classified by the chemistry used to detect the PCR product, specific or non-specific fluorochromes.\n\nA DNA-binding dye binds to all double-stranded (ds) DNA in PCR, causing fluorescence of the dye. An increase in DNA product during PCR therefore leads to an increase in fluorescence intensity measured at each cycle. However, dsDNA dyes such as SYBR Green will bind to all dsDNA PCR products, including nonspecific PCR products (such as Primer dimer). This can potentially interfere with, or prevent, accurate monitoring of the intended target sequence.\n\nIn real-time PCR with dsDNA dyes the reaction is prepared as usual, with the addition of fluorescent dsDNA dye. Then the reaction is run in a real-time PCR instrument, and after each cycle, the intensity of fluorescence is measured with a detector; the dye only fluoresces when bound to the dsDNA (i.e., the PCR product). \nThis method has the advantage of only needing a pair of primers to carry out the amplification, which keeps costs down; multiple target sequences can be monitored in a tube by using different types of dyes.\n\nFluorescent reporter probes detect only the DNA containing the sequence complementary to the probe; therefore, use of the reporter probe significantly increases specificity, and enables performing the technique even in the presence of other dsDNA. Using different-coloured labels, fluorescent probes can be used in multiplex assays for monitoring several target sequences in the same tube. The specificity of fluorescent reporter probes also prevents interference of measurements caused by primer dimers, which are undesirable potential by-products in PCR. However, fluorescent reporter probes do not prevent the inhibitory effect of the primer dimers, which may depress accumulation of the desired products in the reaction.\n\nThe method relies on a DNA-based probe with a fluorescent reporter at one end and a quencher of fluorescence at the opposite end of the probe. The close proximity of the reporter to the quencher prevents detection of its fluorescence; breakdown of the probe by the 5' to 3' exonuclease activity of the Taq polymerase breaks the reporter-quencher proximity and thus allows unquenched emission of fluorescence, which can be detected after excitation with a laser. An increase in the product targeted by the reporter probe at each PCR cycle therefore causes a proportional increase in fluorescence due to the breakdown of the probe and release of the reporter.\n\n\nReal-time PCR permits the identification of specific, amplified DNA fragments using analysis of their melting temperature (also called \"T\" value, from m\"elting\" t\"emperature\"). The method used is usually PCR with double-stranded DNA-binding dyes as reporters and the dye used is usually SYBR Green. The DNA melting temperature is specific to the amplified fragment. The results of this technique are obtained by comparing the dissociation curves of the analysed DNA samples.\n\nUnlike conventional PCR, this method avoids the previous use of electrophoresis techniques to demonstrate the results of all the samples. This is because, despite being a kinetic technique, quantitative PCR is usually evaluated at a distinct end point. The technique therefore usually provides more rapid results and / or uses fewer reactants than electrophoresis. If subsequent electrophoresis is required it is only necessary to test those samples that real time PCR has shown to be doubtful and / or to ratify the results for samples that have tested positive for a specific determinant.\n\nUnlike end point PCR (conventional PCR), real time PCR allows monitoring of the desired product at any point in the amplification process by measuring fluorescence (in real time frame, measurement is made of its level over a given threshold). A commonly employed method of DNA quantification by real-time PCR relies on plotting fluorescence against the number of cycles on a logarithmic scale. A threshold for detection of DNA-based fluorescence is set 3-5 times of the standard deviation of the signal noise above background. The number of cycles at which the fluorescence exceeds the threshold is called the threshold cycle (C) or, according to the MIQE guidelines, quantification cycle (C).\n\nDuring the exponential amplification phase, the quantity of the target DNA template (amplicon) doubles every cycle. For example, a DNA sample whose C precedes that of another sample by 3 cycles contained 2 = 8 times more template. However, the efficiency of amplification is often variable among primers and templates. Therefore, the efficiency of a primer-template combination is assessed in a titration experiment with serial dilutions of DNA template to create a standard curve of the change in (C) with each dilution. The slope of the linear regression is then used to determine the efficiency of amplification, which is 100% if a dilution of 1:2 results in a (C) difference of 1. The cycle threshold method makes several assumptions of reaction mechanism and has a reliance on data from low signal-to-noise regions of the amplification profile that can introduce substantial variance during the data analysis.\n\nTo quantify gene expression, the (C) for an RNA or DNA from the gene of interest is subtracted from the (C) of RNA/DNA from a housekeeping gene in the same sample to normalize for variation in the amount and quality of RNA between different samples. This normalization procedure is commonly called the \"ΔC-method\" and permits comparison of expression of a gene of interest among different samples. However, for such comparison, expression of the normalizing reference gene needs to be very similar across all the samples. Choosing a reference gene fulfilling this criterion is therefore of high importance, and often challenging, because only very few genes show equal levels of expression across a range of different conditions or tissues. Although cycle threshold analysis is integrated with many commercial software systems, there are more accurate and reliable methods of analysing amplification profile data that should be considered in cases where reproducibility is a concern.\n\nMechanism-based qPCR quantification methods have also been suggested, and have the advantage that they do not require a standard curve for quantification. Methods such as MAK2 have been shown to have equal or better quantitative performance to standard curve methods. These mechanism-based methods use knowledge about the polymerase amplification process to generate estimates of the original sample concentration. An extension of this approach includes an accurate model of the entire PCR reaction profile, which allows for the use of high signal-to-noise data and the ability to validate data quality prior to analysis.\n\nAccording to research of Ruijter et al. MAK2 assumes constant amplification efficiency during the PCR reaction. However, theoretical analysis of polymerase chain reaction, from which MAK2 was derived, has revealed that amplification efficiency is not constant throughout PCR. While MAK2 quantification provides reliable estimates of target DNA concentration in a sample under normal qPCR conditions, MAK2 does not reliably quantify target concentration for qPCR assays with competimeters.\n\nThere are numerous applications for quantitative polymerase chain reaction in the laboratory. It is commonly used for both diagnostic and basic research. Uses of the technique in industry include the quantification of microbial load in foods or on vegetable matter, the detection of GMOs (Genetically modified organisms) and the quantification and genotyping of human viral pathogens.\n\nQuantifying gene expression by traditional DNA detection methods is unreliable. Detection of mRNA on a Northern blot or PCR products on a gel or Southern blot does not allow precise quantification. For example, over the 20-40 cycles of a typical PCR, the amount of DNA product reaches a plateau that is not directly correlated with the amount of target DNA in the initial PCR.\n\nReal-time PCR can be used to quantify nucleic acids by two common methods: relative quantification and absolute quantification. Absolute quantification gives the exact number of target DNA molecules by comparison with DNA standards using a calibration curve. It is therefore essential that the PCR of the sample and the standard have the same amplification efficiency.\nRelative quantification is based on internal reference genes to determine fold-differences in expression of the target gene. The quantification is expressed as the change in expression levels of mRNA interpreted as complementary DNA (cDNA, generated by reverse transcription of mRNA). Relative quantification is easier to carry out as it does not require a calibration curve as the amount of the studied gene is compared to the amount of a control reference gene.\n\nAs the units used to express the results of relative quantification are unimportant the results can be compared across a number of different RT-Q-PCR. The reason for using one or more housekeeping genes is to correct non-specific variation, such as the differences in the quantity and quality of RNA used, which can affect the efficiency of reverse transcription and therefore that of the whole PCR process. However, the most crucial aspect of the process is that the reference gene must be stable.\n\nThe selection of these reference genes was traditionally carried out in molecular biology using qualitative or semi-quantitative studies such as the visual examination of RNA gels, Northern blot densitometry or semi-quantitative PCR (PCR mimics). Now, in the genome era, it is possible to carry out a more detailed estimate for many organisms using transcriptomic technologies. However, research has shown that amplification of the majority of reference genes used in quantifying the expression of mRNA varies according to experimental conditions. It is therefore necessary to carry out an initial statistically sound methodological study in order to select the most suitable reference gene.\n\nA number of statistical algorithms have been developed that can detect which gene or genes are most suitable for use under given conditions. Those like geNORM or BestKeeper can compare pairs or geometric means for a matrix of different reference genes and tissues.\n\nDiagnostic qualitative PCR is applied to rapidly detect nucleic acids that are diagnostic of, for example, infectious diseases, cancer and genetic abnormalities. The introduction of qualitative PCR assays to the clinical microbiology laboratory has significantly improved the diagnosis of infectious diseases, and is deployed as a tool to detect newly emerging diseases, such as new strains of flu, in diagnostic tests.\n\nQuantitative PCR is also used by microbiologists working in the fields of food safety, food spoilage and fermentation and for the microbial risk assessment of water quality (drinking and recreational waters) and in public health protection.\n\nThe agricultural industry is constantly striving to produce plant propagules or seedlings that are free of pathogens in order to prevent economic losses and safeguard health. Systems have been developed that allow detection of small amounts of the DNA of \"Phytophthora ramorum\", an oomycete that kills Oaks and other species, mixed in with the DNA of the host plant. Discrimination between the DNA of the pathogen and the plant is based on the amplification of ITS sequences, spacers located in ribosomal RNA gene's coding area, which are characteristic for each taxon. Field-based versions of this technique have also been developed for identifying the same pathogen.\n\nqPCR using reverse transcription (RT-qPCR) can be used to detect GMOs given its sensitivity and dynamic range in detecting DNA. Alternatives such as DNA or protein analysis are usually less sensitive. Specific primers are used that amplify not the transgene but the promoter, terminator or even intermediate sequences used during the process of engineering the vector. As the process of creating a transgenic plant normally leads to the insertion of more than one copy of the transgene its quantity is also commonly assessed. This is often carried out by relative quantification using a control gene from the treated species that is only present as a single copy.\n\nViruses can be present in humans due to direct infection or co-infections which makes diagnosis difficult using classical techniques and can result in an incorrect prognosis and treatment. The use of qPCR allows both the quantification and genotyping (characterization of the strain, carried out using melting curves) of a virus such as the Hepatitis B virus. \nThe degree of infection, quantified as the copies of the viral genome per unit of the patient's tissue, is relevant in many cases; for example, the probability that the type 1 herpes simplex virus reactivates is related to the number of infected neurons in the ganglia. This quantification is carried out either with reverse transcription or without it, as occurs if the virus becomes integrated in the human genome at any point in its cycle, such as happens in the case of HPV (human papillomavirus), where some of its variants are associated with the appearance of cervical cancer.\n\n"}
{"id": "419620", "url": "https://en.wikipedia.org/wiki?curid=419620", "title": "Semi-active radar homing", "text": "Semi-active radar homing\n\nSemi-active radar homing (SARH) is a common type of missile guidance system, perhaps the most common type for longer-range air-to-air and surface-to-air missile systems. The name refers to the fact that the missile itself is only a passive detector of a radar signal – provided by an external (“offboard”) source — as it reflects off the target(in contrast to active radar homing, which uses an active radar: transceiver). Semi-active missile systems use bistatic continuous-wave radar.\n\nThe NATO brevity code for a semi-active radar homing missile launch is Fox One.\n\nThe basic concept of SARH is that since almost all detection and tracking systems consist of a radar system, duplicating this hardware on the missile itself is redundant. The weight of a transmitter reduces the range of any flying object, so passive systems have greater reach. In addition, the resolution of a radar is strongly related to the physical size of the antenna, and in the small nose cone of a missile there isn't enough room to provide the sort of accuracy needed for guidance. Instead the larger radar dish on the ground or launch aircraft will provide the needed signal and tracking logic, and the missile simply has to listen to the signal reflected from the target and point itself in the right direction. Additionally, the missile will listen rearward to the launch platform's transmitted signal as a reference, enabling it to avoid some kinds of radar jamming distractions offered by the target.\n\nThe SARH system determines the closing velocity using the flight path geometry shown in Figure 1. The closing velocity is used to set the frequency location for the CW receive signal shown at the bottom of the diagram (spectrum). Antenna offset angle of the missile antenna is set after the target is acquired by the missile seeker using the spectrum location set using closing speed. The missile seeker antenna is a monopulse radar receiver that produces angle error measurements using that fixed position. Flight path is controlled by producing navigation input to the steering system (tail fins or gimbaled rocket) using angle errors produced by the antenna. This steers the body of the missile to hold the target near the centerline of the antenna while the antenna is held in a fixed position. The offset angle geometry is determined by flight dynamics using missile speed, target speed, and separation distance.\n\nTechniques are nearly identical using jamming signals, optical guidance video, and infra-red radiation for homing.\n\nMaximum range is increased in SARH systems using navigation data in the homing vehicle to increase the travel distance before antenna tracking is needed for terminal guidance. Navigation relies on acceleration data, gyroscopic data, and global positioning data. This maximizes distance by minimizing corrective maneuvers that waste flight energy.\n\nContrast this with beam riding systems, like the RIM-8 Talos, in which the radar is pointed at the target and the missile keeps itself centered in the beam by listening to the signal at the rear of the missile body. In the SARH system the missile listens for the reflected signal at the nose, and is still responsible for providing some sort of “lead” guidance. The disadvantages of beam riding are twofold: One is that a radar signal is “fan shaped”, growing larger, and therefore less accurate, with distance. This means that the beam riding system is not accurate at long ranges, while SARH is largely independent of range and grows more accurate as it approaches the target, or the source of the reflected signal it listens for. Reduced accuracy means the missile must use a very large warhead to be effective (i.e.: nuclear). Another requirement is that a beam riding system must accurately track the target at high speeds, typically requiring one radar for tracking and another “tighter” beam for guidance.\n\nThe SARH system needs only one radar set to a wider pattern.\n\nModern SARH systems use continuous-wave radar (CW radar) for guidance. Even though most modern fighter radars are pulse Doppler sets, most have a CW function to guide radar missiles. A few Soviet aircraft, such as some versions of the MiG-23 and MiG-27, used an auxiliary guidance pod or aerial to provide a CW signal. The Vympel R-33 AA missile for MiG-31 interceptor uses SARH as the main type of guidance (with supplement of inertial guidance on initial stage).\n\nSARH missiles require tracking radar to acquire the target, and a more narrowly focused illuminator radar to \"light up\" the target in order for the missile to lock on to the radar return reflected off target. The target must remain illuminated for the entire duration of the missile's flight. This could leave the launch aircraft vulnerable to counterattack, as well as giving the target's electronic warning systems time to detect the attack and engage countermeasures. Because most SARH missiles require guidance during their entire flight, older radars are limited to one target per radar emitter at a time.\n\nThe maximum range of a SARH system is determined by energy density of the transmitter. Increasing transmit power can increase energy density. Reducing the noise bandwidth of the transmitter can also increase energy density. Spectral density matched to the receive radar detection bandwidth is the limiting factor for maximum range.\n\nRecent-generation SARH weapons have superior electronic counter-countermeasure (ECCM) capability, but the system still has fundamental limitations. Some newer missiles, such as the SM-2, incorporate terminal semi-active radar homing (TSARH). TSARH missiles use inertial guidance for most of their flight, only activating their SARH system for the final attack. This can keep the target from realising it is under attack until shortly before the missile strikes. Since the missile only requires guidance during the terminal phase, each radar emitter can be used to engage more targets. Some of these weapons, like the SM-2, allow the firing platform to update the missile with mid-course updates via datalink.\n\nSome of the more effective methods used to defeat semi-active homing radar are flying techniques. These depend upon the pilot knowing that a missile has been launched. The global positioning system allows a missile to reach the predicted intercept with no datalink, greatly increasing lethality by postponing illumination for most of the missile flight. The pilot is unaware that a launch has occurred, so flying techniques become almost irrelevant. One difficulty is testing, because this feature creates public safety risks if a fault prevents datalink self-destruct signals when a missile is heading in the wrong direction. Most coastlines are heavily populated, so this risk exists at test centers for sea-based systems that are near the coastlines.\n\nThe combat record of U.S. SARH missiles was unimpressive during the Vietnam War. USAF and US Navy fighters armed with AIM-7 Sparrow attained a success rate of barely 10%, which tended to amplify the effect of deleting the gun on most F-4 Phantoms, which carried 4 Sparrows. While some of the failures were attributable to mechanical failure of 1960s-era electronics, which could be disturbed by pulling a cart over uneven pavement, or pilot error; the intrinsic accuracy of these weapons was low relative to Sidewinder and guns.\n\nSince Desert Storm, most F-15 Eagle combat victories have been scored with the Sparrow at beyond visual range. Similar performance has been achieved with the sea-launched Standard Missile.\n\nSoviet systems using SARH have achieved a number of notable successes, notably in the Yom Kippur War, where 2K12 Kub (NATO name SA-6) tactical SAM systems were able to effectively deny airspace to the Israeli Air Force. A 2K12 also shot down a U.S. F-16 in the Bosnian War.\n\nSARH is a commonly used modern missile guidance methodology, used in multiple missile systems, such as:\n\n\n"}
{"id": "41332278", "url": "https://en.wikipedia.org/wiki?curid=41332278", "title": "Smart module", "text": "Smart module\n\nSmart modules are a type of solar panel that has a power optimizer embedded into the solar module at the time of manufacturing. Typically the power optimizer is embedded in the junction box of the solar module. Power optimizers attached to the frame of a solar module, or connected to the photovoltaic circuit through a connector, are not properly considered smart modules.\n\nSmart modules are different from traditional solar panels because the power electronics embedded in the module offers enhanced functionality such as panel-level maximum power point tracking, monitoring, and enhanced safety.\n\nSolar panel installers saw significant growth between 2008 and 2013. Due to that growth many installers had projects that were not \"ideal\" solar roof tops to work with and had to find solutions to shaded roofs and orientation difficulties. This challenge was initially addressed by the re-popularization of micro-inverters and later the invention of power optimizers.\n\nSolar panel manufacturers partnered with micro-inverter companies to create AC modules and power optimizer companies partnered with module manufacturers to create smart modules. In 2013 many solar panel manufacturers announced and began shipping their smart module solutions.\n\nIt is estimated that smart module shipments will grow rapidly with some manufacturers expecting smart modules will account for 30% of their shipments.\n"}
{"id": "40078607", "url": "https://en.wikipedia.org/wiki?curid=40078607", "title": "Stripper (agriculture)", "text": "Stripper (agriculture)\n\nStripper was a type of harvesting machine common in Australia in the late 19th and early 20th century. John Ridley is now accepted as its inventor, though John Wrathall Bull argued strongly for the credit.\n\nThe stripper plucks the ears of grain (generally wheat) without winnowing, and leaving the straw standing. The first strippers were drawn by bullocks and consisted of a large, wheeled, box-like machine with a row of spiked prongs in front and with a long pole at the back of the machine for steering. It had the advantage over the early reaper machines in being able to reap more quickly (of benefit in a hot climate) and having fewer components subject to wearing out.\n\nThe first strippers were improved by adding a beater to knock the heads off the stems. The machines became headers. Later headers had reciprocating cutter bars at the back of the combs to cut the stems just short of the heads. \n\nA stripper-harvester also winnowed the grain, removing the chaff.\n\nNotable manufacturers were Sunshine Harvester, J. and D. Shearer and Mellor Bros. (who specialised in \"bike strippers\", ie. light enough to be drawn by a bicycle).\n\nThe Gallic reaper that is seen in Roman times, in the first three centuries of the current era led to the stripper developed in the 19th century. The Gallic reaper had a comb at the front to collect grain heads. An operator would knock the heads into a tray for collection. The stripper developed in South Australia used the principles, with a comb at the front, using a mechanical beater to knock the heads off. Later innovations were including a cutter bar similar to the binder reaper and an elevator to lift the heads into a storage bin for later threshing. The combined header-harvester added the winnower to thresh the grain from the heads. \n\n\"The Australian National Dictionary\" Oxford University Press 1988 \n"}
{"id": "25450119", "url": "https://en.wikipedia.org/wiki?curid=25450119", "title": "Sustainable Electronics Initiative", "text": "Sustainable Electronics Initiative\n\nSustainable Electronics Initiative (SEI) is an initiative started in the United States in the summer of 2009 by the Illinois Sustainable Technology Center, which is a division of the Institute of Natural Resource Sustainability of the University of Illinois at Urbana-Champaign. SEI is dedicated to developing and implementing sustainable means for the design, manufacturing, remanufacturing, and recycling of electronics (computers, cell phones, televisions, printers, etc.) Members of SEI include individuals from academia, non-profit organizations, government agencies, manufacturers, designers, recyclers and refurbishers.\n\nThe goals of the Sustainable Electronics Initiative are to provide research, education, technical assistance, and data management for the general public and other interested parties with regards to electronics and electronic waste. SEI conducts collaborative research with professors and industry representatives, facilitates networking and information exchange among its participants, promotes the diffusion of technology through demonstration projects, and it provides a forum for discussion of applicable policies and legislation.\n\nSEI was created as a response to the growing national and international demand for more sustainable electronic designs and more environmentally friendly ways to handle electronics once they reach the end of their useful lives. Currently, individual states have passed laws regarding ewaste, but federal legislation has not yet been passed. The United States House of Representatives and United States Senate are, however, currently considering federal legislation which will fund electronics research, recycling, and refurbishing.\n\nBill S.1397 was proposed by Senator Amy Klobuchar (D-MN) and Senator Kirsten Gillibrand (D-NY) in July, 2009. The bill addresses the illegal dumping of electronic waste to undeveloped countries. More importantly, however, the bill also focuses on the importance of research and development for the designing and manufacturing of sustainable electronics, which would be recycled, reused, and refurbished more easily. The European Union, on the other hand, has passed several strict laws governing electronic equipment and its disposal.\n\nWhile one of the SEI goals is to minimize waste, this goal can be reached through the application of life cycle analyses. By analyzing the complete life cycle of a product, SEI will take into consideration the design, processing, manufacturing, use, and disposal stages of electronic equipment. With the use of life cycle analyses, SEI plans to make the overall process of computers and other electronics more sustainable and less environmentally harmful.\n\nThis was the first free public talk in the new Lecture Series presented by the Institute of Natural Resource Sustainability at the University of Illinois at Urbana-Champaign. Willie Cade, the founder and CEO of PC Rebuilders and Recyclers (PCRR) gave a lecture focussing on the current e-waste problems and how refurbishing can help alleviate the current e-waste problem. Cade also spoke about current assumptions with regards to e-waste and their possible inaccuracies. In addition, free cell phone recycling was offered to attendees of the event, with proceeds going to local nonprofits.\n\nDesign for Energy and the Environment is the first annual symposium hosted by SEI, which will include presentations from representatives from industry, government agencies, and academia. Topics of the symposium include: education, materials and design, life cycle analysis, policies, design for end-of-life, and electronics recovery. In addition, the symposium will include keynote speakers from the United States Environmental Protection Agency (EPA), Dell, and Walmart. The symposium welcomes designers, manufacturers, chemists, electrical engineers, government representatives, and all others.\n\nThe DEE Lab is a new campus research laboratory bringing together faculty and students from design, marketing, business and engineering to solve real world product development problems for industry. Companies are finding that they can no longer compete on the basis of technology alone. User needs must be met through technology, and successful companies must create innovative products that delight customers. For this reason, the DEE Lab links user-centered/customer focused and technology driven disciplines to work toward common goals and solutions. DEE Lab integrates research and instruction to solve real world problems for industry while preparing students for leadership positions in collaborative product development. DEE Lab’s integral disciplines are industrial design, graphic design, architecture and marketing, given their user centered roles, and engineering to support the integration of technology and manufacturing innovation. DEE Lab’s design process methodology combines customer, technology, business and marketing research with structured analysis, interdisciplinary brainstorming, rapid prototyping and evaluation. Research deliverable include development of smart products and technology transfer, strategic product and systems concepts, sustainable product development, ethnographic studies, trends forecasting, ergonomic verification and envisioning new products and systems. Research guides companies in articulating the future or addressing immediate needs through a fresh independent perspective.\n\nThe DEE Lab is located within the Illinois Sustainable Technology Center and is a part of the Sustainable Electronics Initiative. William Bullock, a professor with the University of Illinois at Urbana-Champaign School of Art and Design is the director of the DEE lab.\n\nThe Sustainable E-Waste Design Competition is a way for University of Illinois students to get involved in the Sustainable Electronics Initiative. During the spring 2009 semester, students were challenged to create appealing, useful products from e-waste through a School of Art and Design course. Students conducted an e-waste collection on campus and twenty teams each developed useful items from the collected e-waste. The projects were judged by a group of industry representatives. Six of the teams were awarded a total of fifteen thousand dollars in scholarships provided by industry sponsors Wal-Mart, Dell, Motorola and Microsoft.\n\nThe E-Waste Design competition will occur during the Spring 2010, with the final judging occurring on April 20, 2010. This year, the competition will be international in scope, with participants submitting projects in the form of videos on YouTube.\n\nSEI offers an extensive section of resources designated for research regarding sustainable electronics through RefWorks.\n\nIn addition, the Greater Lakes Regional Pollution Prevention Roundtable offers sector resources specifically geared toward electronic waste.\n\n"}
{"id": "15705625", "url": "https://en.wikipedia.org/wiki?curid=15705625", "title": "Sélection de Grains Nobles", "text": "Sélection de Grains Nobles\n\nSélection de Grains Nobles (SGN) is French for \"selection of noble berries\" and refers to wines made from grapes affected by noble rot. SGN wines are sweet dessert wines with rich, concentrated flavours. Alsace wines were the first to be described as \"Sélection de Grains Nobles\", with the legal definition introduced in 1984, but the term is also seen in some other wine regions France, such as Loire.\n\nFor Alsace wines, SGN is the highest official category for late harvest wines, while the step below is called \"Vendange tardive\".\n\nIn 2001, the minimum must weight requirements for SGN in Alsace were increased to 18.2% for Gewürztraminer and Pinot gris, and 16.4% for Riesling and Muscat, expressed as potential alcohol. Only these four \"noble varieties\" may carry the SGN designation, or the \"Vendange tardive\" designation.\n\nThe required level ripeness of the grapes are as follows, expressed as sugar content of the must and potential alcohol:\n\nThese requirements make SGN roughly equivalent to a German Beerenauslese, but the Alsace style tend to favour slightly higher alcohol levels, which means that the residual sugar often is a little lower than in German wines, especially for Riesling and Muscat.\n\nOn rare occasions, the designation Quintessence de Grains Nobles (QGN) is seen for wines that significantly exceed the minimum requirements for SGN wines. Unlike the German designation Trockenbeerenauslese (TBA), QGN is no official designation, but it could be thought of as the Alsatian equivalent of a high-grade TBA. The term was invented in 1983 by Domaine Weinbach to describe an exceptional cuvee of that year's vintage, and while still used primarily by Weinbach, it has been adopted by a few other producers, including Marcel Deiss.\n\nIn the case of Loire valley wines, the designation SGN usually denotes wines that are extra sweet, and produced by one of the region's so-called \"sugar hunters\". Often, the same producer will have another wine from the same appellation with less residual sugar without the SGN designation. The SGN designation can be used in the appellations Coteaux du Layon and Coteaux de l'Aubance and in both cases this requires the grape must used for the wine to have a minimum sugar content of 294 grams per liter rather than the basic level of 221 or 230 grams per liter respectively.\n\n"}
{"id": "18063991", "url": "https://en.wikipedia.org/wiki?curid=18063991", "title": "TN 753", "text": "TN 753\n\nThe TN 753 is a hand thrown fragmentation grenade. The grenade is made of metal with pre-divided fragments and covered in plastic.\n"}
{"id": "44064649", "url": "https://en.wikipedia.org/wiki?curid=44064649", "title": "Table computer", "text": "Table computer\n\nA table computer, or a table PC, or a tabletop is a device class of a full-featured large-display portable all-in-one computer with an internal battery. It can either be used on a table's top, hence the name, or carried around the house.\n\nTable computers feature an 18-inch or larger multi-touch touchscreen display, a battery capable of at least 2 hours of autonomous work and a full-featured desktop operating system, such as Windows 10. They are typically shipped with pre-installed multi-user touch-enabled casual games and apps, and typically marketed as family entertainment devices. Manufacturers of some table computers provide a specialized graphical user interface to simplify a simultaneous interaction of multiple users, one example is \"Aura\" interface, which is installed in Lenovo IdeaCentre Horizon tabletop.\n\nA number of manufacturers released their own versions of tabletops, some prominent examples are HP Envy Rove 20, Dell XPS 18 and Sony VAIO Tap 20.\n\n"}
{"id": "33332909", "url": "https://en.wikipedia.org/wiki?curid=33332909", "title": "Technology Specialist", "text": "Technology Specialist\n\nAn IT Specialist, computer professional, or an IT professional may be:\n\nJob titles for a computer professional include:\n"}
{"id": "5340045", "url": "https://en.wikipedia.org/wiki?curid=5340045", "title": "Test kitchen", "text": "Test kitchen\n\nA test kitchen is a kitchen used for the process of developing new kinds of food. On the largest scale, they are run by the research and development departments of large companies in the food industry. Other test kitchens are owned by individuals who enjoy the craft of developing new recipes.\n\nThe name has been given to a popular American television show called America's Test Kitchen.\n\n"}
{"id": "8400809", "url": "https://en.wikipedia.org/wiki?curid=8400809", "title": "The W. Alton Jones Cell Science Center", "text": "The W. Alton Jones Cell Science Center\n\nThe W. Alton Jones Cell Science Center (1971–1995) was a non-profit research and education center on 10 Old Barn Road in Lake Placid, New York. The Center was established by a gift of of land and $3 million to the Tissue Culture Association from the W. Alton Jones Foundation through efforts of Nettie Marie Jones, widow of W. Alton Jones who was former chairman of the Board of Cities Service Company (see Citgo). The original tax-free gift was accompanied by the institutional charter that use of the facility would be restricted forever to non-profit activities related to research and education on the biology of cells.\n\nThe Cell Center was largely the vision of cell culture pioneer Dr. George Otto Gey, director of the Finney-Howell Cancer Research Laboratory at the Johns Hopkins Hospital, a founder and first President of The Tissue Culture Association (now the Society for In Vitro Biology). Dr. Gey was introduced to Nettie Marie Jones, widow of W. Alton Jones, through her daughter Patricia Jones, an employee or acquaintance at Johns Hopkins. A highlight of the W. Alton Jones Cell Science Center building was the George and Margaret Gey Library. The objective was to provide a center in the peaceful setting of the Adirondack Mountains where experts in the fields of genetics, immunology, virology, insect physiology and other invertebrates unified by common interest in the art and science of culturing cells outside the body could come together, pool their ideas and techniques, and convey them to others.\n\nIn the period 1971 to 1980, the Cell Center consisted of research groups oriented around the theme of cell and tissue culture, provided specialty 1 to 3 week courses and hosted international meetings on the theme. The first Director was Dr. Donald Merchant followed by Dr. Paul Chapple.\n\nFor the period 1971 through 1979 the W. Alton Jones Foundation contributed annually to the operating expenses and mission of the Cell Center through the influence of Nettie Marie Jones. In 1979, Mrs. Jones was in poor health and nearing age 100. At that time Charlottesville, Virginia-based daughter of Mrs. Jones, Patricia Jones Edgerton, took charge of the W. Alton Jones Foundation and together with longtime family associate William C. Battle, ambassador to Australia under the Kennedy administration, established an independent corporation called the W. Alton Jones Cell Science Center, Inc. Edgerton and Battle and associates maintained concurrent control of the W. Alton Jones Foundation and the W. Alton Jones Cell Science Center, Inc. In the early 1980s, the Tissue Culture Association, subsequently the Society for In Vitro Biology (SIVB), under President Keith R. Porter, was pressured to relinquish deed to the Cell Center property and facility originally donated to them tax-free by the W. Alton Jones Foundation to the newly established W. Alton Jones Cell Science Center, Inc. Without sufficient resources to support legal action to retain ownership of the property and enforce the original non-profit charter and mission, the deed was relinquished. Subsequently the SIVB agreed retroactively to relingquish enforcement of the \"non-profit use only\" stipulation of the original charter along with the earlier transfer of the deed to the property for a donation of $50,000 from the Adirondack Biomedical Institute, Inc. (Director, Dr. James Stevens).\n\nIn 1982 the W. Alton Jones Foundation donated $17.5 million to the W. Alton Jones Cell Science Center, Inc. of to support the recruitment and program of Dr. Gordon H. Sato as Director. Dr. Sato's mission was to build a financially independent world-class basic research and teaching institute in the Adirondack Mountains generally oriented around the applications of cell culture technologies to broad problems in human health and disease through translational biotechnology to industry and the clinic. In his own words he envisioned \"a self-endowed Rockefeller University-type institution\" in the middle of the Adirondack Park, New York's statewide counterpart of New York City's Central Park.\n\nIn the period 1983-1993 the research staff of the Center increased by 10 fold. Sato purchased several local properties for staff and student housing. He established an international Ph.D. program in Chemical Biology with nearby Clarkson University, Potsdam (village), New York and a joint program with the University of Vermont, Burlington, Vermont. He established the annual W. Alton Jones International Symposium in Cellular Endocrinology centered on honoring movers and shakers in the field. During his administration the Center acquired program project grants from the National Cancer Institute and National Institute of Diabetes and Digestive and Kidney Diseases. Through Sato's efforts, the Cell Center acquired worldwide recognition through contributions of its researchers to basic research and biotechnological applications. Its former researchers and students hold leadership positions in academics and biotechnology worldwide. During this period the Lake Placid center became the headquarters of The Manzanar Project, a global action project aimed at attacking the planet's most critical problems as poverty, hunger, environmental pollution, and global warming through low tech biotechnological methods in salt water deserts that can be transferred to the indigenous inhabitants\n\nTo ensure financial independence, a permanent endowment for the research center, \"to give scientists security and remove any temptation they may have felt to modify their research because of monetary support,\" Sato founded Upstate Biotechnology, Inc. (UBI) to be solely owned by the W. Alton Jones Cell Science Center, Inc. Startup for UBI was financed by loans from the W. Alton Jones Cell Science Center, Inc. from the $17.5 million gift from the W. Alton Jones Foundation to ensure that ownership and proceeds of UBI flowed solely into support and long term endowment of the Cell Center without interference by private interests. As profitability of UBI increased, private interests and the controlling overlapping members of the Boards of both the for-profit UBI, the non-profit W. Alton Jones Cell Science Center, Inc. and the non-profit W. Alton Jones Foundation and their associates diverted the UBI mission away from the goal of providing permanent support and endowment of the Cell Center in its Lake Placid, New York location in the Adirondack Mountains. This precipitated the resignation of Dr. Sato as Director.\n\nIn 1996 Edgerton and Battle and associates recruited venture capitalist Sheridan Snyder to become involved with UBI, later called Upstate USA, Inc., Upstate Group or simply Upstate. In 1996 the not-for-profit W. Alton Jones Cell Science Center, Inc. was dissolved and the assets, which included Upstate Biotechnology, Inc., were transferred to a newly incorporated non-for-profit entity, the Adirondack Biomedical Research Institute (ABRI), Inc. In 1998 the ABRI ceased operations as a non-profit basic research entity and announced the facility would become an \"incubator facility\" for biotechnology companies in the Adirondack Mountains region with Upstate, Inc. and Argonex (a Snyder startup) as lead companies. \n\nIn 2000, the ABRI corporation was dissolved and the property and facilities purchased by for-profit Upstate, Inc. of which Argonex was a major shareholder for $1 million. In 2004 Upstate was sold to Serologicals, Inc. for $204 million. Upstate and Serologicals, Inc. are now a division of Millipore Corporation. The site was offered for sale on the Lake Placid real estate market by Millipore Corporation for $5.9 million and was reportedly sold in 2007 to a local partnership of Lake Placid real estate and business investors for about $3 million.\n\nIn 2000 the Ivy Foundation chaired by William C. Battle was established with a reported endowment of $7 million from funds from the closure of the Adirondack Biomedical Research Foundation, Inc. (formerly the W. Alton Jones Cell Science Center, Inc.) that included Upstate Biotechnology, Inc. Subsequent to the sale of the Upstate Group in 2005 the Ivy Foundation listing Board of Directors as William C. Battle, Arthur Garson, Jr., William Black, Sheridan Snyder, Patricia J. Edgerton, Aaron Shatkin and Dr. Robert W. Battle made a gift of $45 million to the University of Virginia, Charlottesville, Virginia, the largest single gift in the history of the University.\n\nJust after the establishment of the Ivy Foundation, but prior to the sale of the privately held Upstate Biotechnology, Inc. to Serologicals, Inc., the Charlottesville, Virginia-based 56-year-old W. Alton Jones Foundation suddenly dissolved in 2001. The $400 million endowment was split into three separate Foundations, the Blue Moon Fund run by Patricia Jones Edgerton (daughter of W. Alton Jones) and her daughter Diane Edgerton Miller, the Oak Hill Foundation run by son William Edgerton, and the Edgerton Foundation run by son Brad Edgerton.\n"}
{"id": "20623501", "url": "https://en.wikipedia.org/wiki?curid=20623501", "title": "Timeline of Australian inventions", "text": "Timeline of Australian inventions\n\nThis is a timeline of Australian inventions consisting of products and technology invented in Australia from pre-European-settlement in 1788 to the present. The inventions are listed in chronological order based on the date of their introduction.\n\nAustralian inventions include the very old, such as woomera, and the very new, such as the scramjet, first fired at the Woomera rocket range. The Australian government has suggested that Australian inventiveness springs from the nation's geography and isolation. Perhaps due to its status as an island continent connected to the rest of the world Australians have been leaders in inventions relating to both maritime and aeronautical matters, including powered flight, the black box flight recorder, the inflatable escape slide, the surf ski and the wave-piercing catamaran winged keel. Since the earliest days of European settlement, Australia's main industries have been agriculture and mining. As a result of this, Australians have made many inventions in these areas, including the grain stripper, the stump jump plough, mechanical sheep shears, the Dethridge water wheel, the froth flotation ore separation process, the instream ore analysis process and the buffalo fly trap.\n\nAustralian inventions also include a number of weapons or weapons systems, including the woomera, the tank, and the underwater torpedo. In recent years, Australians have been at the forefront of medical technology with inventions including ultrasound, the bionic ear, the first plastic spectacle lenses, the electronic pacemaker, the multi-focal contact lens, spray-on artificial skin and anti-flu medication. Australians also developed a number of useful household items, including Vegemite, and the process for producing permanently creased fabric.\n\nMany of Australia's inventions were realised by individuals who get little credit or who are often overlooked for more famous Americans or Europeans.\n\nAustralian Aborigine David Unaipon is known as \"Australia's Leonardo\" for his contributions to science and the Aboriginal people. His inventions include a tool for sheep-shearing, a centrifugal motor, a multi-radial wheel and mechanical propulsion device. Unaipon appears on Australia's $50 note.\n\nThe Commonwealth Scientific and Industrial Research Organisation (CSIRO) is an Australian-government-funded institution. A number of CSIRO funded scientists and engineers are featured in this list. CSIRO scientists lead Australian research across a number of different fields, and work with industry and government to solve problems such as using insects to tackle weeds, growing more sustainable crops and improving transportation.\n\nDidgeridoo – The didgeridoo is a wind instrument of northern Australia. It is sometimes described as a \"drone pipe,\" but musicologists classify it as an aerophone. Traditionally, a didgeridoo was made by selecting a section of a Eucalyptus branch, then burying it near a termite mound so that the termites would hollow it out, to produce a long, hollow piece of wood suitable for fashioning the instrument.\n\nWoomera – The woomera is a type of spear thrower, adding thrust to a spear as part of a throwing action.\n\n1843 – Grain stripper – John Wrathall Bull invented and John Ridley manufactured in South Australia the world's first mechanised grain stripper. It utilised a comb to lift the ears of the crop to where revolving beaters deposited the grain into a bin.\n\n1856 – Refrigerator – Using the principle of vapour compression, James Harrison produced the world's first practical ice making machine and refrigerator.\n1858 – Australian rules football – began its development when Tom Wills wrote a letter published in \"Bell's Life in Victoria & Sporting Chronicle\" on 10 July 1858, calling for a \"foot-ball club, a rifle club, or other athletic pursuits\" to keep cricketers fit during winter. An experimental match was played by Wills and others at the Richmond Paddock, later known as Yarra Park next to the Melbourne Cricket Ground on 31 July 1858.\nThe Melbourne Football Club rules of 1859 are the oldest surviving set of laws for Australian football. They were drawn up at the Parade Hotel, East Melbourne, on 17 May, by Wills, W. J. Hammersley, J. B. Thompson and Thomas Smith.\nThe Melbourne club's game was not immediately adopted by neighbouring clubs. Before each match the rules had to be agreed by the two teams involved. By 1866, several other clubs had agreed to play by an updated version of Melbourne's rules.\n\n1859 — Photolithography — developed by John Walter Osborne at the Victorian government's Crown Lands Office. During a land boom the Office had trouble producing the many maps and documents required to keep land records updated. Instead of having to copy surveyor's originals, or having to store stone originals, master copies saved were glass slides of around square.\n\n1874 – Underwater torpedo – Invented by Louis Brennan, the torpedo had two propellers, rotated by wires which were attached to winding engines on the shore station. By varying the speed at which the two wires were extracted, the torpedo could be steered to the left or right by an operator on the shore.\n\n1876 – Stump jump plough – Richard and Clarence Bowyer Smith developed a plough which could jump over stumps and stones, enabling newly cleared land to be cultivated.\n\n1877 – Mechanical clippers – Various mechanical shearing patents were registered in Australia before Frederick York Wolseley finally succeeded in developing a practical hand piece with a comb and reciprocating cutter driven by power transmitted from a stationary engine.\n\n1889 – Electric drill – Arthur James Arnot patented the world's first electric drill on 20 August 1889 while working for the Union Electric Company in Melbourne. He designed it primarily to drill rock and to dig coal.\n\n1892 – Coolgardie safe – Arthur Patrick McCormick noticed that a wet bag placed over a bottle cooled its contents, and the cooling was more pronounced in a breeze. The Coolgardie safe was a box made of wire and hessian sitting in water, which was placed on a verandah so that any breeze would evaporate the water in the hessian and via the principle of evaporation, cool the air inside the box. The Coolgardie safe was used into the middle of the 20th century as a means of preserving food.\n\n1902 – Notepad – For 500 years, paper had been supplied in loose sheets. Launceston stationer J.A. Birchall decided that it would be a good idea to cut the sheets in half, back them with cardboard and glue them together at the top.\n\n1903 – Froth flotation – The process of separating minerals from rock by flotation was developed by Charles Potter and Guillaume Delprat in New South Wales. Both worked independently at the same time on different parts of the process for the mining company Broken Hill Pty. Ltd. (BHP)\n1906 – Feature film – The world's first feature-length film, The Story of the Kelly Gang, was a little over an hour long.\n\n1906 – Surf life-saving reel – The first surf life-saving reel in the world was demonstrated at Bondi Beach on 23 December 1906 by its designer, Bondi surfer Lester Ormsby.\n\n1907 – Michell thrust block bearing – Fluid-film thrust bearings were invented by Australian engineer George Michell. Michell bearings contain a number of sector-shaped pads, arranged in a circle around the shaft, and that are free to tilt. These create wedge-shaped regions of oil inside the bearing between the pads and a rotating disk, which support the applied thrust and eliminate metal-on-metal contact. The small size (one-tenth the size of old bearing designs), low friction and long life of Michell's invention made possible the development of larger propellers and engines in ships. They were used extensively in ships built during World War I, and have become the standard bearing used on turbine shafts in ships and power plants worldwide.\n\n1910 – Humespun pipe-making process – The Humespun process was developed by Walter Hume of Humes Ltd for making concrete pipes of high strength and low permeability. The process used centrifugal force to evenly distribute concrete onto wire reinforcing, revolutionising pipe manufacture.\n\n1910 – Dethridge wheel – The wheel, used to measure the water flow in an irrigation channel, consisting of a drum on an axle, with eight v-shaped vanes fixed to its outside, was invented by John Dethridge, Commissioner of the Victorian State Rivers and Water Supply Commission.\n1912 – Surf ski – Harry McLaren and his brother Jack used an early version of the surf ski for use around the family's oyster beds on Lake Innes, near Port Macquarie, New South Wales, and the brothers used them in the surf on Port Macquarie's beaches. The board was propelled in a sitting position with two small hand blades, which was probably not a highly efficient method to negotiate the surf. The deck is flat with a bung plug at the rear and a nose ring with a leash, possibly originally required for mooring. The rails are square and there is pronounced rocker. The boards' obvious buoyancy indicates hollow construction, with thin boards of cedar fixed longtitudinally down the board.\n\n1912 – Tank – South Australian Lance de Mole submitted a proposal to the British War Office, for a 'chain-rail vehicle which could be easily steered and carry heavy loads over rough ground and trenches,' complete with extensive drawings. The British war office rejected the idea at the time, but De Mole made several more proposals to the British War Office in 1914 and 1916, and formally requested he be recognised as the inventor of the Mark I tank. The British Royal Commission on Awards to Inventors eventually made a payment of £987 to De Mole to cover his expenses and promoted him to an honorary corporal.\n\n1912 – Self-Propelled Rotary Hoe – At the age of 16 Cliff Howard of Gilgandra invented a machine with rotating hoe blades on an axle that simultaneously hoed the ground and pulled the machine forward.\n\n1913 – Automatic totalisator -The world's first automatic totalisator for calculating horse-racing bets was made by Sir George Julius.\n\n1928 – Electronic Pacemaker – Developed by Edgar H Booth and Mark C Liddell, the heart pacemaker had a portable apparatus which 'plugged into a lighting point. One pole was applied to a skin pad soaked in strong salt solution' while the other pole 'consisted of a needle insulated except at its point, and was plunged into the appropriate cardiac chamber'. 'The pacemaker rate was not good from about 80 to 120 pulses per minute, and likewise the voltage variable from 1.5 to 120 volts.' The apparatus was used to revive a potentially stillborn infant at Crown Street Women's Hospital, Sydney whose heart continued 'to beat on its own accord', 'at the end of 10 minutes' of stimulation.\n1930 – Clapperboard – The wooden marker used to synchronise sound and film was invented by Frank Thring Sr of Efftee Studios in Melbourne.\n1934 – Coupé utility – The car body style, known colloquially as the ute in Australia and New Zealand, combines a two-door \"coupé\" cabin with an integral cargo bed behind the cabin—using a light-duty passenger vehicle-derived platform. It was designed by Lewis Bandt at the Ford Motor Company in Geelong, Victoria. The first ute rolled off the Ford production lines in 1934. The idea came from a Geelong farmer's wife who wrote to Ford in 1933 advising the need for a new sort of vehicle to take her 'to church on Sundays and pigs to market on Mondays.'\n\n1938 – Polocrosse – Inspired by a training exercise witnessed at the National School of Equitation at Kingston Vale near London, Mr. and Mrs. Edward Hirst of Sydney invented the combination polo and lacrosse sport which was first played at Ingleburn near Sydney in 1939.\n\n1940 – Zinc Cream – This white sun block made from zinc oxide was developed by the Fauldings pharmaceutical company.\n1943 – Splayd – The combination of knife, fork and spoon was invented by William McArthur after seeing ladies struggle to eat at barbecues with standard cutlery from plates on their laps.\n\n1945 – Hills Hoist – The famous Hills Hoist rotary clothes line with a winding mechanism allowing the frame to be lowered and raised with ease was developed by Lance Hill in 1945, although the clothes line design itself was originally patented by Gilbert Toyne in Adelaide in 1926.\n1952 – Atomic Absorption Spectrophotometer – The atomic absorption spectrophotometer is used in chemical analysis to determine low concentrations of metals in gases or vaporized solutions. It was developed by Sir Alan Walsh of the CSIRO using ionization lamps specific to the metal being detected.\n\n1953 – Solar hot water – Developed by a team at the CSIRO led by Roger N Morse\n\n1955 – Distance Measuring Equipment (DME) – Invented and developed by Edward George Bowen of the CSIRO, the first DME network, operating in the 200 MHz band, became operational in Australia.\n\n1956 – Pneumatic broadacre air seeder – Invented and patented by Albert Fuss in 1956, the lightweight air seeder uses a spinning distributor, blew the seeds through a pipe into the plating tynes. It was first used that same year to sow wheat near Dalby in Queensland.\n\n1956 – Stainless Steel Braces – Percy Raymond Begg of Adelaide collaborated with metallurgist Arthur Wilcock to develop a gentler, stainless steel system in 1956 involving gradual adjustments rather than earlier brute force methods used to straighten teeth.\n\n1957 – Flame ionisation detector – The flame ionisation detector is one of the most accurate instruments ever developed for the detection of emissions. It was invented by Ian McWilliam. The instrument, which can measure one part in 10 million, has been used in chemical analysis in the petrochemical industry, medical and biochemical research, and in the monitoring of the environment.\n\n1957 – Wool clothing with a permanent crease – \"SiroSet,\" the process for producing permanently creased fabric, was invented by Dr Arthur Farnworth of the CSIRO.\n1958 – Black box flight recorder – The 'black box' voice and instrument data recorder was invented by Dr David Warren in Melbourne.\n\n1960 – Plastic spectacle lenses – The world's first plastic spectacle lenses, 60 per cent lighter than glass lenses, were designed by Scientific Optical Laboratories in Adelaide.\n\n1961 – Ultrasound – David Robinson and George Kossoff's work at the Australian Department of Health, resulted in the first commercially practical water path ultrasonic scanner in 1961. \n\n1963 - Rostrum camera – The first animation rostrum was commissioned by Graphik Animation, later known as Raymond Lea Animation in 1963. Designed and constructed by Jack Kennedy with the assistance of Jim Lynich, it was in operation by 1964.\n\n1965 – Inflatable escape slide – The inflatable aircraft escape slide which doubles as a raft was invented by Jack Grant of Qantas.\n1965 – Wine cask – Invented by Thomas Angove of Renmark, South Australia, the wine cask is a cardboard box housing a plastic container which collapses as the wine is drawn off, thus preventing contact with the air. Angroves' original design with a resealable spout was replaced with a tap by the Penfolds wine company in 1972\n\n1970 – Staysharp knife – The self-sharpening knife was developed by Wiltshire.\n\n1971 – Variable rack and pinion steering – The variable ratio rack and pinion steering in motor vehicles allowing smooth steering with minimal feedback was invented by Australian engineer, Arthur Bishop.\n\n1972 – Orbital engine – The orbital internal combustion process engine was invented by engineer Ralph Sarich of Perth, Western Australia. The system uses a single piston to directly inject fuel into 5 orbiting chambers. It has never challenged the dominance of four-stroke combustion engines but has replaced many two-stroke engines with a more efficient, powerful and cleaner system. Orbital engines now appear in boats, motorcycles and small cars.\n\n1972 – Instream analysis – To speed-up analysis of metals during the recovery process, which used to take up to 24 hours, Amdel Limited developed an on-the-spot analysis equipment called the In-Stream Analysis System, for the processing of copper, zinc, lead and platinum – and the washing of coal. This computerised system allowed continuous analysis of key metals and meant greater productivity for the mineral industry worldwide.\n\n1972 – Power board – Peter Talbot, working under Frank Bannigan at Kambrook, invented the power board. This allows multiple electrical devices to be powered where only a single wall socket is available. This is a well-known example of failing to protect intellectual property. Kambrook was more interested in immediate commercial release than patenting its idea and has never received any royalties from this now ubiquitous product.\n\n1974 – Super Sopper – Gordon Withnall at the age of 56 invented the Super Sopper, a giant rolling sponge used to quickly soak up water from sporting grounds so that play can continue.\n\n1978 – Synroc – The synthetic ceramic Synroc that incorporates radioactive waste into its crystal structure was invented in 1978 by a team led by Dr Ted Ringwood at the Australian National University.\n\n1979 – Digital sampler – The Fairlight CMI (Computer Musical Instrument) was the first polyphonic digital sampling synthesizer. It was designed in 1979 by the founders of Fairlight, Peter Vogel and Kim Ryrie in Sydney, Australia.\n\n1979 – RaceCam – Race Cam was developed by Geoff Healey, an engineer with Australian Television Network Seven in Sydney. The tiny lightweight camera is used in sports broadcasts and provides viewers with spectacular views of events such as motor racing, which are impossible with conventional cameras.\n1979 – Bionic ear – The cochlear implant was invented by Professor Graeme Clark of the University of Melbourne.\n\n1980 – Dual flush toilet – Bruce Thompson, working for Caroma in Australia, developed the Duoset cistern, with two buttons, and two flush volumes as a water-saving measure, now responsible for savings in excess of 32000 litres of water per household per year.\n\n1980 – Wave-piercing catamaran – The first high speed, stable catamarans were developed by Phillip Hercus and Robert Clifford of Incat in Tasmania.\n\n1981 – CPAP mask – Professor Colin Sullivan of Sydney University developed the Continuous Positive Airflow Pressure (CPAP) mask. The CPAP system first developed by Sullivan has become the most common treatment for sleep disordered breathing. The invention was commercialised in 1989 by Australian firm ResMed, which is currently one of the world's two largest suppliers of CPAP technology.\n1983 – Winged Keel – Ben Lexcen designed a winged keel that helped Australia II end the New York Yacht Club's 132-year ownership of the America's Cup. The keel gave the yacht better steering and manoeuvrability in heavy winds.\n\n1984 – Frozen embryo baby- The world's first frozen embryo baby was born in Melbourne on 28 March 1984\n\n1984 – Baby Safety Capsule – In 1984, for the first time babies had a bassinette with an air bubble in the base and a harness that distributed forces across the bassinette protecting the baby. New South Wales public hospitals now refuse to allow parents take a baby home by car without one.\n\n1985 – Technegas – Technegas is an inhalable aerosol radioactively labelled with the isotope 99mTc, and is employed in nuclear medicine imaging for lung ventilation scanning. Technegas lung scans in conjunction with lung perfusion scans demonstrate the presence of the life-threatening condition of pulmonary embolism. Technegas was invented in Australia by Dr Richard Fawdry and Dr Bill Burch.\n\n1986 – Gene shears – The discovery of gene shears was made by CSIRO scientists, Wayne Gerlach and Jim Haseloff. So-called hammerhead ribozymes are bits of genetic material that interrupt a DNA code at a particular point, and can be used to cut out genes that cause disease or harmful proteins.\n\n1988 – Polymer banknote – The development of the polymer bank note was made by CSIRO scientists led by Dr. David Solomon. Securency Pty Ltd, a joint venture between the Reserve Bank of Australia (RBA) and UCB, brought the note into full production and polymer bank notes are now used in 30 countries besides Australia. The chief advantages are high counterfeiting resistance and longer circulation lifetimes.\n\n1989 – Polilight forensic lamp – Ron Warrender and Milutin Stoilovic, forensic scientists at the Australian National University in Canberra, developed Unilite which could be set to just the right wavelength to show fingerprints up well against any background. Rofin Australia Pty Ltd, developed this product into the portable Polilight which shows up invisible clues such as fingerprints and writing that has been scribbled over, as well as reworked sections on paintings.\n\n1991 – Buffalo fly trap – In 1991 the CSIRO developed a low-tech translucent plastic tent with a dark inner tunnel lined with brushes. When a cow walks through, the brushed flies fly upwards toward the light and become trapped in the solar-heated plastic dome where they quickly die from desiccation (drying out) and fall to the ground, where ants eat them.\n\n1992 – Multi-focal contact lens – The world's first multi-focal contact lens was invented by optical research scientist, Stephen Newman in Queensland.\n\n1992 – Spray-on skin – Developed by Dr Fiona Wood at Royal Perth Hospital\n\n1992 – Product Activation – Patented by Ric Richardson of Sydney's northern beaches initially to allow digital distribution of his own software. Now the process is used by the majority of software publishers in the world.\n\n1992 – Wi-Fi – A method developed by CSIRO researchers used to \"unsmear\" radio waves that echo off indoor surfaces was patented. This method has caused WiFi to be attributed as an Australian invention, although the Wi-Fi trademark, under which most products are sold, is under the ownership of the Wi-Fi Alliance based in Austin, Texas.\n\n1993 – Underwater PC – The world's first underwater computer with a five-button hand-held keypad was developed by Bruce Macdonald at the Australian Institute of Marine Science.\n\n1993 – Frazier lens – The Frazier lens is a special camera lens designed by Australian photographer Jim Frazier. The Frazier lens provides a massive depth of field, allowing the foreground and background of an image to be in focus. Frazier's lenses have been widely used in Hollywood and wildlife cinematography.\n\n1995 – EXELGRAM – The world's most sophisticated optical anti-counterfeiting technology was developed by the CSIRO.\n\n1995 – Gene silencing – A CSIRO team led by Dr Peter Waterhouse discovered that double-stranded RNA was the trigger for RNA interference (RNAi) or gene silencing.\n\n1995 – Jindalee Radar System – Developed by Scientists at the CSIRO, the Jindalee Radar System detects stealth aircraft and missiles by searching for the air turbulence generated by such vehicles.\n1996 – Anti-flu Medication – Relenza was developed by a team of scientists at the Victorian College of Pharmacy at Monash University in Melbourne. The team was led by Mark von Itzstein in association with the CSIRO. Relenza was discovered as a part of the Australian biotechnology company Biota's project to develop antiviral agents via rational drug design.\n\n2002 – Scramjet – On 30 July 2002, the University of Queensland's HyShot team and their international partners conducted the first ever successful test flight of a scramjet. This test was conducted at the rocket range in outback South Australia called Woomera.\n\n2003 – Blast Glass – A ballistic and blast resistant glass system was invented by Peter Stephinson. Unlike conventional bulletproof glass it incorporates an air cavity to absorb the shock wave of explosions, and was effective in protecting the Australian Embassy in the Jakarta bombings of 2004.\n\n2006 – Cervical Cancer Vaccine – Professor Ian Frazer from University of Queensland created a preventative for cervical cancer, working with researchers in the United States. The commercial application, Gardasil, is a vaccine to work against certain types of human papillomavirus (HPV).\n\n2010 – Robotic Visual Horizon – An automated system that allows unmanned aeroplanes to perform complex manoeuvres was adapted from the way a bee's brain processes visual information during flight by researchers and engineers at the Vision Centre, the Queensland Brain Institute and the School of Information Technology and Electrical Engineering at the University of Queensland.\n\n2011 – Anti-Hacking Software Kernel – National ICT Australia (NICTA), and Open Kernel Labs (OK Labs) released the seL4 microkernel, a small operating system kernel which regulates access to a computer's hardware and is able to distinguish between trusted and untrusted software, allowing secure financial or secret data to be used on the same platform as everyday applications, protecting the secure data from hackers.\n\n2012 – Quantum bit – A team of Australian scientists built the first quantum bit, the basic unit of quantum computing, using a single phosphorus atom implanted into a silicon chip. Research leaders include Andrew Dzurak of the University of Sydney and Andrea Morello of the University of NSW.\n2013 – Blood test to prevent stillbirth – A Melbourne medical research institution, Mercy Health, identified a method of analysing RNA fragments in a mother's blood that indicates oxygen and nutrient deprivation in the foetus.\n\n2015 - Quantum Logic Gate - Engineers at the University of New South Wales successfully built a Quantum Logic Gate using two qubits into silicon. Logic gates are the main idea behind computational theory, allowing qubits to be utilised for computation, paving the way for commercial applications.\n\n2018 - Modular self-fit hearing aid - Collaboration between Government of Victoria, RMIT University, Swinburne University of Technology and Professor Peter Blamey and Professor Elaine Saunders release the first hearing aid with a modular design allowing users with severe dexterity issues to self-manage their own hearing aids. \n"}
{"id": "19063270", "url": "https://en.wikipedia.org/wiki?curid=19063270", "title": "Timeline of hydrogen technologies", "text": "Timeline of hydrogen technologies\n\nThis is a timeline of the history of hydrogen technology. \n\n\n\n\n\n\n"}
{"id": "147728", "url": "https://en.wikipedia.org/wiki?curid=147728", "title": "Toothbrush", "text": "Toothbrush\n\nThe toothbrush is an oral hygiene instrument used to clean the teeth, gums, and tongue. It consists of a head of tightly clustered bristle--atop of which toothpaste is supposed to go—mounted on a handle which facilitates the cleaning of hard-to-reach areas of the mouth.\n\nToothbrushes are available with different bristle textures, sizes, and forms. Most dentists recommend using a soft toothbrush since hard bristled toothbrushes can damage tooth enamel and irritate the gums.\n\nAlthough first made as an oral hygiene instrument, the toothbrush has seen other use as a precise cleaning tool as well, most specifically in the military. This is because of the many small strands that allow it to clean in small places many conventional cleaning tools cannot reach.\n\nBefore the invention of the toothbrush a variety of oral hygiene measures had been used. This has been verified by excavations in which chew sticks, tree twigs, bird feathers, animal bones and porcupine quills were recovered.\n\nThe predecessor of the toothbrush is the chew stick. Chew sticks were twigs with frayed ends used to brush the teeth while the other end was used as a toothpick. The earliest chew sticks were discovered in Sumer Mesopotamia in 3500 BC, an Egyptian tomb dating from 3000 BC, and mentioned in Chinese records dating from 1600 BC. The Greeks and Romans used toothpicks to clean their teeth and toothpick like twigs have been excavated in Qin Dynasty tombs. Chew sticks remain common in Africa the rural Southern United States and in the Islamic world the use of chewing stick Miswak is considered a pious action and has been prescribed to be used before every prayer five times a day. Miswaks have been used by Muslims since 7th century.\n\nThe first bristle toothbrush resembling the modern one was found in China. Used during the Tang Dynasty (619–907), it consisted of hog bristles. The bristles were sourced from hogs living in Siberia and northern China because the colder temperatures provided firmer bristles. They were attached to a handle manufactured from bamboo or bone, forming a toothbrush. In 1223, Japanese Zen master Dōgen Kigen recorded on Shōbōgenzō that he saw monks in China clean their teeth with brushes made of horsetail hairs attached to an oxbone handle. The bristle toothbrush spread to Europe, brought from China to Europe by travellers. It was adopted in Europe during the 17th century. The earliest identified use of the word toothbrush in English was in the autobiography of Anthony Wood who wrote in 1690 that he had bought a toothbrush from J. Barret. Europeans found the hog bristle toothbrushes imported from China too firm and preferred softer bristle toothbrushes made from horsehair. Mass-produced toothbrushes made with horse or boar bristle continued to be imported to England from China until the mid 20th century.\n\nIn Europe, William Addis of England is believed to have produced the first mass-produced toothbrush in 1780. In 1770, he had been jailed for causing a riot. While in prison he decided that using a rag with soot and salt on the teeth was ineffective and could be improved. After saving a small bone from a meal, he drilled small holes into the bone and tied into the bone tufts of bristles that he had obtained from one of the guards, passed the tufts of bristle through the holes in the bone and sealed the holes with glue. After his release, he became wealthy after starting a business manufacturing toothbrushes. He died in 1808, bequeathing the business to his eldest son. It remained within family ownership until 1996. Under the name Wisdom Toothbrushes, the company now manufactures 70 million toothbrushes per year in the UK. By 1840 toothbrushes were being mass-produced in England, France, Germany, and Japan. Pig bristles were used for cheaper toothbrushes and badger hair for the more expensive ones.\n\nThe first patent for a toothbrush was granted to H.N. Wadsworth in 1857 (U.S.A. Patent No. 18,653) in the United States, but mass production in the United States did not start until 1885. The improved design had a bone handle with holes bored into it for the Siberian boar hair bristles. Unfortunately, animal bristle was not an ideal material as it retained bacteria, did not desiccate efficiently and the bristles were often extricated from their intended fixed insertions. In addition to bone, handles were made of wood or ivory. In the United States, brushing teeth did not become routine until after World War II, when American soldiers had to clean their teeth daily.\nDuring the 1900s, celluloid gradually replaced bone handles. Natural animal bristles were also replaced by synthetic fibers, usually nylon, by DuPont in 1938. The first nylon bristle toothbrush made with nylon yarn went on sale on February 24, 1938. The first electric toothbrush, the Broxodent, was invented in Switzerland in 1954. By the turn of the 21st century nylon had come to be widely used for the bristles and the handles were usually molded from thermoplastic materials.\n\nJohnson & Johnson, a leading medical supplies firm, introduced the \"Reach\" toothbrush in 1977. It differed from previous toothbrushes in three ways: it had an angled head, similar to dental instruments, to reach back teeth; the bristles were concentrated more closely than usual to clean each tooth of potentially cariogenic (cavity-causing) materials; and the outer bristles were longer and softer than the inner bristles. Other manufacturers soon followed with other designs aimed at improving effectiveness.\nIn spite of the changes with the number of tufts and the spacing, the handle form and design, the bristles were still straight and difficult to maneuver. In 1978 Dr. George C. Collis developed the Collis Curve toothbrush which was the first toothbrush to have curved bristles. The curved bristles follow the curvature of the teeth and safely reach in between the teeth and into the sulcular areas. \nIn January 2003, the toothbrush was selected as the number one invention Americans could not live without according to the Lemelson-MIT Invention Index.\n\nIt has been discovered that compared to a manual brush, the multi-directional power brush might reduce the incidence of gingivitis and plaque, when compared to regular side-to-side brushing. These brushes tend to be more costly. An electric toothbrush performs rotations of its bristles and cleans hard to reach places. Most studies report performances equivalent to those of manual brushings, possibly with a decrease in plaque and gingivitis although the electric version can be more comfortable. An additional timer and pressure sensors can encourage a more efficient cleaning process.\nElectric toothbrushes can be classified, according to the speed of their movements as: standard power toothbrushes, sonic toothbrushes, or ultrasonic toothbrushes. Any electric toothbrush is technically a power toothbrush. If the motion of the toothbrush is sufficiently rapid to produce a hum in the audible frequency range (20 Hz to 20,000 Hz), it can be classified as a sonic toothbrush. Any electric toothbrush with movement faster than this limit can be classified as an ultrasonic toothbrush. Certain ultrasonic toothbrushes, such as the Megasonex and the Ultreo, have both sonic and ultrasonic movements.\n\nAn interdental or interproximal (\"proxy\") brush is a small brush, typically disposable, either supplied with a reusable angled plastic handle or an integral handle, used for cleaning between teeth and between the wires of dental braces and the teeth.\n\nThe use of interdental brushes in conjunction with tooth brushing, has been shown to reduce both the amount of plaque and the incidence of gingivitis when compared to toothbrushing alone. Although there is some evidence that after tooth brushing with a conventional tooth brush, interdental brushes remove more plaque than dental floss, a systematic review reported insufficient evidence to determine such an association.\n\nThe size of interdental brushes is standardized in ISO 16409. The brush size, which is a number between 0 (small space between teeth) and 8 (large space), indicates the \"passage hole diameter\". This corresponds to the space between two teeth that is just sufficient for the brush to go through without bending the wire. The color of the brushes differs between producers. The same is the case with respect to the wire diameter.\n\nA Sulcabrush is a type of toothbrush used specifically for cleaning along the gumline adjacent to the teeth. The bristles are usually shaped in a pointed arrow pattern to allow closer adaptation to the gums. A Sulcabrush is ideal for cleaning specific difficult-to-reach areas, such as between crowns, bridgework and crowded teeth. End-tufted brushes may also be used around fixed orthodontic appliances, such as braces.\n\nThe small round brush head comprises seven tufts of tightly packed soft nylon bristles, trimmed so the bristles in the center can reach deeper into small spaces. The brush handle is ergonomically designed for a firm grip, giving the control and precision necessary to clean where most other cleaning aids cannot reach. These areas include the posterior of the wisdom teeth (third molars), orthodontic structures (braces), crowded teeth, and tooth surfaces that are next to missing teeth. It can also be used to clean areas around implants, bridges, dentures and other appliances.\n\nA chewable toothbrush is a miniature plastic moulded toothbrush which can be placed inside the mouth. While not commonly used, they are useful to travelers and are sometimes available from bathroom vending machines. They are available in different flavors such as mint or bubblegum and should be disposed of after use. Other types of disposable toothbrushes include those that contain a small breakable plastic ball of toothpaste on the bristles, which can be used without water.\n\nCommonly, toothbrushes are made of plastic. Such brushes constitute a source of pollution. In order to reduce the environmental impact, some manufacturers have switched to using biodegradable materials and/or use replaceable heads. In order to avoid plastic altogether, alternative toothbrushes on offer consist of wooden handles (often bamboo) and bristles of bamboo viscose or pig bristles.\n\nTeeth can be damaged by several factors including poor oral hygiene, but also by wrong oral hygiene. Especially for sensitive teeth, damage to dentin and gums can be prevented by several measures including a correct brushing technique.\n\nIt is beneficial, when using a straight bristled brush, not to scrub horizontally over the necks of teeth, not to press the brush too hard against the teeth, to choose a toothpaste that is not too abrasive, and to wait at least 30 minutes after consumption of acidic food or drinks before brushing.\nThe invention of the Collis CurveTM curved bristle toothbrush allows for a simplified simultaneous brushing technique described specifically for this brush. \"The curved bristles rotate on their axes and slip into the sulcus as far as the junctional epithelium without lacerating it\" \nHarder tooth brushes reduce plaque more efficiently but are more stressful to teeth and gum; using a medium to soft tooth brush for a longer cleaning time was rated to be the best compromise between cleaning result and gum and tooth health.\n\nA study by University College London found that advice on brushing technique and frequency given by 10 national dental associations, toothpaste and toothbrush companies, and in dental textbooks was inconsistent.\n\n\n"}
