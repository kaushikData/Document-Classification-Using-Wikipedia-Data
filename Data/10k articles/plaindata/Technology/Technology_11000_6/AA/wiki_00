{"id": "39900727", "url": "https://en.wikipedia.org/wiki?curid=39900727", "title": "Advanced Library Format", "text": "Advanced Library Format\n\nAdvanced Library Format (ALF), also known as IEEE 1603 or IEC 62265, is an IEEE and IEC standard that describes a data specification language for library elements used in ASIC design applications for integrated circuits. ALF can model behavior, timing, power and noise, hot electron, electromigration, antenna effects, physical abstraction and physical implementation rules of library elements.\n\n\n"}
{"id": "53283864", "url": "https://en.wikipedia.org/wiki?curid=53283864", "title": "Aetas bsk", "text": "Aetas bsk\n\nAetas (AΣtas)  is a South African e-commerce company headquartered in KwaZulu-Natal, South Africa. Aetas develops Ready - to - Trade Online start-up companies and e-commerce systems for individuals and companies.\n\nAetas was founded in 2015 in the small coastal town Amanzimtoti, KwaZulu-Natal, South Africa. The company claims they were the first registered company in South Africa to sell Ready-to-Trade online start-up companies. The company focuses on crowd participation.\n\nAetas was founded in 2015 by Walter Paul van Zyl, after trying to open Aetas Online Campus (an online Campus that offered online short study courses).\n\nIn February 2016 Aetas launched an online boutique platform called Aetas Fashion Network. The platform helps individuals and companies to start their own online company by selling clothing and apparel online.\n\nIn 2017 Aetas Launched Aetas Mobile MVNO. It is a South African mobile network service provider. The company offers consumers a wide range of prepaid, hybrid and postpaid products and services, including voice, data and messaging services. With its focus on affordable access and a money back incentive.\n\nSince its launch, Aetas has been well received by popular magazines such as \"Fast Company\", who say \"It's about allowing people from all walks of life the opportunity to run their own business-and run it well\". The service was profiled in publications including \"Tech Smart\" magazine.\n\n"}
{"id": "1166525", "url": "https://en.wikipedia.org/wiki?curid=1166525", "title": "Agricultural biodiversity", "text": "Agricultural biodiversity\n\nAgricultural biodiversity is a sub-set of general biodiversity. The Convention on Biological Diversity (CBD) defines it as\n\n\"Agricultural biodiversity is a broad term that includes all components of biological diversity of relevance to food and agriculture, and all components of biological diversity that constitute the agricultural ecosystems, also named agro-ecosystems: the variety and variability of animals, plants and micro-organisms, at the genetic, species and ecosystem levels, which are necessary to sustain key functions of the agro-ecosystem, its structure and processes\" \n\nIt includes all forms of life directly relevant to agriculture: rare seed varieties and animal breeds (farm biodiversity), but also many other organisms such as soil fauna, weeds, pests, predators, and all of the native plants and animals (wild biodiversity) existing on and flowing through the farm. It also includes diversity of ecosystems. Agricultural biodiversity is essential to cope with the predicted impacts of climate change, not simply as a source of traits but as the underpinnings of more resilient farm ecosystems. Agricultural biodiversity is the basis of our agricultural food chain, developed and safeguarded by farmers, livestock breeders, forest workers, fishermen and indigenous peoples throughout the world. The use of agricultural biodiversity (as opposed to non diverse production methods) can contribute to food security, nutrition security and livelihood security.\n\nAlthough the term \"agricultural biodiversity\" is relatively new - it has come into wide use in recent years as evidenced by bibliographic references - the concept itself is quite old. It is the result of the careful selection and inventive developments of farmers, herders and fishers over millennia. Agricultural biodiversity is a vital sub-set of biodiversity. It is a use of life, i.e. ancillary biotechnologies, by Mankind whose food and livelihood security depend on the sustained management of those diverse biological resources that are important for food and agriculture. As for everything, agricultural biodiversity can be used, not used, misused and even abused. Agricultural biodiversity includes:\n\n\nHowever, agricultural biodiversity, sometimes called Agrobiodiversity, \"\"encompasses the variety and variability of animals, plants and micro-organisms which are necessary to sustain key functions of the agroecosystem, its structure and processes for, and in support of, food production and food security\". It further \"comprises genetic, population, species, community, ecosystem, and landscape components and human interactions with all these.\"\"\n\nHowever, most attention in this field is given to crop varieties and to crop wild relatives. Cultivated varieties can be broadly classified into “modern varieties” and “farmer's or traditional varieties”. Modern varieties are the outcome of formal breeding and are often characterized as 'high yielding'. For example, the short straw wheat and rice varieties of the Green Revolution. In contrast, farmer's varieties (also known as landraces) are the product of (breeding and) selection carried out by farmers. Together, these varieties represent high levels of genetic diversity and are therefore the focus of most crop genetic resources conservation efforts.\n\nAquatic diversity is also an important component of agricultural biodiversity. The conservation and sustainable use of local aquatic ecosystems, ponds, rivers, coastal commons by artisanal fisherfolk and smallholder farmers is important to the survival of both humans and the environment. Since aquatic organisms, including fish, provide much of our food supply as well as underpinning the income of coastal peoples, it is critical that fisherfolk and smallholder farmers have genetic reserves and sustainable ecosystems to draw upon as aquaculture and marine fisheries management continue to evolve.\n\nGenetic erosion in agricultural and livestock biodiversity is the loss of genetic diversity, including the loss of individual genes, and the loss of particular combinations of genes (or gene complexes) such as those manifested in locally adapted landraces or breeds. The term genetic erosion is sometimes used in a narrow sense, such as for the loss of alleles or genes, as well as more broadly, referring to the loss of varieties or even species. The major driving forces behind genetic erosion in crops are: variety replacement, land clearing, overexploitation of species, population pressure, environmental degradation, overgrazing, policy and changing agricultural systems.\n\nThe main factor, however, is the replacement of local varieties by high yielding or exotic varieties or species. A large number of varieties can also often be dramatically reduced when commercial varieties (including GMOs) are introduced into traditional farming systems. Many researchers believe that the main problem related to agro-ecosystem management is the general tendency towards genetic and ecological uniformity imposed by the development of modern agriculture. Pressures for that ecological uniformity on farmers and breeders is caused by the food industry demand for more and more raw materials consistency.\n\nIn the case of Animal Genetic Resources for Food and Agriculture, major causes of genetic erosion are reported to include indiscriminate cross-breeding, increased use of exotic breeds, weak policies and institutions in animal genetic resources management, neglect of certain breeds because of a lack of profitability or competitiveness, the intensification of production systems, the effects of diseases and disease management, loss of pastures or other elements of the production environment, and poor control of inbreeding.\n\nIn plant breeding, a population of plants is considered genetically vulnerable if there is little \ngenetic diversity within the population, and this lack of diversity makes the population as a whole particularly vulnerable to disease, pests, or other factors. The problem of genetic vulnerability often arises with modern crop varieties, which are uniform by design.\n\nAn example of the consequences of genetic vulnerability occurred in 1970 when corn blight struck the US corn belt, destroying 15% of the harvest. A particular plant cell characteristic known as Texas male sterile cytoplasm conferred vulnerability to the blight - a subsequent study by the National Academy of Sciences found that 90% of American maize plants carried this trait.\n\nSince 1961, human diets across the world have become more diverse in the consumption of major commodity staple crops, with a corollary decline in consumption of local or regionally important crops, and thus have become more homogeneous globally. The differences between the foods eaten in different countries were reduced by 68% between 1961 and 2009. The modern \"global standard\" diet contains an increasingly large percentage of a relatively small number of major staple commodity crops, which have increased substantially in the share of the total food energy (calories), protein, fat, and food weight that they provide to the world's human population, including wheat, rice, sugar, maize, soybean (by +284%), palm oil (by +173%), and sunflower (by +246%). Whereas nations used to consume greater proportions of locally or regionally important crops, wheat has become a staple in over 97% of countries, with the other global staples showing similar dominance worldwide. Other crops have declined sharply over the same period, including rye, yam, sweet potato (by -45%), cassava (by -38%), coconut, sorghum (by -52%) and millets (by -45%).\n\nAgricultural biodiversity is not only the result of human activity but human life is dependent on it not just for the immediate provision of food and other natural resources based goods, but for the maintenance of areas of land and waters that will sustain production and maintain agroecosystems and the wider biological and environmental services (biosphere).\n\nAgricultural Biodiversity provides:\n\n\nResearch supporting these findings addresses multifunctional agriculture in Europe, home gardens from around the world, smallholder farms in the tropics, among others.\n\nThe general trend noticed by the analysis of biodiversity present in different cropping systems (e.g., industrial agriculture and organic farming) was that a greater the diversity of crops (temporally and spacially) resulted in a greater overall biodiversity of the agroecosystem, though this is not always the case. A meta-analysis of studies comparing biodiversity noted that, when compared to organic cropping systems, conventional systems had significantly lower species richness and abundance (30% greater richness and 50% greater abundance in organic systems, on average), though 16% of studies did find a greater level of species richness in conventional systems. Another study found that cropping systems that required heavy use of chemical amendments (e.g., the widespread broadcasting of pesticides and glyphosate, a practice ubiquitously found throughout the United States and Canada) had significantly greater levels of pollination deficits, whereas organic fields of the same crop (Canola) witnessed no pollination deficits. Other cropping systems like permaculture have undergone little study to determine relative levels of biodiversity compared to other cropping systems, but because they continue to reinforce the goals of increasing overall crop biodiversity, it can be extrapolated that an even greater level of biodiversity would be observed.\n\nAgricultural biodiversity has spatial, temporal and scale dimensions especially at agroecosystem levels. These agroecosystems - ecosystems that are used for agriculture - are determined by three sets of factors: the genetic resources (biodiversity), the physical environment and the human management practices. There are not many ecosystems in the world that are \"natural\" in the sense of having escaped human influence. Most ecosystems have been to some extent modified or cultivated by human activity for the production of food and income and for livelihood security. However, most agricultural areas can be returned to their natural landscape after subsequent generations.\n\n\n\n\n\n"}
{"id": "46750828", "url": "https://en.wikipedia.org/wiki?curid=46750828", "title": "ApprenNet", "text": "ApprenNet\n\nApprenNet is a Philadelphia-based educational technology startup company founded in 2011 by Emily Foote and Drexel University School of Law Professor Karl Okamoto. The company provides apprenticeship-like job experiences online.\n\nAppreNet's first release was LawMeets, an online experience similar to a moot court competition. With LawMeets, students enact their response to a legal problem, and can not only then review their own performance, but also receive feedback, including critiques by experts. According to the \"Journal of the American Bar Association\", LawMeets \"quickly [became] a very big deal.\"\n\nApprenNet next added K12Meets, a program enabling teachers to practice their classroom techniques, and created a training program for employees at a Philadelphia restaurant.\n\nIn 2013 ApprenNet was one of five startup companies selected to participate in the University of Pennsylvania's Education Design Studio Inc. (EDSi), an innovation incubator dedicated to funding and launching education technology companies.\n\nIn 2012 the National Science Foundation awarded Okamoto a $500,000 grant to expand LawMeets' approach to learning in other disciplines. Okamoto told an interviewer that ApprenNet technology could be applied to many fields, not only training teachers and restaurant employees, but even musical training. \"We'll take care of law first,\" said Okamoto, \"and then use it in lots of different places. Why can't we crowdsource violin?\"\n\nBy 2015, after ApprenNet hired Columbia Business School graduate Rachel Jacobs as CEO, the start-up had received more than a million dollars in Small Business Innovation Research grants. Jacobs was hired to lead ApprenNet in an expansion from its original focus on educating lawyers to applying its online teaching technology in training health care professionals, college level instructors and K-12 teachers.\n\nFollowing Jacobs's death in May 2015, ApprenNet merged with Handsfree Learning of California.\n\nIn June 2016, ApprenNet changed its name to Practice and announced a $4 million Series A fundraising round. \n"}
{"id": "39045480", "url": "https://en.wikipedia.org/wiki?curid=39045480", "title": "Automate This", "text": "Automate This\n\nAutomate This: How Algorithms Came to Rule Our World is a book written by Christopher Steiner and published by Penguin Group. Steiner begins his study of algorithms on Wall Street in the 1980s but also provides examples from other industries. For example, he explains the history of Pandora Radio and the use of algorithms in music identification. He expresses concern that such use of algorithms may lead to the homogenization of music over time. Steiner also discusses the algorithms that eLoyalty (now owned by Mattersight Corporation following divestiture of the technology) was created by dissecting 2 million speech patterns and can now identify a caller's personality style and direct the caller with a compatible customer support representative.\n\nSteiner's book shares both the warning and the opportunity that algorithms bring to just about every industry in the world, and the pros and cons of the societal impact of automation (e.g. impact on employment).\n\n"}
{"id": "54311973", "url": "https://en.wikipedia.org/wiki?curid=54311973", "title": "Besi", "text": "Besi\n\nBE Semiconductor Industries N.V., simply called Besi, is a Dutch multinational company that designs and manufacturers semiconductor equipment. The company was founded in May, 1995 by Richard Blickman, who still leads the company today. The company employees 1500 people, of which 200 are at its headquarters in Duiven, a small town in the east of the Netherlands. It outsources production to its subsidiaries in China and Malaysia.\n\nBesi is a publicly traded company, and its shares are listed on the Euronext Amsterdam stock market under the BESI ticker symbol. In June, 2017, Besi was valued at around $2 billion.\n"}
{"id": "19540379", "url": "https://en.wikipedia.org/wiki?curid=19540379", "title": "Bilibo", "text": "Bilibo\n\nBilibo is a \"shell-shaped, hard-wearing piece of plastic\" used as a toy. It is produced in six different colors. It was developed in 2001 by Swiss designer Alex Hochstrasser in consultation with experts for child development.\n\nThe form has been designed for children of various ages to sit comfortably in the shell whilst controlling their movements by touching the ground with their hands and feet.\n\nAccording to a child's age and interests Bilibo is used as an accessory for role- and fantasy-play, it can be filled with objects, sand or water and emptied again or stacked. Rocking, spinning and balancing in or on the shell helps develop motor skills and the child's sense of balance.\nThe shells are also used in physiotherapy to stimulate the vestibular and proprioceptive systems.\n\nChildren on \"Richard & Judy\" could generally be seen to prefer cardboard boxes to the Bilibo, and confirmed this preference when asked.\n\nBilibo has received several international awards, including Spiel Gut in Germany, the Swiss Product Design Award in 2002 and Toy of the Year – UK Good Toy Awards in 2006.\n\n"}
{"id": "146382", "url": "https://en.wikipedia.org/wiki?curid=146382", "title": "Bomb disposal", "text": "Bomb disposal\n\nBomb disposal is the process by which hazardous explosive devices are rendered safe. \"Bomb disposal\" is an all-encompassing term to describe the separate, but interrelated functions in the military fields of explosive ordnance disposal (EOD) and improvised explosive device disposal (IEDD), and the public safety roles of public safety bomb disposal (PSBD) and the bomb squad.\n\n\"Bomb disposal\" does not encompass the remediation of soils polluted with explosive materials.\n\nThe first professional civilian bomb squad was established by Sir Vivian Dering Majendie. As a Major in the Royal Artillery, Majendie investigated an explosion on 2 October 1874 in the Regent's Canal, when the barge 'Tilbury', carrying six barrels of petroleum and five tons of gunpowder, blew up, killing the crew and destroying Macclesfield Bridge and cages at nearby London Zoo.\n\nIn 1875, he framed The Explosives Act, the first modern legislation for explosives control. He also pioneered many bomb disposal techniques, including remote methods for the handling and dismantling of explosives. His advice during the Fenian dynamite campaign of 1881–85 was officially recognised as having contributed to the saving of lives. After Victoria Station was bombed on 26 February 1884 he defused a bomb with a clockwork mechanism which might have gone off at any moment.\n\nThe New York City Police Department established its first bomb squad in 1903. Known as the \"Italian Squad\", its primary mission was to deal with dynamite bombs used by the Mafia to intimidate immigrant Italian merchants and residents. It would later be known as the \"Anarchist Squad\" and the \"Radical Squad\".\n\nBomb Disposal became a formalized practice in the First World War. The swift mass production of munitions led to many manufacturing defects, and a large proportion of shells fired by both sides were found to be \"duds\". These were hazardous to attacker and defender alike. In response, the British dedicated a section of Ordnance Examiners from the Royal Army Ordnance Corps to handle the growing problem.\n\nIn 1918, the Germans developed delayed-action fuzes that would later develop into more sophisticated versions during the 1930s, as Nazi Germany began its secret course of arms development. These tests led to the development of UXBs (unexploded bombs), pioneered by Herbert Ruehlemann of Rheinmetall, and first employed during the Spanish Civil War of 1936–37. Such delayed-action bombs provoked terror in the civilian population because of the uncertainty of time, and also complicated the task of disarming them. The Germans saw that unexploded bombs caused far more chaos and disruption than bombs that exploded immediately. This caused them to increase their usage of delayed-action bombs in World War II.\n\nInitially there were no specialized tools, training, or core knowledge available, and as Ammunition Technicians learned how to safely neutralize one variant of munition, the enemy would add or change parts to make neutralization efforts more hazardous. This trend of cat-and-mouse extends even to the present day, and the various techniques used to disarm munitions are not publicized.\n\nModern EOD Technicians across the world can trace their heritage to the Blitz, when the United Kingdom's cities were subjected to extensive bombing raids by Nazi Germany. In addition to conventional air raids, unexploded bombs (UXBs) took their toll on population and morale, paralyzing vital services and communications. Bombs fitted with delayed-action fuzes provoked fear and uncertainty in the civilian population.\n\nThe first UXBs were encountered in the autumn of 1939 before the Blitz and were for the most part easily dealt with, mostly by Royal Air Force or Air Raid Precautions personnel. In the spring of 1940, when the Phony War ended, the British realized that they were going to need professionals in numbers to deal with the coming problem. 25 sections were authorized for the Royal Engineers in May 1940, another 109 in June, and 220 by August. Organization was needed, and as the Blitz began, 25 \"Bomb Disposal Companies\" were created between August 1940 and January 1941. Each company had ten sections, each section having a bomb disposal officer and 14 other ranks to assist. Six companies were deployed in London by January 1941.\n\nThe problem of UXBs was further complicated when Royal Engineer bomb disposal personnel began to encounter munitions fitted with anti-handling devices e.g. the Luftwaffe's ZUS40 anti-removal bomb fuze of 1940. Bomb fuzes incorporating anti-handling devices were specifically designed to kill bomb disposal personnel. Scientists and technical staff responded by devising methods and equipment to render them safe, including the work of Eric Moxey.\n\nThe United States War Department felt the British Bomb Disposal experience could be a valuable asset, based on reports from U.S. Army, Navy, and Marine Corps observers at RAF Melksham in Wiltshire, England in 1940. The next year, the Office of Civilian Defense (OCD) and War Department both sponsored a bomb disposal program. After the attack on Pearl Harbor, the British sent instructors to Aberdeen Proving Ground, where the U.S. Army would inaugurate a formal bomb disposal school under the Ordnance Corps. Col. Thomas J. Kane became the U.S. Army Ordnance Bomb Disposal School commandant, and later served as ETO Director of Bomb Disposal under Dwight D. Eisenhower. In May 1941, British colleagues helped establish the Naval Mine Disposal School at the Naval Gun Factory, Washington, D.C. Not to be outdone, the U.S. Navy, under the command of Lt. Draper L. Kauffman (who would go on to found the Underwater Demolition Teams better known as UDTs or the U.S. Navy Frogmen), created the Naval Bomb Disposal School at University Campus, Washington, D.C..\n\nThe first US Army Bomb Disposal companies were deployed in North Africa and Sicily, but proved cumbersome and were replaced with mobile seven-man squads in 1943. Wartime errors were rectified in 1947 when Army personnel started attending a new school at Indian Head, Maryland, under U.S. Navy direction. That same year, the forerunner of the EOD Technology Center, the USN Bureau of Naval Weapons, charged with research, development, test, and evaluation of EOD tools, tactics and procedures was born.\n\nThe Ammunition Technicians of the Royal Logistic Corps (formerly RAOC) became highly experienced in bomb disposal, after many years of dealing with bombs planted by the Provisional Irish Republican Army (PIRA) and other groups. The bombs employed by the PIRA ranged from simple pipe bombs to sophisticated victim-triggered devices and infrared switches. The roadside bomb was in use by PIRA from the early 1970s onwards, evolving over time with different types of explosives and triggers. Improvised mortars were also developed by the IRA, usually placed in static vehicles, with self-destruct mechanisms. During the 38-year campaign in Northern Ireland, 23 British ATO bomb disposal specialists were killed in action.\n\nA specialist Army unit, 321 EOD Unit (later 321 EOD Company, and now part of 11 Explosive Ordnance Disposal Regiment RLC), was deployed to tackle increased IRA violence and willingness to use bombs against both economic and military targets. The unit's radio call-sign was Felix. Many believe this to be an allusion to the cat with nine lives and led to the phrase \"Fetch Felix\" whenever a suspect device was encountered, which later became the title of the 1981 book \"Fetch Felix\". However, the real reason could be either of two possibilities. All units in Northern Ireland had a callsign to be used over the radios. 321 Company, a newly formed unit, didn't have such a callsign, so a young signaller was sent to the OC of 321 Coy. The OC, having lost two technicians that morning, decided on \"Phoenix\". This was misheard as \"Felix\" by the signaller and was never changed. The other possible reason is that the callsign for RAOC was \"Rickshaw\"; however, the 321 EOD felt it needed its own callsign, hence the deliberate choice of \"Felix the Cat with nine lives\". 321 Coy RAOC (now 321 EOD Sqn RLC) is unique in that it is the most decorated unit (in peace time) in the British Army with over 200 gallantry awards, notably for acts of great bravery during Operation Banner (1969–2007) in Northern Ireland.\n\nBritish Ammunition Technicians of 11 EOD Regiment RLC were requested by the US Forces commanders to operate in support of the US Marine Corps in clearing the Iraqi oilfields of booby traps and were among the first British service personnel sent into Iraq in 2003 prior to the actual ground invasion.\n\nThe eruption of low intensity conflicts and terrorism waves at the beginning of the 21st century caused further development in the techniques and methods of bomb disposal. EOD Operators and Technicians had to adapt to rapidly evolving methods of constructing improvised explosive devices ranging from shrapnel-filled explosive belts to 100 kg bombs. Since improvised explosives are generally unreliable and very unstable they pose great risk to the public and especially to the EOD Operator trying to render them safe. Therefore, new methods like greater reliance on remote techniques such as advanced remotely operated vehicles similar to the British Wheelbarrow or armored bulldozers evolved. Many nations have developed their own versions such as the D7 MCAP and the armored D9R.\nThe British Armed Forces have become experts in bomb disposal after many years of dealing with bombs planted by the IRA. These came in many different forms, particularly car bombs rigged to detonate via a variety of manners including command wire and remote trigger. Some of the first personnel sent into Iraq in 2003 were British bomb disposal experts of 11 EOD Regiment RLC. Besides large mine-clearing vehicles such as the Trojan, the British Army also uses small remote controlled vehicles such as Dragon Runner and Chevette.\n\nDuring the al-Aqsa Intifada, Israeli EOD forces have disarmed and detonated thousands of explosive charges, lab bombs and explosive ammunition (such as rockets). Two Israeli EOD teams gained high reputation for leading the efforts in that area: the Army's Israeli Engineering Corps' Sayeret Yahalom and the Israeli Border Guard Gaza-area EOD team.\n\nIn the Iraq War, the International coalition multinational force in Iraq forces have faced many bombs on travel routes. Such charges can easily destroy light vehicles such as the Humvee, and large ones can even destroy main battle tanks. Such charges caused many casualties and along with car bombs and suicide bombers were a major cause of casualties in Iraq.\n\nIn Spain's autonomous Basque Country, where bombings by Basque separatist groups were common during the 1980s and 1990s, there are three corps in charge of bomb disposal: Policia Nacional, Guardia Civil, and Ertzaintza. The Ertzaintza handle general civilian threats, while the Policia Nacional and Guardia Civil maintain capabilities mainly to defend their own assets and personnel. In other parts of the country, the Guardia Civil and Policia Nacional develop their tasks within their own abilities, with the exception of Mossos d'Esquadra in Catalonia (where the situation is the same as in the Basque Country).\n\nIn the United Kingdom, EOD Operators are held within all three Services. Each Service has differing responsibilities for UXO, however they will often work closely on operations.\n\nAmmunition Technical Officers and Ammunition Technicians of the Royal Logistic Corps deal with many aspects of EOD, including conventional munitions and homemade bombs. They are also trained in chemical, biological, incendiary, radiological (\"dirty bombs\"), and nuclear weapons. They provide support to VIPs, help civilian authorities with bomb problems, teach personnel from all three services about bomb safety, and a variety of other tasks.\n\nThe Royal Engineers of 33 Engineer Regiment (EOD) provide EOD expertise for air dropped munitions in peace time and conventional munitions on operations, as well as battle area clearance and High Risk Search in support of improvised explosive device disposal.\n\nRoyal Engineers providing search advice and assets and Ammunition Technicians and Ammunition Technical Officers of 11 EOD Regiment RLC Royal Logistic Corps providing Improvised Explosive Device Disposal (IEDD), Conventional Munitions Disposal (CMD) and Biological, Chemical Munitions Disposal (BCMD). They also provide expertise in Advanced IEDD and in the investigation of accidents and incidents involving ammunition and explosives, where they are seen as Subject Matter Experts (SMEs).\n\nWeapons Intelligence is supplied by Royal Military Police, Intelligence Corps and Ammunition Technical Personnel who tap into the CEXC units of the USA.\n\nAll prospective Ammunition Technicians attend a gruelling course of instruction at The Army School of Ammunition and the Felix Centre, United Kingdom. The time frame for an RLC Ammunition Technician to complete all necessary courses prior to finally being placed on an EOD team is around 36 months. Whereas the Engineer EOD training period although shorter in total is spread over a number of years and interspersed with operational experience, RE personnel may be posted to core trades such as carpentry or bridge building within their time as engineers.\n\nRoyal Air Force armourers from 5131 (BD) Squadron and Royal Navy clearance divers also deploy teams both in the UK and on operations working on both IEDD (Improvised Explosive Device Disposal) teams as well as the disposal of conventional munitions. Both the RAF and Royal Navy personnel spend their entire service working with and around explosives, and associated sciences. As such are given responsibilities relevant to their roles when it comes to conventional weapons;\n\nUS EOD covers both on and off base calls in the US unless there is a local PSBT or \"Public Safety Bomb Technician\" that can handle the bomb - ordnance should only be handled by the EOD experts. Also called a \"Hazardous Devices Technician\", PSBTs are usually members of a Police department, although there are teams formed by fire departments or emergency management agencies.\n\nTo be certified, PSBTs must attend the joint U.S. Army-FBI Hazardous Devices School at Redstone Arsenal, Alabama which is modeled on the International IEDD Training school at The Army School of Ammunition, known as the Felix Centre. This school helps them to become knowledgeable in the detection, diagnosis and disposal of hazardous devices. They are further trained to collect evidence in hazardous devices, and present expert witness testimony in court on bombing cases.\n\nBefore bombing ranges can be re utilized for other purposes, these ranges must be cleared of all unexploded ordnance. This is usually performed by civilian specialists trained in the field, often with prior military service in explosive ordnance disposal. These technicians use specialized tools for subsurface examination of the sites. When munitions are found, they safely neutralize them and remove them from the site.\n\nIn addition to neutralizing munitions or bombs, conducting training and presenting evidence, EOD Technicians and Engineers also respond to other problems.\n\nEOD Technicians help dispose old or unstable explosives, such as ones used in quarrying, mining, or old/unstable fireworks and ammunition. They also assist specialist police units, raid and entry teams with boobytrap detection and avoidance, and they help in conducting post-blast investigations.\n\nThe EOD technician's training and experience with bombs make them an integral part of any bombing investigation. Another part of an EOD technician's job involves supporting the government intelligence units. This involves searching all places that the high ranking government officers or other protected dignitaries travel, stay or visit.\n\nGenerally, EOD \"render safe procedures\" (RSP) are a type of tradecraft protected from public dissemination in order to limit access and knowledge, depriving the enemy of specific technical procedures used to render safe ordnance or an improvised device. Another reason for keeping tradecraft secret is to hinder the development of new anti-handling devices by their opponents: if the enemy has thorough knowledge of specific EOD techniques, they can develop fuze designs which are more resistant to existing render-safe procedures.\n\nMany techniques exist for the making safe of a bomb or munition. Selection of a technique depends on several variables. The greatest variable is the proximity of the munition or device to people or critical facilities. Explosives in remote localities are handled very differently from those in densely populated areas. Contrary to the image portrayed in modern-day movies, the role of the modern Bomb Disposal Operator is to accomplish their task as remotely as possible. Actually laying hands on a bomb is only done in an extremely life-threatening situation, where the hazards to people and critical structures cannot be reduced.\n\nAmmunition technicians have many tools for remote operations, one of which is the RCV, or remotely controlled vehicle, also known as the \"Wheelbarrow\". Outfitted with cameras, microphones, and sensors for chemical, biological, or nuclear agents, the Wheelbarrow can help the technician get an excellent idea of what the munition or device is. Many of these robots even have hand-like manipulators in case a door needs to be opened, or a munition or bomb requires handling or moving. The first ever Wheelbarrow was conceived by Major Robert John Wilson 'Pat' Patterson RAOC and his team at the Bomb Disposal School, CAD Kineton in 1972 and used by Ammunition technicians in the battle against Provisional Irish Republican Army bombs.\nAlso of great use are items that allow ammunition technicians to remotely diagnose the innards of a munition or bomb. These include devices similar to the X-ray used by medical personnel, and high-performance sensors that can detect and help interpret sounds, odors, or even images from within the munition or bomb. Once the technicians determine what the munition or device is, and what state it is in, they will formulate a procedure to disarm it. This may include things as simple as replacing safety features, or as difficult as using high-powered explosive-actuated devices to shear, jam, bind, or remove parts of the item's firing train. Preferably, this will be accomplished remotely, but there are still circumstances when a robot won't do, and a technician must put themself at risk by personally going near the bomb. The technician will don a specialized protective suit, using flame and fragmentation-resistant material similar to bulletproof vests. Some suits have advanced features such as internal cooling, amplified hearing, and communications back to the control area. This suit is designed to increase the odds of survival for the technician should the munition or bomb function while they are near it.\n\nRarely, the specifics of a munition or bomb will allow the technician to first remove it from the area. In these cases, a containment vessel is used. Some are shaped like small water tanks, others like large spheres. Using remote methods, the technician places the item in the container and retires to an uninhabited area to complete the neutralization. Because of the instability and complexity of modern bombs, this is rarely done. After the munition or bomb has been rendered safe, the technicians will assist in the removal of the remaining parts so the area can be returned to normal. All of this, called a Render Safe Procedure, can take a great deal of time. Because of the construction of devices, a waiting period must be taken to ensure that whatever render-safe method was used worked as intended.\n\nAnother technique is Trepanation, in which a bore is cut into the sidewall of a bomb and the explosive contents are extracted through a combination of steam and acid bath liquification of bomb contents.\n\nAlthough professional EOD personnel have expert knowledge, skills and equipment, they are not immune to misfortune because of the inherent dangers: in June 2010, construction workers in Göttingen discovered an Allied 500 kilogram bomb dating from World War II buried approximately 7 metres below the ground. German EOD experts were notified and attended the scene. Whilst residents living nearby were being evacuated and the EOD personnel were preparing to disarm the bomb, it detonated, killing three of them and injuring 6 others. The dead and injured each had over 20 years of hands-on experience, and had previously rendered safe between 600 and 700 unexploded bombs. The bomb which killed and injured the EOD personnel was of a particularly dangerous type because it was fitted with a delayed-action chemical fuze, which had become highly unstable after over 65 years under ground.\n\nPortable X-ray systems are used to radiograph the bomb before intervention. The purpose is for example to determine if a chemical charge is present or to check the status of the detonator. High steel thickness require high energy and high power sources.\n\nProjected water disruptors use a water-projectile shaped charge to destroy bombs, blasting the device apart and severing any detonating connections faster than any fuse or anti-tampering device on the bomb can react. One example is the BootBanger, deployed under the rear compartment of cars suspected to be carrying bombs. Projected water disruptors can be directional, such as the BootBanger; or omni-directional, an example being the Bottler.\n\nPigstick is the British Army term for the waterjet disruptor commonly deployed on the Wheelbarrow remotely operated vehicle against IRA bombs in the 1970s. It fires a jet of water driven by a propellent charge to disrupt the circuitry of a bomb and disabling it with a low risk of detonation. The modern pigstick is reliable and can be fired many times with minimal maintenance. It is now used worldwide. It is 485 mm long and weighs 2.95 kg. It is made of hardened steel, and can be mounted on a remotely operated vehicle (ROV). These factors make it an effective way to render IEDs safe. It is not a panacea however, it can not deal with IEDs packed in hard containers like industrial gas bottles or beer kegs, for instance, and other disruptors have been designed to deal with those and a range of other situations including car bombs.\n\nThe name \"pigstick\" is an odd analogy coming from the verb meaning \"to hunt the wild boar on horseback with a spear.\"\n\nThe device was developed by the scientists Mike Barker MBE and Peter Hubbard OBE at RARDE Fort Halstead in late 1971 working under great pressure over a period of several weeks after an ATO lost his life in Northern Ireland attempting to render safe the first IED in the theatre to contain anti-handling devices.\n\nThey started with a prototype equipment designed to disrupt limpet mines attached to a ship's hull and through a process of many trials and error developed a disruptor that could deal with the crop of IEDs with anti-handling devices prevalent at the time. Barker used the device operationally for the first time in Northern Ireland during a visit there to demonstrate their prototype to George Styles and his team. The Pigstick prototype was re-engineered by a member of Hubbard's team, Bob White MBE, down from its original 20 kg to its current 2.95 kg form but its internal ballistic design remained true to the original.\n\nThe ZEUS-HLONS (HMMWV Laser Ordnance Neutralization System), commonly known as ZEUS, was developed for surface land mines and unexploded ordnance (UXO) neutralization by the U.S. Naval Explosive Ordnance Disposal Technology Division (NAVEODTECHDIV). It uses a moderate-power commercial solid state laser (SSL) and beam control system, integrated onto a Humvee (HMMWV), to clear surface mines, improvised bombs, or unexploded ordnance (UXO) from supply routes and minefields.\n\nThere are a wide range of containment chambers available. The simplest are sometimes danger suppression vessels that merely contain some of the fragments generated by the explosion. The other end of the spectrum features top-of-the-line gas-tight chambers that can withstand multiple shots while remaining able to contain chemical, biological, or radioactive agents. Containment chambers of all types may be fitted onto towed trailers, or specialised EOD vehicles.\n\nThere is a long history of IEDD within the UK and protection for this role has evolved over the years. Starting with the Mk1 in 1969, in response to the Maoist Terrorist threat in Hong Kong, through to the Mk 2 in 1974, in response to the IRA threat in Northern Ireland (NI), with further developments of the Mk3 in 1980 to include a new helmet. The Mk4 EOD Suit, introduced into service in 1993, combines fragmentation and blast protection that is prioritised over the most vulnerable parts of the body (head, face and torso). The current system, MKV/VI, was introduced in 2004, and was a combined MOD/NP Aerospace project. The only part of the body that has no protection at all is the hands.\n\n\n\n\n"}
{"id": "15819918", "url": "https://en.wikipedia.org/wiki?curid=15819918", "title": "Chris Andrews (entrepreneur)", "text": "Chris Andrews (entrepreneur)\n\nChris Andrews has been an important figure in the evolution of today's Information Society. He is a pioneer in digital media, electronic publishing, and the Internet. He was the world's first CD-ROM producer, launched the first CD-Recordable system which began the \"user generated content\" revolution, and has developed new technologies in other areas including live webcasting, use of audio and video on the internet, and intellectual property.\n\nChris is the author of the twice-published book \"The Education of a CD-ROM Publisher - An Insiders History of Electronic Publishing.\" \n\nChris' life story has been featured in the media including profiles on CBS' \"60 Minutes\" and other international media. In 2001, he began to pursue the restitution of a building in Vienna, Austria that was taken from his family by the Nazis in World War II. This became a life-changing experience for him, making him an activist in particular in World War II restitution. (See Resources)\n\nChris also worked at Hewlett-Packard, NewsBank, Meridian Data, and the National Academy of Recording Arts & Sciences. He launched several companies including the webcast software company Livecast, the multimedia publishing company UniDisc, and VentureMakers LLC - an intellectual property development company.\n\n"}
{"id": "54543869", "url": "https://en.wikipedia.org/wiki?curid=54543869", "title": "CodeSignal", "text": "CodeSignal\n\nCodeSignal (formerly CodeFights) is a skills-based assessment platform operated by American company BrainFights, Inc., whose mission is to discover, develop and promote technical talent. Founded in 2014 and headquartered in San Francisco, CodeSignal applies game mechanics that offer developers of all skill levels online computer programming challenges for both instructional and recruiting purposes.\n\nAs of August 2017, CodeSignal has reported that it had nearly 1 million developers using CodeSignal for Developers.\n\nAs of 10th July 2018, CodeFights has been renamed to CodeSignal with additional features as per the company blog.\n\nCodeSignal was founded in 2014 by Tigran Sloyan, Aram Shatakhtsyan, and Felix Desroches. The idea for CodeSignal was developed from Sloyan and Shatakhtsyan’s experiences of participating in international coding and mathematics competitions including the International Olympiad in Informatics and Mathematics. Sloyan even used this concept of a game-based teaching platform for his Computer Science master’s thesis at MIT, but abandoned the idea to work in established Silicon Valley companies including Oracle, Google, and Premise (backed by Google Ventures).\n\nCodeSignal first launched with challenges only for JavaScript and were based on “code battles”, a 3-minute person versus person competition to determine who can more quickly and accurately debug existing code. Before each game, players can either choose to race against the clock, or let CodeSignal automatically match them with other online players to complete the challenge head-to-head. Successful challengers level up and earn badges towards language fluency. Even though players are first presented with short challenges that take only a few minutes to solve, each challenge increases in difficulty and the amount of time given to solve the problem.\n\nSoon, CodeSignal expanded to support Java, C++ and Python challenges. Today, CodeSignal supports 38 programming languages and has also expanded to include more types of challenges, interview practice, and access to companies with open software engineering positions.\n\nEven though CodeSignal launched as a platform to help users learn and improve their coding skills, CodeSignal has also taken on changing recruiting all together. CodeFight’s main goal is to help programmers get hired based on their skills, not their resume.\n\nCodeSignal originally launched as an online community where developers can practice their skills through a series of Head-to-Head coding challenges, which appealed to competitive programmers. The main goal was to help developers build their coding skills by solving and discussing programming challenges with other developers on the platform.\n\nIn the first 2 years, the CodeSignal platform had 6 distinct game modes: Interview Practice, Company Bots, Arcade, Tournaments, Head-to-Head, and Challenges; all geared towards helping developers build skills and getting prepared for technical real-world jobs. In all of these modes, the CodeSignal system runs a user’s solution to a coding challenge against tests, and the solution is only accepted when all test cases are satisfied. All CodeFight modes for developers are free of charge.\n\nAs of August 2017, CodeSignal has reported that it had nearly 1 million developers using CodeSignal for Developers.\n\nInterview Practice is the newest and most popular mode on CodeSignal for Developers. This mode first launched in beta in February 2017, and then launched the expanded the version in June 2017. The Interview Practice game mode is specifically targeted towards job seekers who are preparing for engineering technical interviews. Developers can use Interview Practice to solve real interview questions, master key computer science topics, and learn by reviewing solutions provided within the community.\n\nLaunched in November 2015, Company Bots are curated challenges that simulate real-world problems that companies are facing. This type of assessment is based on the premise that solving on-the-job coding challenges can allow companies to better assess the skills of a potential candidate.\n\nDuring a Company Bot challenge, participants are faced with multiple rounds of challenges of varying difficulty. Both the bot and the challenger are attempting to solve the coding challenge side by side. Each participant gains points based on speed and accuracy. After the participant submits their solution, it is evaluated and only accepted if it passes all the tests. Only after winning the Bot challenge, can the participant be provided with the opportunity to submit their information to the recruiter of the company running the Company Bot challenge.\n\nCodeSignal launched Company Bots with a partnership with Uber to create Uberbot, a Uber-branded gaming challenge on CodeSignal that would help Uber find and evaluate the programming skills of candidates. Candidates who attempted the Uberbot coding game are challenged to solve real-world problems facing Uber’s engineering team, such as finding the most optimal route for a Uber ride, or the most efficient method of matching riders for an uberPOOL.\n\nCodeSignal now has 15 company bots including bots from Asana, Dropbox, Quora, Instacart, SpaceX, Thumbtack, and others.\n\nIn addition to being a learning tool for developers, CodeSignal Recruiter (previously known as CodeSignalR) is also a skills-based recruiting platform that uses a data-driven approach to help companies improve their hiring process, find better qualified candidates, and make more objective hiring decisions.\n\nLaunched in October 2017, CodeSignal Recruiter is the sourcing, testing, and interview platform for technical recruiters. Companies with a CodeSignal Recruiter account can contact developers within the CodeSignal community who have done well on challenges and have signalled that they are open to new jobs. Once contacted, recruiters can use CodeSignal to send out custom programming tests to candidates that include plagiarism checks, live recordings, timed assessments, all within a developer-focused IDE (integrated development environment).\n\nCodeSignal Recruiter is also integrated with Applicant Tracking System (ATS) software including Greenhouse, Lever and SmartRecruiters, which allows recruiters to manage and sync candidate data between CodeSignal and their recruiting platform including send coding tests, evaluating results and managing candidate lifecycles.\n\nSome of CodeFight’s Recruiter customers include Evernote, Uber, Thumbtack, Dropbox, Asana, Ascend, Wizeline and Quora.\n\nCodeSignal launched their rating system, called Coding Score, on July 10, 2018. The Coding Score is a measure of a developer's overall implementation and problem-solving ability. It is a predictor of how well a developer will perform in technical interviews. To get a initial Coding Score on CodeSignal, each developer has to solve at least 3 tasks on CodeSignal. To get a more accurate score, developers are asked to vary the difficulty of the tasks that they solve.\n\nCodeSignal Recruiter also has a testing feature that allows recruiters to send out technical assessments to their potential candidates. These tests can be customized to simulate real-world challenges that the candidate may face on the job, which has been reported to be a better measurement for competency than regular interview questions or theoretical programming tests. The testing suite also has a built in plagiarism checker that predicts the probability of plagiarism by comparing to other completed tests on the CodeSignal platform and solutions from known sites. During the on-boarding process CodeSignal works with customers to create custom tests so the online tests are calibrated with the interview process and job responsibilities.\n\nThe CodeSignal Recruiter Interview feature allows recruiters to conduct online and in-person interviews in a shared coding environment that supports 38 programming languages. The interview environment allows for timed assessments, live recordings, and an extensive library of skills-based coding tasks. During each live interview, the hiring manager can watch and conduct a coding skills assessment using pre-defined coding tasks while also talking to the candidate over a live video stream. Each live interview is also recorded so that other hiring managers in the recruitment process can review and share the candidates live assessment.\n\nAs of July 2017, CodeSignal has raised a total of $12.5 million in 2 rounds from 23 investors. The company raised an initial $2.5 million in seed funding in April 2015, which included investments by Felicis Ventures (Aydin Senkut), Sutter Hill Ventures (Mike Speiser), LiveRamp CEO Auren Hoffman, Google Shopping Express founder Tom Fallows, Twitter VP of Engineering Raffi Krikorian, Quora CEO Adam D'Angelo and GoDaddy VP of Engineering Marek Olszewski. CodeSignal raised $10 million in November 2016. The Series A funding round was led by e.ventures. Other investors in that round included SV Angel, A Capital, Granatus Ventures, and Felicis Ventures.\n\nWithin the first 6 months of its launch, CodeSignal featured over 1,500 challenges, which attracted over 70,000 users who solve over 1.5 million challenges. From there, CodeSignal was reported to grow by 30-40% month-over-month.\n\nAs of August 2017, CodeSignal has reported that it had nearly 1 million developers using CodeSignal for Developers.\n\nCodeSignal supports 38 different coding languages on its platform. However, not all tasks on the site can be solved using every language, based on the challenge type.\n\nCodeSignal customers include Evernote, Uber, Thumbtack, Dropbox, Asana, Wizeline and Quora.\n\n\n"}
{"id": "30281534", "url": "https://en.wikipedia.org/wiki?curid=30281534", "title": "Cyberstalking legislation", "text": "Cyberstalking legislation\n\nCyberstalking and cyberbullying are relatively new phenomena, but that does not mean that crimes committed through the network are not punishable under legislation drafted for that purpose. Although there are often existing laws that prohibit stalking or harassment in a general sense, legislators sometimes believe that such laws are inadequate or do not go far enough, and thus bring forward new legislation to address this perceived shortcoming. In the United States, for example, nearly every state has laws that address cyberstalking, cyberbullying, or both.\n\nCyberbullying and cyberstalking, by their nature, define adversarial relationships. One person (or group), the provocateur, is exerting a view or opinion that the other person (or group), the target, finds offensive, hurtful, or damaging in some way. In a general sense, it would seem simple to legislate this type of behavior; slander and libel laws exist to tackle these situations. However, just as with slander and libel, it is important to balance the protection of freedom of speech of both parties with the need for protection of the target. Thus, something that may be deemed cyberbullying at first glance may, in fact, be more akin to something like parody or similar.\n\nA 2006 National Crime Prevention Council survey found that some 40% of teens had experienced cyberbullying at some point in their lives, making the problem particularly widespread. Not only is the issue of cyberbullying extensive, it has adverse effects on adolescents: increased depression, suicidal behavior, anxiety, and increased susceptibility of drug use and aggressive behavior.\n\nAustralia does not have specific cyberbullying legislation, although the scope of existing laws can be extended to deal with cyberbullying.\n\nState laws can deal with some forms of cyberbullying, such as documents containing threats, and threats to destroy and damage property.\n\nCommonwealth offences that criminalise the misuse of telecommunication services are also relevant when technology is used to communicate harassment or threats.\n\nThe Family Law Act 1975 (Cth) protects individuals from harassment, including harassment that occurs via electronic communications. However, this is limited to the victims of family violence.\n\nThe Australian government has proposed specific cyberbullying laws to protect children.\n\nIn the US, in practice, there is little legislative difference between the concepts of \"cyberbullying\" and \"cyberstalking.\" The primary distinction is one of age; if adults are involved, the act is usually termed \"cyberstalking,\" while among children it is usually referred to as \"cyberbullying.\" However, this distinction is one of semantics, and many laws treat \"bullying\" and \"stalking\" as much the same issue.\n\nFirst Amendment concerns often arise when questionable speech is uttered or posted online. This is equally true when dealing with cyberbullying. Particularly in instances where there are no laws explicitly against cyberbullying, it is not uncommon for defendants to argue that their conduct amounts to an exercise of their freedom of speech.\n\nThe courts have variously come down on either side of that debate, even within the same state. For example, a student in California who was suspended from school based on cyberbullying claims took the school district to court, citing a breach of her First Amendment rights; the court agreed with the student and found the school district had overstepped its authority. In another California case, in which a student was harassed after posting personal information online, the court found that threatening posts were \"not\" protected speech.\n\nThat said, \"true\" threats are not considered to be protected speech.\n\nOrganizations such as the American Civil Liberties Union have taken the view that cyberbullying is an overly expansive term, and that the First Amendment protects all speech, even the reprehensible; this protection would extend to the Internet.\n\nIn general, such organizations argue that while the need for legislation against cyberbullying may exist, legislators must take a cautious, reasoned approach to enacting laws, and not rush into creating laws that would curtail speech too much.\n\nInternet free speech issues have certainly made their way through the court systems, even as far back as cases from the mid-90s. In the case of United States v. Baker, for example, an undergraduate at the University of Michigan was charged with crimes related to snuff stories he had posted on Internet newsgroups, stories that named one of his fellow students. After progressing through the courts, the charges against Baker were dismissed primarily on grounds that there was no evidence that Baker would actually act out the fantasies contained in those stories. This case is now considered a landmark in the realm of First Amendment issues on the Internet.\n\nThe focus on legislating cyberbullying and cyberstalking has largely come about as a result of the perceived inadequacy, generally by legislators and parents of bullying victims, of existing laws, whether those existing laws cover stalking, unauthorized use of computer resources, or the like.\nThe motivation behind the bill in 1990 where 50 U.S. states and the federal government passed a bill to \"criminalize\" stalking was due to the cases of stalking against celebrities (Spitzberg & Hoobler, 2002).\n\nFor example, in the case of \"United States v. Lori Drew\", in which Megan Meier had committed suicide after being bullied on MySpace, three of the four charges against the defendant (Drew) were actually in response to alleged violations of the Computer Fraud and Abuse Act, since specific statues against cyberbullying were not on the books. The jury eventually found Drew innocent of the charges (but guilty of a misdemeanor), a verdict that was later set aside by the judge. In this situation, legislators in Missouri, at the urging of the public and Meier's parents, passed \"Megan's Law\", primarily aimed at the crime of a person over 21 years of age bullying a person under 18 years of age.\n\nIn addition, prosecutors will sometimes use other legal avenues to prosecute offenders. In the case of Tyler Clementi, who killed himself after video of his homosexual encounter was broadcast on the Internet, prosecutors charged the defendants with invasion of privacy and computer crimes. Like the Meier case, the Clementi case spurred legislators (this time, in New Jersey) to pass a law specifically aimed at bullying, an \"Anti-bullying Bill of Rights\".\n\nWhile some laws are written such that the focus on cyberbullying is the set of acts that occur within a school, others are more general, targeting cyberbullying no matter where it occurs. In addition, some of these newly written laws (like one in Connecticut) put more of an onus on the school system, mandating that the school's administration must intervene at the first sign of bullying.\n\nFinally, it's not uncommon for cyberbullying to be coupled with \"traditional\", in-person bullying, for example, in the suicide of Phoebe Prince. Students at her school had bullied her for months in school, and that harassment eventually moved online as well. As in Connecticut, New Jersey, and Missouri, the Prince case led to stricter anti-bullying legislation in Massachusetts.\n\nSome U.S. states have begun to address the problem of cyberbullying. States that have passed legislation have done so generally in response to incidents within that state, to address what they believe to be shortcomings in federal laws, or to expand protection to victims above and beyond existing statutes.\n\nThere are laws that only address online harassment of children or focus on child predators as well as laws that protect adult cyberstalking victims, or victims of any age. While some sites specialize in laws that protect victims age 18 and under, Working to Halt Online Abuse is a help resource listing current and pending cyberstalking-related United States federal and state laws. It also lists those states that do not have laws yet and related laws from other countries.\n\nCalifornia passed the first cyberstalking law in 1999. (§646.9 of the California Penal Code.) Its first use resulted in a six-year sentence for a man who harassed a woman who could identify him. After sending hundreds of threatening e-mails to an actress, another male convicted after spending months in jail waiting for trial was sentenced in 2001 to five years probation, forbidden access to computers and forced to attend mental health counseling. In 2011, a man was ordered to undergo psychiatric evaluation before sentencing for cyberstalking. On January 1, 2009, a California law became effective that allows schools to suspend or expel students who harass other students online. It also mandates that schools develop policies to address the problem. In addition, Section 1708.7 of the California Civil Code outlines grounds for an individual suing their cyberstalker and any accomplices for general damages, special damages, and punitive damages for cyberstalking.\n\nUnder Florida Statute 784.048, \"cyberstalking,\" defined as \"to engage in a course of conduct to communicate, or to cause to be communicated, words, images, or language by or through the use of electronic mail or electronic communication, directed at a specific person, causing substantial emotional distress to that person and serving no legitimate purpose,\" is classified as a first degree misdemeanor. Cyberstalking a child under the age of 16 or a person of any age for which the offender has been ordered by the courts not to contact is considered \"aggravated stalking,\" a third degree felony under Florida law. Cyberstalking in conjunction with a credible threat is also considered aggravated stalking.\n\nIn 2008, Florida passed the \"Jeffrey Johnston Stand Up For All Students Act\" in response to the suicide of 15-year-old Jeffrey Johnston, who had suffered cyberbullying over a long period of time. Unusual among state laws regarding cyberbullying is a provision that withholds funding for schools who are not in compliance with the provision that they must inform parents of those involved in cyberbullying—both the bully and the target.:)\n\nAccording to \"Who@: Working to Halt Online Abuse\":\n(a) Harassment through electronic communications is the use of electronic communication for any of the following\npurposes:\n\n(b) As used in this Act: \n\nIn response to Phoebe Prince's suicide, as well as that of Carl Walker—both of whom had been bullied before taking their lives—the Massachusetts legislature in 2010 passed what advocates call one of the toughest anti-bullying laws in the nation. The law prohibits both online taunting and physical or emotional abuse, and mandates training for faculty and students at schools. It further mandates that school administrators inform parents of bullying that occurs within the schools themselves.\n\nAs noted previously, in 2008 Missouri revised its statutes on harassment to include harassment and stalking through electronic and telephonic communications and cyber-bullying after the suicide of Megan Meier.\n\nNew York state passed the Dignity For All Students Act, focusing primarily on elementary and middle school students.\n\nTexas enacted the Stalking by Electronic Communications Act in 2001.\n\nWashington takes the approach of putting the focus on cyberbullying prevention and response directly on the schools. The law also requires schools to create policies to address bullying in a general sense.\n\nAttempts at legislating cyberbullying have been tried at the federal level, primarily because the Commerce Clause of the U.S. Constitution specifically provides that only the federal government can regulate commerce between the states; this includes electronic communication over the Internet. An early example, the Violence Against Women Act, passed in 2000, included cyberbullying in a part of interstate status on harassment.\n\nIn 2009, Representative Linda Sánchez (D-CA) brought legislation titled the \"Megan Meier Cyberbullying Prevention Act\" before the U.S. House of Representatives. Her efforts were met with little enthusiasm, however, as Representatives from both the Republican and Democratic parties were concerned with the bill's impact on the freedom of speech. One of the oft-cited arguments against the bill comes from talk radio, with the concern expressed being that the law would be used to silence political opponents who use the airwaves to espouse divergent viewpoints. Another issue is that would make violation of the law a felony, rather than a misdemeanor as has been done in most states. Opponents of the bill argue that since the target of such legislation is nominally teenagers, this would put an undue burden on the prison system—since there are no long-term facilities for teenage offenders at the federal level. In addition, opponents call the proposed sentences (up to two years incarceration) excessive.\n\nWhile Sánchez' bill was discussed in committee, it has not passed that stage .\n\nIn early March 2011, U.S. Senator Frank Lautenberg (D-NJ) and Representative Rush D. Holt, Jr. (D-NJ-12) introduced the \"Tyler Clementi Higher Education Anti-Harassment Act\", which would mandate that colleges and universities that receive federal funding have policies in place to address harassment—including cyberbullying. Universities would be required to address harassment that focuses on real or perceived race, color, national origin, sex, disability, sexual orientation, gender identity, or religion. The bill would also enable the U.S. Department of Education to provide training to institutes of higher education to prevent or address harassment. Furthermore, the bill addresses not just student-to-student harassment, but also harassment of students by faculty or staff as well.\n\nHowever, like the Megan Meier Cyberbullying Prevention Act, this bill also has its detractors. Opponents point out that harassment on college campuses is already prohibited under existing laws; furthermore, they point out that harassment based on sexual orientation is also covered under existing statutes. In addition, as with the Sánchez bill, there are questions as to the free speech implications.\n\nPrior to February 2013, there were no laws that directly regulate cyberstalking in India. India's Information Technology Act of 2000 (IT Act) was a set of laws to regulate the cyberspace. However, it merely focused on financial crimes and neglected interpersonal criminal behaviours such as cyberstalking (Behera, 2010; Halder & Jaishankar, 2008; Nappinai, 2010). In 2013, Indian Parliament made amendments to the Indian Penal Code, introducing cyberstalking as a criminal offence. Stalking has been defined as a man or woman who follows or contacts a man or woman, despite clear indication of disinterest to such contact by the man or woman, or monitoring of use of internet or electronic communication of a man or a woman. A man or a woman committing the offence of stalking would be liable for imprisonment up to three years for the first offence, and shall also be liable to fine and for any subsequent conviction would be liable for imprisonment up to five years and with fine.\n\nCyberstalking has been illegal since 2011.\n\nNew Zealand Minister of Justice Judith Collins plans to introduce a law that would make it an offence to incite people to commit suicide, or post material that is grossly offensive by the end of 2013.\n\nInternational law emphasizes a supranational concept related to cybercrime. This is the Convention on Cybercrime, signed by the Council of Europe in Budapest on November 23, 2001.\n\nThe Global Cyber Law Database (GCLD) aims to become the most comprehensive and authoritative source of cyber laws for all countries.\n\n\nNo one shall be subjected to torture or to cruel, inhuman or degrading treatment.\n\nSpitzberg, B. H., & Hoobler, G. (2002). Cyberstalking and the technologies of interpersonal terrorism. New Media & Society, 4(1), 71-92.\nBehera, A. (2010). Cyber Crimes And Law In India. Indian Journal of Criminology & Criminalistics, 31(1), 16 – 30.\nHalder, D. & Jaishankar, K. (2008). Cyber Crimes against Women in India: Problems, Perspectives and Solutions. TMC Academic Journal, 3(10), 48-62.\nNappinai, N, S. (2010). Cyber Crime Law in India: Has Law Kept Pace with Emerging Trends? An Empirical Study. Journal of International Commercial Law and Technology, 5(1), 22-27.\n\n"}
{"id": "15479869", "url": "https://en.wikipedia.org/wiki?curid=15479869", "title": "ERMES", "text": "ERMES\n\nERMES (European Radio Messaging System or Enhanced Radio Messaging System) was a pan-European radio paging system. \n\nIn 1990, the European Telecommunications Standard Institute (ETSI) developed the European Telecommunications Standard ETS 300 133 for ERMES operating in the frequency band 169.4125-169.8125 MHz.\n\n\n\n\nDuring the 1990s, ERMES aimed to achieve a standardised digital platform throughout Europe. It was intended that paging systems based on the ERMES standard would be able to receive text messages transmitted from personal computers, enabling companies to contact their employees over the PSTN. Also, GSM handsets would receive ERMES messages on their displays.\n\nERMES was most widely used in France, where around one million ERMES pagers were in use in 1998. Also in 1998, an ERMES MoU organisation was set up, to lobby for its adoption as the European standard. \n\nERMES never achieved recognition as a leading paging standard. There were questions over costs and also the ERMES standard was in competition with the US-based FLEX standard, a rivalry seen at the time as damaging to the development of the paging industry in Europe. Ultimately paging technology was largely superseded by SMS text messaging.\n\nIn 1999, it was decided that the 169.4-169.8 MHz frequency band would no longer be reserved for the sole use of ERMES and this frequency band was later reassigned to different use.\n\n"}
{"id": "12252139", "url": "https://en.wikipedia.org/wiki?curid=12252139", "title": "Electrostatic separator", "text": "Electrostatic separator\n\nAn electrostatic separator is a device for separating particles by mass in a low energy charged beam.\n\nIt works on the principle of corona discharge, where two plates are placed close together and high voltage is applied. This high voltage is used to separate the ionized particles.\n\nUsually these are used in power plants where the harmful gases coming out of the chimneys are first treated using electrostatic separator. Here the two electrodes are oppositely charged, with a negative electrode the positive ions gets attracted and thus results in a reddish flame whereas the positive electrode is used to treat the negatively charged ions resulting in a bluish white flame that is visible at nights.\n\nElectrostatic separation is a process that uses electrostatic charges to separate crushed particles of material. An industrial process used to separate large amounts of material particles, electrostatic separating is most often used in the process of sorting mineral ore. This process can help remove valuable material from ore, or it can help remove foreign material to purify a substance. In mining, the process of crushing mining ore into particles for the purpose of separating minerals is called beneficiation.\n\nGenerally, electrostatic charges are used to attract or repel differently charged material. When electrostatic separation uses the force of attraction to sort particles, conducting particles stick to an oppositely charged object, such as a metal drum, thereby separating them from the particle mixture. When this type of beneficiation uses repelling force, it is normally employed to change the trajectory of falling objects to sort them into different places. This way, when a mixture of particles falls past a repelling object, the particles with the correct charge fall away from the other particles when they are repelled by the similarly charged object.\n\nCharge is the measure of the electric current flowing through an object. A charge can be positive or negative — objects with a positive charge repel other positively charged objects, thereby causing them to push away from each other, while a positively charged object would attract to a negatively charged object, thereby causing the two to draw together. Electrostatic charges are the charges associated with static electricity. Generally, electrostatic charges can build up by friction and can be observed when a balloon attracts a person's hair after the balloon is rubbed against the person's head.\n\nExperiments showing electrostatic sorting in action can help make the process more clear. To exhibit electrostatic separation at home, an experiment can be conducted using peanuts that are still in their shells. When the shells are rubbed off of the peanuts and gently smashed into pieces, an electrostatically charged device, like a comb rubbed quickly against a wool sweater, will pick up the peanut shells with static electricity. The lightweight crushed shells that are oppositely charged from the comb easily move away from the edible peanut parts when the comb is passed nearby.\n\nThe electrostatic separation of conductors is one method of beneficiation; another common beneficiation method is magnetic beneficiation. Electrostatic separation is a preferred sorting method when dealing with separating conductors from electrostatic separation non-conductors. In a similar way to that in which electrostatic separation sorts particles with different electrostatic charges magnetic beneficiation sorts particles that respond to a magnetic field. Electrostatic beneficiation is effective for removing particulate matter, such as ash from mined coal, while magnetic separation functions well for removing the magnetic iron ore from deposits of clay in the earth.\n"}
{"id": "58542272", "url": "https://en.wikipedia.org/wiki?curid=58542272", "title": "Elizabeth Rossiello", "text": "Elizabeth Rossiello\n\nElizabeth Rossiello is the founder and Chief Executive Officer of digital payments platform BitPesa. She grew up in Queens, New York, and now lives in Dakar, Senegal. She is an expert in East African financial product development and establishing practices in risk, governance and IT for local banks. \n\nRossiello has a Masters in International Finance from the School of International and Public Affairs at Columbia University. Prior to founding BitPesa, she was Deputy Director of Planet Rating's East and Southern African office, served as an Analyst at Credit Suisse in NY, London and Zurich, worked at Goldman Sachs and served as a Robert Bosch Fellow at the German Bundestag in Frankfurt. She is a founding member of the World Economic Forum’s Future Council on Blockchain and the Global Blockchain Business Council, as well as an active advocate for blockchain technology in emerging markets. In 2018 she was named to Fortune Magazine's \"The Ledger 40 Under 40\" list of young people who are transforming business at the leading edge of finance and technology. \n"}
{"id": "35952993", "url": "https://en.wikipedia.org/wiki?curid=35952993", "title": "EthosCE", "text": "EthosCE\n\nEthosCE is a learning management system for the administration of continuing medical education in nursing, pharmacy and other healthcare-related programs. Developed by DLC Solutions, it provides interfaces for organizations to produce and manage continuing education websites. Distribution is provided as a software-as-a-service web application.\n\nThe administrative interface allows users to administer the system through a web browser and supports online and mobile registration of learners and communication for in-person and online training events. The application includes online transcripts, tracking of professional certifications, and an integration with the Accreditation Council for Continuing Medical Education (ACCME) Program and Reporting System (PARS) web service.\n\nThe EthosCE Course object API can be used to define learning objects to be added to a workflow with built in support for Drupal nodes to be part of a course requirement workflow including multiple course objects. The API allows other content/assessments or non-Drupal (external) objects to be delivered and tracked as course content, and it provides access to taking or enrolling learners into courses.\n\nEthosCE was originally released 14 April 2009 by DLC Solutions. Version 7.4 discontinued support for creating Moodle course objects and integrated an external SCORM/TinCan/xAPI player and was released 12 December 2014.\n\n"}
{"id": "6454043", "url": "https://en.wikipedia.org/wiki?curid=6454043", "title": "Focus finder", "text": "Focus finder\n\nA focus finder is a simple optical tool used to examine a virtual image in an optical device to achieve a precise point of focus.\n\nThey are most commonly used in photographic enlarging to ensure that the negative image is accurately focussed on the easel.\n\nFocus finders are designed so that their optical path is exactly equal to the optical path of the uninterrupted light. \n\nIn enlarging, this is achieved by mounting an angled front-silvered mirror on a small plinth and using a strong magnifying eyepiece and graticle to examine the reflected virtual image. The enlarger lens is then carefully focussed until the grain structure of the film can be seen in the plane of the graticle.\n\n"}
{"id": "26105075", "url": "https://en.wikipedia.org/wiki?curid=26105075", "title": "Government performance management", "text": "Government performance management\n\nGovernment performance management (GPM) consists of a set of processes that help government organizations optimize their business performance. It provides a framework for organizing, automating, and analyzing business methodologies, metrics, processes and systems that drive business performance. Some commentators see GPM as the next generation of business intelligence (BI) for governments. GPM helps governments to make use of their financial, human, material, and other resources. In the past, owners have sought to drive strategy down and across their organizations; they have struggled to transform strategies into actionable metrics and they have grappled with meaningful analysis to expose the cause-and-effect relationships that, if understood, could give profitable insight to their operational decision-makers. GPM software and methods allow a systematic, integrated approach that links government strategy to core processes and activities. \"Running by the numbers\" now means something: planning, budgeting, analysis, and reporting can give the measurements that empower management decisions.\n\nAccording to Gartner, the Enterprise Performance Management (EPM) suite market continues to experience strong momentum, growing 19% during 2007. This is slightly in advance of their earlier market sizing and forecast analysis, which anticipated 2007 revenue to be $1.836 million, representing an 18% year-over-year growth. In the latest forecast, Gartner believe that the market for EPM will be more than $3 billion by 2011, representing a 14.4% compound annual growth rate. Several factors contributed to the continued significant growth in EPM revenue during 2007:\n\nGartner expects the business Intelligence software market to reach $3 billion in 2009. \"Companies around the world have purchased more than US$40 billion worth of enterprise applications, including ERP, CRM and HR, during the past few years,\" said Colleen Graham, principal research analyst at Gartner. \"This has generated significant volumes of data in support of the operational processes they automate. By investing in BI, companies can further leverage their enterprise application investments and turn the torrent of data into meaningful insight to better measure performance, respond more quickly to market changes and opportunities and comply with an increasingly complex regulatory environment.\"\n\n"}
{"id": "31498101", "url": "https://en.wikipedia.org/wiki?curid=31498101", "title": "GreenChip", "text": "GreenChip\n\nGreenChip is a technology brand of Philips Semiconductors (now NXP Semiconductors) and is used in the company’s range of power adapter ICs with the same name. GreenChip ICs are used in power adapters and power supplies, as well as energy-saving CFL bulbs and LED lighting products. In 2011, NXP introduced the GreenChip smart lighting solution—including the GreenChip iCFL for compact fluorescent lamps and the GreenChip iSSL for LEDs—to enable quick start times, dimming, extended lifetimes, and wireless connectivity via IPv4 or IPv6 using JenNet-IP network layer software.\n\nThe TEA1504 was the first IC to bear the GreenChip name. Released in 1998, it was an SMPS controller with integrated standby burst mode aimed at CRT monitor applications. Improvements to standby behavior meant the chip realized a < 3 W consumption (the Energy Star target at the time), without needing a separate standby power supply.\n\nThe second generation GreenChip II products appeared in 2000 with the release of the TEA1507, a quasi resonant flyback SMPS controller targeted at standby power savings in monitors, TVs and VCRs. It could achieve a standby performance of < 1 W and boasted efficiency improvements well beyond 90%. The IC was built in close cooperation with Philips CE TV and was included in the designs of numerous TV manufacturers.\n\nIn 2002/2003, the GreenChip II TEA155x series was released. Derived from the TEA1507, the new series was specifically aimed at notebook adapters. The GreenChip II series was capable of reducing notebook adapter no-load power to below 500 mW (a 100% improvement on the state-of-the-art 1 W at that time). It also improved system robustness by integrating more adapter specific protections and signaling conditioning functionality. The TEA155x grew to take a dominant share in the notebook adapter market.\n\nThe third generation GreenChip III arrived in 2007 with the launch of the TEA1750. It established market leadership in the high power and high efficiency notebook and 75-250 W computing markets. GreenChip III combines an SMPS controller with an active PFC pre-conditioner to deliver a highly integrated solution with minimal external components. TEA175x achieves < 300 mW no-load performance, with typical efficiency levels at or beyond 90% over the power supply’s complete operating range. GreenChip III is used in the power adapters of notebook PCs, power supplies for small PCs and all-in-one integrated systems like the iMac.\n\nThe GreenChip SR, a secondary control IC for notebook adapters, was launched in 2006 with the TEA176x family. The GreenChip SR family was extended in 2009 with the TEA179x family. GreenChip SR is a control circuit for the output stage of power supplies. By replacing diode rectifiers with active MOSFET rectification, GreenChip SR enabled typical loss reductions of 20-30%.\n\nIn 2006, Philips also introduced the GreenChip PC, a chipset featuring a new topology designed to increase the overall efficiency of desktop PC power supplies to more than 80 percent; at the time, most desktop PC power supplies were only 60 to 70 percent efficient when operating. The GreenChip PC was based on patented Philips technology integrating the standby supply into the main converter, reducing the number of external components required.\n\nAddressing 10-70 W class power supplies, the TEA173x GreenChip Low Power generation was launched towards the end of 2009. The TEA173x targeted high volume computing (netbook, printer, monitor) and consumer (STB, DVD, Blu-ray, audio) applications. The GreenChip Low Power series enabled efficiencies of up to 90% and standby performances of less than 100 mW.\n\nGreenChip Resonant ICs such as the TEA1713, launched in early 2010, extend the higher end of the Greenchip portfolio. They feature a resonant converter capable of being applied from 90-600 W and delivering efficiencies of over 95%. They are suitable for a broad range of applications including LCD TV and high efficiency computing power like high density travel adapters.\n\n2010 also saw the advent of GreenChip ICs for lamps. These enable the creation of compact energy-saving CFL bulbs that resemble the familiar incandescent bulbs. GreenChip lighting ICs offer a number of benefits over other solutions including greater reliability (extended lifetimes), faster start-up, dimming capability and low noise operation.\n\nThe first and second generation of GreenChip ICs used a combination of High Voltage DMOS and more dense BiCMOS for control in multi-chip configurations. In the second generation and after, BiCMOS was replaced by the A-BCD2 (Advanced Bipolar CMOS DMOS 2) process.\n\n"}
{"id": "9336966", "url": "https://en.wikipedia.org/wiki?curid=9336966", "title": "Happy path", "text": "Happy path\n\nIn the context of software or information modeling, a happy path is a default scenario featuring no exceptional or error conditions. For example, the happy path for a function validating credit card numbers would be where none of the validation rules raise an error, thus letting execution continue successfully to the end, generating a positive response.\n\nProcess steps for a happy path are also used in the context of a use case. In contrast to the happy path, process steps for alternate paths and exception paths may also be documented.\n\nHappy path testing is a well-defined test case using known input, which executes without exception and produces an expected output.\n\nHappy day (or sunny day) scenario and golden path are synonyms for happy path.\n\nIn use case analysis, there is only one happy path, but there may be any number of additional alternate path scenarios which are all valid optional outcomes. If valid alternatives exist, the happy path is then identified as the default or most likely positive alternative. The analysis may also show one or more exception paths. An exception path is taken as the result of a fault condition. Use cases and the resulting interactions are commonly modeled in graphical languages such as the Unified Modeling Language or SysML.\n\nThere is no agreed name for the opposite of happy paths: they may be known as sad paths, bad paths, or exception paths.\n"}
{"id": "47881061", "url": "https://en.wikipedia.org/wiki?curid=47881061", "title": "Humphrey visual field analyser", "text": "Humphrey visual field analyser\n\nHumphrey field analyser (HFA), is a tool for measuring the human visual field, it is used by optometrists, orthoptists and ophthalmologists, particularly for detecting monocular visual field.\n\nThe results of the Analyser identify the type of vision defect. Therefore, it provides information regarding the location of any disease processes or lesion(s) throughout the visual pathway. This guides and contributes to the diagnosis of the condition affecting the patient's vision. These results are stored and used for monitoring the progression of vision loss and the patient's condition.\n\nThe Analyser can be used for screening, monitoring and assisting in the diagnosis of certain conditions. There are numerous testing protocols to select, based on the purpose. The first number denotes the extent of the field measured on the temporal side, from the centre of fixation, in degrees. The '-2' represents the pattern of the points tested. They include:\nThe above tests can be performed in either SITA-Standard or SITA-Fast. SITA-Fast is a quicker method of testing. It produces similar results compared to SITA-Standard, however repeatability is questionable and it is slightly less sensitive\nThe \"Test Library\" houses additional tests for more specific purposes such as the following:\n\nThe Analyser test takes approximately 5–8 minutes, excluding patient set up. There are multiple steps which need to be done before commencement of the test to ensure reliable results are attained.\nThe test type and eye are firstly selected and the patient's details are entered, including their refractive error. The Analyser will provide a lens strength and type (either spherical and/or cylindrical), if required for the test. In these instances, wire-rimmed trial lenses are generally used, with the cylindrical lens placed closest to the patient so the axis is easily read. The clinician can alter the fixation targets as per necessary (see Fixation Targets for advice).\n\nBefore putting the patient onto the machine, the requirements of the test itself are clearly explained. The patient is instructed to maintain fixation on the central target and is given a buzzer to only press when they see a light stimulus. It is not possible to see every light and some lights appear brighter/duller and slower/faster than others. The eye not being tested is patched and the room lights are dimmed prior to commencement of the test.\n\nThe patient is positioned appropriately and comfortably against the forehead rest and chin rest. Minor adjustments to the head position are made to centre the pupil on the display screen to allow eye monitoring throughout the test. The lens holder should be as close to the patient's eye as possible to avoid artefacts (see Disadvantages for possible artefacts).\n\nIt is important for the patient to blink normally, relax and maintain concentration throughout the test. This will increase the reliability of results. \n\nThe Analyser projects a series of white light stimuli of varying intensities (brightness), throughout a uniformly illuminated bowl. The patient uses a handheld button that they press to indicate when they see a light. This assesses the retina's ability to detect a stimulus at specific points within the visual field. This is called retinal sensitivity and is recorded in 'decibels' (dB). \nThe Analyser currently utilises the Swedish Interactive Thresholding Algorithm (SITA); a formula which allows the fastest and most accurate visual field assessment to date. Results are then compared against an age-matched database which highlights unusual and suspicious vision loss, potentially caused by pathology.\n\nThere are different targets a patient can fixate on during the test. They are chosen on the basis of the patient's conditions. \n\nIssues of reliability are critical in result interpretation. These include, but not limited to, the patient losing concentration, closing their eyes or pressing the buzzer too frequently. Monitoring fixation is made visible via the display screen and gaze tracker, located at the bottom of the printout. The degree of reliability is determined by the reliability indices located on the printout (Fig. 4). These are assessed first and allow the examiner to determine if the end results are reliable. These indices include:\n\nAfter reliability is determined, the remaining data is assessed.\n\nRepresents raw values of patient's retinal sensitivity at specific retinal points in dB. Higher numbers equate to higher retinal sensitivities. Sensitivity is greatest in the central field and decreases towards the periphery. Normal values are approximately 30 dB while recorded values of <0 dB equate to no sensitivity measured.\n\nA graphical representation of the numerical display, allowing for easy interpretation of the field loss. Lower sensitivities are indicated by darker areas and higher sensitivities are represented with a lighter tone. This scale is used to demonstrate vision changes to the patient but is not used for diagnostic purposes.\n\nThe \"numerical total\" demonstrates the difference between measured values and population age-norm values at specific retinal points.\nThe \"statistical display\" (located below the numerical total) demonstrates the percentage of the normal population who measure below the patient's value at a specific retinal point. The probability display provides this percentage a key for interpreting the statistical display. For example, the darkest square in the key represents that <0.5% of the population would also attain this result, indicating that the vision loss is extensive. The Total Deviation plots highlight diffuse vision loss (i.e. the total departure from the age-norm).\n\nProvides a numerical total and statistical display as the Total Deviation plot. However, it accounts for general reductions of vision caused by media opacities (e.g. cataract), uncorrected refractive error, reductions in sensitivity due to age and pupil miosis. This highlights focal loss only (i.e. vision loss suspected from only pathological processes). Therefore, this is the main plot referred to when making a diagnosis. The Pattern Deviation plot is generally lighter than the Total Deviation because of the factors accounted for.\n\nThese provide a statistical summary of the field with one number. Although not used for initial diagnosis, they are essential for monitoring glaucoma progression. They include: \n\nProvides assessment of the visual field where glaucomatous damage is often seen. It compares five corresponding and mirrored areas in the superior and inferior visual fields. The result of either 'Outside Normal Limits' (significant difference in superior and inferior fields), 'Borderline' (suspicious differences) or 'Within Normal Limits' (no differences) is only considered when the patient has, or is a suspect for, glaucoma. This is only available in 30-2 and 24-2 Analyser protocol.\n\nThe VFI reflects retinal ganglion cell loss and function, as a percentage, with central points weighted more.\n\nIt is expressed as a percentage of visual function; with 100% being a perfect age-adjusted visual field and 0% represents a perimetrically blind field. The pattern deviation probability plot (or total deviation probability plot when MD is worse than -20dB) is used to identify abnormal points and age corrected sensitivity at each point is calculated using total deviation numerical map. VFI is a reliable index on which glaucomatous visual field severity staging can be based. \n\nThe shaded pattern of vision loss provided on the Pattern Deviation plot allows for diagnosis of the type of vision loss present. This contributes to other clinical findings in the diagnosis of certain conditions. The types of vision loss and associated conditions are not described in the extent of this article, however Figure 5 provides typical examples of visual field loss seen. Refer to #See also for more information.\n\n\n\n\n"}
{"id": "27761763", "url": "https://en.wikipedia.org/wiki?curid=27761763", "title": "IPhone (1st generation)", "text": "IPhone (1st generation)\n\nThe iPhone is the first smartphone model designed and marketed by Apple Inc. After years of rumors and speculation, it was officially announced on January 9, 2007, and was later released in the United States on June 29, 2007. It featured quad-band GSM cellular connectivity with GPRS and EDGE support for data transfer.\n\nOn June 9, 2008, Apple announced its successor, the iPhone 3G. After this announcement, the first-generation iPhone became referred to by some sources as the iPhone 2G due to the fact that it was the only iPhone to solely support 2G-speed networks. The original iPhone has not received software updates from Apple since iPhone OS (now iOS) 3.1.3.\n\nSince June 2013, the original iPhone is considered \"vintage\" by some service providers in the US, and \"obsolete\" in Apple retail stores and all other regions. Apple does not service vintage or obsolete products, and replacement parts for obsolete products are not available to service providers.\n\nIn 2005, Apple CEO Steve Jobs conceived an idea of using a multi-touch touchscreen to interact with a computer in a way in which he could directly type onto the display. He decided that it needed to have a triple layered touch screen, a very new and high tech technology at the time. This helped out with removing the physical keyboard and mouse, the same as a tablet computer. Jobs recruited a group of Apple engineers to investigate the idea as a side project. When Jobs reviewed the prototype and its user interface, he conceived a second idea of implementing the technology onto a mobile phone. The whole effort was called Project Purple 2 and began in 2005.\n\nApple created the device during a secretive and unprecedented collaboration with AT&T, formerly Cingular Wireless. The development cost of the collaboration was estimated to have been $150 million over a thirty-month period. Apple rejected the \"design by committee\" approach that had yielded the Motorola ROKR E1, a largely unsuccessful collaboration with Motorola. Instead, Cingular Wireless gave Apple the liberty to develop the iPhone's hardware and software in-house.\n\nThe original iPhone was introduced by Steve Jobs on January 9, 2007 in a keynote address at the Macworld Conference & Expo held in Moscone West in San Francisco, California. In his address, Jobs said, \"This is a day, that I have been looking forward to for two and a half years\", and that \"today, Apple is going to reinvent the phone.\" Jobs introduced the iPhone as a combination of three devices: a \"widescreen iPod with touch controls\"; a \"revolutionary mobile phone\"; and a \"breakthrough Internet communicator\".\n\nSix weeks before the iPhone was to be released, the plastic screen was replaced with a glass one, after Jobs was upset that the screen of the prototype he was carrying in his pocket had been scratched by his keys. The quick switch led to a bidding process for a manufacturing contractor that was won by Foxconn, which had just opened up a new wing of its Shenzhen factory complex specifically for this bid.\n\nIn February 2007, LG Electronics accused Apple of \"copying\" their LG Prada phone, which was introduced around the same time as iPhone.\n\nThe iPhone was released in the United States on June 29, 2007 at the price of $499 for the 4 GB model and $599 for the 8 GB model, both requiring a 2-year contract. Thousands of people were reported to have waited outside Apple and AT&T retail stores days before the device's launch; many stores reported stock shortages within an hour of availability. To avoid repeating the problems of the PlayStation 3 launch, which caused burglaries and even a shooting, off-duty police officers were hired to guard stores overnight.\n\nIt was later made available in the United Kingdom, France, Germany, Portugal, the Republic of Ireland, and Austria in November 2007.\n\nSix out of ten Americans surveyed said they knew the iPhone was coming before its release.\n\nThe iPhone's main competitors in both consumer and business markets were considered to be the LG Prada, LG Viewty, Samsung Ultra Smart F700, Nokia N95, Nokia E61i, Palm Treo 750, Palm Centro, HTC Touch, Sony Ericsson W960 and BlackBerry.\n\nThe iPod Touch, a touchscreen device with the media and internet abilities and interface of the iPhone but without the ability to connect to a cellular network for phone functions or internet access, was released on September 5, 2007. At the same time, Apple significantly dropped the price of the 8 GB model (from $599 to $399, still requiring a 2-year contract with AT&T) while discontinuing the 4 GB model. Apple sold the one millionth iPhone five days later, or 74 days after the release. After receiving \"hundreds of emails...upset\" about the price drop, Apple gave store credit to early adopters.\n\nA 16 GB model was released on February 5, 2008 for $499, the original launch price of the 4 GB model. Apple released an SDK on March 6, 2008, allowing developers to create the apps that would be available starting in iPhone OS version 2.0, a free upgrade for iPhone users. On June 9, Apple announced the iPhone 3G, which began shipping July 11. The original iPhone was discontinued 4 days later; total sales volume came to 6,124,000 units.\n\nDuring release, the iPhone was marketed as running \"OS X\". The name of the operating system was revealed as iPhone OS with the release of the iPhone SDK. The original iPhone supported three major versions of the operating system before it was discontinued: iPhone OS 1, 2, and 3. However, the full iPhone OS 3 feature set was not supported, and the last update the original iPhone received was iPhone OS 3.1.3\n\nThe original operating system for the original iPhone was iPhone OS 1, marketed as OS X, and included Visual Voicemail, multi-touch gestures, HTML email, Safari web browser, threaded text messaging, and YouTube. However, many features like MMS, apps, and copy and paste were not supported at release, leading hackers jailbreaking their phones to add these features. Official software updates slowly added these features.\n\niPhone OS 2 was released on July 11, 2008, around the same time as the release of the iPhone 3G, and introduced third-party applications, Microsoft Exchange support, push e-mail, and other enhancements.\n\niPhone OS 3 was released on June 17, 2009, and introduced copy and paste functionality, Spotlight search for the home screen, and new features for the YouTube app. iPhone OS 3 was available for the original iPhone as well as the iPhone 3G. However, not all features of iPhone OS 3 were supported on the original iPhone.\n\niPhone OS 3.1.3 was the last version of iPhone OS (now iOS) to be released for the original iPhone.\n\nOnly four writers were given review models of the original iPhone: David Pogue of \"The New York Times\", Walt Mossberg of \"The Wall Street Journal\", Steven Levy of \"Newsweek\", and Ed Baig of \"USA Today\". \"The New York Times\" and \"The Wall Street Journal\" published positive, but cautious, reviews of the iPhone, their primary criticisms being the relatively slow speed of the AT&T's 2.5G EDGE network and the phone's inability to connect using 3G services. \"The Wall Street Journal\"s technology columnist, Walt Mossberg, concluded that \"despite some flaws and feature omissions, the iPhone is, on balance, a beautiful and breakthrough handheld computer.\" \"Time\" magazine named it the Invention of the Year in 2007.\n\n\"Mobile Gazette\" reported that whilst the iPhone has many impressive points, it equally has many bad ones too, noting the lack of 3G, MMS, third-party applications, and its weak camera without autofocus and flash.\n\n\n"}
{"id": "30446277", "url": "https://en.wikipedia.org/wiki?curid=30446277", "title": "James Acord", "text": "James Acord\n\nJames Leroy Acord (19 October 1944 – 9 January 2011) was an artist who worked directly with radioactive materials. He attempted to create sculpture and events that probed the history of nuclear engineering and asked questions about the long-term storage of nuclear waste. For 15 years he lived in Richland, Washington, the dormitory town for the Hanford Nuclear Reservation, at one time home to nine nuclear reactors and five plutonium-processing complexes and the most contaminated nuclear site in the United States. His major ambition while there was to build a \"nuclear Stonehenge\" on a heavily contaminated area of land in the site, incorporating twelve uranium breeder-blanket assemblies. \n\nAcord was the only private individual in the world licensed to own and handle radioactive materials, and acquired nuclear fuel rods containing depleted uranium from the completed but not operated German SNR-300 breeder reactor to use as artistic materials. He had his nuclear license number tattooed onto his neck. He spoke on art and nuclear science at both art and nuclear industry events in the US and the UK and organised many forums that brought together artists, activists and nuclear industry experts. \n\nHe was profiled by Philip Schuyler for \"The New Yorker\" in 1991, and was the inspiration for the character of Reever in \"The Book of Ash\" by James Flint. The extensive audio interviews Flint did with Acord in Alaska in 1998 as part of his research for the novel have now been archived and catalogued by the British Library.\n\nFrom 1998 to 1999 he was Artist in Residence at Imperial College London, a residency set up by arts commissioning organisation The Arts Catalyst, and funded by Arts Council England and the Calouste Gulbenkian Foundation.\n\nHe committed suicide in Seattle on January 9, 2011 at the age of 66.\n\nHis sculpture, \"Monstrance for a Grey Horse\", is installed on the Southwestern University campus in Georgetown, Texas.\n\n"}
{"id": "37394465", "url": "https://en.wikipedia.org/wiki?curid=37394465", "title": "KAoS", "text": "KAoS\n\nKAoS is a policy and domain services framework created by the Florida Institute for Human and Machine Cognition. It uses W3C’s Web Ontology Language (OWL) standard for policy representation and reasoning, and a software guard technology for efficient enforcement of a compiled version of its policies. It has been used in a variety of government-sponsored projects for distributed host and network management and for the coordination of human-agent-robot teams, including DARPA's CoABS Grid, Cougaar, and Common Object Request Broker Architecture (CORBA) models.\n\n"}
{"id": "27592349", "url": "https://en.wikipedia.org/wiki?curid=27592349", "title": "List of repetitive strain injury software", "text": "List of repetitive strain injury software\n\nRepetitive strain injuries (RSI) are to the body's muscles, joints, tendons, ligaments, bones, or nerves caused by repetitive movements. Such injuries are more likely if the movements required force or were accompanied by vibrations, compression, or the maintenance of sustained or awkward positions. Prolonged use of computer equipment can result in upper limb disorders, notably in the wrist or the back. RSIs are a subset of musculoskeletal disorders.\n\nThis article discusses and lists some specialized software that is available to aid individuals avoid injury or manage current discomfort/injury associated with computer use.\n\nSoftware for RSIs generally address these functional categories:\n\nThis can be an important component for many users. Considerations for selecting a tool include the mechanism the tools uses to decide when to alert you to take a break, how it tells you to take a break, and how flexibly the tool can meet your particular needs.\n\nMany tools are simple timers (e.g. remind me to rest every 60 minutes). That may work well if your job requires constant and consistent computer work, but can be irritating if you don't work constantly on the computer because breaks will often be suggested when you weren't working much prior to the suggestion. Other tools consider your natural rests and delay break suggestions accordingly. Some tools also consider your activity, and will suggest breaks sooner/later during periods of intense/light activity. These tools can be less frustrating to people whose computer work is interspersed with other activity throughout the day.\n\nThe various mechanisms for reminding you to take a break can include visual and audio indicators, workflow limiters (e.g. popup windows, screen dimmers/blankers), and much more. The best tools allow you to select which of these mechanisms you want to use.\n\nFlexibility is important since each person has different needs. Some tools have extensive customization capability that allows you to configure exactly how and when breaks will be suggested. Features to enforce breaks can also be helpful to people who want to take breaks but whose personalities are such that they have a hard time stopping work. Some tools have advanced features like the ability to block break suggestions during some activities (e.g. when showing a presentation, or in full-screen mode).\n\nApplications with these tools seek to mitigate the impact of particular activities by either changing or reducing the associated exposure. This could involve changing or reducing input device use, improving a user-interface to reduce stress, speeding up a process to reduce the time a user needs to be at the computer, etc.\n\nAn example of a tool that changes the impact would be speech recognition. Speech recognition replaces keyboard (and sometimes mouse) input with vocal input. This type of solution can be very helpful at reducing some types of strain, but it's important to recognize that another significant strain may be created.\n\nAn example of a tool that reduces the impact would be a hotkey tool or automatic clicking tool. These tools ideally reduce the number of keystrokes and mouse clicks that a user need do to accomplish a particular task.\nYou can find a list of software names in the .\n\nAn example of a tool that reduces the impact would also be breathing scrolling. Breathing scrolling requires no mouse or keyboard for scrolling. It uses microphone to scroll websites. \n\nA tip, in order to use the mouse less often in the software menus, is to learn the keyboard shortcuts.\n\nThis is an alphabetical list, This list does not rank application quality, nor is it complete. Many other applications exist. A \"pages-of-Google-hits\" score is provided with the reference to each program's home page.\n\n\n\n\n\nVisionProtect\n\n\n"}
{"id": "1009912", "url": "https://en.wikipedia.org/wiki?curid=1009912", "title": "Medical glove", "text": "Medical glove\n\nMedical gloves are disposable gloves used during medical examinations and procedures to help prevent cross-contamination between caregivers and patients. Medical gloves are made of different polymers including latex, nitrile rubber, polyvinyl chloride and neoprene; they come unpowdered, or powdered with cornstarch to lubricate the gloves, making them easier to put on the hands. \n\nCornstarch replaced tissue-irritating Lycopodium powder and talc, but even cornstarch can impede healing if it gets into tissues (as during surgery). As such, unpowdered gloves are used more often during surgery and other sensitive procedures. Special manufacturing processes are used to compensate for the lack of powder. \n\nThere are two main types of medical gloves: examination and surgical. Surgical gloves have more precise sizing with a better precision and sensitivity and are made to a higher standard. Examination gloves are available as either sterile or non-sterile, while surgical gloves are generally sterile.\n\nCaroline Hampton became the chief nurse of the operating room when Johns Hopkins Hospital opened in 1889. When \"(i)n the winter of 1889 or 1890\" she developed a skin reaction to mercuric chloride that was used for asepsis, William Halsted, soon-to-be her husband, asked the Goodyear Rubber Company to produce thin rubber gloves for her protection. In 1894 Halsted implemented the use of sterilized medical gloves at Johns Hopkins. \n\nThe first disposable latex medical gloves were manufactured in 1964 by Ansell. They based the production on the technique for making condoms. These gloves have a range of clinical uses ranging from dealing with human excrement to dental applications.\n\nCriminals have also been known to wear medical gloves during commission of crimes. These gloves are often chosen because their thinness and tight fit allow for dexterity. However because of the thinness of these gloves, fingerprints may actually pass through the material as glove prints, thus transferring the wearer's prints onto the surface touched or handled.\n\nThe participants of the Watergate burglaries infamously wore rubber surgical gloves in an effort to hide their fingerprints.\n\nGenerally speaking, examination gloves are sized in XS, S, M and L. Some brands may offer size XL. Surgical gloves are usually sized more precisely since they are worn for a much longer period of time and require exceptional dexterity. The sizing of surgical gloves are based on the measured length across the palm in inches, at a level slightly above the thumb's sewn. Typical sizing ranges from 5.5 to 9.0 at an increment of 0.5. Some brands may also offer size 5.0 which is particularly relevant to women practitioners. First-time users of surgical gloves may take some time to find the right size and brand that suit their hand geometry the most. People with a thicker palm may need a size larger than the measurement and vice versa.\n\nResearch on a group of American surgeons found that the most common surgical glove size for men is 7.0, followed by 6.5; and for women 6.0 followed by 5.5.\n\nTo facilitate donning of gloves, powders have been used as lubricants. Early powders derived from pines or club moss were found to be toxic. Talcum powder was used for decades but linked to postoperative granuloma and scar formation. Corn starch, another agent used as lubricant, was also found to have potential side effects such as inflammatory reactions and granuloma and scar formation.\n\nWith the availability of non-powdered medical gloves that were easy to don, calls for the elimination of powdered gloves became louder. By 2016, healthcare systems in Germany and the United Kingdom had eliminated their use. In March 2016, the FDA issued a proposal to ban their medical use as well. The United States Food and Drug Administration (FDA) passed a rule on December 19, 2016 banning all powdered gloves intended for medical use. The rule became effective on January 18, 2017.\n\nPowder-free medical gloves are used in medical cleanroom environments, where the need for cleanliness is often similar to that in a sensitive medical environment.\n\nTo make them easier to don without the use of powder, gloves can be treated with Chlorine. Chlorination affects some of the beneficial properties of latex, but also reduces the quantity of allergenic latex proteins.\n\nDue to the increasing rate of latex allergy among health professionals, and in the general population, gloves made of non-latex materials such as polyvinyl chloride, nitrile rubber, or neoprene have become widely used. Chemical processes may be employed to reduce the amount of antigenic protein in Hevea latex, resulting in alternative natural-rubber-based materials such Vytex Natural Rubber Latex. However, non-latex gloves have not yet replaced latex gloves in surgical procedures, as gloves made of alternative materials generally do not fully match the fine control or greater sensitivity to touch available with latex surgical gloves. (High-grade isoprene gloves are the only exception to this rule, as they have the same chemical structure as natural latex rubber. However, fully artificial polyisoprene—rather than \"hypoallergenic\" cleaned natural latex rubber—is also the most expensive natural latex substitute available.) Other high-grade non-latex gloves, such as nitrile gloves, can cost over twice the price of their latex counterparts, a fact that has often prevented switching to these alternative materials in cost-sensitive environments, such as many hospitals. Nitrile is a synthetic rubber. It has no latex protein content and is more resistant to tearing. Also it is very resistant to many chemicals and is very safe for people who are allergic to latex protein. Nitrile gloves are the most durable type of disposable gloves. Although nitrile gloves are known for their durability, extra care should be taken while handling silver and other highly reactive metals because those substances can react with sulfur, an accelerant in nitrile gloves.\n\nDouble gloving is the practice of wearing two layers of medical gloves to reduce the danger of infection from glove failure or penetration of the gloves by sharp objects during medical procedures (For people with HIV and Hepatitis, surgeons use antivirus glove). This should better protect the patient against infections transmitted by the surgeon. A systematic review of the literature has shown double gloving to offer significantly more protection against inner glove perforation in surgical procedures compared to the use of a single glove layer. But it was unclear if there was better protection against infections transmitted by the surgeon. Another systematic review studied if double gloving protected the surgeon better against infections transmitted by the patient. Pooled results of 12 studies (RCTs) with 3,437 participants showed that double gloving reduced the number of perforations in inner gloves with 71% compared to single gloving. On average ten surgeons/nurses involved in 100 operations sustain 172 single gloves perforations but with double gloves only 50 inner gloves would be perforated. This is a considerable reduction of the risk.\n"}
{"id": "48622872", "url": "https://en.wikipedia.org/wiki?curid=48622872", "title": "Micro carbon residue", "text": "Micro carbon residue\n\nMicro carbon residue, commonly known as \"MCR\" is a laboratory test used to determine the amount of carbonaceous residue formed after evaporation and pyrolysis of petroleum materials under certain conditions. The test is used to provide some indication of a material's coke-forming tendencies. The test results are equivalent to the test results obtained from the Conradson Carbon Residue test.\n\nA quantity of sample is weighed, placed in a glass vial, and heated to 500 °C. Heating is performed in a controlled manner, for a specific period of time, and under an inert (nitrogen) atmosphere . The sample experiences coking reactions, with volatiles formed being swept away by the nitrogen. The carbonaceous residue remaining is reported as a mass percent of the original sample, and noted as “carbon residue (micro).” \n\n\nMicro carbon residue offers the same range of applicability as the test to which it is equivalent, Conradson Carbon Residue. Advantages of MCR include better control of test conditions, smaller samples, and less operator attention. Applications include:\n\n\n"}
{"id": "16021852", "url": "https://en.wikipedia.org/wiki?curid=16021852", "title": "Mood board", "text": "Mood board\n\nA mood board is a type of collage consisting of images, text, and samples of objects in a composition. It can be based upon a set topic or can be any material chosen at random. A mood board can be used to give a general idea of a topic, or to show how different something is from the modern day. They may be physical or digital, and can be effective presentation tools.\n\nGraphic designers, interior designers, industrial designers, photographers and other creative artists use mood boards to visually illustrate the style they wish to pursue. However, these boards can also be used by design professionals to visually explain a certain style of writing, or an imaginary setting for a storyline. In short, mood boards are not limited to visual subjects, but serve as a visual tool to quickly inform others of the overall \"feel\" (or \"flow\") of an idea. In creative processes, mood boards can balance coordination and creative freedom.\n\nTraditionally, mood boards are made from foam board which can be cut up with a scalpel and can also have spray mounted cut-outs put onto it. Creating mood boards in a digital form may be easier and quicker, especially when it comes to collaboration or modification of projects, but physical objects often tend to have a higher impact on people because of the more complete palette of sensations physical mood boards offer, in contrast with the digital mood boards. Mood boards can also be painted.\n\n\n"}
{"id": "25057050", "url": "https://en.wikipedia.org/wiki?curid=25057050", "title": "National Union of Distributive and Allied Workers", "text": "National Union of Distributive and Allied Workers\n\nThe National Union of Distributive and Allied Workers (NUDAW) was a trade union in the United Kingdom.\n\nThe union was founded in 1921, when the Amalgamated Union of Co-operative Employees merged with the National Union of Warehouse and General Workers. The Co-operative Insurance Staff union split in 1922, but several small unions joined during the 1920s, and membership reached 96,000 by 1926, rising to 274,000 in 1946. By this point, four-tenths of its members were women.\n\nIn 1947, NUDAW merged with the National Amalgamated Union of Shop Assistants, to form the Union of Shop, Distributive and Allied Workers. Joseph Hallsworth was General Secretary of the union for its entire existence.\n"}
{"id": "4113994", "url": "https://en.wikipedia.org/wiki?curid=4113994", "title": "Netfrastructure", "text": "Netfrastructure\n\nNetfrastructure is both a web application development and database tool for Java and the name of the company which produces it. It was founded by Jim Starkey, a well known database architect. In 2006, MySQL AB acquired Netfrastructure.\n\nNetfrastructure database was the basis for the Falcon storage engine of the MySQL database. \n"}
{"id": "1041015", "url": "https://en.wikipedia.org/wiki?curid=1041015", "title": "Nitrogen trichloride", "text": "Nitrogen trichloride\n\nNitrogen trichloride, also known as trichloramine, is the chemical compound with the formula NCl. This yellow, oily, pungent-smelling and explosive liquid is most commonly encountered as a byproduct of chemical reactions between ammonia-derivatives and chlorine (for example, in swimming pools).\n\nThe compound is prepared by treatment of ammonium salts, such as ammonium nitrate with chlorine.\n\nIntermediates in this conversion include chloramine and dichloramine, NHCl and NHCl, respectively.\n\nLike ammonia, NCl is a pyramidal molecule. The N-Cl distances are 1.76 Å, and the Cl-N-Cl angles are 107°.\n\nThe chemistry of NCl has been well explored. It is moderately polar with a dipole moment of 0.6 D. The nitrogen center is basic but much less so than ammonia. It is hydrolyzed by hot water to release ammonia and hypochlorous acid.\n\nNCl explodes to give N and chlorine gas. This reaction is inhibited for dilute gases.\n\nNitrogen trichloride can form in small amounts when public water supplies are disinfected with monochloramine, and in swimming pools by disinfecting chlorine reacting with urea in urine and sweat from bathers. \n\nNitrogen trichloride, trademarked as Agene, was at one time used to bleach flour, but this practice was banned in the United States in 1949 due to safety concerns.\n\nNitrogen trichloride can irritate mucous membranes—it is a lachrymatory agent, but has never been used as such. The pure substance (rarely encountered) is a dangerous explosive, being sensitive to light, heat, even moderate shock, and organic compounds. Pierre Louis Dulong first prepared it in 1812, and lost two fingers and an eye in two explosions. In 1813, an NCl explosion blinded Sir Humphry Davy temporarily, inducing him to hire Michael Faraday as a co-worker. They were both injured in another NCl explosion shortly thereafter.\n\n\n\n"}
{"id": "5226865", "url": "https://en.wikipedia.org/wiki?curid=5226865", "title": "Optical mount", "text": "Optical mount\n\nAn optical mount is a device used to join a normal camera and another optical instrument, such as a microscope or telescope. The optical mount is generally attached to the camera as a lens would on one end, and fastened to the other instrument in a similar fashion. Optical mounts are used extensively in scientific imaging applications in biology and astronomy.\n"}
{"id": "6585471", "url": "https://en.wikipedia.org/wiki?curid=6585471", "title": "Outline of energy development", "text": "Outline of energy development\n\nThe following outline is provided as an overview of and topical guide to energy development:\n\nEnergy development – the effort to provide sufficient primary energy sources and secondary energy forms for supply, cost, impact on air pollution and water pollution, mitigation of climate change with renewable energy.\n\nEnergy development\n\n\nHistory of energy development\n\n\nList of emerging energy technologies\n\n"}
{"id": "880815", "url": "https://en.wikipedia.org/wiki?curid=880815", "title": "Paul J. Perrone", "text": "Paul J. Perrone\n\nPaul J. Perrone has written numerous books and articles on various Java-based software technologies. He has also founded Perrone Robotics fusing open and standard software technologies with the field of robotics. He has helped push robotics into the mainstream and brought the term \"popular robotics\" into the public eye.\n\nIn addition to robotics software for personal and professional use. Perrone was the team lead for Team Jefferson, a 2005 DARPA Grand Challenge team, and a DARPA Grand Challenge (2007) team.\n\nHe has been a frequent presenter at Sun Microsystem's JavaOne conference, namely James Gosling's keynote sessions presenting an autonomous dune buggy (Tommy), an unmanned helicopter , an autonomous Scion Xb (Tommy Jr) , and Lincvolt, an energy-efficient vehicle owned and spearheaded by rock star Neil Young. He received a Duke Award for Tommy, and a Golden Duke and Lifetime Achievement Award for Lincvolt .\n\nHe is featured in a documentary about a robotic car entered into the historic 2005 DARPA Grand Challenge and was featured in a Discovery Science special called \"RoboCars\" for his robotic car entered into the 2007 DARPA Urban Challenge .\n\nIn 2010, he was also one of 25 individuals living in Charlottesville, Virginia who are well regarded in their respective fields .\n\n"}
{"id": "54064049", "url": "https://en.wikipedia.org/wiki?curid=54064049", "title": "Phanteks", "text": "Phanteks\n\nPhanteks is a Dutch company which mainly produces PC cases, fans and other case accessories. The company has a base in the United States.\n\nPhanteks's first product was the PH-TC14PE, a CPU cooler, which gave the young company a good reputation right from the beginning. Later, Phanteks started its case product line, beginning with the Enthoo Series.\n\nThe company has several different versions of the Enthoo series and a few of the Eclipse series. Furthermore, their cases' cooling solutions have been extended by more CPU coolers, fans and accessories. Since the beginning of 2017, the company also produces liquid cooling blocks and fittings. One of their most recent computer cases, shown at Computex 2018, is the Evolv X. The Evolv X is the successor of the Evolv ATX; while similar to the original in design on the outside, the Evolv X has been redesigned from the inside out.\n\n"}
{"id": "33223346", "url": "https://en.wikipedia.org/wiki?curid=33223346", "title": "Pharmacopeia biotechnology", "text": "Pharmacopeia biotechnology\n\nPharmacopeia was a public biotechnology company that pioneered the field of small molecule combinatorial chemistry.\n\nPharmacopeia was founded by Larry Bock and Drs. Michael Wigler, Clark Still and Jack Chabala.\n\nPharmacopeia went public in 1996 and was acquired by Ligand Pharmaceuticals on 12/23/2008.\n\n"}
{"id": "19858613", "url": "https://en.wikipedia.org/wiki?curid=19858613", "title": "Pilot line", "text": "Pilot line\n\nA pilot line is a pre-commercial production line that produces small volumes of new technology-based products, or employs new production technology, as a step towards the commercialisation of the new technology.\n\nPilot lines help bridge the gap between research and commercialisation, which is caused by the fact that new technology that has been proved only in a laboratory usually is not ready yet for application. They help bridge this gap in two ways:\n\nIn terms of the Technology Readiness Level framework, pilot plants validate production technology at levels TRL5-6: validation and demonstration of technology in a relevant environment.\n\nA word similar to pilot line is pilot plant. Essentially, pilot plants and pilot lines perform the same functions, but 'pilot plant' is used in the context of (bio)chemical and advanced materials production systems, whereas 'pilot line' is used for new technology in general, e.g. by the European Union.\n\nAn industrial pilot line for micro-fabricated medical devices is established as part of the ECSEL JU project InForMed. The pilot line will be hosted by a large industrial end-user, and is specifically targeted and equipped to bridge the gap between concept creation and full-scale production.\n"}
{"id": "733259", "url": "https://en.wikipedia.org/wiki?curid=733259", "title": "Polyphenylene sulfide", "text": "Polyphenylene sulfide\n\nPolyphenylene sulfide (PPS) is an organic polymer consisting of aromatic rings linked by sulfides. Synthetic fiber and textiles derived from this polymer resist chemical and thermal attack. PPS is used in filter fabric for coal boilers, papermaking felts, electrical insulation, film capacitors, specialty membranes, gaskets, and packings. PPS is the precursor to a conductive polymer of the semi-flexible rod polymer family. The PPS, which is otherwise insulating, can be converted to the semiconducting form by oxidation or use of dopants.\n\nPolyphenylene sulfide is an engineering plastic, commonly used today as a high-performance thermoplastic. PPS can be molded, extruded, or machined to tight tolerances. In its pure solid form, it may be opaque white to light tan in color. Maximum service temperature is . PPS has not been found to dissolve in any solvent at temperatures below approximately .\nAn easy way to identify the compound is by the metallic sound it makes when struck.\n\nPPS is marketed by different brand names by different manufacturers. The major industry players are China Lumena New Materials, Solvay, Kureha, SK Chemicals, Celanese, DIC Corporation, Toray Industries, Zhejiang NHU Special Materials, SABIC, and Tosoh. Other manufacturers include Chengdu Letian Plastics, Lion Idemitsu Composites, and Initz (a joint venture of SK Chemicals and Teijin).\n\nThe following are examples of brand names by manufacturer and PPS type:\n\n\nPPS is one of the most important high temperature thermoplastic polymers because it exhibits a number of desirable properties. These properties include resistance to heat, acids, alkalies, mildew, bleaches, aging, sunlight, and abrasion. It absorbs only small amounts of solvents and resists dyeing.\n\nThe Federal Trade Commission definition for sulfur fiber is \"A manufactured fiber in which the fiber-forming substance is a long chain synthetic polysulfide in which at least 85% of the sulfide (—S—) linkages are attached directly to two (2) aromatic rings.\"\n\nThe PPS (polyphenylene sulfide) polymer is formed by reaction of sodium sulfide with p-dichlorobenzene:\nThe process for commercially producing PPS (Ryton) was initially developed by Dr. H. Wayne Hill Jr. and James T. Edmonds at Phillips Petroleum Company. NMP is used as the reaction solvent because it is stable at the high temperatures required for the synthesis and it dissolves both the sulfiding agent and the oligomeric intermediates.\n\nLinear, high-molecular-weight PPS that is capable of being extruded into film and melt spun into fiber was invented by Robert W. Campbell.\n\nThe first U.S. commercial sulfur fiber was produced in 1983 by Phillips Fibers Corporation, a subsidiary of Phillips 66 Company.\n"}
{"id": "2421159", "url": "https://en.wikipedia.org/wiki?curid=2421159", "title": "Printer Working Group", "text": "Printer Working Group\n\nThe Printer Working Group charter is to develop standards that make printers, operating systems and applications work better.\n\nIn 1991 a consortium of printer and network manufacturers (Insight Development, Intel, LAN Systems, Lexmark and Texas Instruments) formed the \"Network Printing Alliance\" (NPA). Later members included QMS, Kyocera, GENICOM, Okidata, Unisys, Canon, IBM, Kodak, Adaptec, Tektronix, Digital Products, Pennant Systems, Extended Systems and NEC.\n\nIn 1993, the NPA was reformed as the \"Printer Working Group\" (PWG) and added HP, Compaq, Microsoft, Xerox, Xircom, Farpoint Communications, Zenith, Castelle, Fujitsu, 3M, Cirrus Logic, Amp, National Semiconductor and Ricoh.\n\nIn September 1999, the IEEE formalized an alliance with PWG as part of the IEEE Industry Standards and Technology Organization (IEEE-ISTO).\n\nThe PWG has supported the development of:\n\nThe \"Network Printing Alliance Protocol\" (NPAP) was developed as a protocol for returning printer configuration and status via parallel, serial, network and later USB. In 1997, NPAP was approved as IEEE 1284.1 TIPSI. However, SNMP became the standard for network printer management and thus NPAP was never widely accepted. Lexmark appears to be the only manufacturer still supporting NPAP.\n\n"}
{"id": "2253990", "url": "https://en.wikipedia.org/wiki?curid=2253990", "title": "Renard series", "text": "Renard series\n\nRenard series are a system of preferred numbers dividing an interval from 1 to 10 into 5, 10, 20, or 40 steps. This set of preferred numbers was proposed in the 1877 by French army engineer Colonel Charles Renard. His system was adopted by the ISO in 1949 to form the ISO Recommendation R3, first published in 1953 or 1954, which evolved into the international standard ISO 3.\nRenard's system of preferred numbers divides the interval from 1 to 10 into 5, 10, 20, or 40 steps. The factor between two consecutive numbers in a Renard series is approximately constant (before rounding), namely the 5th, 10th, 20th, or 40th root of 10 (approximately 1.58, 1.26, 1.12, and 1.06, respectively), which leads to a geometric sequence. This way, the maximum relative error is minimized if an arbitrary number is replaced by the nearest Renard number multiplied by the appropriate power of 10. One application of the Renard series of numbers is to current rating of electric fuses.\n\nThe most basic R5 series consists of these five rounded numbers, which are powers of the fifth root of 10, rounded to two digits. Note that the Renard numbers are not always rounded to the closest three-digit number to the theoretical geometric sequence:\n\n\nIf a finer resolution is needed, another five numbers are added to the series, one after each of the original R5 numbers, and one ends up with the R10 series. These are rounded to a multiple of 0.05. Where an even finer grading is needed, the R20, R40, and R80 series can be applied. The R20 series is usually rounded to a multiple of 0.05, and the R40 and R80 values interpolate between the R20 values, rather than being powers of the 80th root of 10 rounded correctly. In the table below, the additional R80 values are written to the right of the R40 values in the column named \"R80 add'l\". The R40 numbers 3.00 and 6.00 are higher than they \"should\" be by interpolation, in order to give rounder numbers.\n\nIn some applications more rounded values are desirable, either because the numbers from the normal series would imply an unrealistically high accuracy, or because an integer value is needed (e.g., the number of teeth in a gear). For these needs, more rounded versions of the Renard series have been defined in ISO 3. In the table below, rounded values that differ from their less rounded counterparts are shown in bold.\n\nAs the Renard numbers repeat after every 10-fold change of the scale, they are particularly well-suited for use with SI units. It makes no difference whether the Renard numbers are used with metres or millimetres. But one would need to use an appropriate number base to avoid ending up with two incompatible sets of nicely spaced dimensions, if for instance they were applied with both inches and feet. In the case of inches and feet a root of 12 would be desirable, that is, where \"n\" is the desired number of divisions within the major step size of twelve. Similarly a base of two, eight, or sixteen would fit nicely with the binary units commonly found in computer science.\n\nEach of the Renard sequences can be reduced to a subset by taking every \"n\"th value in a series, which is designated by adding the number \"n\" after a slash. For example, \"R10″/3 (1…1000)\" designates a series consisting of every third value in the R″10 series from 1 to 1000, that is, 1, 2, 4, 8, 15, 30, 60, 120, 250, 500, 1000.\n\n\n"}
{"id": "22366849", "url": "https://en.wikipedia.org/wiki?curid=22366849", "title": "Rio Grande Project", "text": "Rio Grande Project\n\nThe Rio Grande Project is a United States Bureau of Reclamation irrigation, hydroelectricity, flood control, and interbasin water transfer project serving the upper Rio Grande basin in the southwestern United States. The project irrigates along the river in the states of New Mexico and Texas. Approximately 60 percent of this land is in New Mexico. Some water is also allotted to Mexico to irrigate some on the south side of the river. The project was authorized in 1905, but its final features were not implemented until the early 1950s.\n\nThe project consists of two large storage dams, 6 small diversion dams, two flood-control dams, of canals and their branches and of drainage channels and pipes. A small hydroelectric plant at one of the project's dams also supplies electricity to the region.\nThe first people to use the waters of the Rio Grande were the Pueblo Indians, who used simple irrigation systems that were noted by the Spanish in the 16th century while conducting expeditions from Mexico to North America. In the mid-19th century, American settlers began intensive irrigation development of the Rio Grande watershed. Small dikes, dams, canals, and other irrigation works were constructed along the Rio Grande and its tributaries. The river would take out some of these primitive structures in its annual floods, and a large, coordinated project would be needed to construct permanent replacements. However, investigations to begin this project did not begin until the early twentieth century.\n\nLike many rivers of the American Southwest, runoff in the Rio Grande basin is limited and varies widely from year to year. By the 1890s, water use in the upper basin was so great that the river's flow near El Paso, Texas, was reduced to a trickle in dry summers. To resolve these problems, plans were drafted up for a large storage dam at Elephant Butte, about downstream of Albuquerque, New Mexico. The Newlands Reclamation Act was passed in 1902, authorizing the Rio Grande Project as a Bureau of Reclamation undertaking. For the next two years, surveyors and engineers undertook a comprehensive feasibility study for the project's dams and reservoirs.\nThe first elements of the project to be built were the Leasburg Diversion Dam and about of supporting canal, begun in 1906 and finished in 1908. Elephant Butte Dam, the largest dam on the Rio Grande, was authorized by the United States Congress on February 15, 1905. Construction began in 1908, when groundworks were laid. Conflicts over the lands to be submerged under the future reservoir bogged down the project for a while, but work resumed in 1912 and the reservoir began to fill by 1915. The Franklin Canal was an existing 1890 canal purchased by the Bureau of Reclamation in 1912 and rebuilt from 1914 to 1915. The Mesilla and Percha Diversion Dams, East Side Canal, West Side Canal, Rincon Valley Canal, and an extension of the Leasburg Canal were built in the period between 1914 and 1919.\n\nIn the late 1910s, a problem developed with rising local groundwater levels caused by irrigation. In response, Reclamation began planning for the extensive drainage system of the Rio Grande Project in 1916. Contracts for the construction of these drainage systems, as well as distribution canals (laterals) were not awarded until the period from 1917 to 1918. Before 1929, the entire irrigation system would be overhauled. This involved repairing, rebuilding and extending old canals; and construction of new laterals. Work is still in progress, as agricultural development in the region continues to grow.\n\nThe last major components of the project were constructed from the 1930s to the early 1950s. Caballo Dam, the second major storage facility of the project located north of Truth or Consequences, New Mexico was built from 1936-1938. Caballo was built to provide flood protection for the projects downstream, stabilize outflows from Elephant Butte, and replace storage lost in Elephant Butte Reservoir due to sedimentation. With the benefit of flow regulation, a small hydroelectric plant was completed in 1940 at the base of Elephant Butte Dam. The construction of power transmission lines was begun in 1940, and was finally completed by 1952.\n\nThe Elephant Butte Irrigation District is a historic district providing recognition and limited protection for the history of much of the system, which was listed on the National Register of Historic Places in 1997. The listing included three contributing buildings and 214 contributing structures. Noted as historic are the diversion dams and the unlined irrigation canals; most of the mechanical fixtures in the system have been routinely replaced and are non-historic.\n\nThe Elephant Butte Dam (also referred to as Elephant Butte Dike) is the main storage facility for the Rio Grande Project. It is a long concrete gravity dam standing above the river and high from its foundations. The dam is thick at the base and tapers to about thick at the crest. The dam took of material to construct.\n\nThe full volume of Elephant Butte Reservoir is some , accounting for about 85% of the project's storage capacity. The outlet works of the dam can release , while the service spillway can release . \n\nThe reservoir and dam receive water from a catchment of , about 16% of the Rio Grande's total drainage area. The Elephant Butte hydroelectric station is a base load power plant that draws water from the reservoir and has a capacity of 27.95 megawatts.\n\nCaballo Dam is the second major storage dam of the Rio Grande Project, located about below Elephant Butte. The dam is high above the river, high from its foundations, and long. It forms the Caballo Reservoir, which can store up to of water. \n\nThe outlet works can release cubic feet per second, while the spillway has a capacity of per second. The dam has no power generation facilities, although it has been proposed that a small hydroelectric plant be installed at its base for local irrigation districts.\n\nPercha Diversion Dam lies downstream from and west of the Caballo Dam. It consists of a concrete overflow section flanked by earthen wing dikes totaling in length, standing high above the riverbed and above its foundations.\n. The dam diverts water into the Rincon Valley Main Canal, which is long and has a capacity of . Water from the canal irrigates of land in the Rincon Valley.\n\nLeasburg Diversion Dam is downstream and nearly identical in design to the Percha Diversion Dam. It is high above the river and high above its foundations. The dam and adjacent dikes total in length. The dam's spillway is a broad-crested weir about long with a capacity of . The dam diverts water into the Leasburg Canal, which irrigates of land in the upper Mesilla Valley. The canal has a capacity of per second.\n\nPichacho North and Pichacho South dams impound North Pichacho Arroyo and South Pichacho Arroyo, respectively, to provide flood protection for the Leasburg Canal. Both arroyos are ephemeral, and so the dams operate only during storm events. The dams were both built in the 1950s.\n\nThe Mesilla Diversion Dam is located about upstream of El Paso and consists of a gated overflow structure. The dam is high above the Rio Grande, high above its foundations, and measures long. The spillway has a capacity of . The dam diverts water into the East Side Canal and West Side Canal, which provide irrigation water to of land in the lower Mesilla Valley. The East Side Canal is long, and has a capacity of . The West Side Canal is larger at long, and has a capacity of . Near its end, the West Side Canal crosses underneath the Rio Grande via the Montoya Siphon.\n\nThe American Diversion Dam is a gated dam flanked by earthen dikes about northwest of El Paso and just above the Mexico – United States border. It is high above the riverbed, and from crest to foundation. The spillway is long and has a capacity of . The dam diverts water into the American Canal, which carries up to of water for to the beginning of the Franklin Canal. The Franklin Canal is long and takes water into the El Paso Valle, where it irrigates .\n\nRiverside Diversion Dam is the lowermost dam of the Rio Grande Project. The dam is above the streambed, above its foundations, and long. Its service spillway consists of six x radial gates, and an uncontrolled overflow weir serves as an emergency spillway. The Riverside Canal carries water to the El Paso Valley, and has a capacity of about . The Tornillo Canal, with a capacity of , branches off the Riverside Canal. Excess waters from the canals are diverted to irrigate about in Hudspeth County, Texas.\n\nThe Rio Grande Project furnishes irrigation water year-round to a long, narrow area of in the Rio Grande Valley in south-central New Mexico and western Texas. Crops grown in the region include grain, pecans, alfalfa, cotton, and many types of vegetables. Power generated at the Elephant Butte power plant is distributed through an electrical grid totaling of 115-kilovolt transmission lines and 11 substations. Originally built by Reclamation, the power grid remained under its ownership until 1977, when it was sold to a local company.\n\nCaballo and Elephant Butte reservoirs are both popular recreational areas. Elephant Butte Reservoir, with of water at full pool, is popular for swimming, boating, and fishing. Cabins, fishing tackle, and boat rental services are available at the reservoir. Downstream Caballo Reservoir, with an area of , is also a popular site for picnicking, fishing and boating. Elephant Butte Lake State Park and Caballo Lake State Park serve the two reservoirs, respectively.\n\nEven before the Rio Grande Project, the waters of the Rio Grande were already overtaxed by human development in the region. At the end of the 19th century, there were some 925 diversions of the river in the state of Colorado alone. In 1896, it was affirmed by the United States Geological Survey (USGS) that the river's flow was decreasing by annually. The river has run dry many times since the 1950s at Big Bend National Park. At El Paso, Texas, the river is non-existent for much of the year. Tributaries of the river, both on the Mexican and American sides, have been diverted heavily for irrigation. The Rio Grande is said to be \"one of the most stressed river basins in the world\". In 2001, the river failed to reach the Gulf of Mexico but instead ended from the shore behind a sandbar, \"not with a roar but with a whimper in the sand\".\nThe river's decreasing flow has posed problems for international security. In the past, the river was wide, deep and fast-flowing in its section through Texas, where it forms a large section of the Mexico – United States border. Illegal immigrants once had to swim across the river at the border, but with the river so low immigrants need only wade across for most of the year. Other than extensive diversions, exotic introduced, fast-growing and water-consuming plants, such as water hyacinth and hydrilla, are also leading to reduced flows. The United States government has recently attempted to slow or stop the progress of these weeds by introducing insects and fish that feed on the invasive plants.\n\n\n"}
{"id": "57079238", "url": "https://en.wikipedia.org/wiki?curid=57079238", "title": "SMART home technology", "text": "SMART home technology\n\nSMART home technology use devices connected to the Internet of things (IoT) to automate and monitor in-home systems. It stands for \"Self-Monitoring Analysis and Reporting Technology\". The technology was originally developed by IBM and was referred to as Predictive failure analysis. The first contemporary SMART home technology products became available to consumers between 1998 and the early 2000s. SMART home technology allows users to control and monitor their connected home devices from SMART home apps, smartphones, or other networked devices. Users can remotely control connected home systems whether they are home or away. This allows for more efficient energy and electric use as well as ensuring your home is secure. SMART home technology contributes to health and well-being enhancement by accommodating people with special needs, especially older people . SMART home technology is now being used to create \"SMART cities\". A Smart city functions similar to a SMART home, where systems are monitored to more efficiently run the cities and save money.\n\nSMART home technology devices can range in the following:\n\nAs of 2015, the most common piece of SMART home technology in the United States were wireless speaker systems with 17 percent of people having one or more. SMART thermostats were the second most prevalent piece of SMART home technology with 11 percent of people using the device. A 2012 consumer report that pulled data from the \"National Association of Home Builders\" looked for what SMART home devices homeowners wanted most and found that top five were wireless security systems (50%), programmable thermostats (47%), security cameras (40%), lighting control systems and wireless home audio systems (39%), and home theater and multi-zone HVAC systems (37%).\n\nSMART home technology systems were exploited in order to carry out the directed denial-of-service attack (DDoS) in October 2016. These devices, which are connected by the Internet of Things, have inherent risks of security breaches. Hackers targeted unsecured devices that includes SMART home technology, and infected them with malicious code to form a botnet and carry out the attack. A study estimates that at least 15 percent of home routers are unsecured with weak or default passwords. There are over 13 billion interconnected digital and electronic devices across the world; the October 2016 DDoS attack showed that a small percentage of vulnerable devices can have a devastating impact.\n"}
{"id": "8646876", "url": "https://en.wikipedia.org/wiki?curid=8646876", "title": "Screed", "text": "Screed\n\nScreed has three meanings in building construction: \n\nIn the US, screeding is the process a person called a concrete finisher performs by cutting off excess wet concrete to bring the top surface of a slab to the proper grade and smoothness. A power concrete screed has a gasoline motor attached which helps smooth and vibrate concrete as it is flattened. After the concrete is flattened it is smoothed with a concrete float or power trowel. A concrete floor is sometimes called a solid ground floor.\n\nA plasterer also may use a screed to level a wall or ceiling surface in plasterwork.\n\nThis sense of screed has been extended to asphalt paving where a free floating screed is part of a machine which spreads the pavement.\n\nA \"weep screed\" or \"sill screed\" is a screed rail which has drainage holes to allow moisture which penetrated an exterior plaster or stucco coating to drain through the screed.\n\nFlowing screeds are made from inert fillers such as sand, with a binder system based on cement or often calcium sulphate. Flow screeds are often preferred to traditional screeds as they are easier and faster to install and provide a similar finish. Flow screed is often used in combination with underfloor heating installation.\n\nLiquid flow screed is self-levelling. No vibration is necessary to remove bubbles and densify the liquid mass.\nDue to the easy consolidation thickness can sometimes be reduced in comparison to conventional screeds. This minimises heat storage leading to a floor that reacts quickly to user requirement hence raising the efficacy of underfloor heating.\n\nA development in the UK is the delivery, mixing, and pumping of screed from a single vehicle. Where previously screed jobs required a separate pump to administer the screed, these new machines can now administer the screed directly from the mixing pan to the floor at a range of up to 60 meters. For example, the material called granolithic.\n\n\n"}
{"id": "382619", "url": "https://en.wikipedia.org/wiki?curid=382619", "title": "TV dinner", "text": "TV dinner\n\nA TV dinner (also called prepackaged meal, ready-made meal, ready meal, frozen dinner, frozen meal and microwave meal) is a packaged frozen meal that usually comes portioned for an individual, but may also be a single dish intended to be shared. It requires very little preparation and may contain a number of separate elements that comprise a single-serving meal.\n\nA TV dinner in the United States usually consists of a type of meat for the main course, and sometimes vegetables, potatoes, and/or a dessert. The main dish can also be pasta or fish. In European TV dinners, Indian and Chinese meals are common.\n\nThe term \"TV dinner\" was first used as part of a brand of packaged meals developed in 1953 by the company C.A. Swanson & Sons (the name in full was \"TV Brand Frozen Dinner\"). The original \"TV Dinner\" came in an aluminum tray and was heated in an oven. In the United States, the term is synonymous with any prepackaged meal or dish (\"dinner\") purchased frozen in a supermarket and heated at home.\n\nNow, most frozen food trays are made of a microwaveable and disposable material, usually plastic.\n\nSeveral smaller companies had conceived of frozen dinners earlier (see Invention section below), but the first to achieve success was Swanson. The first Swanson-brand TV Dinner was produced in the United States and consisted of a Thanksgiving meal of turkey, cornbread dressing, frozen peas and sweet potatoes packaged in a tray like those used at the time for airline food service. Each item was placed in its own compartment. The trays proved to be useful: the entire dinner could be removed from the outer packaging as a unit, the tray with its aluminum foil covering could be heated directly in the oven without any extra dishes, and one could eat the meal directly from the tray. The product was cooked for 25 minutes at and fit nicely on a TV tray table. The original TV Dinner sold for 98 cents, and had a production estimate of 5,000 dinners for the first year.\n\nThe name \"TV dinner\" was coined by Gerry Thomas, its inventor. At the time it was introduced, televisions were status symbols and a growing medium. Thomas thought the name \"TV Dinner\" sounded like the product was made for convenience (which it was), and the Swanson executives agreed.\n\nMuch has changed since the first TV Dinners were marketed. For instance, a wider variety of main courses – such as fried chicken, spaghetti, Salisbury steak and Mexican combinations – have been introduced. Competitors such as Banquet and Morton began offering prepackaged frozen dinners at a lower price than Swanson. Other changes include:\n\nModern-day frozen dinners tend to come in microwave-safe containers. Product lines also tend to offer a larger variety of dinner types. These dinners, also known as microwave meals, can be purchased at most supermarkets. They are stored frozen. To prepare them, the plastic cover is removed or vented, and the meal is heated in a microwave oven for a few minutes. They are convenient since they essentially require no preparation time other than the heating, although some frozen dinners may require the preparer to briefly carry out an intermediate step (such as stirring mashed potatoes midway through the heating cycle) to ensure adequate heating and uniform consistency of component items.\n\nIn the United Kingdom, prepared frozen meals first became widely available in the late 1970s. Since then they have steadily grown in popularity with the increased ownership of home freezers and microwave ovens. Demographic trends such as the growth of smaller households have also influenced the sale of this and other types of convenience food. In 2003, the United Kingdom spent £5 million a day on ready meals, and was the largest consumer in Europe.\n\nUnfrozen pre-cooked ready meals, which are merely chilled and require less time to reheat, are also popular and are sold by most supermarkets. Chilled ready meals are intended for immediate reheating and consumption. Although most can be frozen by the consumer after purchase, they can either be heated from frozen or may have to be fully defrosted before reheating.\n\nMany different varieties of frozen and chilled ready meals are now generally available in the UK, including \"gourmet\" recipes, organic and vegetarian dishes, traditional British and foreign cuisine, and smaller children's meals.\n\nThe identity of the TV Dinner's inventor has been disputed. In one account, first publicized in 1996, retired Swanson executive Gerry Thomas said he conceived the idea after the company found itself with a huge surplus of frozen turkeys because of poor Thanksgiving sales. Thomas' version of events has been challenged by the \"Los Angeles Times\", members of the Swanson family and former Swanson employees. They credit the Swanson brothers with the invention.\n\nSwanson's concept was not original. In 1944, William L. Maxson's frozen dinners were being served on airplanes. Other prepackaged meals were also marketed before Swanson's TV Dinner. In 1948, plain frozen fruits and vegetables were joined by what were then called 'dinner plates' with a main course, potato, and vegetable. In 1952 the first frozen dinners on oven-ready aluminum trays were introduced by Quaker States Foods under the One-Eye Eskimo label. Quaker States Foods was joined by other companies including Frigi-Dinner, which offered such fare as beef stew with corn and peas, veal goulash with peas and potatoes, and chicken chow mein with egg rolls and fried rice. Swanson, a large producer of canned and frozen poultry in Omaha, Nebraska, was able to promote the widespread sales and adaptation of frozen dinner by using its nationally recognized brand name with an extensive national marketing campaign nicknamed \"Operation Smash\" and the clever advertising name of \"TV Dinner,\" which tapped into the public's excitement around the new device.\n\nThe production process of TV dinners is highly automated and undergoes three major steps. Those steps are food preparation, tray loading, and freezing. During food preparation, vegetables and fruits are usually placed on a movable belt and washed, then are placed into a container to be steamed or boiled for 1–3 minutes. This process is referred to as blanching, and is used as a method to destroy enzymes in the food that can cause chemical changes negatively affecting overall flavor and color of the fruit and vegetables. As for meats, prior to cooking, they are trimmed of fat and cut into proper sizes. The fish is usually cleaned and cut into fillets, and poultry is usually washed thoroughly and dressed. Meats are then seasoned, placed on trays, and are cooked in an oven for a predetermined amount of time. After all the food is ready to be packaged, it is sent to the filling lines. The food is placed in its compartments as the trays pass under numerous filling machines; to ensure that every packaged dinner gets an equal amount of food, the filling devices are strictly regulated.\n\nThe food undergoes a process of cryogenic freezing with liquid nitrogen. After the food is placed on the conveyor belt, it is sprayed with liquid nitrogen that boils on contact with the freezing food. This method of flash-freezing fresh foods is used to retain natural quality of the food. When the food is chilled through cryogenic freezing, small ice crystals are formed throughout the food that, in theory, can preserve the food indefinitely if stored safely. Cryogenic freezing is widely used as it is a method for rapid freezing, requires almost no dehydration, excludes oxygen thus decreasing oxidative spoilage, and causes less damage to individual freezing pieces. Due to the fact that the cost of operating cryogenic freezing is high, it is commonly used for high value food products such as TV dinners, which is a $4.5 billion industry a year that is continuing to grow with the constant introduction of new technology.\n\nFollowing this, the dinners are either covered with aluminum foil or paper, and the product is tightly packed with a partial vacuum created to ensure no evaporation takes place that can cause the food to dry out. Then the packaged dinners are placed in a refrigerated storage facility, transported by refrigerated truck, and stored in the grocer's freezer. TV dinners prepared with the aforementioned steps -- that is, frozen and packaged properly -- can remain in near-perfect condition for a long time, so long as they are stored at -18 °C during shipping and storage.\n\nThe freezing process tends to degrade the taste of food and the meals are thus heavily processed with extra salt and fat to compensate. In addition, stabilizing the product for a long period typically means that companies will use partially hydrogenated vegetable oils for some items (typically dessert). Partially hydrogenated vegetable oils are high in trans fats and are shown to adversely affect cardiovascular health. The dinners are almost always significantly less nutritious than fresh food and are formulated to remain edible after long periods of storage, thus often requiring preservatives such as butylated hydroxytoluene. There is, however, some variability between brands.\n\nIn recent years there has been a push by a number of independent manufacturers and retailers to make meals that are low in salt and fat and free of artificial additives. In the UK, most British supermarkets also produce their own \"healthy eating\" brands. Nearly all chilled or frozen ready meals sold in the UK are now clearly labeled with the salt, sugar and fat content and the recommended daily intake. Concern about obesity and government publicity initiatives such as those by the Food Standards Agency and the National Health Service have encouraged manufacturers to reduce the levels of salt and fat, but curiously not industrial carbohydrates, in ready prepared food.\n\nMore recently, frozen dinners have been created that are designed to be used with a steamer, allowing rapid cooking of essentially raw ingredients (typically fish and vegetables) immediately before consumption.\n\n\n"}
{"id": "22557991", "url": "https://en.wikipedia.org/wiki?curid=22557991", "title": "TechniSat", "text": "TechniSat\n\nTechniSat is a German manufacturer of direct broadcast satellite receivers. The company produces televisions, car navigation and entertainment systems as well. Together with Loewe and Metz, it is one of the few remaining independent consumer electronics companies which develop and produce consumer electronics products in Germany and Europe.\n\nTechniSat was established in 1987 in Daun by Peter Lepper. In 1992, the company launched then the smallest digital antenna in the world. In 2007, it started its own TechniTipp-TV channel, which was closed down in September 2008. \n\nIn 2009, TechniSat had a revenue of nearly 350 million Euro (up from 305 million in 2008).\n"}
{"id": "73273", "url": "https://en.wikipedia.org/wiki?curid=73273", "title": "Waterloo, Ontario", "text": "Waterloo, Ontario\n\nWaterloo is a city in Ontario, Canada. It is the smallest of three cities in the Regional Municipality of Waterloo (and previously in Waterloo County, Ontario), and is adjacent to the city of Kitchener.\n\nKitchener and Waterloo are often jointly referred to as \"Kitchener–Waterloo\", \"KW\", or the \"Twin Cities\" (when the reference includes the nearby city of Cambridge, Ontario, the term \"Tri-Cities\" or \"the Tri-City\" are used). While there were several unsuccessful attempts to combine the cities of Kitchener and Waterloo, following the 1973 establishment of the Region of Waterloo there was less motivation to do so. At the time of the 2016 census, the population of Waterloo was 104,986.\n\nWaterloo started on land that was part of a parcel of assigned in 1784 to the Iroquois alliance that made up the League of Six Nations. The rare gift of land from Britain to indigenous people took place to compensate for wartime alliance during the American Revolution. Almost immediately—and with much controversy—the native groups began to sell some of the land. Between 1796 and 1798, were sold through a Crown Grant to Richard Beasley, with the Six Nations Indians continuing to hold the mortgage on the lands.\n\nThe first wave of immigrants to the area comprised Mennonites from Pennsylvania. They bought deeds to land parcels from Beasley and began moving into the area in 1804. The following year, a group of 26 Mennonites pooled resources to purchase all of the unsold land from Beasley and to discharge the mortgage held by the Six Nations Indians.\n\nMany of the pioneers arriving from Pennsylvania after November 1803 bought land in a 60,000 acre section of Block Two from the German Company which had been established by a group of Mennonites from Lancaster County, Pennsylvania. The Tract included most of Block 2 of the previous Grand River Indian Lands. Many of the first farms were least four hundred acres in size. The German Company, represented by Daniel Erb and Samuel Bricker, had acquired the land from previous owner Richard Beasley; he had gotten into financial difficulties after buying the land in 1796 from Joseph Brant who represented the Six Nations. The payment to Beasly, in cash, arrived from Pennsylvania in kegs, carried in a wagon surrounded by armed guards.\n\nThe Mennonites divided the land into smaller lots; two lots owned by Abraham Erb became the central core of Waterloo. Erb, often called the founder of Waterloo, had come to the area in 1806 from Franklin County, Pennsylvania. He bought 900 acres of bush land in 1806 from the German Company and founded a sawmill (1808) and grist mill (1816); these the focal point of the area. The grist mill operated continuously for 111 years. Other early settlers of what would become Waterloo included Samuel and Elia Schneider who arrived in 1816. Until about 1820, settlements such as this were quite small.\n\nIn 1816 the new township was named after Waterloo, Belgium, the site of the Battle of Waterloo (1815), which had ended the Napoleonic Wars in Europe. After that war, the new township became a popular destination for German immigrants. By the 1840s, German settlers had overtaken the Mennonites as the dominant segment of the population. Many Germans settled in the small hamlet to the southeast of Waterloo. In their honour, the village was named Berlin in 1833 (renamed to Kitchener in 1916).\n\nBy 1831, Waterloo had a small post office in the King and Erb Street area, operated by Daniel Snyder, some 11 years before one would open in neighbouring Berlin. The \"Smith's Canadian Gazetteer\" of 1846 states that the Township of Waterloo (smaller than Waterloo County) consisted primarily of Pennsylvanian Mennonites and immigrants directly from Germany who had brought money with them. At the time, many did not speak English. There were eight grist and twenty saw mills in the township. In 1841, the population count was 4424. In 1846 the village of Waterloo had a population of 200, \"mostly Germans\". There was a grist mill and a sawmill and some tradesmen. By comparison, Berlin (Kitchener) had a population of about 400, also \"mostly German\", and more tradesmen than the village of Waterloo.\"\n\nBerlin was chosen as the site of the seat for the County of Waterloo in 1853. By 1869, the population was 2000. Waterloo was incorporated as a village in 1857 and became the Town of Waterloo in 1876 and the City of Waterloo in 1948.\n\nIn 2016, a corduroy road was unearthed in the King St. area of the business district; a second section was discovered near the Conestoga mall. The road was probably built by Mennonites using technology acquired in Lancaster County Pennsylvania, between the late 1790s and 1816. The log road was buried in about 1840 and a new road built on top of it.\n\nWaterloo's city centre is located near the intersection of King and Erb streets. Since 1961, the centrepiece has been the Waterloo Town Square shopping centre, which underwent a thorough renovation in 2006. Much of the mall was torn down and has been replaced by buildings that emphasize street-facing storefronts.\n\nResidents refer to the Waterloo city centre as \"uptown\" (often capitalized), while \"downtown\" is reserved for the Kitchener city centre, as Kitchener had been the dominant centre, and Waterloo was a small town on the Kitchener's north side. Waterloo surged into a significant City in the third-quarter of the 20th Century, due in large part to its role as a university city. It has also benefited with the growth of Insurance companies. Waterloo has prospered with the relationship between the Tech Sector, which has blossomed, and the University of Waterloo whose technology graduates have excelled. Blackberry, formerly Research In Motion, is the best example.\n\nThe city centre was once along Albert Street, near the Marsland Centre and the Waterloo Public Library. The town hall, fire hall, and farmers' market were located there. Amidst some controversy, all were demolished between 1965 and 1969.\n\nThere are five main parks in the city:\n\nThe Grand River flows southward along the east side of the city. Its most significant tributary within the city is Laurel Creek, whose source lies just to the west of the city limits and its mouth just to the east, and crosses much of the city's central areas including the University of Waterloo lands and Waterloo Park; it flows under the uptown area in a culvert. In the west end of the city, the Waterloo Moraine provides over 300,000 people in the region with drinking water. Much of the gently hilly Waterloo Moraine underlies existing developed areas. Ongoing urban growth, mostly in the form of low-density residential suburbs (in accordance with requests by land developers), will cover increasing amounts of the remaining undeveloped portions of the Waterloo Moraine.\n\nThe Climate of Waterloo is Humid continental (Köppen) (\"Dfb\")\nWaterloo has a humid continental climate of the warm summer subtype (\"Dfb\" under the Köppen climate classification); this means that there are large seasonal differences, usually very warm to hot (and humid) summers and cold (to very cold) winters. Compared to the rest of Canada, it has moderate weather. Winter temperatures generally last from the middle of December until the middle of March, while summer temperatures generally occur between the middle of May to close to the end of September. Temperatures can exceed 30℃ (86℉) several times a year. Waterloo has approximately 140 frost-free days per year.\n\nMany locals are of ethnic German descent. There is also a strong Mennonite presence. The universities and colleges along with its thriving technology and electronics presence attract a large number of individuals from elsewhere in Canada and the world.\n\nThe population according to the 2016 Canadian Census is 104,986 (Sources other than the census may indicate higher numbers due to treatment of the student population.)\n\nAccording to the Canada 2011 Census, the population of Waterloo was 98,780, a 1.3% increase from 2006. The population density was 1,542.9 people per square km. The median age was 37.6 years old, lower than the national median age at 40.6 years old. There are 42,984 private dwellings with an occupancy rate of 87.3%. According to the 2011 National Household Survey, the median value of a dwelling in Waterloo is $324,837 which is a bit higher than the national average at $280,552. The median household income (after-taxes) in Waterloo is $67,150, fairly higher than the national average at $54,089.\n\n\nFrom the 2001 census data, excluding post-secondary students temporarily residing in Waterloo:\n\nWaterloo has a strong knowledge- and service-based economy with significant insurance and high-tech sectors as well as two universities. The city's largest employers are Sun Life Financial, the University of Waterloo, Manulife Financial, BlackBerry, Sandvine and Wilfrid Laurier University.\n\nThe city is also home to three well-known think tanks – the Perimeter Institute for Theoretical Physics, an advanced centre for the study of foundational, theoretical physics and award-winning educational outreach in science; the Institute for Quantum Computing, based at the University of Waterloo, which carries out innovative research in computer, engineering, mathematical and physical sciences; and the Centre for International Governance Innovation, an independent, nonpartisan think tank that addresses international governance challenges.\n\nThe city is part of Canada's Technology Triangle (CTT), a joint economic development initiative of Waterloo, Kitchener, Cambridge and the Region of Waterloo that markets the region internationally. Despite its name, CTT does not focus exclusively on promoting technology industries, but on all aspects of economic development.\n\nWaterloo has a strong technology sector with hundreds of high-tech firms. The dominant technology company in the city is BlackBerry, makers of the BlackBerry, which has its headquarters in the city and owns several office buildings near the University of Waterloo's main campus.\n\nNotable Waterloo-based high-tech companies include:\n\nMany other high-tech companies, with headquarters elsewhere, take advantage of the concentration of high-tech employees in the Waterloo area, and have research and development centres there. Shopify, SAP, Google, Oracle, Intel, McAfee, NCR Corporation, Electronic Arts and Agfa are among the large, international technology companies with development offices in Waterloo.\n\nBefore it became known for technology, Waterloo was sometimes referred to as \"the Hartford of Canada\" because of the many insurance companies based in the area. Manulife, Sun Life Financial, Equitable Life of Canada and Economical Insurance have a significant presence in the city.\n\nBreweries and distilleries had been a significant industry in the Waterloo area until 1993 when a Labatt-owned brewery was shut down. Now the only major brewery is the Brick Brewing Company. Waterloo was the original home of distiller Seagram (also home town of many descendants of J.P. Seagram), which closed its Waterloo plant in 1992. Of the remaining Seagram buildings, one became home of the Centre for International Governance Innovation (CIGI), while others were converted into condominiums.\n\nThe city encourages location filming of movies and TV series and many have taken advantage of Waterloo locations. Recent titles include \"Downsizing\" (released in 2017), \"The Demolisher\" (2015) and \"Degrassi: The Next Generation\" (2015).\n\n\n\nWaterloo is home to several notable tourist attractions and areas of interest. These include:\n\n\nOther nearby attractions include:\n\nThere are two lawn bowling clubs serving Waterloo: Heritage Greens LBC and Kitchener LBC, which both function as part of District 7 of the Ontario Lawn Bowling Association. Both clubs offer programs for all ages.\n\nThere are three sports teams and two university varsity teams.\n\nBoth play at Ontario University Athletics of Canadian Interuniversity Sport.\n\nWaterloo City Council consists of seven councillors, each representing a ward, and a mayor. The number of wards expanded from five to seven in the November 2006 elections. The current mayor of Waterloo is Dave Jaworsky, who was elected in October 2014. The current Waterloo City Council is constituted as follows:\nPast and present city councils have been committed to providing for the explosive population growth that is coming with the local economic boom. Rapidly developing subdivisions are often described by their critics as urban sprawl that threatens environmentally sensitive areas and valuable agricultural land.\n\nWaterloo was part of Waterloo County, Ontario until 1973 when a restructuring created the Regional Municipality of Waterloo, (often referred to as Waterloo Region or the Region of Waterloo), which consists of the cities of Waterloo, Kitchener, and Cambridge, and the townships of Woolwich, Wilmot, Wellesley, and North Dumfries. The Region handles many services, including paramedic services, policing, waste management, recreation, planning, roads and social services.\n\nIn federal politics, the city of Waterloo is located entirely within the electoral district of the same name. In provincial politics, the city is contained within the Kitchener–Waterloo electoral district.\n\nThe Waterloo Award, established in 1997, is the highest civic honour a person can receive from the City of Waterloo.\n\nThe Conestoga Parkway, numbered as Highway 86 within Waterloo, connects Waterloo with Kitchener, Highway 7/8 (the continuation of the Conestoga Parkway), Ontario Highway 8, Highway 401 and Cambridge south of Highway 401.\n\nWaterloo shares several of its north-south arterial roads with neighbouring Kitchener. They include (from east to west) Bridge Street, Weber Street, King Street, Westmount Road, Fischer-Hallman Road, and Ira Needles Boulevard. Regina Street (located between Weber and King Streets) and Albert Street (located between King Street and Westmount Road) are north-south collector roads located entirely within Waterloo.\n\nThe city's east-west thoroughfares are almost entirely located within city limits, with the exception of Union Street, which has a small section in Kitchener, and Bridgeport Road which has its eastern end in the Bridgeport area of Kitchener. Waterloo's major east-west arterial roads are (from south to north) Union Street, Erb Street, Bridgeport Road, University Avenue, Columbia Street, and Northfield Drive.\n\nThere are numerous bicycle pathways. The Iron Horse Trail, which originates in Kitchener, enters Uptown Waterloo and links with the Laurel Trail that extends into the northern part of the city.\n\nAs of early 2017, construction of Phase 1 of the Region of Waterloo's light rail project was about 90 per cent complete although other major road reconstruction projects scheduled for 2017 and 2018 will bring new detours.\n\nPublic transport throughout Waterloo Region is provided by Grand River Transit, created by a merger of Kitchener Transit (which served Waterloo) and Cambridge Transit in January 2000. GRT operates a number of bus routes in Waterloo, with many running into Kitchener. In September 2005 an express bus route called iXpress was added for runs from downtown Cambridge to Fairview Park Mall in south Kitchener to Conestoga Mall in north Waterloo. The ION light rail system is currently under construction and will supplement and integrate with the bus network upon completion.\n\nThe Galt, Preston and Hespeler electric railway (later called the Grand River Railway) began to operate in 1894 connecting Preston and Galt. In 1911, the line reached Hespeler, Kitchener (then called Berlin) and Waterloo; by 1916 it had been extended to Brantford/Port Dover. The electric rail system ended passenger services in April, 1955, leaving the city and region with no local rail services for more than 60 years.\n\nIn June 2011, Waterloo Region council confirmed approval of the plan for a light rail transit line between Conestoga Mall in north Waterloo and Fairview Park Mall in south Kitchener, with rapid buses through to the \"downtown Galt\" area of Cambridge. In this Stage 1, the Ion rapid transit train will run through the downtown/uptown areas of Kitchener and Waterloo.\n\nConstruction on the light rail system began in August 2014 and the Stage 1 service was expected to begin in 2017. Most of the rails had been installed by the end of 2016; the \nmaintenance facility and all underground utility work had been completed. The start date of service was postponed to early 2018, however, because of delays in the manufacture and delivery of the vehicles by Bombardier Transportation. As of 24 February 2017, only a single sample of a train car had arrived for testing.\n\nUntil light rail transit is extended to the \"downtown Galt\" area of Cambridge from Kitchener in Stage 2, rapid transit will be provided by bus; adapted iXpress buses will run between Fairview Park Mall and the Ainslie Street Transit Terminal. In late February 2017, plans for the Stage 2 (Cambridge section) of the Ion rail service were still in the very early stage; public consultations were just getting started at the time.\n\nWaterloo is not currently served by any regularly scheduled passenger rail service. Via Rail trains between Sarnia and Toronto stop at the nearby Kitchener railway station southeast of uptown Waterloo at the corner of Victoria Street and Weber Street. The station is accessible by local buses via Kitchener's downtown Charles Street transit terminal. A tourist train that previously ran out of Waterloo Station was moved to depart from St. Jacobs Farmers' Market when construction began on the ION Light Rail line.\n\nThe nearest GO Transit railway station is Kitchener GO Station, as the Kitchener Line (formerly the Georgetown Line) has extended to Kitchener on December 19, 2011. In addition, Waterloo is served by GO buses which stop at the University of Waterloo and Wilfrid Laurier University, with destinations of Square One City Centre Terminal, Milton GO Station, and York University.\n\nIn May 2007, city council gave approval for a non-profit tourist train to run between Waterloo Station and St. Jacobs, reviving the route of the Waterloo-St. Jacobs Railway from the late 1990s. In 2015, the railway lost regular running rights south of Northfield Drive to make way for the Ion rapid transit project. All Market Train service now departs from the St. Jacobs Farmers Market. The Waterloo Central Railway are run on trains at 10am, 12pm, and 2pm from April to November. The Waterloo Station continues to operate as a Visitor & Heritage Information Centre and is located at 10 Father David Bauer Drive.\n\nIn addition to GO bus services mentioned above, Coach Canada offers service from the Charles St. Terminal in Kitchener to McMaster University and Hamilton, Ontario, with various stops, including Sheffield, Rockton and Dundas in between. Greyhound buses to Toronto Bus Terminal also service Waterloo at a stop on the University of Waterloo campus, with some trips operating via Guelph.\n\nThe closest airport to Waterloo is the Region of Waterloo International Airport in nearby Breslau, but while it is a thriving general-aviation field, it is not heavily served by scheduled airlines. Most air travelers use Toronto's Lester B. Pearson International Airport or John C. Munro Hamilton International Airport. WestJet has scheduled daily non-stop service to Calgary from Waterloo International Airport using Boeing 737-700 aircraft in winter season and larger Boeing 737-800 aircraft in spring and summer season. They started service out of Region of Waterloo International Airport on May 14, 2007, for the summer season and then decided to fly year-round due to strong passenger demand. During the winter months Sunwing Airlines offers service to Dominican Republic. Recent upgrades to the runways, approach lighting and terminal building have permitted larger aircraft to use this airport. Airlines that no longer serve the airport include Trillium and Bearskin (to Ottawa), Mesaba (Northwest Airlines' feeder to Detroit), American Airlines (to Chicago) and Sky Service (to sun destinations).\n\nThe hospitals serving Waterloo are all located in Kitchener. There is Grand River Hospital, which includes the K-W and Freeport health centres (formerly independent hospitals that amalgamated in April 1995), and St. Mary's General Hospital.\n\nThe Intelligent Community Forum named Waterloo the Top Intelligent Community of 2007.\n\nUntil the 1960s, with a few minor exceptions, Waterloo students would attend high school in Berlin/Kitchener. In 1914, Waterloo Lutheran Seminary added a high school department, named the College School, primarily to provide secondary education for prospective seminary students. The College School was discontinued in 1929. Between 1940 and 1950, due to overcrowding in Kitchener–Waterloo Collegiate and Vocational School, some grade nine classes were housed in Elizabeth Ziegler Public School.\n\nStarting in the 1960s, several high schools opened in Waterloo. In 1958 it was announced that Waterloo would have its own secondary school. A $1,247,268 school was built on a 20-acre (81,000 m) site on Hazel Street. Waterloo Collegiate Institute opened on September 6, 1960. In 1968, Laurel Vocational School (later University Heights Secondary School) opened, and in 1972 Waterloo's third public high school, Bluevale Collegiate Institute, opened. In 1965, St. David Senior School, which served grades 7–10, opened in the north of the city. St. David was turned into a high school in 1985 and was renamed St. David Catholic Secondary School. University Heights Secondary School closed in 2004 and Sir John A. Macdonald Secondary School opened that same year.\n\nAs of 2007, there were five high schools based in Waterloo. Three are operated by the Waterloo Region District School Board: Bluevale Collegiate Institute (east), Sir John A. Macdonald Secondary School (west), and Waterloo Collegiate Institute (central). two are operated by the Waterloo Catholic District School Board: St. David Catholic Secondary School and Resurrection Catholic Secondary School.\n\nThe main campuses of the University of Waterloo and Wilfrid Laurier University are located in Waterloo. This includes the many associated universities and colleges, including St. Jerome's University, St. Paul's University College, Conrad Grebel University College, Renison University College and the Balsillie School of International Affairs. Kitchener-based Conestoga College also has a Waterloo campus, located at the former University Heights Secondary School on University Avenue near Weber Street. Conestoga purchased the building in January 2006 for nearly $6 million from the Waterloo Region District School Board. It is double the size of its previous Waterloo campus on King Street, which was sold after the University Heights building was acquired.\n\n"}
{"id": "39993572", "url": "https://en.wikipedia.org/wiki?curid=39993572", "title": "Wayward Pines", "text": "Wayward Pines\n\nWayward Pines is an American mystery, science fiction television series based on the \"Wayward Pines\" novels by Blake Crouch. Developed for television by Chad Hodge, the pilot was directed by M. Night Shyamalan, with both as executive producers. The series premiered on Fox on May 14, 2015, and the first season concluded on July 23, 2015.\n\nOn December 9, 2015, Fox renewed the series for a second season which aired from May 25 to July 27, 2016. Since the second–season finale, the series is unlikely to return for a third season, as no official announcements have been made by Fox about a renewal or further production.\n\nIn the first season, Ethan Burke (Matt Dillon) is a U.S. Secret Service agent investigating the disappearance of two fellow agents in the mysterious small town of Wayward Pines, Idaho. Ethan awakens from a car accident unable to contact the outside world and unable to leave. He finds one of the agents dead and the other, his former lover Kate Hewson (Carla Gugino), settled down in the seemingly idyllic town. But the inhabitants of Wayward Pines are trapped there by an electrified fence and set of rules enforced by the strict Sheriff Arnold Pope (Terrence Howard). Any attempt to escape is punished by a public execution known as a \"reckoning\". Ethan reconnects with his wife and son while working to discover the truth.\n\nIn the second season, Dr. Theo Yedlin (Jason Patric) is a surgeon who is caught in the battle between Jason Higgins (Tom Stevens), leader of the First Generation that took over Wayward Pines following the Abbie incident, and the underground rebels led by Ethan's son Ben Burke (Charlie Tahan).\n\n\n\"Wayward Pines\" is executive produced by Chad Hodge with M. Night Shyamalan, Donald De Line and Ashwin Rajan. The pilot episode was written by Hodge and directed by Shyamalan. The series was officially picked up on May 13, 2013, with a ten-episode order. Filming took place between August 19, 2013, and February 14, 2014, in Burnaby (interiors) and Agassiz (exteriors), in British Columbia.\n\nThe plot of Crouch's first novel in the trilogy, \"Pines\" (2012), is covered over the first five episodes of the TV series. The second and third novels, \"Wayward\" (2013) and \"The Last Town\" (2014), make up the remaining five episodes. After reading the source material Shyamalan said of the project, \"As long as everybody isn't dead, I'm in\", his \"only rule\" to secure his participation. He noted that the TV series varies from the books in some ways, but as Crouch was still writing the novels while the show was in development, there was \"all kinds of cross pollinating\" between the two. In June 2015, \"Deadline Hollywood\" reported that Fox was considering a second season based on the series' impressive ratings. Though Hodge asserted that, from a creative standpoint, \"\"Wayward Pines\" was always designed to be just these 10 episodes\" in concert with the plotline of the books, he allowed for the possibility of another season. He said that in the finale, viewers would \"see a window to that, but it also is a complete ending as it is\".\n\nWith the tenth episode having been billed as the \"series finale\", the show was effectively finished. However, on December 9, 2015, Fox renewed \"Wayward Pines\" for a second season, to premiere in mid-2016. After the conclusion of the first season, Chad Hodge stepped down from his position as showrunner and executive producer for the series. Mark Friedman succeeded Hodge as showrunner for season two. Season two has a largely new main cast, with several lead actors from season one either not returning at all or appearing only as recurring characters in the second season. This is in part because it took so long for season one to make it to air, a few actors notably Terrence Howard and Juliette Lewis had committed to other series in the interim. Also, the surprise decision to renew the series for season two occurred after several more actors had moved onto other projects.\n\nOn May 12, 2014, Fox announced that \"Wayward Pines\" would premiere in 2015 as mid-season replacement. The series was picked up for broadcast by Fox in the United Kingdom, and by FX in Australia from May 14, 2015, where the premiere was the second most watched program on subscription television with 101,000 viewers.\n\nFox made the pilot available on demand and through various online outlets from April 23 to 30, 2015, in what the network called \"the first-ever global preview event\". The series subsequently debuted on May 14, 2015, simultaneously in more than 126 countries in what Fox called \"the world's largest day-and-date launch for a scripted series ever\". Due to time zones, episodes of the first season aired first in Australia.\n\nA digital companion series, also produced by Fox, airs alongside the weekly episodes of \"Wayward Pines\", titled \"Gone\" and written and directed by Christopher Leone. The series follows Eric Barlow, a rocket scientist, as he searches for his missing wife Sarah, a journalist, after she leaves a goodbye message and leaves him. He finds himself led toward a mountain in Idaho with the help of Sarah's colleague Elena, where he finds a man involved in the construction of Wayward Pines. Arriving at the site, a technician leads him to Sarah who is frozen in a prototype chamber, but it is really him they were after.\n\n\"Wayward Pines\" has received mostly favorable reviews from critics. On Rotten Tomatoes, the first season holds a rating of 79%, based on 63 reviews. The site's critical consensus reads, \"Creepy and strange in the best way possible, \"Wayward Pines\" is a welcome return to form for M. Night Shyamalan.\" The fifth episode, \"The Truth\", received an individual rating of 83%, the only episode of the series to do so. On Metacritic, the first season has a score of 66 out of 100, based on 34 critics for season 1 \"generally favorable reviews\". \n\nThe second season however, received mixed reception from critics with a score of 46 out of 100, based on 9 critics for season 2.\n\n"}
{"id": "1029051", "url": "https://en.wikipedia.org/wiki?curid=1029051", "title": "Worst-case execution time", "text": "Worst-case execution time\n\nThe worst-case execution time (WCET) of a computational task is the maximum length of time the task could take to execute on a specific hardware platform.\n\nWorst case execution time is typically used in reliable real-time systems, where understanding the worst case timing behaviour of software is important for reliability or correct functional behaviour. \n\nAs an example, a computer system that controls the behaviour of an engine in a vehicle might need to respond to inputs within a specific amount of time. One component that makes up the response time is the time spent executing the software – hence if the software worst case execution time can be determined, then the designer of the system can use this with other techniques such as schedulability analysis to ensure that the system responds fast enough.\n\nWhile WCET is potentially applicable to many real-time systems, in practice an assurance of WCET is mainly used by real-time systems that are related to high reliability or safety. For example, in airborne software some attention to software is required by DO178B section 6.3.4. The increasing use of software in automotive systems is also driving the need to use WCET analysis of software.\n\nIn the design of some systems, WCET is often used as an input to schedulability analysis, although a much more common use of WCET in critical systems is to ensure that the pre-allocated timing budgets in a partition-scheduled system such as ARINC 653 are not violated.\n\nSince the early days of embedded computing, embedded software developers have either used:\n\nBoth of these techniques have limitations. End to end measurements place a high burden on software testing to achieve the longest path; counting instructions is only applicable to simple software and hardware. In both cases, a margin for error is often used to account for untested code, hardware performance approximations or mistakes. A margin of 20% is often used, although there is very little justification used for this figure, save for historical confidence (\"it worked last time\"). \n\nAs software and hardware have increased in complexity, they have driven the need for tool support. Complexity is increasingly becoming an issue in both static analysis and measurements. It is difficult to judge how wide the error margin should be and how well tested the software system is. System safety arguments based on a high-water mark achieved during testing are widely used, but become harder to justify as the software and hardware become less predictable.\n\nIn the future, it is likely that a requirement for safety critical systems is that they are analyzed using both static and measurement-based approaches.\n\nThe problem of finding WCET by analysis is equivalent to the halting problem and is therefore insoluble in the general case. Fortunately for the kind of systems that engineers typically want to find WCET for, the software is typically well structured, will always terminate and is analyzable.\n\nMost methods for finding a WCET involve approximations (usually a rounding upwards when there are uncertainties) and hence in practice the exact WCET itself is often regarded as unobtainable. Instead, different techniques for finding the WCET produce estimates for the WCET. Those estimates are typically pessimistic, meaning that the estimated WCET is known to be higher than the real WCET (which is usually what is desired). Much work on WCET analysis is on reducing the pessimism in analysis so that the estimated value is low enough to be valuable to the system designer.\n\nWCET analysis usually refers to the execution time of single thread, task or process. However, on modern hardware, especially multi-core, other tasks in the system will impact the WCET of a given task if they share cache, memory lines and other hardware features. Further, task scheduling events such as blocking or to be interruptions should be considered in WCET analysis if they can occur in a particular system. Therefore, it is important to consider the context in which WCET analysis is applied.\n\nThere are many automated approaches to calculating WCET beyond the manual techniques above. These include:\n\nA static WCET tool attempts to estimate WCET by examining the computer software without executing it directly on the hardware. Static analysis techniques have dominated research in the area since the late 1980s, although in an industrial setting, end-to-end measurements approaches were the standard practice.\n\nStatic analysis tools work at a high-level to determine the structure of a program's task, working either on a piece of source code or disassembled binary executable. They also work at a low-level, using timing information about the real hardware that the task will execute on, with all its specific features. By combining those two kinds of analysis, the tool attempts to give an upper bound on the time required to execute a given task on a given hardware platform.\n\nAt the low-level, static WCET analysis is complicated by the presence of architectural features that improve the average-case performance of the processor: instruction/data caches, branch prediction and instruction pipelines, for example. It is possible, but increasingly difficult, to determine tight WCET bounds if these modern architectural features are taken into account in the timing model used by the analysis. For example, cache locking techniques can be used for simplifying WCET estimation and providing predictability.\n\nCertification authorities such as the European Aviation Safety Agency, therefore, rely on model validation suites. \n\nStatic analysis has resulted in good results for simpler hardware, however a possible limitation of static analysis is that the hardware (the CPU in particular) has reached a complexity which is extremely hard to model. In particular, the modelling process can introduce errors from several sources: errors in chip design, lack of documentation, errors in documentation, errors in model creation; all leading to cases where the model predicts a different behavior to that observed on real hardware. Typically, where it is not possible to accurately predict a behavior, a pessimistic result is used, which can lead to the WCET estimate being much larger than anything achieved at run-time. \n\nObtaining tight static WCET estimation is particularly difficult on multi-core processors. \n\nThere are a number of commercial and academic tools that implement various forms of static analysis.\n\nMeasurement-based and hybrid approaches usually try to measure the execution times of short code segments on the real hardware, which are then combined in a higher level analysis. Tools take into account the structure of the software (e.g. loops, branches), to produce an estimate of the WCET of the larger program. The rationale is that it's hard to test the longest path in complex software, but it is easier to test the longest path in many smaller components of it. A worst case effect needs only to be seen once during testing for the analysis to be able to combine it with other worst case events in its analysis.\n\nTypically, the small sections of software can be measured automatically using techniques such as instrumentation (adding markers to the software) or with hardware support such as debuggers, and CPU hardware tracing modules. These markers result in a trace of execution, which includes both the path taken through the program and the time at which different points were executed. The trace is then analyzed to determine the maximum time that each part of the program has ever taken to execute, what the maximum observed iteration time of each loop is and whether there are any parts of the software that are untested (Code coverage).\n\nMeasurement-based WCET analysis has resulted in good results for both simple and complex hardware, although like static analysis it can suffer excessive pessimism in multi-core situations, where the impact of one core on another is hard to define. A limitation of measurement is that it relies on observing the worst-case effects during testing (although not necessarily at the same time). It can be hard to determine if the worst case effects have necessarily been tested.\n\nThere are a number of commercial and academic tools that implement various forms of measurement-based analysis.\n\nThe most active research groups are in Sweden (Mälardalen, Linköping), Germany (Saarbrücken, Dortmund, Braunschweig), France (Toulouse, Saclay, Rennes), Austria (Vienna), UK (University of York and Rapita Systems Ltd), Italy (Bologna), Spain (Cantabria, Valencia), and Switzerland (Zurich). Recently, the topic of code-level timing analysis has found more attention outside of Europe by research groups in the US (North Carolina, Florida), Canada, Australia, Bangladesh(MBI LAB and RDS), Kingdom of Saudi Arabia-UQU(HISE LAB) and Singapore.\n\nThe first international WCET Tool Challenge took place during the autumn of 2006. It was organized by the University of Mälardalen and sponsored by the ARTIST2 Network of Excellence on Embedded Systems Design. The aim of the Challenge was to inspect and to compare different approaches in analyzing the worst-case execution time. All available tools and prototypes able to determine safe upper bounds for the WCET of tasks have participated. The final results were presented in November 2006 at the ISoLA 2006 International Symposium in Paphos, Cyprus.\n\nA second Challenge took place in 2008.\n\n\n\n"}
{"id": "6061230", "url": "https://en.wikipedia.org/wiki?curid=6061230", "title": "XTS-400", "text": "XTS-400\n\nThe XTS-400 is a multilevel secure computer operating system. It is multiuser and multitasking that uses multilevel scheduling in processing data and information. It works in networked environments and supports Gigabit Ethernet and both IPv4 and IPv6.\n\nThe XTS-400 is a combination of Intel x86 hardware and the Secure Trusted Operating Program (STOP) operating system. XTS-400 was developed by BAE Systems, and originally released as version 6.0 in December 2003.\n\nSTOP provides \"high-assurance\" security and was the first general-purpose operating system with a Common Criteria assurance level rating of EAL5 or above. The XTS-400 can host, and be trusted to separate, multiple, concurrent data sets, users, and networks at different sensitivity levels.\n\nThe XTS-400 provides both an \"untrusted\" environment for normal work and a trusted environment for administrative work and for privileged applications. The untrusted environment is similar to traditional Unix environments. It provides binary compatibility with Linux applications running most Linux commands and tools as well as most Linux applications without the need for recompiling. This untrusted environment includes an X Window System GUI, though all windows on a screen must be at the same sensitivity level.\n\nTo support the trusted environment and various security features, STOP provides a set of proprietary APIs to applications. In order to develop programs that use these proprietary \nAPIs, a special software development environment (SDE) is needed. The SDE is also needed in order to port some complicated Linux/Unix applications to the XTS-400.\n\nA new version of the STOP operating system, STOP 7 has since been introduced, with claims to have improved performance and new features such as RBAC.\n\nAs a high-assurance, MLS system, XTS-400 can be used in cross-domain solutions, which typically need a piece of privileged software to be developed which can temporarily circumvent one or more security features in a controlled manner. Such pieces are outside the CC evaluation of the XTS-400, but they can be accredited.\n\nThe XTS-400 can be used as a desktop, server, or network gateway. The interactive environment, typical Unix command line tools, and a GUI are present in support of a desktop solution. Since the XTS-400 supports multiple, concurrent network connections at different sensitivity levels, it can be used to replace several single-level desktops connected to several different networks.\n\nIn support of server functionality, the XTS-400 can be implemented in a rackmount configuration, accepts a uninterruptible power supply (UPS), allows multiple network connections, accommodates many hard disks on a SCSI subsystem (also saving disk blocks using a \"sparse file\" implementation in the file system), and provides a trusted backup/save tool. Server software, such as an Internet daemon, can be ported to run on the XTS-400.\n\nA popular application for high-assurance systems like the XTS-400 is to guard information flow between two networks of differing security characteristics. Several customer guard solutions are available based on XTS systems.\n\nXTS-400 version 6.0.E completed a Common Criteria (CC) evaluation in March 2004 at EAL4 augmented with ALC_FLR.3 (validation report CCEVS-VR-04-0058.) Version 6.0.E also conformed with the protection profiles entitled Labeled Security Protection Profile (LSPP) and Controlled Access Protection Profile (CAPP), though both profiles are surpassed in functionality and assurance.\n\nXTS-400 version 6.1.E completed evaluation in March 2005 at EAL5 augmented with ALC_FLR.3 and ATE_IND.3 (validation report CCEVS-VR-05-0094), still conforming to the LSPP and CAPP. The EAL5+ evaluation included analysis of covert channels and additional vulnerability analysis and testing by the National Security Agency.\n\nXTS-400 version 6.4.U4 completed evaluation in July 2008 at EAL5 augmented with ALC_FLR.3 and ATE_IND.3 (validation report CCEVS-VR-VID10293-2008), also still conforming to the LSPP and CAPP. Like its predecessor, it also included analysis of covert channels and additional vulnerability analysis and testing by the National Security Agency.\n\nThe official postings for all the XTS-400 evaluations can be seen on the Validated Product List.\n\nThe main security feature that sets STOP apart from most operating systems is the mandatory sensitivity policy. Support for a mandatory integrity policy, also sets STOP apart from most MLS or trusted systems. While a sensitivity policy deals with preventing unauthorized disclosure, an integrity policy deals with preventing unauthorized deletion or modification (such as the damage that a virus might attempt). Normal (i.e., untrusted) users do not have the \"discretion\" to change the sensitivity or integrity levels of objects. The Bell–LaPadula and Biba formal models are the basis for these policies.\n\nBoth the sensitivity and integrity policies apply to all users and all objects on the system. STOP provides 16 hierarchical sensitivity levels, 64 non-hierarchical sensitivity categories, 8 hierarchical integrity levels, and 16 non-hierarchical integrity categories. The mandatory sensitivity policy enforces the United States Department of Defense data sensitivity classification model (i.e., \"Unclassified,\" \"Secret,\" \"Top Secret\"), but can be configured for commercial environments.\n\nOther security features include:\n\nSTOP comes in only a single package, so that there is no confusion about whether a particular package has all security features present. Mandatory policies cannot be disabled. Policy configuration does not require a potentially complicated process of defining large sets of domains and data types (and the attendant access rules).\n\nTo maintain the trustworthiness of the system, the XTS-400 must be installed, booted, and configured by trusted personnel. The site must also provide physical protection of the hardware components. The system, and software upgrades, are shipped from BAE Systems in a secure fashion.\n\nFor customers who want them, XTS-400 supports a Mission Support Cryptographic Unit (MSCU) and Fortezza cards. The MSCU performs \"type 1\" cryptography and has been separately scrutinized by the United States National Security Agency.\n\nThe CC evaluation forces particular hardware to be used in the XTS-400. Though this places restrictions on the hardware configurations that can be used, several configurations are possible. The XTS-400 uses only standard PC, commercial off-the-shelf (COTS) components, except for an optional Mission Support Cryptographic Unit (MSCU).\n\nThe hardware is based on an Intel Xeon (P4) central processing unit (CPU) at up to 2.8 GHz speeds, supporting up to 2 GB of main memory.\n\nA Peripheral Component Interconnect (PCI) bus is used for add-in cards such as Gigabit Ethernet. Up to 16 simultaneous Ethernet connections can be made, all of which can be configured at different mandatory security and integrity levels.\n\nA SCSI subsystem is used to allow a number of high-performance peripherals to be attached. One SCSI peripheral is a PC Card reader that can support Fortezza. Multiple SCSI host adapters can be included.\n\nThe XTS-400 has been preceded by several evaluated ancestors, all developed by the same group: Secure Communications Processor (SCOMP), XTS-200, and XTS-300. All of the predecessor products were evaluated under Trusted Computer System Evaluation Criteria (TCSEC) (a.k.a. Orange Book) standards. SCOMP completed evaluation in 1984 at the highest functional and assurance level then in place: A1. Since then the product has evolved from proprietary hardware and interfaces to commodity hardware and Linux interfaces.\n\nThe XTS-200 was designed as a general-purpose operating system supporting a Unix-like application and user environment. XTS-200 completed evaluation in 1992 at the B3 level.\n\nThe XTS-300 transitioned from proprietary, mini-computer hardware to COTS, Intel x86 hardware. XTS-300 completed evaluation in 1994 at the B3 level. XTS-300 also went through several ratings maintenance cycles (a.k.a. RAMP), very similar to an \"assurance continuity\" cycle under CC, ultimately ending up with version 5.2.E being evaluated in 2000.\n\nDevelopment of the XTS-400 began in June 2000. The main customer-visible change was specific conformance to the Linux API. Though the security features of the XTS system put some restrictions on the API and require additional, proprietary interfaces, conformance is close enough that most applications will run on the XTS without recompilation. Some security features were added or improved as compared to earlier versions of the system and performance was also improved.\n\nAs of July 2006, enhancements continue to be made to the XTS line of products.\n\nOn September 5, 2006, the United States Patent Offices granted BAE Systems Information Technology, LLC. United States Patent # 7,103,914 \"Trusted computer system\".\n\nSTOP is a monolithic kernel operating system (as is Linux). Though it provides a Linux-compatible API, STOP is not derived from Unix or any Unix-like system. STOP is highly layered, highly modularized, and relatively compact and simple. These characteristics have historically facilitated high-assurance evaluations.\n\nSTOP is layered into four \"rings\" and each ring is further subdivided into layers. The innermost ring has hardware privilege and applications, including privileged commands, run in the outermost. The inner three rings constitute the \"kernel\". Software in an outer ring is prevented from tampering with software in an inner ring. The kernel is part of every process’s address space and is needed by both normal and privileged processes.\n\nA \"security kernel\" occupies the innermost and most privileged ring and enforces all mandatory policies. It provides a virtual process environment, which isolates one process from another. It performs all low-level scheduling, memory management, and interrupt handling. The security kernel also provides I/O services and an IPC message mechanism. The security kernel’s data is global to the system.\n\nTrusted system services (TSS) software executes in ring 1. TSS implements file systems, implements TCP/IP, and enforces the discretionary access control policy on file system objects. TSS's data is local to the process within which it is executing.\n\nOperating system services (OSS) executes in ring 2. OSS provides Linux-like API to applications as well as providing additional proprietary interfaces for using the security features of the system. OSS implements signals, process groups, and some memory devices. OSS's data is local to the process within which it is executing.\n\nSoftware is considered trusted if it performs functions upon which the system depends to enforce the security policy (e.g., the establishment of user authorization). This determination is based on integrity level and privileges. Untrusted software runs at integrity level 3, with all integrity categories, or lower. Some processes require privileges to perform their functions—for example the Secure Server needs to access the User Access Authentication database, kept at \"system high\", while establishing a session for a user at a lower sensitivity level.\n\nThe XTS-400 can provide a high level of security in many application environments, but trade-offs are made to attain it. Potential weaknesses for some customers may include:\n\n"}
{"id": "20399858", "url": "https://en.wikipedia.org/wiki?curid=20399858", "title": "Zinc oxide nanorod sensor", "text": "Zinc oxide nanorod sensor\n\nA zinc oxide nanorod sensor or ZnO nanorod sensor is an electronic or optical device detecting presence of certain gas or liquid molecules (e.g. humidity, NO, hydrogen, etc.) in the ambient atmosphere. The sensor exploits enhanced surface area (and thus surface activity) intrinsic to all nano-sized materials, including ZnO nanorods. Adsorption of molecules on the nanorods can be detected through variation of the nanorods' properties, such as photoluminescence, electrical conductivity, vibration frequency, mass, etc. The simplest and thus most popular way is to pass electrical current through the nanorods and observe its changes upon exposure to gas.\n\n\n\n"}
