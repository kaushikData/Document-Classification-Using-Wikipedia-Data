{"id": "153209", "url": "https://en.wikipedia.org/wiki?curid=153209", "title": "Air-augmented rocket", "text": "Air-augmented rocket\n\nAir-augmented rockets (also known as rocket-ejector, ramrocket, ducted rocket, integral rocket/ramjets, or ejector ramjets) use the supersonic exhaust of some kind of rocket engine to further compress air collected by ram effect during flight to use as additional working mass, leading to greater effective thrust for any given amount of fuel than either the rocket or a ramjet alone.\n\nIt represents a hybrid class of rocket/ramjet engines, similar to a ramjet, but able to give useful thrust from zero speed, and is also able in some cases to operate outside the atmosphere, with fuel efficiency not worse than both a comparable ramjet or rocket at every point.\n\nIn a conventional chemical rocket engine, the rocket carries both its fuel and its oxidizer in its fuselage. The chemical reaction between the fuel and the oxidizer produces reactant products which are nominally gasses at the pressures and temperatures in the rocket's combustion chamber. The reaction is also highly energetic (exothermic) releasing tremendous energy in the form of heat; that is imparted to the reactant products in the combustion chamber giving this mass enormous internal energy which, when expanded through a nozzle is capable of producing very high exhaust velocities. The exhaust is directed rearward through the nozzle, thereby producing a thrust forward. \n\nIn this conventional design, the fuel/oxidizer mixture is both the working mass and energy source that accelerates it. It is easy to demonstrate that the best performance is had if the working mass is as low as possible. Hydrogen, by itself, is the theoretical best rocket fuel. Mixing this with oxygen in order to burn it lowers the overall performance of the system by raising the mass of the exhaust, as well as greatly increasing the mass that has to be carried aloft – oxygen is much heavier than hydrogen.\n\nOne potential method of increasing the overall performance of the system is to collect either the fuel or the oxidizer during flight. Fuel is hard to come by in the atmosphere, but oxidizer in the form of gaseous oxygen makes up to 20% of the air. There are a number of designs that take advantage of this fact. These sorts of systems have been explored in the liquid air cycle engine (LACE).\n\nAnother idea is to collect the working mass. With an air-augmented rocket, an otherwise conventional rocket engine is mounted in the center of a long tube, open at the front. As the rocket moves through the atmosphere the air enters the front of the tube, where it is compressed via the ram effect. As it travels down the tube it is further compressed and mixed with the fuel-rich exhaust from the rocket engine, which heats the air much as a combustor would in a ramjet. In this way a fairly small rocket can be used to accelerate a much larger working mass than normal, leading to significantly higher thrust within the atmosphere.\n\nThe effectiveness of this simple method can be dramatic. Typical solid rockets have a specific impulse of about 260 seconds (2.5 kN·s/kg), but using the same fuel in an air-augmented design can improve this to over 500 seconds (4.9 kN·s/kg), a figure even the best hydrogen/oxygen engines can't match. This design can even be slightly more efficient than a ramjet, as the exhaust from the rocket engine helps compresses the air more than a ramjet normally would; this raises the combustion efficiency as a longer, more efficient nozzle can be employed. Another advantage is that the rocket works even at zero forward speed, whereas a ramjet requires forward motion to feed air into the engine.\n\nIt might be envisaged that such an increase in performance would be widely deployed, but various issues frequently preclude this. The intakes of high-speed engines are difficult to design, and they can't simply be located anywhere on the airframe whilst getting reasonable performance – in general, the entire airframe needs to be built around the intake design. Another problem is that the air thins out as the rocket climbs, so the amount of additional thrust is limited by how fast the rocket climbs. Finally, the air ducting weighs about 5× to 10× more than an equivalent rocket that gives the same thrust. This slows the vehicle quite a bit towards the end of the burn.\n\nThe first serious attempt to make a production air-augmented rocket was the Soviet Gnom rocket design, implemented by Decree 708-336 of the Soviet Ministers of 2 July 1958. More recently NASA has re-examined similar technology for the GTX program as part of an effort to develop SSTO spacecraft.\n\nAir-augmented rockets finally entered mass production in 2016 when the Meteor Air to Air Missile was introduced into service.\n\n"}
{"id": "25828373", "url": "https://en.wikipedia.org/wiki?curid=25828373", "title": "Alpha case", "text": "Alpha case\n\nIn metallurgy, alpha case is the oxygen-enriched surface phase that occurs when titanium and its alloys are exposed to heated air or oxygen. Alpha case is hard and brittle, and tends to create a series of microcracks which will reduce the metal's performance and its fatigue properties. Alpha case can be minimized or avoided by processing titanium at very deep vacuum levels. However once present on the surface, the currently applied method to remove the Alpha case is by the subtractive methods of machining and /or chem milling. An emerging technique is to subject the metal to an electrochemical treatment in molten salts, such as calcium chloride or lithium chloride at elevated temperatures. This method, as proven in laboratory, is effective in removal of the dissolved oxygen from the alpha case, and hence recovery of the metal. However, the unwanted consequence of the high temperature treatment is the growth of the grains in the metal. Grain growth may be limited by lowering the molten salt temperature. Alternatively, the metal may be rolling-pressed again to break the large grains into smaller ones \n"}
{"id": "5055767", "url": "https://en.wikipedia.org/wiki?curid=5055767", "title": "Automated pool cleaner", "text": "Automated pool cleaner\n\nAn automated pool cleaner is a vacuum cleaner intended to collect debris and sediment from swimming pools with minimal human intervention. Popularly dubbed a <nowiki>\"</nowiki>creepy-crawly<nowiki>\"</nowiki> or \"Kreepy Krauly\" in South Africa, it is one of several types of swimming pool vacuum cleaners. Other major types are battery-powered or manually powered wands effective only for very small pools, kiddie or wading pools and small spas and hot tubs, and battery-powered, handheld/extended reach pool and spa vacuums. The latter are powered by rechargeable batteries and can be hand held attached to a telescopic pole used for extended reach. These are used for small to medium-sized pools, larger spas, and to spot clean larger pools. The name <nowiki>\"</nowiki>creepy-crawly<nowiki>\"</nowiki> derives from the vacuum's webbed-nozzle crawling creepily through the underwater mist as well as for its creepy suction noise. <nowiki>\"</nowiki>Creepy crawly<nowiki>\"</nowiki> originally referred to strange creatures that crawl on the bottom of the ocean, as the webbed nozzle of the vacuum slightly resembles an octopus in both appearance and suction ability.\n\nSwimming pool cleaners evolved from two areas of science: development of the water filter and early cistern cleaners.\nThe forerunner of today's pool cleaners were cistern cleaners. A cistern (Middle English cisterne, from the Latin cisterna, from cista, box, from Greek kistê, basket) is a waterproof receptacle for holding liquids, usually water. Often cisterns were and still are built to catch and store rainwater. The great palaces of antiquity had both lavish pools and cisterns. They were prevalent in early America as well. United States Patent and Trademark Office makes reference to a cistern cleaner patent filed (though never issued) as early as 1798. Before swimming pools were affordable and fashionable, many swam in their larger cisterns.\n\nIn 1883 John E. Pattison of New Orleans filed an application for a \"Cistern and Tank Cleaner \" and the first discovered patent was issued the following year. It swept and scraped the bottom of a cistern or tank, and through a combination of suction and manipulation of the water pressure was able to separate and remove sediment without removing the water. Over the next 20 years his invention was improved on numerous occasions. Many pool cleaner patents issued in the modern era refer to some of the cistern cleaners as predecessors of their invention.\n\nThe first swimming pool cleaner was invented in 1912 by Pittsburgh, Pennsylvania local citizen John M. Davison. On November 26, 1912, he submitted a patent application to the United States Patent and Trademark Office entitled \"Cleaning Apparatus For Swimming Pools And The Like,\" patent number 1,056,779 that was issued on March 25, 1913.\n\nThe first suction-side pool cleaner was invented by Roy B. Everson of Chicago in 1937, which he entitled \"Swimming Pool Cleaner\".\n\nNineteen years later, the first suction-side pool cleaner was the work of Joseph Eistrup of San Mateo, California, who called his invention simply \"Pool Cleaner\".\n\nTwo years later, the first truly automatic, aptly named \"Automatic Swimming Pool Cleaner\" was created by Andrew L. Pansini of Greenbrae, California, founder of the industry icon Jandy Corporation. Patent Number 3,032,044 was touted by Pansini as \"an automatic swimming pool cleaner\", which is effective to remove the scum, dirt and other accumulations from both the bottom and side walls of a pool to disperse foreign matter in the, water for removal therefrom by normal pump-filter system of the pool.\n\nThe first robotic pool cleaner that used electricity was the work of Robert B. Myers of Boca Raton, Florida in 1967, who filed a patent.\n\nThe third and last of the generally accepted pool cleaner technologies, the pressure-side cleaner, was invented by Melvyn L. Henkin of Tarzana, California in 1972. His \"Automatic Swimming Pool Cleaner, United States Patent Number 3,822,754 utilized three wheels to allow the machine \"to travel underwater along a random path on the pool vessel surface for dislodging debris therefrom\". The design is probably familiar to pool owners as the Polaris Pool Cleaner.\n\nIndependently from his American counterparts Ferdinand Chauvier invented the Kreepy Krauly in South Africa in 1974 in Springs.\n\nThere are three main types of automated or automatic swimming pool cleaners, classified by the drive mechanism and source of power used:\n\nIn this type, water pumped out of the pool via its skimmer or drains is used for locomotion and debris suction and returned after being filtered via pool return or outlet valves. This is the least expensive and most popular type. It traces a random course. This type of cleaner is usually attached via a 1.5 inch hose to a vacuum plate in the skimmer, or to a dedicated extraction or \"vac\" line on the side of the pool. The suction action of the pool's pump provides motive force to the machine to randomly traverse the floor and walls of the pool, extracting dirt and debris in its path. The first automatic pool cleaner was a suction cleaner.\n\nThese are the least expensive and most widely used pool cleaners with purchase costs ranging in the $100–$300 price range. They are powered solely by the main pump of the pool and utilize the pool's filter system to remove dirt and debris from the water. These machines effectively diminish the suction of the main pump - using them will increase the electricity costs and require the main pump and filter system to be serviced more frequently. There is minimal maintenance and part replacement costs on these devices over time.\n\nIn this design, pool water inflow is further pressurized using a secondary \"booster\" pump on most but not all models. This high-pressure water is used for locomotion and debris suction, employing the venturi effect. It traces a random course. The requirement of a booster pump makes this type the highest in electricity use of the three types of pool cleaners.\n\nThe pressure causes turbulence in the water, distributing some debris on the floor and walls of the pool, some of which is re-floated to the pool surface and then sucked into the main filter through the skimmer inlets. A portion of the dirt and debris is caught in an attached filter bag. The purchase cost of this type of cleaner range from a minimum of $200 to about $700 plus the costs of the booster pump, usually over $200. Some more sophisticated models can cost over $1,000.\n\nBoth suction-side and pressure-side cleaners are dependent on the pool's main pump and filter system to remove contaminants from the pool water, so cannot remove particles smaller than the pore size of the pool's existing filter element. Such elements can be made of sand, diatomaceous earth, zeolite or other natural or synthetic materials. That particle size ranges from under 5 µm for diatomaceous filters to well in excess of 50 µm for sand filters. Disadvantages of these types of pool cleaners are the additional electricity use, and filtration limitation by the pore size of the main filter element, as well as the time and effort needed to attach the device to the ports that connect to the main pump and filter, and the increased burden of maintenance time and expense on the pool's mechanics.\n\nThese cleaners are independent from the pool's main filter and pump system and are powered by a separate electricity source, usually in the form of a set-down transformer that is kept at least from the water in the pool, often on the pool deck. They have two internal motors: one to suck in water through a self-contained filter bag and then return the filtered water at a high rate of speed back into the pool water. The second is a drive motor that is connected to tractor-like rubber or synthetic tracks and \"brushes\" connected by rubber or plastic bands via a metal shaft. The brushes, resembling paint rollers, are located on the front and back of the machine and help remove contaminating particles from the pool's floor, walls (and in some designs even the pool steps) depending on size and configuration. They also direct the particles into the internal filter bag.\n\nAn internal microchip is pre-programmed to turn on and off and reverse the direction of the drive motors. The chip will cause the machine to change direction when it reaches a wall or the water surface after climbing the pool walls.\n\nThese machines may also be directed by sensors located in the bump bars which, on contact with objects such as a wall, cause a reverse in direction, with a small offset allowing it to move one machine's width over on each crossing of the pool. The delay timer is an important feature for many pools, as many switch off a number of circulation pumps during the night to allow suspended particles to settle on the bottom of the pool; after a couple of hours the pool cleaner begins its cleaning cycle. This cleaning cycle is set up to complete before the pumps are turned back on. Though not necessary for adequate pool cleaning, this feature saves energy and improves cleaning efficiency.\n\nIn order to move forward and backward and negotiate walls and steps electric robotic cleaners rely on three natural principles, traction and movement caused by the drive motor and tracks, buoyancy created by the large areas inside the machine that fills with air, and the force resulting from the high pressure of water being emitted from the top of the machine that pushes it against the floor and walls. Some electric robotic machines use brushes made out of polyvinyl alcohol (PVA) Polyvinyl alcohol that has an adherence quality that allows the unit to almost cling to the walls, steps and floors. They also are resistant to dirt and oil improving lifespan over rubber or other synthetic materials.\n\nThe combination of these three natural principles along with an internal mercury switch that tells the microchip that the unit has gone from a horizontal to vertical position as it climbs a wall allows it to change direction from ascending to descending the wall at pre-programmed intervals based on the average height of a pool walls. Some machines have delayed timers that cause the robot to remain at the water line, where more dirt accumulates, for momentarily resulting in a scrubbing action, much like the wheels of a powerful automobile spinning or peeling out.\n\nThe major benefits of these machines are efficiency in time, energy, and cleaning ability as well as low maintenance requirements and costs. The major disadvantage is purchase cost which can range from $500 for floor-cleaning-only machines to over $2,000 for the most sophisticated residential units.\n\nAccording to P.K. Data of Duluth, a Georgia consulting and market research firm that has been retained for many years by the pool and spa industry's internal trade organization, The Association of Pool & Spa Professionals (APSP) there are approximately 14,000,000 residential pools and spas in the United States and over 400,000 commercial or public pools. As a result, this has created a market for larger, more powerful commercial pool cleaners. All commercial pool cleaners are electric robotic and can range in price from a little over $1,000 to upwards of $15,000 or more. They closely resemble residential models but in addition to their addition size they are made with heavy duty components and often more sophisticated computer guidance and on and off systems.\n\nThere have been attempts for nearly 100 years to mandate the use of pool cleaners, primarily addressed to public pools. Currently the Center for Disease Control and Prevention located in the Greater Atlanta, Georgia metropolitan area on a grant provided by the National Swimming Pool Foundation (NSPF) is about to publish the first uniform Model Aquatic Health Code (MAHC). Included is a section on pool filtration proposed regulations directed to the nation's 3200+ state and local agencies that enforce laws and ordinances relating to the operation of swimming pools and spas.\n\nThe proposed MAHC is not the first attempt to propose a uniform aquatic health code. The credit for that goes to the American Public Health Association(APHA) which 100 years ago recognized the dangers of improperly maintained aquatic facilities and formed a committee in 1918 that, for the next 66 years, issued eleven so-called \"Swimming Pools and Other Public Bathing Places Standards For Design, Construction, Equipment And Operation\" recommended ordinances and regulations. But for a variety of reasons none of these recommendations were adopted, at least not formally or completely adopted.\n\nThe APHA has tried to develop a uniform aquatic health code, or what it referred to for years as referenced above, and published short reports annually from 1920 through 1925 that it simply referred to as \"Report of the Committee On Bathing Places\". and finally in 1926 published in its journal its first comprehensive report it called \"Standards for Design, Construction, Equipment and Operation\" for \"Swimming Pools and Other Public Bathing Places\". Twelve others were published through 1981. However, its lack of authority to enforce them is implied by the changing description of what was limited to their recommendations or suggestions and the expressed purposes in issuing them.\n\n\nIn 1912, coincidentally the same year when the United States Patent and Trademark Office issued the first patent for a swimming pool cleaner, the Sanitary Engineering Section of the American Public Health Association (APHA) convened in New York City to lay the groundwork for the first recommended pool and spa regulations. As reported in the American Journal of Public Health in April 1912 a meeting was held in Havana the previous December and at the New York meeting among the subjects that the committee was to be studying was \"Hygiene of swimming pools\".\n\nSix years later a committee on swimming pools was appointed at the APHA's annual meeting in Chicago and in 1920 a similar committee was appointed at the meeting in Washington, D.C. In 1921 and periodically over the next seven decades until the work of the APHA on this subject matter went through a series of divisions and consolidations, diverted elsewhere its committees and joint committees with other health-orientated public and quasi-public organizations issued proposed ordinances and regulations in the form of unenforceable recommendations. Despite their intended and published goals, none became law, uniform, much less national.\n\nNone of the proposed Standards included more than a passing reference of the need to properly clean a pool. A few, but curiously not all of these recommended ordinances and regulations, related to the use of a vacuum, although the first that included any specificity in 1923 at least required a certain level of clarity. The 1921 report, barely a few pages in length, made this reference to the need to clean the pool.\n\nThe 1923 report of the American Journal of Public Health, Sanitary Engineering Section American Public Health Association read before the Sanitary Engineering Section of the American Public Health Association at the Fifty-second Annual Meeting at Boston, Massachusetts, October 8, 1923. slightly longer, but still very brief stated:\n\nIt further stated:\n\nTherefore, in 1921 it was recognized that infectious material, namely pathogens collect in the pool and should be removed.\n\nIt was not until 1926 twelve years after the organization recognized the need to address swimming pool \"hygiene\" and eight years after the committee was organized that the first true report was issued and later published in the Journal of the American Public Health Association. Of all of its reports from 1920 through 1981 the first major report by the APHA the 1926 one, written in narrative form as were the succeeding nine ones though 1957 the committee included the detailed provisions relating to pool cleaning, vacuuming and vacuums:\n\nThe 1964 report included the following language:\n\nThe CDC was founded (in 1946), followed by the Cabinet-level Department of Health, Education and Welfare (in 1953), now the Department of Health, and Human Services and its 11 operating divisions, the National Health Service Corps (in 1977) and along the way a variety of private and non-profit aquatic organizations such as the National Spa and Pool Association (in 1956), now the Association of Pool and Spa Professionals the National Swimming Pool Foundation (in 1965).\n\nPresently a variety, but not by a long shot the majority, of states and jurisdictions that have codified the requirement of inclusion of an independent vacuum cleaner including the two states with the highest number and concentration of both residential and public pools:\n\nCalifornia: 2010 Title 24, Part 2, Vol. 2 California Building Code. Section 3140B, Cleaning Systems:\n\nFlorida:\n\nFlorida Department of Health section 64E-9.007 Recirculation and Treatment System Requirements\n\nIn 2005 the CDC in response to a growing concern and feared epidemic with the pathogen Cryptosporidium the Center for Disease Control Center for Disease Control, much like the American Public Health Association did in 1912 convened many of the country's foremost medical and other scientific experts to study the concern for aquatic health. As a result, in 2007 they began their quest, again much like the APHC for a uniform aquatic health code.\n\nEach health and safety segment has been assigned to a committee to study it and draft a proposed module open for public comment before being adopted and then recommended to the nation's 3200+ state and local health agencies that enact ordinances and regulations for swimming pools and spa and other aquatic facilities, inspect and monitor them and then enforce the regulations. Since the focus of the MAHC was to respond to the threat of Cryptosporidium the Technical Committee of Recirculation Systems and Filtration is a major focus. The University of North Carolina Charlotte Associate Professor James Amburgey is the Chairperson of the Center For Disease Control, Model Aquatic Health Code Technical Committee on Recirculation Systems and Filtration.\n\nAmburgey has conducted many tests to evaluation existing swimming pool filters and his conclusions have been they are extremely ineffective in most cases to help remove Cryptosporidium. He is reported to be working with several manufacturers of swimming pool and spa vacuum cleaners to develop a filter bag that will result in exponential advancements in the current filter bags, cleaners and vacuums on the market.\n\n\n"}
{"id": "1911275", "url": "https://en.wikipedia.org/wiki?curid=1911275", "title": "Automatic weather station", "text": "Automatic weather station\n\nAn automatic weather station (AWS) is an automated version of the traditional weather station, either to save human labour or to enable measurements from remote areas. An AWS will typically consist of a weather-proof enclosure containing the data logger, rechargeable battery, telemetry (optional) and the meteorological sensors with an attached solar panel or wind turbine and mounted upon a mast. The specific configuration may vary due to the purpose of the system. The system may report in near real time via the Argos System and the Global Telecommunications System, or save the data for later recovery. \n\nIn the past, automatic weather stations were often placed where electricity and communication lines were available. Nowadays, the solar panel, wind turbine and mobile phone technology have made it possible to have wireless stations that are not connected to the electrical grid or hardline telecommunications network.\n\nMost automatic weather stations have\n\nSome stations can also have\n\nUnlike manual weather stations, automated airport weather stations cannot report the class and amount of clouds. Also, precipitation measurements are difficult, especially for snow, as the gauge must empty itself between observations. For present weather, all phenomena that do not touch the sensor, such as fog patches, remain unobserved. The change from manual observations to automatic weather stations is a major non-climatic change in the climate record. The change in instrumentation, enclosure and location can lead to a jump in, for example, the measured temperature or precipitation values, which can lead to erroneous estimates of climate trends. This change, and related non-climatic changes, have to be removed by homogenization.\n\nThe data-logger is the heart of the Automatic Weather Station.\nIn high quality weather stations, the data-logger may be designed by the supplier to be the perfect solution for a particular meteorological client.\nIndeed, usually data-loggers found in the market don't fit the requirement in terms of power consumption, inputs, communication, protection against animals (ants, rats, etc.), humidity, salty air, sand, etc.\nThe main functions of a data-logger are:\n\nEnclosures used with automatic weather stations are typically weather proof fiberglass, ABS or stainless steel, With ABS being the cheapest, cast aluminium paint or stainless steel the most durable and fiberglass being a compromise.\n\nThe main power source for an automatic weather station depends on its usage. Many stations with lower power equipment usually use one or more solar panels connected in parallel with a regulator and one or more rechargeable batteries. As a rule of thumb, solar output is at its optimum for only 5 hours each day. As such, mounting angle and position are vital. In the Northern Hemisphere, the solar panel would be mounted facing south and vice versa for the Southern Hemisphere. The output from the solar panels may be supplemented by a wind turbine to provide power during periods of poor sunlight, or by direct connection to the local electrical grid. Most automated airport weather stations are connected to the commercial power grid due to the higher power needs of the ceilometer and present weather sensors, which are active sensors and emit energy directly into the environment.\n\nThe standard mast heights used with automatic weather stations are 2, 3, 10 and 30 meters. Other sizes are available, but typically these sizes have been used as standards for differing applications.\n\n"}
{"id": "18322062", "url": "https://en.wikipedia.org/wiki?curid=18322062", "title": "Bank Night", "text": "Bank Night\n\nBank Night was a lottery game franchise in the United States during the Great Depression. It was invented and marketed by Charles U. Yaeger, a former booking agent for 20th Century Fox.\n\nIn 1936, Bank Night was played at 5,000 of America's 15,000 active theaters, and copies of it were played at countless more. The popularity of Bank Night and similar schemes contributed to the resiliency of the film industry during the Great Depression more than any other single business tactic.\n\nBank Night was run as a franchise which was leased to theaters for from $5 to $50 a week, depending on their size. The payment entitled the owner to run an event called Bank Night, and each owner was given a film reel with a Bank Night trailer, as well as a registration book and equipment to draw numbers to pick winners.\n\nAnyone could enter their name in a book kept by the theater manager, and on Bank Night, a name would be drawn at random. The person selected must reach the stage within a set amount of time to claim their prize, usually a few minutes (they would not be required to purchase a ticket to enter the theater). While not technically requiring any purchase, and thus circumventing the numerous local lottery laws of the time, Bank Night had the effect of drawing people to theaters, many of whom bought tickets anyway.\n\nYeager invented Bank Night in 1931 in Denver, Colorado. The concept was immediately successful. Although lucrative, the franchise faced scrutiny from state and municipal authorities, who often challenged theaters in court for holding Bank Nights. It quickly spawned copycats to get around the franchise fee, such as \"Prosperity Night\", \"Treasure Night\" and \"Movie Sweepstakes\".\n\nAn example was the sweepstakes held at the Palace Theatre in Marion, Ohio. Starting May 7, 1936, the Palace held a weekly, Wednesday night sweepstakes. Patrons were given a free sweepstakes ticket when a Wednesday matinee movie ticket was purchased. Also, any adult who wanted to participate in the Wednesday night drawing could simply stop by the theatre and register. It was not necessary to purchase a movie ticket or to be inside the theatre when the winner was announced. Each participant received a ticket with the name of a horse. On the Palace stage, a drawing was made in which the names of 20 horses were picked from a huge wire hopper. Once announced, each horse name was placed on a blackboard opposite one of 20 numbers. Then, four films were carried to the stage, each of them showing 20 horses in a race. Someone chose a film and it was taken to the projection booth, and the film was shown. Each horse had a number, and the audience watched the race. The order of finish determined the winner of the first ($200), second ($10) and third ($5) prizes. If the winner was not in the theatre the prize accumulated for the next week. The legality of the popular sweepstakes was challenged by a local prosecutor under Ohio's anti-lottery rules after a Keystone Cops-like raid on the theatre during a performance. The prosecutor's legal theory was rejected by the jury and the theatre management immediately reinstated the sweepstakes.\n\nThe fad lost much of its popularity by the late 1930s, first to competing games such as Screeno and other lottery-like games, but popularity further declined as cities such as Chicago and New York City took stances against Bank Night and similar games in 1936, and soon theater owners in many jurisdictions faced arrest or fines for running Bank Nights. The improving economy and World War II also contributed to changing consumer tastes. Bank Night was the plot of the 1936 Charley Chase short \"Neighborhood House\". Bank Nights were jokingly referenced in the films \"After the Thin Man\" and \"Calling All Husbands\" (1940) . It is also mentioned in at least two Warner Brothers cartoons directed by Tex Avery: \"A Day at the Zoo\" and \"Thugs with Dirty Mugs\" (both released in 1939). Bank Nights were also humorously referenced in the Hope/Crosby film \"Road to Singapore\" (1940) and the 1939 film adaptation of the Rodgers and Hart musical \"On Your Toes\".\n\n"}
{"id": "11272361", "url": "https://en.wikipedia.org/wiki?curid=11272361", "title": "Black start", "text": "Black start\n\nA black start is the process of restoring an electric power station or a part of an electric grid to operation without relying on the external electric power transmission network to recover from a total or partial shutdown.\n\nNormally, the electric power used within the plant is provided from the station's own generators. If all of the plant's main generators are shut down, station service power is provided by drawing power from the grid through the plant's transmission line. However, during a wide-area outage, off-site power from the grid is not available. In the absence of grid power, a so-called black start needs to be performed to bootstrap the power grid into operation.\n\nTo provide a black start, some power stations have small diesel generators, normally called the \"black start diesel generator\" (BSDG), which can be used to start larger generators (of several megawatts capacity), which in turn can be used to start the main power station generators. Generating plants using steam turbines require station service power of up to 10% of their capacity for boiler feedwater pumps, boiler forced-draft combustion air blowers, and for fuel preparation. It is uneconomical to provide such a large standby capacity at each station, so black-start power must be provided over designated tie lines from another station. Often hydroelectric power plants are designated as the black-start sources to restore network interconnections. A hydroelectric station needs very little initial power to start (just enough to open the intake gates and provide excitation current to the generator field coils), and can put a large block of power on line very quickly to allow start-up of fossil-fuel or nuclear stations. Certain types of combustion turbine can be configured for black start, providing another option in places without suitable hydroelectric plants. In 2017 a utility in Southern California has successfully demonstrated the use of a battery energy storage system to provide a black start, firing up a combined cycle gas turbine from an idle state.\n\nOne method of black start (based on a real scenario) might be as follows:\n\nPower is finally re-applied to the general electricity distribution network and sent to the consumers. Often this will happen gradually; starting the entire grid at once may be unfeasible. In particular, after a lengthy outage during summer, all buildings will be warm, and if the power were restored at once, the demand from air conditioning units alone would be more than the grid could supply. In colder climates a similar issue can occur in winter with the use of heating devices.\n\nIn a larger grid, black start will often involve starting multiple \"islands\" of generation (each supplying local load areas), and then synchronising and reconnecting these islands to form a complete grid. The power stations involved have to be able to accept large step changes in load as the grid is reconnected.\n\nThere are multiple methods of commencing a black start: hydro-electric dams, diesel generators, open cycle gas turbines, compressed air storage, and so on. Different generating networks take different approaches, dependent on factors such as cost, complexity, the availability of local resources (i.e. suitable valleys for dams), the interconnectivity with other generating networks, and the response time necessary for the black start process.\n\nIn the United Kingdom, the grid operator has commercial agreements in place with some generators to provide black start capacity, recognising that black start facilities are often not economic in normal grid operation. It is typical of power stations from the CEGB era to have a number of open-cycle gas turbines (i.e. no heat recovery modules attached) that can run the entirety of the plant necessary to operating a full generating unit; these would normally be started by diesel generators, fed in turn by battery backups. Once up to speed these gas turbines are capable of running the entire plant associated with the rest of the power station, negating the need to bring power in from other sources.\n\nIn the North American independent system operators, the procurement of black start varies somewhat. Traditionally, black start was provided by integrated utilities and the costs were rolled into a broad tariff for cost recovery from ratepayers. In those areas which are not part of organized electricity markets, this is still the usual procurement mechanism. In the deregulated environment, this legacy of cost-based provision has persisted, and even recent overhauls of black-start procurement practices, such as that by the ISO New England, have not necessarily shifted to competitive procurement, although deregulated jurisdictions have a bias for market solutions rather than cost-of-service (COS) solutions.\n\nIn the United States, there are currently three methods of procuring black start. The most common is cost-of-service, as it is the simplest and is the traditional method. It is currently used by the California Independent System Operator (CAISO), the PJM Interconnection and the New York Independent System Operator (NYISO). Although the exact mechanisms differ somewhat the same approach is used, namely that units are identified for black start and their documented costs are then funded and rolled into a tariff for cost recovery. The second method is a new method used by the Independent System Operator of New England (ISO-NE). The new methodology is a flat rate payment which increases black start remuneration to encourage provision. The monthly compensation paid to a generator is determined by multiplying a flat rate (in $/KWyr and referred to as the $Y value) by the unit's Monthly Claimed Capability for that month. The purpose of this change was to simplify procurement and encourage provision of the black start service. The final method of procurement is competitive procurement as used by the Electric Reliability Council of Texas (ERCOT). Under this approach ERCOT runs a market for black start services. Interested participants submit an hourly standby cost in $/hr (e.g. $70 per hour), often termed an availability bid, that is unrelated to the capacity of the unit. Using various criteria ERCOT evaluates these bids and the selected units are paid as bid, presuming an 85% availability. Each black start unit must be able to demonstrate that it can startup another unit in close proximity to begin the islanding and synchronization of the grid.\n\nIn other jurisdictions there are differing methods of procurement. The New Zealand System Operator procures the blackstart capability via competitive tender. Other jurisdictions also appear to have some sort of competitive procurement, although not as structured as ERCOT. These include the Alberta Electric System Operator, as well as Independent Electric System Operator of Ontario, both of which use a long-term \"request for proposals\" approach similar to New Zealand and ERCOT.\n\nThough black-outs are common occurrences in North America, they are virtually unknown in Germany. The first black start on a Germany's grid was tested in 2017 at WEMAG battery power station in Schwerin on a disconnected, isolated grid. The WEMAG battery plant proved that it can restore the power grid after major disruption or blackout.\n\nNot all generating plants are suitable for black-start capability. Wind turbines are not suitable for black start because wind may not be available when needed. Wind turbines, mini-hydro, or micro-hydro plants, are often connected to induction generators which are incapable of providing power to re-energize the network. The black-start unit must also be stable when operated with the large reactive load of a long transmission line. Many high-voltage direct current (HVDC) converter stations cannot operate into a \"dead\" system, either, since they require commutation power from the system at the load end. A Pulse-width modulation (PWM)-based voltage-source converter HVDC scheme has no such restriction.\n\n\n"}
{"id": "1880796", "url": "https://en.wikipedia.org/wiki?curid=1880796", "title": "Brush hook", "text": "Brush hook\n\nA brush hook (also called a bush hook, ditch blade, ditch bank blade, or ditch blade axe) is a gardening instrument resembling an axe, generally with a to curved blade and a to handle. It is commonly used by surveying crews and firefighters to clear out heavy undergrowth from trails, as well as by homeowners and gardeners to clear thick brush.\n\n"}
{"id": "3166402", "url": "https://en.wikipedia.org/wiki?curid=3166402", "title": "Design Academy Eindhoven", "text": "Design Academy Eindhoven\n\nDesign Academy Eindhoven is an interdisciplinary educational institute for art, architecture and design in Eindhoven, Netherlands. The work of its faculty and alumni have brought it international recognition.\n\nThe Design Academy Eindhoven was established in 1955 and was originally named the Akademie Industriële Vormgeving Eindhoven. In 1997, the Academy moved into \"\" (The White Lady) building and subsequently changed its name to Design Academy Eindhoven (DAE). In 1999, Li Edelkoort, was elected chairwoman of the Academy. In 2009 she left the Design Academy to pursue personal projects and was replaced by Anne Mieke Eggenkamp as chair. She was in turn succeeded by Thomas Widdershoven from 2013 until September 2016. Since 2017, Joseph Grima has taken place as CD.\n\nThe bachelor's program is split into eight interdisciplinary departments, broadly covering art, architecture, fashion design, graphic design and industrial design. As the structure of the bachelor's program is intended to be flexible, students are free to move between departments and all students graduate with the same degree, a Bachelor of Design.\n\nThe DAE also offers four distinct master's programs: M.Des Contextual Design, M.Des Social Design, M.Des Information Design, and M.Des Design Curating and Writing.\n\nThe Design Academy Eindhoven's (DAE) emphasis on engaging complex social and cultural issues has gained it an international reputation. The Dutch collective Droog Design features several DAE alumni, \"Orange Alert\" organizes exhibitions and events of DAE students work in New York and several DAE alumni have been nominated for the \"Designer of the Year\" awards at the Design Museum in London.\n\nThe book \"House of Concepts: Design Academy Eindhoven\" tells the 60-year history of the academy, outlining how many significant Dutch artists, architects and designers have been associated with the DAE.\n\nEvery year, in the fall, the Dutch Design Week is held in Eindhoven with an array of Design-related events. These include the graduation exhibit (featuring the works of that year's graduates of the DAE) and the Dutch Design Awards.\n\n\n\n\n"}
{"id": "610149", "url": "https://en.wikipedia.org/wiki?curid=610149", "title": "Detonating cord", "text": "Detonating cord\n\nDetonating cord (also called detonation cord, detacord, det. cord, detcord, primer cord or sun cord) is a thin, flexible plastic tube usually filled with pentaerythritol tetranitrate (PETN, pentrite). With the PETN exploding at a rate of approximately 6400 m/s, any common length of detonation cord appears to explode instantaneously. It is a high-speed fuse which explodes, rather than burns, and is suitable for detonating high explosives. The velocity of detonation is sufficient to use it for synchronizing multiple charges to detonate almost simultaneously even if the charges are placed at different distances from the point of initiation. It is used to reliably and inexpensively chain together multiple explosive charges. Typical uses include mining, drilling, demolitions, and warfare.\n\n\"Cordtex\" and \"Primacord\" are two of many trademarks which have slipped into use as a generic term for this material.\n\nAs a transmission medium, it can act as a downline between the initiator (usually a trigger) and the blast area, and as a trunkline connecting several different explosive charges. As a timing mechanism, detonation cord detonates at a very reliable rate (about 7,000–8,000 m/s), enabling engineers to control the pattern in which charges are detonated. This is particularly useful for demolitions, when structural elements need to be destroyed in a specific order to control the collapse of a building.\n\nWhile it looks like nylon cord, the core is a compressed powdered explosive, usually PETN (pentrite), and it is initiated by the use of a blasting cap. Detonation cord will initiate most commercial high explosives (dynamite, gelignite, sensitised gels, etc.) but will not initiate less sensitive blasting agents like ANFO on its own. 25 to 50 grain/foot (5.3 to 10.6 g/m) detonation cord has approximately the same initiating power as a #8 blasting cap in every 2 to 4 inches (5 to 10 cm) along its entire length. A small charge of PETN, TNT, or other explosive booster is required to bridge between the cord and a charge of insensitive blasting agent like ANFO or most water gels.\n\nDetonating cord is rated in explosive mass per unit length. This is expressed in grains per foot in the United States, or in grams per metre elsewhere. A \"grams per metre\" rating will be roughly one fifth the \"grains per foot\" rating. For example, \"50 grain det. cord\" refers to detonating cord which has 50 grains of explosive per foot of length—or approximately 10 g/m. This is a typical \"default\" rating for connecting charges for blasting; lighter detonating cords may be used for \"low noise blasting\" and movie special effects, while heavier cords, used where the cord is employed to have some direct explosive effect—such as for precision rock carving work—may use 50 to 250 grain/foot (10 to 50 g/m) detonating cord.\n\nLow-yield detonating cord can be used as a precision cutting charge to remove cables, pipes, wiring, fiber optics, and other utility bundles by placing one or more complete wraps around the target. Detonation cord is used in commercial boilers to break up clinkers (solidified coal ash slag) adhering to tube structures. Also a vertical centered cord being lowered into the water of a drilled well can remove any clogging that obstructs water flow. Higher-yield detonating cord can be used to cut down small trees, although the process is very uneconomical compared to using bulk explosive, or even a chainsaw. High-yield detonating cord placed by divers has been used to remove old dock pilings and other underwater obstructions. Creating a slipknot from detonating cord yields a field-improvised device that can be quickly employed to cut a locked doorknob off a door. Detonation cord can be taped in several rings to the outline of a military man-sized target and detonated, breaching a man-sized hole into wooden doors or light interior walls. Detonating cord is also employed directly in building demolition where thin concrete slabs need be broken via channels drilled parallel to the surface, an advantage over dynamite since a lower minimum of explosive force may be used and smaller diameter holes are sufficient to contain the explosive. Anything much more substantial than these uses requires the use of additional explosives.\n\nThe Finnish army colloquialism for detonation cord is \"anopin pyykkinaru\" (mother-in-law's clothesline), as it resembles ordinary clothesline.\n\nIn Filipino, the corresponding word \"mitsa\" has come to be used in a phrase “mitsa ng buhay” which translates to \"detonating cord of [one’s] life\", a metaphor for something that is very likely to cause one’s death via direct jeopardization (e.g. extreme sports, versus smoking).\n\nDetonation cord was referenced in the 2009 film \"A Perfect Getaway\" by Timothy Olyphant's character as \"a handy tool\".\n\nA length of detonation cord was used to clear a path through a minefield in the 2009 film \"Terminator Salvation\".\n\nDetonation cord was shown to be used by US cavalry troops to clear trees from a landing zone in the 2002 movie \"We Were Soldiers\".\n\nA spool of primer cord was used by Alan Alda's character Hawkeye in the TV series \"M.A.S.H.\" (Season 9, Episode 12, \"Depressing News\") to demolish his newly built replica of the Washington Monument crafted from tongue depressors.\n\nFiona Glenanne from the TV show Burn Notice uses detonating cord very often to breach enemy buildings.\n\n"}
{"id": "27632337", "url": "https://en.wikipedia.org/wiki?curid=27632337", "title": "Development Driller III", "text": "Development Driller III\n\nDevelopment Driller III is a fifth generation, Vanuatu-flagged dynamic positioning semi-submersible ultra-deepwater drilling rig owned by Transocean and operated under lease agreements by various petroleum exploration and production companies worldwide. The vessel is capable of drilling in water depths up to with drilling depth of , upgradeable to .\n\nOn May 2, 2010, \"Development Driller III\" started drilling a relief well on Macondo Prospect to stop massive oil spill caused by the explosion and subsequent loss of the \"Deepwater Horizon\".\n\n"}
{"id": "31388746", "url": "https://en.wikipedia.org/wiki?curid=31388746", "title": "Digital Scholarship in the Humanities", "text": "Digital Scholarship in the Humanities\n\nDigital Scholarship in the Humanities is a peer-reviewed academic journal of the European Association for Digital Humanities that covers all aspects of computing and information technology applied to Arts and Humanities research. It is the main journal in the field of Digital Humanities. The journal is published by Oxford University Press. The journal was formerly known as \"Literary and Linguistic Computing\", but was renamed to emphasise a broadening in its subject focus beyond literary studies. \n"}
{"id": "4306579", "url": "https://en.wikipedia.org/wiki?curid=4306579", "title": "Doug Richard", "text": "Doug Richard\n\nDoug Richard (born May 6, 1958) is an American entrepreneur best known for his television appearances in the United Kingdom. He appeared as a \"Dragon\" on the first two series of \"Dragons' Den\" and was also a government adviser. \n\nRichard received his undergraduate degree from University of California at Berkeley majoring in psychology in 1980. He received his Doctorate of Law at University of California at Los Angeles in 1985. He received his Executive Management Certificate from UCLA School of Business in 1989.\n\nRichard founded his first company, ITAL Computers in 1985, which sold services that integrated computer aided design and manufacture systems to the southern California aerospace industry. ITAL Computers was sold in a private transaction in 1991 and the profits used to found his second company, Visual Software. Richard co-founded, managed and sold Visual Software with his partner John Halloran. Visual Software was sold to Micrografx, a NASDAQ listed public company for $12,000,000 in shares in 1996. In 1997, Richard became the president and CEO of Micrografx, the company by whom he had been acquired the year before. Richard turned around the fortunes of Micrografx by shifting its focus from consumer software to business and technical marketplaces. Micrografx was sold to Corel Corporation in 2001.\n\nAfter the sale of Corel Corporation, Richard re-located to Cambridge with his family. Shortly after the move, Richard co-founded the Cambridge Angels, an angel investment group focusing on technology startups in the Cambridge region, with Robert Sansom in 2001. He retired from the Cambridge Angels in 2008.\n\nRichard was an active angel investor from 2001-2008. His first investment was in a Manchester based startup, Designer Servers, known as DSVR. Richard and the founders successfully exited from the company in 2004 when it was sold to the company that became Legend Communications, PLC.\n\nAlso in 2001, Richard co-founded Library House with a group of entrepreneurs and angel investors from the Cambridge Cluster. Library House was founded as a buy-side research house focusing on technology startups and be-spoke analysis for venture capital firms. The financial downturn in 2008 led venture firms to reduce their investments in research forcing Library House into administration as another victim of the global downturn. Its database of transactions, which was the only database of European venture activity, was sold to Dow Jones.\n\nIn 2004, Richard co-founded Trutap, with David Whitewood, a mobile software company offering free text and interactive messaging across the internet, prior to the introduction of smart phones. Trutap received three rounds of funding from two investors, Tudor Investments and the Cambridge Angels. The company closed in good order and the remaining investment was returned to Investors after the company was unable to pivot with the introduction and success of the iPhone.\n\nIn 2008, Richard founded School for Startups Ltd.\n\nIn late 2004, Richard was approached by the BBC to join a new reality TV show called \"Dragons' Den\". Richard was joined by Peter Jones, Duncan Bannatyne, Rachel Elnaugh and Simon Woodroffe as the first 'Dragons'. Richard appeared on the first two series of the programme. He voluntarily stepped down after the end of the second series after accepting a position as a venture partner with the hedge fund Tudor Investments.\n\nIn 2006, he received an honorary Queen's Award for Enterprise Promotion. The award was honorary as Richard was a US citizen. \nIn 2009, Richard was awarded an honorary doctorate from the University of Essex for his work supporting entrepreneurship.\n\nIn 2010, he was awarded Enterprise Educator of the Year by National Council on Graduate Entreprise.\n\nIn 2013, Richard was awarded an honorary doctorate from University of Plymouth for his work with Plymouth in supporting the growth of small business in the south west.\n\nRichard was appointed as an ambassador for the British Library’s Business and IP Centre.\n\nHe is a fellow of the Royal Society of the Arts.\n\nIn 2008, Richard wrote and published the Richard Report on Small Business, as the Chairman of the Conservative Party Task Force, a policy guideline on support for small business for the Conservative Party while in opposition.\n\nIn 2010, Richard published the “Entrepreneurs Manifesto”, a call-to-action to drive entrepreneurship in the UK.\n\nIn 2012, Richard published the Richard Review of Apprenticeships, a government requested review of the apprenticeship system which was supported by all political parties and formed the basis for the reform of the apprenticeship system in the UK.\n\nIn 2013, Richard published his first Book, “How to Start a Creative Business”, on entrepreneurship in the creative industries.\n\nRichard served as a Counsellor at One Young World in 2012, 2013 & 2015.\n\nRichard is a former non-executive director of Innovate UK, the UK's Innovation Agency.\n\nRichard founded School for Startups in 2008 to help people start their own businesses through training and support courses, and to help governments and regions drive economic growth based on the principles of entrepreneurial economics. The school has taught over 30,000 people how to start their own business. The school is best known for a series of high-profile programmes including:\nMore recently Richard has focussed School for Startups on programs in developing countries. Recent programs include:\n\nIn January 2015, the \"Daily Mirror\" reported that Richard had been arrested on suspicion of having sex with an underage girl. Richard strongly denied the allegations.\n\nOn 7 September 2015, Richard was charged with three counts of sexual activity with a child and one count of causing or inciting a child to engage in sexual activity. He appeared before the City of London Magistrates' Court on 5 October and was bailed until his appearance at the Old Bailey in January 2016 for trial.\n\nOn 25 January 2016, Richard appeared at the Old Bailey, where he admitted sexual activity did occur but said it was consensual, believing her to be 17 at the time. Richard had met the girl through a dating website called SeekingArrangement. The court heard that the 13-year-old girl in question told Richard on a number of occasions that she was 17. On 29 January 2016, Richard was cleared of the child sex offences brought against him.\n"}
{"id": "1370229", "url": "https://en.wikipedia.org/wiki?curid=1370229", "title": "Electrorheological fluid", "text": "Electrorheological fluid\n\nElectrorheological (ER) fluids are suspensions of extremely fine non-conducting but electrically active particles (up to 50 micrometres diameter) in an electrically insulating fluid. The apparent viscosity of these fluids changes reversibly by an order of up to 100,000 in response to an electric field. For example, a typical ER fluid can go from the consistency of a liquid to that of a gel, and back, with response times on the order of milliseconds. The effect is sometimes called the Winslow effect after its discoverer, the American inventor Willis Winslow, who obtained a US patent on the effect in 1947 and wrote an article published in 1949.\n\nThe normal application of ER fluids is in fast acting hydraulic valves and clutches, with the separation between plates being in the order of 1 mm and the applied potential being in the order of 1 kV. In simple terms, when the electric field is applied, an ER hydraulic valve is shut or the plates of an ER clutch are locked together, when the electric field is removed the ER hydraulic valve is open or the clutch plates are disengaged. Other common applications are in ER brakes (think of a brake as a clutch with one side fixed) and shock absorbers (which can be thought of as closed hydraulic systems where the shock is used to try to pump fluid through a valve).\n\nThere are many novel uses for these fluids. Potential uses are in accurate abrasive polishing and as haptic controllers and tactile displays.\n\nER fluid has also been proposed to have potential applications in flexible electronics, with the fluid incorporated in elements such as rollable screens and keypads, in which the viscosity-changing qualities of the fluid allowing the rollable elements to become rigid for use, and flexible to roll and retract for storing when not in use. Motorola filed a patent application for mobile device applications in 2006.\n\nThe change in apparent viscosity is dependent on the applied electric field, i.e. the potential divided by the distance between the plates. The change is not a simple change in viscosity, hence these fluids are now known as ER fluids, rather than by the older term Electro Viscous fluids. The effect is better described as an electric field dependent shear yield stress. When activated an ER fluid behaves as a Bingham plastic (a type of viscoelastic material), with a yield point which is determined by the electric field strength. After the yield point is reached, the fluid shears as a fluid, i.e. the incremental shear stress is proportional to the rate of shear (in a Newtonian fluid there is no yield point and stress is directly proportional to shear). Hence the resistance to motion of the fluid can be controlled by adjusting the applied electric field.\n\nER fluids are a type of smart fluid. A simple ER fluid can be made by mixing cornflour in a light vegetable oil or (better) silicone oil.\n\nThere are two main theories to explain the effect: the interfacial tension or 'water bridge' theory, and the electrostatic theory. The water bridge theory assumes a three phase system, the particles contain the third phase which is another liquid (e.g. water) immiscible with the main phase liquid (e.g. oil). With no applied electric field the third phase is strongly attracted to and held within the particles. This means the ER fluid is a suspension of particles, which behaves as a liquid. When an electric field is applied the third phase is driven to one side of the particles by electro osmosis and binds adjacent particles together to form chains. This chain structure means the ER fluid has become a solid. The electrostatic theory assumes just a two phase system, with dielectric particles forming chains aligned with an electric field in an analogous way to how magnetorheological fluid (MR) fluids work. An ER fluid has been constructed with the solid phase made from a conductor coated in an insulator. This ER fluid clearly cannot work by the water bridge model. However, although demonstrating that some ER fluids work by the electrostatic effect, it does not prove that all ER fluids do so. The advantage of having an ER fluid which operates on the electrostatic effect is the elimination of leakage current, i.e. potentially there is no direct current. Of course, since ER devices behave electrically as capacitors, and the main advantage of the ER effect is the speed of response, an alternating current is to be expected.\n\nThe particles are electrically active. They can be ferroelectric or, as mentioned above, made from a conducting material coated with an insulator, or electro-osmotically active particles. In the case of ferroelectric or conducting material, the particles would have a high dielectric constant. There may be some confusion here as to the dielectric constant of a conductor, but \"if a material with a high dielectric constant is placed in an electric field, the magnitude of that field will be measurably reduced within the volume of the dielectric\" (see main page: Dielectric constant), and since the electric field is zero in an ideal conductor, then in this context the dielectric constant of a conductor is infinite.\n\nAnother factor that influences the ER effect is the geometry of the electrodes. The introduction of parallel grooved electrodes showed slight increase in the ER effect but perpendicular grooved electrodes doubled the ER effect. A much larger increase in ER effect can be obtained by coating the electrodes with electrically polarisable materials. This turns the usual disadvantage of dielectrophoresis into a useful effect. It also has the effect of reducing leakage currents in the ER fluid.\n\nThe giant electrorheological (GER) fluid was discovered in 2003, and is able to sustain higher yield strengths than many other ER fluids. The GER fluid consists of Urea coated nanoparticles of Barium Titanium Oxalate suspended in silicone oil. The high yield strength is due to the high dielectric constant of the particles, the small size of the particles and the Urea coating. Another advantage of the GER is that the relationship between the electrical field strength and the yield strength is linear after the electric field reaches 1 kV/mm. The GER is a high yield strength, but low electrical field strength and low current density fluid compared to many other ER fluids. The procedure for preparation of the suspension is given in. The major concern is the use of oxalic acid for the preparation of the particles as it is a strong organic acid.\n\nA major problem is that ER fluids are suspensions, hence in time they tend to settle out, so advanced ER fluids tackle this problem by means such as matching the densities of the solid and liquid components, or by using nanoparticles, which brings ER fluids into line with the development of magnetorheological fluids. Another problem is that the breakdown voltage of air is ~ 3 kV/mm, which is near the electric field needed for ER devices to operate.\n\nAn advantage is that an ER device can control considerably more mechanical power than the electrical power used to control the effect, i.e. it can act as a power amplifier. But the main advantage is the speed of response, there are few other effects able to control such large amounts of mechanical or hydraulic power so rapidly.\n\nUnfortunately, the increase in apparent viscosity experienced by most Electrorheological fluids used in shear or flow modes is relatively limited. The ER fluid changes from a Newtonian liquid to a partially crystalline \"semi-hard slush\". However, an almost complete liquid to solid phase change can be obtained when the electrorheological fluid additionally experiences compressive stress. This effect has been used to provide electrorheological Braille displays and very effective clutches.\n\n"}
{"id": "47556865", "url": "https://en.wikipedia.org/wiki?curid=47556865", "title": "European environmental research and innovation policy", "text": "European environmental research and innovation policy\n\nThe European environmental research and innovation policy is a set of strategies, actions and programmes to promote more and better research and innovation for building a resource-efficient and climate resilient society and economy in sync with the natural environment. It is based on the Europe 2020 strategy for a smart, sustainable and inclusive economy and it realises the European Research Area (ERA) and Innovation Union in the field of environment.\nThe aim of the European environmental research and innovation policy is to contribute to a transformative agenda for Europe in the coming years, where the quality of life of the citizens and the environment are steadily improved, in sync with the competitiveness of businesses, the societal inclusion and the management of resources.\n\nThe European environmental research and innovation policy has a multidisciplinary character and involve efforts across many different sectors to provide safe, economically feasible, environmentally sound and socially acceptable solutions along the entire value chain of human activities. To reduce resource use and environmental impacts whilst increasing competitiveness requires a decisive societal and technological transition to an economy based on a sustainable relationship between nature and human well-being. The availability of sufficient raw materials is addressed as well as the creation of opportunities for growth and new jobs. Innovative options are developed in policies ranging across science, technology, economy, regulations, society and citizens’ behavior, and governance. Research and innovation activities improve the understanding and forecasting of climate and environmental change in a systemic and cross-sectoral perspective, reduce uncertainties, identify and assess vulnerabilities, risks, costs, mitigation measures and opportunities, as well as expand the range and improve the effectiveness of societal and policy responses and solutions.\n\nThe European environmental research and innovation policy was placed in the context of the process at the United Nations to develop a set of Sustainable Development Goals (SDGs) that were agreed at the Rio+20 Conference on Sustainable development in 2012 and are now integrated into the United Nations development agenda beyond 2015. These goals have succeeded the Millennium Development Goals and are universally applicable to all Nations, hence also to the European Union and its Member States.\n\nThe implementation of the European environmental research and innovation policy relies on a systemic approach to innovation for a system-wide transformation. For large extent, it is carried out through the Framework Programmes for Research and Technological Development.\n\nThe current Framework Programme is called Horizon 2020 and environmental research and innovation is envisaged across the entire programme with an interdisciplinary approach. Current price estimates suggest that more than 6,5 € billion per year could be made available for activities related to sustainable development during the duration of Horizon 2020, addressing both research and innovation differently from previous FPs. Horizon 2020 is open to cooperation with researchers and innovators world-wide in order to foster co-design and co-creation of solutions that may have a global impact.\n\nNew calls for research and innovation proposals have been opened on 14 October 2015. Information is contained in the Horizon 2020 participant portal.\n\n"}
{"id": "44962990", "url": "https://en.wikipedia.org/wiki?curid=44962990", "title": "Grid compass", "text": "Grid compass\n\nA grid compass known as well as grid steering compass, is a navigating instrument. It is a design of magnetic compass that facilitates steering a steady course without the risk of parallax error.\n\nThe \"grid compass\" is the simplest steering compass from the pilot's or helmsman's point of view, because he doesn't need to watch the number (or the division mark) of the wanted course. He has only to steer the craft so that the N/S compass needle lies parallel between the lines of the overlay disc. The principle is similar to the compass-controlled autopilot. Although sophisticated electronics have taken over for commercial navigation, light aircraft, gliders and yachtsmen still use the grid compass because of its simplicity and ease of use.\n\nThe compass card is in the form of a bold parallel sided arrow which indicates magnetic north. Some models have an east/west cross bar as well. Overalying this but in the same gimbal or suspension is a transparent plate which can be rotated around the same axis as the compass card but has sufficient friction (or a mechanical clamp) to stay fixed relative to the gimbal system once set to a course. Across this disk are engraved a series of parallel lines. The outer edge of this disk is marked in clockwise in degrees, the radial line meeting 0º being parallel to the engraved lines, so that a course can be laid for any bearing from 0º to 359º. By keeping the arrow on the card and the lines on the overlay parallel, the pilot or helmsman can keep the course set. The frequency of the degree markings depend upon the size of the compass.\n\nTo set a course the rotating ring is (unlocked and) turned so that the heading in degrees on the ring alines with the centre line of the craft. The craft comes on to the required course when the arrow on the compass card is parallel with the lines on the ring.\n\nThe grid steering compasses (Type P8 to Type P11) were fitted in World War II Spitfire aeroplanes, replacing the old P4 series of instruments. They were used for course setting and reading, and as a check compass on aircraft fitted with a remote indicating compass.\n\n\n\n"}
{"id": "4596704", "url": "https://en.wikipedia.org/wiki?curid=4596704", "title": "HERO (robot)", "text": "HERO (robot)\n\nHERO (Heathkit Educational RObot) is the name of several educational robots sold by Heathkit during the 1980s. The Heath Company began the HERO 1 project in October 1979. The first units were available in 1982. Models included the HERO 1, HERO Jr., and HERO 2000. Heathkit supported the HERO robot line up until 1995. All three were available as kits, or for more money, prebuilt by Heathkit. Since 2013 the 1980s models are considered collectors items, due to their apparent rarity. For the most part, they cannot perform practical tasks, but are more geared toward entertainment and education above all.\n\nHERO 1 was a self-contained mobile robot controlled by an onboard computer with a Motorola 6808 CPU and 4 kB of RAM. The robot featured light, sound, and motion detectors as well as a sonar ranging sensor. An optional arm mechanism and speech synthesizer was available for the kit form and included in the assembled form.\n\nTo make this power available in a simple way, high-level programming languages were created. For example, the ANDROTEXT language was a HERO 1 editor and compiler developed in 1982 for the IBM PC.\n\nHERO 1 was featured on a few episodes of the children's television program \"Mr. Wizard's World\". \"Byte\" magazine called HERO 1 \"a product of extraordinary flexibility and function ... If you are interested in robotics, Heath will show you the way\".\n\nA smaller version of HERO was released later, called HERO Jr. Heathkit intended it for the home market, and therefore made it less complex, and more self-contained. Like HERO 1, HERO Jr. had a 6808 processor, but only 2 kB of RAM. As well, it sported onboard speech synthesis, a Polaroid sonar range sensor, a light sensor, and a sound sensor. An optional infrared sensor was available as well. Other optional components included a pair of extra batteries to double the operational time between charges, from an estimated 4 hours to 8 hours. A remote control accessory allowed users to drive the robot around. It included a motion sensor that caused the robot to croak \"SOM-THING-MOVE\" when it detected a source of motion.\n\nHeathkit released several add-ons to increase the robot's capabilities, including a transmitter to activate a home security system in the event it sensed movement while on \"guard duty\". Also, additional cartridges with programs and games were available, as well as a components to allow the user to directly program the robot.\n\nThe drive mechanism is backward compared to the HERO 1, with the drive and steering wheel in the back of the robot. The head section featured an indentation to allow the robot to transport up to . The robot could speak several phrases from various films that either involved robots or computers. It was also capable of remembering and repeating back its masters name, as well as singing songs, reciting poems, acting as an alarm clock, and making its own combinations of phonemes to create a robotic gibberish.\n\nThe much more powerful HERO 2000 included several onboard microprocessors, onboard speech synthesis, several sensors, and the ability to add expansion cards using a passive backplane.\n\nThe HE-RObot was the result of a strategic partnership between Heathkit and White Box Robotics. When available, it cost as much as $8000. The HE-RObot was marketed to the educational market. Heathkit sold approximately 50 of these robots before their bankruptcy in 2012.\n\n"}
{"id": "2096519", "url": "https://en.wikipedia.org/wiki?curid=2096519", "title": "Hand truck", "text": "Hand truck\n\nA hand truck, also known as a two wheeler, stack truck, trundler, box cart, sack barrow, cart, dolly, sack truck, or bag barrow, is an L-shaped box-moving handcart with handles at one end, wheels at the base, with a small ledge to set objects on, flat against the floor when the hand-truck is upright. The objects to be moved are tilted forward, the ledge is inserted underneath them, and the objects allowed to tilt back and rest on the ledge. The truck and object are then tilted backward until the weight is balanced over the large wheels, making otherwise bulky and heavy objects easier to move. It is a first-class lever.\n\nSack trucks were originally used in the 18th century to move large sacks of spices on docks by young boys, from the age of 11, who were unable to lift the large sacks by hand. By using this method they were able to work as well as grown men in moving items around. Later, such trucks were amended for use in many different industries, such as brewing, where hops were moved in sacks.\n\nSome hand trucks are equipped with stairclimber wheels, which, as the name implies, are designed to go up and down stairs. Stairclimber wheels can sometimes be problematic when trying to turn on flat ground as four wheels in a fixed position will be in contact with the ground.\n\nHand trucks are fabricated from many different types of materials, including steel tube, aluminum tube, aluminum extrusion and high impact plastics. Most commercial hand trucks used for beverage and food service deliveries are rugged and very light. They are usually constructed from two extruded aluminum channel side rails and cast aluminum or magnesium parts.\n\nSome of the options that may be considered are the types of wheels, stair climber, handle type and size of wheels.\n\nOther things to be considered should be the load shape compared with the backrest shape, \"e.g.\", cylindrical loads should sit on curved backrests, and the environmental conditions in which the hand truck will operate. For example, on loose or uneven ground oversize wheels are a great advantage; solid or puncture-proof foam filled tires may be used where punctures could deflate pneumatic tires.\n\nA rule of thumb is that the toe or nose of the truck should be at least one-third of the length of the load.\n\nHand trucks are sometimes used as baggage carts by porters in train stations and skycaps at airports.\n\nA piano tilter is a type of hand truck for moving an upright piano without damaging it. Unlike a traditional dolly which pivots around a smaller wheel or point, the piano tilter has large curved sections to gently tilt an upright piano until it is lying flat on its back.\n\n"}
{"id": "21673323", "url": "https://en.wikipedia.org/wiki?curid=21673323", "title": "Henry Milward &amp; Sons", "text": "Henry Milward &amp; Sons\n\nHenry Milward & Sons (now known as ENTACO) is an English manufacturer of sewing needles based in Redditch.\n\nThe earliest reference to the Milward family in connection with needle making is a James Milward who was a needle maker on Fish Hill in 1676.\nSymon Milward created the company of Henry Milward & Sons aka Milward's Needles (Milward's) in 1730 at the age of 40, in Redditch, United Kingdom. It was however, his son Henry who takes credit for the foundation of the company as the company was registered in his name during the first year of his birth.[see Redditch Museum Family tree] \nFrom the first half of the 18th century, the name of Henry Milward and Sons became well known as the makers of good quality needles.\n\nAt one point they were the largest manufacturer of its type in the world, producing knitting needles, surgical needles, and fishing tackle, from a number of factories both in the UK and globally, such as Murcia, Spain.\n\nThe manufacture of needles was originally a cottage industry. During the Industrial Revolution the industry prospered and became mechanised. By the late 1800s the company was the largest manufacturer in the whole district.\n\nRedditch became the hub of the needle industry in the UK, with trade directories showing that in the Redditch area in 1868/9 there were 117 firms listed as manufacturers in various aspects of needle and fish tackle trades. Small needle businesses amalgamated, often through inter-marriage between the leading needle makers of Redditch. Henry Milward and sons amalgamated with John James, W. Avery & Son, Wm. Bartlett, and thus the \"Milward empire\" grew.\n\nWilliam Hall of Studley amalgamated with Thos. Harper, Samuel Thomas, H. Wilkes and William Vale among others and formed a group of needle makers known as Amalgamated Needles & Fish Hooks. This resulted in Milwards and the aforementioned group being the 2 largest needle makers in the district. \n\nIn 1912 John James and Sons merged into Henry Milward and sons and a new board was announced. Members of the board consisted of C.B James as chairman and C.F Milward as deputy chairman, H.T Milward, A.D Barfleet and C.E James.\n\nIn 1930 two of the great needlemaker giants Milward's of Redditch and Hall's of Studley joined forces under a single holding company, Amalgamated Needles and Fish Hooks Limited, who formed in 1932 a separate manufacturing organisation called The English Needle and Fishing Tackle Co. Ltd (ENTACO).\n\nHowever Milward's needles continued in Redditch until the 1950s, there were satellite factories setup during World War II in Evesham, Bewdley, Kidderminster and Moreton-in-Marsh. The basic needle plant was dispersed between Studley and Redditch during the war and in 1952 Arrow Works was officially opened and were still producing some brands of Milward's needles as were first made and marketed under the name of Henry Milward & Sons.\n\nThe years of greatest growth both in factory growth and world sales were at the end of the 19th century when the fortunes of the company were in the hands of Charles Frederick and Henry Tomson.\n\nColonel Henry Milward was a fervent fly angler who decided to mix business with pleasure when he created Milward’s Fishing Tackle Company as part of his centuries-old needle-factory. A dozen workers left needle- and syringe-making to become experts in hooks, flies, devons, spoons and split-cane rods. This remarkable adventure lasted until 1965, when Milward died. \n\nMilward, alongside Allcock, Partridge, Wilkins, Lee, Martinez & Bird and J.W. Young was just one of many fishing tackle manufacturers located in Redditch, just south of Birmingham\n\nColonel C.F. Milward, Deputy Lieutenant, was born in the United States and returned to England at the age of 5 and in due course he entered the family business, he was responsible for the companies amalgamations prior to 1930, he became the major chairman of the board of the holding company A.N. & F.H. Limited. His father was a justice of the peace.\n\nHe was elected to the Mastership of the Livery Company, The Worshipful Company of Needle Makers, and was High Sheriff of Worcestershire. He was known as the Colonel.\n\nH.T Milward, justice of the peace, was known as Mr Harry, and was the Colonel's cousin. Harry's father was Colonel Victor Milward, a Member of Parliament. Harry's main interest in the family business was sales and included Russia in his sales trips.\n\nIn 1930, the board of directors were: Charles F Milward (Chairman of the company), Henry T Milward J.P, G.H. Corbet-Milward, C. Ernest James, Gerald Bartleet, Arthur D. Bartleet, and L. Sidney Milward.\n\nWhen these 2 great figures died, others took over the running of the company, now renamed Needle Industries Limited, but still the name Milward was the major trading company.\n\nThe next Joint Managing Directors were Basil King OBE and Alderman L. Haines of William Hall and Company, whilst Colonel G.V. Milward DL of Worcestershire, was a director of the company. Colonel Milward became the Managing Director of Milwards Fishing Tackle. He was also responsible for Surgical needles and braided suture manufacture after he returned to the company after he saw active service between 1939-1945.\n\nDuring the term of G.B King's Managing Directorship that Arrow Works in Studley was built, thus creating the business as we knew it to this present day. During this phase, the last of the Milward family was in the business: Ian Newton, grandson of Mr Harry briefly was Sales Manager (Needles) until 1968.\n\nSince 1952 the fortunes of the Milward business were in the hands of L.H.Beare and in the 1980s J.A Mackrael was the managing director. The trend of taking over companies continued, taking over Abel Morrall's Aero Needles (1989). Although Milwards owned many companies, each retained their own trade mark and name!\n\nIn 1973 the entire share capital was purchased by Coats Patons Ltd, now known as Coats PLC. Needle Industries continued until the 1990s when Coats sold it to its management. Coats retained the Milwards brand name for its own use.\n\nFollowing the management buyout of Needle Industries, led by Victor Barley, a new company was registered in 1991 called ENTACO, (an acronym of English Needle & Tackle Company) in the old Victoria Works, adjacent to Needle Industries. Mr Barley was Managing Director of Entaco from 1991-1997. \n\nEntaco were involved in a cartel with Prym and Coats at the later end of the 20th century.\n\nHenry Milward and Sons and its employees boast over a quarter of a millennium making needles.\n"}
{"id": "8391019", "url": "https://en.wikipedia.org/wiki?curid=8391019", "title": "History of perfume", "text": "History of perfume\n\nThe word \"perfume\" is used today to describe scented mixtures and is derived from the Latin word, \"\"per fumus\",\" meaning \"through smoke\". The word Perfumery refers to the art of making perfumes. Perfume was further refined by the Romans, the Persians and the Arabs. Although perfume and perfumery also existed in East Asia, much of its fragrances were incense based. The basic ingredients and methods of making perfumes are described by Pliny the Elder in his Naturalis Historia.\n\nThe world's first recorded chemist is a woman named Tapputi, a perfume maker whose existence was recorded on a 1200 BCE Cuneiform tablet in Babylonian Mesopotamia. She held a powerful role in the Mesopotamian government and religion, as the overseer of the Mesopotamian Royal Palace. She developed methods for scent extraction techniques that would lay the basis for perfume making. She recorded her techniques and methods and those were passed on, with her most groundbreaking technique in using solvents.\n\nPerfume and perfumery also existed in Indus civilization, which existed from 3300 BCE to 1300 BCE. One of the earliest distillation of Ittar was mentioned in the Hindu Ayurvedic text \"Charaka Samhita\" and \"Sushruta Samhita\". The perfume references are part of a larger text called Brihat-Samhita written by Varāhamihira, an Indian astronomer, mathematician, and astrologer living in the city of Ujjain. He was one of the ‘nine jewels’ in the court of the Maharaja of Malwa. The perfume portion mainly deals with the manufacture of perfumes to benefit ‘royal personages and inmates of harems’. The text is written as Sanskrit slokas with commentary by a 10th century Indian commentator Utpala.\n\nAccording to a 1975 report, archeologist Dr Paolo Rovesti found a terracotta distillation apparatus in the Indus valley together with oil containers made of the same material, carbon dated to 3000 BCE. The report also states that terracotta vessels with plugged orifices of woven materials were used so that when fragrant plant materials were covered with boiling water the vapours impregnated the material, which was subsequently wrung out to isolate the oil.\n\nTo date, the oldest perfumery was discovered on the island of Cyprus. Excavations in 2004-2005 under the initiative of an Italian archaeological team unearthed evidence of an enormous factory that existed 4,000 years ago during the Bronze Age. This covered an estimated surface area of over 4,000m² indicating that perfume manufacturing was on an industrial scale. The news of this discovery was reported extensively through the world press and many artifacts are already on display in Rome.\nThe Bible describes a sacred perfume (Exodus 30:22-33) consisting of liquid myrrh, fragrant cinnamon, fragrant cane, and cassia. Its use was forbidden, except by the priests.\nThe women wore perfume to present their beauty.\n\nIslamic cultures contributed significantly to the development of Western perfumery in two significant areas: perfecting the extraction of fragrances through steam distillation and introducing new raw materials. Both have greatly influenced Western perfumery and scientific developments, particularly chemistry.\n\nWith a rise of Islam, Muslims improved perfume production and continued to use perfumes in daily life and in practicing religion. They used musk, roses and amber, among other materials. As traders, Islamic cultures such as the Arabs and Persians had wider access to a wide array of spices, resins, herbs, precious woods, herbs and animal fragrance materials such as ambergris and musk. In addition to trading, many of the flowers and herbs used in perfumery were cultivated by the Muslims — rose and jasmine were native to the region, and many other plants (i.e.: bitter orange and other citrus trees, all of which imported from China and southeast Asia) could be successfully cultivated in the Middle East, and are to this day key ingredients in perfumery.\n\nIn Islamic culture, perfume usage has been documented as far back as the 6th century and its usage is considered a religious duty. Muhammad said:\n\nThey often used to blend extracts with the cement of which mosques were built. Such rituals gave incentives to scholars to search and develop a cheaper way to produce incenses and in mass production.\n\nMany great discoveries originates from the region in 10th century, the time when the still was invented, and as a consequence of which the distill techniques considerably improved. Thanks to the hard work of two talented Arabian chemists: Jābir ibn Hayyān (Geber, born 722, Iraq), and Al-Kindi (Alkindus, born 801, Iraq) who established the perfume industry. Jabir developed many techniques, including distillation, evaporation and filtration, which enabled the collection of the odour of plants into a vapour that could be collected in the form of water or oil.\n\nAl-Kindi, however, was the real founder of perfume industry as he carried out extensive research and experiments in combining various plants and other sources to produce a variety of scent products. He elaborated a vast number of ‘recipes’ for a wide range of perfumes, cosmetics and pharmaceuticals. His work in the laboratory is reported by a witness who said:\n\nThe writer goes on in the same section to speak of the preparation of a perfume called \"ghaliya\", which contained musk, amber and other ingredients; too long to quote here, but which reveals a long list of technical names of drugs and apparatus. Al-Kindi also wrote in the 9th century a book on perfumes which he named ‘\"Book of the Chemistry of Perfume and Distillations\"’. It contained more than a hundred recipes for fragrant oils, salves, aromatic waters and substitutes or imitations of costly drugs. The book also described one hundred and seven methods and recipes for perfume-making, and even the perfume making equipment, like the alembic, still bears its Arabic name.\n\nThe Persian Muslim doctor and chemist Avicenna (also known as Ibn Sina) introduced the process of extracting oils from flowers by means of distillation, the procedure most commonly used today. He first experimented with the rose. Until his discovery, liquid perfumes were mixtures of oil and crushed herbs, or petals which made a strong blend. Rose water was more delicate, and immediately became popular. Both of the raw ingredients and distillation technology significantly influenced western perfumery and scientific developments, particularly chemistry.\n\nEventually perfume arrived to European courts through Al-Andalus in the west, and on the other side, with the crusaders in the east. For instance, eggs and floral perfumes were brought to Europe in the 11th and 12th centuries from Arabia, by returning crusaders, through trade with the Islamic world. Those who traded for these were most often also involved in trade for spices and dyestuffs. There are records of the Pepperers Guild of London, going back to 1179; which show them trading with Muslims in spices, perfume ingredients and dyes. Catharina de Medici initiated the perfume industry in Europe when she left Italy in the 16th century to marry the French crown prince.\n\nKnowledge of something perfumery came to Europe as early as the 14th century due partially to Arabic influences and knowledge. But it was the Hungarians who ultimately introduced the first modern perfume. The first modern perfume, made of scented oils blended in an alcohol solution, was made in 1370 at the command of Queen Elizabeth of Hungary and was known throughout Europe as Hungary Water. The art of perfumery prospered in Renaissance Italy, and in the 16th century, Italian refinements were taken to France by Catherine de' Medici's personal perfumer, Rene le Florentin. His laboratory was connected with her apartments by a secret passageway, so that no formulas could be stolen en route.\n\nFrance quickly became the European center of perfume and cosmetic manufacture. Cultivation of flowers for their perfume essence, which had begun in the 14th century, grew into a major industry in the south of France mainly in Grasse now considered the world capital of perfume. During the Renaissance period, perfumes were used primarily by royalty and the wealthy to mask body odors resulting from the sanitary practices of the day. Partly due to this patronage, the western perfumery industry was created.\nPerfume enjoyed huge success during the 17th century. Perfumed gloves became popular in France and in 1656, the guild of glove and perfume-makers was established. Perfumers were also known to create poisons; for instance, a French duchess was murdered when a perfume/poison was rubbed into her gloves and was slowly absorbed into her skin.\n\nPerfume came into its own when Louis XV came to the throne in the 18th century. His court was called \"la cour parfumée\" (the perfumed court). Madame de Pompadour ordered generous supplies of perfume, and King Louis demanded a different fragrance for his apartment every day. The court of Louis XIV was even named due to the scents which were applied daily not only to the skin but also to clothing, fans and furniture. Perfume substituted for soap and water. The use of perfume in France grew steadily. By the 18th century, aromatic plants were being grown in the Grasse region of France to provide the growing perfume industry with raw materials. Even today, France remains the centre of the European perfume design and trade.\n\nAfter Napoleon came to power, exorbitant expenditures for perfume continued. Two quarts of violet cologne were delivered to him each week, and he is said to have used sixty bottles of double extract of jasmine every month. Josephine had stronger perfume preferences. She was partial to musk, and she used so much that sixty years after her death the scent still lingered in her boudoir.\n\nPerfume use peaked in England during the reigns of Henry VIII (reigned 1509-1547) and Queen Elizabeth I (reigned 1558-1603). All public places were scented during Queen Elizabeth's rule, since she could not tolerate bad smells. It was said that the sharpness of her nose was equaled only by the slyness of her tongue. Ladies of the day took great pride in creating delightful fragrances and they displayed their skill in mixing scents.\n\nAs with industry and the arts, perfume underwent profound change in the 19th century. Changing tastes and the development of modern chemistry laid the foundations of modern perfumery as alchemy gave way to chemistry.\n\nPerfume manufacture in Russia grew after 1861 and became globally significant by the early 20th century. The production of perfume in the Soviet Union became a part of the planned economy in the 1930s, although output was not high.\n\nIn early America, the first scents were colognes and scented water by French explorers in New France. Florida water, an uncomplicated mixture of eau de cologne with a dash of oil of cloves, cassia and lemongrass, was popular.\n\n"}
{"id": "3433830", "url": "https://en.wikipedia.org/wiki?curid=3433830", "title": "IEC 61131", "text": "IEC 61131\n\nIEC 61131 is an IEC standard for programmable controllers. It was known as IEC 1131 before the change in numbering system by IEC. The parts of the IEC 61131 standard are prepared and maintained working group 7, programmable control systems, of subcommittee SC 65B of Technical Committee TC65 of the IEC.\n\nStandard \"IEC 61131\" is divided into several parts:\n\nA further part is currently being worked on:\n\nIEC 61499 Function Block\n\nPLCopen has developed several standards and working groups.\n\n"}
{"id": "22690961", "url": "https://en.wikipedia.org/wiki?curid=22690961", "title": "IEEE Control Systems Society", "text": "IEEE Control Systems Society\n\nIEEE Control Systems Society (CSS) is an organizational unit of the IEEE. It was founded in 1954 and is dedicated to the advancement of the theory and practice of systems and control in engineering.\n\nThe history of the \"IEEE Control Systems Society\" goes back to one of the merged societies forming the IEEE: the Institute of Radio Engineers (IRE) organised the first meeting of the IRE Professional Group on Automatic Control on October 19, 1954.\n\n\n\n\n"}
{"id": "1666672", "url": "https://en.wikipedia.org/wiki?curid=1666672", "title": "Ice blasting", "text": "Ice blasting\n\nIce blasting is the use of explosives to break up ice in rivers, greatly aiding navigation systems. \nThis is done during the spring when snow is melting and river ice is breaking up. There is always a chance that the ice flows could collide creating an ice jam and blocking the river. The river, filled with melt water, will quickly flood and often cause damage to nearby settlements. Thus in most northern areas governments quickly act to break up the ice jams before they can do much damage. This is most easily done with explosives. These explosives may be planted from the shore, or in some cases by helicopter. Explosives can also be remotely delivered by artillery or dropped by bombers. In the large rivers of the Siberia the Russian airforce is sometimes called in to bomb ice jams.\n\nSome districts, where flooding is especially common, do preemptive ice blasting. The city of Ottawa, Ontario, Canada, for instance, blasts the Rideau River each spring to break up the ice. In 1994, for instance, 10,000 sticks of dynamite were used to break up ice along 9 kilometres of the river.\n\nIce blasting has a number of disadvantages. It is expensive and dangerous requiring highly skilled explosives experts. When blasting is occurring the public must be warned to keep their distance. The blasting has negative environmental consequences. Fish and other river creatures are inevitably killed and the river bottom is scarred. Unexploded ordnance can also be a concern where remote delivery is used.\n\n"}
{"id": "19572035", "url": "https://en.wikipedia.org/wiki?curid=19572035", "title": "Innovative Interstellar Explorer", "text": "Innovative Interstellar Explorer\n\nInnovative Interstellar Explorer was a NASA \"Vision Mission\" study funded by NASA following a proposal under NRA-03-OSS-01 on 11 September 2003. This study focused on measuring the interstellar medium, the region outside the influence of the nearest star, the Sun. It proposes to use a radioisotope thermal generator to power ion thrusters. \n\nThe project is a study of a proposed interstellar precursor mission that would probe the nearby interstellar medium and measure the properties of magnetic fields and cosmic rays and their effects on a spacecraft leaving the Solar System. Mission launch plans analyzed direct, one planet, multi-planet, and upper-stage trades. As a concept study, a number of technologies, configurations, and mission goals were considered, leading to the choice of a spacecraft propelled with ion engines powered by a radioisotope thermoelectric generator (RTG). The focus was getting a spacecraft launched by about 2014, achieving 200 AU by the year 2031.\n\nA variety of strategies were assessed, including using launch windows (not counting backups) for a Jupiter assist in 2014, 2026, 2038, and 2050about every 12 years. \nThe launch opportunity for the 2014 window passed, but for example it could have resulted in a Jupiter flyby by early 2016 and then go on to reach 200 AU by 2044. With an ion drive, a speed of about 7.9 AU/yr could be attained by the time its xenon propellant was depleted, enabling a travel distance of 200 AU by 2044 and perhaps 1000 AU after one hundred years from launch. Different launch times and configurations have various timelines and options. One configuration for launch saw the use of a Delta IV Heavy and for the upper stages a stack of Star 38 and Star 37 leading to various gravity assist options. Another launch stack that was considered was the Atlas 551 with a Star 48.\n\nIn 2011, the study's primary author gave an update to website Centauri Dreams, giving a retrospective on the mission and its feasibility since its publication in 2003. By that time, some of earliest launch windows were no longer feasible without a ready spacecraft. Some retrospectives were the advantages and potential of solar sails, but the need for them to be more advanced for a mission, and also the utility of a radioisotope propulsion (REP) for such a mission. REP was the combination of using an RTG to power an ion drive.\n\n\n"}
{"id": "7069468", "url": "https://en.wikipedia.org/wiki?curid=7069468", "title": "Instructograph", "text": "Instructograph\n\nThe Instructograph was a paper tape-based machine used for the study of Morse code.\n\nThe paper tape mechanism consisted of two reels which passed a paper tape across a reading device that actuated a set of contacts which changed state dependent on the presence or absence of hole punches in the tape. The contacts could operate an audio oscillator for the study of International Morse Code (used by radio), or a sounder for the study of American Morse Code (used by railroads), or a light bulb (Aldis Lamp - used by Navy ship to ship or by Heliograph).\n\nThe Instructograph was in production from about 1920 through 1983. The first US patent, No. 1,725,145, was granted to Otto Bernard Kirkpatrick, of Chicago, IL, on August 20, 1929. Most of them would be wound by hand or be plugged into a wall outlet. Most plugin outlet based instructographs would have a set of knobs that can control the speed and volume. The latest version of the Instructograph was the model 500 which included a built in solid state oscillator. This model was available to be purchased as new through at least 1986.\n\n"}
{"id": "9850794", "url": "https://en.wikipedia.org/wiki?curid=9850794", "title": "JC3IEDM", "text": "JC3IEDM\n\nJC3IEDM, or Joint Consultation, Command and Control Information Exchange Data Model is a model that, when implemented, aims to enable the interoperability of systems and projects required to share Command and Control (C2) information. JC3IEDM is an evolution of the C2IEDM standard that includes joint operational concepts, just as the Land Command and Control Information Exchange Data Model (LC2IEDM) was extended to become C2IEDM. The program is managed by the Multilateral Interoperability Programme (MIP).\n\nJC3IEDM is produced by the MIP-NATO Management Board (MNMB) and ratified under NATO STANAG 5525. JC3IEDM a fully documented standard for an information exchange data model for the sharing of C2 information.\n\nThe overall aim of JC3IEDM is to enable \"international interoperability of C2 information systems at all levels from corps to battalion (or lowest appropriate level) in order to support multinational (including NATO), combined and joint operations and the advancement of digitisation in the international arena.\"\n\nAccording to JC3IEDM's documentation, this aim is attempted to be achieved by \"specifying the minimum set of data that needs to be\nexchanged in coalition or multinational operations. Each nation, agency or community of\ninterest is free to expand its own data dictionary to accommodate its additional\ninformation exchange requirements with the understanding that the added specifications\nwill be valid only for the participating nation, agency or community of interest. Any\naddition that is deemed to be of general interest may be submitted as a change proposal\nwithin the configuration control process to be considered for inclusion in the next version\nof the specification.\"\n\n\"JC3IEDM is intended to represent the core of the data identified for exchange across multiple functional areas and multiple views of the requirements. Toward that end, it lays down a common approach to describing the information to be exchanged in a command and control (C2) environment.\n\nJC3IEDM has been developed from the initial Generic Hub (GH) Data Model, which changed its name to Land C2 Information Exchange Data Model (LC2IEDM) in 1999. Development of the model continued in a Joint context and in November 2003 the C2 Information Exchange Data Model (C2IEDM) Edition 6.1 was released. Additional development to this model, incorporating the NATO Corporate Reference model, resulted in the model changing its name again to JC3IEDM with JC3IEDM Ed 0.5 being issued in December 2004.\n\nSubsequent releases have seen areas of the model developed in greater depth than others and there is variation in the number of sub-types and attributes for each type in the current version. An example is HARBOUR within the FACILITY type which has 43 attributes compared to a VESSEL-TYPE with 12 attributes or a WEAPON-TYPE with 4 attributes. The associated attributes of a certain type also lack support for exploiting with those of other types. For example, VESSEL-TYPE does not support the length or width of a vessel in its attributes but HARBOUR has both maximum vessel length and width attributes.\n\nThe UK Ministry of Defence has mandated JC3IEDM as the C2 Information Exchange Model, in Joint Service Publication (JSP) 602:1007, for use on all systems and/or projects exchanging C2 information within and interoperating with the Land Environment at a Strategic and Operational Level. It is strongly recommended for other environments and mandated for all environments at the Tactical level. JSP 602:1005 for Collaborative Services has also mandated JC3IEDM in the tactical domain for all systems/projects providing data sharing collaborative services.\n\nNote: Link for Ref 3 is broken. Link for Ref 5 is wrong. As of 05.05.2017 all MIP links are broken and point to different directions.\n\n"}
{"id": "549152", "url": "https://en.wikipedia.org/wiki?curid=549152", "title": "Limit price", "text": "Limit price\n\nA limit price (or limit pricing) is a price, or pricing strategy, where products are sold by a supplier at a price low enough to make it unprofitable for other players to enter the market.\n\nIt is used by monopolists to discourage entry into a market, and is illegal in many countries. The quantity produced by the incumbent firm to act as a deterrent to entry is usually larger than would be optimal for a monopolist, but might still produce higher economic profits than would be earned under perfect competition.\n\nThe problem with limit pricing as strategic behavior is that once the entrant has entered the market, the quantity used as a threat to deter entry is no longer the incumbent firm's best response. This means that for limit pricing to be an effective deterrent to entry, the threat must in some way be made credible. A way to achieve this is for the incumbent firm to constrain itself to produce a certain quantity whether entry occurs or not. An example of this would be if the firm signed a union contract to employ a certain (high) level of labor for a long period of time. Another example is to build excess production capacity as a commitment device.\n\nIt is important to note that due to the often ambiguous nature of cost in production, it may be relatively easy for a firm to avoid legal difficulties when undertaking such action. Due to this ambiguous nature, limit pricing may well be a commonly used strategy even in modern economies. However, it is often very hard to regulate, since limit pricing is often synonymous with a market monopoly. When a monopoly exists, it becomes very difficult to compare alternative prices with other, similar firms to confirm claims that limit pricing may be occurring.\n\nIn a simple case, suppose industry demand for good X at market price P is given by:\n\nSuppose there are two potential producers of good X, Firm A, and Firm B. Firm A has no fixed costs and constant marginal cost equal to formula_2. Firm B also has no fixed costs, and has constant marginal cost equal to formula_3, where formula_4 (so that Firm B's marginal cost is greater than Firm A's).\n\nSuppose Firm A acts as a monopolist. The profit-maximizing monopoly price charged by Firm A is then:\n\nSince Firm B will never sell below its marginal cost, as long as formula_6, Firm B will not enter the market when Firm A charges formula_7. That is, the market for good X is an effective monopoly if:\n\nSuppose, on the contrary, that:\n\nIn this case, if Firm A charges formula_7, Firm B has an incentive to enter the market, since it can sell a positive quantity of good X at a price above its marginal cost, and therefore make positive profits. In order to prevent Firm B from having an incentive to enter the market, Firm A must set its price no greater than formula_3. To maximize its profits subject to this constraint, Firm A sets price formula_12 (the limit price).\n\n"}
{"id": "301928", "url": "https://en.wikipedia.org/wiki?curid=301928", "title": "Lumped element model", "text": "Lumped element model\n\nThe lumped element model (also called lumped parameter model, or lumped component model) simplifies the description of the behaviour of spatially distributed physical systems into a topology consisting of discrete entities that approximate the behaviour of the distributed system under certain assumptions. It is useful in electrical systems (including electronics), mechanical multibody systems, heat transfer, acoustics, etc.\n\nMathematically speaking, the simplification reduces the state space of the system to a finite dimension, and the partial differential equations (PDEs) of the continuous (infinite-dimensional) time and space model of the physical system into ordinary differential equations (ODEs) with a finite number of parameters.\n\nThe lumped matter discipline is a set of imposed assumptions in electrical engineering that provides the foundation for lumped circuit abstraction used in network analysis. The self-imposed constraints are:\n\n1. The change of the magnetic flux in time outside a conductor is zero.\n\n2. The change of the charge in time inside conducting elements is zero.\n\n3. Signal timescales of interest are much larger than propagation delay of electromagnetic waves across the lumped element.\n\nThe first two assumptions result in Kirchhoff's circuit laws when applied to Maxwell's equations and are only applicable when the circuit is in steady state. The third assumption is the basis of the lumped element model used in network analysis. Less severe assumptions result in the distributed element model, while still not requiring the direct application of the full Maxwell equations.\n\nThe lumped element model of electronic circuits makes the simplifying assumption that the attributes of the circuit, resistance, capacitance, inductance, and gain, are concentrated into idealized electrical components; resistors, capacitors, and inductors, etc. joined by a network of perfectly conducting wires.\n\nThe lumped element model is valid whenever formula_3, where formula_4 denotes the circuit's characteristic length, and formula_5 denotes the circuit's operating wavelength.\nOtherwise, when the circuit length is on the order of a wavelength, we must consider more general models, such as the distributed element model (including transmission lines), whose dynamic behaviour is described by Maxwell's equations. Another way of viewing the validity of the lumped element model is to note that this model ignores the finite time it takes signals to propagate around a circuit. Whenever this propagation time is not significant to the application the lumped element model can be used. This is the case when the propagation time is much less than the period of the signal involved. However, with increasing propagation time there will be an increasing error between the assumed and actual phase of the signal which in turn results in an error in the assumed amplitude of the signal. The exact point at which the lumped element model can no longer be used depends to a certain extent on how accurately the signal needs to be known in a given application.\n\nReal-world components exhibit non-ideal characteristics which are, in reality, distributed elements but are often represented to a first-order approximation by lumped elements. To account for leakage in capacitors for example, we can model the non-ideal capacitor as having a large lumped resistor connected in parallel even though the leakage is, in reality distributed throughout the dielectric. Similarly a wire-wound resistor has significant inductance as well as resistance distributed along its length but we can model this as a lumped inductor in series with the ideal resistor.\n\nA lumped capacitance model, also called lumped system analysis, reduces a thermal system to a number of discrete “lumps” and assumes that the temperature difference inside each lump is negligible. This approximation is useful to simplify otherwise complex differential heat equations. It was developed as a mathematical analog of electrical capacitance, although it also includes thermal analogs of electrical resistance as well.\n\nThe lumped capacitance model is a common approximation in transient conduction, which may be used whenever heat conduction within an object is much faster than heat transfer across the boundary of the object. The method of approximation then suitably reduces one aspect of the transient conduction system (spatial temperature variation within the object) to a more mathematically tractable form (that is, it is assumed that the temperature within the object is completely uniform in space, although this spatially uniform temperature value changes over time). The rising uniform temperature within the object or part of a system, can then be treated like a capacitative reservoir which absorbs heat until it reaches a steady thermal state in time (after which temperature does not change within it).\n\nAn early-discovered example of a lumped-capacitance system which exhibits mathematically simple behavior due to such physical simplifications, are systems which conform to \"Newton's law of cooling\". This law simply states that the temperature of a hot (or cold) object progresses toward the temperature of its environment in a simple exponential fashion. Objects follow this law strictly only if the rate of heat conduction within them is much larger than the heat flow into or out of them. In such cases it makes sense to talk of a single \"object temperature\" at any given time (since there is no spatial temperature variation within the object) and also the uniform temperatures within the object allow its total thermal energy excess or deficit to vary proportionally to its surface temperature, thus setting up the Newton's law of cooling requirement that the rate of temperature decrease is proportional to difference between the object and the environment. This in turn leads to simple exponential heating or cooling behavior (details below).\n\nTo determine the number of lumps, the Biot number (Bi), a dimensionless parameter of the system, is used. Bi is defined as the ratio of the conductive heat resistance within the object to the convective heat transfer resistance across the object's boundary with a uniform bath of different temperature. When the thermal resistance to heat transferred into the object is larger than the resistance to heat being diffused completely within the object, the Biot number is less than 1. In this case, particularly for Biot numbers which are even smaller, the approximation of \"spatially uniform temperature within the object\" can begin to be used, since it can be presumed that heat transferred into the object has time to uniformly distribute itself, due to the lower resistance to doing so, as compared with the resistance to heat entering the object.\n\nIf the Biot number is less than 0.1 for a solid object, then the entire material will be nearly the same temperature with the dominant temperature difference will be at the surface. It may be regarded as being \"thermally thin\". The Biot number must generally be less than 0.1 for usefully accurate approximation and heat transfer analysis. The mathematical solution to the lumped system approximation gives Newton's law of cooling.\n\nA Biot number greater than 0.1 (a \"thermally thick\" substance) indicates that one cannot make this assumption, and more complicated heat transfer equations for \"transient heat conduction\" will be required to describe the time-varying and non-spatially-uniform temperature field within the material body.\n\nThe single capacitance approach can be expanded to involve many resistive and capacitive elements, with Bi < 0.1 for each lump. As the Biot number is calculated based upon a characteristic length of the system, the system can often be broken into a sufficient number of sections, or lumps, so that the Biot number is acceptably small.\n\nSome characteristic lengths of thermal systems are:\n\nFor arbitrary shapes, it may be useful to consider the characteristic length to be volume / surface area.\n\nA useful concept used in heat transfer applications once the condition of steady state heat conduction has been reached, is the representation of thermal transfer by what is known as thermal circuits. A thermal circuit is the representation of the resistance to heat flow in each element of a circuit, as though it were an electrical resistor. The heat transferred is analogous to the electric current and the thermal resistance is analogous to the electrical resistor. The values of the thermal resistance for the different modes of heat transfer are then calculated as the denominators of the developed equations. The thermal resistances of the different modes of heat transfer are used in analyzing combined modes of heat transfer. The lack of \"capacitative\" elements in the following purely resistive example, means that no section of the circuit is absorbing energy or changing in distribution of temperature. This is equivalent to demanding that a state of steady state heat conduction (or transfer, as in radiation) has already been established.\n\nThe equations describing the three heat transfer modes and their thermal resistances in steady state conditions, as discussed previously, are summarized in the table below:\n\nIn cases where there is heat transfer through different media (for example, through a composite material), the equivalent resistance is the sum of the resistances of the components that make up the composite. Likely, in cases where there are different heat transfer modes, the total resistance is the sum of the resistances of the different modes. Using the thermal circuit concept, the amount of heat transferred through any medium is the quotient of the temperature change and the total thermal resistance of the medium.\n\nAs an example, consider a composite wall of cross-sectional area formula_6. The composite is made of an formula_7 long cement plaster with a thermal coefficient formula_8 and formula_9 long paper faced fiber glass, with thermal coefficient formula_10. The left surface of the wall is at formula_11 and exposed to air with a convective coefficient of formula_12. The right surface of the wall is at formula_13 and exposed to air with convective coefficient formula_14.\n\nUsing the thermal resistance concept, heat flow through the composite is as follows:\n\nformula_15\n\nwhere\n\nformula_16, formula_17, formula_18, and formula_19\n\nNewton's law of cooling is an empirical relationship attributed to English physicist Sir Isaac Newton (1642 - 1727). This law stated in non-mathematical form is the following:\n\nOr, using symbols:\n\nAn object at a different temperature from its surroundings will ultimately come to a common temperature with its surroundings. A relatively hot object cools as it warms its surroundings; a cool object is warmed by its surroundings. When considering how quickly (or slowly) something cools, we speak of its \"rate\" of cooling - how many degrees' change in temperature per unit of time.\n\nThe rate of cooling of an object depends on how much hotter the object is than its surroundings. The temperature change per minute of a hot apple pie will be more if the pie is put in a cold freezer than if it is placed on the kitchen table. When the pie cools in the freezer, the temperature difference between it and its surroundings is greater. On a cold day, a warm home will leak heat to the outside at a greater rate when there is a large difference between the inside and outside temperatures. Keeping the inside of a home at high temperature on a cold day is thus more costly than keeping it at a lower temperature. If the temperature difference is kept small, the rate of cooling will be correspondingly low.\n\nAs Newton's law of cooling states, the rate of cooling of an object - whether by conduction, convection, or radiation - is approximately proportional to the temperature difference Δ\"T\". Frozen food will warm up faster in a warm room than in a cold room. Note that the rate of cooling experienced on a cold day can be increased by the added convection effect of the wind. This is referred to as wind chill. For example, a wind chill of -20 °C means that heat is being lost at the same rate as if the temperature were -20 °C without wind.\n\nThis law describes many situations in which an object has a large thermal capacity and large conductivity, and is suddenly immersed in a uniform bath which conducts heat relatively poorly. It is an example of a thermal circuit with one resistive and one capacitative element. For the law to be correct, the temperatures at all points inside the body must be approximately the same at each time point, including the temperature at its surface. Thus, the temperature difference between the body and surroundings does not depend on which part of the body is chosen, since all parts of the body have effectively the same temperature. In these situations, the material of the body does not act to \"insulate\" other parts of the body from heat flow, and all of the significant insulation (or \"thermal resistance\") controlling the rate of heat flow in the situation resides in the area of contact between the body and its surroundings. Across this boundary, the temperature-value jumps in a discontinuous fashion.\n\nIn such situations, heat can be transferred from the exterior to the interior of a body, across the insulating boundary, by convection, conduction, or diffusion, so long as the boundary serves as a relatively poor conductor with regard to the object's interior. The presence of a physical insulator is not required, so long as the process which serves to pass heat across the boundary is \"slow\" in comparison to the conductive transfer of heat inside the body (or inside the region of interest—the \"lump\" described above).\n\nIn such a situation, the object acts as the \"capacitative\" circuit element, and the resistance of the thermal contact at the boundary acts as the (single) thermal resistor. In electrical circuits, such a combination would charge or discharge toward the input voltage, according to a simple exponential law in time. In the thermal circuit, this configuration results in the same behavior in temperature: an exponential approach of the object temperature to the bath temperature.\n\nNewton's law is mathematically stated by the simple first-order differential equation:\n\nwhere\n\nPutting heat transfers into this form is sometimes not a very good approximation, depending on ratios of heat conductances in the system. If the differences are not large, an accurate formulation of heat transfers in the system may require analysis of heat flow based on the (transient) heat transfer equation in nonhomogeneous or poorly conductive media.\n\nIf the entire body is treated as lumped capacitance heat reservoir, with total heat content which is proportional to simple total heat capacity formula_22, and formula_23, the temperature of the body, or formula_24. It is expected that the system will experience exponential decay with time in the temperature of a body.\n\nFrom the definition of heat capacity formula_22 comes the relation formula_26. Differentiating this equation with regard to time gives the identity (valid so long as temperatures in the object are uniform at any given time): formula_27. This expression may be used to replace formula_28 in the first equation which begins this section, above. Then, if formula_29 is the temperature of such a body at time formula_30, and formula_31 is the temperature of the environment around the body:\n\nwhere\n\nformula_33 is a positive constant characteristic of the system, which must be in units of formula_34, and is therefore sometimes expressed in terms of a characteristic time constant formula_35 given by: formula_36. Thus, in thermal systems, formula_37. (The total heat capacity formula_22 of a system may be further represented by its mass-specific heat capacity formula_39 multiplied by its mass formula_40, so that the time constant formula_35 is also given by formula_42).\n\nThe solution of this differential equation, by standard methods of integration and substitution of boundary conditions, gives:\n\nIf:\n\nthen the Newtonian solution is written as:\n\nThis same solution is almost immediately apparent if the initial differential equation is written in terms of formula_48, as the single function to be solved for.\n\nThis mode of analysis has been applied to forensic sciences to analyze the time of death of humans. Also, it can be applied to HVAC (heating, ventilating and air-conditioning, which can be referred to as \"building climate control\"), to ensure more nearly instantaneous effects of a change in comfort level setting.\n\nThe simplifying assumptions in this domain are:\n\nIn this context, the lumped component model extends the distributed concepts of Acoustic theory subject to approximation. In the acoustical lumped component model, certain physical components with acoustical properties may be approximated as behaving similarly to standard electronic components or simple combinations of components.\n\n\nThe simplifying assumption in this domain are:\n\nSeveral publications can be found that describe how to generate LEMs of buildings. In most cases, the building is considered a single thermal zone and in this case, turning multi-layered walls into Lumped Elements can be one of the most complicated tasks in the creation of the model. Ramallo-González's method (Dominant Layer Method) is the most accurate and simple so far. In this method, one of the layers is selected as the dominant layer in the whole construction, this layer is chosen considering the most relevant frequencies of the problem. In his thesis, Ramallo-González shows the whole process of obtaining the LEM of a complete building.\n\nLEMs of buildings have also been used to evaluate the efficiency of domestic energy systems In this case the LEMs allowed to run many simulations under different future weather scenarios.\n\n\n"}
{"id": "7960048", "url": "https://en.wikipedia.org/wiki?curid=7960048", "title": "MaRS Discovery District", "text": "MaRS Discovery District\n\nMaRS Discovery District is a not-for-profit corporation founded in Toronto, Ontario, Canada in 2000. Its stated goal is to commercialize publicly funded medical research and other technologies with the help of local private enterprises and as such is a public-private partnership. As part of its mission MaRS says, \"MaRS helps create successful global businesses from Canada's science, technology and social innovation.\" , startup companies emerging from MaRS had created more than 4,000 jobs, and in the period of 2011 to 2014 had raised over $750 million in capital investments.\n\nThe name MaRS was originally drawn from a file name, and later attributed with the title \"Medical and Related Sciences.\" It has since abandoned this association as it also works in other fields such as information and communications technology, engineering, and social innovation.\n\nIt is located on the corner of College Street and University Avenue in the city of Toronto’s Discovery District, adjacent to the University of Toronto and its affiliated research hospitals at the University Health Network.\n\nThe MaRS development consists of two phases.\n\nMaRS Discovery District Phase 1 was designed by Adamson Associates Architects and includes:\n\n\nInside the Heritage Building's four-storey brick façade (preserved) are tenant spaces occupied by professional services, industry associations, pharmaceutical companies and offices of Canadian universities and the Province of Ontario. In 2006, the MaRS Centre received the Heritage Toronto Award of Excellence for Architectural Conservation and Craftsmanship. The building was designed by Pearson and Darling and opened in 1911.\n\nThe MaRS atrium is a glass-roofed public thoroughfare that provides walkway access to Heritage Building tenants and retail vendors, as well as access to the South and Medical Discovery Towers. Its bottom level features a sub-dividable conference area that hosts public and private events. MaRS encourages events from across Toronto's arts, culture and broader urban community. The Atrium's lower level also features a media centre, video conferencing rooms and a public food court.\n\nThis eight-storey structure houses incubator programs and shared laboratory and research facilities. The , wet-lab-capable building spans eight floors in the MaRS Centre. The tower boasts advanced mechanical and electrical systems, floors with enhanced load bearing capabilities and slab-to-slab clearances.\n\nOccupying the second and third floors of the South Tower — directly above the MaRS corporate offices, is the MaRS Incubator – a dedicated space that houses offices and laboratories for approximately two dozen life science and technology firms.\n\nWith of state-of-the-art wet labs, the 15-storey Toronto Medical Discovery Tower accommodates leading-edge scientific equipment and houses the basic research activities of two of Canada's premier research hospitals: the University Health Network and the Hospital for Sick Children.\n\nSituated on the corner of College and Elizabeth Street, the building was designed with typical research and development lab floors configured with a side core arrangement and sheathed in metal and glass. The tower portions rest on a three-storey limestone podium that aligns with the heights of the adjoining College Wing and the formal landscape forecourt that extends the full block.\n\nThe shell and core of the TMDT is designed to accommodate a full lab program based on 80 percent wet lab and 20 percent dry lab. The lab floors have been configured to maximize future flexibility. The mechanical and electrical rooms, power and communication distribution systems, general and special exhaust risers, floor drains and service zones, have been established to allow for fit-out by future tenants.\n\nPhase 1 began operations in 2005.\n\nWest Tower (formerly known as Phase 2), designed by Bregman + Hamann Architects, constitutes a addition to the MaRS centre in the form of a 20-story tower on the complex’s west wing. The tower was developed by Alexandria Real Estate Equities. Construction began in late 2007, and was scheduled to be completed in spring 2010. In November 2008, Phase 2 construction was put on hold due to the economic downturn. Construction resumed in July 2011, with a target completion date of Fall 2013.\n\nIn 2011, construction of Phase 2 restarted. Phase 2 construction was expected to be completed in September 2013. Phase 2 of the project was facing criticism because the Government of Ontario provided a bailout for the organization which is facing difficulty leasing the floors. The Ontario Progressive Conservative Party was calling for a full accounting of MaRS Phase 2. By November 2016, 93% of the building had been leased, with the rest in negotiation.\n\nIn 2016, IBM announced plans to join the finTech hub at MaRS. The hub has financial services companies, including CIBC, Manulife and Moneris.\n\nMaRS is supported, in part, by professional service organizations which offer their expertise at no cost through education and training, and advisory hours.\n\nThe current list of organizations include:\nIn April 2010, criticism of the $471,874 salary collected by MaRS CEO Ilse Treurnicht in 2008 was raised. It also criticized Liberal government-led funding, lack of accountability and rigor in measuring results, claims of public-private partnerships and the absence of visible minorities among MaRS's team of advisors.\n\nOn August 27, 2010, the National Post relayed some of these criticisms\n\nRenewed criticism was published in 2011, pointing in particular to the $100,000 increase in Treurnicht's salary, her $534,000 salary in 2010, and questioning the public and private funding of the Phase II expansion. The Toronto Sun published articles on the topic as well, questioning the high compensation levels at the charitable trust institution.\n\n\n"}
{"id": "4876552", "url": "https://en.wikipedia.org/wiki?curid=4876552", "title": "Maadiran Group", "text": "Maadiran Group\n\nThe Maadiran group is an Iranian conglomerate group. \nThe group is composed of a public company as well as several fully private entities. The group is a multi-generational family company with no links to the government of the Islamic Republic of Iran.\n\nThe core business of the Maadiran Group revolves around technology manufacturing, distribution, solutions, and support services. It has a distribution channel of over 1800 direct sales dealers and over 6000 sales sub-dealers. It expanded with significant investments into venture capital, consumer finance, banking, green tech, plastics production, and real estate development.\n\nThe group was established in 1964 and was formerly known as Iran Office Machines Company Ltd. It is an exclusive distributor for Sharp Corporation, LG Group, AOC, Dell, Asus, Acer, CTS Electronics, TCL, Optoma, Wacom, Olivetti, and Epson as well as a number of other brands. Maadiran also has several own brand product lines including a range of specialised technical solutions and consumables operating under the brand name Meva, and a range of AV and home appliance products being sold under X-Vision brand, lifestyle products under the xelle brand, and biodegradable products under kisabz.\n\nIn 2004 the group began to export its locally manufactured products to the Middle East. This trend increased dramatically in early 2005 when LG Electronics began to buy Iranian-produced LG-Maadiran monitors for its own exports to the Persian Gulf markets. Maadiran also markets its Meva and X-Vision brands to regional markets.\n\nMaadiran's after-sales support subsidiary, has 21 regional branches through which it provides support all over Iran. These 21 branches in turn support a larger network of over 500 authorized service dealers across the country.\n\nMaadiran is the number one manufacturer of DVB-T Set-top and mobile devices within Iran with a total output of 710,000 units in 2013. Its X-Vision brand is also the third most popular television brand within the country.\n\nThe original Maadiran logo consisted of two bars with the company name written in both Persian and English inside the bars.\n\nThe Maadiran Group logo was created by Iranian calligrapher and logo-designer Mr. Chehreh-Pardaz. This logo was first used by the group in 2000 and was the first logo in which the company's new name, 'Maadiran', appeared.\n\nMaadiran distributes a broad range of products including:\n\n\n\n\nSince 2003, the Group's slogan has been \"Nemaad Etminaan\" which translates to \"Symbol of Reliability\". Previous to this the group carried no slogan.\n\nMaadiran offers nationwide after-sales support on all of its consumer electronics products.\n\n\n"}
{"id": "49835953", "url": "https://en.wikipedia.org/wiki?curid=49835953", "title": "MangoO Microfinance Management", "text": "MangoO Microfinance Management\n\nMangoO Microfinance Management is a free and open-source software solution for the management of small-scale microfinance institutions (MFIs).\n\nDevelopment of mangoO Microfinance Management started in September 2014 when the SACCO of Luweero Diocese in Uganda sought to improve their administration. The major target was to computerise their hitherto manual and paper-based book-keeping. As other available software solutions seemed to target big-scale MFIs, they decided to design their own tailor-made solution.\n\nThe main target in the development of mangoO was to design a microfinance software with an extremely simple user interface. Most workers at Luweero Diocese SACCO were inexperienced in the use of computers in general. The software, therefore, needed to focus on ease of use and maximum automation. As a result, mangoO is best equipped to serve small-scale (primarily single-branch) MFIs.\n\nFor its major part, mangoO was written in PHP with some additional JavaScript and CSS. The database uses MySQL. As the required webserver can run locally as well as remotely, the system offers a maximum of flexibility for different scenarios. On the client side, only an ordinary web browser is needed.\nMangoO was published on GitHub in 2015 under the GNU General Public Licence (GPLv3).\n\n"}
{"id": "17286136", "url": "https://en.wikipedia.org/wiki?curid=17286136", "title": "Margaret Dreier Robins", "text": "Margaret Dreier Robins\n\nMargaret Dreier Robins (6 September 1868 – 21 February 1945) was an American labor leader.\n\nShe was born in Brooklyn, New York on 6 September 1868. Her parents, Theodor Dreier, a successful businessman, and Dorthea Dreier, were both immigrants from Germany. Her mother's maiden name was Dreier and her parents were cousins from Bremen, Germany. Their ancestors where civic leaders and merchants. Theodor came to the United States in 1849 and became partner of the English iron firm of Naylor, Benson and Company's New York branch. He married Dorothea in 1864 during a visit to Bremen and brought her back with him to the United States and they lived in a brownstone house in Brooklyn Heights, New York.\nMargaret Dreier had a brother and three sisters. Her sister Mary was a social reformer. Her sisters Dorothea and Katherine were painters.\n\nShe was privately educated because her parents believed that the study of the arts was too often neglected in traditional education. In her teens, Robins suffered from physical ailments which left her depressed and weak.\n\nAt age nineteen, she began doing charity work at Brooklyn Hospital and soon became involved in other progressive causes. She met the reformer Josephine Shaw Lowell in 1902, and through Lowell joined in the Woman’s Municipal League, an organization that helped women avoid prostitution. Another collaborator was Frances Kellor, with whom she founded the New York Association for Household Research which provided lodging and placement for women domestic workers.\nIn 1904, increasingly interested in workers’ rights, Dreier joined the Women's Trade Union League, then only a small, budding organization. She became the president of its New York chapter in 1905; president of the Chicago chapter 1907-1914; and treasurer of the national organization and rose quickly in its ranks. In 1907, she was elected president of the national organization and began a fifteen-year tenure as its leader. Meanwhile, she married the lawyer and social worker Raymond Robins in 1905. The newleyweds split their time between running a settlement house in Chicago, Illinois and Chinsegut Hill in Brooksville, Florida.\nAs president of the League, Robins helped organize women into unions, educate women workers, and advocate for progressive legislation. She created a Training School for Women to educate women workers about organizing and leadership skills. She supported and became active in a number of well publicized strikes, most notably the International Ladies Garment Workers’ strike in 1910. She pushed for protective legislation limiting the hours of women’s work, and she presided over the League during its most influential period.\n\nShe served on the executive board of the Chicago Federation of Labor after 1908, and in 1915 was appointed to the unemployment commission by the governor of Illinois.\n\nIn 1919, Robins played an important role in the creation of the first International Congress of Working Women. Robins agreed to send both Rose Schneiderman and Mary Anderson to the Paris Peace Conference, where with other female labor leaders they organized an international labor women’s conference to prepare for the upcoming International Labour Organization convention in October in Washington, D.C.\n\nIn 1924, Robins retired from her activist work and moved full-time with her husband to Florida. She died in 1945.\n\n\n"}
{"id": "5904075", "url": "https://en.wikipedia.org/wiki?curid=5904075", "title": "Mechanical heat treatment", "text": "Mechanical heat treatment\n\nMechanical heat treatment (MHT) is an alternative waste treatment technology. This technology is also commonly termed autoclaving. MHT involves a mechanical sorting or pre-processing stage with technology often found in a material recovery facility. The mechanical sorting stage is followed by a form of thermal treatment. This might be in the form of a waste autoclave or processing stage to produce a refuse derived fuel pellet. MHT is sometimes grouped along with mechanical biological treatment. MHT does not however include a stage of biological degradation (anaerobic digestion or composting).\n\nDifferent MHT systems may be configured to meet various objectives with regard to the waste outputs from the process. The alternatives (depending on the system employed) may be one or more of the following:\n\n"}
{"id": "13402281", "url": "https://en.wikipedia.org/wiki?curid=13402281", "title": "Medicon Valley", "text": "Medicon Valley\n\nMedicon Valley is a leading international life-sciences cluster in Europe, spanning the Greater Copenhagen region of eastern Denmark and southern Sweden. It is one of Europe's strongest life science clusters, with a large number of life science companies and research institutions located within a relatively small geographical area. The name has officially been in use since 1997.\n\nMajor life science sectors of the Medicon Valley cluster includes pharmacology, biotechnology, health tech and medical technology. It is specifically known for its research strengths in the areas of neurological disorders, inflammatory diseases, cancer and diabetes.\nThe population of Medicon Valley reaches close to 4 million inhabitants. In 2008, 60% of Scandinavian pharmaceutical companies were located in the region. The area includes 17 universities, 32 hospitals, and more than 400 life science companies. 20 are large pharmaceutical or medical technology firms and 170 are dedicated biotechnology firms. Between 1998 and 2008, 100 new biotechnology and medical technology companies were created here. The biotechnology industry alone employs around 41,000 people in the region, 7,000 of whom are academic researchers. \n\nInternational companies with major research centres in the region include Novo Nordisk, Baxter, Lundbeck, LEO Pharma, HemoCue and Ferring Pharmaceuticals. There are more than 7 science parks in the region, all with a significant focus on life science, including the Medicon Village in Lund, established in 2010. Companies within Medicon Valley account for more than 20% of the total GDP of Denmark and Sweden combined.\n\nMedicon Valley is promoted by Invest in Skåne and Copenhagen Capacity. \n\nMany of the region's universities have a strong heritage in biological and medical research and have produced several Nobel Prize winners. The almost century-long presence of a number of research-intensive and fully integrated pharmaceutical companies, such as Novo Nordisk, H. Lundbeck and LEO Pharma, has also contributed significantly to the medical research and business development of the region by strengthening abilities within applied research, attracting suppliers and producing spin-offs. \n\nScience parks Medicon Valley includes:\n\n\n\n"}
{"id": "2381616", "url": "https://en.wikipedia.org/wiki?curid=2381616", "title": "Mobile phone operator", "text": "Mobile phone operator\n\nA mobile phone operator, wireless provider, or carrier is a mobile telecommunications company that provides wireless Internet GSM services for mobile device users. The operator gives a SIM card to the customer who inserts it into the mobile device to gain access to the service.\n\nThere are two types of mobile operators:\n\n\nPrior to 1973, cellular mobile device technology was limited to devices installed in cars and other vehicles. The first fully automated telephone calling system for vehicles was launched in Sweden in 1960, called MTA (mobile telephone system A). Calls from the car were direct dial, whereas incoming calls required an operator to determine which base station the device was currently at. In 1962, an upgraded version called \"Mobile System B (MTB)\" was introduced. In 1971, the MTD version was launched, opening for several different brands of equipment and gaining commercial success. The network remained open until 1983 and still had 600 customers when it closed.\n\nIn 1958, development began on a similar service in the USSR, the Altay system for motorists. In 1963, the service started in Moscow, and by 1970, was deployed in 30 cities across the USSR. Versions of the Altay system are still in use today as a trunking system in some parts of Russia.\n\nIn 1959 a private telephone company located in Brewster, Kansas, USA, the S&T Telephone Company, (still in business today) with the use of Motorola radio telephone equipment and a private tower facility, offered to the public cellular telephone services in that local area of NW Kansas.\n\nIn 1966, Bulgaria presented the pocket mobile automatic telephone RAT-0,5 combined with a base station RATZ-10 (RATC-10) on Interorgtechnika-66 international exhibition. One base station, connected to one telephone wire line, could serve up to six customers.\n\nOne of the first successful public commercial mobile phone networks was the ARP network in Finland, launched in 1971.\n\nOn April 3, 1973, Martin Cooper, a Motorola researcher and executive, made the first analog mobile phone call using a heavy prototype model. He called Dr. Joel S. Engel of Bell Labs.\n\nThe first commercially automated cellular network (the 1G generation) was launched in Japan by NTT in 1979. The initial launch network covered the full metropolitan area of Tokyo's over 20 million inhabitants with a cellular network of 23 base stations. Within five years, the NTT network had been expanded to cover the whole population of Japan and became the first nationwide 1G network. Several other countries also launched 1G networks in the early 1980s including the UK, Mexico and Canada.\n\nIn the 1990s, the 'second generation' (2G) mobile phone systems emerged, primarily using the GSM standard. In 1991, the first GSM network (Radiolinja) launched in Finland.\n\nAmerican and Canadian wireless providers tend to subsidize phones for consumers but tend to require 2 or 3 year contracts, while Asians and Europeans sell the phone at full cost while the monthly fees charged are lower.\n\n"}
{"id": "164174", "url": "https://en.wikipedia.org/wiki?curid=164174", "title": "Optical communication", "text": "Optical communication\n\nOptical communication, also known as optical telecommunication, is communication at a distance using light to carry information. It can be performed visually or by using electronic devices. The earliest basic forms of optical communication date back several millennia, while the earliest electrical device created to do so was the photophone, invented in 1880.\n\nAn optical communication system uses a transmitter, which encodes a message into an optical signal, a channel, which carries the signal to its destination, and a receiver, which reproduces the message from the received optical signal. When electronic equipment is not employed the 'receiver' is a person visually observing and interpreting a signal, which may be either simple (such as the presence of a beacon fire) or complex (such as lights using color codes or flashed in a Morse code sequence).\n\nFree-space optical communication has been deployed in space, while terrestrial forms are naturally limited by geography, weather and the availability of light. This article provides a basic introduction to different forms of optical communication.\n\nVisual techniques such as smoke signals, beacon fires, hydraulic telegraphs, ship flags and semaphore lines were the earliest forms of optical communication. Hydraulic telegraph semaphores date back to the 4th century BCE Greece. Distress flares are still used by mariners in emergencies, while lighthouses and navigation lights are used to communicate navigation hazards.\n\nThe heliograph uses a mirror to reflect sunlight to a distant observer. When a signaler tilts the mirror to reflect sunlight, the distant observer sees flashes of light that can be used to transmit a prearranged signaling code. Naval ships often use signal lamps and Morse code in a similar way.\n\nAircraft pilots often use visual approach slope indicator (VASI) projected light systems to land safely, especially at night. Military aircraft landing on an aircraft carrier use a similar system to land correctly on a carrier deck. The coloured light system communicates the aircraft's height relative to a standard landing glideslope. As well, airport control towers still use Aldis lamps to transmit instructions to aircraft whose radios have failed.\n\nIn the present day a variety of electronic systems optically transmit and receive information carried by pulses of light. Fiber-optic communication cables are now employed to send the great majority of the electronic data and long distance telephone calls that are not conveyed by either radio, terrestrial microwave or satellite. Free-space optical communications are also used every day in various applications.\n\nA 'semaphore telegraph', also called a 'semaphore line', 'optical telegraph', 'shutter telegraph chain', 'Chappe telegraph', or 'Napoleonic semaphore', is a system used for conveying information by means of visual signals, using towers with pivoting arms or shutters, also known as blades or paddles. Information is encoded by the position of the mechanical elements; it is read when the shutter is in a fixed position.\n\nSemaphore lines were a precursor of the electrical telegraph. They were far faster than post riders for conveying a message over long distances, but far more expensive and less private than the electrical telegraph lines which would later replace them. The maximum distance that a pair of semaphore telegraph stations can bridge is limited by geography, weather and the availability of light; thus, in practical use, most optical telegraphs used lines of relay stations to bridge longer distances. Each relay station would also require its complement of skilled operator-observers to convey messages back and forth across the line.\n\nThe modern design of semaphores was first foreseen by the British polymath Robert Hooke, who first gave a vivid and comprehensive outline of visual telegraphy in a 1684 submission to the Royal Society. His proposal (which was motivated by military concerns following the Battle of Vienna the preceding year) was not put into practice during his lifetime.\n\nThe first operational optical semaphore line arrived in 1792, created by the French engineer Claude Chappe and his brothers, who succeeded in covering France with a network of 556 stations stretching a total distance of . It was used for military and national communications until the 1850s.\n\nMany national services adopted signaling systems different from the Chappe system. For example, Britain and Sweden adopted systems of shuttered panels (in contradiction to the Chappe brothers' contention that angled rods are more visible). In Spain, the engineer Agustín de Betancourt developed his own system which was adopted by that state. This system was considered by many experts in Europe better than Chappe's, even in France.\n\nThese systems were popular in the late 18th to early 19th century but could not compete with the electrical telegraph, and went completely out of service by 1880.\n\nSemaphore Flags is the system for conveying information at a distance by means of visual signals with hand-held flags, rods, disks, paddles, or occasionally bare or gloved hands. Information is encoded by the position of the flags, objects or arms; it is read when they are in a fixed position.\n\nSemaphores were adopted and widely used (with hand-held flags replacing the mechanical arms of shutter semaphores) in the maritime world in the 19th century. They are still used during underway replenishment at sea and are acceptable for emergency communication in daylight or, using lighted wands instead of flags, at night.\n\nThe newer flag semaphore system uses two short poles with square flags, which a signaler holds in different positions to convey letters of the alphabet and numbers. The transmitter holds one pole in each hand, and extends each arm in one of eight possible directions. Except for in the rest position, the flags cannot overlap. The flags are colored differently based on whether the signals are sent by sea or by land. At sea, the flags are colored red and yellow (the Oscar flags), while on land, they are white and blue (the Papa flags). Flags are not required, they just make the characters more obvious.\n\nOptical fiber is the most common type of channel for optical communications. The transmitters in optical fiber links are generally light-emitting diodes (LEDs) or laser diodes. Infrared light, rather than visible light is used more commonly, because optical fibers transmit infrared wavelengths with less attenuation and dispersion. The signal encoding is typically simple intensity modulation, although historically optical phase and frequency modulation have been demonstrated in the lab. The need for periodic signal regeneration was largely superseded by the introduction of the erbium-doped fiber amplifier, which extended link distances at significantly lower cost.\n\nSignal lamps (such as Aldis lamps), are visual signaling devices for optical communication (typically using Morse code). Modern signal lamps are a focused lamp which can produce a pulse of light. In large versions this pulse is achieved by opening and closing shutters mounted in front of the lamp, either via a manually operated pressure switch or, in later versions, automatically.\n\nWith hand held lamps, a concave mirror is tilted by a trigger to focus the light into pulses. The lamps are usually equipped with some form of optical sight, and are most commonly deployed on naval vessels and also used in airport control towers with coded aviation light signals.\n\nAviation light signals are used in the case of a radio failure, an aircraft not equipped with a radio, or in the case of a hearing-impaired pilot. Air traffic controlers have long used signal light guns to direct such aircraft. The light gun's lamp has a focused bright beam capable of emitting three different colors: red, white and green. These colors may be flashing or steady, and provide different instructions to aircraft in flight or on the ground (for example, \"cleared to land\" or \"cleared for takeoff\"). Pilots can acknowledge the instructions by wiggling their plane's wings, moving their ailerons if they are on the ground, or by flashing their landing or navigation lights during night time. Only 12 simple standardized instructions are directed at aircraft using signal light guns as the system is not utilized with Morse code.\n\nThe photophone (originally given an alternate name, radiophone) is a communication device which allowed for the transmission of speech on a beam of light. It was invented jointly by Alexander Graham Bell and his assistant Charles Sumner Tainter on February 19, 1880, at Bell's 1325 'L' Street laboratory in Washington, D.C. Both were later to become full associates in the Volta Laboratory Association, created and financed by Bell.\n\nOn June 21, 1880, Bell's assistant transmitted a wireless voice telephone message of considerable distance, from the roof of the Franklin School to the window of Bell's laboratory, some 213 meters (about 700 ft.) away.\n\nBell believed the photophone was his most important invention. Of the 18 patents granted in Bell's name alone, and the 12 he shared with his collaborators, four were for the photophone, which Bell referred to as his\" 'greatest achievement\"', telling a reporter shortly before his death that the photophone was \"the greatest invention [I have] ever made, greater than the telephone\".\n\nThe photophone was a precursor to the fiber-optic communication systems which achieved popular worldwide usage starting in the 1980s. The master patent for the photophone ( \"Apparatus for Signalling and Communicating, called Photophone\"), was issued in December 1880, many decades before its principles came to have practical applications.\n\nFree-space optics (FSO) systems are employed for 'last mile' telecommunications and can function over distances of several kilometers as long as there is a clear line of sight between the source and the destination, and the optical receiver can reliably decode the transmitted information. Other free-space systems can provide high-data-rate, long-range links using small, low-mass, low-power-consumption subsystems which make them suitable for communications in space. Various planned satellite constellations intended to provide global broadband coverage take advantage of these benefits and employ laser communication for inter-satellite links between the several hundred to thousand satellites effectively creating a space-based optical mesh network.\n\nMore generally, transmission of unguided optical signals is known as optical wireless communications (OWC). Examples include medium-range visible light communication and short-distance IrDA, using infrared LEDs.\n\nA heliograph ( \"helios\", meaning \"sun\", and \"graphein\", meaning \"write\") is a wireless solar telegraph that signals by flashes of sunlight (generally using Morse code) reflected by a mirror. The flashes are produced by momentarily pivoting the mirror, or by interrupting the beam with a shutter.\n\nThe heliograph was a simple but effective instrument for instantaneous optical communication over long distances during the late 19th and early 20th century. Its main uses were in military, surveys and forest protection work. They were standard issue in the British and Australian armies until the 1960s, and were used by the Pakistani army as late as 1975.\n\n\n"}
{"id": "9750806", "url": "https://en.wikipedia.org/wiki?curid=9750806", "title": "Palo (OLAP database)", "text": "Palo (OLAP database)\n\nPalo is a memory resident multidimensional (online analytical processing (OLAP) or multidimensional online analytical processing (MOLAP)) database server and typically used as a business intelligence tool for controlling and budgeting purposes with spreadsheet software acting as the user interface. Beyond the multidimensional data concept, Palo enables multiple users to share one centralised data storage (single version of the truth).\n\nThis type of database is suitable to handle complex data models for business management and statistics. Apart from multidimensional queries, data can also be written back and consolidated in real-time. To give rapid access to all data, Palo stores them in the memory during run time. The server is available as open-source and proprietary software.\n\nJedox was founded by Kristian Raue in 2002 and developed by Jedox AG, a company based in Freiburg, Germany. The firm currently employs approximately 100 people. Kristian Raue's departure from Jedox was announced in June 2014.\n\nPalo for Excel is an open source plug-in for Microsoft Excel. There is also an open source plug-in for OpenOffice.org named PalOOCa (discontinued), with Java and web client also available from the JPalo project. Palo can also be integrated into other systems via its client libraries for Java, PHP, C/C++, or .NET Framework. It is fairly easy to communicate with Palo OLAP Server, since it uses representational state transfer (REST).\n\nStarting in October 2008, Palo supports XML for Analysis and MultiDimensional eXpressions (MDX) APIs for connectivity, and OLE DB for OLAP interface which allows standard Excel pivot tables to serve as a client tool.\nStarting September 2011, Palo supports SDX dialect of LINQ.\n\nPalo also provides a web-based spreadsheet interface called Palo Web.\n\nPalo Suite is a tightly integrated framework consisting of: Palo MOLAP Server, Palo ETL Server, Palo Web (Palo Spreadsheet - Connection, User, ETL, File and Report Manager), Palo for Excel, Palo Supervision Server and the Palo Client Libraries.\n\nThe Data in Palo database is stored as a cube in the Palo MOLAP server. The Palo Excel Add-In component is used as a service to communicate between the Excel and the Palo MOLAP Server.\n\nJedox announced only commercial licensing is available since 5.1 version (2015).\n\n\n\n"}
{"id": "14498589", "url": "https://en.wikipedia.org/wiki?curid=14498589", "title": "Paysafecard", "text": "Paysafecard\n\npaysafecard is a prepaid online payment method based on vouchers with a 16-digit PIN code, independent of bank account, credit card, or other personal information. Customers can purchase vouchers at local sales outlets and pay online by entering the code at the checkout of the respective website (e. g. an online game). paysafecard codes are \"not\" designated to be passed by mail or telephone.\n\npaysafecard is issued and distributed country-wise; cross-border and cross-currency usage is possible within some limits. The scope of services and partner webshops varies by country. In most countries, a personal account called \"my paysafecard\" for uploading PINs is available. Starting in Austria in 2000, as of 2018 paysafecard is available in over 40 countries. Two former competitors, Dutch \"Wallie\" and British Ukash, have been absorbed into paysafecard.\n\nIn 2013, paysafecard was acquired by British digital wallet provider Skrill, and in 2015 as part of the Skrill Group by the \"Optimal Payments Group\", an global online payment processing provider regulated in the United Kingdom. Optimal Payments subsequently rebranded as Paysafe Group.\n\nWhile the brand name of the payment method has always been \"paysafecard\", in practice it is often shortened to \"Paysafe\",\nthe \"card\" part also being translated into other languages.\n\nIn 2015, paysafecard became a part of the multinational \"Optimal Payments\" online payment corporation, which subsequently rebranded as Paysafe Group. The group now directly offers certain products and services under the \"Paysafe\" brand. paysafecard, however, continues to be a subsidiary brand of the Paysafe Group in its own right, alongside some others, like the digital wallets Skrill and Neteller.\n\nSome sources erreneously interpret \"Paysafe\" to be the name of the company originally issuing the \"paysafecard\" product, and sometimes suggest this to be the core company of the Paysafe Group.\n\nAs of 2018 paysafecard is issued in over 40 countries—covering most of Europe and North America, Australia and New Zealand, and some countries in Western Asia and South America.\n\nCountries with paysafecard available include Argentina, Australia, Austria, Belgium, Bulgaria, Canada, Croatia, Cyprus, the Czech Republic, Denmark, Finland, France, Georgia, Germany, Greece, Hungary, Ireland, Italy, Kuwait, Latvia, Lithuania, Luxembourg, Malta, Mexico, the Netherlands, New Zealand, Norway, Peru, Poland, Portugal, Romania, Saudi-Arabia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Turkey, the United Arab Emirates, the United Kingdom, the United States and Uruguay.\n\nWeb offers payable with paysafecard include online games, social media, telecommunication, music, and others. The scope of websites accepting paysafecard varies per country for several reasons. The most widespread use are online games; for example, League of Legends accepts paysafecard in 41 countries as of 2018. Examples of other partners accepting paysafecard in 20 or more countries include \nPlayStation 4,\nSteam,\nGuild Wars,\nSecond Life,\nSkype,\nFacebook\nand Newegg.\n\nThe company behind paysafecard is based in Vienna, Austria, and was founded in 2000 as \"paysafecard.com Wertkarten AG\" (\"paysafecard.com prepaid card Inc.\"), later transferred into a GmbH, a private limited company. Technical partner providing expertise and the original hardware was IBM Austria. The following year, paysafecard launched also in Germany.\n\nIn 2005, paysafecard received an EU funding under the \"eTEN\" program supporting electronic services with a trans-European dimension. Subsequently, paysafecard launched in Slovenia, Greece, Slovakia, Spain and the United Kingdom.\nIn 2008, British subsidiary \"Prepaid Services Company Ltd.\" received an EU-wide e-money licence by the British Financial Services Authority (FSA), later the Financial Conduct Authority (FCA) under the Financial Services and Markets Act 2000, expanding a \"small e-money issuers certificate\" valid for the United Kingdom in 2006.\nThis authorised the company to issue e-money throughout the European Union. \nAlso in 2008, paysafecard Switzerland obtained a license as a financial intermediary.\n\nIn August 2011, paysafecard absorbed Dutch competitor Wallie. In February 2013, Skrill, one of the largest payment services providers in Europe completed the acquisition of paysafecard.com Wertkarten AG. In November 2014, Skrill acquired British paysafecard competitor Ukash and merged it into paysafecard. In August 2015, Skrill group competitor \"Optimal Payments\" group completed the acquisition of Skrill group, including paysafecard, and subsequently rebranded as \"Paysafe Group\".\n\nCore of paysafecard is a 16-digit PIN, sold (most usually) on a printout, a pre-printed prepaid card, or as a digital information online. In the respective countries, paysafecard is available in demoninations ranging from 10 to 100 euros, United States dollars, pounds sterling, and Australian dollars, and approximately equivalent sums in other currencies, e. g. 100-1000 Norwegian kroner.\n\nWhen paying in an online shop, the user enters the 16 digit PIN, and the amount tendered is deducted from the paysafecard balance. Hence, the same PIN code can be used multiple times. For larger sums it is possible to combine up to ten paysafecard PINs.\n\nThe current balance of each paysafecard as well as its transaction history and production date can be viewed at the official site by entering the respective 16 PIN code.\n\nThe paysafecard company earns varying proportions of merchants' transaction volumes, shared with the distribution partners. They do not charge any fees to the customer for purchasing and using paysafecard, as well as checking the balance of a PIN or its payment history online. Nonetheless, in the following situations fees are to be paid:\n\nIn contrast to in-house vouchers and prepaid products, paysafecard is sold and accepted at distinct companies (\"Third Party Billing\"). For this reason, dealing with them is a banking transaction and requires either a banking partner or a banking licence. In the beginning, paysafecard was issued by the BAWAG P.S.K. in Austria and the Commerzbank in Germany.\n\nSince 2008, paysafecard's subsidiary company \"Prepaid Services Company Limited\" holds a licence of the Financial Conduct Authority (resp. its predecessor Financial Services Authority) \"to issue electronic money (e-money) and provide payment services\", valid for all countries in the European Economic Area. Therefore in those countries, paysafecard is issued by \"Prepaid Services\" without external banking partner. Further, Swiss paysafecard subsidiary \"paysafecard.com Schweiz GmbH\" is a licensed financial intermediary in Switzerland and issuer of the product also in other countries, e.g. in Australia.\n\nIn the United States, payment regulations require an external banking partner issuing paysafecard, a role assumed by \"The Bancorp Bank\".\n\nIn the beginning, paysafecard was sold as pre-printed scratchcard. Starting in 2002, paysafecards are usually sold as printouts called \"eVouchers\" by the company, uniquely numbered receipts from in-store sales terminals. paysafecard is distributed country-wise by varying external partners, which is why also shop types offering paysafecard vary by country. Typical examples include newsagents, petrol stations, post offices, pharmacies, supermarkets, electrical retailers and vending machines. The world-wide number of points of sales sums up to 600,000.\n\nAside normal paysafecard, the company offers several other services, most notably in most countries an online account called \"my paysafecard\" to upload PINs and to do repeated payments more comfortably. At the payment checkout of a webshop, customers may choose between entering a PIN and signing in to their \"my paysafecard\" account. The account also includes a loyalty program called \"my PLUS\" - doing payments will earn you points that can be exchanged for different benefits.\n\nIn several countries, paysafecard also issues a prepaid \"paysafecard Mastercard\".\n\npaysafecard has limitations manifesting especially when used for online gambling. Payments (thus also deposits in gambling accounts) with paysafecard are inherently limited to the amount of €1000; for classic paysafecard (not \"my paysafecard\") this amount has been lowered to €250 in 2016 due to compliance regulations. Many other online payment methods allow a multiple of those amounts. Also, in contrast to other payment methods, accountless paysafecard does not enable a customer to withdraw money. However, a withdrawal to \"my paysafecard\" is possible and supported by several online casinos.\n\nUnlike credit cards, paysafecard does not give the opportunity to overdraw your fundings, which may be appreciated as a means of spending control.\n\npaysafecard is a payment method that enables the customer to stay anonymous not only to the merchants, but also with respect to the payment provider. The amount of money exposed to loss by accident or fraud is limited to the denomination of the voucher purchased; no bank account or credit card data are put at risk.\n\nNonetheless, there are varying instances of abuse:\npaysafecard sets clear that PINs are only to be entered at authorised online shops, and never to be passed in any other way. The payment method is never used for governmental fines or reminder fees.\n\nFrom 2009 onwards, paysafecard has won several \"Paybefore Awards\" (since 2016 \"Pay Awards\"), issued by the US \"Paybefore\" media group, a daughter of Informa, with a focus on prepaid electronic payment solutions. \nThe first of those awards was 2009 \"Best Non-U.S. Prepaid Program\". Other examples include 2012 \"Most Innovative Prepaid Solution Europe\" and 2016 \"Best Online or Mobile Commerce Solution\".\n\nIn the annual contest \"Austria’s Leading Companies\", awarding top businesses in all states of Austria and the entire republic, paysafecard has won 1st places in Vienna in 2015 and 2017, respectively ranking 3rd and 2nd in Austria overall.\n\nIn the 2018 Austrian \"Great Place to Work\" contest, paysafecard won 1st place in the category \"New Working World and Quality of Living\", as well as 10th place among all medium-sized companies.\n\n"}
{"id": "37119188", "url": "https://en.wikipedia.org/wiki?curid=37119188", "title": "Photovoltaic retinal prosthesis", "text": "Photovoltaic retinal prosthesis\n\nPhotovoltaic retinal prosthesis is a technology for restoring sight to patients blinded by degenerative retinal diseases, such as retinitis pigmentosa and age-related macular degeneration (AMD), when patients lose the 'image capturing' photoreceptors, but neurons in the 'image-processing' inner retinal layers are relatively well-preserved . This subretinal prosthesis is designed to restore a patients' sight by electrically stimulating the surviving inner retinal neurons, primarily the bipolar cells. Photovoltaic retinal implants are completely wireless and powered by near-infrared illumination (880nm) projected from the video goggles. Therefore, they do not require such complex surgical methods as needed for other retinal implants, which are powered via extraocular electronics connected to the retinal array by a trans-scleral cable . Optical activation of the photovoltaic pixels allows scaling the implants to thousands of electrodes. \n\nStudies in rats with retinal degeneration demonstrated that prosthetic vision with such subretinal implants preserves many features of natural vision, including flicker fusion at high frequencies (>20 Hz), adaptation to static images, center-surround organization and non-linear summation of subunits in receptive fields, providing high spatial resolution. Grating visual acuity measured with 70μm pixels matches the sampling density limit (pixel pitch). Clinical trial with these implants (PRIMA, Pixium Vision) having 100μm pixels started in 2018, and the initial results already confirmed that patients indeed perceive projected patterns with spatial resolution limited by the pixel size. Implants with pixels of 50μm and smaller are being developed by Palanker group at Stanford University. \n"}
{"id": "6758231", "url": "https://en.wikipedia.org/wiki?curid=6758231", "title": "Portable engine", "text": "Portable engine\n\nA portable engine is an engine, either a steam engine or an internal combustion engine, that sits in one place while operating (providing power to machinery), but (unlike a stationary engine) is portable and thus can be easily moved from one work site to another. Mounted on wheels or skids, it is either towed to the work site or moves there via self-propulsion.\n\nPortable engines were in common use in industrialised countries from the early 19th through early 20th centuries, during an era when mechanical power transmission was widespread. Before that, most power generation and transmission were by animal, water, wind, or human; after that, a combination of electrification (including rural electrification) and modern vehicles and equipment (such as tractors, trucks, cars, engine-generators, and machines with their engines built in) displaced most use of portable engines. In developing countries today, portable engines still have some use (typically in the form of modern small engines mounted on boards), although the technologies mentioned above increasingly limit their demand there as well. In industrialised countries they are no longer used for commercial purposes, but preserved examples can often be seen at steam fairs driving appropriate equipment for demonstration purposes.\n\nPortable engines during their heyday were typically towed to their work sites by draft horses or oxen, or, in the latter part of that era, motive power including self-propulsion or towing by traction engines, steam tractors, other tractors, or trucks. They were used to drive agricultural machinery (such as threshing machines), milling machinery (such as gristmills, sawmills, and ore mills), pumps and fans (such as in mines and oil wells), and factory line shafts (for machine tools, power hammers, presses, and other machines).\n\nIn common with many other areas of steam technology, the initial design and development of portable engines took place in England, with many other countries initially importing British-built equipment rather than developing their own.\n\nEarly steam engines were too large and expensive for use on the average farm; however, the first positive evidence of steam power being used to drive a threshing machine was in 1799 in north Yorkshire. The next recorded application was in 1812, when Richard Trevithick designed the first 'semi-portable' stationary steam engine for agricultural use, known as a \"barn engine\". This was a high-pressure, rotative engine with a Cornish boiler, for Sir Christopher Hawkins of Probus, Cornwall. It was used to drive a corn threshing machine and was much cheaper to run than the horses it replaced. Indeed, it was so successful that it remained in use for nearly 70 years, and has been preserved by the Science Museum in London. Although termed 'semi-portable', as they could be transported and installed without being dismantled, these engines were essentially stationary. They were used to drive such barn machinery as pumps and hammer mills, bone-crushers, chaff and turnip cutters, and fixed and mobile threshing drums.\n\nIt was not until about 1839 that the truly portable engine appeared, allowing the application of steam power beyond the confines of the farmyard. William Tuxford of Boston, Lincolnshire started manufacture of an engine built around a locomotive-style boiler with horizontal smoke tubes. A single cylinder and the crankshaft were mounted on top of the boiler, and the whole assembly was mounted on four wheels: the front pair being steerable and fitted with shafts for horse-haulage between jobs. A large flywheel was mounted on the crankshaft, and a stout leather belt was used to transfer the drive to the equipment being driven.\n\nRansomes built an early portable in 1841 and exhibited it at the Royal Agricultural Society show that year. The next year Ransomes converted the steam engine to self driving, thus making the first traction engine\nSeveral Tuxford engines were displayed at the Royal Agricultural Society's Show at Bristol in 1842, and other manufacturers soon joined in, using the basic design of the Tuxford engine as a pattern for the majority of portable engines produced thereafter.\n\nEarly manufacturers in the UK included:\n\nThis last manufacturer is particularly noteworthy here. The first Clayton & Shuttleworth portable was built in 1845, a two-cylinder engine. In 1852, the company won a gold medal for a portable engine at the Royal Agricultural Society's Gloucester show, and thereafter the business expanded rapidly: they established a second works, in Vienna in 1857, to target the European market, and by 1890 the company had manufactured over 26,000 portable engines, many being exported all over the world.\n\nIn the 1850s, John Fowler used a Clayton & Shuttleworth portable engine to drive apparatus in the first public demonstrations of the application of cable haulage to cultivation.\n\nIn parallel with the early portable engine development, many engineers attempted to make them self-propelled – the fore-runners of the traction engine. In most cases this was achieved by fitting a sprocket on the end of the crankshaft, and running a chain from this to a larger sprocket on the rear axle. These experiments met with mixed success.\n\nAs noted early on by Thomas Aveling (later of Aveling & Porter fame), it was absurd to use four horses to pull a steam engine from job-to-job, when the engine possessed ten times the strength of the horses. It was therefore inevitable, once self-propelled traction engines had become sufficiently reliable, that they would take over the roles of many portable engines, and this indeed started to happen from the late 1860s.\n\nOther builders manufactured engines around the world. Small\nmachine shops could assemble units with a small engine and vertical\nboiler and put it on wheels. In North America dozens of builders\nentered the market—Case, Sawyer Massey, and Gaar Scott for example.\nNative builders erected engines in France, Italy, Sweden and Germany.\n\nHowever, the portable engine was never completely replaced by the traction engine. Firstly, the portable, having no gearing, was markedly cheaper, and secondly, numerous applications benefitted from a simple steam engine that could be moved, but did not require the additional complexity of one that could move itself.\n\nSmall numbers of portables continued to be built even after traction engine production ceased. Robey and Company of Lincoln were still offering portables for sale into the 1960s. The English builders produced in the order of 100,000 portable steam engines in the hundred year time period both for home use and export abroad.\n\nFrom about 1900 onward, the requirement for a small cheap source of power on farms was increasingly taken over by internal combustion engines, such as hit-and-miss engines and, later, stationary and portable industrial versions of car and truck engines, either for belt use or built into engine-generators.\n\nApart from threshing work, portable engines were used to drive corn-mills, centrifugal pumps, stone-crushers, dynamos, chaff-cutters, hay-balers and saw benches. They were even used to generate electricity for floodlighting at football matches, the first instance being at Bramall Lane, Sheffield in 1878.\n\nIn general, the portable engine is hauled to the work area, often a farmyard or field, and a long drive belt is fitted between the engine's flywheel and the driving wheel of the equipment to be powered.\n\nIn a number of cases, rather than being towed from site-to-site, the portable engine was semi-permanently installed in a building as a stationary steam engine, although the wheels were not necessarily removed.\n\nA more extreme use occurs where the engine is removed from the boiler and is re-used as a stationary engine. Often, the boiler is also re-used (without its wheels) to provide the steam. As of 2007, there are still examples of such dismantled portable engines working commercially in small rice mills in Burma (and, no doubt, elsewhere too). Such examples are easy to identify due to the curved saddle, below the cylinder block, that was used to mount the engine to the boiler.\nThousands of 1 horsepower portable engines were built in China, during\nthe Great Leap Forward of 1958. Every village and peasant was encouraged to build a small iron smelter and produce pig iron. To power the blowers to sustain the retort temperatures, small straw burning engines were built. It was all to no effect as the end pig iron was of very poor quality for any use.\n\nThe most common arrangement follows the original Tuxford design. Although this closely resembles the common layout of a traction engine, the engine of a portable is usually reversed, with the cylinders at the firebox end and the crankshaft at the smokebox end. This layout was designed to position the regulator close to the firebox, making it easier for the engineman to maintain the fire and control the engine speed from the one location. An added bonus is that the flywheel is clear of the rear road wheels so the latter can be set on a narrower track, making the engine easier to manoeuvre through field gates.\n\nA few makers (e.g. Fowler) made their portable engines in the same style as traction engines, with the cylinder at the smokebox end. This was probably to reduce manufacturing costs, as there is no other obvious benefit of doing this. (Thomas Aveling realised that, for a traction engine, it would be better to position the flywheel within reach of the driver in case he carelessly allowed the crank to stop on top dead centre (where it could not self-start) and most other traction engine manufacturers followed this same idea.)\n\nThis is usually a fire-tube boiler with a locomotive-type firebox. However, some designs (e.g. the Marshall \"Britannia\" (pictured)) have circular, marine-type, fireboxes. This latter type were also known by British manufacturers as 'colonial' boilers, as they were mainly intended for export to 'the Colonies', and had a high ground clearance for travelling along rough tracks.\n\nFuel is usually coal but the engine may be designed to use wood fuel, straw or bagasse (sugar cane residue) instead. A longer, circular firebox is particularly suitable for burning logs rather than shorter wood billets. Machines designed for wood-burning may be fitted with spark arrestors.\n\nMost portable engines are single-cylinder but two-cylinder engines were also built. The slide valve is usually driven by a single eccentric and no reversing gear is fitted. There is usually a belt-driven governor to keep the engine running at constant speed, even if the load fluctuates.\n\nThe engine may have one or two flywheels mounted on the same crankshaft. Where two are provided, they are mounted either side of the engine and may be of different diameters. A smaller flywheel provides a slower speed for farmyard work (e.g. chopping feedstuffs) than is required for driving a threshing machine (for example).\n\nThe crankshaft drives a boiler feedwater pump which draws water from a barrel placed alongside the engine. Many engines have a simple, but effective, feedwater heater which works by blowing a small portion of the exhaust steam into the water barrel. The barrel also acts as an oil separator. Oil in the exhaust steam rises to the top of the barrel and can be skimmed off.\n\nA tall chimney is provided to ensure a good draught for the fire. To permit negotiation of overhead obstacles, the chimney is hinged at its base, and is folded down for transport and storage. A suitably shaped bracket is usually provided towards the firebox end to support the chimney when folded.\n\nMost designs are fitted with four wheels and no suspension of any kind. The first portables had wooden wheels, but as the engines became more powerful (and heavier), fabricated steel wheels were fitted instead.\n\nThe 'front' wheels are normally smaller than those at the back. This is because they are mounted on the swivelling \"fore-carriage\", under the smokebox, and large wheels would be liable to hit the boiler when the engine was turned around a corner. An added bonus is that a larger diameter flywheel may be fitted, providing a more steady power output.\n\nMany portable engines still survive, as they were built in large quantities and were shipped to many remote corners of the Earth. A substantial number of them have been preserved, with many restored to full working order: their relatively small size and simpler construction, compared to a traction engine, makes them a much more viable proposition for restoration by the average enthusiast. (That is, provided the boiler is in reasonable condition: boiler repairs can be very expensive; replacement boilers even more so.)\n\nIt is usually possible to see portable engines working at traction engine rallies and steam festivals. At the Great Dorset Steam Fair, for example, portable engines may be found in the relevant demonstration areas driving saw benches, threshing machines, rock crushers and other contemporary equipment.\n\nNumerous agricultural and industrial museums include portable engines within their collections.\n\nWhat is thought to be the oldest surviving Marshall product, works no. 415, a 2.5 nhp portable from 1866, may be seen at the Turon Technology Museum (Museum of Power), in New South Wales. This engine is also the oldest \"documented\" portable in Australia.\n\n\n\n"}
{"id": "4683778", "url": "https://en.wikipedia.org/wiki?curid=4683778", "title": "Region of interest", "text": "Region of interest\n\nA region of interest (often abbreviated ROI), are samples within a data set identified for a particular purpose. The concept of a ROI is commonly used in many application areas. For example, in medical imaging, the boundaries of a tumor may be defined on an image or in a volume, for the purpose of measuring its size. The endocardial border may be defined on an image, perhaps during different phases of the cardiac cycle, for example, end-systole and end-diastole, for the purpose of assessing cardiac function. In geographical information systems (GIS), a ROI can be taken literally as a polygonal selection from a 2D map. In computer vision and optical character recognition, the ROI defines the borders of an object under consideration. In many applications, symbolic (textual) labels are added to a ROI, to describe its content in a compact manner. Within a ROI may lie individual points of interest (POIs). \n\n\nA ROI is a form of annotation, often associated with categorical or quantitative information (e.g., measurements like volume or mean intensity), expressed as text or in a structured form.\n\nThere are three fundamentally different means of encoding a ROI:\n\nMedical imaging standards such as DICOM provide general and application-specific mechanisms to support various use-cases.\n\nFor DICOM images (two or more dimensions):\n\nFor DICOM radiotherapy:\n\nFor DICOM time-based waveforms:\n\nHL7 Clinical Document Architecture also has a subset of mechanisms similar to (and intended to be compatible with) DICOM for referencing image-related spatial coordinates as observations; it allows for a circle, ellipse, polyline or point to be defined as integer pixel-relative coordinates referencing an external multi-media image object, which may be of a consumer rather than medical image format (e.g., a GIF, PNG or JPEG).\n\nIn Optical Character Recognition (OCR) and Document Layout Analysis, regions of interest (ROIs) hierarchically encompass pages, text or graphical blocks, down to individual line-strip images, word and character image boxes. The de facto standard in archives and libraries is the tuplet {image_file,xml_file}, usually in the form of a *.tif file and its accompanying *.xml file.\n\nAs far as non-medical standards are concerned, in addition to the purely graphic markup languages (such as PostScript or PDF) and vector graphic (such as SVG) and 3D (such as VRML) drawing file formats that are widely available, and which carry no specific ROI semantics, some standards such as JPEG 2000 specifically provide mechanisms to label and/or compress to a different degree of fidelity, what they refer to as regions of interest.\n"}
{"id": "9150059", "url": "https://en.wikipedia.org/wiki?curid=9150059", "title": "Repeater insertion", "text": "Repeater insertion\n\nRepeater insertion is a technique for reducing the time delay associated with long wire lines in integrated circuits. The technique involves cutting the long wire into one or more short wires and inserting a repeater between each new pair of short wires.\n\nThe time it takes for a signal to travel from one end of a wire to the other end is known as \"wire-line delay\" or just \"delay.\" In an integrated circuit, this delay is characterized by RC, the resistance of the wire (\"R\") multiplied by the wire's capacitance (\"C\"). Thus, if the wire's resistance is 100 ohms and its capacitance is 0.01 microfarad (μF), the wire's delay is one microsecond (µs).\n\nTo first order, the resistance of a wire on an integrated circuit is directly proportional, or \"linear\", according to the wire's length. If a 1 mm length of the wire has 100 ohms resistance, then a 2 mm length will have 200 ohms resistance.\n\nFor the purposes of our highly simplified discussion, the capacitance of a wire also increases linearly along its length. If a 1 mm length of the wire has 0.01 µF capacitance, a 2 mm length of the wire will have 0.02 µF, a 3 mm wire will have 0.03 µF, and so on.\n\nThus, the time delay through a wire increases with the square of the wire's length. This is true, to first order, for any wire whose cross-section remains constant along the length of the wire.\n\nThe interesting consequence of this behavior is that, while a single 2 mm length of wire has a delay of 4 µs, two separate 1 mm wires only have a delay of 1 µs each. The two separate wires cover the same distance in half the time! By cutting the wire in half, we can double its speed.\n\nTo make this magic trick work properly, an active circuit must be placed between the two separate wires so as to move the signal from one to the next. An active circuit used for such a purpose is known as a \"repeater.\" In a CMOS integrated circuit, the repeater is often a simple inverter.\n\nReducing the delay of a wire by cutting it in half and inserting a repeater is known as \"repeater insertion\". The cost of this procedure is the additional new delay through the repeater itself, plus power cost because the repeater is an active circuit that must be powered, whereas the plain unrepeated wire was originally an unpowered passive component.\n\nFor more details, see for example Anikreddy and Burleson's paper, \n\"Repeater Insertion in deep sub-micron CMOS: Ramp-based \"\n\"Analytical Model and Placement Sensitivity Analysis,\"\nin ISCAS 2000, the IEEE International Symposium on Circuits and Systems, \nMay 28–31, 2000, Geneva, Switzerland\n(http://ieeexplore.ieee.org/iel5/6910/18588/00856173.pdf).\n"}
{"id": "41682", "url": "https://en.wikipedia.org/wiki?curid=41682", "title": "Secondary frequency standard", "text": "Secondary frequency standard\n\nIn telecommunications, a secondary frequency standard is a frequency standard that does not have inherent accuracy, and therefore must be calibrated against a primary frequency standard. \n\nSecondary standards include crystal oscillators and rubidium standards. A crystal oscillator depends for its frequency on its physical dimensions, which vary with fabrication and environmental conditions. A rubidium standard is a secondary standard even though it uses atomic transitions, because it takes the form of a gas cell through which an optical signal is passed. The gas cell has inherent inaccuracies because of gas pressure variations, including those induced by temperature variations. There are also variations in the concentrations of the required buffer gases, which variations cause frequency deviations.\n"}
{"id": "51616709", "url": "https://en.wikipedia.org/wiki?curid=51616709", "title": "Smokeless Powder Company", "text": "Smokeless Powder Company\n\nIn 1888 the 'Smokeless Powder Company', owned by James Dalziel Dougall Junior, the son of the famous glaswegian gunsmith -J D Dougall, took a 99-year lease for 126 arces around 'The Outpost', from the Youngsbury Estate. The site's name was changed from 'The Outpost' to Barwick and Barwick was formed as a 'factory hamlet'.\n\nThe 'factory hamlet' was designed and superintend by the company's engineer Ernest Spon A.I.C.E. Mr Ernest Spon was well known for his civil engineering books, such as 'Workshop Receipts' and 'The Present Practice of Sinking and Boring Wells...'\n\nThe Smokeless Powder Company (S.P.C.) manufactured various high explosive powders for use in torpedoes, artillery shells, small arms ammunition (for the military and sporting) and mine blasting. The 'Smokeless Powder Company' was a world leader in its high explosive powders. S.P.C had over 100 employees at the factory.\n\nOn 26 May 1893, there was an explosion & fire in one of the drying houses. As a result, company employees Mr A Aylott & Mr A Ginn both died in this incident[4] The accident was thoroughly investigated by H.M. Chief Inspector of Explosives - Colonel V.D.Majendie C.B; on 20 June 1893.\n\nIn 1896 the Smokeless Powder Company, was purchased by the New Schultze Gunpowder Company Limited, located at Eyeworth, Fritham, Hampshire.[5] As a result of this sale, the company was renamed the Smokeless Powder & Ammunition Company Limited in 1896. The company had two of Great Britain's greatest ballistics' experts working for it - Mr F W Jones & Mr R W S Griffith. The Smokeless Powder & Ammunition Company continued to produce high explosive powders until it ceased trading in circa 1910.\n"}
{"id": "26031935", "url": "https://en.wikipedia.org/wiki?curid=26031935", "title": "Technology scouting", "text": "Technology scouting\n\nTechnology scouting is an element of technology management in which \nIt is a starting point of a long term and interactive matching process between external technologies and internal requirements of an existing organization for strategic purposes. This matching may also be aided by technology roadmapping. Technology scouting is also known to be part of competitive intelligence, which firms apply as a tool of competitive strategy. It can also be regarded as a method of technology forecasting or in the broader context also an element of corporate foresight. Technology scouting may also be applied as an element of an open innovation approach. Technology scouting is seen as an essential element of a modern technology management system.\n\nThe technology scout is either an employee of the company or an external consultant who engages in boundary spanning processes to tap into novel knowledge and span internal boundaries. He or she may be assigned part-time or full-time to the scouting task. The desired characteristics of a technology scout are similar to the characteristics associated with the technological gatekeeper. These characteristics include being a lateral thinker, knowledgeable in science and technology, respected inside the company, cross-disciplinary orientated, and imaginative personality. Technology scouts would also often play a vital role in a formalised technology foresight process.\n\nDocumented case studies include:\n\n\n"}
{"id": "35801977", "url": "https://en.wikipedia.org/wiki?curid=35801977", "title": "The Mindy Project", "text": "The Mindy Project\n\nThe Mindy Project was an American romantic comedy television series that premiered on Fox on September 25, 2012. It then began airing on Hulu on September 15, 2015. The series, created by and starring Mindy Kaling, was co-produced by Universal Television and 3 Arts Entertainment.\n\nOn March 29, 2017, \"The Mindy Project\" was renewed for a sixth and final season, which premiered on September 12, 2017, and concluded on November 14, 2017.\n\nThe series follows obstetrician/gynecologist Mindy Lahiri (Mindy Kaling) as she tries to balance her personal and professional life, surrounded by quirky co-workers in a small medical practice in New York City. The character was inspired by Kaling's own mother, an OB/GYN. Mindy explores life with the help of her co-workers: Danny Castellano, her best friend and love interest, whose religious sensibilities occasionally cause some tension; Jeremy Reed, an English physician who manages the practice; Peter Prentice, another physician who was a fraternity jock while attending Dartmouth; Morgan Tookers, a wacky, yet loveable registered nurse and an ex-con; Tamra Webb, a blunt nurse; and Beverley, the libidinous older office receptionist.\n\n\nThe series was initially commissioned by NBC, but the pilot with the working title \"It's Messy\" was released from NBC's projects on January 27, 2012. NBC then sent the script to Fox executives who read it over that following weekend. On January 30, 2012, Fox greenlit the pilot, with Mindy Kaling attached to star.\n\nOn May 9, 2012, Fox placed a series order for the comedy. Two days later, the title was changed from \"It's Messy\" to \"The Mindy Project.\" On August 27, 2012, the pilot episode was made available to view online on various sites including Fox, in an attempt to garner interest in the series. On October 8, 2012, Fox ordered a full season of \"The Mindy Project\". On March 4, 2013, the series was renewed for a second season, which began on September 17, 2013. On November 21, 2013, Fox announced that \"The Mindy Project\" would take a mid-season hiatus, before returning on April 1, 2014. Fox announced the third season renewal of \"The Mindy Project\" on March 7, 2014.\n\nOn May 6, 2015, Fox cancelled the series after three seasons. On May 15, 2015 Hulu picked up the show, commissioning a 26 episode fourth season. On May 4, 2016, Hulu announced it had picked up the series for a 16-episode season 5, which was later reduced to 14. On March 29, 2017, Kaling announced the series would return for a sixth and final season; the final episode of the series aired on November 14, 2017.\n\nCasting announcements for the remaining series regular roles began in February 2012, with Ed Weeks cast in the role of Dr. Jeremy Reed, a sexy, British doctor in the practice. Zoe Jarman and Dana Delorenzo then joined in series regular roles. Jarman signed on to play Betsy Putch, an upbeat receptionist at the practice; whilst Delorenzo joined as Shauna Dicanio, a young, party loving receptionist in the practice. In mid-March, Chris Messina joined the cast as Dr. Danny Castellano, an arrogant doctor who works at the practice. Shortly after, Anna Camp boarded the series as Gwen Grandy, a stay-at-home mom and Mindy's best friend.\n\nA few months later, Amanda Setton, Stephen Tobolowsky and Ike Barinholtz joined the series. Setton replaced Dana DeLorenzo in the role of Shauna; Tobolowsky joined in the series regular role of Dr. Marc Shulman, the senior partner of the practice; and Barinholtz signed onto the recurring role of Morgan Tookers, a quirky rehabilitated ex-con who joins the practice as a nurse.\n\nThe first change occurred when Stephen Tobolowsky's role was eliminated after the second episode – following re-shoots that saw his character be re-written into a mere authoritarian figure in the office – when it was decided that \"they didn't really want Mindy to have a boss in the office\". Tobolowsky returned, albeit in a voice-over only role, in the eighth episode to explain that his character had retired. The second change of the season, which was first reported on November 20, 2012, saw Amanda Setton's role also eliminated entirely and Anna Camp downgraded to a recurring cast member. They both departed after the twelfth episode. Despite becoming a recurring cast member, Camp only made a single appearance in the seventeenth episode. Neither Setton or Camp's characters had their absence referenced, nor were they mentioned again.\n\nThe first change for the second season, which occurred before production of the season began, saw Xosha Roquemore upgraded to a series regular role after recurring in the final three episodes of the previous season. The second change, which occurred just prior to the production of the season's fifth episode, resulted in Adam Pally also being upgraded to a series regular role. He had previously signed onto the series in a recurring role but had filmed only two episodes prior to being promoted.\n\nThe first change of the third season occurred prior to the start of production and saw Zoe Jarman's role eliminated from the show. Like Setton and Camp's departures in the first season, the absence of her character was never addressed. The second cast change of the season was announced on November 21, 2014, and saw Adam Pally leave the series after the season's thirteenth episode. His departure after the thirteenth episode of the season was a condition of his upgrade to a series regular that had occurred the previous year. It was reported that Pally is \"expected to make occasional guest appearances\". Pally made multiple guest appearances after his departure, where he was still credited amongst the regular cast in episodes he appeared, making his final appearance as a part of the main cast in the third-season finale.\n\nIn December 2015 it was reported that Fortune Feimster had been upgraded to a series regular role beginning with the fourteenth episode of the fourth season, after appearing in seven of the first thirteen episodes. Garret Dillahunt joined the show in a major recurring role as Dr. Jody Kimball-Kinney.\n\nIn June 2016, it was announced that Chris Messina would not be returning as a series regular for season five, but will instead appear as a guest star.\n\n\"The Mindy Project\" received positive reviews from critics, with many highlighting Kaling's performance and her unique character, as well as the writing and directing. On Rotten Tomatoes, the first season has a rating of 81%, based on 48 reviews, with an average rating of 7/10. The site's critical consensus reads, \"\"The Mindy Project\" is such a charming comedy, led by Mindy Kaling's impressive talent, that its faults are easy to forgive.\" Review aggregation website Metacritic, which assigns a weighted mean based on reviews from mainstream critics, season one received a score of 69 out of 100, based on 32 critics, indicating \"generally favorable reviews\". It was the number-six best-reviewed show according to the site's fall 2012 season.\n\nOn Rotten Tomatoes, Season 2 has a rating of 89%, based on 18 reviews, with an average rating of 6.8/10. The site's critical consensus reads, \"Mindy Kaling earns consistent laughs with wit, charm and physical comedy, as she and her cast grow into well-rounded and familiar, albeit peculiar, characters.\" On Metacritic, the second season has a score of 55 out of 100, based on four critics, indicating \"mixed or average reviews\". Season 3 of the show holds a rating of 82% on Rotten Tomatoes with the critical consensus reading, \"\"The Mindy Project\", while still wildly funny, travels further into rom-com country this season.\"\n\nSeason 4 of the show holds a rating of 93% on Rotten Tomatoes with the critical consensus reading, \"\"The Mindy Project\" begins its online migration with some of the show's best-ever episodes, further refining the balance between heart and humor struck during the first three seasons.\" On Metacritic the season has a score of 81 out of 100, based on 5 critics, indicating \"universal acclaim\".\n\n\"The Mindy Project\" has been the object of intense scrutiny not only from traditional media forms but also from feminists and people of color, who often weigh in on the show's decisions in casting and writing. At the end of the first season, Jezebel posted an article called \"Mindy Kaling Only Makes Out with White Guys\", a critique of the casting of romantic leads on the show. The same article contains a response from Nisha Chittal, who has defended Kaling's success in the past, who applauded the show for not making Kaling's race central to her character, and for showing that an Indian woman could date non-Indian men.\n\nDuring the second season, the episode \"Mindy Lahiri Is a Racist\" provoked discussion as to whether or not Kaling had made \"reparations\" for showing a predominantly white cast.\n\nBefore the third season began airing, Kaling was criticized for a comment in a \"Flare\" article entitled \"She's the Boss\", in which she stated that the show would not address abortion, as \"it would be demeaning to the topic to talk about it in a half-hour sitcom.\" In an appearance on \"The Colbert Report\" two weeks later, she clarified:\n\nA lot of women look to me and look to the show, and they want me to be a spokesperson for a lot of issues, and I actually think that's a responsibility that's cool, I want to live up to that ... I want to be able to talk plainly on things but I also want to create an entertaining show. We haven't found a hilarious take on abortion that has not been done yet—but we might. I have faith in us.\n\n\n"}
{"id": "45418747", "url": "https://en.wikipedia.org/wiki?curid=45418747", "title": "Timed Text Markup Language", "text": "Timed Text Markup Language\n\nTimed Text Markup Language (TTML), previously referred to as Distribution Format Exchange Profile (DFXP), is one of W3C's standards regulating timed text on the internet.\n\nIn 2010, after discussions about its adoption in HTML5, WHATWG opted for a new but more lightweight standard based on the popular SRT format, now named WebVTT. Nonetheless, in February 2012 the FCC declared the SMPTE closed-captioning standard for online video content, a superset of TTML, as a \"safe harbor interchange, delivery format\".\n\nIt is not clear whether the HTML5 specification will document the usage of any other timed text format with the codice_1 tag. However, as of February 2015, W3C only documents the usage of WebVTT with HTML5 and its specification had 434 entries in its commit history, ranging from March 2013 to January 2015, while the latest TTML specification hadn't been modified since September 2013. Recent versions of Internet Explorer introduced support for TTML files in the codice_1 element, while other browsers still require JavaScript, such as in the W3C Timed Text Working Group's 2009 demo of TTML features. TTML is still the format of choice for some applications that don't rely on HTML5, such as the popular set-top box Roku.\n\nThe TTML standard specifies a wide range of features, many of which are not necessary for specific applications. For this reason, the standard developed the concept of profiles, which are subsets of required features from the full specification. This way, a player that only implements a partial set of features, for example, can define its own profile and claim support to it. The specification was thus renamed to TTML, accounting only to the markup language, and defined three standard profiles: DFXP Transformation, DFXP Presentation and DFXP Full. Not satisfied with the list of features defined by TTML, the Society of Motion Picture and Television Engineers (SMPTE) created its own profile, which extended the full DFXP profile and defined new features, not included in the TTML specification.\n\nThis profile defines the minimum feature requirements that a transformation processor (e.g. caption converter) needs to support in order to be considered TTML compliant.\n\nThis profile defines the minimum feature requirements that a presentation processor (e.g. video player) needs to support in order to be considered TTML compliant.\n\nThis profile requires the support of all the feature defined by TTML specification.\n\nThis profile extends the TTML and with three SMPTE-specific elements aimed at legacy formats. Interoperability with pre-existing and regionally-specific formats (such as CEA-708, CEA-608, DVB Subtitles, and\nWST (World System Teletext)) is provided by means of tunneling data or bit map images and adding necessary metadata.\n\n"}
{"id": "41805", "url": "https://en.wikipedia.org/wiki?curid=41805", "title": "Transceiver", "text": "Transceiver\n\nA transceiver is a device comprising both a transmitter and a receiver that are combined and share common circuitry or a single housing. When no circuitry is common between transmit and receive functions, the device is a transmitter-receiver. The term originated in the early 1920s. Similar devices include transponders, transverters, and repeaters.\n\nIn radio terminology, a transceiver means a unit which contains both a receiver and a transmitter. From the beginning days\nof radio the receiver and transmitter were separate units and remained so until around 1920. Amateur radio or \"ham\" radio operators can build their own equipment and it is now easier to design and build a simple unit containing both of the functions: transmitting and receiving. Almost all modern amateur radio equipment is now a transceiver but there is an active market for pure radio receivers, mainly for shortwave listening (SWL) operators. An example of a transceiver would be a walkie-talkie or a CB radio.\n\nOn a wired telephone, the handset contains the transmitter and receiver for the audio and in the 20th century was usually wired to the base unit by tinsel wire. The whole unit is colloquially referred to as a \"receiver\". On a mobile telephone or other radiotelephone, the entire unit is a transceiver, for both audio and radio.\n\nA cordless telephone uses an audio and radio transceiver for the handset, and a radio transceiver for the base station. If a speakerphone is included in a wired telephone base or in a cordless base station, the base also becomes an audio transceiver in addition to the handset.\n\nA modem is similar to a transceiver, in that it sends and receives a signal, but a modem uses modulation and demodulation. It modulates a signal being transmitted and demodulates a signal being received.\n\nTransceivers are called Medium Attachment Units (MAUs) in IEEE 802.3 documents and were widely used in 10BASE2 and 10BASE5 Ethernet networks. Fiber-optic gigabit, 10 Gigabit Ethernet, 40 Gigabit Ethernet, and 100 Gigabit Ethernet utilize transceivers known as GBIC, SFP, SFP+, QSFP, XFP, XAUI, CXP, and CFP.\n\n\n"}
{"id": "19189578", "url": "https://en.wikipedia.org/wiki?curid=19189578", "title": "TransferJet", "text": "TransferJet\n\nTransferJet is a close proximity wireless transfer technology initially proposed by Sony and demonstrated publicly in early 2008. By touching (or bringing very close together) two electronic devices, TransferJet allows high speed exchange of data. The concept of TransferJet consists of a touch-activated interface which can be applied for applications requiring high-speed data transfer between two devices in a peer-to-peer mode without the need for external physical connectors.\n\nTransferJet's maximum physical layer transmission rate is 560 Mbit/s. After allowing for error correction and other protocol overhead, the effective maximum throughput is 375 Mbit/s. TransferJet will adjust the data rate downward according to the wireless environment, thereby maintaining a robust link even when the surrounding wireless condition fluctuates.\n\nTransferJet has the capability of identifying the unique MAC addresses of individual devices, enabling users to choose which devices can establish a connection. By allowing only devices inside the household, for example, one can prevent data theft from strangers while riding a crowded train. If, on the other hand, one wishes to connect the device with any other device at a party, this can be done by simply disabling the filtering function.\n\nTransferJet uses the same frequency spectrum as UWB, but occupies only a section of this band available as a common worldwide channel. Since the RF power is kept under -70 dBm/MHz, it can operate in the same manner as that of UWB devices equipped with DAA functionality. In addition, this low power level also ensures that there will be no interference to other wireless systems, including other TransferJet systems, operating nearby.\n\nBy reducing the RF power and spatial reach down to a few centimeters (about an inch or less), a TransferJet connection in its most basic mode does not require any initial setup procedure by the user for either device, and the action of spontaneously touching one device with another will automatically trigger the data transfer. More complex usage scenarios will require various means to select the specific data to send as well as the location to store (or method to process) the received data.\n\nTransferJet utilizes a newly developed TransferJet Coupler based on the principle of electric induction field as opposed to radiation field for conventional antennas. The functional elements of a generic TransferJet Coupler consist of a coupling electrode or plate, a resonant stub and ground. Compared to conventional radiating antennas, the TransferJet Coupler achieves higher transmission gain and more efficient coupling in the near-field while providing sharp attenuation at longer distances. Because the Coupler generates longitudinal electric fields, there is no polarization and the devices can be aligned at any angle.\n\nTransferJet Specifications\nAlthough sometimes confused with Near Field Communication, TransferJet depends on an entirely different technology and is also generally targeted for different usage scenarios focusing on high-speed data transfer. Thus these two systems will not interfere with each other and can even co-exist in the same location, as already implemented in certain products.\nOther recent products combine TransferJet with wireless power to allow both data transfer and wireless charging capability simultaneously in the same location. TransferJet, NFC and wireless power are the three major near-field (contact-less) technologies that are expected to eliminate the physical connections and cables currently required to interface devices with each other.\n\nComparison with NFC\nThe TransferJet Consortium was established in July 2008 to advance and promote the TransferJet Format, by developing the technical specifications and compliance testing procedures as well as creating a market for TransferJet-compliant, interoperable products. In September 2011, the consortium was registered as an independent non-profit industry association. As of June 2015, the Consortium is led by five Promoter companies, consisting of: JRC, NTT, Olympus, Sony (consortium administrator), and Toshiba. The Consortium currently also has around thirty Adopter companies.. The TransferJet regular typeface and TransferJet logos are trademarks managed and licensed by the TransferJet Consortium.\n\nCommercial products have been introduced since January 2010 and the initial product categories include digital cameras, laptop PCs, USB cradle accessories, USB dongle accessories and office/business equipment. Compliance testing equipment is provided by Agilent technologies and certification services are offered by Allion Test Labs. The first commercially available TransferJet development platform for embedded systems was launched by Icoteq Ltd in February 2015. Smartphones with integrated TransferJet functionality were launched in June 2015 from Fujitsu, and Bkav.. Other product vendors include Buffalo and E-Globaledge.\n\nTransferJet X is a new second-generation TransferJet specification capable of data transfer speeds of 13.1 Gbit/sec and above, or about 20 times the speed of current TransferJet. This specification uses the 60 GHz band and requires only 2 msec or less to establish a connection prior to the actual data transfer, thereby enabling the exchange of large content files even in the short amount of time it takes, for example, for a person to walk through a wicket gate. The TransferJet Consortium is currently defining the details of the TransferJet X ecosystem, based on the IEEE 802.15.3e standard completed and published in June 2017. The HRCP Research and Development Partnership, established in 2016, is developing an SoC solution for implementing TransferJet X in a variety of products and services to be released starting around 2020. \n\n"}
{"id": "39154794", "url": "https://en.wikipedia.org/wiki?curid=39154794", "title": "Tunable resistive pulse sensing", "text": "Tunable resistive pulse sensing\n\nTunable Resistive Pulse Sensing (TRPS) is a technique that allows high-throughput single particle measurements as colloids and/or biomolecular analytes driven through a size-tunable nanopore, one at a time.\n\nThe technique adapts the principle of resistive pulse-sensing, which monitors current flow through an aperture, combined with the use of tunable nanopore technology, allowing the passage of ionic current and particles to be regulated by adjusting the pore size. \n\nParticles crossing a pore are detected one at a time as a transient change in the ionic current flow, which is denoted as a blockade event with its amplitude denoted as the blockade magnitude. As blockade magnitude is proportional to particle size, accurate particle sizing can be achieved after calibration with a known standard.\n\nNanopore-based detection allows particle-by-particle assessment of complex mixtures. Optimization of pore size to particle size, by adjusting the stretch of the pore, can improve measurement accuracy.\n\nThrough combination with fine-control of pressure TRPS has been used to determine sample concentration and to accurately derive particle electrophoretic mobility & surface charge in addition to particle size information.\n\nTRPS has been applied in product development by nanotechnology instrument manufacturers Izon Science Ltd in the first commercially available nanopore-based particle characterization systems. These systems have been applied to measure a wide range of biological and synthetic particle types including viruses and nanoparticles. TRPS has been applied in both academic and industrial research fields, including:\n"}
{"id": "33380874", "url": "https://en.wikipedia.org/wiki?curid=33380874", "title": "Vertebrae bend restrictor", "text": "Vertebrae bend restrictor\n\nA Vertebrae Bend Restrictor (VBR) is used in the oil and gas industry as part of offshore deep sea drilling operations. It is designed to prevent damage to an umbilical cable from overbending. It offsets the action of applied loads which could kink or buckle the internal conduits of an umbilical, cable, flexible riser pipe or MUX line.\n\nMost people in the subsea industry have never seen a VBR break during testing or regular use, but manufacturers of VBRs occasionally test VBR assemblies to ultimate failure.\n\n\n"}
