{"id": "223352", "url": "https://en.wikipedia.org/wiki?curid=223352", "title": "Alternator", "text": "Alternator\n\nAn alternator is an electrical generator that converts mechanical energy to electrical energy in the form of alternating current. For reasons of cost and simplicity, most alternators use a rotating magnetic field with a stationary armature. Occasionally, a linear alternator or a rotating armature with a stationary magnetic field is used. In principle, any AC electrical generator can be called an alternator, but usually the term refers to small rotating machines driven by automotive and other internal combustion engines. An alternator that uses a permanent magnet for its magnetic field is called a magneto. Alternators in power stations driven by steam turbines are called turbo-alternators. Large 50 or 60 Hz three phase alternators in power plants generate most of the world's electric power, which is distributed by electric power grids.\n\nAlternating current generating systems were known in simple forms from the discovery of the magnetic induction of electric current in the 1830s. Rotating generators naturally produced alternating current but, since there was little use for it, it was normally converted into direct current via the addition of a commutator in the generator. The early machines were developed by pioneers such as Michael Faraday and Hippolyte Pixii. Faraday developed the \"rotating rectangle\", whose operation was \"heteropolar\" – each active conductor passed successively through regions where the magnetic field was in opposite directions. Lord Kelvin and Sebastian Ferranti also developed early alternators, producing frequencies between 100 and 300 Hz.\n\nThe late 1870s saw the introduction of first large scale electrical systems with central generation stations to power Arc lamps, used to light whole streets, factory yards, or the interior of large warehouses. Some, such as Yablochkov arc lamps introduced in 1878, ran better on alternating current, and the development of these early AC generating systems was accompanied by the first use of the word \"alternator\". Supplying the proper amount of voltage from generating stations in these early systems was left up to the engineer's skill in \"riding the load\". In 1883 the Ganz Works invented the constant voltage generator that could produce a stated output voltage, regardless of the value of the actual load. The introduction of transformers in the mid-1880s led to the widespread use of alternating current and the use of alternators needed to produce it. After 1891, polyphase alternators were introduced to supply currents of multiple differing phases. Later alternators were designed for various alternating current frequencies between sixteen and about one hundred hertz, for use with arc lighting, incandescent lighting and electric motors. Specialized radio frequency alternators like the Alexanderson alternator were developed as longwave radio transmitters around World War 1 and used in a few high power wireless telegraphy stations before vacuum tube transmitters replaced them.\n\nA conductor moving relative to a magnetic field develops an electromotive force (EMF) in it (Faraday's Law). This EMF reverses its polarity when it moves under magnetic poles of opposite polarity. Typically, a rotating magnet, called the rotor turns within a stationary set of conductors wound in coils on an iron core, called the stator. The field cuts across the conductors, generating an induced EMF (electromotive force), as the mechanical input causes the rotor to turn.\n\nThe rotating magnetic field induces an AC voltage in the stator windings. Since the currents in the stator windings vary in step with the position of the rotor, an alternator is a synchronous generator.\n\nThe rotor's magnetic field may be produced by permanent magnets, or by a field coil electromagnet. Automotive alternators use a rotor winding which allows control of the alternator's generated voltage by varying the current in the rotor field winding. Permanent magnet machines avoid the loss due to magnetizing current in the rotor, but are restricted in size, due to the cost of the magnet material. Since the permanent magnet field is constant, the terminal voltage varies directly with the speed of the generator. Brushless AC generators are usually larger than those used in automotive applications.\n\nAn automatic voltage control device controls the field current to keep output voltage constant. If the output voltage from the stationary armature coils drops due to an increase in demand, more current is fed into the rotating field coils through the voltage regulator (VR). This increases the magnetic field around the field coils which induces a greater voltage in the armature coils. Thus, the output voltage is brought back up to its original value.\n\nAlternators used in central power stations also control the field current to regulate reactive power and to help stabilize the power system against the effects of momentary faults. Often there are three sets of stator windings, physically offset so that the rotating magnetic field produces a three phase current, displaced by one-third of a period with respect to each other.\n\nOne cycle of alternating current is produced each time a pair of field poles passes over a point on the stationary winding. The relation between speed and frequency is formula_1, where formula_2 is the frequency in Hz (cycles per second). formula_3 is the number of poles (2,4,6...) and formula_4 is the rotational speed in revolutions per minute (RPM). Very old descriptions of alternating current systems sometimes give the frequency in terms of alternations per minute, counting each half-cycle as one \"alternation\"; so 12,000 alternations per minute corresponds to 100 Hz.\n\nThe output frequency of an alternator depends on the number of poles and the rotational speed. The speed corresponding to a particular frequency is called the \"synchronous speed\" for that frequency. This table gives some examples:\n\nAlternators may be classified by method of excitation, number of phases, the type of rotation, cooling method, and their application.\n\nThere are two main ways to produce the magnetic field used in the alternators, by using permanent magnets which create their own persistent magnetic field or by using field coils. The alternators that use permanent magnets are specifically called magnetos. \nIn other alternators, wound field coils form an electromagnet to produce the rotating magnetic field.\n\nA device that uses permanent magnets to produce alternating current is called a permanent magnet alternator (PMA). A permanent magnet generator (PMG) may produce either alternating current, or direct current if it has a commutator.\n\nThis method of excitation consists of a smaller direct-current (DC) generator fixed on the same shaft with the alternator. The DC generator generates a small amount of electricity just enough to \"excite\" the field coils of the connected alternator to generate electricity. A variation of this system is a type of alternator which uses direct current from the battery for initial excitation upon start-up, after which the alternator becomes self-excited.\nThis method depends on residual magnetism retained in the iron core to generate weak magnetic field which would allow a weak voltage to be generated. This voltage is used to excite the field coils for the alternator to generate stronger voltage as part of its \"build up\" process. After the initial AC voltage buildup, the field is supplied with rectified voltage from the alternator.\n\nA brushless alternator is composed of two alternators built end-to-end on one shaft. Smaller brushless alternators may look like one unit but the two parts are readily identifiable on the large versions. The larger of the two sections is the main alternator and the smaller one is the exciter. The exciter has stationary field coils and a rotating armature (power coils). The main alternator uses the opposite configuration with a rotating field and stationary armature. A bridge rectifier, called the rotating rectifier assembly, is mounted on the rotor. Neither brushes nor slip rings are used, which reduces the number of wearing parts. The main alternator has a rotating field as described above and a stationary armature (power generation windings).\n\nVarying the amount of current through the stationary exciter field coils varies the 3-phase output from the exciter. This output is rectified by a rotating rectifier assembly, mounted on the rotor, and the resultant DC supplies the rotating field of the main alternator and hence alternator output. The result of all this is that a small DC exciter current indirectly controls the output of the main alternator. \nSmall-scale examples are ubiquitous in engine-driven motive-power applications. For example, early Honda four-cylinder motorcycles (CB750F, CB350F, CB500F, CB550F) used a brushless Hitachi 200W alternator. This had a fixed \"rotor\" winding on the outer cover; the outer end of the iron core was a disc that closed the outer rotor pole. The rotor comprised two intermeshed six-pole \"claws\" welded to and spaced apart by a non-magnetic ring. This was bolted directly to the end of the five-bearing crank via the hub of one pole. The other pole had an open end to receive the stator winding. The outer cover also held the three-phase stator windings. The magnetic circuit had two auxiliary air gaps between the rotor and its stationary core. The regulator was a conventional automotive type with vibrating points. As it had no slip rings, it was very compact and rugged, but due to the auxiliary air gaps, it had poor efficiency.\n\nAnother way to classify alternators is by the number of phases of their output voltage. The output can be single phase, or polyphase. Three-phase alternators are the most common, but polyphase alternators can be two phase, six phase, or more.\n\nThe revolving part of alternators can be the armature or the magnetic field. The revolving armature type has the armature wound on the rotor, where the winding moves through a stationary magnetic field. The revolving armature type is not often used. The revolving field type has magnetic field on the rotor to rotate through a stationary armature winding. The advantage is that then the rotor circuit carries much less power than the armature circuit, making the slip ring connections smaller and less costly; only two contacts are needed for the direct-current rotor, whereas often a rotor winding has three phases and multiple sections which would each require a slip-ring connection. The stationary armature can be wound for any convenient medium voltage level, up to tens of thousands of volts; manufacture of slip ring connections for more than a few thousand volts is costly and inconvenient.\n\nMany alternators are cooled by ambient air, forced through the enclosure by an attached fan on the same shaft that drives the alternator. In vehicles such as transit buses, a heavy demand on the electrical system may require a large alternator to be oil-cooled. In marine applications water-cooling is also used. Expensive automobiles may use water-cooled alternators to meet high electrical system demands.\n\nMost power generation stations use synchronous machines as their generators. Connection of these generators to the utility grid requires synchronization conditions to be met.\n\nAlternators are used in modern automobiles to charge the battery and to power the electrical system when its engine is running.\n\nUntil the 1960s, automobiles used DC dynamo generators with commutators. With the availability of affordable silicon diode rectifiers, alternators were used instead.\n\nIn later diesel electric locomotives and diesel electric multiple units, the prime mover turns an alternator which provides electricity for the traction motors (AC or DC).\n\nThe traction alternator usually incorporates integral silicon diode rectifiers to provide the traction motors with up to 1200 volts DC (DC traction, which is used directly) or the common inverter bus (AC traction, which is first inverted from dc to three-phase ac).\n\nThe first diesel electric locomotives, and many of those still in service, use DC generators as, before silicon power electronics, it was easier to control the speed of DC traction motors. Most of these had two generators: one to generate the excitation current for a larger main generator.\n\nOptionally, the generator also supplies head end power (HEP) or power for electric train heating. The HEP option requires a constant engine speed, typically 900 RPM for a 480 V 60 Hz HEP application, even when the locomotive is not moving.\n\nMarine alternators used in yachts are similar to automotive alternators, with appropriate adaptations to the salt-water environment. Marine alternators are designed to be explosion proof so that brush sparking will not ignite explosive gas mixtures in an engine room environment. They may be 12 or 24 volt depending on the type of system installed. Larger marine diesels may have two or more alternators to cope with the heavy electrical demand of a modern yacht. On single alternator circuits, the power may be split between the engine starting battery and the domestic or house battery (or batteries) by use of a split-charge diode (battery isolator) or a voltage-sensitive relay.\n\nHigh frequency alternators of the variable-reluctance type were applied commercially to radio transmission in the low-frequency radio bands. These were used for transmission of Morse code and, experimentally, for transmission of voice and music. In the Alexanderson alternator, both the field winding and armature winding are stationary, and current is induced in the armature by virtue of the changing magnetic reluctance of the rotor (which has no windings or current carrying parts). Such machines were made to produce radio frequency current for radio transmissions, although the efficiency was low.\n\n"}
{"id": "56016417", "url": "https://en.wikipedia.org/wiki?curid=56016417", "title": "Atelectotrauma", "text": "Atelectotrauma\n\nAtelectrotrauma, atelectrauma, cyclic atelectasis or repeated alveolar collapse and expansion (RACE) are medical terms for the damage caused to the lung by mechanical ventilation under certain conditions. When parts of the lung collapse at the end of expiration, due to a combination of a diseased lung state and a low functional residual capacity, then reopen again on inspiration, this repeated collapsing and reopening causes shear stress which has a damaging effect on the alveolus. Clinicians attempt to reduce atelectotrauma by ensuring adequate positive end-expiratory pressure (PEEP) to maintain the alveoli open in expiration. This is known as \"open lung ventilation\". High frequency oscillatory ventilation (HFOV) with its use of 'super CPAP' is especially effective in preventing atelectotrauma since it maintains a very high mean airway pressure (MAP), equivalent to a very high PEEP. Atelectotrauma is one of several means by which mechanical ventilation may damage the lungs leading to ventilator-associated lung injury. The other means are volutrauma, barotrauma, rheotrauma and biotrauma. Attempts have been made to combine these factors in an all encompassing term: mechanical power.\n"}
{"id": "25453985", "url": "https://en.wikipedia.org/wiki?curid=25453985", "title": "Atomic clock", "text": "Atomic clock\n\nAn atomic clock is a clock device that uses an electron transition frequency in the microwave, optical, or ultraviolet region of the electromagnetic spectrum of atoms as a frequency standard for its timekeeping element. Atomic clocks are the most accurate time and frequency standards known, and are used as primary standards for international time distribution services, to control the wave frequency of television broadcasts, and in global navigation satellite systems such as GPS.\n\nThe principle of operation of an atomic clock is based on atomic physics; it uses the microwave signal that electrons in atoms emit when they change energy levels. Early atomic clocks were based on masers at room temperature. Currently, the most accurate atomic clocks first cool the atoms to near absolute zero temperature by slowing them with lasers and probing them in atomic fountains in a microwave-filled cavity. An example of this is the NIST-F1 atomic clock, one of the national primary time and frequency standards of the United States.\n\nThe accuracy of an atomic clock depends on two factors. The first factor is temperature of the sample atoms—colder atoms move much more slowly, allowing longer probe times. The second factor is the frequency and intrinsic width of the electronic transition. Higher frequencies and narrow lines increase the precision.\n\nNational standards agencies in many countries maintain a network of atomic clocks which are intercompared and kept synchronized to an accuracy of 10 seconds per day (approximately 1 part in 10). These clocks collectively define a continuous and stable time scale, the International Atomic Time (TAI). For civil time, another time scale is disseminated, Coordinated Universal Time (UTC). UTC is derived from TAI, but has added leap seconds from UT1, to account for the rotation of the Earth with respect to the solar time.\n\nThe idea of using atomic transitions to measure time was suggested by Lord Kelvin in 1879. Magnetic resonance, developed in the 1930s by Isidor Rabi, became the practical method for doing this. In 1945, Rabi first publicly suggested that atomic beam magnetic resonance might be used as the basis of a clock. The first atomic clock was an ammonia absorption line device at 23870.1 MHz built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). It was less accurate than existing quartz clocks, but served to demonstrate the concept. The first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen and Jack Parry in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET). This led to the internationally agreed definition of the latest SI second being based on atomic time. Equality of the ET second with the (atomic clock) SI second has been verified to within 1 part in 10. The SI second thus inherits the effect of decisions by the original designers of the ephemeris time scale, determining the length of the ET second.\n\nSince the beginning of development in the 1950s, atomic clocks have been based on the hyperfine transitions in hydrogen-1, caesium-133, and rubidium-87. The first commercial atomic clock was the Atomichron, manufactured by the National Company. More than 50 were sold between 1956 and 1960. This bulky and expensive instrument was subsequently replaced by much smaller rack-mountable devices, such as the Hewlett-Packard model 5060 caesium frequency standard, released in 1964.\n\nIn the late 1990s four factors contributed to major advances in clocks:\n\nIn August 2004, NIST scientists demonstrated a chip-scale atomic clock. According to the researchers, the clock was believed to be one-hundredth the size of any other. It requires no more than 125 mW, making it suitable for battery-driven applications. This technology became available commercially in 2011. Ion trap experimental optical clocks are more precise than the current caesium standard.\n\nIn April 2015, NASA announced that it planned to deploy a Deep Space Atomic Clock (DSAC), a miniaturized, ultra-precise mercury-ion atomic clock, into outer space. NASA said that the DSAC would be much more stable than other navigational clocks.\n\nSince 1967, the International System of Units (SI) has defined the second as the duration of of radiation corresponding to the transition between two energy levels of the ground state of the caesium-133 atom. In 1997, the International Committee for Weights and Measures (CIPM) added that the preceding definition refers to a caesium atom at rest at a temperature of absolute zero.\n\nThis definition makes the caesium oscillator the primary standard for time and frequency measurements, called the caesium standard. The definitions of other physical units, e.g., the volt and the metre, rely on the definition of the second.\n\nThe actual time-reference of an atomic clock consists of an electronic oscillator operating at microwave frequency. The oscillator is arranged so that its frequency-determining components include an element that can be controlled by a feedback signal. The feedback signal keeps the oscillator tuned in resonance with the frequency of the electronic transition of caesium or rubidium.\n\nThe core of the atomic clock is a tunable microwave cavity containing a gas. In a hydrogen maser clock the gas emits microwaves (the gas \"mases\") on a hyperfine transition, the field in the cavity oscillates, and the cavity is tuned for maximum microwave amplitude. Alternatively, in a caesium or rubidium clock, the beam or gas absorbs microwaves and the cavity contains an electronic amplifier to make it oscillate. For both types the atoms in the gas are prepared in one electronic state prior to filling them into the cavity. For the second type the number of atoms which change electronic state is detected and the cavity is tuned for a maximum of detected state changes.\n\nMost of the complexity of the clock lies in this adjustment process. The adjustment tries to correct for unwanted side-effects, such as frequencies from other electron transitions, temperature changes, and the spreading in frequencies caused by ensemble effects. One way of doing this is to sweep the microwave oscillator's frequency across a narrow range to generate a modulated signal at the detector. The detector's signal can then be demodulated to apply feedback to control long-term drift in the radio frequency. In this way, the quantum-mechanical properties of the atomic transition frequency of the caesium can be used to tune the microwave oscillator to the same frequency, except for a small amount of experimental error. When a clock is first turned on, it takes a while for the oscillator to stabilize. In practice, the feedback and monitoring mechanism is much more complex.\n\nA number of other atomic clock schemes are in use for other purposes. Rubidium standard clocks are prized for their low cost, small size (commercial standards are as small as 17 cm) and short-term stability. They are used in many commercial, portable and aerospace applications. Hydrogen masers (often manufactured in Russia) have superior short-term stability compared to other standards, but lower long-term accuracy.\n\nOften, one standard is used to fix another. For example, some commercial applications use a rubidium standard periodically corrected by a global positioning system receiver (see GPS disciplined oscillator). This achieves excellent short-term accuracy, with long-term accuracy equal to (and traceable to) the U.S. national time standards.\n\nThe lifetime of a standard is an important practical issue. Modern rubidium standard tubes last more than ten years, and can cost as little as US$50. Caesium reference tubes suitable for national standards currently last about seven years and cost about US$35,000. The long-term stability of hydrogen maser standards decreases because of changes in the cavity's properties over time.\n\nModern clocks use magneto-optical traps to cool the atoms for improved precision.\n\nThe power consumption of atomic clocks varies with their size. Atomic clocks on the scale of one chip require less than 30 milliwatt; Primary frequency and time standards like the United States Time Standard atomic clocks, NIST-F1 and NIST-F2, use far greater quantities of power.\n\nThe evaluated accuracy \"u\" reports of various primary frequency and time standards are published online by the International Bureau of Weights and Measures (BIPM). Several frequency and time standards groups as of 2015 reported \"u\" values in the to range.\n\nIn 2011, the NPL-CsF2 caesium fountain clock operated by the National Physical Laboratory (NPL), which serves as the United Kingdom primary frequency and time standard, was improved regarding the two largest sources of measurement uncertainties — distributed cavity phase and microwave lensing frequency shifts. In 2011 this resulted in an evaluated frequency uncertainty reduction from \"u\" = to \"u\" = ;— the lowest value for any primary national standard at the time. At this frequency uncertainty, the NPL-CsF2 is expected to neither gain nor lose a second in about 138 million () years.\n\nThe NIST-F2 caesium fountain clock operated by the National Institute of Standards and Technology (NIST), was officially launched in April 2014, to serve as a new U.S. civilian frequency and time standard, along with the NIST-F1 standard. The planned \"u\" performance level of NIST-F2 is . \"At this planned performance level the NIST-F2 clock will not lose a second in at least 300 million years.\" NIST-F2 was designed using lessons learned from NIST-F1. The NIST-F2 key advance compared to the NIST-F1 is that the vertical flight tube is now chilled inside a container of liquid nitrogen, at . This cycled cooling dramatically lowers the background radiation and thus reduces some of the very small measurement errors that must be corrected in NIST-F1.\n\nThe first in-house accuracy evaluation of NIST-F2 reported a \"u\" of . However, a published scientific criticism of that NIST F-2 accuracy evaluation described problems in its treatment of distributed cavity phase shifts and the microwave lensing frequency shift, which is treated significantly differently than in the majority of accurate fountain clock evaluations. The next NIST-F2 submission to the BIPM in March, 2015 again reported a \"u\" of , but did not address the standing criticism. There have been neither subsequent reports to the BIPM from NIST-F2 nor has an updated accuracy evaluation been published.\n\nAt the request of the Italian standards organization, NIST fabricated many duplicate components for a second version of NIST-F2, known as IT-CsF2 to be operated by the (INRiM), NIST's counterpart in Turin, Italy. As of February 2016 the IT-CsF2 caesium fountain clock started reporting a \"u\" of in the BIPM reports of evaluation of primary frequency standards.\n\nMost research focuses on the often conflicting goals of making the clocks smaller, cheaper, more portable, more energy efficient, more accurate, more stable and more reliable.\nThe Atomic Clock Ensemble in Space is an example of clock research.\n\nA list of frequencies recommended for secondary representations of the second is maintained by the International Bureau of Weights and Measures (BIPM) since 2006 and is available online. The list contains the frequency values and the respective standard uncertainties for the rubidium microwave transition and for several optical transitions. These secondary frequency standards are accurate at the level of parts in ; however, the uncertainties provided in the list are in the range of parts in – since they are limited by the linking to the caesium primary standard that currently (2015) defines the second.\n\nFor context, a femtosecond () is to a second what a second is to about 31.71 million () years and an attosecond () is to a second what a second is to about 31.71 billion () years.\n\n21st century experimental atomic clocks that provide non-caesium-based secondary representations of the second are becoming so precise that they are likely to be used as extremely sensitive detectors for other things besides measuring frequency and time. For example, the frequency of atomic clocks is altered slightly by gravity, magnetic fields, electrical fields, force, motion, temperature and other phenomena. The experimental clocks tend to continue improving, and leadership in performance has been shifted back and forth between various types of experimental clocks.\n\nIn March 2008, physicists at NIST described a quantum logic clock based on individual ions of beryllium and aluminium. This clock was compared to NIST's mercury ion clock. These were the most accurate clocks that had been constructed, with neither clock gaining nor losing time at a rate that would exceed a second in over a billion years. In February 2010, NIST physicists described a second, enhanced version of the quantum logic clock based on individual ions of magnesium and aluminium. Considered the world's most precise clock in 2010 with a fractional frequency inaccuracy of , it offers more than twice the precision of the original.\nThe accuracy of experimental quantum clocks has since been superseded by experimental optical lattice clocks based on strontium-87 and ytterbium-171.\n\nThe theoretical move from microwaves as the atomic \"escapement\" for clocks to light in the optical range (harder to measure but offering better performance) earned John L. Hall and Theodor W. Hänsch the Nobel Prize in Physics in 2005. One of 2012's Physics Nobelists, David J. Wineland, is a pioneer in exploiting the properties of a single ion held in a trap to develop clocks of the highest stability.\n\nNew technologies, such as femtosecond frequency combs, optical lattices, and quantum information, have enabled prototypes of next-generation atomic clocks. These clocks are based on optical rather than microwave transitions. A major obstacle to developing an optical clock is the difficulty of directly measuring optical frequencies. This problem has been solved with the development of self-referenced mode-locked lasers, commonly referred to as femtosecond frequency combs. Before the demonstration of the frequency comb in 2000, terahertz techniques were needed to bridge the gap between radio and optical frequencies, and the systems for doing so were cumbersome and complicated. With the refinement of the frequency comb, these measurements have become much more accessible and numerous optical clock systems are now being developed around the world.\n\nAs in the radio range, absorption spectroscopy is used to stabilize an oscillator—in this case a laser. When the optical frequency is divided down into a countable radio frequency using a femtosecond comb, the bandwidth of the phase noise is also divided by that factor. Although the bandwidth of laser phase noise is generally greater than stable microwave sources, after division it is less.\n\nThe primary systems under consideration for use in optical frequency standards are:\n\nThese techniques allow the atoms or ions to be highly isolated from external perturbations, thus producing an extremely stable frequency reference.\n\nAtomic systems under consideration include Al, Hg, Hg, Sr, Sr, In, Mg, Ca, Ca, Yb, Yb and Th.\n\nThe rare-earth element ytterbium (Yb) is valued not so much for its mechanical properties but for its complement of internal energy levels. \"A particular transition in Yb atoms, at a wavelength of 578 nm, currently provides one of the world's most accurate optical atomic frequency standards,\" said Marianna Safronova. The estimated amount of uncertainty achieved corresponds to a Yb clock uncertainty of about one second over the lifetime of the universe so far, 15 billion years, according to scientists at the Joint Quantum Institute (JQI) and the University of Delaware in December 2012.\n\nIn 2013 optical lattice clocks (OLCs) were shown to be as good as or better than caesium fountain clocks. Two optical lattice clocks containing about of strontium-87 were able to stay in synchrony with each other at a precision of at least , which is as accurate as the experiment could measure. These clocks have been shown to keep pace with all three of the caesium fountain clocks at the Paris Observatory. There are two reasons for the possibly better precision. Firstly, the frequency is measured using light, which has a much higher frequency than microwaves, and secondly, by using many atoms, any errors are averaged.\nUsing ytterbium-171 atoms, a new record for stability with a precision of over a 7-hour period was published on 22 August 2013. At this stability, the two optical lattice clocks working independently from each other used by the NIST research team would differ less than a second over the age of the universe (); this was better than previous experiments. The clocks rely on atoms cooled to and trapped in an optical lattice. A laser at excites the atoms between two of their energy levels. Having established the stability of the clocks, the researchers are studying external influences and evaluating the remaining systematic uncertainties, in the hope that they can bring the clock's accuracy down to the level of its stability. An improved optical lattice clock was described in a 2014 Nature paper.\nIn 2015 JILA evaluated the absolute frequency uncertainty of an strontium-87 optical lattice clock at , which corresponds to a measurable gravitational time dilation for an elevation change of on planet Earth that according to JILA/NIST Fellow Jun Ye is \"getting really close to being useful for relativistic geodesy\".\nAt this frequency uncertainty, this JILA optical lattice optical clock is expected to neither gain nor lose a second in more than 15 billion () years.\n\nIn 2017 JILA reported an experimental 3D quantum gas strontium optical lattice clock in which strontium-87 atoms are packed into a tiny three-dimensional (3-D) cube at 1,000 times the density of previous one-dimensional (1-D) clocks, like the 2015 JILA clock. A synchronous clock comparison between two regions of the 3D lattice yielded a record level of synchronization of in 1 hour of averaging time.\nThe 3D quantum gas strontium optical lattice clock’s centerpiece is an unusual state of matter called a degenerate Fermi gas (a quantum gas for Fermi particles). The experimental data shows the 3D quantum gas clock achieved a precision of in about two hours. According to Jun Ye \"This represents a significant improvement over any previous demonstrations.\" Ye further commented \"The most important potential of the 3D quantum gas clock is the ability to scale up the atom numbers, which will lead to a huge gain in stability.\" and \"The ability to scale up both the atom number and coherence time will make this new-generation clock qualitatively different from the previous generation.\"\nIn 2018 JILA reported the 3D quantum gas clock reached a frequency precision of over 6 hours.\nAt this frequency uncertainty, this 3D quantum gas clock would lose or gain about 0.1 seconds over the age of the universe.\n\nOptical clocks are currently (2018) still primarily research projects, less mature than rubidium and caesium microwave standards, which regularly deliver time to the International Bureau of Weights and Measures (BIPM) for establishing International Atomic Time (TAI).\nAs the optical experimental clocks move beyond their microwave counterparts in terms of accuracy and stability performance this puts them in a position to replace the current standard for time, the caesium fountain clock.\nIn the future this might lead to redefine the caesium microwave based SI second and other new dissemination techniques at the highest level of accuracy to transfer clock signals will be required that can be used in both shorter-range and longer-range (frequency) comparisons between better clocks and to explore their fundamental limitations without significantly compromising their performance.\nIn June 2015, the European National Physical Laboratory (NPL) in Teddington, UK; the French department of Time-Space Reference Systems at the Paris Observatory (LNE-SYRTE); the German German National Metrology Institute (PTB) in Braunschweig; and Italy’s labs have started tests to improve the accuracy of current state-of-the-art satellite comparisons by a factor 10, but it will still be limited to one part in . These 4 European labs are developing and host a variety of experimental optical clocks that harness different elements in different experimental set-ups and want to compare their optical clocks against each other and check whether they agree. In a next phase these labs strive to transmit comparison signals in the visible spectrum through fibre-optic cables. This will allow their experimental optical clocks to be compared with an accuracy similar to the expected accuracies of the optical clocks themselves. Some of these labs have already established fibre-optic links, and tests have begun on sections between Paris and Teddington, and Paris and Braunschweig. Fibre-optic links between experimental optical clocks also exist between the American NIST lab and its partner lab JILA, both in Boulder, Colorado but these span much shorter distances than the European network and are between just two labs. According to Fritz Riehle, a physicist at PTB, \"Europe is in a unique position as it has a high density of the best clocks in the world\".\nIn August 2016 the French LNE-SYRTE in Paris and German PTB in Braunschweig reported the comparison and agreement of two fully independent experimental strontium lattice optical clocks in Paris and Braunschweig at an uncertainty of via a newly established phase-coherent frequency link connecting Paris and Braunschweig, using of telecom fibre-optic cable. The fractional uncertainty of the whole link was assessed to be , making comparisons of even more accurate clocks possible.\n\nThe development of atomic clocks has led to many scientific and technological advances such as a system of precise global and regional navigation satellite systems, and applications in the Internet, which depend critically on frequency and time standards. Atomic clocks are installed at sites of time signal radio transmitters. They are used at some long wave and medium wave broadcasting stations to deliver a very precise carrier frequency. Atomic clocks are used in many scientific disciplines, such as for long-baseline interferometry in radioastronomy.\n\nThe Global Positioning System (GPS) operated by the US Air Force Space Command provides very accurate timing and frequency signals. A GPS receiver works by measuring the relative time delay of signals from a minimum of four, but usually more, GPS satellites, each of which has at least two onboard caesium and as many as two rubidium atomic clocks. The relative times are mathematically transformed into three absolute spatial coordinates and one absolute time coordinate.\nGPS Time (GPST) is a continuous time scale and theoretically accurate to about 14 ns. However, most receivers lose accuracy in the interpretation of the signals and are only accurate to 100 ns.\nThe GPST is related to but differs from TAI (International Atomic Time) and UTC (Coordinated Universal Time). GPST remains at a constant offset with TAI (TAI – GPST = 19 seconds) and like TAI does not implement leap seconds. Periodic corrections are performed to the on-board clocks in the satellites to keep them synchronized with ground clocks. The GPS navigation message includes the difference between GPST and UTC. As of July 2015, GPST is 17 seconds ahead of UTC because of the leap second added to UTC on 30 June 2015. Receivers subtract this offset from GPS Time to calculate UTC and specific timezone values.\n\nThe GLObal NAvigation Satellite System (GLONASS) operated by the Russian Aerospace Defence Forces provides an alternative to the Global Positioning System (GPS) system and is the second navigational system in operation with global coverage and of comparable precision. GLONASS Time (GLONASST) is generated by the GLONASS Central Synchroniser and is typically better than 1,000 ns. Unlike GPS, the GLONASS time scale implements leap seconds, like UTC.\n\nThe Galileo Global Navigation Satellite System is operated by the European GNSS Agency and European Space Agency. Galileo started offering global Early Operational Capability (EOC) on 15 December 2016, providing the third and first non-military operated Global Navigation Satellite System, and is expected to reach Full Operational Capability (FOC) in 2019. To achieve Galileo's FOC coverage constellation goal 6 planned extra satellites need to be added. Galileo System Time (GST) is a continuous time scale which is generated on the ground at the Galileo Control Centre in Fucino, Italy, by the Precise Timing Facility, based on averages of different atomic clocks and maintained by the Galileo Central Segment and synchronised with TAI with a nominal offset below 50 ns. According to the European GNSS Agency Galileo offers 30 ns timing accuracy.\nThe March 2018 Quarterly Performance Report by the European GNSS Service Centre reported the UTC Time Dissemination Service Accuracy was ≤ 7.6 ns, computed by accumulating samples over the previous 12 months and exceeding the ≤ 30 ns target. Each Galileo satellite has two passive hydrogen maser and two rubidium atomic clocks for onboard timing. The Galileo navigation message includes the differences between GST, UTC and GPST (to promote interoperability).\n\nThe BeiDou-2 satellite navigation systems is under construction in 2017 but has to add planned extra satellites to achieve its full-scale global coverage constellation goal. BeiDou Time (BDT) is a continuous time scale starting at 1 January 2006 at 0:00:00 UTC and is synchronised with UTC within 100 ns. BeiDou became operational in China in December 2011, with 10 satellites in use, and began offering services to customers in the Asia-Pacific region in December 2012. The BeiDou global navigation system should be finished by 2020.\n\nA radio clock is a clock that automatically synchronizes itself by means of government radio time signals received by a radio receiver. Many retailers market radio clocks inaccurately as atomic clocks; although the radio signals they receive originate from atomic clocks, they are not atomic clocks themselves. \nNormal low cost consumer grade receivers solely rely on the amplitude-modulated time signals and use narrow band receivers (with 10 Hz bandwidth) with small ferrite loopstick antennas and circuits with non optimal digital signal processing delay and can therefore only be expected to determine the beginning of a second with a practical accuracy uncertainty of ± 0.1 second. This is sufficient for radio controlled low cost consumer grade clocks and watches using standard-quality quartz clocks for timekeeping between daily synchronization attempts, as they will be most accurate immediately after a successful synchronization and will become less accurate from that point forward until the next synchronization.\nInstrument grade time receivers provide higher accuracy. Such devices incur a transit delay of approximately 1 ms for every 300 kilometres (186 mi) of distance from the radio transmitter. Many governments operate transmitters for time-keeping purposes.\n\n \n"}
{"id": "6604694", "url": "https://en.wikipedia.org/wiki?curid=6604694", "title": "Bike boom", "text": "Bike boom\n\nThe term \"bike boom\" or \"bicycle craze\" refers to any of several specific historic periods marked by increased bicycle enthusiasm, popularity, and sales.\n\nProminent examples include 1819 and 1868, as well as the decades of the 1890s and 1970s — the latter especially in North America and 2010s in the United Kingdom.\n\nThe first period which may be called a bicycle craze actually refers to a precursor of the bicycle which was pushed along by the feet on the ground as in walking, and did not have pedals. This machine was invented by Baron Karl von Drais in Germany, and was called variously a \"draisine\" (English) or \"draisienne\" (French) after his name, a \"velocipede\" from the Latin terms for \"fast foot\", a \"hobby horse\", or a \"dandy horse\", the last name being perhaps the most popular. Drais got a patent for his invention in 1818, and the craze swept Europe and the United States during the summer of 1819 while many manufacturers (notably Denis Johnson of London) either copied Drais's machine or created their own versions, then quickly died out as many pedestrians began to feel threatened by the machines and municipalities enacted laws prohibiting their use.\n\nDuring the next 43 years, chiefly in England, inventors continued to explore the concept of human-powered transport, but on vehicles with three or four wheels (called \"tricycles\" and \"quadracycles\" respectively), which were thought to be more stable, not requiring the balance that is necessary for two-wheeled vehicles. But none of these achieved much popularity.\n\nIn the early 1860s the first true bicycle was created in Paris, France, by attaching rotary cranks and pedals to the front wheel hub of a dandy-horse. The Olivier brothers recognized the commercial potential of this invention, and set up a partnership with blacksmith and bicycle maker Pierre Michaux, using Michaux's name, already famous among enthusiasts of the new sport, for the company. They began the first mass-production of bicycles (still called \"velocipedes\") in 1868, as the first real bicycle craze had begun the year before, reaching full force all over Europe and America in 1868 and 1869. But exactly as with the dandy-horse, pedestrians complained about them, and the craze again faded quickly. Another factor in their demise was the extremely uncomfortable ride, because of the stiff wrought-iron frame and wooden wheels surrounded by tires made of iron — this led to the pejorative name \"boneshaker\", which is still used today to refer to this type of bicycle.\n\nAgain, England was the only place where the concept remained popular during the early 1870s. But the design changed drastically, with the front wheel becoming larger and larger, and with many other improvements making the ride more comfortable. This type of bicycle was known in its day as the \"ordinary\", but people later began calling it a \"penny-farthing\" because of the resemblance of its wheel sizes to the largest and smallest English copper coins of the time; it is also known as a \"high-wheel\". Front-wheel sizes quickly grew to as much as 5 feet (~1.5 meters), and the bicycles were considered by the general public to be quite dangerous. In addition, they were expensive, and thus riders were mostly wealthy young men who formed an elite brotherhood. However, bicycle races were staged and well-attended by the public, which spread interest for the high-wheeler worldwide because of the far-flung British colonies, by the end of the decade. Albert Pope purchased Lallement's original patent and created his \"Columbia\" bicycle in the U.S. in 1878, and went on to manufacture thousands of bicycles.\n\nThe 1890s saw one of the biggest bicycle crazes of all, driven by several significant developments in bicycles: the invention of the \"safety bicycle\" with its chain-drive transmission, whose gear ratios allowed smaller wheels without a concurrent loss of speed, and the subsequent invention of the pneumatic (inflatable air-filled) bicycle tire. Experiments with chain-drive had been attempted in 1869 and 1879, but the first well known chain-drive bicycle was the \"Rover\" produced in 1885 by John Kemp Starley. Very quickly, the penny-farthing passed out of fashion, and multitudes of people all over the world began riding the \"safety\".\n\nSeptember 13, 1892 saw the opening of a Bicycle Railroad between Mount Holly, New Jersey and the H. B. Smith Manufacturing Company in Smithville, NJ during the Mount Holly fair, with 3,000 riders its first week (for amusement instead of commuting).\n\nConey Island wanted one, and the World's Columbian Exposition in Chicago featured one. Several others were built for amusement in Atlantic City, Ocean City and Gloucester City, NJ (the first two in 1893 and last in 1894).\n\nIn the year 1896, there was simultaneously an increase in bicycle popularity and a severe economic depression. Bicycles were one of the few areas of the economy where sales were growing; people were buying bicycles \"whether they could afford them or not\". This attracted hundreds of manufacturers into the bicycle business. This increase in production resulted in a downward spiral of market saturation, over-supply and intense price competition. Many bicycle manufacturers, faced with excess inventory and prices too low to make a profit, went out of business. Several dozen bicycle companies consolidated into the American Bicycle Company in 1899.\n\nThe application of the internal-combustion engine to the bicycle during the 1890s resulted in the motorcycle, and then soon after, the engine was applied to 4-wheel carriages resulting in the motor car or \"automobile\" which in later decades largely supplanted its unmotorized ancestor.\n\nUS bike boom of 1965–1975: The period of 1965–1975 saw adult cycling increase sharply in popularity — with \"Time\" magazine calling it \"the bicycle's biggest wave of popularity in its 154-year history\" The period was followed by a sudden fall in sales, resulting in a large inventory of unsold bicycles. Seven million bicycles were sold in the U.S. in 1970. Of those, 5½ million were children's bikes, 1.2 million were coaster brake, balloon-tired adult bicycles, and only 200,000 were lightweight 3-speed or derailleur-equipped bikes. Total bicycle sales had doubled by 1972 to 14 million — with children's bikes remaining constant at 5½ million, adult balloon-tired bicycles falling to about 1/2 million, and lightweight bicycles exploding fortyfold, to 8 million. \"Time\" magazine reported in 1971 that \"for the first time since the 1890s, nearly one-half of all bicycle production\" was \"geared for adults.\"\n\nThe boom received a kick start in the mid 60s with the advent of the Schwinn Sting-Ray and other wheelie bikes. Sales reached 4 million units per year for the first time. At the height of the boom, in 1972, 1973, and 1974, more bicycles than automobiles were sold in the U.S.\n\nAdditional factors contributing to the U.S. bike boom included affordable and versatile 10-speed derailleur-geared racing bicycles becoming widely available, the arrival of many post-World War II baby boomers at adulthood and demanding inexpensive transportation for recreation and exercise, and increasing interest in reducing pollution. The 1973 oil crisis, which increased the cost of driving an automobile, making bicycle commuting a more attractive option, is commonly assumed to have propelled the bike boom, but in fact, bicycle sales had already peaked when the crisis struck in October, 1973.\n\nThe United Kingdom experienced a mountain bike boom during the 1990s. Road bike customers were seen as reluctant to spend money, while the mountain bike market offered new features such as suspensions and materials such as carbon fibre, aluminum and titanium. The market began to implode by the time of the 2001 United Kingdom foot-and-mouth outbreak. Since then it has barely recovered to the level of its prime years.\n\nAs of 2008 some industry analysts see signs of surging bicycle popularity.\n\nThe British press cite 2012 as a bike boom, fuelled by the successes that summer of UK cyclists in the Tour de France and at the London Olympics, Bradley Wiggins and Laura Trott in particular, and the subsequent success in the Tour by Chris Froome.\n\nIn 2016, Simon Mottram of the cycling clothing brand Rapha traced the boom back to the after-effects of the 7 July 2005 London bombings, which closed parts of the Tube and encouraged many commuters onto bicycles. Two years later, the Tour de France started in London on the anniversary of the attack, and the following year Team GB dominated cycling at the Beijing Olympics. He also credits the financial incentive of the Cycle to Work scheme (introduced in 1999), and the increasing emphasis on health and congestion.\" British Cycling, the sport's governing body, claimed that over \"two million people across the country now cycle at least once a week, an all-time high\". Halfords, responsible for a third of bikes sold in the UK, reported an increase of sales of 11% up to March 2015. Much attention has been given to the so-called mamils, middle-aged men in lycra, but many customers are younger men, and the most rapidly growing group is women. One of the older men, the architecture writer and cycle campaigner Peter Murray, described how business relationships are created and nurtured during longer rides; the BBC referred to cycling being \"the new golf\" in this sense.\"\n\nWith the development of technology, the 2010s also saw the rise of Bicycle-sharing systems around the world.\n\n\n"}
{"id": "1577354", "url": "https://en.wikipedia.org/wiki?curid=1577354", "title": "Buell dryer", "text": "Buell dryer\n\nThe Buell dryer is a multiple hearth direct heated industrial dryer (commonly known as a turbo dryer) that has modified for drying china clay. Its design is very similar to the Herreshoff Kiln, which is a type of calciner. The Buell dryer was first developed by English Clays Lovering Pochin & Co. Ltd for their china clay processing plants in Cornwall. The first Buell was installed in the old cooperage in Nanpean, and was the first operating mechanical dryer in the Cornish china clay industry, having started in 1945.\n\nThe dryer itself is composed of a large upright cylindrical chamber, inside of which are 25 to 30 layers of trays or 'hearths'. Hot air and gasses are distributed throughout the dryer by a series of fans and ducts. At the centre of the dryer is a rotating column, to which the trays are attached and positioned radially within the dryer. Material enters the top of the dryer and lands on one of the top trays. As the central column rotates, fixed arms push the material off the tray, dropping it down onto the one below it. Gradually the material works its way down through the dryer in this manner, and exits the bottom of the dryer with the assistance of fans.\n\nMaterial to be dried usually enters the top of the dryer with a moisture content of around 18%, and exits as a product of around 10 to 8%. Generally these figures all depend on the dewatering processes employed before the material reaches the dryer. Commonly, the Buell dryer handles shredded filter press cakes from standard square plate filter presses.\n\nWhile the Buell dryer is now thought to be a relatively obsolete drying technology, a very similar dryer made by Wyssmont called the TURBO-DRYER is still widely used worldwide.\n\nTURBO-DRYER is the registered trademark of the Wyssmont company, inc. in Fort Lee, NJ USA. it is usually used for a thermal processor consisting of a stack of rotating toroidal shelves with radial slots wrapped around a set of central fans on a vertical axis in a stationary housing.\n\nWhile essentially like the Buell dryer it has many improvements over that dryer so that it is not considered an outdated dryer. It frequently out-performs other dryers like vacuum dryers, fluid bed dryers, rotary dryers, flash dryers, indirect contact dryers and belt dryers.\n\nWyssmont was the first company to do closed-circuit solvent-recovery drying and many of its installation are still used for this type of application.\n\nAlthough most used with hot air drying it is frequently used for inert gas drying of metals and polymers. It is known for its gentle handling, uniform product, easy startup, low maintenance, low power and \"turn it on and forget it\" operation.\n"}
{"id": "50953608", "url": "https://en.wikipedia.org/wiki?curid=50953608", "title": "Chamber of Digital Commerce", "text": "Chamber of Digital Commerce\n\nThe Chamber of Digital Commerce is an American advocacy group that promotes the emerging industry behind blockchain technology, bitcoin, digital currency and digital assets.\n\nHeadquartered in Washington, D.C., the organization was founded in July 2014 by Perianne Boring. In October 2014, the chamber received 501(c)(6) non-profit status from the Internal Revenue Service. In 2015, economist and former JPMorgan Chase executive Blythe Masters was appointed to the advisory board.\n\nIn 2017, internet pioneer and Transmission Control Protocol (TCP) inventor Bob Kahn keynoted the DC Blockchain Summit at Georgetown University.\nBeginning in 2017, several multinational corporations and international financial services holding companies announced their intentions to join the Chamber of Digital Commerce including BNY Mellon, BNP Paribas, Accenture, IHS Markit, Cisco, and Intuit. In October 2017, the Chamber of Digital Commerce, along with the U.S. Department of Commerce lead, the first Blockchain Certified Trade Mission to the United Arab Emirates.\n\nIn August 2014, political news site \"The Hill\" reported that the Chamber had registered a political action committee with the United States Federal Election Commission. As \"The Hill\" piece noted, “formation of the PAC is a sign of increasing maturity for Bitcoin and a signal that politicians could face political pressure to support virtual currencies.”\n"}
{"id": "583901", "url": "https://en.wikipedia.org/wiki?curid=583901", "title": "Combined cycle", "text": "Combined cycle\n\nIn electric power generation a combined cycle is an assembly of heat engines that work in tandem from the same source of heat, converting it into mechanical energy, which in turn usually drives electrical generators. The principle is that after completing its cycle (in the first engine), the temperature of the working fluid in the system is still high enough that a second subsequent heat engine extracts energy from the heat that the first engine produced. By combining these multiple streams of work upon a single mechanical shaft turning an electric generator, the overall net efficiency of the system may be increased by 50–60%. (That is, from an overall efficiency of say 34% (in a single cycle) to possibly an overall efficiency of 62% in net Carnot thermodynamic efficiency.)\n\nThis can be done because heat engines are only able to use a portion of the energy their fuel generates (usually less than 50%). In an ordinary (non-combined cycle) heat engine the remaining heat (i.e., hot exhaust gas) from combustion is wasted.\n\nCombining two or more thermodynamic cycles results in improved overall efficiency, reducing fuel costs. In stationary power plants, a widely used combination is a gas turbine (operating by the Brayton cycle) burning natural gas or synthesis gas from coal, whose hot exhaust powers a steam power plant (operating by the Rankine cycle). This is called a Combined Cycle Gas Turbine (CCGT) plant, and can achieve a best-of-class real (HHV - see below) thermal efficiency of around 62% in base-load operation, in contrast to a single cycle steam power plant which is limited to efficiencies of around 35–42%. Many new gas power plants in North America and Europe are of the Combined Cycle Gas Turbine type. Such an arrangement is also used for marine propulsion, and is called a \"combined gas and steam (COGAS)\" plant. Multiple stage turbine or steam cycles are also common.\n\nOther historically successful combined cycles have used hot cycles with mercury vapour turbines, magnetohydrodynamic generators or molten carbonate fuel cells, with steam plants for the low temperature \"bottoming\" cycle. Bottoming cycles operating from a steam condenser's heat exhaust are theoretically possible, but uneconomical because of the very large, expensive equipment needed to extract energy from the small temperature differences between condensing steam and outside air or water. However, it is common in cold climates (such as Finland) to drive community heating systems from a power plant's condenser heat. Such cogeneration systems can yield theoretical efficiencies above 95%.\n\nIn automotive and aeronautical engines, turbines have been driven from the exhausts of Otto and Diesel cycles. These are called turbo-compound engines (not to be confused with turbochargers).\n\nThe thermodynamic cycle of the basic combined cycle consists of two power plant cycles. One is the Joule or Brayton cycle which is a gas turbine cycle and the other is Rankine cycle which is a steam turbine cycle. The cycle 1-2-3-4-1 which is the gas turbine power plant cycle is the topping cycle. It depicts the heat and work transfer process taking place in high temperature region.\n\nThe cycle a-b-c-d-e-f-a which is the Rankine steam cycle takes place at a low temperature and is known as the bottoming cycle. Transfer of heat energy from high temperature exhaust gas to water and steam takes place by a waste heat recovery boiler in the bottoming cycle. During the constant pressure process 4-1 the exhaust gases in the gas turbine reject heat. The feed water, wet and super heated steam absorb some of this heat in the process a-b, b-c and c-d.\n\nThe steam power plant gets its input heat from the high temperature exhaust gases from gas turbine power plant. The steam generated thus can be used to drive steam turbine. The Waste Heat Recovery Boiler (WHRB) has 3 sections: Economiser, evaporator and superheater.\n\nThe efficiency of a heat engine, the fraction of input heat energy that can be converted to useful work, is limited by the temperature difference between the heat entering the engine and the exhaust heat leaving the engine.\n\nIn a thermal power station, water is the working medium. High pressure steam requires strong, bulky components. High temperatures require expensive alloys made from nickel or cobalt, rather than inexpensive steel. These alloys limit practical steam temperatures to 655 °C while the lower temperature of a steam plant is fixed by the temperature of the cooling water. With these limits, a steam plant has a fixed upper efficiency of 35–42%.\n\nAn open circuit gas turbine cycle has a compressor, a combustor and a turbine. For gas turbines the amount of metal that must withstand the high temperatures and pressures is small, and lower quantities of expensive materials can be used. In this type of cycle, the input temperature to the turbine (the firing temperature), is relatively high (900 to 1,400 °C). The output temperature of the flue gas is also high (450 to 650 °C). This is therefore high enough to provide heat for a second cycle which uses steam as the working fluid (a Rankine cycle).\n\nIn a combined cycle power plant, the heat of the gas turbine's exhaust is used to generate steam by passing it through a heat recovery steam generator (HRSG) with a live steam temperature between 420 and 580 °C. The condenser of the Rankine cycle is usually cooled by water from a lake, river, sea or cooling towers. This temperature can be as low as 15 °C.\n\nPlant size is important in the cost of the plant. The larger plant sizes benefit from economies of scale (lower initial cost per kilowatt) and improved efficiency.\n\nFor large-scale power generation, a typical set would be a 270 MW primary gas turbine coupled to a 130 MW secondary steam turbine, giving a total output of 400 MW. A typical power station might consist of between 1 and 6 such sets.\n\nGas turbines for large-scale power generation are manufactured by at least four separate groups – General Electric, Siemens, Mitsubishi-Hitachi, and Ansaldo Energia. These groups are also developing, testing and/or marketing gas turbine sizes in excess of 300 MW (for 60 Hz applications) and 400 MW (for 50 Hz applications). Combined cycle units are made up of one or more such gas turbines, each with a waste heat steam generator arranged to supply steam to a single or multiple steam turbines, thus forming a combined cycle block or unit. Combined cycle block sizes offered by three major manufacturers (Alstom, General Electric and Siemens) can range anywhere from 50 MW to well over 1300 MW with costs approaching $670/kW.\n\nThe heat recovery boiler is item 5 in the COGAS figure shown above. Hot gas turbine exhaust enters in super heater and the evaporator and then in to economiser section as it flows out from the boiler.\nFeed water comes in through the economizer and then exits after having attained saturation temp in the water or steam circuit. Finally it then flows through evaporator and super heater. If the temperature of the gases entering the heat recovery boiler is higher, then the temperature of the exiting gases is also high.\n\nIt is often desirable if high heat is recovered from the exiting gases. Hence dual pressure boiler is employed for this purpose. It has two water/steam drums. Low pressure drum is connected to low pressure economizer or evaporator. The low pressure steam is generated in low temperature zone. The low pressure steam is supplied to the low temperature turbine. Super heater can be provided in the low pressure circuit.\n\nSome part of the feed water from the low-pressure zone is transferred to the high-pressure economizer by a booster pump. This economizer heats up the water to its saturation temperature. This saturated water goes through the high-temperature zone of the boiler and is supplied to the high-pressure turbine.\nSupplementary firing may be used in combined cycles (in the HRSG) raising exhaust temperatures from 600 °C (GT exhaust) to 800 or even 1000 °C. Using supplemental firing will however not raise the combined cycle efficiency for most combined cycles. For single boilers it may raise the efficiency if fired to 700–750 °C; for multiple boilers however, supplemental firing is often used to improve peak power production of the unit, or to enable higher steam production to compensate for failure of a second unit.\n\nMaximum supplementary firing refers to the maximum fuel that can be fired with the oxygen available in the gas turbine exhaust. The steam cycle is conventional with reheat and regeneration. Hot gas turbine exhaust is used as the combustion air. Regenerative air preheater is not required. A fresh air fan which makes it possible to operate the steam plant even when the gas turbine is not in operation, increases the availability of the unit.\n\nThe use of large supplementary firing in Combined Cycle Systems with high gas turbine inlet temperatures causes the efficiency to drop. For this reason the Combined Cycle Plants with maximum supplementary firing are only of minimal importance today, in comparison to simple Combined Cycle installations. However, they have two advantages that is a) coal can be burned in the steam generator as the supplementary fuel, b) has very good part load efficiency.\n\nThe HRSG can be designed with supplementary firing of fuel after the gas turbine in order to increase the quantity or temperature of the steam generated. Without supplementary firing, the efficiency of the combined cycle power plant is higher, but supplementary firing lets the plant respond to fluctuations of electrical load. Supplementary burners are also called \"duct burners\".\n\nMore fuel is sometimes added to the turbine's exhaust. This is possible because the turbine exhaust gas (flue gas) still contains some oxygen. Temperature limits at the gas turbine inlet force the turbine to use excess air, above the optimal stoichiometric ratio to burn the fuel. Often in gas turbine designs part of the compressed air flow bypasses the burner and is used to cool the turbine blades.\n\nSupplementary firing raises the temperature of the exhaust gas from 800 to 900 degree Celsius. Relatively high flue gas temperature raises the condition of steam (84 bar, 525 degree Celsius) thereby improving the efficiency of steam cycle.\n\nCombined cycle plants are usually powered by natural gas, although fuel oil, synthesis gas or other fuels can be used. The supplementary fuel may be natural gas, fuel oil, or coal. Biofuels can also be used. Integrated solar combined cycle power stations combine the energy harvested from solar radiation with another fuel to cut fuel costs and environmental impact (look ISCC section). Next generation nuclear power plants are also on the drawing board which will take advantage of the higher temperature range made available by the Brayton top cycle, as well as the increase in thermal efficiency offered by a Rankine bottoming cycle.\n\nThe improvement in shale gas extraction has increased natural gas supplies and reserves dramatically. Because of this fact, it is becoming the fuel of choice for an increasing amount of private investors and consumers because it is more versatile than coal or oil and can be used in 90% of energy applications. Chile which once depended on hydro-power for 70% of its electricity supply, is now boosting its gas supplies to reduce reliance on its drought afflicted hydro dams. Similarly China is tapping its gas reserves to reduce reliance on coal, which is currently burned to generate 80% of the country's electricity supply.\n\nWhere the extension of a gas pipeline is impractical or cannot be economically justified, electricity needs in remote areas can be met with small-scale Combined Cycle Plants, using renewable fuels.\nInstead of natural gas, Combined Cycle Plants can be filled with biogas derived from agricultural and forestry waste, which is often readily available in rural areas.\n\nGas turbines burn mainly natural gas and light oil. Crude oil, residual, and some distillates contain corrosive components and as such require fuel treatment equipment. In addition, ash deposits from these fuels result in gas turbine deratings of up to 15%. They may still be economically attractive fuels however, particularly in combined-cycle plants.\n\nSodium and potassium are removed from residual, crude and heavy distillates by a water washing procedure. A simpler and less expensive purification system will do the same job for light crude and light distillates. A magnesium additive system may also be needed to reduce the corrosive effects if vanadium is present. Fuels requiring such treatment must have a separate fuel-treatment plant and a system of accurate fuel monitoring to assure reliable, low-maintenance operation of gas turbines.\n\nA single shaft combined cycle plant comprises a gas turbine and a steam turbine driving a common generator. In a multi-shaft combined cycle plant, each gas turbine and each steam turbine has its own generator. The single shaft design provides slightly less initial cost and slightly better efficiency than if the gas and steam turbines had their own generators. The multi-shaft design enables two or more gas turbines to operate in conjunction with a single steam turbine, which can be more economical than a number of single shaft units.\n\nThe primary disadvantage of single shaft combined cycle power plants is that the number of steam turbines, condensers and condensate systems – and perhaps the number of cooling towers and circulating water systems – increases to match the number of gas turbines. For a multi-shaft combined cycle power plant there is only one steam turbine, condenser and the rest of the heat sink for up to three gas turbines; only their size increases. Having only one large steam turbine and heat sink results in low cost because of economies of scale. A larger steam turbine also allows the use of higher pressures and results in a more efficient steam cycle. Thus the overall plant size and the associated number of gas turbines required have a major impact on whether a single shaft combined cycle power plant or a multiple shaft combined cycle power plant is more economical.\n\nThe combined-cycle system includes single-shaft and multi-shaft configurations. The single-shaft system consists of one gas turbine, one steam turbine, one generator and one Heat Recovery Steam Generator (HRSG), with the gas turbine and steam turbine coupled to the single generator in a tandem arrangement on a single shaft. Key advantages of the single-shaft arrangement are operating simplicity, smaller footprint, and lower startup cost. Single-shaft arrangements, however, will tend to have less flexibility and equivalent reliability than multi-shaft blocks. Additional operational flexibility is provided with a steam turbine which can be disconnected, using a synchro-self-shifting (SSS) Clutch, for start up or for simple cycle operation of the gas turbine.\n\nMulti-shaft systems have one or more gas turbine-generators and HRSGs that supply steam through a common header to a separate single steam turbine-generator. In terms of overall investment a multi-shaft system is about 5% higher in costs.\n\nSingle- and multiple-pressure non-reheat steam cycles are applied to combined-cycle systems equipped with gas turbines having rating point exhaust gas temperatures of approximately 540 °C or less. Selection of a single- or multiple-pressure steam cycle for a specific application is determined by economic evaluation which considers plant installed cost, fuel cost and quality, plant duty cycle, and operating and maintenance cost.\n\nMultiple-pressure reheat steam cycles are applied to combined-cycle systems with gas turbines having rating point exhaust gas temperatures of approximately 600 °C.\n\nThe most efficient power generation cycles are those with unfired HRSGs with modular pre-engineered components. These unfired steam cycles are also the lowest in cost. Supplementary-fired combined-cycle systems are provided for specific application.\n\nThe primary regions of interest for cogeneration combined-cycle systems are those with unfired and supplementary fired steam cycles. These systems provide a wide range of thermal energy to electric power ratio and represent the range of thermal energy capability and power generation covered by the product line for thermal energy and power systems.\n\nBy combining both gas and steam cycles, high input temperatures and low output temperatures can be achieved. \nThe efficiency of the cycles add, because they are powered by the same fuel source. \nSo, a combined cycle plant has a thermodynamic cycle that operates between the gas-turbine's high firing temperature and the waste heat temperature from the condensers of the steam cycle. \nThis large range means that the Carnot efficiency of the cycle is high. \nThe actual efficiency, while lower than the Carnot efficiency, is still higher than that of either plant on its own.\n\nThe electric efficiency of a combined cycle power station, if calculated as electric energy produced as a percentage of the lower heating value of the fuel consumed, can be over 60% when operating new, i.e. unaged, and at continuous output which are ideal conditions. \nAs with single cycle thermal units, combined cycle units may also deliver low temperature heat energy for industrial processes, district heating and other uses. This is called cogeneration and such power plants are often referred to as a combined heat and power (CHP) plant.\n\nIn general, combined cycle efficiencies in service are over 50% on a lower heating value and Gross Output basis. \nMost combined cycle units, especially the larger units, have peak, steady-state efficiencies on the LHV basis of 55 to 59%. \nResearch aimed at turbine inlet temperature has led to even more efficient combined cycles and nearly 60% LHV efficiency (54% HHV efficiency) has been reached in the combined cycle unit of Baglan Bay, a GE H-technology gas turbine with a NEM 3 pressure reheat boiler, using steam from the HRSG to cool the turbine blades. \n\nIn May 2011 Siemens AG announced they had achieved a 60.75% net efficiency with a 578 megawatts SGT5-8000H gas turbine at the Irsching Power Station. The General Electric 9HA was billed to attain 41.5% simple cycle efficiency and 61.4% in combined cycle mode, with a gas turbine output of 397 MW to 470 MW and a combined output of 592 MW to 701 MW. \nIts firing temperature is between , its overall pressure ratio is 21.8 to 1 and is used by Électricité de France in Bouchain. On April 28, 2016, this plant was certified by Guinness World Records as the worlds most efficient combined cycle power plant at 62.22%. \nThe Chubu Electric’s Nishi-ku, Nagoya power plant 405 MW 7HA is expected to have 62% gross combined cycle efficiency.\nFor 2018, GE offers its 826 MW HA at over 64% efficiency in combined cycle due to advances in additive manufacturing and combustion breakthroughs, up from 63.7% in 2017 orders and on track to achieve 65% by the early 2020s.\n\nAs of January 2017, Mitsubishi claims a LHV efficiency of greater than 63% for some members of its J Series turbines.\n\nTo avoid confusion, the efficiency of heat engines and power stations should be stated relative to the Higher Heating Value (HHV) or Lower Heating Value (LHV) of the fuel, to include or exclude the heat that can be obtained from condensing the flue gas. It should also be specified whether Gross output at the generator terminals or Net Output at the power station fence is being considered.\n\nThe LHV figure is NOT a computation of electricity net energy compared to energy content of fuel input; it is 11% higher than that. The HHV figure is a computation of electricity net energy compared to energy content of fuel input. If the LHV approach were used for some new condensing boilers, the efficiency would calculate to be over 100%. Manufacturers prefer to cite the higher LHV efficiency, e.g. 60%, for a new CCGT, but utilities, when calculated how much electricity the plant will generate, divide this by 1.11 to get the real, e.g. 54%, HHV efficiency of that CCGT. Coal plant efficiencies are computed on a HHV basis (it doesn't make nearly as much difference for coal burn, as for gas).\n\nThe difference between HHV and LHV for gas, can be estimated (using USA units) by 1055Btu/Lb * w, where w is the lbs of water after combustion per lb of fuel. To convert the HHV of natural gas, which is 23875 Btu/lb, to an LHV (methane is 25% hydrogen) would be: 23875 – (1055*0.25*18/2) = 21500. Because the efficiency is determined by dividing the energy output by the input, and the input on an LHV basis is smaller than the HHV basis, the overall efficiency on an LHV basis is higher. So, using the ratio: 23875/21500 = 1.11 you can convert the HHV to an LHV.\n\nSo a real best-of-class baseload CCGT efficiency of 54%, as experienced by the utility operating the plant, translates to 60% LHV as the manufacturer's published headline CCGT efficiency.\n\nThe efficiency of CCGT and GT can be boosted by pre-cooling combustion air. This is practised in hot climates and also has the effect of increasing power output. This is achieved by evaporative cooling of water using a moist matrix placed in front of the turbine, or by using Ice storage air conditioning. The latter has the advantage of greater improvements due to the lower temperatures available. Furthermore, ice storage can be used as a means of load control or load shifting since ice can be made during periods of low power demand and, potentially in the future the anticipated high availability of other resources such as renewables during certain periods.\n\nA natural gas integrated power and syngas (hydrogen) generation cycle uses semi-closed (sometimes called closed) gas turbine cycles where fuel is combusted with pure oxygen, and the working fluid of the cycle is a mix of combustion products CO (carbon dioxide) and HO (steam).\n\nThe integrated cycle implies that, before combustion, methane (primer natural gas component) is mixed with working fluid and converted into syngas (mix of H and CO (carbon monoxide)) in a catalytic adiabatic (without an indirect heat supply) reactor by using sensible heat of the hot working fluid leaving, in the simplest case, the gas turbine outlet. The largest part of produced syngas (about 75%) is directed into the combustion chamber of the gas-turbine cycle to generate power, but another part of syngas (about 25%) is withdrawn from the power generation cycle as hydrogen, carbon monoxide, or their blend to produce chemicals, fertilizers, synthetic fuels, and etc. The thermodynamic benefit owing to this modification is substantiated by exergy analysis. There are numerous technological options to separate syngas from working fluid and withdraw it from the cycle (e.g., condensing vapors and removing liquids, taking out gases and vapors by membrane and pressure swing adsorption separation, amine gas treating, and glycol dehydration).\n\nAll the environmental advantages of semi-closed gas turbine cycles as to an absence of NO and the release of non-diluted CO in the flue gas stay the same. An effect of integration becomes apparent with the following clarification. Assigning the efficiency of syngas production in the integrated cycle a value equal to a regular syngas production efficiency through steam-methane reforming (some part of methane is combusted to drive endothermic reforming), the net-power generation efficiency (with accounting for the consumed electricity required to separate air) can reach levels higher than 60% at a maximum temperature in the cycle (at the gas turbine inlet) of about 1300C.\n\nThe natural gas integrated cycle with adiabatic catalytic reactor was firstly proposed at Chemistry Department of Moscow State Lomonosov University (Russia) in Prof. M. Safonov (late) group by M. Safonov,M. Granovskii, and S. Pozharskii in 1993.\n\nAn integrated gasification combined cycle, or IGCC, is a power plant using synthesis gas (syngas). Syngas can be produced from a number of sources, including coal and biomass. The system uses gas and steam turbines, the steam turbine operating off of the heat left over from the gas turbine. This process can raise electricity generation efficiency to around 50%.\n\nAn Integrated Solar Combined Cycle (ISCC) is a hybrid technology in which a solar thermal field is integrated within a combined cycle plant. In ISCC plants, solar energy is used as an auxiliary heat supply, supporting the steam cycle, which results in increased generation capacity or a reduction of fossil fuel use.\n\nThermodynamic benefits are that daily steam turbine startup losses are eliminated.\n\nMajor factors limiting the load output of a combined cycle power plant are the allowed pressure and temperature transients of the steam turbine and the heat recovery steam generator waiting times to establish required steam chemistry conditions and warm-up times for the balance of plant and the main piping system. Those limitations also influence the fast start-up capability of the gas turbine by requiring waiting times. And waiting gas turbines consume gas. The solar component, if the plant is started after sunshine, or before, if we have heat storage, allows us to preheat the steam to the required conditions. That is, the plant is started faster and we consume less gas before achieving operating conditions. Economic benefits are that the solar components costs are 25% to 75% those of a Solar Energy Generating Systems plant of the same collector surface.\n\nThe first such system to come online was the Archimede combined cycle power plant, Italy in 2010, followed by Martin Next Generation Solar Energy Center in Florida, and in 2011 by the Kuraymat ISCC Power Plant in Egypt, Yazd power plant in Iran, Hassi R'mel in Algeria, Ain Beni Mathar in Morocco.\n\nCombined cycles have traditionally only been used in large power plants. BMW, however, has proposed that automobiles use exhaust heat to drive steam turbines. This can even be connected to the car or truck's cooling system to save space and weight, but also to provide a condenser in the same location as the radiator and preheating of the water using heat from the engine block.\n\nIt may be possible to use the pistons in a reciprocating engine for both combustion and steam expansion as in the Crower engine.\n\nA turbocharged car is also a combined cycle. Bowman of Southampton offer a commercially proven add-on turbocharger which additionally can generate electric power lowering overall fuel consumption by about 8%.\n\nSome versions of the Wright R-3350 were produced as turbo-compound engines. Three turbines driven by exhaust gases, known as \"power recovery turbines\" (unofficially \"parts recovery turbines\" because they ate exhaust valves on a regular basis) provided nearly 600 hp at takeoff. These turbines added power to the engine crankshaft through bevel gears and fluid couplings.\n\nThere have been many successful turbo-compound engine designs particularly for aircraft but their mechanical complexity and weight are less economical than multistage turbine engines. Stirling engines are also a good theoretical fit for this application.\n\n\n"}
{"id": "15515870", "url": "https://en.wikipedia.org/wiki?curid=15515870", "title": "Contamination control", "text": "Contamination control\n\nContamination control is the generic term for all activities aiming to control the existence, growth and proliferation of contamination in certain areas. Contamination control may refer to the atmosphere as well as to surfaces, to particulate matter as well as to microbes and to contamination prevention as well as to decontamination.\n\nThe aim of all contamination control activities is to permanently ensure a sufficient level of cleanliness in controlled environments. This is accomplished by maintaining, reducing, or eradicating viable and non-viable contamination for either sanitary purposes or in order to maintain an efficient rate of production.\n\nOne of the most common environments that incorporates contamination control into its standards protocol is the cleanroom. There are many preventive procedures in place within a cleanroom environment. They include subjecting cleanroom staff to strict clothing regulations, and there is often a gowning room where the staff can change clothes under sterile conditions so as to prevent any particulates from entering from the outside environment. Certain areas in the cleanroom have more stringent measures than others: packaging areas, corridors, gowning rooms and transfer hatches incorporate strict contamination control measures in order to maintain cleanroom standards.\n\nContamination control is also an important asset for industrial laboratories in the pharmaceutical and life science sectors. Other places of use include automotive paint shops, entrances to industrial kitchens and food service providers, many manufacturing areas, and electronic component assembly areas.\n\nMore recently, effective contamination control has been a concern for laboratories and other sensitive environments as a bio-security crisis management measure. Some banks and insurance companies use contamination control products as part of their disaster management protocols. Preventive measures are devised as preparation for combating potential pandemics or the proliferation of biohazards in any potential terrorist attack.\n\nBeside particulate matter such as ions and molecules, the most common types of contamination are:\n\nMany types organisms are potentially detrimental to processes in a critical environment. Seven of the most common contaminants are:\n\nThese and many other damaging contaminants can infiltrate critical areas in a number of ways. Particulates can enter by air, or on the feet of any carrier moving between the external environment and inside the critical area, for example.\n\nContamination poses a significant risk to technical processes, experiments or production activities, as well as to the individuals involved. Unguarded proliferation of contamination can quickly lead to product damage, yield reduction, product recalls and other outcomes highly detrimental to business. Products in a range of industries are recalled due to ineffective contamination control systems.\n\nBased on this evidence it could be argued that many businesses are not adequately protecting themselves from the harmful effects of contamination, and many products in many industries are being recalled due to unsafe manufacturing processes.\n\nBody movement causes contamination, and protective clothing such as hats, cleanroom suits and face masks are accordingly basic items of contamination control. Apart from people, another common way for contamination to enter is on the wheels of trolleys used to transport equipment.\n\nTo prevent airborne contamination, high-efficiency particulate air (HEPA) filters, airlocks and cleanroom suits are used. HEPA filtration systems used in the medical sector incorporate high-energy ultraviolet light units to kill the live bacteria and viruses trapped by the filter media. These measures restrict the number of particulates within the atmosphere and inhibit the growth of those that are viable.\n\nStudies by 3M show that over 80% of contamination enters the cleanroom through entrances and exits, mostly at or near floor level. To combat this problem, suitable flooring systems are used that effectively attract, retain and inhibit the growth of viable organisms. Studies show that the most effective type of flooring system is one of polymer composition.\n\nPolymer mats are particularly effective due to their suppleness as they allow for more contact with serrations on shoes and wheels and can accommodate more particles while still remaining effective. An electrostatic potential adds to the effectiveness of this type of contamination control as it enables particles to be retained until the flooring is cleaned. This method of attracting and retaining particles is more effective than mats with an active adhesive coating which needs to be peeled and is often not as supple. As long as the tack level of the mat is greater than the donor's (such as a foot or a wheel), the contamination touching the surface will be removed. Very high tack surfaces pose a contamination threat because they are prone to pulling off overshoe protection. Polymeric flooring is produced to ensure a higher level of tackiness than the surfaces it comes into contact with, without causing discomfort and potentially damaging 'stickiness'.\n\nCopper-alloy surfaces have intrinsic properties which effectively and quickly destroy microbes and they are being installed in healthcare facilities and in a subway transit system as a protective public health measure in addition to regular cleaning. The United States Environmental Protection Agency (EPA) has approved the registration of 355 different antibacterial copper alloys that kill \"E. coli\" O157:H7, \"methicillin\"-resistant \"Staphylococcus aureus\" (\"MRSA\"), \"Staphylococcus\", \"Enterobacter aerogenes\", and \"Pseudomonas aeruginosa\". The EPA has determined that when cleaned regularly, these copper alloy surfaces:\n\n\nAs a contamination control measure, EPA has approved a long list of antimicrobial copper products \"with public health benefits\" made from these copper alloys, such as bedrails, handrails, over-bed tables, sinks, faucets, door knobs, toilet hardware, computer keyboards, health club equipment, shopping cart handles, etc. (For a comprehensive list of products, see: ).\n\n\n"}
{"id": "32749140", "url": "https://en.wikipedia.org/wiki?curid=32749140", "title": "Cooling capacity", "text": "Cooling capacity\n\nCooling capacity is the measure of a cooling system's ability to remove heat. The SI units are watts (W). Another common unit is the ton of refrigeration, which describes the amount of water at freezing temperature that can be frozen in a day (24 hours). 1 ton of refrigeration is equivalent to 211 kJ/min or 200 Btu/min.\n\nThe basic SI units equation for deriving cooling capacity is of the form:\nWhere\n"}
{"id": "9399676", "url": "https://en.wikipedia.org/wiki?curid=9399676", "title": "Culinology", "text": "Culinology\n\nCulinology, according to Jeff Cousminer in \"Food Product Design Magazine\", is a term that was coined by the first president and founder of the Research Chefs Association, Winston Riley. The original meaning of the word was quite different from what it has come to mean today. Originally, the word was designed to be a combination of two words: \"culinary\" and \"technology\". So the first meaning of the word was the convergence of culinary arts and all technology, which includes communications, chemistry, physiology, economics and many others.\n\nThere are accredited culinology educational programs offered by many institutions. The curriculums of such courses combine the disciplines of cooking and food science. According to industry professionals, such as Harry Crane, culinology should \"help jump-start product development.\"\n\nCulinologists work in diverse aspects of food—from experimental chefs and menu planners to food manufacturing to fine dining. The word is protected by the professional association, the Research Chefs Association, which owns the registered trademark.\n\nMolecular gastronomy\n\n"}
{"id": "33622226", "url": "https://en.wikipedia.org/wiki?curid=33622226", "title": "DataWind", "text": "DataWind\n\nDatawind, founded in Montreal, Quebec, Canada, is a developer and manufacturer of low-cost tablet computers and smartphones. Datawind manufactures low cost tablets and sells these primarily in India, Nigeria, the United Kingdom, Canada, and the United States of America. The company is known for its development of the Aakash tablet computer, which is the world's cheapest tablet at US $37.99/unit. The Aakash tablet was developed for India's Ministry for Human Resource Development (MHRD).\n\nDatawind manufactures a series of mobile, internet devices, such as PocketSurfer smartphones, UbiSurfer netbooks, and Ubislate tablets, and formerly the Aakash tablets on behalf of the Government of India. The company's devices use a patented, web-delivery platform for faster and low-cost internet access on mobile networks. Datawind used to be listed on the Toronto Stock Exchange, but was relisted on the Toronto Venture Exchange in October 2018 after failing to meet listing requirements.\n\nDatawind has offices in Montreal, Mississauga, London, Delhi and Amritsar.\n\nDatawind was founded in Montreal in 2001 by brothers Suneet and Raja Tuli. Suneet and Raja Tuli grew up in Northern Alberta, Canada. Suneet Tuli is a civil engineer, who graduated from the University of Toronto. Raja is a computer engineer, who graduated from the University of Alberta. Suneet Tuli is Datawind's chief executive officer and is responsible for its vision, strategy, and execution. Raja Tuli is the company's co-chairman and chief technology officer; he is also an inventor with dozens of patents across a broad range of technologies related to the internet, to imaging, and to energy sustenance.\n\nDatawind's product range includes PocketSurfer Smartphones and UbiSlate tablets. The company has a portfolio of 14 international patents for its web-delivery platform, which serves as the basis for its product innovations. The company manufactures, markets, and sells UbiSlate tablets to consumers worldwide and also provides a year of free internet browsing in various countries. In collaboration with the government of India, the company has also developed the Aakash tablet, which has gained worldwide attention for being the cheapest tablet.\n\nWith its research and development based in Montreal, the company launched its first products in 2004 and their products were sold primarily in the United Kingdom. In 2004, the company was described as a \"small tech shop\" marketing its key product, the PocketSurfer, a PDA/Mobile Phone/Web Browsing device. Several iterations of the PocketSurfer followed.\n\nIn 2009, the Indian government had pledged a low-cost laptop to improve the quality of education within the country; however, the development process was beset by delays until Datawind won the tender for the tablet in late 2011. In the same year, Datawind made its entry into the Indian market with the launch of the Aakash tablet, developed for the Indian government to enhance the quality of education. To enable ubiquitous, nationwide internet use, the Indian government announced at the October 2011 launch of the Aakash tablet that it will be offered to students at a subsidized price of $35 and to the public (as the \"Ubislate 7\") for $60. At the subsidized price, the tablet would cost the same as a pair of shoes or a basic mobile phone. Some analysts claim that the tablet will not only have a positive impact on the education sector but will also lead to positive changes in healthcare and other sectors besides improving internet penetration in the country. In his study, Rajat Kathuria, external consultant at the Indian Council for Research on International Economic Relations (ICRIER), stated that the Indian economy could grow 10.08% faster with every 10% increase in internet and broadband connections in the country.\n\nThe Ubislate tablets were commercially launched in April 2012, by which time bookings for the device had exceeded three-million units - more than ten times the size of the total market for tablets in India in the previous year. Since the commercial launch of the UbiSlate tablets, the company has been among the top three suppliers of tablets in India, according to Cybermedia Research. In the first quarter of 2013, Datawind dominated the Indian, tablet market with the largest market share of tablets sold in India, excluding the Aakash devices that were to be supplied to the government. Going forward, the company has plans to expose itself to tablet markets in the UK, the US, and Canada. Datawind is also working with governments, NGOs and distributors in Latin American and African countries, where its products are being deployed.\n\nIn 2012, Forbes magazine named the CEO of Datawind, Suneet Tuli, among the Impact 15 list of \"classroom revolutionaries\", who use innovative technologies to reinvent education for students and teachers throughout the world.\n\nOn 26 April 2012, Datawind launched Ubislate 7+ and 7C tablets in India, by which time three- million units had already been booked.\n\nIn 2013, according to Cyber Media Research, India‘s quarterly market report, during the first quarter of 2013, as published in The Times of India, Datawind had a market share of 15.3%; the company pulled ahead of rivals Micromax and Apple, although this strong market did not include the Aakash series of tablets\n\nIn July 2014, the company completed its initial public offering of 6,316,000 common shares at $4.75 CAD per share, totaling $30 million CAD. Datawind's shares traded on the Toronto Stock Exchange under the symbol, \"DW\". In November 2016, Datawind launched its state-of-the-art manufacturing facility in Hyderabad.\n\nBy October 2018, the company was de-listed from the Toronto Stock Exchange, after its share price had fallen to $0.02 per share.\n\nDatawind's low-cost devices allow access to the internet at lower, data costs and faster speeds across congested mobile wireless networks by using the company's patented data compression technology, which serves Web pages as compressed images. In a 2011 interview, the company stated that it would lower the price of the tablet by developing patents to shift the device's processing burden to \"backend servers in the cloud,\" DataWind identified the need for low-cost devices for web access as a means to provide an alternative to increasing costs of mobile internet devices caused by the corresponding increases in web complexity, which require greater memory and processing power. DataWind's web-delivery platform reduces bandwidth consumption by creating a parallel processing environment, which shifts the burden of memory and processing power to back-end servers, thus allowing users to access the web with a lower-cost device and lower data costs.\n\nDatawind's web-delivery platform serves as the basis for its product innovations. Based on a patent portfolio of 14 US and international patents, DataWind's technology accelerates the delivery of web-content across wireless networks, and reduces data consumption by a factor of ten.\n\nDatawind's acceleration and data-reduction is accomplished through the use of proprietary algorithms on its application servers, which constitute a gateway between the primary (Internet-based) content server, selected by the user, and the end user.\n\nDatawind claims that its patented, acceleration technology allows its devices to deliver the fastest, mobile-web experience across cellular networks. A product review by PC Magazine of the Pocket Surfer stated that \"it’s the only device that allows users to browse the web on a GPRS connection with any alacrity.\" In 2005, CNET stated that it was impressed with Pocket Surfer's page-load times. Various media outlets conducted live, speed tests on television with competitive products, including some on 3G networks, which resulted in DataWind's Pocket Surfer beating its competition in terms of page download time.\n\nDatawind's business model focuses on providing to entry-level users a cost-effective, web-access device with free Internet access. The company, as a mobile, virtual-network operator (MVNO), purchases wholesale access to mobile data from wireless-network operators or as part of a bundling relationship in order to offer free, mobile-internet access to its customers. Datawind's web-delivery platform reduces bandwidth consumption.\n\nIn November 2012, in an interview with the New York Times, Datawind‘s CEO explained that the company‘s business model was focused on pursuing price-sensitive, entry-level consumers, on forgoing hardware margins, and on driving a recurring revenue stream (i.e. after sales revenue from network operators, content, subscriptions, device warranties, page impressions, location-based content and advertising). Datawind's advertising revenues are generated by loading applications on its devices. The company has formulated a full-service ecosystem of revenue streams that drive down the cost of hardware; hence, the devices act as customer- acquisition tools that provide \"free mobile-internet services\". Datawind has partnered with numerous firms as part of a comprehensive supplier-and-partner strategy to build applications and generate content for its devices.\n\nDatawind has partnerships with multiple firms to develop installed, free content for its devices. The content includes educational, language-based, interactive smart books, multimedia, games, and productivity tools.\n\nOn 20 November 2013, according to an article published in The Financial Express, \"DataWind, driving its mission to see smart devices as education tools, partnered with American education provider CK-12 Foundation providing high quality free mathematics and science learning content that can be used by teachers, students, schools and parents.\" CK-12 content would be installed on most of the company's devices; the company stated that installed content on tablets will allow students to learn at their own pace, regardless of their locations; also, this content may be updated at regular intervals.\n\nDatawind announced content partnerships with The Indian Express Group and Yahoo!. According to this agreement the Ubislate 7+ and the Ubislate 7C will come with the Indian Express news application, and customers of these new tablets will receive a 50% discount on annual subscriptions of The Indian Express and The Financial Express. The tablets will also come with Yahoo! Cricket, the Yahoo! Mail application. Yahoo! India will be the default-browser homepage for the Ubislate 7+.\n\nDatawind has also partnered with Blueworld portal to encourage a socially conscious approach to the internet. Blueworld is a social community, which allows anyone to create a profile and to connect with friends, to meet people, to upload an unlimited number of photos, to share videos, to write blogs, to support peers through groups, and to send text messages.\n\nOn December 3, 2013, Datawind partnered with IT firm Happiest Minds Technologies to develop applications and solutions for its UbiSlate range of tablets. Under the partnership, Happiest Minds will develop Datawind's app store, which will be designed to create a synergistic customer experience for first-time Android users.\n\nIn July 2009, Datawind collaborated with Vodafone to offer a bundled SIM and GPRS modem within the price of its Pocket Surfer devices, thus allowing free access to the Internet across the United Kingdom.\n\nIn December 2013, BSNL, one of India's largest network operators, entered a partnership with DataWind to offer its services on the company's tablets.\n\nIn March 2015, Datawind partnered with Reliance Communications to offer bundled, unlimited, internet browsing for one year with any of its devices.\n\nIn September 2015, Datawind entered a partnership with Telenor India, a global, mobile- network operator with major Indian operations, to offer unlimited internet browsing for one year on all Datawind devices.\n\nDatawind has partnered with a number of governments and non-governmental organizations(NGOs). Subsequent to the initial launch of the Aakash tablet for higher-learning institutions, Kapil Sibal, the Indian minister of human resources and development, announced that Aakash tablets will be provided free to all 220 million students in India over the next few years.\n\nDuring the 2011 elections in Thailand, the Pheu Thai Party promised free tablets for all students, with numerous placards promising \"One Tablet PC per Child\". Analysts estimated that approximately 20 million units would be required. A senior figure said that the Pheu Thai party would fulfill its promise by importing the $35 Android tablets from India.\n\nDatawind has sponsored a number of contests. On 29 November 2011, the Nasscom Foundation partnered with Datawind to announce a contest wherein 10 NGOs will have an opportunity to win 20 tablets each, mainly to improve their operations and program-implementation procedures. To win these tablets, the organizations had to showcase how they can best use the Aakash tablet for promoting education, health, and livelihood.\n\nOn 11 January 2013, Datawind and the United Nations, in partnership with Agnite Education, American Digital University, Applications for Good, BluWorld, Cat in Woods, and Equal Access International, launched a contest for developing socially responsible applications to empower women. The contest measured apps from the perspective of leadership and mentorship, jobs and entrepreneurship, and education and conflict resolution. The winners of the contest were the My Rights application, the Pictoson application, and the Talking English application.\n\nTo develop a new generation of software programmers who would focus on applications for humanitarian causes, Datawind regularly sponsors \"hackathons\". In 2012 and 2013, Datawind sponsored two \"hackathons\" with Geeks Without Bounds, an accelerator for humanitarian projects.\n\nDatawind partners with and supports a number of charitable and non-government organizations to deploy its technology for humanitarian causes.\n\nOn 23 October 2013, in an article published in The Washington Post, Chris Evans, a renowned philanthropist, donated 100 Aakash tablets to Raleigh schools in Wake County, North Carolina for its \"Smart Summer\" program—-a summer camp that prepares disadvantaged African-American children for school. Ubislate tablets that contain science and mathematics applications proved to be effective learning tools.\n\nDatawind also partnered with Virginia Advanced Study Strategies (VASS), a non-profit organization, to launch a contest in which six school districts enrolled 85 students in either of two coding courses to compete for a DataWind tablet by completing either course and developing an application.\n\nIn November 2013, Datawind and World Vision Canada collaborated to supply $40 tablets in African countries. As part of their on-going field testing at the community level, World Vision used the Ubislate tablets in Niger and Rwanda to collect data at the project level to monitor change and to measure the impact and the effectiveness of their development in the areas of health, education to families, and aid to children in need.\n\nIn January 2014, Datawind was named in MIT Technology Review's 50 Smartest Companies List Recognizing the World's Most Innovative Companies.\n\nAt the 2014 CES, the company was also featured in \"Dave's Top Ten\" and \"Into Tomorrow with Dave Graveline\".\n\nIn November 2012, United Nations Secretary General Ban Ki Moon launched the Aakash 2 tablet at the United Nations. The United Nations secretary general referred to DataWind's products as the \"great enabler with potential to transform people’s lives\".\n\nIn 2012, Forbes magazine named CEO Suneet Tuli among the Impact 15 list of \"classroom revolutionaries\" who are using innovative technologies to reinvent education for students and teachers throughout the world.\n\nDatawind was also acknowledged with several other awards:\n\n\nThe Pocket Surfer is Datawind's handheld, mobile-network-access device. The first version of the Pocket Surfer was launched in 2004.\n\nThe device featured a QWERTY keyboard and a 5.3 inch (diagonal) screen. Reviews for the Pocket Surfer had been mixed as PC Magazine's review of the product exclaimed that \"it’s the only device that lets you browse the web on a GPRS connection with any alacrity\" and CNET in 2005 stated that it was impressed with Pocket Surfer's page-load times.\n\nIn February 2007, at the 3GSM forum in Barcelona, Datawind introduced the improved \"Pocket Surfer 2\". It was described as the world's fastest handheld Internet device. The sleek mobile device offered free mobile-internet access and provided a desktop-like experience with a 640-pixel-wide, color screen that displayed webpages with HTML, graphics, Java-Script, Ajax, Frames, and other complex, web functionality. The Pocket Surfer 2 includes a built-in GPS receiver for location-based services with page-load times of less than seven seconds. Engadget referred to the original Pocket Surfer devices as \"Iconic\"\n\nIn December 2013, on the tenth anniversary of the launch of the original Pocket Surfer, the company entered the smart-phone market with the launch of the Pocket Surfer range of smartphones in India. Featuring 5\" inch touchscreens, these dual-sim smartphones were available at an extremely low, price point. In reaction to the launch of the smartphone in India, The Economic Times exclaimed \"Aakash maker DataWind enters smartphone market, breaks all price points!\"\n\nIn March 2015, the company launched its range of low-cost smartphones, the Pocket Surfer 2G4 and the Pocket Surfer 3G4 with one year of free internet browsing on its proprietary UbiSurfer browser.\n\nDatawind has partnered with Reliance Communication, one of the largest Telecom operators in the India to offer one year of free internet browsing along with its devices.\n\nPocketSurfer Smartphone Range\n\nThe UbiSurfer is Datawind's range of netbooks. In April 2010, Datawind launched UbiSurfer netbooks in India. The UbiSurfer name is an amalgamated form of the two words, \"ubiquitous\" and \"surfing\". The netbook offers mobile web browsing with an integrated modem and a SIM-card slot. The UbiSurfer supports both CDMA (1x) and GSM networks. The device offers WiFi and LAN connectivity, an in-built cellular modem, and an embedded SIM card that allows access to the internet with a mobile-phone signal. The UbiSurfer is also equipped with an ARM processor that runs at 450 MHz with 128MB RAM, 1GB Flash memory, and up to 50GB of online storage. The UbiSurfer has a seven-inch, bright TFT screen that supports a 800x480 pixel resolution. The device is also equipped with the UbiSurfer browser, which allows webpage delivery times of under seven seconds.\n\nThe UbiSurfer 9 was made available in India at a price of Rs. 7999, which also included a one-year free Internet browsing plan. On 7 April 2011, Datawind unveiled its new version of the mobile internet device, the UbiSurfer 9 3G, in London, England. This netbook offered users free Internet access in the UK and low cost roaming in Europe and USA. It sported a sleek nine-inch screen and weighed in at 700g, its free-usage model eliminated the user from any form of binding contracts, activation fees or credit checks.\n\nIn 2009, the National Mission on Education through Information Communication Technology (NME-ICT) initiative of the Indian Government cited that a lot could be done for education using information and communication technologies. An initiative by India's Ministry for Human Resource Development (MHRD) was launched to procure a low cost mobile web access device with built in features ensuring that technologies and content were accessible to everyone. In late 2011, Datawind won the Indian government tender to design the Aakash tablet computers with an initial trial run of 100,000 units. Upon its launch, The Wall Street Journal stated that the Aakash was the World's cheapest tablet.\n\nOn 5 October 2011, the Aakash was launched by Kapil Sibal, the then Minister of Human Resource and Development. Datawind designed, developed and manufactured this first version of Aakash on the specifications set by IIT Rajasthan. It's extremely low price garnered global attention and the tablet is known as the world's cheapest tablet device.\n\nOn 11 November 2012, President of India, Pranab Mukherjee launched the Aakash 2,an improved version of the Aakash tablet. This updated new version had a better processor based on the ARM Cortex A8 architecture and had a multi-touch projective capacitive screen in place of a resistive one. The Aakash 2 (Ubislate 7Ci) was considered the World's most cost effective web access device, it utilized a 1 GHz processor, of the same caliber as the original iPad, and contained 512 MB of RAM (twice that of the original iPad). The tablet supported a flash memory of 4 GB that can be supplemented by up to 32GB with the use of a micro-SD card. In addition it was loaded with Wi-Fi supported connectivity and also an external 3G and an EVDO dongle for mobile broadband data. Other features included Google's Android 4.0 operating system, a VGA camera, G-sensors, an internal microphone, speakers and a headphone jack. The Aakash 2 is considered as a full-featured tablet computer.\n\nOn 12th Nov 2013, The Indian Express was quoted as saying that the \"Aakash 3 was better than its predecessor. The difference with previous versions was the addition of calling functionality and a more powerful battery that provided longer backup. Proposed Akash 4 (Ubislate 7CZ) the tablet with proposed technical specifications set by India’s Department of Electronics and technology-Ministry of Information and technology-Government of India.\n\nThe UbiSlates are a series of Android tablets with touch screens that are also capable of performing as smart-phones and are built for mobile web access, multimedia content , android games and applications. The UbiSlate tablet has an High-Definition video co-processor for high quality video and comes equipped with Datawind's UbiSurfer browser that accelerates web page delivery. The device includes WiFi & GPRS connectivity as well as allows for mobile internet access through SIM and Phone connectivity. Optional 3G modems are also supported via USB. In addition to a micro-SD card slot, a full-sized USB port is integrated into the unit allowing pen-drives, external keyboards, web-cams, dongles and other inexpensive accessories can also be attached.\n\nOn 26th April,2012 Datawind launched Ubislate 7+ and 7C tablets in India, in the same month the company declared that it had 3 million pre-bookings for Ubislate tablets. The commercial version of Aakash was released online as the UbiSlate7 tablet PC at INR 3000 (US$46) and the Ubislate7+ tablet PC at INR 3500 (US$54) on 11 November 2012 with plans to offer it at a subsidized cost for students of INR 1130 (US$17). In October 2013, Datawind launched UbiSlate 7Cx, UbiSlate 3G7 and UbiSlate 9Ci tablets in India.\n\nIn December 2013, Datawind officially launched three products from its range of UbiSlate Tablets in Canada, the United States and the United Kingdom.\n\nUbislate Tablet Range\n"}
{"id": "27613940", "url": "https://en.wikipedia.org/wiki?curid=27613940", "title": "Dinghao Market", "text": "Dinghao Market\n\nDinghao Market is one of five major electronics markets in Zhongguancun, Beijing.\n\nLi Zhongjin is the current chief business officer of Dinghao Market.\n\nDinghao Market is home to Lenovo's flagship store and Starbucks 1000th milestone Asian location.\n\n\n"}
{"id": "19631836", "url": "https://en.wikipedia.org/wiki?curid=19631836", "title": "Drag reducing agent", "text": "Drag reducing agent\n\nDrag-reducing agents (DRA), or drag-reducing polymers (DRP's), are additives in pipelines that reduce turbulence in a pipe. Usually used in petroleum pipelines, they increase the pipeline capacity by reducing turbulency and therefore allowing the oil to flow more efficiently.\n\nDrag reducing agents can be broadly classified under the following four categories – Polymers, Solid-particle suspensions, Biological additives, and Surfactants. These agents are made out of high molecular weight polymers or micellar systems. The polymers help with drag reduction by decreasing turbulence in the oil lines. This allows for oil to be pumped through at lower pressures, saving energy and money. Although these drag reducing agents are mostly used in oil lines, there is research being done to see how helpful polymers could be in reducing drag in veins and arteries.\n\nUsing just a few parts per million of the drag reducer helps to reduce the turbulence inside the pipe. Because the oil pushes up against the inside wall of the pipe, the pipe pushes the oil back down causing a swirling of turbulence to occur. When the polymer is added, it interacts with the oil and the wall to help reduce the contact of the oil with the wall.\n\nDegradation can occur on the polymers during the flow. Because of the pressure and temperature on the polymers, it is easier to break them down. Because of this, the drag reducing agent is re-injected after points like pumps and turns, where the pressure and temperature can be extra high. To safeguard against degradation at high temperature, a different class of drag reducing agents are at times used, namely, surfactants. \"Surfactant\" is a very convenient contraction of the term \"Surface-active agent\". It connotes an organic molecule or an unformulated compound having surface-active properties. All three classes of surfactants, namely, anionic, cationic and nonionic surfactants, have been successfully tried as drag-reducing agents.\n\nKnowing what will create the ideal drag reducer is key in this process. Ideal molecules have a high molecular weight, shear degradation resistance, are quick to dissolve in whatever is in the pipe, and have low degradation in heat, light, chemicals, and biological areas.\n\nWith drag reduction, there are many factors which play a role in how well the drag is reduced. A main factor in this is temperature. With a higher temperature, the drag reducing agent is easier to degrade. At a low temperature the drag reducing agent will tend to cluster together. This problem can be solved easier than degradation though, by adding another chemical, such as aluminum to help lower the drag reducing agent's attraction to one another. Another factor is the pipe diameter. With a decreasing pipe diameter, the drag reduction is increased. Going along with this, the roughness of the inside of the pipe has a factor. The rougher the inside, the higher the percent drag reduction occurring. Increasing the pressure in a pipe will help with drag reduction as well, but often that pressure is greater than what the pipe can withstand.\n\nDrag reducing agents have been found useful in reducing turbulence in the shipbuilding industry, for fire-fighting operations, oil-well fracturing processes, in irrigation systems and in central heating devices. Drag reducers can work in a couple of different fields. The most popular are crude oil, refined products and non-potable water. Currently there are several studies with ongoing tests in rats looking to see if drag reducers can help with blood flow.\n\nThe earliest works that recorded a decrease in pressure drop during turbulent flow were undertaken in the thirties and concerned the transportation of paper pulp. This was, however, not explicitly referred to as a drag reduction phenomenon. Toms was the first to recognize the tremendous reduction in wall shear stress caused by the addition of small amount of linear macromolecules to a turbulent flowing fluid. An extensive bibliography of the first 25 years of drag reduction by polymer additives literature identified over 270 references.\n\nDrag reducers were introduced into the market in the early 1970s by Conoco Inc. (now known as LiquidPower Specialty Products Inc. (LSPI), a Berkshire Hathaway Company). Its use has allowed pipeline systems to greatly increase in traditional capacity and extended the life of existing systems. The higher flow rates possible on long pipelines have also increased the potential for surge on older systems not previously designed for high velocities.\n\nBoth proprietary (such as Conoco T-83) and non-proprietary (such as poly-isobutylene) drag reduction additives have been evaluated by the U.S. Army Mobility Equipment Research and Development Center for enhancement of military petroleum pipeline systems.\n"}
{"id": "27909207", "url": "https://en.wikipedia.org/wiki?curid=27909207", "title": "Energy analyser", "text": "Energy analyser\n\nAn instrument for measuring various parameters of an electrical power distribution system is often called an energy analyser. The term is also used for computer software for analysing the use of electrical or other energy use, for example determining how electrical power is consumed in a building with a view to reducing waste, calculate loads and costs associated with air conditioning, heating, and on-site power generation. In the context of HPC/Grid/Cloud computing, EnergyAnalyzer tool development project, an ongoing DST funded project (2011-2014) from, the HPCCLoud Research Laboratory, India is on progress. The EnergyAnalyzer tool development project aims at designing and implementing the EnergyAnalyzer tool which does energy consumption analysis and tuning of HPC applications using distributed semantic agents.\n\nAn electrical energy analyser might measure for single- and three-phase systems volts RMS, amps RMS, power factor, instantaneous power in watts (W), instantaneous volt-amperes (VA), reactive volt-amperes (VAr), frequency (Hz), average and maximum powers (W), harmonic distortion, energy in watt-hours (Wh), reactive volt-ampere-hours (VArh), and phase angles. Power- and energy-related parameters tend to be stated in kilowatts rather than watts—kW, kVA, kVAr, kWh, kVArh. For non-sinusoidal periodic waveforms, some instruments can measure harmonics. The instrument, in addition to displaying values, may print or store them, network with other similar instruments at different locations, interact with computer software, etc.\n\nThe term is also used for a quite different electrostatic analyzer or electrostatic energy analyzer, an instrument used in ion optics that employs an electric field to allow the passage of only those ions or electrons that have a given specific energy.\n"}
{"id": "8518824", "url": "https://en.wikipedia.org/wiki?curid=8518824", "title": "Extensible Forms Description Language", "text": "Extensible Forms Description Language\n\nExtensible Forms Description Language (XFDL) is a high-level computer language that facilitates defining a form as a single, stand-alone object using elements and attributes from the Extensible Markup Language (XML). Technically, it is a class of XML originally specified in a World Wide Web Consortium (W3C) Note. See Specifications below for links to the current versions of XFDL. XFDL It offers precise control over form layout, permitting replacement of existing business/government forms with electronic documents in a human-readable, open standard.\n\nIn addition to precision layout control, XFDL provides multiple page capabilities, step-by-step guided user experiences, and digital signatures. XFDL also provides a syntax for in-line mathematical and conditional expressions and data validation constraints as well as custom items, options, and external code functions. Current versions of XFDL (see Specifications below) are capable of providing these interactive features via open standard markup languages including XForms, XPath, XML Schema and XML Signatures.\n\nXFDL not only supports multiple digital signatures, but the signatures can apply to specific sections of a form and prevent changes to signed content.\n\nThese advantages to XFDL led large organizations such as the United States Army and Air Force to migrate to XFDL from using forms in other formats. Later, though, the lack of portable software capable of creating XFDL led them to investigate moving away from it. The Army migrated to Adobe fillable PDFs in 2014.\n\n\n\n"}
{"id": "25908079", "url": "https://en.wikipedia.org/wiki?curid=25908079", "title": "FastPort", "text": "FastPort\n\nThe FastPort was a proprietary polyconnection interface used on all Sony Ericsson cellphones between 2005 and 2010. Designed in response to Nokia's proprietary Pop-Port, FastPort provided data transfer, charging, headset and speaker connections through a common interface. It was discontinued in 2010 and replaced with a micro-USB for charging and data, and a TRRS connection for audio (headphones).\n\nA USB FastPort-cable enables file and data transfer between a computer and a Sony Ericsson cellphone. Most models could act as a USB-storage-device, modem, phone and could load new firmware either with Sony Ericsson Update Service application, or with 3rd party software. FastPort was the interface to the PC to realize these functions.\n\nThe port can charge the battery and power the phone while it is connected to, for example, a hands-free solution in a car. The FastPort became the only way to get external power to the phones. Chargers comes in several varieties, from 12/24 volt DC to use in cars, to 100-250 volt AC to use elsewhere. Some charger-models can only charge the phone (the cable is attached at the middle), in others all the connector pins through to the plug end, thus supporting data/signal transfer while the phone is being charged.\n\nThe port also connects wired headsets or speakers, etc.\n\nOriginally, the FastPort was placed on the bottom edge of the phone (when viewed from the front), for a while on the top edge, and finally on the left edge. These changes caused some accessories to become unusable, such as holders with charging options and docks.\n\nThe connector has 12 pins for electrical connections (both power and data), 2 double-sided \"hooks\" on the plug and matching holes in the phones connector for keeping the plug safely in place. One hook contains a small polarity key to prevent the connector being inserted upside down. The dimension of the connector on the phone is approximately . To help users identify the type of cable and see how to correctly insert the plug, a small symbol is placed on the side intended to be towards the front of the phone. Powerplugs display a small lightning bolt, headsets and hands-free-plugs show an old-fashioned headset, data-cables present a computer screen and music accessories reveal a note-sign.\n\n"}
{"id": "2091393", "url": "https://en.wikipedia.org/wiki?curid=2091393", "title": "Fault tolerance", "text": "Fault tolerance\n\nFault tolerance is the property that enables a system to continue operating properly in the event of the failure of some (one or more faults within) of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a native designed system in which even a small failure can cause total breakdown. Fault tolerance is particularly sought after in high-availability or life-critical systems. The ability of maintaining functionality when portions of a system break down is referred to as graceful degradation.\n\nA fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails. The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure. That is, the system as a whole is not stopped due to problems either in the hardware or the software. An example in another field is a motor vehicle designed so it will continue to be drivable if one of the tires is punctured, or a structure that is able to retain its integrity in the presence of damage due to causes such as fatigue, corrosion, manufacturing flaws, or impact.\n\nWithin the scope of an \"individual\" system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication. In any case, if the consequence of a system failure is so catastrophic, the system must be able to use reversion to fall back to a safe mode. This is similar to roll-back recovery but can be a human action if humans are present in the loop.\n\nA highly fault-tolerant system might continue at the same level of performance even though one or more components have failed. For example, a building with a backup electrical generator will provide the same voltage to wall outlets even if the grid power fails.\n\nA system that is designed to fail safe, or fail-secure, or fail gracefully, whether it functions at a reduced level or fails completely, does so in a way that protects people, property, or data from injury, damage, intrusion, or disclosure. In computers, a program might fail-safe by executing a graceful exit (as opposed to an uncontrolled crash) in order to prevent data corruption after experiencing an error. A similar distinction is made between \"failing well\" and \"failing badly\".\n\nFail-deadly is the opposite strategy, which can be used in weapon systems that are designed to kill or injure targets even if part of the system is damaged or destroyed.\n\nA system that is designed to experience graceful degradation, or to fail soft (used in computing, similar to \"fail safe\") operates at a reduced level of performance after some component failures. For example, a building may operate lighting at reduced levels and elevators at reduced speeds if grid power fails, rather than either trapping people in the dark completely or continuing to operate at full power. In computing an example of graceful degradation is that if insufficient network bandwidth is available to stream an online video, a lower-resolution version might be streamed in place of the high-resolution version. Progressive enhancement is an example in computing, where web pages are available in a basic functional format for older, small-screen, or limited-capability web browsers, but in an enhanced version for browsers capable of handling additional technologies or that have a larger display available.\n\nIn fault-tolerant computer systems, programs that are considered robust are designed to continue operation despite an error, exception, or invalid input, instead of crashing completely. Software brittleness is the opposite of robustness. Resilient networks continue to transmit data despite the failure of some links or nodes; resilient buildings and infrastructure are likewise expected to prevent complete failure in situations like earthquakes, floods, or collisions.\n\nA system with high failure transparency will alert users that a component failure has occurred, even if it continues to operate with full performance, so that failure can be repaired or imminent complete failure anticipated. Likewise, a fail-fast component is designed to report at the first point of failure, rather than allow downstream components to fail and generate reports then. This allows easier diagnosis of the underlying problem, and may prevent improper operation in a broken state.\n\nRedundancy is the provision of functional capabilities that would be unnecessary in a fault-free environment.\nThis can consist of backup components that automatically \"kick in\" if one component fails. For example, large cargo trucks can lose a tire without any major consequences. They have many tires, and no one tire is critical (with the exception of the front tires, which are used to steer, but generally carry less load, each and in total, than the other four to 16, so are less likely to fail).\nThe idea of incorporating redundancy in order to improve the reliability of a system was pioneered by John von Neumann in the 1950s.\n\nTwo kinds of redundancy are possible: space redundancy and time redundancy. Space redundancy provides additional components, functions, or data items that are unnecessary for fault-free operation. Space redundancy is further classified into hardware, software and information redundancy, depending on the type of redundant resources added to the system. In time redundancy the computation or data transmission is repeated and the result is compared to a stored copy of the previous result. The current terminology for this kind of testing is referred to as 'In Service Fault Tolerance Testing or ISFTT for short.\n\nProviding fault-tolerant design for every component is normally not an option. Associated redundancy brings a number of penalties: increase in weight, size, power consumption, cost, as well as time to design, verify, and test. Therefore, a number of choices have to be examined to determine which components should be fault tolerant:\n\n\nAn example of a component that passes all the tests is a car's occupant restraint system. While we do not normally think of the \"primary\" occupant restraint system, it is gravity. If the vehicle rolls over or undergoes severe g-forces, then this primary method of occupant restraint may fail. Restraining the occupants during such an accident is absolutely critical to safety, so we pass the first test. Accidents causing occupant ejection were quite common before seat belts, so we pass the second test. The cost of a redundant restraint method like seat belts is quite low, both economically and in terms of weight and space, so we pass the third test. Therefore, adding seat belts to all vehicles is an excellent idea. Other \"supplemental restraint systems\", such as airbags, are more expensive and so pass that test by a smaller margin.\n\nAnother excellent and long-term example of this principle being put into practice is the braking system: whilst the actual brake mechanisms are critical, they are not particularly prone to sudden (rather than progressive) failure, and are in any case necessarily duplicated to allow even and balanced application of brake force to all wheels. It would also be prohibitively costly to further double-up the main components and they would add considerable weight. However, the similarly critical systems for actuating the brakes under driver control are inherently less robust, generally using a cable (can rust, stretch, jam, snap) or hydraulic fluid (can leak, boil and develop bubbles, absorb water and thus lose effectiveness). Thus in most modern cars the footbrake hydraulic brake circuit is diagonally divided to give two smaller points of failure, the loss of either only reducing brake power by 50% and not causing as much dangerous brakeforce imbalance as a straight front-back or left-right split, and should the hydraulic circuit fail completely (a relatively very rare occurrence), there is a failsafe in the form of the cable-actuated parking brake that operates the otherwise relatively weak rear brakes, but can still bring the vehicle to a safe halt in conjunction with transmission/engine braking so long as the demands on it are in line with normal traffic flow. The culmulatively unlikely combination of total footbrake failure with the need for harsh braking in an emergency will likely result in a collision, but still one at lower speed than would otherwise have been the case.\n\nIn comparison with the footpedal activated service brake, the parking brake itself is a less critical item, and unless it is being used as a one-time backup for the footbrake, will not cause immediate danger if it is found to be nonfunctional at the moment of application. Therefore, no redundancy is built into it per se (and it typically uses a cheaper, lighter, but less hardwearing cable actuation system), and it can suffice, if this happens on a hill, to use the footbrake to momentarily hold the vehicle still, before driving off to find a flat piece of road on which to stop. Alternatively, on shallow gradients, the transmission can be shifted into Park, Reverse or First gear, and the transmission lock / engine compression used to hold it stationary, as there is no need for them to include the sophistication to first bring it to a halt.\n\nOn motorcycles, a similar level of fail-safety is provided by simpler methods; firstly the front and rear brake systems being entirely separate, regardless of their method of activation (that can be cable, rod or hydraulic), allowing one to fail entirely whilst leaving the other unaffected. Secondly, the rear brake is relatively strong compared to its automotive cousin, even being a powerful disc on sports models, even though the usual intent is for the front system to provide the vast majority of braking force; as the overall vehicle weight is more central, the rear tyre is generally larger and grippier, and the rider can lean back to put more weight on it, therefore allowing more brake force to be applied before the wheel locks up. On cheaper, slower utility-class machines, even if the front wheel should use a hydraulic disc for extra brake force and easier packaging, the rear will usually be a primitive, somewhat inefficient, but exceptionally robust rod-actuated drum, thanks to the ease of connecting the footpedal to the wheel in this way and, more importantly, the near impossibility of catastrophic failure even if the rest of the machine, like a lot of low-priced bikes after their first few years of use, is on the point of collapse from neglected maintenance.\n\nThe basic characteristics of fault tolerance require:\n\n\nIn addition, fault-tolerant systems are characterized in terms of both planned service outages and unplanned service outages. These are usually measured at the application level and not just at a hardware level. The figure of merit is called availability and is expressed as a percentage. For example, a five nines system would statistically provide 99.999% availability.\n\nFault-tolerant systems are typically based on the concept of redundancy.\n\nSpare components address the first fundamental characteristic of fault tolerance in three ways:\n\nAll implementations of RAID, redundant array of independent disks, except RAID 0, are examples of a fault-tolerant storage device that uses data redundancy.\n\nA lockstep fault-tolerant machine uses replicated elements operating in parallel. At any time, all the replications of each element should be in the same state. The same inputs are provided to each replication, and the same outputs are expected. The outputs of the replications are compared using a voting circuit. A machine with two replications of each element is termed dual modular redundant (DMR). The voting circuit can then only detect a mismatch and recovery relies on other methods. A machine with three replications of each element is termed triple modular redundant (TMR). The voting circuit can determine which replication is in error when a two-to-one vote is observed. In this case, the voting circuit can output the correct result, and discard the erroneous version. After this, the internal state of the erroneous replication is assumed to be different from that of the other two, and the voting circuit can switch to a DMR mode. This model can be applied to any larger number of replications.\n\nLockstep fault-tolerant machines are most easily made fully synchronous, with each gate of each replication making the same state transition on the same edge of the clock, and the clocks to the replications being exactly in phase. However, it is possible to build lockstep systems without this requirement.\n\nBringing the replications into synchrony requires making their internal stored states the same. They can be started from a fixed initial state, such as the reset state. Alternatively, the internal state of one replica can be copied to another replica.\n\nOne variant of DMR is pair-and-spare. Two replicated elements operate in lockstep as a pair, with a voting circuit that detects any mismatch between their operations and outputs a signal indicating that there is an error. Another pair operates exactly the same way. A final circuit selects the output of the pair that does not proclaim that it is in error. Pair-and-spare requires four replicas rather than the three of TMR, but has been used commercially.\n\nFault-tolerant design's advantages are obvious, while many of its disadvantages are not:\n\n\nHardware fault tolerance sometimes requires that broken parts be taken out and replaced with new parts while the system is still operational (in computing known as \"hot swapping\"). Such a system implemented with a single backup is known as single point tolerant, and represents the vast majority of fault-tolerant systems. In such systems the mean time between failures should be long enough for the operators to have time to fix the broken devices (mean time to repair) before the backup also fails. It helps if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.\n\nFault tolerance is notably successful in computer applications. Tandem Computers built their entire business on such machines, which used single-point tolerance to create their NonStop systems with uptimes measured in years.\n\nFail-safe architectures may encompass also the computer software, for example by process replication.\n\nData formats may also be designed to degrade gracefully. HTML for example, is designed to be forward compatible, allowing new HTML entities to be ignored by Web browsers that do not understand them without causing the document to be unusable.\n\nThere is a difference between fault tolerance and systems that rarely have problems. For instance, the Western Electric crossbar systems had failure rates of two hours per forty years, and therefore were highly \"fault resistant\". But when a fault did occur they still stopped operating completely, and therefore were not \"fault tolerant\".\n\n\n"}
{"id": "21650857", "url": "https://en.wikipedia.org/wiki?curid=21650857", "title": "Federal furniture", "text": "Federal furniture\n\nFederal furniture refers to American furniture produced in the Federal Period, which lasted from approximately 1789 to 1823. Notable furniture makers who worked in the federal style included Duncan Phyfe and Charles-Honoré Lannuier. It was influenced by the Georgian and Adam styles, and was superseded by the American Empire style.\n\nPieces in this style are characterized by their sharply geometric forms, legs that are usually straight rather than curved, contrasting veneers, and geometric inlay patterns on otherwise flat surfaces. Pictorial motifs, when extant, usually reference the new federal government with symbols such as the eagle. \n\nThe Oval Office grandfather clock, made by between 1795–1805 in Boston by John and Thomas Seymour, is a noted example of the federal style of furniture. \n\n"}
{"id": "22552305", "url": "https://en.wikipedia.org/wiki?curid=22552305", "title": "Filmlook, Inc.", "text": "Filmlook, Inc.\n\nFilmlook, Inc. is a post-production company based in Burbank, California. Established in 1989, it specializes in a form of image processing used on television programs, commonly known as film look. The company has won an Emmy Award for its technical achievements.\n\nIn 1987, company founder Robert Faber began developing the company's process. By 1989, the company was founded and introduced to the industry.\n\nThe Filmlook process affects three main features to achieve the appearance of film: motion characteristics, gray scale/contrast, and grain pattern.\n\n\nInvented in 1989, the Filmlook image processing was first used in a test run in a 1991 episode of the ABC sitcom \"Growing Pains\" titled \"Not With My Carol You Don't\". However, the first television series to regularly use Filmlook was \"Beakman's World\", a kid-oriented science series which ran from 1992–1996 on CBS. In 1995, Filmlook was used on the LL Cool J sitcom \"In the House\". However, when the series moved from NBC to UPN in 1996, the series began using unprocessed video.\n\nIn recent years, Filmlook has become known for its use on nearly all Disney Channel Original Series made from 2002 to 2008 (except \"Phil of the Future\" which was shot on film). \"That's So Raven\", which at one point was the channel's most-watched series, was the first Disney Channel show to use the processing. Since then, four other original series on the channel have had their taped product processed by the company: \"The Suite Life of Zack and Cody\", \"Hannah Montana\", \"That's So Raven\" spinoff \"Cory in the House\" , \"Wizards of Waverly Place\" and \"The Suite Life on Deck\". Filmlook processing has also been used on segments within the Nickelodeon series \"The Amanda Show\" for commercial parodies and the mock teen series \"Moody's Point.\"\n\n†Denotes series that were previously or otherwise broadcast with unprocessed video.\n\n\n"}
{"id": "23880274", "url": "https://en.wikipedia.org/wiki?curid=23880274", "title": "Flame projector", "text": "Flame projector\n\nIn pyrotechnics, a flame projector is a special effects device that projects a column of flame upwards, for a short, determined and controllable, period, usually on the order of a few seconds. The simplest form of flame projector is simply a vertical tube mounted on a base, containing powder and a hole for an electric match.\n\nFlame projectors can produce flames of different colours by simply adding a colouring agent. Potassium compounds make purple compounds, for instance. Lithium and strontium are red, sodium is bright yellow, copper and boron compounds are blue or green. \n"}
{"id": "355956", "url": "https://en.wikipedia.org/wiki?curid=355956", "title": "Frederic Vester", "text": "Frederic Vester\n\nFrederic Vester (November 23, 1925 – November 2, 2003) was a German biochemist, and an expert in the field of ecology.\n\nVester was born in Saarbrücken, and studied chemistry at the universities of Mainz, Paris and Hamburg. From 1955 to 1957 he was postdoctoral fellow at Yale University and Cambridge. From 1957 to 1966 he worked at Saarland University, Saarbrücken, and from 1969 he worked in Munich, first at the Max Planck Institute. In 1970 he founded the private Munich-based \"Frederic Vester Studiengruppe für Biologie und Umwelt GmbH\" (\"Frederic Vester Study Group for Biology and Environment, Ltd.), renamed \"Frederic Vester GmbH\" (\"Frederic Vester, Ltd.\") after his death.\n\nFrom 1982 to 1989 he was a professor at the Bundeswehr University Munich, and from 1989 to 1991 he was Professor for Applied Economics at the Hochschule St. Gallen, Switzerland. Vester's ideas influenced the formation of the environmental movement and the Green Party in Germany. He was a member of the Club of Rome. He was married to Anne Vester. The couple had three children and six grandchildren. He died in Munich.\n\nVester was known as pioneer of networked thinking, a combination of cybernetic and systemic ideas and complexity. Central ideas of network thinking include viewing a system as a network of interrelated effects, leading to emergent behavior of the system as a whole. These networks can be described by using protocols, mathematical networks, computer software, so that even someone with the most basic understanding of networks will see relations, including positive and negative feedback loops. Simulations of systemic networks can help to decide the long-term effects of singular measures.\n\nVester's \"Sensitivity Model\" combines these ideas, and has been used since the 1980s in studies by Ford, UNESCO and other organizations.\n\nMost of Vesters books were published in German as well as in other languages, though seldom in English. A list of his works includes:\n\nVester is also the author of the software tool \"Sensitivity model\" and of several cybernetic games:\n\n\n"}
{"id": "39487322", "url": "https://en.wikipedia.org/wiki?curid=39487322", "title": "George Oates", "text": "George Oates\n\nGeorge Oates (birth name Georgina Oates, born 1973) is an Australian-born designer and entrepreneur, best known for being the first designer of the photo-sharing website Flickr and for creating the Flickr Commons program. Since 2007 she has worked in the cultural heritage sector and is regarded as \"increasingly a go-to expert on digital archives\". She has also written a book called \"If Only The Grimms Had Known Alice\", a retelling of the Grimm brothers' fairy tales to include female characters.\n\nOates was born in Adelaide, Australia, to an Australian father and a British mother, and is the youngest of three siblings.\n\nIn 1996, Oates was in the first group of employees at the Ngapartji Multimedia Centre in Adelaide, where she taught the general public how to use the internet and went on to teach courses in HTML and web design. After working in the web industry there for the next seven years, she left Australia in 2003 to start work at Ludicorp, the company that went on to make Flickr. After four years responsible for Flickr's design, Oates invented the Flickr Commons program, designed to make public photography collections available on Flickr with no known copyright restrictions. The first partner for the program was the Library of Congress, and it launched in January 2008. Oates was laid off by Yahoo at the end of 2008.\n\nIn 2009, she started work as director of the Open Library project at the Internet Archive. In her time there she also designed new interfaces for the Book Reader, the Wayback Machine, and the 9/11 Archive.\n\nFrom 2011 to 2014, Oates was art director at San Francisco data visualization studio Stamen Design. While in San Francisco, she was a judge for the 2013 Information is Beautiful Awards.\n\nIn 2014 she launched her own company called Good, Form & Spectacle, which has completed projects for institutions like The British Museum, The Victoria and Albert Museum and Wellcome Library.\n\nOates has spoken publicly about her work around the world since 2005, including at keynote speeches at Smithsonian 2.0, OCLC Futurecast, and Europeana Tech 2015, and is a public advocate for open cultural data and content.\n\nIn 2011, Oates was appointed a Research Associate at Smithsonian Libraries. She is also a non-executive director of Postal Heritage Services, a subsidiary of The Postal Museum, and is on the advisory board of the British Library Labs initiative, a Mellon Foundation-funded program to increase access to the library's collections.\n\n"}
{"id": "25113595", "url": "https://en.wikipedia.org/wiki?curid=25113595", "title": "Green building in France", "text": "Green building in France\n\nGreen building in France\n\nIn July 2007, the French government established six working groups to address ways to redefine France's environment policy. The proposed recommendations were then put to public consultation, leading to a set of recommendations released at the end of October 2007. These recommendations will be put to the French parliament in early 2008.\n\nThe name of the process, \"Le Grenelle de l'Environnement\", refers to a 1968 conference when government negotiated with unions to end weeks of social unrest.\n\nThe six working groups addressed climate change, biodiversity and natural resources, health and the environment, production and consumption, democracy and governance, and competitiveness and employment.\n\nRecommendations include:\n\n20% reduction in France's energy consumption by 2020 and a boosting of the use of renewable energy, such as wind power and biofuels, by 20% by 2020;\n\nBuilding labels\n\nThe French regulation (RT) for new construction was following an incremental logic with a regular (every five years) increase in the exigence level requested to achieve by 2020 (RT 2020) a 40% reduction of energy consumption with respect to the RT 2000. Current label are: THPE 2005=20% better than the RT2005. THPE EnR 2005= 30% better than RT2005+ Renewable energy production for the majority of heating.\nWithin the framework of the \"Grenelle de l’envronnement\", a performance acceleration is expected to meet with the following objectives for tertiary buildings:\n\nI. Low consumption buildings (BBC) by 2010 with minimum requirements concerning the levels of renewable energy and CO absorption materials by 2012.\n\nII. Passive new buildings (BEPAS) or Positive buildings (BEPOS) by 2020.\n\nLabels for refurbishment of existing BBC buildings.\n\nAll these developments match with the European and international regulations and frameworks.\n\n\n"}
{"id": "43807943", "url": "https://en.wikipedia.org/wiki?curid=43807943", "title": "Hydrophosphination", "text": "Hydrophosphination\n\nHydrophosphination is the addition of a phosphorus-hydrogen bond across a carbon-carbon multiple bond (Scheme 1) forming a new phosphorus-carbon bond. Hydrophosphination has a high atom economy.\n\nLike other metal-catalyzed heterofunctionalizations, addition of an E-H bond to an unsaturated substrate, a problem is encountered when E is a donor (hydroamination, hydration, hydrothiolation) because the product phosphines can poison the catalyst. Another consideration is that the P-H addition to a carbon-carbon multiple bond can occur in a variety of different ways. The selectivity of the addition is influenced by the catalyst.\n\nA variety of routes exist for the synthesis of phosphines without a catalyst, e.g. using radical, thermal, photochemical.\n\nUltraviolet irradiation or conventional free-radical sources initiate radical hydrophosphinations. H-atom abstraction from P-H bond produces the phosphino radical, a seven electron species (Scheme 2). Phosphines with more than one P-H bond can react more than once to form the tertiary phosphines.\n\nThe steps proposed for metal-catalyzed hydrophosphinations typically include coordination of the primary or secondary phosphine to the metal, activation of the P-H bond (via oxidative addition or proton abstraction by an external base), P-C bond formation (through either insertion, Michael addition, or [2+2] cycloaddition), reductive elimination or addition of the abstracted proton and finally substitution of the newly made phosphine by the starting primary or secondary phosphine.\n\nMost early metal hydrophosphination catalysts are electron-deficient, \"d\" metal complexes. Catalysis typically involves inner-sphere P-C bond forming through metal-phosphido intermediates due to the nucleophilicity of the phosphido, being polarized by the metal.\n\nHydrophosphination of simple alkenes and alkynes is catalyzed by lanthanocene complexes. The catalytic cycle for the hydrophosphination of \"α,ω\"-pentenylphosphine is shown in Scheme 3. Typical of most electron-poor early metal catalysts for hydrophosphination, this one includes the characteristic P-H bond cleavage and P-C and C-H bond formation steps. The primary phosphine undergoes a \"σ\"-bond metathesis with the bis(trimethylsilyl)methylene ligand forming the lanthanide-phosphido. This is then followed by a 1,2-insertion of the pendant terminal alkene or alkyne on the phosphine into the Ln-P bond. Finally, protonolysis of the Ln-C bond with the starting primary phosphine releases the new phosphine and regenerates the catalyst. Given that the metal is electron-poor, the M-C bond is polar enough to be protonolyzed by the substrate primary phosphine; this is characteristic of electron-poor, early metals.\nSince many of the transition metal catalysts that are involved in hydrophosphination employ a phosphido intermediate, it is surprising that there are so few examples of catalysts that involve the metal-phosphinidene intermediate, M=PR. One such example is the Ti-catalyzed hydrophosphination of diphenylacetylene with phenylphosphine (Scheme 4) by Mindiola et al. This system involves a cationic catalyst precursor that is stabilized by the bulky 2,4,6-tri(isopropyl)phenyl- substituent on the phosphinidene and the close ionic association of methyltris(pentafluorophenyl)borate. This precursor undergoes exchange with phenylphosphine to make the titanium-phenylphosphinidene complex which is the catalyst. The Ti=PPh undergoes a [2+2] cycloaddition with diphenylacetylene to make the corresponding metallacyclobutene. The substrate, phenylphosphine, protonolyzes the Ti-C bond and after a proton shift regenerates the catalyst and releases the new phosphine.\n\nTitanium-catalyzed 1,4-hydrophosphination of 1,3-dienes with diphenylphosphine has been demonstrated (Scheme 5). It is a rare example of a non-\"d\" early transition metal catalyst that catalyzes hydrophosphination. In the first step, the Ti precursor undergoes an oxidative addition of the P-H bond in diphenylphosphine which generates the Ti catalyst. The rest of the catalysis involve the usual steps: P-C bond formation via 1,2-insertion followed by C-H bond formation and P-H bond activation via protonolysis with the substrate secondary phosphine.\n\nLate transition metal hydrophosphination catalysts generally require alkenes and alkynes with electron withdrawing substituents. This is due to the fact that mechanism for the P-C bond forming step usually involves nucleophilic attack of the phosphorus to the unsaturated carbon. Generally speaking, these metal complexes will first be bound by a primary or secondary phosphine, which then undergoes proton abstraction to make a nucleophilic phosphido ligand. The proton abstraction usually occurs by an external base.This base acts as a co-catalyst which is common to late transition metal hydrophosphination catalysts. This complex catalyzes hydrophosphination of enones (Scheme 6). The bound secondary phosphine becomes deprotonated by the base triethylamine, forming the phosphido. Following this the phosphido attacks the β-carbon of the enone, which is bound to the palladium through its oxygen atom. The abstracted proton from the secondary phosphine is delivered to the enone oxygen. Substitution by another secondary phosphine regenerates the catalyst and releases the 1,2-addition product. Other common characteristics of such late metal catalysts that do not require oxidative addition of the P-H bond are: the metal in a 2+ oxidation state, and cationic complexes. These two features are important given that a base is needed because being cationic and in an oxidation state of 2+ increases the acidity of the coordinated primary or secondary phosphine proton and allows for weaker bases/conjugate acids to be used in catalysis.\n\nSome late metal hydrophosphination catalysts rely on oxidative addition of a P-H bond. For example, a Pt(0) catalyst that undergoes oxidative addition of a secondary phosphine to form the corresponding Pt(II) phosphido complex (Scheme 7). Following this, alkenes such as acrylonitrile inserts into the Pt-P bond and then reductively eliminate to afford the hydrophosphinated acrylonitrile. This P-C bond forming step was found to occur through an outer-sphere, Michael-type addition because telomerization was occurring. The carbanion intermediate would attack another molecule of acrylonitrile affording a product that would be expected from many 1,2-insertions of the alkene.\n\nAs stated in the last example, the alkene inserted into the metal-phosphorus bond, but insertion into the metal-hydrogen bond is also possible (Scheme 7). The Ni(0) catalyst involves oxidation addition of a P-H bond to the metal, followed by insertion of the alkene into the M-H bond (Scheme 8). As a result of this, the product phosphine is produced through reductive elimination of a P-C bond rather than a P-H bond in Glueck's system.\n\nA prospective hydrophosphination catalyst utilizes a ruthenium(II) complex (Scheme 9). Like the other late metal catalysts that require a base co-catalyst, this complex requires a base to deprotonate the coordinated secondary phosphine, generating a nucleophilic phosphido complex. The following reactivity varies from the other late metal catalysts in that P-C bond forming step involves a [2+2] cycloaddition with alkenes and alkynes forming metallacycles. The Ru-C bond of the metallacycle is then protonolyzed by the conjugate acid of the base co-catalyst. The newly formed phosphine is then substituted by the starting secondary phosphine to regenerate the catalyst and release the new phosphine. What makes this catalyst unique is that it forms metallacycles, or rather P-C bonds, with strongly and mildly activated, simple and electron-rich alkenes and alkynes, whereas the other catalysts discussed are rather limited; generally, early metals catalyzing unactivated substrates and late metals predominantly activated ones.\n\nAs stated, characteristic of the late metal catalysts for hydrophosphination that require a base co-catalyst is that they are in the 2+ oxidation state and cationic. Thus, efforts are also underway to use the similar halide-free, cationic ruthenium(II) complex for catalysis.\n"}
{"id": "2736877", "url": "https://en.wikipedia.org/wiki?curid=2736877", "title": "Intelligent Small World Autonomous Robots for Micro-manipulation", "text": "Intelligent Small World Autonomous Robots for Micro-manipulation\n\nIntelligent Small World Autonomous Robots for Micro-manipulation (I-Swarm) is a European research project to develop millimeter-scale robots for dangerous activities. It is coordinated by Jörg Seyfried at the University of Karlsruhe in Germany. The robots operate on solar power and can communicate between each other.\n\nThe I-Swarm project aims to develop and produce a large scale swarm (up to 1,000) of microrobots.\n\nThe robots' proposed size is 2 x 2 x 1 mm. This small size means that sensory and computational capabilities will be limited. This is to be compensated for by collective behavior and emerging swarm effects.\nThe robots will differ in the type of sensors, manipulators and the amount of computational power.\n\nThe robot swarm is expected to have a variety of applications, including micro assembly, biological, medical or cleaning tasks.\n\n"}
{"id": "1945730", "url": "https://en.wikipedia.org/wiki?curid=1945730", "title": "Launch loop", "text": "Launch loop\n\nA launch loop or Lofstrom loop is a proposed system for launching objects into orbit using a moving cable-like system situated inside a sheath attached to the Earth at two ends and suspended above the atmosphere in the middle. The design concept was published by Keith Lofstrom and describes an active structure maglev cable transport system that would be around 2,000 km (1,240 mi) long and maintained at an altitude of up to 80 km (50 mi). A launch loop would be held up at this altitude by the momentum of a belt that circulates around the structure. This circulation, in effect, transfers the weight of the structure onto a pair of magnetic bearings, one at each end, which support it.\n\nLaunch loops are intended to achieve non-rocket spacelaunch of vehicles weighing 5 metric tons by electromagnetically accelerating them so that they are projected into Earth orbit or even beyond. This would be achieved by the flat part of the cable which forms an acceleration track above the atmosphere.\n\nThe system is designed to be suitable for launching humans for space tourism, space exploration and space colonization, and provides a relatively low 3\"g\" acceleration.\n\nLaunch loops were described by Keith Lofstrom in November 1981 Reader's Forum of the American Astronautical Society News Letter, and in the August 1982 L5 News.\n\nIn 1982, Paul Birch published a series of papers in \"Journal of the British Interplanetary Society\" which described orbital rings and described a form which he called Partial Orbital Ring System (PORS).\nThe launch loop idea was worked on in more detail around 1983–1985 by Lofstrom. It is a fleshed-out version of PORS specifically arranged to form a mag-lev acceleration track suitable for launching humans into space; but whereas the orbital ring used superconducting magnetic levitation, launch loops use electromagnetic suspension (EMS).\n\nA launch loop is proposed to be a structure 2,000 km long and 80 km high. The loop runs along at 80 km above the earth for 2000 km then descends to earth before looping back on itself rising back to 80 km above the earth to follow the reverse path then looping back to the starting point. The loop would be in the form of a tube, known as the \"sheath\". Floating within the sheath is another continuous tube, known as the \"rotor\" which is a sort of belt or chain. The rotor is an iron tube approximately 5 cm (2 inches) in diameter, moving around the loop at 14 km/s (31,000 miles per hour).\n\nWhen at rest, the loop is at ground level. The rotor is then accelerated up to speed. As the rotor speed increases, it curves to form an arc. The structure is held up by the force from the rotor, which attempts to follow a parabolic trajectory. The ground anchors force it to go parallel to the earth upon reaching the height of 80 kilometers.\nOnce raised, the structure requires continuous power to overcome the energy dissipated. Additional energy would be needed to power any vehicles that are launched.\n\nTo launch, vehicles are raised up on an 'elevator' cable that hangs down from the West station loading dock at 80 km, and placed on the track. The payload applies a magnetic field which generates eddy currents in the fast-moving rotor. This both lifts the payload away from the cable, as well as pulls the payload along with 3\"g\" (30 m/s²) acceleration. The payload then rides the rotor until it reaches the required orbital velocity, and leaves the track.\n\nIf a stable or circular orbit is needed, once the payload reaches the highest part of its trajectory then an on-board rocket engine (\"kick motor\") or other means is needed to circularize the trajectory to the appropriate Earth orbit.\n\nThe eddy current technique is compact, lightweight and powerful, but inefficient. With each launch the rotor temperature increases by 80 kelvins due to power dissipation. If launches are spaced too close together, the rotor temperature can approach 770 °C (1043 K), at which point the iron rotor loses its ferromagnetic properties and rotor containment is lost.\n\nClosed orbits with a perigee of 80 km quite quickly decay and re-enter, but in addition to such orbits, a launch loop by itself would also be capable of directly injecting payloads into escape orbits, gravity assist trajectories past the Moon, and other non closed orbits such as close to the Trojan points.\n\nTo access circular orbits using a launch loop a relatively small 'kick motor' would need to be launched with the payload which would fire at apogee and would circularise the orbit. For GEO insertion this would need to provide a delta-v of about 1.6 km/s, for LEO to circularise at 500 km would require a delta-v of just 120 m/s. Conventional rockets require delta-vs of roughly 14 and 10 km/s to reach GEO and LEO respectively.\n\nLaunch loops in Lofstrom's design are placed close to the equator and can only directly access equatorial orbits. However other orbital planes might be reached via high altitude plane changes, lunar perturbations or aerodynamic techniques.\n\nLaunch rate capacity of a launch loop is ultimately limited by the temperature and cooling rate of the rotor to 80 per hour, but that would require a 17 GW power station; a more modest 500 MW power station is sufficient for 35 launches per day.\n\nFor a launch loop to be economically viable it would require customers with sufficiently large payload launch requirements.\n\nLofstrom estimates that an initial loop costing roughly $10 billion with a one-year payback could launch 40,000 metric tons per year, and cut launch costs to $300/kg. For $30 billion, with a larger power generation capacity, the loop would be capable of launching 6 million metric tons per year, and given a five-year payback period, the costs for accessing space with a launch loop could be as low as $3/kg.\n\nCompared to space elevators, no new high-tensile strength materials have to be developed, since the structure resists Earth's gravity by supporting its own weight with the kinetic energy of the moving loop, and not by tensile strength.\n\nLofstrom's launch loops are expected to launch at high rates (many launches per hour, independent of weather), and are not inherently polluting. Rockets create pollution such as nitrates in their exhausts due to high exhaust temperature, and can create greenhouse gases depending on propellant choices. Launch loops as a form of electric propulsion can be clean, and can be run on geothermal, nuclear, wind, solar or any other power source, even intermittent ones, as the system has huge built-in power storage capacity.\n\nUnlike space elevators which would have to travel through the Van Allen belts over several days, launch loop passengers can be launched to low earth orbit, which is below the belts, or through them in a few hours. This would be a similar situation to that faced by the Apollo astronauts, who had radiation doses 200 times lower than the space elevator would give.\n\nUnlike space elevators which are subjected to the risks of space debris and meteorites along their whole length, launch loops are to be situated at an altitude where orbits are unstable due to air drag. Since debris does not persist, it only has one chance to impact the structure. Whereas the collapse period of space elevators is expected to be of the order of years, damage or collapse of loops in this way is expected to be rare. In addition, launch loops themselves are not a significant source of space debris, even in an accident. All debris generated has a perigee that intersects the atmosphere or is at escape velocity.\n\nLaunch loops are intended for human transportation, to give a safe 3\"g\" acceleration which the vast majority of people would be capable of tolerating well, and would be a much faster way of reaching space than space elevators.\n\nLaunch loops would be quiet in operation, and would not cause any sound pollution, unlike rockets.\n\nFinally, their low payload costs are compatible with large-scale commercial space tourism and even space colonisation.\n\nA running loop would have an extremely large amount of energy in its linear momentum. While the magnetic suspension system would be highly redundant, with failures of small sections having essentially no effect, if a major failure did occur the energy in the loop (1.5×10 joules or 1.5 petajoules) would be approaching the same total \"energy\" release as a nuclear bomb explosion (350 kilotons of TNT equivalent), although not emitting nuclear radiation.\n\nWhile this is a large amount of energy, it is unlikely that this would destroy very much of the structure due to its very large size, and because most of the energy would be deliberately dumped at preselected places when the failure is detected. Steps might need to be taken to lower the cable down from 80 km altitude with minimal damage, such as parachutes.\n\nTherefore, for safety and astrodynamic reasons, launch loops are intended to be installed over an ocean near the equator, well away from habitation.\n\nThe published design of a launch loop requires electronic control of the magnetic levitation to minimise power dissipation and to stabilise the otherwise under-damped cable.\n\nThe two main points of instability are the turnaround sections and the cable.\n\nThe turnaround sections are potentially unstable, since movement of the rotor away from the magnets gives reduced magnetic attraction, whereas movements closer gives increased attraction. In either case, instability occurs. This problem is routinely solved with existing servo control systems that vary the strength of the magnets. Although servo reliability is a potential issue, at the high speed of the rotor, very many consecutive sections would need to fail for the rotor containment to be lost.\n\nThe cable sections also share this potential issue, although the forces are much lower. However, an additional instability is present in that the cable/sheath/rotor may undergo meandering modes (similar to a Lariat chain) that grow in amplitude without limit. Lofstrom believes that this instability also can be controlled in real time by servo mechanisms, although this has never been attempted.\n\nIn works by Alexander Bolonkin it is suggested that Lofstrom's project has many non-solved problems and that it is very far from a current technology. For example, the Lofstrom project has expansion joints between 1.5 meter iron plates. Their speeds (under gravitation, friction) can be different and Bolonkin claims that they could wedge in the tube; and the force and friction in the ground 28 km diameter turnaround sections are gigantic. In 2008, Bolonkin proposed a simple rotated close-loop cable to launch the space apparatus in a way suitable for current technology.\n\nAnother project, the space cable, is a smaller design by John Knapman that is intended for launch assist for conventional rockets and suborbital tourism. The space cable design uses discrete bolts rather than a continuous rotor, as with the launch loop architecture. John Knapman has also mathematically shown that the meander instability can be tamed.\n\nThe skyhook is another launch system concept. Skyhook could be either rotating or non-rotating. The non-rotating skyhook hangs from a low Earth orbit down to just above the Earth's atmosphere (skyhook cable is not attached to Earth). The rotating skyhook changes this design to decrease the speed of the lower end; the entire cable rotates around its center of gravity. The advantage of this is an even greater velocity reduction for the launch vehicle flying to the bottom end of the rotating skyhook which makes for an even larger payload and a lower launch cost. The two disadvantages of this are: the greatly reduced time available for the arriving launch vehicle to hook up at the lower end of the rotating skyhook (approximately 3 to 5 seconds), and the lack of choice regarding the destination orbit.\n\n"}
{"id": "25616973", "url": "https://en.wikipedia.org/wiki?curid=25616973", "title": "List of Byzantine inventions", "text": "List of Byzantine inventions\n\nThis is a list of Byzantine inventions. The Byzantine or Eastern Roman Empire represented the continuation of the Roman Empire after a part of it collapsed. Its main characteristics were Roman state traditions, Greek culture and Christian faith.\n\n\n\n\n\n\n\n<br>\n"}
{"id": "304604", "url": "https://en.wikipedia.org/wiki?curid=304604", "title": "Mechatronics", "text": "Mechatronics\n\nMechatronics, which is also called mechatronic engineering, is a multidisciplinary branch of engineering that focuses on the engineering of both electrical and mechanical systems, and also includes a combination of robotics, electronics, computer, telecommunications, systems, control, and product engineering. As technology advances over time, various subfields of engineering have succeeded in both adapting and multiplying. The intention of mechatronics is to produce a design solution that unifies each of these various subfields. Originally, the field of mechatronics was intended to be nothing more than a combination of mechanics and electronics, hence the name being a combination of both mechanics and electronics; however, as the complexity of technical systems continued to evolve, the definition had been broadened to include more technical areas. \n\nThe word \"mechatronics\" originated in Japanese-English and was created by Tetsuro Mori, an engineer of Yaskawa Electric Corporation. The word \"mechatronics\" was registered as trademark by the company in Japan with the registration number of \"46-32714\" in 1971. However, afterward the company released the right of using the word to public, the word begun being used across the world. Nowadays, the word is translated in each language and the word is considered as an essential term for industry.\n\nFrench standard NF E 01-010 gives the following definition: \"approach aiming at the synergistic integration of mechanics, electronics, control theory, and computer science within product design and manufacturing, in order to improve and/or optimize its functionality\".\n\nMany people treat \"mechatronics\" as a modern buzzword synonymous with robotics and electromechanical engineering.\n\nA mechatronics engineer unites the principles of mechanics, electronics, and computing to generate a simpler, more economical and reliable system.\nThe term \"mechatronics\" was coined by Tetsuro Mori, the senior engineer of the Japanese company Yaskawa in 1969. An industrial robot is a prime example of a mechatronics system; it includes aspects of electronics, mechanics, and computing to do its day-to-day jobs.\n\nEngineering cybernetics deals with the question of control engineering of mechatronic systems. It is used to control or regulate such a system (see control theory). Through collaboration, the mechatronic modules perform the production goals and inherit flexible and agile manufacturing properties in the production scheme. Modern production equipment consists of mechatronic modules that are integrated according to a control architecture. The most known architectures involve hierarchy, polyarchy, heterarchy, and hybrid. The methods for achieving a technical effect are described by control algorithms, which might or might not utilize formal methods in their design. Hybrid systems important to mechatronics include production systems, synergy drives,\nplanetary exploration rovers, automotive subsystems such as anti-lock braking systems and spin-assist, and everyday equipment such as autofocus cameras, video, hard disks, and CD players.\n\nMechatronics students take courses in various fields:\n\n\n\nMechanical modeling calls for modeling and simulating physical complex phenomena in the scope of a multi-scale and multi-physical approach. This implies to implement and to manage modeling and optimization methods and tools, which are integrated in a systemic approach.\nThe specialty is aimed for students in mechanics who want to open their mind to systems engineering, and able to integrate different physics or technologies, as well as students in mechatronics who want to increase their knowledge in optimization and multidisciplinary simulation techniques.\nThe speciality educates students in robust and/or optimized conception methods for structures or many technological systems, and to the main modeling and simulation tools used in R&D. Special courses are also proposed for original applications (multi-materials composites, innovating transducers and actuators, integrated systems, …) to prepare the students to the coming breakthrough in the domains covering the materials and the systems.\nFor some mechatronic systems, the main issue is no longer how to implement a control system, but how to implement actuators. Within the mechatronic field, mainly two technologies are used to produce movement/motion.\n\nAn emerging variant of this field is biomechatronics, whose purpose is to integrate mechanical parts with a human being, usually in the form of removable gadgets such as an exoskeleton. This is the \"real-life\" version of cyberware.\n\nAnother variant that we can consider is Motion control for Advanced Mechatronics, which presently is recognized as a key technology in mechatronics. The robustness of motion control will be represented as a function of stiffness and a basis for practical realization. Target of motion is parameterized by control stiffness which could be variable according to the task reference. However, the system robustness of motion always requires very high stiffness in the controller.\n\nAvionics is also considered a variant of mechatronics as it combines several fields such as electronics and telecom with Aerospace Engineering.\n\nThe Internet of things (IoT) is the inter-networking of physical devices, embedded with electronics, software, sensors, actuators, and network connectivity which enable these objects to collect and exchange data.\n\nIoT and mechatronics are complementary. Many of the smart components associated with the Internet of Things will be essentially mechatronic. The development of the IoT is forcing mechatronics engineers, designers, practitioners and educators to research the ways in which mechatronic systems and components are perceived, designed and manufactured. This allows them to face up to new issues such as data security, machine ethics and the human-machine interface.\n\n\n\n\n"}
{"id": "3042527", "url": "https://en.wikipedia.org/wiki?curid=3042527", "title": "Mobility scooter", "text": "Mobility scooter\n\nA mobility scooter has a seat over three, four or now five wheels, a flat area or foot plates for the feet, and handlebars or a delta-style steering arrangement in front to turn one, two or three steerable wheels. The seat may swivel to allow access when the front is blocked by the handlebars. Mobility scooters are usually battery powered. A battery or two is stored on board the scooter and is charged via an onboard or separate battery charger unit from standard electric power. Gasoline-powered scooters may also be available in some countries, though they are rapidly being replaced by electric models. User-powered propelled by a lever used in a push-pull rowing motion to provide exercise and mobility at the same time.\n\nThe tiller, with forward/reverse directions and speed controls, is the steering column centrally located at the front of the scooter. The tiller may contain other features as well, for example, a speed limiter, lighting controls (for nighttime use) and turn signals. A battery use indicator is also often included. Forward/reverse direction can be controlled by thumb paddles, finger controls, or a switch. There are two types of mobility scooters: front-wheel drive (FD) or rear-wheel drive (RD). The front-wheel drive is usually a smaller device and is best used indoors. Rider weight capacity is a minimum of generally upwards to maximum. The rear-wheel drive is used both indoors and outdoors with rider weight capacity of . A heavy duty rear-drive can carry up to , varying by manufacturer.\n\nThe first crude mobility scooter was introduced in 1954 and was billed by Sears as an electric wheelchair, but it had more in common with mobility scooter with its large seat, extra large battery capacity and three-wheel design. It was not a commercial success.\n\nMobility scooters come in various types:\n\nUsually mid-range mobility scooters have a speed of about .\n\nAssistive and small sit-down electric mobility scooters provide important advantages to people with mobility problems throughout the world. A scooter is useful for persons without the stamina or arm/shoulder flexibility necessary to use a manual wheelchair. Also, swiveling the seat of an electric scooter is generally easier than moving the foot supports on most conventional wheelchairs. A mobility scooter is very helpful for persons with systemic or whole-body disabling conditions (coronary or lung issues, some forms of arthritis, obesity, etc.) who are still able to stand and walk a few steps, sit upright without torso support, and control the steering tiller.\n\nA major selling point of mobility scooters for many users is that they do not look like a wheelchair, disability still being seen by many as shameful. Mobility scooters are in general more affordable than powered wheelchairs, leading to them being procured as a cheaper alternative.\n\nRecently, manufacturers have been modifying the appearance of scooters to appeal to users. There are now mobility scooters which look like short, thin, small cars, and others that look very much like motorcycles.\n\nWhile a mobility scooter eliminates much of the manual strength problems of an unpowered wheelchair, its tiller steering mechanism still requires upright posture, shoulder and hand strength, and some upper-body mobility and strength. The arm-rest mounted controller typical of powerchair designs may be more suitable for many users. Scooters also have fewer options for body support, such as head or leg rests. They are rarely designed for ease of patient transfer from seat to bed.\n\nOther drawbacks include longer length, which limits their turning radius and ability to use some lifts or wheelchair-designed access technologies such as kneeling bus lifts. The longer length may also make it difficult to reach door-opener buttons or doorknobs. Some mobility scooter have low ground clearance which can make it difficult to navigate certain obstacles, such as travelling in cities without proper curb cuts. Navigating in restricted spaces, whether in the home or in public spaces and buildings can also be a problem.\n\nWhile new public buildings are usually designed with accessibility features, at least in North America, the longer length and wider turning radius may make it difficult to use them. This is a greater problem in older buildings which may have had to make compromises in retrofitting accessibility aids. For example, an elevator or lift may be adequate for a wheelchair, but too short for a mobility scooter. Hallways may be too narrow to make a right-angle turn. Or the \"privacy\" wall in most washrooms may restrict the entry so that the scooter cannot maneuver around it.\n\nThe weight minimum and limitations may be cause for concern as well with the minimum weight requirement being 170 lbs and the maximum being between 250-400 pounds depending on the make.\n\nThese limitations may prevent some disabled individuals from using scooters. In addition, scooter limitations may vary depending on model and manufacturer. A limitation of one make/model does not necessarily carry over to all. Individual needs may affect the suitability of a particular model. Four-wheel scooters have a larger turning radius in general, than a three-wheel scooter. In particular, a purchaser should compare length, width, turning radius and ground clearance to ensure the scooter will fit with most commonly encountered obstacles in the user's environment.\n\nCurrently in the United States, Medicare will not approve a power wheelchair for persons who do not need to use the chair \"inside their own home\", even if their medical needs restrict the use of a mobility scooter. For example, a person with severe arthritis of both shoulders and hands may not be the best candidate for a scooter, but because they can walk a few steps in their own home, such persons are not seen as approved candidates for a power wheelchair either. Various disability rights groups are campaigning for Medicare to change this policy. For those who do qualify for Medicare, they can reimburse up to 80% of the Medicare allowable value of the scooter.\n\nSuch restrictions are also applied in at least some Canadian provinces. For example, to be eligible for partial funding by the Ontario Assistive Devices Program, the user must require the scooter for use in their own home.\n\nSimilar restrictions on NHS powerchair provision exist in the UK, with manual wheelchairs prescribed for users with any ability to walk. This has led to many users who might be better served by a powerchair privately procuring a mobility scooter as a cheaper substitute.\n\nIn the UK mobility scooters are widely available with government subsidy under the Motability scheme. They are legally classified by the Use of Invalid Carriages on Highways Regulations 1988 as either Class II or Class III Invalid Carriages for legal purposes. A Class II scooter must be limited to for use on a footway only, while a Class III scooter must be limited to for road/highway use and have an additional 4 mph limiter for footway use. a government consultation is under way to determine how the law should adapt to increasing scooter use, whether higher road speeds should be allowed and on a replacement for the archaic term \"invalid carriage\". Due to concerns over safety issues and problems with bringing prosecutions against irresponsible users under existing laws, the consultation will also consider whether to make third party insurance mandatory, consider the introduction of compulsory training for users and discuss how to bring scooter users under wider road traffic legislations.\n\nA class 2 invalid carriage does not require registration. Class 3 invalid carriages may not be driven by a person under 14 years; they need to be registered with the DVLA and are subject to vehicle excise duty (\"car tax\"), though the rate is zero.\n\nDisability Essex told the Committee about four deaths resulting from mobility scooter accidents in one year in Essex alone, but there is little evidence to suggest that fatalities on this scale are replicated nationwide.\n\nIn Canada, mobility scooter users are classified as pedestrians and thus may legally use sidewalks for travel. This classification is based on wheel diameter. However, a mobility scooter weighs approximately without the driver and can cause serious injury or death. Users must use care while driving around pedestrians on sidewalks. There is some growing concern, and even hostility, about the use of mobility scooters in crowded urban areas.\n\nAn Australian senate enquirey into mobility scooters was announced in 2017, after John Williams' wife Nancy was injured by one.\n\n\n"}
{"id": "19714462", "url": "https://en.wikipedia.org/wiki?curid=19714462", "title": "Mountbatten Medal", "text": "Mountbatten Medal\n\nThe IET Mountbatten Medal is awarded annually for an outstanding contribution, or contributions over a period, to the promotion of electronics or information technology and their application. The Medal was established by the National Electronics Council in 1992 and named after Louis Mountbatten, The Earl Mountbatten of Burma, Admiral of the Fleet and Governor-General of India. Since 2011, the medal has been awarded as one of the IET Achievement Medals.\n\nOne of the IET's Prestige Achievement Medals, the Medal is awarded to an individual for an outstanding contribution, or contributions\nover a period, to the promotion of electronics or information technology and in the dissemination of the understanding of electronics and information technology to young people, or adults.\n\nIn selecting a winner, the Panel give particular emphasis to:\n\n"}
{"id": "14980843", "url": "https://en.wikipedia.org/wiki?curid=14980843", "title": "NASA Historical Advisory Committee", "text": "NASA Historical Advisory Committee\n\nThe National Aeronautics and Space Administration (NASA) Historical Advisory Committee was established in 1964.\n\nThe NASA Historical Office was established under its first chief historian, Dr. Eugene Emme in 1960. The committee was first made up of a wide variety of members, who initially sought to find support and prestige for the new historical program. From 1969-70, the committee began to be increasingly composed of professional historians from universities, who made known their dissatisfaction with the NASA historical program. As a result, the Historical Advisory Committee was reduced in size and reorganized to be composed of only university-based professional historians to oversee the work of the NASA Historical Office.\n\n\n\n"}
{"id": "49506167", "url": "https://en.wikipedia.org/wiki?curid=49506167", "title": "On Line Opinion", "text": "On Line Opinion\n\nOn Line Opinion, or Online Opinion, is an open access electronic journal, specialising in social and political debate. The journal is published in Australia, although content is not necessarily limited to Australian issues, and extends at times to publication in wider areas, such as, religion, ethics, and philosophy.\n\nOn Line Opinion was established in 1999 by political commentator Graham Young. Since then, On Line Opinion has become recognized as one of the leading examples of alternative journalism and citizen journalism. In 2012, the report of the Independent Inquiry into Media and Media Regulation by the Hon Raymond Antony Finkelstein QC stated that: \"While the start-up costs for new print publications have been prohibitive and inhibited new enterprises emerging, the streamlining of the relationship between content producers and consumers has led to many new websites and web-based services. Among the most important such websites that have grown up in Australia over the last decade are \"Inside Story\", \"Australian Policy Online\", Online Opinion, \"The Drum\", \"The Conversation\", and New Matilda.\" The journal is referenced by the Australian National Library, with .\n\nOn Line Opinion was founded as an experiment in participatory democracy, through the means of participatory journalism. The contributors to On Line Opinion constitute a wide range of writers, such as what might be described as high-profile contributors, including politicians and academics, and what might be called ordinary citizens, that is, those with no particular public profile. One of the motivations behind On Line Opinion was to address the limitations of academic publishing, and thus On Line Opinion aims to publish accessible material, sometimes called grey literature, on a more timely basis and with a wider audience, than would otherwise be the case. Research has suggested that, in comparative terms, publication within On Line Opinion is indeed more timely than is the case with other citizen journals, with on average five new articles per day appearing on the website.\n"}
{"id": "41580", "url": "https://en.wikipedia.org/wiki?curid=41580", "title": "Primary time standard", "text": "Primary time standard\n\nIn telecommunications, a primary time standard is a time standard that does not require calibration against another time standard. \n\nExamples of primary time, (\"i.e.\", frequency standards) are caesium standards and hydrogen masers. \n\nThe international second is based on the microwave frequency (9,192,631,770 Hz) associated with the atomic resonance of the hyperfine ground state levels of the caesium-133 atom in a magnetically neutral environment. Realizable caesium frequency standards use a strong electromagnet to deliberately introduce a magnetic field which overwhelms that of the Earth. The presence of this strong magnetic field introduces a slight, but known, increase in the atomic resonance frequency. However, very small variations in the calibration of the electric current in the electromagnet introduce minuscule frequency variations among different caesium oscillators.\n"}
{"id": "143830", "url": "https://en.wikipedia.org/wiki?curid=143830", "title": "Projectile point", "text": "Projectile point\n\nIn archaeological terms, a projectile point is an object that was hafted to weapon that was capable of being thrown or projected, such as a spear, dart, or arrow, or perhaps used as a knife. They are thus different from weapons presumed to have been kept in the hand, such as axes and maces, and the stone mace or axe-heads often attached to them. \n\nStone tools, including projectile points, can survive for long periods, were often lost or discarded, and are relatively plentiful, especially at archaeological sites, providing useful clues to the human past, including prehistoric trade. A distinctive form of point, identified though lithic analysis of the way it was made, is often a key diagnostic factor in identifying an archaeological industry or culture. Scientific techniques exist to track the specific kinds of rock or minerals that used to make stone tools in various regions back to their original sources. \n\nAs well as stone, projectile points were also made of worked bone, antler or ivory; all of these are less common in the Americas. In regions where metallurgy emerged, projectile points were eventually made from copper, bronze, or iron, though the change was by no means immediate. In North America, some late prehistoric points were fashioned from copper that was mined in the Lake Superior region and elsewhere.\n\nA large variety of prehistoric arrowheads, dart points, and spear points have been discovered. Flint, obsidian, quartz and many other rocks and minerals were commonly used to make points in North America. The oldest projectile points found in North America were long thought to date from about 13,000 years ago, during the Paleo-Indian period, however recent evidence suggests that North American projectile points may date to as old as 15,500 years. Some of the more famous Paleo-Indian types include Clovis, Folsom and Dalton points.\n\nProjectile points fall into two general types: dart/spear points, and arrow points. Larger points were used to tip spears and atlatl darts. Arrow points are smaller and lighter than dart points, and were used to tip arrows. The question of how to distinguish an arrow point from a point used on a larger projectile is non-trivial. According to some investigators, the best indication is the width of the hafting area, which is thought to correlate to the width of the shaft. An alternative approach is to distinguish arrow points by their necessarily smaller size (weight, length, thickness).\n\nProjectile points come in an amazing variety of shapes and styles, which vary according to chronological periods, cultural identities, and intended functions.\n\nTypological studies of projectile points have become more elaborate through the years. For instance, Gregory Perino began his categorical study of projectile point typology in the late 1950s. Collaborating with Robert Bell, he published a set of four volumes defining the known point types of that time. Perino followed this several years later with a three-volume study of \"Selected Preforms, Points and Knives of the North American Indians\". Another recent set of typological studies of North American projectile points has been produced by Noel Justice.\n\n\n\n"}
{"id": "52893764", "url": "https://en.wikipedia.org/wiki?curid=52893764", "title": "Purism, SPC", "text": "Purism, SPC\n\nPurism is a computer technology company based in South San Francisco, California and registered as a social purpose corporation in the state of Washington. Purism manufactures the Librem personal computing devices with a focus on software freedom, computer security and Internet privacy.\n\nPurism was founded in 2014 with the launch of a crowdfunding campaign for the Librem 15, notable for being the first attempt at custom-manufacturing an Intel-based high-end laptop for GNU/Linux with freedom-respecting components.\n\nWith the success of the initial crowdfunding campaign, a second campaign was made for a 13-inch model of the Librem, where an additional focus on privacy was given with hardware kill switches built into the product design. As the 15-inch model was still undergoing design and production, the kill switches feature was backported to it.\n\nThe two campaigns raised through the support of 1042 initial backers, which allowed the production and delivery of Librem laptops starting at the end of 2015.\n\nAs campaign orders have gradually been fulfilled, Purism announced that it would be transitioning from a purely build to order order fulfillment model to a build to stock model in 2017.\n\nIn February 2017, Purism transitioned to become a social purpose corporation, with its articles of incorporation detailing its social purpose. The move was announced in May 2017.\n\nOn August 24th 2017, Purism announced plans to build a smartphone, called the Librem 5. This phone, with launch delayed to early April 2019, will feature baseband processor separate from the main CPU and hardware kill switches for the baseband, Wi-Fi, Bluetooth, camera, and microphone. Further, this phone would run entirely free and open source software. Purism announced a crowdfunding effort for the phone that would run 60 days, intended to collect . Purism has reached its goal two weeks before the end of the campaign.\n\nPurism manages the development of PureOS, a fully free GNU/Linux distribution based on Debian. PureOS mostly ships with software from the Debian repositories but has all software that violates Purism's guidelines removed. Most notably, the Mozilla Firefox web browser is rebranded as PureBrowser and ships with plugins to enhance user privacy. PureOS received endorsement from the Free Software Foundation in December 2017. Although Librem laptops ship with PureOS by default and an optional Qubes OS USB drive, Purism ensure the laptop owners have the freedom to install any OS they like.\n\nLibrem line of laptops is the flagship product of Purism. The Librem laptops come in two sizes, the 13 and the 15 featuring a 13 and a 15 inch screen, respectively. All of these products ship with Purism's own operating system, PureOS, a derivative of Debian GNU/Linux, and an optional Qubes OS USB drive. Purism's products also feature hardware kill switches to allow the user to shut off the camera, Wi-Fi, Bluetooth, and the cellular modem on products that have one (or can be purchased air gapped). \n\nLibrem 11, sometimes referred to as Librem 10 or 12, is a proposed convertible tablet-to-laptop device. Librem 11 would have an 11-inch touch screen in an 11.6-inch body with a detachable keyboard and an optional docking station. Development on the device started in April 2016 and was suspended in October 2018.\n\nOn August 24th 2017, Purism launched a funding campaign for a $599 \"security and privacy focused phone\" Librem 5. The 60-day funding campaign aimed to collect collect , however the goal was surpassed two weeks early and concluded with US$2,677,609.10 raised, 78% over the target. The phone would run entirely free firmware and software: Debian derivative with a choice of desktop environments GNOME or KDE Plasma Mobile or Ubuntu Touch. Furthermore, unlike in any other phone, the baseband processor would be separated from the CPU main bus and instead connected via a fast USB interface, thus isolated from the main CPU RAM bus. Also, the phone would feature hardware kill switches for the isolated baseband processor, Wi-Fi, Bluetooth, camera, and microphone. \n\nLibrem Key is a hardware USB security token supporting multiple features, including integration with tamper-evident Heads BIOS that ensures Librem laptop BIOS was not maliciously altered since the last laptop launch, and one-time password storage (3x HOTP (<nowiki>RFC 4226</nowiki>), 15 x TOTP (<nowiki>RFC 6238</nowiki>)), integrated password manager (16 entries), 40 kbit/s true random number generator, and tamper-resistant smart card. The key supports type A USB 2.0, has dimensions of 48 x 19 x 7 mm, and weights 6 g.\n\n"}
{"id": "44529648", "url": "https://en.wikipedia.org/wiki?curid=44529648", "title": "Rachel Burnett", "text": "Rachel Burnett\n\nRachel Burnett, FBCS, CITP is a British expert on information technology law. \n\nAfter obtaining a degree in sociology at the University of Exeter, she worked in a range of roles including systems analysis and development and project management. During this time she studied law via a correspondence course and in 1985 she became a partner in an early IT legal practice. \n\nAfter partnerships in several IT legal practices, Burnett ran her own IT law company for seven years before becoming head of IT and intellectual property at Paris Smith LLP in Southampton, Hampshire.\n\nShe wrote a book, \"Outsourcing IT - The Legal Aspects\", published by Gower and co-authored \"Drafting and Negotiating Computer Contracts\" with Paul Klinger.\n\nIn 2007 to 2008. Burnett served as president of the British Computer Society (BCS). She has chaired the BCS Management Forum and sat on the BCS Security Committee. In addition, Burnett is a former chair of the Association of Women Solicitors and of the Institute for the Management of Information Systems. \n\nShe currently lectures part-time for the Open University in the United Kingdom.\n"}
{"id": "16537825", "url": "https://en.wikipedia.org/wiki?curid=16537825", "title": "Rapid Execution and Combat Targeting System", "text": "Rapid Execution and Combat Targeting System\n\nThe United States Air Force's Rapid Execution and Combat Targeting System (REACT) is a modification of the LGM-30 Minuteman launch control centers (LCC's) that provides continual monitoring and rapid retargeting of Minuteman ICBMs. It integrates communication systems and weapon systems into a single console.\n\nThere is now 1 launch key and three cooperative switches that must be turned simultaneously by the two missile combat crew members to initiate a launch vote. Prior to REACT, there were two keys.\n\nThe previous Minuteman III command and control system, designated Command Data Buffer or CDB, required over 20 hours to retarget the entire Minuteman force and 30 minutes to retarget a single ICBM. REACT system needs less than 10 hours to retarget all missiles, while individual missiles can be retargeted in matter of minutes.\n\nREACT was previously known as the \"ICBM integrated Electronics Upgrade\" (I2EU)\n\nREACT incorporates a secure communication link between LCC and Higher Authority (SACCS) to receive configuration and targeting data without any need for manual data input.\n\nAutomatic processing and decoding of Emergency War Orders is another feature which reduces missile crews' workload.\n\nThe REACT upgrades began around 1994. All Minuteman LCC's were upgraded.\n\n"}
{"id": "27649750", "url": "https://en.wikipedia.org/wiki?curid=27649750", "title": "Rizzoli &amp; Isles", "text": "Rizzoli &amp; Isles\n\nRizzoli & Isles is a TNT television series starring Angie Harmon as police detective Jane Rizzoli and Sasha Alexander as medical examiner Dr. Maura Isles. The one-hour drama is based on the series of \"Rizzoli & Isles\" novels by Tess Gerritsen. It premiered on July 12, 2010, and aired 105 episodes in seven seasons, concluding on September 5, 2016.\n\nThe series' backstory is inspired by the Maura Isles/Jane Rizzoli series of novels by Tess Gerritsen. Rizzoli appears in the series' first novel, \"The Surgeon\", and Isles is introduced in the second, \"The Apprentice\", which serves as the basis for the television series. Boston detective Jane Rizzoli has been investigating a serial killer named Charles Hoyt. Hoyt, who was banned from medical school for fondling a corpse, used his vast medical knowledge to systematically torture and kill people, usually choosing couples so that he could induce the most fear in his victims. Jane ascertains Hoyt's location, but as she is searching for him, she is hit in the back of the head and knocked unconscious. She is pinned to the floor by scalpels and awakes as Hoyt prepares to cut her throat, but Vince Korsak, Jane's partner, locates her and shoots Hoyt, saving her life. Jane, reasoning that Korsak would never trust her as his partner after seeing her so vulnerable, applies for a new partner.\n\nThe series pilot, \"See One. Do One. Teach One\", is largely based on the novel \"The Apprentice\". Jane and medical examiner Maura Isles investigate a killer with Hoyt's modus operandi plus an interest in necrophilia. Jane and Maura discover that the copycat was John Stark (Brendan McCarthy), a soldier who met Hoyt in medical school, had his identity erased for CIA black operations, and mimicked Hoyt's MO in a killing spree during said operations. Meanwhile, Hoyt escapes from prison and rejoins his apprentice to continue in his killing. Later, Jane's home is broken into, and she is told by someone posing as part of BPD that her neighbor had been killed; she rushes into the van to see the body but finds Hoyt instead. Hoyt and his apprentice knock her out and kidnap her. When she wakes up they attempt to kill her, but she manages to disarm them by Tasing them and burning Hoyt's eye with a flare. In self-defense, she shoots the apprentice to death, and, when Hoyt reaches for her gun, she shoots him through the hands, giving him injuries similar to the ones he gave her.\n\nNew twists are introduced when Jane and Maura discover that a recent murder victim is actually Maura's previously unknown half-brother, resulting in her discovery that her father is notorious criminal Patrick Doyle.\n\nHoyt returns 18 months later through another apprentice, Lola, in \"I'm Your Boogie Man\". Having murdered Lola's abusive husband two years earlier, Hoyt uses her Stockholm syndrome to his advantage and uses her to stalk Jane. Lola seduces and captures Frankie Rizzoli before tying Jane up. She plans to kidnap Jane until Hoyt can escape from prison, but Jane manages to distract Lola long enough for Frankie to kill Lola with her own revolver.\n\nHoyt returns again when he arranges for another inmate to be stabbed while he is dying of cancer, luring Jane into the prison so that he can taunt her about another unsolved murder he committed. While speaking to Jane in the prison infirmary, Hoyt tells her to look at what he is reading—Tess Gerritsen's novel \"The Silent Girl\". Although she manages to find the bodies of his victims—the family of an old college professor of Hoyt's who was unaware of his expulsion—Hoyt, aided by a third apprentice, manages to capture Jane and Maura, only to have Jane beat Hoyt and then stab him when he and his apprentice try to kill Maura.\n\nThe untitled project was on TNT's development slate as early as March 2008. In October 2009, TNT placed a cast-contingent pilot order under the original title, \"Rizzoli\". The pilot script was written by Janet Tamaro. Angie Harmon was the first actress cast, taking the title role of police detective Jane Rizzoli. Sasha Alexander won the role of medical examiner Dr. Maura Isles after auditioning with Harmon. Bruce McGill signed as Rizzoli's former partner, Sgt. Vince Korsak. Lee Thompson Young was cast as her new partner, Barry Frost. The role of Rizzoli's younger brother Frankie was filled by Jordan Bridges. Lorraine Bracco signed on as Rizzoli's mother, Angela. In early 2010, Billy Burke was announced as FBI agent Gabriel Dean.\n\nIn late January 2010, TNT green-lighted the pilot to series with the new title \"Rizzoli & Isles\". Ten episodes were ordered and the show premiered on July 12, 2010. The series is produced on the Paramount Pictures lot in Hollywood, California. Owing to a sponsorship deal between MillerCoors and Turner Broadcasting for the summer 2010 season, the series included product placement for MGD 64, including billboards in the backgrounds of some scenes. The first season was additionally sponsored by Vonage. After having aired three episodes in 2010, the series was renewed for a second season, which aired from July 11 to December 26, 2011. In August 2011, TNT ordered a third season, which aired from June 5 through December 25, 2012. On June 29, 2012, TNT ordered a fourth season, which aired from June 25, 2013, through March 18, 2014. On December 9, 2014, TNT renewed \"Rizzoli & Isles\" for an 18-episode sixth season.\n\nIn August 2013, production on the fourth season was suspended when cast member Lee Thompson Young was found dead at his Los Angeles apartment from a self-inflicted gunshot wound. In the second half of the fifth season, a new character, crime scene analyst Nina Holiday, played by Idara Victor, was introduced. Series executive producer Jan Nash stated that Nina would not replace Young's character, Frost, but is her own character. Instead, the existing characters were given \"new dynamics\", according to Nash. For example, Jane and Korsak work more cases together, and Frankie is more involved and has more access.\n\n\n\n\"Rizzoli & Isles\" started to air on GEM in Australia on . The series started to air on Super Channel in Canada on . As of 2016, Superchannel is no longer carrying the show.\n\nThe series moved to Showcase in Canada on , on TVNorge in Norway on , M-Net Series in South Africa on , and on Warner TV in Singapore on .\n\n\"Rizzoli & Isles\" started to air on Alibi in the United Kingdom on . A subscription is required. The series second season premiered on .\n\nThe series started strong in the ratings, as 7.55 million viewers tuned in for the premiere episode. \"Rizzoli & Isles\" was the second most-watched cable program on the evening of July 12, 2010, behind its lead-in, \"The Closer\", which had 110,000 more viewers. The show finished the week in third behind \"The Closer\" and the final episode of \"Deadliest Catch\".\n\nThe premiere set a record as the highest-rated debut for a commercial-supported cable series, and it was the second-highest debut ever for a basic cable TV series. \"Rizzoli & Isles\" is second only to the 2008 premiere of \"Raising the Bar\", which attracted 7.7 million viewers during its commercial-free debut. Live + 7 day ratings for the premiere updated the show's status as the all-time most-watched cable series launch, with DVR viewers increasing the show's rating to just over 9 million viewers.\n\nThe show has ranked in the top five cable programs all five seasons and was the number one basic cable program in its fifth season.\n\nThe show has been described as having lesbian undertones, and the purported sexual attraction between Rizzoli and Isles has been the subject of critical and fan attention, with sites such as \"The Advocate\" connecting the show's lesbian subtext to its popularity. Harmon said that she was not surprised by the attention and that, while it was \"'super fun' to play a role that has some same-sex romantic vibes\", the characters are \"straight\" and \"just best friends\". Alexander said that she was not initially aware of the subtext but believed it reflected the characters' chemistry. She also expressed skepticism that Rizzoli and Isles' friendship was out of the ordinary.\n\n"}
{"id": "230613", "url": "https://en.wikipedia.org/wiki?curid=230613", "title": "Scythe", "text": "Scythe\n\nA scythe (, ) is an agricultural hand tool for mowing grass or reaping crops. It has largely been replaced by horse-drawn and then tractor machinery, but is still used in some areas of Europe and Asia.\n\nThe word \"scythe\" derives from Old English \"siðe\". In Middle English and after it was usually spelt \"sithe\" or \"sythe\". However, in the 15th century some writers began to use the \"sc-\" spelling as they thought (wrongly) the word was related to the Latin \"scindere\" (meaning \"to cut\"). Nevertheless, the \"sithe\" spelling lingered and notably appears in Noah Webster's dictionaries.\n\nA scythe consists of a shaft about long called a \"snaith\", \"snath\", \"snathe\" or \"sned\", traditionally made of wood but now sometimes metal. Simple snaiths are straight with offset handles, others have an \"S\" curve or are steam bent in three dimensions to place the handles in an ergonomic configuration but close to shaft. The snaith has either one or two short handles at right angles to it, usually one near the upper end and always another roughly in the middle. The handles are usually adjustable to suit the user. A curved, steel blade between long is mounted at the lower end at 90°, or less, to the snaith. Scythes almost always have the blade projecting from the left side of the snaith when in use, with the edge towards the mower; left-handed scythes are made but cannot be used together with right-handed scythes as the left-handed mower would be mowing in the opposite direction and could not mow in a team.\n\nThe use of a scythe is traditionally called \"mowing\", now often \"scything\" to distinguish it from machine mowing. The mower holds the top handle in the left hand and the central one in the right, with the arms straight, the blade parallel and very close to the ground and the uncut grass to the right. The body is then twisted to the right, the blade hooks the grass and is swung steadily to the left in a long arc ending in front of the mower and depositing the cut grass neatly to the left. The mower takes a small step forward and repeats the motion, proceeding with a steady rhythm, stopping at frequent intervals to hone the blade. The correct technique has a slicing action on the grass, cutting a narrow strip with each stroke, leaving a uniform stubble on the ground and forming a regular windrow on the left.\n\nThe mower moves along the mowing-edge with the uncut grass to the right and the cut grass laid in a neat row to the left, on the previously mown land. Each strip of ground mown by a scythe is called a \"swathe\" (pronounced : rhymes with \"bathe\") or \"swath\" (: rhymes with \"Goth\"). Mowing may be done by a team of mowers, usually starting at the edges of a meadow then proceeding clockwise and finishing in the middle. Mowing grass is easier when it is damp, and so hay-making traditionally began at dawn and often stopped early, the heat of the day being spent raking and carting the hay cut on previous days or peening the blades.\n\nScythes are designed for different tasks. A long, thin blade is most efficient for mowing grass or wheat, while a shorter, more robust scythe is more appropriate for clearing weeds, cutting reed or sedge and can be used with the blade under water for clearing ditches and waterways. Skilled mowers using traditional long-bladed scythes honed very sharp were used to maintain short lawn grass until the invention of the lawnmower. Many cultures have used a variety of 'cradles' to catch cut different kinds of grain stems, keeping the seed heads aligned and laying them down in an orderly fashion to make them easier to sheaf and winnow.\n\nMowing with a scythe is a skilled task made to look easy by experienced mowers but needs time to learn the skill. Long-bladed traditional scythes, typically around (such as in the example below) and suitable for mowing grass or wheat are harder to use at first, consequently, beginners usually start on shorter blades, say or less. Common beginner's errors include: setting up the snaith with the handles in the wrong locations to suit the body, setting the blade at the wrong turn-in and turn-up angles to suit the conditions, choosing a blade that is too long for the skill level, failing to start with a sharp edge and persevering with a dull one during use, chopping or hacking at the grass, trying to cut too wide a strip of grass at once and striking the ground with the blade. Traditionally, beginners relied on mentors to help them set up and maintain their scythe and to teach them to mow comfortably and efficiently.\n\nThe following photographs by Avraham Pisarek in 1945 show a man mowing rye with a scythe.\nThe cutting edge of a tensioned scythe blade is traditionally maintained by occasional peening followed by frequent honing. Peening reforms the malleable edge, by hammering, to create the desired profile, to work harden the metal for improved edge stability, and to remove minor nicks and dents. For mowing fine grass the bevel angle may be peened extremely fine, while for coarser work a larger angle is created to give a more robust edge. Peening requires some skill and is done using a peening hammer and special anvils or by using a peening jig. Historically, a peening station was set up on the edge of the field during harvest but now more likely back in the workshop.\n\nIn the example below, a short scythe blade, being used to clear brambles, is being sharpened. Before going to the forest the blade is peened back in the workshop; this reforms the malleable steel to create an edge profile that can then be honed. Peening is done only occasionally; how often depends on the hardness of the steel and the nature of the work. The Austrian blade shown is being used to cut tough-stemmed brambles and it is being peened about every thirty hours of work. Nicks and cuts to the blade edge can usually be worked out of the blade by peening and a new edge profile formed for honing.\nA peening jig is being used here but blades can be free-peened using various designs of peening anvils. The peening jig shown has two interchangeable caps that set different angles; a coarse angle is set first about 3 mm back from the edge and the fine angle is then set on the edge, leaving an edge that lends itself to being easily honed. The blade is then honed using progressively finer honing stones and then taken to the field. In the field the blade is honed using a fine, ovoid whetstone (or \"rubber\"), fine-grained for grass, coarser for cereal crops. Honing is done the moment the mower senses that the edge has gone off; this may be every half hour or more depending on the conditions. The laminated honing stone shown here has two grades of stone and is carried into the field soaking in a water-filled holster on the belt. A burr is set up on the outside of the blade by stroking the blade on the inside, the burr is then taken off by gently stroking it on the outside. There are a great many opinions, regional traditions and variations on exactly how to do this; some eastern European countries even set up the burr on the inside.\n\nUnlike continental European blades, typical American, English, and Nordic style blades are made of harder steel and are not usually peened for risk of cracking them. The harder blade holds an edge longer and requires less frequent honing in the field but after heavy use or damage the edge must be reshaped by grinding. Because of the greater wear resistance of the hard steel, and the reduced need for honing as a result, this usually only needs to be done around 1–3 times a season. Many examples have a laminated construction with a hard, wear-resistant core providing the edge and softer sides providing strength. In American and English blades the edge steel is typically clad on either side with the tough iron, while some Nordic laminated blades have a layer of iron on the top only, with the edge steel comprising the bottom layer.\n\nThe scythe may have dated back as far as , and seems to have been used in Ancient Rome, but became more common in Europe with agricultural developments during the Carolingian era (8th century AD). Initially used mostly for mowing hay, it had replaced the sickle for reaping crops by the 16th century as the scythe was better ergonomically and consequently more efficient. In about 1800 the \"grain cradle\", was sometimes added to the standard scythe when mowing grain; the cradle was an addition of light wooden fingers above the scythe blade which kept the grain stems aligned and the heads together to make the collection and threshing easier. In the developed world the scythe has largely been replaced by the motorised lawn mower and combine harvester. However, the scythe remained in common use for many years after the introduction of machines because a side-mounted finger-bar mower, whether horse or tractor drawn, could not mow in front of itself and scythes were still needed to \"open up\" a meadow by clearing the first swathe to give the mechanical mower room to start.\n\nThe Dictionary of Greek and Roman Antiquities of Sir William Smith claims that the scythe, known in Latin as the \"falx foenaria\" as opposed to the sickle, the \"falx messoria\", was used by the ancient Romans. According to ancient Greek mythology, Gaia the Greek goddess and mother of the Titans gave a sickle made out of the strongest metal to her youngest son Kronos, who is also the youngest of the Titans and god of the harvest, to seek vengeance against her husband Ouranos for torturing their oldest sons. To illustrate this, Smith cites an image of Saturn holding a scythe, from an ancient Italian cameo. The Grim Reaper and the Greek Titan Cronus were often depicted carrying or wielding a scythe. According to Jack Herer and \"Flesh of The Gods\" (Emboden, W.A., Jr., Praeger Press, New York, 1974), the ancient Scythians grew hemp and harvested it with a hand reaper that we still call a scythe.\n\nThe Abbeydale Industrial Hamlet in Sheffield, England is a museum of a scythe-making works that was in operation from the end of the 18th century until the 1930s. This was part of the former scythe-making district of north Derbyshire, which extended into Eckington. Other English scythe-making districts include that around Belbroughton.\n\nThe German Renaissance scythe sword, the Greek and Roman harpe and the Egyptian khopesh were scythes or sickles modified as weapons or symbols of authority. An improvised conversion of the agricultural scythe to a war scythe by re-attaching the blade parallel to the snaith, similar to a bill has also been used throughout history as a weapon. See \"Scythes in art\" below for an example.\n\nThe scythe is still an indispensable tool for farmers in developing countries and in mountainous terrain.\n\nIn Romania, for example, in the highland landscape of the Transylvanian Apuseni mountains, scything is a very important annual activity, taking about 2–3 weeks to complete for a regular house. As scything is a tiring physical activity and is relatively difficult to learn, farmers help each other by forming teams. After each day's harvest, the farmers often celebrate by having a small feast where they dance, drink and eat, while being careful to keep in shape for the next day's hard work. In other parts of the Balkans, such as in Serbian towns, scything competitions are held where the winner takes away a small silver scythe. In small Serbian towns, scything is treasured as part of the local folklore, and the winners of friendly competitions are rewarded richly with food and drink, which they share with their competitors.\n\nAmong Basques scythe-mowing competitions are still a popular traditional sport, called \"segalaritza\" (from Spanish verb \"segar\": to mow). Each contender competes to cut a defined section of grown grass before his rival does the same.\n\nThere is an international scything competition held at Goricko where people from Austria, Hungary, Serbia and Romania, or as far away as Asia appear to showcase their culturally unique method of reaping crops. In 2009, a Japanese gentleman showcased a wooden reaping tool with a metal edge, which he used to show how rice was cut. He was impressed with the speed of the local reapers, but said such a large scythe would never work in Japan.\n\nThe Norwegian municipality of Hornindal has three scythe blades in its coat-of-arms.\n\nScythes are beginning a comeback in American suburbs, since they \"don't use gas, don't get hot, don't make noise, do make for exercise, and do cut grass\".\n\nWinslow Homer, \"The Veteran in a New Field' 1865'\n\n\n"}
{"id": "14205596", "url": "https://en.wikipedia.org/wiki?curid=14205596", "title": "Shaheen-III", "text": "Shaheen-III\n\nThe Shaheen-III (Urdu: ; lit. \"White Falcon-III\") is a land-based surface-to-surface medium range ballistic missile, which was test fired for the first time by military service on 9 March 2015.\n\nDevelopment began in secrecy in the early 2000s in response to India's \"Agni-III\", \"Shaheen\" was successfully tested on 9 March 2015 with 2750 km (1700 mi) range, which could enable it to reach all corners of India and reach deep into the Middle East parts of North Africa. This missile, according to a former Director General of Pakistan’s Strategic Plans Division, is designed to reach Indian islands so that India cannot use them as “strategic bases” to establish a “second strike capability.”\n\nThe \"Shaheen\" program is composed of the solid-fuel system in a contrast to \"Ghauri\" program that is primarily based on liquid-fuel system. With the successful launch of the \"Shaheen\"', it surpasses the range of \"Shaheen-II\"— hence it is the longest-range missile to be launched by the military.\n\nUS Air Force National Air and Space Intelligence Center estimates that as of June 2017 this missile type was not yet operationally deployed.\n\nThe range of the Shaheen-3 is sufficient to target all of mainland India from launch positions in most of Pakistan to the south of Islamabad. But apparently, the missile was developed to do more than that. According to Gen. Kidwai, the range of 2750 km was determined by a need to be able to target the Nicobar and Andaman Islands in the eastern part of the Indian Ocean that are “developed as strategic bases” where “India might think of putting its weapons”. But for a 2750-km range Shaheen-3 to reach the Andaman and Nicobar Islands, it would need to be launched from positions in the very Eastern parts of Pakistan, close to the Indian border. If deployed in the Western parts of the Balochistan province, the range of the Shaheen-3 would for the first time bring Israel within range of Pakistani nuclear missiles.\n\nIn 2000, the Space Research Commission concluded at least two design studies for its space launch vehicle. Initially, there were two earlier designs were shown in IDEAS held in 2002 and its design was centered on developing a space booster based on the design technologies of the Shaheen-I. Since then, Shaheen owes its existence largely to the joint efforts led by NDC of NeScom and Space Research Commission.\n\nThe \"Shaheen-III\" was shrouded in top secrecy and very little information was available to the public, mostly provided in 2002 IDEAS. Majority of the efforts and funding was being made available to \"Ghauri-III\" to seek strike in Eastern region of India. In May 2000, the \"Ghauri-III\" was cancelled due to its less advance and lack of technological gain. Despite strong advocacy by Abdul Qadeer Khan for the Ghauri-III program made to be feasible, the program was terminated by then-President Pervez Musharraf who made the funding available for \"Shaheen-III\" program which was to be led under Samar Mubarakmand. The Air Force, however, pressed for \"Shaheen-III\" to make it feasible as liquids were being developed that would allow the missiles to be left in a ready-to-shoot form for extended periods.\n\nThe \"Shaheen-III\" was initially purposed as the space booster for the space program to make it possible for installing the satellite payload applications. Despite its efforts, the existence of \"Shaheen-III\" continued to be speculated in news media as Pakistan Ministry of Defence and the Joint Staff HQ nor confirms or deny the existence of the program.\n\nIn a press conference held in Lahore in 2009, Samar Mubarakmand stated that: \"Pakistan would launch its own satellite in April 2011.\" Although no confirmation or denial of \"Shaheen\" program's existence was given by Dr. Mubarakmand, the rumors and speculations yet to be continued for the existence of the program.\n\nAfter years of speculations, the \"Shaheen-III\" was eventually revealed and tested on 9 March 2015 with a 2750 km (1700-mile) range.\n\nIt uses WS51200 transporter erector launcher TEL manufactured in China by Wanshan Special Vehicle.\n\nOn 9 March 2015, the ISPR released a press statement on notifying the successful testing of the Shaheen-III that was conducted from the southern coast off the Arabian Sea.\n\nMilitary officials from JS HQ, SPD scientists and engineers, oversaw the launch of the system and witnessed the impact point in the Arabian Sea. Reports summed up by NTI, there had been series of testings taken place of the rocket engine nozzles before the eventual tests took place in 2015.\n\nSeveral Pakistani nuclear and military strategists reportedly quoted that the \"Shaheen-III has a range greater than that of any other missile system in-service. Earlier testings of \"Shaheen-II\" had the maximum range of about 2,500km, which meant it can reach all parts of India, even the eastern frontier.\n\nAir Marshal Shahid Latif, a retired senior commander in the Pakistan Air Force, noted the strategic significance of missile: \"Now, India doesn’t have its safe havens anymore. It's all a reaction to India, which has now gone even for tests of extra-regional missiles. It sends a [very] loud message: If you hurt us, we are going to hurt you back.!\"\n\nMansoor Ahmad, a professor of Strategic studies at the Islamabad's Quaid-i-Azam University, stated that: \"Pakistan's military, however, is not interested in a \"tit-for-tat\" arms race with India.\" and speculated that developmental work may be under progress to make missile capable of delivering multiple warheads which would make them harder to defend against.\n\nIn a views of political scientist, dr. Farrukh Saleem, the \"Shaheen-III\" seems to be a reaction to Indian-generated threats. Dr. Saleem, on the other hand, stressed that: \"Pakistan does not seem to be aiming at competing with India but Pakistan's aims seem to revolve around the creation of a credible deterrence, and a credible deterrence is bound to strengthen strategic stability.\"\n\nDr. Farrukh Saleem's views were also echoed by Mansoor Ahmad who maintained that: \"Pakistan hopes to improve \"existing capabilities,\" including new delivery systems for evading an Indian missile defense shield.\n\n\n"}
{"id": "43320329", "url": "https://en.wikipedia.org/wiki?curid=43320329", "title": "Single instruction, multiple threads", "text": "Single instruction, multiple threads\n\nSingle instruction, multiple thread (SIMT) is an execution model used in parallel computing where single instruction, multiple data (SIMD) is combined with multithreading.\n\nThe processors, say a number of them, seem to execute many more than tasks. This is achieved by each processor having multiple \"threads\" (or \"work-items\" or \"Sequence of SIMD Lane operations\"), which execute in lock-step, and are analogous to SIMD lanes.\n\nThe SIMT execution model has been implemented on several GPUs and is relevant for general-purpose computing on graphics processing units (GPGPU), e.g. some supercomputers combine CPUs with GPUs.\n\nSIMT was introduced by Nvidia:\n\nATI Technologies (now AMD) released a competing product slightly later on May 14, 2007, the TeraScale 1-based \"R600\" GPU chip.\n\nAs access time of all the widespread RAM types (e.g. DDR SDRAM, GDDR SDRAM, XDR DRAM, etc.) is still relatively high, engineers came up with the idea to hide the latency that inevitably comes with each memory access. Strictly, the latency-hiding is a feature of the zero-overhead scheduling implemented by modern GPUs. This might or might not be considered to be a property of 'SIMT' itself.\n\nSIMT is intended to limit instruction fetching overhead, i.e. the latency that comes with memory access, and is used in modern GPUs (such as those of Nvidia and AMD) in combination with 'latency hiding' to enable high-performance execution despite considerable latency in memory-access operations. This is where the processor is oversubscribed with computation tasks, and is able to quickly switch between tasks when it would otherwise have to wait on memory. This strategy is comparable to multithreading in CPUs (not to be confused with multi-core).\n\nA downside of SIMT execution is the fact that thread-specific control-flow is performed using \"masking\", leading to poor utilization where a processor's threads follow different control-flow paths. For instance, to handle an \"IF\"-\"ELSE\" block where various threads of a processor execute different paths, all threads must actually process both paths (as all threads of a processor always execute in lock-step), but masking is used to disable and enable the various threads as appropriate. Masking is avoided when control flow is coherent for the threads of a processor, i.e. they all follow the same path of execution. The masking strategy is what distinguishes SIMT from ordinary SIMD, and has the benefit of inexpensive synchronization between the threads of a processor.\n\n"}
{"id": "5914496", "url": "https://en.wikipedia.org/wiki?curid=5914496", "title": "Spin-transfer torque", "text": "Spin-transfer torque\n\nCharge carriers (such as electrons) have a property known as spin which is a small quantity of angular momentum intrinsic to the carrier. An electric current is generally unpolarized (consisting of 50% spin-up and 50% spin-down electrons); a spin polarized current is one with more electrons of either spin. By passing a current through a thick magnetic layer (usually called the “fixed layer”), one can produce a spin-polarized current. If this spin-polarized current is directed into a second, thinner magnetic layer (the “free layer”), angular momentum can be transferred to this layer, changing its orientation. This can be used to excite oscillations or even flip the orientation of the magnet. The effects are usually only seen in nanometer scale devices.\n\nSpin-transfer torque can be used to flip the active elements in magnetic random-access memory. Spin-transfer torque magnetic random-access memory (STT-RAM or STT-MRAM) has the advantages of lower power consumption and better scalability over conventional magnetoresistive random-access memory (MRAM) which uses magnetic fields to flip the active elements . Spin-transfer torque technology has the potential to make possible MRAM devices combining low current requirements and reduced cost; however, the amount of current needed to reorient the magnetization is at present too high for most commercial applications, and the reduction of this current density alone is the basis for present academic research in spin electronics.\n\nHynix Semiconductor and Grandis formed a partnership in April 2008 to explore commercial development of STT-RAM technology. \n\nHitachi and Tohoku University demonstrated a 32-Mbit STT-RAM in June 2009.\n\nOn August 1, 2011, Grandis announced that it had been purchased by Samsung Electronics for an undisclosed sum.\n\nIn 2011, Qualcomm presented a 1 Mbit Embedded STT-MRAM, manufactured in TSMC's 45 nm LP technology at the Symposium on VLSI Circuits.\n\nIn May 2011, Russian Nanotechnology Corp. announced an investment of $300 million in Crocus Nano Electronics (a joint venture with Crocus Technology) which will build an MRAM factory in Moscow, Russia.\n\nIn 2012 Everspin Technologies released the first commercially available DDR3 dual in-line memory module ST-MRAM which has a capacity of 64 Mb. \n\nOther companies working on STT-RAM include Avalanche Technology, Crocus Technology and Spin Transfer Technologies.\n\n\n"}
{"id": "4386247", "url": "https://en.wikipedia.org/wiki?curid=4386247", "title": "Spudger", "text": "Spudger\n\nA spudger (or sometimes spludger) is a tool that has a wide flat-head screwdriver-like end that extends as a wedge, used to separate pressure-fit plastic components without causing damage during separation. The flat end of the spudger is often used to loosen or release components inside electronics, for example during the replacement of batteries or touch screens for smartphones. The other end is often a point or a hook depending on application. When applied to separate pressure-fit panels, there is often a point to create an initial gap before the wedge end is utilized.\n\nA spudger is also a wiring tool used for poking or adjusting small wires or components, generally in the electronics and telecommunications industries. A typical spudger is an insulating stick, made of either wood, plastic or a nylon fiberglass material. For instances where the spudger is used for prying it is commonly made of stainless steel or other metals.\n\nThe most common modern spudger is a black or yellow nylon stick with a metal hook at one end. Various versions have blunt, sharpened, or insulated hooks. The hook can be used for pulling bridge clips from 66 blocks, manipulating wires in a crowded wire wrap block, or setting DIP switches. The body of a plastic spudger is usually contoured to offer a better grip. Some spudgers are made of orangewood, used in electronics assembly and soldering because of its heat tolerance and dense grain. The same orangewood sticks are commonly used in filmmaking, manicure and pedicure, but these industries do not use the term \"spudger\".\n\nIn telecom applications like punch-down terminal blocks and cell phone repair, the spudger is made of a non-conductive material to prevent transmission of a static shock or direct short to sensitive electrical components' inputs or outputs. This is critical with high density applications where uninsulated terminals are in close proximity, like a battery or with telephone patch junctions.\n\nThe spudger is called a non-marring nylon black stick tool or simply black stick in many electronics repair manuals, where it is the recommended tool for prying open certain laptops, audio file players, keyboards, LCDs and other tight fitting electronic enclosures and assemblies.\n\n"}
{"id": "2240363", "url": "https://en.wikipedia.org/wiki?curid=2240363", "title": "Television antenna", "text": "Television antenna\n\nA television antenna, or TV aerial, is an antenna specifically designed for the reception of over-the-air broadcast television signals, which are transmitted at frequencies from about 41 to 250 MHz in the VHF band, and 470 to 960 MHz in the UHF band in different countries. Television antennas are manufactured in two different types: \"indoor\" antennas, to be located on top of or next to the television set, and \"outdoor\" antennas, mounted on a mast on top of the owner's house. They can also be mounted in a loft or attic, where the dry conditions and increased elevation are advantageous for reception and antenna longevity. Outdoor antennas are more expensive and difficult to install, but are necessary for adequate reception in fringe areas far from television stations. The most common types of indoor antennas are the dipole (\"rabbit ears\") and loop antennas, and for outdoor antennas the yagi, log periodic, and for UHF channels the multi-bay reflective array antenna.\n\nIn most countries, television broadcasting is allowed in the Very high frequency (VHF) band from 47 to 68 MHz, called VHF low band or band I in Europe; 174 to 216 MHz, called VHF high band or band III in Europe, and in the ultra high frequency (UHF) band from 470 to 698 MHz, called band IV and V in Europe. The boundaries of each band vary somewhat in different countries. \n\nTo cover this range, antennas generally consist of multiple conductors of different lengths depending on the wavelength of the radio waves they receive. The elements of most antennas are half-wave dipoles; metal rods half the wavelength of the signal they are intended to receive. The wavelength of a signal equals the speed of light (c) divided by the frequency. The above range of frequencies is too wide to be covered by a single antenna, so often separate antennas for the VHF and UHF bands are used; or a combined antenna that has both VHF and UHF elements mounted on the same boom. During the last decade many countries in the world switched from broadcasting using an older analog television standard to newer digital television (DTV). However generally the same broadcast frequencies are used, so the same antennas used for the older analog television will also receive the new DTV broadcasts. Sellers often claim to supply a special \"digital\" or \"high-definition television\" (HDTV) antenna advised as a replacement for an existing analog television antenna; at best this is misinformation to generate sales of unneeded equipment, at worst it may leave the viewer with a UHF-only antenna in a local market (particularly in North America) where some digital stations remain on their original high VHF frequencies.\n\nIndoor antennas may be mounted on the television itself or stand on a table next to it, connected to the television by a short feedline. Due to space constraints indoor antennas cannot be as large and elaborate as outdoor antennas, and they are not mounted at as high an elevation; for these reasons indoor antennas generally do not give as good reception as outdoor antennas. They are often perfectly adequate in urban and suburban areas which are usually within the strong radiation \"footprint\" of local television stations, but in rural fringe reception areas only an outdoor antenna may give adequate reception. A few of the simplest indoor antennas are described below, but a great variety of designs and types exist. Many have a dial on the antenna with a number of different settings to alter the antenna's reception pattern. This should be rotated with the set on while looking at the screen, until the best picture is obtained. \n\nThe oldest and most widely used indoor antenna is the \"rabbit ears\" or \"bunny ears\", which are often provided with new television sets. It is a simple half-wave dipole antenna used to receive the VHF television bands, consisting in the US of 52 to 88 MHz (band I) and 174 to 216 MHz (band III), with wavelengths of 5.5 to 1.4 m. It is constructed of two telescoping rods attached to a base, which extend out to about 1 meter length (approximately one quarter wavelength at 52 MHz), and can be collapsed when not in use. For best reception the rods should be adjusted to be a little less than 1/4 wavelength at the frequency of the television channel being received. However the dipole has a wide bandwidth, so often adequate reception is achieved without adjusting the length. The half wave dipole has a low gain of about 2.14 dBi; this means it is not as directional and sensitive to distant stations as a large rooftop antenna, but its wide angle reception pattern may allow it to receive several stations located in different directions without requiring readjustment when the channel is changed. Dipole antennas are bi-directional, that is, they have two main lobes in opposite directions, 180° apart. Instead of being fixed in position like other antennas, the elements are mounted on ball-and-socket joints and can be adjusted to various angles in a \"V\" shape, allowing them to be moved out of the way in crowded quarters. Another reason for the V shape is that when receiving channels at the top of the band with the rods fully extended, the antenna elements will typically resonate at their 3rd harmonic. In this mode the direction of maximum gain (the main lobe) is no longer perpendicular to the rods, but the radiation pattern will have lobes at an angle to the rods, making it advantageous to be able to adjust them to various angles\n\nSome portable televisions use a whip antenna. This consists of a single telescoping rod about a meter long attached to the television, which can be retracted when not in use. It functions as a quarter-wave monopole antenna. The other side of the feedline is connected to the ground plane on the TV's circuit board, which acts as ground. The whip antenna generally has an omnidirectional reception pattern, with maximum sensitivity in directions perpendicular to the antenna axis, and gain similar to the half-wave dipole.\n\nThe UHF channels are often received by a single turn loop antenna. Since a \"rabbit ears\" antenna only covers the VHF bands, it is often combined with a UHF loop mounted on the same base to cover all the TV channels.\n\nSoon after television broadcasting switched from analog to digital broadcasting, indoor antennas have evolved beyond the traditional \"rabbit ears.\" RCA is one manufacturer which has commercially sold a flat antenna. Flat antennas are very lightweight, very thin, and square-shaped like a thin notebook. They connect to televisions, or to digital converter boxes, with a single coaxial cable, and may be sold with an optional signal amplifier. Internally, the thin, flat square is a loop antenna, with its circular metallic wiring embedded into conductive plastic. The amplifier must be plugged into a power source, but the flat antenna does not require a power source. The flat antenna may need some moving around to achieve an optimum reception, but it eliminates a lot of manipulation which is inherent in use of the \"rabbit ears\".\n\nAn outdoor TV antenna is a high-gain directional antenna often needed to achieve adequate reception in fringe reception areas, greater than 15 miles from the television station. Outdoor antennas have a unidirectional radiation pattern so the correct end of the antenna must be pointed at the TV station. The received television signal passes down a feed line (transmission line) into the house to the television. Older antennas used flat 300 ohm twin-lead cable. This had to be kept several inches away from metal objects such as the antenna tower or gutters, so it had to be mounted on standoff insulators. Modern antennas use 50 ohm RG-6 coaxial cable which attaches to the television with a type F connector. \n\nOutdoor antenna designs are often based on the Yagi-Uda antenna or log-periodic dipole array (LPDA). These are composed of multiple half-wave dipole elements, consisting of metal rods approximately half of the wavelength of the television signal, mounted in a line on a support boom. These act as resonators; the electric field of the incoming radio wave pushes the electrons in the rods back and forth, creating standing waves of oscillating voltage in the rods. The antenna can have a smaller or larger number of rod elements; in general the more elements the higher the gain. Another design, used mainly for UHF reception, is the reflective array antenna, consisting of a vertical metal screen with multiple dipole elements mounted in front of it. \n\nThe television broadcast bands are too wide in frequency to be covered by a single antenna, so either separate antennas are used for the VHF and UHF bands, or a combination (combo) VHF/UHF antenna. A VHF/UHF antenna is really two antennas feeding the same feedline mounted on the same support boom. Longer elements which pick up VHF frequencies are located at the \"back\" of the boom and often function as a log-periodic antenna. Shorter elements which receive the UHF stations are located at the \"front\" of the boom and often function as a Yagi antenna.\n\nOutdoor antennas are highly directional. They have a narrow main lobe; that is, their maximum sensitivity (gain) is only achieved over a narrow angle along their axis, so they must be pointed at the transmitting antenna. This presents a problem when the television stations to be received are located in different directions. In this case two or more directional rooftop antennas each pointed at a different transmitter are often mounted on the same mast and connected to one receiver. An alternative is to use a single antenna mounted on a \"rotator\"; a remote servo system that rotates the antenna to a new direction when a dial next to the television is turned. \n\nSometimes television transmitters are organised such that all receivers in a given location need receive transmissions in only a relatively narrow band of the full UHF television spectrum and from the same direction, so that a single antenna provides reception from all stations.\n\nAntennas are commonly placed on rooftops, and sometimes in attics. Placing an antenna indoors significantly attenuates the level of the available signal. Directional antennas must be pointed at the transmitter they are receiving; in most cases great accuracy is not needed. In a given region it is sometimes arranged that all television transmitters are located in roughly the same direction and use frequencies spaced closely enough that a single antenna suffices for all. A single transmitter location may transmit signals for several channels. CABD (communal antenna broadcast distribution) is a system installed inside a building to receive free-to-air TV/FM signals transmitted via radio frequencies and distribute them to the audience.\n\nAnalog television signals are susceptible to ghosting in the image, multiple closely spaced images giving the impression of blurred and repeated images of edges in the picture. This is due to the signal being reflected from nearby objects (buildings, tree, mountains); several copies of the signal, of different strengths and subject to different delays, are picked up. This is different for different transmissions. Careful positioning of the antenna can produce a compromise position which minimizes the ghosts on different channels. Ghosting is also possible if multiple antennas connected to the same receiver pick up the same station, especially if the lengths of the cables connecting them to the splitter/merger are different lengths or the antennas are too close together. Analog television is being replaced by digital, which is not subject to ghosting but is far more prone to interference; the same reflected signal that causes ghosting in an analog signal would produce no viewable content at all in digital.\n\nAerials are attached to roofs in various ways, usually on a pole to elevate it above the roof. This is generally sufficient in most areas. In some places, however, such as a deep valley or near taller structures, the antenna may need to be placed significantly higher, using a guide mast or mast. The wire connecting the antenna to indoors is referred to as the \"\" or \"drop\", and the longer the downlead is, the greater the signal degradation in the wire. Certain cables may help reduce this tendency. \n\nThe higher the antenna is placed, the better it will perform. An antenna of higher gain will be able to receive weaker signals from its preferred direction. Intervening buildings, topographical features (mountains), and dense forest will weaken the signal; in many cases the signal will be reflected such that a usable signal is still available. There are physical dangers inherent to high or complex antennas, such as the structure falling or being destroyed by weather. There are also varying local ordinances which restrict and limit such things as the height of a structure without obtaining permits. For example, in the United States, the Telecommunications Act of 1996 allows any homeowner to install \"An antenna that is designed to receive local television broadcast signals\", but that \"masts higher than 12 feet above the roof-line may be subject to local permitting requirements.\"\n\nAs discussed previously, antennas may be placed indoors where signals are strong enough to overcome antenna shortcomings. The antenna is simply plugged into the television receiver and placed conveniently, often on the top of the receiver (\"set-top\"). Sometimes the position needs to be experimented with to get the best picture. Indoor antennas can also benefit from RF amplification, commonly called a TV booster. Indoor antennas will never be an option in weak signal areas.\n\nSometimes it is desired not to put an antenna on the roof; in these cases, antennas designed for outdoor use are often mounted in the attic or loft, although antennas designed for attic use are also available. Putting an antenna indoors significantly decreases its performance due to lower elevation above ground level and intervening walls; however, in strong signal areas reception may be satisfactory. One layer of asphalt shingles, roof felt, and a plywood roof deck is considered to attenuate the signal to about half.\n\nIt is sometimes desired to receive signals from transmitters which are not in the same direction. This can be achieved, for one station at a time, by using a rotator operated by an electric motor to turn the antenna as desired. Alternatively, two or more antennas, each pointing at a desired transmitter and coupled by appropriate circuitry, can be used. To prevent the antennas from interfering with each other, the vertical spacing between the booms must be at least half the wavelength of the lowest frequency to be received (Distance=λ/2). The wavelength of 54 MHz (Channel 2) is 5.5 meters (λ x f = c) so the antennas must be a minimum of 2.25 metres, or about 89 inches apart. It is also important that the cables connecting the antennas to the signal splitter/merger be exactly the same length, to prevent phasing issues, which cause ghosting with analog reception. That is, the antennas might both pick up the same station; the signal from the one with the shorter cable will reach the receiver slightly sooner, supplying the receiver with two pictures slightly offset. There may be phasing issues even with the same length of down-lead cable. Bandpass filters or \"signal traps\" may help to reduce this problem.\n\nFor side-by-side placement of multiple antennas, as is common in a space of limited height such as an attic, they should be separated by at least one full wavelength of the lowest frequency to be received at their closest point.\n\nOften when multiple antennas are used, one is for a range of co-located stations and the other is for a single transmitter in a different direction.\n\n\n\n"}
{"id": "14229991", "url": "https://en.wikipedia.org/wiki?curid=14229991", "title": "Timeline of Irish inventions and discoveries", "text": "Timeline of Irish inventions and discoveries\n\nIrish inventions and discoveries are objects, processes or techniques which owe their existence either partially or entirely to an Irish person. Often, things which are discovered for the first time, are also called \"inventions\", and in many cases, there is no clear line between the two. Below is a list of such inventions.\n\n\n\n\n\n\n\n"}
{"id": "558010", "url": "https://en.wikipedia.org/wiki?curid=558010", "title": "Toner", "text": "Toner\n\nToner is a powder mixture used in laser printers and photocopiers to form the printed text and images on the paper, in general through a toner cartridge. Mostly granulated plastic, early mixtures only added carbon powder and iron oxide, however mixtures have since been developed containing polypropylene, fumed silica, and various minerals for triboelectrification. Toner using plant-derived plastic also exists as an alternative to petroleum plastic. Toner particles are melted by the heat of the fuser, and are thus bonded to the paper.\n\nIn earlier photocopiers, this low-cost carbon toner was poured by the user from a bottle into a reservoir in the machine. Later copiers, and laser printers from the first 1984 Hewlett-Packard LaserJet, feed directly from a sealed toner cartridge.\n\nLaser toner cartridges for use in color copiers and printers come in sets of cyan, magenta, yellow and black (CMYK), allowing a very large color gamut to be generated by mixing.\n\nThe specific polymer used varies by manufacturer but can be a styrene acrylate copolymer, a polyester resin, a styrene butadiene copolymer, or a few other special polymers. Toner formulations vary from manufacturer to manufacturer and even from machine to machine. Typically formulation, granule size and melting point vary the most.\n\nOriginally, the particle size of toner averaged 14–16 micrometres or greater. To improve image resolution, particle size was reduced, eventually reaching about 8–10 micrometers for 600 dots per inch resolution. Further reductions in particle size producing further improvements in resolution are being developed through the application of new technologies such as Emulsion-Aggregation. Toner manufacturers maintain a quality control standard for particle size distribution in order to produce a powder suitable for use in their printers.\n\nToner has traditionally been made by compounding the ingredients and creating a slab which was broken or pelletized, then turned into a fine powder with a controlled particle size range by air jet milling. This process results in toner granules with varying sizes and aspherical shapes. To get a finer print, some companies are using a chemical process to grow toner particles from molecular reagents. This results in more uniform size and shapes of toner particles. The smaller, uniform shapes permit more accurate colour reproduction and more efficient toner use.\n\nToner can be washed off skin and garments with cold water. Hot or warm water softens the toner, causing it to bond in place. Toner fused to skin eventually wears off, or can be partially removed using an abrasive hand cleaner. Toner fused to clothing usually cannot be removed.\n\nToner particles have electrostatic properties by design and can develop static-electric charges when they rub against other particles, objects, or the interiors of transport systems and vacuum cleaner hoses. Because of this and the small particle size, toner should not be vacuumed with a conventional home vacuum cleaner. Static discharge from charged toner particles theoretically may ignite dust in the vacuum cleaner bag or create a small explosion if sufficient toner is airborne. Toner particles are so fine that they are poorly filtered by household vacuum cleaner filter bags and can blow through the vacuum motor into the room. So they also can cause overheat by clogging the motor filter and short circuit by their electric conductivity (carbon, iron) when they melt inside the motor.\n\nIf toner spills into the laser printer, a special type of vacuum cleaner with an electrically conductive hose and a high efficiency (HEPA) filter may be needed for effective cleaning. These are called electrostatic discharge-safe (ESD-safe) or toner vacuums. Similar HEPA-filter equipped vacuums should be used for clean-up of larger toner spills.\n\nUnfused toner is easily cleaned from most water-washable clothing. Because toner is a wax or plastic powder with a low melting temperature, it must be kept cold while cleaning. The washing machine should be filled with cold water before adding the garment. Two complete wash cycles improve the chances of success. The first may use hand wash dish detergent, the second may use regular laundry detergent. Residual toner floating in the rinse water of the first cycle will remain in the garment and may cause permanent graying. A clothes dryer or iron should not be used until all toner has been removed.\n\nAs a fine powder, toner can remain suspended in the air for some period, and is considered to have health effects comparable to inert dust. It can be an irritant to people with respiratory conditions such as asthma or bronchitis. Following studies on bacteria in the 1970s that raised concerns about health effects resulting from pyrrole, a contaminant created during manufacture of the carbon black used in black toner, manufacturing processes were changed to eliminate pyrrole from the finished product.\n\nResearch by the Queensland University of Technology has indicated that some laser printers emit submicrometer particles which have been associated in other environmental studies with respiratory diseases .\n\nA study at the University of Rostock has found that the microscopic particles in toner are carcinogenic, similar to asbestos. Several technicians who had been working with printers and copiers on a daily basis were observed for several years. They showed increased lung problems. This confirms previous research published in 2006.\n\nThe toner container can be a simple pack, for toner storage and transportation, or further, a consumable component of the printer.\nThe most common way to consume toner is with a toner cartridge (or \"laser toner\"), \nas an office supply of a laser printers.\n\nSeveral toner manufacturers offer toner in wholesale quantities. Typically, bulk loose toner is sold in barrels or 10 kg (22-pound) bags.\n\nToner is then used by a variety of industries in order to provide consumers with a finished laser toner cartridge.\n\nOriginal Equipment manufacturers such as HP and Canon as well as manufacturers of compatible toner cartridges use the toner in the process of manufacturing a brand new OEM cartridge.\nRemanufacturers of toner cartridges use the bulk toner in the process of creating a remanufactured toner cartridge. Other companies use the toner to provide a toner refill service.\n\nMost toner cartridges are available to the average consumer through retail outlets or local remanufacturing operations. Remanufactured and refilled toner cartridges are generally offered at a lower cost than original toner cartridges, having been either wholly remanufactured and then refilled with toner (the more-optimal method) or just refilled with toner (the less-optimal method)..\n\nRecycling of pre-consumer waste toner is practiced by most manufacturers. Classifying toner to the desired size distribution produces off-size rejects, but these become valuable feedstocks for the compounding operation, and are recycled this way. Post-consumer waste toner appears primarily in the cleaning operation of the photo-printing machine. In early printers, as much as 20 to 25% of feed toner would wind up in the cleaner sump and be discarded as waste. Improved printer efficiencies have reduced this waste stream to lower levels, although on average 13% of the toner in each cartridge is still wasted. Some printer designs have attempted to divert this waste toner back into the virgin toner reservoir for direct reuse in the printer; these attempts have met with mixed success as the composition of the toner will change by expending fusibles while retaining developer particles. Some consideration and fewer industry attempts have been made to reclaim waste toner by cleaning it and \"remanufacturing\" it.\n\nMost toner goes to printed pages, a large fraction of which are ultimately recycled in paper recovery and recycling operations. Removal of toner from the pulp is not easy, and toner formulations to ease this step have been reported. Hydrolyzable, water-soluble, and caustic-soluble toner resins have been reported, but do not appear to enjoy widespread application. Most paper recycling facilities mix toner with other waste material, such as inks and resins, into a sludge with no commercial use.\n\nIn the UK, large compatible ink cartridge manufacturers like Jet Tec & Dubaria have implemented toner recycling programs in order to receive back empty cartridges for refilling of HP, Lexmark, Dell, etc. cartridges, as no compatible version is readily available.\n\n\n"}
{"id": "2751096", "url": "https://en.wikipedia.org/wiki?curid=2751096", "title": "Unstructured data", "text": "Unstructured data\n\nUnstructured data (or unstructured information) is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in fielded form in databases or annotated (semantically tagged) in documents.\n\nIn 1998, Merrill Lynch cited a rule of thumb that somewhere around 80-90% of all potentially usable business information may originate in unstructured form. This rule of thumb is not based on primary or any quantitative research, but nonetheless is accepted by some. Other sources have reported similar or higher percentages of unstructured data.\n\nIDC and EMC project that data will grow to 40 zettabytes by 2020, resulting in a 50-fold growth from the beginning of 2010. More recently, IDC and Seagate predict that the global datasphere will grow to 163 zettabytes by 2025 and majority of that will be unstructured.The Computer World magazine states that unstructured information might account for more than 70%–80% of all data in organizations.\n\nThe earliest research into business intelligence focused in on unstructured textual data, rather than numerical data. As early as 1958, computer science researchers like H.P. Luhn were particularly concerned with the extraction and classification of unstructured text. However, only since the turn of the century has the technology caught up with the research interest. In 2004, the SAS Institute developed the SAS Text Miner, which uses Singular Value Decomposition (SVD) to reduce a hyper-dimensional textual space into smaller dimensions for significantly more efficient machine-analysis. The mathematical and technological advances sparked by machine textual analysis prompted a number of businesses to research applications, leading to the development of fields like sentiment analysis, voice of the customer mining, and call center optimization. The emergence of Big Data in the late 2000s led to a heightened interest in the applications of unstructured data analytics in contemporary fields such as predictive analytics and root cause analysis.\n\nThe term is imprecise for several reasons:\n\nTechniques such as data mining, natural language processing (NLP), and text analytics provide different methods to find patterns in, or otherwise interpret, this information. Common techniques for structuring text usually involve manual tagging with metadata or part-of-speech tagging for further text mining-based structuring. The Unstructured Information Management Architecture (UIMA) standard provided a common framework for processing this information to extract meaning and create structured data about the information.\n\nSoftware that creates machine-processable structure can utilize the linguistic, auditory, and visual structure that exist in all forms of human communication. Algorithms can infer this inherent structure from text, for instance, by examining word morphology, sentence syntax, and other small- and large-scale patterns. Unstructured information can then be enriched and tagged to address ambiguities and relevancy-based techniques then used to facilitate search and discovery. Examples of \"unstructured data\" may include books, journals, documents, metadata, health records, audio, video, analog data, images, files, and unstructured text such as the body of an e-mail message, Web page, or word-processor document. While the main content being conveyed does not have a defined structure, it generally comes packaged in objects (e.g. in files or documents, …) that themselves have structure and are thus a mix of structured and unstructured data, but collectively this is still referred to as \"unstructured data\". For example, an HTML web page is tagged, but HTML mark-up typically serves solely for rendering. It does not capture the meaning or function of tagged elements in ways that support automated processing of the information content of the page. XHTML tagging does allow machine processing of elements, although it typically does not capture or convey the semantic meaning of tagged terms.\n\nSince unstructured data commonly occurs in electronic documents, the use of a content or document management system which can categorize entire documents is often preferred over data transfer and manipulation from within the documents. Document management thus provides the means to convey structure onto document collections.\n\nSearch engines have become popular tools for indexing and searching through such data, especially text.\n\nSpecific computational workflows have been developed to impose structure upon the unstructured data contained within text documents. These workflows are generally designed to handle sets of thousands or even millions of documents, or far more than manual approaches to annotation may permit. Several of these approaches are based upon the concept of online analytical processing, or OLAP, and may be supported by data models such as text cubes. Once document metadata is available through a data model, generating summaries of subsets of documents (i.e., cells within a text cube) may be performed with phrase-based approaches.\n\nBiomedical research generates one major source of unstructured data as researchers often publish their findings in scholarly journals. Though the language in these documents is challenging to derive structural elements from (e.g., due to the complicated technical vocabulary contained within and the domain knowledge required to fully contextualize observations), the results of these activities may yield links between technical and medical studies and clues regarding new disease therapies. Recent efforts to enforce structure upon biomedical documents include self-organizing map approaches for identifying topics among documents, general-purpose unsupervised algorithms, and an application of the CaseOLAP workflow to determine associations between protein names and cardiovascular disease topics in the literature. CaseOLAP defines phrase-category relationships in an accurate (identifies relationships), consistent (highly reproducible), and efficient manner. This platform offers enhanced accessibility and empowers the biomedical community with phrase-mining tools for widespread biomedical research applications.\n\n\n\n"}
{"id": "2530148", "url": "https://en.wikipedia.org/wiki?curid=2530148", "title": "Wetware computer", "text": "Wetware computer\n\nA wetware computer is an organic computer (which can also be known as an artificial organic brain or a neurocomputer) composed of organic material such as living neurons. Wetware computers composed of neurons are significantly different than conventional computers because they are thought to be capable in a way of \"thinking for themselves\", because of the dynamic nature of neurons. While wetware is still largely conceptual, there has been limited success with construction and prototyping, which has acted as a proof of the concept's realistic application to computing in the future. The most notable examples of prototyping have stemmed from the research completed by biological engineer William Ditto during his time at the Georgia Institute of Technology. His work constructing a simple neurocomputer capable of basic addition from leech neurons in 1999 was a significant discovery for the concept. This research acted as a primary example driving interest in the creation of these artificially constructed, but still organic brains.\n\nThe concept of wetware is an application of specific interest to the field of computer manufacturing. A specific application of wetware of interest in the computer industry is in regards to Moore’s law. This observation by Gordon Moore which states that the number of transistors, which can be placed on a silicon chip, is doubled roughly every two years. Moore's law has acted as a goal for the industry for decades, but as the size of computers continues to get smaller, the ability to meet this goal has become much more difficult, threatening to reach a plateau. Due to the difficulty in reducing the size of computers because of size limitations of transistors and integrated circuits wetware provides an unconventional alternative. A wetware computer composed of neurons is an ideal concept because, unlike conventional materials which operate in binary (on/ off) a neuron can shift between thousands of states, constantly altering its chemical conformation, and redirecting electrical pulses through over 200,000 channels in any of its many synaptic connections. Because of this large difference in the possible settings for any one neuron compared to the binary limitations of conventional computers the space limitations are far less.\n\nThe word wetware is a distinct and unconventional concept which draws slight resonance with both hardware and software from conventional computers. While “hardware” is understood as the physical architecture of tradition computational devices, built from the electrical circuitry and silicone plates, software is the conceptual opposite of hardware, it represents the encoded architecture of storage and instructions. Wetware is a separate concept which utilizes the formation of organic molecules, mostly complex cellular structures (such as neurons) to create a computational device such as a computer. In wetware the idea of hardware and software are intertwined and interdependent. The molecular, and chemical composition of the organic or biological structure would represent not only the physical structure of the wetware but also the \"software\", being continually reprogrammed by the discrete shifts in electrical pulses and chemical concentration gradients as the molecules change their structures to communicate signals. The responsiveness of a cell, proteins, and molecules to changing conformations both within their own structures and around them tie the idea of internal programming, and external structure together in a way which is completely alien to the current model of conventional computer architecture.\n\nThe structure of wetware represents a model where the external structure and internal programming are interdependent and unified; meaning that changes to the programming or internal communication between molecules of the device would represent a physical change in the structure. The dynamic nature of wetware borrows from the function of complex cellular structures in biological organisms. The combination of “hardware” and “software” into one dynamic, and interdependent system which utilizes organic molecules and complexes to create an unconventional model for computational devices is a specific example of applied biorobotics.\n\nCells in many ways can be seen as their own form of naturally occurring wetware, similar to the concept that the human brain is the preexisting model system for complex wetware. In his book \"Wetware: A Computer in Every Living Cell\" (2009) Dennis Bray explains his theory that cells, which are the most basic form of life, are just a highly complex computational structure, like a computer. To simplify one of his arguments a cell can be seen as a type of computer, utilizing its own structured architecture. In this architecture, much like a traditional computer many smaller components operate in tandem to receive input, process the information, and compute an output. In an overly simplified, and non-technical analysis cellular function can be broken into the following components. Information and instructions for execution are stored as DNA in the cell, RNA acts as a source for distinctly encoded input which processed by ribosomes and other transcription factors to access and process the DNA and to output a protein. Bray's argument in favor of viewing cells and cellular structures as models of natural computational devices is important when considering the more applied theories of wetware in relation to biorobotics.\n\nWetware and biorobotics are closely related concepts, which both borrow from similar overall principles. A biorobotic structure can be defined as a system modeled from a preexisting organic complex or model such as cells (neurons) or more complex structures like organs (brain) or whole organisms. Unlike wetware the concept of biorobotics is not always a system composed of organic molecules, but instead could be composed of conventional material which is designed and assembled in a structure similar or derived from a biological model. Biorobotics have many applications, and are used to address the challenges of conventional computer architecture. Conceptually, designing a program, robot, or computational device after a preexisting biological model such as a cell, or even a whole organism provides the engineer or programmer the benefits of incorporating into the structure the evolutionary advantages of the model.\n\nIn 1999 William Ditto and his team of researchers at Georgia Institute of technology and Emory University created a basic form of a wetware computer capable of simple addition by harnessing leech neurons. Leeches were used as a model organism due to the large size of their neuron, and the ease associated with their collection and manipulation.The computer was able to complete basic addition through electrical probes inserted into the neuron. The manipulation of electrical currents through neurons was not a trivial accomplishment, however. Unlike conventional computer architecture which is based on the binary on/ off states, neurons are capable of existing in thousands of states and communicate with each other through synaptic connections which each contain over 200,000 channels. Each can be dynamically shifted in a process called \"self-organization\" to constantly form and reform new connections. A conventional computer program called the \"dynamic Clamp\" was written by Eve Marder, a neurobiologist at Brandeis University that was capable of reading the electrical pulses from the neurons in real times, and interpreting them. This program was used to manipulate the electrical signals being input into the neurons to represent numbers, and to communicate with each other to return the sum. While this computer is a very basic example of a wetware structure it represents a small example with fewer neurons than found in a more complex organ. It is thought by Ditto that by increasing the amount of neurons present the chaotic signals sent between them will \"self-organize\" into a more structured pattern, such as the regulation of heart neurons into a constant heartbeat found in humans and other living organisms.\n\nAfter his work creating a basic computer from leech neurons Ditto continued to work not only with organic molecules and wetware, but also on the concept of applying the chaotic nature of biological systems and organic molecules to conventional material and logic gates. Chaotic systems have advantages for generating patterns and computing higher order functions like memory, arithmetic logic, input/output operations. In his article \"Construction of a Chaotic Computer Chip\" Ditto discusses the advantages in programming of using chaotic systems, with their greater sensitivity to respond and reconfigure logic gates in his conceptual chaotic chip. The main difference between a chaotic computer chip, and a conventional computer chip is the reconfigurability of the chaotic system. A traditional computer chip, where a programmable gate array element must be reconfigured through the switching of many single-purpose logic gates, a chaotic chip is able to reconfigure all logic gates through the control of the pattern generated by the non-linear chaotic element.\n\nThe concept of cognitive biology evaluates cognition as a basic biological function. W. Tecumseh Fitch, a professor of cognitive biology at the University of Vienna is a leading theorist on ideas of cellular intentionality. The idea that not only do whole organisms have a sense of \"aboutness\" of intentionality, but that single cells also carry a sense of intentionality through cells ability to adapt and reorganize in response to certain stimuli. Fitch discusses the idea of nano-intentionality, specifically in regards to neurons, in their ability to adjust rearrangements in order to create neural networks. He discusses the ability of cells such as neurons to respond independently to stimuli such as damage to be what he considers \"intrinsic intentionality\" in cells, explaining that \"[w]hile at a vastly simpler level than intentionality at the human cognitive level, I propose that this basic capacity of living things [response to stimuli] provides the necessary building blocks for cognition, and higher-order intentionality\". Fitch describes the value of his research to specific areas of computer science such as artificial intelligence, and computer architecture. He states that \"[I]f a researcher aims to make a conscious machine, doing it with rigid switches (whether vacuum tubes or static silicon chips) is barking up the wrong tree.\" Fitch believes that an important aspect of the development of areas such as artificial intelligence is wetware with nano-intentionalility, and autonomous ability to adapt and restructure itself.\n\nIn a review of the above-mentioned research conducted by Fitch, Daniel Dennett a professor at Tufts University discusses the importance of the distinction between the concept of hardware and software when evaluating the idea of wetware, and organic material such as neurons. Dennett discusses the value of observing the human brain, as a preexisting example of wetware. He sees the brain as having \"the competence of a silicon computer to take on an unlimited variety of temporary cognitive roles\". Dennett disagrees with Fitch on certain areas, such as the relationship of software/ hardware versus wetware, and what a machine with wetware might be capable of. Dennett highlights the importance of additional research into human cognition to better understand the intrinsic mechanism by which the human brain can operate, to better create an organic computer.\n\nThe subfield of organic computers and wetware is still largely hypothetical and in a preliminary stage. While there has yet to be major developments in the creation of an organic computer since the neuron based calculator developed by William Ditto in the 1990s the research mentioned in the sections above continues to push the field forward. Specific examples of research such as the modeling of chaotic pathways in silicon chips by William Ditto have made new discoveries in ways of organizing traditional silicon chips, and structuring computer Architecture to be more efficient, and better structured. Ideas emerging from the field of cognitive biology also help to continue to push discoveries in ways of structuring systems for artificial intelligence, to better imitate preexisting systems in humans.\n\n\n"}
{"id": "1065209", "url": "https://en.wikipedia.org/wiki?curid=1065209", "title": "Worldspan", "text": "Worldspan\n\nWorldspan is a provider of travel technology and content and a part of the Travelport GDS business. It offers worldwide electronic distribution of travel information, Internet products and connectivity, and e-commerce capabilities for travel agencies, travel service providers and corporations. Its primary system is commonly known as a Global Distribution System (GDS), which is used by travel agents and travel related websites to book airline tickets, hotel rooms, rental cars, tour packages and associated products. Worldspan also hosts IT services and product solutions for major airlines.\n\nIn December, 2006, Travelport, owner of the Galileo GDS, Gullivers Travel Associates (GTA) and a controlling share in Orbitz, agreed to acquire Worldspan. However, at the time, management of Travelport did not commit to the eventual merging of the two GDS systems, saying that they were considering all options, including running both systems in parallel. On August 21, 2007, the acquisition was completed for $1.4 billion and Worldspan became a part of Travelport GDS, which also includes Galileo and other related businesses. On September 28, 2008, the Galileo and Apollo GDS were moved from the Travelport datacenter in Denver, Colorado to the Worldspan datacenter in Atlanta, Georgia (although they continue to be run as separate systems from the Worldspan GDS).\n\nIn 2012, Worldspan customers were migrated from the TPF-based FareSource pricing engine to Travelport's Linux-based 360 Fares pricing engine already used by Galileo and Apollo. Although the three systems share a common pricing platform, they continue to operate as separate GDS.\n\nWorldspan was formed in early 1990 by Delta Air Lines, Northwest Airlines, and TWA to operate and sell its GDS services to travel agencies worldwide. Worldspan operated very effectively and profitably, successfully expanding its business in markets throughout North America, South America, Europe, and Asia. As a result, in mid-2003, Worldspan was sold by its owner airlines to Citigroup Venture Capital and Ontario Teachers' Pension Fund which in turn sold the business to Travelport in 2007.\n\nWorldspan was formed in 1990 by combining the PARS partnerships companies (owned by TWA and Northwest Airlines, Inc.) and DATAS II, a division of Delta Air Lines, Inc. One of Worldspan’s predecessors – TWA PARS – became the first GDS to be installed in travel agencies in 1976. ABACUS, an Asian company owned by a number of Asian airlines, owned a small portion of Worldspan, and Worldspan owned a small portion of Abacus. Worldspan and Abacus entered into a series of business and technology relationships. These relationships were terminated after Abacus engaged in fraudulent and deceptive practices, for which Worldspan received a sizable judgement in an arbitration in London.\n"}
