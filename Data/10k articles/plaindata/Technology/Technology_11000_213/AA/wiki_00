{"id": "26236025", "url": "https://en.wikipedia.org/wiki?curid=26236025", "title": "Acuris", "text": "Acuris\n\nAcuris, formerly Mergermarket Group (Mergermarket Ltd.), is a media company that provides specialist news, research, analysis and data on financial markets. It is owned by BC Partners, a private equity group. Singapore sovereign wealth fund GIC Private Limited has a minority stake.\n\nThe company has 1,300 staff, including 600 journalists and analysts, in 67 locations around the world, with headquarters in London, New York and Hong Kong.\n\nAcuris began with a single product, Mergermarket, which was established in December 1999 by founders Caspar Hobbs, Charlie Welsh and Gawn Rowan Hamilton. Its founding idea was that in the M&A market, “There is a lot of information out there but very little intelligence.”\n\nIn August 2006 the company, then known as Mergermarket Ltd, was acquired by The Financial Times Group for £101m, publisher of the Financial Times newspaper and FT.com. FT Group was a division of Pearson PLC, the international media group.\n\nIn 2013, the company was renamed to Mergermarket Group. In November 2013, Pearson agreed to sell Mergermarket Group to Londonprivate equity investor BC Partners in a transaction valuing the business intelligence and news service at £382m including debt. Based on the deal, Mergermarket Group was valued at 15 times its last year operating income.\n\nOn 15 January 2014, Moody's Investors Service assigned a B3 corporate family rating to the Mergermarket Group.\n\nIn July 2017, the Mergermarket Group rebranded as Acuris. The announcement of this move fueled speculation that a sale of the company might come in the next year. Indeed, BC Partners sold a minority stake of around 30% to Singapore sovereign wealth fund GIC in July 2017.\n\nAcuris sells web-based subscription services organised under five divisions:\n\nFixed Income\n\nTransactions & Infrastructure\n\nCompliance\n\nEquities\n\nResearch\n\n2007 – Infinata\n\n2009 – Capital Profile\n\n2010 – Xtract Research\n\n2012 – Inframation Group\n\n2014 – Perfect Information\n\n2014 – Law Report Group\n\n2015 – AVCJ and Unquote\n\n2016 – C6 Group\n\n2016 – Creditflux\n\n2017 – Sale of Infinata\n\n2017 – TIM Group\n\n2018 - Spark Spread\n"}
{"id": "5835059", "url": "https://en.wikipedia.org/wiki?curid=5835059", "title": "Advanced Tactical Airborne Reconnaissance System", "text": "Advanced Tactical Airborne Reconnaissance System\n\nAdvanced Tactical Airborne Reconnaissance System (ATARS) is a system for image acquisition, data storage, and data link used by the United States Marine Corps on its F/A-18D Hornet aircraft. It consists of the Advanced Tactical Airborne Reconnaissance System (ATARS) with infrared and visible light sensors, two digital tape recorders, and a Reconnaissance Management System (RMS); an interface with the APG-73 Radar Upgrade (Phase II) which records synthetic aperture radar (SAR) imagery; and a digital data link mounted in a centerline pod. ATARS fits in the nose in place of the nose gun, with a small datalink pod mounted on the centerline station. The digital data link will transmit imagery and auxiliary data to any Common Imaging Ground/Surface Station (CIG/SS) compatible system including the Joint Services Imagery Processing System (JSIPS) or Marine Tactical Exploitation Group (TEG) based ashore and Navy JSIPS (JSIPS-N) aboard ship.\n\nEach of the four U.S. Marine Corps F/A-18D squadrons have three ATARS aircraft, giving a total of 12 ATARS equipped aircraft altogether. The first operational use of ATARS equipped aircraft occurred in February 2000 when MCAS Beaufort based VMFA(AW)-332 deployed to Hungary in Operation Allied Force. ATARS is a considerable advance in capability on the Marines old RF-4B/C aircraft.\n\nATARS also is the acronym for an unrelated Air Force program, Aircrew Training and Rehearsal Support. The ATARS program acquires, sustains and supports mission qualification training and rehearsal system hardware, software and courseware (to include instructors) for Air Force Special Operations Forces (AFSOF), Combat Search and rescue (CSAR) and UH-1 operations. Weapon systems include the MH-53J/M, UH-1N, HH-60G, CV-22, MC-130E/H/P, AC-130H/U, HC-130P/N, and EC-130J.\n\n"}
{"id": "2215760", "url": "https://en.wikipedia.org/wiki?curid=2215760", "title": "Baby oil", "text": "Baby oil\n\nBaby oil is a skin-care product for infants.\n\nThe skin of an infant, especially a premature one, is sensitive, thin and fragile. Its neutral pH on the surface significantly reduces the protection against excessive bacterial growth. The epidermis and dermis are thinner than those of adults and the epidermal barrier is not yet fully developed. Consequences can for example be dry skin, infections, peeling, blister formation and poor thermoregulation. The application of different oils to the skin of the newborn is routinely practiced in many countries. In general, these oils are used for cleansing, to maintain the skin's moisture and to protect its surface. Additionally, baby oil is used for the massage of newborns and as additive in lotions and creams.\n\nBaby oils can be classified by the base formulation of the product. They are based on \n\n\nTypical components of baby oils are the highly purified mineral oil products such as liquid paraffin (INCI name: paraffinum liquidum) and vaseline (INCI name: petrolatum). These compounds are odorless and tasteless, dermatologically tested and approved, not allergenic, hydrophobic and contain no pesticides or herbicides. Preservatives or antioxidants are not necessary, because in contrast to vegetable oils, there is no risk of rancidity with paraffins. Nevertheless, the use of mineral oil in cosmetics is being criticized. Natural cosmetic companies claim that the use of mineral oil results in skin occlusion. Conventional cosmetic manufacturers and even dermatogists and cosmetic chemists argue against that and studies weren't able to show any statisitical difference between paraffin oil and vegetable oils in terms of skin penetration and skin occlusion. On the contrary, petrolatum-based preparations have been shown to be effective to the skin barrier function, even in premature infants. \n\nVegetable oils are produced by plants with the highest concentration being present in seeds and fruits. About 95% of each vegetable oil is primarily composed of triglycerides. Coconut oil and palm oil contain mainly saturated fatty acids, while other oils largely contain unsaturated fatty acids, for example oleic acid and linoleic acid. Accompanying substances in vegetable oils are, inter alia, phospholipids, glycolipids, sulfolipids, squalene, carotenoids, vitamen E, polyphenols and triterpene alcohols. To avoid rancidity, preservatives or antiocidants are added to baby oils based on vegetable oils. On cosmetic products, these oils are listed according to the International Nomenclature of Cosmetic Ingredients (INCI), e.g.:\n\n\nVegetable oils are not to be confused with essential oils, both being sourced from plants. \n\nBaby oils are largely used as skin care products and their principle use remains as skin moisturizers. In particular, baby oils find application in the treatment of various skin diseases like atopic dermatitis, xerosis, psoriasis and other eczematous conditions. Another area of use is the oil massage of the newborn which has been a tradition in India and other Asian countries since time immemorial. The massage itself has benefits including the stimulation of circulatory and gastrointestinal systems, better weight gain, lesser stress behaviour, positive effects on neurological and neuromotor development and infant-parent bonding. Massage with oil results in improved skin condition, namely increased hydration and surface lipid content. The barrier function and thermoregulation are improved and the loss of transepidermal water is reduced.\n"}
{"id": "33155817", "url": "https://en.wikipedia.org/wiki?curid=33155817", "title": "Bank out wagon", "text": "Bank out wagon\n\nA bank out wagon is a vehicle, either towed or self-propelled, with a hopper, used to transport grain from a combine harvester to nearby trailers or storage.\n\n"}
{"id": "11748426", "url": "https://en.wikipedia.org/wiki?curid=11748426", "title": "Brainport", "text": "Brainport\n\nBrainPort is a technology whereby sensory information can be sent to one's brain through an electrode array which sits atop the tongue. It was initially developed by Paul Bach-y-Rita as an aid to people's sense of balance, particularly of stroke victims. Bach-y-Rita founded \"Wicab\" in 1998.\n\nIt has also been developed for use as a visual aid, demonstrating its ability to allow a blind person to see his or her surroundings in polygonal and pixel form. In this scenario, a camera picks up the image of the surrounding, the information is processed by a chip which converts it into impulses which are sent through an electrode array, via the tongue, to the person's brain. The human brain is able to interpret these impulses as visual signals and they are then redirected to the visual cortex, allowing the person to \"see.\" This is similar in part to how a cochlear implant works, in that it transmits electrical stimuli to a receiving device in the body.\n\nThe BrainPort V100 oral electronic vision aid was approved by the Food and Drug Administration (FDA) on June 18, 2015.\n\n"}
{"id": "15317497", "url": "https://en.wikipedia.org/wiki?curid=15317497", "title": "Briglin Pottery", "text": "Briglin Pottery\n\nThe Briglin Pottery was a studio pottery founded in 1948 by Brigitte Goldschmidt (later known as Brigitte Appleby) and Eileen Lewenstein in the basement of premises at 66 Baker Street, London. Its object was \"to produce well designed, attractive pots that could be used in the home, and to sell them at affordable prices.\" \nIt produced a large quantity of domestic pottery, much of it recognisable from its dark earthenware body, muted colours, white glaze and wax resist designs. In some ways Briglin was atypical of post-war studio potteries: it made tin-glazed earthenware when most others were making stoneware, it employed staff at the time when most studio potters worked alone or with a few assistants, and its pottery and shop were in the West End of London when many potters preferred the country. \n\nAppleby said of the pottery, “While the London location presupposes high salaries and overheads, it has the advantage of easy access to raw materials, a perpetually changing patronage as well as an unlimited choice of assistants. We employ at our studio fifteen people and make over three thousand pieces a week.” One of her staff, Michael Crosby-Jones, commented, “Yes, Briglin is very commercial. This means that the pottery is a viable concern and the ten full-time employees plus half a dozen part-time employees are very pleased about this.” Their staff included Donald Mills, who worked with them from 1948–52, and Anthony Barson who specialised in painted decoration. \n\nDue to a fire in 1952, the pottery moved to 22 Crawford Street where it continued until its closure in 1990. Lewenstein left the pottery in 1959 to set up her own studio. Both she and Appleby were active in forming the Craft Potters' Association and served it for many years. Lewenstein co-edited its journal \"Ceramic Review\" from 1970 until 1997.\n\nBrigitte Appleby died in April, 2000\nand Eileen Lewenstein in March, 2005.\n\nIn 2002 Anthea Arnold published a history of the pottery.\n\nExample of Briglin's pottery are included in the collection of the University of Warwick\n\nBriglin.com | Collecting Briglin Pottery\n"}
{"id": "40271105", "url": "https://en.wikipedia.org/wiki?curid=40271105", "title": "Catchment-sensitive farming", "text": "Catchment-sensitive farming\n\nCatchment-sensitive farming is a sustainable agriculture program developed by the Environment Agency and Natural England that aims to protect watersheds from pollution by agricultural runoff. It offers grants to farmers to help develop environmentally sustainable farming methods that limit the contamination of nearby bodies of water. From January to July 2013, over £11 million worth of grants had been issued. In addition to funding, Natural England offers free advice, farm inspections, and training programs to farmers.\n\nThe goal of Catchment Sensitive Farming (CSF) is ultimately to improve the environmental impacts that farms have on our Planet. Farmers are advised by the CSF project, where they use trained, experienced staff to teach farmers about the methods that can be used. Farmers are taught about the following subjects: manure management, nutrient management, soil condition, pesticide management, and farm infrastructure. Catchment Sensitive Farming has teamed with four different organizations to help farmers:\"The\" Agriculture and Horticulture Development Board, the \"Professional Nutrient Management Group\", The Rivers Trust, and \"The Voluntary Initiative\".\n\n\"The Agriculture and Horticulture Development Board\" discusses soil management to increase productivity as well as decrease farming’s impact on the environment. If soil structure is sub-par, then crop growth is sub-par, and soil loss will enter rivers and streams polluting the environment. For proper management of soil these steps are necessary to take-assess the soil, soil sample, cultivation to improve soil, and protect the environment. To assess the soil health, and structure a soil pit is dug. Once the soil pit is dug the soil is sampled and analyzed. Information is collected in regards to nutrient content and pH of the soil, and micro nutrients are examined by broad spectrum analysis. The third step, cultivation and rotating the soil improves both soil structure and health. During cultivation and rotation the farmer’s must be educated on visual signs of compaction, and know when to cultivate arable soil. The final step for soil management according to The Agriculture and Horticulture Development Board is to minimize farmer’s impact – reduce and prevent runoff through the use of banks, buffer strips and tramline management.\n\nIn 2009, there was a study conducted on the effects of tillage on soil and water. The results showed the some farmers have on the environment. They found phosphorus and nitrogen levels were higher in cropped areas. This was because minimal tillage and full stubble retention increases nutrient concentrations in soil, this leads to run off, and leads further to increases in higher concentrations of nutrients filtering into water supplies. After the study, the results showed that the best method for farmers to use is a method of direct drilling combined with stubble retention in higher rainfall zones to better retain the nutrients rather than run off into the water supply \n\n\"Professional Nutrient Management Group\" provides assistance to farmers in regards to improving nutrient management through tools and resources. The management group known as \"Tried & Tested\" has a toolkit with nutrient, manure, and feed planning guidance. Inside this toolkit one would find a booklet with step by step how to farm, recording forms for comparison of fertiliser, a nutrient management glossary with all the terms, and planning/recording sheets for use of organic manures. The goal of this booklet is to explain nutrient management goals, help the individual farmer create a plan, show the farmer the most important questions that should be understood, explain how to calculate how much fertiliser is necessary, and help farmers understand how they are benefiting.\n\n\"The Rivers Trust\" and \"Catchment Sensitive Farming\" created Pinpoint to reduce water pollution created from farming. Pinpoint supplies training, and support to keep farmers up to date and informed on the methods for sustainability. They have a number of courses on the process of how to diffuse water pollution from our environment and agriculture, and courses on educating farmers on sediment-related problems. \"The Rivers Trust\" focuses heavily on why it is important to reduce diffuse water pollution from agriculture. Diffuse water pollution is created from many sources making it difficult to track. Run off has many sources like transportation, urban living, remnants of contaminated land; nutrients; pesticides, and sediment created by farmers. To best reduce diffuse water pollution from agriculture, farmers must be educated. Small steps can lead to a huge impact like moving their gate entrance to prevent field erosion, or separating clean and dirty water.\n\n\"The Voluntary Initiative\" works towards promoting safe Pesticide use. It’s important when using pesticides to plan ahead, ensure proper application time, ensure proper application, and clean up properly. To plan ahead farmers should avoid erosion and run off, be certain to use pesticides from a trusted source, and be certain to fill pesticides away from drains and waterways. To ensure proper timing according to the Voluntary Initiative the sprayer should be checked for leaks on a regular basis, do not apply pesticides to dry cracked surfaces and saturated soils, and don’t spray within 48 hours of rainfall. This specifically allows for easier transportation of the pesticides into the water supply. To ensure proper application of pesticides avoid spills and splashes when filling the sprayer, make sure no pesticides get into the water supply, clean up any spills, and apply with care. The final step is to clean up when finished with the application. Spray wash on the crop and target area when finished, and clean off any mud or surfaces that could have come into contact with the pesticides before leaving the target area.\n"}
{"id": "2505402", "url": "https://en.wikipedia.org/wiki?curid=2505402", "title": "Ceiling projector", "text": "Ceiling projector\n\nThe ceiling projector or cloud searchlight is used to measure the height of the base of clouds (called the ceiling) above the ground. It is used in conjunction with an alidade, usually positioned 1000 ft (304.8 m) away and wherever possible set at the same level.\n\nThe projector is normally set at 90°, although 71° 31' may be used, in relation to the terrain. The projector consists of a 430 W incandescent bulb set in a weatherproof housing. Inside the housing are two mirrors; the first, above the bulb, reflects the light downwards to the second mirror, that then reflects the light upwards to the cloud. Both mirrors are focused to produce a high intensity beam of light that renders a visible spot on the base of the cloud.\n\nThe alidade is mounted on a post at a height of 5 ft (1.524 m) from the ground. It consists of an arm with a pointer and open sight at one end and a rubber eyepiece at the other. The arm is mounted onto a curved scale that is marked both in meters and the coded cloud height (feet). The observer looks through the eyepiece and sets the sight onto the spot projected on the cloud and reads the height from the attached scale.\n\nWhen the cloud is thin the beam of light may penetrate into the cloud. The observer should read the scale where the light first enters the cloud and not at the top. However, a remark may be made as to how far into the cloud the light was able to penetrate as this may be useful. In the case of fog or blizzard conditions the observer should read the scale where the beam disappears.\n\n\n"}
{"id": "8253590", "url": "https://en.wikipedia.org/wiki?curid=8253590", "title": "Cinebulle", "text": "Cinebulle\n\nThe Cinebulle is a hot air balloon specially adapted for filming. It uses a propeller on the back like a microlight.\n\nThe gondola is a two-seat bench for the pilot and cameraman. The camera is held by a Ronford mini 7 head, fitted on a pivoting device enabling the camera to be set either between the pilot and cameraman or underneath the seat, from where panoramic and near-vertical filming is possible.\n\nWhen the camera is under the seat, a battery-operated black and white video viewfinder allows for framing and focusing without being bothered by sun light.\n\nWeather permitting, the Cinebulle is steered with a propulsion engine located at the back of the gondola.\n\nAs a hot air balloon, the Cinebulle cannot fly in high winds, and is most easily operated in mornings or evenings, which has the advantage of being the times when light is most suitable for filming. \n\nThe Cinebulle was invented by Dany Cleyet-Marrel starting in 1994 with a canopy holding 14,000 m, and a version with 2 electric motors was tested in 2008. The balloon is now constructed in France as DynaBulle by the \"Balloon Chaize\".\n\n\n"}
{"id": "4333228", "url": "https://en.wikipedia.org/wiki?curid=4333228", "title": "Crann-nan-gad", "text": "Crann-nan-gad\n\nThe crann-nan-gad was a type of plough formerly used in the Western Isles of Scotland. It was one of the earliest types of plough used in Hebridean crofting, and consisted of a small crooked piece of wood with an iron tip at one end and a top-mounted handle or stilt (thus, a single-stilted plough). Its curving coulter and sock were both made of iron. It could be drawn by up to four horses, although one or two were more usual. The design of the \"crann-nan-gad\" was specifically related to the environmental conditions of Hebridean farms, facilitating paring and skimming of the shallow soil. The plough's tip could run along the ground, which enabled its user to lift it over large stones embedded in the earth. \n\nIt was widely used in the larger Hebridean islands, spreading from Harris to Lewis in the mid-18th century. In addition to Lewis and Harris, it was also used on the Uists and Barra. Its use survived into the 20th century in the parish of Barvas on Lewis. A single example of the \"crann-nan-gad\" is preserved in the collections of the National Museum of Scotland.\n"}
{"id": "1739973", "url": "https://en.wikipedia.org/wiki?curid=1739973", "title": "Dale R. Corson", "text": "Dale R. Corson\n\nDale Raymond Corson (April 5, 1914 – March 31, 2012) was the eighth president of Cornell University. Born in Pittsburg, Kansas, in 1914, Corson received a B.A. degree from the College of Emporia in 1934, his M.A. degree from the University of Kansas in 1935, and his Ph.D. in physics from the University of California, Berkeley in 1938.\n\nIn 1946 Corson came to Cornell University as an assistant professor of physics and helped design the Cornell synchrotron. He was appointed associate professor of physics in 1947, became a full professor in 1956, was named chairman of the physics department in 1956, and became dean of the College of Engineering in 1959. Following the 1969 resignation of James A. Perkins, Corson became president of Cornell and served until 1977 after which he served for three years as chancellor. In 1979, he was elected by the Board of Trustees as president emeritus.\n\nCorson led the university through the final years of the Vietnam War and student activism, and through the economic recession of the 1970s. His role was to return the university to stability: to concentration on research, teaching, and scholarship. \n\nCorson brought together the state and endowed components of Cornell, forming one university enjoying public and private support, as envisioned by White and Cornell and articulated by Jacob Gould Schurman. Significant support was provided for the research programs at Arecibo, the Wilson Synchrotron Laboratory, and the Nanofabrication Facility. He revitalized the Department of Geology, expanded the Division of Biological Sciences, and added new programs, such as Medieval studies. The I.M. Pei-designed Herbert F. Johnson Museum of Art was completed. He encouraged such multidisciplinary programs as Science, Technology, and Society, the Materials Science Center, environmental programs, radio physics, and space research. \n\nThe status of women on campus was greatly improved during the Corson presidency. A Women's Studies Program was formally established in 1972. A Provost's Advisory Committee on the Status of Women was created and presented specific recommendations. The university's policy statement on equal opportunity was changed to include gender among the proscribed criteria with regard to admission to the university. New employment procedures were implemented, and increasing numbers of women were appointed to the faculty and to high administrative positions. Corson provided support for the Africana Studies and Research Center, which had developed from the black studies movement. He recommended the formation of an Affirmative Action Advisory Board to monitor the status of women and minorities and to propose more effective procedures. \n\nDuring his presidency, university governance was overhauled including the establishment of a faculty-student-employee University Senate and the addition of Student and Employee representatives to the Board of Trustees. A new campus judicial system and campus code of conduct were established.\n\nCorson also served on NACA\"s \"Special Committee on Space Technology\" also called the \"Stever Committee,\" named after its chairman. It was a special steering committee that was formed with the mandate to coordinate various branches of the Federal government, private companies as well as universities within the United States with NACA's objectives and also harness their expertise in order to develop a space program. Dr. Corson therefore played a pivotal role in the process of establishing the nascent United States space program.\n\nHe was the author of a very important textbook on electromagnetism with the following two editions:\nThe latter incorporated some of the ideas of relativistic electromagnetism.\n\nAs part of his Ph.D. work in UC Berkeley, Corson was a co-discoverer of the element astatine. In 1987 he was awarded the Public Welfare Medal from the National Academy of Sciences.\n\n"}
{"id": "43440252", "url": "https://en.wikipedia.org/wiki?curid=43440252", "title": "Digital buffer", "text": "Digital buffer\n\nA digital buffer (or a voltage buffer) is an electronic circuit element that is used to isolate the input from the output, providing either no voltage or a voltage that is same as the input voltage. It draws very little current and will not disturb the original circuit. It is also called a unity gain buffer because it provides a gain of 1, which means it provides at most the same voltage as the input voltage, serving no amplification function.\n\nA voltage buffer has a very high input impedance (the opposition to current flow viewed from the load). The high input impedance is the reason a voltage buffer is used. A circuit with a voltage buffer will always draw a little amount of current because of the high input impedance of the buffer. As a result, the power source will not be affected.\n\nThe digital buffer is important because it can control the on and off of data transmission, which is used widely in the world of registers (sophisticated data storage device) and buses (data transferring device). A typical digital buffer that is used to control multiple data inputs written onto a bus is a Tri-State Digital Buffer, which controls the data flow by \"tri-state\" pins.\n\nA digital buffer serves to transfer a voltage from a circuit that has a high output impedance level, to a second circuit with a low input impedance level. If we have a power source and a low impedance (resistor) load without a buffer, according to Ohm's law (voltage is equal to current times resistance), a huge amount of current is drawn from the source. As a result, huge amounts of power is drawn by the power source, which causes high disturbances. A voltage buffer always has a very high input impedance, approaching infinity; as a result, no matter what value the load impedance is, the source voltage will be totally spanned on the buffer impedance (because of Ohm's law); the voltage across the buffer impedance is the input voltage. Because the resistance is infinity, the circuit will draw very little current, and will not disturb the original circuit. Because the output current is generated by the voltage source via buffer, the buffer acts as a barrier between the source and load, thus preventing the load resistance affecting the source network.\n\nThis kind of buffer produces the state opposite to the input. If the input is high, the output is low and vice versa. Graphically, it is often represented as a triangle with a small circle attaching to the tip. The inverter is a basic building block in digital electronics. Decoders, state machines, and other sophisticated digital devices may use inverters.\n\nThis kind of buffer performs no inversion or decision-making possibilities. A single input digital buffer is different from an inverter. It does not invert or alter its input signal in any way. It reads an input and outputs a value. Usually, the input side reads either HIGH or LOW input and outputs a HIGH or LOW value, correspondingly. Whether the output terminal sends off HIGH or LOW signal is determined by its input value. The output value will be high if and only if the input value is high. In other words, Q will be high if and only if A is HIGH.\n\ntri-state buffer\nUnlike the single input digital buffer which has only one input, Tri-state digital buffer has two inputs: a data input and a control input. (A control input is analogous to a valve, which controls the data flow.) When the control input is active, the output value is the input value, and the buffer is not different from the single input digital buffer.\n\nActive high Tri-state digital buffer is a buffer with control input high as an active state.\n\nWhen the data input is nothing and the control input is open, the output value will be \"Z\", which means no current and high impedance. Both the data and the control input are inactive; the buffer shuts down.\n\nData input is nothing but control input is active. The output value will be \"Z\", the impedance is approaching infinity. Although we allow the data transmission by turning on the control input, we don't have data coming in because of the infinite resistance.\n\nData input is active but control input is inactive. The output value is 0, which means no current.> Although we input something, the control valve prevents the input from going out.\n\nBoth data input and control input is active. The output value will be 1 and current will flow through the element. In this case, the input data is sent to the output.\n\nIt is basically the same as active high digital buffer except the fact that the buffer is active when the control input is at a low state.\n\nSingle input voltage buffers are used in many places for measurements including:\nTri-state voltage buffers are used widely on buses, which allows multiple devices communicate with each other. A bus can only read one data input from a device at one time, and that is when a tri-state buffer is applied. A Tri-state buffer, with its control input, can prevent a bus from reading excessive input.\n"}
{"id": "197215", "url": "https://en.wikipedia.org/wiki?curid=197215", "title": "Disposable camera", "text": "Disposable camera\n\nA disposable or single-use camera is a simple box camera meant to be used once. Most use fixed-focus lenses. Some are equipped with an integrated flash unit, and there are even waterproof versions for underwater photography. Internally, the cameras use a 135 film or an APS cartridge.\n\nWhile some disposables contain an actual cartridge as used for loading normal, reusable cameras, others just have the film wound internally on an open spool. The whole camera is handed in for processing. Some of the cameras are recycled, i.e. refilled with film and resold. The cameras are returned for \"processing\" in the same fashion as film cameras.\n\nIn general the one-time-use camera represents a return to the business model pioneered by Kodak for their KODAK camera, predecessor to the Brownie camera; it is particularly popular in situations where a reusable camera would be easily stolen or damaged, when one's regular camera is forgotten, or if one cannot afford a regular camera.\n\nA company called Photo-Pac produced a cardboard camera beginning in 1949 which shot eight exposures and which was mailed-in for processing. Cameras were expensive, and would often have been left safely at home when lovely scenes presented themselves. Frustrated with missing photo opportunities, H. M. Stiles had invented a way to enclose 35mm film in an inexpensive enclosure without the expensive precision film transport mechanism. It cost $1.29. Though incredibly similar to the familiar single-use cameras today, Photo-Pac failed to make a permanent impression on the market.\n\nIn 1966, a French company called FEX introduced a disposable bakelite camera called \"Photo Pack Matic\", featuring 12 photos (4×4 cm).\n\nThe currently familiar disposable camera was developed by Fujifilm in 1986. Their \"Utsurun-Desu\" (\"It takes pictures\") or QuickSnap line used 35 mm film, while Eastman Kodak's 1987 Fling was based on 110 film. Kodak released a 35 mm version in 1988, and in 1989 renamed the 35 mm version the FunSaver and discontinued the 110 Fling.\n\nIn Japan, the Utsurun was released in 1986 for 1380 yen and became widely accepted. Because of the immediate appeal, companies like Konica, Canon and Nikon soon produced their own models. To stay competitive, Fuji introduced advanced features to its original model such as panoramic photography, waterproofing and the inclusion of a flash. Some cameras even have a manual zoom feature which works by shifting two lenses in front of the shutter.\n\nBy 2005 disposable cameras were a staple of the consumer film camera market and flash-equipped \ndisposables were the norm.\n\nDisposable cameras are popular with tourists and people traveling around the world to save pictures of their adventures.\n\nSince the late 1990s, disposable cameras have become increasingly popular as wedding favors. Usually they are placed on tables at wedding receptions to be used by guests to capture their unique perspective of the event. More commonly they are available in colors to match the wedding theme such as ivory, blue, white, gold, etc.\n\nSo-called \"accident camera kits\" containing film-based disposable cameras are increasingly being carried in vehicles to take images as evidence after an accident. Film photography is potentially a more credible form of photography in the event of a dispute due to the ease with which digital photography can be edited.\n\nThey often have cheap plastic lenses, questionable film quality, fixed focal lengths but quick and 'point and shoot' ease make the disposable camera popular with many photographers who enjoy the 'less than perfect' style these cameras provide, in a move away from digital imagery, which can also be seen in the rise in popularity of 'lomography'. This has also led to a number of 'lost art' type projects where disposable cameras are left in public spaces with a message for anyone finding the camera to take some images and then post the camera back, or pass it on to another person. The low cost of the cameras makes them a perfect tool for these sorts of projects.\n\nDigital one-time-use cameras (and also digital one-time-use camcorders) are available in some markets; for example the US saw the introduction of a digital camera in 2004. Digital disposables have not had the success of their film based counterparts, possibly from the expense of the process (especially compared to normal digital camera use) and the poor quality of the images compared to either a typical digital camera, or a disposable film camera. Usually, the display shows the number of shots remaining, and once this is completed, the camera is returned to the store. The digital files are then extracted from the camera, and in return for keeping the camera, they are printed out or stored to CD (or DVD in the case of the Video Camera ) for the customer. Almost all digital 'single use' cameras have been successfully hacked to eliminate the need to return them to the store. The motivations for such hacking include saving money and, more commonly, the challenge of overcoming artificial impositions (such as a 25 shot limit on an internal memory that can store 100 images).\n\nThe high-voltage photo flash capacitors in some cameras are sometimes extracted and used to power devices such as coil guns, stun guns,\nhomemade Geiger counter projects and \"RFID zapper\" EMP devices.\n\n\n"}
{"id": "55552490", "url": "https://en.wikipedia.org/wiki?curid=55552490", "title": "Dye-and-pry", "text": "Dye-and-pry\n\nDye-n-Pry, also called Dye And Pry, Dye and Pull, Dye Staining, or Dye Penetrant, is a destructive analysis technique used on surface mount technology (SMT) components to either perform failure analysis or inspect for solder joint integrity. It is an application of dye penetrant inspection.\n\nDye-n-Pry is a useful technique in which a dye penetrant material is used to inspect for interconnect failures in integrated circuits (IC). This is mostly commonly done on solder joints for ball grid array (BGA) components, although in some cases it can be done with other components or samples. The component of interest is submerged in a dye material, such as red steel dye, and placed under vacuum. This allows the dye to flow underneath the component and into any cracks or defects. The dye is then dried in an oven (preferably overnight) to prevent smearing during separation, which could lead to false results. The part of interest is mechanically separated from the printed circuit board (PCB) and inspected for the presence of dye. Any fracture surface or interface will have dye present, indicating the presence of cracks or open circuits. IPC-TM-650 Method 2.4.53 specifies a process for dye-n-pry.\n\nDye-n-Pry is a useful failure analysis technique to detect cracking or open circuits in BGA solder joints. This has some practical advantages over other destructive techniques, such as cross sectioning, as it can inspect a full ball grid array which may consist of hundreds of solder joints. Cross sectioning, on the other hand, may only be able to inspect a single row of solder joints and requires a better initial idea of the failure site.\n\nDye-n-pry can be useful for detecting several different failure modes. This includes pad cratering or solder joint fracture from mechanical drop/shock, thermal shock, or thermal cycling. This makes it useful technique to incorporate into a reliability test plan as part of the post test failure inspection. It is also a useful method to inspect or diagnose failures due to manufacturing defects or design flaws. This includes defects such as black pad for PCBs with ENIG surface finishes or early failures due to excessive board flexure from depaneling or In-circuit test (ICT).\n\n"}
{"id": "19318108", "url": "https://en.wikipedia.org/wiki?curid=19318108", "title": "Embrace (non-profit)", "text": "Embrace (non-profit)\n\nEmbrace is a non profit organization providing low-cost incubators to prevent neonatal deaths in rural areas in developing countries. The organization was developed in 2008 during the multidisciplinary Entrepreneurial Design For Extreme Affordability course at Stanford University by group members Jane Chen, Linus Liang, Rahul Panicker, Razmig Hovaghimian, and Naganand Murty. \n\nIn 2015 Embrace became part of Thrive Networks (also called East Meets West) which is a non-governmental organization founded in 1988 by Le Ly Hayslip. Thrive Networks focuses on areas such as health, water, sanitation, and education in various countries throughout the world. \n\nThe Embrace infant warmer is a low-cost solution that maintains premature and low-birth-weight babies’ body temperature, that would give premature infants a better chance at survival. A baby born two weeks premature lacks the ability to regulate its own body temperature. The child will likely die if not transferred to an incubator within an hour. With the Embrace Warmer, which is a specially designed polymer blanket, that vital time span becomes 4 hours. The infant warmer is portable, safe, reusable, and requires only\nintermittent access to electricity. Each baby warmer is priced at approximately $25.\nThe Embrace development team won the fellowship at the Echoing Green competition in 2008 for this concept. Embrace also won the 2007-2008 Business Association of Stanford Entrepreneurial Students Social E-Challenge competition grand prize. At a ceremony at BAFTA in London on December 3rd 2013 Jane Chen, Linus Liang, Naganand Murty and Rahul Panicker won an innovation award from the Economist. Embrace also partners with UniversalGiving to raise fund for its project, which is to provide the Embrace infant warmers in Kabul, Afghanistan.\n\n"}
{"id": "42975385", "url": "https://en.wikipedia.org/wiki?curid=42975385", "title": "Emma Mulqueeny", "text": "Emma Mulqueeny\n\nEmma Elizabeth Mulqueeny (née Knight; born 12 July 1971) is the co-founder and chief executive officer of Young Rewired State and Rewired State. She is a Commissioner for the Speaker's Commission on Digital Democracy and a Google Fellow.\n\nShe is the daughter of Kenneth G Knight and Sara Nicholls (née McArtney), and was educated at the University of Surrey.\n\nMulqueeny was included in the 166th annual edition of \"Who’s Who\", and was voted onto the Wired 100 list, Tech City 100 and BIMA Hot 10. She was voted one of the top ten women in technology by \"The Guardian\" and was named in the top ten Tech Heroes for Good by NESTA.\n\nShe was appointed as an Officer of the Order of the British Empire (OBE) in the 2016 Birthday Honours for services to technology and education.\n"}
{"id": "36535890", "url": "https://en.wikipedia.org/wiki?curid=36535890", "title": "Film applicator", "text": "Film applicator\n\nA film applicator is a device used to evenly spread a substance, such as paint, ink, or cosmetics, over a substrate such as a drawdown card. \n\nApplicators are usually metal bars that are manufactured to high tolerances to give consistent, repeatable results. Each bar will give a \"theoretical wet film thickness\" or, in other words, the thickness of the coating that should remain on the drawdown card after application. Even with high manufacturing tolerances, the actual wet film thickness can vary from 50% to 90% of the gap. \n\nThere are multiple types of bar applicators, their forms and uses are shown below. Film applicators follow the ASTM standard D823. Applicators can be used either manually or automatically. \n\nWhen using an applicator manually, small variations in speed and applied pressure are inevitable. These variations can affect the quality of the drawdown and thus the measurements of film properties such as abrasion resistance, hiding power and gloss. \n\nThe use of an automatic film applicator guarantees consistent speed and pressure, providing repeatable and high quality results. \n\nAutomatic film applicators vary in their construction. The most modern types of applicators use vacuum plates to hold the drawdown cards and allow a variety of application bars to be mounted. The speed and pressure of the bar can be adjusted to allow for customization of the final film thickness. The drawdown process is governed by ASTM standard D823.\n\nUsed for high viscous coatings, this type of applicator has four different clearances built in, one on each horizontal surface.\n\nUsed for low viscous coatings, this applicator also has four different clearances built in. It is used for non-rigid substances and has a two sided opening in the center of the bar.\n\nThis applicator has one, two or four clearances and contains a slanted trailing edge, also used for high viscous coatings. This is the most common type of film applicator.\n\nThis applicator also has two clearances but features a U-shaped form instead of a straight bar.\n\nThe square applicator is the more versatile than previously mentioned applicators. It has 8 clearances in the form of a square frame. This tool combines the accuracy of fixed applicators with the versatility of adjustable applicators.\n\nThis adjustable applicator has a metal frame with an adjustable \"knife\" that acts as the gap clearance. These applicators are the most versatile manual applicator, but is also the least precise.\n\nThese applicators differ from the bar applicators as they consist of a metal rod wound with wire of varying thickness. The coating will pass through the gaps between the wires and level off at a uniform thickness.\nThere are two main types of wire bars. Close wire wound bars will produce coating layers from 4 to 120μm. Higher coating thickness up to 500μm can be obtained using open wound bars.\n\nA standard procedure for testing substances that are applied in a liquid form, and then dry to a solid film, is to create a controlled, standardized test film of the substance, usually on a standardized substrate, such as a metal, plastic or paper panel. \n"}
{"id": "25313249", "url": "https://en.wikipedia.org/wiki?curid=25313249", "title": "Frank Jay Haynes", "text": "Frank Jay Haynes\n\nFrank Jay Haynes (October 28, 1853 – March 10, 1921), known as F. Jay or the \"Professor\" to almost all who knew him, was a professional photographer, publisher, and entrepreneur from Minnesota who played a major role in documenting through photographs the settlement and early history of the great Northwest. He became both the official photographer of the Northern Pacific Railway and of Yellowstone National Park as well as operating early transportation concessions in the park. His photographs were widely published in articles, journals, books and turned into stereographs, and postcards in the late 19th and early 20th century.\n\nF. Jay was born in Saline, Michigan on October 28, 1853 to Levi H. Haynes, a merchant and Caroline Oliphant. When he was a small boy, the family moved east to Detroit, Michigan. F. Jay worked in his father's store and took various other odd jobs. As a boy, he had visited the photographic studios of Mrs. Gillette in Detroit and became interested in photography. After several traveling salesman jobs, F. Jay ended up in Ripon, Wisconsin and secured a position as an apprentice in the \"Doctor\" William H. Lockwood's \"Temple of Photography\". He worked for Lockwood for 16 months, learned the photography trade and met his future wife, a co-worker, Lily Snyder. In September, 1876 F. Jay left the Lockwood Studio to start his own photographic business in Moorhead, Minnesota with the backing of his brother-in-law, Gus Henderson.\n\nIt was in Moorhead that F. Jay began his long and prosperous relationship with the Northern Pacific Railway. A year later, F. Jay's business was booming in Moorhead and in January 1878 he married Lily Snyder in Ripon, Wisconsin and brought her to Moorhead to help with the business. They had a daughter, Bessie Loa and two sons, George and Jack Ellis. Jack Ellis Haynes (1884–1962) inherited his father's business in Yellowstone in 1916 and continued as official park photographer until his death in 1962.\n\nThe first \"Haynes Studio\" was established in Moorhead, Minnesota in December 1876. From this studio, F. Jay was able to build on his railroad business with the sales of local cabinet portraits, views and stereoviews of his railroad photographs. By early 1879, F. Jay had relocated his Moorhead studio to a much larger facility. However, in the fall of 1879, F. Jay closed his Moorhead studio and moved west across the Red River to Fargo, North Dakota.\n\nLived here until he moved in 1889.\n\nIn 1889, F. Jay established his final studio in St. Paul, Minnesota.\n\nIn 1885, F. Jay bought a Pullman Car from the Northern Pacific Railroad and had it refitted as a photographic studio. The NPR charged F. Jay a nominal fee to haul the car around the railroad system. In 1901, the fee was $.35/mile contingent on F. Jay providing the railroad with a nominal number of free photographs of rolling stock and railroad buildings for publicity purposes. Whenever the Palace Studio Car would visit a town, F. Jay or his employees would take photos for local customers and provide them with prints on later visits. F. Jay operated the car successfully between 1885 and 1905.\n\nShortly after his move to Moorhead, Minnesota in 1876 F. Jay began doing photographic work for elements of the Northern Pacific Railway as the railway expanded operations west. By October 1876, he had a contract with the railway for work in the 1877 season. His job was to supply publicity photos and stereoscopic views of rolling stock, depots, sights along the railway and construction activities from St. Paul, Minnesota to Bismarck, North Dakota. F. Jay was a good businessman and had arranged with the railroad to supply a fixed number of prints (or views) from each negative while he retained the rights to the negatives from which he could print and sell views for his own benefit. The railroad provided F. Jay with a free pass on all the railway trains from St. Paul to Bismarck. This allowed F. Jay access to all the railroad's territory from which he could photograph anything he wanted to. From his Moorhead studio, F. Jay could hardly keep up with the demand for his \"Northern Pacific Views\" and local portrait work.\n\nIn 1879, F. Jay met Charles S. Fee, the private secretary to the railroad's General Manager, H.E. Sargent. In 1883, Fee became the railroad's general passenger and ticket agent responsible for marketing the railroad. It was a position Fee held until 1904. Fee became F. Jay's biggest supporter within the railroad and they became lifelong friends.\n\nIn 1877, F. Jay made the acquaintance of Philetus Norris, then superintendent of Yellowstone National Park. At the time, Norris encouraged F. Jay to visit the park with him and photograph its wonders. Because of his railroad work, F. Jay was unable to make the trip until 1881. By 1881, Northern Pacific Railroad tracks had reached Glendive, Montana. Even before visiting the park, F. Jay's knack for business prompted him in early 1881 to apply to the Secretary of the Interior for the position of Official Photographer of Yellowstone National Park. The Secretary was unable to confer that position, but did, with Norris's backing, grant F. Jay a lease for a small photographic studio within the park along that was not made official until 1884. In September, 1881, traveling overland from Glendive, and with explicit support from Charles Fee, F. Jay made his first visit to Yellowstone National Park. In close to two months in the park, he was able to visit all the major attractions and take over 200 photographs. F. Jay returned to Yellowstone every year after that first visit until his death in 1921.\n\nIn May 1883, President Chester A. Arthur—under stress from the first years of his unexpected presidency—was encouraged to take a good rest by his advisors. One of those advisors, Senator George Vest of Missouri, suggested a trip to the new national park—Yellowstone. By early summer, the unusual trip was being arranged. President Arthur would visit the park for two weeks in August, unaccompanied by any journalists. Through his notoriety with the Northern Pacific Railroad and early trips to Yellowstone, F. Jay Haynes was selected as the official photographer for the trip.\n\nIn December 1886, Haynes was selected to accompany arctic explorer, Frederick Schwatka on a winter tour through the park. The expedition was sponsored by the \"New York World\" newspaper and The \"Century Magazine\". The expedition started at Mammoth on January 5, 1887. On skis and shoeshoes, pulling sleds laden with gear Schwatka, Haynes and eleven other guides made their way from Mammoth to Norris in two days. By the time the group got to Norris, the cold and altitude had gotten to Schwatka and he had to abandon the tour. Haynes, and three other guides he knew and could depend on, decided to continue the expedition, visiting the lower and upper geyser basins and Yellowstone Falls before trouble struck. In an attempt to get to Yancey's from Canyon, the party got stranded for 72 hours on the slopes of Mount Washburn in a frigid and blinding snowstorm with little or no food or shelter. They almost perished. Once the weather cleared they made their way to Yancey's to recuperate before returning to Mammoth. The 29-day tour of the park covered nearly 200 miles through a wintry environment, with temperatures varying from to . Despite the problems on Mount Washburn, Haynes returned with 42 photographs of Yellowstone in the middle of winter, the first ever taken during that time of year.\n\nUpon F. Jay's death in 1921, Horace Albright, then superintendent of Yellowstone National Park officially named a peak in the Madison Canyon area of the park Mount Haynes . Additionally Albright had a large granite boulder taken from the Golden Gate section of the Mammoth to Norris road shipped to St Paul, MN to be placed on F. Jay's grave. Many of F. Jay Haynes' original photographs are highly valued collectors items, especially his larger 20-24 inch mammoth prints of the Yellowstone region. Original Haynes photos have been priced between $5,000 and $25,000 and some might command $50,000 today.\n\n\n"}
{"id": "7217129", "url": "https://en.wikipedia.org/wiki?curid=7217129", "title": "Gail Williams", "text": "Gail Williams\n\nGail Ann Williams (born in Berkeley, California) has been the director of The WELL since 1998. She graduated from the University of California, Berkeley in the 1970s and got involved in political theater as both a creative and management member of the Plutonium Players troupe. She was a principal in their long-touring satirical show spoofing anti-feminist politics, \"Ladies Against Women\", throughout the years of the Ronald Reagan presidency.\n\nIn December 1991, she joined the management team of the prototypic online community, The WELL, in the role of community manager. In 1998 she took the post of Executive Director. When The WELL was acquired by Salon.com in 1999, Williams stayed on. Currently she serves as Salon's Director of Communities, overseeing The WELL and Salon's Table Talk community.\n\nShe is a member of the International Academy of Digital Arts and Sciences. In 2000 Williams was named as one of the Top 25 Women of the Web.\n\n"}
{"id": "8911974", "url": "https://en.wikipedia.org/wiki?curid=8911974", "title": "History of the steel industry (1850–1970)", "text": "History of the steel industry (1850–1970)\n\nThe history of the modern steel industry began in the late 1850s, but since then, steel has been basic to the world's industrial economy. This article is intended only to address the business, economic and social dimensions of the industry, since the bulk production of steel began as a result of Henry Bessemer's development of the Bessemer converter in 1857. Previously steel was very expensive to produce and only used in small expensive items such as knives, swords and armour.\n\nSteel is an alloy composed of between 0.2% and 2.0% carbon, and the balance of iron. From prehistory through the creation of the blast furnace, iron was produced from iron ore as \"wrought iron\", 99.82% - 100% Fe, and the process of making steel involved adding carbon to the iron, usually via serendipity in the forge or via the cementation process. The introduction of the blast furnace reversed the problem. A blast furnace produces \"pig iron\", which is an alloy of approximately 90% iron and 10% carbon. If the process of steelmaking begins with \"pig iron\" instead of \"wrought iron\", the challenge is to remove a sufficient amount of carbon to get it to the 0.2 to 2 percent for steel.\n\nBefore about 1860 steel was an expensive product, made in small quantities and used mostly for swords, tools and cutlery; all large metal structures were made of wrought or cast iron. Steelmaking was centered in Sheffield, Britain, which supplied the European and the American markets. The introduction of cheap steel was due to the Bessemer and the open hearth processes, two technological advances made in England. In the Bessemer process, molten pig iron is converted to steel by blowing air through it after it was removed from the furnace. The air blast burned the carbon and silicon out of the pig iron, releasing heat and causing the temperature of the molten metal to rise. Henry Bessemer demonstrated the process in 1856 and had a successful operation going by 1864. By 1870 Bessemer steel was widely used for ship plate. By the 1850s, the speed, weight, and quantity of railway traffic was limited by the strength of the wrought iron rails in use. The solution was to turn to steel rails, which the Bessemer process made competitive in price. Experience quickly proved steel had much greater strength and durability and could handle the increasingly heavy and faster engines and cars.\n\nAfter 1890 the Bessemer process was gradually supplanted by open-hearth steelmaking and by the middle of the 20th century was no longer in use. The open-hearth process originated in the 1860s in Germany and France. The usual open-hearth process used pig iron, ore, and scrap, and became known as the Siemens-Martin process. Its process allowed closer control over the composition of the steel; also, a substantial quantity of scrap could be included in the charge. The crucible process remained important for making high-quality alloy steel into the 20th century. By 1900 the electric arc furnace was adapted to steelmaking and by the 1920s, the falling cost of electricity allowed it to largely supplant the crucible process for specialty steels.\n\nBritain led the world's Industrial Revolution with its early commitment to coal mining, steam power, textile mills, machinery, railways, and shipbuilding. Britain's demand for iron and steel, combined with ample capital and energetic entrepreneurs, made it the world leader in the first half of the 19th century.\n\nIn 1875, Britain accounted for 47% of world production of pig iron and almost 40% of steel. 40% of British output was exported to the U.S., which was rapidly building its rail and industrial infrastructure. Two decades later in 1896, however, the British share of world production had plunged to 29% for pig iron and 22.5% for steel, and little was sent to the U.S. The U.S. was now the world leader and Germany was catching up to Britain. Britain had lost its American market, and was losing its role elsewhere; indeed American products were now underselling British steel in Britain.\n\nThe growth of pig iron output was dramatic. Britain went from 1.3 million tons in 1840 to 6.7 million in 1870 and 10.4 in 1913. The US started from a lower base, but grew faster; from 0.3 million tons in 1840, to 1.7 million in 1870, and 31.5 million in 1913. Germany went from 0.2 million tons in 1859 to 1.6 in 1871 and 19.3 in 1913. France, Belgium, Austria-Hungary, and Russia, combined, went from 2.2 million tons in 1870 to 14.1 million tons in 1913, on the eve of the World War. During the war the demand for artillery shells and other supplies caused a spurt in output and a diversion to military uses.\n\nAbé (1996) explores the record of iron and steel firms in Victorian England by analyzing Bolckow Vaughan & Company. It was wedded for too long to obsolescent technology and was a very late adopter of the open hearth furnace method. Abé concludes that the firm—and the British steel industry—suffered from a failure of entrepreneurship and planning.\n\nBlair (1997) explores the history of the British Steel industry since the Second World War to evaluate the impact of government intervention in a market economy. Entrepreneurship was lacking in the 1940s; the government could not persuade the industry to upgrade its plants. For generations the industry had followed a patchwork growth pattern which proved inefficient in the face of world competition. In 1946 the first steel development plan was put into practice with the aim of increasing capacity; the \"Iron and Steel Act of 1949\" meant nationalization of the industry. However, the reforms were dismantled by the Conservative governments in the 1950s. In 1967, under Labour Party control again, the industry was again nationalized. But by then twenty years of political manipulation had left companies such as British Steel with serious problems: a complacency with existing equipment, plants operating under capacity (low efficiency), poor quality assets, outdated technology, government price controls, higher coal and oil costs, lack of funds for capital improvement, and increasing world market competition. By the 1970s the Labour government had its main goal to keep employment high in the declining industry. Since British Steel was a main employer in depressed regions, it had kept many mills and facilities that were operating at a loss. In the 1980s, Conservative Prime Minister Margaret Thatcher re-privatized BSC as British Steel.\n\nIn Australia, the Minister for Public Works, Arthur Hill Griffith, had consistently advocated for the greater industrialization of Newcastle, then, under William Holman, personally negotiated the establishment of a steelworks with G. D. Delprat of the Broken Hill Proprietary Co. Ltd. Griffith was also the architect of the Walsh Island establishment.\n\nIn 1915, Broken Hill Proprietary Company ventured into steel manufacturing with its operation in Newcastle, which was closed in 1999. The 'long products' side of the steel business was spun off to form OneSteel in 2000.\nBHP's decision to move from mining ore to open a steelworks at Newcastle was precipitated by the technical limitations in recovering value from mining the 'lower-lying sulphide ores'. The discovery of Iron Knob and Iron Monarch near the western shore of the Spencer Gulf in South Australia combined with the development by the BHP metallurgist, A. D. Carmichael, of a technique for 'separating zinc sulphides from the accompanying earth and rock' led BHP 'to implement the startlingly simple and cheap process for liberating vast amounts of valuable metals out of sulphide ores, including huge heaps of tailings and slimes up to' high.\n\nThe Ruhr Valley provided an excellent location for the German iron and steel industry because of the availability of raw materials, coal, transport, a skilled labor force, nearby markets, and an entrepreneurial spirit that led to the creation of many firms, often in close conjunction with coal mines. By 1850 the Ruhr had 50 iron works with 2,813 full-time employees. The first modern furnace was built in 1849. The creation of the German Empire in 1871 gave further impetus to rapid growth, as Germany started to catch up with Britain. From 1880 to World War I, the industry of the Ruhr area consisted of numerous enterprises, each working on a separate level of production. Mixed enterprises could unite all levels of production through vertical integration, thus lowering production costs. Technological progress brought new advantages as well. These developments set the stage for the creation of combined business concerns.\n\nThe leading firm was Friedrich Krupp AG run by the Krupp family. Many diverse, large-scale family firms such as Krupp's reorganized in order to adapt to the changing conditions and meet the economic depression of the 1870s, which reduced the earnings in the German iron and steel industry. Krupp reformed his accounting system to better manage his growing empire, adding a specialized bureau of calculation as well as a bureau for the control of times and wages. The rival firm GHH quickly followed, as did Thyssen AG, which had been founded by August Thyssen in 1867. Germany became Europe's leading steel-producing nation in the late 19th century, thanks in large part to the protection from American and British competition afforded by tariffs and cartels.\n\nBy 1913 American and German exports dominated the world steel market, and Britain slipped to third place. German steel production grew explosively from 1 million metric tons in 1885 to 10 million in 1905 and peaked at 19 million in 1918. In the 1920s Germany produced about 15 million tons, but output plunged to 6 million in 1933. Under the Nazis, steel output peaked at 22 million tons in 1940, then dipped to 18 million in 1944 under Allied bombing.\nThe merger of four major firms into the German Steel Trust (Vereinigte Stahlwerke) in 1926 was modeled on the U.S. Steel corporation in the U.S. The goal was to move beyond the limitations of the old cartel system by incorporating advances simultaneously inside a single corporation. The new company emphasized rationalization of management structures and modernization of the technology; it employed a multi-divisional structure and used return on investment as its measure of success. It represented the \"Americanization\" of the German steel industry because its internal structure, management methods, use of technology, and emphasis on mass production. The chief difference was that consumer capitalism as an industrial strategy did not seem plausible to German steel industrialists.\n\nIn iron and steel and other industries, German firms avoided cut-throat competition and instead relied on trade associations. Germany was a world leader because of its prevailing \"corporatist mentality\", its strong bureaucratic tradition, and the encouragement of the government. These associations regulated competition and allowed small firms to function in the shadow of much larger companies.\n\nWith the need to rebuild the bombed-out infrastructure after the Second World War, Marshall Plan (1948–51) enabled West Germany to rebuild and modernize its mills. It produced 3 million of steel in 1947, 12 million in 1950, 34 million in 1960 and 46 million in 1970. East Germany produced about a 10th as much.\n\nThe French iron industry lagged behind the United Kingdom and Belgium in the early 19th century, and after 1850 also lagged behind Germany and Luxembourg. Its industry comprised too many small, inefficient firms. 20th century growth was not robust, due more to traditional social and economic attitudes than to inherent geographic, population, or resource factors. Despite a high national income level, the French steel industry remained laggard. The industry was based on large supplies of coal and iron ore, and was dispersed across the country. The greatest output came in 1929, at 10.4 million metric tons. The industry suffered sharply during the Great Depression and World War II. Prosperity returned by mid-1950s, but profits came largely from strong domestic demand rather than competitive capacity. Late modernization delayed the development of powerful unions and collective bargaining.\n\nIn Italy a shortage of coal led the steel industry to specialize in the use of hydro-electrical energy, exploiting ideas pioneered by Ernesto Stassano from 1898 (Stassano furnace). Despite periods of innovation (1907–14), growth (1915–18), and consolidation (1918–22), early expectations were only partly realized. Steel output in the 1920s and 1930s averaged about 2.1 million metric tons. Per capita consumption was much lower than the average of Western Europe. Electrical processes were an important substitute, yet did not improve competitiveness or reduce prices. Instead, they reinforced the dualism of the sector and initiated a vicious circle that prevented market expansion. Italy modernized its industry in the 1950s and 1960s and it grew rapidly, becoming second only to West Germany in the 1970s. Strong labour unions kept employment levels high. Troubles multiplied after 1980, however, as foreign competition became stiffer. In 1980 the largest producer Nuova Italsider lost 746 billion lira in its inefficient operations. In the 1990s the Italian steel industry, then mostly state-owned, was largely privatised. Today the country is the world's seventh-largest steel exporter.\n\nFrom 1875 to 1920 American steel production grew from 380,000 tons to 60 million tons annually, making the U.S. the world leader. The annual growth rates in steel 1870–1913 were 7.0% for the US; 1.0% for Britain; 6.0% for Germany; and 4.3% for France, Belgium, and Russia, the other major producers. This explosive American growth rested on solid technological foundations and the continuous rapid expansion of urban infrastructures, office buildings, factories, railroads, bridges and other sectors that increasingly demanded steel. The use of steel in automobiles and household appliances came in the 20th century.\n\nSome key elements in the growth of steel production included the easy availability of iron ore, and coal. Iron ore of fair quality was abundant in the eastern states, but the Lake Superior region contained huge deposits of exceedingly rich ore; the Marquette Iron Range was discovered in 1844; operations began in 1846. Other ranges were opened by 1910, including the Menominee, Gogebic, Vermilion, Cuyuna, and, greatest of all, (in 1892) the Mesabi range in Minnesota. This iron ore was shipped through the Lakes to ports such as Chicago, Detroit, Cleveland, Erie and Buffalo for shipment by rail to the steel mills. Abundant coal was available in Pennsylvania and Ohio. Manpower was short. Few native Americans wanted to work in the mills, but immigrants from Britain and Germany (and later from Eastern Europe) arrived in great numbers. \n\nIn 1869 iron was already a major industry, accounting for 6.6% of manufacturing employment and 7.8% of manufacturing output. By then the central figure was Andrew Carnegie, who made Pittsburgh the center of the industry. He'd sold his operations to US Steel in 1901, which became the world's largest steel corporation for decades.\n\nIn the 1880s, the transition from wrought iron puddling to mass-produced Bessemer steel greatly increased worker productivity. Highly skilled workers remained essential, but the average level of skill declined. Nevertheless, steelworkers earned much more than ironworkers despite their fewer skills. Workers in an integrated, synchronized mass production environment wielded greater strategic power, for the greater cost of mistakes bolstered workers' status. The experience demonstrated that the new technology did not decrease worker bargaining leverage by creating an interchangeable, unskilled workforce.\nIn Alabama, industrialization was generating a ravenous appetite for the state’s coal and iron ore. Production was booming, and unions were attempting to organize unincarcerated miners. Convicts provided an ideal captive work force: cheap, usually docile, unable to organize and available when unincarcerated laborers went on strike.\"The Southern agrarian economy did not accommodate convict leasing as well as the industrial economy did, whose jobs were often unappealing or dangerous, offering hard-labor and low pay. The competition, expansion, and growth of mining and steel companies also created a high demand for labor, but union labor posed a threat to expanding companies. As unions bargained for higher wages and better conditions, often organizing strikes in order to achieve their goals, the growing companies would be forced to agree to union demands or face abrupt halts in production. The rate companies paid for convict leases, which paid the laborer nothing, was regulated by government and state officials who entered the labor contracts with companies. \"The companies built their own prisons, fed and clothed the convicts, and supplied guards as they saw fit.\" (Blackmon 2001) Alabama's use of convict leasing was commanding; 51 of its 67 counties regularly leased convicts serving for misdemeanors at a rate of about $5-20 per month, equal to about $160-500 in 2015. Although the influence of labor unions forced some states to move away from the profitable convict lease agreements and run traditional prisons, plenty of companies began substituting convict labor in their operations in the twentieth century. \"The biggest user of forced labor in Alabama at the turn of the century was Tennessee Coal, Iron & Railroad Co., [of] U.S. Steel\"\n\nCarnegie's great innovation was in the cheap and efficient mass production of steel rails for railroad lines. This could not have happened without the prior invention of Bessemer Steel. Thus Carnegie's \"innovation\" was scale, not anything technical.\nIn the late 1880s, The Carnegie Steel was the largest manufacturer of pig iron, steel rails, and coke in the world, with a capacity to produce approximately 2,000 tons of pig iron per day. In 1888, he bought the rival Homestead Steel Works, which included an extensive plant served by tributary coal and iron fields, a 425-mile (685 km) long railway, and a line of lake steamships. A consolidation of Carnegie's assets and those of his associates occurred in 1892 with the launching of the Carnegie Steel Company. \n\nAround that time, he asked his cousin, George Lauder to join him in America from Scotland. Lauder was a leading mechanical engineer who had studied under Lord Kelvin. Lauder devised several new systems for the Carnegie Steel Company including the process for washing and coking dross from coal mines, which resulted in a significant increase in scale, profits, and enterprise value. \n\nLauder would go on to lead the development of the use of steel in armor and armaments for the Carnegie Steel Company, spending significant time at the Krupp factory in Germany in 1886 before returning to build the massive armor plate mill at the Homestead Steel Works that would revolutionize warfare forever. \n\nBy 1889, the U.S. output of steel exceeded that of Britain, and Andrew Carnegie owned a large part of it. By 1900, the profits of Carnegie Bros. & Company alone stood at $480,000,000 with $225,000,000 being Carnegie's share.\nCarnegie's empire grew to include the J. Edgar Thomson Steel Works (named for John Edgar Thomson, Carnegie's former boss and president of the Pennsylvania Railroad), Pittsburgh Bessemer Steel Works, the Lucy Furnaces, the Union Iron Mills, the Union Mill (Wilson, Walker & County), the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines. Carnegie, through Keystone, supplied the steel for and owned shares in the landmark Eads Bridge project across the Mississippi River in St. Louis, Missouri (completed 1874). This project was an important proof-of-concept for steel technology which marked the opening of a new steel market.\n\nThe Homestead Strike was a violent labor dispute in 1892 that ended in a battle between strikers and private security guards. The dispute took place at Carnegie's Homestead Steel Works between the Amalgamated Association of Iron and Steel Workers and the Carnegie Steel Company. The final result was a major defeat for the union and a setback for efforts to unionize steelworkers.\n\nCarnegie sold all his steel holdings in 1901; they were merged into U.S. Steel and it was non-union until the late 1930s.\n\nBy 1900 the US was the largest producer and also the lowest cost producer, and demand for steel seemed inexhaustible. Output had tripled since 1890, but customers, not producers, mostly benefitted. Productivity-enhancing technology encouraged faster and faster rates of investment in new plants. However, during recessions, demand fell sharply taking down output, prices, and profits. Charles M. Schwab of Carnegie Steel proposed a solution: consolidation. Financier J. P. Morgan arranged the buyout of Carnegie and most other major forms, and put Elbert Gary in charge.\n\nUS Steel combined finishing firms (American Tin Plate (controlled by William Henry \"Judge\" Moore), American Steel and Wire, and National Tube) with two major integrated companies, Carnegie Steel and Federal Steel. It was capitalized at $1.466 billion, and included 213 manufacturing mills, one thousand miles of railroad, and 41 mines. In 1901, it accounted for 66% of America's steel output, and almost 30% of the world's. During World War I, its annual production exceeded the combined output of all German and Austrian firms.\n\nThe Steel Strike of 1919 disrupted the entire industry for months, but the union lost and its membership sharply declined. Rapid growth of cities made the 1920s boom years. President Harding and social reformers forced it to end the 12-hour day in 1923.\n\nEarnings were recorded at $2.650 billion for 2016.\n\nCharles M. Schwab (1862–1939) and Eugene Grace (1876–1960) made Bethlehem Steel the second-largest American steel company by the 1920s. Schwab had been the operating head of Carnegie Steel and US Steel. In 1903 he purchased the small firm Bethlehem Steel, and in 1916 made Grace president. Innovation was the keynote at a time when U.S. Steel under Judge Gary moved slowly. Bethlehem concentrated on government contracts, such as ships and naval armor, and on construction beams, especially for skyscrapers and bridges. Its subsidiary Bethlehem Shipbuilding Corporation operated 15 shipyards in World War II. It produced 1,121 ships, more than any other builder during the war and nearly one-fifth of the U.S. Navy's fleet. Its peak employment was 180,000 workers, out of a company-wide wartime peak of 300,000. After 1945 Bethlehem doubled its steel capacity, a measure of the widespread optimism in the industry. However the company ignored the new technologies then being developed in Europe and Japan. Seeking labor peace in order to avoid strikes, Bethlehem like the other majors agreed to large wage and benefits increases that kept its costs high. After Grace retired the inbred executives concentrated on short term profits and postponed innovations that led to long-term inefficiency. It went bankrupt in 2001.\n\nCyrus Eaton (1883–1979) in 1925 purchased the small Trumbull Steel Company of Warren, Ohio, for $18 million. In the late 1920s he purchased undervalued steel and rubber companies. In 1930, Eaton consolidated his steel holdings into the Republic Steel, based in Cleveland; it became the third-largest steel producer in the U.S., after US Steel and Bethlehem Steel.\n\nThe American Federation of Labor (AFL) tried and failed to organize the steelworkers in 1919. Although the strike gained widespread middle-class support because of its demand and the 12-hour day, the strike failed and unionization was postponed until the late 1930s. The Mills ended the 12-hour day in the early 1920s.\n\nThe second surge of unionization came under the auspices of the militant CIO in the late 1930s, when it set up the Steel Workers Organizing Committee. The SWOC focused almost exclusively on the achievement of a signed contract, with \"Little Steel\" (the major producers except for US Steel). At the grassroots however, women of the steel auxiliaries, workers on the picket line, and middle-class liberals from across Chicago sought to transform the strike into something larger than a showdown over union recognition.In Chicago, the Little Steel Strike raised the possibility that steelworkers might embrace the ‘civic unionism’ that animated the left-led unions of the era. The effort failed, and while the strike was won, the resulting powerful United Steelworkers of America union suppressed grassroots opinions.\n\nIntegration was the watchword as the various processes were brought together by large corporations, from mining the iron ore to shipping the finished product to wholesalers. The typical steelworks was a giant operation, including blast furnaces, Bessemer converters, open-hearth furnaces, rolling mills, coke ovens and foundries, as well as supported transportation facilities. The largest ones were operated in the region from Chicago to St. Louis to Baltimore, Philadelphia and Buffalo. Smaller operations appeared in Birmingham, Alabama, and in California.\n\nThe industry grew slowly but other industries grew even faster, so that by 1967, as the downward spiral began, steel accounted for 4.4% of manufacturing employment and 4.9% of manufacturing output. After 1970 American steel producers could no longer compete effectively with low-wage producers elsewhere. Imports and local mini-mills undercut sales.\n\nPer-capita steel consumption in the U.S. peaked in 1977, then fell by half before staging a modest recovery to levels well below the peak.\n\nMost mills were closed. Bethlehem went bankrupt in 2001. In 1984, Republic merged with Jones and Laughlin Steel Company; the new firm went bankrupt in 2001. US Steel diversified into oil (Marathon Oil was spun off in 2001). Finally US Steel reemerged in 2002 with plants in three American locations (plus one in Europe) that employed fewer than one-tenth the 168,000 workers of 1902. By 2001 steel accounted for only 0.8% of manufacturing employment and 0.8% of manufacturing output.\n\nThe world steel industry peaked in 2007. That year, ThyssenKrupp spent $12 billion to build the two most modern mills in the world, in Alabama and Brazil. The worldwide great recession starting in 2008, however, with its heavy cutbacks in construction, sharply lowered demand and prices fell 40%. ThyssenKrupp lost $11 billion on its two new plants, which sold steel below the cost of production. Finally in 2013, ThyssenKrupp offered the plants for sale at under $4 billion.\nThe President of the United States is authorized to declare each May \"Steelmark Month\" to recognize the contribution made by the steel industry to the United States.\n\nYonekura shows the steel industry was central to the economic development of Japan. The nation's sudden transformation from feudal to modern society in the late nineteenth century, its heavy industrialization and imperialist war ventures in 1900–1945, and the post-World War II high-economic growth, all depended on iron and steel. The other great Japanese industries, such as shipbuilding, automobiles, and industrial machinery are closely linked to steel. From 1850 to 1970, the industry increased its crude steel production from virtually nothing to 93.3 million tons (the third largest in the world).\n\nThe government's activist Ministry of International Trade and Industry (MITI) played a major role in coordination. The transfer of technology from the West and the establishment of competitive firms involved far more than buying foreign hardware. MITI located steel mills and organized a domestic market; it sponsored Yawata Steel Company. Japanese engineers and entrepreneurs internally developed the necessary technological and organizational capabilities, planned the transfer and adoption of technology, and gauged demand and sources of raw materials and finances.\n\nThe Bengal Iron Works was founded at Kulti in Bengal in 1870 which began its production in 1874 followed by The Tata Iron and Steel Company (TISCO) was established by Dorabji Tata in 1907, as part of his father's conglomerate. By 1939 it operated the largest steel plant in the British Empire. The company launched a major modernization and expansion program in 1951.\n\nPrime Minister Jawaharlal Nehru, a believer in socialism, decided that the technological revolution in India needed maximization of steel production. He, therefore, formed a government owned company, Hindustan Steel Limited (HSL) and set up three steel plants in the 1950s.\n\nThe Indian steel industry began expanding into Europe in the 21st century. In January 2007 India's Tata Steel made a successful $11.3 billion offer to buy European steel maker Corus Group. In 2006 Mittal Steel (based in London but with Indian management) merged with Arcelor after a takeover bid for $34.3 billion to become the world's biggest steel maker, ArcelorMittal (based in Luxembourg City), with 10% of the world's output.\n\nCommunist party dictator Mao Zedong disdained the cities and put his faith in the Chinese peasantry for a Great Leap Forward. Mao saw steel production as the key to overnight economic modernization, promising that within 15 years China's steel production would surpass that of Britain. In 1958 he decided that steel production would double within the year, using backyard steel furnaces run by inexperienced peasants. The plan was a fiasco, as the small amounts of steel produced were of very poor quality, and the diversion of resources out of agriculture produced a massive famine in 1959–61 that killed millions.\n\nWith economic reforms brought in by Deng Xiaoping, who led China from 1978 to 1992, China began to develop a modern steel industry by building new steel plants and recycling scrap metal from the United States and Europe. As of 2013 China produced 779 million metric tons of steel each year, making it by far the largest steel producing country in the world. This is compared to 165 for the European Union, 110 for Japan, 87 for the United States and 81 for India. China's 2013 steel production was equivalent to an average of 3.14 cubic meters of steel per second.\n\n\n\n\n\n\n"}
{"id": "41798998", "url": "https://en.wikipedia.org/wiki?curid=41798998", "title": "Hydroacoustic Position Reference", "text": "Hydroacoustic Position Reference\n\nHydroacoustic Position Reference is a system to measure the relative position between a transmitter and a receiver under water. It is sometimes used for vessels with dynamic positioning to either measure the relative position between a fixed underwater transmitter or to a mobile under water vehicle (for example ROV).\n"}
{"id": "15287653", "url": "https://en.wikipedia.org/wiki?curid=15287653", "title": "Hydrogen purifier", "text": "Hydrogen purifier\n\nA hydrogen purifier is a device to purify hydrogen if hydrogen production is done from hydrocarbon sources, the ultra-high purified hydrogen is needed for applications like PEM fuel cells .\n\nThe palladium membrane is typically a metallic tube of a palladium and silver alloy material possessing the unique property of allowing only monatomic hydrogen to pass through its crystal lattice when it is heated above 300°C.\n\nDense thin-metal membrane purifiers are compact, relatively inexpensive and simple to use.\n\nPressure swing adsorption is used for the removal of carbon dioxide (CO) as the final step in the large-scale commercial synthesis of hydrogen. It can also remove methane, carbon monoxide, nitrogen, moisture and in some cases, argon, from hydrogen.\n\nCatalytic recombination or deoxygenation is used to remove oxygen (O) impurities. The process is also known as a 'deoxo' process. The oxygen reacts with the hydrogen to form water vapor, which can then be removed by a dryer if necessary. The catalysts that are used are based on platinum group metals (PGM). A typical system could handle up to 3% O2 in H2 in the feed, and reduce the O2 content to less than 1ppm.\n\nThe electrochemical purifier works similar to a fuel cell, a voltage is applied to the membrane and the resulting electric current pulls hydrogen through the membrane. A well designed system can simulanously compress the hydrogen.\n\nHydrogen purifiers are used in metalorganic vapour phase epitaxy reactors for LED production.\n\n\n"}
{"id": "28739443", "url": "https://en.wikipedia.org/wiki?curid=28739443", "title": "IBM System/360 architecture", "text": "IBM System/360 architecture\n\nThe IBM System/360 architecture is the model independent architecture for the entire S/360 line of mainframe computers, including but not limited to the instruction set architecture. The elements of the architecture are documented in the \"IBM System/360 Principles of Operation\" and the \"IBM System/360 I/O Interface Channel to Control Unit Original Equipment Manufacturers' Information\" manuals.\nThe System/360 architecture provides the following features:\n\n\nMemory (\"storage\") in System/360 is addressed in terms of 8-bit bytes. Various instructions operate on larger units called \"halfword\" (2 bytes), \"fullword\" (4 bytes), \"doubleword\" (8 bytes), \"quad word\" (16 bytes) and 2048 byte storage block, specifying the leftmost (lowest address) of the unit. Within a halfword, fullword, doubleword or quadword, low numbered bytes are more significant than high numbered bytes; this is sometimes referred to as big-endian. Many uses for these units require aligning them on the corresponding boundaries. Within this article the unqualified term \"word\" refers to a \"fullword\".\n\nThe original architecture of System/360 provided for up to 2 = 16,777,216 bytes of memory. The later Model 67 extended the architecture to allow up to 2 = 4,294,967,296 bytes of virtual memory.\n\nSystem/360 uses truncated addressing. That means that instructions do not contain complete addresses, but rather specify a base register and a positive offset from the addresses in the base registers. In the case of System/360 the base address is contained in one of 15 general registers. In some instructions, for example shifts, the same computations are performed for 32-bit quantities that are not addresses.\n\nThe S/360 architecture defines formats for characters, integers, decimal integers and hexadecimal floating point numbers. Character and integer instructions are mandatory, but decimal and floating point instructions are part of the Decimal arithmetic and Floating-point arithmetic features.\n\n\nInstructions in the S/360 are two, four or six bytes in length, with the opcode in byte 0. Instructions have one of the following formats:\n\nInstructions must be on a two-byte boundary in memory; hence the low-order bit of the instruction address is always 0.\n\nThe Program Status Word (PSW) contains a variety of controls for the currently operating program. The 64 bit PSW describes (among other things) the address of the current instruction being executed, condition code and interrupt masks. \n\nLoad Program Status Word (LPSW) is a privileged instruction that loads the Program Status Word (PSW), including the program mode, protection key, and the address of the next instruction to be executed. LPSW is most often used to \"return\" from an interruption by loading the \"old\" PSW which is associated with the interruption class. Other privileged instructions (e.g., SSM, STNSM, STOSM, SPKA, etcetera) are available for manipulating subsets of the PSW without causing an interruption or loading a PSW; and one non-privileged instruction (SPM) is available for manipulating the program mask.\n\nThe architecture defines 5 classes of interruption. An interruption is a mechanism for automatically changing the program state; it is used for both synchronous and asynchronous events. \n\nThere are two storage fields assigned to each class of interruption on the S/360; an old PSW double-word and a new PSW double-word. The processor stores the PSW, with an interruption code inserted, into the old PSW location and then loads the PSW from the new PSW location. This generally replaces the instruction address, thereby effecting a branch, and (optionally) sets and/or resets other fields within the PSW, thereby effecting a mode change.\n\nThe S/360 architecture defines a priority to each interruption class, but it is only relevant when two interruptions occur simultaneously; an interruption routine can be interrupted by any other enabled interruption, including another occurrence of the initial interruption. For this reason, it is normal practice to specify all of the mask bits, with the exception of machine-check mask bit, as 0 for the \"first-level\" interruption handlers. \"Second-level\" interruption handlers are generally designed for stacked interruptions (multiple occurrences of interruptions of the same interruption class).\n\nAn I/O interruption occurs at the completion of a channel program, after fetching a CCW with the PCI bit set and also for asynchronous events detected by the device, control unit or channel, e.g., completion of a mechanical movement. The system stores the device address into the interruption code and stores channel status into the CSW at location 64 ('40'X).\n\nA Program interruption occurs when an instruction encounters one of 15 exceptions; however, if the Program Mask bit corresponding to an exception is 0 then there is no interruption for that exception. \nOn 360/65, 360/67 and 360/85 the Protection Exception and Addressing Exception interruptions can be imprecise, in which case they store an Instruction Length Code of 0.\nThe Interruption code may be any of\n\n\nA Supervisor Call interruption occurs as the result of a Supervisor Call instruction; the system stores bits 8-15 of the SVC instruction as the Interruption Code.\n\nAn External interruption occurs as the result of certain asynchronous events. Bits 16-24 of the External Old PSW are set to 0 and one or more of bits 24-31 is set to 1\n\nA Machine Check interruption occurs to report unusual conditions associated with the channel or CPU that cannot be reported by another class of interruption. The most important class of conditions causing a Machine Check is a hardware error such as a parity error found in registers or storage, but some models may use it to report less serious conditions. Both the interruption code and the data stored in the scanout area at '80'x (128 decimal) are model dependent.\n\nThis article describes I/O from the CPU perspective. It does not discuss the channel cable or connectors, but there is a summary elsewhere and details can be found in the IBM literature.\n\nI/O is carried out by a conceptually separate processor called a channel. Channels have their own instruction set, and access memory independently of the program running on the CPU. On the smaller models (through 360/50) a single microcode engine runs both the CPU program and the channel program. On the larger models the channels are in separate cabinets and have their own interfaces to memory. A channel may contain multiple subchannels, each containing the status of an individual channel program. A subchannel associated with multiple devices that cannot concurrently have channel programs is referred to as shared; a subchannel representing a single device is referred to as unshared.\n\nThere are three types of channels on the S/360:\n\n\nConceptually peripheral equipment is attached to a S/360 through \"control units\", which in turn are attached through channels. However, the architecture does not require that control units be physically distinct, and in practice they are sometimes integrated with the devices that they control. Similarly, the architecture does not require the channels to be physically distinct from the processor, and the smaller S/360 models (through 360/50) have integrated channels that steal cycles from the processor.\n\nPeripheral devices are addressed with 16-bit addresses., referred to as \"cua\" or \"cuu\"; this article will use the term \"cuu\". The high 8 bits identify a channel, numbered from 0 to 6, while the low 8 bits identify a device on that channel. A device may have multiple \"cuu\" addresses.\n\nControl units are assigned an address \"capture\" range. For example, a CU might be assigned range 20-2F or 40-7F. The purpose of this is to assist with the connection and prioritization of multiple control units to a channel. For example, a channel might have three disk control units at 20-2F, 50-5F, and 80-8F. Not all of the captured addresses need to have an assigned physical device. Each control unit is also marked as High or Low priority on the channel.\n\nDevice selection progresses from the channel to each control unit in the order they are physically attached to their channel. At the end of the chain the selection process continues in reverse back towards the channel. If the selection returns to the channel then no control unit accepted the command and SIO returns Condition Code 3. Control units marked as High Priority check the outbound CUU to be within their range. If so, then the I/O was processed. If not, then the selection was passed to the next outbound CU. Control units marked as Low Priority check for inbound (returning) CUU to be within their range. If so, then the I/O is processed. If not, then the selection is passed to the next inbound CU (or the channel). The connection of three controls unit to a channel might be physically -A-B-C and, if all are marked as High then the priority would be ABC. If all are marked low then the priority would be CBA. If B was marked High and AC low then the order would be BCA. Extending this line of reasoning then the first of N controllers would be priority 1 (High) or 2N-1 (Low), the second priority 2 or 2N-2, the third priority 3 or 2N-3, etc. The last physically attached would always be priority N.\n\nThere are three storage fields reserved for I/O; a double word I/O old PSW, a doubleword I/O new PSW and a fullword \"Channel Address Word\" (CAW). Performing an I/O normally requires the following:\n\n\nA channel program consists of a sequence of \"Channel Control Words\" (CCWs) chained together (see below.) Normally the channel fetches CCWs from consecutive doublewords, but a control unit can direct the channel to skip a CCW and a \"Transfer In Channel\" (TIC) CCW can direct the channel to start fetching CCWs from a new location.\n\nThere are several defined ways for a channel command to complete. Some of these allow the channel to continue fetching CCWs, while others terminate the channel program. In general, if the CCW does not have the chain-command bit set and is not a TIC, then the channel will terminate the I/O operation and cause an I/O interruption when the command completes. Certain status bits from the control unit suppress chaining.\n\nThe most common ways for a command to complete are for the count to be exhausted when chain-data is not set and for the control unit to signal that no more data transfers should be made. If Suppress-Length-Indication (SLI) is not set and one of those occurs without the other, chaining is not allowed. The most common situations that suppress chaining are unit-exception and unit-check. However, the combination of unit-check and status-modifier does not suppress chaining; rather, it causes the channel to do a command retry, reprocessing the same CCW.\n\nIn addition to the interruption signal sent to the CPU when an I/O operation is complete, a channel can also send a Program-Controlled interruption (PCI) to the CPU while the channel program is running, without terminating the operation, and a delayed device-end interruption after the I/O completion interruption.\n\nThese conditions are detected by the channel and indicated in the CSW.\n\n\nThese conditions are presented to the channel by the control unit or device. In some cases they are handled by the channel and in other cases they are indicated in the CSW. There is no distinction between conditions detected by the control unit and conditions detected by the device.\n\n\nThe fullword \"Channel Address Word\" (CAW) contains a 4-bit storage protection key and a 24-bit address of the channel program to be started.\n\nA \"Channel Command Word\" is a doubleword containing the following:\n\n\nThe low order 2 or 4 bits determine the six types of operations that the channel performs;. The encoding is\nThe meaning of the high order six or four bits, the modifier bits, M in the table above, depends upon the type of I/O device attached, see e.g., DASD CKD CCWs. All eight bits are sent to and interpreted in the associated control unit (or its functional equivalent).\n\nControl is used to cause a state change in a device or control unit, often associated with mechanical motion, e.g., rewind, seek.\n\nSense is used to read data describing the status of the device. The most important case is that when a command terminates with unit check, the specific cause can only be determined by doing a Sense and examining the data returned. A Sense command with the modifier bits all zero is always valid.\n\nA noteworthy deviation from the architecture is that DASD use Sense command codes for Reserve and Release, instead of using Control.\n\nThe flags in a CCW affect how it executes and terminates.\nThe \"Channel Status Word\" (CSW) provides data associated with an I/O interruption.\n\nThe architecture of System/360 specified the existence of several common functions, but did not specify their means of implementation. This allowed IBM to use different physical means, e.g., dial, keyboard, pushbutton, roller, image or text on a CRT, for selecting the functions and values on different processors. Any reference to \"key\" or \"switch\" should be read as applying to, e.g., a light-pen selection, an equivalent keyboard sequence.\n\n\n\nOn some models the alignment requirements for some problem-state instructions were relaxed. There is no mechanism to turn off this feature, and programs depending on receiving a program check type 6 (alignment) on those instructions must be modified.\n\nThe decimal arithmetic feature provides instructions that operate on packed decimal data. A packed decimal number has 1-31 decimal digits followed by a 4-bit sign. All of the decimal arithmetic instructions except PACK and UNPACK generate a Data exception if a digit is not in the range 0-9 or a sign is not in the range A-F.\n\nThe \"Direct Control\" feature provides six external signal lines and an 8-bit data path to/from storage.\n\nThe floating-point arithmetic feature provides 4 64-bit floating point registers and instructions to operate on 32 and 64 bit hexadecimal floating point numbers. The 360/85 and 360/195 also support 128 bit extended precision floating point numbers.\n\nIf the interval timer feature is installed, the processor decrements the word at location 80 ('50'X) at regular intervals; the architecture does not specify the interval but does require that value subtracted make it appear as though 1 were subtracted from bit 23 300 times per second. The smaller models decremented at the same frequency (50 Hz or 60 Hz) as the AC power supply, but larger models had a high resolution timer feature. The processor causes an External interruption when the timer goes to zero.\n\n\"Multi-system operation\" is a set of features to support multi-processor systems, e.g., Direct Control, direct address relocation (prefixing).\n\nIf the storage protection feature is installed, then there is a 4-bit storage key associated with every 2,048-byte block of storage and that key is checked when storing into any address in that block by either a CPU or an I/O channel. A CPU or channel key of 0 disables the check; a nonzero CPU or channel key allows data to be stored only in a block with the matching key.\n\nStorage Protection was used to prevent a defective application from writing over storage belonging to the operating system or another application. This permitted testing to be performed along with production. Because the key was only four bits in length, the maximum number of different applications that could be run simultaneously was 15.\n\nAn additional option available on some models was fetch protection. It allowed the operating system to specify that blocks were protected from fetching as well as from storing.\n\nThe System/360 Model 20 is radically different and should not be considered to be a S/360.\n\nThe System/360 Model 44 is missing certain instructions, but a feature allowed the missing instructions to be simulated in hidden memory thus allowing the use of standard S/360 operating systems and applications.\n\nSome models have features that extended the architecture, e.g., emulation instructions, paging, and some models make minor deviations from the architecture. Examples include:\n\n\nSome deviations served as prototypes for features of the S/370 architecture.\n\n\n"}
{"id": "10065819", "url": "https://en.wikipedia.org/wiki?curid=10065819", "title": "International Service for the Acquisition of Agri-biotech Applications", "text": "International Service for the Acquisition of Agri-biotech Applications\n\nThe International Service for the Acquisition of Agri-biotech Applications (ISAAA) is a non-profit international organization that shares agricultural biotechnology, focusing on genetic engineering.\n\nISAAA operates three regional centers; ISAAA \"SEAsia\"Center, ISAAA \"Afri\"Center and ISAAA \"Ameri\"Center. ISAAA \"SEAsia\"Center is hosted by the International Rice Research Institute (IRRI) in Los Baños, Laguna, Philippines. This center also serves as the Global Coordination Office as well as the home of the Global Knowledge Center on Crop Biotechnology. ISAAA \"Afri\"Center is hosted by the International Livestock Research Institute (ILRI) located in Nairobi, Kenya. ISAAA \"Ameri\"Center is located in Cornell University, Ithaca, New York. It serves as the administrative and financial headquarters of the organization.\n\nThe Global Knowledge Center on Crop Biotechnology is the information network of ISAAA. It published a weekly e-newsletter called \"Crop Biotech Update\" that summarizes global news on agricultural biotechnology. CBU also comes with a bi-weekly Biofuels Supplement that features new developments in energy crops production, processing, Policy, and Economics..\n\nThe ISAAA receives funding from both public and private donors. Some of the ISAAA's funding agencies and companies include the USDA, US Grains Council, Monsanto, Bayer, two banks – Fondazione Bussolera in Italy and Ibercaja in Spain, USAID and the Agricultural Biotechnology Support Project II.\n\nThe organization releases an annual publication on the global status of commercially approved genetically engineered crops. The publication is authored by Clive James, the founder and chair emeritus of ISAAA. The annual brief provides research on global trends in the adoption of major biotech crops since they were first planted commercially. Various environmental groups have accused the ISAAA of inflating the size and impact of genetically modified crops in their report. James says that the report is base on a multiple public and private sources and that he considers it conservative.\n\nThe 2015 report says that \"18 million farmers planted 179.7 million hectares of biotech crops in 28 countries, a marginal decrease of 1% (1.8 million hectares) from 2014.\" As per International Service for the Acquisition of Agri-Biotech Applications (ISAAA)’s latest ‘Global Status of Commercialized Biotech/ GM Crops in 2017’ report, India has the world’s fifth largest cultivated area under genetically modified (GM) crops. The country with the highest area under transgenic crops, at 75 mh, is the United States.\n\nISAAA documents approved GM crops worldwide and presents them in a database available in the organization's website. Each biotech event is featured with a brief description about the crop, trait, transformation method, developer, and summary of regulatory approval. Entries in the database were sourced from Biotechnology Clearing Houses/Regulatory Institutions of approving countries.\n\n\n"}
{"id": "3548103", "url": "https://en.wikipedia.org/wiki?curid=3548103", "title": "Jaquet-Droz automata", "text": "Jaquet-Droz automata\n\nThe Jaquet-Droz automata, among all the numerous automata built by the Jaquet-Droz family, refer to three doll automata built between 1768 and 1774 by Pierre Jaquet-Droz, his son Henri-Louis, and Jean-Frédéric Leschot: the musician, the draughtsman and the writer. The dolls are still functional, and can be seen at the \"Musée d'Art et d'Histoire\" of Neuchâtel, in Switzerland. They are considered to be among the remote ancestors of modern computers.\n\nThe automata were designed and built by Pierre Jaquet-Droz, Henri-Louis Jaquet-Droz and Jean-Frédéric Leschot as advertisement and entertainment toys designed to improve the sales of watches among the nobility of Europe in the 18th century. They were carried around, and lost at several points. The History and Archeology society of Neuchâtel eventually bought them in 1906, for 75,000 gold francs, and gave them to the museum.\n\nThe musician is modelled as a female organ player. The music is not recorded or played by a musical box: the doll plays a genuine, custom-built instrument by pressing the keys with her fingers. Movements of her chest show her \"breathing\", and she follows her fingers with her head and eyes. The automaton also makes some of the movements that a real player would do, such as balancing the torso.\nThe draughtsman is modelled as a young child, and is capable of drawing four different images: a portrait of Louis XV, a royal couple (believed to be Marie Antoinette and Louis XVI), a dog with \"Mon toutou\" (\"my doggy\") written beside it, and a scene of Cupid driving a chariot pulled by a butterfly.\n\nThe draughtsman works by using a system of cams that code the movements of the hand in two dimensions, plus one to lift the pencil. The automaton also moves on his chair, and he periodically blows on the pencil to remove dust.\nThe writer is the most complex of the three automata. Using a system similar to the one used for the draughtsman for each letter, he is able to write any custom text up to 40 letters long (the text is rarely changed; one of the latest instances was in honour of president François Mitterrand when he toured the city). The text is coded on a wheel where characters are selected one by one. He uses a goose feather to write, which he inks from time to time, including a shake of the wrist to prevent ink from spilling. His eyes follow the text being written, and his head moves when he takes some ink.\n\n\n"}
{"id": "45221164", "url": "https://en.wikipedia.org/wiki?curid=45221164", "title": "Krossblade Aerospace Systems", "text": "Krossblade Aerospace Systems\n\nKrossblade Aerospace Systems is an aviation company founded in 2014 in Phoenix, Arizona, USA. The company is known for developing a 5-seat hybrid, vertical take-off and landing, VTOL concept, SkyCruiser, and for its drone/UAV prototype, SkyProwler. Both aircraft employ the switchblade transformation mechanism to transform from a multirotor aircraft for vertical take-off and landing, to a pure winged aircraft, for rapid and efficient cruise.\n\nSkyCruiser is a concept of a vertical take-off and landing transformer aircraft with limited road drive capability. It has 5 seats and is powered by a hybrid power train. While all rotors and propellers are driven by electric motors, the electric energy is produced by a 400 hp internal combustion engine mated to a generator. A small battery provides backup storage of electric energy enabling SkyCruiser to briefly fly purely on stored electric power. SkyCruiser is able to fly at speeds in excess of 300 mph with a range of around 1,000 miles. Although capable of driving on roads, SkyCruiser is optimized for flight. Fast cruise bridges larger distances rapidly, while the VTOL capability enables it to land at or very close to its passengers’ destination. SkyCruiser is scheduled to come to market at the end of the current decade.\n\nSkyProwler is a UAV/drone prototype used by Krossblade to develop the essential mechanisms and concepts for the larger SkyCruiser. It is a purely electrical aircraft that utilizes the switchblade mechanism to transform from multirotor-mode for VTOL to a pure aircraft mode for fast and efficient cruise. Possible applications include delivery. SkyProwler flies with speeds of up to 55 mph.\n\nRather than employing a rigid frame to mount motors and rotors as is done in quadcopters and other multirotors, in the switchblade mechanism motors and rotors are mounted on movable arms. These arms can pivot and so enable storage of the rotors and motors inside the fuselage, where they do not cause aerodynamic drag and hence enable to aircraft to fly faster and more efficient. Compared to a pure winged aircraft, the VTOL system, including the switchblade mechanism, increases the weight of an aircraft by around 15%.\n\n\n"}
{"id": "343857", "url": "https://en.wikipedia.org/wiki?curid=343857", "title": "Lawn mower", "text": "Lawn mower\n\nA lawn mower (also named as mower or lawnmower) is a machine utilizing one or more revolving blades to cut a grass surface to an even height. The height of the cut grass may be fixed by the design of the mower, but generally is adjustable by the operator, typically by a single master lever, or by a lever or nut and bolt on each of the machine's wheels. The blades may be powered by muscle, with wheels mechanically connected to the cutting blades so that when the mower is pushed forward, the blades spin, or the machine may have a battery-powered or plug-in electric motor. The most common power source for lawn mowers is a small (typically one cylinder) internal combustion engine. Smaller mowers often lack any form of propulsion, requiring human power to move over a surface; \"walk-behind\" mowers are self-propelled, requiring a human only to walk behind and guide them. Larger lawn mowers are usually either self-propelled \"walk-behind\" types, or more often, are \"ride-on\" mowers, equipped so the operator can ride on the mower and control it. A robotic lawn mower (\"lawn-mowing bot\", \"mowbot\", etc.) is designed to operate either entirely on its own, or less commonly by an operator by remote control.\n\nTwo main styles of blades are used in lawn mowers. Lawn mowers employing a single blade that rotates about a single vertical axis are known as rotary mowers, while those employing a cutting bar and multiple blade assembly that rotates about a single horizontal axis are known as cylinder or reel mowers (although in some versions, the cutting bar is the only blade, and the rotating assembly consists of flat metal pieces which force the blades of grass against the sharp cutting bar).\n\nThere are several types of mowers, each suited to a particular scale and purpose. The smallest types, non-powered push mowers, are suitable for small residential lawns and gardens. Electrical or piston engine-powered push-mowers are used for larger residential lawns (although there is some overlap). Riding mowers, which sometimes resemble small tractors, are larger than push mowers and are suitable for large lawns, although commercial riding lawn mowers (such as zero-turn mowers) can be \"stand-on\" types, and often bear little resemblance to residential lawn tractors, being designed to mow large areas at high speed in the shortest time possible. The largest multi-gang (multi-blade) mowers are mounted on tractors and are designed for large expanses of grass such as golf courses and municipal parks, although they are ill-suited for complex terrain.\n\nThe first lawn mower was invented by Edwin Budding in 1830 in Thrupp, just outside Stroud, in Gloucestershire, England. Budding's mower was designed primarily to cut the grass on sports grounds and extensive gardens, as a superior alternative to the scythe, and was granted a British patent on August 31, 1830.\nBudding's first machine was wide with a frame made of wrought iron. The mower was pushed from behind. Cast-iron gear wheels transmitted power from the rear roller to the cutting cylinder, allowing the rear roller to drive the knives on the cutting cylinder; the ratio was 16:1. Another roller placed between the cutting cylinder and the main or land roller could be raised or lowered to alter the height of cut. The grass clippings were hurled forward into a tray-like box. It was soon realized, however, that an extra handle was needed in front to help pull the machine along. Overall, these machines were remarkably similar to modern mowers.\n\nTwo of the earliest Budding machines sold went to Regent's Park Zoological Gardens in London and the Oxford Colleges. In an agreement between John Ferrabee and Edwin Budding dated May 18, 1830, Ferrabee paid the costs of enlarging the small blades, obtained letters of patent and acquired rights to manufacture, sell and license other manufacturers in the production of lawn mowers. Without patent, Budding and Ferrabee were shrewd enough to allow other companies to build copies of their mower under license, the most successful of these being Ransomes of Ipswich, which began making mowers as early as 1832.\n\nHis machine was the catalyst for the preparation of modern-style sporting ovals, playing fields (pitches), grass courts, etc. This led to the codification of modern rules for many sports, including for football, lawn bowls, lawn tennis and others.\n\nIt took ten more years and further innovations to create a machine that could be drawn by animals, and sixty years before a steam-powered lawn mower was built. In the 1850s, Thomas Green & Son of Leeds introduced a mower called the Silens Messor (meaning silent cutter), which used a chain drive to transmit power from the rear roller to the cutting cylinder. These machines were lighter and quieter than the gear-driven machines that preceded them, although they were slightly more expensive. The rise in popularity of lawn sports helped prompt the spread of the invention. Lawn mowers became a more efficient alternative to the scythe and domesticated grazing animals.\n\nManufacture of lawn mowers took off in the 1860s. By 1862, Ferrabee's company was making eight models in various roller sizes. He manufactured over 5000 machines until production ceased in 1863. The first grass boxes were flat trays but took their present shape in the 1860s. James Sumner of Lancashire patented the first steam-powered lawn mower in 1893. His machine burned petrol and/or paraffin (kerosene) as fuel. These were heavy machines that took several hours to warm up to operating pressure. After numerous advances, these machines were sold by the Stott Fertilizer and Insecticide Company of Manchester and Sumner. The company they both controlled was called the Leyland Steam Motor Company.\n\nAround 1900, one of the best known English machines was the Ransomes' Automaton, available in chain- or gear-driven models. Numerous manufacturers entered the field with petrol (gasoline) engine-powered mowers after the start of the 20th century. The first was produced by Ransomes in 1902. JP Engineering of Leicester, founded after World War I, produced a range of very popular chain-driven mowers. About this time, an operator could ride behind animals that pulled the large machines. These were the first riding mowers.\nThe first United States patent for a reel lawn mower was granted to Amariah Hills on January 12, 1868. In 1870, Elwood McGuire of Richmond, Indiana designed a human-pushed lawn mower, which was very lightweight and a commercial success. John Burr patented an improved rotary-blade lawn mower in 1899, with the wheel placement altered for better performance. Amariah Hills went on to found the Archimedean Lawn Mower Co. in 1871.\n\nIn the United States, gasoline-powered lawn mowers were first manufactured in 1914 by Ideal Power Mower Co. of Lansing, Michigan, based on a patent by Ransom E. Olds. Ideal Power Mower also introduced the world's first self-propelled, riding lawn tractor in 1922, known as the \"Triplex\". The roller-drive lawn mower has changed very little since around 1930. \"Gang mowers\", those with multiple sets of blades to cut a wider swath, were built in the United States in 1919 by the Worthington Mower Company.\n\nIn the 1920s one of the most successful companies to emerge during this period was Atco, at that time a brand name of Charles H Pugh Ltd. The Atco motor mower, launched in 1921 was an immediate success. Just 900 of the 22-inch-cut machines were made in 1921, each costing £75. Within five years, annual production had accelerated to tens of thousands. Prices were reduced and a range of sizes was available, making the Standard the first truly mass-produced engine-powered mower.\n\nRotary mowers were not developed until engines were small enough and powerful enough to run the blades at sufficient speed. Many people experimented with rotary blade mowers in the late 1920s and early 1930s, and Power Specialties Ltd. introduced a gasoline-powered rotary mower. Kut Kwick replaced the saw blade of the \"Pulp Saw\" with a double-edged blade and a cutter deck, converting the \"Pulp Saw\" into the first ever out-front rotary mower.\n\nOne company that produced rotary mowers commercially was the Australian Victa company, starting in 1952. Its mowers were lighter and easier to use than similar ones that had come before. The first Victa mowers were made at Mortlake, an inner suburb of Sydney, by local resident Mervyn Victor Richardson. He made his first model out of scrap in his garage. The first Victa mowers were then manufactured, going on sale on 20 September 1952. The new company, Victa Mowers Pty Ltd, was incorporated on 13 February 1953.\n\nThe venture was so successful that by 1958 the company moved to much larger premises in Parramatta Road, Concord, and then to Milperra, by which time the mower incorporated an engine, designed and manufactured by Victa, which was specially designed for mowing, rather than employing a general-purpose engine bought from outside suppliers. Two Victa mowers, from 1958 and 1968 respectively, are held in the collection of the National Museum of Australia. The Victa mower is regarded as something of an Australian icon, appearing en masse, in simulated form, at the opening of the Sydney Olympic Games in 2000.\n\nA cylinder mower or reel mower carries a fixed, horizontal cutting blade at the desired height of cut. Over this is a fast-spinning reel of blades which force the grass past the cutting bar. Each blade in the blade cylinder forms a helix around the reel axis, and the set of spinning blades describes a cylinder.\n\nOf all the mowers, a properly adjusted cylinder mower makes the cleanest cut of the grass, and this allows the grass to heal more quickly. The cut of a well-adjusted cylinder mower is straight and definite, as if cut with a pair of scissors. This clean cut promotes healthier, thicker and more resilient lawn growth that is more resistant to disease, weeds and parasites. Lawn cut with a cylinder mower is less likely to result in yellow, white or brown discolouration as a result of leaf shredding. While the cutting action is often likened to that of scissors, it is not necessary for the blades of the spinning cylinder to contact the horizontal cutting bar. If the gap between the blades is less than the thickness of the grass blades, a clean cut can still be made. If more, however, the grass will slip through. Reel mowers also have more difficulty mowing over uneven terrain.\n\nThere are many variants of the cylinder mower. Push mowers have no engine and are usually used on smaller lawn areas where access is a problem, where noise pollution is undesirable and where air pollution is unwanted. As the mower is pushed along, the wheels drive gears which rapidly spin the reel. Typical cutting widths are . Advances in materials and engineering have resulted in these mowers being very light and easy to operate and manoeuvre compared with their predecessors while still giving all the cutting advantages of professional cylinder mowers. Their distinct environmental benefits, both in noise and air pollution, are also strong selling points, something not lost on many international zoos, animal sanctuaries and exclusive hotel groups.\n\nThe basic push mower mechanism is also used in gangs towed behind a tractor. The individual mowers are arranged in a \"v\" behind the tractor with each mower's track slightly overlapping that of the mower in front of it. Gang mowers are used over large areas of turf such as sports fields or parks.\n\nA gasoline engine or electric motor can be added to a cylinder mower to power the cylinder, the wheels, the roller, or any combination of these. A typical arrangement on electric powered machines for residential lawns is for the motor to power the cylinder while the operator pushes the mower along. The electric models can be corded or cordless. On petrol machines the engine drives both the cylinder and the rear roller. Some variants have only three blades in a reel spinning at great speed, and these models are able to cut grass which has grown too long for ordinary push mowers. One type of reel mower, now largely obsolete, was a powered version of the traditional side-wheel push mower, which was used on residential lawns. An internal combustion engine sat atop the reel housing and drove the wheels, usually through a belt. The wheels in turn drove the reel, as in the push mower.\n\nGreens mowers are used for the precision cutting of golf greens and have a cylinder made up of at least eight, but normally ten, blades. The machine has a roller before and after the cutting cylinder which smooths the freshly cut lawn and minimizes wheel marks. Due to the weight, the engine also propels the mower. Much smaller and lighter variants of the roller mower are sometimes used for small patches of ornamental lawns around flower beds, and these have no engine.\n\nRiding reel mowers are also produced. Typically, the cutting reels are ahead of the vehicle's main wheels, so that the grass can be cut before the wheels push the grass over onto the ground. The reels are often hydraulically powered.\n\nThe main parts of a cylinder or reel mower are:\n\n\nA rotary mower rotates about a vertical axis with the blade spinning at high speed relying on impact to cut the grass. This tends to result in a rougher cut and bruises and shreds the grass leaf resulting in discolouration of the leaf ends as the shredded portion dies. This is particularly prevalent if the blades become clogged or blunt. Most rotary mowers need to be set a little higher than cylinder equivalents to avoid scalping and gouging of slightly uneven lawns, although some modern rotaries are fitted with a rear roller to provide a more formal striped cut. These machines will also tend to cut lower (13 mm) than a standard four-wheeled rotary.\n\nThe main parts of a rotary mower are:\n\nExtensive grass trimming was not common before the widespread application of the vertical shaft single cylinder gasoline/petrol engine. In the United States this development paralleled the market penetration of companies such as the Briggs & Stratton company of Wisconsin.\n\nMost rotary push mowers are powered by internal combustion engines. Such engines are usually four-stroke engines, used for their greater torque and cleaner combustion (although a number of older models used two-stroke engines), running on gasoline (petrol) or other liquid fuels. Internal combustion engines used with lawn mowers normally have only one cylinder. Power generally ranges from four to seven horsepower. The engines usually have a carburetor and require a manual pull crank to start them, although an electric starter is offered on some models, particularly large riding and commercial mowers. Some mowers have a throttle control on the handlebar with which the operator can adjust the engine speed. Other mowers have a fixed, pre-set engine speed. All are equipped with a governor (often centrifugal/mechanical or air vane style) to open the throttle as needed to maintain the pre-selected speed when the force needed to cut the thicker or taller grass is encountered. Gasoline mowers have the advantages over electric mowers of greater power and distance range. They do create a significant amount of pollution due to the combustion in the engine, and their engines require periodic maintenance such as cleaning or replacement of the spark plug and air filter, and changing the engine oil.\n\nElectric mowers are further subdivided into corded and cordless electric models. Both are relatively quiet, typically producing less than 75 decibels, while a gasoline lawn mower can be 95 decibels or more.\n\nCorded electric mowers are limited in range by their trailing power cord, which may limit their use with lawns extending outward more than 100–150 feet (30–45 m) from the nearest available power outlet. There is the additional hazard with these machines of accidentally mowing over the power cable, which stops the mower and may put users at risk of receiving a dangerous electric shock. Installing a residual-current device (GFCI) on the outlet may reduce the shock risk.\n\nCordless electric mowers are powered by a variable number (typically 1–4) of 12-volt, 56-volt, and 80-volt rechargeable batteries. Typically, more batteries mean more run time and/or power (and more weight). Batteries can be in the interior of the lawn mower or on the outside. If on the outside, the depleted batteries can be quickly swapped with recharged batteries. Cordless mowers have the maneuverability of a gasoline-powered mower and the environmental friendliness of a corded electric mower, but they are more expensive and come in fewer models (particularly the self-propelling type) than either. The eventual disposal of worn-out batteries is problematic (though some manufacturers offer to recycle them), and the motors in some cordless mowers tend to be less powerful than gasoline motors of the same total weight (including batteries).\n\nWhile considered antiquated with the invention of powered mowers, the original type of push-powered reel mowers are still available. The reel is attached to the mower's wheels by gears, so that when the mower is pushed forward, the reel spins several times faster than the plastic or rubber-tired wheels turn. These types of reel mowers offer the benefit of zero pollution being produced. Since all of the energy necessary comes from the user, however, this method of mowing is the most strenuous and is not recommended for large lawns. Depending on the placement of the reel, these mowers often cannot cut grass very close to lawn obstacles, like trees, driveways, edging, etc., and also require a very smooth lawn surface to operate properly without bottoming out the cutter bar.\n\nHover mowers are powered rotary push lawn mowers that use an impeller above the spinning blades to drive air downward, thereby creating an air cushion that lifts the mower above the ground. The operator can then easily move the mower as it floats over the grass. Hover mowers are necessarily light in order to achieve the air cushion and typically have plastic bodies with an electric motor. The most significant disadvantage, however, is the cumbersome usability in rough terrain or on the edges of lawns, as the lifting air-cushion is destroyed by wide gaps between the chassis and the ground. Hover mowers are built to operate on steep slopes, waterfronts, and high-weeded areas, so they are often used by golf course greenskeepers and commercial landscapers. Grass collection is often available, but can be poor in some models. The quality of cut can be inferior if the grass is pushed away from the blade by the cushion of air.\n\nA robotic mower is contained by a border wire around the lawn that defines the area to be mowed. The robot uses this wire to locate the boundary of the area to be trimmed and in some cases to locate a recharging dock. Robotic mowers are capable of maintaining up to of grass. Robotic lawn mowers are increasingly sophisticated, are usually self-docking and contain rain sensors, nearly eliminating human interaction for mowing grass. Multiple robotic mowers can be used to mow an even larger area.\n\nTractor pulled mowers are usually in the form of an attachment to a tractor. The attachments can simply function by the movement of the tractor similar to manual push cylinder mowers, but also sometimes may have powered moving blades. They are commonly mounted on either the side or the back of the tractor.\n\nRiding mowers (U.S. and Canada) or ride-on mowers (U.K. and Canada) are a popular alternative for large lawns. The operator is provided with a seat and controls on the mower and literally rides on the machine. Most use the horizontal rotating blade system, though usually with multiple blades. A common form of ride-on mower is the lawn tractor. These are usually designed to resemble a small agricultural tractor, with the cutting deck mounted amidships between the front and rear axles.\n\nThe drives for these mowers are in several categories. The most common transmission for tractors is a manual transmission. The second most common transmission type is a form of continuously variable transmission, called the hydrostatic transmission. These transmissions take several forms, from pumps driving separate motors, which may incorporate a gear reduction, to fully integrated units containing a pump, motor and gear reduction. Hydrostatic transmissions are more expensive than mechanical transmissions, but they are easier to use and can transmit greater torque to the wheels compared to a typical mechanical transmission. The least common drive type, and the most expensive, is electric.\n\nThere have been a number of attempts to replace hydrostatic transmissions with lower cost alternatives, but these attempts, which include variable belt types, e.g., MTD's \"Auto Drive\", and toroidal, have various performance or perception problems that have caused their market life to be short or their market penetration to be limited.\n\nRiding lawn mowers can often mount other devices, such as rototillers/rotavators, snow plows, snow blowers, yard vacuums, occasionally even front buckets or fork-lift tines (these are more properly known as \"lawn tractors\" in this case, being designed for a number of tasks).\n\nThe deck of a rotary mower is typically made of steel. Lighter steel is used on less expensive models, and heavier steel on more expensive models for durability. Other deck materials include aluminium, which does not rust and is a staple of higher priced mowers, and hard composite plastic, which does not rust and is lighter and less expensive than aluminium. Electric mowers typically have a plastic deck.\n\nRiding mowers typically have an opening in the side or rear of the housing where the cut grass is expelled, as do most rotary lawn mowers. Some have a grass catcher attachment at the opening to bag the grass clippings.\n\nMulching mowers\nSpecial mulching blades are available for rotary mowers. The blade is designed to keep the clippings circulating underneath the mower until the clippings are chopped quite small. Other designs have twin blades to mulch the clippings to small pieces. This function has the advantages of forgoing the additional work collecting and disposing grass clippings while reducing lawn waste in such a way that also creates convenient compost for the lawn, forgoing the expense and adverse environmental effect of fertilizer.\n\nMower manufacturers market their mowers as side discharge, 2-in-1, meaning bagging and mulching or side discharging and mulching, and 3-in-1, meaning bagging, mulching, and side discharge. Most 2-in-1 bagging and mulching mowers require a separate attachment to discharge grass onto the lawn. Some side discharge mower manufacturers also sell separate \"mulching plates\" that will cover the opening on the side discharge mower and, in combination with the proper blades, will convert the mower to a mulching mower. These conversions are impractical when compared with 2- or 3-in-1 mowers which can be converted in the field in seconds. There are two types of bagging mowers. A rear bag mower features an opening on the back of the mower through which the grass is expelled into the bag. Hi-vac mowers have a tunnel that extends from the side discharge to the bag. Hi-vac is also the type of grass collection used on some riding lawn mowers and lawn tractors and is suitable for use in dry conditions but less suitable for long wet lush grass as they often clog up. Mulching and bagging mowers are not well suited to long grass or thick weeds. In some ride-on mowers, the cut grass is dropped onto the ground and then collected by a set of rotating bristles, allowing even long, wet grass to be collected.\n\nRotary mowers with internal combustion engines come in three price ranges. Low priced mowers use older technology, smaller motors, and lighter steel decks. These mowers are targeted at the residential market and typically price is the most important selling point.\n\nProfessional grass-cutting equipment (used by large establishments such as universities, sports stadiums and local authorities) usually take the form of much larger, dedicated, ride-on platforms or attachments that can be mounted on, or behind, a standard tractor unit (a \"gang-mower\"). Either type may use rotating-blade or cylindrical-blade type cutters, although high-quality mowed surfaces demand the latter. Wide-area mowers (WAMs) are commercial grade mowers which have decks extended to either side, many to . These extensions can be lowered for large area mowing or raised to decrease the mower's width and allow for easy transport on city roads or trailers. Commercial lawn-mowing companies have also enthusiastically adopted types such as the zero-turn mower (in both ride-on and stand-on versions), which allow high-speed over the grass surface, and rapid turn-around at the end of rows, as well as excellent maneuverability around obstacles.\n\nRotary mowers can throw out debris with extreme velocity and energy. Additionally, the blades of a self-powered push mower (gasoline or electric) can injure a careless or inattentive user; as such, many come equipped with a dead man's switch to immediately disable the blade rotation when the user is no longer holding the handle. In the United States, over 12,000 people per year are hospitalized as a result of lawn mower accidents. The vast majority of these injuries can be prevented by wearing protective footwear when mowing. The American Academy of Pediatrics recommends that children be at least 12 years old before they are allowed to use a walk-behind lawn mower and at least 16 years of age before using a riding mower. They also should demonstrate proper judgment and maturity. Persons using a mower should wear heavy footwear, eye protection, and hearing protection in the case of engine-powered mowers.\n\nA 2001 study showed that some mowers produce the same amount of pollution (emissions other than carbon dioxide) in one hour as driving a 1992 model vehicle for . Another estimate puts the amount of pollution from a lawn mower at four times the amount from a car, per hour, although this report is no longer available. Beginning in 2011, the United States Environmental Protection Agency set standards for lawn equipment emissions and expects a reduction of at least 35 percent.\n\nLawn mowers produce GHG emissions. A minimum-maintained lawn management practice with clipping recycling, and minimum irrigation and mowing, is recommended to mitigate global warming effects from urban turfgrass system\n\nMowers can create significant noise pollution, and could cause hearing loss if used without hearing protection for prolonged periods of time. Lawn mowers also present an occupational hearing hazard to the nearly 1 million people who work in lawn service and ground-keeping. A recent study assessed the occupational noise exposure among groundskeepers at several North Carolina public universities and found noise levels from push lawn mowers measured between 86-95 decibels (A-weighted) and from riding lawn mowers between 88 and 96 dB(A); both types exceeded the National Institute for Occupational Safety and Health (NIOSH) Recommended Exposure Limit of 85 dB(A).\n\nTo reduce the possibility of developing hearing loss and reduce noise pollution, users might consider the use of reel mowers or newer \"green\" or battery-operated mowers. Appropriate hearing protection such as earplugs or earmuffs will also help reduce the amount of sound that the user hears, which in turn can reduce hearing loss caused by lawnmowers.\n\n"}
{"id": "9268625", "url": "https://en.wikipedia.org/wiki?curid=9268625", "title": "Ministry of Information and Communication", "text": "Ministry of Information and Communication\n\nMinistry of Information and Communication or Communications may refer to:\n\n\n"}
{"id": "49182501", "url": "https://en.wikipedia.org/wiki?curid=49182501", "title": "Music technology", "text": "Music technology\n\nMusic technology is the use of any device, mechanism, machine or tool by a musician or composer to make or perform music; to compose, notate, play back or record songs or pieces; or to analyze or edit music. The earliest known applications of technology to music was prehistoric peoples' use of a tool to hand-drill holes in bones to make simple flutes. Ancient Egyptians developed stringed instruments, such as harps, lyres and lutes, which required making thin strings and some type of peg system for adjusting the pitch of the strings. Ancient Egyptians also used wind instruments such as double clarinets and percussion instruments such as cymbals. In Ancient Greece, instruments included the double-reed aulos and the lyre. Numerous instruments are referred to in the Bible, including the horn, pipe, lyre, harp, and bagpipe. During Biblical times, the cornet, flute, horn, organ, pipe, and trumpet were also used. During the Middle Ages, music notation was used to create a written record of the notes of plainchant melodies.\n\nDuring the Renaissance music era, the printing press was invented, which made it much easier to mass-produce music (which had previously been hand-copied). This helped to spread musical styles more quickly and across a larger area. During the Baroque era (1600–1750), technologies for keyboard instruments developed, which led to improvements in the designs of pipe organs and harpsichords, and the development of a new keyboard instrument in about 1700, the piano. In the classical era, Beethoven added new instruments to the orchestra to create new sounds, such as the piccolo, contrabassoon, trombones, and untuned percussion in his Ninth Symphony. During the Romantic music era (c. 1810–1900), one of the key ways that new compositions became known to the public was by the sales of sheet music, which amateur music lovers would perform at home on their piano or other instruments. In the 19th century, new instruments such as saxophones, euphoniums, Wagner tubas, and cornets were added to the orchestra.\n\nAround the turn of the 20th century, with the invention and popularization of the gramophone record (commercialized in 1892), and radio broadcasting (starting on a commercial basis ca. 1919-1920), there was a vast increase in music listening, and it was easier to distribute music to a wider public. The development of sound recording had a major influence on the development of popular music genres, because it enabled recordings of songs and bands to be widely distributed. The invention of sound recording gave rise to new subgenre of classical music, the Musique concrete style of electronic composition. The invention of multitrack recording enabled pop bands to overdub many layers of instrument tracks and vocals, creating new sounds that would not be possible in a live performance. In the early 20th century, electric technologies such as electromagnetic pickups, amplifiers and loudspeakers were used to develop new electric instruments such as the electric piano (1929), electric guitar (1931), electro-mechanical organ (1934) and electric bass (1935). The 20th-century orchestra gained new instruments and new sounds. Some orchestra pieces used the electric guitar, electric bass or the Theremin.\n\nThe invention of the miniature transistor in 1947 enabled the creation of a new generation of synthesizers, which were used first in pop music in the 1960s. Unlike prior keyboard instrument technologies, synthesizer keyboards do not have strings, pipes, or metal tines. A synthesizer keyboard creates musical sounds using electronic circuitry, or, later, computer chips and software. Synthesizers became popular in the mass market in the early 1980s. With the development of powerful microchips, a number of new electronic or digital music technologies were introduced in the 1980s and subsequent decades, including drum machines and music sequencers. Electronic and digital music technologies are any device, such as a computer, an electronic effects unit or software, that is used by a musician or composer to help make or perform music. The term usually refers to the use of electronic devices, computer hardware and computer software that is used in the performance, playback, recording, composition, sound recording and reproduction, mixing, analysis and editing of music.\n\nFindings from paleolithic archaeology sites suggest that prehistoric people used carving and piercing tools to create instruments. Archeologists have found Paleolithic flutes carved from bones in which lateral holes have been pierced. The Divje Babe flute, carved from a cave bear femur, is thought to be at least 40,000 years old. Instruments such as the seven-holed flute and various types of stringed instruments, such as the Ravanahatha, have been recovered from the Indus Valley Civilization archaeological sites. India has one of the oldest musical traditions in the world—references to Indian classical music (\"marga\") are found in the Vedas, ancient scriptures of the Hindu tradition. The earliest and largest collection of prehistoric musical instruments was found in China and dates back to between 7000 and 6600 BC.\n\nIn prehistoric Egypt, music and chanting were commonly used in magic and rituals, and small shells were used as whistles. Evidence of Egyptian musical instruments dates to the Predynastic period, when funerary chants played an important role in Egyptian religion and were accompanied by clappers and possibly the flute. The most reliable evidence of instrument technologies dates from the Old Kingdom, when technologies for constructing harps, flutes and double clarinets were developed. Percussion instruments, lyres and lutes were used by the Middle Kingdom. Metal cymbals were used by ancient Egyptians. In the early 21st century, interest in the music of the pharaonic period began to grow, inspired by the research of such foreign-born musicologists as Hans Hickmann. By the early 21st century, Egyptian musicians and musicologists led by the musicology professor Khairy El-Malt at Helwan University in Cairo had begun to reconstruct musical instruments of Ancient Egypt, a project that is ongoing.\n\nThe Indus Valley civilization has sculptures that show old musical instruments, like the seven-holed flute. Various types of stringed instruments and drums have been recovered from Harappa and Mohenjo Daro by excavations carried out by Sir Mortimer Wheeler.\n\nAccording to the Scriptures, Jubal was the father of harpists and organists (Gen. 4:20–21). The harp was among the chief instruments and the favorite of David, and it is referred to more than fifty times in the Bible. It was used at both joyful and mournful ceremonies, and its use was \"raised to its highest perfection under David\" (1 Sam. 16:23). Lockyer adds that \"It was the sweet music of the harp that often dispossessed Saul of his melancholy (1 Sam. 16:14–23; 18:10–11). When the Jews were captive in Babylon they hung their harps up and refused to use them while in exile, earlier being part of the instruments used in the Temple (1 Kgs. 10:12). Another stringed instrument of the harp class, and one also used by the ancient Greeks, was the lyre. A similar instrument was the lute, which had a large pear-shaped body, long neck, and fretted fingerboard with head screws for tuning. Coins displaying musical instruments, the Bar Kochba Revolt coinage, were issued by the Jews during the Second Jewish Revolt against the Roman Empire of 132–135 AD. In addition to those, there was the psaltery, another stringed instrument which is referred to almost thirty times in Scripture. According to Josephus, it had twelve strings and was played with a quill, not with the hand. Another writer suggested that it was like a guitar, but with a flat triangular form and strung from side to side.\n\nAmong the wind instruments used in the biblical period were the cornet, flute, horn, organ, pipe, and trumpet. There were also silver trumpets and the double oboe. Werner concludes that from the measurements taken of the trumpets on the Arch of Titus in Rome and from coins, that \"the trumpets were very high pitched with thin body and shrill sound.\" He adds that in \"War of the Sons of Light Against the Sons of Darkness\", a manual for military organization and strategy discovered among the Dead Sea Scrolls, these trumpets \"appear clearly capable of regulating their pitch pretty accurately, as they are supposed to blow rather complicated signals in unison.\" Whitcomb writes that the pair of silver trumpets were fashioned according to Mosaic law and were probably among the trophies which the Emperor Titus brought to Rome when he conquered Jerusalem. She adds that on the Arch raised to the victorious Titus, \"there is a sculptured relief of these trumpets, showing their ancient form. (see photo)\n\nThe flute was commonly used for festal and mourning occasions, according to Whitcomb. \"Even the poorest Hebrew was obliged to employ two flute-players to perform at his wife's funeral.\" The shofar (the horn of a ram) is still used for special liturgical purposes such as the Jewish New Year services in orthodox communities. As such, it is not considered a musical instrument but an instrument of theological symbolism which has been intentionally kept to its primitive character. In ancient times it was used for warning of danger, to announce the new moon or beginning of Sabbath, or to announce the death of a notable. \"In its strictly ritual usage it carried the cries of the multitude to God,\" writes Werner.\n\nAmong the percussion instruments were bells, cymbals, sistrum, tabret, hand drums, and tambourines. The tabret, or timbrel, was a small hand-drum used for festive occasions, and was considered a woman's instrument. In modern times it was often used by the Salvation Army. According to the Bible, when the children of Israel came out of Egypt and crossed the Red Sea, \"Miriam took a timbrel in her hands; and all the women went out after her with timbrels and with dance.\"\n\nIn Ancient Greece, instruments in all music can be divided into three categories, based on how sound is produced: string, wind, and percussion. The following were among the instruments used in the music of ancient Greece:\n\n\nIn the \"Aeneid\", Virgil makes numerous references to the trumpet. The lyre, kithara, aulos, hydraulis (water organ) and trumpet all found their way into the music of ancient Rome.\n\nThe Romans may have borrowed the Greek method of 'enchiriadic notation' to record their music, if they used any notation at all. Four letters (in English notation 'A', 'G', 'F' and 'C') indicated a series of four succeeding tones. Rhythm signs, written above the letters, indicated the duration of each note. Roman art depicts various woodwinds, \"brass\", percussion and stringed instruments. Roman-style instruments are found in parts of the Empire where they did not originate, and indicate that music was among the aspects of Roman culture that spread throughout the provinces.\n\nRoman instruments include:\n\nDuring the medieval music era (476 to 1400) the plainchant tunes used for religious songs were primarily monophonic (a single line, unaccompanied melody). In the early centuries of the medieval era, these chants were taught and spread by oral tradition (\"by ear\"). The earliest Medieval music did not have any kind of notational system for writing down melodies. As Rome tried to standardize the various chants across vast distances of its empire, a form of music notation was needed to write down the melodies. Various signs written above the chant texts, called \"neumes\" were introduced. By the ninth century, it was firmly established as the primary method of musical notation. The next development in musical notation was \"heighted neumes\", in which neumes were carefully placed at different heights in relation to each other. This allowed the neumes to give a rough indication of the size of a given interval as well as the direction.\n\nThis quickly led to one or two lines, each representing a particular note, being placed on the music with all of the neumes relating back to them. The line or lines acted as a reference point to help the singer gauge which notes were higher or lower. At first, these lines had no particular meaning and instead had a letter placed at the beginning indicating which note was represented. However, the lines indicating middle C and the F a fifth below slowly became most common. The completion of the four-line staff is usually credited to Guido d’ Arezzo (c. 1000-1050), one of the most important musical theorists of the Middle Ages. It should be noted that the neumatic notational system, even in its fully developed state, did not clearly define any kind of rhythm for the singing of notes or playing of melodies. The development of music notation made it faster and easier to teach melodies to new people, and facilitated the spread of music over long geographic distances.\n\nInstruments used to perform medieval music include earlier, less mechanically sophisticated versions of a number of instruments that continue to be used in the 2010s. Medieval instruments include the flute, which was made of wood and could be made as a side-blown or end-blown instrument (it lacked the complex metal keys and airtight pads of 2010s-era metal flutes); the wooden recorder and the related instrument called the gemshorn; and the pan flute (a group of air columns attached together). Medieval music used many plucked string instruments like the lute, mandore, gittern and psaltery. The dulcimers, similar in structure to the psaltery and zither, were originally plucked, but became struck by hammers in the 14th century after the arrival of new technology that made metal strings possible.\n\nBowed strings were used as well. The bowed lyra of the Byzantine Empire was the first recorded European bowed string instrument. The Persian geographer Ibn Khurradadhbih of the 9th century (d. 911) cited the Byzantine lyra as a bowed instrument equivalent to the Arab rabāb and typical instrument of the Byzantines along with the \"urghun\" (organ), \"shilyani\" (probably a type of harp or lyre) and the \"salandj\" (probably a bagpipe). The hurdy-gurdy was a mechanical violin using a rosined wooden wheel attached to a crank to \"bow\" its strings. Instruments without sound boxes like the jaw harp were also popular in the time. Early versions of the organ, fiddle (or vielle), and trombone (called the sackbut) existed in the medieval era.\n\nThe Renaissance music era (c. 1400 to 1600) saw the development of many new technologies that affected the performance and distribution of songs and musical pieces. Around 1450, the printing press was invented, which made printed sheet music much less expensive and easier to mass-produce (prior to the invention of the printing press, all notated music was laboriously hand-copied). The increased availability of printed sheet music helped to spread musical styles more quickly and across a larger geographic area.\n\nMany instruments originated during the Renaissance; others were variations of, or improvements upon, instruments that had existed previously in the medieval era. Brass instruments in the Renaissance were traditionally played by professionals. Some of the more common brass instruments that were played included:\n\n\nStringed instruments included:\n\nPercussion instruments included:\n\nWoodwind instruments included:\n\n\nDuring the Baroque era of music (ca. 1600-1750), technologies for keyboard instruments developed, which led to improvements in the designs of pipe organs and harpsichords, and to the development of the first pianos. During the Baroque period, organ builders developed new types of pipes and reeds that created new tonal colors. Organ builders fashioned new stops that imitated various instruments, such as the viola da gamba. The Baroque period is often thought of as organ building's \"golden age,\" as virtually every important refinement to the instrument was brought to a peak. Builders such as Arp Schnitger, Jasper Johannsen, Zacharias Hildebrandt and Gottfried Silbermann constructed instruments that displayed both exquisite craftsmanship and beautiful sound. These organs featured well-balanced mechanical key actions, giving the organist precise control over the pipe speech. Schnitger's organs featured particularly distinctive reed timbres and large Pedal and Rückpositiv divisions.\n\nHarpsichord builders in the Southern Netherlands built instruments with two keyboards which could be used for transposition. These Flemish instruments served as the model for Baroque-era harpsichord construction in other nations. In France, the double keyboards were adapted to control different choirs of strings, making a more musically flexible instrument (e.g., the upper manual could be set to a quiet lute stop, while the lower manual could be set to a stop with multiple string choirs, for a louder sound. Instruments from the peak of the French tradition, by makers such as the Blanchet family and Pascal Taskin, are among the most widely admired of all harpsichords, and are frequently used as models for the construction of modern instruments. In England, the Kirkman and Shudi firms produced sophisticated harpsichords of great power and sonority. German builders extended the sound repertoire of the instrument by adding sixteen foot choirs, adding to the lower register and two foot choirs, which added to the upper register.\n\nThe piano was invented during the Baroque era by the expert harpsichord maker Bartolomeo Cristofori (1655–1731) of Padua, Italy, who was employed by Ferdinando de' Medici, Grand Prince of Tuscany. Cristofori invented the piano at some point before 1700. While the clavichord allowed expressive control of volume, with harder or louder key presses creating louder sound (and vice versa) and fairly sustained notes, it was too quiet for large performances. The harpsichord produced a sufficiently loud sound, but offered little expressive control over each note. Pressing a harpsichord key harder or softer had no effect on the instrument's loudness. The piano offered the best of both, combining loudness with dynamic control. Cristofori's great success was solving, with no prior example, the fundamental mechanical problem of piano design: the hammer must strike the string, but not remain in contact with it (as a tangent remains in contact with a clavichord string) because this would damp the sound. Moreover, the hammer must return to its rest position without bouncing violently, and it must be possible to repeat the same note rapidly. Cristofori's piano action was a model for the many approaches to piano actions that followed. Cristofori's early instruments were much louder and had more sustain than the clavichord. Even though the piano was invented in 1700, the harpsichord and pipe organ continued to be widely used in orchestra and chamber music concerts until the end of the 1700s. It took time for the new piano to gain in popularity. By 1800, though, the piano generally was used in place of the harpsichord (although pipe organ continued to be used in church music such as Masses).\n\nFrom about 1790 onward, the Mozart-era piano underwent tremendous changes that led to the modern form of the instrument. This revolution was in response to a preference by composers and pianists for a more powerful, sustained piano sound, and made possible by the ongoing Industrial Revolution with resources such as high-quality steel piano wire for strings, and precision casting for the production of iron frames. Over time, the tonal range of the piano was also increased from the five octaves of Mozart's day to the 7-plus range found on modern pianos.\nEarly technological progress owed much to the firm of Broadwood. John Broadwood joined with another Scot, Robert Stodart, and a Dutchman, Americus Backers, to design a piano in the harpsichord case—the origin of the \"grand\". They achieved this in about 1777. They quickly gained a reputation for the splendour and powerful tone of their instruments, with Broadwood constructing ones that were progressively larger, louder, and more robustly constructed.\n\nThey sent pianos to both Joseph Haydn and Ludwig van Beethoven, and were the first firm to build pianos with a range of more than five octaves: five octaves and a fifth (interval) during the 1790s, six octaves by 1810 (Beethoven used the extra notes in his later works), and seven octaves by 1820. The Viennese makers similarly followed these trends; however the two schools used different piano actions: Broadwoods were more robust, Viennese instruments were more sensitive.\n\nBeethoven's instrumentation for orchestra added piccolo, contrabassoon, and trombones to the triumphal finale of his Symphony No. 5. A piccolo and a pair of trombones help deliver storm and sunshine in the Sixth. Beethoven's use of piccolo, contrabassoon, trombones, and untuned percussion in his Ninth Symphony expanded the sound of the orchestra.\n\nDuring the Romantic music era (c. 1810 to 1900), one of the key ways that new compositions became known to the public was by the sales of sheet music, which amateur music lovers would perform at home on their piano or in chamber music groups, such as string quartets. Saxophones began to appear in some 19th-century orchestra scores. While appearing only as featured solo instruments in some works, for example Maurice Ravel's orchestration of Modest Mussorgsky's \"Pictures at an Exhibition\" and Sergei Rachmaninoff's \"Symphonic Dances\", the saxophone is included in other works, such as Ravel's \"Boléro\", Sergei Prokofiev's Romeo and Juliet Suites 1 and 2. The euphonium is featured in a few late Romantic and 20th-century works, usually playing parts marked \"tenor tuba\", including Gustav Holst's \"The Planets\", and Richard Strauss's \"Ein Heldenleben\". The Wagner tuba, a modified member of the horn family, appears in Richard Wagner's cycle \"Der Ring des Nibelungen\" and several other works by Strauss, Béla Bartók, and others; it has a prominent role in Anton Bruckner's Symphony No. 7 in E Major. Cornets appear in Pyotr Ilyich Tchaikovsky's ballet \"Swan Lake\", Claude Debussy's \"La Mer\", and several orchestral works by Hector Berlioz.\n\nThe piano continued to undergo technological developments in the Romantic era, up until the 1860s. By the 1820s, the center of piano building innovation had shifted to Paris, where the Pleyel firm manufactured pianos used by Frédéric Chopin and the Érard firm manufactured those used by Franz Liszt. In 1821, Sébastien Érard invented the double escapement action, which incorporated a \"repetition lever\" (also called the \"balancier\") that permitted repeating a note even if the key had not yet risen to its maximum vertical position. This facilitated rapid playing of repeated notes, a musical device exploited by Liszt. When the invention became public, as revised by Henri Herz, the double escapement action gradually became standard in grand pianos, and is still incorporated into all grand pianos currently produced.\nOther improvements of the mechanism included the use of felt hammer coverings instead of layered leather or cotton. Felt, which was first introduced by Jean-Henri Pape in 1826, was a more consistent material, permitting wider dynamic ranges as hammer weights and string tension increased. The sostenuto pedal, invented in 1844 by Jean-Louis Boisselot and copied by the Steinway firm in 1874, allowed a wider range of effects.\n\nOne innovation that helped create the sound of the modern piano was the use of a strong iron frame. Also called the \"plate\", the iron frame sits atop the soundboard, and serves as the primary bulwark against the force of string tension that can exceed 20 tons in a modern grand. The single piece cast iron frame was patented in 1825 in Boston by Alpheus Babcock, combining the metal hitch pin plate (1821, claimed by Broadwood on behalf of Samuel Hervé) and resisting bars (Thom and Allen, 1820, but also claimed by Broadwood and Érard). The increased structural integrity of the iron frame allowed the use of thicker, tenser, and more numerous strings. In 1834, the Webster & Horsfal firm of Birmingham brought out a form of piano wire made from cast steel; according to Dolge it was \"so superior to the iron wire that the English firm soon had a monopoly.\"\n\nOther important advances included changes to the way the piano is strung, such as the use of a \"choir\" of three strings rather than two for all but the lowest notes, and the implementation of an over-strung scale, in which the strings are placed in two separate planes, each with its own bridge height. The mechanical action structure of the upright piano was invented in London, England in 1826 by Robert Wornum, and upright models became the most popular model, also amplifying the sound.\n\nWith 20th-century music, there was a vast increase in music listening, as the radio gained popularity and phonographs were used to replay and distribute music. The invention of sound recording and the ability to edit music gave rise to new subgenre of classical music, including the acousmatic and Musique concrète schools of electronic composition. Sound recording was also a major influence on the development of popular music genres, because it enabled recordings of songs and bands to be widely distributed. The introduction of the multitrack recording system had a major influence on rock music, because it could do much more than record a band's performance. Using a multitrack system, a band and their music producer could overdub many layers of instrument tracks and vocals, creating new sounds that would not be possible in a live performance.\n\nThe 20th-century orchestra was far more flexible than its predecessors. In Beethoven's and Felix Mendelssohn's time, the orchestra was composed of a fairly standard core of instruments which was very rarely modified. As time progressed, and as the Romantic period saw changes in accepted modification with composers such as Berlioz and Mahler, the 20th century saw that instrumentation could practically be hand-picked by the composer. Saxophones were used in some 20th-century orchestra scores such as Vaughan Williams' Symphonies No.6 and 9 and William Walton's \"Belshazzar's Feast\", and many other works as a member of the orchestral ensemble. In the 2000s, the modern orchestra became standardized with the modern instrumentation that includes a string section, woodwinds, brass instruments, percussion, piano, celeste, and even, for some 20th century or 21st century works, electric instruments such as electric guitar, electric bass and/or electronic instruments such as the Theremin or synthesizer.\n\nElectric music technology refers to musical instruments and recording devices that use electrical circuits, which are often combined with mechanical technologies. Examples of electric musical instruments include the electro-mechanical electric piano (invented in 1929), the electric guitar (invented in 1931), the electro-mechanical Hammond organ (developed in 1934) and the electric bass (invented in 1935). None of these electric instruments produce a sound that is audible by the performer or audience in a performance setting unless they are connected to instrument amplifiers and loudspeaker cabinets, which made them sound loud enough for performers and the audience to hear. Amplifiers and loudspeakers are separate from the instrument in the case of the electric guitar (which uses a guitar amplifier), electric bass (which uses a bass amplifier) and some electric organs (which use a Leslie speaker or similar cabinet) and electric pianos. Some electric organs and electric pianos include the amplifier and speaker cabinet within the main housing for the instrument.\n\nAn electric piano is an electric musical instrument which produces sounds when a performer presses the keys of the piano-style musical keyboard. Pressing keys causes mechanical hammers to strike metal strings or tines, leading to vibrations which are converted into electrical signals by magnetic pickups, which are then connected to an instrument amplifier and loudspeaker to make a sound loud enough for the performer and audience to hear. Unlike a synthesizer, the electric piano is not an electronic instrument. Instead, it is an electro-mechanical instrument. Some early electric pianos used lengths of wire to produce the tone, like a traditional piano. Smaller electric pianos used short slivers of steel, metal tines or short wires to produce the tone. The earliest electric pianos were invented in the late 1920s.\n\nAn electric guitar is a guitar that uses a pickup to convert the vibration of its strings into electrical impulses. The most common guitar pickup uses the principle of direct electromagnetic induction. The signal generated by an electric guitar is too weak to drive a loudspeaker, so it is amplified before being sent to a loudspeaker. The output of an electric guitar is an electric signal, and the signal can easily be altered by electronic circuits to add \"color\" to the sound. Often the signal is modified using electronic effects such as reverb and distortion. Invented in 1931, the electric guitar became a necessity as jazz guitarists sought to amplify their sound in the big band format.\n\nThe Hammond organ is an electric organ, invented by Laurens Hammond and John M. Hanert and first manufactured in 1935. Various models have been produced, most of which use sliding drawbars to create a variety of sounds. Until 1975, Hammond organs generated sound by creating an electric current from rotating a metal tonewheel near an electromagnetic pickup. Around two million Hammond organs have been manufactured, and it has been described as one of the most successful organs. The organ is commonly used with, and associated with, the Leslie speaker. The organ was originally marketed and sold by the Hammond Organ Company to churches as a lower-cost alternative to the wind-driven pipe organ, or instead of a piano. It quickly became popular with professional jazz bandleaders, who found that the room-filling sound of a Hammond organ could form small bands such as organ trios which were less costly than paying an entire big band.\n\nThe electric bass (or bass guitar) was invented in the 1930s, but it did not become commercially successful or widely used until the 1950s. It is a stringed instrument played primarily with the fingers or thumb, by plucking, slapping, popping, strumming, tapping, thumping, or picking with a plectrum, often known as a pick. The bass guitar is similar in appearance and construction to an electric guitar, but with a longer neck and scale length, and four to six strings or courses. The electric bass usually uses metal strings and an electromagnetic pickup which senses the vibrations in the strings. Like the electric guitar, the bass guitar is plugged into an amplifier and speaker for live performances.\n\nElectronic or digital music technology is any device, such as a computer, an electronic effects unit or software, that is used by a musician or composer to help make or perform music. The term usually refers to the use of electronic devices, computer hardware and computer software that is used in the performance, playback, recording, composition, sound recording and reproduction, mixing, analysis and editing of music. Electronic or digital music technology is connected to both artistic and technological creativity. Musicians and music technology experts are constantly striving to devise new forms of expression through music, and they are physically creating new devices and software to enable them to do so. Although in the 2010s, the term is most commonly used in reference to modern electronic devices and computer software such as digital audio workstations and Protools digital sound recording software, electronic and digital musical technologies have precursors in the electric music technologies of the early 20th century, such as the electromechanical Hammond organ, which was invented in 1929. In the 2010s, the ontological range of music technology has greatly increased, and it may now be electronic, digital, software-based or indeed even purely conceptual.\nA synthesizer is an electronic musical instrument that generates electric signals that are converted to sound through instrument amplifiers and loudspeakers or headphones. Synthesizers may either imitate existing sounds (instruments, vocal, natural sounds, etc.), or generate new electronic timbres or sounds that did not exist before. They are often played with an electronic musical keyboard, but they can be controlled via a variety of other input devices, including music sequencers, instrument controllers, fingerboards, guitar synthesizers, wind controllers, and electronic drums. Synthesizers without built-in controllers are often called sound modules, and are controlled using a controller device.\n\n"}
{"id": "39347716", "url": "https://en.wikipedia.org/wiki?curid=39347716", "title": "Nellie Johnstone No. 1", "text": "Nellie Johnstone No. 1\n\nNellie Johnstone No. 1 was the first commercially productive oil well in Oklahoma (at that time in Indian Territory). Completed on April 15, 1897, the well was drilled in the Bartlesville Sand near Bartlesville, opening an era of oil exploration and development in Oklahoma. It was abandoned as a well in 1964. The site was donated to the city of Bartlesville and is now a park, listed on the National Register of Historic Places, featuring a restored drilling rig.\n\nThe well was backed by George B. Keeler and William Johnstone, Keeler had been adopted into the Osage Nation and Johnstone had been adopted into the Delaware Nation after marrying Native American women. Keeler and Johnstone left Bartles to open their own store near the Osage Indian Agency on the Caney River, and was named for Johnstone's daughter. Keeler and Johnstone, together with partner Frank Overlees and their Native American wives, leased from the Cherokee Nation on an area of oil seep and engaged the Cudahy Oil Company to finance the actual drilling operation.\n\nThe firm of McBride and Bloom, headquartered in Independence, Kansas, had already been drilling in the Red Fork field. The original drilling rig had been used at a dry hole near Sapulpa. It took two weeks to move it by oxcart overland to the Bartlesville site.\nThe well went to , and was completed using a then-usual technique of placing a \"torpedo\" (containing a liquid nitroglycerine charge) into the well to fracture the bore and release the oil. Keeler's stepdaughter, Jennie Cass, dropped the \"go devil\" charge, causing the explosive to detonate on impact, in front of fifty spectators. The ensuing gusher produced between 50 and 75 barrels a day, and had to be capped for two years until means could be found to move the oil to a more distant market.\n\nAccording to Kenny Franks' article in the \"Encyclopedia of Oklahoma History & Culture\", the Nellie Johnstone well had not been properly sealed before it was capped. Oil continued seeping into the sump while the well was blocked, eventually overflowing into the nearby Caney River. During the unusually cold winter that followed, a group of children ice skating on the frozen river, built a bonfire to keep themselves warm. Somehow the fire spread close to the oil seep, igniting it. The fire then spread to the Nellie Johnstone, causing major damage to the facility.\n\nThe well was uncapped in 1900, after the Kansas, Oklahoma Central and Southwestern Railway, later acquired by Atchison, Topeka and Santa Fe Railroad, came to Bartlesville, stimulating the development of the Bartlesville field by offering to transport crude to market in Neodesha, Kansas. Nellie Johnstone Cannon, who was six years old at the time the well was drilled and named for her, was granted the land on which the well was drilled by allotment through her Native American ancestry. She sold the land to Bartlesville in 1917. The area is now Johnstone Park.\n\nA replica drilling rig was built over the well in 1948, while the rig was still producing. After the Johnstone No. 1 well was abandoned in 1963, and interest in maintaining the site as a historical monument had begun to grow, the rig scene was reconstructed, using redwood timbers for the derrick. The derrick was rebuilt in 2008.\n\nThe well site was listed on the National Register of Historic Places in 1972.\n\n"}
{"id": "19781493", "url": "https://en.wikipedia.org/wiki?curid=19781493", "title": "Nettop", "text": "Nettop\n\nA nettop (or miniature PC, Mini PC or Smart Micro PC) is a small-sized, inexpensive, low-power, legacy-free desktop computer designed for basic tasks such as Internet surfing, accessing web-based applications, document processing, and audio/video playback. The word \"nettop\" is a portmanteau of Internet and desktop. It is the desktop counterpart of the netbook.\n\nCompared to ordinary desktop computers, nettops are not only smaller and cheaper, but they also consume much less power. For example, CompuLab's fit-PC2 consumes no more than 8 watts of power whereas a typical desktop would easily consume more than 100 watts of power; consequently, nettops require significantly less cooling and may even be completely fanless. Some do not have an optical disk drive and use a solid state drive, making them completely silent. The tradeoff is that the hardware specifications and processing power are usually reduced and hence make nettops less appropriate for running complex or resource-intensive applications.\n\nNettops and Mini PCs have what could be considered an unusual history. The \"first wave\" of such devices, which occurred in the mid to late 2000s, were commonly referred to as \"nettops\". These included devices such as the Acer Aspire Revo seen above, and were commonly considered to be a kind of \"temporary substitute\" PC of a lower cost for users needing a second PC or for use in developing countries. Another commonly held view at the time was their use as a stepping stone towards a Thin Client-based always online computer that would \"replace inefficient PCs\".\n\nAs demand for these devices quickly waned, the industry responded by addressing the chief complaint that these devices would be better as portable devices such as a new form of laptop. The result was the netbook, a device which was considered the true future of the nettop. However, prevailing attitudes and economic issues in 2008 onward made these popular due to their low cost and portability along with the then-expanding feature-set. In August 2009, reports from reviewers were that a netbook of the time and a traditional laptop of the same price were otherwise identical. The implications that the price of standard notebooks should be dropped was a financial liability, due to huge unsold inventories of standard laptops in retail chains and an unfavorable market in which to unload them meant that cannibalisation of laptop sales by netbooks would be financially undesirable for the industry. A clearance sale was also not an option under these conditions, especially among multiple retail chains and online shopping sites. These factors, along with a desire to keep netbook sales going to recoup R&D, design and manufacturing costs, were all likely contributing factors of an industry-wide effort to sabotage netbooks through purposefully limited devices that could be sold cheaply while acting as a form of social engineering towards discrediting netbook devices.\n\nThe direct lineage between nettops and netbooks meant that the concept of a \"net-\" prefix was considered a failed idea. Devices such as Chromebooks, Tablet PCs, Ultrabooks and other devices responded by branding themselves as a different type of device such as Chrome OS being exclusively a pure web client or the proposal that the ultrabook succeeded by compensating for its lighter weight and otherwise equal-performance parts with a higher price tag.\n\nIn 2015, a revival of the concept came about from a likely unrelated source, a technological form of convergent evolution. Via the likely-observed success of the stick PC, the idea of combining a System on a chip with a Single-board computer has led to a continuation of the nettop's original product goals. Mini PCs such as the MINIX Z83-4 or the Azulle Access Plus are exclusively referred to as \"Mini PCs\", despite being identical or near-identical on paper to the nettop architecture.\n\nThere are three platforms that are primarily intended for nettops and netbooks:\n\nSome nettops have also adopted system-on-a-chip designs. Although many major parts such as chipsets, video cards and storage devices can also be found on desktops, the CPUs that are put inside nettops are the fundamental component that differentiate them from normal desktops. The list below contains a range of hardware components that a typical nettop may be assembled from.\n\n\nIntel's Atom processor has been adopted by several hardware manufacturers, such as ASUS, MSI, and Sony, for nettops. Nvidia has also released its first generation ION platform, which puts GeForce 9400M Motherboard GPU alongside the Atom processor to provide better high definition video playback ability and lower power consumption. In addition, Nvidia has announced that it will support VIA's CPUs this year. To further reduce the manufacturing cost and improve power efficiency, many manufacturers and start-up companies have chosen to use CPUs that were originally targeted at embedded computing devices such as AMD's Geode and ARM Cortex-based CPUs.\n\nMany net-top models are x86-processor-based and as such are capable of running standard PC OSs. There are also operating systems designed specifically for nettops and other machines in the same performance class. Some high-end nettops are capable of running Windows 10. Google's Android linux distribution is another option. Although Google's Android was originally designed for smartphones, it has also taken a seat in the nettop market. Another linux system is Ubuntu.\n\nNettops fell into Intel's category of \"Basic PC\", which usually cost from $100 to $299. Intel described nettops as a large potential market at that time. Nettops were said to be able serve as an affordable first computer for people in developing countries, or as an environmentally friendly choice as a secondary computer for people in developed countries.\n\nHowever, as stated above, the primary flaw to the average consumer was that it was far more useful as a portable system. This led then to the development of netbooks. When in the midst of an economic crisis netbooks were seen to be hurting sales of traditional laptops there was likely a collaborative effort to destroy demand for them. Hence the original source of this computer class died off in the marketplace.\n\nAs a result of successes with Stick PCs and Linux Mint-based Nettop-like computers, as well as continued success of the Mac Mini, the idea was possibly unintentionally revived as Mini PCs, which continue to be sold through online retailers as of August, 2017.\n\n\n"}
{"id": "57659684", "url": "https://en.wikipedia.org/wiki?curid=57659684", "title": "OIP Sensor Systems", "text": "OIP Sensor Systems\n\nOIP Sensor Systems is a Belgian defence and space company.\n\nThe company was founded in Ghent in as (Precision Optics and Instruments), as an effort of Belgium to develop its own optical industry for its army.\n\nFrom its foundation until the 1950s, OIP developed military lenses and objectives, medical and scientific microscopes, as well as cameras and photocopy machines for the industry. From the 1960s on, it was one of the first European companies to enter the emerging electro-optical market, with innovations such as one of the first heads-up display for the Lockheed F-104 Starfighter and fire control systems for the Leopard 1 battle tanks in Belgium, Canada and Australia together with SABCA. The company also manufactures holographic night vision googles, and participates in space projects.\n\nOn , the company was acquired from Delft Instruments () by Elbit Systems.\n\n"}
{"id": "13422760", "url": "https://en.wikipedia.org/wiki?curid=13422760", "title": "Reclaimed lumber", "text": "Reclaimed lumber\n\nReclaimed lumber is processed wood retrieved from its original application for purposes of subsequent use. Most reclaimed lumber comes from timbers and decking rescued from old barns, factories and warehouses, although some companies use wood from less traditional structures such as boxcars, coal mines and wine barrels. Reclaimed or antique lumber is used primarily for decoration and home building, for example for siding, architectural details, cabinetry, furniture and flooring.\n\nIn the United States of America, wood once functioned as the primary building material because it was strong, relatively inexpensive and abundant. Today, many of the woods that were once plentiful are only available in large quantities through reclamation. One common reclaimed wood, longleaf pine, was used to build factories and warehouses during the Industrial Revolution. The trees were slow-growing (taking 200 to 400 years to mature), tall, straight, and had a natural ability to resist mold and insects. They were also abundant. Longleaf pine grew in thick forests that spanned over of North America. Reclaimed longleaf pine is often sold as Heart Pine, where the word \"heart\" refers to the heartwood of the tree.\n\nPreviously common woods for building barns and other structures were redwood (Sequoia sempervirens) on the U.S. west coast and American Chestnut on the U.S. east coast. Beginning in 1904, a chestnut blight spread across the US, killing billions of American Chestnuts, so when these structures were later dismantled, they were a welcome source of this desirable but later rare wood for subsequent reuse. American Chestnut wood can be identified as pre- or post-blight by analysis of worm tracks in sawn timber. The presence of worm tracks suggests the trees were felled as dead standing timber, and may be post-blight lumber.\n\nBarns are one of the most common sources for reclaimed wood in the United States. Those constructed through the early 19th century were typically built using whatever trees were growing on or near the builder's property. They often contain a mix of oak, chestnut, poplar, hickory and pine timber. Beam sizes were limited to what could be moved by man and horse. The wood was often hand-hewn with an axe and/or adze. Early settlers likely recognized American oak from their experience with its European species. Red, white, black, scarlet, willow, post, and pin oak varieties have all been used in North American barns.\n\nMill buildings throughout the Northeast also provide an abundant source of reclaimed wood. Wood that is reclaimed from these buildings includes structural timbers - such as beams, posts, and joists - along with decking, flooring, and sheathing. These buildings often have no economic or reuse possibility, can be a fire hazard, and may require varying degrees of environmental cleanup. Reclaiming lumber and brick from these retired mills is considered a better use of materials than landfill-based disposal.\n\nAnother source of reclaimed wood is old snowfence. At the end of their tenure on the mountains and plains of the Rocky Mountain region, snowfence boards are a valued source of consistent, structurally sound and reliable reclaimed wood.\n\nOther woods recycled and reprocessed into new wood products include coast redwood, hard maple, Douglas Fir, walnuts, hickories, red and White Oak, and Eastern white pine.\n\nReclaimed lumber is popular for many reasons: the wood's unique appearance, its contribution to green building, the history of the wood's origins, and the wood's physical characteristics such as strength, stability and durability. The increased strength of reclaimed wood is often attributed to the wood often having been harvested from virgin growth timber, which generally grew more slowly, producing a denser grain. \n\nReclaimed beams can often be sawn into wider planks than newly harvested lumber, and many companies claim their products are more stable than newly-cut wood because reclaimed wood has been exposed to changes in humidity for far longer.\n\nThe reclaimed lumber industry gained momentum in the early 1980s on the West Coast when large-scale reuse of softwoods began. The industry grew due to a growing concern for environmental impact as well as declining quality in new lumber. On the East Coast, industry pioneers began selling reclaimed wood in the early 1970s but the industry stayed mostly small until the 1990s as waste disposal increased and deconstruction became a more economically alternative to demolition. A trade association, the Reclaimed Wood Council, was formed in May 2003 but dissolved in January 2008 due to a lack of participation among the larger reclaimed wood distributors.\n\nReclaimed lumber is sold under a number of names, such as antique lumber, distressed lumber, recovered lumber, upcycled lumber, and others. It is often confused with salvage logging.\n\nThe Leadership in Energy and Environmental Design (LEED) Green Building Rating System is the US Green Building Council's (USGBC) benchmark for designing, building and operating green buildings. To be certified, projects must first meet the prerequisites designated by the USGBC and then earn a certain number of credits within six categories: sustainable sites, water efficiency, energy and atmosphere, materials and resources, indoor environmental quality, innovation and design process. \n\nUsing reclaimed wood can earn credits towards achieving LEED project certification. Because reclaimed wood is considered recycled content, it meets the 'materials and resources' criteria for LEED certification, and because some reclaimed lumber products are Forest Stewardship Council (FSC) certified, they can qualify for LEED credits under the 'certified wood' category.\n\nWith reclaimed material being so popular, it is becoming more difficult to source. With such a high demand, some sellers try to pass newer wood off as antique. \n\nIt is also common (although not necessarily done intentionally) for species to be misidentified because it is difficult to tell the difference in older material unless it is cut open and examined, leaving the material less desirable. Professionals in the field and with established reclaimed wood enterprises do not have difficulty identifying the species.\n\nReclaimed lumber sometimes has pieces of metal embedded in it, such as broken off nails, so milling the material can often ruin planer knives, saw blades, and moulder knives. Nail-compatible saw blades are advisable for the same reason, as well as for safety. The alternative is to remove all metal from the reclaimed lumber, which is a costly and tedious process commonly achieved by scanning each piece of wood with a metal detector and then manually pulling out all nails, bolts, bullets, screws, buckshot, and other miscellaneous metal hardware. This process can make the cost of reclaimed lumber higher than new lumber.\n\nMany sources of reclaimed wood cannot verify what the wood might have been treated with over its lifetime. This uncertainty leads to fears of harmful offgassing of volatile organic compounds associated with lead paint or various stains and treatments that may have been used on the wood. These fears are particularly pressing when the wood is for an interior application.\n\n"}
{"id": "16132980", "url": "https://en.wikipedia.org/wiki?curid=16132980", "title": "Renewable Fuels Agency", "text": "Renewable Fuels Agency\n\nThe Renewable Fuels Agency (or RFA) was a UK Government non-departmental public body, created by the Department for Transport to implement the Renewable Transport Fuel Obligation or RTFO. The Agency ceased to exist at midnight on 31 March 2011\nThe Renewable Fuels Agency (RFA) was the UK’s independent sustainable fuels regulator. The agency awards Renewable Transport Fuel Certificates (RTFCs) to suppliers of biofuels in the UK, ensures companies meet their annual obligations and runs the RTFO’s carbon and sustainability reporting system.\n\nThe key stated aim of the UK Government in introducing the RTFO was to reduce carbon emissions. Under the RTFO, the RFA asks fuel suppliers to report on the specific type and origin of biofuels, the compliance of biofuel crops with existing environmental and social sustainability criteria and the greenhouse gas emissions reductions achieved by using biofuels (based on an analysis of direct lifecycle contributions of crops to greenhouse gas emissions).\n\nThe RFA was responsible for publishing updates on the progress of the RTFO, including progress on achieving compliance with sustainability criteria, on a monthly basis, as well as quarterly reports to the Department for Transport and annual reports to parliament.\n\nThe organisation allocates Renewable Transport Fuel Certificates (RTFCs) to suppliers of biofuels in the UK, ensures obligated companies meet their annual obligation and runs the RTFO's world leading carbon and sustainability reporting system. The RFA promotes the supply of sustainable biofuels, and more than just an administrator, its work helps to drive forward the biofuel sustainability agenda.\n\nSuppliers of biofuels in the UK wishing to claim RTFCs must report to the RFA through the online ‘RFA Operating System (ROS)' the volume of biofuel they supply, and its carbon and sustainability characteristics. The RFA ensures that the data is verifiable and robust, and has a continual program of testing and reviewing its systems to ensure that they are resilient to the possibility of fraud.\n\nTo make a positive contribution to a low carbon future, biofuels must be sustainable. The reports published by the RFA on the carbon and sustainability of biofuel supplied in the UK are the first of their kind in the world. The Agency reports every month on the biofuels supplied in the UK, every quarter on the performance of individual suppliers and every year on the wider impacts of the RTFO.\n\nOn 28 January 2010, the RFA published \"Year One of the RTFO\", the first Annual Report to Parliament on the impacts of the RTFO. The report includes verified data comparing the carbon and sustainability performance of individual fuel suppliers. This is the first time that data of this sort has been published anywhere in the world.\n\nTo assist suppliers in their reporting, the RFA provides a ‘Carbon calculator' to determine the lifecycle emissions from their fuels. The Agency benchmarks feedstock sustainability schemes against the RTFO 'Meta-Standard' for biofuel sustainability.\n\nIn order to understand the impacts of the RTFO, the RFA undertakes and commissions research. These research projects consider one or more of the impact areas outlined in the RTFO order, which are: carbon emissions, agriculture, sustainable development, other economic impacts and the environment generally. Results of the research are communicated through the Agency's regular stakeholder events, through their web site and at appropriate conferences.\n\nThe agency also engages with standards bodies and other stakeholders to promote biofuel sustainability and the systems and mechanisms which support this.\n\nIn July 2008 the RFA published the 'Gallagher Review' into the indirect effects of biofuels production. The report was commissioned by the Secretary of State for Transport in response to growing concern about the impact of rising global demands for biofuels on food prices, biodiversity and greenhouse gas emissions. A growing body of academic research suggested that biofuels would effectively displace agricultural production and cause damaging land-use change in other parts of the world. This led to calls for a moratorium on biofuels policies, particularly from NGOs, whilst the agricultural and biofuels sectors questioned the conclusions of the research and modelling. Governments and policy makers in the EU and elsewhere were trying to make sense of this new evidence at a time when many had recently introduced policies to support renewable fuels to tackle global warming and growing concerns about fuel security. In the EU the Renewable Energy Directive (RED), with ambitious proposals for renewable energy targets, was in the final stages of negotiation.\n\nThe review concluded that projected increased global demand for biofuels did carry significant risks that required urgent mitigation. It found that, whilst there was probably sufficient land for food, feed and biofuels, current policies did not ensure that additional production occurred in appropriate areas. As a result, the displacement of existing agricultural production was likely to lead to reductions in biodiversity and possibly increases in overall greenhouse gas emissions. It also found that biofuels would contribute to rising prices for some commodities that would adversely affect the poorest, but that the scale of these effects was complex and uncertain to model. On the basis of evidence gathered, the Gallagher Review concluded that a slowdown in targets was needed whilst appropriate mitigation measures were put in place.\n\nThe two pieces of legislation that relate directly to this public body and the Government policy it implements are the Energy Act 2004 and the Renewable Transport Fuel Obligation Order 2007.\n\nThe RFA was a small organisation, led by an independent board with six members including the chief executive. The agency had an annual budget of about £1.5 million and was based on the south coast of England in St Leonards-on-Sea.\n\nThe Agency confirmed its closure in 2011 following the UK Government's review of non-departmental public bodies.\n\n"}
{"id": "179898", "url": "https://en.wikipedia.org/wiki?curid=179898", "title": "Robert Hart (horticulturist)", "text": "Robert Hart (horticulturist)\n\nRobert Adrian de Jauralde Hart (1 April 1913 – 7 March 2000) was an English pioneer of forest gardening in temperate zones. He created a model forest garden from a 0.12 acre (500 m²) orchard on his farm. He credits the inspiration for his work to an article by James Sholto Douglas, which was in turn inspired by the work of Toyohiko Kagawa.\n\nHart, son of a lawyer father and a soprano mother, was born in London and educated at Westminster School, after which he was employed at Reuters news agency in charge of special Indian mails, work which involved compiling digests of weekly articles by Mahatma Gandhi, whose nonviolence philosophy he took on board.\n\nAt the outbreak of World War II he considered registering as a conscientious objector but changed mind following retreat from Dunkirk in 1940 and enlisted in the Corps of Military Police, later transferring to the Intelligence Corps to work in code-breaking.\n\nAfter demobilisation, he was a dairy farmer in Norfolk and Somerset before moving to Shropshire where he took on a smallholding on Wenlock Edge.\n\nRobert Hart began with a smallholding called Highwood Hill farm at Wenlock Edge. His intention was to provide a healthy and therapeutic environment for himself and his brother Lacon, who was born with severe learning disabilities.\n\nHart though soon discovered that maintaining large annual vegetable beds, rearing livestock and taking care of an orchard were tasks beyond his strength. However, he also observed that a small bed of perennial vegetables and herbs he had planted was looking after itself with little or no intervention. Furthermore, these plants provided interesting and unusual additions to the diet, and seemed to promote health and vigour in both body and mind.\n\nNoting the maxim of Hippocrates to \"make food your medicine and medicine your food\", Hart adopted a vegan, 90% raw food diet. The three main products from a forest garden are fruit, nuts and green leafy vegetables. Hart's forest garden at Wenlock Edge was a vegan organic food production system.\n\nHe also began to examine the interactions and relationships that take place between plants in natural systems, particularly in woodland, the climax ecosystem of a cool temperate region such as the British Isles. This led him to evolve the agroforestry concept of the \"Forest Garden\": Based on the observation that the natural forest can be divided into distinct layers or 'storeys', he developed an existing small orchard of apples and pears into an edible landscape consisting of seven dimensions;\n\n\nHart had a vision for the spread of the forest garden throughout even the most heavily built up areas, as he explains:\n\nHart died in March 2000 aged 86 and is buried in St Peter's churchyard at Rushbury, Shropshire.\n\n\n"}
{"id": "9665803", "url": "https://en.wikipedia.org/wiki?curid=9665803", "title": "Safety Network International e.V.", "text": "Safety Network International e.V.\n\nSafety Network International e.V. is an association that is based in Ostfildern and is registered at Esslingen district court.\n\nSafety Network International e.V. was established by eight founding companies: ASK Systems GmbH, Dürr AG, Daimler AG, EMG Automation GmbH, Festo AG & Co.KG., SICK AG, Pilz GmbH & Co.KG. and Volkswagen AG. It was established in 1999 under the name \"SafetyBUS p Club International e.V.\" In 2006 the association changed its name to Safety Network International e.V. Almost 70 companies and institutions are now members of Safety Network International e.V. (as of 2013).\n\nIn addition to the headquarters in Germany there are also the following regional organisations of the Safety Network International e.V.\n\n\nThe purpose of the association is to promote the use and dissemination of the safety-related bus system SafetyBUS p and the industrial communication system SafetyNET p . A further objective of the association is to promote integration of the safety-related bus system SafetyBUS p and the industrial communication system SafetyNET p into existing and future automation technologies. Members of the association work together in the Security Workgroup, Infrastructure Committee and Implementation Committee. This is where members discuss common viewpoints, define standards and formulate recommendations. Since 2007 Safety Network International e.V. has been a \"Liaison D\" member of the IEC and is committed to working on standards. The association publishes its own magazine on safety and automation under the name “Connected”.\n"}
{"id": "47158025", "url": "https://en.wikipedia.org/wiki?curid=47158025", "title": "Salux cloth", "text": "Salux cloth\n\nSalux cloth also called Salux nylon, is a type of Japanese washcloth originating in 1966 made of part nylon and polyester. The cloth is known for its ability to soften and smooth skin. Salux may also reduced ingrown hairs after shaving and help exfoliation. Hannah Johnson of xoVain describes the cloth as a \"total game-changer\". Salux won the Japanese Invention Award in 1974.\n\n"}
{"id": "1461224", "url": "https://en.wikipedia.org/wiki?curid=1461224", "title": "Samuel Goudsmit", "text": "Samuel Goudsmit\n\nSamuel Abraham Goudsmit (July 11, 1902 – December 4, 1978) was a Dutch-American physicist famous for jointly proposing the concept of electron spin with George Eugene Uhlenbeck in 1925.\n\nGoudsmit was born in The Hague, Netherlands, of Dutch Jewish descent. He was the son of Isaac Goudsmit, a manufacturer of water-closets, and Marianne Goudsmit-Gompers, who ran a millinery shop. In 1943 his parents were deported to a concentration camp by the German occupiers of the Netherlands and were murdered there.\nGoudsmit studied physics at the University of Leiden under Paul Ehrenfest, where he obtained his PhD in 1927. After receiving his PhD, Goudsmit served as a Professor at the University of Michigan between 1927 and 1946. In 1930 he co-authored a text with Linus Pauling titled \"The Structure of Line Spectra.\"\n\nDuring World War II he worked at the Massachusetts Institute of Technology. He was also the scientific head of the Alsos Mission and successfully reached the German group of nuclear physicists around Werner Heisenberg and Otto Hahn at Hechingen (then French zone) in advance of the French physicist Yves Rocard, who had previously succeeded in recruiting German scientists to come to France. \nAlsos, part of the Manhattan Project, was designed to assess the progress of the Nazi atomic bomb project. In the book \"Alsos,\" published in 1947, Goudsmit concludes that the Germans did not get close to creating a weapon. He attributed this to the inability of science to function under a totalitarian state and the German scientists' lack of understanding how to make an atomic bomb. Both of these conclusions have been disputed by later historians (see Heisenberg) and contradicted by the fact that the totalitarian Soviet state produced the bomb shortly after the book's release.\n\nAfter the war he was briefly a professor at Northwestern University, and from 1948-1970 was a senior scientist at the Brookhaven National Laboratory, chairing the Physics Department 1952-1960. He meanwhile became well known as the Editor-in-chief of the leading physics journal Physical Review, published by the American Physical Society. In July 1958 he started the journal Physical Review Letters. On his retirement as editor in 1974, Goudsmit moved to the faculty of the University of Nevada in Reno, where he remained until his death four years later.\n\nHe also made some scholarly contributions to Egyptology published in \"Expedition\", Summer 1972, pp. 13–16 ; \"American Journal of Archaeology\" 78, 1974 p. 78; and \"Journal of Near Eastern Studies\" 40, 1981 pp. 43–46. The Samuel A. Goudsmit Collection of Egyptian Antiquities resides at the Kelsey Museum of Archaeology at the University of Michigan in Ann Arbor, Michigan.\n\nGoudsmit became a corresponding member of the Royal Netherlands Academy of Arts and Sciences in 1939, though he resigned the next year. He was readmitted in 1950.\n\n"}
{"id": "58743551", "url": "https://en.wikipedia.org/wiki?curid=58743551", "title": "Sarah M. N. Woolley", "text": "Sarah M. N. Woolley\n\nSarah M. N. Woolley is a neuroscientist and Professor of Psychology at Columbia University's Zuckerman Institute. Her work centers on the neuroscience of communication, using songbirds to understand how the brain learns and understands vocal communication.\n\nWoolley received her Bachelors of Arts in 1991 from University of Colorado Boulder, studying Biology and Psychology. She then attended the University of Washington School of Medicine, where she received her PhD in 1999 in Neurobiology and Behavior in the laboratory of Edwin Rubel. Her research centered on how Bengalese finches learn and maintain songs, which serves as a useful model for understanding how experiences contribute to developing and controlling a motor behavior that is socially meaningful. In particular, she found that while male Bengalese finches don't typically change their song patterns in adulthood, their song patterns do require auditory feedback. As a result, if a finch becomes deaf, his song will degrade in about one week. Woolley, however, found that not all sound frequencies are required to maintain a male finch's song. Finches that lost the ability to hear in high-frequency ranges still maintained their ability to sustain a consistent song pattern. She also characterized how these finches can regenerate auditory hair cells, which can restore hearing within eight weeks following damage to the hair cells.\n\nFor her postdoctoral fellowship, she stayed at the University of Washington, moving to the laboratory of John Casseday. There, she performed work to understand the avian auditory midbrain (or the mesencephalicus lateralis, dorsalis, MLd) of zebra finches, which processes multiple parallel inputs and conveys that processed information to the forebrain. Specifically, she characterized how different tones were processed over time in the auditory midbrain. She found that this region of the brain is well-suited to encoding a wide variety of complex sounds with a high degree of temporal accuracy, as opposed to just responding very well to specific sound cues.\n\nIn 2001, Woolley began a second postdoctoral fellowship at the University of California, Berkeley, working with Frederic E. Theunissen. There she studied how zebra finches were able to distinguish vocalizations of specific individuals and also differentiate vocalizations from other sounds coming from the world around them. She found that the finch's auditory neurons were better able to more accurately distinguish between different zebra finch songs than between synthetic sound segments, suggesting that their neurons are more finely tuned to understanding finch vocalizations. She looked at how single neurons and populations of neurons in the auditory midbrain encode song versus generic noise. She found that the majority of auditory midbrain neurons were able to consistently and precisely tune in to finch vocalizations, while they exhibited a high degree of variability in response to generic noise.\n\nIn 2006, Woolley joined the faculty at Columbia University in the Department of Psychology. Between 2013 and 2016 she served as Chairperson for the department and in 2014 became an elected member of the Kavli Institute for Brain Science. Her lab studies the underlying neuroscience of how bengalese, zebra, and long-tailed finches learn, perform, and understand vocalizations as a model to better understand how humans communicate through sound in a variety of contexts. For instance, her lab has studied how song affects mating choice. Male songbirds perform songs to court females, so a bird's song is vitally important to their evolutionary success. Both male and female songbird brains are finely tuned to convert sound waves to social messages. However, Woolley's group found that male and female brains are tuned and wired in different ways. Male songbird brains are wired to learn their songs from the father, while a female songbird's brain is wired to internalize her father's song and compare it to those of potential mates. Females specifically analyze every detail of a potential mate's song, listening for complexity.\n\nHer research is supported by the National Science Foundation and the National Institutes of Health.\n\n"}
{"id": "28433988", "url": "https://en.wikipedia.org/wiki?curid=28433988", "title": "Sea ice thickness", "text": "Sea ice thickness\n\nSea ice thickness spatial extent, and open water within sea ice packs can vary rapidly in response to weather and climate. Sea ice concentration are measured by satellites, with the Special Sensor Microwave Imager / Sounder (SSMIS), and the \"European Space Agency's\" Cryosat-2 satellite to map the thickness and shape of the Earth's polar ice cover. The sea ice volume is calculated with the Pan-Arctic Ice Ocean Modeling and Assimilation System (PIOMAS), which blends satellite-observed data, such as sea ice concentrations into model calculations to estimate sea ice thickness and volume. Sea ice thickness determines a number of important fluxes such as heat flux between the air and ocean surface—see below—as well as salt and fresh water fluxes between the ocean since saline water ejects much of its salt content when frozen—see sea ice growth processes. It is also important for navigators on icebreakers since there is an upper limit to the thickness of ice any ship can sail through.\n\nIce thickness can be measured in various ways, directly by taking an ice core and measuring it or more efficient with satellite measurements. Measurements of ice depth below the waterline (or draft) by submarine sonar or radar systems can give good estimates of ice thickness provided there isn't too much snow (which is less dense than ice) on top.\n\n\"Sea ice freeboard\" is the difference between the height of the surface of sea ice and the water in open leads. Since 7/8ths of the ice is below the waterline, the computation of the thickness is fairly simple, however accurate measurement of ice freeboard is hindered by several factors including snow cover, and modeling of this data is being constantly improved.\n\nThe Ice, Cloud, and land Elevation Satellite (ICESat), measured ice sheet mass balance, cloud and aerosol heights, and land topography and vegetation characteristics, with an active service period from February 2003 to October 2009.\n\nThe European Space Agency Soil Moisture and Ocean Salinity (SMOS) mission is the first orbit mission to measure salinity of the Earth’s surface and able to show data through most clouds and during darkness.\n\nThe sea ice volume is calculated with the Pan-Arctic Ice Ocean Modeling and Assimilation System (PIOMAS).\n\nThe E-M Bird ice thickness meter, designed by the Alfred Wegener Institute for Polar and Marine Research, is carried aloft by helicopter and measures ice thickness with a combination of a pair of inductance coils that measure the ice-water interface based inductance variations—similar to a metal detector—and a laser altimeter which measures the ice surface. It was used on a small scale in 2007 to supplement microwave radiometer measurements during the Pol-Ice campaign and on a much larger scale during the GreenICE (Greenland Arctic Shelf Ice and Climate Experiment) campaign conducted in 2004 and 2005.\n\n"}
{"id": "627008", "url": "https://en.wikipedia.org/wiki?curid=627008", "title": "Skyhook (structure)", "text": "Skyhook (structure)\n\nA skyhook is a proposed momentum exchange tether that aims to reduce the cost of placing payloads into space. A heavy orbiting station is connected to a cable which extends down towards the upper atmosphere. Payloads, which are much lighter than the station, are hooked to the end of the cable as it passes, and are then flung into orbit by rotation of the cable around the centre of mass. The station can then be reboosted to its original altitude by electromagnetic propulsion, rocket propulsion, or by deorbiting another object equal in mass to the payload.\n\nA skyhook differs from a geostationary orbit space elevator in that a skyhook would be much shorter and would not come in contact with the surface of the Earth. A skyhook would require a suborbital launch vehicle to reach its lower end, while a space elevator would not.\n\nDifferent synchronous non-rotating orbiting skyhook concepts and versions have been proposed, starting with Isaacs in 1966, Artsutanov in 1967, Pearson and Colombo in 1975, Kalaghan in 1978, and Braginski in 1985. The versions with best potential involve a much shorter tether in low Earth orbit which rotates in its orbital plane and whose ends brush the upper Earth atmosphere, with the rotational motion cancelling the orbital motion at ground level. These \"rotating\" skyhook versions were proposed by Moravec in 1976, and Sarmont in 1994.\n\nWhen the Italian scientist Giuseppe Colombo proposed in the early 1970s the idea of using a tidally stabilized tether for downward-looking Earth observation satellites, NASA officially began to assess in 1979 the possible scientific applications for long tethers in space and whether the development of a tethered system was justified. This resulted in a Shuttle-based tether system: the TSS-1R mission, launched 22 February 1996 on STS-75 that focused in characterizing basic space tether behavior and space plasma physics. The Italian satellite was deployed to a distance of from the Space Shuttle.\n\nAn engineer speculated in 1994 that the skyhook could be cost competitive with what is realistically thought to be achievable using a space elevator.\n\nIn 2000 and 2001, Boeing Phantom Works, with a grant from NASA Institute for Advanced Concepts, performed a detailed study of the engineering and commercial feasibility of various skyhook designs. They studied in detail a specific variant of this concept, called \"Hypersonic Space Tether Orbital Launch System\" or HASTOL. This design called for a hypersonic ramjet or scramjet aircraft to intercept a rotating hook while flying at Mach 10.\n\nWhile no skyhook has yet been built, there have been a number of flight experiments exploring various aspects of the space tether concept in general.\n\nA non-rotating skyhook is a vertical gravity-gradient stabilized tether whose lower endpoint appears to hang from the sky. It was this appearance that led to the adoption of the name skyhook for the construct. \n\nBy rotating the tether around the orbiting center of mass in a direction opposite to the orbital motion, the speed of the hook relative to the ground can be reduced. This reduces the required strength of the tether, and makes coupling easier.\n\nThe rotation of the tether can be made to exactly match the orbital speed (around 7–8 km/s). In this configuration, the hook would trace out a path similar to a cardioid. From the point of view of the ground, the hook would appear to descend almost vertically, come to a halt, and then ascend again. This configuration minimises aerodynamic drag, and thus allows the hook to descend deep into the atmosphere. However, according to the HASTOL study, a skyhook of this kind in Earth orbit would require a very large counterweight, on the order of 1000–2000 times the mass of the payload, and the tether would need to be mechanically reeled in after collecting each payload in order to maintain synchronization between the tether rotation and its orbit.\n\nPhase I of Boeing's Hypersonic Airplane Space Tether Orbital Launch (HASTOL) study, published in 2000, proposed a 600 km-long tether, in an equatorial orbit at 610–700 km altitude, rotating with a tip speed of 3.5 km/s. This would give the tip a ground speed of 3.6 km/s (Mach 10), which would be matched by a hypersonic airplane carrying the payload module, with transfer at an altitude of 100 km. The tether would be made of existing commercially available materials: mostly Spectra 2000 (a kind of ultra-high-molecular-weight polyethylene), except for the outer 20 km which would be made of heat-resistant Zylon PBO. With a nominal payload mass of 14 tonnes, the Spectra/Zylon tether would weigh 1300 tonnes, or 90 times the mass of the payload. The authors stated:\nThe primary message we want to leave with the Reader is: \"We don't need magic materials like 'Buckminster-Fuller-carbon-nanotubes' to make the space tether facility for a HASTOL system. Existing materials will do.\"\nThe second phase of the HASTOL study, published in 2001, proposed increasing the intercept airspeed to Mach 15–17, and increasing the intercept altitude to 150 km, which would reduce the necessary tether mass by a factor of three. The higher speed would be achieved by using a reusable rocket stage instead of a purely air-breathing aircraft. The study concluded that although there are no \"fundamental technical show-stoppers\", substantial improvement in technology would be needed. In particular, there was concern that a bare Spectra 2000 tether would be rapidly eroded by atomic oxygen; this component was given a technology readiness level of 2.\n\n"}
{"id": "1226276", "url": "https://en.wikipedia.org/wiki?curid=1226276", "title": "Staffordshire Potteries", "text": "Staffordshire Potteries\n\nThe Staffordshire Potteries is the industrial area encompassing the six towns, Tunstall, Burslem, Hanley, Stoke, Fenton and Longton that now make up the city of Stoke-on-Trent in Staffordshire, England. North Staffordshire became a centre of ceramic production in the early 17th century, due to the local availability of clay, salt, lead and coal. Hundreds of companies produced decorative or industrial items.\n\nThe boom came after the discovery in 1720 by potter John Astbury of Shelton, that by adding heated and ground flint powder to the local reddish clay he could create a more palatable white or cream ware. The flint was sourced from either the South Coast of England or France, then shipped to the Port of Liverpool or to Shardlow on the River Trent. After shipping by pack horses to the watermills local to the potteries, or to commercial flint grinding mills in either the Churnet Valley or Moddershall Valley, it was sorted to remove flint that had reddish hues, then heated to to create an easily ground product. A group involving James Brindley later patented a water based process that reduced the generation of fine siliceous dust, thereby reducing the risk to workers of suffering silicosis. In the early 1900s the process was converted to grinding bone, which had a similar effect.\n\nWith the coming of pottery products distribution by railway that began in the 1840s, mainly by the London and North Western Railway and Midland Railway, there was a considerable increase in business. \n\nPotteries active in the 19th century include Aynsley, Burleigh, Doulton, Dudson, Minton, Moorcroft, Twyford, and Wedgwood.\n\nThe Chartist 1842 General Strike was ignited by striking collieries in the Potteries, and led to the 1842 Pottery Riots.\n\n\n"}
{"id": "3609638", "url": "https://en.wikipedia.org/wiki?curid=3609638", "title": "Standing frame", "text": "Standing frame\n\nA standing frame (also known as a stand, stander, standing technology, standing aid, standing device, standing box, tilt table) is assistive technology that can be used by a person who relies on a wheelchair for mobility. A standing frame provides alternative positioning to sitting in a wheelchair by supporting the person in the standing position.\n\nCommon types of standers include: sit to stand, prone, supine, upright, multi-positioning standers, and standing wheelchairs. Long leg braces are also a standing device but not used often today.\n\n\nStanders are used by people with mild to severe disabilities such as spinal cord injury, traumatic brain injury, cerebral palsy, spina bifida, muscular dystrophy, multiple sclerosis, stroke, Rett syndrome, and post-polio syndrome.\n\nSpinal cord injury:\n\nStanders are used by people with both paraplegia and quadriplegia since a variety of support options are available to accommodate for mild to severe disabilities. Doug Betters and Mike Utley are both former NFL football players who are quadriplegics due to spinal cord injury. They both stand using active standers.\n\nBone mineral loss and osteoporosis are common consequences after spinal cord injury. Therapeutic standing, a weight-bearing intervention that can be applied using a standing frame, has traditionally been incorporated into rehabilitation programs for those with chronic spinal cord injury in order to prevent osteoporosis. A systematic review of the literature conducted by Biering-Sorenson et al. (2009) shows that therapeutic standing in the chronic phase of injury, defined as one year after injury, has no effect on maintaining bone density. Results on the effectiveness of therapeutic standing during the first year of injury are conflicting and show that shorter, less aggressive intervention is less effective. If therapeutic standing is to be incorporated into treatment, it should be more aggressive and initiated in the early stages of injury if any beneficial impacts on bone mineral density are hoped to be achieved.\n\nStanding devices are used in a variety of settings including:\n\nFunding (government funding or health insurance) for standing equipment is achievable in most developed countries, but usually requires medical justification and a letter of medical necessity (a detailed medical prescription) written by a physical therapist or medical professional.\n\n\n"}
{"id": "28518791", "url": "https://en.wikipedia.org/wiki?curid=28518791", "title": "Sustainable coffee", "text": "Sustainable coffee\n\nSustainable coffee is coffee that is grown and marketed for its sustainability. This includes coffee certified as organic, fair trade, and Rainforest Alliance. Coffee has a number of classifications used to determine the participation of growers (or the supply chain) in various combinations of social, environmental, and economic standards. Coffees fitting such categories and that are independently certified or verified by an accredited third party have been collectively termed \"sustainable coffees\". This term has entered the lexicon and this segment has quickly grown into a multibillion-dollar industry of its own with potentially significant implications for other commodities as demand and awareness expand.\n\nCoffee has several types of classifications used to determine the participation of growers (or the supply chain) in various combinations of social, environmental, and economic standards. Coffees fitting such categories and that are independently certified or verified by an accredited third party have been collectively termed \"sustainable coffees.\" The term \"sustainable coffee\" was first introduced in expert meetings convened by the Smithsonian Institution Migratory Bird Center (SMBC), NAFTA’s Commission for Environmental Cooperation (CEC) and the Consumer Choice Council (CCC) in 1998. The CCC's 1999 report, \"Sustainable Coffee at the Crossroads\" is the first use of the term in the public sphere. It discusses interpretations of sustainability and identifies options such as organic and fair trade as \"sustainable coffee\", though it does not offer a single functional definition.\n\nThe CCC report emerged during the same period as notable World Bank publications and an IMF paper that were among the first to identify the economic and social problems in coffee origins that would be the basis of the coffee crisis that more fully unfolded early in the 2000s. The SMBC contributed some of the earliest evidence of the environmental impacts occurring in some of the most important coffee growing regions of Central America. The ecological and economic concerns were discussed at meetings hosted by the CEC (\"Workshop of Experts on Sustainably-produced Mexican Coffee\" in Oaxaca in 2000 that resulted in The Oaxaca Declaration. The International Coffee Organization (ICO) voiced and documented some of the factors leading to the crisis, especially the dramatic decline in coffee prices to producers.\n\nInitial trade volumes were estimates because no agency, including the certifiers themselves, accurately tracked them at the time. The first thorough assessment and the first concise definition appeared in research documents commissioned by several organizations in 2001. The Summit Foundation, the Nature Conservancy, the Commission for Environmental Cooperation, the Specialty Coffee Association of America, and the World Bank combined to fund and publish the first large-scale assessment of the markets, the value and the volumes for these coffees (a statistically significant random sample across North America of 1558 retailers, 570 roasters, 312 wholesalers, 120 distributors, and 94 importers). The resulting \"Sustainable Coffee Survey of the North American Specialty Coffee Industry\". indicated the availability of four primary certified sustainable coffees(in order of importance then): Organic, Fair Trade, Bird Friendly (Smithsonian Institution Migratory Bird Center), and Rainforest Alliance.\n\nDuring the nadir of the recent coffee crisis (2001–2003), prices reached record low levels (49 US cents/lb according to the ICO indicator price, April 2001) and left many producers in very difficult conditions. By 2003, the idea of sustainable coffee was starting to become a common topic at conferences, in research, and in policy discussions. \"The State of Sustainable Coffee\" published by the International Coffee Organization and the International Institute for Sustainable Development (IISD) in 2003 noted that sustainable coffees provide new opportunities to coffee producers who face difficult prices and production conditions that otherwise keep them in poverty. The book was the first dedicated to the topic of sustainable coffee and outlines the development of evolving concepts for sustainability in coffee and was also the first to identify the market channels, market conditions and volumes for sustainable coffees in European markets and Japan.\n\nDavid Hallam, the Food and Agriculture Organization of the United Nations' (FAO) commodity chief, in 2003 notes that \"...organic and fair trade products can also command a premium price.\" However, these premiums were somewhat limited. By 2004, a World Bank report, \"Coffee Markets: New Paradigms in Global Supply and Demand\" substantiated that structural shifts in the global industry of coffee will likely hinder significant advances for many producing nations to more equitably participate in what is the world’s most valuable agricultural trade product. It also confirmed coffee's importance in more than 50 countries and its value in a number of producer countries as a primary, and sometimes only, source of cash income for many farmers. It noted that \"differentiated segments\", in which certified coffees such as organic and fair trade are included, \"can provide producers with competitive advantages and added value.\" It further suggested that these are \"important because of their growth rates and their potential to provide better social, economic, or environmental benefits for farmers\". By this point, in mid-decade, the category of sustainable coffees was firmly established as one of the emerging paradigms in the global production and trade of coffee. The same World Bank report identified that the production of such sustainable coffees had expanded beyond mostly Latin American origins to include modest exports from Africa and Asia.\n\nBy the mid-2000s, sustainable coffees came to include new certification initiatives such as UTZ Certified and Common Code for the Coffee Community (4C) as well as certifications used exclusively by individual firms (Starbucks and Nespresso). Most certifications, by the decade’s end are now widely available not only in specialty stores and cafés but also in major supermarkets and under national brand names of global food companies such as Kraft and Sara Lee. At the ICO 2010 World Coffee Conference, former World Bank coffee expert Daniele Giovannucci noted that in 2009 more than 8% of the global trade in raw (green) coffee was certified to one or another of the major sustainability initiatives. Though growing quickly, sustainable certified coffees still constitute only a few percent of the total purchasing of the largest coffee brands owned by Nestlé, Kraft, and Sara Lee. The leading global brands, in terms of volumes purchased, are Starbucks, whose private certification (C.A.F.E. Practices) covers nearly 90% of its purchases, and Nespresso whose purchase of sustainable coffees (Rainforest Alliance Certified) now accounts for more than half of its total buying. \n\nFrom a market share of zero to a share 8% of the global coffee industry in one decade suggests that sustainable coffees are no longer a small niche. Efforts are underway by various certification bodies, non-governmental organizations (NGOs) and global food companies to develop the production of sustainable coffees in the poorest regions of the world, such as Africa, and to measure the actual impacts that the various initiatives, standards and certifications. Whilst a number of papers have been published on the topic, high quality research is still lacking. Resources for the Future, a research think tank, undertook a broad literature review in 2010 and identified 37 relevant studies, only 14 of which use methods likely to generate credible results. Allen Blackman and Jorge Rivera, the authors of \"The Evidence Base for Environmental and Socioeconomic Impacts of 'Sustainable' Certification\" conclude that empirical evidence is limited and that much more research is necessary to understand whether these initiatives are having the claimed impacts.\n\nThe International Social and Environmental Accreditation and Labelling (ISEAL) Alliance is a global association for social and environmental standards whose members include many of the major standards systems active in sustainable coffee such as: Fair trade, Rainforest Alliance, UTZ Certified and the 4C Association. Its members have resolved to abide by applying a new Impacts Code in 2010 that requires them to develop a transparent Assessment Plan to provide reasonable measurement of their impacts. Another initiative is already developing and applying scientific metrics to understand sustainability impacts at the field level. The non-profit Committee on Sustainability Assessment (COSA), is a consortium of global organizations led by the International Institute for Sustainable Development, (IISD) and the United Nations Conference on Trade and Development (UNCTAD), as part of their Sustainable Commodity Initiative (SCI), is already developing and applying scientific metrics to understand sustainability impacts at the field level. COSA’s stated purpose is to measure sustainability and its mandate is to achieve \"a credible set of common global measures for agricultural sustainability along the three balanced principles (environmental, social, and economic)\". The unanimous International Coffee Organization endorsement of the COSA program notes that COSA builds management capacity with local partnerships in producing countries to facilitate an understanding of the effects (costs and benefits) of the many sustainability initiatives. The United Nations International Trade Centre (ITC) and its Trade for Sustainable Development program is also developing a global online platform to better understand the distinctions of the diverse sustainability initiatives with basic comparisons of the standards and also a mapping system of their availability. ITC has also announced partnering with COSA to make the COSA database of thousands of scientific observations on this topic available publicly in 2011-12.\n\n"}
{"id": "27379753", "url": "https://en.wikipedia.org/wiki?curid=27379753", "title": "Tech News Today", "text": "Tech News Today\n\nTech News Today (TNT) was a podcast with discussions of new stories in conversation with journalists. The netcast is part of the TWiT Network. The show premiered on June 1, 2010. It was originally hosted by Tom Merritt for the first 912 episodes of the show. Becky Worley co-hosted the show from 2010 to 2012. At the end of 2015, Jason Howell and Megan Morrone took over the co-hosting duties for the show.\n\n\"Tech News Today\" was started by Merritt, Becky Worley, Erik Lanigan, and Laporte (although Laporte was only there for the first episode). Sarah Lane joined the show as co-host on Mondays and Fridays and started working every day on the show after Worley left the show. Lane's last Tech News Today show was episode 941 and she went on to host Tech News 2Night, TWiT's evening news show every weeknight. Iyaz Akhtar also joined the show as a guest in episode 4 and joined as a co-host on episode 209. Akhtar's last \"Tech News Today\" show was episode 911 and he went on to become Senior Editor at CNET. Jason Howell joined Tech News Today at episode 83. Howell's last \"Tech News Today\" was episode 1122. He went on to replace Chad Johnson, becoming \"Laporte's producer,\" producing \"This Week in Tech\", \"MacBreak Weekly\", \"This Week in Google\", and \"Triangulation\" netcasts. The show is now hosted by Mike Elgan who joined \"Tech News Today\" at episode 913, and rotating co-anchors. Episode 1 of TNT appeared on June 1, 2010. \"Tech News Today\" is shown through a live video feed along with many of the rest of the TWiT network shows.\n\nMerritt moved to Los Angeles, California, to accommodate his wife Eileen Rivera's new job at YouTube. \"Tech News Today\" continued on with Merritt being a Skype host, and he did his first show as a host via Skype in Episode 671 from his home studio in Los Angeles.\n\nOn December 5, 2013, host Leo Laporte announced in a blog post on the TWiT website, that Merritt would not be returning to the show due to Laporte wanting an \"in studio host/news director.\" Later, Jordan Settlemyre leaked that Leo claims that Tom Merritt wanted a $100K raise that TWiT couldn't afford, saying \"He sucked twit dry for his own personal gain.\"\n\nOn December 8, 2015, TWiT announced that Elgan would move on, leaving TWiT.tv without a news director. TWiT went on to also drop the companion show Tech News 2Night. Tech News 2Night host Megan Morrone and producer Jason Howell took over \"Tech News Today\" on a daily basis at 4PM each weekday, starting on January 4, 2016. The end of \"Tech News Today\" was announced on October 1, 2017. It was replaced with a weekly show called \"Tech News Weekly\".\n\nThe show featured three segments during the Merritt era of \"Tech News Today\": The News Fuse where each host talked about the top stories of that day for around 5 minutes (this was followed by an ad read), Merritt would then welcome the guest(s) on that show to discuss the news of the day and talk about/analyze it and the Randomizer were the audience decided based on a Strawpoll what the final unusual tech news story was going to be. Elgan had a similar structure when he hosted Top News replacing The News Fuse, The Conversation, and In Other News replacing the Randomizer. This format was replaced, with the shows merged into one segment.\n\nLen Peralta, a graphic artist, illustrator, and cartoonist, joined \"Tech News Today\" at Episode 756 on Fridays to illustrate the show. The drawings would then go up to Peralta's web site for sale. As of the beginning of Elgan's tenure, this has stopped.\n\n\n\"Tech News Today\" was the recipient of the 2012 International Academy of Web Television award for Best News Web Series. \"Tech News Today\" was also named Best of 2010 in Podcasts by iTunes Rewind.\n\n\n"}
{"id": "37984303", "url": "https://en.wikipedia.org/wiki?curid=37984303", "title": "The Nexus Trilogy", "text": "The Nexus Trilogy\n\nThe Nexus Trilogy is a postcyberpunk thriller novel trilogy written by American author Ramez Naam and published between 2012-2015. The novel series follows the protagonist Kaden Lane, a scientist who works on an experimental nano-drug, Nexus, which allows the brain to be programmed and networked, connecting human minds together. As he pursues his work, he becomes entangled in government and corporate intrigue. The story takes place in the year 2040.\n\n\"Nexus\" tied for Best Novel in the 2014 Prometheus Awards given out by the Libertarian Futurist Society. It was also shortlisted for the 2014 Arthur C. Clarke award. \"Nexus\" was published in 2012. Its sequel, \"Crux\", was published in 2013. The third volume of the trilogy, \"Apex\", was published in 2014, and won the 2015 Philip K. Dick Award. The film rights to \"Nexus\" were purchased by Paramount in 2013.\n\nSamantha Cataranes (Sam), an agent for the Emerging Risks Directorate (ERD) of the United States government, arrives undercover at a party looking for Kaden Lane. Kaden is there testing Nexus 5, an illegal, experimental nano-drug for direct input and output of brain signals. Sam talks with Kaden about his work and he invites her to be a part of a Nexus 5 study. Sam goes to the study and meets Kaden's close friends and colleagues: Rangan Shankari, Ilya Alexander, and Watson Cole (Wats). Sam takes Nexus 5, connecting her mind with the others, and they discover who she is and Kade uses Nexus to knock her out. When Sam awakes she threatens the group with prison, and promises a pardon in exchange for Kade's help. Wats escapes before the ERD extracts the group. The ERD describe a mission to spy on Su-Yong Shu, a brilliant Chinese neuro-scientist who is implicated in murder and brain control coercion. Kade agrees to work with the ERD and hand over Nexus 5. Kade and the group are sent to retrieve the Nexus 5 data, and on the way, they install a backdoor into the Nexus 5 operating system.\n\nSam is required to have permanent integration with Nexus 5, despite her disagreement with Warren Becker, the Enforcement Division Deputy Director at the ERD. Kade and Sam, now with the pseudonym of Robyn Rodriguez, travel to Bangkok for a conference that Kade is invited to by Shu. Wats follows in hopes of setting Kade free and spreading Nexus 5 to the general public. At registration, Kade hears an inspiring talk from Somdet Phra Ananda, discussing a Nexus-like topic, and meets Narong, a PHD student. Narong invites Kade to a student mixer the following night. After returning to the hotel Kade finds a secret note left by Wats, informing him of escape if needed. Kade recognizes that he needs to stay and attempts to notify Wats. Wats never receives the message.\n\nAt the opening night reception Sam discovers Narong is a known associate of Suk Prat-Nung, a nephew of Thanom Prat-Nung, a Thai Drug Dealer. Sam decides it is important to continue to track Narong in hopes of catching the Thai drug ring leaders. Kade finds Shu and is invited for lunch the following day, later changed to a dinner as she meets first with Ananda. Kade also discovers that Professor Ananda is under the influence of Nexus. After returning to the hotel, Sam reviews tapes from the day and discovers the interaction between Ananda and Kade and that Ananda also followed Kade home. Sam also discovers that a note has been passed to Kade, however she does not know that it was from Wats. Kade meets with Shu for dinner where she reaches into his mind to discover what he knows. Kade fights back and employs a mantra to rearrange his memories. Shu reverses the effect and discovers that Kade is working with the ERD. Kade discovers that Shu is the first mind uploaded to a computer system. She is trans-human and attempts to convince Kade to join her in fighting humans. Kade asks for time to think about her proposal. She creates false memories so that Sam and the ERD will not discover their true conversation.\n\nKade and Sam attend the student mixer to meet with Narong. He invites them to an after party in another area. On the way they go to Sukchai, an underground black market for everything trans-human. Kade realizes that legalizing these products would protect people, while Sam struggles with what is right. At the after party, Kade and Sam are invited to a Synchronicity party with Narong to try Nexus and another drug, Empathek. While leaving the party Sam and Kade are attacked. Sam defeats the attackers and calls for backup. The ERD backup arrives and extracts them. Sam confronts Kade about the interaction with Ananda, which Kade denies and arranges his memories to cover it up. The following day, Kade is approached by Shu at the conference. He recounts the previous nights events and Shu denies it was her people. Shu again tries to convince Kade that he should join the trans-humans. Kade takes a call from Ilya that reminds him that technology in the hands of only the elite is dangerous. Kade talks privately with Ananda. Ananda describes Buddhism as a democracy and how technology and knowledge must be shared.\n\nMeanwhile, Wats discovers that Suk Prat-Nung set up the ambush on Kade and Sam. He also discovers a plot to ambush them again during their Synchronicity party and sends an email to Kade to warn him. Sam sees the email and notifies ERD to be available in case of an ambush. At the Synchronicity party, Sam and Kade take Nexus and Empathek. They both use their mantras to change their memories. Sam, believing she is now Robyn Rodriguez, talks with Mai, a young girl that was born with Nexus abilities. Mai unlocks Sam's true memories and contributes to a great change inside Sam. Sam, overwhelmed by her sudden realization of the power of these drugs and released from her horrible past, talks with Kade. She releases his true memories and recounts her dark past. They both fall asleep. Upon waking they find Thanom Prat-Nung and his guards in the room. Narong, under the influence of Nexus 5 and the ERD, draws a gun on Thanom. Narong is killed by Thanom's guards and the ERD sends three team of men into the building, open firing despite Sam's warnings of civilians. Mai and the other members of the party are killed. Sam kills several ERD agents. Kade fights the ERD soldiers. Wats joins the fight and is killed. The ERD detonate explosions in the skulls of the soldiers. Sam and Kade escape.\n\nSuk Prat-Nung still alive, with a few of his men, continues to fight Sam and Kade. Kade is captured while Sam defeats some of Suk's men. Kade uses Nexus 5 to control the actions of his captor, Suk, to escape. Suk is killed. Feng, Shu's driver and a super soldier clone, rescues Kade and Sam, bringing them to a monastery. Sam and Kade spend some time recovering at the monastery while the ERD searches for them. An ERD recon spider robot discovers Sam and Kade's location, and the ERD sends a team to retrieve them. On her way to the monastery, Shu and Feng discover the ERD helicopters going to the monastery. They get a message to the monastery. Kade decides to mass distribute the Nexus 5 instructions. The ERD capture Kade placing him bound on a helicopter. Sam forces her way onto the helicopter, along with Feng. Shu takes control of the helicopter with her mind and forces it to return to the monastery. Before arriving two Thai fighter jets destroy the helicopters just as Kade, Sam, and Feng leap into a nearby lake.\n\nAfter returning to the monastery, the ERD recon spider robots shoots a neuro toxin at Kade and Shu. Feng cuts Kades right arm off to prevent the spreading of the toxin. Shu is killed. Nexus 5 is spread around the world, despite the efforts of government forces. Warren Becker, the Enforcement Division Deputy Director at the ERD, commits suicide. Kade uses gecko genes to grow back his arm.\n\nSix months after the upload of the construction plan of the Nexus, a nano drug, which allows the brain to be programmed and networked, connecting human minds together, the world faces terrorism and massive abuse of the new technology. The Liberation Front, a terror cell secretly created and headed by the US-American government, spread terror in the name of posthumanism, to prevent people from using the new technology and bring up an atmosphere of two 'n' eight to take drastic measures against Nexus.\n\nIn the meantime, our protagonist Kade and his new friend, clone warrior Feng, are fleeing from the CIA, which want see them both killed. On the elopement Kade is trying to stem the misuse of the nano drug to prevent a war between posthumans and humans. He has a code to the Nexus system that he can use to hack it.\n\nThe secret services are very interested in the code. Ilya Aleander dies as prisoner because she didn't give the code to her turnkeys. Rangan Shankari can escape.\n\nSu-Yong Shu died in Nexus and now lives on as a computer intelligence and prisoner on a server belonging to the Chinese government. Ling Shu, her daughter, tries to help her mother escape. At the end of the book, the mother uses Ling Shus Nexus system to hack her brain and take over the body of her daughter.\n\nIn Thailand, Samantha Cataranes helps in a protectory.\n\nA new nano cyber drug called Nexus is released in the year 2040. It connects human minds and allows the brain to be programmed. The protagonist Kaden Lane works on the illegal drug and is suddenly entangled in government intrigue. The nanomedicine is the breakthrough to posthumanism which governments and corporations fear and try to stop. Su-Yong Shu, a brilliant Chinese neuroscientist, with a mind uploaded to the network, tries to start a posthuman revolution. Soon Kaden Lane, who is summoned to spy on her, becomes her ally. The US American government hunts both and at the end Su-Yong Shu's physical body dies while her mind is isolated in a Chinese data center. Kaden Lane flees with his friends and makes the nano drugs available for all of humanity. As a result global unrest spreads with terrorists using the nano drug for assassination and governments trying to assassinate Nexus users. Meanwhile, Su-Yong Shu is nearly killed by the Chinese government by cutting off power to the data center. In the end she is rescued by her daughter.\n\nThe United States and China in particular and the Earth in general are aroused by disturbances. Unrest and riots spread with Nexus-upgraded protesters and police. Su-Yong Shu, the former dead neuroscientist who stole her daughter's body by downloading herself into it, tries to take over all electronic systems and with them the entire world, recreating it to fit her imagination. The posthumans are called Apex, the climax, and reinstatement of humankind.\n\nThe film rights to the novel series were purchased by Paramount in 2013.\n\nThe novel is heavily based in, and extends concepts in the author Ramez Naam's 2007 non-fiction work \"More Than Human: Embracing the promise of biological enhancement,\" in which the author argues for a technology like the fictional drug Nexus.\n\nA series of brain-computer interface experiments have been conducted in order to test the brain's ability to communicate directly to an external device. These experiments are often directed at assisting, augmenting, or repairing human cognitive or sensory-motor functions.\n\nAn experiment conducted at Duke University led by a scientist, Miguel Nicolelis, discovered that by implanting electrode arrays into a monkey's brain they were able to detect the monkey's motor intent and thus able to control reaching and grasping movements performed by a robotic arm. On January 15, 2008, Dr. Nicolelis's lab went even further and had the monkey control a robot 600 miles away, walking on a treadmill in Kyoto, Japan.\n\nLater work done by Phillip Kennedy and colleagues built the first intracortical brain–computer interface by implanting neurotrophic-cone electrodes into one man, Johnny Ray, a stroke victim unable to move muscles below his face. With the connection made, the electrodes can record the electrical impulses that travel across neurons when thinking occurs. Those signals are transferred to a transmitter embedded beneath the scalp. Amplified 1,000 times, the signals go to a computer, which translates them into cursor movement, allowing him the ability to communicate simply by thinking.\n\nOther research has seen success with sensory data, including advances in cochlear implants and neural visual prosthesis.\n\nNot only have advances been made in imputing sensory data to the brain, but also in retrieving data from the brain. \nJack Gallant at UC Berkeley showed that by using an MRI machine they could construct a video of what the person was currently seeing.\n\nOnly a small amount of data is being used in these experiments, only about 256 electrodes (in contrast the brain has around one hundred billion neurons). A possible proposal from Rodolfo Llinás uses carbon nanotubes and if implemented could achieve higher volumes of electrodes, over a million.\n\nThe genetic enhancements to boost strength, speed, and stamina, as described in \"Nexus\", are likely already possible, argued so by Ramez Naam. Over the decades researchers looking to cure muscular dystrophy, anemia, and other ailments have shown that a single injection loaded with additional copies of selected genes, and delivered by a tame virus, can have lifelong impact on the strength and fitness of animals, ranging from mice to baboons.\n\nThe Nexus backdoor that is created by Kade and Rangan in the novel is based on the Karger and Schell Multics backdoor, implemented experimentally by Ken Thompson, co-inventor of the Unix operating system.\n\n"}
{"id": "9566962", "url": "https://en.wikipedia.org/wiki?curid=9566962", "title": "Tornado Intercept Vehicle", "text": "Tornado Intercept Vehicle\n\nThe Tornado Intercept Vehicle 1 (TIV 1) and Tornado Intercept Vehicle 2 (TIV 2) are vehicles used to film with an IMAX camera from very close proximity to or within a tornado. They were designed by film director Sean Casey. On May 27, 2013 the TIV2 filmed the inside of a tornado in Kansas with Casey inside.\n\nThe Tornado Intercept Vehicle 1 (TIV 1) is a heavily modified 1997 Ford F-Super Duty cab & chassis truck used as a storm chasing platform and built by Sean Casey. This heavily armored vehicle can drive into a weak to relatively strong tornado (EF0 to EF3) to film it and take measurements. Work began on the TIV in 2003 and took around eight months to finish, at a total cost of around US$81,000. TIV's armored shell consists of 1/8–1/4 inch steel plate welded to a two inch square steel tubing frame. The windows are bullet resistant polycarbonate, measuring thick on the windshield and thick on the sides. The TIV weighs approximately fully loaded and is powered by a 7.3 litre Ford Powerstroke turbocharged diesel engine manufactured by Navistar-International, otherwise known as the International T444E. The vehicle's speed was limited by the factory Ford PCM, giving it a top speed of . The TIV has a fuel capacity of , giving it a range of around . The TIV is featured in a series called \"Storm Chasers\" which began airing on the Discovery Channel in October 2007. TIV was succeeded in 2008 by TIV 2, but returned to service to finish out the 2008 storm chasing season after TIV 2 suffered mechanical problems. In a June 2011 interview with NPR's \"All Things Considered\", Casey said that TIV is still in service and is designated as the backup vehicle in the event TIV 2 breaks down during a shoot.\n\nCasey and his team developed and built the second Tornado Intercept Vehicle, dubbed TIV 2, to be featured in their next IMAX movie and the \"Storm Chasers\" series. Work began in September 2007 by forty welding students at the Great Plains Technology Center in Lawton, Oklahoma and was completed in time for the 2008 tornado chase season. TIV 2 was designed to address some of the problems experienced with the original TIV, namely its low ground clearance, lack of four-wheel drive, and low top speed. It is based on a Dodge Ram 3500 that was strengthened and converted to six-wheel drive by adding a third axle. After season two the six-wheel drive system was modified to four-wheel drive. It is powered by a 6.7-liter Cummins turbocharged diesel engine, modified with propane and water injection to produce . This gives TIV 2 an estimated top speed of over . Its fuel capacity is , giving TIV 2 an approximate range of . The body of TIV 2 is constructed of a 1/8-inch steel skin welded over a square tubing steel frame. The windows in TIV 2 are all bullet-resistant interlayered polycarbonate sheets and tempered glass. TIV 2 also features an IMAX filming turret similar to the one on the original TIV. The original TIV's somewhat cumbersome hydraulic claws were not used on TIV 2 in favor of six hydraulic skirts that drop down to deflect wind over the TIV to stabilize it and protect the underside from debris, and four hydraulically operated anchoring spikes.\n\nTIV 2 debuted on the second season of \"Storm Chasers\", which began airing on the Discovery Channel in October 2008. Its initial performance did not go well, as it was plagued by mechanical failures, including several broken axles, which forced Casey to abandon TIV 2 and return to chasing in the original TIV until TIV 2's issues could be resolved. Although Casey hoped he would be back in TIV 2 before the end of the season, repairs and modifications on TIV 2 took longer than expected and Casey was shown on \"Storm Chasers\" ending the season in the original TIV.\n\nIn the fall of 2008, TIV 2 received several modifications, mostly focused on reducing the vehicle's weight. To achieve this, certain less crucial areas of TIV 2's armor were converted from steel to aluminum while more vital areas were reinforced with supplemental composite armor consisting of thin layers of steel, Kevlar, polycarbonate, and rubber. In all, the weight reduction measures brought TIV 2's weight down to . The safety systems were also improved, with the three front wind skirts being consolidated into one and new hydraulic stabilizing spikes to further increase stability in high winds. Other modifications included additional doors that provided every seat position with an exit (wind skirts up or down), and a redesigned IMAX turret with 50% more windows. The third axle was disconnected from the drive train, thus changing TIV2 to 6×4 from its 6×6 design. The third axle now acts as a brace for the vehicle's weight.\n\nThe TIV 2 appeared again in the fourth season of \"Storm Chasers\", and also in an episode of another Discovery Channel series, \"Mythbusters\", wherein both the TIV 2 and the SRV Dominator vehicle operated by Reed Timmer of TornadoVideos.Net were tested to determine their endurance to storm-force winds by being parked behind a Boeing 747 with the engines at full throttle. When tested at a wind speed of , the TIV 2 had the driver's door pulled open, though this was due to human error, as Casey forgot to lock the door prior to the test. When tested again at (equivalent to an EF5 tornado), the TIV 2 suffered no ill effects other than the anchoring spikes being slightly bent; the Dominator ended up being blown approximately , although it remained upright.\n\nIn 2011, a siren was added to the vehicle to allow the TIV 2 to act as a mobile warning system for civilians in the path of incoming tornadoes, after several incidents earlier that year where the TIV team was unable to effectively warn locals of the imminent danger of the tornadoes they were tracking, especially during the 2011 Super Outbreak. On April 27, 2011, the TIV 2 team intercepted an EF4 tornado that hit near Enterprise, Mississippi, while not in the path but 200 yards from it, it was the first tornado he shot with his new 3-Dimensional IMAX camera.\n\nCasey removed the rear flap in early 2012 and built a new set of two hydraulic spikes that go into the ground during an intercept.\n\nOn May 27, 2013, TIV 2 intercepted a large tornado near Smith Center, Kansas. The vehicle was struck by large debris from a nearby farm and suffered damage to the roof-mounted anemometer and at least two breaches of the crew compartment when the roof hatch and one of the doors were compromised. Before the anemometer was disabled, it recorded winds of , placing the tornado in the EF3 to EF4 range.\n\nAlthough primarily designed to shoot film from near or within tornadoes, the TIV's have at times been outfitted with meteorological instrumentation atop masts to complement the Doppler on Wheels (DOW) radar trucks of the Center for Severe Weather Research run by atmospheric scientist and inventor Joshua Wurman.\n\n\n"}
{"id": "17632964", "url": "https://en.wikipedia.org/wiki?curid=17632964", "title": "UltraBattery", "text": "UltraBattery\n\nUltraBattery is a hybrid energy storage device invented by Australia’s Commonwealth Scientific and Industrial Research Organisation (CSIRO). UltraBattery combines ultracapacitor technology with lead-acid battery technology in a single cell with a common electrolyte.\n\nResearch conducted by independent laboratories, such as the United States's Sandia National Laboratories, the Advanced Lead-Acid Battery Consortium (ALABC), the Commonwealth Scientific and Industrial Research Organisation (CSIRO) and commercial tests by East Penn Manufacturing, Furukawa Battery and Ecoult indicate that in comparison with conventional valve regulated lead acid (VRLA) batteries, UltraBattery technology has higher energy efficiencies, a longer lifetime and superior charge acceptance under partial state of charge (SoC) conditions.\n\nCombining the two technologies in one battery cell means that UltraBattery works very efficiently compared with conventional lead acid technologies largely due to the fact that it can be operated for long periods in a partial state of charge (pSoC), whereas conventional lead acid batteries are more typically designed for high SoC use (i.e. when the battery is close to fully charged). Operating in the partial SoC range extends the battery’s life chiefly by reducing sulfation and by reducing time spent operating at very high and very low states of charge, where various side reactions tend to cause deterioration. A conventional VRLA battery tends to deteriorate quickly when operated in this partial SoC range.\n\nUltraBattery was invented in Australia by CSIRO.\n\nThe development of UltraBattery was funded by the Australian government. Japanese company Furukawa Battery Co., Ltd also contributed to the development of UltraBattery technology, and the Japanese government funded part of its development through the New Energy and Industrial Technology Development Organization (NEDO).\n\nIn 2007, East Penn Manufacturing obtained a global head license to manufacture and commercialize UltraBattery technology for motive and automotive applications (in various territories) and for stationary energy storage applications (globally, outside Japan and Thailand, where Furukawa Battery is the head license holder).\n\nThe United States Department of Energy has also funded UltraBattery for research into grid-scale stationary energy storage applications. In 2007, CSIRO formed a subsidiary company, Ecoult, to address this market. Ecoult also received support from the Australian Government to further the development of Ultrabattery. In May 2010, US battery manufacturer East Penn Manufacturing acquired Ecoult from CSIRO.\n\nIn March 2013, the Australian Government announced further funding through the Australian Renewable Energy Agency’s Emerging Renewables Program to further develop UltraBattery technology as cost-effective energy storage for residential and commercial renewable energy systems.\n\nUltraBattery is a hybrid device that combines ultracapacitor technology with lead-acid battery technology in a single cell with a common electrolyte.\n\nPhysically, UltraBattery has a single positive electrode and a twin negative electrode – one part carbon, one part lead, in a common electrolyte. Together these make up the negative electrode of the UltraBattery unit, but specifically the carbon is the electrode of the capacitor and lead is the electrode of the lead-acid cell. The single positive electrode (lead oxide) is typical of all lead acid batteries and is common to the lead acid cell and the ultracapacitor.\n\nThis technology (specifically the addition of the carbon electrode) gives UltraBattery different performance characteristics to conventional VRLA batteries. In particular UltraBattery technology suffers significantly less from the development of permanent (or hard) sulfation on the negative battery electrode – a problem commonly exhibited in conventional lead acid batteries.\n\nDuring normal lead-acid battery operation, lead sulfate crystals grow on the negative electrode during discharging and dissolve again during charging. The formation of these crystals is called sulfation. Over time sulfation can become permanent, as some crystals grow and resist being dissolved. This is particularly the case when the battery is forced to perform at very high rates of discharge, which tends to promote lead sulfate crystal growth on the surface of the electrode. At moderate rates of discharge, the lead sulfate crystals grow throughout the cross section of the electrode plate (which has a sponge-like consistency) since the electrolyte (dilute sulfuric acid) is drawn diffused through the body of the electrode to allow the reaction can take place throughout the plate.\n\nBut at very fast rates of discharge, the acid already inside the body of the plate is used up quickly and fresh acid cannot diffuse through the electrode in time to continue the reaction. Hence the reaction is favored toward the outer wall of the electrode, where crystals may form in a dense mat, rather than in dispersed clumps throughout the plate. This mat of crystals further impedes electrolyte transfer. The crystals then grow larger, and because the larger crystals have a large volume compared to their surface area it becomes difficult to remove them chemically during charging, particularly as the concentration of the sulfuric acid in the electrolyte is likely to be high (since only limited lead sulfate has been created on the surface of the plate) and lead sulfate is less soluble in concentrated sulfuric acid (above about 10% concentration by weight) than it is in dilute sulfuric acid.\n\nThis condition is sometimes termed the “hard” sulfation of the battery electrode [REF]. Hard sulfation increases the battery’s impedance (since the lead sulfate crystals tend to insulate the electrode from the electrolyte) and decreases its power, capacity and efficiency due to increased undesirable side reactions, some of which occur inside the negative plate due to charging taking place with low availability of lead sulfate (inside the plate body). One undesirable effect is the production of hydrogen inside the plate, further reducing the efficiency of the reaction. “Hard” sulfation is generally irreversible since the side reactions tend to dominate as more and more energy is pushed into the battery.\n\nTo reduce the likelihood of hard sulfation, conventional VRLA batteries should therefore be discharged at specific rates, determined by various charging algorithms. [REF] Furthermore, they must be frequently refreshed and are most suited to operation toward the top end of the SoC (between 80% and 100% charged). [REF] While operating in this limited state of charge mitigates permanent sulfation on the negative electrode, battery operation exclusively at or near a full SoC is highly inefficient. [REF] The inefficiency is largely due to increases the incidence of side reactions (for instance electrolysis) which dissipate energy.\n\nThe presence of the ultracapacitor integrated in the UltraBattery acts to limit the formation of hard sulfation inside the cell. [REF] This supports the battery’s ability to operate for long periods in a partial SoC where the battery operates more efficiently. [REF] Conventional VRLAs are somewhat constrained to operate in the inefficient region toward the top of their charge capacity in order to protect them against damage by sulfation. Research continues into the reasons why the presence of the ultracapacitor reduces sulfation so successfully. Experimental results show that the presence of carbon within VRLA cells has some mitigating effect but the protective effects of the parallel-connected ultracapacitor within the UltraBattery are much more significant. Hund et al., for instance, found that typical VRLA battery failure modes (water loss, negative plate sulfation, and grid corrosion) are all minimized in the UltraBattery. Hund’s results also showed that the UltraBattery, used in a high rate partial state of charge application, exhibits reduced gassing, mimimized negative plate hard sulfation, enhanced power performance and minimized operating temperature compared with conventional VRLA cells.\n\nLead forms part of the negative battery electrode.\n\nCarbon forms part of the negative ultracapacitor electrode.\n\nThe electrolyte solution is made up of sulfuric acid and water.\n\nLead Sulfate is a white crystal or powder. Normal lead acid battery operation sees small lead sulfate crystals growing on the negative electrode during discharging and dissolving back into the electrolyte during charging.\n\nThe electrodes are constructed of a lead grid, with a lead-based active material compound – lead oxide – forming the remainder of the positive plate.\n\nUltraBattery can be used for a range of energy storage applications, such as:\n\nUltraBattery is virtually 100 per cent recyclable and can be made at existing battery manufacturing facilities.\n\nUltraBattery has several advantages over the existing nickel-metal hydride (Ni-MH) batteries currently used in hybrid electric vehicles. They are approximately 70 per cent less expensive, with comparable performance in terms of fuel consumption and faster charge and discharge rates than Ni-MH batteries.\n\nWhen used in hybrid electric vehicles, the UltraBattery’s ultracapacitor acts as a buffer during high-rate discharging and charging, enabling it to provide and absorb charge rapidly during vehicle acceleration and braking.\n\nTesting of the Ultrabattery’s performance in hybrid electric vehicles by Advanced Lead Acid Battery Consortium achieved more than 100,000 miles on a single battery pack without significant degradation. Laboratory results of UltraBattery prototypes show that their capacity, power, available energy, cold cranking and self-discharge meets, or exceeds, all performance targets set for minimum and maximum power-assist hybrid electric vehicles.\n\nUltraBattery can be used to smooth and shift (i.e. store for later use) renewable energy sources on microgrids to improve predictable power availability. UltraBattery can also be used in standalone microgrid systems, renewables power systems and hybrid microgrids.\nStandalone microgrid systems combine diesel or other fossil fuels with UltraBattery storage to improve the efficiency of fossil-fuel energy generation. Including energy storage in the system reduces the size of the gen-set (i.e. array of generators) because the batteries can handle peaks in the load. UltraBattery also reduces the fuel consumption of the gen-set, because the generators can run at their highest efficiency, regardless of variations in the load on the system.\n\nRenewables power systems combine UltraBattery technology with the renewable generation source to deliver local power. They can use either photovoltaic, wind or solar thermal energy, and commonly incorporate a back-up diesel generator. Hybrid microgrids integrate renewable generation sources with UltraBattery energy storage and fossil-fuel gen-sets to maximize the efficiency of base-load generation. This can greatly reduce the cost of energy compared with diesel-only powered microgrids. They also substantially decrease greenhouse gas emissions. An example of this type of microgrid is the King Island Renewable Energy Integration Project (KIREIP), being undertaken by Hydro Tasmania. This megawatt-scale renewable energy project aims to reduce both the cost of delivering power to the island and carbon pollution.\n\nUltraBattery can be used to backup an uninterruptible power supply (UPS). In conventional UPS systems, the batteries sit, essentially unused, until a grid outage event occurs. Because the UltraBattery can provide frequency regulation and related grid services, it can generate revenue for the UPS asset owner at the same time as providing backup power.\n\nFor community applications, UltraBattery can be used as back-up in the event of grid outage (see Section 5.1) and for peak shaving. Also known as peak lopping, peak shaving is the ability to charge batteries during off-peak time, and use the power from the batteries during peak times to avoid higher charges for electricity. Another example of a community application is a 300 kW smart grid demonstration system set up by Furukawa Battery in the Maeda Area in Kitakyushu, Japan. This load-levelling application uses 336 UltraBattery cells (1000 Ah, 2 volts). The company has also installed two smart grid demonstrations of UltraBattery peak shifting technology at Kitakyushu Museum of Natural History & Human History.\n\nIn Japan, the Shimizu Corporation has set up a microgrid (see Section 5.2) in a commercial building. The ‘smart building’ system, which includes 163 UltraBattery cells (500 Ah, 2 volts), also monitors cell voltage, impedance and temperature. A second system, installed at Furukawa Battery’s Iwaki Factory, incorporates 192 UltraBattery cells, a 100 kW power conditioning system and a battery management system. This load-levelling application was set up to control the factory’s demand for power.\n\nFor residential applications, local use of rooftop solar could be improved by using UltraBattery to both store power for use by the resident who owns the panels, and feed power or regulation services into the grid during high-value peaks.\n\nUltraBattery can manage variability on electricity grids in five main ways: frequency regulation, renewable energy integration (smoothing and shifting), spinning reserve, ramp-rate control, and power quality and weak-grid support.\n\nElectricity grids must manage the constant fluctuations in supply and demand of power to keep a constant frequency in order to maintain the physical operation of the grid. UltraBattery can absorb and deliver power to the grid to help manage the balance between supply and demand, and to maintain consistent voltage. Ecoult implemented a grid-scale energy storage system which provides 3 MW of regulation services on the grid of Pennsylvania-Jersey-Maryland (PJM) Interconnection in the United States. Four strings of UltraBattery cells are connected to the grid in Lyon Station, Pennsylvania. The project provides continuous frequency regulation services bidding into the open market on PJM.\n\nUltraBattery technology can be used to integrate renewable energy sources, such as solar and wind, into the electricity grid, by managing the fluctuations in renewable output. It does this by ‘smoothing’ and ‘shifting’ energy.\n\nSmoothing turns the inherent variability of power from photovoltaic panels or wind turbines into a smooth, predictable signal. The system monitors the output of the intermittent renewable source, and when the solar (or wind) signal varies, UltraBattery responds immediately to either release energy or absorb excess energy. Managing the variability of the renewable signal in this way makes renewable energy more reliable.\n\nShifting energy refers to UltraBattery’s ability to store the excess energy produced by renewable resources in off-peak times, and to then release it when needed during periods of peak demand. This allows electricity utilities to improve their overall system performance at peak times.\n\nPNM, the leading electric utility company in New Mexico, United States, has integrated an UltraBattery energy storage system with a solar energy-generating farm to demonstrate smoothing and shifting of solar power for use as a dispatchable renewable resource. The PNM Prosperity project features one of the United States’ largest combinations of photovoltaic energy and solar panel battery storage.\n\nMany small-scale deployments of rooftop photovoltaic panels tend to multiply the effect of the intermittency of solar generation – creating a problem for grid operators. [REF] UltraBattery energy storage has been used to reduce renewable intermittency by ramping the power on the electricity grid in a controlled manner, making renewable-generated power more predictable.\n\nUltraBattery can also be used for demand management, which addresses the problems of grid supply and demand, but not necessarily those related to renewable intermittency. At the periphery of large grids, or on old grid infrastructure – such as single-wire earth return networks – the effect of a demand spike or variance in supply can be amplified because of its scale relative to other local activity. UltraBattery can reduce these effects and ensure power quality from the grid for local users.\n\nUltraBattery has five main characteristics that form points of difference between this technology and conventional VRLA battery technology: higher capacity turnover, lower lifetime cost per kilowatt hour, higher DC–DC efficiency, fewer refresh charges required and higher rate of charge acceptance. \n\nA battery’s capacity turnover is the amount of possible energy throughput in relation to the capacity of the battery. It is a normalized measure of how many times the theoretical capacity of a battery can be used over its lifetime. A higher capacity turnover indicates the battery provides more energy over its lifetime\n\nWhen UltraBattery and standard VRLA (used in a partial SoC regime) are compared in experimental conditions, UltraBattery has been shown to achieve about 13 times the capacity turnover of a standard absorbed glass matt VRLA battery.\n\nThe lifetime of a battery depends on how it is used, and how many cycles of charging and discharging it is put through. In a situation where batteries are put through four 40% cycles per day and where throughput is the life-limiting factor, UltraBattery will last about three to four times longer than a conventional VRLA battery.\n\nCSIRO, claims “The UltraBattery is about 70 per cent cheaper to make than batteries with comparable performance and can be made using existing manufacturing facilities”.\n\nA battery’s DC–DC efficiency describes the amount of energy available to be discharged to the load connected to a battery as a proportion of the amount of energy put into the battery during charging. During charging and discharging, some of the battery’s stored energy is lost as heat, and some is lost in side reactions. The lower the energy losses of a battery, the more efficient the battery is.\n\nUltraBattery’s developers claim it can achieve a DC–DC efficiency of 93–95% (rate dependent) performing variability management applications in a partial SoC regime, depending on discharge rate, and 86–95% (rate dependent), when performing energy shifting applications.By comparison, standard VRLA batteries applied to energy shifting (using the typical top of charge regime) achieve much lower efficiencies – for instance in states of charge from 79% to 84% charged, tests show efficiencies around 55%.\n\nThe high DC–DC efficiency of UltraBattery is achievable because (like conventional VRLA batteries) it operates very efficiently below 80% SoC. Experiments indicate that for VRLA batteries “from zero SOC to 84% SOC the average overall battery charging efficiency is 91%”. While conventional VRLA batteries cannot tolerate working in this range for any significant length of time without frequent refreshing, UltraBattery can tolerate working at much lower states of charge without significant degradation. Hence it can achieve much greater efficiencies since it can operate for long periods in the most efficient zone for lead acid batteries.\n\nDuring operation, conventional VRLA batteries must be refreshed (overcharged) to dissolve the sulfate crystals that have accumulated on the negative electrode and replenish the capacity of the battery. Refreshing the battery also helps return the battery cells in the string (where multiple batteries are used together) to a consistent operating voltage. However the overcharging process is complicated by the fact that not only is the battery out of service during refresh cycles, but the high currents required to complete the overcharge process (within a reasonable timeframe) are also the cause of various parasitic losses. These include thermal losses and losses due to various side reactions (chiefly hydrogen evolution, oxygen evolution and grid corrosion).\n\nUltraBattery can operate without a refresh charge for extended periods. For stationary cycling applications such as renewable energy or grid support, this may be between one and four months depending on workload; standard VRLA batteries in the same applications need refreshing every one to two weeks if performing daily cycles - and performance deteriorates rapidly even with weekly refresh cycles.\n\nIn automotive applications in a hybrid electric vehicle, UltraBatteries can be operated more or less continuously in a partial SoC regime without being refreshed. Furukawa reports: “In the field driving test of the Honda Insight hybrid electric vehicle with an UltraBattery pack installed, a target drive of 100,000 miles (approx. 160,000 km) was achieved without the recovering charging.\n\nBecause UltraBattery operates effectively in the partial SoC range, it can accept charge more efficiently than conventional VRLA batteries, which typically operate at high states of charge. Sandia National Laboratory tests show VRLA batteries typically achieve less than 50% efficiency at greater than 90% charged, about 55% efficiency between 79% and 84% charged, and over 90% efficiency if charged at between zero and 84% of the full capacity.\nIn comparison with conventional VRLA batteries, UltraBattery can be charged efficiently and at high charging/discharging rates. Hund et al.’s test results showed that the Ultrabattery was able to cycle at the 4C1 rate for around 15,000 cycles. The VRLA battery using this test procedure could only cycle at the 1C1 rate. A 1C rate indicates that the battery’s entire capacity would be used (or replaced if charging) in one hour at this rate. A 4C rate is four times faster – i.e. the battery would be fully discharged (or charged) in 15 minutes at the 4C rate.\n\nThe exact chemical process by which carbon so significantly delays sulfation is not fully understood. However the presence of UltraBattery’s parallel ultracapacitor apparently protects the negative terminal from the large surface preponderance of lead sulfate crystals that affects VRLA batteries operated at high rates of discharge or for long periods in pSoC operation, increasing the rechargeability of the cell (see also Hard Sulfation). \nReduced sulfation also significantly enhances charge acceptance by reducing hydrogen gas production at the electrode . This is not unexpected since excessive hydrogen gas production (which robs significant energy from the charging process) is caused when electrons pushed into the negative plate during charging (which would usually react with the lead sulfate crystals inside the plate) are unable to easily react with large crystals of lead sulfate on the surface of the plate, so instead tend to reduce the electrolyte’s abundant hydrogen ions to hydrogen gas.\n\nUltraBattery is manufactured by East Penn Manufacturing in the United States, to the global requirements of ISO 9001:2008, ISO/TS 16949:2009 and ISO 14001:2004 certification standards.\n\nUltraBattery’s electrolyte solution contains H2SO4 in water, and its lead electrodes are inert. As the electrolyte is largely water, UltraBattery is fire retarding. UltraBatteries have the same transport and hazard restrictions as conventional VRLA batteries\n\nEvery part of each UltraBattery – lead, plastic, steel and acid – is virtually 100% recyclable for later reuse. Large-scale recycling facilities for these batteries are already available and 96% of lead acid batteries used in the US are recycled. Battery manufacturers recover and separate the lead, plastics and acid from VRLA batteries. The lead is smelted and refined for reuse. Plastic parts are cleaned, ground, extruded and moulded into new plastic parts. The acid is reclaimed, cleaned and used in new batteries.\n\nTests have been conducted by independent laboratories, as well as by East Penn Manufacturing, Furukawa and Ecoult, to compare the performance of UltraBattery with conventional VRLA batteries.\n\nMicro hybrid electric vehicles batteries were tested at a 70% SoC in a pulse charge-discharge pattern. UltraBattery had about 1.8 times more capacity turnover, and therefore cycle life, than a conventional VRLA battery.\n\nThe Advanced Lead Acid Battery Consortium (ALABC) tested the durability of UltraBattery in the high-rate, partial state-of-charge operation of a Honda Civic hybrid electric vehicle. The test car had comparable miles per gallon performance as the same model powered by Ni-MH batteries.\n\nUnder micro, mild and full hybrid electric vehicle duties, the cycling performance of the UltraBattery was at least four times longer than conventional state-of-the-art VRLA batteries and was comparable or even better than that of Ni-MH cells. UltraBattery also demonstrated good acceptance of the charge from regenerative braking, and so did not require equalization charges during the field trial.\n\nWh (watt-hours) efficiency tests of UltraBattery in a stationary application for an electricity smart grid showed that over 30 cycles of charge-discharge at rates of 0.1 C10A, Wh efficiencies ranged from 91% to 94.5%, depending on the battery’s state of charge. [REF] This is compared with a Sandia National Laboratories study into lead-acid battery efficiency which found that traditional lead-acid batteries operating between 79% and 84% state-of-charge (the “top” charge mode to which traditional lead-acid batteries are generally restricted to prolong their life) achieve only 55% incremental charging efficiency.\n\nBatteries were subjected to 3-hour charge and discharge tests at a 60% state of charge, with a 20-hour recovery charge conducted every 90 cycles. Capacity tests showed that after 270 cycles, the UltraBattery capacity ratio was equal to or greater than 103%, compared to 93% for a conventional lead storage battery. The tests showed that the UltraBattery had a longer cycle life and better recovery charge characteristics than the conventional battery when operating in a partial state of charge.\n\nHigh-rate, partial state-of-charge cycle tests were performed to measure the ability of UltraBattery for use in utility ancillary service applications for energy storage and wind farm energy smoothing. Using a high-rate, partial state-of-charge cycling profile at the 1C1 to 4C1 rate, the UltraBattery was capable of more than 15,000 cycles with less than 20% capacity loss, and could cycle at the 4C1 rate. An absorbed glass matt (AGM) VRLA battery tested under the same conditions could only cycle at the 1C1 rate, required a recovery charge after about 100 cycles, and after 1100 cycles lost more than 20% of its capacity. UltraBattery was also able to cycle for more than ten times the number of cycles between recovery charges than the AGM VRLA battery (1000 vs.100).\n\nA wind farm field trial in Hampton, New South Wales (Australia), is testing a system designed to demonstrate the use of energy storage to address the short-term intermittency of wind generation. The trial compared the performance of the UltraBattery and three other lead-acid battery types for renewable energy smoothing applications. Measurements of the variations in cell voltage in each string of 60 cells connected in series showed that the UltraBattery had far less variation over a 10-month period (a 32% increase in standard deviation of voltage range variation, compared to 140%–251% for the other three battery types).\n\nTests by Sandia National Laboratories show that UltraBattery performs for much longer than conventional VRLA batteries in utility cycling. The cycling profile in these tests was intended to mimic frequency regulation duty with approximately 4 cycles per hour with a peak power intended to give a SoC range expected to be typical. The results showed that a conventional VRLA battery (cycling in a partial state of charge (PSoC) and 10% depth of discharge) dropped to 60% of its initial capacity after about 3000 cycles. In the same test an UltraBattery manufactured by East Penn ran for more than 22,000 cycles, maintaining essentially 100% of its initial capacity without having been supplied a recovery charge.\n\nTests also showed that UltraBattery performs for much longer than conventional VRLA batteries in energy applications, as shown in a simulated photovoltaic hybrid cycle-life test by Sandia National Laboratories. The testing concluded that even at 40-day deficit charges (cycles where more is taken from the battery each day than is put back in). UltraBatteries have performance far surpassing traditional VRLA batteries even when the traditional VRLA batteries are operating on only 7 day deficit charge regimes. In a deficit charge regime there is no recovery by taper charge, also known as refreshing/equalization of the batteries so sulfation is a typical failure mode for conventional VRLAs in this operating regime.\n\nAfter 100 days of cycling with 60% depth of discharge, a conventional VRLA battery receiving a refresh cycle every 30 days had dropped to 70% of its initial capacity. Two UltraBattery units (one made by Furukawa, one by East Penn) each experiencing 40-day deficit charges were still performing significantly better than the traditional VRLA battery which was receiving more frequent refreshes (it experienced only a maximum 7-day deficit charge). After 430 days of cycling, the East Penn UltraBattery and Furukawa UltraBattery still had not failed. The East Penn Battery was maintaining 85% of its initial capacity and the Furukawa battery was at very close to 100% of its initial capacity.\n\n\n"}
{"id": "170578", "url": "https://en.wikipedia.org/wiki?curid=170578", "title": "Vermicompost", "text": "Vermicompost\n\nVermicompost (vermi-compost, vermiculture) is the product of the composting process using various species of worms, usually red wigglers, white worms, and other earthworms, to create a mixture of decomposing vegetable or food waste, bedding materials, and vermicast. \n\nVermicast (also called worm castings, worm humus, worm manure, or worm feces) is the end-product of the breakdown of organic matter by earthworms. These castings have been shown to contain reduced levels of contaminants and a higher saturation of nutrients than the organic materials before vermicomposting.\n\nVermicompost contains water-soluble nutrients and is an excellent, nutrient-rich organic fertilizer and soil conditioner. It is used in farming and small scale sustainable, organic farming.\n\nVermicomposting can also be applied for treatment of sewage sludge. A variation of the process is vermifiltration (or vermidigestion) which is used to remove organic matter, pathogens and oxygen demand from wastewater or directly from blackwater of flush toilets.\n\nVermicomposting has gained popularity in both industrial and domestic settings because, as compared with conventional composting, it provides a way to treat organic wastes more quickly. It also generates products that have lower salinity levels that are therefore more beneficial to plant mediums.\nThe earthworm species (or composting worms) most often used are red wigglers (\"Eisenia fetida\" or \"Eisenia andrei\"), though European nightcrawlers (\"Eisenia hortensis\" or \"Dendrobaena veneta\") could also be used. Red wigglers are recommended by most vermicomposting experts, as they have some of the best appetites and breed very quickly. Users refer to European nightcrawlers by a variety of other names, including \"dendrobaenas\", \"dendras\", Dutch nightcrawlers, and Belgian nightcrawlers.\n\nContaining water-soluble nutrients, vermicompost is a nutrient-rich organic fertilizer and soil conditioner in a form that is relatively easy for plants to absorb. Worm castings are sometimes used as an organic fertilizer. Because the earthworms grind and uniformly mix minerals in simple forms, plants need only minimal effort to obtain them. The worms' digestive systems create environments that allow certain species of microbes to thrive to help create a \"living\" soil environment for plants. The fraction of soil which has gone through the digestive tract of earthworms is called the Drilosphere.\n\nOne of the species most often used for composting is the red wiggler or tiger worm (\"Eisenia fetida\" or \"Eisenia andrei\"); \"Lumbricus rubellus\" (a.k.a. red earthworm or dilong (China)) is another breed of worm that can be used, but it does not adapt as well to the shallow compost bin as does \"Eisenia fetida\". European nightcrawlers (\"Eisenia hortensis\") may also be used. Users refer to European nightcrawlers by a variety of other names, including dendrobaenas, dendras, and Belgian nightcrawlers. African Nightcrawlers (\"Eudrilus eugeniae\") are another set of popular composters. \"Lumbricus terrestris\" (a.k.a. Canadian nightcrawlers (US) or common earthworm (UK)) are not recommended, since they burrow deeper than most compost bins can accommodate.\n\nBlueworms (\"Perionyx excavatus\") may be used in the tropics.\n\nThese species commonly are found in organic-rich soils throughout Europe and North America and live in rotting vegetation, compost, and manure piles. They may be an invasive species in some areas. As they are shallow-dwelling and feed on decomposing plant matter in the soil, they adapt easily to living on food or plant waste in the confines of a worm bin.\n\nComposting worms are available to order online, from nursery mail-order suppliers or angling shops where they are sold as bait. They can also be collected from compost and manure piles. These species are not the same worms that are found in ordinary soil or on pavement when the soil is flooded by water.\n\nLarge-scale vermicomposting is practiced in Canada, Italy, Japan, India, Malaysia, the Philippines, and the United States. The vermicompost may be used for farming, landscaping, to create compost tea, or for sale. Some of these operations produce worms for bait and/or home vermicomposting.\n\nThere are two main methods of large-scale vermiculture. Some systems use a windrow, which consists of bedding materials for the earthworms to live in and acts as a large bin; organic material is added to it. Although the windrow has no physical barriers to prevent worms from escaping, in theory they should not due to an abundance of organic matter for them to feed on. Often windrows are used on a concrete surface to prevent predators from gaining access to the worm population.\n\nThe windrow method and compost windrow turners were developed by Fletcher Sims Jr. of the Compost Corporation in Canyon, Texas. The Windrow Composting system is noted as a sustainable, cost-efficient way for farmers to manage dairy waste.\nThe second type of large-scale vermicomposting system is the raised bed or flow-through system. Here the worms are fed an inch of \"worm chow\" across the top of the bed, and an inch of castings are harvested from below by pulling a breaker bar across the large mesh screen which forms the base of the bed.\n\nBecause red worms are surface dwellers constantly moving towards the new food source, the flow-through system eliminates the need to separate worms from the castings before packaging. Flow-through systems are well suited to indoor facilities, making them the preferred choice for operations in colder climates.\n\nFor vermicomposting at home, a large variety of bins are commercially available, or a variety of adapted containers may be used. They may be made of old plastic containers, wood, Styrofoam, or metal containers. The design of a small bin usually depends on where an individual wishes to store the bin and how they wish to feed the worms.\n\nSome materials are less desirable than others in worm bin construction. Metal containers often conduct heat too readily, are prone to rusting, and may release heavy metals into the vermicompost. Styrofoam containers may release chemicals into the organic material. Some cedars, Yellow cedar, and Redwood contain resinous oils that may harm worms, although Western Red Cedar has excellent longevity in composting conditions. Hemlock is another inexpensive and fairly rot-resistant wood species that may be used to build worm bins.\n\nBins need holes or mesh for aeration. Some people add a spout or holes in the bottom for excess liquid to drain into a tray for collection. The most common materials used are plastic: recycled polyethylene and polypropylene and wood. Worm compost bins made from plastic are ideal, but require more drainage than wooden ones because they are non-absorbent. However, wooden bins will eventually decay and need to be replaced.\n\nSmall-scale vermicomposting is well-suited to turn kitchen waste into high-quality soil amendments, where space is limited. Worms can decompose organic matter without the additional human physical effort (turning the bin) that bin composting requires.\n\nComposting worms which are detritivorous (eaters of trash), such as the red wiggler \"Eisenia fetidae\", are epigeic (surface dwellers) and together with symbiotic associated microbes are the ideal vectors for decomposing food waste. Common earthworms such as \"Lumbricus terrestris\" are anecic (deep burrowing) species and hence unsuitable for use in a closed system. Other soil species that contribute include insects, other worms and molds.\n\nThere may be differences in vermicomposting methods depending on the climate. It is necessary to monitor the temperatures of large-scale bin systems (which can have high heat-retentive properties), as the raw materials or feedstocks used can compost, heating up the worm bins as they decay and killing the worms.\n\nThe most common worms used in composting systems, redworms (\"Eisenia foetida,\" \"Eisenia andrei,\" and \"Lumbricus rubellus\") feed most rapidly at temperatures of 15–25 °C (59-77 °F). They can survive at 10 °C (50 °F). Temperatures above 30 °C (86 °F) may harm them. This temperature range means that indoor vermicomposting with redworms is possible in all but tropical climates. Other worms like Perionyx excavatus are suitable for warmer climates. If a worm bin is kept outside, it should be placed in a sheltered position away from direct sunlight and insulated against frost in winter.\n\nThere are few food wastes that vermicomposting cannot compost, although meat waste and dairy products are likely to putrefy, and in outdoor bins can attract vermin. Green waste should be added in moderation to avoid heating the bin.\n\nSuch systems usually use kitchen and garden waste, using \"earthworms and other microorganisms\nto digest organic wastes, such as kitchen scraps\".\nThis includes:\n\nSuch vermicomposting systems need reliable sources of large quantities of food.\nSystems presently operating use:\n\nVermicompost is ready for harvest when it contains few-to-no scraps of uneaten food or bedding. There are several methods of harvesting from small-scale systems: \"dump and hand sort\", \"let the worms do the sorting\", \"alternate containers\" and \"divide and dump.\" These differ on the amount of time and labor involved and whether the vermicomposter wants to save as many worms as possible from being trapped in the harvested compost.\n\nThe pyramid method of harvesting worm compost is commonly used in small-scale vermiculture, and is considered the simplest method for single layer bins. In this process, compost is separated into large clumps, which is placed back into composting for further breakdown, and lighter compost, with which the rest of the process continues. This lighter mix is placed into small piles on a tarp under the sunlight. The worms instinctively burrow to the bottom of the pile. After a few minutes, the top of the pyramid is removed repeatedly, until the worms are again visible. This repeats until the mound is composed mostly of worms.\n\nWhile harvesting, it's also a good idea to try to pick out as many eggs/cocoons as possible and return them to the bin. Cocoons are small, lemon-shaped yellowish objects that can usually be seen pretty easily with the naked eye and picked out. The cocoons can hold up to 20 worms (though 2-3 is most common). Cocoons can lay dormant for as long as two years if conditions are not conducive for hatching. \n\nVermicompost has been shown to be richer in many nutrients than compost produced by other composting methods. It has also outperformed a commercial plant medium with nutrients added, but levels of magnesium required adjustment, as did pH.\n\nHowever, in one study it has been found that homemade backyard vermicompost was lower in microbial biomass, soil microbial activity, and yield of a species of ryegrass than municipal compost.\n\nIt is rich in microbial life which converts nutrients already present in the soil into plant-available forms.\n\nUnlike other compost, worm castings also contain worm mucus which helps prevent nutrients from washing away with the first watering and holds moisture better than plain soil.\n\nIncreases in the total nitrogen content in vermicompost, an increase in available nitrogen and phosphorus, as well as the increased removal of heavy metals from sludge and soil have been reported. The reduction in the bioavailability of heavy metals has been observed in a number of studies.\n\nSoil\n\nPlant growth\n\nEconomic\n\nEnvironmental\n\nVermicompost can be mixed directly into the soil.\n\nThe dark brown waste liquid, or leachate, that drains into the bottom of some vermicomposting systems as water-rich foods break down, is best applied back to the bin when added moisture is needed due to the possibility of phytotoxin content and organic acids that may be toxic to plants.\n\nThe pH, nutrient, and microbial content of these fertilizers varies upon the inputs fed to worms. Pulverized limestone, or calcium carbonate can be added to the system to raise the pH.\n\nWhen closed, a well-maintained bin is odorless; when opened, it should have little smell—if any smell is present, it is earthy. Worms require gaseous oxygen. Oxygen can be provided by airholes in the bin, occasional stirring of bin contents, and removal of some bin contents if they become too deep or too wet. If decomposition becomes anaerobic from excess wet feedstock added to the bin, or the layers of food waste have become too deep, the bin will begin to smell of ammonia.\n\nMoisture must be maintained above 50%, as lower moisture content will not support worm respiration and can increase worm mortality. Operating moisture-content range should be between 70-90%, with a suggested content of 70-80% for vermicomposting-oriented vermiculture operations. If decomposition has become anaerobic, to restore healthy conditions and prevent the worms from dying, excess waste water must be reduced and the bin returned to a normal moisture level. To do this, first reduce addition of food scraps with a high moisture content and second, add fresh, dry bedding such as shredded newspaper to your bin, mixing it in well.\n\nPests such as rodents and flies are attracted by certain materials and odors, usually from large amounts of kitchen waste, particularly meat. Eliminating the use of meat or dairy product in a worm bin decreases the possibility of pests.\n\nPredatory ants can be a problem in African countries.\n\nIn warm weather, fruit and vinegar flies breed in the bins if fruit and vegetable waste is not thoroughly covered with bedding. This problem can be avoided by thoroughly covering the waste by at least of bedding. Maintaining the correct pH (close to neutral) and water content of the bin (just enough water where squeezed bedding drips a couple of drops) can help avoid these pests as well.\n\nWorms generally stay in the bin, but may try to leave the bin when first introduced, or often after a rainstorm when outside humidity is high. Maintaining adequate conditions in the worm bin and putting a light over the bin when first introducing worms should eliminate this problem.\n\nCommercial vermicomposters test, and may amend their products to produce consistent quality and results. Because the small-scale and home systems use a varied mix of feedstocks, the nitrogen, potassium and phosphorus content of the resulting vermicompost will also be inconsistent. NPK testing may be helpful before the vermicompost or tea is applied to the garden.\n\nIn order to avoid over-fertilization issues, such as nitrogen burn, vermicompost can be diluted as a tea 50:50 with water, or as a solid can be mixed in 50:50 with potting soil.\n\nAdditionally, the mucous layer created by worms which surrounds their castings allows for a \"time release\" effect, meaning not all nutrients are released at once. This also reduces the risk of burning the plants, as is common with the use and overuse of commercial fertilizers.\n\nVermicomposting (also known as vermiculture) is widely used in North America for on-site institutional processing of food scraps, such as in hospitals, universities, shopping malls, and correctional facilities. Vermicomposting is used for medium-scale on-site institutional organic material recycling, such as for food scraps from universities and shopping malls. It is selected either as a more environmentally friendly choice than conventional disposal, or to reduce the cost of commercial waste removal.\n\nResearchers from the Pondicherry University discovered that worm composts can also be used to clean up heavy metals. The researchers found substantial reductions in heavy metals when the worms were released into the garbage and they are effective at removing lead, zinc, cadmium, copper and manganese.\n\n\n"}
{"id": "21592156", "url": "https://en.wikipedia.org/wiki?curid=21592156", "title": "WH Group", "text": "WH Group\n\nWH Group (), formerly known as Shuanghui Group (), is a privately owned Chinese meat and food processing company headquartered in Luohe, Henan, China. Sometimes also known as Shineway Group in English-speaking countries, the company's businesses include hog raising, consumer meat products, flavoring products, and logistics. It is the largest pork producer in the world, and the largest meat producer in China.\n\nShuanghui has 13 facilities that produce more than 2.7 million tons of meat per year. It slaughters more than 15 million pigs a year, but only raises about 400,000; the rest are purchased from suppliers. The company holds more than 500 patents and produces 1,000 different products.\n\nWan Long (), nicknamed China's \"number one butcher\" because of the large number of pigs the company slaughters, is the chairman of Shuanghui. As of May 2013, Zhijun Yang is the managing director. The company employs over 60,000 people and is 30% employee owned.\n\nIn January 2014, Shanghui International changed its name to WH Group, though one of its subsidiaries, Henan Shuanghui Investment & Development Co., Ltd., retained the Shanghui name. The new name is derived from the initials of \"Wanzhou Holdings,\" where the Chinese characters \"wan\" and \"zhou\" connote eternity and continents, respectively. When it acquired Smithfield Foods in 2013, it was still known as Shanghui.\n\nShuanghui was set up by the local Luohe city government in 1958 as a single processing plant. Wan Long was appointed chairman in 1984. Under Wan, the company has expanded aggressively. In his first year, he turned around a struggling company, bringing it from a net loss to a net profit of 5 million yuan ($1.7 million). The company introduced its first branded meat product to the market in February 1992. Later that year, Shuanghui formed a joint venture with 16 institutional investors across six countries. In 1994, the venture was incorporated as Shuanghui Group. Shuanghui subsidiary Henan Shuanghui Investment & Development Company Limited () was established and listed on the Shenzhen Stock Exchange in 1998. In 2000, Shuanghui started a post-secondary educational-work research division.\n\nBy 2006, Shuanghui was the largest food processor in China by company value, and 131st largest company overall. The company was valued at $1.3 billion, and controlled more than 50% of China's high-temperature processed meat market at that time. That year, Luohe government sold its share of Shuanghui to a joint venture of Goldman Sachs and private equity firm CDH Investments. Goldman Sachs later sold most of its share, reportedly for a large profit, but owned 5.2% of the company as of May 2013.\n\nIn 2011, state-owned CCTV revealed that Shuanghui pork contained clenbuterol, a banned chemical that can be harmful to humans.\n\nOn 29 May 2013, Shuanghui announced its intention to purchase American pork producer Smithfield Foods, Inc., for $34 per share, or approximately US$4.72 billion total. Including assumed debt, the total value of the deal was about $7.1 billion. The agreed purchase price represented a 31% premium over Smithfield's market price at the time when the deal was announced. The businesses had been in discussion for four years before they were able to come to terms.\n\nBefore it was finalized, the deal had to be approved by Smithfield shareholders and the U.S. Committee on Foreign Investment in the United States. However, \"The Wall Street Journal\" predicted the deal was \"unlikely to face serious opposition\" from regulators. \"The administration clearly has a public policy of open arms – it's hard for me to believe that there are going to be many speed bumps in this transaction,\" said U.S. China Economic and Security Review Committee member Michael Wessel. Several Congressmen expressed concerns over the purchase but stopped short of saying they would oppose it. It was ultimately approved by Smithfield shareholders on September 24, 2013, and the merger was to be completed two days later.\n\nTo allay potential concerns about food safety, Smithfield CEO Larry Pope stated the deal would \"[preserve] the same old Smithfield, only with more opportunities and new markets and new frontiers.\" No Chinese pork would be imported to the United States, he stated, but rather Shuanghui desired to export American pork. There is a growing demand for foreign food products in China due to recent food scandals. Smithfield's existing management team would remain intact and no major changes to its workforce would occur. Analyst Derek Scissors said companies such as Shuanghui are \"not looking to cause any trouble in the American market ...They want to gain from what the U.S. is able to do.\" China has been a net importer of pork since 2008.\n\nThe deal was the largest ever takeover of a U.S. company by a Chinese company, roughly doubling the number of US jobs tied to direct investment by China. The previous largest takeover was Dalian Wanda Group's acquisition of AMC Theatres for $2.6 billion. Smithfield ceased to be publicly traded at the deal's completion.\n\nIn July 2013, Shuanghui announced its plan to list Smithfield on the Hong Kong Stock Exchange after completing the takeover. The IPO was expected to see the firm valued at around $4 billion. However, the IPO plan was ultimately scrapped in 2014.\n"}
