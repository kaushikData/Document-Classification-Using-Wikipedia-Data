{"id": "56098061", "url": "https://en.wikipedia.org/wiki?curid=56098061", "title": "2069 Alpha Centauri mission", "text": "2069 Alpha Centauri mission\n\nNASA introduced in December 2017 a mission concept to launch an interstellar probe by the year 2069 to search for biosignatures on planets around the stars in the Alpha Centauri system. The mission concept is as yet not funded, unnamed and the technology to get there does not exist yet.\n\nA preliminary mission concept suggests using solar sails propelled by high energy lasers to increase propulsion. The proposed launch would be on the 100th anniversary of the Apollo 11 mission.\n\n"}
{"id": "49642822", "url": "https://en.wikipedia.org/wiki?curid=49642822", "title": "3D microfabrication", "text": "3D microfabrication\n\nThree-dimensional (3D) microfabrication refers to a manufacturing technique that involves the layering of material to produce a three-dimensional structure. These structures are usually on the scale of micrometers and are popular in microelectronics and microelectromechanical systems.\n\nMuch like their macroscopic analog, microstructures can be produced using rapid prototyping methods. These techniques generally involve the layering of some resin, with each layer being much thinner than that used for conventional processes in order to produce higher resolution microscopic components. Layers in processes such as electrochemical fabrication can be as thin as 5 to 10 μm. The creation of microscopic structures is similar to conventional additive manufacturing techniques in that a computer aided design model is sliced into an appropriate number of two-dimensional layers in order to create a toolpath. This toolpath is then followed by a mechanical system to produce the desired geometry.\n\nA popular application is stereolithography (SLA), which involves the use of a UV light or laser beam on a surface to create a layer, which are then lowered into a tank so that a new layer can be formed on top. Another commonly used method is fused deposition modeling (FDM), in which a moving head creates a layer by melting the model material (usually a polymer) and extrudes the melted material onto a surface. Other methods such as selective laser sintering (SLS) are also used in the additive manufacturing of 3D microstructures.\n\nLaser-based techniques are the most common approach for producing microstructures. Typical techniques involve the use of lasers to add or subtract material from a bulk sample. Recent applications of lasers involve the use of ultrashort pulses of lasers focused to a small area in order to create a pattern that is layered to create a structure. The use of lasers in such a manner is known as laser direct-writing (LDW). Microscopic mechanical elements such as micromotors, micropumps, and other microfluidic devices can be produced using direct-write concepts. In addition to additive and subtractive processes, LDW allows for the modification of the properties of a material. Mechanisms that allow for these modifications include sintering, microstereolithography, and multiphoton processes. These use a series of laser pulses to deliver a precise amount of energy to induce a physical or chemical change that can result in annealing and surface structuring of a material.\n\nMicrostereolithography is a common technique based on stereolithography principles. 3D components are fabricated by repeatedly layering photopolymerizable resin and curing under an ultraviolet laser. Earlier systems that employ this technique use a scanning principle in which a focused light beam is fixed onto one location and the translation stage moves to fabricate each layer vector by vector. A faster alternate involves using a projection principle in which the image is projected onto the surface of the resin so that the irradiation of a layer is done in one step only. The high-resolution results allow for the fabrication of complex shapes that would otherwise be difficult to produce at such small scales.\n\nMultiphoton Lithography can be used to 3D print structures with sub-micrometer resolution. The process uses the focal point of a laser to photopolymerize the resin or glass at a specific point. By moving the focal point around in three dimensional space and solidifying the medium at different points, the desired geometry can be built. There are currently limits to the resolution of the features in geometries built through this method. The limits relate to the medium that the geometry is being constructed from as well as the precision of the focal point of the laser.\n\nAdditive processes involve the layering of materials in a certain pattern. These include laser chemical vapor deposition (LCVD), which use organic precursors to write patterns on a structure or bulk material. This application can be found in the field of electronics, particularly in the repair of transistor arrays for displays. Another additive process is laser-induced forward transfer (LIFT), which uses pulsed lasers aimed at a coated substrate to transfer material in the direction of the laser flow. LIFT has been used to produce transfer thermo-electric materials, polymers and has been used to print copper wires. \n\nFocus on the 3D microstructures now, it have been focused in a lot of microsystems like electronic, mechanical, micro-optical and analysis systems. And when this technology is developing, we found that the traditional and conventional micro machining technologies like surface micromachining, bulk micromachining and GIGA process are not sufficient to fabricate or produce oblique and curved 3D microstructures.\n\nThe basic setup of inclined UV exposure has conventional UV source, a contact stage, and a tilting stage. Plus, we place a photomask and a photoresist coated substrate between the upper and lower plates of the contact stage, and it is fixed by pushing up the lower plate with a screw. Then, we can expose the photoresist to the inclined UV.\n\nAn example of the fabrication process: Su-8 is a negative thick photoresist, which used in novel 3D micro fabrication method with inclined/rotated UV lithography. During the process, we coat SU-8 50 on a silicon wafer with a thickness of about 100ųm. Then, soft bake the resist on a 65 °C hot plate for 10 minutes and on a 95 °C plate for 30 minutes. It is contacted with a photomask using the contact stage. This stage, is leaned against the tilting stage and the resist is exposed to the UV. The dose of 365 nm UV is 500mJ/cm². After the exposure, the resist is post-expoure baked on a 65 °C hot plate for 3 minutes and on a 95 °C plate for 10 minutes. In the end, the resist is developed in the SU-8 for about 10 to 15 minutes at the room temperature with mild agitation and then, rinsed with isopropyl alcohol. Besides that, there can be a lot of other procedures. For example, inclined UV lithography, inclined and rotated UV lithography and lithography using reflected UV.\n\nWhen the trace of the incident UV with a right angle is on a straight line, so the patterns of a photomask are transcribed to the resist. When talking about inclined UV exposure processes, the UV is refracted and reflected, this makes it possible to fabricate various of 3D structures.The microstructures fabricated by the 3D micro fabrication technology can be allied to a lot of microsystems directly. Also, it can be used as the molds for electroplating. As a result, these technology can be applied to a variety of fields like filters, mixers, jets, micro channels, light guide panels of LCD monitor and more.\n\nDesign of complicated 3D microstructure can be highly challenging task for development of novel materials for optics, biotechnology and micro/nano electronics. 3D materials can be fabricated using a lot of methods like two-photon photolithography, interference lithography and molding. But 3D structuring using these techniques is very complicated, experimentally. This can limit their upscaling and broad applicability.\n\nNature offers a large number of ideas for the design of novel materials with superior properties. Self-assembly and self-organization being the main principle of structure formation in nature attract significant interest as promising concepts for the design of intelligent materials.\n\nStimuli-responsive hydrogels mimic swelling/shrinking behavior of plant cells and produce macroscopic actuation is response to small variation of environmental conditions. Mostly, homogenous expansion or contraction in all directions can result a change of conditions. Also, inhomogeneous expansion and shrinkage can result more complex behavior like bending, twisting and folding and they can happen with different magnitudes in different directions. Utilization of these phenomena for the design of structured materials can be highly attractive because they allow simple, template-free fabrication of very complex repetitive 2D and 3D patterns. However, they cannot be prepared by using sophisticated fabrication methods like two-photon and interference photolithography as mentioned before. There is an advantage of the self-folding approach, is the possibility of quick, reversible, and reproducible fabrication of 3D hollow objects with controlled chemical properties and morphology of both the exterior and the interior.\n\nOne experimental application of self-folding materials is pasta that ships flat but folds into the desired shape on contact with boiling water.\n\nOne factor that limit broad applicability of self-folding polymer films is the manufacturing cost. Actually, polymer can be deposited by spinning and dipping coating at ambient conditions, the fabrication of polymer self-folding films is substantially cheaper than fabrication of inorganic ones, which are produced by vacuum deposition. In another word, there is no method, which is cheap and large-scale production of self-folding polymer films that substantially limits their application.\n\nTo solve these issues, the future research myst be focused on deeper investigation of folding to allow design of complex 3D structures using just 2D shapes. On the other hand, searching a way, which is cheap and fast manufacturing of large quantity of self-folding films can be greatly helpful.\n"}
{"id": "14706356", "url": "https://en.wikipedia.org/wiki?curid=14706356", "title": "Algorithms + Data Structures = Programs", "text": "Algorithms + Data Structures = Programs\n\nAlgorithms + Data Structures = Programs is a 1976 book written by Niklaus Wirth covering some of the fundamental topics of computer programming, particularly that algorithms and data structures are inherently related. For example, if one has a sorted list one will use a search algorithm optimal for sorted lists.\n\nThe book was one of the most influential computer science books of the time and, like Wirth's other work, was extensively used in education.\n\nThe Turbo Pascal compiler written by Anders Hejlsberg was largely inspired by the \"Tiny Pascal\" compiler in Niklaus Wirth's book.\n\n\n"}
{"id": "11438514", "url": "https://en.wikipedia.org/wiki?curid=11438514", "title": "Analog device", "text": "Analog device\n\nAnalog device is usually a combination of both analog machine and analog media that can together measure, record, reproduce, or broadcast continuous information, for example, the almost infinite number of grades of transparency, voltage, resistance, rotation, or pressure. In theory, the continuous information (also analog signal) has an infinite number of possible values with the only limitation on resolution being the accuracy of the analog device.\n\nThere are notable non-electrical analog devices, such as clocks (sundials, water clocks, pendulum clocks, analog watches), the astrolabe, slide rules, the governor of a steam engine, the planimeter (a simple device that measures the area of a closed shape), Kelvin's mechanical tide predictor, acoustic rangefinders, servomechanisms (e.g. the thermostat), a simple mercury thermometer, a bathroom scale, and the speedometer \nThe telautograph is an analogue precursor to the modern fax machine. It transmits electrical impulses recorded by potentiometers to stepping motors attached to a pen, thus being able to reproduce a drawing or signature made by the sender at the receiver's station. It was the first such device to transmit drawings to a stationary sheet of paper; previous inventions in Europe used rotating drums to make such transmissions.\n\nAn analog synthesizer is a synthesizer that uses analog circuits and analog computer techniques to generate sound electronically.\n\nThe analog television encodes television and transports the picture and sound information as an analogue signal, that is, by varying the amplitude and/or frequencies of the broadcast signal. All systems preceding digital television, such as NTSC, PAL or UNIT are analog television systems.\n\nAn analog computer is a form of computer that uses electrical, mechanical or hydraulic phenomena to model the problem being solved. More generally an analog computer uses one kind of physical quantity to represent the behaviour of another physical system, or mathematical function. Modeling a real physical system in a computer is called simulation.\n\nIn electronics, a digital-to-analog converter (DAC or D-to-A) is a circuit for converting a digital signal (usually binary) to an analog signal (current, voltage or electric charge). Digital-to-analog converters are interfaces between the digital world and analog world. An analog-to-digital converter (abbreviated ADC, A/D or A to D) is an electronic circuit that converts continuous signals to discrete digital numbers.\n\nThere is quite a bit of controversy in the photographic world as to whether or not a film camera should be considered an \"analog device\" since the photographic film isn't a truly analog recording medium and requires chemistry to process an image. \"Analog\" has become the common term for referring to cameras that are \"not digital\" and is an easy shorthand for people to differentiate between the two mediums.\n"}
{"id": "51194697", "url": "https://en.wikipedia.org/wiki?curid=51194697", "title": "Annual Biocontrol Industry Meeting", "text": "Annual Biocontrol Industry Meeting\n\nThe Annual Biocontrol Industry Meeting (ABIM) in Basel is an annual conference of manufacturers of biological plant protection products worldwide. Every year since 2005, 700 – 800 delegates from 300 – 400 firms take part in this English-speaking meeting.\n\nThe goal of the conference is the exchange of business and scientific experience and presentation of commercial and scientific advances on the subject of the protection of plants and pest control in plant crops by natural (biological) methods, with particular reference to Bioeffectors.\n\nThe meeting takes place every autumn in Basel, and is organised by the Swiss Research Institute for Biological Agriculture (FiBL). In parallel the annual meeting of the International Biocontrol Manufacturers' Association (IBMA), the association representing the biological plant protection industry, is held.\n\nImportant sponsors of the meeting are BASF, Bayer, Biobest, Biogard, De Sangosse, Koppert, Monsanto Bio AG, Oro Agri, Sumitomo, Syngenta, Vallent Biosciences\n\n"}
{"id": "32345395", "url": "https://en.wikipedia.org/wiki?curid=32345395", "title": "Association of Plumbing and Heating Contractors", "text": "Association of Plumbing and Heating Contractors\n\nThe Association of Plumbing and Heating Contractors (APHC) is a trade association for the plumbing and heating industry in England and Wales, representing around 1500 businesses employing some 60,000 specialist engineers ranging from those employed by large companies to sole traders working in domestic properties.\n\nThe APHC represents these specialists on the Specialist Engineering Contractors Group, a member of the Strategic Forum for Construction.\n\nThe APHC started in 1925 as the National Federation of Plumbers and Domestic Engineers, focused on industrial and commercial aspects of plumbing that had previously been managed by the Institute of Plumbers (today the CIPHE), which remained focused on education, training and technical matters.\n\nIt became the National Federation of Plumbers and Domestic Heating Engineers in 1965 to reflect the increased amount of members' work on heating systems. In 1972 it became an association: the National Association of Plumbing, Heating and Mechanical Services Contractors. It adopted its current name in 1996.\n\n"}
{"id": "34026570", "url": "https://en.wikipedia.org/wiki?curid=34026570", "title": "Axess (CRS)", "text": "Axess (CRS)\n\nAxess (アクセス国際ネットワーク) is a Computer reservations system based in Japan which provides its services in the Japanese market. It originated as the IT department of Japan Airlines called Jalinfotech. In 1991 it was established as an independent company it began to be marketed to travel agencies in Japan. In 1995 Axess partnered with Sabre Holdings in order to provide travel agencies in Japan with booking and ticketing capabilities for a wider range of international airlines.\n\nOn April 23 2012 Travelport, the business services provider to the global travel industry, announced a long-term agreement with AXESS International Network, the leading Japanese GDS owned by Japan Airlines (JAL). Under the new agreement, AXESS will be hosted by Travelport in its Atlanta data center as a partition of the Travelport global distribution system. The new upgraded AXESS GDS system, which will provide enhanced functionality to connected travel agency users, will be implemented by 2013.\n\nBy adopting Travelport's technology infrastructure, the AXESS GDS system will be enhanced with improved connectivity to airlines and a significant increase in the range of fares, shopping, hotel and car rental capabilities available for use by Japanese travel agencies and corporations. \n\nAXESS confirmed it had selected Travelport as its partner due to the two companies' mutual alignment on strategic thinking and saw the agreement as a significant step forward in reinforcing AXESS's position as the GDS of choice in the Japanese travel industry. \n\n"}
{"id": "270925", "url": "https://en.wikipedia.org/wiki?curid=270925", "title": "Bulletin board", "text": "Bulletin board\n\nA bulletin board (pinboard, pin board, noticeboard, or notice board in British English) is a surface intended for the posting of public messages, for example, to advertise items wanted or for sale, announce events, or provide information. Bulletin boards are often made of a material such as cork to facilitate addition and removal of messages, as well as a writing surface such as blackboard or whiteboard. A bulletin board which combines a pinboard (corkboard) and writing surface is known as a combination bulletin board. Bulletin boards can also be entirely in the digital domain and placed on computer networks so people can leave and erase messages for other people to read and see, as in a bulletin board system.\n\nBulletin boards are particularly prevalent at universities. They are used by many sports groups and extracurricular groups and anything from local shops to official notices. Dormitory corridors, well-trafficked hallways, lobbies, and freestanding kiosks often have cork boards attached to facilitate the posting of notices. At some universities, lampposts, bollards, trees, and walls often become impromptu posting sites in areas where official boards are sparse in number.\n\nInternet forums are a replacement for traditional bulletin boards. Online bulletin boards are sometimes referred to as message boards. The terms bulletin board, message board and even Internet forum are interchangeable, although often one bulletin board or message board can contain a number of Internet forums or discussion groups. An online board can serve the same purpose as a physical bulletin board.\n\nMagnet boards, or magnetic bulletin boards, are a popular substitute for cork boards because they lack the problem of board deterioration from the insertion and removal of pins over time.\n\n\n"}
{"id": "8738917", "url": "https://en.wikipedia.org/wiki?curid=8738917", "title": "CAD navigation", "text": "CAD navigation\n\nCAD navigation refers to software tools which are used for the correlation of electronic semiconductor design data with a physical semiconductor device. CAD navigation tools consist of software that is capable of reading and displaying the physical layout and logical schematic for the device. The logical design consists of a netlist and/or a schematic. The physical design consists of a set of polygons which precisely represent the location of all electrical conductors, diffusions and interconnections in the physical semiconductor device. CAD navigation tools are often used to provide a cross-correlation between the logical design and the physical design. CAD navigation tools are used extensively with E-beam probers, focused-ion beam systems and photon probers for the purpose of semiconductor failure analysis.\n"}
{"id": "1111897", "url": "https://en.wikipedia.org/wiki?curid=1111897", "title": "Chamberlain Group, Inc. v. Skylink Technologies, Inc.", "text": "Chamberlain Group, Inc. v. Skylink Technologies, Inc.\n\nThe Chamberlain Group, Inc. v. Skylink Technologies, Inc., 381 F.3d 1178 (Fed. Cir. 2004) is a legal case heard by the United States Court of Appeals for the Federal Circuit concerning the anti-trafficking provision of the Digital Millennium Copyright Act (DMCA), , in the context of two competing universal garage door opener companies. It discusses the statutory structure and legislative history of the DMCA to help clarify the intent of the anti-circumvention provisions and decide who holds the burden of proof. It expresses that the statute creates a cause of action for liability and does not create a property right, and holds that as Chamberlain had alleged that Skylink was in violation of the anti-trafficking provision, it had the burden to prove and failed to show that access was unauthorized and its rights were infringed under the Copyright Act. As Chamberlain incorrectly argued that Skylink had the burden of proof and failed to prove their claim, the court upheld summary judgment in favor of Skylink.\n\nThis case involves two competitors that produce universal garage door openers (GDOs). Universal garage door openers are used when people want to replace or purchase a spare transmitter to open their garage door. They are designed to interoperate with existing GDO systems, regardless of model.\n\nChamberlain markets a \"Security+\" line of GDOs which includes rolling code software that actively alters the transmitted signal by cycling through a series of strings (of which only some are able to open the door). This rolling code is designed to protect against a potential \"code grabbing\" attack where a nearby burglar may try to record the garage door opening signal. Chamberlain claims that the rolling code system makes it unlikely for a burglar to send a valid signal by replaying the recorded one. With rolling code protection, a garage door will open if and only if the transmitted code is not among the last 1024 used codes and it is among the next 4096 codes. The Security+ has additional functionality that will cause the GDO to resynchronize when two signals out of the acceptable range are transmitted in rapid succession. This was added in the case that homeowners use the same transmitter on multiple garage doors.\n\nIn 1992, Skylink produced a universal transmitter called Model 39 that was designed to work for both rolling code and non-rolling code GDOs. The Model 39 bypasses the Chamberlain's rolling code system by imitating Security+'s resynchronization feature. The Model 39 transmitter sends three fixed codes in rapid succession; this either causes the door to open due to the first code or it causes the door to resynchronize and open due to the latter two codes.\n\nThis case involves the anti-trafficking provisions of the DMCA. 17 U.S.C. § 1201(b) states:\nNo person shall manufacture, import, offer to the public, provide, or otherwise traffic in any technology, product, service, device, component, or part thereof, that—\n\n(A) is primarily designed or produced for the purpose of circumventing a technological measure that effectively controls access to a work protected under this title;\n\nChamberlain sued Skylink in the United States District Court for the Northern District of Illinois in two cases. In the first case, \"Chamberlain I\", Chamberlain alleged that Skylink's actions violated the anti-trafficking provisions of the DMCA and moved for summary judgment. The court denied Chamberlain's motion for summary judgment. In the second case, \"Chamberlain II\", Chamberlain alleged that Skylink infringed their patents and violated the anti-trafficking provisions of the DMCA.\n\nWith regards to the DMCA claim in \"Chamberlain II\", Chamberlain contended that:\n\nIn response, Skylink claimed that \"consumers use the Model 39 transmitter to activate the Security+ GDOs with Chamberlain's consent.\" §1201(a)(3)(A) states that to \"circumvent a technological measure\" means to \"descramble a scrambled work, to decrypt an encrypted work, or otherwise to avoid, bypass, remove, deactivate, or impair a technological measure, without the authority of the copyright owner.\" They also claimed that the Model 39 transmitter served a variety of functions that were unrelated to circumvention, that Chamberlain had failed to demonstrate that its GDOs contained a computer program protected by copyright, that Skylink had not violated the DMCA because its acts fell within a safe harbor provision per §1201(f), and that Chamberlain's rolling code computer program did not protect a copyrighted computer program, but instead protects an uncopyrightable process.\n\nChamberlain claimed that (1) Skylink had the burden to prove that their use was authorized and (2) Chamberlain \"never gave consumers explicit authorization to program competing universal transmitters into its rolling code openers.\"\n\nThe District Court agreed with Skylink and because Chamberlain did not explicitly restrict the consumer's use of alternate transmitters, this was deemed an unconditional sale that implicitly authorized customers to use other transmitters. The court also noted that Chamberlain's construction and interpretation of the DMCA would make its own consumers violate §1201(a)(1), which prohibits circumvention of a technological measure that controls access.\n\nAs Chamberlain did not show that Skylink's access was unauthorized, the District Court granted the motion for summary judgment to Skylink and dismissed the patent claims.\n\nChamberlain appealed the District Court's decision of summary judgment granted to Skylink in \"Chamberlain II\" and the case was heard before the United States Court of Appeals for the Federal Circuit. In this case, Chamberlain claimed the District Court incorrectly placed the burden on Chamberlain to prove that the circumvention of its technological measures was unauthorized when it should have placed the burden on Skylink to show that the use was authorized.\n\nThe Federal Circuit affirmed the District Court's ruling. The court explained that by explicitly stating that circumvention occurs \"without the authority of the copyright owner,\" the DMCA requires the plaintiff alleging circumvention to show that the defendant's access was unauthorized.\n\nThe Federal Circuit went on to clarify the nature of the DMCA's anti-circumvention provisions. The DMCA established causes of action for liability and did not establish a property right. Therefore, circumvention is not infringement in itself.\n\nIn response to Chamberlain's assertions that the DMCA \"renders the pre-DMCA history in the GDO industry irrelevant,\" \"fundamentally altered the legal landscape,\" and \"overrode all pre-existing consumer expectations about the legitimate uses of products containing copyrighted embedded software,\" the court disagreed. In Chamberlain's view, all use of products that contain copyrighted software and use protective technological measures would violate the DMCA and this would give companies a loophole around antitrust laws as well as take away the fair use defense. By examining the structure and history of the statute and the intent of Congress, the court attempted to interpret the statutory language. The court found that the goals of the DMCA were to establish a balance between the competing interests of content owners and information users and balance access control measures with fair use.\n\nChamberlain had the burden to prove that (1) they had ownership of a copyrighted work, (2) it was controlled by a technological measure that was circumvented, (3) third parties can access it (4) without authorization in a way that (5) infringes rights protected by the Copyright Act due to a product created, advertised, or provided by the defendant Skylink. Once the plaintiff proves those five they must also prove that the defendant trafficked in a product which was (i) designed or produced primarily for circumvention; (ii) made available despite only limited commercial significance other than circumvention; or (iii) marketed for use in circumvention of the controlling technological measure. Chamberlain never claimed that Skylink infringed its copyrights or contributed to third-party infringement nor did it show that its users were unauthorized to use the product. As Chamberlain failed to show the fourth and fifth requirements to prove their claim, the Federal Circuit affirmed the District Court's grant of summary judgment to Skylink, writing,\nChamberlain, however, has failed to show not only the requisite lack of authorization, but also the necessary fifth element of its claim, the critical nexus between access and protection. Chamberlain neither alleged copyright infringement nor explained how the access provided by the Model 39 transmitter facilitates the infringement of any right that the Copyright Act protects. There can therefore be no reasonable relationship between the access that homeowners gain to Chamberlain's copyrighted software when using Skylink's Model 39 transmitter and the protections that the Copyright Act grants to Chamberlain.\n\n"}
{"id": "9780125", "url": "https://en.wikipedia.org/wiki?curid=9780125", "title": "Color framing", "text": "Color framing\n\nIn video engineering, color framing refers to the color frame sequence of fields in a composite video signal through which the video frame timing and chrominance subcarrier signal timing—in particular, that of the color burst -- cycle through all possible phase relationships.\n\nThe exact nature of the color frame sequence depends on the video standard being used. In the case of the three main composite video standards, PAL video has an 8-field (4 frame) color frame sequence, and NTSC and SECAM both have 4-field (2 frame) color frame sequences.\n\nPreserving the color framing sequence of video across edits and between channels in video effects was an important issue in early analog composite videotape editing systems, as cuts between different color sequences would cause jumps in subcarrier phase, and mixing two signals of different field dominance would result in color artifacts on the part of the signal that was not in sync with the output color frame sequence. \n\nTo help prevent these problems, SMPTE time code contains a color framing bit, which can be used to indicate that the video material the timecode refers to follows a standard convention regarding the synchronization of video time code and the color framing sequence. If the color framing bit was set in both types of material, the editing system could then always ensure that color framing was preserved by constraining edit decisions between input sources to keep the correct relationship between the timecode sequences, and hence the color framing sequences.\n\nColor framing has become largely an issue of historical interest, first with the advent in the 1980s of digital composite video timebase correctors and frame stores, which could regenerate the color frame sequence of a composite signal at any phase, and later with analog component video editing and modern digital video systems, in which subcarrier phase is no longer relevant. \n\n"}
{"id": "58977586", "url": "https://en.wikipedia.org/wiki?curid=58977586", "title": "Diffuse design", "text": "Diffuse design\n\nDiffuse design refers to the designing capability of individuals who are not formally trained as designers. Drawing on the natural human ability to adopt a design approach, nonexpert designers bring diffuse design into the world via a combination of critical sense, creativity, and practical sense.\n\nDiffuse design was coined by Italian design scholar Ezio Manzini and was a central theme of his 2015 book \"Design, When Everybody Designs\". Manzini asserts that everybody is endowed with the ability to design, though not everyone is a competent designer and fewer still become professional designers. He also suggests it is the role of expert designers in social innovation contexts to improve the conditions by which different social actors can take part in co-design processes in a more expert fashion.\n"}
{"id": "17910574", "url": "https://en.wikipedia.org/wiki?curid=17910574", "title": "Digital ecosystem", "text": "Digital ecosystem\n\nA digital ecosystem is a distributed, adaptive, open socio-technical system with properties of self-organisation, scalability and sustainability inspired from natural ecosystems. Digital ecosystem models are informed by knowledge of natural ecosystems, especially for aspects related to competition and collaboration among diverse entities. The term is used in the computer industry, the entertainment industry, and the World Economic Forum.\n\nThe concept of Digital Business Ecosystem was put forward in 2002 by a group of European researchers and practitioners, including Francesco Nachira, Paolo Dini and Andrea Nicolai, who applied the general notion of digital ecosystems to model the process of adoption and development of ICT-based products and services in competitive, highly fragmented markets like the European one\n. Elizabeth Chang, Ernesto Damiani and Tharam Dillon started in 2007 the IEEE Digital EcoSystems and Technologies Conference (IEEE DEST). Richard Chbeir, Youakim Badr, Dominique Laurent, and Hiroshi Ishikawa started in 2009 the ACM Conference on Management of Digital EcoSystems (MEDES)\n\nThe digital ecosystem metaphor and models have been applied to a number of business areas related to the production and distribution of knowledge-intensive products and services, including higher education. The perspective of this research is providing methods and tools to achieve a set of objectives of the ecosystem (e.g. sustainability, fairness, bounded information asymmetry, risk control and gracious failure). These objectives are seen as desirable properties whose emergence should be fostered by the digital ecosystem self-organization, rather than as explicit design goals like in conventional IT.\n\n\n"}
{"id": "8197284", "url": "https://en.wikipedia.org/wiki?curid=8197284", "title": "Enhanced mini-USB", "text": "Enhanced mini-USB\n\nAn enhanced mini-USB (EMU) connector is a type of hybrid electrical connector which carries Universal Serial Bus data and power as well as other connections such as bidirectional audio. It was invented for and is mainly used on mobile phones. Motorola, HTC Corporation, and other mobile phone manufacturers use EMU connectors. There is more than one standard for EMU connectors, which are incompatible between manufacturers, but all are physically and electrically compatible with standard mini-USB connectors. The EMU connector has five pins for USB on one side. While regular USB connectors are empty on the other side, EMU has more pins intended for headsets. In HTC's version, two pins are for the microphone, three are for stereo sound, and one is for the push-to-talk switch.\n\nIn the CEA-936-A standard, there are no extra pins — the USB data pins are also used for RS-232 transmit and receive, stereo audio left and right, or for microphone and speaker/earpiece. Devices select which function the pins perform depending on user settings or on the context or mode in which the device is being operated. Two different functions cannot be used at once.\n\nUsing the connector may require a breakout cable or special headset. Most often, the user must buy a special adapter or pigtail to make the correct connections to a 2.5mm TRS connector for a monophonic headset or 3.5mm for stereo headphones. True breakout cables which provide all connections are unavailable, thus a phone cannot be charged at the same time as a headset or headphones are inserted, even for EMUs with extra pins.\n\nSome mobile phone companies have used the extra pin \"X\" to enforce the use of their own battery chargers. Verizon Wireless is the first company to require first-party battery chargers, having colluded with Motorola to put an arbitrary 1.4 volts on pin X. This voltage violates the USB-IF standards, as pin X should either be tied to ground or not connected at all. Without this connection, the phone will refuse to charge, displaying \"unauthorized charger\" despite receiving the proper current. The phone will charge while connected to a personal computer only if the PC is running a special device driver.\n\n\n"}
{"id": "13525100", "url": "https://en.wikipedia.org/wiki?curid=13525100", "title": "Enterprise test software", "text": "Enterprise test software\n\nEnterprise test software (ETS) is a type of software that electronics and other manufacturers use to standardize product testing enterprise-wide, rather than simply in the test engineering department. It is designed to integrate and synchronize test systems to other enterprise functions such as research and development (R&D), new product introduction (NPI), manufacturing, and supply chain, overseeing the collaborative test processes between engineers and managers in their respective departments.\n\nLike most enterprise software subcategories, ETS represents an evolution away from custom-made, in-house software development by original equipment manufacturers (OEM). It typically replaces a cumbersome, unsophisticated, test management infrastructure that manufacturers have to redesign for every new product launch. Some large companies, such as Alcatel, Cisco, and Nortel, develop ETS systems internally to standardize and accelerate their test engineering activities, while others such as Harris Corporation and Freescale Semiconductor choose commercial off-the-shelf ETS options for advantages that include test data management and report generation. This need results from the extensive characterization efforts associated with IC design, characterization, validation, and verification. ETS accelerates design improvements through test system management and version control.\n\nETS supports test system development and can be interconnected with manufacturing execution services (MES), enterprise resource planning (ERP), and product lifecycle management (PLM) software packages to eliminate double-data entry and enable real-time information sharing throughout all company departments.\n\nETS covers five major enterprise-wide test applications.\n\n\n\n"}
{"id": "22381897", "url": "https://en.wikipedia.org/wiki?curid=22381897", "title": "ExxonMobil Electrofrac", "text": "ExxonMobil Electrofrac\n\nExxonMobil Electrofrac is an \"in situ\" shale oil extraction technology proposed by ExxonMobil for converting kerogen in oil shale to shale oil.\n\nExxonMobil Electrofrac uses a series of fractures created in the oil shale formation. Preferably these fractures should be longitudinal vertical fractures created from horizontal wells and conducting electricity from the heel to the toe of each heating well. For conductivity, an electrically-conductive material such as calcined petroleum coke is injected into the wells in fractures, forming a heating element. Heating wells are placed in a parallel row with a second horizontal well intersecting them at their toe. This allows opposing electrical charges to be applied at either end. Laboratory experiments have demonstrated that electrical continuity is unaffected by kerogen conversion and that hydrocarbons are expelled from heated oil shale even under \"in situ\" stress. Planar heaters should be used because they require fewer wells than wellbore heaters and offer a reduced surface footprint. The shale oil is extracted by separate dedicated production wells.\n\n"}
{"id": "38599727", "url": "https://en.wikipedia.org/wiki?curid=38599727", "title": "Fiatallis", "text": "Fiatallis\n\nFiat-Allis (1974 to 1985), later renamed Fiatallis (1985 to early 1990s), was a brand of heavy equipment (also called construction equipment, earthmoving equipment, or engineering vehicles), such as loaders, bulldozers, backhoes, scrapers, and graders. It began in 1974, when Allis-Chalmers's construction equipment business was reorganized into a joint venture with Fiat SpA, which bought a 65% majority stake at the outset. \n\nAllis-Chalmers's construction equipment business was often not profitable during the 1950s and 1960s. It faced stiff competition from companies such as Caterpillar, Case, Euclid, John Deere, Fiat MMT, and others. The 1974 formation of a new company, Fiat-Allis, was essentially a divestment. \n\nOver the next decade, Fiat continued to invest in Fiat-Allis, more so than Allis-Chalmers did. The two parent firms disagreed on decisions made at, and about, Fiat-Allis. In 1985, the joint venture ended. Fiat bought out Allis's remaining minority stake (which had shrunk from its original 35%). Fiat renamed the company Fiatallis. \n\nCooperation with Hitachi yielded Fiat-Hitachi branding for some years. Fiatallis was eventually sold to CNH, of which Fiat is a major stockholder. CNH retired the Fiatallis brand and merged the company's assets with others from Case and New Holland Machine Company. CNH's construction equipment line is called New Holland Construction. \n\nIn Argentina, Fiat-Allis was produced by Crybsa. This product under license 605-B excavator, C-130 and crawler tractor 7D.\n\n"}
{"id": "3428844", "url": "https://en.wikipedia.org/wiki?curid=3428844", "title": "Foundation for Open Project Documentation", "text": "Foundation for Open Project Documentation\n\nThe Foundation for Open Project Documentation \nis an organization. Its main idea is to create detailed and public documentation for all stages of software creation. The foundation was established in 2002 on the grand opening of the semifinals of ACM International Collegiate Programming Contest in Saint Petersburg by Anatoly Shalyto. This foundation was announced on the Linux Summit 2004, Russian Outsourcing and Software Summit 2004 and Open Source Forum Russia 2005. Project documentation development approbation in the context of this foundation was made in projects created using Switch-technology, intended for supporting Automata-Based Programming.\n\nThe motivation for creating \"detailed\" documentation is considered the following:\n\nThe motivation for making documentation \"open\" is considered the following:\n\nFoundation for Open Project Documentation gives a project a greater understandability and re-usability potential, than Open Source foundation. It is loosely connected with Free Software Foundation. OpenDoc project can also be commercial.\n\n\n"}
{"id": "884353", "url": "https://en.wikipedia.org/wiki?curid=884353", "title": "Gary Price (librarian)", "text": "Gary Price (librarian)\n\nGary Price (born 1965) is an American librarian and is currently co-editor, with Shirl Kennedy, of the \"InfoDocket\" feature of the online edition of \"Library Journal\". The two also operate FullTextReports.com and the stand-alone InfoDocket.com website, which are updated daily with new online search tools, library and research news, commentary, and full text reports from governments, think tanks, NGOs, academicians, and others.\n\nPrice lives near Washington, DC, and grew up in the Chicago suburbs where he attended New Trier High School.\n\nPrice received a bachelor of arts degree from the University of Kansas, and a master's in library and information science from Wayne State University. He was for a time a reference librarian at George Washington University. \nPrice co-authored the book \"The Invisible Web\" with Chris Sherman in July 2001.\n\nPrice has worked as a librarian at George Washington University, and for the search engine Ask.com as their director of online information resources.\n\nHe also does frequent consulting projects and has written for a number of publications. He is a contributing editor at \"Search Engine Land\", a news site devoted to search-engine optimization and marketing.\n\nBefore launching InfoDocket.com and FullTextReports.com in February 2011, Gary Price and Shirl Kennedy worked together for 10 years as founders and co-editors of other websites, including ResourceShelf.com, a search tool for librarians and researchers, and DocuTicker.com, focused on government and non-profit organization news.\n\nPrice received the Special Libraries Association's Innovations in Technology Award in 2002, and their News Division's Agnes Henebry Roll of Honor Award in 2004. He was also among the Wayne State University Library and Information Program Alumni of the Year Award recipients.\n\nFrom 1998 to 2002, he maintained \"Price's List of Lists\" (\"LoL\") \"a database of ranked listings of companies, people and resources freely available on the Internet\", later operated by SpecialIssues.com until 2013.\n\n"}
{"id": "36126758", "url": "https://en.wikipedia.org/wiki?curid=36126758", "title": "Gate driver", "text": "Gate driver\n\nA gate driver is a power amplifier that accepts a low-power input from a controller IC and produces a high-current drive input for the gate of a high-power transistor such as an IGBT or power MOSFET. Gate drivers can be provided either on-chip or as a discrete module. In essence, a gate driver consists of a level shifter in combination with an amplifier.\n\nIn contrast to bipolar transistors, MOSFETs do not require constant power input, as long as they are not being switched on or off. The isolated gate-electrode of the MOSFET forms a capacitor (gate capacitor), which must be charged or discharged each time the MOSFET is switched on or off. As a transistor requires a particular gate voltage in order to switch on, the gate capacitor must be charged to at least the required gate voltage for the transistor to be switched on. Similarly, to switch the transistor off, this charge must be dissipated, i.e. the gate capacitor must be discharged.\n\nWhen a transistor is switched on or off, it does not immediately switch from a non-conducting to a conducting state; and may transiently support both a high voltage and conduct a high current. Consequently, when gate current is applied to a transistor to cause it to switch, a certain amount of heat is generated which can, in some cases, be enough to destroy the transistor. Therefore, it is necessary to keep the switching time as short as possible, so as to minimize switching loss. Typical switching times are in the range of microseconds. The switching time of a transistor is inversely proportional to the amount of current used to charge the gate. Therefore, switching currents are often required in the range of several hundred milliamperes, or even in the range of amperes. For typical gate voltages of approximately 10-15V, several watts of power may be required to drive the switch. When large currents are switched at high frequencies, e.g. in DC-to-DC converters or large electric motors, multiple transistors are sometimes provided in parallel, so as to provide sufficiently high switching currents and switching power.\n\nThe switching signal for a transistor is usually generated by a logic circuit or a microcontroller, which provides an output signal that typically is limited to a few milliamperes of current. Consequently, a transistor which is directly driven by such a signal would switch very slowly, with correspondingly high power loss. During switching, the gate capacitor of the transistor may draw current so quickly that it causes a current overdraw in the logic circuit or microcontroller, causing overheating which leads to permanent damage or even complete destruction of the chip. To prevent this from happening, a gate driver is provided between the microcontroller output signal and the power transistor.\n\nCharge pumps are often used in H-Bridges in \"high side drivers\" for gate driving the high side n-channel power MOSFETs and IGBTs. These devices are used because of their good performance, but require a gate drive voltage a few volts above the power rail. When the centre of a half bridge goes low the capacitor is charged via a diode, and this charge is used to later drive the gate of the high side FET gate a few volts above the source or emitter pin's voltage so as to switch it on. This strategy works well provided the bridge is regularly switched and avoids the complexity of having to run a separate power supply and permits the more efficient n-channel devices to be used for both high and low switches.\n\n"}
{"id": "24948768", "url": "https://en.wikipedia.org/wiki?curid=24948768", "title": "General Purpose Serial Interface", "text": "General Purpose Serial Interface\n\nGeneral Purpose Serial Interface, also known as GPSI, 7-wire interface, or 7WS, is a 7 wire communications interface. It is used as an interface between Ethernet MAC and PHY blocks.\n\nData is received and transmitted using separate data paths (TXD, RXD) and separate data clocks (TXCLK, RXCLK). Other signals consist of transmit enable (TXEN), receive carrier sense (CRS), and collision (COL).\n\n"}
{"id": "9676821", "url": "https://en.wikipedia.org/wiki?curid=9676821", "title": "Heat-shrinkable sleeve", "text": "Heat-shrinkable sleeve\n\nHeat-shrinkable sleeve (or commonly \"shrink sleeve\") is a corrosion protective coating for pipelines in the form of a wraparound or tubular sleeve that is field-applied.\n\nThe first heat-shrinkable sleeves were introduced as polyethylene pipeline coatings started to replace bituminous or tape coatings in the oil and gas industry. At the time, the processing for polyethylene to make the sleeve backing was new technology and the adhesives used in sleeves were much the same as those used on pipeline coating.\n\nThe technology used to make sleeves has advanced significantly since then, with new methods of cross-linking the polyolefin backings and new-generation adhesives that are formulated to provide performance under more-demanding pipeline conditions.\n\nHeat-shrinkable means just that, heat them up and they shrink, or more correctly, they recover in length. A heat-shrinkable sleeve starts out with a thick extruded poly olefin sheet (polyethylene or polypropylene) that is formulated to be cross-linkable. After extruding the thick sheet, it is taken to the “beam” where it is passed under a unit that subjects the sheet to electron irradiation. The irradiation process cross-links the polyolefin. This improves the molecular structure such that the polyolefin will work as part of a heat-shrinkable sleeve and provide the required level of mechanical protection while in-service. It makes the polyolefin perform more like a tough, heat-resistant, elastic material or rubber, rather than like a plastic material.\n\nAfter cross-linking, the sheet is stretched by feeding it into a machine that heats it up, stretches it and cools it down. Because the sheet has been cross-linked, after stretching, it will want to recover to its original length when re-heated.\n\nIn recent years, many manufacturers had already developed their technologies of extruding and expansion of polyolefin backing. In the past, the production process of backing was done by extruding, cross-linking and expansion. However, in order to increase the production efficiency, some of manufacturers expand the backing during extruding, and then send the backing to e-beam for the cross-linking process.\n\nAn adhesive is then applied to the sheet and various manufacturers use proprietary techniques depending on the type, viscosity and melting temperature of the adhesive. The adhesive is the key to ultimate performance of the installed system, which is why different adhesive types will be specified depending on the pipeline operating conditions.\n\nThe adhesive has many functions; it adheres the installed sleeve to the steel at the coating cutback and mainline coating, it resists shear forces imparted by soil pressure after the pipeline is buried and provides long term corrosion protection to the steel. The choice of which adhesive to use is based on the pipeline design and operating conditions. As an example, for small diameter flow lines operating at ambient temperatures, a soft mastic-based adhesive may be chosen, while on large diameter pipelines operating at higher temperatures, a hard, semi-crystalline hot-melt adhesive is used. The adhesive needs to be chosen based on its corrosion protection properties, adhesion strength, and resistance to shear forces imparted by pipe movement and the effects of soil pressures.\n\nThe coated sheet is then cut into individual sleeves suitable for application on a pipeline. As mentioned before, the sheet is stretched and wants to recover when heated, so a sealing strip or “closure” is applied during sleeve installation so that the sleeve will stay in place during and after recovery.\n\nA final component is an optional epoxy primer. Primers for heat-shrinkable sleeves work in the same manner as an FBE primer does when it is specified on 3-layer polyolefin pipeline coatings and is typically applied between 150 µm and 300 µm thick. Usually, the primer of heat shrinkable sleeve is two components non-solvent Epoxy, one is primer base and the other is curing agent.\n\nWhen steel pipelines are built, they commonly consist of 10~12m long sections of steel pipe that has had a corrosion protective coating applied to it in a factory. The factory will leave an uncoated area at each end of the pipe called a “cutback” so that when welding the pipe sections together, the coating is not damaged. Heat-shrinkable sleeves are applied onto the cutback at the field weld or “field joint” during the construction of a pipeline.\n\nAs described above, the heat-shrinkable sleeves have an adhesive that sticks the sleeve to the cutback and the factory applied mainline coating and also acts as a corrosion protective layer. The backing provides mechanical protection against abrasion and soil stress forces after the pipeline is buried.\n\nHeat wrap tape may used in addition for pipe bends, or as an alternative method for wrapping the whole pipe.\n\n\n"}
{"id": "11919353", "url": "https://en.wikipedia.org/wiki?curid=11919353", "title": "Hori hori", "text": "Hori hori\n\nA Hori-Hori, sometimes referred to as a \"soil knife\" or a \"weeding knife\", is a heavy serrated multi-purpose steel blade for gardening jobs such as digging or cutting. The blade is sharp on both sides and comes to a semi-sharp point at the end.\n\nThe word “Hori” (ホリ) means \"to dig\" in Japanese and \"hori-hori\" is the onomatopoeia for a digging sound. The tool itself is commonly referred to as a レジャーナイフ, \"leisure knife\" or a 山菜ナイフ, \"Sansai(=Mountain-vegetable)knife\" in Japan.\n\nThe size of the knife varies from eleven to fifteen inches in total length, depending on the size of the handle. The size of the blade can vary, but it is normally around 6” × 1”.\n\nThe stainless steel blade is often polished to a mirror-like finish and is usually paired with a scabbard. \n\nThe blade is razor sharp and is serrated for cutting through roots and tough soil. Functions include a knife, a saw, a digging tool, or as a measuring device for planting bulbs.\n\nThe Hori-Hori has uses in gardening such as weeding, cutting roots, transplanting, removing plants, sod cutting, and splitting perennials. The blade is made of carbon or stainless steel that is concave shaped to make it ideal for digging and prying. The blade has a large smooth wooden handle for comfortable use with one hand. It can serve as a small hand axe.\n\nThe Hori-Hori digging tool, first implemented in Japan, was originally used for carefully excavating plants such as Sansai in the mountains .\n"}
{"id": "9799456", "url": "https://en.wikipedia.org/wiki?curid=9799456", "title": "InvivoGen", "text": "InvivoGen\n\nInvivogen is a manufacturer of life science research products. It is based in San Diego, California and conducts business worldwide. \nInvivoGen is a provider of Toll-like receptor related products (mainly ligands and engineered mammalian cell lines), selection antibiotics and mycoplasma detection & elimination products. To date, about 6500 academic papers cite InvivoGen's products.\nInvivogen also provides a collection of more 1000 open reading frame of human and rodent origins.\n\nInvivogen was founded in 1997. The company is known for its mycoplasma detection and removal agents and its toll-like receptor product line. Although its first products focused on gene therapy, the company now produces tools for innate immunity research, immunology research, cancer research, RNA interference, cell culture, cloning and gene expression.\n\n"}
{"id": "16010970", "url": "https://en.wikipedia.org/wiki?curid=16010970", "title": "Lip liner", "text": "Lip liner\n\nLip liner, also known as a lip pencil, is a cosmetic product. It is intended to fill in uneven areas on the outer edges of the lips before applying lipstick to give a smoother shape. It is also used to outline the lips, keeping lipstick inside the lip area and preventing it from \"bleeding\", creating a bigger contrast and making the lips stand out more. Alternatively, lip liner can be used to fill in the entire lip before the application of lipstick, and in some cases is worn as a lipstick on its own. The product is usually sold in a retractable tube or pencil form which can be sharpened. Lip liner is usually available in the same range of colors as lipsticks: e.g., reds, pinks, browns, plums, etc. Lip liner also comes in invisible, for giving the illusion of smooth lips without adding or affecting color.\n\nLike lipstick, lip liners are composed of waxes, oils, and pigment. Compared to lipstick, lip liners are firmer in consistency and more deeply pigmented, making them suitable for drawing on to the lip with precision. For these reasons, lip liners have less oil but more wax and pigment than most lipsticks. A popular wax used in the making of lip liners is Japan wax.\n\nDesigner brands such as Chanel, Dior, and Marc Jacobs produce their own lip liners. Other brands include but are not limited to Urban Decay, Clinique, and MAC Cosmetics.\n\n"}
{"id": "56160366", "url": "https://en.wikipedia.org/wiki?curid=56160366", "title": "Lisa Winning", "text": "Lisa Winning\n\nLisa Winning is an entrepreneur and digital technologist. She was the Founder and CEO of New York and Silicon Valley based startup, HeTexted. The website and app for crowdsourced advice led to book deals with Penguin Random House , and Simon & Schuster, \n\nAlongside Margaret Zhang, Roxy Jacenko, Tammy Barton, and Marita Cheng, Winning was recognized by Startup Daily as number 15 of the top 50 female entrepreneurs aged under 40 in Australian technology .\n\nHeTexted was described as \"an incredibly addicting formula\", and Winning's unique idea to crowdsource advice for Millennials was covered by ABC News’ Good Morning America , CNN , The Wall Street Journal’s All Things Digital , The Guardian , and Today .\n\nWinning raised venture capital from Silicon Valley investors including Dave McClure of 500 Startups , and is represented by US talent agency William Morris Endeavor, allowing the tech startup to expand into other media categories including books and film. \n\nShe has been interviewed by Good Morning America , the Huffington Post , CNBC , the Evening Standard and recognized by Marie Claire for International Women's Day .\n\nWinning has contributor columns with Forbes writing about technology and startups. She previously contributed to Virgin.\n"}
{"id": "8750146", "url": "https://en.wikipedia.org/wiki?curid=8750146", "title": "List of Chinese inventions", "text": "List of Chinese inventions\n\nChina has been the source of many innovations, scientific discoveries and inventions. This includes the \"Four Great Inventions\": papermaking, the compass, gunpowder, and printing (both woodblock and movable type). The list below contains these and other inventions in China attested by archaeological or historical evidence.\n\nThe historical region now known as China experienced a history involving mechanics, hydraulics and mathematics applied to horology, metallurgy, astronomy, agriculture, engineering, music theory, craftsmanship, naval architecture and warfare. By the Warring States period (403–221 BC), inhabitants of the Warring States had advanced metallurgic technology, including the blast furnace and cupola furnace, while the finery forge and puddling process were known by the Han Dynasty (202 BC–AD 220). A sophisticated economic system in imperial China gave birth to inventions such as paper money during the Song Dynasty (960–1279). The invention of gunpowder during the mid 9th century led to an array of inventions such as the fire lance, land mine, naval mine, hand cannon, exploding cannonballs, multistage rocket and rocket bombs with aerodynamic wings and explosive payloads. With the navigational aid of the 11th century compass and ability to steer at high sea with the 1st century sternpost rudder, premodern Chinese sailors sailed as far as East Africa. In water-powered clockworks, the premodern Chinese had used the escapement mechanism since the 8th century and the endless power-transmitting chain drive in the 11th century. They also made large mechanical puppet theaters driven by waterwheels and carriage wheels and wine-serving automatons driven by paddle wheel boats.\n\nThe contemporaneous Peiligang and Pengtoushan cultures represent the oldest Neolithic cultures of China and were formed around 7000 BC. Some of the first inventions of Neolithic China include semilunar and rectangular stone knives, stone hoes and spades, the cultivation of millet, rice, and the soybean, the refinement of sericulture, the building of rammed earth structures with lime-plastered house floors, the creation of pottery with cord-mat-basket designs, the creation of pottery tripods and pottery steamers and the development of ceremonial vessels and scapulimancy for purposes of divination. The British sinologist Francesca Bray argues that the domestication of the ox and buffalo during the Longshan culture (c. 3000–c. 2000 BC) period, the absence of Longshan-era irrigation or high-yield crops, full evidence of Longshan cultivation of dry-land cereal crops which gave high yields \"only when the soil was carefully cultivated,\" suggest that the plough was known at least by the Longshan culture period and explains the high agricultural production yields which allowed the rise of Chinese civilization during the Shang Dynasty (c. 1600–c. 1050 BC). Later inventions such as the multiple-tube seed drill and heavy moldboard iron plough enabled China to sustain a much larger population through greater improvements in agricultural output.\n\nFor the purposes of this list, inventions are regarded as technological firsts developed in China, and as such does not include foreign technologies which the Chinese acquired through contact, such as the windmill from the Middle East or the telescope from early modern Europe. It also does not include technologies developed elsewhere and later invented separately by the Chinese, such as the odometer, water wheel, and chain pump. Scientific, mathematical or natural discoveries, changes in minor concepts of design or style and artistic innovations do not appear on the list.\n\nThe following is a list of the \"Four Great Inventions\"—as designated by Joseph Needham (1900–1995), a British scientist, author and sinologist known for his research on the history of Chinese science and technology.\n\nAlthough it is recorded that the Han Dynasty (202 BC – AD 220) court eunuch Cai Lun (50 AD – AD 121) invented the pulp papermaking process and established the use of new materials used in making paper, ancient padding and wrapping paper artifacts dating to the 2nd century BC have been found in China, the oldest example of pulp papermaking being a map from Fangmatan, Tianshui; by the 3rd century, paper as a writing medium was in widespread use, replacing traditional but more expensive writing mediums such as strips of bamboo rolled into threaded scrolls, strips of silk, wet clay tablets hardened later in a furnace, and wooden tablets. The earliest known piece of paper with writing on it was discovered in the ruins of a Chinese watchtower at Tsakhortei, Alxa League, where Han Dynasty troops had deserted their position in AD 110 following a Xiongnu attack. In the paper making process established by Cai in 105, a boiled mixture of mulberry tree bark, hemp, old linens and fish nets created a pulp that was pounded into paste and stirred with water; a wooden frame sieve with a mat of sewn reeds was then dunked into the mixture, which was then shaken and then dried into sheets of paper that were bleached under the exposure of sunlight; K.S. Tom says this process was gradually improved through leaching, polishing and glazing to produce a smooth, strong paper.\n\nWoodblock printing: The earliest specimen of woodblock printing is a single-sheet \"dharani\" sutra in Sanskrit that was printed on hemp paper between 650 and 670 AD; it was unearthed in 1974 from a Tang tomb near Xi'an. A Korean miniature \"dharani\" Buddhist sutra discovered in 1966, bearing extinct Chinese writing characters used only during the reign of China's only self-ruling empress, Wu Zetian (r.690–705), is dated no earlier than 704 and preserved in a Silla Korean temple stupa built in 751. The first printed periodical, the Kaiyuan Za Bao was made available in AD 713. However, the earliest known book printed at regular size is the \"Diamond Sutra\" made during the Tang Dynasty (618–907), a 5.18 m (17 ft) long scroll which bears the date 868 AD. Joseph Needham and Tsien Tsuen-hsuin write that the cutting and printing techniques used for the delicate calligraphy of the \"Diamond Sutra\" book are much more advanced and refined than the miniature \"dharani\" sutra printed earlier.\n\nMovable type: The polymath scientist and official Shen Kuo (1031–1095) of the Song Dynasty (960–1279) was the first to describe the process of movable type printing in his \"Dream Pool Essays\" of 1088. He attributed the innovation of reusable fired clay characters to a little-known artisan named Bi Sheng (990–1051). Bi had experimented with wooden type characters, but their use was not perfected until 1297 to 1298 with the model of the official Wang Zhen (fl. 1290–1333) of the Yuan Dynasty (1271–1368), who also arranged written characters by rhyme scheme on the surface of round table compartments. It was not until 1490 with the printed works of Hua Sui (1439–1513) of the Ming Dynasty (1368–1644) that the Chinese perfected metal movable type characters, namely bronze. The Qing Dynasty (1644–1912) scholar Xu Zhiding of Tai'an, Shandong developed vitreous enamel movable type printing in 1718.\n\nEvidence of gunpowder's first use in China comes from the Tang dynasty (618–907). The earliest known recorded recipes for gunpowder were written by Zeng Gongliang, Ding Du and Yang Weide in the \"Wujing Zongyao\", a military manuscript compiled in 1044 during the Song Dynasty (960–1279). Its gunpowder formulas describe the use of incendiary bombs launched from catapults, thrown down from defensive walls, or lowered down the wall by use of iron chains operated by a swape lever. Bombs launched from trebuchet catapults mounted on forecastles of naval ships ensured the victory of Song over Jin forces at the Battle of Caishi in 1161, while the Mongol Yuan Dynasty (1271–1368) during their failed invasion of Japan in 1274 and 1281. During the 13th and 14th centuries, gunpowder formulas became more potent (with nitrate levels of up to 91%) and gunpowder weaponry more advanced and deadly, as evidenced in the Ming Dynasty (1368–1644) military manuscript \"Huolongjing\" compiled by Jiao Yu (fl. 14th to early 15th century) and Liu Bowen (1311–1375). It was completed in 1412, a long while after Liu's death, with a preface added by the Jiao in its Nanyang publication.\n\nAlthough an ancient hematite artifact from the Olmec era in Mexico dating to roughly 1000 BC indicates the possible use of the lodestone compass long before it was described in China, the Olmecs did not have iron which the Chinese would discover could be magnetised by contact with lodestone. Descriptions of lodestone attracting iron were made in the \"Guanzi\", \"Master Lu's Spring and Autumn Annals\" and \"Huainanzi\". The Chinese by the Han Dynasty (202 BC – 220 AD) began using north-south oriented lodestone ladle-and-bowl shaped compasses for divination and geomancy and not yet for navigation. The \"Lunheng\", written by Han dynasty writer, scientist, and philosopher Wang Chong (27 – c. 100 AD) stated in chapter 52: \"This instrument resembles a spoon and when it is placed on a plate on the ground, the handle points to the south\". There are, however, another two references under chapter 47 of the same text to the attractive power of a magnet according to Needham (1986), but Li Shu-hua (1954) considers it to be lodestone, and states that there is no explicit mention of a magnet in \"Lunheng\". The Chinese polymath Shen Kuo (1031–1095) of the Song Dynasty (960–1279) was the first to accurately describe both magnetic declination (in discerning true north) and the magnetic needle compass in his \"Dream Pool Essays\" of 1088, while the Song dynasty writer Zhu Yu (fl. 12th century) was the first to mention use of the compass specifically for navigation at sea in his book published in 1119. Even before this, however, the \"Wujing Zongyao\" military manuscript compiled by 1044 described a thermoremanence compass of heated iron or steel shaped as a fish and placed in a bowl of water which produced a weak magnetic force via remanence and induction; the \"Wujing Zongyao\" recorded that it was used as a pathfinder along with the mechanical south-pointing chariot.\n\nInventions which originated in what is now China during the Neolithic age and prehistoric Bronze Age are listed in alphabetical order below.\n\n\n\n\n\n\n\n\nInventions which made their first appearance in China after the Neolithic age, specifically during and after the Shang Dynasty (\"c\". 1600–1050 BC), are listed in alphabetical order below.\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "38861135", "url": "https://en.wikipedia.org/wiki?curid=38861135", "title": "List of energy storage projects", "text": "List of energy storage projects\n\nThis is a list of energy storage projects worldwide, other than pumped hydro storage. \nMany individual energy storage projects augment electrical grids by capturing excess electrical energy during periods of low demand and storing it in other forms until needed on an electrical grid. The energy is later converted back to its electrical form and returned to the grid as needed.\nMost of the world's grid energy storage by capacity is in the form of pumped-storage hydroelectricity, which is covered in List of pumped-storage hydroelectric power stations. \nThis article list projects using all other forms of energy storage.\n\nAnother energy storage method is the consumption of surplus or low-cost energy (typically during night time) for conversion into resources such as hot water, cool water or ice, which is then used for heating or cooling at other times when electricity is in higher demand and at greater cost per kilowatt hour (kWh). Such thermal energy storage is often employed at end-user sites such as large buildings, and also as part of district heating, thus 'shifting' energy consumption to other times for better balancing of supply and demand.\n\nFor a list of systems and forms of energy storage see energy storage and grid energy storage.\n\n"}
{"id": "2189647", "url": "https://en.wikipedia.org/wiki?curid=2189647", "title": "List of nuclear weapons tests", "text": "List of nuclear weapons tests\n\nNuclear weapons testing is defined in treaty language by specifying a space and time requirement.\n\nThis definition is inclusive of \"zero yield\" safety tests of warheads, whether the test is successful (there is no nuclear yield) or the test is unsuccessful (there is a nuclear yield).\nIt does not include hydronuclear, cold or subcritical tests because no nuclear explosions are possible, even in failure. In these sorts of tests there may be small amounts of chain reaction occurring, but they stop before materially adding to the chemical explosion that causes them. The line here is finely drawn, but, among other things, subcritical testing is not prohibited by the Comprehensive Nuclear Test Ban Treaty, while safety tests are.\n\nThe table in this section summarizes all worldwide nuclear testing (including the two bombs dropped in combat which were not tests). The country names are links to summary articles for each country, which may in turn be used to drill down to test series articles which contain details on every known nuclear explosion and test. The notes attached to various table cells detail how the numbers therein are arrived at. As of 1993, worldwide, 520 atmospheric nuclear explosions (including 8 underwater) have been conducted with a total yield of 545 megaton (Mt): 217 Mt from fission and 328 Mt from fusion, while the estimated number of underground nuclear tests conducted in the period from 1957 to 1992 is 1,352 explosions with a total yield of 90 Mt.\n\nIn the following subsections, a selection of significant tests (by no means exhaustive) are listed, representative of the testing effort in each nuclear country.\n\nThe standard \"official\" list of tests for American devices is arguably the United States Department of Energy DoE-209 document. The United States conducted around 1,054 nuclear tests (by official count) between 1945 and 1992, including 216 atmospheric, underwater, and space tests. Some significant tests conducted by the United States include:\n\n\nAfter the fall of the USSR, the American government (as a member of the International Consortium \"International Science and Technology Center\") hired a number of top scientists in Sarov (aka Arzamas-16, the Soviet equivalent of Los Alamos and thus sometimes called \"Los Arzamas\") to draft a number of documents about the history of the Soviet atomic program. One of the documents was the definitive list of Soviet nuclear tests. Most of the tests have no code names, unlike the American tests, so they are known by their test numbers from this document. Some list compilers have detected discrepancies in that list; one device was abandoned in its cove in a tunnel in Semipalatinsk when the Soviets abandoned Kazakhstan, and one list lists 13 other tests which apparently failed to provide any yield. The source for that was the well respected \"Russian Strategic Nuclear Forces\" which confirms 11 of the 13; those 11 are in the Wikipedia lists.\n\nThe Soviet Union conducted 715 nuclear tests (by the official count) between 1949 and 1990, including 219 atmospheric, underwater, and space tests. Most of them took place at the Semipalatinsk Test Site in Kazakhstan and the Northern Test Site at Novaya Zemlya. Additional industrial tests were conducted at various locations in Russia and Kazakhstan, while a small number of tests were conducted in Ukraine, Uzbekistan, and Turkmenistan.\n\nIn addition, the large-scale military exercise was conducted by Soviet army to explore the possibility of defensive and offensive warfare operations on the nuclear battlefield. The exercise, under code name of \"Snezhok\" (Snowball), involved detonation of a nuclear bomb twice as powerful as the one used in Nagasaki and approximately 45,000 soldiers coming through the epicenter immediately after the blast The exercise was conducted on September 14, 1954, under command of Marshal Georgy Zhukov to the north of Totskoye village in Orenburg Oblast, Russia.\n\nSome significant Soviet tests include:\n\nThe last Soviet test took place on October 24, 1990. After the dissolution of the USSR in 1992, Russia inherited the USSR's nuclear stockpile, while Kazakhstan inherited the Semipalatinsk nuclear test area, as well as the Baikonur Cosmodrome, the Sary Shagan missile/radar test area and three ballistic missile fields. Semipalatinsk included at least the one unexploded device, later blown up with conventional explosives by a combined USA/Kazakh team. No testing has occurred in the former territory of the USSR since its dissolution.\n\nThe United Kingdom has conducted 45 tests (21 in Australian territory, including 9 in mainland South Australia at Maralinga and Emu Field, 3 at Malden Island and 6 at Kiritibati (Christmas Island) in the Line Islands of the central Pacific, and 24 in the U.S. as part of joint test series). Often excluded from British totals are the 31 safety tests of Operation Vixen in Maralinga. British test series include:\n\nLast test: Julin Bristol, November 26, 1991, vertical shaft.\n\nAtmospheric tests involving nuclear material but conventional explosions:\n\nFrance conducted 210 nuclear tests between February 13, 1960 and January 27, 1996. Four were tested at Reggane, Algeria, 13 at In Ekker, Algeria and the rest at Moruroa and Fangataufa Atolls in French Polynesia. Often skipped in lists are the 5 safety tests at Adrar Tikertine in Algeria.\n\nThe foremost list of Chinese tests compiled by the Federation of American Scientists skips over two Chinese tests listed by others. The People's Republic of China conducted 45 tests (23 atmospheric and 22 underground, all conducted at Lop Nur Nuclear Weapons Test Base, in Malan, Xinjiang)\n\nIndia announced it had conducted a test of a single device in 1974 near Pakistan's eastern border under the codename \"Operation Smiling Buddha\". After 24 years, India publicly announced five further nuclear tests on May 11 and May 13, 1998. The official number of Indian nuclear tests is six, conducted under two different code-names and at different times.\n\nPakistan conducted 6 official tests, under 2 different code names, in the final week of May 1998. From 1983 to 1994, around 24 nuclear cold tests were carried out by Pakistan; these remained unannounced and classified until 2000. In May 1998, Pakistan responded publicly by testing 6 nuclear devices.\n\nOn October 9, 2006, North Korea announced they had conducted a nuclear test in North Hamgyong Province on the northeast coast at 10:36 AM (11:30 AEST). There was a 3.58 magnitude earthquake reported in South Korea. There was a 4.2 magnitude tremor detected 240 miles north of P'yongyang. The low estimates on the yield of the test—potentially less than a kiloton in strength—have led to speculation as to whether it was a fizzle (unsuccessful test), or not a genuine \"nuclear\" test at all.\n\nOn May 25, 2009, North Korea announced having conducted a second nuclear test. A tremor, with magnitude reports ranging from 4.7 to 5.3, was detected at Mantapsan, 233 miles northeast of P'yongyang and within a few kilometers of the 2006 test location. While estimates as to yield are still uncertain, with reports ranging from 3 to 20 kilotons, the stronger tremor indicates a significantly larger yield than the 2006 test.\n\nOn 12 February 2013, North Korean state media announced it had conducted an underground nuclear test, its third in seven years. A tremor that exhibited a nuclear bomb signature with an initial magnitude 4.9 (later revised to 5.1) was detected by both Comprehensive Nuclear-Test-Ban Treaty Organization Preparatory Commission (CTBTO) and the United States Geological Survey (USGS). The tremor occurred at 11:57 local time (02:57 UTC) and the USGS said the hypocenter of the event was only one kilometer deep. South Korea's defense ministry said the event reading indicated a blast of six to seven kilotons. However, there are some experts who estimate the yield to be up to 15 kt, since the test site's geology is not well understood. In comparison, the atomic (fission) bombs dropped by the \"Enola Gay\" on Hiroshima (Little Boy, a \"gun-type\" atomic bomb) and on Nagasaki by \"Bockscar\" (Fat Man, an \"implosion-type\" atomic bomb) had blast yields of the equivalents of 13 and 21 kilotons of TNT, respectively.\n\nOn January 5, 2015, North Korean TV news anchors announced that they had successfully tested a \"miniaturized atomic bomb\", about 5 miles from the Punggye-ri nuclear site where a test was conducted in 2013.\n\nOn January 6, 2016, North Korea announced that it conducted a successful test of a hydrogen bomb. The seismic event, at a magnitude of 5.1, occurred 19 kilometers (12 miles) east-northeast of Sungjibaegam.\n\nOn September 9, 2016, North Korea announced another successful nuclear weapon test at the Punggye-ri Test Site. This is the first warhead the state claims to be able to mount to a missile or long range rocket previously tested in June 2016. Estimates for the explosive yield range from 20–30 kt and coincided with a 5.3 magnitude earthquake in the region.\n\nOn September 3, 2017, North Korea successfully detonated its first weapon self-designated as a hydrogen bomb. Initial yield estimates place it at 100 kt. Reports indicate that the test blast caused a magnitude 6.3 earthquake, and possibly resulted in a cave-in at the test site.\n\nThere have been a number of significant alleged/disputed/unacknowledged accounts of countries testing nuclear explosives. Their status is either not certain or entirely disputed by most mainstream experts.\n\n\"Hitlers Bombe\", a book published in German by the historian Rainer Karlsch in 2005, has alleged that there is evidence that Nazi Germany performed some sort of test of a \"nuclear device\" (a hybrid fusion device unlike any modern nuclear weapons), allegedly on 4 March 1945 near the Ohrdruf concentration camp, though the evidence for this has not yet been confirmed, and has been doubted by many historians.\n\nIsrael was reported by a West German army report to have made an underground test in 1963. Historian Taysir Nashif reported a zero yield implosion test in 1966. Scientists from Israel participated in the earliest French nuclear tests before DeGaulle cut off further cooperation.\n\nOn September 9, 2004, it was reported by South Korean media that there had been a large explosion at the Chinese/North Korean border. This explosion left a crater visible by satellite and precipitated a large (2 mile diameter) mushroom cloud. The United States and South Korea quickly downplayed this, explaining it away as a forest fire that had nothing to do with the DPRK's nuclear weapons program.\n\nNorth Korea has conducted six nuclear tests, in 2006, 2009, 2013, twice in 2016, and 2017. The 3 September 2017 test, like their January 2016 test, is claimed to be a hydrogen bomb (but may only be a boosted fission weapon rather than an actual staged Teller–Ulam thermonuclear weapon).\n\nBecause Pakistan's nuclear programme was conducted under extreme secrecy, it raised concerns in the Soviet Union and India, who suspected that since the 1974 test it was inevitable that Pakistan would further develop its programme. The pro-Soviet newspaper, \"The Patriot\", reported that \"Pakistan has exploded a nuclear device in the range of 20 to 50 kilotons\" in 1983. But it was widely dismissed by Western diplomats as it was pointed out that \"The Patriot\" had previously engaged in spreading disinformation on several occasions. In 1983, India and the Soviet Union both investigated secret tests but, due to lack of any scientific data, these statements were widely dismissed.\n\nIn their book, \"The Nuclear Express\", authors Thomas Reed and Danny Stillman also allege that the People's Republic of China allowed Pakistan to detonate a nuclear weapon at its Lop Nur test site in 1990, eight years before Pakistan held its first official weapons test.\n\nHowever, senior scientist Abdul Qadeer Khan strongly rejected the claim in May 1998. According to Khan, due to its sensitivity, no country allows another country to use their test site to explode the devices. Such an agreement only existed between the United States and the United Kingdom since the 1958 US–UK Mutual Defense Agreement which among other things allows Britain access to the American Nevada National Security Site for testing. Dr. Samar Mubarakmand, another senior scientist, also confirmed Dr. Khan's statement and acknowledged that cold tests were carried out, under codename \"Kirana-I\", in a test site which was built by the Corps of Engineers under the guidance of the PAEC.\nAdditionally, the UK conducted nuclear tests in Australia in the 1950s.\n\nThe Yekaterinburg Fireball of November 14, 2014 is alleged by some to have been a nuclear test in space, which would not have been detected by the CTBTO because the CTBTO does not have autonomous ways to monitor space nuclear tests (i.e. satellites) and relies thus on information that member States would accept to provide. The fireball happened a few days before a conference in Yekaterinburg on the theme of air / missile defence. The affirmation, however, is disputed as the Russian Ministry of Emergency Situations claimed it was an \"on-ground\" explosion. \"The Siberian Times\", a local newspaper, noted that \"the light was not accompanied by any sound\".\n\nThe Vela Incident was an unidentified \"double flash\" of light detected by a partly functional, decommissioned American Vela Satellite on September 22, 1979 in the Indian Ocean (near the Prince Edward Islands off Antarctica), other sensors which could have recorded proof of a nuclear test were not functioning on this satellite. It is possible that this was produced by a nuclear device. If this flash detection was actually a nuclear test, a popular theory favored in the diary of then sitting American President Jimmy Carter, is that it resulted from a covert joint South African and Israeli nuclear test of an advanced highly miniaturized Israeli artillery shell sized device which was unintentionally detectable by satellite optical sensor due to a break in the cloud cover of a typhoon. Analysis of the South African nuclear program later showed only six of the most crude and heavy designs weighing well over 340 kg had been built when they finally declared and disarmed their nuclear arsenal. The 1986 Vanunu leaks analyzed by nuclear weapon miniaturization pioneer Ted Taylor revealed very sophisticated miniaturized Israeli designs among the evidence presented. Also suspected were France testing a neutron bomb near their Kerguelen Islands territory, the Soviet Union making a prohibited atmospheric test, as well as India or Pakistan doing initial proof of concept tests of early weaponized nuclear bombs.\n\nMissiles and nuclear warheads have usually been tested separately, because testing them together is considered highly dangerous; they are certainly the most extreme type of live fire exercise. The only US live test of an operational missile was the following:\n\nOther live tests with the nuclear explosive delivered by rocket by the USA include:\n\nThe Soviet Union tested nuclear explosives on rockets as part of their development of a localised anti-ballistic missile system in the 1960s. Some of the Soviet nuclear tests with warheads delivered by rocket include:\n\nThe People's Republic of China conducted CHIC-4 with a Dongfeng-2 rocket launch in October 27, 1966. The warhead exploded with a yield of 12 kt.\n\nThe following list contains all known nuclear tests conducted with a yield of 1.4 Mt TNT equivalent and more.\n\n\n"}
{"id": "12853210", "url": "https://en.wikipedia.org/wiki?curid=12853210", "title": "List of video connectors", "text": "List of video connectors\n\n\"This is a list of physical RF and video connectors and related video signal standards. See also:\" Comparison of display connectors\n\n"}
{"id": "10280304", "url": "https://en.wikipedia.org/wiki?curid=10280304", "title": "Malaria vaccine", "text": "Malaria vaccine\n\nMalaria vaccine is a vaccine that is used to prevent malaria. The only approved vaccine as of 2015 is RTS,S. It requires four injections, and has a relatively low efficacy. Due to this low efficacy, WHO does not recommend the use of RTS,S vaccine in babies between 6 and 12 weeks of age.\n\nThe vaccine is going to be studied further in Africa in 2018. Research continues into recombinant protein and attenuated whole organism vaccines.\n\nRTS,S (developed by PATH Malaria Vaccine Initiative (MVI) and GlaxoSmithKline (GSK) with support from the Bill and Melinda Gates Foundation) is the most recently developed recombinant vaccine. It consists of the \"P. falciparum\" circumsporozoite protein (CSP) from the pre-erythrocytic stage. The CSP antigen causes the production of antibodies capable of preventing the invasion of hepatocytes and additionally elicits a cellular response enabling the destruction of infected hepatocytes. The CSP vaccine presented problems in trials due to its poor immunogenicity. RTS,S attempted to avoid these by fusing the protein with a surface antigen from hepatitis B, hence creating a more potent and immunogenic vaccine. When tested in trials an emulsion of oil in water and the added adjuvants of monophosphoryl A and QS21 (SBAS2), the vaccine gave protective immunity to 7 out of 8 volunteers when challenged with \"P. falciparum.\"\n\nRTS,S/AS01 (commercial name Mosquirix), was engineered using genes from the outer protein of \"P. falciparum\" malaria parasite and a portion of a hepatitis B virus plus a chemical adjuvant to boost the immune response. Infection is prevented by inducing high antibody titers that block the parasite from infecting the liver. The developers are non-profit In November 2012 a Phase III trial of RTS,S found that it provided modest protection against both clinical and severe malaria in young infants.\n\nIn a bid to accommodate a larger group and guarantee a sustained availability for the general public, GSK applied for a marketing license with the European Medicines Agency (EMA) in July 2014. GSK treated the project as a non-profit initiative, with most funding coming from the Gates Foundation, a major contributor to malaria eradication.\n\nOn 24 July 2015, Mosquirix received a positive opinion from the EMA on the proposal of the vaccine to be used to vaccinate children aged 6 weeks to 17 months outside the European Union.\n\nThe task of developing a preventive vaccine for malaria is a complex process. There are a number of considerations to be made concerning what strategy a potential vaccine should adopt.\n\n\"P. falciparum\" has demonstrated the capability, through the development of multiple drug-resistant parasites, for evolutionary change. The \"Plasmodium\" species has a very high rate of replication, much higher than that actually needed to ensure transmission in the parasite’s life cycle. This enables pharmaceutical treatments that are effective at reducing the reproduction rate, but not halting it, to exert a high selection pressure, thus favoring the development of resistance. The process of evolutionary change is one of the key considerations necessary when considering potential vaccine candidates. The development of resistance could cause a significant reduction in efficacy of any potential vaccine thus rendering useless a carefully developed and effective treatment. \n\nThe parasite induces two main response types from the human immune system. These are anti-parasitic immunity and anti-toxic immunity.\n\n\nTaking this information into consideration an ideal vaccine candidate would attempt to generate a more substantial cell-mediated and antibody response on parasite presentation. This would have the benefit of increasing the rate of parasite clearance, thus reducing the experienced symptoms and providing a level of consistent future immunity against the parasite.\n\nBy their very nature, protozoa are more complex organisms than bacteria and viruses, with more complicated structures and life cycles. This presents problems in vaccine development but also increases the number of potential targets for a vaccine. These have been summarised into the life cycle stage and the antibodies that could potentially elicit an immune response.\n\nThe epidemiology of malaria varies enormously across the globe, and has led to the belief that it may be necessary to adopt very different vaccine development strategies to target the different populations. A Type 1 vaccine is suggested for those exposed mostly to \"P. falciparum\" malaria in sub-Saharan Africa, with the primary objective to reduce the number of severe malaria cases and deaths in infants and children exposed to high transmission rates. The Type 2 vaccine could be thought of as a ‘travellers’ vaccine’, aiming to prevent all cases of clinical symptoms in individuals with no previous exposure. This is another major public health problem, with malaria presenting as one of the most substantial threats to travellers’ health. Problems with the current available pharmaceutical therapies include costs, availability, adverse effects and contraindications, inconvenience and compliance, many of which would be reduced or eliminated entirely if an effective (greater than 85–90%) vaccine was developed. \n\nThe life cycle of the malaria parasite is particularly complex, presenting initial developmental problems. Despite the huge number of vaccines available at the current time, there are none that target parasitic infections. The distinct developmental stages involved in the life cycle present numerous opportunities for targeting antigens, thus potentially eliciting an immune response. Theoretically, each developmental stage could have a vaccine developed specifically to target the parasite. Moreover, any vaccine produced would ideally have the ability to be of therapeutic value as well as preventing further transmission and is likely to consist of a combination of antigens from different phases of the parasite’s development. More than 30 of these antigens are currently being researched by teams all over the world in the hope of identifying a combination that can elicit immunity in the inoculated individual. Some of the approaches involve surface expression of the antigen, inhibitory effects of specific antibodies on the life cycle and the protective effects through immunization or passive transfer of antibodies between an immune and a non-immune host. The majority of research into malarial vaccines has focused on the \"Plasmodium falciparum\" strain due to the high mortality caused by the parasite and the ease of a carrying out in vitro/in vivo studies. The earliest vaccines attempted to use the parasitic circumsporozoite (CS) protein. This is the most dominant surface antigen of the initial pre-erythrocytic phase. However, problems were encountered due to low efficacy, reactogenicity and low immunogenicity. \n\n\nIncreasing the potential immunity generated against \"Plasmodia\" can be achieved by attempting to target multiple phases in the life cycle. This is additionally beneficial in reducing the possibility of resistant parasites developing. The use of multiple-parasite antigens can therefore have a synergistic or additive effect.\n\nOne of the most successful vaccine candidates currently in clinical trials consists of recombinant antigenic proteins to the circumsporozoite protein. (This is discussed in more detail below.)\n\nThe selection of an appropriate system is fundamental in all vaccine development, but especially so in the case of malaria. A vaccine targeting several antigens may require delivery to different areas and by different means in order to elicit an effective response. Some adjuvants can direct the vaccine to the specifically targeted cell type—e.g. the use of Hepatitis B virus in the RTS,S vaccine to target infected hepatocytes—but in other cases, particularly when using combined antigenic vaccines, this approach is very complex. Some methods that have been attempted include the use of two vaccines, one directed at generating a blood response and the other a liver-stage response. These two vaccines could then be injected into two different sites, thus enabling the use of a more specific and potentially efficacious delivery system.\n\nTo increase, accelerate or modify the development of an immune response to a vaccine candidate it is often necessary to combine the antigenic substance to be delivered with an adjuvant or specialised delivery system. These terms are often used interchangeably in relation to vaccine development; however in most cases a distinction can be made. An adjuvant is typically thought of as a substance used in combination with the antigen to produce a more substantial and robust immune response than that elicited by the antigen alone. This is achieved through three mechanisms: by affecting the antigen delivery and presentation, by inducing the production of immunomodulatory cytokines, and by affecting the antigen presenting cells (APC). Adjuvants can consist of many different materials, from cell microparticles to other particulated delivery systems (e.g. liposomes).\n\nAdjuvants are crucial in affecting the specificity and isotype of the necessary antibodies. They are thought to be able to potentiate the link between the innate and adaptive immune responses. Due to the diverse nature of substances that can potentially have this effect on the immune system, it is difficult to classify adjuvants into specific groups. In most circumstances they consist of easily identifiable components of micro-organisms that are recognised by the innate immune system cells. The role of delivery systems is primarily to direct the chosen adjuvant and antigen into target cells to attempt to increase the efficacy of the vaccine further, therefore acting synergistically with the adjuvant.\n\nThere is increasing concern that the use of very potent adjuvants could precipitate autoimmune responses, making it imperative that the vaccine is focused on the target cells only. Specific delivery systems can reduce this risk by limiting the potential toxicity and systemic distribution of newly developed adjuvants.\n\nStudies into the efficacy of malaria vaccines developed to date have illustrated that the presence of an adjuvant is key in determining any protection gained against malaria. A large number of natural and synthetic adjuvants have been identified throughout the history of vaccine development. Options identified thus far for use combined with a malaria vaccine include mycobacterial cell walls, liposomes, monophosphoryl lipid A and squalene.\n\nA completely effective vaccine is not yet available for malaria, although several vaccines are under development. SPf66 a synthetic peptide based vaccine developed by Manuel Elkin Patarroyo team in Colombia was tested extensively in endemic areas in the 1990s, but clinical trials showed it to be insufficiently effective, 28% efficacy in South America and minimal or no efficacy in Africa. Other vaccine candidates, targeting the blood-stage of the parasite's life cycle, have also been insufficient on their own. Several potential vaccines targeting the pre-erythrocytic stage are being developed, with RTS,S showing the most promising results so far.,<ref name=\"doi10.1056/NEJMoa1208394\"></ref>\n\nIn 2015, researchers used a repetitive antigen display technology to engineer a nanoparticle that displayed malaria specific B cell and T cell epitopes. The particle exhibited icosahedral symmetry and carried on its surface up to 60 copies of the RTS,S protein. The researchers claimed that the density of the protein was much higher than the 14% of the GSK vaccine.\n\nThe PfSPZ vaccine is a candidate malaria vaccine developed by Sanaria using radiation-attenuated sporozoites to elicit an immune response. Clinical trials have been promising, with trials taking place in Africa, Europe, and the US protecting over 80% of volunteers. It has been subject to some criticism regarding the ultimate feasibility of large-scale production and delivery in Africa, since it must be stored in liquid nitrogen.\n\nThe PfSPZ vaccine candidate has granted fast track designation by the U.S. Food and Drug Administration in September 2016.\n\nIndividuals who are exposed to the parasite in endemic countries develop acquired immunity against disease and death. Such immunity does not however prevent malarial infection; immune individuals often harbour asymptomatic parasites in their blood. This does, however, imply that it is possible to create an immune response that protects against the harmful effects of the parasite.\n\nResearch shows that if immunoglobulin is taken from immune adults, purified and then given to individuals who have no protective immunity, some protection can be gained.\n\nIn 1967, it was reported that a level of immunity to the Plasmodium berghei parasite could be given to mice by exposing them to sporozoites that had been irradiated by x-rays. Subsequent human studies in the 1970s showed that humans could be immunized against Plasmodium vivax and Plasmodium falciparum by exposing them to the bites of significant numbers of irradiated mosquitos.\n\nFrom 1989 to 1999, eleven volunteers recruited from the United States Public Health Service, United States Army, and United States Navy were immunized against Plasmodium falciparum by the bites of 1001 to 2927 mosquitos that had been irradiated with 15,000 rads of gamma rays from a Co-60 or Cs-137 source. This level of radiation being sufficient to attenuate the malaria parasites so that while they could still enter hepatic cells, they could not develop into schizonts or infect red blood cells. Over a span of 42 weeks, 24 of 26 tests on the volunteers showed that they were protected from malaria infection.\n\n\n"}
{"id": "372374", "url": "https://en.wikipedia.org/wiki?curid=372374", "title": "Mandoline", "text": "Mandoline\n\nA mandoline ( or ; ) is a cooking utensil used for slicing and for cutting juliennes; with suitable attachments, it can make crinkle-cuts. Its name is derived from the wrist-motion of a skilled user of a mandolin, which resembles that of a player of the musical instrument mandolin.\n\nA mandoline consists of two parallel working surfaces, one of which can be adjusted in height. A food item is slid along the adjustable surface until it reaches a blade mounted on the fixed surface, slicing it and letting it fall.\n\nOther blades perpendicular to the main blade are often mounted so that the slice is cut into strips. The mandoline juliennes in several widths and thicknesses. It also makes slices, waffle cuts and crinkle cuts, and dices firm vegetables and fruits.\n\nWith a mandoline, slices are uniform in thickness, which is important with foods that are deep-fried or baked (e.g. potato chips), as well as for presentation. Slices can be very thin, and be made very quickly, with significantly less skill and effort than would be required if cutting with a knife or other blade.\n\nA mandoline is used by running a piece of food (with some protection for fingers) along an adjustable inclined plane into one or more blades. On some models vertical blades cut to produce julienne, or a wavy blade is used that produces crinkle cuts. In these models a quarter turn to the food between passes produces dice and waffle cuts.\n\n\n"}
{"id": "766854", "url": "https://en.wikipedia.org/wiki?curid=766854", "title": "Meat carving", "text": "Meat carving\n\nMeat carving is the process and skill of cutting portions of meat, such as roast and poultry, to obtain a maximum or satisfactory number of meat portions, using a carving knife or meat-slicing machine. A meat carver disjoints the meat and slices in uniform portions. Meat carving is sometimes considered a skill for the private dinner table. \n\n"}
{"id": "481601", "url": "https://en.wikipedia.org/wiki?curid=481601", "title": "Mobile network operator", "text": "Mobile network operator\n\nA mobile network operator or MNO, also known as a wireless service provider, wireless carrier, cellular company, or mobile network carrier, is a provider of wireless communications services that owns or controls all the elements necessary to sell and deliver services to an end user including radio spectrum allocation, wireless network infrastructure, back haul infrastructure, billing, customer care, provisioning computer systems and marketing and repair organizations.\n\nIn addition to obtaining revenue by offering retail services under its own brand, an MNO may also sell access to network services at wholesale rates to mobile virtual network operators.\n\nA key defining characteristic of a mobile network operator is that an MNO must own or control access to a radio spectrum license from a regulatory or government entity. A second key defining characteristic of an MNO is that an MNO must own or control the elements of the network infrastructure necessary to provide services to subscribers over the licensed spectrum.\n\nA mobile network operator typically also has the necessary provisioning, billing and customer care computer systems and the marketing, customer care and engineering organizations needed to sell, deliver and bill for services. However, an MNO can outsource any of these systems or functions and still be considered a mobile network operator.\n\n"}
{"id": "849843", "url": "https://en.wikipedia.org/wiki?curid=849843", "title": "Nano-RAM", "text": "Nano-RAM\n\nNano-RAM is a proprietary computer memory technology from the company Nantero. It is a type of nonvolatile random access memory based on the position of carbon nanotubes deposited on a chip-like substrate. In theory, the small size of the nanotubes allows for very high density memories. Nantero also refers to it as NRAM.\n\nThe first generation Nantero NRAM technology was based on a three-terminal semiconductor device where a third terminal is used to switch the memory cell between memory states. The second generation NRAM technology is based on a two-terminal memory cell. The two-terminal cell has advantages such as a smaller cell size, better scalability to sub-20 nm nodes (see semiconductor device fabrication), and the ability to passivate the memory cell during fabrication.\n\nIn a non-woven fabric matrix of carbon nanotubes (CNTs), crossed nanotubes can either be touching or slightly separated depending on their position. When touching, the carbon nanotubes are held together by Van der Waals forces. Each NRAM \"cell\" consists of an interlinked network of CNTs located between two electrodes as illustrated in Figure 1. The CNT fabric is located between two metal electrodes, which is defined and etched by photolithography, and forms the NRAM cell.\n\nThe NRAM acts as a resistive non-volatile random access memory (RAM) and can be placed in two or more resistive modes depending on the resistive state of the CNT fabric. When the CNTs are not in contact the resistance state of the fabric is high and represents an \"off\" or \"0\" state. When the CNTs are brought into contact, the resistance state of the fabric is low and represents an \"on\" or \"1\" state. NRAM acts as a memory because the two resistive states are very stable. In the 0 state, the CNTs (or a portion of them) are not in contact and remain in a separated state due to the stiffness of the CNTs resulting in a high resistance or low current measurement state between the top and bottom electrodes. In the 1 state, the CNTs (or a portion of them) are in contact and remain contacted due to Van der Waals forces between the CNTs, resulting in a low resistance or high current measurement state between the top and bottom electrodes. Note that other sources of resistance such as contact resistance between electrode and CNT can be significant and also need to be considered.\n\nTo switch the NRAM between states, a small voltage greater than the read voltage is applied between top and bottom electrodes. If the NRAM is in the 0 state, the voltage applied will cause an electrostatic attraction between the CNTs close to each other causing a SET operation. After the applied voltage is removed, the CNTs remain in a 1 or low resistance state due to physical adhesion (Van der Waals force) with an activation energy (E) of approximately 5eV. If the NRAM cell is in the 1 state, applying a voltage greater than the read voltage will generate CNT phonon excitations with sufficient energy to separate the CNT junctions. This is the phonon driven RESET operation. The CNTs remain in the OFF or high resistance state due to the high mechanical stiffness (Young's Modulus 1 TPa) with an activation energy (E) much greater than 5 eV. Figure 2 illustrates both states of an individual pair of CNTs involved in the switch operation. Due to the high activation energy (> 5eV) required for switching between states, the NRAM switch resists outside interference like radiation and operating temperature that can erase or flip conventional memories like DRAM.\n\nNRAMs are fabricated by depositing a uniform layer of CNTs onto a prefabricated array of drivers such as transistors as shown in Figure 1. The bottom electrode of the NRAM cell is in contact with the underlying via (electronics) connecting the cell to the driver. The bottom electrode may be fabricated as part of the underlying via or it may be fabricated simultaneously with the NRAM cell, when the cell is photolithographically defined and etched. Before the cell is photolithographically defined and etched, the top electrode is deposited as a metal film onto the CNT layer so that the top metal electrode is patterned and etched during the definition of the NRAM cell. Following the dielectric passivation and fill of the array, the top metal electrode is exposed by etching back the overlying dielectric using a smoothing process such as chemical-mechanical planarization. With the top electrode exposed, the next level of metal wiring interconnect is fabricated to complete the NRAM array. Figure 3 illustrates one circuit method to select a single cell for writing and reading. Using a cross-grid interconnect arrangement, the NRAM and driver, (the cell), forms a memory array similar to other memory arrays. A single cell can be selected by applying the proper voltages to the word line (WL), bit line (BL), and select lines (SL) without disturbing the other cells in the array.\n\nNRAM has a density, at least in theory, similar to that of DRAM. DRAM includes capacitors, which are essentially two small metal plates with a thin insulator between them. NRAM has terminals and electrodes roughly the same size as the plates in a DRAM, the nanotubes between them being so much smaller they add nothing to the overall size. However it seems there is a minimum size at which a DRAM can be built, below which there is simply not enough charge being stored on the plates. NRAM appears to be limited only by lithography. This means that NRAM may be able to become much denser than DRAM, perhaps also less expensive. Unlike DRAM, NRAM does not require power to \"refresh\" it, and will retain its memory even after power is removed. Thus the power needed to write and retain the memory state of the device is much lower than DRAM, which has to build up charge on the cell plates. This means that NRAM might compete with DRAM in terms of cost, but also require less power, and as a result also be much faster because write performance is largely determined by the total charge needed. NRAM can theoretically reach performance similar to SRAM, which is faster than DRAM but much less dense, and thus much more expensive.\n\nCompared with other non-volatile random-access memory (NVRAM) technologies, NRAM has several advantages. In flash memory, the common form of NVRAM, each cell resembles a MOSFET transistor with a control gate (CG) modulated by a floating gate (FG) interposed between the CG and the FG. The FG is surrounded by an insulating dielectric, typically an oxide. Since the FG is electrically isolated by the surrounding dielectric, any electrons placed on the FG will be trapped on the FG which screens the CG from the channel of the transistor and modifies the threshold voltage (VT) of the transistor. By writing and controlling the amount of charge placed on the FG, the FG controls the conduction state of the MOSFET flash device depending on the VT of the cell selected. The current flowing through the MOSFET channel is sensed to determine the state of the cell forming a binary code where a 1 state (current flow) when an appropriate CG voltage is applied and a 0 state (no current flow) when the CG voltage is applied.\n\nAfter being written to, the insulator traps electrons on the FG, locking it into the 0 state. However, in order to change that bit, the insulator has to be \"overcharged\" to erase any charge already stored in it. This requires higher voltage, about 10 volts, much more than a battery can provide. Flash systems include a \"charge pump\" that slowly builds up power and releases it at higher voltage. This process is not only slow, but degrades the insulators. For this reason flash has a limited number of writes before the device will no longer operate effectively.\n\nNRAM reads and writes are both \"low energy\" in comparison to flash (or DRAM for that matter due to \"refresh\"), meaning NRAM could have longer battery life. It may also be much faster to write than either, meaning it may be used to replace both. Modern phones include flash memory for storing phone numbers, DRAM for higher performance working memory because flash is too slow, and some SRAM for even higher performance. Some NRAM could be placed on the CPU to act as the CPU cache, and more in other chips replacing both the DRAM and flash.\n\nNRAM is one of a variety of new memory systems, many of which claim to be \"universal\" in the same fashion as NRAM – replacing everything from flash to DRAM to SRAM.\n\nAn alternative memory ready for use is ferroelectric RAM (FRAM or FeRAM). FeRAM adds a small amount of a ferro-electric material to a DRAM cell. The state of the field in the material encodes the bit in a non-destructive format. FeRAM has advantages of NRAM, although the smallest possible cell size is much larger than for NRAM. FeRAM is used in applications where the limited number of writes of flash is an issue. FeRAM read operations are destructive, requiring a restoring write operation afterwards.\n\nOther more speculative memory systems include magnetoresistive random-access memory (MRAM) and phase-change memory (PRAM). MRAM is based on a grid of magnetic tunnel junctions. MRAM's reads the memory using the tunnel magnetoresistance effect, allowing it to read the memory both non-destructively and with very little power. Early MRAM used field induced writing, reached a limit in terms of size, which kept it much larger than flash devices. However, new MRAM techniques might overcome the size limitation to make MRAM competitive even with flash memory. The techniques are Thermal Assisted Switching (TAS), developed by Crocus Technology, and Spin-transfer torque on which Crocus, Hynix, IBM, and other companies were working in 2009.\n\nPRAM is based on a technology similar to that in a writable CD or DVD, using a phase-change material that changes its magnetic or electrical properties instead of its optical ones. The PRAM material itself is scalable but requires a larger current source.\nNantero was founded in 2001, and headquartered in Woburn, Massachusetts.\nDue to the massive investment in flash semiconductor fabrication plants, no alternative memory has replaced flash in the marketplace, despite predictions as early as 2003 of the impending speed and density of NRAM.\nIn 2005, NRAM was promoted as universal memory, and Nantero predicted it would be in production by the end of 2006.\nIn August 2008, Lockheed Martin acquired an exclusive license for government applications of Nantero's intellectual property.\nBy early 2009, Nantero had 30 US patents and 47 employees, but was still in the engineering phase.\nIn May 2009, a radiation-resistant version of NRAM was tested on the STS-125 mission of the US .\n\nThe company was quiet until another round of funding and collaboration with the Belgian research center imec was announced in November 2012.\nNantero raised a total of over $42 million through the November 2012 series D round.\nInvestors included Charles River Ventures, Draper Fisher Jurvetson, Globespan Capital Partners, Stata Venture Partners and Harris & Harris Group.\nIn May 2013, Nantero completed series D with an investment by Schlumberger.\n\"EE Times\" listed Nantero as one of \"10 top startups to watch in 2013\".\n\n31 Aug 2016. Two Fujitsu semiconductor businesses are licensing Nantero NRAM technology with joint Nantero-Fujitsu development to produce chips in 2018. They will have several thousand times faster rewrites and many thousands of times more rewrite cycles than embedded flash memory.\n\n\n"}
{"id": "33407114", "url": "https://en.wikipedia.org/wiki?curid=33407114", "title": "PaveGen", "text": "PaveGen\n\nPavegen Systems is a technology company that has developed paving slabs to convert energy from people's footsteps into small amounts of electrical power.\n\nPavegen Systems was founded in 2009 by Laurence Kemball-Cook. Cook, a graduate in Industrial Technology and Design from Loughborough University, took on a university placement with E.ON, and proposed using footfall as a potential power source.\n\nThe development of the first prototype of the Pavegen flooring tile was funded by a Royal Society of Arts International Design Directions prize. The tile that converts kinetic energy from footsteps into electricity, while collecting data about walking traffic patterns.\n\nThe first generation tile was made from recycled polymer, with the top surface made from recycled truck tires. Power is generated when a footfall compresses the slab by about . The exact technology is a secret, but PaveGen officials have said it involves electromagnetic induction by copper coils and magnets. Pavegen says each pedestrian generates an average of 5 watts per footstep at 12-48 volts DC, enough to run an LED street lamp for 30 seconds. The technology was developed by Pavegen founder Laurence Kemball-Cook.\n\nAn improved tile was developed in 2016 which, according to the company, improved energy conversion by 'about 20 times'. The amount of energy generated has been criticised, with one calculation claiming that walking for 4 hours on PaveGen paving would generate 0.02% of the average European's energy needs. It has been suggested that the technology's strength rests in its ability to track volume and direction of traffic flow, thus providing useful metrics in a range of scenarios.\n\nAmong other installations, the slabs have been laid at London's West Ham Underground station for the 2012 Olympic Games. In April 2013, a demonstration installation with Schneider Electric harvested energy from the runners in the Paris Marathon.\nPaveGen has also put these tiles on a public soccer field in Rio de Janeiro to allow play after sunset.\n\nA study of a central building at Macquarie University in Sydney, Australia, suggested that if pavers covered the 3.1% of the floor that sees the most foot traffic, it would generate an estimated 1.1 megawatt-hour per year, about 0.5% of the building's energy needs.\n\nIn 2012, Pavegen raised £350,000 through London Business Angels, which helped the company create a tangible business. In 2015, the company raised £1.9m through the Crowdcube platform, allowing them to gain 1500 investors and valued the company at about £17m. \n\nIn 2015, Kemball-Cook acts as CEO of the company, For his invention, he was chosen as Businessman of the Year at the PEA Awards, and presented with a Shell LiveWIREGrand Ideas Award. He also was named as honorary Enterprise and Innovation Fellow by Loughborough University.\n\nPaveGen have distributors in Australia, New Zealand, Korea, Singapore, Thailand, Portugal, India and Japan.\n\nThe Register points out that this device generates only \"tiny, pointless amounts of energy\". \n. This borne out by an article published by the Institution of Mechanical Engineers in which the output from 54,267 steps on a Pavegen system is shown to have generated 217,028 watt-seconds, a mere 0.06 kWh.\n\n\n"}
{"id": "4691922", "url": "https://en.wikipedia.org/wiki?curid=4691922", "title": "Phased adoption", "text": "Phased adoption\n\nPhased adoption is a strategy of implementing an innovation (i.e., information systems, new technologies, processes, etc.) in an organization in a phased way, so that different parts of the organization are implemented in different subsequent time slots. Other concepts that are used are: phased implementation, phased conversion, phased approach, phased strategy, phased introduction and staged conversion.\n\nInformation Technology has revolutionized the way of working in organizations (Eason, 1988). With the introduction of high-tech Enterprise Resource Planning Systems (ERP), Content Management Systems (CMS), Customer and Supplier Relationship Management Systems (CRM and SRM), came the task to implement these systems in the organizations that are about to use it. The following entry will discuss just a small fraction of what has to be done or can be done when implementing such a system in the organization.\n\nThe phased approach takes the conversion one step at a time. The implementation requires a thoroughly thought out scenario for starting to use the new system. And at every milestone one has to instruct the employees and other users. The old system is taken over by the new system in predefined steps until it is totally abounded. The actual installation of the new system will be done in several ways, per module or per product and several instances can be carried out. This may be done by introducing some of the functionalities of the system before the rest or by introducing some functionalities to certain users before introducing them to all the users. This gives the users the time to cope with the changes caused by the system.\n\nIt is common to organize an implementation team that moves from department to department. By moving, the team learns and so gains expertise and knowledge, so that each subsequent implementation will be a lot faster than the first one.\n\nThe visualising technique used in this entry is a technique developed by the O&I group of the University of Utrecht (Weerd, 2005). The technique is described in the following Wiki: Meta-modeling technique.\n\nAs can be seen in figure 1, phased adoption has a loop in it. Every department that is to be connected to the system is going through the same process. First based on the previous training sessions security levels are set (see ITIL) In this way every unique user has its own profile which describes, which parts of the system are visible and/or usable to that specific user. Then the document and policies are documented. All processes and procedures are described in process descriptions, can be in paper or on the intranet. Then the actual conversion is depicted. As described in the above text, certain departments and or parts of an organization may be implemented in different time slots. In figure 1 that is depicted by implementing an additional module or even a total product. HRM needs different modules of an ERP system than Finance (module) or Finance may need an additional accounting software package (Product). Tuning of the system occurs to solve existing problems. After the certain department has been conversed the loop starts over, and another department or user group may be conversed. If all of the departments or organization parts are conversed and the system is totally implemented the system is officially delivered to the organization and the implementation team may be dissolved.\n\nPhased adoption makes it possible to introduce modules that are ready whilst programming the other future modules. This does make the implementation scenario more critical, since certain modules depend on one another. Project Management techniques can be adopted to tackle these problems. See the techniques section below.\n\nHowever, the actual adoption of the system by the users can be more problematic. The system may work just fine but if it is not used it’s worthless. Users base their attitude towards the system on their first experience (Eason, 1988). As this creates an extra weight on the first interaction, the implementers should be concerned with making the first interaction especially a pleasant one.\n\nIn the technique used in this entry each CONCEPT requires a proper definition which is preferably copied from a standard glossary of which the source is given, if applicable. All CONCEPT names in the text are with capital characters. In Table 1 the concept definition list is presented.\n\nTable 1: Concept Diagram\n\nThe Phased adoption method has certain pros, cons and risks (Koop, R., Rooimans, R. & Theye, M. de (2003), Eason (1988))\n\nPros:\n\n\nCons:\n\n\nRisks:\n\n\n\"The following sections are supplemental to the entry about adoption (software implementation) and are specific to phased adoption:\"\n\nThe configuration and specification of the hardware in place used by the legacy system and to run the new system is delivered in the hardware specifications. The hardware configuration is tested to assure proper functioning. This is reported in the hardware configuration report.\nThe configuration and specification of the software in place, i.e., the legacy system and the future new system is made clear to assure proper functioning once the system is installed. The act of specifying the system already installed is key to the implementation. Which parts or even total systems will be taken over by the new system? All this is reported in the software installation and software test reports.\nThe actual installation of the software of the new system is also done here in a confined area to support the training sessions described in the following section.\n\nThe system training will teach users the keystrokes and transactions required to run the system (Umble, 2003) . The pilot exercises the systems and tests the users understanding of the system. The project team creates a skeletal business case test environment which takes the business processes from the beginning, when a customer order is received, to the end, when the customer order is shipped.\nTraining as such is not enough for adopting an information system. The users have learning needs (Eason, 1988). Known learning needs are the emotional guidance. Users need to make emotional steps in order to make cognitive\nsteps. If they fear the system due to its difficult handling they may not be able to understand the cognitive steps needed to successfully carry out the tasks.\n\nIn the implementation field several techniques are used. A well-known method, and specifically oriented on the implementation field, is the Regatta method by Sogeti. Other techniques are the SAP Implementation method, which is adapted to implementing SAP systems. Systems are installed in several different ways. Different organizations may have their own methods, When implementing a system, it is considered a project and thus must be handled as such. Well known theories and methods are used in the field such as the PRINCE2 method with all of its underlying techniques, such as a PERT diagram, Gantt chart and critical path methods.\n\nThe EMR implementation at the University Physicians Group (UPG) in Staten Island and Brooklyn, New York.\n\nThe University Physicians Group in New York went with a complete technical installation of an EMR (Electronic Medical Record) software package. The UPG found that some vendors of the EMR package recommended a rolling out that would be done all-at-one, also called the Big Bang. But they found out that the Big Bang would have overwhelmed the physicians and staff due to the following factors:\n\n\nThus they chose a phased approach: “\"Hence, a phased adoption to us, offered the greatest chance of success, staff adoption, and opportunity for the expected return-on-investment once the system was completely adopted.\"” (J. Hyman, M.D.)\n\nThere also was a group who were somewhat reluctant about any new systems. By introducing the system to certain early adopters (Rogers, 1995) the late majority would be able to get to know the system. As it was introduced phased through the organisation. Per loop (see figure 5, A) the UPG was introduced to the system.\n\n\n"}
{"id": "3973669", "url": "https://en.wikipedia.org/wiki?curid=3973669", "title": "Plastic bending", "text": "Plastic bending\n\nPlastic bending is a nonlinear behavior particular to members made of ductile materials that frequently achieve\n\nmuch greater ultimate bending strength than indicated by a linear elastic bending analysis. In both the plastic and\nelastic bending analyses of a straight beam, it is assumed that the strain distribution is linear about the neutral\naxis (plane sections remain plane). In an elastic analysis this assumption leads to a linear stress distribution but\nin a plastic analysis the resulting stress distribution is nonlinear and is dependent on the beam’s material.\n\nThe limiting plastic bending strength formula_1 (see Plastic moment) can generally be thought of as an upper limit to a beam’s load–carrying capability as it only represents the strength at a particular cross–section and not the load–carrying capability of the overall beam. A beam may fail due to global or local instability before formula_1 is reached at any point on its length. Therefore, beams should also be checked for local buckling, local crippling, and global lateral–torsional buckling modes of failure.\n\nNote that the deflections necessary to develop the stresses indicated in a plastic analysis are generally excessive, frequently to the point of incompatibility with the function of the structure. Therefore, separate analysis may be required to ensure design deflection limits are not exceeded. Also, since working materials into the plastic range can lead to permanent deformation of the structure, additional analyses may be required at limit load to ensure no detrimental permanent deformations occur. The large deflections and stiffness changes usually associated with plastic bending can significantly change the internal load distribution, particularly in statically indeterminate beams. The internal load distribution associated with the deformed shape and stiffness should be used for calculations.\n\nPlastic bending begins when an applied moment causes the outside fibers of a cross-section to exceed the material's yield strength. Loaded only by a moment, the peak bending stresses occurs at the outside fibers of a cross-section. The cross-section will not yield linearly through the section. Rather, outside regions will yield first, redistributing stress and delaying failure beyond what would be predicted by elastic analytical methods. The stress distribution from the neutral axis is the same as the shape of the stress-strain curve of the material (this assumes a non-composite cross-section). After a cross-section reaches a sufficiently high condition of plastic bending, it acts as a Plastic hinge.\n\nElementary Elastic Bending theory requires that the bending stress varies linearly with distance from the neutral axis, but plastic bending shows a more accurate and complex stress distribution. The yielded areas of the cross-section will vary somewhere between the yield and ultimate strength of the material. In the elastic region of the cross-section, the stress distribution varies linearly from the neutral axis to the beginning of the yielded area. Predicted failure occurs when the stress distribution approximates the material's stress-strain curve. The largest value being that of the ultimate strength. Not every area of the cross-section will have exceeded the yield strength. \n\nAs in the basic Elastic Bending theory, the moment at any section is equal to an area integral of bending stress across the cross-section. From this and the above additional assumptions, predictions of deflections and failure strength are made.\n\nPlastic theory was validated at the beginning of the last century by C. v. Bach.\n\nStrength of materials\n\nBending\n\nPlastic moment\n\nPlastic hinge\n\nStephen Timoshenko\n"}
{"id": "3181603", "url": "https://en.wikipedia.org/wiki?curid=3181603", "title": "Production tubing", "text": "Production tubing\n\nProduction tubing is a tube used in a wellbore through which production fluids are produced (travel).\n\nProduction tubing is run into the drilled well after the casing is run and cemented in place. Production tubing protects wellbore casing from wear, tear, corrosion, and deposition of by-products, such as sand / silt, paraffins, and asphaltenes. Along with other components that constitute the production string, it provides a continuous bore from the production zone to the wellhead through which oil and gas can be produced. It is usually between five and ten centimeters in diameter and is held inside the casing through the use of expandable packing devices. Purpose and design of production tubing is to enable quick, efficient, and safe installation, removal and re-installation. \n\nIf there is more than one zone of production in the well, up to four lines of production tubing can be run.\n\n\n\n"}
{"id": "1510665", "url": "https://en.wikipedia.org/wiki?curid=1510665", "title": "Revox", "text": "Revox\n\nReVox is a brand name, registered by Studer on March 27, 1951, for Swiss audio equipment.\n\nThe first Studer-designed tape recorders were branded Dynavox. After the first production series of Dynavox recorders, a new marketing company was formed in 1950 called ELA AG. Revox was adopted as the brand name for amateur recorders, while the professional machines retained the Studer name.\n\nThe first Revox-branded tape recorder was the T26, in 1952, successor to the Dynavox 100.\n\nThe T26 was also made available as a radio-recorder combination unit. 2500 T26 recorders were made, priced at 1395.00 Swiss francs.\n\nThe A36, the first 36 series recorder. became available in 1954. Unusual features for the time were pushbutton solenoid transport operations and a direct-drive capstan with no belts or idler wheels.\nThe B36 of 1956 was the first 3-head model, the D36 of 1960 was the first stereo model.\n\nThe company moved to Löffingen, West Germany, in 1966, due to labour issues in Switzerland. But the building of a second factory was started in Regensdorf, Switzerland in 1967 - to open in 1968.\n\nIn 1967 the 36-series tape recorders ended with the G36, and were superseded by the transistorised A77 with a servo-controlled direct drive capstan. Over 80,000 36-series recorders had been manufactured. The A77 was complemented by an integrated amplifier (A78) and FM tuner (A76). The start of the 1980s saw the introduction of the \"B7xx\" series of high fidelity components, which was in turn replaced by the B2xx series in the mid 1980s. The B2xx series was versatile (the B250 amplifier offering 10 signal inputs, each with automatic sensitivity calibration) and feature-rich, all components containing microprocessors. Contrastingly, the \"H\" line, with \"H\" standing for Human, went to the opposite extreme, with minimalist control interfaces: several \"H\" components having only three buttons.\n\nIn 1990 Willi Studer sold the Studer Revox Group to Motor-Columbus AG, including all subsidiary companies. In 1991, Motor-Columbus split the Studer Revox Group into Studer (Pro), Revox (HiFi) and a Manufacturing-division. Motor-Columbus sold several subsidiaries and plants.\n\nThe extensive reorganization culminated in the sale of the Studer Group to Harman International Inc., in March 1994. The Revox Group was excluded and sold to private investors. On March 17, 1994, Harman International Industries completed its acquisition and acquired from Motor-Columbus AG 100% of Studer Revox AG. Harman paid 100 Swiss Francs (approximately US $70.00) for all of the issued and outstanding stock in Studer Revox. Harman assumed post-acquisition indebtedness of Studer Revox of approximately 23 million Swiss Francs (approximately US $16 million).\n\nThe founder of Revox, Willi Studer, died on March 1, 1996.\n\nThe company logo is the word \"REVOX\" in capitals, with a \"V\" larger than the other letters; when printed in fixed-size type, the mixed-case spelling \"ReVox\" is sometimes used, but in all older and most newer references the name is shown as \"Revox\".\nMany consider ReVox open-reel tape recorders to be high-end audio equipment. The most famous of these are the G36 (valve type), A77 (solid state with relay controls) and B77 (solid state with logic control and direct-drive). The A700 was the top of the line 1970s machine with 3-speed quartz PLL capstan, or, like the B77 which appeared in 1979, variable tape speed from 2.5 to 22.5 inches per second. Pictured to the right is the PR99 Mk2, a variant of the B77 for professional users, differing from the B77 in having balanced line in/out, and a real-time counter and auto-locator. The PR99 series was superseded by the C270 series, available also in multi-track formats (C274, C278).\n\nRevox also produced well-regarded cassette tape recorders, notably the B710 and later the B215, which used the same transport and substantially the same electronics as the more expensive Studer A710 and Studer A721, respectively. It is worth noting that the Revox H11 and C115 cassette recorders are not true Revox or Studer products, but rebadged Philips Model FC60. As such, they do not meet the exacting performance standards of the B710, B215 and H1 which are Studer designed and built.\n\nIn the same fashion, the Revox B225 and B226 compact disc players were very heavily based on the Studer A725 and A727 professional CD players.\nLikewise, the Revox FM tuners were almost identical to the Studer professional models. A limited run of the B226 CD player was issued to celebrate Studer's 40th anniversary. Dubbed 'The Signature', it featured a black faceplate (in contrast to the then silver (anodized aluminium) B226) and used the high performance Silver Crown version of the Philips TDA1541 DAC, which later featured in non-Signature B226-S versions.\n\nRevox was one of the first manufacturers to market a plasma television set, the model E542 in 1999. Before that, they commissioned Loewe to manufacture a limited number of models of S-VHS and VHS video cassette recorders and CRT television sets branded with the Revox name. A DVD player bearing the Revox name (model S27) was built by the French company Micromega.\n\nIn 2002, Revox introduced with the Re:system M51 a modular audio video system in the market. This system can provide up to 32 rooms in 4 different listening zones with music. Different modules allow the user to select which sources he would like to use in his home, available sources (modules) for the M51 and M10 are amongst others FM-tuner, SAT-radio, Internet-radio, Multiroom, Audio server and also Audio streams. Solutions are available for all imaginable application possibilities up to the control via Apple products with the corresponding Apps. The operation of Multiroom require the availability of the corresponding Multiroom module and small amplifiers of the type M219 in the remote rooms. Also the first audio server of the company, the Revox M57, was perfectly in line with the Multiroom concept.\n\nThe M51 MKII is available with two different amplifiers. The analogue version performs 5 x 60 Watt sinus, the digital Class-D-amplifier 5 x 200 Watt sinus.\n\nThe M-series was extended in 2006 by the M10, a 19“ rack solution as central nerve and source centre and in 2010 by the M100, an audiophile Hifi-stereo amplifier with 2 x 200 Watt sinus power, FM-tuner and DVD-drive in an extremely compact housing. For the models M10 and M51, the latter is currently available in version MKII, the insertable modules are identical. For the M100 the modules, except for Multiroom, are offered as pluggable supplementations to the basic model.\n\nIn the Multiroom operation the control of the rooms is effected by wall control units of the type M217/M218, by the remote controls M207 or M208 or by Apps, which are momentarily only available for the iOS-operating system.\n\nIn addition to the pure Audio server M37 MKII the Multimedia server M117 was introduced in 2011. In 2012 Revox presented the new audiophile network receivers Revox Joy S120, S119 and S118.\n\nThe workmanship of the devices is typically for Revox on a very high level and is characterised by the selection of precious materials like aluminium and glass.\n\nAs of May 2016, Revox announced the re-introduction of a reel-to-reel tape player, to be released in the first quarter of 2017. Manufactured and developed in conjunction with HORCH HOUSE (a division of Lutz Precision k.s.), it will initially be a playback-only machine, with a recorder/reproducer to follow.\n\nFor more than 40 years, Revox has been producing loudspeakers. The first speaker of the brand Revox was the series 46 (1970), which was developed for the former HiFi system A77, A76 and A50. The AX- (1976), the BX- (1977) and the BR-series (1980) followed. All speakers were developed and manufactured in the Revox plant in Ewattingen / Black Forest.\n\nNumerous other speakers went along with the company in the 1980s. The most famous speakers were the Symbol B (1983) as well as the first Revox active speaker Agora B (1986). A milestone in the speaker development was the two digital active speakers Scala 4.7 (1994) and Scala 3.6 (1996).\n1980 also saw the introduction of the Triton, one of the first sub-woofer / satellite systems for home high fidelity. The Triton subwoofer comprised a 200 lb cabinet designed to also act as a piece of furniture on which the entire B7xx series (including B77) could stand. The Triton system was superseded by the smaller and more compact Piccolo, and later the Power Cube, which was a Piccolo bass unit with built-in power amplifiers and optional remote control and multi-room controller.\n\nIn 1987, Revox acquired the rights to manufacture the Stereolith Duetto, a single-box stereo loudspeaker. Built in the shape of a triangular prism, the Duetto was designed to complement, in acoustics and appearance, the Piccolo bass unit (passive operation) or the Power Cube (active operation).\n\nToday’s Revox range includes a very comprehensive speaker assortment. Classical three-way speakers such as the Prestige of the Re:sound G or Re:sound S series, medium and small speakers as well as a very comprehensive range of installation speakers, which come into operation especially in the multi-room field.\n\nIn \"Chaplin (film)\" (1992), a Revox machine is being used in Charles Chaplin's sessions with his biographer.\n\nThe film Diva (1981) features Revox B700 series components as playback equipment for the bootleg operatic recording of the diva.\n\nDerren Brown, in his TV Show Trick of the mind, convinces Simon Pegg that his ideal birthday present would be a BMX bike, by having a Revox B77 tape recorder playing in the room, and suggesting that the visual image of such a machine subliminally suggests a bicycle. Link to Video Clip\n\nIn Deutschland 83 (2015 TV series), Revox PR99 reel to reel recorders (among other brands) are used by East German authorities to covertly record Nato operations.\n\nRobert Fripp used Revox machines to produce his Frippertronics recordings in the late 1970s onward, until he replaced the recorders with digital delays in the 1990s.\n\n"}
{"id": "2572921", "url": "https://en.wikipedia.org/wiki?curid=2572921", "title": "Rishab Aiyer Ghosh", "text": "Rishab Aiyer Ghosh\n\nRishab Aiyer Ghosh (born 1975) is a journalist, computer scientist and Open-source software advocate. A former Open Source Initiative board member, he is Founding International and Managing Editor of peer-reviewed journal First Monday, and Programme Leader of Free/Libre and Open Source Software at UNU-MERIT. Ghosh has undertaken several studies on free software, which he terms \"FLOSS\" - an alternative term for free software which he is credited with coining. FLOSS emphasizes the essential value of the term \"libre\" (meaning with few or no restrictions). Ghosh's work represents an effort to reshape the global understanding of FLOSS, including the governmental and academic spheres.\n\nGhosh was a founder of Topsy Labs, a social search and analytics company that was acquired by Apple Inc in December 2013. \n\n"}
{"id": "46225889", "url": "https://en.wikipedia.org/wiki?curid=46225889", "title": "Robert Duggan (venture capitalist)", "text": "Robert Duggan (venture capitalist)\n\nRobert W. Duggan (born 1944/1945) is an American billionaire entrepreneur and investor. He is the former CEO of biopharmaceutical company Pharmacyclics and was previously CEO of surgical systems maker Computer Motion from 1997 to 2003.\n\nDuggan attended, but did not graduate from, the University of California, Santa Barbara and University of California, Los Angeles.\n\nDuggan received $3.5 billion from the sale of Pharmacyclics to AbbVie in \"one of the biggest paydays ever from the buyout of a publicly held company.\"\n\nAs CEO and chairman of Pharmacyclics from 2008 to 2015, Duggan had opted not to receive compensation from the company. As of February 2017, he owned a 15.4% stake in the pharmaceutical company Pulse Biosciences Inc.\n\nDuggan is married, with eight children, and lives in Clearwater, Florida. He met his future wife, Patricia J. \"Trish\" Hagerty, when both were students at UCSB. \n\nDuggan and his wife are members of the Church of Scientology. Duggan has been referred to as the church's \"largest financial supporter\" by former Scientology executive Mark Rathbun.\n"}
{"id": "989359", "url": "https://en.wikipedia.org/wiki?curid=989359", "title": "Sewing needle", "text": "Sewing needle\n\nA sewing needle ,for hand-sewing, is a long slender tool with a pointed tip at one end and a hole (or \"eye\") at the other. The earliest needles were made of bone or wood; modern ones are manufactured from high carbon steel wire and are nickel- or 18K gold-plated for corrosion resistance. The highest quality embroidery needles are plated with two-thirds platinum and one-third titanium alloy. Traditionally, needles have been kept in needle books or needlecases which have become objects of adornment. Sewing needles may also be kept in an étui, a small box that held needles and other items such as scissors, pencils and tweezers.\n\nHand sewing needles come in a variety of types/ classes designed according to their intended use and in a variety of sizes within each type.\n\n\nNeedle size is denoted by one or more numbers on the manufacturer's packet. The general convention for sizing of needles, like that of wire gauges, is that within any given class of needle the length and thickness of a needle \"increases\" as the size number \"decreases\". For example, a size 9 needle will be thicker and longer than a size 12 needle. However, the needle sizes are not standardized and so a size 10 of one class may be (and in some cases actually is) either thinner or finer than a size 12 of another type. Where a packet contains a needle count followed by two size numbers such as \"20 Sharps 5/10\" the second set of numbers correspond to the range of sizes of needle within the packet, in this case typically ten sharps needles of size 5 and ten of size 10 (for a total of 20 needles). As another example, a packet labeled \"16 Milliners 3/9\" would contain 16 milliners needles ranging in sizes from 3 to 9.\n\nThe first form of sewing was probably tying together animal skins using thorns and sharpened rocks as needles, with animal sinew or plant material as thread. The early limitation was the ability to produce a small enough hole in a needle matrix, such as a bone sliver, not to damage the material. Traces of this survive in the use of bodkins to make eyelet holes in fabric by separating rather than cutting the threads. A point that might be from a bone needle dates to 61,000 years ago and was discovered in Sibudu Cave, South Africa. A needle made from bird bone and attributed to archaic humans, the Denisovans, is estimated to be around 50,000 years-old, was found in Denisova Cave. A bone needle, dated to the Aurignacian age (47000 to 41,000 years ago), was discovered in Potok Cave () in the Eastern Karavanke, Slovenia. Bone and ivory needles found in the Xiaogushan prehistoric site in Liaoning province date between 30,000 and 23,000 years old. Ivory needles were also found dated to 30,000 years ago at the Kostenki site in Russia. Flinders Petrie found copper needles at Naqada, ranging from 4400 BC to 3000 BC. Iron sewing needles were found at the Oppidum of Manching, dating to the third century BC.\n\nA form of needle lace named nålebinding seems to generally predate knitting and crochet by thousands of years, partly because it can use far shorter rough-graded threads than knitting does.\n\nNative Americans were known to use sewing needles from natural sources. One such source, the agave plant, provided both the needle and the \"thread.\" The agave leaf would be soaked for an extended period of time, leaving a pulp, long, stringy fibres and a sharp tip connecting the ends of the fibres. The \"needle\" is essentially what was the tip end of the leaf. Once the fibres dried, the fibres and \"needle\" could then be used to sew together for deer bones.\n\nSewing needles are an application of wire-making technology, which started to appear in the second millennium B.C. Some fine examples of Bronze Age gold torques are made of very consistent gold wire, which is more malleable than bronze. However, copper and bronze needles do not need to be as long: the eye can be made by turning the wire back on itself and redrawing it through the die.\n\nThe next major break-through in needle-making was the arrival of high-quality steel-making technology from China in the tenth century, principally in Spain in the form of the Catalan furnace, which soon extended to produce reasonably high quality steel in significant volumes. This technology later extended to Germany and France, although not significantly in England. England began creating needles in 1639 at Redditch, creating the drawn-wire technique still in common use today. About 1655, needle manufacturers were sufficiently independent to establish a Guild of Needlemakers in London, although Redditch remained the principal place of manufacture. In Japan, Hari-Kuyo, the Festival of Broken Needles, dates back to the 1600s.\n\n"}
{"id": "28230973", "url": "https://en.wikipedia.org/wiki?curid=28230973", "title": "Smart TV", "text": "Smart TV\n\nA smart TV is a traditional television set with integrated Internet and interactive \"Web 2.0\" features which allows you to stream music and videos, browse the internet, and view photos. Smart TV is a technological convergence between computers and flatscreen television sets and set-top boxes. Besides the traditional functions of television sets and set-top boxes provided through traditional broadcasting media, these devices can also provide Internet TV, online interactive media, over-the-top content (OTT), as well as on-demand streaming media, and home networking access.\n\nSmart TV should not be confused with Internet TV, IPTV or Web television. Internet TV refers to receiving television content over the Internet instead of traditional systems (terrestrial, cable and satellite) (although Internet itself is received by these methods). IPTV is one of the Internet television technology standards for use by television broadcasters. Web television is a term used for programs created by a wide variety of companies and individuals for broadcast on Internet TV.\n\nIn smart TVs, the operating system is preloaded or is available through the set-top box. The software applications or \"apps\" can be preloaded into the device, or updated or installed on demand via an app store or marketplace, in a similar manner to how the apps are integrated in modern smartphones.\n\nThe technology that enables smart TVs is also incorporated in external devices such as set-top boxes and some Blu-ray players, game consoles, digital media players, hotel television systems, smartphones, and other network-connected interactive devices that utilize television-type display outputs. These devices allow viewers to find and play videos, movies, TV shows, photos and other content from the Web, cable or satellite TV channel, or from a local storage device.\n\nA \"smart TV\" device is either a television set with integrated Internet capabilities or a set-top box for television that offers more advanced computing ability and connectivity than a contemporary basic television set. Smart TVs may be thought of as an information appliance or the computer system from a handheld computer integrated within a television set unit, as such a smart TV often allows the user to install and run more advanced applications or plugins/addons based on a specific platform. Smart TVs run a complete operating system or mobile operating system software providing a platform for application developers.\n\nSmart TV platforms or middleware have a public Software development kit (SDK) and/or Native development kit (NDK) for apps so that third-party developers can develop applications for it, and an app store so that the end-users can install and uninstall apps themselves. The public SDK enables third-party companies and other interactive application developers to “write” applications once and see them run successfully on any device that supports the smart TV platform or middleware architecture which it was written for, no matter who the hardware manufacturer is.\n\nSmart TVs deliver content (such as photos, movies and music) from other computers or network attached storage devices on a network using either a Digital Living Network Alliance / Universal Plug and Play media server or similar service program like Windows Media Player or Network-attached storage (NAS), or via iTunes. It also provides access to Internet-based services including traditional broadcast TV channels, catch-up services, video-on-demand (VOD), electronic program guide, interactive advertising, personalisation, voting, games, social networking, and other multimedia applications. Smart TV enables access to movies, shows, video games, apps and more. Some of those apps include Netflix, Spotify, YouTube, and Amazon.\n\nIn the early 1980s, \"intelligent\" television receivers were introduced in Japan. The addition of an LSI chip with memory and a character generator to a television receiver enabled Japanese viewers to receive a mix of programming and information transmitted over spare lines of the broadcast television signal. A patent was published in 1994 (and extended the following year) for an \"intelligent\" television system, linked with data processing systems, by means of a digital or analog network. Apart from being linked to data networks, one key point is its ability to automatically download necessary software routines, according to a user's demand, and process their needs. \n\nThe mass acceptance of digital television in late 2000s and early 2010s greatly improved smart TVs. Major TV manufacturers have announced production of smart TVs only, for their middle-end to high-end TVs in 2015. Smart TVs are expected to become the dominant form of television by the late 2010s. At the beginning of 2016, Nielsen reported that 29 percent of those with incomes over $75,000 a year had a smart TV.\n\nSmart TV devices also provide access to user-generated content (either stored on an external hard drive or in cloud storage) and to interactive services and Internet applications, such as YouTube, many using HTTP Live Streaming (also known as HLS) adaptive streaming. Smart TV devices facilitate the curation of traditional content by combining information from the Internet with content from TV providers. Services offer users a means to track and receive reminders about shows or sporting events, as well as the ability to change channels for immediate viewing. Some devices feature additional interactive organic user interface / natural user interface technologies for navigation controls and other human interaction with a Smart TV, with such as second screen companion devices, spatial gestures input like with Xbox Kinect, and even for speech recognition for natural language user interface. Smart TV develops new features to satisfy consumers and companies, such as new payment processes. LG and PaymentWall have collaborated to allow consumers to access purchased apps, movies, games, and more using a remote control, laptop, tablet, or smartphone. This is intended for an easier and more convenient way for checkout.\n\nSmart TV technology and software is still evolving, with both proprietary and open source software frameworks already available. These can run applications (sometimes available via an 'app store' digital distribution platform), interactive on-demand media, personalized communications, and have social networking features.\n\nAndroid TV, Boxee, Firefox OS, Frog, Google TV, Horizon TV, httvLink, Inview, Kodi Entertainment Center, MeeGo, Mediaroom, OpenTV, Opera TV, Plex, Roku, RDK(Reference Development Kit), Smart TV Alliance, ToFu Media Platform, Ubuntu TV, and Yahoo! Smart TV are framework platforms managed by individual companies. HbbTV, provided by the Hybrid Broadcast Broadband TV association, CE-HTML, part of Web4CE, OIPF, part of HbbTV, and Tru2way are framework platforms managed by technology businesses. Current Smart TV platforms used by vendors are Amazon, Apple, Google, Haier, Hisense, Hitachi, Insigna, LG, Microsoft, Netgear, Panasonic, Philips, Samsung, Sharp, Sony, TCL, TiVO, Toshiba, Sling Media, and Western Digital. Sony, Panasonic, Samsung, LG, and Roku TV are some platforms ranked under the best Smart TV platforms.\n\nAccording to a report from research group NPD In-Stat, in 2012 only about 12 million U.S. households had their Web-capable TVs connected to the Internet, although an estimated 25 million households owned a set with the built-in network capability. In-Stat predicted that by 2016, 100 million homes in North America and western Europe would be using television sets blending traditional programming with internet content.\n\nThe number of households using over-the-top television services has rapidly increased over the years. In 2015, 52% of U.S. households subscribed to Netflix, Amazon Prime, or Hulu Plus; 43% of pay-TV subscribers also used Netflix, and 43% of adults used some streaming video on demand service at least monthly. Additionally, 19% of Netflix subscribers shared their subscription with people outside of their households. Ten percent of adults at the time showed interest in HBO Now.\n\nSome smart TV platforms come prepackaged, or can be optionally extended, with social networking technology capabilities. The addition of social networking synchronization to smart TV and HTPC platforms may provide an interaction with both on-screen content and other viewers than is currently available to most televisions, while simultaneously providing a much more cinematic experience of the content than is currently available with most computers.\n\nSome smart TV platforms also support interactive advertising, addressable advertising with local advertising insertion and targeted advertising, and other advanced advertising features such as ad telescoping using VOD and DVR, enhanced TV for consumer call-to-action and audience measurement solutions for ad campaign effectiveness. The marketing and trading possibilities offered by Smart TVs are sometimes summarized by the term t-commerce. Taken together, this bidirectional data flow means that smart TVs can be and are used for clandestine observation of the owners. Even in sets that are not configured off-the-shelf to do so, default security measures are often weak and will allow hackers to easily break into the TV.\n\nThere is evidence that a smart TV is vulnerable to attacks. Some serious security bugs have been discovered, and some successful attempts to run malicious code to get unauthorized access were documented on video. There is evidence that it is possible to gain root access to the device, install malicious software, access and modify configuration information for a remote control, remotely access and modify files on TV and attached USB drives, access camera and microphone..\n\nThere have also been concerns that hackers may be able to remotely turn on the microphone or web-camera on a smart TV, being able to eavesdrop on private conversations. A common loop antenna may be set for a \"bi\"directional transmission channel, capable of uploading data rather than only receiving. Since 2012, security researchers discovered a similar vulnerability present in more series of Smart Tvs, which allows hackers to get an external root access on the device.\n\nAnticipating growing demand for an antivirus for a smart TV, some security software companies are already working with partners in digital TV field on the solution. At this writing it seems like there is only one antivirus for smart TVs available: 'Neptune', a cloud-based antimalware system developed by Ocean Blue Software in partnership with Sophos. However, antivirus company Avira has joined forces with digital TV testing company Labwise to work on software to protect against potential attacks. The privacy policy for Samsung's Smart TVs has been called Orwellian (a reference to George Orwell and the dystopian world of constant surveillance he depicted in \"1984\"), and compared to Telescreens because of eavesdropping concerns.\n\nHackers have misused Smart TV's abilities such as operating source codes for applications and its unsecured connection to the Internet. Passwords, IP address data, and credit card information can be accessed by hackers and even companies for advertisement. A company caught in the act is Vizio. The confidential documents, codenamed Vault 7 and dated from 2013–2016, include details on CIA's software capabilities, such as the ability to compromise smart TVs.\n\nInternet websites can block smart TV access to content at will, or tailor the content that will be received by each platform. Google TV-enabled devices were blocked by NBC, ABC, CBS, and Hulu from accessing their Web content since the launch of Google TV in October 2010. Google TV devices were also blocked from accessing any programs offered by Viacom’s subsidiaries.\n\nHigh-end Samsung Smart TVs stopped working for at least seven days after a software update. Application providers are rarely upgrading Smart TV apps to the latest version; for example, Netflix does not support older TV versions with new Netflix upgrades.\n\n"}
{"id": "57079276", "url": "https://en.wikipedia.org/wiki?curid=57079276", "title": "Solar Decathlon Latin America and Caribbean", "text": "Solar Decathlon Latin America and Caribbean\n\nThe Solar Decathlon is an initiative of the Department of Energy of the United States (DOE) in which universities around the world compete with the design and construction of sustainable housing that works 100% with solar energy. It is called “Decathlon\" since universities and their prototypes are evaluated in 10 criteria: architecture, engineering and construction, energy efficiency, energy consumption, comfort, sustainability, positioning, communications, urban design and feasibility and innovation.\n\nThe 2019 edition of the Solar Decathlon Latin America and Caribbean will take place in Cali, Colombia.\n\n\nThe first Solar Decathlon Latin America and Caribbean was held on the campus of Universidad del Valle in Santiago de Cali, Colombia, in December 2015.\n\nThe top finishers were:\n\n\nThe other participating teams were:\n\n\n\n"}
{"id": "9059911", "url": "https://en.wikipedia.org/wiki?curid=9059911", "title": "Tempest prognosticator", "text": "Tempest prognosticator\n\nThe tempest prognosticator, also known as the leech barometer, is a 19th-century invention by George Merryweather in which leeches are used in a barometer. The twelve leeches are kept in small bottles inside the device; when they become agitated by an approaching storm they attempt to climb out of the bottles and trigger a small hammer which strikes a bell. The likelihood of a storm is indicated by the number of times the bell is struck.\n\nDr. Merryweather, honorary curator of the Whitby Literary and Philosophical Society's Museum, detailed the sensitivity that medicinal leeches displayed in reaction to electrical conditions in the atmosphere. He was inspired by two lines from Edward Jenner's poem \"Signs of Rain\": \"The leech disturbed is newly risen; Quite to the summit of his prison.\" Merryweather spent much of 1850 developing his ideas and came up with six designs for what he originally referred to as \"An Atmospheric Electromagnetic Telegraph, conducted by Animal Instinct.\" These ranged from a cheap version, which he envisaged would be used by the government and the shipping industries, to a more expensive design. The expensive design, which took inspiration from the architecture of Indian temples, was made by local craftsmen and shown in the 1851 Great Exhibition at The Crystal Palace in London.\n\nOn 27 February 1851 he gave a nearly three-hour essay to members of the Philosophical Society entitled \"Essay explanatory of the Tempest Prognosticator in the building of the Great Exhibition for the Works of Industry of All Nations.\"\n\nThe tempest prognosticator comprises twelve pint bottles in a circle around and beneath a large bell. Atop the glasses are small metal tubes which contain a piece of whalebone and a wire connecting them to small hammers positioned to strike the bell. In his essay Merryweather described the workings of the device:\nThe leech would have difficulty entering the metal tubes but would endeavour to do so if sufficiently motivated by the likelihood of bad weather. By ringing the bell it would signify that that individual leech is indicating that a storm is approaching. Merryweather referred to the leeches as his \"jury of philosophical councilors\" and that the more of them that rang the bell the more likely that a storm would occur.\n\nIn his essay Merryweather also noted other features of the design, including the fact that the leeches were placed in glass bottles placed in a circle to prevent them from feeling \"the affliction of solitary confinement\".\n\nMerryweather spent all of 1850 testing the device, sending a letter to the president of the Philosophical Society and the Whitby Institute, Henry Belcher, to warn him of an impending storm. The results of 28 of these predictions are kept in the library of Whitby Museum. Merryweather stated in his essay the great success that he had had with the device.\n\nMerryweather lobbied for the government to make use of his design around the British coastline but they instead opted for Robert FitzRoy's storm glass.\n\nThe original device has been lost, but at least three replicas have been made. The hundredth anniversary of the invention brought renewed interest as a replica was made for the 1951 Festival of Britain. This non-working version was made from the description in a printed copy of Merryweather's essay and a copperplate drawing of the original. The device was shown in the Dome of Discovery and given to the Whitby Philosophical Society when the festival ended. Plans and photographs of this replica were then used to create faithful working models, one at Barometer World near Okehampton, Devon, and another at the Great Dickens Christmas Fair in San Francisco.\n\n\n"}
{"id": "4510033", "url": "https://en.wikipedia.org/wiki?curid=4510033", "title": "Tracing paper", "text": "Tracing paper\n\nTracing paper is paper made to have low opacity, allowing light to pass through. It was originally developed for architects and design engineers to create drawings which could be copied precisely using the diazo copy process; it then found many other uses. The original use for drawing and tracing was largely superseded by technologies which do not require diazo copying or manual copying (by tracing) of drawings.\n\nThe transparency of the paper is achieved by careful selection of the raw materials and the process used to create transparency. Cellulose fibre forms the basis of the paper, usually from wood species but also from cotton fibre. Often, paper contains other filler materials to enhance opacity and print quality. For tracing or translucent paper, it is necessary to remove any material which obstructs the transmission of light.\n\nTracing paper is paper made to have low opacity, allowing light to pass through. It is named as such for its ability for an image to be traced onto it. It was originally developed for architects and design engineers to create drawings which could be copied precisely using the diazo copy process;\n\nWhen tracing paper is placed onto a picture, the picture is easily visible through the paper. Thus, it becomes easy to find edges in the picture and trace the image onto the tracing paper. Pure cellulose fiber is translucent, and it is the air trapped between fibers that makes paper opaque and look white. If the fibers are refined and beaten until all the air is taken out, then the resulting sheet will be translucent. Translucent papers are dense and contain up to 10% moisture at 50% humidity.\n\nTracing paper is usually made from sulfite pulp by reducing the fibres to a state of fine subdivision and hydrolysing them by very prolonged beating in water.\n\nThere are three main processes to manufacture this type of paper, as follows:-\n\n\nThe sizing in production will determine whether it is for laser printer or inkjet/offset printing.\n\nTracing paper may be uncoated or coated. Natural tracing paper for laser printing is usually uncoated.\n\nThe HS code for tracing paper is 4806.30.\n\nTracing paper can be recycled and also can be made from up to 30% recycled fibre.\n\n\nThe follows are common standards for tracing paper Though, generally it is manufactured in grammages over 60 GSM:\n\n\n"}
{"id": "2244594", "url": "https://en.wikipedia.org/wiki?curid=2244594", "title": "User innovation", "text": "User innovation\n\nUser innovation refers to innovation by intermediate users (e.g. user firms) or consumer users (individual end-users or user communities), rather than by suppliers (producers or manufacturers). This is a concept closely aligned to co-design, and has been proven to result in more innovative solutions than traditional consultation methodologies.\n\nEric von Hippel and others observed that many products and services are actually developed or at least refined, by users, at the site of implementation and use. These ideas are then moved back into the supply network. This is because products are developed to meet the widest possible need; when individual users face problems that the majority of consumers do not, they have no choice but to develop their own modifications to existing products, or entirely new products, to solve their issues. Often, user innovators will share their ideas with manufacturers in hopes of having them produce the product, a process called free revealing.\n\nBased on research on the evolution of Internet technologies and open source software Ilkka Tuomi further highlighted the point that users are fundamentally social. User innovation, therefore, is also socially and socio-technically distributed innovation. According to Tuomi, key uses are often unintended uses invented by user communities that reinterpret and reinvent the meaning of emerging technological opportunities.\n\nThe existence of user innovation, for example, by users of industrial robots, rather than the manufacturers of robots is a core part of the argument against the Linear Innovation Model i.e. innovation comes from research and development, is then marketed and 'diffuses' to end-users. Instead innovation is a non-linear process involving innovations at all stages.\n\nIn 1986 Eric von Hippel introduced the lead user method that can be used to systematically learn about user innovation in order to apply it in new product development. In 2007 another specific type of user innovator, the creative consumer was introduced. These are consumers who adapt, modify, or transform a proprietary offering as opposed to creating completely new products.\n\nUser innovation has a number of degrees: innovation of use, innovation in services, innovation in configuration of technologies, and finally the innovation of novel technologies themselves. While most user innovation is concentrated in use and configuration of existing products and technologies, and is a normal part of long term innovation, new technologies that are easier for end-users to change and innovate with, and new channels of communication are making it much easier for user innovation to occur and have an impact.\n\nRecent research has focused on Web-based forums that facilitate user (or customer) innovation - referred to as virtual customer environment, these forums help companies partner with their customers in various phases of product development as well as in other value creation activities. For example, Threadless, a T-shirt manufacturing company, relies on the contribution of online community members in the design process. The community includes a group of volunteer designers who submit designs and vote on the designs of others. In addition to free exposure, designers are provided monetary incentives including a $2,500 base award as well as a percentage of T-shirt sales. These incentives allow Threadless to encourage continual user contribution.\n\n\n"}
{"id": "51030417", "url": "https://en.wikipedia.org/wiki?curid=51030417", "title": "ZOTAC", "text": "ZOTAC\n\nAside from its international headquarters in Macau, it also has three offices overseas in Korea, the United States, and Germany.\nZOTAC was established in 2006 under the umbrella company of PC Partner. Its name was derived from the words \"zone\" and \"tact\". A year later, ZOTAC created its first ever video card, the ZOTAC GeForce 7300 GT.\nIn 2008, ZOTAC became the first hardware company to ship video cards with a factory overclock. It also created the first Mini PC, the MAG ND01. The MAG ND01 officially debuted in 2009. It featured 2GB of DDR2 memory, an Intel Atom 230 or 330 depending on the model, and a 160GB HDD. \n\nIn 2015, ZOTAC created a Steam Machine called the NEN. It featured a Nvidia GeForce 960 and a Intel Core i5-6400T Processor.\n\nIn 2016, The MAGNUS EN980 debuted at Computex Taipei. It was the first ever Mini PC that was considered \"VR ready\" by Nvidia, and it featured a Nvidia GTX 980 and an i5 Processor. Also launched is the smallest Mini PC line-up, P Series, and ZOTAC VR GO.\n\nIn 2017, Zotac released their GTX 1000 series line including their 1080, 1070, 1060, and also their mini series including the GeForce GTX 1080 Ti Mini. They also introduced their new brand now known as 'Zotac Gaming'. The first product launched under it was the MEK Gaming PC, which was a Mini ITX desktop. In addition to the MEK Gaming PC and graphic cards, Zotac also released a external enclosure that supported Thunderbolt 3 and could host a graphic card up to nine inches long. \n\nIn 2018, Zotac has announced their 2000 graphic card series including the GeForce RTX 2080 Ti, GeForce RTX 2080 series, and the GeForce RTX 2070 Series. In addition to graphic cards, Zotac has also released their new line of ZBOX mini PCs in Q2 2018. \n\nZotac's GeForce series includes their slightly modified stock graphic cards and their own Amp! and Amp! Extreme products. Their Amp! and Amp! Extreme series are modified versions of NVIDIA's stock graphics cards that includes a modified cooling system, advertised as quieter.\n\nZotac's ZBox Mini PC Series includes USB 3.0, Wi-Fi, SD card slot, headphone and microphone ports, DVI, HDMI, DisplayPort, and a VGA port. The ZBox Mini PC Series is meant to be completely portable with ports to external displays. \n\n\n"}
