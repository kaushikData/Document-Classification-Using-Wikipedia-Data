{"id": "57096844", "url": "https://en.wikipedia.org/wiki?curid=57096844", "title": "ACP 125", "text": "ACP 125\n\nACP 125 is the short name for \"Allied Communications Publication 125: Communications Instructions—Radiotelephone Procedures\", developed and published by the Combined Communications Electronics Board, for use by the Five Eyes nations and the rest of NATO. According to the latest version, \"The aim of ACP 125 is to prescribe the voice procedure for use by the armed forces of Allied nations on secure and non-secure tactical voice nets. Its purpose is to provide a standardized way of passing speech and data traffic as securely as possible consistent with accuracy, speed and the needs of command and control.\"The standard defines the procedures for communicating by voice over two-way radio, and has served as the basis for radio communications procedures for many non-military organizations, as well as numerous U.S. government organizations, including the United States Department of State and the Civil Air Patrol. \n\nFirst published as ACP 125(A) in about 1951, the current version is ACP 125(G), published in 2016. The standard itself is ACP 125, with the letter in parenthesis indicating the major revision level. There are at least two supplements, including: \n\n\nAlthough the standard is designed for use by all NATO countries, especially when operating in conjunction with each other, some efforts have been made in the past to translate the procedure words into the native language of member countries. For example, NATO memo SGM-623-56 from 1956, \"French Equivalents to English Expressions Used in ACP 125(B)\", includes the following table of prowords:\n\n\n"}
{"id": "22390148", "url": "https://en.wikipedia.org/wiki?curid=22390148", "title": "Alert Standard Format", "text": "Alert Standard Format\n\nAlert Standard Format (ASF) (also sometimes referred to as \"Alert Standard Forum\", \"Alerting Specifications Forum\", \"Alert Specification Function\", etc.) is a DMTF standard for remote monitoring, management and control of computer systems in both OS-present and OS-absent environments. These technologies are primarily focused on minimizing on-site I/T maintenance, maximizing system availability and performance to the local user.\n\nASF, unlike other DMTF standards, defines both external-facing network protocols (for use with Remote Management Consoles and Alert Sending Devices) and system-internal protocols and data models (for use in System Firmware, Remote Control Devices, Alert Sending Devices, and Sensors).\n\nASF v1.0 (DSP0114) was published by the DMTF Pre-OS Working Group in June 2001.\n\nASF v2.0 (DSP0136), adding secure remote authentication and data integrity, was published by the DMTF Pre-OS Working Group in April 2003.\n\n\n\n\n"}
{"id": "15482602", "url": "https://en.wikipedia.org/wiki?curid=15482602", "title": "Anomalous photovoltaic effect", "text": "Anomalous photovoltaic effect\n\nThe anomalous photovoltaic effect (APE), also called the bulk photovoltaic effect in certain cases, is a type of a photovoltaic effect which occurs in certain semiconductors and insulators. The \"anomalous\" refers to those cases where the photovoltage (i.e., the open-circuit voltage caused by the light) is larger than the band gap of the corresponding semiconductor. In some cases, the voltage may reach thousands of volts.\n\nAlthough the voltage is unusually high, the short-circuit current is unusually low. Overall, materials that exhibit the anomalous photovoltaic effect have very low power generation efficiencies, and are never used in practical power-generation systems.\n\nThere are several situations in which APE can arise.\n\nFirst, in polycrystalline materials, each microscopic grain can act as a photovoltaic. Then the grains add in series, so that the overall open-circuit voltage across the sample is large, potentially much larger than the bandgap.\n\nSecond, in a similar manner, certain ferroelectric materials can develop stripes consisting of parallel ferroelectric domains, where each domain acts like a photovoltaic and each domain wall acts like a contact connecting the adjacent photovoltaics (or vice versa). Again, domains add in series, so that the overall open-circuit voltage is large.\n\nThird, a perfect single crystal with a non-centrosymmetric structure can develop a giant photovoltage. This is specifically called the bulk photovoltaic effect, and occurs because of non-centrosymmetry. Specifically, the electron processes—photo-excitation, scattering, and relaxation—occur with different probabilities for electron motion in one direction versus the opposite direction.\n\nThis effect was discovered by Starkiewicz et al. in 1946 on PbS films and was later observed on other semiconducting polycrystalline films including CdTe, Silicon, Germanium, ZnTe and InP, as well as on amorphous silicon films and in nanocrystalline silicon systems. Observed photovoltages were found to reach hundreds, and in some cases even thousands of volts. The films in which this effect was observed were generally thin semiconducting films that were deposited by vacuum evaporation onto a heated insulating substrate, held at an angle with respect to the direction of the incident vapor. However, the photovoltage was found to be very sensitive to the conditions and procedure at which the samples were prepared. This made it difficult to get reproducible results which is probably the reason why no satisfactory model for it has been accepted thus far. Several models were, however, suggested to account for the extraordinary phenomenon and they are briefly outlined below.\n\nThe oblique deposition can lead to several structure asymmetries in the films. Among the first attempts to explain the APE were few that treated the film as a single entity, such as considering the variation of sample thickness along its length or a non-uniform distribution of electron traps. However, studies that followed generally supported models that explain the effect as resulting from a series of microelements contributing additively to the net photovoltage. The more popular models used to explain the photovoltage are reviewed below.\n\nWhen photogenerated electrons and holes have different mobilities, a potential difference can be created between the illuminated and non-illuminated faces of a semiconductor slab. Generally this potential is created through the depth of the slab, whether it is a bulk semiconductor or a polycrystalline film. The difference between these cases is that in the latter, a photovoltage can be created in each one of the microcrystallites. As was mentioned above, in the oblique deposition process inclined crystallites are formed in which one face can absorb light more than the other. This may cause a photovoltage to be generated along the film, as well as through its depth. The transfer of carriers at the surface of crystallites is assumed to be hindered by the presence of some unspecified layer with different properties, thus cancellation of consecutive Dember voltages is being prevented. To explain the polarity of the PV which is independent of the illumination direction one must assume that there exists a large difference in recombination rates at opposite faces of a crystallite, which is a weakness of this model.\n\nThis model suggests that when a material crystallizes both in cubic and hexagonal structures, an asymmetric barrier can be formed by a residual dipole layer at the interface between the two structures. A potential barrier is formed due to a combination of the band gap difference and the electric fields produced at the interface. One should remember that this model can be invoked to explain anomalous PV effect only in those materials that can demonstrate two types of crystal structure.\n\nIt was suggested by Starkiewicz that the anomalous PV is developed due to a distribution gradient of positive and negative impurity ions through the microcrystallites, with an orientation such as to give a non-zero total photovoltage. This is equivalent to an array of p-n junctions. However, the mechanism by which such p-n junctions may be formed was not explained.\n\nThe interface between crystallites may contain traps for charge carriers. This may lead to a surface charge and an opposite space charge region in the crystallites, in case that the crystallites are small enough. Under illumination of the inclined crystallites electron-hole pairs are generated and cause a compensation of the charge in the surface and within the crystallites. If it is assumed that the optical absorption depth is much less than the space charge region in the crystallites, then, because of their inclined shape more light is absorbed in one side than in the other. Thus a difference in the reduction of the charge is created between the two sides. This way a photovoltage parallel to the surface is developed in each crystallite.\n\nA perfect single crystal with a non-centrosymmetric structure can develop a giant photovoltage. This is specifically called the bulk photovoltaic effect, and occurs because of non-centrosymmetry. The electron processes like photo-excitation, scattering, and relaxation may occur with different probabilities for electrons moving one direction versus the opposite direction.\n\nThis effect was first discovered in the 1960s. It has been observed in lithium niobate (LiNbO), barium titanate (BaTiO) and many other materials.\n\nTheoretical calculations using density functional theory or other methods can predict the extent to which a material will exhibit the bulk photovoltaic effect.\n\nShown at right is an example of a simple system that would exhibit the bulk photovoltaic effect. There are two electronic levels per unit cell, separated by a large energy gap, say 3 eV. The blue arrows indicate radiative transitions, i.e. an electron can absorb a UV photon to go from A to B, or it can emit a UV photon to go from B to A. The purple arrows indicate nonradiative transitions, i.e. an electron can go from B to C by emitting many phonons, or can go from C to B by absorbing many phonons.\n\nWhen light is shining, an electron will occasionally move right by absorbing a photon and going from A to B to C. However, it will almost never move in the reverse direction, C to B to A, because the transition from C to B cannot be excited by photons, but instead requires an improbably large thermal fluctuation. Therefore, there is a net rightward photocurrent.\n\nBecause the electrons undergo a \"shift\" each time they absorb a photon (on average), this photocurrent is sometimes called a \"shift current\".\n\nThere are several aspects of the bulk photovoltaic effect that distinguish it from other kinds of effects: In the power-generating region of the I-V curve (between open-circuit and short-circuit), electrons are moving in the \"opposite direction\" that you would expect from the drift-diffusion equation, i.e. electrons are moving towards higher fermi level or holes are moving towards lower fermi level. This is unusual: For example, in a normal silicon solar cell, electrons move in the direction of decreasing electron-quasi-fermi level, and holes move in the direction of increasing hole-quasi-fermi-level, consistent with the drift-diffusion equation. Power generation is possible \"only\" because the quasi-fermi-levels are split. A bulk photovoltaic, by contrast, can generate power without any splitting of quasi-fermi-levels.\n\nThis also explains why large open-circuit voltages tend to be seen only in crystals that (in the dark) have very low conductivity: Any electrons that can freely move through the crystal (i.e., not requiring photons to move) will follow the drift-diffusion equation, which means that these electrons will \"subtract\" from the photocurrent and reduce the photovoltaic effect.\n\nEach time one electron absorbs one photon (in the power-generating region of the I-V curve), the resulting electron displacement is, on average, \"at most\" one or two unit cells or mean-free-paths (this displacement is sometimes called the \"anisotropy distance\"). This is required because if an electron is excited into a mobile, delocalized state, and then it scatters a few times, then its direction is now randomized and it will naturally start following the drift-diffusion equation. However, in the bulk photovoltaic effect, the desired net electron motion is \"opposite\" the direction predicted by the drift-diffusion equation.\n\nFor example, it might be the case that when an electron absorbs a photon, it is disproportionately likely to wind up in a state where it is moving leftward. And perhaps each time a photon excites an electron, the electron moves leftward a bit and then immediately relaxes into (\"gets stuck in\") an immobile state—until it absorbs another photon and the cycle repeats. In this situation, a leftward electron current is possible \"despite\" an electric field pushing electrons in the opposite direction. However, if when a photon excites an electron, it does \"not\" quickly relax back to an immobile state, but instead keeps moving around the crystal and scattering randomly, then the electron will eventually \"forget\" that it was moving left, and it will wind up being pulled rightward by the electric field. Again, the total leftward motion of an electron, per photon absorbed, cannot be much larger than the mean free path.\n\nA consequence is that the quantum efficiency of a thick device is extremely low. It may require millions of photons to bring a single electron from one electrode to the other. As thickness increases, the current goes down as much as the voltage goes up.\n\nIn some cases, the current has different signs depending on the light polarization. This would not occur in an ordinary solar cell like silicon.\n\nThe bulk photovoltaic effect is believed to play a role in the photorefractive effect in lithium niobate.\n\n"}
{"id": "56910813", "url": "https://en.wikipedia.org/wiki?curid=56910813", "title": "Archives management", "text": "Archives management\n\nArchives management is the area of management concerned with the maintenance and use of archives. It is concerned with acquisition, care, arrangement, description and retrieval of records once they have been transferred from an organisation to the archival repository. Once records have been selected and transferred to archival custody, they become archives.\n\nThe steps involved in managing archives include acquiring and receiving from the office of the origin, arranging and describing according to archival principles and practices, providing easy retrieval and access to archives. \n\nAn increasingly relevant aspect of archives management is ensuring the accessibility of archives and archive materials to all users regardless of physical ability. Most archivist and library associations now include resources on educating archivists on how to manage their archives to be more accessible. Both archivists and special collections librarians are faced with the issue of making their resources more accessible to the public, as their items were created prior to the consideration of accessibility.\n"}
{"id": "28279554", "url": "https://en.wikipedia.org/wiki?curid=28279554", "title": "Bankcard-Servicenetz", "text": "Bankcard-Servicenetz\n\nThe Bankcard-Servicenetz (\"bank card service network\") is a German ATM card interbank network group provided by the Volksbanken und Raiffeisenbanken services group. Technically it is not an interbank network but uses the pre-existing girocard network. Member banks of this cash credit group charge ATM usage fees at a low level and most customers of the co-operative banks enjoy free withdrawal from their accounts. With 19,200 ATMs the Bankcard-Servicenetz group is the second largest ATM group in Germany (after the savings banks network). \n\nIn March 2012 more than 99% of the co-operative banks associated in the Federal Association of German 'Volksbanken und Raiffeisenbanken' Co-operative Banks participated in the Bankcard-Servicenetz network. The federal association publishes a list of banks that are \"not\" offering this service.\n\nUnlike other ATM groups like the Cash Group and CashPool, the ATM service network of Bankcard-Servicenetz is not free of charge – the service fee is not waived but the compact restricts the service fee to a common low level (usually 1,02 Euro and in some locations 2,05 Euro). However, in most tariffs of checking accounts, the service fee is taken over by the bank making the usage of the Bankcard-Servicenetz to be effectively free of charge to the customers. Some banks will show this by presenting a line of \"Übernahme Gebühr: 1,02 EUR\" (\"Takeover (of) Fees: 1,02 EUR\") on the statement of account.\n\nSome banks with a very scattered structure of customers choose to not join the ATM group as the share of paying other bank's service fees would be significantly larger than being able to charge service fees within the interbank network. This is often the case with some industry trades co-operatives and most of the ecclesiastical banks.\n\nBanks that are not \"Volksbanken und Raiffeisenbanken\" but still members of the Bankcard-Servicenetz:\n\nSome CashPool member banks have chosen to join also the Bankcard-Servicenetz. However, they point their customers to use CashPool ATMs for free withdrawal while offering to use the Bankcard-Servicenetz ATMs as a fallback option that allows depositors to withdraw cash money at a low fee that is actually charged to customers. This is the case for BBBank (2 times per month the fee is taken over), Bank für Sozialwirtschaft, and Sparda-Bank.\n\nThe list of 'Volksbanken und Raiffeisenbanken' institutes that do not participate in the Bankcard-Servicenetz is listed on the homepage of the Bundesverband der Deutschen Volksbanken und Raiffeisenbanken (BVR). As of July 2015 the list did include only the \n\n\n"}
{"id": "44091727", "url": "https://en.wikipedia.org/wiki?curid=44091727", "title": "Binary repository manager", "text": "Binary repository manager\n\nA binary repository manager is a software tool designed to optimize the download and storage of binary files used and produced in software development. It centralizes the management of all the binary artifacts generated and used by the organization to overcome the complexity arising from the diversity of binary artifact types, their position in the overall workflow and the dependencies between them.\n\nA binary repository is a software repository for packages, artifacts and their corresponding metadata. It can be used to store binary files produced by an organization itself, such as product releases and nightly product builds, or for third party binaries which must be treated differently for both technical and legal reasons.\n\nSoftware development can be a complex process involving many developers, or teams of developers working on shared code bases, accessing the same build tools, downloading and using a shared set of binary resources, and deploying components into the same software product. To manage the source files used in software development, organizations will typically use revision control. The many source files used in software development are eventually built into the binary artifacts (also known as “binaries”) which constitute the components of a software product. In addition, in order to provide their functionality and feature set, software products may use many 3rd party artifacts downloaded from free open source repositories or purchased from commercial sources. Consequently, a software product may comprise tens, hundreds and even thousands of individual binary artifacts which must be managed in order to efficiently maintain a coherent and functional software product. This function of managing the binary artifacts is done by a binary repository manager. A binary repository manager can be thought of as being to binaries what revision control is to source files.\n\nThe software and technology industry continues to change and grow, binary repository managers are no different. They are beginning to shift towards positioning as a universal package managers. These package managers aim to standardize the way enterprises treat all package types used in the software development process. They give users the ability to apply security and compliance metrics across all artifact types. Universal package managers have been referred to as being at the center of a DevOps toolchain.\n\nNotable Universal package managers include:\n\n\nAs part of the development lifecycle, source code is continuously being built into binary artifacts using continuous integration. This may interact with a binary repository manager much like a developer would by getting artifacts from the repositories and pushing builds there. Tight integration with CI servers enables the storage of important metadata such as:\n\nArtifacts and packages inherently mean different things. Artifacts are simply an output or collection of files (ex. JAR, WAR, DLLS, RPM etc.) and one of those files may contain metadata (e.g. POM file). Whereas packages are a single archive file in a well-defined format (ex. NuGet) that contain files appropriate for the package type (ex. DLL, PDB). Many artifacts result from builds but other types are crucial as well. Packages are essentially one of two things: a library or an application.\n\nCompared to source files, binary artifacts are often larger by orders of magnitude, they are rarely deleted or overwritten (except for rare cases such as snapshots or nightly builds), and they are usually accompanied by lots of metadata such as id, package name, version, license and more.\n\nMetadata describes a binary artifact, is stored and specified separately from the artifact itself, and can have several additional uses. The following table shows some common metadata types and their uses:\nKey factors and features when considering the adoption of a package manager include:\n\n\n"}
{"id": "7379750", "url": "https://en.wikipedia.org/wiki?curid=7379750", "title": "Bor S. Luh International Award", "text": "Bor S. Luh International Award\n\nThe Bor S. Luh International Award has been awarded every year since 1956. Before 2005, this award was named the International Award. It is given to an individual or institution that had outstanding efforts in one of the following areas in food technology: 1) International exchange of ideas, 2) better international understanding, and/or 3) practical successful technology transfer to an economically depressed area in a developed or developing area.\n\nThe award was renamed for Bor S. Luh (1916-2001), who was born and educated in China before completing his education in the United States. Luh was the first president of the Chinese American Food Society in 1974-5 and received its Professional Achievement Award in 1984.\n\nAward winners receive a plaque from the Bor S. Luh Endowment Fund of the Institute of Food Technologists Foundation and a USD 3000 honorarium.\n\n"}
{"id": "4892591", "url": "https://en.wikipedia.org/wiki?curid=4892591", "title": "Breast implant", "text": "Breast implant\n\nA breast implant is a prosthesis used to change the size, shape, and contour of a woman's breast. In reconstructive plastic surgery, breast implants can be placed to restore a natural looking breast mound for post–mastectomy breast reconstruction patients or to correct congenital defects and deformities of the chest wall. They are also used cosmetically to enhance or enlarge the appearance of the breast through breast augmentation surgery.\nThere are three general types of breast implant devices, defined by their filler material: saline solution, silicone gel, and composite filler. The saline implant has an elastomer silicone shell filled with sterile saline solution during surgery; the silicone implant has an elastomer silicone shell pre-filled with viscous silicone gel; and the alternative composition implants featured miscellaneous fillers, such as soy oil, polypropylene string, etc. Composite implants are typically not recommended for use anymore and, in fact, their use is banned in the United States and Europe due to associated health risks and complications.\n\nIn surgical practice, for the reconstruction of a breast, the tissue expander device is a temporary breast prosthesis used to form and establish an implant pocket for the future permanent breast implant. For the correction of male breast defects and deformities, the pectoral implant is the breast prosthesis used for the reconstruction and the aesthetic repair of a man's chest wall (see: gynecomastia and mastopexy).\n\nSince the late nineteenth century, breast implants have been used to surgically augment the size (volume), modify the shape (contour), and enhance the feel (tact) of a woman's breasts. In 1895, surgeon Vincenz Czerny effected the earliest breast implant emplacement when he used the patient's autologous adipose tissue, harvested from a benign lumbar lipoma, to repair the asymmetry of the breast from which he had removed a tumor. In 1889, surgeon Robert Gersuny experimented with paraffin injections, with disastrous results.\n\nFrom the first half of the twentieth century, physicians used other substances as breast implant fillers—ivory, glass balls, ground rubber, ox cartilage, Terylene wool, gutta-percha, Dicora, polyethylene chips, Ivalon (polyvinyl alcohol—formaldehyde polymer sponge), a polyethylene sac with Ivalon, polyether foam sponge (Etheron), polyethylene tape (Polystan) strips wound into a ball, polyester (polyurethane foam sponge) Silastic rubber, and teflon-silicone prostheses.\n\nIn the mid-twentieth century, Morton I. Berson, in 1945, and Jacques Maliniac, in 1950, each performed flap-based breast augmentations by rotating the patient's chest wall tissue into the breast to increase its volume. Furthermore, throughout the 1950s and the 1960s, plastic surgeons used synthetic fillers—including silicone injections received by some 50,000 women, from which developed silicone granulomas and breast hardening that required treatment by mastectomy. In 1961, the American plastic surgeons Thomas Cronin and Frank Gerow, and the Dow Corning Corporation, developed the first silicone breast prosthesis, filled with silicone gel; in due course, the first augmentation mammoplasty was performed in 1962 using the Cronin–Gerow Implant, prosthesis model 1963. In 1964, the French company Laboratoires Arion developed and manufactured the saline breast implant, filled with saline solution, and then introduced for use as a medical device in 1964.\n\nToday, there are two types of breast implants commonly used for mammaplasty, breast reconstruction, and breast augmentation procedures:\n\nThe saline breast implant—filled with saline solution (biological-concentration salt water 0.90% w/v of NaCl, ca. 300 mOsm/L.)—was first manufactured by the Laboratoires Arion company, in France, and was introduced for use as a prosthetic medical device in 1964. The contemporary models of saline breast implant are manufactured with thicker, room-temperature vulcanized (RTV) shells made of a silicone elastomer. The study \"In vitro Deflation of Pre-filled Saline Breast Implants\" (2006) reported that the rates of deflation (filler leakage) of the pre-filled saline breast implant made it a second-choice prosthesis for corrective breast surgery. Nonetheless, in the 1990s, the saline breast implant was the prosthesis most common device used for breast augmentation surgery in the United States, because of the U.S. FDA's restriction against the implantation of silicone-filled breast implants outside of clinical studies. Saline breast implants have enjoyed little popularity in the rest of the world, possessing negligible market share.\n\nThe technical goal of saline-implant technology was a physically less invasive surgical technique for emplacing an empty breast implant device through a smaller surgical incision. In surgical praxis, after having emplaced the empty breast implants to the implant pockets, the plastic surgeon then filled each device with saline solution, and, because the required insertion-incisions are short and small, the resultant incision-scars will be smaller and shorter than the surgical scars usual to the long incisions required for inserting pre-filled, silicone-gel implants.\n\nWhen compared to the results achieved with a silicone-gel breast implant, the saline implant can yield acceptable results, of increased breast-size, smoother hemisphere-contour, and realistic texture; yet, it is likelier to cause cosmetic problems, such as the rippling and the wrinkling of the breast-envelope skin, accelerated lower breast pole stretch, and technical problems, such as the presence of the implant being noticeable to the eye and to the touch. The occurrence of such cosmetic problems is likelier in the case of the woman with very little breast tissue, and in the case of the woman who requires post-mastectomy breast reconstruction; thus, the silicone-gel implant is the technically superior prosthetic device for breast augmentation, and for breast reconstruction. In the case of the woman with much breast tissue, for whom sub-muscular emplacement is the recommended surgical approach, saline breast implants can produce an aesthetic result much like that afforded by silicone breast implants, albeit with greater implant palpability.\n\nAs a medical device technology, there are five generations of silicone breast implant, each defined by common model-manufacturing techniques.\n\nThe modern prosthetic breast was invented in 1961 by the American plastic surgeons Thomas Cronin and Frank Gerow, and manufactured by the Dow Corning Corporation; in due course, the first augmentation mammoplasty was performed in 1962.\n\nThe Cronin–Gerow Implant, prosthesis model 1963, was a silicone rubber envelope-sac, shaped like a teardrop, which was filled with viscous silicone-gel. To reduce the rotation of the emplaced breast implant upon the chest wall, the model 1963 prosthesis was affixed to the implant pocket with a fastener-patch, made of Dacron material (Polyethylene terephthalate), which was attached to the rear of the breast implant shell.\n\nIn the 1970s, manufacturers presented the second generation of breast implant prostheses that featured functional developments and aesthetic improvements to the technology:\n\n\n\nIn the 1980s, the models of the Third and of the Fourth generations of breast implant devices were sequential advances in manufacturing technology, such as elastomer-coated shells that decreased gel-bleed (filler leakage), and a thicker (increased-cohesion) filler gel. Sociologically, the manufacturers of prosthetic breasts then designed and made anatomic models (natural breast) and shaped models (round, tapered) that realistically corresponded with the breast- and body- types of women. The tapered models of breast implant have a uniformly textured surface, which reduces the rotation of the prosthesis within the implant pocket; the round models of breast implant are available in smooth-surface- and textured-surface- types.\n\nSince the mid-1990s, the fifth generation of silicone-gel breast implant is made of a high-strength, highly cohesive silicone gel that mostly eliminates the occurrences of filler leakage (“silicone gel bleed”) and of the migration of the silicone filler from the implant pocket to elsewhere in the woman's body. These implants are commonly referred to as \"gummy bear breast implants\" for their firm, pliant consistency, which is similar to gummy candies. The studies \"Experience with Anatomical Soft Cohesive Silicone gel Prosthesis in Cosmetic and Reconstructive Breast Implant Surgery\" (2004) and \"Cohesive Silicone gel Breast Implants in Aesthetic and Reconstructive Breast Surgery\" (2005) reported low incidence-rates of capsular contracture and of device-shell rupture; and greater rates of improved medical-safety and technical-efficacy than that of early generation breast implant devices.\n\nThe breast augmentation patient usually is a young woman whose personality profile indicates psychological distress about her personal appearance and her bodily self image, and a history of having endured criticism (teasing) about the aesthetics of her person. The studies \"Body Image Concerns of Breast Augmentation Patients\" (2003) and \"Body Dysmorphic Disorder and Cosmetic Surgery\" (2006) reported that the woman who underwent breast augmentation surgery also had undergone psychotherapy, suffered low self-esteem, presented frequent occurrences of psychological depression, had attempted suicide, and suffered body dysmorphia, a type of mental illness.\n\nPost-operative patient surveys about mental health and quality-of-life, reported improved physical health, physical appearance, social life, self-confidence, self-esteem, and satisfactory sexual functioning. Furthermore, the women reported long-term satisfaction with their breast implant outcomes; some despite having suffered medical complications that required surgical revision, either corrective or aesthetic. Likewise, in Denmark, 8 per cent of breast augmentation patients had a pre-operative history of psychiatric hospitalization.\n\nIn 2008, the longitudinal study \"Excess Mortality from Suicide and other External Causes of Death Among Women with Cosmetic Breast Implants\" (2007), reported that women who sought breast implants are almost 3 times as likely to commit suicide as are women who have not sought breast implants. Compared to the standard suicide-rate for women of the general populace, the suicide-rate for women with augmented breasts remained constant until 10-years post-implantation, yet, it increased to 4.5 times greater at the 11-year mark, and so remained until the 19-year mark, when it increased to 6 times greater at 20-years post-implantation. Moreover, additional to the suicide-risk, women with breast implants also faced a trebled death-risk from alcoholism and the abuse of prescription and recreational drugs. Although seven studies have statistically connected a woman's breast augmentation to a greater suicide-rate, the research indicates that breast augmentation surgery does not increase the death rate; and that, in the first instance, it is the psychopathologically-inclined woman who is more likely to undergo a breast augmentation procedure.\n\nThe study \"Effect of Breast Augmentation Mammoplasty on Self-Esteem and Sexuality: A Quantitative Analysis\" (2007), reported that the women attributed their improved self image, self-esteem, and increased, satisfactory sexual functioning to having undergone breast augmentation; the cohort, aged 21–57 years, averaged post-operative self-esteem increases that ranged from 20.7 to 24.9 points on the 30-point Rosenberg self-esteem scale, which data supported the 78.6 per cent increase in the woman's libido, relative to her pre-operative level of libido. Therefore, before agreeing to any surgery, the plastic surgeon evaluates and considers the woman's mental health to determine if breast implants can positively affect her self-esteem and sexual functioning.\n\nA mammoplasty procedure for the placement of breast implant devices has three (3) purposes:\n\n\nThe operating room (OR) time of post–mastectomy breast reconstruction, and of breast augmentation surgery is determined by the procedure employed, the type of incisions, the breast implant (type and materials), and the pectoral locale of the implant pocket.\n\nRecent research has indicated that mammograms should not be done with any increased frequency than used in normal procedure in patients undergoing breast surgery, including breast implant, augmentation, mastopexy, and breast reducation.\n\nBreast implant emplacement is performed with five (5) types of surgical incisions:\n\n\nThe four surgical approaches to emplacing a breast implant to the implant pocket are described in anatomical relation to the pectoralis major muscle.\n\n\nThe surgical scars of a breast augmentation mammoplasty develop approximately at 6-weeks post-operative, and fade within months. Depending upon the daily-life physical activities required of the woman, the breast augmentation patient usually resumes her normal life at 1-week post-operative. Moreover, women whose breast implants were emplaced beneath the chest muscles (submuscular placement) usually have a longer, slightly more painful convalescence, because of the healing of the incisions to the chest muscles. Usually, she does not exercise or engage in strenuous physical activities for approximately 6 weeks. During the initial post-operative recovery, the woman is encouraged to regularly exercise (flex and move) her arm to alleviate pain and discomfort; if required, analgesic indwelling medication catheters can alleviate pain Moreover, significantly improved patient recovery has resulted from refined breast-device implantation techniques (submuscular, subglandular) that allow 95 per cent of women to resume their normal lives at 24-hours post-procedure, without bandages, fluid drains, pain pumps, catheters, medical support brassières, or narcotic pain medication.\n\nThe plastic surgical emplacement of breast implant devices, either for breast reconstruction or for aesthetic purpose, presents the same health risks common to surgery, such as adverse reaction to anesthesia, hematoma (post-operative bleeding), late hematoma (post-operative bleeding after 6 months or more), seroma (fluid accumulation), incision-site breakdown (wound infection). Complications specific to breast augmentation include breast pain, altered sensation, impeded breast-feeding function, visible wrinkling, asymmetry, thinning of the breast tissue, and symmastia, the “bread loafing” of the bust that interrupts the natural plane between the breasts. Specific treatments for the complications of indwelling breast implants—capsular contracture and capsular rupture—are periodic MRI monitoring and physical examinations. Furthermore, complications and re-operations related to the implantation surgery, and to tissue expanders (implant place-holders during surgery) can cause unfavorable scarring in approximately 6–7 per cent of the patients.\n\nBecause a breast implant is a Class III medical device of limited product-life, the principal rupture-rate factors are its age and design; Nonetheless, a breast implant device can retain its mechanical integrity for decades in a woman's body. When a saline breast implant ruptures, leaks, and empties, it quickly deflates, and thus can be readily explanted (surgically removed). The follow-up report, \"Natrelle Saline-filled Breast Implants: a Prospective 10-year Study\" (2009) indicated rupture-deflation rates of 3–5 per cent at 3-years post-implantation, and 7–10 per cent rupture-deflation rates at 10-years post-implantation.\n\nWhen a silicone breast implant ruptures it usually does not deflate, yet the filler gel does leak from it, which can migrate to the implant pocket; therefore, an intracapsular rupture (in-capsule leak) can become an extracapsular rupture (out-of-capsule leak), and each occurrence is resolved by explantation. Although the leaked silicone filler-gel can migrate from the chest tissues to elsewhere in the woman's body, most clinical complications are limited to the breast and armpit areas, usually manifested as granulomas (inflammatory nodules) and axillary lymphadenopathy (enlarged lymph glands in the armpit area).\n\nThe suspected mechanisms of breast implant rupture are:\n\nSilicone implant rupture can be evaluated using magnetic resonance imaging; from the long-term MRI data for single-lumen breast implants, the European literature about second generation silicone-gel breast implants (1970s design), reported silent device-rupture rates of 8–15 per cent at 10-years post-implantation (15–30% of the patients).\n\nThe study \"Safety and Effectiveness of Mentor’s MemoryGel Implants at 6 Years\" (2009), which was a branch study of the U.S. FDA's core clinical trials for primary breast augmentation surgery patients, reported low device-rupture rates of 1.1 per cent at 6-years post-implantation. The first series of MRI evaluations of the silicone breast implants with thick filler-gel reported a device-rupture rate of 1 percent, or less, at the median 6-year device-age. Statistically, the manual examination (palpation) of the woman is inadequate for accurately evaluating if a breast implant has ruptured. The study, \"The Diagnosis of Silicone Breast implant Rupture: Clinical Findings Compared with Findings at Magnetic Resonance Imaging\" (2005), reported that, in asymptomatic patients, only 30 per cent of the ruptured breast implants are accurately palpated and detected by an experienced plastic surgeon, whereas MRI examinations accurately detected 86 per cent of breast implant ruptures. Therefore, the U.S. FDA recommended scheduled MRI examinations, as silent-rupture screenings, beginning at the 3-year-mark post-implantation, and then every two years, thereafter. Nonetheless, beyond the U.S., the medical establishments of other nations have not endorsed routine MRI screening, and, in its stead, proposed that such a radiologic examination be reserved for two purposes: (i) for the woman with a suspected breast implant rupture; and (ii) for the confirmation of mammographic and ultrasonic studies that indicate the presence of a ruptured breast implant.\n\nFurthermore, \"The Effect of Study design Biases on the Diagnostic Accuracy of Magnetic Resonance Imaging for Detecting Silicone Breast Implant Ruptures: a Meta-analysis\" (2011) reported that the breast-screening MRIs of asymptomatic women might overestimate the incidence of breast implant rupture. In the event, the U.S. Food and Drug Administration emphasised that “breast implants are not lifetime devices. The longer a woman has silicone gel-filled breast implants, the more likely she is to experience complications.”\n\nThe human body's immune response to a surgically installed foreign object—breast implant, cardiac pacemaker, orthopedic prosthesis—is to encapsulate it with scar tissue capsules of tightly woven collagen fibers, in order to maintain the integrity of the body by isolating the foreign object, and so tolerate its presence. Capsular contracture—which should be distinguished from normal capsular tissue—occurs when the collagen-fiber capsule thickens and compresses the breast implant; it is a painful complication that might distort either the breast implant, or the breast, or both.\n\nThe cause of capsular contracture is unknown, but the common incidence factors include bacterial contamination, device-shell rupture, filler leakage, and hematoma. The surgical implantation procedures that have reduced the incidence of capsular contracture include submuscular emplacement, the use of breast implants with a textured surface (polyurethane-coated); limited pre-operative handling of the implants, limited contact with the chest skin of the implant pocket before the emplacement of the breast implant, and irrigation of the recipient site with triple-antibiotic solutions.\n\nThe correction of capsular contracture might require an open capsulotomy (surgical release) of the collagen-fiber capsule, or the removal, and possible replacement, of the breast implant. Furthermore, in treating capsular contracture, the closed capsulotomy (disruption via external manipulation) once was a common maneuver for treating hard capsules, but now is a discouraged technique, because it can rupture the breast implant. Non-surgical treatments for collagen-fiber capsules include massage, external ultrasonic therapy, leukotriene pathway inhibitors such as zafirlukast (Accolate) or montelukast (Singulair), and pulsed electromagnetic field therapy (PEMFT).\n\nWhen the patient is unsatisfied with the outcome of the augmentation mammoplasty; or when technical or medical complications occur; or because of the breast implants’ limited product life, it is likely she might require replacing the breast implants. Common revision surgery indications include major and minor medical complications, capsular contracture, shell rupture, and device deflation. Revision incidence rates were greater for breast reconstruction patients, because of the post-mastectomy changes to the soft-tissues and to the skin envelope of the breast, and to the anatomical borders of the breast, especially in women who received adjuvant external radiation therapy. Moreover, besides breast reconstruction, breast cancer patients usually undergo revision surgery of the nipple-areola complex (NAC), and symmetry procedures upon the opposite breast, to create a bust of natural appearance, size, form, and feel. Carefully matching the type and size of the breast implants to the patient's pectoral soft-tissue characteristics reduces the incidence of revision surgery. Appropriate tissue matching, implant selection, and proper implantation technique, the re-operation rate was 3 percent at the 7-year-mark, compared with the re-operation rate of 20 per cent at the 3-year-mark, as reported by the U.S. Food and Drug Administration.\n\nSince the early 1990s, a number of independent systemic comprehensive reviews have examined studies concerning links between silicone gel breast implants and claims of systemic diseases. The consensus of these reviews (outlined below under Safety of Breast Implants heading) is that there has been no evidence of a causal link between the implantation of saline or silicone breast implants and systemic disease After investigating this issue, the U.S. FDA has concurred and since reaffirmed that “the weight of the epidemiological evidence published in the literature does not support an association between fibromyalgia and breast implants.”. A comprehensive systemic review by Lipworth (2011) concludes that \"any claims that remain regarding an association between cosmetic breast implants and CTDs are not supported by the scientific literature\".\n\nPlatinum is a catalyst used in the making of silicone implant polymer shells and other silicone devices used in medicine. The literature indicates that small amounts of platinum leaches (leaks) from these implants and is present in the surrounding tissue. The FDA reviewed the available studies from the medical literature on platinum and breast implants in 2002 and concluded there was little evidence suggesting toxicity from platinum in implant patients. The FDA revisited this study and additional literature several years later, reaffirming prior conclusions that platinum catalysts used in implants is likely not ionized and therefore would not represent a risk to women. \n\nThe FDA has identified that breast implants may be associated with a rare form of cancer called anaplastic large-cell lymphoma, believed to be associated with chronic bacterial inflammation. Similar ALCL phenomena have been seen with other types of medical implants including vascular access ports, orthopedic hip implants, and jaw (TMJ) implants. As of February 1, 2017, the FDA has received a total of 359 medical device reports of breast-implant-associated ALCL (BIALCL), including 9 deaths. Most cases of breast implant-associated ALCL had implants in for many years prior to the condition, and are usually treated successfully by simple removal of the implant and the capsule surrounding the implant without the need for chemotherapy if no evidence of systemic disease exists. If women with implants present with delayed swelling or fluid collection, cytologic studies and test for a marker \"CD30\" are suggested. The American Society of Plastic Surgery (ASPS) states, \"CD30 is the main diagnostic test that must be performed on the seroma fluid as routine pathology or H&E staining can frequently miss the diagnosis.\" Diagnosis and treatment of breast implant associated ALCL now follows standardized guidelines established by the National Comprehensive Cancer Network.\n\nThe current lifetime risk of BIA-ALCL in the U.S. is unknown, but estimates have ranged between estimated to be between 1 in 70,000 to 1 in 500,000 women with breast implants according to MD Anderson. Certain geographic locations have demonstrated variable risks. For instance, a December 2016 update from the Therapeutic Goods Administration of Australia and New Zealand reported a risk of 1:1,000 to 1:10,000 for textured implants.\". To date (2017), there has not been a case of BIAL reported where the patient had only implantation of smooth shell breast implants or a textured tissue expander that was exchanged for a smooth implant. The paucity of cases reported in Asian populations has raised the possibility that there may be a range of genetic susceptibility to the phenomena, or alternatively merely reflect differences in how cases are identified and reported.\n\nThe ASPS and the Plastic Surgery Foundation (PSF) have partnered with the FDA to study this condition and in doing so created the Patient Registry and Outcomes For breast Implants and anaplastic large cell Lymphoma Etiology and epidemiology (PROFILE). The United States FDA strongly encourages all physicians to report cases to PROFILE in an effort to better understand the role of breast implants in ALCL and the management of this disease.\n\nThe presence of breast implants currently presents no contraindication to breast feeding, and no evidence to support that the practice may present health issues to a breast feeding infant is recognized by the USFDA.\n\nWomen with breast implants may have functional breast-feeding difficulties; mammoplasty procedures that feature periareolar incisions are especially likely to cause breast-feeding difficulties. Surgery may also damage the lactiferous ducts and the nerves in the nipple-areola area.\n\nFunctional breast-feeding difficulties arise if the surgeon cut the milk ducts or the major nerves innervating the breast, or if the milk glands were otherwise damaged. Milk duct and nerve damage are more common if the incisions cut tissue near the nipple. The milk glands are most likely to be affected by subglandular implants (under the gland), and by large-sized breast implants, which pinch the lactiferous ducts and impede milk flow. Small-sized breast implants, and submuscular implantation, cause fewer breast-function problems; however, it is impossible to predict whether a woman who undergoes breast augmentation will be able to successfully breast feed since some women are able to breast-feed after periareolar incisions and subglandular placement and some are not able to after augmentation using submuscular and other types of surgical incisions.\n\nThe presence of radiologically opaque breast implants (either saline or silicone) might interfere with the radiographic sensitivity of the mammograph, that is, the image might not show any tumor(s) present. In this case, an Eklund view mammogram is required to ascertain either the presence or the absence of a cancerous tumor, wherein the breast implant is manually displaced against the chest wall and the breast is pulled forward, so that the mammograph can visualize a greater volume of the internal tissues; nonetheless, approximately one-third of the breast tissue remains inadequately visualized, resulting in an increased incidence of mammograms with false-negative results.\nThe breast cancer studies \" Cancer in the Augmented Breast: Diagnosis and Prognosis\" (1993) and \"Breast Cancer after Augmentation Mammoplasty\" (2001) of women with breast implant prostheses reported no significant differences in disease-stage at the time of the diagnosis of cancer; prognoses are similar in both groups of women, with augmented patients at a lower risk for subsequent cancer recurrence or death. Conversely, the use of implants for breast reconstruction \"after\" breast cancer mastectomy appears to have no negative effect upon the incidence of cancer-related death. That patients with breast implants are more often diagnosed with palpable—but not larger—tumors indicates that equal-sized tumors might be more readily palpated in augmented patients, which might compensate for the impaired mammogram images. The ready palpability of the breast-cancer tumor(s) is consequent to breast tissue thinning by compression, innately in smaller breasts \"a priori\" (because they have lesser tissue volumes), and that the implant serves as a radio-opaque base against which a cancerous tumor can be differentiated.\n\nThe breast implant has no clinical bearing upon lumpectomy breast-conservation surgery for women who developed breast cancer after the implantation procedure, nor does the breast implant interfere with external beam radiation treatments (XRT); moreover, the post-treatment incidence of breast-tissue fibrosis is common, and thus a consequent increased rate of capsular contracture. The study \"Breast Cancer Detection and Survival among Women with Cosmetic Breast Implants: Systematic Review and Meta-analysis of Observational Studies\", reported an average later stage in the diagnoses of women who developed breast cancer after undergoing breast augmentation, when compared to breast cancer patients who had not undergone breast augmentation, although this did not ultimately affect the patients prognosis. The use of implants for breast reconstruction \"after\" breast cancer mastectomy appears to have no negative effect upon the incidence of cancer-related death. \n\nIn 1988, twenty-six years after the 1962 introduction of breast implants filled with silicone gel, the U.S. Food and Drug Administration (FDA) investigated breast implant failures and the subsequent complications, and re-classified breast implant devices as Class III medical devices, and required from manufacturers the documentary data substantiating the safety and efficacy of their breast implant devices. In 1992, the FDA placed silicone-gel breast implants in moratorium in the U.S., because there was “inadequate information to demonstrate that breast implants were safe and effective”. Nonetheless, medical access to silicone-gel breast implant devices continued for clinical studies of post-mastectomy breast reconstruction, the correction of congenital deformities, and the replacement of ruptured silicone-gel implants. The FDA required from the manufacturers the clinical trial data, and permitted their providing breast implants to the breast augmentation patients for the statistical studies required by the U.S. Food and Drug Administration. In mid–1992, the FDA approved an adjunct study protocol for silicone-gel filled implants for breast reconstruction patients, and for revision-surgery patients. Also in 1992, the Dow Corning Corporation, a silicone products and breast implant manufacturer, announced the discontinuation of five implant-grade silicones, but would continue producing 45 other, medical-grade, silicone materials—three years later, in 1995, the Dow Corning Corporation went bankrupt when it faced large class action lawsuits claiming a variety of illnesses.\n\n\n\nThe U.S. Food and Drug Administration established the age ranges for women seeking breast implants; for breast reconstruction, silicone-gel filled implants and saline-filled implants were approved for women of all ages; for breast augmentation, saline implants were approved for women 18 years of age and older; silicone implants were approved for women 22 years of age and older. Because each breast implant device entails different medical risks, the minimum age of the patient for saline breast implants is different from the minimum age of the patient for silicone breast implants—because of the filler leakage and silent shell-rupture risks; thus, periodic MRI screening examinations are the recommended post-operative, follow-up therapy for the patient. In other countries, in Europe and Oceania, the national health ministries' breast implant policies do not endorse periodic MRI screening of asymptomatic patients, but suggest palpation proper—with or without an ultrasonic screening—to be sufficient post-operative therapy for most patients.\n\nIn the early 1990s, the national health ministries of the listed countries reviewed the pertinent studies for causal links among silicone-gel breast implants and systemic and auto-immune diseases. The collective conclusion is that there is no evidence establishing a causal connection between the implantation of silicone breast implants and either type of disease. The Danish study \"Long-term Health Status of Danish Women with Silicone Breast Implants\" (2004) reported that women who had breast implants for an average of 19 years were no more likely to report an excessive number of rheumatic disease symptoms than would the women of the control group. The follow-up study \"Mortality Rates Among Augmentation Mammoplasty Patients: An Update\" (2006) reported a decreased standardized mortality ratio and an increased risk of lung cancer death among breast implant patients, than among patients for other types of plastic surgery; the mortality rate differences were attributed to tobacco smoking. The study \"Mortality Among Canadian Women with Cosmetic Breast Implants\" (2006), about some 25,000 women with breast implants, reported a 43 per cent lower rate of breast cancer among them than among the general populace, and a lower-than-average risk of cancer.\n\n"}
{"id": "2733263", "url": "https://en.wikipedia.org/wiki?curid=2733263", "title": "CGMS-A", "text": "CGMS-A\n\nCopy Generation Management System – Analog (CGMS-A) is a copy protection mechanism for analog television signals. It consists of a waveform inserted into the non-picture Vertical Blanking Interval (VBI) of an analogue video signal. If a compatible recording device (for example, a DVD recorder) detects this waveform, it may block or restrict recording of the video content.\n\nIt is not the same as the broadcast flag, which is designed for use in digital television signals, although the concept is the same. There is a digital form of CGMS specified as CGMS-D which is required by the DTCP (\"5C\") protection standard.\n\nCGMS-A has been in existence since 1995, and has been standardized by various organizations including the IEC and EIA/CEA. It is used in devices such as PVRs/DVRs, DVD players and recorders, D-VHS, and Blu-ray recorders, as well certain television broadcasts. More recent TiVo firmware releases comply with CGMS-A signals.\n\nImplementation of CGMS-A is required for certain applications by DVD CCA license. D-VHS and some DVD recorders comply with CGMS-A signal on analog inputs. The technology requires minimal signal processing.\n\nWhere the source signal is analogue (e.g. VHS, analogue broadcast), the CGMS-A signalling may be present in that source.\n\nWhere the source signal is digital (e.g. DVD, digital broadcast), then the Copy Control Information (CCI) is carried in metadata in the digital transport or program stream, and a compliant hardware device (e.g. a DVD player) will read that data, and encode it into the analogue video signal generated within the device itself.\n\nThere is no blanket legal requirement for devices which record video to detect or act upon the CGMS-A information. For example, the DMCA \"does not require manufacturers of consumer electronics, telecommunications or computing equipment to design their products affirmatively to respond to any particular technological measure.\".\n\nCGMS-A is standardized through the IEC, CEA, EIA-J and ETSI as follows:\n\nIn all these standards, the CGMS-A information is only two out of many bits of information that are defined.\n\nOn 60 Hz systems (commonly known as \"NTSC\"), the system is highly extensible, though beyond the CGMS-A bits, only the aspect ratio of the video signal and the analogue protection system (APS) bits are commonly used. The signalling is typically present on every video frame, but CEA-805-D states that \"the transmission rate for any given packet type defined in CEA-805-D shall be at least once every three frames\", meaning that in theory for two out of three frames, different header values can be used to send data not defined in the standard. Type A signalling (20 bits in total; the only type defined for 480i) offers some extensibility by re-using the 14 data bits via one of the 14 undefined values for the four header bits. Type B signalling (134 bits in total) already defines bits to carry an Active Format Descriptor, Colorimetry, Redistribution Control, and a pixel-accurate definition of the location of any letterbox or pillarbox bars in the image, plus two bytes reserved for future use. Different header bit values may also be used for further extensibility.\n\nOn 50 Hz systems (commonly, though incorrectly known as \"PAL\"), the bits that are widely used and interpreted as CGMS-A are not named as such, and are added at the end of an existing signalling standard originally created for the PALplus video format (but still in common use in Europe in standard PAL video) called Widescreen signaling.\n\nSome references quote EIA-J CPR1204-1 as the authoritative reference for CGMS-A on 480p60 (525p) systems, since this was the first published standard to mention CGMS-A on 480p. This EIA-J document does not define the meaning of the bits, only their timing on the analogue video signal. The 480p signalling is based on the existing 480i standard but with a double speed clock, and IEC-61880-2 formalises this by defining bit meanings which are the same as for 480i. However, CEA-805 re-defines the aspect ratio signalling bits. Hence 480p Type A line 40 CGMMS-A data generated in accordance with CEA-805 cannot signal the aspect ratio of the video image, and in this way is incompatible with the same data generated in accordance with IEC-61880-2, and is no longer a straight \"double speed clock\" version of the 480i standard.\n\nCEA-805 is now on its fourth major version (CEA-805-D), and there have been errata issued to at least one version. CEA-805-D recognises that, in respect of Type B signalling, earlier versions of the standard were unclear regarding the \"order\" of bits as represented in the analogue video signal vs as used for the CRC calculation, and also \"which\" bits were to be used for the CRC calculation. Issue D requires sink devices to perform multiple CRC calculations for Type B signalling, taking account of various possible implementations in source devices. There is no such confusion surrounding Type A signalling.\n\nCEA-608-B specifies meaning of the 7-bit field placed on the data lines. The bits 4 and 3 contain the CGMS-A values, the bits 2 and 1 contain the Analog Protection System (APS) value, the bit 0 is the Analog Source Bit (ASB) specifying if the signal originates from a pre-recorded material, bits 5 and 6 are reserved.\n\nCGMS-A is signalled by 2 bits in the vertical blanking interval (VBI) signal of analog television broadcasts according to the following matrix:\n\n<nowiki>*</nowiki>CopyNoMore was not a part of the original standard. The 0,1 value originally was \"Reserved\".\n\nThe signal itself can be easily stripped by normalizing the VBI, e.g. using a video stabilizer to counter the side effects from Macrovision's manipulation of the VBI. CGMS-A VBI data is commingled or generally near captioning signals, so removal of CGMS-A will likely remove captioning as well.\n\nThe scheme can be made more robust by adding the Rights Assertion Mark (RAM); when the RAM is present but CGMS-A is not, copying is denied, turning the scheme into a permission-based one. The RAM can be encoded by using the VEIL technology.\n\n"}
{"id": "3349094", "url": "https://en.wikipedia.org/wiki?curid=3349094", "title": "Carbon filtering", "text": "Carbon filtering\n\nCarbon filtering is a method of filtering that uses a bed of activated carbon to remove contaminants and impurities, using chemical adsorption.\n\nEach particle, or granule, of carbon provides a large surface area, or pore structure, allowing contaminants the maximum possible exposure to the active sites within the filter media. One gram of activated carbon has a surface area in excess of 3,000 m2 (32,000 sq ft).\n\nActivated carbon works via a process called adsorption, whereby pollutant molecules in the fluid to be treated are trapped inside the pore structure of the carbon substrate. Carbon filtering is commonly used for water purification, air filtering and industrial gas processing, for example the removal of siloxanes and hydrogen sulfide from biogas. It is also used in a number of other applications, including respirator masks, the purification of sugarcane and in the recovery of precious metals, especially gold. It is also used in cigarette filters.\n\nActive charcoal carbon filters are most effective at removing chlorine, particles such as sediment, volatile organic compounds (VOCs), taste and odor from water. They are not effective at removing minerals, salts, and dissolved inorganic substances.\n\nTypical particle sizes that can be removed by carbon filters range from 0.5 to 50 micrometres. The particle size will be used as part of the filter description. The efficacy of a carbon filter is also based upon the flow rate regulation. When the water is allowed to flow through the filter at a slower rate, the contaminants are exposed to the filter media for a longer amount of time.\n\nFor small-scale production of hydrogen, water purifiers are installed to prevent formation of minerals on the surface of the electrodes and to remove organics and chlorine from utility water. First the water passes through a 20 micrometer interference (mesh or screen filter) filter to remove sand and dust particles, second, a charcoal filter (activated carbon) to remove organics and chlorine, third stage, a de-ionizing filter to remove metallic ions. A test can be done before and after the filter for proper functioning on barium, calcium, potassium, magnesium, sodium, and silicon.\n\n\n"}
{"id": "27171047", "url": "https://en.wikipedia.org/wiki?curid=27171047", "title": "Center for Electronic Governance at UNU-IIST", "text": "Center for Electronic Governance at UNU-IIST\n\nThe Center for Electronic Governance at UNU-IIST is an International Center of Excellence on research and practice in Electronic Governance, part of United Nations University - International Institute for Software Technology located in Macao, China.\n\nEstablished in 2007, the Center has been built upon the contribution of UNU-IIST to the eMacao Project (2004–2006) and eMacao Program (2007-now), a collaborative initiative to build and utilize a foundation for Electronic Government in Macao SAR. Since 2010, it has become an official programme of UNU-IIST.\n\nThe mission of the Center for Electronic Governance at UNU-IIST is to support governments in developing countries in strategic use of technology to transform the working of public organizations and their relationships with citizens, businesses, civil society, and with one another.\n\nActivities at the Center include applied and policy research, capacity building and various forms of development – strategy development, software development, institutional development and development of communities of practice.\n\nThe Center focuses its research on various aspects of Electronic Governance:\n\nThe Center regularly organizes and conducts schools, seminars, lectures and presentations for government leaders, managers, researchers, educators, etc. on various aspects of Electronic Governance. Various courses and presentation materials from these events are available.\n\nHere is a selection of projects currently executed by the Center:\n\n\nThe Center engages in Electronic Governance research, practice and the interaction between them. It works to transform the experience, insights and results obtained through this interaction into instruments and capacity for development.\n\nThe Center established in 2007 and since then leads the organization of a series of International Conferences on Electronic Governance (ICEGOV), with the first three editions in Macao (ICEGOV2007), Cairo (ICEGOV2008) and Bogota (ICEGOV2009). The fourth edition (ICEGOV2010) will take place in Beijing, China during 25–28 October 2010.\n\nAcademic publications and other outputs such as policy and technical reports, software, courseware, etc. produced by the Center are available here.\n\n"}
{"id": "409665", "url": "https://en.wikipedia.org/wiki?curid=409665", "title": "Clipper chip", "text": "Clipper chip\n\nThe Clipper chip was a chipset that was developed and promoted by the United States National Security Agency (NSA) as an encryption device that secured “voice and data messages\" with a built-in backdoor. It was intended to be adopted by telecommunications companies for voice transmission. It can encipher and decipher messages. It was part of a Clinton Administration program to “allow Federal, State, and local law enforcement officials the ability to decode intercepted voice and data transmissions.\" “Each clipper chip ha[d] a unique serial number and a secret ‘unit key,’ programmed into the chip when manufactured.\" This way, each device was meant to be different from the next. \nIt was announced in 1993 and by 1996 was entirely defunct.\n\nThe Clipper chip used a data encryption algorithm called Skipjack to transmit information and the Diffie–Hellman key exchange-algorithm to distribute the cryptokeys between the peers. Skipjack was invented by the National Security Agency of the U.S. Government; this algorithm was initially classified SECRET, which prevented it from being subjected to peer review from the encryption research community. The government did state that it used an 80-bit key, that the algorithm was symmetric, and that it was similar to the DES algorithm. The Skipjack algorithm was declassified and published by the NSA on June 24, 1998. The initial cost of the chips was said to be $16 (unprogrammed) or $26 (programmed), with its logic designed by Mykotronx, and fabricated by VLSI Technology, Inc (see the VLSI logo on the image on this page).\n\nAt the heart of the concept was key escrow. In the factory, any new telephone or other device with a Clipper chip would be given a cryptographic key, that would then be provided to the government in escrow. If government agencies \"established their authority\" to listen to a communication, then the key would be given to those government agencies, who could then decrypt all data transmitted by that particular telephone. The newly formed Electronic Frontier Foundation preferred the term \"key surrender\" to emphasize what they alleged was really occurring.\n\nThe Clinton Administration argued that the Clipper chip was essential for law enforcement to keep up with the constantly progressing technology in the United States. While many believed that the device would act as an additional way for terrorists to receive information, the Clinton Administration said it would actually increase national security. They argued that because “terrorists would have to use it to communicate with outsiders — banks, suppliers, and contacts — the Government could listen in on those calls.”\n\nOrganizations such as the Electronic Privacy Information Center and the Electronic Frontier Foundation challenged the Clipper chip proposal, saying that it would have the effect not only of subjecting citizens to increased and possibly illegal government surveillance, but that the strength of the Clipper chip's encryption could not be evaluated by the public as its design was classified secret, and that therefore individuals and businesses might be hobbled with an insecure communications system. Further, it was pointed out that while American companies could be forced to use the Clipper chip in their encryption products, foreign companies could not, and presumably phones with strong data encryption would be manufactured abroad and spread throughout the world and into the United States, negating the point of the whole exercise, and, of course, materially damaging U.S. manufacturers en route. Then-Senators John Ashcroft and John Kerry were opponents of the Clipper chip proposal, arguing in favor of the individual's right to encrypt messages and export encryption software.\n\nThe release and development of several strong cryptographic software packages such as Nautilus, PGP and PGPfone was in response to the government push for the Clipper chip. The thinking was that if strong cryptography was freely available on the internet as an alternative, the government would be unable to stop its use.\n\nIn 1994, Matt Blaze published the paper \"Protocol Failure in the Escrowed Encryption Standard\". It pointed out that the Clipper's escrow system has a serious vulnerability: the chip transmitted a 128-bit \"Law Enforcement Access Field\" (LEAF) that contained the information necessary to recover the encryption key. To prevent the software that transmitted the message from tampering with the LEAF, a 16-bit hash was included. The Clipper chip would not decode messages with an invalid hash; however, the 16-bit hash was too short to provide meaningful security. A brute-force attack would quickly produce another LEAF value that would give the same hash but not yield the correct keys after the escrow attempt. This would allow the Clipper chip to be used as an encryption device, while disabling the key escrow capability. In 1995 Yair Frankel and Moti Yung published another attack which is inherent to the design and which shows that the key escrow device tracking and authenticating capability (namely, the LEAF) of one device, can be attached to messages coming from another device and will nevertheless be received, thus bypassing the escrow in real time. In 1997, a group of leading cryptographers published a paper, \"The Risks of Key Recovery, Key Escrow, and Trusted Third-Party Encryption\", analyzing the architectural vulnerabilities of implementing key escrow systems in general, including but not limited to the Clipper chip Skipjack protocol. The technical flaws described in this paper were instrumental in the demise of the Clipper chip as a public policy option. While many leading voices in the computer science community expressed opposition to the Clipper chip and key recovery in general, some supported the concept, including Dorothy E. Denning.\n\nThe Clipper chip was not embraced by consumers or manufacturers and the chip itself was no longer relevant by 1996. The U.S. government continued to press for key escrow by offering incentives to manufacturers, allowing more relaxed export controls if key escrow were part of cryptographic software that was exported. These attempts were largely made moot by the widespread use of strong cryptographic technologies, such as PGP, which were not under the control of the U.S. government.\n\nHowever, strongly encrypted voice channels are still not the predominant mode for current cell phone communications. Secure cell phone devices and smartphone apps exist, but may require specialized hardware, and typically require that both ends of the connection employ the same encryption mechanism. Such apps usually communicate over secure Internet pathways (e.g. ZRTP) instead of through phone voice data networks.\n\nFollowing the Snowden disclosures from 2013, Apple and Google announced that they would lock down all data stored on their smartphones with encryption, in such a way that Apple and Google themselves could not break the encryption even if ordered to do so with a warrant. This prompted a strong reaction from the authorities, with one of the more iconic responses being the chief of detectives for Chicago’s police department stating that \"Apple['s iPhone] will become the phone of choice for the pedophile\". Washington Post posted an editorial insisting that \"smartphone users must accept that they cannot be above the law if there is a valid search warrant\", and after agreeing that backdoors would be undesirable, suggested implementing a \"golden key\" backdoor which would unlock the data with a warrant. The members of \"The Risks of Key Recovery, Key Escrow, and Trusted Third-Party Encryption\" 1997 paper, as well as other researchers at MIT, wrote a follow-up article in response to the revival of this debate, arguing that mandated government access to private conversations would be an even worse problem now than twenty years ago.\n\n\n"}
{"id": "24219490", "url": "https://en.wikipedia.org/wiki?curid=24219490", "title": "Cylinder manifold", "text": "Cylinder manifold\n\nA cylinder manifold is a group of large gas cylinders, commonly used to supply gases via a pipeline to a building such as a hospital. Cylinders are often arranged into two groups; a primary and secondary group. In a hospital, size J cylinders are commonly used, which are capable of holding 6800 litres of oxygen each or 13,600 of nitrous oxide. \n\nInitially, the gas is used up from the primary group first, with gas being expended equally from all cylinders, as they are connected in parallel through a common outlet. Once the levels in the cylinders are sufficiently low, a pressure transducer switches to the secondary manifold; allowing the primary manifold to be replenished. Manifolds are used to supply nitrous oxide, Entonox, air or oxygen; although a vacuum insulated evaporator is more commonly used to store oxygen.\n\nGas manifolds should be stored in an area separate from the main building. It should not be exposed to the environment and should be well ventilated.\n"}
{"id": "25323835", "url": "https://en.wikipedia.org/wiki?curid=25323835", "title": "Czech Technical University in Prague", "text": "Czech Technical University in Prague\n\nCzech Technical University in Prague (CTU, ) is one of the largest universities in the Czech Republic, and is one of the oldest institutes of technology in Central Europe. It is also the oldest non-military technical university in Europe and the best technical university in the Czech Republic.\n\nIn the academic year 2017/18 Czech Technical University offered 128 degree programs in Czech and 87 in English.\n\nIt was established as the Institute of Engineering Education in 1707, but not as a tertiary university but only secondary education (high school), by Emperor Joseph I as a response to Christian Josef Willenberg's petition addressed to preceding emperor Leopold I. In 1806 the institute of Engineering Education was transformed into Prague Polytechnical Institute (or Prague Polytechnic), when the university studies began. After the disintegration of the Austro-Hungarian Empire, the name of the school was changed in 1920 to the Czech Technical University in Prague.\n\nIn 1705, asked Emperor Leopold I for permission to teach \"the art of engineering\". Later, the emperor's only son, who succeeded him on the throne in 1707 as Joseph I, ordered the Czech state of Prague to provide engineering education. Due to various reasons the request was avoided long periods of time but in October 1716 Willenberg repeated the request and finally on 9 November 1717 a decree by the Czech state granted Willenberg the professorship (first engineering professorship in Central Europe) and on 7 January 1718 he began teaching. Initially, Willenberg started teaching only 12 students in his own apartment (six barons , four knights and two burghers), but gradually students proliferated (in 1779 there were around 200) and they started studying in more suitable premises. Initially, the training focused mainly on the military. Teaching in the first year lasted one hour per day in the second year almost two.\n\nThe successor of prof. Willenberg was Johann Ferdinand Schor, builder of hydraulic structures in the basin of the Vltava and author of textbooks used at the school of mathematics. He began under Willenberg's leadership by teaching optics, perspectivity, technical drawing and geography. The third was professor František Antonín Herget, who mainly focused on civil engineering, particularly construction.\n\nIn September 1776 Maria Theresa allowed Herget to use the Clementinum building; in 1786 the school moved to the new and better building.\n\nIn 1787 the School of Engineering was established at the decree of Emperor Joseph II.\n\nThe CTU is the best technical university in the Czech Republic. In 2010, in the world rating of THES-QS universities in the category of technical sciences, the CTU took the 121st place, in the category of natural sciences – 246th place. In 2018 Czech Technical University was ranked as 220th in Engineering and Technology in the QS World University Rankings.\nStudents apply to faculty. Each faculty has different admissions requirements. Acceptance rate ranges from 52.32% (Faculty of Information Technology) to 81.51% (Faculty of Transportation Sciences). The percentage of international students grew from 2.5% in 2000 to 16.4% in 2017.\n\nDue to the pace and difficulty of CTU coursework, high percentage of students fail to complete first year of their studies. First year failure rates range from 23% (Faculty of Civil Engineering) to 47% (Faculty of Information Technology). Overall, only 48% of undergraduate students end up graduating.\n\nCTU has international agreements with 484 foreign universities. Many of them are ranked in the first hundred in QS World University Rankings such as National University of Singapore, Nanyang Technological University, Purdue University, Korea Institute of Science and Technology (KAIST), Hong Kong University of Science and Technology, Technical University of Munich, Delft University of Technology or KU Leuven.\n\nCTU has many bilateral agreements with universities outside of Europe. The most sought after universities are from Canada, Australia, Singapore, USA and Japan. That said, every year many students choose to study in attractive destinations such as Argentina, Brazil, China, Hong Kong, Indie, Indonesia, South Africa, South Korea, Costa Rica, Mexico, New Zaeland, Peru, Russia or Taiwan.\n\nCTU also participates in the European programmes Erasmus and Leonardo.\nCTU has currently over 3500 international students from 117 countries. About 750 of them are an exchange students. One of the organizations that takes care of international students is International Student Club (ISC), which organizes Buddy Programme and extra-curricular activities.\n\nCTU has currently 21 agreements with universities such as Technical University of Munich, RWTH Aachen or Trinity College Dublin.\n\n\n\n\n\nStudent clubs within the CTU are integrated in the Student Union. It has 27 members and covers wide range of free time activities, with biggest club called Silicon Hill. The Student Union also organizes social events for students through the year.\n\n\n"}
{"id": "11296918", "url": "https://en.wikipedia.org/wiki?curid=11296918", "title": "Discovery shopping", "text": "Discovery shopping\n\nDiscovery shopping (also known as discovery shopping search) is a type of online shopping that emphasizes the browsing aspects of the shopping experience. Discovery shopping search offers shoppers guided queries for more personalized results. The goal is to recreate the experience of live shopping as a leisure activity, where the items are selected by sampling or viewing a variety of similar or related goods. This is sometimes referred to as window shopping.\n\nUnlike a Comparison Shopping Engine which evaluates prices and feature sets for identical or very closely related products, discovery shopping enables users to tailor product results to suit their preferences. To achieve this experience online, discovery shopping sites offer features such as specifying styles, colors and brands, showing similar items, and displaying results in a visually engaging format. Such tools allow shoppers to narrow down from a large to number of choices to a set of products that they find appealing. Comprehensiveness and relevancy are also critical factors, since choice and accuracy increase a shopper's chances of finding a product they wish to purchase.\n\nDiscovery search was pegged as a hot trend for 2007, according to a recent report from Forrester Research.\n\nAccording to Silicon Valley strategy consultant Sramana Mitra, examples of discovery shopping sites include TheFind.com, Listar, Dealply, Mutex.me, ShopSultry.com, and ShopStyle.com.\n\n"}
{"id": "41215364", "url": "https://en.wikipedia.org/wiki?curid=41215364", "title": "Elektronika VM-12", "text": "Elektronika VM-12\n\nElektronika VM-12 was the first Soviet VHS-compatible videocassette recorder. It was capable to record SECAM-IIIB D/K (OIRT), PAL and black-and-white video on a 12,65-mm wide magnetic tape.\n\nElektronika VM-12 was 480х367х136 mm in size and weighted 10 kg.\n\nSP - 2,339±0,5%\n"}
{"id": "3541053", "url": "https://en.wikipedia.org/wiki?curid=3541053", "title": "Emergency rations", "text": "Emergency rations\n\nEmergency rations are items of food and drink that a person stores and relies on in case of an emergency. Emergency food supplies can be purchased for camping trips or wilderness adventures. These supplies are meant to last for several days. Many people also purchase long shelf life emergency food in case of natural disasters or other emergency situations. The food can come in the form of a powder, freeze dried, smoked or salted. The rations are to help people survive until help arrives and are often carried while hill walking or mountaineering, because of the risk of being stranded by an accident. In some organised events, such as Ten Tors, it is obligatory to carry emergency rations.\n\nEmergency rations are often carried by camping enthusiasts, especially back-pack campers, who are more likely to be far from food supplies. Emergency foodstuffs are usually high in caloric content, and sometimes also in nutritional content. Typical emergency foodstuffs are high-calorie foods such as candy bars, nutritional bars, energy or sports bars, hard bread or biscuit, dried meat (such as jerky), and dried fruit. If water is available, rations with little water content are lighter to carry.\n\nEmergency rations are generally carried on the person by people on foot in case of becoming lost or separated from normal food supplies. Water or other drinks are carried if water is not readily available from the environment.\n"}
{"id": "33042712", "url": "https://en.wikipedia.org/wiki?curid=33042712", "title": "Eyelash perm", "text": "Eyelash perm\n\nEyelash permanent wave, or more commonly called an eyelash perm, and may also refer to permanent relaxer that straightens the hair is a cosmetics procedure performed only by licensed Cosmetologists to flip up eyelashes using hair perming technology.\n\nLashes are treated with a perm solution which is then followed by the neutralizer and sometimes depending on which system you use will have a nourishing oil as the last step to recondition and strengthen the lashes. Each being left on for about ten to fifteen minutes. Traditionally when lash perming was introduced a soft like sponge in the shape of a rod would be placed at the base of the lashes where the lashes would take on its new shape. Although, in recent years people have now started using a silicone pad that is placed on your eyelid. There is small, medium, or large silicone pads depending on the natural length of your lashes or how curly you would like them to be. This new form or perming lashes compared to the original sponge rod creates more of a natural shape and curve to the lashes. The original way of perming lashes which is still done oftentimes gives less of a natural curve or shape to the lash.\n\nAnother way to perm lashes is done by using a plastic instrument that looks a lot like a lash curler. This plastic instrument is then placed at the base of your lashes and then the instrument pinches them up wards the way a lash curler works. It rests on your eye for support and the rest of the procedure is performed the same way you would with rods or silicone pads.\n\nOnce your lashes have been applied (by a special glue that does not harm the lashes) to the perm rod or silicone pad the perming solution is applied to the upturned lash, and then it is left to set for ten to fifteen minutes, followed by the neutralizer which is also left on for ten to fifteen minutes. In certain systems there will be a nourishing oil as the last step, this nourishing oil also helps to break up the glue that was used to place the lashes on the silicone pad making it easier for the removal of the silicone pad. Once the silicone pad has been removed the eyelids and lashes are cleansed of any residue left over. The over all procedure takes about 30–45 minutes to perform.\n\nThe results may lasts for two to three months. The chemical reaction breaks and reforms the bonds of the hair. This reaction, softening the lash's inner structure between the protein chains of the hair. The hair swells, stretches and softens, then is molded around the shape of the perm rod or silicone pad to form a curl.\n\nThe procedure is performed in a salon or spa by a trained, licensed and certified individual. Every state requires individuals performing the procedure to have a valid Cosmetology license.\n\nHistorically, to achieve an upturned look, lash curlers and mascara were used. The first eyelash curler was invented by the Kurlash company. An early alternative method for curling hair that was suitable for use on people was invented in 1905 by German hairdresser Charles Nessler. Eyelash perming developed as a widely available solution sometime after 2000.\n\nDue to the chemicals involved, like any procedure, there is some risk. Side effects of the procedure have been known to include stinging around the eyes. The U.S. Food and Drug Administration (FDA), while approving of hair treatment, has not yet approved the procedure for use around the eyes.\n\n\n"}
{"id": "33451994", "url": "https://en.wikipedia.org/wiki?curid=33451994", "title": "F.B.T. Productions, LLC v. Aftermath Records", "text": "F.B.T. Productions, LLC v. Aftermath Records\n\nF.B.T. Productions, LLC, et al. v. Aftermath Records, et al. 621 F.3d 958 was a case in which the United States Court of Appeals for the Ninth Circuit dealt with how Federal Copyright Law applied to the sales and licensing contracts of music downloads and other downloadable copyrighted material. Specifically, the circuit court ruled that a licensing provision in the contract between F.B.T. Productions and Aftermath Records unambiguously applied to permanent downloads and mastertones offered through third party distributors. After reviewing the First Sale Doctrine and the nature of Aftermath's contracts with its distributors, the circuit court concluded that such downloads constituted a licensing of copyrights rather than a sale, causing Aftermath to pay higher royalties to F.B.T. under their agreement.\n\nIn 1995 Plaintiff F.B.T. Productions, LLC (\"FBT\") signed fellow Plaintiff, rap artist Marshall Bruce Mathers III (stage name Eminem), to a recording contract. Subsequent contract agreements in 1998 and 2000 between Plaintiffs and Defendant Aftermath Records (\"Aftermath\"), a subsidiary of Interscope Records, UMG Recordings, Inc., and Ary, Inc., allowed Aftermath the right to distribute recordings of Eminem, and then ultimately transferred all exclusive rights of Eminem's recordings to Aftermath. In exchange for these rights, the agreements provided that Aftermath pay FBT royalties between 12% - 20% of the retail price of copies of Eminem's records sold (\"Records Sold\" provision). Furthermore, the agreements provided that FBT receive 50% of the net revenue Aftermath obtained by licensing out the use of Eminem's master recordings (\"Masters Licensed\" provision).\n\nBeginning in 2001, Defendants licensed various third parties the right to distribute over the Internet, recordings of Eminem in the form of music downloads and ringtones. These parties included, but were not limited to, the iTunes music store, Sprint, Nextel, Cingular, and T-Mobile.\n\nIn 2005 auditors hired by Plaintiffs allegedly found that Defendants were remitting to Plaintiffs royalties for music downloads and ringtones under the lower percentage \"Records Sold\" provision of their agreement. Based on these findings, Plaintiffs filed complaints in 2007 and 2008, alleging breach of contract and motioned for summary judgment. Plaintiffs claimed that language in the record contracts stipulated that music downloads and ringtones be classified not as \"Records Sold\", but instead as the higher percentage \"Masters Licensed\", and that these higher royalties be remitted to FBT as such.\n\nUnited States District Judge Philip S. Gutierrez denied Plaintiffs' motion for summary judgment on the grounds that the contract agreements were too ambiguous as to how to calculate royalties for downloads and ringtones. Based on the third party agreements, the court could not definitively conclude if downloads were licenses. Also, neither party was able to establish any prevailing industry custom as to how downloads and ringtones were traditionally calculated. In lieu of conclusive information, the court took into consideration that the agreements gave the Defendants the right to sell recordings, \"...in any [or] all forms of media now known and hereinafter developed...\" and that Plaintiffs never objected to the lower royalty rate until after the audit, therefore providing insight as to Plaintiffs' original intent. As such, the court ruled that both parties' reasonable expectations for download and ringtone royalties at the time of the agreement was that of a sale.\n\nAccordingly, the issue went to trial where a jury, \"...returned a verdict in favor of Aftermath, and the district court awarded Aftermath its attorneys’ fees of over $2.4 million.\"\n\nUpon appeal by Plaintiffs to the US Court of Appeals for the Ninth Circuit Decision, \"[t]he judgment in favor of defendants was reversed, the district court's order granting attorneys' fees to defendants was vacated, and the case was remanded for further proceedings.\"\n\nIn its reversal, the circuit court emphasized its right to review the district court's denial of Plaintiffs' motion for summary judgment, as the district court's \"determination of whether an ambiguity exists remains 'a question of law, subject to independent review on appeal.'\" Thus it started by reiterating the district court's conclusion that since either the \"Records Sold\" or the \"Masters Licensed\" provision could apply to the downloads, that neither the Plaintiffs' nor the Defendant's motions for summary judgment could be granted due to an inherent ambiguity. However, the circuit court pointed out the specific use of the word \"notwithstanding,\" explaining that the provision states \"notwithstanding\" the \"Records Sold\" provision, the \"Masters Licensed\" provision would apply. The circuit court interpreted this to mean that the \"Masters Licensed\" provision had overarching scope, concluding that no ambiguity existed.\n\nA contractual term is not ambiguous just because it is broad. Here, the Masters Licensed provision explicitly applies to (1) masters (2) that are licensed to third parties for the manufacture of records “or for any other uses,” (3) “notwithstanding” the Record Sold provision. This provision is admittedly broad, but it is not unclear or ambiguous.\nThe court then discussed whether or not Aftermath licensed the music to third party distributors, making extensive use of the Copyright Act of 1976, including references to sections , , and the First-sale doctrine as expressed in . The court reaffirmed the fundamental differences between a sale and a license, particularly pointing out that \"a 'sale' of a work may either be a transfer in title of an individual copy of a work, or a sale of all exclusive intellectual property rights in a work.\" Relying heavily on the Supreme Court's interpretation of these statutes, the court of appeals thus ruled that Defendants' dealings with third parties were license agreements and not sales. This was mainly based on the fact that Defendants' transfer of copyrighted material to third parties did not include ownership title of copyrighted material, and that Defendants reserved the right to reclaim copyrighted material at any time, therefore no sale was made.\n\nFurthermore, the court established that the copyrighted material transferred to the third parties qualified as a master recording due to the quality and fidelity of the recordings. Aftermath had also argued that F.B.T. failed to complain about the lower royalty rates until the audit in 2006, claiming this demonstrated acquiescence on the part of the Plaintiff. The court ruled against this as well, determining that such actions by the Plaintiff were reasonable and contributed nothing to their intentions, stating, \"F.B.T. had no obligation to audit the statements any earlier than it did, and it immediately raised the issue with Aftermath after the audit.\" The court therefore concluded that the Defendants owed Plaintiffs a royalty rate of 50% for downloads and ringtones under the \"Masters Licensed\" provision.\n\nThe Defendants petitioned the Supreme Court of the United States, seeking a review of the Ninth Circuit's judgment. In March 2011 the US Supreme Court denied the Defendants' petition for writ of certiorari.\n\n"}
{"id": "338191", "url": "https://en.wikipedia.org/wiki?curid=338191", "title": "FLOX", "text": "FLOX\n\nFLOX is a combustion process developed by \"WS Wärmeprozesstechnik GmbH\".\n\nIn April 1990 Joachim Alfred Wünning made experiments with industrial gasoline engines thereby observing that beyond a temperature of 850 °C the flames were blown away. The German word \"flammenlose Oxidation\" (flameless oxidation) is the origin of the name for the FLOX-Technology that was derived from the effect. Its specific advantages led to additional funding including a project \"FLoxCoal\" at the Stuttgart University to engineer a prototype of an atomizing burner for coal without flame reactions.\n\nThe NO formation is specifically located in the flame front so that by suppressing peak flame the NO emission can be reduced theoretically to zero. In the practical implementations in current burner technology a reduction of 20% was observed for Rhenisch brown coal and 65% for Polish black coal. \n\nThe role of combustion temperature in NO formation has been understood for some time. Reduction of the combustion temperature in gasoline engines, by reducing the compression ratio, was among the first steps taken to comply with the U.S. clean air act in the 1970s. This lowers the NO emissions by lowering the temperature at the flame front.\n\n"}
{"id": "8831257", "url": "https://en.wikipedia.org/wiki?curid=8831257", "title": "Fermented bean paste", "text": "Fermented bean paste\n\nFermented bean paste is a category of fermented foods typically made from ground soybeans, which are indigenous to the cuisines of East and Southeast Asia. In some cases, such as the production of \"miso\", other varieties of beans such as broad beans, may also be used.\n\nThe pastes are usually salty and savory, but may also be spicy, and are used as a condiment to flavor foods such as stir-fries, stews, and soups. The colours of such pastes range from light tan to reddish brown and dark brown. The differences in colour are due to different production methods, such as the conditions of fermentation, the addition of wheat flour, pulverized mantou, rice, or sugar and the presence of different microflora, such as bacteria or molds used in their production, as well as whether the soybeans are roasted (as in \"chunjang\") or aged (as in \"tauchu\") before being ground.\n\nFermented bean pastes are sometimes the starting material used in producing soy sauces, such as tamari, or an additional product created from the same fermented mass.\n\nDue to the protein content of the beans, the fermentation process releases a large amount of free amino acids, which when combined with the large amounts of salt used in its production, produces a highly umami product. This is particularly true with miso, which can be used as the primary ingredient in certain dishes, such as miso soup.\n\nVarious types of fermented bean paste (all of which are based on soy and cereal grains) include:\n\n"}
{"id": "37852", "url": "https://en.wikipedia.org/wiki?curid=37852", "title": "Fusion rocket", "text": "Fusion rocket\n\nA fusion rocket is a theoretical design for a rocket driven by fusion propulsion which could provide efficient and long-term acceleration in space without the need to carry a large fuel supply. The design relies on the development of fusion power technology beyond current capabilities, and the construction of rockets much larger and more complex than any current spacecraft. A smaller and lighter fusion reactor might be possible in the future when more sophisticated methods have been devised to control magnetic confinement and prevent plasma instabilities. Inertial fusion could provide a lighter and more compact alternative, as might a fusion engine based on an FRC.\n\nFor space flight, the main advantage of fusion would be the very high specific impulse, and the main disadvantage the (likely) large mass of the reactor. However, a fusion rocket may produce less radiation than a fission rocket, reducing the mass needed for shielding. The surest way of building a fusion rocket with current technology is to use hydrogen bombs as proposed in Project Orion, but such a spacecraft would also be massive and the Partial Nuclear Test Ban Treaty prohibits the use of nuclear bombs. Therefore, the use of nuclear bombs to propel rockets on Earth is problematic, but possible in space in theory. An alternate approach would be electrical (e.g. ion) propulsion with electric power generation via fusion power instead of direct thrust.\n\nMany spacecraft propulsion methods such as ion thrusters require an input of electric power to run but are highly efficient. In some cases their maximum thrust is limited by the amount of power that can be generated (for example, a mass driver). An electric generator that ran on fusion power could be installed purely to drive such a ship. One disadvantage is that conventional electricity production requires a low-temperature energy sink, which is difficult (i.e. heavy) in a spacecraft. Direct conversion of the kinetic energy of the fusion products into electricity is in principle possible and would mitigate this problem.\n\nAn attractive possibility is to simply direct the exhaust of fusion product out the back of the rocket to provide thrust without the intermediate production of electricity. This would be easier with some confinement schemes (e.g. magnetic mirrors) than with others (e.g. tokamaks). It is also more attractive for \"advanced fuels\" (see aneutronic fusion). Helium-3 propulsion is a proposed method of spacecraft propulsion that uses the fusion of helium-3 atoms as a power source. Helium-3, an isotope of helium with two protons and one neutron, could be fused with deuterium in a reactor. The resulting energy release could be used to expel propellant out the back of the spacecraft. Helium-3 is proposed as a power source for spacecraft mainly because of its abundance on the moon. Currently, scientists estimate that there are 1 million tons of helium-3 present on the moon, mainly due to solar wind colliding with the moon's surface and depositing it, among other elements, into the soil. Only 20% of the power produced by the D-T reaction could be used this way; the other 80% is released in the form of neutrons which, because they cannot be directed by magnetic fields or solid walls, would be very difficult to use for thrust. Helium-3 is also produced via beta decay of tritium, which in turn can be produced from deuterium, lithium, or boron.\n\nEven if a self-sustaining fusion reaction cannot be produced, it might be possible to use fusion to boost the efficiency of another propulsion system, such as a VASIMR engine.\n\nTo sustain a fusion reaction, the plasma must be confined. The most widely studied configuration for terrestrial fusion is the tokamak, a form of magnetic confinement fusion. Currently tokamaks weigh a great deal, so the thrust to weight ratio would seem unacceptable. NASA's Glenn Research Center has proposed a small aspect ratio spherical torus reactor for its \"Discovery II\" conceptual vehicle design. \"Discovery II\" could deliver a manned 172 000-kilogram payload to Jupiter in 118 days (or 212 days to Saturn) using 861 metric tons of hydrogen propellant, plus 11 metric tons of Helium-3-Deuterium (D-He3) fusion fuel. The hydrogen is heated by the fusion plasma debris to increase thrust, at a cost of reduced exhaust velocity (348–463 km/s) and hence increased propellant mass.\n\nThe main alternative to magnetic confinement is inertial confinement fusion (ICF), such as that proposed by Project Daedalus. A small pellet of fusion fuel (with a diameter of a couple of millimeters) would be ignited by an electron beam or a laser. To produce direct thrust, a magnetic field would form the pusher plate. In principle, the Helium-3-Deuterium reaction or an aneutronic fusion reaction could be used to maximize the energy in charged particles and to minimize radiation, but it is highly questionable whether it is technically feasible to use these reactions. Both the detailed design studies in the 1970s, the Orion drive and Project Daedalus, used inertial confinement. In the 1980s, Lawrence Livermore National Laboratory and NASA studied an ICF-powered \"Vehicle for Interplanetary Transport Applications\" (VISTA). The conical VISTA spacecraft could deliver a 100-tonne payload to Mars orbit and return to Earth in 130 days, or to Jupiter orbit and back in 403 days. 41 tonnes of deuterium/tritium (D-T) fusion fuel would be required, plus 4,124 tonnes of hydrogen expellant. The exhaust velocity would be 157 km/s.\n\nMagnetized target fusion (MTF) is a relatively new approach that combines the best features of the more widely studied magnetic confinement fusion (i.e. good energy confinement) and inertial confinement fusion (i.e. efficient compression heating and wall free containment of the fusing plasma) approaches. Like the magnetic approach, the fusion fuel is confined at low density by magnetic fields while it is heated into a plasma, but like the inertial confinement approach, fusion is initiated by rapidly squeezing the target to dramatically increase fuel density, and thus temperature. MTF uses \"plasma guns\" (i.e. electromagnetic acceleration techniques) instead of powerful lasers, leading to low cost and low weight compact reactors. The NASA/MSFC Human Outer Planets Exploration (HOPE) group has investigated a manned MTF propulsion spacecraft capable of delivering a 163933-kilogram payload to Jupiter's moon Callisto using 106-165 metric tons of propellant (hydrogen plus either D-T or D-He3 fusion fuel) in 249–330 days. This design would thus be considerably smaller and more fuel efficient due to its higher exhaust velocity (700 km/s) than the previously mentioned \"Discovery II\", \"VISTA\" concepts.\n\nAnother popular confinement concept for fusion rockets is inertial electrostatic confinement (IEC), such as in the Farnsworth-Hirsch Fusor or the Polywell variation being researched by the Energy-Matter Conversion Corporation. The University of Illinois has defined a 500-tonne \"Fusion Ship II\" concept capable of delivering a 100,000 kg manned payload to Jupiter's moon Europa in 210 days. Fusion Ship II utilizes ion rocket thrusters (343 km/s exhaust velocity) powered by ten D-He3 IEC fusion reactors. The concept would need 300 tonnes of argon propellant for a 1-year round trip to the Jupiter system. Robert Bussard published a series of technical articles discussing its application to spaceflight throughout the 1990s. His work was popularised by an article in the Analog Science Fiction and Fact publication, where Tom Ligon (who has also written several science fiction stories) described how the fusor would make for a highly effective fusion rocket. It was also featured in this role in the science fiction novel \"The Wreck of the River of Stars\", by Michael Flynn.\n\nA still more speculative concept is antimatter catalyzed nuclear pulse propulsion, which would use tiny quantities of antimatter to catalyze a fission and fusion reaction, allowing much smaller fusion explosions to be created.\n\n\n\n"}
{"id": "37422023", "url": "https://en.wikipedia.org/wiki?curid=37422023", "title": "HP Flexible Data Center", "text": "HP Flexible Data Center\n\nHP Flexible Data Center, also termed FlexDC, is a modular data center built from prefabricated components by Hewlett-Packard and introduced in 2010. It is housed in five large buildings that form the shape of a butterfly. The Flexible DC looks like a traditional building, but it is fabricated off-site in order to circumvent the two years it often takes for traditional building construction. The building consists of a central admin area (the Core), surrounded by 1-4 data halls (the Quadrants). FDC offers cooling options that are optimal for each type of climate.\n\nThe FlexDC product line follows from HP's acquisition of EYP Mission Critical Facilities in November 2007. HP currently positions FlexDC at the top end of their modular datacenter product line (above their PODs, which are housed in shipping containers), up to 3.6MW in capacity per facility.\n\n"}
{"id": "1252549", "url": "https://en.wikipedia.org/wiki?curid=1252549", "title": "Hand Held Products", "text": "Hand Held Products\n\nHand Held Products was the world's largest manufacturer of linear and 2D handheld barcode scanners based on imaging technology. Its product range included rugged mobile computers, image kiosks, and barcode verification devices. Its range of data collection and communication products were designed for mobile, on-site, and transaction processing applications.\n\nThe company was created in a merger between the former Welch Allyn Data Collection, Inc. and Hand Held Products, Inc. in late 1999.\nSince the merger in 1999, the company has maintained its company headquarters in NY.\nWelch Allyn Data Collection was formed in 1972 as a subsidiary of Welch Allyn, headquartered in Skaneateles Falls, New York, and by the 1990s was manufacturing a complete line of handheld bar code readers.\n\nThe original Hand Held Products company, established in 1981 in Charlotte, North Carolina, began life as a manufacturer of memory modules for Hewlett-Packard and Texas Instruments programmable calculators, but quickly came to the industry spotlight in 1983 thanks to an RFP hand-off by Hewett-Packard, as a package delivery company Federal Express selected their Micro-Wand portable data collection device to begin its first major proof of delivery system for package tracking.\n\nIn 1996, Hand Held Products launched the first mobile computer with 2D imaging technology, the Dolphin 7200. In 1997, the Hand Held Product's Dolphin portable computer was selected by the United States Postal Service to be part of their own proof of delivery system, although a third party provided the manufacturing for this deal, which was the largest single order in the history of the industry – 300,000 units.\nIn 2006–2007 United States Postal Service started replacing these portable computers with newly designed Motorola portable computers (370,000 units). The bar code reading hardware (2D imager) and decoding and image processing software in these new portable computers are designed and supplied by Hand Held Products.\n\nHand Held Products has a significant presence in Europe, with its EMEA headquarters in Eindhoven, The Netherlands, where its European repair center is also based. It also has offices in the UK, Germany and France and sales representation and resellers in most countries in Europe, Middle East and Africa. Hand Held Products also has offices in Latin America and Asia.\n\nAs of January 1, 2005, the company dropped HHP as its brand name and returned to its full name Hand Held Products. (The legal company name has always been Hand Held Products, Inc.)\n\nOn October 15, 2007, Hand Held Products announced they had entered an agreement with Honeywell to be purchased for a reported $390 M usd. On December 20, 2007 Honeywell’s acquisition of Hand Held was approved, and Hand Held officially became a part of Honeywell.\nHand Held formed a new line of business within the Honeywell Security Group, initially called Honeywell Imaging and mobility. With Honeywell's acquisition of Metrologic in 2008 they decided to call the business unit Honeywell Scanning and Mobility.\n"}
{"id": "15425838", "url": "https://en.wikipedia.org/wiki?curid=15425838", "title": "Hydrogen pipeline transport", "text": "Hydrogen pipeline transport\n\nHydrogen pipeline transport is a transportation of hydrogen through a pipe as part of the hydrogen infrastructure.\n\n\nHydrogen pipeline transport is used to transport hydrogen from the point of production or delivery to the point of demand. Although hydrogen pipeline transport is technologically mature, and the transport costs are similar to those of CNG, most hydrogen is produced in the place of demand, with an industrial production facility every \n\nHydrogen has problems with both hydrogen embrittlement and corrosion. Hydrogen has an active electron, and therefore behaves somewhat like a halogen. For this reason, hydrogen pipes have to resist corrosion. The problem is compounded because hydrogen can easily migrate into the crystal structure of most metals. For process metal piping at pressures up to , high-purity stainless steel piping with a maximum hardness of 80 HRB is preferred.\n\nComposite pipes are assessed like:\nFiber-Reinforced Polymer pipelines (or FRP pipeline) and reinforced thermoplastic pipes are researched.\n\n\n\n"}
{"id": "3677421", "url": "https://en.wikipedia.org/wiki?curid=3677421", "title": "Hypersonic wind tunnel", "text": "Hypersonic wind tunnel\n\nA hypersonic wind tunnel is designed to generate a hypersonic flow field in the working section, thus simulating the typical flow features of this flow regime - including compression shocks and pronounced boundary layer effects, entropy layer and viscous interaction zones and most importantly high total temperatures of the flow. The speed of these tunnels vary from Mach 5 to 15. The power requirement of a wind tunnel increases with the cross section, the flow density and is directly proportional to the third power of the test velocity. Hence installation of a continuous, closed circuit wind tunnel remains a costly affair. The first continuous Mach 7-10 wind tunnel with 1x1 m test section was planned at Kochel am See, Germany during WW II and finally put into operation as 'Tunnel A' in the late 1950s at AEDC Tullahoma, TN, USA for an installed power of 57 MW. In view of these high facility demands, also intermittently operated experimental facilities like blow-down wind tunnels are designed and installed to simulate the hypersonic flow. A hypersonic wind tunnel comprises in flow direction the main components: heater/cooler arrangements, dryer, convergent/divergent nozzle, test section, second throat and diffuser. A blow-down wind tunnel has a low vacuum reservoir at the back end, while a continuously operated, closed circuit wind tunnel has a high power compressor installation instead. Since the temperature drops with the expanding flow, the air inside the test section has the chance of becoming liquefied. For that reason, preheating is particularly critical (the nozzle may require cooling).\n\nThere are several technological problems in designing and constructing a hyper-velocity wind tunnel:\nSimulations of a flow at 5.5 km/s, 45 km altitude would require tunnel temperatures of over 9000 K, and a pressure of 3 GPa (see ).\n\nOne form of HWT is known as a Gun Tunnel or hot shot tunnel (up to \"M\"=27), which can be used for analysis of flows past ballistic missiles, space vehicles in atmospheric entry, and plasma physics or heat transfer at high temperatures. It runs intermittently, but has a very low running time (less than a second).\nThe method of operation is based on a high temperature and pressurized gas (air or nitrogen) produced in an arc-chamber, and a near-vacuum in the remaining part of the tunnel. The arc-chamber can reach several MPa, while pressures in the vacuum chamber can be as low as 0.1 Pa. This means that the pressure ratios of these tunnels are in the order of 10 million. Also, the temperatures of the hot gas are up to 5000 K. The arc chamber is mounted in the gun barrel. The high pressure gas is separated by the vacuum by a diaphragm that breaks down as its resistance is exceeded.\n\nPrior to a test run commencing, a membrane separates the compressed air from the gun barrel breech. A rifle (or similar) is used to rupture the membrane. Compressed air rushes into the breech of the gun barrel, forcing a small projectile to accelerate rapidly down the barrel. Although the projectile is prevented from leaving the barrel, the air in front of the projectile emerges at hypersonic velocity into the working section. Naturally the duration of the test is extremely brief, so high speed instrumentation is required to get any meaningful data.\n\nISRO commissioned two major facilities — a Hypersonic Wind Tunnel and Shock Tunnel — at the Vikram Sarabhai Space Center here as part of its continuous and concerted efforts to minimize cost of access to space. ISRO Chairman A. S. Kiran Kumar said commissioning of such facilities would provide adequate data for design and development of current and future space transportation systems in India.\n\nThe MARHy Hypersonic low density Wind Tunnel, located at the ICARE Laboratory in Orléans, France, is a research facility used extensively for fundamental and applied research of fluid dynamic phenomena in rarefied compressible flows, applied to space research(contact Viviana Lago: head of the Fast team, viviana.lago@cnrs-orleans.fr). Its name is an acronym for Mach Adaptable Rarefied Hypersonic and the wind tunnel is recorded under this name under the European portal MERIL.\n\n\n"}
{"id": "35745756", "url": "https://en.wikipedia.org/wiki?curid=35745756", "title": "Immudex", "text": "Immudex\n\nImmudex is a Danish Reagents and Diagnostics company established in 2009. The company is operating from offices located in Copenhagen, Denmark, and in Fairfax, Virginia. Immudex specializes in the production of MHC Dextramers. MHC Dextramers are chemical reagents that are designed to detect antigen-specific T cells.\n\nAll Immudex products, whether on the marked or in development, are based on its proprietary MHC Dextramer technology. The use of MHC Dextramers offers a new method to accurately detect and quantify antigen-specific CD8+ T cells.\n\nWhile some of the technology is currently only sold for Research use only (RUO), clinical applications are being actively pursued. Currently, Immudex has several development projects for clinically applicable MHC Dextramers. Even so, the first MHC Dextramers clinical reagent has been developed and approved for clinical in vitro diagnostics in Europe. Namely the CE-marked kit for enumeration of cytomegalovirus-specific T cells. Immudex primarily focuses on the identification of producing large quantities of CD8+ T cells by the means of MHC class I molecules. Even so, the research and development into the application of MHC class II Dextramers might expand the technology.\n\nOriginally, Immudex operated as an individual division within the much larger Danish pharmaceutical company Dako. Immudex was later spun out as a separate business, when the focus of Dako largely shifted to cancer diagnostics. Immudex currently have clients world wide with major markets in the United States and Europe. \nPatents for the MHC Dextramer technology were filed at Dako in 2001, and the Immudex division was launched in 2005.\n\nIn 2011, Immudex received the prestigious “Entrepreneur of the Year 2011” award in the Life Sciences category granted by Ernst & Young.\nImmudex received this award because of the commitment of the company, articulated in the company's \"make it work and make it better\" strategy to generate results for both practitioners and patients. The award is given to companies not only focused on financial growth, but also for softer values such as the entrepreneur's social responsibility and the ability to aspire an motivate the organization.\n\nA large number of immunotherapies, currently in development, seek to induce, expand, activate or eliminate antigen-specific T cells. It is therefore important to be able to quantify and characterize the T cell responses before, during and after treatment in order to understand if the proposed change to the T cells has been effectuated.\nThe process of negative selection in the thymus guarantees that virtually all T cells have very weak affinity for self-antigens. Therefore, the study of these lymphocytes in autoimmune diseases and cancer has been difficult, until the fairly recent introduction of MHC multimers. Tetramers, which are among the most popular form of these multimer complexes, have four binding sites. The Tetramers were originally developed in Academia. Many further developments in this technology have taken place at biotechnology companies.\nThe Immudex MHC Dextramers, which represent the latest advancement in this field, provide an exponentially greater probability of successful identification of antigen-specific T cells. This can be done, because the MHC Dextramers have nine, or more binding sites per dextran backbone. Scientists have been optimistic about the potential of such technology, since it will help broaden the understanding of T cell responses and will in the future be clinically applicable to many different antigen-specific T cells.\n\nThe MHC Dextramers are fluorescent MHC multimer reagents, developed for sensitive, specific and accurate detection of antigen-specific T cells by using Flow cytometry. The MHC Dextramers covers human, mouse, and monkey alleles that all display disease relevant antigenic peptides. The MHC Dextramers can thereby be used for monitoring antigen-specific T cell responses.\n\nVirus Dextramer collection 1 - provides reagents for detection, quantification and isolation of virus specific T cells. It furthermore enables detection for CMV-, EBV-, influenza-, and BK virus specific CD8+ T cells.\n\nDextramer CMV kit - These MHC Dextramers provide a method for quantification of CMV-specific CD8+ T cells in whole blood samples as an aid in predicting and monitoring patients at risk of developing CMV-related disease.\n\nMelanoma Dextramer collection 1 - comprises MHC Dextramers specific for 6 different melanoma-associated antigens and can be used for the detection, enumeration and isolation of melanoma-specific CD8+ T cells from blood or tumor tissue.\n\nCancer Testis Antigen CD1d Dextramer - These MHC Dextramers are flow cytometry reagents for the identification and sorting of Natural killer cells (NKT), as well as an extensive range of single reagents for detection of cancer- and viral-specific T cells. The CD1d/α-GalCer displays human CD1d molecules loaded with α-GalCer.\n\nThough Immudex specializes in MHC Dextramer technology, the company is also pursuing development projects in diagnostics for Lyme disease/Borreliosis, based on the detection of Borrelia-specific T cells in diseased patients.\n\nClinical application of the CE-market Dextramer CMV kit has been approved for the European market. The CMV kit will also be attempted approved for the US market through a number of clinical trials and S10K approval. The CMV kit is intended for identification and enumerication of CMV-specific CD8+ T cells in whole blood samples. Furthermore, the MHC Dextramer CMV kit can be used for the assessment of CMV-specific immune status in HLA-matched patients.\n\nKnowledge of the level of viral-specific T cells allow evaluation of the patients preparedness for fighting the viral infection and thereby avoid unnecessary antiviral treatment. CMV infections are often unnoticed by healthy people, but can be life threatening for immune compromised individuals, such as HIV-infected persons and organ-transplant recipients.\n\nIn 2013, Immudex entered into a co-distribution and co-promotional partnership with JPT Peptide Technologies. The goal of this partnership is to synergistically combine JPT's innovative peptide technologies along with existing products with Immudex's sophisticated MHC Dextramer products. This product combination will provide a comprehensive portfolio of high quality T cell relevant products and services for the immunotherapy and vaccine community.\n\nProficiency panel testing comprises an interlaboratory system for the regular testing of accuracy that the participating laboratories can achieve. The MHC multimer panel tests the proficiency of a given laboratory in performing the flow cytometry based MHC multimer assay on blood and in performing an Elispot Assay.\n\nIn the fall of 2013 Immudex entered an agreement with the US cancer Immunotherapy consortium (CIC) and the European Association for Cancer Immunotherapy (CIMIT). The agreement dictates that Immudex will provide MHC-peptide multimers and Elispot proficiency panels to laboratories worldwide, independently of their background and associations.\n\nCIC and CIMT originally established proficiency panel programs to offer an external validation of assay performance and to enhance assay harmonization. Over the years, more than one hundred laboratories have participated. Not only from the cancer field, but also from other immunological fields utilizing immune monitoring. As a result, two frequently used immune monitoring methods, have now reached a high degree of harmonization.\n\nImmudex, specialized in the detection of antigen-specific T cells has been an active participant in the proficiency panel process and is globally recognized for its expertise in immune monitoring. Thus Immudex was the logical choice as a partner for proficiency panels.\n\nImmudex will continue the external validation program established by CIC and CIMT under the successfully implemented harmonization guidelines and will promote a more widespread use of these technologies in the development of new cancer immunotherapies and vaccines against other diseases such as HIV, TB, diabetes and others.\n\n"}
{"id": "481468", "url": "https://en.wikipedia.org/wiki?curid=481468", "title": "Industrialization of Sweden", "text": "Industrialization of Sweden\n\nThe industrialization of Sweden began during the second half of the 19th century. By the end of the century, the first multinational companies based on advanced technology had emerged.\n\nDuring the early phase of World War I, in which Sweden remained neutral, the country benefited from increasing demand. However, with the German submarine war, Sweden was cut off from its markets, which led to a severe economic downturn. Between the world wars, major Swedish exports were steel, ball-bearings, wood pulp, and matches. Prosperity following World War II provided the foundations for the social welfare policies characteristic of modern Sweden.\n\nForeign policy concerns in the 1930s centered on Soviet and German expansionism, which stimulated abortive efforts at Nordic defence co-operation.\n\nThe main line railways (), built and owned by the State, were of major importance for the development of Swedish industry and economy in general.\n\nThe two first main line railways, were the Southern Main Line, stretching from Stockholm to Malmö in the south, and the Western Main Line, from Stockholm to Gothenburg on the west coast. They were completed between 1860–1864. The Northern railways (East Coast Line, Northern Main Line & Main Line Through Upper Norrland) runs parallel to the Baltic coast up to Boden in northern Sweden, and was finished in 1894. The Inland Line runs through the central parts of northern Sweden, and was built between 1908–1937.\n\nThe construction of the early main lines provided a fast and safe connection from the mines in the north to the rest of Sweden. It also facilitated business (and private) travel, that had earlier required horse-driven carriages.\n\nThe Iron Ore Line (not a main line), from Luleå to Narvik in Norway, provided a highly efficient transportation linkage from the iron ores near Kiruna and Gällivare to harbours on both the Atlantic and the Baltic coasts. The sections of the Iron Ore Line were completed in stages between 1888 and 1903.\n\n"}
{"id": "44079804", "url": "https://en.wikipedia.org/wiki?curid=44079804", "title": "Internet Digital DIOS", "text": "Internet Digital DIOS\n\nThe LG Internet Digital DIOS (also known as R-S73CT) is an internet refrigerator released by LG Electronics in June 2000. The technology is the result of a project that started in 1997 and staffed by a team of 55 researchers with a budget cost of Won15 bil (US$49.2 million).\n\nThe refrigerator has a TFT-LCD (thin-film transistor-liquid crystal display) screen with TV functionality and Local Area Network (LAN) port. It includes a LCD information window that features electronic pen, data memo, video messaging and schedule management functions and provides information, such as inside temperature, the freshness of stored foods, nutrition information and recipes. Other features are a webcam that is used as a scanner and tracks what is inside the refrigerator, a MP3 player and a three-level automatic icemaker. In addition, the electricity consumption is half the level of conventional refrigerators and the noise level is only 23 decibels.\n"}
{"id": "89373", "url": "https://en.wikipedia.org/wiki?curid=89373", "title": "MIT OpenCourseWare", "text": "MIT OpenCourseWare\n\nMIT OpenCourseWare (MIT OCW) is an initiative of the Massachusetts Institute of Technology (MIT) to publish all of the educational materials from its undergraduateand graduate-level courses online, freely and openly available to anyone, anywhere. MIT OpenCourseWare is a large-scale, web-based publication of MIT course materials. The project was announced on April 4, 2001 and uses Creative Commons Attribution-Noncommercial-Share Alike license. The program was originally funded by the William and Flora Hewlett Foundation, the Andrew W. Mellon Foundation, and MIT. Currently, MIT OpenCourseWare is supported by MIT, corporate underwriting, major gifts, and donations from site visitors. The initiative has inspired more than 250 other institutions to make their course materials available as open educational resources through the Open Education Consortium.\n\n, over 2,400 courses were available online. While a few of these were limited to chronological reading lists and discussion topics, a majority provided homework problems and exams (often with solutions) and lecture notes. Some courses also included interactive web demonstrations in Java, complete textbooks written by MIT professors, and streaming video lectures.\n\n, 100 courses included complete video lectures. The videos were available in streaming mode, but could also be downloaded for viewing offline. All video and audio files were also available from YouTube, iTunes U and the Internet Archive.\n\nThe concept of MIT OpenCourseWare grew out of the MIT Council on Education Technology, which was charged by MIT provost Robert Brown in 1999 with determining how MIT should position itself in the distance learning/e-learning environment. MIT OpenCourseWare was then initiated to provide a new model for the dissemination of knowledge and collaboration among scholars around the world, and contributes to the “shared intellectual commons” in academia, which fosters collaboration across MIT and among other scholars. The project was spearheaded by professors Dick K.P Yue, Shigeru Miyagawa, Hal Abelson and other MIT Faculty.\n\nThe main challenge in implementing the MIT OCW initiative had not been faculty resistance, but rather, the logistical challenges presented by determining ownership and obtaining publication permission for the massive amount of intellectual property items that are embedded in the course materials of MIT's faculty, in addition to the time and technical effort required to convert the educational materials to an online format. Copyright in MIT OpenCourseWare material remains with MIT, members of its faculty, or its students.\n\nIn September 2002, the MIT OpenCourseWare proof-of-concept pilot site opened to the public, offering 32 courses. In September 2003, MIT OpenCourseWare published its 500th course, including some courses with complete streaming video lectures. By September 2004, 900 MIT courses were available online.\n\nIn 2005, MIT OpenCourseWare and other leading open educational resources projects formed the OpenCourseWare Consortium, which seeks to extend the reach and impact of open course materials, foster new open course materials and develop sustainable models for open course material publication.\n\nIn 2007, MIT OpenCourseWare introduced a site called Highlights for High School that indexes resources on the MIT OCW applicable to advanced high school study in biology, chemistry, calculus and physics in an effort to support US STEM education at the secondary school level.\n\nIn 2011, MIT OpenCourseWare introduced the first of fifteen OCW Scholar courses, which are designed specifically for the needs of independent learners. While still publications of course materials like the rest of the site content, these courses are more in-depth and the materials are presented in logical sequences that facilitate self-study. No interaction with other students is supported by the OCW site, but study groups on collaborating project OpenStudy are available for some OCW Scholar courses.\n\n, some MIT OCW courses are delivered by the European MooC platform Eliademy.\n\nMIT OCW was originally served by a custom content management system based on Microsoft's Content Management Server, which was replaced in mid-2010 with a Plone-based content management system. The publishing process is described by MIT as a \"large-scale digital publishing infrastructure consists of planning tools, a content management system (CMS), and the MIT OpenCourseWare content distribution infrastructure\".\n\nVideo content for the courses were originally primarily in RealMedia format. In 2008, OCW transitioned to using YouTube as the primary digital video streaming platform for the site, embedding YouTube video back into the OCW site. OCW video and audio files are also provided in full for offline downloads on iTunesU and the Internet Archive. In 2011, OCW introduced an iPhone App called LectureHall in partnership with Irynsoft.\n\n, the annual cost of running MIT OCW is about $3.5 million. \"MIT's goal for the next decade is to increase our reach ten-fold\" and to secure funding for this.\n\n"}
{"id": "44731696", "url": "https://en.wikipedia.org/wiki?curid=44731696", "title": "Mahavihara", "text": "Mahavihara\n\nMahavihara () is the Sanskrit and Pali term for a great vihara (Buddhist monastery) and is used to describe a monastic complex of viharas.\n\nA range of monasteries grew up in ancient Magadha (modern Bihar) and Bengal. According to Tibetan sources, five great mahaviharas stood out during the Pāla period: Vikramashila, the premier university of the era; Nalanda, past its prime but still illustrious, Somapura, Odantapurā, and Jaggadala. The five monasteries formed a network; \"all of them were under state supervision\" and there existed \"a system of co-ordination among them . . it seems from the evidence that the different seats of Buddhist learning that functioned in eastern India under the Pāla were regarded together as forming a network, an interlinked group of institutions,\" and it was common for great scholars to move easily from position to position among them.\n\nThe famous Nalanda Mahavihara was founded a few centuries earlier; Xuanzang speaks about its magnificence and grandeur. Reference to this monastery is found in Tibetan and Chinese sources. During the Pāla period, Nālānda was less singularly outstanding, as other Pālā establishments \"must have drawn away a number of learned monks from Nālānda when all of the . . came under the aegis of the Pālās.\" Nonetheless, the fame of this monastery lingered even after the Pala period.\n\nOdantapuri, also called Odantapura or Uddandapura, was a Buddhist vihara in what is now Bihar, India. It was established by King Gopala of the Pala dynasty in the 7th century. It is considered the second oldest of India's universities and was situated in Magadh. Currently it is known as the Bihar Sharif city (Headquarters of Nalanda District). Acharya Sri Ganga of Vikramashila had been a student here. According to the Tibetan records there were about 12,000 students at Odantapuri. Odantpuri was situated at a mountain called Hiranya Prabhat Parvat and the bank of the river Panchanan.\n\nReference to a monastery known as Vikramashila is found in Tibetan records. The Pala ruler Dharmapala was its founder. The exact site of this vihara is at Antichak, a small village in Bhagalpur district (Bihar). The monastery had 107 temples and 50 other institutions providing room for 108 monks. It attracted scholars from neighbouring countries.\n\nSomapura Mahavihara was located at Paharpur, 46.5 km to the north-west of Mahasthangarh in Bangladesh. The available data suggests that the Pala ruler Dharmapala founded the vihara. It followed the traditional cruciform plan for the central shrine. There were 177 individual cells around the central courtyard. There were central blocks in the middle of the eastern, southern and western sides. These might have been subsidiary chapels. It was the premier vihara of its kind and its fame lingered till the 11th century CE.\n\nJagaddala Mahavihara was a Buddhist monastery and seat of learning in Varendra, a geographical unit in present north Bengal. It was founded by the later kings of the Pāla dynasty, probably Ramapala (), most likely at a site near the present village of Jagdal in Dhamoirhat Upazila in the north-west Bangladesh on the border with India, near Paharapur.\n\nalso known as Bhavadev Bihar is another large monastery which flourished between 7th to 12th centuries AD. Located in Comilla, Bangladesh,it was established by King Bhava Deva in the Lalmai Hills Ridge.\n\nThe Anuradhapura Maha Viharaya (Pali for \"Great Monastery\") was an important monastery for Theravada Buddhism in Sri Lanka. It was founded by king Devanampiya Tissa (247–207 BCE) in his capital Anuradhapura. The Mahavihara was the place where the Mahavihara orthodoxy was established by monks such as Buddhaghosa. The traditional Theravadin account provided by the Mahavamsa stands in contrast to the writings of the Chinese Buddhist monk Faxian (Ch. 法顯), who journeyed to India and Sri Lanka in the early 5th century (between 399 and 414 CE). He recorded that the Mahavihara was not only intact, but housed 3000 monks.\n\n\n"}
{"id": "19638", "url": "https://en.wikipedia.org/wiki?curid=19638", "title": "Microelectromechanical systems", "text": "Microelectromechanical systems\n\nMicroelectromechanical systems (MEMS, also written as \"micro-electro-mechanical\", \"MicroElectroMechanical\" or \"microelectronic and microelectromechanical systems\" and the related \"micromechatronics\") is the technology of microscopic devices, particularly those with moving parts. It merges at the nano-scale into nanoelectromechanical systems (NEMS) and nanotechnology. MEMS are also referred to as micromachines in Japan, or \"micro systems technology\" (\"MST\") in Europe.\n\nMEMS are made up of components between 1 and 100 micrometers in size (i.e., 0.001 to 0.1 mm), and MEMS devices generally range in size from 20 micrometres to a millimetre (i.e., 0.02 to 1.0 mm), although components arranged in arrays (e.g., digital micromirror devices) can be more than 1000 mm. \nThey usually consist of a central unit that processes data (the microprocessor) and several components that interact with the surroundings such as microsensors. Because of the large surface area to volume ratio of MEMS, forces produced by ambient electromagnetism (e.g., electrostatic charges and magnetic moments), and fluid dynamics (e.g., surface tension and viscosity) are more important design considerations than with larger scale mechanical devices. MEMS technology is distinguished from molecular nanotechnology or molecular electronics in that the latter must also consider surface chemistry.\n\nThe potential of very small machines was appreciated before the technology existed that could make them (see, for example, Richard Feynman's famous 1959 lecture There's Plenty of Room at the Bottom). MEMS became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. These include molding and plating, wet etching (KOH, TMAH) and dry etching (RIE and DRIE), electro discharge machining (EDM), and other technologies capable of manufacturing small devices. An early example of a MEMS device is the resonistor, an electromechanical monolithic resonator patented by Raymond J. Wilfinger, and the resonant gate transistor developed by Harvey C. Nathanson.\n\nThe fabrication of MEMS evolved from the process technology in semiconductor device fabrication, i.e. the basic techniques are deposition of material layers, patterning by photolithography and etching to produce the required shapes.\n\nSilicon is the material used to create most integrated circuits used in consumer electronics in the modern industry. The economies of scale, ready availability of inexpensive high-quality materials, and ability to incorporate electronic functionality make silicon attractive for a wide variety of MEMS applications. Silicon also has significant advantages engendered through its material properties. In single crystal form, silicon is an almost perfect Hookean material, meaning that when it is flexed there is virtually no hysteresis and hence almost no energy dissipation. As well as making for highly repeatable motion, this also makes silicon very reliable as it suffers very little fatigue and can have service lifetimes in the range of billions to trillions of cycles without breaking.\n\nEven though the electronics industry provides an economy of scale for the silicon industry, crystalline silicon is still a complex and relatively expensive material to produce. Polymers on the other hand can be produced in huge volumes, with a great variety of material characteristics. MEMS devices can be made from polymers by processes such as injection molding, embossing or stereolithography and are especially well suited to microfluidic applications such as disposable blood testing cartridges.\n\nMetals can also be used to create MEMS elements. While metals do not have some of the advantages displayed by silicon in terms of mechanical properties, when used within their limitations, metals can exhibit very high degrees of reliability. Metals can be deposited by electroplating, evaporation, and sputtering processes. Commonly used metals include gold, nickel, aluminium, copper, chromium, titanium, tungsten, platinum, and silver.\n\nThe nitrides of silicon, aluminium and titanium as well as silicon carbide and other ceramics are increasingly applied in MEMS fabrication due to advantageous combinations of material properties. AlN crystallizes in the wurtzite structure and thus shows pyroelectric and piezoelectric properties enabling sensors, for instance, with sensitivity to normal and shear forces. TiN, on the other hand, exhibits a high electrical conductivity and large elastic modulus, making it possible to implement electrostatic MEMS actuation schemes with ultrathin membranes. Moreover, the high resistance of TiN against biocorrosion qualifies the material for applications in biogenic environments and in biosensors.\n\nOne of the basic building blocks in MEMS processing is the ability to deposit thin films of material with a thickness anywhere between one micrometre, to about 100 micrometres. The NEMS process is the same, although the measurement of film deposition ranges from a few nanometres to one micrometre. There are two types of deposition processes, as follows.\n\nPhysical vapor deposition (\"PVD\") consists of a process in which a material is removed from a target, and deposited on a surface. Techniques to do this include the process of sputtering, in which an ion beam liberates atoms from a target, allowing them to move through the intervening space and deposit on the desired substrate, and evaporation, in which a material is evaporated from a target using either heat (thermal evaporation) or an electron beam (e-beam evaporation) in a vacuum system.\n\nChemical deposition techniques include chemical vapor deposition (\"CVD\"), in which a stream of source gas reacts on the substrate to grow the material desired. This can be further divided into categories depending on the details of the technique, for example, LPCVD (Low Pressure chemical vapor deposition) and PECVD (Plasma-enhanced chemical vapor deposition).\n\nOxide films can also be grown by the technique of thermal oxidation, in which the (typically silicon) wafer is exposed to oxygen and/or steam, to grow a thin surface layer of silicon dioxide.\n\nPatterning in MEMS is the transfer of a pattern into a material.\n\nLithography in MEMS context is typically the transfer of a pattern into a photosensitive material by selective exposure to a radiation source such as light. A photosensitive material is a material that experiences a change in its physical properties when exposed to a radiation source. If a photosensitive material is selectively exposed to radiation (e.g. by masking some of the radiation) the pattern of the radiation on the material is transferred to the material exposed, as the properties of the exposed and unexposed regions differs.\n\nThis exposed region can then be removed or treated providing a mask for the underlying substrate. Photolithography is typically used with metal or other thin film deposition, wet and dry etching. Sometimes, photolithography is used to create structure without any kind of post etching. One example is SU8 based lens where SU8 based square blocks are generated. Then the photoresist is melted to form a semi-sphere which acts as a lens.\n\nElectron beam lithography (often abbreviated as e-beam lithography) is the practice of scanning a beam of electrons in a patterned fashion across a surface covered with a film (called the resist), (\"exposing\" the resist) and of selectively removing either exposed or non-exposed regions of the resist (\"developing\"). The purpose, as with photolithography, is to create very small structures in the resist that can subsequently be transferred to the substrate material, often by etching. It was developed for manufacturing integrated circuits, and is also used for creating nanotechnology architectures.\n\nThe primary advantage of electron beam lithography is that it is one of the ways to beat the diffraction limit of light and make features in the nanometer range. This form of maskless lithography has found wide usage in photomask-making used in photolithography, low-volume production of semiconductor components, and research & development.\n\nThe key limitation of electron beam lithography is throughput, i.e., the very long time it takes to expose an entire silicon wafer or glass substrate. A long exposure time leaves the user vulnerable to beam drift or instability which may occur during the exposure. Also, the turn-around time for reworking or re-design is lengthened unnecessarily if the pattern is not being changed the second time.\n\nIt is known that focused-ion beam lithography has the capability of writing extremely fine lines (less than 50 nm line and space has been achieved) without proximity effect. However, because the writing field in ion-beam lithography is quite small, large area patterns must be created by stitching together the small fields.\n\nIon track technology is a deep cutting tool with a resolution limit around 8 nm applicable to radiation resistant minerals, glasses and polymers. It is capable of generating holes in thin films without any development process. Structural depth can be defined either by ion range or by material thickness. Aspect ratios up to several 10 can be reached. The technique can shape and texture materials at a defined inclination angle. Random pattern, single-ion track structures and aimed pattern consisting of individual single tracks can be generated.\n\nX-ray lithography is a process used in electronic industry to selectively remove parts of a thin film. It uses X-rays to transfer a geometric pattern from a mask to a light-sensitive chemical photoresist, or simply \"resist\", on the substrate. A series of chemical treatments then engraves the produced pattern into the material underneath the photoresist.\n\nA simple way to carve or create patterns on the surface of nanodiamonds without damaging them could lead to a new photonic devices.\n\nDiamond patterning is a method of forming diamond MEMS. It is achieved by the lithographic application of diamond films to a substrate such as silicon. The patterns can be formed by selective deposition through a silicon dioxide mask, or by deposition followed by micromachining or focused ion beam milling.\n\nThere are two basic categories of etching processes: wet etching and dry etching. In the former, the material is dissolved when immersed in a chemical solution. In the latter, the material is sputtered or dissolved using reactive ions or a vapor phase etchant.\n\nWet chemical etching consists in selective removal of material by dipping a substrate into a solution that dissolves it. The chemical nature of this etching process provides a good selectivity, which means the etching rate of the target material is considerably higher than the mask material if selected carefully.\n\nEtching progresses at the same speed in all directions. Long and narrow holes in a mask will produce v-shaped grooves in the silicon. The surface of these grooves can be atomically smooth if the etch is carried out correctly, with dimensions and angles being extremely accurate.\n\nSome single crystal materials, such as silicon, will have different etching rates depending on the crystallographic orientation of the substrate. This is known as anisotropic etching and one of the most common examples is the etching of silicon in KOH (potassium hydroxide), where Si <111> planes etch approximately 100 times slower than other planes (crystallographic orientations). Therefore, etching a rectangular hole in a (100)-Si wafer results in a pyramid shaped etch pit with 54.7° walls, instead of a hole with curved sidewalls as with isotropic etching.\n\nHydrofluoric acid is commonly used as an aqueous etchant for silicon dioxide (, also known as BOX for SOI), usually in 49% concentrated form, 5:1, 10:1 or 20:1 BOE (buffered oxide etchant) or BHF (Buffered HF). They were first used in medieval times for glass etching. It was used in IC fabrication for patterning the gate oxide until the process step was replaced by RIE.\n\nHydrofluoric acid is considered one of the more dangerous acids in the cleanroom. It penetrates the skin upon contact and it diffuses straight to the bone. Therefore, the damage is not felt until it is too late.\n\nElectrochemical etching (ECE) for dopant-selective removal of silicon is a common method to automate and to selectively control etching. An active p-n diode junction is required, and either type of dopant can be the etch-resistant (\"etch-stop\") material. Boron is the most common etch-stop dopant. In combination with wet anisotropic etching as described above, ECE has been used successfully for controlling silicon diaphragm thickness in commercial piezoresistive silicon pressure sensors. Selectively doped regions can be created either by implantation, diffusion, or epitaxial deposition of silicon.\n\nXenon difluoride () is a dry vapor phase isotropic etch for silicon originally applied for MEMS in 1995 at University of California, Los Angeles. Primarily used for releasing metal and dielectric structures by undercutting silicon, has the advantage of a stiction-free release unlike wet etchants. Its etch selectivity to silicon is very high, allowing it to work with photoresist, , silicon nitride, and various metals for masking. Its reaction to silicon is \"plasmaless\", is purely chemical and spontaneous and is often operated in pulsed mode. Models of the etching action are available, and university laboratories and various commercial tools offer solutions using this approach.\n\nModern VLSI processes avoid wet etching, and use plasma etching instead. Plasma etchers can operate in several modes by adjusting the parameters of the plasma. Ordinary plasma etching operates between 0.1 and 5 Torr. (This unit of pressure, commonly used in vacuum engineering, equals approximately 133.3 pascals.) The plasma produces energetic free radicals, neutrally charged, that react at the surface of the wafer. Since neutral particles attack the wafer from all angles, this process is isotropic.\n\nPlasma etching can be isotropic, i.e., exhibiting a lateral undercut rate on a patterned surface approximately the same as its downward etch rate, or can be anisotropic, i.e., exhibiting a smaller lateral undercut rate than its downward etch rate. Such anisotropy is maximized in deep reactive ion etching. The use of the term anisotropy for plasma etching should not be conflated with the use of the same term when referring to orientation-dependent etching.\n\nThe source gas for the plasma usually contains small molecules rich in chlorine or fluorine. For instance, carbon tetrachloride (CCl4) etches silicon and aluminium, and trifluoromethane etches silicon dioxide and silicon nitride. A plasma containing oxygen is used to oxidize (\"ash\") photoresist and facilitate its removal.\n\nIon milling, or sputter etching, uses lower pressures, often as low as 10−4 Torr (10 mPa). It bombards the wafer with energetic ions of noble gases, often Ar+, which knock atoms from the substrate by transferring momentum. Because the etching is performed by ions, which approach the wafer approximately from one direction, this process is highly anisotropic. On the other hand, it tends to display poor selectivity. Reactive-ion etching (RIE) operates under conditions intermediate between sputter and plasma etching (between 10–3 and 10−1 Torr). Deep reactive-ion etching (DRIE) modifies the RIE technique to produce deep, narrow features.\n\nIn reactive-ion etching (RIE), the substrate is placed inside a reactor, and several gases are introduced. A plasma is struck in the gas mixture using an RF power source, which breaks the gas molecules into ions. The ions accelerate towards, and react with, the surface of the material being etched, forming another gaseous material. This is known as the chemical part of reactive ion etching. There is also a physical part, which is similar to the sputtering deposition process. If the ions have high enough energy, they can knock atoms out of the material to be etched without a chemical reaction. It is a very complex task to develop dry etch processes that balance chemical and physical etching, since there are many parameters to adjust. By changing the balance it is possible to influence the anisotropy of the etching, since the chemical part is isotropic and the physical part highly anisotropic the combination can form sidewalls that have shapes from rounded to vertical.\nDeep RIE (DRIE) is a special subclass of RIE that is growing in popularity. In this process, etch depths of hundreds of micrometres are achieved with almost vertical sidewalls. The primary technology is based on the so-called \"Bosch process\", named after the German company Robert Bosch, which filed the original patent, where two different gas compositions alternate in the reactor. Currently there are two variations of the DRIE. The first variation consists of three distinct steps (the original Bosch process) while the second variation only consists of two steps.\n\nIn the first variation, the etch cycle is as follows:\n\n(i) isotropic etch;\n(ii) passivation;\n(iii) anisoptropic etch for floor cleaning.\n\nIn the 2nd variation, steps (i) and (iii) are combined.\n\nBoth variations operate similarly. The creates a polymer on the surface of the substrate, and the second gas composition ( and ) etches the substrate. The polymer is immediately sputtered away by the physical part of the etching, but only on the horizontal surfaces and not the sidewalls. Since the polymer only dissolves very slowly in the chemical part of the etching, it builds up on the sidewalls and protects them from etching. As a result, etching aspect ratios of 50 to 1 can be achieved. The process can easily be used to etch completely through a silicon substrate, and etch rates are 3–6 times higher than wet etching.\n\nAfter preparing a large number of MEMS devices on a silicon wafer, individual dies have to be separated, which is called die preparation in semiconductor technology. For some applications, the separation is preceded by wafer backgrinding in order to reduce the wafer thickness. Wafer dicing may then be performed either by sawing using a cooling liquid or a dry laser process called stealth dicing.\n\nBulk micromachining is the oldest paradigm of silicon based MEMS. The whole thickness of a silicon wafer is used for building the micro-mechanical structures. Silicon is machined using various etching processes. Anodic bonding of glass plates or additional silicon wafers is used for adding features in the third dimension and for hermetic encapsulation. Bulk micromachining has been essential in enabling high performance pressure sensors and accelerometers that changed the sensor industry in the 1980s and 90's.\n\nSurface micromachining uses layers deposited on the surface of a substrate as the structural materials, rather than using the substrate itself. Surface micromachining was created in the late 1980s to render micromachining of silicon more compatible with planar integrated circuit technology, with the goal of combining MEMS and integrated circuits on the same silicon wafer. The original surface micromachining concept was based on thin polycrystalline silicon layers patterned as movable mechanical structures and released by sacrificial etching of the underlying oxide layer. Interdigital comb electrodes were used to produce in-plane forces and to detect in-plane movement capacitively. This MEMS paradigm has enabled the manufacturing of low cost accelerometers for e.g. automotive air-bag systems and other applications where low performance and/or high g-ranges are sufficient. Analog Devices has pioneered the industrialization of surface micromachining and has realized the co-integration of MEMS and integrated circuits.\n\nBoth bulk and surface silicon micromachining are used in the industrial production of sensors, ink-jet nozzles, and other devices. But in many cases the distinction between these two has diminished. A new etching technology, deep reactive-ion etching, has made it possible to combine good performance typical of bulk micromachining with comb structures and in-plane operation typical of surface micromachining. While it is common in surface micromachining to have structural layer thickness in the range of 2 µm, in HAR silicon micromachining the thickness can be from 10 to 100 µm. The materials commonly used in HAR silicon micromachining are thick polycrystalline silicon, known as epi-poly, and bonded silicon-on-insulator (SOI) wafers although processes for bulk silicon wafer also have been created (SCREAM). Bonding a second wafer by glass frit bonding, anodic bonding or alloy bonding is used to protect the MEMS structures. Integrated circuits are typically not combined with HAR silicon micromachining.\n\nSome common commercial applications of MEMS include:\n\n\nThe global market for micro-electromechanical systems, which includes products such as automobile airbag systems, display systems and inkjet cartridges totaled $40 billion in 2006 according to Global MEMS/Microsystems Markets and Opportunities, a research report from SEMI and Yole Developpement and is forecasted to reach $72 billion by 2011.\n\nCompanies with strong MEMS programs come in many sizes. Larger firms specialize in manufacturing high volume inexpensive components or packaged solutions for end markets such as automobiles, biomedical, and electronics. Smaller firms provide value in innovative solutions and absorb the expense of custom fabrication with high sales margins. Both large and small companies typically invest in R&D to explore new MEMS technology.\n\nThe market for materials and equipment used to manufacture MEMS devices topped $1 billion worldwide in 2006. Materials demand is driven by substrates, making up over 70 percent of the market, packaging coatings and increasing use of chemical mechanical planarization (CMP). While MEMS manufacturing continues to be dominated by used semiconductor equipment, there is a migration to 200 mm lines and select new tools, including etch and bonding for certain MEMS applications.\n\n\nA (not so) short introduction to MEMS: an open online free book on MEMS\n"}
{"id": "49771188", "url": "https://en.wikipedia.org/wiki?curid=49771188", "title": "Nuruk", "text": "Nuruk\n\nNuruk () is a traditional Korean fermentation starter. It imparts a unique flavor to Korean food and is used to make different types of Korean alcoholic beverages including \"takju\", \"cheongju\", and soju. It is an essential ingredient in Shindari and is mixed with rice (Nowicki 22). Historically, it was used in a variety of provinces of Korea, including Jeju Island.\n\nWheat, rice (of both the glutinous and non-glutinous types), and barley are used to make \"nuruk,\" either as whole grain or in the form of grits or flour. The dry grain is moistened, shaped into a large cake, and hung up to ferment for 2‒4 weeks in an ondol room. The cake matures at a precise temperature until a mold forms.\n\nChinese history records the first use of \"nuruk\" in Korea in 1123 CE. Similar fermentation starter was first made in China during the Warring States period beginning in the 5th century BCE, and \"nuruk\" has been used in Korea since the period of the Three Kingdoms in the 3rd century CE.\n\nTraditionally, \"nuruk\" was prepared on a small scale by families in summer or autumn, especially in July when the ambient temperature is between on the Korean peninsula. It has been mass-produced in factories since the 1920s.\n\nFermentation is a metabolic process in which an organism converts carbohydrate, such as a sugar, into an alcohol or an acid. Fermentation is an anaerobic process (Nowicki 2), meaning the living organisms involved in this process do not require oxygen to breathe. A small amount of ATP can be produced without oxygen. Wine, beer, and yogurt are produced through fermentation. In Korea, popular fermented foods include “Kimchi” and soybean paste. If the result is harmful or smelly, it is called \"spoilage,\" but if it is useful, then it is called \"fermentation.\"\n\nChemically, it contains 2,6-Dimethoxybenzoquinone (2,6-DMBQ), also found in fermented wheat germ extract. Microorganisms present in \"nuruk\" include \"Aspergillus\", \"Rhizopus\", and yeasts. Together with yeast, \"nuruk\" is used in rice alcohol production in Korea, as it provides the enzyme amylase.\n\nKorea has four seasons with distinct temperature and humidity. Summer, with high temperature and humidity, is best for farming. Even when the sweet fruit of the west was unavailable, staple grains including rice and barley made it easy to produce and distribute the mold necessary for \"nuruk\".\n\nFermented foods provide friendly organisms for the flora across the digestive system that are necessary for normal digestive function and protect the flora of gut from harmful organisms. Fermented foods also have been broken down or “pre-digested\" and therefore, unlike ordinary carbohydrates, place a lighter burden on the digestion system. They have many additional benefits (Helmenstine 25-28). Fermented food and drink such as Shindari are said to improve the immune system, prevent cancer, and increase beneficial bacteria in the digestive system, balancing the production of stomach acid and aiding nutrient absorption. Traditional fermented food helps the body to produce acetylcholine. It is especially beneficial for people with diabetes.\n"}
{"id": "3052588", "url": "https://en.wikipedia.org/wiki?curid=3052588", "title": "Pentamirror", "text": "Pentamirror\n\nA pentamirror is an optical device used in the viewfinder systems of various single-lens reflex cameras instead of the pentaprism. It is used to reverse again the laterally reversed image coming from the reflex mirror.\n\nInstead of the solid block of glass of the prism in pentaprism system, here 3 mirrors are used to perform the same task. This is cheaper and lighter, but generally produces a viewfinder image of lower quality and brightness.\n\nThis optical device is often (more precisely) referred to as \"roof pentamirror\" because of the roof-like ridge.\n\n\n"}
{"id": "23712727", "url": "https://en.wikipedia.org/wiki?curid=23712727", "title": "RAND Corporation", "text": "RAND Corporation\n\nRAND Corporation (\"Research ANd Development\") is an American nonprofit global policy think tank created in 1948 by Douglas Aircraft Company to offer research and analysis to the United States Armed Forces. It is financed by the U.S. government and private endowment, corporations, universities and private individuals. The company has grown to assist other governments, international organizations, private companies and foundations, with a host of defense and non-defense issues, including healthcare. RAND aims for interdisciplinary and quantitative problem solving by translating theoretical concepts from formal economics and the physical sciences into novel applications in other areas, using applied science and operations research.\n\nRAND has approximately 1,850 employees. Its American locations include: Santa Monica, California (headquarters); Arlington, Virginia; Pittsburgh, Pennsylvania; the San Francisco Bay Area; and Boston, Massachusetts. The RAND Gulf States Policy Institute has an office in New Orleans, Louisiana. RAND Europe is located in Cambridge, United Kingdom, and Brussels, Belgium. RAND Australia is located in Canberra, Australia.\nRAND is home to the Frederick S. Pardee RAND Graduate School, one of eight original graduate programs in public policy and the first to offer a PhD. The program aims to provide practical experience for its students, who work with RAND analysts on real-world problems. The campus is at RAND's Santa Monica research facility. The Pardee RAND School is the world's largest PhD-granting program in policy analysis. Unlike many other universities, all Pardee RAND Graduate School students receive fellowships to cover their education costs. This allows them to dedicate their time to engage in research projects and provides them on-the-job training. RAND also offers a number of internship and fellowship programs allowing students and outsiders to assist in conducting research for RAND projects. Most of these projects are short-term and are worked on independently with the mentoring of a RAND staff member.\n\nRAND publishes the \"RAND Journal of Economics\", a peer-reviewed journal of economics.\n\nThirty-two recipients of the Nobel Prize, primarily in the fields of economics and physics, have been associated with RAND at some point in their career.\n\nGeneral Henry H. Arnold, commander of the United States Army Air Forces, established Project RAND with the objective of looking into long-range planning of future weapons. In March 1946 Douglas Aircraft Company was granted a contract for research on intercontinental warfare, using operations research. In May 1946 the \"Preliminary Design of an Experimental World-Circling Spaceship\" was released. In May 1948, Project RAND separated from Douglas and became an independent non-profit organization as Douglas Aircraft feared it would create conflicts of interest jeopardizing future hardware contracts. Initial capital for the spin-off was provided by the Ford Foundation.\n\nRAND was created after individuals in the War Department, the Office of Scientific Research and Development, and industry began to discuss the need for a private organization to connect military planning with research and development decisions. On 1 October 1945, Project RAND was set up under special contract to the Douglas Aircraft Company and began operations in December 1945. By late 1947, Project RAND considered operating as a separate organization from Douglas and in February 1948, the Chief of Staff of the newly created United States Air Force wrote a letter to the president of the Douglas Aircraft Company that approved the evolution of Project RAND into a nonprofit corporation, independent of Douglas. On 14 May 1948, RAND was incorporated as a nonprofit corporation under the laws of the State of California and on 1 November 1948, the Project RAND contract was formally transferred from the Douglas Aircraft Company to the RAND Corporation.\n\nSince the 1950s, RAND research has helped inform United States policy decisions on a wide variety of issues, including the space race, the U.S.-Soviet nuclear arms confrontation, the creation of the Great Society social welfare programs, the digital revolution, and national health care. Its most visible contribution may be the doctrine of nuclear deterrence by mutually assured destruction (MAD), developed under the guidance of then-Defense Secretary Robert McNamara and based upon their work with game theory. Chief strategist Herman Kahn also posited the idea of a \"winnable\" nuclear exchange in his 1960 book \"On Thermonuclear War\". This led to Kahn being one of the models for the titular character of the film \"\", in which RAND is spoofed as the \"BLAND Corporation\".\n\nRAND was incorporated as a non-profit organization to \"further promote scientific, educational, and charitable purposes, all for the public welfare and security of the United States of America\". Its self-declared mission is \"to help improve policy and decision making through research and analysis\", using its \"core values of quality and objectivity\".\n\nThe achievements of RAND stem from its development of systems analysis. Important contributions are claimed in space systems and the United States' space program, in computing and in artificial intelligence. RAND researchers developed many of the principles that were used to build the Internet. RAND also contributed to the development and use of wargaming.\n\nCurrent areas of expertise include: child policy, civil and criminal justice, education, health, international policy, labor markets, national security, infrastructure, energy, environment, corporate governance, economic development, intelligence policy, long-range planning, crisis management and disaster preparation, population and regional studies, science and technology, social welfare, terrorism, arts policy, and transportation.\n\nRAND designed and conducted one of the largest and most important studies of health insurance between 1974 and 1982. The RAND Health Insurance Experiment, funded by the then–U.S. Department of Health, Education and Welfare, established an insurance corporation to compare demand for health services with their cost to the patient.\n\nAccording to the 2005 annual report, \"about one-half of RAND's research involves national security issues\". Many of the events in which RAND plays a part are based on assumptions which are hard to verify because of the lack of detail on RAND's highly classified work for defense and intelligence agencies. The RAND Corporation posts all of its unclassified reports in full on its website.\n\n\nOver the last 60 years, more than 30 Nobel Prize winners have been involved or associated with the RAND Corporation at some point in their careers.\n\n\n\n\n"}
{"id": "5560489", "url": "https://en.wikipedia.org/wiki?curid=5560489", "title": "Retrofitting", "text": "Retrofitting\n\nRetrofitting refers to the addition of new technology or features to older systems.\n\n\nPrincipally retrofitting describes the measures taken in the manufacturing industry to allow new or updated parts to be fitted to old or outdated assemblies (like blades to wind turbines).\n\nThe production of retrofit parts is necessary in manufacture when the design of a large assembly is changed or revised. If, after the changes have been implemented, a customer (with an old version of the product) wishes to purchase a replacement part then retrofit parts and assembling techniques will have to be used so that the revised parts will fit suitably onto the older assembly.\n\nRetrofitting is an important process used for valves and actuators to ensure optimal operation of an industrial plant. One example is retrofitting a 3-way valve into a 2-way valve, which results in closing one of the three openings to continue using the valve for certain industrial systems.\n\nRetrofitting can improve a machine or system’s overall functionality by using advanced and updated equipment and technology—such as integrating Human Machine Interfaces into older factories.\n\nAnother example of this is car customizing, where older vehicles are fitted with new technologies: power windows, cruise control, remote keyless systems, electric fuel pumps, etc.\n\nThe term is also used in the field of environmental engineering, particularly to describe construction or renovation projects on previously built sites, to improve water quality in nearby streams, rivers or lakes. The concept has also been applied to changing the output mix of energy from power plants to cogeneration in urban areas with a potential for district heating.\n\nSites with extensive impervious surfaces (such as parking lots and rooftops) can generate high levels of stormwater runoff during rainstorms, and this can damage nearby water bodies. These problems can often be addressed by installing new stormwater management features on the site, a process that practitioners refer to as stormwater retrofitting. Stormwater management practices used in retrofit projects include rain gardens, permeable paving and green roofs. \"(See also stream restoration.)\"\n\nMany naval vessels have undergone retrofitting and refitting, sometimes entire classes at once. For instance, the New Threat Upgrade program of the US Navy saw many vessels retrofitted for improved anti-air capability. Naval vessels are often retrofit for one of three reasons: to incorporate new technology, to compensate for performance gaps or weaknesses in design, or to change the ship's classification.\n\nMilitaries of the world are often ardent adopters of the latest technology, and many technological advances have been spurred by warfare, especially in fields such as radar and radio communications. Because of this, and the significant investment that a ship hull represents, it is common for retrofitting to be performed whenever new systems are developed. This may be as small as replacing one type of radio with another, or replacing out-dated cryptography equipment with more secure methods of communication, or as major as replacing entire guns and turrets, adding armor plate, or new propulsion systems.\n\nOther ships are retrofit to compensate for weaknesses perceived in their operational capabilities. This was the secondary purpose of the US Navy's New Threat Upgrade program, for instance. Major changes in doctrine or the art of warfare also necessitate changes, such as the anti-aircraft upgrades performed on many World War Two-era vessels as air power became a dominant part of naval strategy and tactics.\n\nAdditionally, because of the investment a hull represents, few navies scrap front-line warships. Many times smaller ships are retrofitted for patrol, coast guard, or specialized roles when they are no longer fit for duty as part of a warfleet. The Japanese Momi class from the interwar period, for example, was converted from destroyers to patrol boats in 1939, as they were no longer capable enough to serve in the role of destroyer. Other times classes are retrofit because they are no longer needed in warfare, due to changes in tactics. For instance, the USS Langley was an aircraft carrier converted from a collier (coal carrying ship for supply coal-fired steam ships with fuel) of the Jupiter-class.\n\nBecause of the heavy use of retrofitting and refitting fictional navies also include the concept. As an example, in the Star Trek MMORPG Star Trek Online players can purchase retrofitted ships of famous Star Trek ship classes such as those manned by the protagonists of the Star Trek TV series. This is done to allow players to pilot iconic ships of the series from old series of the show that wouldn't naturally be latest-and-greatest ships due to their obsolescence or size but are retrofit to be suitable for a maximum-level player-character admiral.\n\n\n"}
{"id": "334990", "url": "https://en.wikipedia.org/wiki?curid=334990", "title": "Rope", "text": "Rope\n\nA rope is a group of yarns, plies, fibers or strands that are twisted or braided together into a larger and stronger form. Ropes have tensile strength and so can be used for dragging and lifting. Rope is thicker and stronger than similarly constructed cord, string, and twine.\n\nRope may be constructed of any long, stringy, fibrous material, but generally is constructed of certain natural or synthetic fibres. Synthetic fibre ropes are significantly stronger than their natural fibre counterparts, they have a higher tensile strength, they are more resistant to rotting than ropes created from natural fibers, and can be made to float on water. But synthetic rope also possess certain disadvantages, including slipperiness, and some can be damaged more easily by UV light.\n\nCommon natural fibres for rope are manila hemp, hemp, linen, cotton, coir, jute, straw, and sisal. Synthetic fibres in use for rope-making include polypropylene, nylon, polyesters (e.g. PET, LCP, Vectran), polyethylene (e.g. Dyneema and Spectra), Aramids (e.g. Twaron, Technora and Kevlar) and acrylics (e.g. Dralon). Some ropes are constructed of mixtures of several fibres or use co-polymer fibres. Wire rope is made of steel or other metal alloys. Ropes have been constructed of other fibrous materials such as silk, wool, and hair, but such ropes are not generally available. Rayon is a regenerated fibre used to make decorative rope.\n\nThe twist of the strands in a twisted or braided rope serves not only to keep a rope together, but enables the rope to more evenly distribute tension among the individual strands. Without any twist in the rope, the shortest strand(s) would always be supporting a much higher proportion of the total load.\n\nThe long history of rope means that many systems have been used to state the size of a rope. In systems that use the \"inch\" (British Imperial and United States Customary Measure), large ropes over 1 inch diameter such as are used on ships are measured by their circumference in inches; smaller ropes have a nominal diameter based on the circumference divided by three (rounded-down value for pi). In metric systems of measurement, nominal diameter is given in millimetres. The current preferred international standard for rope sizes is to give the mass per unit length, in kilograms per metre. However, even sources otherwise using metric units may still give a \"rope number\" for large ropes, which is the circumference in inches.\n\nRope is of paramount importance in fields as diverse as construction, seafaring, exploration, sports, theatre, and communications, and has been used since prehistoric times. To fasten rope, many types of knots have been invented for countless uses.Pulleys redirect the pulling force to another direction, and can create mechanical advantage so that multiple strands of rope share a load and multiply the force applied to the end. Winches and capstans are machines designed to pull ropes.\n\nThe modern sport of rock climbing uses so-called \"dynamic\" rope, which stretches under load in an elastic manner to absorb the energy required to arrest a person in free fall without generating forces high enough to injure them. Such ropes normally use a kernmantle construction, as described below. \"Static\" ropes, used for example in caving, rappelling, and rescue applications, are designed for minimal stretch; they are not designed to arrest free falls. The UIAA, in concert with the CEN, sets climbing-rope standards and oversees testing. Any rope bearing a GUIANA or CE certification tag is suitable for climbing. Despite the hundreds of thousands of falls climbers suffer every year, there are few recorded instances of a climbing rope breaking in a fall; the cases that do are often attributable to previous damage to, or contamination of, the rope. Climbing ropes, however, do cut easily when under load. Keeping them away from sharp rock edges is imperative.\n\nRock climbing ropes come with either a designation for single, double or twin use. A single rope is the most common and it is intended to be used by itself, as a single strand. Single ropes range in thickness from roughly 9 mm to 11 mm. Smaller ropes are lighter, but wear out faster. Double ropes are thinner ropes, usually 9 mm and under, and are intended for use as a pair. These ropes offer a greater margin or security against cutting, since it is unlikely that both ropes will be cut, but they complicate belaying and leading. Double ropes are usually reserved for ice and mixed climbing, where there is need for two ropes to rappel or abseil. They are also popular among traditional climbers, and particularly in the UK, due to the ability to clip each rope into alternating pieces of protection; allowing the ropes to stay straighter and hence reduce rope drag.\nTwin ropes are not to be confused with doubles. When using twin ropes, both ropes are clipped into the same piece of protection, treating the two as a single strand. This would be favourable in a situation where there was a high chance of a rope being cut. However new lighter-weight ropes with greater safety have virtually replaced this type of rope.\n\nThe butterfly coil is a method of carrying a rope used by climbers where the rope remains attached to the climber and ready to be uncoiled at short notice. Another method of carrying a rope is the alpine coil.\n\nRope is also an aerial acrobatics circus skill, where a performer makes artistic figures on a vertical suspended rope. Tricks performed on the rope are, for example, drops, rolls and hangs.They must also be strong\n\nThe use of ropes for hunting, pulling, fastening, attaching, carrying, lifting, and climbing dates back to prehistoric times. It is likely that the earliest \"ropes\" were naturally occurring lengths of plant fibre, such as vines, followed soon by the first attempts at twisting and braiding these strands together to form the first proper ropes in the modern sense of the word. Impressions of cordage found on fired clay provide evidence of string and rope-making technology in Europe dating back 28,000 years. Fossilized fragments of \"probably two-ply laid rope of about 7 mm diameter\" were found in one of the caves at Lascaux, dating to approximately 15,000 BC.\n\nThe ancient Egyptians were probably the first civilization to develop special tools to make rope. Egyptian rope dates back to 4000 to 3500 B.C. and was generally made of water reed fibres. Other rope in antiquity was made from the fibres of date palms, flax, grass, papyrus, leather, or animal hair. The use of such ropes pulled by thousands of workers allowed the Egyptians to move the heavy stones required to build their monuments. Starting from approximately 2800 B.C., rope made of hemp fibres was in use in China. Rope and the craft of rope making spread throughout Asia, India, and Europe over the next several thousand years.\n\nIn the Middle Ages (from the 13th to the 18th centuries), from the British Isles to Italy, ropes were constructed in ropewalks, very long buildings where strands the full length of the rope were spread out and then \"laid up\" or twisted together to form the rope. The cable length was thus set by the length of the available rope walk. This is related to the unit of length termed \"cable length\". This allowed for long ropes of up to 300 yards long or longer to be made. These long ropes were necessary in shipping as short ropes would require splicing to make them long enough to use for sheets and halyards. The strongest form of splicing is the short splice, which doubles the cross-sectional area of the rope at the area of the splice, which would cause problems in running the line through pulleys. Any splices narrow enough to maintain smooth running would be less able to support the required weight.\n\nLeonardo da Vinci drew sketches of a concept for a ropemaking machine, but it was never built. Nevertheless, remarkable feats of construction were accomplished without advanced technology: In 1586, Domenico Fontana erected the 327 ton obelisk on Rome's Saint Peter's Square with a concerted effort of 900 men, 75 horses, and countless pulleys and meters of rope. By the late 18th century several working machines had been built and patented.\n\nSome rope is still made from natural fibres, such as coir and sisal, despite the dominance of synthetic fibres such as nylon and polypropylene, which have become increasingly popular since the 1950s.\n\nLaid rope, also called twisted rope, is historically the prevalent form of rope, at least in modern Western history. Common twisted rope generally consists of three strands and is normally right-laid, or given a final right-handed twist. The ISO 2 standard uses the uppercase letters S and Z to indicate the two possible directions of twist, as suggested by the direction of slant of the central portions of these two letters. The handedness of the twist is the direction of the twists as they progress away from an observer. Thus Z-twist rope is said to be right-handed, and S-twist to be left-handed.\n\nTwisted ropes are built up in three steps. First, fibres are gathered and spun into yarns. A number of these yarns are then formed into strands by twisting. The strands are then twisted together to lay the rope. The twist of the yarn is opposite to that of the strand, and that in turn is opposite to that of the rope. It is this counter-twist, introduced with each successive operation, which holds the final rope together as a stable, unified object.\n\nTraditionally, a three strand laid rope is called a \"plain-\" or \"hawser-laid\", a four strand rope is called \"shroud-laid\", and a larger rope formed by counter-twisting three or more multi-strand ropes together is called \"cable-laid\". Cable-laid rope is sometimes clamped to maintain a tight counter-twist rendering the resulting cable virtually waterproof. Without this feature, deep water sailing (before the advent of steel chains and other lines) was largely impossible, as any appreciable length of rope for anchoring or ship to ship transfers, would become too waterlogged -- and therefore too heavy -- to lift, even with the aid of a capstan or windlass.\n\nOne property of laid rope is partial untwisting when used. This can cause spinning of suspended loads, or stretching, kinking, or hockling of the rope itself. An additional drawback of twisted construction is that every fibre is exposed to abrasion numerous times along the length of the rope. This means that the rope can degrade to numerous inch-long fibre fragments, which is not easily detected visually.\n\nTwisted ropes have a preferred direction for coiling. Normal right-laid rope should be coiled clockwise, to prevent kinking. Coiling this way imparts a twist to the rope. Rope of this type must be bound at its ends by some means to prevent untwisting.\n\nWhile rope may be made from three or more strands, modern braided rope consists of a braided (tubular) jacket over strands of fiber (these may also be braided). Some forms of braided rope with untwisted cores have a particular advantage; they do not impart an additional twisting force when they are stressed. The lack of added twisting forces is an advantage when a load is freely suspended, as when a rope is used for rappelling or to suspend an arborist. Other specialized cores reduce the shock from arresting a fall when used as a part of a personal or group safety system.\n\nBraided ropes are generally made from nylon, polyester, polypropylene or high performance fibers such as high modulus polyethylene (HMPE) and aramid. Nylon is chosen for its strength and elastic stretch properties. However, nylon absorbs water and is 10-15% weaker when wet. Polyester is about 90% as strong as nylon but stretches less under load and is not affected by water. It has somewhat better UV resistance, and is more abrasion resistant. Polypropylene is preferred for low cost and light weight (it floats on water) but it has limited resistance to ultraviolet light, is susceptible to friction and has a poor heat resistance.\n\nBraided ropes (and objects like garden hoses, fibre optic or coaxial cables, etc.) that have no \"lay\" (or inherent twist) uncoil better if each alternate loop is twisted in the opposite direction, such as in figure-eight coils, where the twist reverses regularly and essentially cancels out.\n\nSingle braid consists of an even number of strands, eight or twelve being typical, braided into a circular pattern with half of the strands going clockwise and the other half going anticlockwise. The strands can interlock with either twill or plain weave. The central void may be large or small; in the former case the term hollow braid is sometimes preferred.\n\nDouble braid, also called braid on braid, consists of an inner braid filling the central void in an outer braid, that may be of the same or different material. Often the inner braid fibre is chosen for strength while the outer braid fibre is chosen for abrasion resistance.\n\nIn solid braid, the strands all travel the same direction, clockwise or anticlockwise, and alternate between forming the outside of the rope and the interior of the rope. This construction is popular for general purpose utility rope but rare in specialized high performance line.\nKernmantle rope has a core (kern) of long twisted fibres in the center, with a braided outer sheath or mantle of woven fibres. The kern provides most of the strength (about 70%), while the mantle protects the kern and determines the handling properties of the rope (how easy it is to hold, to tie knots in, and so on). In dynamic climbing line, core fibres are usually twisted, and chopped into shorter lengths, which makes the rope more elastic. Static kernmantle ropes are made with untwisted core fibres and tighter braid, which causes them to be stiffer in addition to limiting the stretch.\n\nPlaited rope is made by braiding twisted strands, and is also called \"square braid\". It is not as round as twisted rope and coarser to the touch. It is less prone to kinking than twisted rope and, depending on the material, very flexible and therefore easy to handle and knot. This construction exposes all fibres as well, with the same drawbacks as described above. Brait rope is a combination of braided and plaited, a non-rotating alternative to laid three-strand ropes. Due to its excellent energy-absorption characteristics, it is often used by arborists. It is also a popular rope for anchoring and can be used as mooring warps. This type of construction was pioneered by Yale Cordage.\n\nEndless winding rope is made by winding single strands of high-performance yarns around two end terminations until the desired break strength or stiffness has been reached. This type of rope (often specified as cable to make the difference between a braided or twined construction) has the advantage of having no construction stretch as is the case with above constructions. Endless winding is pioneered by SmartRigging and FibreMax.\n\nRope made from hemp, cotton or nylon is generally stored in a cool dry place for proper storage. To prevent kinking it is usually coiled. To prevent fraying or unravelling, the ends of a rope are bound with twine (whipping), tape, or heat shrink tubing. The ends of plastic fibre ropes are often melted and fused solid; however, the rope and knotting expert Geoffrey Budworth warns against this practice thus:\n\"Sealing rope ends this way is lazy and dangerous.\" A tugboat operator once sliced the palm of his hand open down to the sinews after the hardened (and obviously \"sharp\") end of a rope that had been heat-sealed pulled through his grasp. There is no substitute for a properly made whipping.\nIf a load-bearing rope gets a sharp or sudden jolt or the rope shows signs of deteriorating, it is recommended that the rope be replaced immediately and should be discarded or only used for non-load-bearing tasks.\n\nThe average rope life-span is 5 years. Serious inspection should be given to line after that point. However, the use to which a rope is put affects frequency of inspection. Rope used in mission-critical applications, such as mooring lines or running rigging, should be regularly inspected on a much shorter timescale than this, and rope used in life-critical applications such as mountain climbing should be inspected on a far more frequent basis, up to and including before each use.\n\nWhen preparing for a climb, it is important to stack the rope on the ground or a tarp and check for any \"dead-spots\".\n\nAvoid stepping on rope, as this might force tiny pieces of rock through the sheath, which can eventually deteriorate the core of the rope.\nRopes may be flemished into coils on deck for safety and presentation/tidiness as shown in the picture. \n\nMany kinds of filaments in ropes are weakened by acids or other corrosive liquids or solvents, and high temperatures. Such damage is treacherous because it often is hard to tell by eye. Rope damaged in such ways is dangerous to use. Ropes therefore should be kept away from all kinds of solvents and from corrosive acids, alkalis, and oxidising agents.\n\nIn addition, ropes should avoid sudden load, as a shock load can destroy a rope easily. Any operation of ropes should obey the principle of safe working load, which is usually much less than its ultimate strength. The rope should be replaced immediately if any evidences of shock load have been found.\n\nA rope under tension – particularly if it has a great deal of elasticity – can be very hazardous if it should part, snapping backward and potentially causing grave or lethal injury to people, or damage to objects, in its path. There are occasions when it is proper to cut a taut rope under load, but this should be done \"only\" when necessary and \"only\" with great forethought and preparation for the potential consequences.\n\n\"Rope\" refers to the manufactured material. Once rope is purposely sized, cut, spliced, or simply assigned a function, the result is referred to as a \"line\", especially in nautical usage. Sail control lines are mainly referred to as sheets (e.g. jibsheet). A halyard, for example, is a line used to raise and lower a sail, and is typically made of a length of rope with a shackle attached at one end. Other examples include clothesline, chalk line, anchor line (\"rode\"), stern line, fishing line, marline and so on.\n\n\n\n\n\n"}
{"id": "48655460", "url": "https://en.wikipedia.org/wiki?curid=48655460", "title": "Rosaghara", "text": "Rosaghara\n\nRosaghara is a traditional kitchen of Jagannatha temple, Puri, Odisha, India. It is world's largest kitchen.\n\nThe food is cooked by \"suara\"s (also known as \"mahasuara\" or \"supakara\"), a sect that is given the charge since the beginning of the temple.\nThe food cooked in \"rosaghara\" is vegetarian and use of onion, garlic, potatoes and bottle gourd are not allowed. A particular kind of earthenware known as \"kudua\" are used for cooking. Water drawn from two wells near the kitchen called \"Ganga\" and \"Jamuna\" are used for cooking. Over 500 varieties of food \"raja bhoga\", \"chatra bhoga\" and \"jajamani bhoga\" are cooked that are offered to Jagannatha, Balabhadra and Subhadra in the temple pedestal \"ratnabedi\" and food offering pedestal \"Bhoga Mandapa\" five times a day.\n\"Chapana bhoga\", 56 varieties of cooked food are offered almost every day.\n\"Kotha Bhoga\" or \"Abadha\" that is offered as lunch around 1 in the afternoon is the most important food. The food, after being offered to Jagannath, is sold at \"Ananda bajara\" as \"abadha\".Ananda bajara is an open market, located to the North-east of the Singhadwara (major entrance) inside the temple complex. Every day food for over 5000-10000 is cooked where in special occasions food for over 10 million people is cooked in \"rosaghara\". There are two passages to the food out from the kitchen; the first one leads to \"bhoga mandapa\" for larger \"kotha bhoga\" and \"chatra bhoga\", and the other one leading to the inner sanctuary of the temple for the \"kotha bhoga\" offering. Except the \"suara\"s no one is allowed to go near the kitchen or even touch the food until they are offered at the traid of the temple.\n\nThe \"rosaghara\" is located in the temple's south-east direction in the outer compound.\n\nIt is 150 feet long, 100 feet wide and about 20 feet high. There are 32 rooms with 250 earthen hearths within. Around 600 chefs known as \"suara\"s and 400 assistants together cook every day There are three types of hearths in the kitchen; \"Anna Chuli\" the rice hearth, \"Ahia Chuli\" and \"Pitha Chuli\" the dessert hearth. The rice hearth is 4 feet long 2.5 feet wide and 2 feet high. The rectangular space created between two rice hearths is known as \"Ahia\". Lentil and other curries like Besara, \"mahura\" are cooked in the \"Ahia Chuli\". There are ten cement-based \"Pitha chuli\" in \"rosaghara\".\n\nThe legends say that the \"mahasuara\"s work is supervised by Lakshmi where there is custom to promptly burying and starting a new batch of food if Lakshmi has any displeasure with the cooking.\n\n"}
{"id": "58416864", "url": "https://en.wikipedia.org/wiki?curid=58416864", "title": "SCORPION program", "text": "SCORPION program\n\nThe SCORPION (Self CORrecting Projectile for Infantry OperatioN) program was a research initiative led by the U.S. Army Research Laboratory (ARL), the U.S. Defense Advanced Research Projects Agency (DARPA), and the Georgia Institute of Technology to integrate Micro Adaptive Flow Control (MAFC) technology into small caliber munitions to develop spinning, guided projectiles. The program led to the creation of a spin-stabilized 40mm grenade, also called SCORPION, that could propel itself to its target by using calculated micro-jet bursts of air to correct its path once launched.\n\nThe SCORPION program began in 2001 as a joint venture between DARPA, Georgia Tech Research Institute, and ARL’s Weapons and Materials Research Directorate in order to improve the precision of small to medium-sized munitions in complex, dynamic environments.\n\nMicro adaptive flow control is largely defined by the active manipulation of aerodynamic flows using small time-dependent actuators in carefully chosen locations. By taking advantage of flow instabilities, the system can generate large amounts of energy from using only a small fraction of the overall flow if the actuators and their activity are properly assigned. As a result, MAFC can enable low power yet highly distributed redundant actuation systems in contrast to flow systems that utilize steady blowing where high velocity flows must be provided through complex and high-loss ducting. The project was spearheaded by the proposition that MAFC and advanced microtechnology could be combined to establish closed loop guidance of a projectile, demonstrating more effective flight control and steering capabilities.\n\nThe program was divided into two phases, Phase I and Phase II. The former focused on constructing the flight control system and determining whether MAFC could be integrated into a 40 mm round to provide adequate guidance to its target, while the latter prioritized determining whether MAFC technology could be used to steer projectiles that were smaller and even faster than those used in Phase I. By 2005, Phase I had been completed, and the SCORPION program was chosen as a DARPA demonstration program in order to further investigate the use of MAFC in high-velocity, small-diameter projectiles.\nA Success Story\nThe SCORPION program concluded in 2007 after successfully demonstrating the use of MCFA to maneuver a 40 mm rifle-launched grenade. In 2009, ARL researchers at White Sands Missile Range, New Mexico tested the renamed XM1100 Scorpion networked sensor and munitions platform and achieved what they called a “mobility kill” for the first time, highlighting the system’s ability to identify and track its targets. According to the researchers, the XM1100 Scorpion successfully demonstrated all of its major functions, including command and control, ground sensor tracking, target engagement, anti-vehicle munitions launch, warhead lethality, and self-destruction. If successful in the government development testing phase, the Pentagon has displayed interest in utilizing the munition system for long-range combat in urban environments.\n\nDesigned to be fired from the M203 grenade launcher, the SCORPION projectile is a 40 mm grenade that was based on the M781BT practice grenade. Due to how 40 mm grenades are spin stabilized and have highly nonlinear aerodynamics, researchers undergone several tests to understand the aerodynamic nonlinearities and flight dynamics of the projectile’s trajectory. Experiments with the base model has shown that these projectiles exhibit fast and slow mode angular precession, meaning that the aerodynamic control system must not only deal with a spin rate of 60 Hz, but also account for the nonlinear response of the round. Both the rotational motion and the precession of the projectile served to greatly complicate how the projectile respond to control forces.\n\nThe SCORPION projectile features a telemetry system based on the Army Research Laboratory’s diagnostic fuze (DFuze), a high-g, projectile-based sensor system that measures inflight ballistic data. Used primarily to determine the projectile flight dynamics along the trajectory, the telemetry system consists of four radial accelerometers, a three-axis magnetometer, an axial accelerometer, a two-axis accelerometer for transverse acceleration, and a suite of four Yawsondes. While the magnetometers serve to measure the SCORPION’s projectile orientation and roll angle to the earth’s magnetic field, the Yawsondes measure its projectile angular orientation to the sun. Through the use of the magnetometers and the Yawsondes, the angular state of the projectile can be identified. The accelerometers help measure the accelerations of the SCORPION in the x, y, and z directions, and an encoder board is present inside the projectile to collect the sensor data and send it to the ground station.\n\nAs a result of its telemetry system, the researchers can monitor the forces on the SCORPION as it flies to its target. However, in order to control its flight path, the projectile employ MAFC through the use of tiny synthetic jets embedded on the surface of the projectile. Developed by researchers at the Georgia Institute of Technology, these synthetic jet actuators are designed to fire bursts of air to alter the flow field and pressure distributions of the surrounding air. When turned on momentarily, these synthetic jets use those tiny, short bursts to correct the trajectory of the projectile as it travels towards its target.\n\nAs active control devices with zero net mass flux, the synthetic jets produce the desired amount of control of the flow field through momentum effects. They act as steering devices for the projectile, using a minute-vibrating diaphragm-like system powered by piezoceramic elements. Due to the tiny air vortices created by the synthetic jets, an asymmetry in the airflow can be created around the projectile. In addition, the resulting Coanda effect can multiply this effects of this phenomenon, producing air flow changes that are strong enough to change the projectile’s trajectory. The SCORPION’s electronics have also undergone g-hardening to withstand the heavy forces generated during each launch.\n\nHowever, issues still remain with this design. The SCORPION projectile lacks sufficient payload room for a full guidance system as well as a greater explosive power. As a result, researchers have investigated using gas-generator actuators, which employ miniature explosive charges to create stronger steering forces and allow the projectile to travel faster in the air.\n\nIn addition to the 40 mm SCORPION, a 25 mm SCORPION has also been developed. Designed similarly to the 40 mm model, it features two axes of rate sensors, three aces of accelerometers, three axes of magnetometers, and two additional radially oriented accelerometers. The 25 mm SCORPION consists of two sections, the electronics control module and the actuator module. This divide serves to prevent potentially hazardous material like the propellant from interacting with the control electronics until just prior to firing. According to flight experiments conducted at the Army Research Laboratory, the 25 mm SCORPION successfully established the charge weight needed to meet the velocity requirements when launched at 0.8 Mach.\n"}
{"id": "1523382", "url": "https://en.wikipedia.org/wiki?curid=1523382", "title": "Satchel charge", "text": "Satchel charge\n\nA satchel charge is a demolition device, primarily intended for combat, whose primary components are a charge of dynamite or a more potent explosive such as C-4 plastic explosive, a carrying device functionally similar to a satchel or messenger bag, and a triggering mechanism; the term covers both improvised and formally designed devices.\n\nIn World War II, combat engineers used satchel charges to demolish heavy stationary targets such as rails, obstacles, blockhouses, bunkers, caves, and bridges. The World War II-era United States Army M37 Demolition Kit contained eight blocks of high explosive, with 2 priming assemblies, in a canvas bag with a shoulder strap. Part or all of this charge could be placed against a structure or slung into an opening. It was usually detonated with a pull igniter. When used as an anti-tank weapon, charges were sufficient to severely damage the tracks. charges were enough to destroy medium tanks.\n\nLater in the Vietnam War, Vietcong and North Vietnamese soldiers assigned elite sappers to stealthily penetrate defenses of sites controlled by enemy forces. Often, this meant using satchel charges as well as Bangalore torpedoes to blast through barbed wire entanglements, minefields, structures, and other fortifications. The later U.S. M183 Demolition Charge Assembly contained of C-4 in each satchel, and could be used with a timed fuse. In the Second Battle of Fallujah in Iraq, U.S. M2 20 lb assault demolitions were used to collapse houses being used as fighting positions by insurgents.\n\nSome special forces may use customized satchel charges designed to destroy their specific mission's target.\n\n"}
{"id": "9779961", "url": "https://en.wikipedia.org/wiki?curid=9779961", "title": "Semantic layer", "text": "Semantic layer\n\nA semantic layer is a business representation of corporate data that helps end users access data autonomously using common business terms. A semantic layer maps complex data into familiar business terms such as product, customer, or revenue to offer a unified, consolidated view of data across the organization.\nBy using common business terms, rather than data language, to access, manipulate, and organize information, a semantic layer simplifies the complexity of business data. Business terms are stored as objects in a semantic layer, which are accessed through business views.\n\nThe semantic layer enables business users to have a common \"look and feel\" when accessing and analyzing data stored in relational databases and OLAP cubes. This is claimed to be core business intelligence (BI) technology that frees users from IT while ensuring correct results.\n\nBusiness Views is a multi-tier system that is designed to enable companies to build comprehensive and specific business objects that help report designers and end users access the information they require. Business Views is intended to enable people to add the necessary business context to their data islands and link them into a single organized Business View for their organization.\n\nSemantic layer maps tables to classes and rows to objects.\n"}
{"id": "11259589", "url": "https://en.wikipedia.org/wiki?curid=11259589", "title": "Snag (website)", "text": "Snag (website)\n\nSnag (formerly known as Snagajob) is an online staffing platform specializing in hourly work. Founded in 2000 with offices in Washington, D.C., Richmond, Virginia, and Charleston, South Carolina, Snag has been named to Fortune Magazine’s \"Great Place to Work\" Best Small & Medium Workplaces list for eight consecutive years. Snag has also been named to Washingtonian's \"Great Places to Work\" and Deloitte's \"Fast 500\", a ranking of the fastest growing technology, media, telecommunications, life sciences and clean technology companies in North America.\n\nShawn Boyer served as Snagajob’s (now Snag) CEO from the website's launch in early 2000, to 2013; leading the company from a start-up to national employment network. Boyer was named America's Small Business Person of the year in 2008 by the Small Business Administration and Virginia Business magazine’s Person of the Year in 2009.\n\nBoyer launched Snagajob.com out of his basement in his home in Washington D.C. In 2005, the company had grown to 25 employees and by 2007, had increased to 100 employees.\n\nIn 2011, Snagajob (Snag) was named the best small business to work for by the Great Place to Work Institute. Virginia Governor Bob McDonnell visited Snagajob headquarters the day of the announcement to offer his congratulations.\n\nPeter Harrison was named chief executive officer of Snag in April 2013 and served until July 2018.\n\nIn June 2016 Snag acquired PeopleMatter, a company that develops and provides software tools that help employers manage their workforce.\n\nSnag is privately held and has raised over $141 million from investors including Adams Street Partners, Baird Venture Partners, C&B Capital, Rho Acceleration, NewSpring Capital, StarVest Partners and August Capital.\n\nAs of December 2016, Snag had over 500 employees, more than 85 million members registered on its platform and was active at over 450,000 customer locations.\n\nOn April 3, 2018 changed its name from Snagajob to Snag and launched a rebrand with a new visual identity to reflect the company’s evolution.\n\nFabio Rosati was named Chairman and CEO of Snag in July 2018.\n\n"}
{"id": "1118544", "url": "https://en.wikipedia.org/wiki?curid=1118544", "title": "Squid (weapon)", "text": "Squid (weapon)\n\nSquid was a British World War II ship-mounted anti-submarine weapon. It consisted of a three-barrelled mortar which launched depth charges. It replaced the Hedgehog system, and was in turn replaced by the Limbo system.\n\nLiterally ordered directly from the drawing board in 1942, under the auspices of the Directorate of Miscellaneous Weapons Development, this weapon was rushed into service in May 1943 on board HMS \"Ambuscade\". The first production unit was installed on HMS \"Hadleigh Castle\"; it went on to be installed on 70 frigates and corvettes during the Second World War. The first successful use was by HMS \"Loch Killin\" on 31 July 1944, when she sank \"U333\"; the system was credited with sinking 17 submarines in 50 attacks. By 1959, 195 Squid installations had been produced.\n\nThis weapon was a three-barrel mortar with the mortars mounted in series but off-bore from each other in order to scatter the projectiles. The barrels were mounted in a frame that could be rotated through 90 degrees for loading. The projectiles weighed with a minol charge. On some vessels, the Squid installations were at the stern – the bombs were fired over the length of the ship and dropping into the sea slightly ahead of it. Sink rate was 43.5 ft/s (13.3 m/s) and a clockwork time fuze was used to determine the detonation depth; all three projectiles had to be set to the same depth; this could be continuously updated right up to the moment of launch to take into account the movements of the target. The maximum depth was .\n\nThe weapons were automatically fired from the sonar range recorder at the proper moment. The pattern formed a triangle about 40 yards (37 m) on a side at a distance of 275 yards (250 m) ahead of the ship. Most Squid installations utilised two sets of mortars. All six bombs were fired in salvo so they formed opposing triangular spreads. The salvos were set to explode above and below the target, the resulting pressure wave crushing the hull of the submarine. Postwar trials found Squid was nine times more effective than conventional depth charges.\n\nDespite its proven effectiveness, some officers, notably Captain Kenneth Adams, RCN, opposed fitting Squid to escorts because it meant sacrificing guns, which would make ships unsuitable for fleet actions.\n\nIn April 1977, the Type 61 frigate \"Salisbury\" became the last ship to fire Squid in Royal Navy service. Examples of the mortars are on display at the Explosion! Museum of Naval Firepower in Gosport, Hampshire and another at Devonport Naval Base. In addition, the system is fitted to , which is part of the historic ships collection in the Historic Dockyard in Chatham, Kent.\n\nIn Swedish service the system soldiered on until 1982 when the \"Östergötland\"-class destroyers were decommissioned.\n\n\n"}
{"id": "46640131", "url": "https://en.wikipedia.org/wiki?curid=46640131", "title": "Superstore (TV series)", "text": "Superstore (TV series)\n\nSuperstore is an American single-camera sitcom television series that premiered on NBC on November 30, 2015. The series was created by Justin Spitzer, who also serves as an executive producer. Starring America Ferrera (who also serves as a producer), Ben Feldman and Mark McKinney, \"Superstore\" follows a group of employees working at \"Cloud 9\", store number 1217, a fictional big-box store in St. Louis, Missouri. The ensemble and supporting cast includes Lauren Ash, Colton Dunn, Nico Santos and Nichole Bloom.\n\nOn February 21, 2018, NBC renewed the series for a 22-episode fourth season, which premiered on October 4, 2018.\n\n\n\n\nThe series was one of three pilots picked up by NBC on January 14, 2015, along with the sitcom \"Crowded\"; both were green lighted to series status on May 7, 2015. The series was the first project for Ruben Fleischer's newly formed company The District as part of a two-year deal with Universal, as he directed the pilot episode. \"Superstore\" was officially picked up as a series on May 7, 2015, by NBC. The first season consisted of eleven episodes, after the episode order was reduced from thirteen on October 19, 2015. It was announced on November 2, 2015, that the series would air the premiere on January 4, 2016, but would be airing two back-to-back episodes on November 30, 2015, following \"The Voice\".\n\nOn February 23, 2016, the series was renewed for a second season by NBC. On May 15, 2016, NBC announced that the series would lead off its Thursday night primetime programming in the 2016–17 season. The second season premiered on September 22, 2016, with a 22-episode order that was announced on September 23, 2016. The season concluded on May 4, 2017. A special Olympics-themed episode aired on August 19, 2016 during the network's coverage of the 2016 Summer Olympic Games in Rio de Janeiro.\n\nIt was announced on February 20, 2015, that Lauren Ash had been cast as a series regular, and would be playing Dina, the store’s assistant manager. On March 2, 2015, \"Deadline\" reported that \"Superstore\" had added three other cast members: Colton Dunn, Mark McKinney and Nico Santos. The website reported that Dunn would be playing Garret, the often-sarcastic narrator of the piece, McKinney would be playing Glenn, the intense store manager, and Santos would be playing Mateo, another new employee and a brown-noser from an impoverished background. On March 12, 2015, Nichole Bloom was announced to have joined the show as Cheyenne, a very pregnant teenage employee.\n\n\"Deadline\" announced on March 13, 2015, that Ben Feldman had landed the male lead in \"Superstore\", as Jonah, a new employee in the superstore Cloud 9. Three days later, \"TVLine\" announced on March 16, 2015, that America Ferrera had landed the female lead as the floor supervisor Amy in the Cloud 9 store. It was also reported that Ferrera was also a producer for the show.\n\nThe Cloud 9 store has also appeared in other series produced by NBCUniversal Television, including Hulu's \"The Mindy Project\" and NBC's \"Good Girls\".\n\nThe Cloud 9 Superstore is a fictional hypermarket discount store. In addition to typical American hypermarket products, Cloud 9 also sells guns and liquor, and has a pharmacy. Additionally Cloud 9 has their own credit union for their employees. They also previously had a photo studio. The former spokesman for Cloud 9 was Daniel Hertzler (as \"Kyle the Cloud 9 Cloud\"), until he was arrested and charged with cannibalism.\n\nThe corporation does not offer paid maternity leave, health insurance or paid overtime to its employees. Under Cloud 9 policy employees may take one bathroom break per shift, and are allotted 15 minutes for lunch.\n\nIn an effort to control whats happening in the individual stores, all locks and lights, as well as temperature control, are controlled from the corporate office. In 2017, Cloud 9 changed their store brand from Halo to Super Cloud.\n\nThe main characters for the show work at store 1217, the \"Ozark Heights\" store which is located in St. Louis, Missouri, on Ozark Highlands Road. The store falls under district manager Jeff Sutton. The store was destroyed by a tornado during the season 2 finale, and reopened during the season 3 premiere. Other area locations include Kirkwood, Fenton and Easton. Additionally there are locations in Austin, Texas, and in Detroit, Michigan. Cloud 9 has locations in multiple countries, with stores in Beijing, Mumbai, Paris, Vancouver and Mexico City.\n\nThe pilot was shot at a redressed Kmart in Burbank, California, though the rest of the series has been shot on sets constructed on two soundstages.\n\nThe series debuted as a \"preview\" on November 30, 2015 following an episode of \"The Voice\" with 7 million viewers making it the second highest new comedy behind \"Life in Pieces\". The series then moved to its regular Monday at 8:00 pm timeslot on January 4, 2016 with more than 6 million viewers making the highest rated comedy that did not have \"The Voice\" as a lead-in since \"The Michael J. Fox Show\" back in September 2013.\n\nEarly reviews for the series were mixed. According to Metacritic, the first season of \"Superstore\" holds a score of 58 out of 100, indicating \"mixed to average reviews\" based on 21 critics. On another review aggregate website Rotten Tomatoes, the first season holds a 54% with a \"Rotten\" rating, based on 24 critics, with an average rating of 4.4/10. The general consensus is: \"\"Superstore\"s talented cast and obvious potential are slightly overshadowed by a tonally jumbled presentation and thin, formulaic writing.\" As the first season went along, however, reviews started to become more positive. Following the finale \"Labor\", the \"Los Angeles Times\" called it one of TV's best new comedies.\" Pilot Viruet of \"The A.V. Club\" wrote that the \"first season ... got better and more confident as it moved on\", and that the first-season finale \"is a nice little cap to a nice little sitcom that could’ve used a little more attention.\" After the series aired its Olympics special, \"Variety\" wrote that the show was \"a funny, pointed and essential workplace comedy\", and that \"there are no weak links in [the] ensemble\". The second season was lauded by critics and the season has a 100% approval rating on Rotten Tomatoes.\n\n"}
{"id": "4182449", "url": "https://en.wikipedia.org/wiki?curid=4182449", "title": "Tablet computer", "text": "Tablet computer\n\nA tablet computer, commonly shortened to tablet, is a mobile device, typically with a mobile operating system and LCD touchscreen display processing circuitry, and a rechargeable battery in a single thin, flat package. Tablets, being computers, do what other personal computers do, but lack some I/O capabilities that others have. Modern tablets largely resemble modern smartphones, the only differences being that tablets are relatively larger than smartphones, with screens or larger, measured diagonally, and may not support access to a cellular network.\n\nThe touchscreen display is operated by gestures executed by finger or stylus instead of the mouse, trackpad, and keyboard of larger computers. Portable computers can be classified according to the presence and appearance of physical keyboards. Two species of tablet, the \"slate\" and \"booklet\", do not have physical keyboards and usually accept text and other input by use of a virtual keyboard shown on their touchscreen displays. To compensate for their lack of a physical keyboard, most tablets can connect to independent physical keyboards by wireless Bluetooth or USB; 2-in-1 PCs have keyboards, distinct from tablets.\n\nThe form of the tablet was conceptualized in the middle of the 20th century (Stanley Kubrick depicted fictional tablets in the 1968 science fiction film \"\") and prototyped and developed in the last two decades of that century. In 2010, Apple released the iPad, the first mass-market tablet to achieve widespread popularity. Thereafter tablets rapidly rose in ubiquity and briefly became a large product category used for personal, educational and workplace applications, with sales stabilizing in the mid-2010s.\n\nThe tablet computer and its associated operating system began with the development of pen computing. Electrical devices with data input and output on a flat information display existed as early as 1888 with the telautograph, which used a sheet of paper as display and a pen attached to electromechanical actuators. Throughout the 20th century devices with these characteristics have been imagined and created whether as blueprints, prototypes, or commercial products. In addition to many academic and research systems, several companies released commercial products in the 1980s, with various input/output types tried out:\n\nTablet computers appeared in a number of works of science fiction in the second half of the 20th century; all helped to promote and disseminate the concept to a wider audience. Examples include:\n\nAdditionally, real-life projects either proposed or created tablet computers, such as:\n\nFollowing earlier tablet computer products such as the Pencept PenPad, and the CIC Handwriter, in September 1989, GRiD Systems released the first commercially successful tablet computer, the GRiDPad. All three products were based on extended versions of the MS-DOS operating system. In 1992, IBM announced (in April) and shipped to developers (in October) the 2521 ThinkPad, which ran the GO Corporation's PenPoint OS. Also based on PenPoint was AT&T's EO Personal Communicator from 1993, which ran on AT&T's own hardware, including their own AT&T Hobbit CPU. Apple Computer launched the Apple Newton personal digital assistant in 1993. It used Apple's own new Newton OS, initially running on hardware manufactured by Motorola and incorporating an ARM CPU, that Apple had specifically co-developed with Acorn Computers. The operating system and platform design were later licensed to Sharp and Digital Ocean, who went on to manufacture their own variants.\n\nIn 1996, Palm, Inc. released the first of the Palm OS based PalmPilot touch and stylus based PDA, the touch based devices initially incorporating a Motorola Dragonball (68000) CPU. Also in 1996 Fujitsu released the Stylistic 1000 tablet format PC, running Microsoft Windows 95, on a 100 MHz AMD486 DX4 CPU, with 8 MB RAM offering stylus input, with the option of connecting a conventional Keyboard and mouse. Intel announced a StrongARM processor-based touchscreen tablet computer in 1999, under the name WebPAD. It was later re-branded as the \"Intel Web Tablet\". In 2000, Norwegian company Screen Media AS and the German company Dosch & Amand Gmbh released the \" FreePad\". It was based on Linux and used the Opera browser. Internet access was provided by DECT DMAP, only available in Europe and provided up to 10Mbit/s. The device had 16 MB storage, 32 MB of RAM and x86 compatible 166 MHz \"Geode\"-Microcontroller by National Semiconductor. The screen was 10.4\" or 12.1\" and was touch sensitive. It had slots for SIM cards to enable support of television set-up box. FreePad were sold in Norway and the Middle East; but the company was dissolved in 2003.\nIn April 2000, Microsoft launched the Pocket PC 2000, using their touch capable Windows CE 3.0 operating system. The devices were manufactured by several manufacturers, based on a mix of: x86, MIPS, ARM, and SuperH hardware. In 2002, Microsoft attempted to define the Microsoft Tablet PC as a mobile computer for field work in business, though their devices failed, mainly due to pricing and usability decisions that limited them to their original purpose - such as the existing devices being too heavy to be held with one hand for extended periods, and having legacy applications created for desktop interfaces and not well adapted to the slate format.\nNokia had plans for an Internet tablet since before 2000. An early model was test manufactured in 2001, the Nokia M510, which was running on EPOC and featuring an Opera browser, speakers and a 10-inch 800×600 screen, but it was not released because of fears that the market was not ready for it. Nokia entered the tablet space in May 2005 with the Nokia 770 running Maemo, a Debian-based Linux distribution custom-made for their Internet tablet line. The user interface and application framework layer, named Hildon, was an early instance of a software platform for generic computing in a tablet device intended for internet consumption. But Nokia didn't commit to it as their only platform for their future mobile devices and the project competed against other in-house platforms and later replaced it with the Series 60. Nokia used the term \"internet tablet\" to refer to a portable information appliance that focused on Internet use and media consumption, in the range between a personal digital assistant (PDA) and an Ultra-Mobile PC (UMPC). They made two mobile phones, the N900 that runs Maemo, and N9 that run Meego. \n\nBefore the release of iPad, Axiotron introduced an aftermarket, heavily modified Apple MacBook called Modbook, a Mac OS X-based tablet computer. The Modbook uses Apple's Inkwell for handwriting and gesture recognition, and uses digitization hardware from Wacom. To get Mac OS X to talk to the digitizer on the integrated tablet, the Modbook was supplied with a third-party driver.\n\nFollowing the launch of the Ultra-mobile PC, Intel started the Mobile Internet Device initiative, which took the same hardware and combined it with a tabletized Linux configuration. Intel co-developed the lightweight Moblin (mobile Linux) operating system following the successful launch of the Atom CPU series on netbooks. In 2010, Nokia and Intel combined the Maemo and Moblin projects to form MeeGo, a Linux-based operating system supports netbooks and tablets. The first tablet using MeeGo was the Neofonie WeTab launched September 2010 in Germany. The WeTab used an extended version of the MeeGo operating system called WeTab OS. WeTab OS adds runtimes for Android and Adobe AIR and provides a proprietary user interface optimized for the WeTab device. On September 27, 2011 the Linux Foundation announced that MeeGo would be replaced in 2012 by Tizen.\n\nAndroid was the first of the 2000s-era dominating platforms for tablet computers to reach the market. In 2008, the first plans for Android-based tablets appeared. The first products were released in 2009. Among them was the Archos 5, a pocket-sized model with a 5-inch touchscreen, that was first released with a proprietary operating system and later (in 2009) released with Android 1.4. The Camangi WebStation was released in Q2 2009. The first LTE Android tablet appeared late 2009 and was made by ICD for Verizon. This unit was called the Ultra, but a version called Vega was released around the same time. Ultra had a 7-inch display while Vega's was 15 inches. Many more products followed in 2010. Several manufacturers waited for Android Honeycomb, specifically adapted for use with tablets, which debuted in February 2011.\n\nApple is often credited for defining a new class of consumer device with the iPad, which shaped the commercial market for tablets in the following years, and was the most successful tablet at the time of its release. iPads and competing devices were tested by the US military in 2011 and cleared for secure use in 2013. Its debut in 2010 pushed tablets into the mainstream. Samsung's Galaxy Tab and others followed, continuing the trends towards the features listed above. In March 2012, \"PC Magazine\" reported that 31% of U.S. Internet users owned a tablet, used mainly for viewing published content such as video and news. The top-selling line of devices was Apple's iPad with 100 million sold between its release in April 2010 and mid-October 2012, but iPad market share (number of units) dropped to 36% in 2013 with Android tablets climbing to 62%. Android tablet sales volume was 121 million devices, plus 52 million, between 2012 and 2013 respectively. Individual brands of Android operating system devices or compatibles follow iPad with Amazon's Kindle Fire with 7 million, and Barnes & Noble's Nook with 5 million.\n\nThe BlackBerry PlayBook was announced in September 2010 that ran the BlackBerry Tablet OS. The BlackBerry PlayBook was officially released to US and Canadian consumers on April 19, 2011. Hewlett Packard announced that the TouchPad, running WebOS 3.0 on a 1.2 GHz Qualcomm Snapdragon CPU, would be released in June 2011. On August 18, 2011, HP announced the discontinuation of the TouchPad, due to sluggish sales. In 2013, the Mozilla Foundation announced a prototype tablet model with Foxconn which ran on Firefox OS. Firefox OS was discontinued in 2016. The Canonical hinted that Ubuntu would be available on tablets by 2014. In February 2016 there was a commercial release of the BQ Aquaris Ubuntu tablet utilizing the Ubuntu Touch operating system. Canonical terminated support for the project due to lack of market interest on 5 April 2017 and it was then adopted by the UBports as a community project.\n\nAs of February 2014, 83% of mobile app developers were targeting tablets, but 93% of developers were targeting smartphones. By 2014 around 23% of B2B companies were said to have deployed tablets for sales-related activities, according to a survey report by Corporate Visions. The iPad holds majority use in North America, Western Europe, Japan, Australia, and most of the Americas. Android tablets are more popular in most of Asia (China and Russia an exception), Africa and Eastern Europe. In 2015 tablet sales did not increase. Apple remained the largest seller but its market share declined below 25%. Samsung vice president Gary Riding said early in 2016 that tablets were only doing well among those using them for work. Newer models were more expensive and designed for a keyboard and stylus, which reflected the changing uses. As of early 2016, Android reigns over the market with 65%. Apple takes the number 2 spot with 26%, and Windows take a distant third with the remaining 9%. In 2018, out of 4.4 billion computing devices Android accounted for 2 billion, iOS for 1 billion, and the remainder were PCs, in various forms (desktop, notebook, or tablet), running various operating systems (Windows, macOS, ChromeOS, Linux, etc.). \n\nIn late 2017, the iPad Pro received the iOS 11 update, adding the ability to run multiple windows, drag and drop from one app to another, and browse a user's files.\n\nTablets can be loosely grouped into several categories by physical size, kind of operating system installed, input and output technology, and uses.\n\nThe size of a slate varies, but slates begin at 6 inches (approximately 15 cm). Some models in the larger than 10-inch (25 cm) category include the Samsung Galaxy Tab Pro 12.2 at 12.2 inches (31 cm), the Toshiba Excite at 13.3 inches (33 cm) and the Dell XPS 18 at 18.4 inches (47 cm). As of March 2013, the thinnest tablet on the market was the Sony Xperia Tablet Z at only 0.27 inches (6.9 mm) thick. On September 9, 2015, Apple released the iPad Pro with a screen size, larger than the regular iPad.\n\nMini tablets are smaller and weigh less than slates, with typical screen sizes between . The first commercially successful mini tablets were introduced by Amazon.com (Kindle Fire), Barnes & Noble (Nook Tablet), and Samsung (Galaxy Tab) in 2011; and by Google (Nexus 7) in 2012. They operate identically to ordinary tablets but have lower specifications compared to them.\n\nOn September 14, 2012, Amazon, Inc. released an upgraded version of the Kindle Fire, the Kindle Fire HD, with higher screen resolution and more features compared to its predecessor, yet remaining only 7 inches. In October 2012, Apple released the iPad Mini with a 7.9 inch screen size, about 2 inches smaller than the regular iPad, but less powerful than the then current iPad 3. On July 24, 2013, Google released an upgraded version of the Nexus 7, with FHD display, dual cameras, stereo speakers, more color accuracy, performance improvement, built-in wireless charging, and a variant with 4G LTE support for AT&T, T-Mobile, and Verizon. In September 2013, Amazon further updated the Fire tablet with the Kindle Fire HDX. In November 2013, Apple released the iPad Mini 2, which remained at 7.9 inches and nearly matched the hardware of the iPad Air.\n\nSmartphones and tablets are similar devices, differentiated by the former typically having smaller screens and most tablets lacking cellular network capability. Since 2010, crossover touchscreen smartphones with screens larger than 5 inches have been released. That size is generally considered larger than a traditional smartphone, creating the hybrid category of the \"phablet\" by Forbes and other publications. \"Phablet\" is a portmanteau of \"phone\" and \"tablet\".\n\nAt the time of the introduction of the first phablets, they had screens of 5.3 to 5.5 inches, but as of 2017 screen sizes up to 5.5 inches are considered typical. Examples of phablets from 2017 and onward are the Samsung Galaxy Note series (newer models of 5.7 inches), the LG V10/V20 (5.7 inches), the Sony Xperia XA Ultra [6 inches], the Huawei Mate 9 (5.9 inches), and the Huawei Honor (MediaPad) X2 (7 inches).\n\nDistinct from tablets, 2-in-1 PCs all have physical keyboards, but they are either concealable by folding them back and under the touchscreen (\"2-in-1 convertible\") or detachable (\"2-in-1 detachable\"), but 2-in-1s typically also can display a virtual keyboard on their touchscreens when their physical keyboards are concealed or detached. Some 2-in-1s have processors and operating systems like those of laptops, while having the flexibility of operation as a tablet. A 2-in-1 PC is a hybrid or combination of a tablet and laptop computer that has features of both. An essential feature of a 2-in-1 is a desktop operating system, such as Windows 10, as opposed to a mobile operating system. Additionally, 2-in-1s may have typical laptop I/O ports, such as USB 3 and DisplayPort, and may connect to traditional PC peripheral devices and external displays. Therefore they are distinct from tablets, although they can operate just like them. Simple tablets are primarily used as media consumption devices, while 2-in-1s have capacity for both media consumption and content creation, and therefore 2-in-1s are often called \"laptop\" or \"desktop replacement computers\".\n\nThere are two species of 2-in-1s:\n\nSome tablets are modified by adding physical gamepad buttons such as D-pad and thumb sticks for better gaming experience combined with the touchscreen and all other features of a typical tablet computer. Most of these tablets are targeted to run native OS games and emulator games. Nvidia's Shield Tablet, with an display, and running Android, is an example. It runs Android games purchased from Google Play store. PC games can also be streamed to the tablet from computers with some higher end models of Nvidia-powered video cards. The Nintendo Switch hybrid console is also a gaming tablet that runs on Nintendo Switch system software, features detachable Joy-Con controllers with motion controls and three gaming modes: table-top mode using its kickstand, traditional docked/TV mode and handheld mode.\n\nBooklets are dual-touchscreen tablet computers with a clamshell design that can fold like a laptop. Examples include the Microsoft Courier, which was discontinued in 2010, the Sony Tablet P (which was considered a flop), and the Toshiba Libretto W100.\n\nCustomized business tablets are built specifically for a business customer's particular needs from a hardware and software perspective, and delivered in a business-to-business transaction. For example, in hardware, a transportation company may find that the consumer-grade GPS module in an off-the-shelf tablet provides insufficient accuracy, so a tablet can be customized and embedded with a professional-grade antenna to provide a better GPS signal. Such tablets may also be ruggedized for field use. For a software example, the same transportation company might remove certain software functions in the Android system, such as the internet browser, to reduce costs from unnecessary cellular network data consumption of an employee, and add custom package management software. Other applications may call for a resistive touchscreen and other special hardware and software.\nA table ordering tablet is a touchscreen tablet computer designed for use in casual restaurants. Such devices allow users to order food and drinks, play games and pay their bill. Since 2013, restaurant chains including Chili's, Olive Garden and Red Robin have adopted them. As of 2014, the two most popular brands were Ziosk and Presto. The devices have been criticized by servers who claim that some restaurants determine their hours based on customer feedback in areas unrelated to service.\n\nTwo major architectures dominate the tablet market, ARM Holdings' ARM architecture and Intel's and AMD's x86. Intel's x86, including x86-64 has powered the \"IBM compatible\" PC since 1981 and Apple's Macintosh computers since 2006. The CPUs have been incorporated into tablet PCs over the years and generally offer greater performance along with the ability to run full versions of Microsoft Windows, along with Windows desktop and enterprise applications. Non-Windows based x86 tablets include the JooJoo. Intel announced plans to enter the tablet market with its Atom in 2010. In October 2013, Intel's foundry operation announced plans to build FPGA-based quad cores for ARM and x86 processors.\n\nARM has been the CPU architecture of choice for manufacturers of smartphones (95% ARM), PDAs, digital cameras (80% ARM), set-top boxes, DSL routers, smart televisions (70% ARM), storage devices and tablet computers (95% ARM). This dominance began with the release of the mobile-focused and comparatively power-efficient 32-bit ARM610 processor originally designed for the Apple Newton in 1993 and ARM3-using Acorn A4 laptop in 1992. The chip was adopted by Psion, Palm and Nokia for PDAs and later smartphones, camera phones, cameras, etc. ARM's licensing model supported this success by allowing device manufacturers to license, alter and fabricate custom SoC derivatives tailored to their own products. This has helped manufacturers extend battery life and shrink component count along with the size of devices.\n\nThe multiple licensees ensured that multiple fabricators could supply near-identical products, while encouraging price competition. This forced unit prices down to a fraction of their x86 equivalents. The architecture has historically had limited support from Microsoft, with only Windows CE available, but with the 2012 release of Windows 8, Microsoft announced additional support for the architecture, shipping their own ARM-based tablet computer, branded the Microsoft Surface, as well as an x86-64 Intel Core i5 variant branded as Microsoft Surface Pro. Intel tablet chip sales were 1 million units in 2012, and 12 million units in 2013. Intel chairman Andy Bryant has stated that its 2014 goal is to quadruple its tablet chip sales to 40 million units by the end of that year, as an investment for 2015.\n\nA key component among tablet computers is touch input on a touchscreen liquid-crystal display (LCD). This allows the user to navigate easily and type with a virtual keyboard on the screen or press other icons on the screen to open apps or files. The first tablet to do this was the GRiDPad by GRiD Systems Corporation; the tablet featured both a stylus, a pen-like tool to aid with precision in a touchscreen device as well as an on-screen keyboard. The system must respond to on-screen touches rather than clicks of a keyboard or mouse. This operation makes precise use of our eye–hand coordination.\n\nTouchscreens usually come in one of two forms:\n\nSince mid-2010s, most tablets use capacitive touchscreens with multi-touch, unlike earlier resistive touchscreen devices which users needed styluses in order to perform inputs.\n\nSome ARM powered tablets, such as the Galaxy Note 10, support a stylus and support handwriting recognition. Wacom and N-trig digital pens provide approximately 2500 DPI resolution for handwriting, exceeding the resolution of capacitive touch screens by more than a factor of 10. These pens also support pressure sensitivity, allowing for \"variable-width stroke-based\" characters, such as Chinese/Japanese/Korean writing, due to their built-in capability of \"pressure sensing\". Pressure is also used in digital art applications such as Autodesk Sketchbook. Apps exist on both iOS and Android platforms for handwriting recognition and in 2015 Google introduced its own handwriting input with support for 82 languages.\n\nAfter 2007, with access to capacitive screens and the success of the iPhone, other features became common, such as multi-touch features (in which the user can touch the screen in multiple places to trigger actions and other natural user interface features, as well as flash memory solid state storage and \"instant on\" warm-booting; external USB and Bluetooth keyboards defined tablets.\n\nMost tablets released since mid-2010 use a version of an ARM processor for longer battery life. The ARM Cortex family is powerful enough for tasks such as internet browsing, light production work and mobile games.\n\nOther features are: High-definition, anti-glare display, touchscreen, lower weight and longer battery life than a comparably-sized laptop, wireless local area and internet connectivity (usually with Wi-Fi standard and optional mobile broadband), Bluetooth for connecting peripherals and communicating with local devices, ports for wired connections and charging, for example USB ports, Early devices had IR support and could work as a TV remote controller, docking station, keyboard and additional connectivity, on-board flash memory, ports for removable storage, various cloud storage services for backup and syncing data across devices, Local storage on a LAN\n\n\nTablets, like conventional PCs, use several different operating systems, though dual-booting is rare. Tablet operating systems come in two classes:\n\n\nDesktop OS-based tablets are currently thicker and heavier. They require more storage and more cooling and give less battery life. They can run processor-intensive graphical applications in addition to mobile apps, and have more ports.\n\nMobile-based tablets are the reverse, and run only mobile apps. They can use battery life conservatively because the processor is significantly smaller. This allows the battery to last much longer than the common laptop.\n\nIn Q1 2018, Android tablets had 62% of the market, Apple's iOS had 23.4% of the market and Windows 10 had 14.6% of the market.\n\nAndroid is a Linux-based operating system that Google offers as open source under the Apache license. It is designed primarily for mobile devices such as smartphones and tablet computers. Android supports low-cost ARM systems and others. The first tablets running Android were released in 2009. Vendors such as Motorola and Lenovo delayed deployment of their tablets until after 2011, when Android was reworked to include more tablet features. Android 3.0 (Honeycomb), released in 2011 and later versions support larger screen sizes, mainly tablets, and have access to the Google Play service. Android includes operating system, middleware and key applications. Other vendors sell customized Android tablets, such as Kindle Fire and Nook, which are used to consume mobile content and provide their own app store, rather than using the larger Google Play system, thereby fragmenting the Android market.\n\nThe iPad runs on iOS, which was created for the iPhone and iPod Touch. The first iPad was released in 2011. Although built on the same underlying Unix implementation as MacOS, its user interface is radically different. iOS is designed for fingers and has none of the features that required a stylus on earlier tablets. Apple introduced multi-touch gestures, such as moving two fingers apart or together to zoom in or out, also known as \"pinch to zoom\". iOS is built for the ARM architecture. \n\nFollowing Windows for Pen Computing for Windows 3.1 in 1991, Microsoft supported tablets running Windows XP under the Microsoft Tablet PC name. Microsoft Tablet PCs were pen-based, fully functional x86 PCs with handwriting and voice recognition functionality. Windows XP Tablet PC Edition provided pen support. Tablet support was added to both Home and Business versions of Windows Vista and Windows 7. Tablets running Windows could use the touchscreen for mouse input, hand writing recognition and gesture support. Following Tablet PC, Microsoft announced the Ultra-mobile PC initiative in 2006 which brought Windows tablets to a smaller, touch-centric form factor. In 2008, Microsoft showed a prototype of a two-screen tablet called Microsoft Courier, but cancelled the project.\n\nIn 2012, Microsoft released Windows 8, which features significant changes to various aspects of the operating system's user interface and platform which are designed for touch-based devices such as tablets. The operating system also introduced an application store and a new style of application optimized primarily for use on tablets. Microsoft also introduced Windows RT, an edition of Windows 8 for use on ARM-based devices. The launch of Windows 8 and RT was accompanied by the release of devices with the two operating systems by various manufacturers (including Microsoft themselves, with the release of Surface), such as slate tablets, hybrids, and convertibles.\n\nReleased in July 2015, Windows 10 introduces what Microsoft described as \"universal apps\"; expanding on Metro-style apps, these apps can be designed to run across multiple Microsoft product families with nearly identical code‍—‌including PCs, tablets, smartphones, embedded systems, Xbox One, Surface Hub and Windows Holographic. The Windows user interface was revised to handle transitions between a mouse-oriented interface and a touchscreen-optimized interface based on available input devices‍—‌particularly on 2-in-1 PCs; both interfaces include an updated Start menu. Windows 10 replaced all earlier editions of Windows.\n\nSeveral hardware companies have built hybrid devices with the possibility to work with both the Windows 10 and Android operating systems.\n\nApps that do not come pre-installed with the system are supplied through online distribution. These sources, known as \"app stores\", provide centralized catalogs of software and allow \"one click\" on-device software purchasing, installation and updates.\n\nMobile device suppliers may adopt a \"walled garden\" approach, wherein the supplier controls what software applications (\"apps\") are available. Software development kits are restricted to approved software developers. This can be used to reduce the impact of malware, provide software with an approved content rating, control application quality and exclude competing vendors. Apple, Google, Amazon, Microsoft and Barnes & Noble all adopted the strategy. B&N originally allowed arbitrary apps to be installed, but, in December 2011, excluded third parties. Apple and IBM have agreed to cooperate in cross-selling IBM-developed applications for iPads and iPhones in enterprise-level accounts. Proponents of open source software say that it violates the spirit of personal control that traditional personal computers have always provided.\n\nAround 2010, tablet use by businesses jumped, as business have started to use them for conferences, events, and trade shows. In 2012, Intel reported that their tablet program improved productivity for about 19,000 of their employees by an average of 57 minutes a day. In October 2012, display screen shipments for tablets began surpassing shipments for laptop display screens. Tablets are increasingly used in the construction industry to look at blueprints, field documentation and other relevant information on the device instead of carrying around large amounts of paper.\n\nAs of the beginning of 2014, 44% of US online consumers own tablets, a significant jump from 5% in 2011. Tablet use has also become increasingly common among children. A 2014 survey found that mobiles were the most frequently used object for play among American children under the age of 12. Mobiles were used more often in play than video game consoles, board games, puzzles, play vehicles, blocks and dolls/action figures. Despite this, the majority of parents said that a mobile was \"never\" or only \"sometimes\" a toy. As of 2014, nearly two-thirds of American 2- to 10-year-olds have access to a tablet or e-reader. The large use of tablets by adults is as a personal internet-connected TV. A 2015 study found that a third of children under five have their own tablet device. By 2017, tablet sales worldwide had surpassed sales of desktop computers, and worldwide PC sales were flat for the first quarter of 2018.\n\nAccording to a survey conducted by the Online Publishers Association (OPA) now called Digital Content Next (DCN) in March 2012, it found that 72% of tablet owners had an iPad, while 32% had an Android tablet. By 2012, Android tablet adoption had increased. 52% of tablet owners owned an iPad, while 51% owned an Android-powered tablet (percentages do not add up to 100% because some tablet owners own more than one type). By end of 2013, Android's market share rose to 61.9%, followed by iOS at 36%. By late 2014, Android's market share rose to 72%, followed by iOS at 22.3% and Windows at 5.7%. As of early 2016, Android has 65% marketshare, Apple has 26% and Windows has 9% marketshare. In Q1 2018, Android tablets had 62% of the market, Apple's iOS had 23.4% of the market and Windows 10 had 14.6% of the market.\n\nThe blue wavelength of light from back-lit tablets may impact one's ability to fall asleep when reading at night, through the suppression of melatonin. Experts at Harvard Medical School suggest limiting tablets for reading use in the evening. Those who have a delayed body clock, such as teenagers, which makes them prone to stay up late in the evening and sleep later in the morning, may be at particular risk for increases in sleep deficiencies. A PC app such as F.lux and Android apps such as CF.lumen and Twilight attempt to decrease the impact on sleep by filtering blue wavelengths from the display. iOS 9.3 includes Night Shift that shifts the colors of the device's display to be warmer during the later hours.\n\nBecause of, among other things, electromagnetic waves emitted by this type of device, the use of any type of electronic device during the take-off and landing phases was totally prohibited on board commercial flights. On 13 November 2013, the European Aviation Safety Agency (EASA) announced that the use of mobile terminals could be authorized on the flights of European airlines during these phases from 2014 onwards, on the condition that the cellular functions are deactivated (\"airplane\" mode activated). EASA then formally confirmed this information on December 11, 2013. Tablets, e-readers, smartphones, MP3 players, and portable gaming consoles are affected by these announcements.\n\nSome French historical monuments are equipped with digital tactile tablets called \"HistoPad\". It is an application integrated with an iPad mini offering an interaction in augmented and virtual reality with several pieces of the visit, the visitor being able to take control of his visit in an interactive and personalized way.\n\nSome professionals - for example, in the construction industry, insurance experts, lifeguards or surveyors - use so-called rugged shelf models in the field that can withstand extreme hot or cold shocks or climatic environments. even hardened. In addition, via satellite Iridium type of connectivity, this type of tablet, such as Thorium X for example, can be used in areas where there is no connectivity.\n"}
{"id": "19683630", "url": "https://en.wikipedia.org/wiki?curid=19683630", "title": "Tally counter", "text": "Tally counter\n\nA tally counter is a mechanical, electronic, or software device used to incrementally count something, typically fleeting. One of the most common things tally counters are used for is counting people, animals, or things that are quickly coming and going from some location.\n\nA tally counter is usually cased in metal and is cylindrical in shape. Part of the circle is flattened out and contains a window of plastic or glass. Inside the counter are a number of rings with the numbers from 0 to 9 in descending order going clockwise. Most counters have four such rings, allowing the user to count up to 9999. A metal ring may be attached to aid in holding the counter, and usually half the ring is bent to allow it to fold flush with the counter when not in use.\n\nThe counter is activated by pressing a button located above the screen. This causes the first ring to advance one number. After the count has reached 0009, then the second ring will advance one click and the first ring will come back to zero displaying 0010. To reset the counter, a knob is located on the side. This knob turns all the rings which are displaying the same number (usually zero). When the number displayed reaches the number on the remaining rings, then they will turn too, until the display is reset back to 0000. (Analog odometers in vehicles operate in a similar fashion.)\n\nElectronic tally counters are available, which use an LCD screen to display the count, and a button to advance the count. Some also have a button to decrement the count, for example if a mistake is made, or if counting a majority. \nThe main application of tally counters is as people counters. At concerts, stadiums, etc., a person will stand by the door with a tally counter recording the number of people that enter. At amusement parks, the rides can only hold a certain number of people, so the operator may use a tally counter to keep track of the number of people who get on the ride. They are also used for traffic analysis, scientific research, counting inventory and on industrial lines as well.\n\nTally counters have also been used in religion to count prayers, often replacing traditional prayer beads. Shri Vidya initiates often use them to keep track of the number of repetitions of the Mula Mantra into which they are initiated. Sikhs may use them to keep track of the number of times they chant the Mul Mantar. Buddhists have also been known to use them to count mantras. Gaudiya Vaishnava Hindus may use tally counters to keep track of the number of times that they chant the Hare Krishna Mahamantra. Initiated devotees are required to chant a certain number of 'rounds' each day, each round consisting of 108 repetitions.\n\n"}
{"id": "31998179", "url": "https://en.wikipedia.org/wiki?curid=31998179", "title": "The Verge", "text": "The Verge\n\nThe Verge is an American technology news and media network operated by Vox Media. It has offices in Manhattan, New York City. The network publishes news items, long-form feature stories, product reviews, podcasts, and an entertainment show.\n\nThe website uses its own proprietary publishing platform with video content. The network's content is financed through advertising and sponsorship and is managed by its editor-in-chief Nilay Patel and executive editor Dieter Bohn. The site launched on November 1, 2011. \"The Verge\" won five Webby Awards for the year 2012 including awards for Best Writing (Editorial), Best Podcast for \"The Vergecast\", Best Visual Design, Best Consumer Electronics Site, and Best Mobile News App.\n\nThroughout the 2010s, AOL began to acquire websites in pursuit of a new ad-driven content strategy for the company. One of their first acquisitions was Weblogs, Inc. in 2005, a company that ran dozens of websites, including \"Engadget\", a tech news website. According to \"Business Insider\", \"Engadget\" \"became the industry-leading gadget site\", and AOL's \"most popular and important media property.\" \"All Things Digital\" called it \"one of the largest in tech\".\n\nJoshua Topolsky became \"Engadget\"s editor-in-chief in 2007, and was responsible for new efforts like \"The Engadget Show\" and their mobile app, and the site's continued growth. Animosities between Topolsky and AOL developed after AOL's September 2010 TechCrunch acquisition, when TechCrunch founder Michael Arrington made several public remarks disparaging \"Engadget\" and Topolsky. When the acrimony between the two editors escalated in January 2011, AOL didn't intervene. The next month, an internal AOL editor training document called \"The AOL Way\", a new content strategy that prioritized profitability metrics, leaked to the press. The document leaked before \"Engadget\" writers and editors saw it internally. \"The AOL Way\" dispirited the \"Engadget\" staff and created an ideological schism between the two entities.\n\nBetween March and April 2011, Topolsky and up to eight of \"Engadget\"s most prominent writers, editors, and product developers left AOL to found a new gadget site that would become \"The Verge\". The other departing editors included managing editor Nilay Patel and staffers Paul Miller, Ross Miller, Joanna Stern, Chris Ziegler, as well as product developers Justin Glow, and Dan Chilton. In early April 2011, Topolsky announced that their unnamed new site would be produced in partnership with sports news website SB Nation, debuting some time in the fall. Topolsky lauded SB Nation's similar interest in the future of publishing, including what he described as their beliefs in independent journalism and in-house development of their own content delivery tools. Jim Bankoff of SB Nation saw an overlap in the two sites' demographics and an opportunity to expand SB Nation's model. Bankoff previously worked at AOL in 2005, where he led their \"Engadget\" acquisition. Other news outlets viewed the partnership as positive for both SB Nation and Topolsky's staff, and negative for AOL's outlook.\n\nBankoff, chairman and CEO of what became Vox Media, the owner of the SB Nation brand, after the launch of The Verge said in a 2011 interview that though the company had started out with a focus on sports, other categories including consumer technology had growth potential for the company. Bankoff also expressed a wish to attract other journalists and bloggers outside of the sports medium to Vox Media.\n\nDevelopment of the Vox Media's content management system (CMS), Chorus, is led by Trei Brundrett, chief product officer at Vox Media.\n\nFollowing news of his untitled partnership with SB Nation in April 2011, Topolsky announced that the \"Engadget\" podcast hosted by Patel, Paul Miller, and himself would continue at an interim site called \"This Is My Next\". By August 2011, the site had reached 1 million unique visitors and 3.4 million page views. By October 2011, the site had 3 million unique views per month and 10 million total page views. \"Time\" listed the site in its Best Blogs of 2011, calling the prototype site \"exemplary\". The site closed upon \"The Verge\"s launch on November 1, 2011.\nOn June 11, 2014, The Verge launched a new section on TheVerge.com called \"This Is My Next\", edited by former editor David Pierce, as a buyer's guide for consumer electronics.\n\n\"The Verge\" launched November 1, 2011, along with an announcement of a new parent company: Vox Media. According to the company, the site launched with 4 million unique visitors and 20 million pageviews. At the time of Topolsky's departure, \"Engadget\" had 14 million unique visitors. Vox Media overall doubled its unique visitors to about 15 million during the last half of 2012. \"The Verge\" had 12 former Engadget staffers working with Topolsky at the time of launch. By 2016, the website's advertising had shifted significantly from display advertisements matched with content to partnerships and advertisements with user-specific tweaks. Vox revamped \"The Verge\" visual design for its fifth anniversary in November 2016.\n\nIn September 2016, The Verge fired deputy editor Chris Ziegler after it learned that he had been working for Apple since July.\n\n\"The Verge\" logo featured a modified Penrose triangle, an impossible object.\n\nOn November 1, 2016, \"The Verge\" launched version 3.0 of its news platform, offering a redesigned website along with a new logo.\n\n\"The Verge\" hosts a product database which allows readers to compare product specifications and research product availability.\n\nThe site's team publishes product reviews for consumer items. Personal computers and cellphones are the most reviewed products. Items receive a \"Verge Score\" out of 10. Users can also submit reviews for products they own.\n\n\"The Verge\" also publishes features, including interviews, editorials and news items.\n\n\"The Verge\" broadcasts a live weekly podcast.\n\nThe inaugural episode was broadcast on November 4, 2011. Unlike many episodes of previous podcasts, it included a video stream of the hosts.\n\nA second weekly podcast was introduced on November 8, 2011. Unlike \"The Vergecast\", \"The Verge Mobile Show\" is primarily focused on mobile phones.\n\n\"The Verge\" What's Tech podcast was named among iTunes's best of 2015.\n\nOn August 6, 2011, in an interview with Edelman, Marty Moe, publisher and co-founder of \"The Verge\", announced that they would soon be launching \"The Verge Show\", a web television series. After the site's launch, the show was named \"On The Verge\". The first episode was taped on Monday, November 14, 2011, with guest Matias Duarte.\n\nThe show is a technology news entertainment show, and its format is similar to that of a late-night talk show, but it is broadcast over the Internet, not on television. The show's first episode was released on November 15, 2011.\n\n\"On The Verge\" is similar to The Engadget Show, an online program run by the technology news blog Engadget.\n\nTen episodes of the show were broadcast, with the most recent episode going out on November 10, 2012.\n\nOn May 24, 2013, it was announced that the show would return under a new weekly format, alongside a new logo and theme tune.\n\nOn May 8, 2013, editor-in-chief Topolsky launched Verge Video, a site that contains the video backlog from The Verge.\n"}
{"id": "695533", "url": "https://en.wikipedia.org/wiki?curid=695533", "title": "Truck wages", "text": "Truck wages\n\nTruck wages are any arrangement under which wages are paid in the form of: payment in kind (i.e. commodities, including goods and/or services); credit with retailers or; a money substitute, such as scrip, chits, vouchers or tokens, rather than with conventional money. Truck, in this context, is an archaic English language word meaning \"exchange\" or \"barter\" (and is derived from the French \"\").\n\nBy contrast, the term truck system usually refers to a specific set of practices under which employees are defrauded and/or exploited. This may take one or both of two forms. Firstly, the payment in kind, credit, or money substitute is demonstrably of a lesser market value than the amount of money that would normally be paid for the same work. Secondly, truck systems are normally regarded as undesirable or illegal because they limit employees' ability to choose how to spend their earnings. For example, credit or company scrip might be usable only for the purchase of goods at a monopolistic company-owned store, at which prices are set artificially high and there is no competition to lower prices. Hence, a truck system relies on a closed economic system in which employees are: required to become indebted, subject to a retail monopoly in essential goods and/or considered unfree labour. Such a system may appear to be a fair, free and legal exchange, whereby an employer offers something of value (typically goods, food or housing) in exchange for labour, with the result being the same as if the laborer had been paid money and then spent the money on those necessities. Truck systems have been specifically outlawed in many countries by legislation (which is part of the larger field of labour law and employment standards, such as the British Truck Acts). \n\nWhile truck systems had long existed in many parts of the world, it was widespread during the 18th and early-19th centuries in Britain. Despite a long history of legislation intended to curb truck systems (Truck Acts), they remained common into the 20th century. In a prosecution brought against a Manchester cotton manufacturer in 1827 one worker gave evidence that he had received wages of only two shillings in nine months; the rest \"he was obliged to take [in goods] from the manufacturer's daughter, who was also the cashier\".\n\nIn Britain the truck system was sometimes referred to as the Tommy system. The 1901 edition of Brewer's Dictionary of Phrase and Fable notes the Tommy shop as:\n\nWhere wages are paid to workmen who are expected to lay out a part of the money for the good of the shop. Tommy means bread or a penny roll, or the food taken by a workman in his handkerchief; it also means goods in lieu of money. \nIn the \"Midland Tour\" of his Rural Rides, the agriculturist and political reformer William Cobbett reports the use of \"the truck or tommy system\" in Wolverhampton and Shrewsbury. He describes the logic of the Tommy as:\nThe manner of carrying on the tommy system is this: suppose there to be a master who employs a hundred men. That hundred men, let us suppose, to earn a pound a week each. This is not the case in the iron-works; but no matter, we can illustrate our meaning by one sum as well as by another. These men lay out weekly the whole of the hundred pounds in victuals, drink, clothing, bedding, fuel, and house-rent. Now, the master finding the profits of his trade fall off very much, and being at the same time in want of money to pay the hundred pounds weekly, and perceiving that these hundred pounds are carried away at once, and given to shopkeepers of various descriptions; to butchers, bakers, drapers, hatters, shoemakers, and the rest; and knowing that, on an average, these shopkeepers must all have a profit of thirty \"per cent.\", or more, he determines to \"keep this thirty per cent. to himself\"; and this is thirty pounds a week gained as a shop-keeper, which amounts to 1,560l. a year. He, therefore, sets up a tommy shop: a long place containing every commodity that the workman can want, liquor and house-room excepted.\n\nAlthough Cobbett sees nothing wrong \"in itself\" in the tommy system, he notes that \"The only question is in this case of the manufacturing tommy work, whether the master charges a higher price than the shop-keepers would charge;\" but given the guaranteed market Cobbett sees no reason why any master should ever abuse the system.\nHowever, in rural regions he notes the virtual monopoly of the shopkeeper:\n\nI have often had to observe on the cruel effects of the suppression of markets and fairs, and on the consequent power of extortion possessed by the country shop-keepers. And what a thing it is to reflect on, that these shopkeepers have the whole of the labouring men of England constantly in their debt; have on an average a mortgage on their wages to the amount of five or six weeks, and make them pay any price that they choose to extort.\nOne reason for the truck system in the early history of the United States is that there was no national form of paper currency and an insufficient supply of coinage. Banknotes were the majority of the money in circulation. Banknotes were discounted relative to gold and silver (e.g. a $5 banknote might be exchanged for $4.50 of coins) and the discount depended on the financial strength of the issuing bank and distance from the bank. During financial crises many banks failed and their notes became worthless.\n\nThe popular song \"Sixteen Tons\" dramatizes this scenario, with the narrator telling Saint Peter (who would welcome him to Heaven upon his death), \"I can't go; I owe my soul to the company store\".\n\n\n\n"}
{"id": "1804907", "url": "https://en.wikipedia.org/wiki?curid=1804907", "title": "Woodchipper", "text": "Woodchipper\n\nA tree chipper or woodchipper is a machine used for reducing wood (generally tree limbs or trunks) into smaller woodchips. They are often portable, being mounted on wheels on frames suitable for towing behind a truck or van. Power is generally provided by an internal combustion engine from to . There are also high power chipper models mounted on trucks and powered by a separate engine. These models usually also have a hydraulic crane.\n\nTree chippers are typically made of a hopper with a collar, the chipper mechanism itself, and an optional collection bin for the chips. A tree limb is inserted into the hopper (the collar serving as a partial safety mechanism to keep human body parts away from the chipping blades) and started into the chipping mechanism. The chips exit through a chute and can be directed into a truck-mounted container or onto the ground. Typical output is chips on the order of to across in size. The resulting wood chips have various uses such as being spread as a ground cover or being fed into a digester during papermaking.\n\nMost woodchippers rely on energy stored in a heavy flywheel to do their work (although some use drums). The chipping blades are mounted on the face of the flywheel, and the flywheel is accelerated by an electric motor or internal combustion engine.\n\nLarge woodchippers are frequently equipped with grooved rollers in the throat of their feed funnels. Once a branch has been gripped by the rollers, the rollers transport the branch to the chipping blades at a steady rate. These rollers are a safety feature and are generally reversible for situations where a branch gets caught on clothing.\n\nThe woodchipper was invented by Peter Jensen (Maasbüll, Germany) in 1884, the \"Marke Angeln\" woodchipper soon became the core business of his company, which already produced and repaired communal- and woodworking-machinery.\n\nShredders that make use of high-torque low-speed grinding rollers are growing in popularity for residential use. These shredders are driven with an electric motor and are very quiet, dust free, and self-feeding. Some of these machines are equipped with an anti-jamming feature.\n\nThe original chipper design employs a steel disk with knives mounted upon it as the chipping mechanism. This technology dates back to an invention by German Heinrich Wigger, for which he obtained a patent in 1922. In this design, (usually) reversible hydraulically powered wheels draw the material from the hopper towards the disk, which is mounted perpendicularly to the incoming material. As the disk spins, the knives cut the material into chips. These are thrown out the chute by flanges on the drum. This design is not as energy-efficient as the drum-style design, but produces chips of more uniform shape and size. Most chippers currently used by commercial tree care companies are disk-type.\n\nConsumer-grade disk-style chippers usually have a material diameter capacity of . Industrial-grade chippers are available with discs as large as in diameter, requiring . One application of industrial disk chippers is to produce the wood chips used in the manufacture of particle board.\n\nNewer chippers employ mechanisms consisting of a large steel drum powered by a motor, usually by means of a belt. The drum is mounted parallel to the hopper and spins towards the output chute. The drum also serves as the feed mechanism, drawing the material through as it chips it. It is colloquially known as a \"chuck-and-duck\" chipper, due to the immediate speed attained by material dropped into the drum.\n\nChippers of this type have many drawbacks and safety issues. If an operator becomes snagged on material being fed into the machine, injury or death is very likely. Chippers of this type are also very loud. The chips produced may be very large, and if thin material is inserted, it may be cut into slivers rather than chips, and since the drum is directly driven by the engine, materials that are too large or long may stall the engine while usually remaining firmly stuck in the drum.\n\nMuch larger machines for wood processing exist. \"Whole tree chippers\" and \"Recyclers\", which can typically handle material diameters of to may employ drums, disks, or a combination of both. The largest machines used in wood processing, often called \"Tub Grinders\", may handle a material diameter of or greater, and use carbide tipped flail hammers to pulverize wood rather than cut it. These machines usually have to . Some are so heavy that they require a semi-trailer truck to be transported. Smaller models can be towed by a medium duty truck.\n\nAlthough chippers vary greatly in size, type, and capacity, the blades processing the wood are similar in construction. They are rectangular in shape and are usually to across by to long. They vary in thickness from about to . Chipper blades are made from high grade steel and usually contain a minimum of 8% chromium for hardness.\n\nThirty-one people were killed in woodchipper accidents between 1992 and 2002 in the US, according to a 2005 report by the \"Journal of the American Medical Association\".\n\n"}
