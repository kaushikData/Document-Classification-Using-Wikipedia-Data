{"id": "39099640", "url": "https://en.wikipedia.org/wiki?curid=39099640", "title": "Agency for Non-conventional Energy and Rural Technology", "text": "Agency for Non-conventional Energy and Rural Technology\n\nThe Agency for Non-Conventional Energy and Rural Technology (ANERT) is a government agency in the Kerala, India. Its mission is gathering and disseminating knowledge about non-conventional energy, energy conservation, and rural technology. The agency was established in 1986 with its headquarters at Thiruvananthapuram.\n\nANERT is an autonomous organization established as an institution under the Indian Department of Science and Technology, and is the primary agency carrying out topical programs in Kerala for the Ministry of New and Renewable Energy (MNRE), a bureau of the Government of India. The body was registered under the Charitable Societies Act by the government of Kerala, and now functions under the state's Power Department.\n\nANERT is administered by a governing body chaired by the state Minister for Electricity, with highest oversight by a director appointed by the Government. This board provides direction for activities in various energy-related areas.\n\nANERT has also an executive committee chaired by the Secretary to the Government of Kerala, Power Department. Other members are the Secretary to Government for the Finance (Expenditure) Department, Government of Kerala, the chairman of the Kerala State Electricity Board, the member secretary of the Kerala State Council for Science, Technology and Environment, and two additional members nominated by the state government.\n\nMajor programmes being implemented by ANERT are:\n\nUnder this programme, ANERT distributes devices which use solar energy. These include solar lanterns, home lighting systems, street light systems, TV power packs, and vaccine refrigerators. These devices are distributed in accordance with MNRE and other programmes using ANERT funds.\n\nThe Solar Thermal Energy Programme aims to supplement thermal energy requirements at various temperatures for different applications; they include cooking, water heating, heating for industrial processes, crop drying, space heating, and water desalination. Programme processes include harnessing solar energy from the Sun and converting it into heat energy using various solar thermal devices and systems.\n\nANERT is in the process of increasing wind power usage at different locations within the state. In association with MNRE, the agency has conducted a detailed study of the wind potential in Kerala. Its largest capacity wind farm is located at Kanjikode, in Palakkad District; it has a capacity of 22 MW\n\nThis program aims to recover energy from waste; study the scientific disposal of waste; convert waste into fertilizer after energy extraction; improve sanitation; protect the environment; and generate relevant employment opportunities. Domestic and industrial biowaste is of major import to the agency, as it can be converted into producer gas via gasification. This gas can be used for heating and generating electricity.\n\nIn 2001, as part of its declared \"Women's Empowerment Year\", MNRE decided to make 10,000 villages throughout India \"smoke free\" by the promotion of improved chulhas, or cooking stoves.\n\nANERT provides training and workshops for field level staffs to improve services.\n\nThe Kerala Renewable Energy Policy was introduced in April 2002 to develop, propagate, and promote non-conventional energy sources, as well as to exploit natural resources for cheaper power projects.\n\nThe Wind Energy Policy was instituted to set up wind farms on private lands in Kerala.\n\nIn November 2013 the Solar Energy Policy was implemented to increase the use of solar energy appliances in Kerala.\n"}
{"id": "51867212", "url": "https://en.wikipedia.org/wiki?curid=51867212", "title": "Alexa Andrzejewski", "text": "Alexa Andrzejewski\n\nAlexa Andrzejewski is an American user experience designer and entrepreneur. She is best known as the co-founder and CEO of Foodspotting, an app that allows users to socially share their food photographs. She was named to \"Inc.\"'s 2011 list of \"30 Under 30\" and to \"Forbes\"' 2012 \"30 Under 30: Food and Wine\" list.\n\nAndrzejewski (née Weber) had an interest in technology from a young age, learning to reprogram her childhood computer at eight years old and design websites for local businesses by the time she was in junior high school.\n\nAfter graduating from college, Andrzejewski joined a design firm called Adaptive Path in San Francisco, California. In 2009, Andrzejewski was inspired to launch Foodspotting, an app that allows users to socially share their food photographs, after being unable to find the Japanese dish okonomiyaki in San Francisco. She began the project alongside programmer Ted Grubb and social media strategist Soraya Darabi. Billed as a \"visual guide to food,\" the app allows users to enter their city and a type of food to get photos showing local restaurants that carry the dish. Since its launch, the service has been named to \"Time's\" list of 50 Best Websites of 2010 and featured at Facebook's f8 conference.\n\nFoodspotting was acquired by OpenTable in 2013 for $10 million. As part of the acquisition, Andrzejewski joined the online reservation platform as a lead user interface designer.\n\nShe was named to \"Inc.\"'s 2011 list of \"30 Under 30\" and to \"Forbes\"' 2012 \"30 Under 30: Food and Wine\" list.\n"}
{"id": "57627813", "url": "https://en.wikipedia.org/wiki?curid=57627813", "title": "Artificial iris", "text": "Artificial iris\n\nAn artificial iris is a surgically implanted device to treat damage or absence of the iris of the eye.\n\nIn 2018, the United States Food and Drug Administration approved the first artificial iris, CustomFlex Artificial Iris, made of medical grade silicone.\n\nThe device improves vision by controlling the amount of light let into the eye. It also improves cosmetic appearance. There are a number of surgical techniques for implanting the prosthetic.\n"}
{"id": "10203313", "url": "https://en.wikipedia.org/wiki?curid=10203313", "title": "Biological computing", "text": "Biological computing\n\nBio computers use systems of biologically derived molecules—such as DNA and proteins—to perform computational calculations involving storing, retrieving, and processing data.\n\nThe development of biocomputers has been made possible by the expanding new science of nanobiotechnology. The term nanobiotechnology can be defined in multiple ways; in a more general sense, nanobiotechnology can be defined as any type of technology that uses both nano-scale materials (i.e. materials having characteristic dimensions of 1-100 nanometers) and biologically based materials. A more restrictive definition views nanobiotechnology more specifically as the design and engineering of proteins that can then be assembled into larger, functional structures \nThe implementation of nanobiotechnology, as defined in this narrower sense, provides scientists with the ability to engineer biomolecular systems specifically so that they interact in a fashion that can ultimately result in the computational functionality of a computer.\n\nBiocomputers use biologically derived materials to perform computational functions. A biocomputer consists of a pathway or series of metabolic pathways involving biological materials that are engineered to behave in a certain manner based upon the conditions (input) of the system. The resulting pathway of reactions that takes place constitutes an output, which is based on the engineering design of the biocomputer and can be interpreted as a form of computational analysis. Three distinguishable types of biocomputers include biochemical computers, biomechanical computers, and bioelectronic computers.\n\nBiochemical computers use the immense variety of feedback loops that are characteristic of biological chemical reactions in order to achieve computational functionality. Feedback loops in biological systems take many forms, and many different factors can provide both positive and negative feedback to a particular biochemical process, causing either an increase in chemical output or a decrease in chemical output, respectively. Such factors may include the quantity of catalytic enzymes present, the amount of reactants present, the amount of products present, and the presence of molecules that bind to and thus alter the chemical reactivity of any of the aforementioned factors. Given the nature of these biochemical systems to be regulated through many different mechanisms, one can engineer a chemical pathway comprising a set of molecular components that react to produce one particular product under one set of specific chemical conditions and another particular product under another set of conditions. The presence of the particular product that results from the pathway can serve as a signal, which can be interpreted—along with other chemical signals—as a computational output based upon the starting chemical conditions of the system (the input).\n\nBiomechanical computers are similar to biochemical computers in that they both perform a specific operation that can be interpreted as a functional computation based upon specific initial conditions which serve as input. They differ, however, in what exactly serves as the output signal. In biochemical computers, the presence or concentration of certain chemicals serves as the output signal. In biomechanical computers, however, the mechanical shape of a specific molecule or set of molecules under a set of initial conditions serves as the output. Biomechanical computers rely on the nature of specific molecules to adopt certain physical configurations under certain chemical conditions. The mechanical, three-dimensional structure of the product of the biomechanical computer is detected and interpreted appropriately as a calculated output.\n\nBiocomputers can also be constructed to perform electronic computing. Again, like both biomechanical and biochemical computers, computations are performed by interpreting a specific output that is based upon an initial set of conditions that serve as input. In bioelectronic computers, the measured output is the nature of the electrical conductivity that is observed in the bioelectronic computer. This output comprises specifically designed biomolecules that conduct electricity in highly specific manners based upon the initial conditions that serve as the input of the bioelectronic system.\n\nThe behavior of biologically derived computational systems such as these relies on the particular molecules that make up the system, which are primarily proteins but may also include DNA molecules. Nanobiotechnology provides the means to synthesize the multiple chemical components necessary to create such a system. The chemical nature of a protein is dictated by its sequence of amino acids—the chemical building blocks of proteins. This sequence is in turn dictated by a specific sequence of DNA nucleotides—the building blocks of DNA molecules. Proteins are manufactured in biological systems through the translation of nucleotide sequences by biological molecules called ribosomes, which assemble individual amino acids into polypeptides that form functional proteins based on the nucleotide sequence that the ribosome interprets. What this ultimately means is that one can engineer the chemical components necessary to create a biological system capable of performing computations by engineering DNA nucleotide sequences to encode for the necessary protein components. Also, the synthetically designed DNA molecules themselves may function in a particular biocomputer system. Thus, implementing nanobiotechnology to design and produce synthetically designed proteins—as well as the design and synthesis of artificial DNA molecules—can allow the construction of functional biocomputers (e.g. Computational Genes).\n\nBiocomputers can also be designed with cells as their basic components. Chemically induced dimerization systems can be used to make logic gates from individual cells. These logic gates are activated by chemical agents that induce interactions between previously non-interacting proteins and trigger some observable change in the cell.\n\nAll biological organisms have the ability to self-replicate and self-assemble into functional components. The economical benefit of biocomputers lies in this potential of all biologically derived systems to self-replicate and self-assemble given appropriate conditions. For instance, all of the necessary proteins for a certain biochemical pathway, which could be modified to serve as a biocomputer, could be synthesized many times over inside a biological cell from a single DNA molecule. This DNA molecule could then be replicated many times over. This characteristic of biological molecules could make their production highly efficient and relatively inexpensive. Whereas electronic computers require manual production, biocomputers could be produced in large quantities from cultures without any additional machinery needed to assemble them.\n\nCurrently, biocomputers exist with various functional capabilities that include operations of \"binary \" logic and mathematical calculations. Tom Knight of the MIT Artificial Intelligence Laboratory first suggested a biochemical computing scheme in which protein concentrations are used as binary signals that ultimately serve to perform logical operations. At or above a certain concentration of a particular biochemical product in a biocomputer chemical pathway indicates a signal that is either a 1 or a 0. A concentration below this level indicates the other, remaining signal. Using this method as computational analysis, biochemical computers can perform logical operations in which the appropriate binary output will occur only under specific logical constraints on the initial conditions. In other words, the appropriate binary output serves as a logically derived conclusion from a set of initial conditions that serve as premises from which the logical conclusion can be made. In addition to these types of logical operations, biocomputers have also been shown to demonstrate other functional capabilities, such as mathematical computations. One such example was provided by W.L. Ditto, who in 1999 created a biocomputer composed of leech neurons at Georgia Tech which was capable of performing simple addition. These are just a few of the notable uses that biocomputers have already been engineered to perform, and the capabilities of biocomputers are becoming increasingly sophisticated. Because of the availability and potential economic efficiency associated with producing biomolecules and biocomputers—as noted above—the advancement of the technology of biocomputers is a popular, rapidly growing subject of research that is likely to see much progress in the future.\n\nIn March 2013. a team of bioengineers from Stanford University, led by Drew Endy, announced that they had created the biological equivalent of a transistor, which they dubbed a \"transcriptor\". The invention was the final of the three components necessary to build a fully functional computer: data storage, information transmission, and a basic system of logic.\n\nMany examples of simple biocomputers have been designed, but the capabilities of these biocomputers are very limited in comparison to commercially available non-bio computers. Some people believe that biocomputers have great potential, but this has yet to be demonstrated.\n\n"}
{"id": "5123117", "url": "https://en.wikipedia.org/wiki?curid=5123117", "title": "Blowout (well drilling)", "text": "Blowout (well drilling)\n\nA blowout is the uncontrolled release of crude oil and/or natural gas from an oil well or gas well after pressure control systems have failed. Modern wells have blowout preventers intended to prevent such an occurrence. An accidental spark during a blowout can lead to a catastrophic oil or gas fire.\n\nPrior to the advent of pressure control equipment in the 1920s, the uncontrolled release of oil and gas from a well while drilling was common and was known as an oil gusher, gusher or wild well.\n\nGushers were an icon of oil exploration during the late 19th and early 20th centuries. During that era, the simple drilling techniques such as cable-tool drilling and the lack of blowout preventers meant that drillers could not control high-pressure reservoirs. When these high pressure zones were breached, the oil or natural gas would travel up the well at a high rate, forcing out the drill string and creating a gusher. A well which began as a gusher was said to have \"blown in\": for instance, the Lakeview Gusher \"blew in\" in 1910. These uncapped wells could produce large amounts of oil, often shooting 200 feet (60 m) or higher into the air. A blowout primarily composed of natural gas was known as a \"gas gusher\".\n\nDespite being symbols of new-found wealth, gushers were dangerous and wasteful. They killed workmen involved in drilling, destroyed equipment, and coated the landscape with thousands of barrels of oil; additionally, the explosive concussion released by the well when it pierces an oil/gas reservoir has been responsible for a number of oilmen losing their hearing entirely; standing too near to the drilling rig at the moment it drills into the oil reservoir is extremely hazardous. The impact on wildlife is very hard to quantify, but can only be estimated to be mild in the most optimistic models—realistically, the ecological impact is estimated by scientists across the ideological spectrum to be severe, profound, and lasting.\n\nTo complicate matters further, the free flowing oil was—and is—in danger of igniting. One dramatic account of a blowout and fire reads,\n\nWith a roar like a hundred express trains racing across the countryside, the well blew out, spewing oil in all directions. The derrick simply evaporated. Casings wilted like lettuce out of water, as heavy machinery writhed and twisted into grotesque shapes in the blazing inferno.\n\nThe development of rotary drilling techniques where the density of the drilling fluid is sufficient to overcome the downhole pressure of a newly penetrated zone meant that gushers became avoidable. If however the fluid density was not adequate or fluids were lost to the formation, then there was still a significant risk of a well blowout.\n\nIn 1924 the first successful blowout preventer was brought to market. The BOP valve affixed to the wellhead could be closed in the event of drilling into a high pressure zone, and the well fluids contained. Well control techniques could be used to regain control of the well. As the technology developed, blowout preventers became standard equipment, and gushers became a thing of the past.\n\nIn the modern petroleum industry, uncontrollable wells became known as blowouts and are comparatively rare. There has been significant improvement in technology, well control techniques, and personnel training which has helped to prevent their occurring. From 1976 to 1981, 21 blowout reports are available.\n\n\nPetroleum or crude oil is a naturally occurring, flammable liquid consisting of a complex mixture of hydrocarbons of various molecular weights, and other organic compounds, that are found in geologic formations beneath the Earth's surface. Because most hydrocarbons are lighter than rock or water, they often migrate upward and occasionally laterally through adjacent rock layers until either reaching the surface or becoming trapped within porous rocks (known as reservoirs) by impermeable rocks above. When hydrocarbons are concentrated in a trap, an oil field forms, from which the liquid can be extracted by drilling and pumping. The down hole pressure in the rock structures changes depending upon the depth and the characteristic of the source rock. Natural gas (mostly methane) may be present also, usually above the oil within the reservoir, but sometimes dissolved in the oil at reservoir pressure and temperature. This dissolved gas often evolves as free gas as the pressure is reduced either under controlled production operations or in a kick or in an uncontrolled blowout. The hydrocarbon in some reservoirs may be essentially all natural gas.\n\nThe downhole fluid pressures are controlled in modern wells through the balancing of the hydrostatic pressure provided by the mud column. Should the balance of the drilling mud pressure be incorrect (i.e., the mud pressure gradient is less than the formation pore pressure gradient), then formation fluids (oil, natural gas and/or water) can begin to flow into the wellbore and up the annulus (the space between the outside of the drill string and the wall of the open hole or the inside of the casing), and/or inside the drill pipe. This is commonly called a \"kick\". Ideally, mechanical barriers such as blowout preventers (BOPs) can be closed to isolate the well while the hydrostatic balance is regained through circulation of fluids in the well. But if the well is not shut in (common term for the closing of the blow-out preventer), a kick can quickly escalate into a blowout when the formation fluids reach the surface, especially when the influx contains gas that expands rapidly with the reduced pressure as it flows up the wellbore, further decreasing the effective weight of the fluid.\n\nEarly warning signs of an impending well kick while drilling are:\n\nOther warning signs during the drilling operation are:\n\nThe primary means of detecting a kick while drilling is a relative change in the circulation rate back up to the surface into the mud pits. The drilling crew or mud engineer keeps track of the level in the mud pits and/or closely monitors the rate of mud returns versus the rate that is being pumped down the drill pipe. Upon encountering a zone of higher pressure than is being exerted by the hydrostatic head of the drilling mud (including the small additional frictional head while circulating) at the bit, an increase in mud return rate would be noticed as the formation fluid influx blends in with the circulating drilling mud. Conversely, if the rate of returns is slower than expected, it means that a certain amount of the mud is being lost to a thief zone somewhere below the last casing shoe. This does not necessarily result in a kick (and may never become one); however, a drop in the mud level might allow influx of formation fluids from other zones if the hydrostatic head at is reduced to less than that of a full column of mud.\n\nThe first response to detecting a kick would be to isolate the wellbore from the surface by activating the blow-out preventers and closing in the well. Then the drilling crew would attempt to circulate in a heavier \"kill fluid\" to increase the hydrostatic pressure (sometimes with the assistance of a well control company). In the process, the influx fluids will be slowly circulated out in a controlled manner, taking care not to allow any gas to accelerate up the wellbore too quickly by controlling casing pressure with chokes on a predetermined schedule.\n\nThis effect will be minor if the influx fluid is mainly salt water. And with an oil-based drilling fluid it can be masked in the early stages of controlling a kick because gas influx may dissolve into the oil under pressure at depth, only to come out of solution and expand rather rapidly as the influx nears the surface. Once all the contaminant has been circulated out, the shut-in casing pressure should have reached zero.\n\nCapping stacks are used for controlling blowouts. The cap is an open valve that is closed after bolted on.\n\nWell blowouts can occur during the drilling phase, during well testing, during well completion, during production, or during workover activities.\n\nBlowouts can eject the drill string out of the well, and the force of the escaping fluid can be strong enough to damage the drilling rig. In addition to oil, the output of a well blowout might include natural gas, water, drilling fluid, mud, sand, rocks, and other substances.\n\nBlowouts will often be ignited from sparks from rocks being ejected, or simply from heat generated by friction. A well control company then will need to extinguish the well fire or cap the well, and replace the casing head and other surface equipment. If the flowing gas contains poisonous hydrogen sulfide, the oil operator might decide to ignite the stream to convert this to less hazardous substances.\n\nSometimes blowouts can be so forceful that they cannot be directly brought under control from the surface, particularly if there is so much energy in the flowing zone that it does not deplete significantly over time. In such cases, other wells (called relief wells) may be drilled to intersect the well or pocket, in order to allow kill-weight fluids to be introduced at depth. When first drilled in the 1930s relief wells were drilled to inject water into the main drill well hole. Contrary to what might be inferred from the term, such wells generally are not used to help relieve pressure using multiple outlets from the blowout zone.\n\nThe two main causes of a subsea blowout are equipment failures and imbalances with encountered subsurface reservoir pressure. Subsea wells have pressure control equipment located on the seabed or between the riser pipe and drilling platform. Blowout preventers (BOPs) are the primary safety devices designed to maintain control of geologically driven well pressures. They contain hydraulic-powered cut-off mechanisms to stop the flow of hydrocarbons in the event of a loss of well control.\n\nEven with blowout prevention equipment and processes in place, operators must be prepared to respond to a blowout should one occur. Before drilling a well, a detailed well construction design plan, an Oil Spill Response Plan as well as a Well Containment Plan must be submitted, reviewed and approved by BSEE and is contingent upon access to adequate well containment resources in accordance to NTL 2010-N10.\n\nThe Deepwater Horizon well blowout in the Gulf of Mexico in April 2010 occurred at a water depth. Current blowout response capabilities in the U.S. Gulf of Mexico meet capture and process rates of 130,000 barrels of fluid per day and a gas handling capacity of 220 million cubic feet per day at depths through 10,000 feet.\n\nAn underground blowout is a special situation where fluids from high pressure zones flow uncontrolled to lower pressure zones within the wellbore. Usually this is from deeper higher pressure zones to shallower lower pressure formations. There may be no escaping fluid flow at the wellhead. However, the formation(s) receiving the influx can become overpressured, a possibility that future drilling plans in the vicinity must consider.\n\nMyron M. Kinley was a pioneer in fighting oil well fires and blowouts. He developed many patents and designs for the tools and techniques of oil firefighting. His father, Karl T. Kinley, attempted to extinguish an oil well fire with the help of a massive explosion—a method that remains a common technique for fighting oil fires. The first oil well put out with explosives by Myron Kinley and his father, was in 1913. Kinley would later form the M.M. Kinley Company in 1923. Asger \"Boots\" Hansen and Edward Owen \"Coots\" Matthews also begin their careers under Kinley.\n\nPaul N. \"Red\" Adair joined the M.M. Kinley Company in 1946, and worked 14 years with Myron Kinley before starting his own company, Red Adair Co., Inc., in 1959.\n\nRed Adair Co. has helped in controlling offshore blowouts, including:\n\nThe 1968 American film, \"Hellfighters\", which starred John Wayne, is about a group of oil well firefighters, based loosely on the life of Adair, who served as a technical advisor on the film, along with his associates, \"Boots\" Hansen, and \"Coots\" Matthews.\n\nIn 1994, Adair retired and sold his company to Global Industries. Management of Adair's company left and created International Well Control (IWC). In 1997, they would buy the company Boots & Coots International Well Control, Inc., which was founded by Hansen and Matthews in 1978.\n\nAfter the Macondo-1 blowout on the Deepwater Horizon, the offshore industry collaborated with government regulators to develop a framework to respond to future subsea incidents. As a result, all energy companies operating in the deep-water U.S. Gulf of Mexico must submit an OPA 90 required Oil Spill Response Plan with the addition of a Regional Containment Demonstration Plan prior to any drilling activity. In the event of a subsea blowout, these plans are immediately activated, drawing on some of the equipment and processes effectively used to contain the Deepwater Horizon well as well as others that have been developed in its aftermath.\n\nIn order to regain control of a subsea well, the Responsible Party would first secure the safety of all personnel on board the rig and then begin a detailed evaluation of the incident site. Remotely operated underwater vehicles (ROVs) would be dispatched to inspect the condition of the wellhead, Blowout Preventer (BOP) and other subsea well equipment. The debris removal process would begin immediately to provide clear access for a capping stack.\n\nOnce lowered and latched on the wellhead, a capping stack uses stored hydraulic pressure to close a hydraulic ram and stop the flow of hydrocarbons. If shutting in the well could introduce unstable geological conditions in the wellbore, a cap and flow procedure would be used to contain hydrocarbons and safely transport them to a surface vessel.\n\nThe Responsible Party works in collaboration with BSEE and the United States Coast Guard to oversee response efforts, including source control, recovering discharged oil and mitigating environmental impact.\n\nSeveral not-for-profit organizations provide a solution to effectively contain a subsea blowout. HWCG LLC and Marine Well Containment Company operate within the U.S. Gulf of Mexico waters, while cooperatives like Oil Spill Response Limited offer support for international operations.\n\nOn Sep. 30, 1966 the Soviet Union in Urta-Bulak, an area about 80 kilometers from Bukhara, Uzbekistan, experienced blowouts on five natural gas wells. It was claimed in Komsomoloskaya Pravda that after years of burning uncontrollably they were able to stop them entirely. The Soviets lowered a specially made 30 kiloton nuclear bomb into a borehole drilled away from the original (rapidly leaking) well. A nuclear explosive was deemed necessary because conventional explosive both lacked the necessary power and would also require a great deal more space underground. When the bomb was set off, it crushed the original pipe that was carrying the gas from the deep reservoir to the surface and glassified all the surrounding rock. This caused the leak and fire at the surface to cease within approximately one minute of the explosion, and proved over the years to have been a permanent solution. A second attempt on a similar well was not as successful and other tests were for such experiments as oil extraction enhancement (Stavropol, 1969) and the creation of gas storage reservoirs (Orenburg, 1970).\n\nData from industry information.\n\n"}
{"id": "37264560", "url": "https://en.wikipedia.org/wiki?curid=37264560", "title": "COBie", "text": "COBie\n\nConstruction Operations Building Information Exchange (COBie) is an international standard relating to managed asset information including space and equipment. It is closely associated with building information modeling (BIM) approaches to design, construction and management of built assets, and was devised by Bill East of the United States Army Corps of Engineers, who authored a pilot standard in June 2007.\n\nCOBie helps capture and record important project data at the point of origin, including equipment lists, product data sheets, warranties, spare parts lists, and preventive maintenance schedules. This information is essential to support operations, maintenance and asset management once the built asset is in service. \n\nIn December 2011, it was approved by the US-based National Institute of Building Sciences as part of its National Building Information Model (NBIMS-US) standard.\n\nCOBie has been incorporated into software for planning, design, construction, commissioning, operations, maintenance, and asset management. COBie may take several approved formats include spreadsheet, STEP-Part 21 (also called IFC file format), and ifcXML. In early 2013 BuildingSMART was working on a lightweight XML format for COBie, COBieLite, which became available for review in April 2013.\n\nIn September 2014, a code of practice regarding COBie was issued as a British Standard: \"BS 1192-4:2014 Collaborative production of information Part 4: Fulfilling employer’s information exchange requirements using COBie – Code of practice\".\n"}
{"id": "1569224", "url": "https://en.wikipedia.org/wiki?curid=1569224", "title": "Carbon button lamp", "text": "Carbon button lamp\n\nThe carbon button lamp is a single-electrode incandescent lamp invented by Nikola Tesla. A carbon button lamp contains a small carbon sphere positioned in the center of an evacuated glass bulb. This type of lamp must be driven by high-frequency alternating current, and depends on an electric arc or perhaps a vacuum arc to produce high current around the carbon electrode. The carbon electrode is then heated to incandescence by collisions by ions which constitute the electric current. Tesla found that these lamps could be used as powerful sources of ionizing radiation.\n\nIn February, 1892, Tesla gave a lecture to the Institution of Electrical Engineers, in which he described the carbon button lamp in detail. He also described several variants of the lamp, one of which uses a ruby drop in place of the carbon button.\n\n\n"}
{"id": "2008124", "url": "https://en.wikipedia.org/wiki?curid=2008124", "title": "Carpet bag", "text": "Carpet bag\n\nA carpet bag is a traveling bag made of carpet, commonly from an oriental rug. They were a popular form of luggage in the United States and Europe in the 19th century. Some modern versions serve as handbags or purses.\n\nThe carpet bag was invented as a type of baggage light enough for a passenger to carry, like a duffel bag, as opposed to a wooden or metal trunk, which required the assistance of porters. It was a good traveling companion: in 1886, the \"Scientific American\" described it as old-fashioned and reliable: the carpet bag \"is still unsurpassed by any, where rough wear is the principal thing to be studied. Such a bag, if constructed of good Brussels carpeting and unquestionable workmanship, will last a lifetime, provided always that a substantial frame is used.\" Its use implied self-sufficiency: in Jules Verne's 1873 novel \"Around the World in Eighty Days\", Phileas Fogg and Passepartout bring only a carpet bag as luggage, which holds a few items of clothing and a great deal of cash.\n\nCarpet bags used to be made of Oriental rugs or the Brussels carpet referred to above, meaning one with \"a heavy pile formed by uncut loops of wool on a linen warp\". Carpet was the chosen material because it was a popular domestic accent piece and the \"remainder\" pieces were easily bought. In a sense, the carpet bag was a sustainable invention because it used remnants of materials which otherwise would have gone unused.\n\nCarpet bags sometimes also served as a \"railway rug\", a common item in the 19th century for warmth in drafty, unheated rail-cars. The rug could either be opened as a blanket, or latched up on the sides as a traveling bag. From Robert Louis Stevenson's \"\" (1879): \"... my railway-rug, which, being also in the form of a bag, made me a double castle for cold nights.\"\n\nA popular carpet bag brand of the mid 1960s (known as \"the California Carpetbagger\") is Jerry Terrence: The Original Carpet Bag, or JT Carpet Bag. The company encouraged the use of brand new carpet material.\n\nThe carpetbaggers of the Reconstruction Era following the American Civil War—Northerners who moved to the South for economic or political opportunity—were given their name from this type of luggage which they carried.\n\nThe fictional nanny Mary Poppins arrived clutching her magical carpetbag, a motif in both the series of children's novels and in the famous 1964 film.\n\nThe Magic Bag of Tricks in the Felix the Cat cartoons resembles a carpetbag.\n"}
{"id": "46498257", "url": "https://en.wikipedia.org/wiki?curid=46498257", "title": "Cenocell", "text": "Cenocell\n\nCenocell is a patented concrete material that is manufactured without the addition of Portland cement. It is produced from a chemical reaction involving fly ash with organic and inorganic chemicals. It was invented by Mulalo Doyoyo and Paul Biju-Duval at the Georgia Institute of Technology. It is generally known as \"cementless concrete\". Fly ash is an unwanted pollutant that is a byproduct mainly of coal combustion in coal-fired power plants, cement production, paper manufacturing, and mining operations. In addition to its role as an environmental-friendly construction material, it is as strong as concrete at lower specific gravity. Its tensile strength is one third of its compressive strength. The tensile strength in traditional concrete is around one tenth. That means that the material requires less reinforcement.\n"}
{"id": "5785589", "url": "https://en.wikipedia.org/wiki?curid=5785589", "title": "Circle hook", "text": "Circle hook\n\nA circle hook is a type of fish hook which is sharply curved back in a circular shape. It has become widely used among anglers in recent years because the hook generally catches more fish and is rarely swallowed. Since the circle hook catches the fish on the lips at the corner of its mouth, it usually decreases the mortality rates of released fish as compared to J-hook (like O'Shaughnessy or Octopus hooks) which are often swallowed by the fish, causing damage to the gills or vital organs.\nThe circle hook's shape allows it to only hook onto an exposed surface, which in the case of a fish means the corner of its mouth. The fish takes the baited hook and swallows it, and as the hook is reeled in, it is safely pulled out of the fish until it reaches the mouth. At this point it will catch the corner of the mouth, resulting in fewer gut-hooked fish.\n\nIt is important to not strike (or set the hook) when the fish bites, but rather just reel in. The act of striking while using a circle hook often results in the hook being pulled out of the fish altogether.\n\nStudies have shown that circle hooks do less damage to billfish than the traditional J-hooks, yet they are at just as effective for catching billfish. This is good for conservation, since it improves survival rates after release.\n\n\n"}
{"id": "441300", "url": "https://en.wikipedia.org/wiki?curid=441300", "title": "Cold Spring Harbor Laboratory", "text": "Cold Spring Harbor Laboratory\n\nCold Spring Harbor Laboratory (CSHL) is a private, non-profit institution with research programs focusing on cancer, neuroscience, plant biology, genomics, and quantitative biology.\n\nIt is one of 68 institutions supported by the Cancer Centers Program of the U.S. National Cancer Institute (NCI) and has been an NCI-designated Cancer Center since 1987. The Laboratory is one of a handful of institutions that played a central role in the development of molecular genetics and molecular biology.\n\nIt has been home to eight scientists who have been awarded the Nobel Prize in Physiology or Medicine. CSHL is ranked among the leading basic research institutions in molecular biology and genetics with Thomson Reuters ranking it #1 in the world. The Laboratory is led by Bruce Stillman, a biochemist and cancer researcher.\n\nSince its inception in 1890, the institution's campus on the North Shore of Long Island has also been a center of biology education. Current CSHL educational programs serve professional scientists, doctoral students in biology, teachers of biology in the K-12 system, and students from the elementary grades through high school. In the past 10 years CSHL conferences & courses have drawn over 81,000 scientists and students to the main campus. For this reason, many scientists consider CSHL a \"crossroads of biological science.\" Since 2009 CSHL has partnered with the Suzhou Industrial Park in Suzhou, China to create Cold Spring Harbor Asia which annually draws some 3,000 scientists to its meetings and courses.\n\nIn 2015, CSHL announced a strategic affiliation with the nearby Northwell Health to advance cancer therapeutics research, develop a new clinical cancer research unit at Northwell Health in Lake Success, NY, to support early-phase clinical studies of new cancer therapies, and recruit and train more clinician-scientists in oncology.\n\nCSHL hosts bioRxiv, the preprint repository for biologists.\n\nResearch staff in CSHL's 52 laboratories numbers over 600, including postdoctoral researchers; an additional 125 graduate students and 500 administrative and support personnel bring the total number of employees to over 1,200.\n\nCell biology and genomics\nRNA interference (RNAi) and small-RNA biology; DNA replication; RNA splicing; signal transduction; genome structure; non-coding RNAs; deep sequencing; single-cell sequencing and analytics; chromatin dynamics; structural biology; advanced proteomics; mass spectrometry; advanced microscopy.\n\nCancer Research\nPrincipal cancer types under study: breast, prostate, blood (leukemia, lymphoma); melanoma; liver; ovarian and cervical; lung; brain; pancreas. Research foci: drug resistance; cancer genomics; tumor microenvironment; growth control in mammalian cells; transcriptional and post-transcriptional gene regulation.\n\nNeuroscience\nStanley Institute for Cognitive Genomics employs deep sequencing and other tools to study genetics underlying schizophrenia, bipolar disorder, and major depression. Swartz Center for the Neural Mechanisms of Cognition studies cognition in the normal brain as a baseline for understanding dysfunction in psychiatric and neurodegenerative disorders. Other research foci: autism genetics; mapping of the mammalian brain; neural correlates of decision making.\n\nPlant Biology\nPlant genome sequencing; epigenetics and stem cell fate; stem cell signaling; plant-environment interactions; using genetic insights to increase yield of staple crops, e.g., maize, rice, wheat; increase fruit yield in flowering plants, e.g., tomato. Other initiatives: genetics of aquatic plants for biofuel development; lead role in building National Science Foundation's iPlant cyberinfrastructure. Much of this work takes place on 12 acres of farmland at the nearby CSHL Uplands Farm, where expert staff raise crops and \"Arabidopsis\" plants for studies. Seven CSHL faculty members conduct research primarily in plant biology - Drs. David Jackson, Zachary Lippman, Robert Martienssen, Richard McCombie, Ullas Pedmale, Doreen Ware, and Thomas Gingeras.\n\nSimons Center for Quantitative Biology\nGenome assembly and validation; mathematical modeling and algorithm development; population genetics; applied statistical and machine learning; biomedical text-mining; computational genomics; cloud computing and Big Data.\n\nIn addition to its research mission, CSHL has a broad educational mission. The Watson School of Biological Sciences (WSBS), established in 1998, awards the Ph.D. degree and fully funds the research program of every student. Students are challenged to obtain their doctoral degree in 4–5 years. The Undergraduate Research Program (URP) for gifted college students (established in 1959), and the Partners for the Future Program for advanced high school students (established in 1990) are now hosted at the WSBS.\n\nThe CSHL Meetings & Courses Program brings over 8,500 scientists from around the world to Cold Spring Harbor annually to share research results – mostly unpublished—in 60 meetings, most held biannually; and to learn new technologies in 30 to 35 professional courses, most offered annually. The Cold Spring Harbor Symposium series, held every year since 1933 with the exception of three years during the Second World War, has been a forum for researchers in genetics, genomics, neuroscience and plant biology. At the Banbury Center, about 25-30 discussion-style meetings are held yearly for a limited number of invited participants. As of 2016 a two-week course at CSHL costs between $3,700 and $4,700 per student and a three-day conferences cost about $1,000 per attendee.\n\nThe DNA Learning Center (DNALC), founded in 1988, was among the early pioneers in developing hands-on genetics lab experiences for middle and high school students. In 2013, 31,000 students on Long Island and New York City were taught genetics labs at the DNALC and satellite facilities in New York. Over 9,000 high school biology teachers have participated in DNALC teacher-training programs.\n\nThe Cold Spring Harbor Laboratory Press has established a program consisting of seven journals, 190 books, laboratory manuals and protocols, and online services for research preprints.\n\nIn 2015, CSHL had an operating budget of $150 million, over $100 million of which was spent on research. Half of the research budget was devoted to cancer; 25% to neuroscience; 15% to genomics and quantitative biology; and 10% to plant sciences. The sources of research funding in 2015 were: 34% Federal (primarily National Institutes of Health and National Science Foundation); 26% auxiliary activities; 22% private philanthropy; 10% endowment; 3% corporate.\n\nThe institution took root as The Biological Laboratory in 1890, a summer program for the education of college and high school teachers studying zoology, botany, comparative anatomy and nature. The program began as an initiative of Eugene G. Blackford and Franklin Hooper, director of the Brooklyn Institute of Arts and Sciences, the founding institution of The Brooklyn Museum. In 1904, the Carnegie Institution of Washington established the Station for Experimental Evolution at Cold Spring Harbor on an adjacent parcel. In 1921, the station was reorganized as the Carnegie Institution Department of Genetics.\n\nBetween 1910 and 1939, the laboratory was the base of the Eugenics Record Office of biologist Charles B. Davenport and his assistant Harry H. Laughlin, two prominent American eugenicists of the period. Davenport was director of the Carnegie Station from its inception until his retirement in 1934. In 1935 the Carnegie Institution sent a team to review the ERO's work, and as a result the ERO was ordered to stop all work. In 1939 the Institution withdrew funding for the ERO entirely, leading to its closure. The ERO's reports, articles, charts, and pedigrees were considered scientific facts in their day, but have since been discredited. However, its closure came 15 years after its findings were incorporated into the National Origins Act (Immigration Act of 1924), which severely reduced the number of immigrants to America from southern and eastern Europe who, Harry Laughlin testified, were racially inferior to the Nordic immigrants from England and Germany. Charles Davenport was also the founder and the first director of the International Federation of Eugenics Organizations in 1925. Today, Cold Spring Harbor Laboratory maintains the full historical records, communications and artifacts of the ERO for historical, teaching and research purposes. The documents are housed in a campus archive and can be accessed online and in a series of multimedia websites.\n\nCarnegie Institution scientists at Cold Spring Harbor made many contributions to genetics and medicine. In 1908 George H. Shull discovered hybrid corn and the genetic principle behind it called heterosis, or \"hybrid vigor.\" This would become the foundation of modern agricultural genetics. Clarence C. Little in 1916 was among the first scientists to demonstrate a genetic component of cancer. E. Carleton MacDowell in 1928 discovered a strain of mouse called C58 that developed spontaneous leukemia – an early mouse model of cancer. In 1933, Oscar Riddle isolated prolactin, the milk secretion hormone and Wilbur Swingle participated in the discovery of adrenocortical hormone, used to treat Addison's disease.\n\nMilislav Demerec was named director of the Laboratory in 1941. Demerec shifted the Laboratory's research focus to the genetics of microbes, thus setting investigators on a course to study the biochemical function of the gene. During World War Two, Demerec directed efforts at Cold Spring Harbor that resulted in major increases in penicillin production.\n\nBeginning in 1941, and annually from 1945, three of the seminal figures of molecular genetics convened summer meetings at Cold Spring Harbor of what they called the Phage Group. Salvador Luria, of Indiana University; Max Delbrück, then of Vanderbilt University; and Alfred Hershey, then of Washington University, St. Louis, sought to discover the nature of genes through study of viruses called bacteriophages that infect bacteria.\n\n\nNobel Prize winners\n\n\nIn 1962, the Department of Genetics, no longer supported by the Carnegie Institution of Washington, formally merged with the Biological Laboratory to form the Cold Spring Harbor Laboratory of Quantitative Biology. In 1970, the name was simplified to Cold Spring Harbor Laboratory.\n\nJames D. Watson served as the Laboratory's director and president for 35 years. Upon taking charge in 1968, he focused the Laboratory on cancer research, creating a tumor virus group and successfully obtaining federal funds for an expansion of cancer research capabilities. Watson placed CSHL on a firm financial footing. Inspired by his Nobel collaborator, Francis Crick, Watson initiated a major push to scale-up CSHL research on the brain and psychiatric disorders, beginning in the late 1980s. In 1990, work was completed on the Arnold and Mabel Beckman Laboratory, and the Marks Neuroscience Building was opened in 1999. In 1994, Watson ceased being director of the Laboratory and assumed the title of president. In 2004 he was named chancellor, a position he held until October 2007, when he retired at the age of 79 after views attributed to him on race and intelligence appeared in the British press. He is now chancellor emeritus.\n\nSince 1994 biochemist and cancer biologist Bruce Stillman has led the Laboratory as director, and since 2003 as president. Stillman, a member of the National Academy of Sciences and a Fellow of the Royal Society, also continues to run a basic research lab, devoted to the study of DNA replication and chromosome maintenance. Stillman is credited with the 1991 discovery and elucidation of the mechanism of the Origin Recognition Complex (ORC), a highly conserved protein complex that recognizes and binds to specific DNA sequences, marking starting points for replication of the entire genome.\n\nStillman has presided over a major expansion of the Laboratory, its size growing threefold since he became director. With construction completed on six linked laboratory buildings on the Hillside Campus in 2009, CSHL added much-needed new laboratory space for cancer and neuroscience research, as well as space for a new program on quantitative biology to bring experts in mathematics, computer science, statistics, and physics to problems in biology.\n\n\n\n"}
{"id": "5042498", "url": "https://en.wikipedia.org/wiki?curid=5042498", "title": "Compete America", "text": "Compete America\n\nCompete America, The Alliance for a Competitive Workforce, is a coalition representing corporations, universities, research institutions and trade associations that advocates for reform of U.S. immigration policy for highly educated foreign professionals.\n\nCompete America supports the idea that scientists, researchers, innovators and engineers will always be in demand and will always drive economic growth and job creation, whether American or foreign-born. Because of this, they argue that arbitrarily low visa quotas and massive backlogs in the system plague the employment- based visa process.\n\nCompete America's legislative goals are:\n\nCompete America believes in the following principles:\n\n\n\n"}
{"id": "3224769", "url": "https://en.wikipedia.org/wiki?curid=3224769", "title": "Computer network programming", "text": "Computer network programming\n\nComputer network programming involves writing computer programs that enable processes to communicate with each other across a computer network.\n\nVery generally, most of communications can be divided into connection-oriented, and connectionless. Whether a communication is a connection-oriented, or connectionless, is defined by the communication protocol, and not by . Examples of the connection-oriented protocols include and , and examples of connectionless protocols include , \"raw IP\", and .\n\nFor connection-oriented communications, communication parties usually have different roles. One party is usually waiting for incoming connections; this party is usually referred to as \"server\". Another party is the one which initiates connection; this party is usually referred to as \"client\".\n\nFor connectionless communications, one party (\"server\") is usually waiting for an incoming packet, and another party (\"client\") is usually understood as the one which sends an unsolicited packet to \"server\".\n\nNetwork programming traditionally covers different layers of OSI/ISO model (most of application-level programming belongs to L4 and up). The table below contains some examples of popular protocols belonging to different OSI/ISO layers, and popular APIs for them.\n\n"}
{"id": "12130019", "url": "https://en.wikipedia.org/wiki?curid=12130019", "title": "Consumer-to-business", "text": "Consumer-to-business\n\nConsumer-to-business (C2B) is a business model in which consumers (individuals) create value and businesses consume that value. For example, when a consumer writes reviews or when a consumer gives a useful idea for new product development then that consumer is creating value for the business if the business adopts the input. In the C2B model, a reverse auction or demand collection model, enables buyers to name or demand their own price, which is often binding, for a specific good or service. Inside of a consumer to business market the roles involved in the transaction must be established and the consumer must offer something of value to the business. \n\nAnother form of C2B is the electronic commerce business model in which consumers can offer products and services to companies, and the companies pay the consumers. This business model is a complete reversal of the traditional business model in which companies offer goods and services to consumers (business-to-consumer = B2C). We can see the C2B model at work in blogs or internet forums in which the author offers a link back to an online business thereby facilitating the purchase of a product (like a book on Amazon.com), for which the author might receive affiliate revenues from a successful sale. Elance was the first C2B model e-commerce site.\n\nC2B is a kind of economic relationship that is qualified as an inverted business type. The advent of the C2B scheme is due to:\n\n\nNowadays people have smartphones or connect to the internet through personal tablets/computers daily allowing consumers to engage with brands online. According to Katherine Arline, in traditional consumer-to-business models companies would promote goods and services to consumers, but a shift has occurred to allow consumers to be the driving force behind a transaction. To the consumers benefit, reverse auctions occur in consumer to business markets allowing the consumer to name their price for a product or service. A consumer can also provide value to a business by offering to promote a business products on a consumers blog or social media platforms. Businesses are provided value through their consumers and vice versa. Businesses gain in C2B from the consumers willingness to negotiate price, contribute data, or market to the company. Consumers profit from direct payment of the reduced-price goods and services and the flexibility of the transaction the C2B market created. Consumer to business markets have their downfall as well. C2B is still a relatively new business practice and has not been fully studied.\n\nConsumer to business is an up and coming business market that can be utilized as a companies entire business model or added to an already existing model. Consumer to business (C2B) is the opposite of business to consumer (B2C) practices and is facilitated by the internet or online forms of technology. Another important distinction between the traditional business to consumer market is that the consumer chooses to be apart of the business relationship inside a consumer to business market. For a relationship to exist though both parties must acknowledge that it exists implying that the relationship is important to both participants.\n\nData and analytics are going to drive the C2B world and enable companies to gain a better understanding of customers. Businesses need to go back to what drives the sales, people. Move away from innovation and the newest technology and go back to who, what, and why of the people interacting with businesses.\n\n"}
{"id": "47720785", "url": "https://en.wikipedia.org/wiki?curid=47720785", "title": "Defaqto", "text": "Defaqto\n\nDefaqto is an independent financial information business, focused on helping financial institutions and consumers make better informed decisions. At its heart is its financial product and fund database consisting of data from across the whole UK market. From this, Defaqto creates a range of products and services – ratings, software solutions, data services, and publications and events.\n\nDefaqto produce a range of independent ratings to help consumers understand how financial products fit into the market place.\n\nThe ratings are created as expert reviews of financial products (such as Michelin reviews) as opposed to consumer revenues (such as TripAdvisor).\n\nRatings are based on the quality and comprehensiveness of the features and benefits it offers. Defaqto rate individual propositions, not the provider of the product, across more than 60 categories – including banking, general insurance, life and protection, and pensions and investments.\n\nStar Ratings give a product or proposition a rating of 1 to 5, depending on the quality and comprehensiveness of the features it offers. A 4 or 5 Star Rating indicates that a product or proposition represents one of the best quality offerings in the market. The ratings focus on the quality of the product as opposed to its price or the service that you may receive from contact with the provider.\n\nRatings help identify where funds and fund families sit in the market, Defaqto Diamond Ratings provide an independent rating based on fund performance, plus various risk attributes where applicable, and a range of other key fund attributes – including cost, scale, access and manager longevity.\n\nDefaqto are based in Haddenham, Buckinghamshire.\n\nDefaqto was founded as the Independent Research Group by Alastair Whitehead in 1994, initially focusing on life pensions products.\n\nThe business was acquired by Find Portal in 2006 in order to grow the presence in the consumer space.\n\nThe management of Defaqto completed a management buy-out supported by Synova Capital in March 2015.\n"}
{"id": "36395445", "url": "https://en.wikipedia.org/wiki?curid=36395445", "title": "Dielectric withstand test", "text": "Dielectric withstand test\n\nA dielectric withstand test or \"high potential\" or \"hipot\" test is an electrical test performed on a component or product to determine the effectiveness of its insulation. The test may be between mutually insulated sections of a part or energized parts and electrical ground. The test is a means to qualify a device's ability to operate safely during rated electrical conditions. If the current through a device under test is less than a specified limit at the required test potential and time duration, the device meets the dielectric withstand requirement. A dielectric withstand test may be done as a factory test on new equipment, or may be done on apparatus already in service as a routine maintenance test.\nVoltage withstand testing is done with a high voltage source and voltage and current meters. A single instrument called a \"hipot tester\" is often used to perform this test. It applies the necessary voltages to a device and monitors leakage current. The current can trip a fault indicator. The tester has output overload protection. The test voltage may be either direct current or alternating current at power frequency or other frequency, like resonant frequency (30 to 300 Hz determined by load) or VLF (0.01 Hz to 0.1 Hz), when convenient. The maximum voltage is given in the test standard for the particular product. The application rate may also be adjusted to manage leakage currents resulting from inherent capacitive effects of the test object. The duration of the test may be up to 60 seconds or more. The applied voltage, rate of application and test duration depend on the specification requirements of the equipment. Different test standards apply for consumer electronics, military electrical devices, high voltage cables, switchgear and other apparatus.\n\nTypical hipot equipment leakage current trip limit settings range between 0.5 and 20 mA and are set by the user according to test object characteristics and rate of voltage application. The objective is to choose a current setting that will not cause the tester to falsely trip during voltage application, while at the same time, selecting a value that minimizes possible damage to the device under test should an inadvertent discharge or breakdown occur.\n\n"}
{"id": "24231219", "url": "https://en.wikipedia.org/wiki?curid=24231219", "title": "Distributed element filter", "text": "Distributed element filter\n\nA distributed element filter is an electronic filter in which capacitance, inductance and resistance (the elements of the circuit) are not localised in discrete capacitors, inductors and resistors as they are in conventional filters. Its purpose is to allow a range of signal frequencies to pass, but to block others. Conventional filters are constructed from inductors and capacitors, and the circuits so built are described by the lumped element model, which considers each element to be \"lumped together\" at one place. That model is conceptually simple, but it becomes increasingly unreliable as the frequency of the signal increases, or equivalently as the wavelength decreases. The distributed element model applies at all frequencies, and is used in transmission line theory; many distributed element components are made of short lengths of transmission line. In the distributed view of circuits, the elements are distributed along the length of conductors and are inextricably mixed together. The filter design is usually concerned only with inductance and capacitance, but because of this mixing of elements they cannot be treated as separate \"lumped\" capacitors and inductors. There is no precise frequency above which distributed element filters must be used but they are especially associated with the microwave band (wavelength less than one metre).\n\nDistributed element filters are used in many of the same applications as lumped element filters, such as selectivity of radio channel, bandlimiting of noise and multiplexing of many signals into one channel. Distributed element filters may be constructed to have any of the bandforms possible with lumped elements (low-pass, band-pass, etc.) with the exception of high-pass, which is usually only approximated. All filter classes used in lumped element designs (Butterworth, Chebyshev, etc.) can be implemented using a distributed element approach.\n\nThere are many component forms used to construct distributed element filters, but all have the common property of causing a discontinuity on the transmission line. These discontinuities present a reactive impedance to a wavefront travelling down the line, and these reactances can be chosen by design to serve as approximations for lumped inductors, capacitors or resonators, as required by the filter.\n\nThe development of distributed element filters was spurred on by the military need for radar and electronic counter measures during World War II. Lumped element analogue filters had long before been developed but these new military systems operated at microwave frequencies and new filter designs were required. When the war ended, the technology found applications in the microwave links used by telephone companies and other organisations with large fixed-communication networks, such as television broadcasters. Nowadays the technology can be found in several mass-produced consumer items, such as the converters (figure 1 shows an example) used with satellite television dishes.\n\nDistributed element filters are mostly used at frequencies above the VHF (Very High Frequency) band (30 to 300 MHz). At these frequencies, the physical length of passive components is a significant fraction of the wavelength of the operating frequency, and it becomes difficult to use the conventional lumped element model. The exact point at which distributed element modelling becomes necessary depends on the particular design under consideration. A common rule of thumb is to apply distributed element modelling when component dimensions are larger than 0.1λ. The increasing miniaturisation of electronics has meant that circuit designs are becoming ever smaller compared to λ. The frequencies beyond which a distributed element approach to filter design becomes necessary are becoming ever higher as a result of these advances. On the other hand, antenna structure dimensions are usually comparable to λ in all frequency bands and require the distributed element model.\n\nThe most noticeable difference in behaviour between a distributed element filter and its lumped-element approximation is that the former will have multiple passband replicas of the lumped-element prototype passband, because transmission line transfer characteristics repeat at harmonic intervals. These spurious passbands are undesirable in most cases.\n\nFor clarity of presentation, the diagrams in this article are drawn with the components implemented in stripline format. This does not imply an industry preference, although planar transmission line formats (that is, formats where conductors consist of flat strips) are popular because they can be implemented using established printed circuit board manufacturing techniques. The structures shown can also be implemented using microstrip or buried stripline techniques (with suitable adjustments to dimensions) and can be adapted to coaxial cables, twin leads and waveguides, although some structures are more suitable for some implementations than others. The open wire implementations, for instance, of a number of structures are shown in the second column of figure 3 and open wire equivalents can be found for most other stripline structures. Planar transmission lines are also used in integrated circuit designs.\n\nDevelopment of distributed element filters began in the years before World War II. Warren P. Mason founded the field of distributed element circuits. A major paper on the subject was published by Mason and Sykes in 1937. Mason had filed a patent much earlier, in 1927, and that patent may contain the first published electrical design which moves away from a lumped element analysis. Mason and Sykes' work was focused on the formats of coaxial cable and balanced pairs of wires – the planar technologies were not yet in use. Much development was carried out during the war years driven by the filtering needs of radar and electronic counter-measures. A good deal of this was at the MIT Radiation Laboratory, but other laboratories in the US and the UK were also involved.\n\nSome important advances in network theory were needed before filters could be advanced beyond wartime designs. One of these was the commensurate line theory of Paul Richards. Commensurate lines are networks in which all the elements are the same length (or in some cases multiples of the unit length), although they may differ in other dimensions to give different characteristic impedances. Richards' transformation allows a lumped element design to be taken \"as is\" and transformed directly into a distributed element design using a very simple transform equation.\n\nThe difficulty with Richards' transformation from the point of view of building practical filters was that the resulting distributed element design invariably included series connected elements. This was not possible to implement in planar technologies and was often inconvenient in other technologies. This problem was solved by K. Kuroda who used impedance transformers to eliminate the series elements. He published a set of transformations known as Kuroda's identities in 1955, but his work was written in Japanese and it was several years before his ideas were incorporated into the English-language literature.\n\nFollowing the war, one important research avenue was trying to increase the design bandwidth of wide-band filters. The approach used at the time (and still in use today) was to start with a lumped element prototype filter and through various transformations arrive at the desired filter in a distributed element form. This approach appeared to be stuck at a minimum \"Q\" of five (see Band-pass filters below for an explanation of \"Q\"). In 1957, Leo Young at Stanford Research Institute published a method for designing filters which \"started\" with a distributed element prototype. This prototype was based on quarter wave impedance transformers and was able to produce designs with bandwidths up to an octave, corresponding to a \"Q\" of about 1.3. Some of Young's procedures in that paper were empirical, but later, exact solutions were published. Young's paper specifically addresses directly coupled cavity resonators, but the procedure can equally be applied to other directly coupled resonator types, such as those found in modern planar technologies and illustrated in this article. The capacitive gap filter (figure 8) and the parallel-coupled lines filter (figure 9) are examples of directly coupled resonators.\n\nThe introduction of printed planar technologies greatly simplified the manufacture of many microwave components including filters, and microwave integrated circuits then became possible. It is not known when planar transmission lines originated, but experiments using them were recorded as early as 1936. The inventor of printed stripline, however, is known; this was Robert M. Barrett who published the idea in 1951. This caught on rapidly, and Barrett's \"stripline\" soon had fierce commercial competition from rival planar formats, especially \"triplate\" and \"microstrip\". The generic term \"stripline\" in modern usage usually refers to the form then known as \"triplate\".\n\nEarly stripline directly coupled resonator filters were end-coupled, but the length was reduced and the compactness successively increased with the introduction of parallel-coupled line filters, interdigital filters, and comb-line filters. Much of this work was published by the group at Stanford led by George Matthaei, and also including Leo Young mentioned above, in a landmark book which still today serves as a reference for circuit designers. The hairpin filter was first described in 1972. By the 1970s, most of the filter topologies in common use today had been described. More recent research has concentrated on new or variant mathematical classes of the filters, such as pseudo-elliptic, while still using the same basic topologies, or with alternative implementation technologies such as suspended stripline and finline.\n\nThe initial non-military application of distributed element filters was in the microwave links used by telecommunications companies to provide the backbone of their networks. These links were also used by other industries with large, fixed networks, notably television broadcasters. Such applications were part of large capital investment programs. However, mass-production manufacturing made the technology cheap enough to incorporate in domestic satellite television systems. An emerging application is in superconducting filters for use in the cellular base stations operated by mobile phone companies.\n\nThe simplest structure that can be implemented is a step in the characteristic impedance of the line, which introduces a discontinuity in the transmission characteristics. This is done in planar technologies by a change in the width of the transmission line. Figure 4(a) shows a step up in impedance (narrower lines have higher impedance). A step down in impedance would be the mirror image of figure 4(a). The discontinuity can be represented approximately as a series inductor, or more exactly, as a low-pass T circuit as shown in figure 4(a). Multiple discontinuities are often coupled together with impedance transformers to produce a filter of higher order. These impedance transformers can be just a short (often λ/4) length of transmission line. These composite structures can implement any of the filter families (Butterworth, Chebyshev, etc.) by approximating the rational transfer function of the corresponding lumped element filter. This correspondence is not exact since distributed element circuits cannot be rational and is the root reason for the divergence of lumped element and distributed element behaviour. Impedance transformers are also used in hybrid mixtures of lumped and distributed element filters (the so-called semi-lumped structures).\nAnother very common component of distributed element filters is the stub. Over a narrow range of frequencies, a stub can be used as a capacitor or an inductor (its impedance is determined by its length) but over a wide band it behaves as a resonator. Short-circuit, nominally quarter-wavelength stubs (figure 3(a)) behave as shunt LC antiresonators, and an open-circuit nominally quarter-wavelength stub (figure 3(b)) behaves as a series LC resonator. Stubs can also be used in conjunction with impedance transformers to build more complex filters and, as would be expected from their resonant nature, are most useful in band-pass applications. While open-circuit stubs are easier to manufacture in planar technologies, they have the drawback that the termination deviates significantly from an ideal open circuit (see figure 4(b)), often leading to a preference for short-circuit stubs (one can always be used in place of the other by adding or subtracting λ/4 to or from the length).\n\nA helical resonator is similar to a stub, in that it requires a distributed element model to represent it, but is actually built using lumped elements. They are built in a non-planar format and consist of a coil of wire, on a former and core, and connected only at one end. The device is usually in a shielded can with a hole in the top for adjusting the core. It will often look physically very similar to the lumped LC resonators used for a similar purpose. They are most useful in the upper VHF and lower UHF bands whereas stubs are more often applied in the higher UHF and SHF bands.\n\nCoupled lines (figures 3(c-e)) can also be used as filter elements; like stubs, they can act as resonators and likewise be terminated short-circuit or open-circuit. Coupled lines tend to be preferred in planar technologies, where they are easy to implement, whereas stubs tend to be preferred elsewhere. Implementing a true open circuit in planar technology is not feasible because of the dielectric effect of the substrate which will always ensure that the equivalent circuit contains a shunt capacitance. Despite this, open circuits are often used in planar formats in preference to short circuits because they are easier to implement. Numerous element types can be classified as coupled lines and a selection of the more common ones is shown in the figures.\n\nSome common structures are shown in figures 3 and 4, along with their lumped-element counterparts. These lumped-element approximations are not to be taken as equivalent circuits but rather as a guide to the behaviour of the distributed elements over a certain frequency range. Figures 3(a) and 3(b) show a short-circuit and open-circuit stub, respectively. When the stub length is λ/4, these behave, respectively, as anti-resonators and resonators and are therefore useful, respectively, as elements in band-pass and band-stop filters. Figure 3(c) shows a short-circuited line coupled to the main line. This also behaves as a resonator, but is commonly used in low-pass filter applications with the resonant frequency well outside the band of interest. Figures 3(d) and 3(e) show coupled line structures which are both useful in band-pass filters. The structures of figures 3(c) and 3(e) have equivalent circuits involving stubs placed in series with the line. Such a topology is straightforward to implement in open-wire circuits but not with a planar technology. These two structures are therefore useful for implementing an equivalent series element.\n\nA low-pass filter can be implemented quite directly from a ladder topology lumped-element prototype with the stepped impedance filter shown in figure 5. This is also called a \"cascaded lines\" design. The filter consists of alternating sections of high-impedance and low-impedance lines which correspond to the series inductors and shunt capacitors in the lumped-element implementation. Low-pass filters are commonly used to feed direct current (DC) bias to active components. Filters intended for this application are sometimes referred to as \"chokes\". In such cases, each element of the filter is λ/4 in length (where λ is the wavelength of the main-line signal to be blocked from transmission into the DC source) and the high-impedance sections of the line are made as narrow as the manufacturing technology will allow in order to maximise the inductance. Additional sections may be added as required for the performance of the filter just as they would for the lumped-element counterpart. As well as the planar form shown, this structure is particularly well suited for coaxial implementations with alternating discs of metal and insulator being threaded on to the central conductor.\n\nA more complex example of stepped impedance design is presented in figure 6. Again, narrow lines are used to implement inductors and wide lines correspond to capacitors, but in this case, the lumped-element counterpart has resonators connected in shunt across the main line. This topology can be used to design elliptical filters or Chebyshev filters with poles of attenuation in the stopband. However, calculating component values for these structures is an involved process and has led to designers often choosing to implement them as m-derived filters instead, which perform well and are much easier to calculate. The purpose of incorporating resonators is to improve the stopband rejection. However, beyond the resonant frequency of the highest frequency resonator, the stopband rejection starts to deteriorate as the resonators are moving towards open-circuit. For this reason, filters built to this design often have an additional single stepped-impedance capacitor as the final element of the filter. This also ensures good rejection at high frequency.\nAnother common low-pass design technique is to implement the shunt capacitors as stubs with the resonant frequency set above the operating frequency so that the stub impedance is capacitive in the passband. This implementation has a lumped-element counterpart of a general form similar to the filter of figure 6. Where space allows, the stubs may be set on alternate sides of the main line as shown in figure 7(a). The purpose of this is to prevent coupling between adjacent stubs which detracts from the filter performance by altering the frequency response. However, a structure with all the stubs on the same side is still a valid design. If the stub is required to be a very low impedance line, the stub may be inconveniently wide. In these cases, a possible solution is to connect two narrower stubs in parallel. That is, each stub position has a stub on \"both sides\" of the line. A drawback of this topology is that additional transverse resonant modes are possible along the λ/2 length of line formed by the two stubs together. For a choke design, the requirement is simply to make the capacitance as large as possible, for which the maximum stub width of λ/4 may be used with stubs in parallel on both sides of the main line. The resulting filter looks rather similar to the stepped impedance filter of figure 5, but has been designed on completely different principles. A difficulty with using stubs this wide is that the point at which they are connected to the main line is ill-defined. A stub that is narrow in comparison to λ can be taken as being connected on its centre-line and calculations based on that assumption will accurately predict filter response. For a wide stub, however, calculations that assume the side branch is connected at a definite point on the main line leads to inaccuracies as this is no longer a good model of the transmission pattern. One solution to this difficulty is to use radial stubs instead of linear stubs. A pair of radial stubs in parallel (one on either side of the main line) is called a butterfly stub (see figure 7(b)). A group of three radial stubs in parallel, which can be achieved at the end of a line, is called a clover-leaf stub.\n\nA band-pass filter can be constructed using any elements that can resonate. Filters using stubs can clearly be made band-pass; numerous other structures are possible and some are presented below.\n\nAn important parameter when discussing band-pass filters is the fractional bandwidth. This is defined as the ratio of the bandwidth to the geometric centre frequency. The inverse of this quantity is called the Q-factor, \"Q\". If ω and ω are the frequencies of the passband edges, then:\n\nThe capacitive gap structure consists of sections of line about λ/2 in length which act as resonators and are coupled \"end-on\" by gaps in the transmission line. It is particularly suitable for planar formats, is easily implemented with printed circuit technology and has the advantage of taking up no more space than a plain transmission line would. The limitation of this topology is that performance (particularly insertion loss) deteriorates with increasing fractional bandwidth, and acceptable results are not obtained with a \"Q\" less than about 5. A further difficulty with producing low-\"Q\" designs is that the gap width is required to be smaller for wider fractional bandwidths. The minimum width of gaps, like the minimum width of tracks, is limited by the resolution of the printing technology.\n\nParallel-coupled lines is another popular topology for printed boards, for which open-circuit lines are the simplest to implement since the manufacturing consists of nothing more than the printed track. The design consists of a row of parallel λ/2 resonators, but coupling over only λ/4 to each of the neighbouring resonators, so forming a staggered line as shown in figure 9. Wider fractional bandwidths are possible with this filter than with the capacitive gap filter, but a similar problem arises on printed boards as dielectric loss reduces the \"Q\". Lower-\"Q\" lines require tighter coupling and smaller gaps between them which is limited by the accuracy of the printing process. One solution to this problem is to print the track on multiple layers with adjacent lines overlapping but not in contact because they are on different layers. In this way, the lines can be coupled across their width, which results in much stronger coupling than when they are edge-to-edge, and a larger gap becomes possible for the same performance. For other (non-printed) technologies, short-circuit lines may be preferred since the short-circuit provides a mechanical attachment point for the line and \"Q\"-reducing dielectric insulators are not required for mechanical support. Other than for mechanical and assembly reasons, there is little preference for open-circuit over short-circuit coupled lines. Both structures can realize the same range of filter implementations with the same electrical performance. Both types of parallel-coupled filters, in theory, do not have spurious passbands at twice the centre frequency as seen in many other filter topologies (e.g., stubs). However, suppression of this spurious passband requires perfect tuning of the coupled lines which is not realized in practice, so there is inevitably some residual spurious passband at this frequency.\n\nThe hairpin filter is another structure that uses parallel-coupled lines. In this case, each pair of parallel-coupled lines is connected to the next pair by a short link. The \"U\" shapes so formed give rise to the name \"hairpin filter\". In some designs the link can be longer, giving a wide hairpin with λ/4 impedance transformer action between sections. The angled bends seen in figure 10 are common to stripline designs and represent a compromise between a sharp right angle, which produces a large discontinuity, and a smooth bend, which takes up more board area which can be severely limited in some products. Such bends are often seen in long stubs where they could not otherwise be fitted into the space available. The lumped-element equivalent circuit of this kind of discontinuity is similar to a stepped-impedance discontinuity. Examples of such stubs can be seen on the bias inputs to several components in the photograph at the top of the article.\n\nInterdigital filters are another form of coupled-line filter. Each section of line is about λ/4 in length and is terminated in a short-circuit at one end only, the other end being left open-circuit. The end which is short-circuited alternates on each line section. This topology is straightforward to implement in planar technologies, but also particularly lends itself to a mechanical assembly of lines fixed inside a metal case. The lines can be either circular rods or rectangular bars, and interfacing to a coaxial format line is easy. As with the parallel-coupled line filter, the advantage of a mechanical arrangement that does not require insulators for support is that dielectric losses are eliminated. The spacing requirement between lines is not as stringent as in the parallel line structure; as such, higher fractional bandwidths can be achieved, and \"Q\" values as low as 1.4 are possible.\n\nThe comb-line filter is similar to the interdigital filter in that it lends itself to mechanical assembly in a metal case without dielectric support. In the case of the comb-line, all the lines are short-circuited at the same end rather than alternate ends. The other ends are terminated in capacitors to ground, and the design is consequently classified as semi-lumped. The chief advantage of this design is that the upper stopband can be made very wide, that is, free of spurious passbands at all frequencies of interest.\n\nAs mentioned above, stubs lend themselves to band-pass designs. General forms of these are similar to stub low-pass filters except that the main line is no longer a narrow high impedance line. Designers have many different topologies of stub filters to choose from, some of which produce identical responses. An example stub filter is shown in figure 12; it consists of a row of λ/4 short-circuit stubs coupled together by λ/4 impedance transformers. The stubs in the body of the filter are double paralleled stubs while the stubs on the end sections are only singles, an arrangement that has impedance matching advantages. The impedance transformers have the effect of transforming the row of shunt anti-resonators into a ladder of series resonators and shunt anti-resonators. A filter with similar properties can be constructed with λ/4 open-circuit stubs placed in series with the line and coupled together with λ/4 impedance transformers, although this structure is not possible in planar technologies.\n\nYet another structure available is λ/2 open-circuit stubs across the line coupled with λ/4 impedance transformers. This topology has both low-pass and band-pass characteristics. Because it will pass DC, it is possible to transmit biasing voltages to active components without the need for blocking capacitors. Also, since short-circuit links are not required, no assembly operations other than the board printing are required when implemented as stripline. The disadvantages are (i) the filter will take up more board real estate than the corresponding λ/4 stub filter, since the stubs are all twice as long; (ii) the first spurious passband is at 2ω, as opposed to 3ω for the λ/4 stub filter.\n\nKonishi describes a wideband 12 GHz band-pass filter, which uses 60° butterfly stubs and also has a low-pass response (short-circuit stubs are required to prevent such a response). As is often the case with distributed element filters, the bandform into which the filter is classified largely depends on which bands are desired and which are considered to be spurious.\n\nGenuine high-pass filters are difficult, if not impossible, to implement with distributed elements. The usual design approach is to start with a band-pass design, but make the upper stopband occur at a frequency that is so high as to be of no interest. Such filters are described as pseudo-high-pass and the upper stopband is described as a vestigial stopband. Even structures that seem to have an \"obvious\" high-pass topology, such as the capacitive gap filter of figure 8, turn out to be band-pass when their behaviour for very short wavelengths is considered.\n\n"}
{"id": "444349", "url": "https://en.wikipedia.org/wiki?curid=444349", "title": "Extracorporeal membrane oxygenation", "text": "Extracorporeal membrane oxygenation\n\nExtracorporeal membrane oxygenation (ECMO), also known as extracorporeal life support (ECLS), is an extracorporeal technique of providing prolonged cardiac and respiratory support to persons whose heart and lungs are unable to provide an adequate amount of gas exchange or perfusion to sustain life. The technology for ECMO is largely derived from cardiopulmonary bypass, which provides shorter-term support with arrested native circulation.\n\nThis intervention has mostly been used on children, but it is seeing more use in adults with cardiac and respiratory failure. ECMO works by removing blood from the person's body and artificially removing the carbon dioxide and oxygenating red blood cells. Generally, it is used either post-cardiopulmonary bypass or in late stage treatment of a person with profound heart and/or lung failure, although it is now seeing use as a treatment for cardiac arrest in certain centers, allowing treatment of the underlying cause of arrest while circulation and oxygenation are supported.\nGuidelines that describe the indications and practice of ECMO are published by the Extracorporeal Life Support Organization (ELSO). Criteria for the initiation of ECMO vary by institution, but generally include acute severe cardiac or pulmonary failure that is potentially reversible and unresponsive to conventional management. Examples of clinical situations that may prompt the initiation of ECMO include the following:\n\nIn those with cardiac arrest or cardiogenic shock, it appears to improve survival and good outcomes.\n\nEarly studies had shown survival benefit with use of ECMO for people in acute respiratory failure especially in the setting of acute respiratory distress syndrome. A registry maintained by ELSO of nearly 51,000 people that have received ECMO has reported outcomes with 75% survival for neonatal respiratory failure, 56% survival for pediatric respiratory failure, and 55% survival for adult respiratory failure. Other observational and uncontrolled clinical trials have reported survival rates from 50 to 70 percent. These reported survival rates are better than historical survival rates.\n\nIn the United Kingdom, veno-venous ECMO deployment is concentrated in designated ECMO centers to potentially improve care and promote better outcomes.\n\nMost contraindications are relative, balancing the risks of the procedure (including the risk of using valuable resources that could be used for others) versus the potential benefits. The relative contraindications are:\n\nThere are several forms of ECMO; the two most common are veno-arterial (VA) ECMO and veno-venous (VV) ECMO. In both modalities, blood drained from the venous system is oxygenated outside of the body. In VA ECMO, this blood is returned to the arterial system and in VV ECMO the blood is returned to the venous system. In VV ECMO, no cardiac support is provided.\n\nIn veno-arterial (VA) ECMO, a venous cannula is usually placed in the right or left common femoral vein for extraction, and an arterial cannula is usually placed into the right or left femoral artery for infusion. The tip of the femoral venous cannula should be maintained near the junction of the inferior vena cava and right atrium, while the tip of the femoral arterial cannula is maintained in the iliac artery. In adults, accessing the femoral artery is preferred because the insertion is simpler. Central VA ECMO may be used if cardiopulmonary bypass has already been established or emergency re-sternotomy has been performed (with cannulae in the right atrium(or SVC/IVC for tricuspid repair) and ascending aorta).\n\nVA ECMO is typically reserved when native cardiac function is minimal to mitigate increased cardiac stroke work associated with pumping against retrograde flow delivered by the aortic cannula.\n\nIn veno-venous (VV) ECMO, cannulae are usually placed in the right common femoral vein for drainage and right internal jugular vein for infusion. Alternatively, a dual-lumen catheter is inserted into the right internal jugular vein, draining blood from the superior and inferior vena cavae and returning it to the right atrium.\n\nECMO should be performed only by clinicians with training and experience in its initiation, maintenance, and discontinuation. ECMO management is commonly performed by a Respiratory Therapist or Perfusionist. Once it has been decided that ECMO will be initiated, the person is anticoagulated with intravenous heparin and then the cannulae are inserted. ECMO support is initiated once the cannulae are connected to the appropriate limbs of the ECMO circuit.\n\nCannulae can be placed percutaneously by the Seldinger technique, a relatively straightforward and common method for obtaining access to blood vessels, or via surgical cutdown. The largest cannulae that can be placed in the vessels are used in order to maximize flow and minimize shear stress.\n\nECMO required for complications post-cardiac surgery can be placed directly into the appropriate chambers of the heart or great vessels. Central cannulation via lateral thoracotomy allows patients awaiting lung transplantation to remain unsedated and ambulatory.\n\nFollowing cannulation and connection to the ECMO circuit, the appropriate amount of blood flow through the ECMO circuit is determined using hemodynamic parameters and physical exam. Goals of maintaining end-organ perfusion via ECMO circuit are balanced with sufficient physiologic blood flow through the heart to prevent stasis and subsequent formation of blood clot.\n\nOnce the initial respiratory and hemodynamic goals have been achieved, the blood flow is maintained at that rate. Frequent assessment and adjustments are facilitated by continuous venous oximetry, which directly measures the oxyhemoglobin saturation of the blood in the venous limb of the ECMO circuit.\n\nVV ECMO is typically used for respiratory failure, while VA ECMO is used for cardiac failure. There are unique considerations for each type of ECMO, which influence management.\n\nNear-maximum flow rates are usually desired during VV ECMO to optimize oxygen delivery. In contrast, the flow rate used during VA ECMO must be high enough to provide adequate perfusion pressure and venous oxyhemoglobin saturation (measured on drainage blood) but low enough to provide sufficient preload to maintain left ventricular output.\n\nSince most people are fluid-overloaded when ECMO is initiated, aggressive diuresis is warranted once the person is stable on ECMO. Ultrafiltration can be easily added to the ECMO circuit if the person has inadequate urine output. ECMO \"chatter\", or instability of ECMO waveforms, represents under-resuscitation and would support cessation of aggressive diuresis of ultrafiltration.\n\nLeft ventricular output is rigorously monitored during VA ECMO because left ventricular function can be impaired from increased afterload, which can in turn lead to formation of thrombus within the heart.\n\nFor those with respiratory failure, improvements in radiographic appearance, pulmonary compliance, and arterial oxyhemoglobin saturation indicate that the person may be ready to be taken off of ECMO support. For those with cardiac failure, enhanced aortic pulsatility correlates with improved left ventricular output and indicates that they may be ready to be taken off of ECMO support. Once the decision has been made to discontinue ECMO, the cannulae are removed.\n\nVV ECMO trials are performed by eliminating all countercurrent sweep gas through the oxygenator. Extracorporeal blood flow remains constant, but gas transfer does not occur. They are then observed for several hours, during which the ventilator settings that are necessary to maintain adequate oxygenation and ventilation off ECMO are determined as indicated by arterial and venous blood gas results.\n\nVA ECMO trials require temporary clamping of both the drainage and infusion lines, while allowing the ECMO circuit to circulate through a bridge between the arterial and venous limbs. This prevents thrombosis of stagnant blood within the ECMO circuit. In addition, the arterial and venous lines should be flushed continuously with heparinized saline or intermittently with heparinized blood from the circuit. In general, VA ECMO trials are shorter in duration than VV ECMO trials because of the higher risk of thrombus formation.\n\nA common consequence in ECMO-treated adults is neurological injury, which may include intracerebral hemorrhage, subarachnoid hemorrhage, ischemic infarctions in susceptible areas of the brain, hypoxic-ischemic encephalopathy, unexplained coma, and brain death. Bleeding occurs in 30 to 40 percent of those receiving ECMO and can be life-threatening. It is due to both the necessary continuous heparin infusion and platelet dysfunction. Meticulous surgical technique, maintaining platelet counts greater than 100,000/mm, and maintaining the target activated clotting time reduce the likelihood of bleeding.\n\nHeparin-induced thrombocytopenia (HIT) is increasingly common among people receiving ECMO. When HIT is suspected, the heparin infusion is usually replaced by a non-heparin anticoagulant.\n\nThere is retrograde blood flow in the descending aorta whenever the femoral artery and vein are used for VA ECMO. Stasis of the blood can occur if left ventricular output is not maintained, which may result in thrombosis.\n\nThe CESAR study has shown an improvement in 6 month survival without severe disability in patients getting ECMO versus conventional ventilation in ARDS.\n\nIn VA ECMO, those whose cardiac function does not recover sufficiently to be weaned from ECMO may be bridged to a ventricular assist device (VAD) or transplant. A variety of complications can occur during cannulation, including vessel perforation with bleeding, arterial dissection, distal ischemia, and incorrect location (e.g., venous cannula placed within the artery), but these events occur highly infrequently.\n\nPreterm infants are at unacceptably high risk for intraventricular hemorrhage (IVH) if administered ECMO at a gestational age less than 32 weeks.\n\nA 2014 study showed that a factor XIIa inhibitory antibody provides thromboprotection in extracorporeal circulation without increasing bleeding risk. Experiments on neonatal animals showed that ECMO treatment can lead to apoptosis of enterocytes, damage of the intestinal mucosal barrier and bacterial translocation. This might explain greater severity of systemic inflammatory response syndrome in neonates. ECMO has also seen its use on cadavers as being able to increase the viability rate of transplanted organs.\n"}
{"id": "50797595", "url": "https://en.wikipedia.org/wiki?curid=50797595", "title": "Form factor (design)", "text": "Form factor (design)\n\nForm factor is an aspect of hardware design which defines and prescribes the size, shape, and other physical specifications of components, particularly in consumer electronics and electronic packaging. A form factor may represent a broad class of similarly sized components, or it may prescribe a specific standard.\n\nAs electronic hardware has become smaller following Moore's law and related patterns, ever-smaller form factors have become feasible. Specific technological advances, such as PCI Express, have had a significant design impact, though form factors have historically been slower to evolve than individual components. Standardization of form factors is vital for compatibility of hardware from different manufacturers.\n\nSmaller form factors may offer more efficient use of limited space, greater flexibility in the placement of components within a larger assembly, reduced use of material, and greater ease of transportation and use. However, smaller form factors typically incur greater costs in the design, manufacturing, and maintenance phases of the engineering lifecycle, and do not allow the same expansion options as larger form factors. In particular, the design of smaller form factor computers and network equipment must entail careful consideration of cooling. End-user maintenance and repair of small form factor electronic devices such as mobile phones is often not possible, and may be discouraged by warranty voiding clauses; such devices require professional servicing—or simply replacement—when they fail.\n\nComputer form factors comprise a number of specific industry standards for motherboards, specifying dimensions, power supplies, placement of mounting holes and ports, and other parameters. Other types of form factors for computers include:\n\n\n\n"}
{"id": "8854512", "url": "https://en.wikipedia.org/wiki?curid=8854512", "title": "History of the motorcycle", "text": "History of the motorcycle\n\nThe history of the motorcycle begins in the second half of the 19th century. Motorcycles are descended from the \"safety bicycle,\" a bicycle with front and rear wheels of the same size and a pedal crank mechanism to drive the rear wheel. Despite some early landmarks in its development, the motorcycle lacks a rigid pedigree that can be traced back to a single idea or machine. Instead, the idea seems to have occurred to numerous engineers and inventors around Europe at around the same time.\n\nIn the 1860s Pierre Michaux, a blacksmith in Paris, founded 'Michaux et Cie' (\"Michaux and company\"), the first company to construct bicycles with pedals called a velocipede at the time, or \"Michauline\". The first steam powered motorcycle, the Michaux-Perreaux steam velocipede, can be traced to 1867, when Pierre's son Ernest Michaux fitted a small steam engine to one of the 'velocipedes'.\n\nThe design went to America when Pierre Lallement, a Michaux employee who also claimed to have developed the prototype in 1863, filed for the first bicycle patent with the US patent office in 1866. In 1868 an American, Sylvester H. Roper of Roxbury, Massachusetts developed a twin-cylinder steam velocipede, with a coal-fired boiler between the wheels. Roper's contribution to motorcycle development ended suddenly when he died demonstrating one of his machines in Cambridge, Massachusetts on June 1, 1896.\n\nAlso in 1868, a French engineer Louis-Guillaume Perreaux patented a similar steam powered single cylinder machine, the Michaux-Perreaux steam velocipede, with an alcohol burner and twin belt drives, which was possibly invented independently of Roper's. Although the patent is dated 1868, nothing indicates the invention had been operable before 1871.\n\nIn 1881, Lucius Copeland of Phoenix, Arizona designed a much smaller steam boiler which could drive the large rear wheel of an American Star high-wheeler at 12 mph. In 1887 Copeland formed the Northrop Manufacturing Co. to produce the first successful 'Moto-Cycle' (actually a three-wheeler).\n\nThe very first commercial design for a self-propelled bicycle was a three-wheel design called the \"Butler Petrol Cycle\", conceived of and built by Edward Butler in England in 1884. He exhibited his plans for the vehicle at the Stanley Cycle Show in London in 1884, two years earlier than Karl Benz invented his first automobile who is generally recognized as the inventor of the modern automobile. Butler's vehicle was also the first design to be shown at the 1885 International Inventions Exhibition in London.\n\nThe vehicle was built by the Merryweather Fire Engine company in Greenwich, in 1888. the Butler Petrol Cycle (first recorded use of the term) It was a three-wheeled vehicle, with the rear wheel directly driven by a /hp (466W) 600 cc (40 in; 2¼×5-inch {57×127-mm}) flat twin four stroke engine (with magneto ignition replaced by coil and battery), equipped with rotary valves and a float-fed carburettor (five years before Maybach), and Ackermann steering, all of which were state of the art at the time. Starting was by compressed air. The engine was liquid-cooled, with a radiator over the rear driving wheel. Speed was controlled by means of a throttle valve lever. No braking system was fitted; the vehicle was stopped by raising and lowering the rear driving wheel using a foot-operated lever; the weight of the machine was then borne by two small castor wheels. The driver was seated between the front wheels. It wasn't, however, a commercial success, as Butler failed to find sufficient financial backing.\nAnother early internal combustion, petroleum fueled motorcycle was the \"Petroleum Reitwagen. It was designed and built by the German inventors Gottlieb Daimler and Wilhelm Maybach in Bad Cannstatt, Germany in 1885. This vehicle was unlike either the safety bicycles or the boneshaker bicycles of the era in that it had zero degrees of steering axis angle and no fork offset, and thus did not use the principles of bicycle and motorcycle dynamics developed nearly 70 years earlier. Instead, it relied on two outrigger wheels to remain upright while turning.\nThe inventors called their invention the \"Reitwagen\" (\"riding car\"). It was designed as an expedient testbed for their new engine, rather than a true prototype vehicle.\n\nIn the decade from the late 1880s, dozens of designs and machines emerged, particularly in Germany and in England, and soon spread to America.\nDuring this early period of motorcycle history there were many manufacturers, since bicycle makers were adapting their designs for the new internal combustion engine.\nIn 1894 Hildebrand & Wolfmüller became the first series production motorcycle, and the first to be called a \"motorcycle\" (). However, only a few hundred examples of this motorcycle were ever built. The first instance of the term \"motor cycle\" also appears in English the same year in materials promoting machines developed by E.J. Pennington, although Pennington's motorcycles never progressed past the prototype stage.\n\nExcelsior Motor Company, originally a bicycle-manufacturing company based in Coventry in Warwickshire (England), began production of their first motorcycle model in 1896, available for purchase by the public. The first production motorcycle in the US was the Orient-Aster, built by Charles Metz in 1898 at his factory in Waltham, Massachusetts.\n\nIn 1898, Peugeot Motocycles presents at the Paris Motorshow the first motorcycle equipped with a Dion-Bouton motor. Peugeot Motocycles remains the oldest motorcycle manufacturer in the world.\n\nIn the early period of motorcycle history, many producers of bicycles adapted their designs to accommodate the new internal-combustion engine. As the engines became more powerful and designs outgrew the bicycle origins, the number of motorcycle producers increased. Many of the nineteenth-century inventors who worked on early motorcycles often moved on to other inventions. Daimler and Roper, for example, both went on to develop automobiles.\n\nAt the turn of the 20th century the first major mass-production firms emerged. \nIn 1901 English quadricycle- and bicycle-maker Royal Enfield introduced its first motorcycle, with a 239 cc engine mounted in the front and driving the rear wheel through a belt. In 1898 English bicycle-maker Triumph decided to extend its focus to include motorcycles, and by 1902 the company had produced its first motorcycle—a bicycle fitted with a Belgian-built engine. A year later it was the largest motorcycle-manufacturer, with an annual production of over 500 units. Other British firms included Norton and Birmingham Small Arms Company who began motorbike production in 1902 and 1910, respectively.\n\nIn 1901 the Indian Manufacturing Company, which had been founded by two former bicycle-racers, designed the so-called \"diamond framed\" Indian Single, whose engine was built by the Aurora Firm in Illinois per Indian's specifications. The Single was made available in the deep blue. Indian's production was up to over 500 bikes by 1902, and would rise to 32,000, its best ever, in 1913.\nIndian produced over 20,000 bikes per year.\nThe oldest surviving Russian-manufactured motorcycle, the Rossiya, dates from 1902.\nThe American company Harley-Davidson started producing motorcycles in 1903.\n\nDuring this period, experimentation and innovation were driven by the popular new sport of motorcycle racing, with its powerful incentive to produce tough, fast, reliable machines. These enhancements quickly found their way to the public's machines.\n\nChief August Vollmer of the Berkeley, California Police Department is credited with organizing the first official police motorcycle-patrol in the United States in 1911.\nBy 1914, motorcycles were no longer just bicycles with engines; they had their own technologies, although many still maintained bicycle elements, like the seats and suspension.\n\nDuring the First World War, motorbike production was greatly ramped up for the war effort to supply effective communications with front line troops. Messengers on horses were replaced with dispatch riders on motorcycles carrying messages, performing reconnaissance personnel and acting as a military police. American company Harley-Davidson was devoting over 50% of its factory output toward military contract by the end of the war. The British company Triumph Motorcycles sold more than 30,000 of its Triumph Type H model to allied forces during the war. With the rear wheel driven by a belt, the Model H was fitted with a 499 cc air-cooled four-stroke single-cylinder engine. It was also the first Triumph not to be fitted with pedals, so was a true motorcycle.\n\nThe Model H in particular, is regarded by many as having been the first \"modern motorcycle\". Introduced in 1915 it had a 550cc side-valve four-stroke engine with a three-speed gearbox and belt transmission. It was so popular with its users that it was nicknamed the \"Trusty Triumph.\"\n\nBy 1920, Harley-Davidson became the largest manufacturer, with their motorcycles being sold by dealers in 67 countries.\nBy the late 1920s or early 1930s, DKW in Germany took over as the largest manufacturer.\nBMW motorcycles came on the scene in 1923 with a shaft drive and an opposed-twin or \"boxer\" engine enclosed with the transmission in a single aluminum housing.\n\nBy 1931, Indian and Harley-Davidson were the only two American manufacturers producing commercial motorcycles.\nThis two-company rivalry in the United States remained until 1953, when the Indian Motorcycle factory in Springfield, Massachusetts closed and Royal Enfield took over the Indian name.\n\nThere were over 80 different makes of motorcycle available in Britain in the 1930s, from the familiar marques like Norton, Triumph and AJS to the obscure, with names like New Gerrard, NUT, SOS, Chell and Whitwood,\nabout twice as many motorcycle makes competing in the world market during the early 21st century.\n\nIn 1937, Joe Petrali set a new land speed record of 136.183 mph (219.165 km/h) on a modified Harley-Davidson 61 cubic inch (1,000 cc) overhead valve-driven motorcycle.\nThe same day, Petrali also broke the speed record for 45 cubic inch (737 cc) engine motorcycles.\n\nIn Europe, production demands, driven by the buildup to World War II, included motorcycles for military use, and BSA supplied 126,000 BSA M20 motorcycles to the British armed forces, starting in 1937 and continuing until 1950. Royal Enfield also produced motorcycles for the military, including a 125 cc lightweight motorcycle that could be dropped (in a parachute-fitted tube cage) from an aircraft.\n\nAfter the World War II, some American veterans found a replacement for the camaraderie, excitement, danger and speed of life at war in motorcycles. Grouped into loosely organized clubs, motorcycle riders in the US created a new social institution—the motorcyclists or \"bikers\"—which was later skewed by the \"outlaw\" persona Marlon Brando portrayed in the 1953 film \"The Wild One\".\n\nIn Europe, on the other hand, post-war motorcycle producers were more concerned with designing practical, economical transportation than the social aspects, or \"biker\" image.\nItalian designer Piaggio introduced the Vespa in 1946, which experienced immediate and widespread popularity. Imports from the UK, Italy and Germany, thus found a niche in US markets that American bikes did not fill.\n\nThe BSA Group purchased Triumph Motorcycles in 1951 to become the largest producer of motorcycles in the world claiming \"one in four\". The German NSU was the largest manufacturer from 1955 until 1959 when Honda became the largest manufacturer.\nBritish manufacturers Triumph, BSA, and Norton retained a dominant position in some markets until the rise of the Japanese manufacturers, led by Honda, in the late 1960s and early 1970s. The role of the motorcycle shifted in the 1960s, from the tool of a life to a toy of a lifestyle. It became part of an image, of status, a cultural icon for individualism, a prop in Hollywood B-movies.\n\nThe motorcycle also became a recreational machine for sport and leisure, a vehicle for carefree youth, not essential transportation for the mature family man or woman, and the Japanese were able to produce modern designs more quickly, more cheaply, and of better quality than their competitors. Their motorbikes were more stylish and more reliable, so the British manufacturers fell behind as mass-market producers.\n\nHonda, which was officially founded in Japan on September 24, 1948, introduced their SOHC inline-four engine CB750 in 1969, which was inexpensive and immediately successful. It established the across-the-frame-four engine configuration as a design with huge potential for power and performance. Shortly after the introduction of the SOHC, Kawasaki demonstrated the potential of the four-stroke four-cylinder engine with the introduction of the KZ900.\n\nSuzuki, Kawasaki and the Yamaha each started producing motorcycles in the 1950s. Meanwhile, the sun was setting on British dominion over the big-displacement motorbike market.\n\nThe excellence of Japanese motorcycles caused similar effects in all Western markets: many Italian bike firms either went bust or only just managed to survive. As a result, BMW's worldwide sales sagged in the 1960s, but came back strongly with the introduction of a completely redesigned \"slash-5\" series for model year 1970.\n\nFrom the 1960s through the 1990s, small two-stroke motorcycles were popular worldwide, partly as a result of the pioneering work of the East German Daniel Zimmermann (rotary disc valve) and MZ's Walter Kaaden who developed the two-stroke expansion chamber in the 1950s. These ideas were taken up by Suzuki when Ernst Degner, the MZ engineer and rider, defected to the West on 13 September 1961 after retiring from the 125cc Swedish Grand Prix at Kristianstad. Degner, an excellent engineer, immediately joined Suzuki and his knowledge became their technology springboard.\n\nHarley-Davidson in the US at the time suffered from the same problems as the European firms, but its unique product range, American tariff laws and nationalism-driven customer loyalty allowed it to survive. One alleged flaw, however, was retaining the characteristic Harley-Davidson 45° engine vee-angle, which causes excess vibration as well as the loping Harley-Davidson sound.\n\nA factory full fairing was introduced by BMW motorcycle in the R100RS of 1977, the first factory fairing produced in quantity. In 1980, BMW stimulated the \"adventure touring\" category of motorcycling with its dual-sport model, the R80G/S. In 1988, BMW was the first motorcycle manufacturer to introduce anti-lock-brakes (ABS) on its sporting K100RS-SE and K1 models.\n\nToday the Japanese manufacturers, Honda, Kawasaki, Suzuki, and Yamaha dominate the large motorcycle industry, although Harley-Davidson still maintains a high degree of popularity, particularly in the United States.\n\nRecent years have seen a resurgence in the popularity around the world of many other motorcycle brands, including BMW, Triumph and Ducati, and the emergence of Victory as a second successful mass-builder of big-twin American cruisers.\n\nIn November 2006, the Dutch company E.V.A. Products BV Holland announced that the first commercially available diesel-powered motorcycle, its Track T-800CDI, achieved production status.\nThe Track T-800CDI uses an 800 cc three-cylinder Daimler Chrysler diesel engine. However, other manufacturers, including Royal Enfield, had been producing diesel-powered bikes since at least 1965.\n\nThere is a large demand for small, cheap motorcycles in the developing world, and many of the firms meeting that demand now also compete in mature markets, such as China's Hongdou which makes a version of Honda's venerable CG125.\n\nMotorcycle taxis are commonplace in the developing world. Scooters, mopeds and motorcycles offer a fast, cheap and risky way around snarled traffic and scarce mass transit, as they can easily squeeze through jams.\n\nThe first ethanol flex fuel motorcycle in the world was launched to the Brazilian market by Honda in March 2009, the CG 150 Titan Mix During the first eight months after its market launch the CG 150 Titan Mix had captured a 10.6% market share, and ranking second in sales of new motorcycles in the Brazilian market in 2009. In September 2009, Honda launched a second flexible-fuel motorcycle, and by December 2010 both Honda flexible-fuel motorcycles had reached cumulative production of 515,726 units, representing an 18.1% market share of the Brazilian new motorcycle sales in that year. As of January 2011 there were four flex-fuel motorcycle models available in the market, and production reached the one million milestone in June 2011.\n\n"}
{"id": "41449526", "url": "https://en.wikipedia.org/wiki?curid=41449526", "title": "Julius Ashkin", "text": "Julius Ashkin\n\nJulius Ashkin (August 23, 1920 – June 4, 1982) was a leader in experimental and theoretical physics known for furthering the evolution of particle physics from nuclear physics. As a theoretical physicist he made contributions in the fields of statistical mechanics, solid state physics, nuclear physics, and elementary particle physics. As an experimental physicist his main contributions concerned the passage of certain particles (pi-mesons, or pions) through solid matter and their subsequent decay. He was recognized for the quality of his research and teaching.\n\nJulius Ashkin was born in Brooklyn, New York, on August 23, 1920. His parents were Isadore and Anna Ashkin. He had two younger siblings, a brother, Arthur, also a physicist, and a sister, Ruth. One older sibling, Gertrude, died while young. The family home was in Brooklyn, New York, at 983 E 27 Street. Isadore had immigrated to the United States from Odessa, Ukraine at the age of 19. Anna, five years younger, also came from the Ukraine (in her case Galicia). Within a decade of his landing in New York, Isadore had become a U.S. citizen and was running a dental laboratory at 139 Delancey Street in Manhattan.\n\nAshkin attended Brooklyn's James Madison High School, graduating in 1936, while still a few weeks shy of his 16th birthday. In his senior year, he received honors and awards. He was awarded a scholarship to attend Columbia University, where he studied four years as an undergraduate from 1936 to 1940, and three as a graduate from 1940 to 1943.\n\nPhysicists then working on Columbia's faculty in those years included professors Enrico Fermi, Isidor Isaac Rabi, Hans Bethe (visiting), Edward Teller (visiting), and instructors Arnold Nordsieck, Hugh Paxton, and Willis Lamb. All these men were recognized as among the finest of their generation and four of them — Fermi, Rabi, Bethe, and Lamb — were to be awarded the Nobel Prize.\n\nAs an undergraduate, Ashkin was invited to join an honorary mathematics society, and received awards. He entered the fall semester as an assistant lecturer and began work toward a master's degree. A year later, having received that degree, he began work toward a Ph.D. under the supervision of Willis Lamb. As a graduate student, Ashkin contributed to one paper in astrophysics. and two papers in statistical mechanics He collaborated with Lamb in writing the first of the two papers on statistical mechanics and with Teller in writing the second. This second paper, \"Statistics of Two-Dimensional Lattices with Four Components\" has since been frequently cited. He received his Ph.D. in Physics in 1943.\n\nDuring the latter part of 1942, before completing his Ph.D. work, Ashkin accepted an offer to work in the Manhattan Project. Early work on the development of the atom bomb had taken place at Columbia during the six years Ashkin was an undergradutate and graduate student there. When the process of nuclear fission was discovered in 1938, scientists in many locations in Europe and the United States began intense work to understand and control the phenomenon. Researchers at Columbia and nearby Princeton University were in the forefront of this work. There's no information on how much, if at all, Ashkin was involved in the effort at this early stage of his career, but it is certain that the Columbia physics department was the workplace for scientists who were devoted to the secret development of a new and phenomenally powerful weapon. These men included men already mentioned — Fermi, Rabi, Teller, and Bethe — as well as Leó Szilárd (who worked with Fermi to demonstrate that a nuclear reaction was possible), Herbert L. Anderson (then, like Ashkin, a Columbia graduate student), John R. Dunning (an associate professor at Columbia who had built a small cyclotron in the mid-1930s), Walter Zinn (a Columbia professor who worked with the Columbia cyclotron to demonstrate the possibility of a sustained chain reaction), George B. Pegram (Dean of Columbia's Faculties of Political Science, Philosophy, and Pure Science, who helped bring Fermi to the United States and brought him together with representatives of the U.S. Navy Department for the first discussion of the atomic bomb) and Harold Urey (associate professor of Chemistry whose work on separation of isotopes resulted in the discovery of deuterium). In addition, Columbia received visits from scientists at Princeton University who were coordinating their work with Columbia colleagues. These included John Wheeler, Edward Creutz, and Robert R. Wilson.\n\nWhen Ashkin accepted an invitation to join the Manhattan Project he was still working on his Ph.D. He spent the last few months of 1942 at the Metallurgical Laboratory at the University of Chicago and then worked at the Los Alamos Laboratory from mid-1943 to mid-1945. The scientists at the Metallurgical Laboratory, or Met Lab as it was called, used a nuclear reactor called the Chicago Pile to produce the world's first controlled chain reaction. They built the reactor in a disused squash court under the bleachers of Stagg Field, the university's old football stadium. They had been brought together from Columbia and Princeton by Arthur Compton who was a professor of physics at the University of Chicago. The Met Lab consisted of two divisions. Fermi, Anderson, Zinn, Creutz, and Szilard were key members of the physics division; Bethe and Teller of the theoretical division.\n\nAlthough Ashkin's interest, experience, and skill would seem to place him with the theoretical division, he worked in the physics division in a group called \"Nuclear Physics — Experimental.\" His placement in this group suggests that Ashkin had prior experience in carrying out nuclear experiments while a student at Columbia. With the other members of the group — Feld, Szilard, Robert F. Christy, Herbert E. Kubitschek, and S. Bernstein (untraced) — Ashkin produced a number of technical reports on the theoretical aspects of nuclear fission. With Feld, he also produced a practical report on \"Poisoning and Production in a Power Plant\" which considered the power potential of sustained nuclear reactions as well as the radiation poisoning and other hazards that accompanied them. All these reports were secret when produced and have since been declassified and released.\n\nAt Los Alamos Ashkin was assigned to work in the Theoretical Division headed by Bethe. There were five groups in this division and Ashkin was assigned to group 4, Diffusion Problems. His responsibilities, broadly, permitted him to build upon work he had done as a graduate student and in his months at Met Lab. T-4's group leader was Richard Feynman; Ashkin was alternate leader. The initial members were Richard Ehrich, and Frederick Reines. joined it in the early spring of 1944. The group's main task was to estimate the rate at which neutrons would diffuse through the explosive core of the bomb during nuclear fission. Somewhat facetiously Feynman later claimed that the work done at Los Alamos was mostly engineering, not science. Welton, however, told of the group's long hard hours, high spirits and cohesiveness, and said they achieved some excellent successes in theoretical physics. Their work required a great number of mathematical computations, which, Welton remembered, they performed using Work Projects Administration math tables and big Marchant mechanical calculators. In fact, their assistant, an enlisted man named Murray Peshkin, remembered the group as having an unending need for calculation. An undergraduate with a major in physics at the time he was recruited, he was put, as he recalled, \"to solving differential equations that were needed to predict the critical mass of a bomb under various assumptions about the many unknown properties of the nuclei in the bomb material.\"\n\nSecurity at Los Alamos was very tight by standards of the time. The site was an uninhabited desert location (formerly a private school) whose perimeter was fenced, with guards at the gates. The scientists were permitted outside the facility but there was little available transportation. (Since they were not members of the military, they could not be ordered to comply with military secrecy orders and instead voluntarily agreed to abide by them.) Although urban-raised scientists, like Ashkin, were far from the amusements that cities afford, they were able to find amusing things to do. It helped that some of the married ones were able to bring their wives to live in the town of Los Alamos and thus parties of men and women could get together for social activities. These include such things as hikes of hills and canyons of the surrounding wilderness areas. It's likely Ashkin played outdoor team sports while at Los Alamos. A Met Lab colleague remembered playing touch football with him and Feld on an open space called the midway at the University of Chicago.\n\nFeynman was particularly adept at leavening hard work with light-hearted games. His genius was as much playful as serious. He took pride in deceiving the mail censors, guessing the combinations of safes in which secret files were stored, picking door locks, and teasing the guards (he would depart from the main gate, circle around the perimeter to a hole in the fence, re-enter the facility, and then exit the gate again, thus causing confusion and consternation both.) He also liked to pound his bongo drums, a practice which made those within hearing range grit their teeth but which he believed put him in touch with the spirits of the Indians who formerly inhabited the place. Physicists are known for their love of music, particularly classical music, and their ability to play it. Feynman wasn't unusual in his affection for drumming but his choice of musical genres was atypical as was his lack of skill as a drummer. It appears that the music produced by his friends offended him as much as his relentless noise offended them. When Ashkin played the recorder, Feynman said he was using \"an infernally popular wooden tube ... for making noises bearing a one-one correspondence to black dots on a piece of paper -- in imitation to music.\"\n\nIn 1946, before the scientists at Los Alamos dispersed, there was a brief period during which they gave lectures on subjects in which they had expertise. The program was styled the \"Los Alamos University\" and some junior members of laboratory personnel received college credit for attending them. Ashkin's were on theoretical mechanics. The course description says it covered the dynamics of particles, rigid bodies, elastic media, and fluids using vector analysis, particle dynamics, Lagrange's equations, and Hamilton's equations.\n\nAlong with other members of the Feynman team, Ashkin produced technical reports while at Los Alamos. These were classified at the time but have since been made public. One example gives an idea of the work carried out by the group. It is \"The Calculation of Critical Masses Including the Effects of the Distribution of Neutron Energies\", by Feynman, R.P.; Welton, T.A.; Ashkin, J.; Ehrlich, R.; Peshkin, M.; and Reines, F. Report LA-524(Del.) January 21, 1947 (extract from abstract: \"Convenient approximate methods are developed for the calculation of critical sizes and multiplication rates of spherical, active cores surrounded by infinite tampers. Special attention is given to those problems arising from the fact that neutrons of different velocities have different properties. The methods consist essentially of approximating the neutron densities at each velocity by fundamental mode shapes for each velocity.\")\n\nOn July 16, 1945 Ashkin was present at the first-ever explosion of a nuclear bomb at the Trinity test site, Alamogordo. Only a few of the many scientists were permitted to witness this unspeakably dramatic culmination of their work. Ashkin was probably there because of his work on radiation poisoning, begun at Met Lab and probably continued afterward.\n\nIn 1950 it was revealed that one of the scientists at Los Alamos, Klaus Fuchs, was providing the Soviet intelligence bureau, NKGB, with secret information about bomb research. Between 1943 and 1946, Fuchs worked at both Columbia and Los Alamos. When the FBI interviewed Bethe and Feynman about their relationship with Fuchs while at Los Alamos, Feynman said Fuchs was quiet, reserved and not inclined to mix with other scientists outside of work. He also said he believed Fuchs was less stand-offish with Ashkin and seemed friendly with him.\n\nOn leaving Los Alamos in 1946 Ashkin obtained a position as assistant professor at the University of Rochester and in 1950 he moved to the Carnegie Institute of Technology (later Carnegie Mellon University). He remained at CMU for the rest of his life, serving as professor and for a period head of the physics department. Robert Marshak brought Ashkin to the University of Rochester in 1946. As an assistant professor he taught mechanics and thermodynamics and theoretical physics and performed pioneering experiments on neutron-proton, proton-proton, and nucleon-nucleon scattering.\n\nWhile at Rochester, Ashkin was the first scientist to formally recognize the importance of the Feynman diagram. Feynman devised the diagram in 1948 to provide a simple visualization of the mathematical expressions governing the behavior of subatomic particles. Although the diagram and its offshoots were later seen as extremely important tools, Feynman did not give them a theoretical framework nor did he explain how he proposed they be used. Physicists had difficulty in understanding their function, distrusted their simplicity, and were reluctant to give them formal recognition. Feynman later said physicists did not realize the diagram's power and would employ a more complex method created by Julian Schwinger. Ashkin, he said, was the first to break with this pattern: \"They somehow or other couldn’t do it. They had to go through this [the Schwinger method] to believe it. But that's all right. The only person who didn't, the first paper where it was used directly — which I kept looking for, I kept flipping through the Physical Review as it came out — was Ashkin. He'd done some calculation for some experiment, and he said, 'We’ve calculated this using Feynman's rules.' Bloop! There it was in writing! Then gradually more and more people did it.\"\n\nWhile he was teaching at the University of Rochester, Ashkin married Claire Ruderman, a biologist studying at the same university. The couple had two daughters, Beth and Laura.\n\nIn 1950 Ashkin joined the physics faculty of Carnegie Mellon University (then the Carnegie Institute of Technology) where Edward Creutz was department head and director of a new 450 MeV proton synchrocyclotron that CIT had built in nearby Saxonburg, Pennsylvania. Joining fellow scientists Lincoln Wolfenstein and Sergio de Benedetti, at this time Ashkin began to transition from mainly theoretical to mainly experimental work. The cyclotron remained in use at the Saxonburg Nuclear Research Center until the mid-1970s when it was dismantled and, using it, Ashkin was able to produce some of his best-known experimental results.\n\nIn 1953, with Bethe, his former director of theoretical work in Los Alamos, Ashkin published an article closely related to the work they had then done. This article, \"Passage of Radiations Through Matter,\" summarized the effects of particles and radiation as they passed through solids. In time it became a standard reference for physics experimenters. Using the CIT cyclotron and following on work done by Bethe and Robert E. Marshak, Ashkin conducted experiments to determine the characteristics of a short-lived particle — the pi-meson or pion — that is produced when high energy cosmic ray protons and other cosmic ray components interact with matter in the Earth's atmosphere. Ashkin served as chair of the physics department between 1961 and 1972. After he died, CMU created the \"Julius Ashkin Teaching Award\" in his honor.\n\nIn 1958–1959 Ashkin won a Ford Foundation grant to spend a sabbatical year in Geneva, Switzerland, at CERN, the European Organization for Nuclear Research. There, he became a member of the first group of scientists to use that institution's new 600 MeV synchrocyclotron. Using this particle accelerator he helped to make a significant discovery which confirmed an aspect of the V-A Theory of weak interactions. Supported by a grant from the Guggenheim Foundation, in 1968 he also spent a year as a Fellow at All Souls College, Oxford.\n\nAshkin died at Montefiore Hospital in Pittsburgh, Pennslylvania on June 4, 1982 after a lengthy illness.\n"}
{"id": "10348099", "url": "https://en.wikipedia.org/wiki?curid=10348099", "title": "Knowledge management software", "text": "Knowledge management software\n\nKnowledge management software (KM software) is a subset of Enterprise content management software, which contains a range of software that specializes in the way information is collected, stored and/or accessed. The concept of knowledge management is based on a range of practices used by an individual, a business, or a large corporation to identify, create, represent and redistribute information for a range of purposes. Software that enables an information practice or range of practices at any part of the processes of information management can be deemed to be called information management software. A subset of information management software that emphasizes an approach to build knowledge out of information that is managed or contained is often called knowledge management software.\n\nKM software in most cases provides a means for individuals, small groups or mid-sized businesses to innovate, build new knowledge in the group, and/or improve customer experience. Knowledge management systems (software) includes a range of about 1,500 or more different approaches to collect and contain information to then build knowledge that can be searched through specialised search tools including concept building tools and or visual search tools that present information in a connected manager not originally conceptualised by those collecting or maintaining the information database.\n\nOne of the main categories of Knowledge Management software is Groupware, which can be used for knowledge sharing and capture. Groupware is a combination of synchronous, asynchronous and community focused tools. Groupware can be used to exchange knowledge and expertise even when the team members are situated around the world. \n\nFeatures of KM software usually include:\nAs business today is becoming increasingly international, the ability to access information in different languages is now a requirement for some organizations. Reported success factors of a KM system include the capability to integrate well with existing internal systems and the scalability of the system to grow within the organization.\n\nKM software ranges from small software packages for an individual to use, such as brainstorming software, to highly specialized enterprise software suitable for use by hundreds of employees. Often KM software provides a key resource for employees working in customer service or telephone support industries, or sectors of large corporations.\n\nKnowledge management software, in general, enables the combination of unstructured information sources, such as individual word processed documents and/or .pdf formats, email, graphic illustrations, unstructured notes, website links, invoices, and other information bearing collections, such as a simple thought, through to a combination of millions of interactions from a website, and through that combination enables the seeker to obtain knowledge that otherwise would not have been discovered. As Internet access speeds increased, many on-demand (or software as a service) products have evolved and are now the leading suppliers of KM software.\n\nOne of the departures from the almost standard keyword search approach are those group of companies developing visual search techniques. Some common visual search approaches include: \n\nNotable knowledge management tools include:\n\n\n"}
{"id": "377436", "url": "https://en.wikipedia.org/wiki?curid=377436", "title": "Lighting technician", "text": "Lighting technician\n\nElectrical lighting technicians (ELT) or simply lighting tech., are involved with rigging stage and location sets and controlling artificial, electric lights for art and entertainment venues (theatre or live music venues) or in video, television, or film production.\n\nIn a theater production, lighting technicians work under the lighting designer and master electrician. In video, television, and film productions, lighting technicians work under the direction of the gaffer or chief lighting technician who takes their direction from the cinematographer. In live music, lighting technicians work under the lighting director. All heads of department report to the production manager.\n\nLighting technicians are responsible for the movement and set up of various pieces of lighting equipment for separation of light and shadow or contrast, depth of field or visual effects. Lighting Technicians may also lay electrical cables, wire fixtures, install color effects or image patterns, focus the lights, and assist in creating effects or programming sequences.\n\nA lighting technician's work concerns safety of rigging and working with objects which can be very heavy and get very hot.\n\nSome local unions such as the International Alliance of Television Stage Employees (IATSE) Hollywood chapter local #728, have been qualifying members by certification and recognition through the Entertainment Stage Technologies Association (ESTA)with their Entertainment Technicians Certification Program (ETCP). Basic skill sets are now standardized, and sets and stage are safer through this program. \n\nOfficially called the electrical lighting technician (ELT), or the rigging electrical lighting technician (RELT), are also called or known as set lighting technicians, lamp operator, electrician, electric, spark or a juicer. \n\nThe lighting technicians on a motion picture set handle all of the electrical needs as well as place and focus all of the lighting under the direction of the gaffer (chief lighting technician). \n\nLighting techs also:\n\n\nAlso specialty duties are called upon the lighting technician such as...\n\n\nHours of work also vary. For example, those employed by large television productions generally work more than 40 hours a week, 60 hours or more are not uncommon. Technicians and other crew members typically work a 12-hour day.\n\nDepending on script requirements, stage and locations bring on their own requirements for lighting and effects. Out of state, or country to get the right look in a script is not uncommon. Location work always brings on its own challenges. Weather is always a factor when going to a location. Technicians are like scouts and have to be prepared for all kinds of weather as per the season.\n\nWorking conditions for lighting technicians vary a great deal from one job to another. Lighting technicians generally spend a lot of time on their feet and the pace of work can become hectic. Last-minute changes are often required and safety precautions must be observed when handling hot lamps, climbing ladders or working on high voltage electrical cables and equipment. Lighting technicians are routinely required to lift and carry the heaviest and more dangerous equipment compared to the other departments and office staff.\n\nThe film set electrical department hierarchy is as follows:\n\n\nIn live music performances, concerts, and other entertainment, stage lighting technicians (also called a lighting tech, lighting operator, stage electrician, \"sparky\", \"lampy\", or \"techie\") set up lighting and make effects for live performances, concerts and any other show/production involving lighting. \n\nDuties include:\n\nThe Stage lighting department hirearchy is as follows:\n\n\n\n"}
{"id": "37220648", "url": "https://en.wikipedia.org/wiki?curid=37220648", "title": "Link Motion Inc", "text": "Link Motion Inc\n\nLink Motion Inc, formerly NQ Mobile, is a multinational technology company that develops, licenses, supports and sells software and services that focus on the smart ride business. Link Motion's portfolio of offerings includes enabling technology solutions and secure connected carputers for car businesses, consumer ride sharing services, as well as legacy mobile security, productivity and other related applications. Link Motion maintains dual headquarters in Dallas, Texas, United States and Beijing, China.\n\nLink Motion was founded as NQ Mobile in 2005 by Dr. Henry Lin, formerly the youngest associate professor at the Beijing University of Posts and Telecommunications, and Dr. Vincent Shi. The company began its business by offering mobile security services and later started offering productivity products to families and enterprise customers. Their services were compatible with a wide range of handset models and almost all currently available operating systems for smartphones, including iOS, Android, Windows Phone and BlackBerry OS. NQ Mobile also collaborated closely with other mobile ecosystem participants, including chipmakers, handset manufacturers, wireless carriers, third party payment channels, retailers and other distribution channels in order to broaden the reach of their services.\n\nNQ Mobile's initial focus was the China marketplace. The company cooperated with China Mobile, China Unicom and China Telecom, the three largest mobile companies in China. NQ Mobile also cooperated with Nokia and Sony to pre-installed NQ products on their companywide mobile phones. NQ Mobile has also worked closely with Symbian, Windows Mobile and Android, developing mobile security applications based on those operating systems. In addition, Samsung, Motorola, Dopod, Lenovo, Tencent, and Baidu have all been the company’s partners.\n\nIn August 2011, Chris Stier was appointed Managing Director for the Americas and became responsible for NQ Mobile's business development throughout the Americas, overseeing sales and marketing operations as well as establishing strategic partnerships with key industry players in the region.\n\nIn October 2011, Geoff Casely was appointed Managing Director for the Europe, Middle East, and Africa (EMEA) region based in London and became responsible for NQ Mobile's business development in EMEA and building strategic partner relationships.\n\nOmar Khan joined the company in January 2012 as co-CEO to direct the Company alongside the current Chairman and Chief Executive Officer Dr. Henry Lin and the company changed its corporate name from NetQin Mobile Inc. to NQ Mobile Inc. Mr. Khan focused on the global expansion of NQ Mobile into markets such as North America, Latin America, Europe, Japan, Korea and India. Dr. Lin continued to focus on the core markets such as China and Taiwan among other developing countries.\n\nDuring the first half of 2012, NQ Mobile expanded its international management with the additions of Gavin Kim as Chief Product Officer, Conrad Edwards as Chief Experience Officer, and Victoria Repice as Senior Director of Product Management.\n\nNQ Mobile expanded its mobile internet services in November 2012 with the acquisition of Feiliu. Feiliu was founded in 2009 and was subsequently rebranded to FL Mobile. It is a leading mobile interest-based community platform with coverage in China that engages users in real-time mobile online activities. FL Mobile provides application recommendation services, interest-based exchanges, and mobile games to its user communities. According to data published by third party marketing research company Sino MR, FL Mobile was the top iOS mobile game publisher and operator in the Chinese market in December 2012. FL Mobile had 87.3 million registered users and 16.1 million monthly active users by the end of June 2013.\n\nEnfoDesk Analysys International (EnfoDesk), a major market tracking company, reported that FL Mobile became the number one publisher on the iOS platform and increased its market share to 36.6 percent in the first half of 2013. The first-place ranking included the top spot for both revenues and number of mobile users. The report also claims FL Mobile ranks third place across all platforms for both revenues and mobile users and maintains 18.8 percent share of total revenues in the first half of the 2013.\n\nNQ Mobile also expanded into enterprise security products and services starting in May 2012 when it acquired 55% of NationSky and the remaining 45% in July 2013. Founded in 2005, NationSky is a leader in providing mobile services to more than 1,250 enterprises in China. By working with carriers and smart phone platform providers, NationSky delivers device agnostic managed mobile services, self developed mobile device management (MDM) software NQSky and other mobile SaaS services. Headquartered in Beijing, NationSky also has offices in Shanghai and Shenzhen.\n\nIn June 2013, NQ Mobile hired Matt Mathison to the senior management position of Vice President, Capital Markets.\n\nIn August 2013, NQ Mobile opened a second global headquarters in Dallas, Texas. The company also further expanded its products and service offerings with the acquisitions of Shanghai Yinlong Information and Technology Co., Ltd. (\"Yinlong\") to develop content-based music information retrieval (MIR) technology based on multi platforms, NQ Mobile (Shenzhen) Co., Ltd. (\"NQ Shenzhen\") to offer online security education and value added services, Best Partners Ltd. (\"Best Partner\") for mobile advertising, Beijing Tianya Co., Ltd. (\"Tianya\") for mobile healthcare applications development and search engine marketing in the healthcare industry in China, Chengdu Ruifeng Technology Co., Ltd. (\"Ruifeng\") to provide enterprise mobility system development and iOS training programs, Tianjin Huayong Wireless Technology Co., Ltd. (\"Huayong \") for research and development and marketing of live wallpapers for smart phones using the Android operating system, and expanded its market with NQ Mobile KK (\"NQ Japan\") in Japan.\n\nIn 2014 NQ Mobile continued expanding through acquisitions with Beijing Trustek Technology Co., Ltd. (\"Trustek\") to provide enterprise mobility solutions and services, including system management, application development, business intelligence and maintenance services, Yipai Tianxia Network Technology Co., Ltd. (\"Yipai\") to provide mobile intelligent interactive advertising services, through integration of media channels of outdoor, newspapers, magazines etc., Beijing Showself Technology Co., Ltd. (\"Showself\") to provide entertainment and dating platforms on mobile internet, and established Beijing NQ Mobile Co., Ltd. (\"NQ Yizhuang\") to engage in software design and development for computer and mobile devices and other technology consulting services. The company also took a controlling stake in Link Motion.\n\nIn May 2015, Mr. Zemin Xu took over as CEO and the company held a press conference in Beijing to announce their new business strategy and reorganized along two lines, a technical division representing mobile security, mobile enterprise and mobile health care, and an entertainment division covering mobile advertising, mobile entertainment and mobile games. During the conference NQ Mobile also announced its new Showself Entertainment brand which includes Showself, Showself Live Wallpaper, Showself Music Radar and Showself Launcher. In June 2015, Mr. Roland Wu was appointed as Chief Financial Officer.\n\nIn August 2015 the company along with the other existing shareholders of FL Mobile Inc. agreed to sell to Beijing Jinxing Rongda Investment Management Co. Ltd., a subsidiary of Tsinghua Holdings Co., Ltd, the entire stake in FL Mobile Inc. that they currently hold for no less than RMB 4 billion (or approximately no less than US $626 million) and also the sale of all of NQ Mobile's interest in Beijing NationSky Network Technology Co., Ltd., to Mr. Hou Shuli, a founder and senior management member of Beijing NationSky, for an aggregate consideration of US $80 million. The company completed the divestment of NationSky for $80 million at the end of 2015.\n\nThroughout 2016 NQ Mobile continued to consolidate and began shifting its core business to smart cars while working on the divestments of FL Mobile and other businesses. On March 30, 2017 the company announced a new agreement to sell FL Mobile for RMB 4 billion along with Beijing Showself for RMB 1.23 million to Tongfang Investment Fund Series SPC, an affiliate of Tsinghua Tongfang. The divestment of FL Mobile and Beijing Showself was completed in December 2017.\n\nIn January 2018, NQ Mobile announced that its board of directors approved a rebranding effort around its new focus as a smart car and smart ride company by change its name from “NQ Mobile Inc.” to “Link Motion Inc.” and its ticker from “NQ” to “LKM.” \n\nIn February 2018, the company hired MZ Group for investor relations and financial communications across all key markets and changed its name to Link Motion Inc. and their ticker to LKM.\n\nIn March 2018, Link Motion Inc. appointed Mr. Duo Tang to executive vice president and the head of the Company's smart ride business.\n\n\nRevenue sources include third-party application referrals from mobile applications, banner ads and intelligent interactive advertising services through user modeling and image recognition technology to search for advertisers’ products and services that are of potential interest.\n\nTrustek offers mobility strategy consulting, architecture design, hardware and software procurement and deployment, mobile device and application management, training, maintenance and other ongoing support services to enterprise customers.\n\nIn October 2005, the company launched its first mobile security product NetQin 1.0.\n\nIn November 2009, The 2009 China Frost & Sullivan Award for Mobile Security Market Leadership of the year was presented to NetQin Tech. Co., Ltd. (NetQin) for its leading market share in China mobile security market, continued commitment and excellence in R&D, and outstanding contribution to the industry.\n\nDeloitte Technology ranked the company 9th in the Fast 50 China Program for its growth of 1687% in the past three years in October, 2010.\n\nIn May 2011, The company announced that its initial public offering of 7,750,000 American depositary shares (\"ADSs\"), each representing five Class A common shares of the Company, was priced at $11.50 per ADS, with a total offering size of US$89.125 million, assuming no exercise of the over-allotment option.\n\nOn May 5, 2011, NQ Mobile started trading on the New York Stock Exchange (NYSE) under the symbol “NQ”.\n\nIn June 2011, NQ Mobile's cloud-based mobile security solution was the top performer among the eight leading mobile security solution providers tested by West Coast Labs, one of the world's leading independent test facilities. The company also received certification from The Anti-Virus Products Testing and Certification Center under The National Computer Virus Emergency Response Center of the People's Republic of China, the only anti-virus products testing organization approved by the Chinese government.\n\nIn July 2011, NQ Mobile reached 100 Million registered users nearly 100% growth since June, 2010 and signed a framework agreement with Telefónica, S.A. (Telefónica) to provide mobile Internet services to the subscribers of Telefónica. Under the agreement, NQ Mobile's mobile internet services will be integrated in Telefónica's and its subsidiary's App Store and in mobile devices distributed by Telefónica and subsidiaries.\n\nIn September 2011, NQ Mobile and Brightstar Corp. signed a global go-to-market agreement to promote adoption of NQ Mobile security solutions. The company also opened the NQ Mobile Security Research Center based in Raleigh, N.C. led by Dr. Xuxian Jiang, who was appointed Chief Scientist.\n\nIn January 2012, NetQin launched its new \"NQ Mobile\" brand, under which it now conducts all of its international business, and announced plans to change the company's corporate name from NetQin Mobile Inc. to NQ Mobile Inc. The company also signed an agreement to pre-install NQ Mobile Security on Motorola Android smartphones in China and released a new version of its antivirus software, Mobile Security V6.0 for Android.\n\nIn February 2012, NQ Mobile integrated the BlueVia payment API from Telefónica, providing a mobile payment option to Telefónica's subscribers.\n\nIn April 2012, NQ Mobile announced that The Cellular Connection (TCC) will offer NQ Mobile Security at more than 800 Verizon Wireless Premium Retail locations across the U.S. Rollout of this program will begin with availability at TCC's nearly 300 corporate stores.\n\nIn May 2012, NQ Mobile visited the NYSE to celebrate the company’s 1-year Anniversary of Listing on the NYSE. In honor of the occasion, Omar Khan and Yu Lin, CO-CEOs of NQ Mobile, rang The Closing Bell. The company also acquired 55% of Beijing NationSky Network Technology, Inc. (\"NationSky\"), a provider of mobile services to enterprises in China and signed a collaboration agreement with A Wireless to offer NQ Mobile Guard in more than 125 Verizon Wireless Premium Retail locations in the US.\n\nIn August 2012, NQ Mobile and MediaTek Inc. reached an agreement regarding NQ Mobile's acquisition of approximately one-third interest in Hesine Technologies International Worldwide Inc. (\"Hesine\"), a wholly owned subsidiary of MediaTek and a premier mobile messaging solution provider. NQ Mobile's Co-founder, Chairman and Co-CEO, Henry Lin joined the Board of Directors of Hesine. The company also announced the launch of NQ Mobile Vault for iPhone.\n\nIn September 2012, NQ Mobile announced the launch of NQ Family Guardian.\n\nIn November 2012, acquired Beijing Feiliu Jiutian Technology Co. (\"Feiliu\") and later rebranded it to FL Mobile. The company also announced that epay Australia, a Division of Euronet Worldwide, Inc. (NASDAQ: EEFT), will offer NQ Mobile Guard in major retail locations across Australia, including Harvey Norman and Allphones, UK retailer Phones 4u will offer NQ Mobile Security at over 600 retail locations across the UK and started collaboration to integrate and offer enterprise endpoint mobile security solutions to Vox Mobile customers globally.\n\nIn December 2012, NQ Mobile announced the launch of a proprietary security check service for HTC's App Store in mainland China.\n\nIn July 2013, NQ Mobile agrees to purchase the remaining 45 percent stake in its subsidiary, NationSky.\nIn September 2013, NQ Mobile announced the release of \"Music Radar,\" a content-based music information retrieval (MIR) application from one of its subsidiaries, Yinlong making the app available in China for both Android and iOS platforms. The app was later renamed Doreso.\n\nIn October 2013, NQ's stock “fell a shattering 47 percent”, followed by lawsuits. The short-seller research firm Muddy Waters LLC alleged that \"at least 72 percent of the company’s revenue in China is fictitious and that its actual market share in China is 1.5 percent instead of 55 percent that it had claimed\". An independent investigation conducted by an independent special committee of its Board of Directors and carried out by its independent counsel Shearman & Sterling LLP and Deloitte & Touche Financial Advisory Services Limited acting as forensic accountants found the companies disclosures were verifiable and investors argued that the company and shareholders should sue Muddy Waters for its unfounded claims.\n\n\nNQ Mobile Security received 4 out of 5 stars when reviewed by PC Advisor.\n\nNQ Mobile Vault received 4 out of 5 star, both from CNet and from PC Magazines.\n\nIn April 2015 an analysis of the NQ Vault product indicated that it only encrypted the first 128 bytes of the data, leaving the rest unencrypted. NQ Mobile responded by saying that the encryption level was \"appropriate\".\n\nIn August 2011, NQ Mobile and MediaTek reached an agreement on mobile security cooperation whereby MediaTek will make NQ Mobile's mobile security service available to the MediaTek's smartphone chipset. The company also signed an agreement with Taiwan Mobile to provide mobile anti-virus services to Taiwan Mobile subscribers in Taiwan.\n\nIn March 2012, eSecuritel, a Brightstar Corporation company, extended its device protection offering to include data protection and privacy solutions from NQ Mobile. The arrangement follows a global distribution agreement reached last year between Brightstar Corp and NQ Mobile.\n\nIn June 2012, NQ Mobile announced an alliance with TDMobility, the joint U.S. venture between Brightstar Corp and Tech Data Corporation. The collaboration will enable TDMobility to bring NQ Enterprise Shield to Tech Data's network of 65,000 Value Added Resellers across the US, serving small, medium, and large businesses. The company also announced the official global launch of NQ Enterprise Shield and scientists from NQ Mobile's Mobile Security Research Center, in collaboration with North Carolina State University disclosed a new way to detect mobile threats without relying on known malware samples and their signatures.\n\nIn October 2012, NQ Mobile announced that its applications, including NQ Mobile Guard, NQ Mobile Vault for Android and NQ Family Guardian, will be offered by GoWireless at more than 350 and Wireless at more than 80 Verizon Wireless Premium Retail locations across the United States. The company also announced an agreement with Caterpillar to bring its mobile security solution to the recently released, Android-based, Cat B10 smartphone.\n"}
{"id": "42643602", "url": "https://en.wikipedia.org/wiki?curid=42643602", "title": "List of countries by mobile banking usage", "text": "List of countries by mobile banking usage\n\nThis is a list of countries by mobile banking usage as measured by the percentage of people who had mobile banking transactions in the previous three months. The data is sourced from Bain, Research Now and Bain along with GMI NPS surveys in 2012.\n\nUnder a broader definition of 'mobile banking', e.g. that provided in Mobile banking, African nations such as Kenya would rank highly. Kenya has 38% of the population as subscribers to M-Pesa as of 2011 \n\n"}
{"id": "201525", "url": "https://en.wikipedia.org/wiki?curid=201525", "title": "Mae Jemison", "text": "Mae Jemison\n\nMae Carol Jemison (born October 17, 1956) is an American engineer, physician and NASA astronaut. She became the first African American woman to travel in space when she went into orbit aboard the Space Shuttle \"Endeavour\" on September 12, 1992. After medical school and a brief general practice, Jemison served in the Peace Corps from 1985 until 1987, when she was selected by NASA to join the astronaut corps. She resigned from NASA in 1993 to found a company researching the application of technology to daily life. She has appeared on television several times, including as an actress in an episode of \"\". She is a dancer and holds nine honorary doctorates in science, engineering, letters, and the humanities. She is the current principal of the 100 Year Starship organization.\n\nMae Carol Jemison was born in Decatur, Alabama, on October 17, 1956, the youngest child of Charlie Jemison and Dorothy Green. Her father was a maintenance supervisor for a charity organization, and her mother worked most of her career as an elementary school teacher of English and math at the Beethoven School in Chicago.\n\nThe family moved to Chicago, Illinois, when Jemison was three years old, to take advantage of the better educational and employment opportunities there. Jemison says that as a young girl growing up in Chicago she always assumed she would get into space. \"I thought, by now, we'd be going into space like you were going to work.\" She said it was easier to apply to be a shuttle astronaut, \"rather than waiting around in a cornfield, waiting for ET to pick me up or something.\"\nIn her childhood, Jemison learned to make connections to science by studying nature. Once when a splinter infected her thumb as a little girl, Jemison's mother turned it into a learning experience. She ended up doing a whole project about pus. Jemison's parents were very supportive of her interest in science, while her teachers were not. \"In kindergarten, my teacher asked me what I wanted to be when I grew up, and I told her a scientist,\" Jemison says. \"She said, 'Don't you mean a nurse?' Now, there's nothing wrong with being a nurse, but that's not what I wanted to be.\" In an interview with Makers, she further explains how her sheer interest in science was not accepted. \"Growing up...I was just like every other kid. I loved space, stars and dinosaurs. I always knew I wanted to explore. At the time of the Apollo airing, everybody was thrilled about space, but I remember being irritated that there were no women astronauts. People tried to explain that to me, and I did not buy it.\"\n\nJemison says she was inspired by Martin Luther King Jr.; to her King's dream was not an elusive fantasy but a call to action. \"Too often people paint him like Santa – smiley and inoffensive,\" says Jemison. \"But when I think of Martin Luther King, I think of attitude, audacity, and bravery.\" Jemison thinks the civil rights movement was all about breaking down the barriers to human potential. \"The best way to make dreams come true is to wake up.\"\n\nJemison began dancing at the age of 11. \"I love dancing! I took all kinds of dance — African dancing, ballet, jazz, modern — even Japanese dancing. I wanted to become a professional dancer,\" said Jemison. At the age of 14, she auditioned for the leading role of \"Maria\" in \"West Side Story.\" She did not get the part but Jemison's dancing skills did get her into the line up as a background dancer. \"I had a problem with the singing but I danced and acted pretty well enough for them to choose me. I think that people sometimes limit themselves and so rob themselves of the opportunity to realise their dreams. For me, I love the sciences and I also love the arts,\" says Jemison. \"I saw the theatre as an outlet for this passion and so I decided to pursue this dream.\" Later during her senior year in college, she was trying to decide whether to go to New York to medical school or become a professional dancer. Her mother told her, \"You can always dance if you're a doctor, but you can't doctor if you're a dancer.\"\n\nJemison graduated from Chicago's Morgan Park High School in 1973 and entered Stanford University at the age of 16. \"I was naive and stubborn enough that it didn’t faze me,\" Jemison said. \"It’s not until recently that I realized that 16 was particularly young or that there were even any issues associated with my parents having enough confidence in me to [allow me to] go that far away from home.\" Jemison graduated from Stanford in 1977, receiving a B.S. degree in chemical engineering and fulfilling the requirements for a B.A. degree in African and Afro-American Studies. At Stanford, she choreographed a musical and dance production called \"Out of the Shadows\". She took initiative to get even further involved in the black community by serving as head of the Black Students Union in college. Jemison said that majoring in engineering as a black woman was difficult because race was always an issue in the United States. \"Some professors would just pretend I wasn't there. I would ask a question and a professor would act as if it was just so dumb, the dumbest question he had ever heard. Then, when a white guy would ask the same question, the professor would say, 'That's a very astute observation.'\" In an interview with the \"Des Moines Register\" in 2008 Jemison said that it was difficult to go to Stanford at 16, but thinks her youthful arrogance may have helped her. \"I did have to say, 'I'm going to do this and I don't give a crap (damn).'\" She points out the unfairness of the necessity for women and minorities to have that attitude in some fields.\n\nJemison obtained her M.D. degree in 1981 at Cornell Medical College. She interned at Los Angeles County-USC Medical Center, and in 1982, she worked as a general practitioner. During medical school, Jemison traveled to Cuba, Kenya and Thailand, to provide primary medical care to people living there. During her years at Cornell Medical College, Jemison took lessons in modern dance at the Alvin Ailey school. Jemison later built a dance studio in her home and has choreographed and produced several shows of modern jazz and African dance.\n\nAfter completing her medical training, Jemison joined the staff of the Peace Corps and served as a Peace Corps Medical Officer from 1983 to 1985 responsible for the health of Peace Corps Volunteers serving in Liberia and Sierra Leone. Jemison's work in the Peace Corps included supervising the pharmacy, laboratory, medical staff as well as providing medical care, writing self-care manuals, and developing and implementing guidelines for health and safety issues. Jemison also worked with the Center for Disease Control (CDC) helping with research for various vaccines.\n\nOnce while serving as a doctor for the Peace Corps, a volunteer became seriously ill, and a doctor diagnosed malaria. The volunteer's condition progressively worsened, and Jemison was sure it was meningitis with life-threatening complications that could not be treated in Sierra Leone. Jemison called for an Air Force hospital plane based in Germany for a military medical evacuation at a cost of $80,000. The embassy questioned whether Jemison had the authority to give such an order, but she told them she did not need anyone's permission for a medical decision. By the time the plane reached Germany with Jemison and the volunteer on board, she had been up with the patient for 56 hours. The patient survived.\n\nAfter the flight of Sally Ride in 1983, Jemison felt the astronaut program had opened up, so she applied. Jemison's inspiration for joining NASA was African-American actress Nichelle Nichols, who portrayed Lieutenant Uhura on \"\". Jemison's involvement with NASA was delayed after the Space Shuttle Challenger disaster in 1986, but after reapplying in 1987, she received the news of her acceptance into the astronaut program. \"I got a call saying 'Are you still interested?' and I said 'Yeah',\" recalls Jemison, as one of fifteen candidates chosen out of roughly 2,000 applicants.\nHer work with NASA before her shuttle launch included launch support activities at the Kennedy Space Center in Florida and verification of Shuttle computer software in the Shuttle Avionics Integration Laboratory (SAIL). \"I did things like help to support the launch of vehicles at Kennedy Space Center,\" said Jemison. \"I was in the first class of astronauts selected after the Challenger accident back in 1986, ... [I] actually worked the launch of the first flight after the Challenger accident.\n\nJemison flew her only space mission from September 12 to 20, 1992, as a Mission Specialist on STS-47, a cooperative mission between the United States and Japan, as well as the 50th shuttle mission. Jemison was a co-investigator of two bone cell research experiments, one of 43 investigations that were done on STS-47. Jemison also conducted experiments on weightlessness and motion sickness on herself and six other crew members. \"The first thing I saw from space was Chicago, my hometown,\" said Jemison. \"I was working on the middeck where there aren't many windows, and as we passed over Chicago, the commander called me up to the flight deck. It was such a significant moment because since I was a little girl I had always assumed I would go into space,\" Jemison added. Despite NASA's rigid protocol, Jemison would begin each shift with a salute that only a Trekkie could appreciate. \"Hailing frequencies open,\" she could be heard repeating throughout the eight-day mission.\n\nBecause of her love of dance and as a salute to creativity, Jemison took a poster from the Alvin Ailey American Dance Theater along with her on the flight. \"Many people do not see a connection between science and dance,\" says Jemison. \"but I consider them both to be expressions of the boundless creativity that people have to share with one another.\" Jemison also took several small art objects from West African countries to symbolize that space belongs to all nations. Also on this flight, according to Bessie Coleman biographer Doris L. Rich, Jemison also took into orbit a photo of Coleman — Coleman was the very first African-American woman to ever fly an airplane.\n\nSTS-47 was a cooperative mission between the United States and Japan that included 44 Japanese and United States life science and materials processing experiments. Jemison logged 190 hours, 30 minutes, 23 seconds in space. One of the experiments she supervised on the mission was to induce female frogs to ovulate, fertilize the eggs and then see how tadpoles developed in zero gravity.\n\nJemison resigned from NASA in March 1993. \"I left NASA because I'm very interested in how social sciences interact with technologies,\" Jemison said. \"People always think of technology as something having silicon in it. But a pencil is technology. Any language is technology. Technology is a tool we use to accomplish a particular task and when one talks about appropriate technology in developing countries, appropriate may mean anything from fire to solar electricity.\" NASA training manager and author Homer Hickam later expressed some regret that she had departed, saying, \"NASA had spent a lot of money training her; she also filled a niche, obviously, being a woman of color.\" Hickam had trained Jemison for her flight on Spacelab-J/STS-47. In an interview with the \"Des Moines Register\" on October 16, 2008, Jemison said that she was not driven to be the \"first black woman to go into space.\" \"I wouldn't have cared less if 2,000 people had gone up before me... I would still have had my hand up, 'I want to do this.'\"\n\nJemison is a Professor-at-Large at Cornell University and was a professor of Environmental Studies at Dartmouth College from 1995 to 2002. Jemison continues to advocate strongly in favor of science education and getting minority students interested in science. She sees science and technology as being very much a part of society, and African-Americans as having been deeply involved in U.S. science and technology from the beginning. She has been a member of various scientific organizations, such as the American Medical Association, the American Chemical Society, the Association for Space Explorers and the American Association for the Advancement of Science. Additionally, she served on the board of directors of the World Sickle Cell Foundation from 1990 to 1992.\n\nIn 1993 Jemison founded her own company, the Jemison Group that researches, markets, and develops science and technology for daily life. \nJemison founded the Dorothy Jemison Foundation for Excellence and named the foundation in honor of her mother. \"My parents were the best scientists I knew,\" Jemison said, \"because they were always asking questions.\" One of the projects of Jemison's foundation is \"The Earth We Share\" (TEWS), an international science camp where students, ages 12 to 16, work to solve current global problems, like \"How Many People Can the Earth Hold\" and \"Predict the Hot Public Stocks of The Year 2030.\" The four-week residential program helps students build critical thinking and problem solving skills through an experiential curriculum. Camps have been held at Dartmouth College, Colorado School of Mines, Choate Rosemary Hall and other sites around the United States. TEWS was introduced internationally to high school students in day programs in South Africa and Tunisia. In 1999, TEWS was expanded overseas to adults at the Zermatt Creativity and Leadership Symposium held in Switzerland.\n\nIn 1999, Jemison founded BioSentient Corp and has been working to develop a portable device that allows mobile monitoring of the involuntary nervous system. BioSentient has obtained the license to commercialize NASA's space-age technology known as Autogenic Feedback Training Exercise (AFTE), a patented technique that uses biofeedback and autogenic therapy to allow patients to monitor and control their physiology as a possible treatment for anxiety and stress-related disorders. BioSentient is examining AFTE as a treatment for anxiety, nausea, migraine and tension headaches, chronic pain, hypertension and hypotension, and stress-related disorders.\"\n\nIn 2012, Jemison made the winning bid for the DARPA 100 Year Starship project through the Dorothy Jemison Foundation for Excellence. The Dorothy Jemison Foundation for Excellence was awarded a $500,000 grant for further work. The new organization maintained the organizational name 100 Year Starship. Jemison is the current principal of the 100 Year Starship.\n\nIn 2018, she collaborated with \"Bayer\" and \"National 4-H Council\" for the initiative called \"Science Matters\" which was aimed at encouraging young children to understand and pursue agricultural sciences.\n\nJemison's first book, \"Find Where the Wind Goes\" (2001), is a memoir of her life written for children. She describes her childhood, her time at Stanford, in the Peace Corps and as an astronaut. \"School Library Journal\" found the stories about her earlier life to be the most appealing. \"Book Report\" found that the \"explanation of her encounters with biased professors and their treatment of her on the basis of stereotypes, rather than intelligence, are presented very realistically.\"\n\nHer \"True Book Series,\" published in 2013, is co-authored with Dana Meachen Rau. Each book in the series has a \"Find the Truth\" challenge, in which the answer is revealed at the end of the story. \"School Library Journal\" found the series to be \"properly tantalizing surveys of our local stellar neighborhood and its ongoing exploration.\"\n\nIn 1993, Jemison appeared as Lieutenant Palmer in \"\", an episode of the science fiction television series \"\", earning her the distinction of being the first real-life astronaut to appear on \"Star Trek\". Her appearance came about when LeVar Burton learned from a friend that Jemison was an avid \"Star Trek\" fan. Burton asked her if she would be interested in being on the show, to which she responded, \"Yeah!!\" Jemison had been inspired by the character of Uhura on \"Star Trek\" and has been a lifelong fan of the show.\n\nJemison has also appeared as host and technical consultant of the Discovery Channel science series \"World of Wonder\".\n\nIn 2006, Jemison participated in \"African American Lives\", a PBS television miniseries hosted by Henry Louis Gates, Jr., that traces the family history of eight famous African Americans using historical research and genetic techniques. Jemison found to her surprise that she is 13% East Asian in her genetic makeup.\n\nFebruary 2, 2013, Jemison appeared as the \"Not My Job\" guest on NPR's \"Wait Wait Don't Tell Me\", answering questions about airport shuttles.\n\nJemison is an active public speaker who appears before private and public groups promoting science and technology as well as providing an inspirational and educational message for young people. \"Having been an astronaut gives me a platform,\" says Jemison,\"but I'd blow it if I just talked about the Shuttle.\" Jemison uses her platform to speak out on the gap in the quality of health-care between the United States and the Third World. \"Martin Luther King [Jr.] … didn't just have a dream, he got things done.\"\n\nJemison sometimes appears at charity events. In 2007, Jemison walked the runway, wearing Lyn Devon, at the Red Dress Heart Truth fashion show during Fashion Week in New York to help raise money to fight heart disease. Also in 2007, in May, Jemison was the graduation commencement speaker and only the 11th person in the 52-year history of Harvey Mudd College being awarded an honorary D.Eng. degree.\n\nOn February 17, 2008, Jemison was the featured speaker for the 100th anniversary of the founding of the Alpha Kappa Alpha Sorority. Jemison paid tribute to Alpha Kappa Alpha by carrying the sorority's banner with her on her shuttle flight. Jemison's space suit is a part of the sorority's national traveling Centennial Exhibit. Jemison is an honorary member of Alpha Kappa Alpha, a sorority founded in 1908 at Howard University to address the social issues of the time and promote scholarship among black women. \"The Des Moines Register\" interviewed Jemison on October 16, 2008 and reported that she has mixed feelings about the term \"role model\". \"Here's the deal: Everybody's a role model... Role models can be good or bad, positive or negative.\"\n\nJemison participated with First Lady, Michelle Obama, in a forum for promising girls in the Washington, D.C. public schools in March 2009. In 2014, Jemison also appeared at Wayne State University for their annual Dr. Martin Luther King, Jr. Tribute Luncheon. In 2016, she partnered with Bayer Corporation to promote and advance science literacy in schools, emphasizing hands-on experimentation.\n\nShe took part in the Michigan State University's lecture series, \"Slavery to Freedom: An American Odyssey,\" in February 2017. In May 2017, Jemison gave the commencement speech at Rice University. She discussed the 100 Year Plan, science and education and other topics at Western Michigan University also in May 2017.\n\nIn the spring of 1996, Jemison filed a complaint against a Texas police officer, accusing him of police brutality during a traffic stop that ended in her arrest. She was pulled over by Nassau Bay, Texas officer Henry Hughes for allegedly making an illegal U-turn and arrested after Hughes learned of an outstanding warrant on Jemison for a speeding ticket. In the process of arresting her, the officer twisted her wrist and forced her to the ground. In her complaint, Jemison said the officer physically and emotionally mistreated her. Jemison's attorney said she believed she'd already paid the speeding ticket years ago. She spent several hours in jail and was treated at an area hospital after release for deep bruises and a head injury. Jemison said in a televised interview that the incident has altered her feelings about police there. \"I always felt safe and comfortable [around the police]. I don't feel that way anymore at Nassau Bay and that's a shame,\" she said. Jemison filed a lawsuit against the city of Nassau Bay and officer Hughes.\n\nIn 2007, diagnostic test provider Gen-Probe Inc. announced that they would not accept the resignation of Jemison from their board of directors. Jemison had failed to be re-elected to the board in a vote of the shareholders of the company at the company's May 31 annual stockholders meeting. The company said it believed that Jemison's failed re-election was the result of a recommendation by advisory firm Institutional Shareholder Services that shareholders vote against her due to her poor attendance at board meetings. Gen-Probe determined that Jemison's two absences in 2006 were for valid reasons and said Jemison had attended all regular and special board and committee meetings since September.\n\nIn 2017, a \"Women of NASA\" LEGO set went on sale featuring (among other things) mini-figurines of Jemison, Margaret Hamilton, Sally Ride, and Nancy Grace Roman.\n\n\n\n\n\n\n\n\n"}
{"id": "1646911", "url": "https://en.wikipedia.org/wiki?curid=1646911", "title": "Mannitol hexanitrate", "text": "Mannitol hexanitrate\n\nMannitol hexanitrate is a powerful explosive. Physically, it is a powdery solid at normal temperature ranges, with density of 1.73 g/cm. The chemical name is hexanitromannitol and it is also known as nitromannite, MHN, and nitromannitol, and by the trademarks Nitranitol and Mannitrin. It is more stable than nitroglycerin, and it is used in detonators.\n\nMannitol hexanitrate is a secondary explosive formed by the nitration of mannitol, a sugar alcohol. The product is used in medicine as a vasodilator and as an explosive in blasting caps. Its sensitivity is high, particularly at high temperatures (> 75 °C) where it is slightly more sensitive than nitroglycerine.Nitromannite is a class B explosive.\n\nThe production of pure MHN is not a trivial task, since most preparations will yield a mixture of MHN and lower esters (pentanitrate and lower).\n\n\n\n"}
{"id": "45406823", "url": "https://en.wikipedia.org/wiki?curid=45406823", "title": "Ministry of Information (Bangladesh)", "text": "Ministry of Information (Bangladesh)\n\nThe Ministry of Information () (abbreviated as MoI) is a branch of the Government of Bangladesh is the apex body for formulation and administration of the rules and regulations and laws relating to information, broadcasting, the press and films in Bangladesh. It is responsible to release government information, media galleries, public domain and government unclassified non-scientific data to the public and international communities.\n\nThe Ministry is responsible for all the press and broadcasting arm of the Bangladesh Government. The Bangladesh Film Censor Board is the other important body under this ministry being responsible for the regulation of motion pictures shown in Bangladesh. The Minister of Information is Hasanul Haq Inu as of 2015.\n\nThere are 14 agencies and departments under the Ministry of Information of Bangladesh. They are as follows:\n\n\n\n"}
{"id": "8510003", "url": "https://en.wikipedia.org/wiki?curid=8510003", "title": "New Media Life", "text": "New Media Life\n\nNew Media Life, headquartered in Long Beach, California, is a designer and manufacturer of new media and information technology products and services for the consumer markets and service partners.\n\nThe company is also located in Seoul, South Korea and New Canaan, Connecticut.\n\nTAVI - 20 or 30 GB w/ MP3, MPG, FM Music, Video, Photos, Voice Recording, Notes, and settings.\n\n"}
{"id": "209424", "url": "https://en.wikipedia.org/wiki?curid=209424", "title": "Newly industrialized country", "text": "Newly industrialized country\n\nThe category of newly-industrialized country (NIC) is a socioeconomic classification applied to several countries around the world by political scientists and economists.\n\nNICs are countries whose economies have not yet reached a developed country's status but have, in a macroeconomic sense, outpaced their developing counterparts. Such countries are still considered developing nations and only differ from other developing nations in the rate at which an NIC's growth is much higher over a shorter allotted time period compared to other developing nations. Another characterization of NICs is that of countries undergoing rapid economic growth (usually export-oriented). Incipient or ongoing industrialization is an important indicator of an NIC. In many NICs, social upheaval can occur as primarily rural, or agricultural, populations migrate to the cities, where the growth of manufacturing concerns and factories can draw many thousands of laborers. NIC's introduce many new immigrants looking to improve their social and or political status thorough newly formed democracy's and increase in wages that most individuals who partake in such changes would obtain.\n\nNewly industrialized countries can bring about an increase of stabilization in a country's social and economic status, allowing the people living in these nations to begin to experience better living conditions and better lifestyles. Another characteristic that appears in newly industrialized countries is the further development in government structures, such as democracy, the rule of law, and less corruption. Other such examples of a better lifestyle people living in such countries can experience are better transportation, electricity, and better access to water, compared to other developing countries.\n\nThe term came into use around 1970, when the Four Asian Tigers of Hong Kong, Singapore, South Korea, and Taiwan rose to global prominence as NICs in the 1970s and 1980s, with exceptionally fast industrial growth since the 1960s; all four economies have since graduated into advanced economies and high-income economies. There is a clear distinction between these countries and the countries now considered NICs. In particular, the combination of an open political process, high GNI per capita, and a thriving, export-oriented economic policy has shown that these countries have now not only reached but surpassed the ranks of many developed countries.\n\nAll four economies are classified as high-income economies by the World Bank and Advanced economies by the International Monetary Fund (IMF) and U.S. Central Intelligence Agency (CIA). All of them, like Western European countries, have a Human Development Index considered \"very high\" by the UN.\n\nThe table below presents the list of countries consistently considered NICs by different authors and experts. Turkey and South Africa are classified as developed countries by the CIA. Turkey was a founding member of the OECD in 1961 and Mexico joined in 1994. The G8+5 group is composed of the original G8 members in addition to China, India, Mexico, South Africa and Brazil.\n\nFor China and India, the immense population of these two countries (each with over 1.2 billion people as of September 2015) means that per capita income will remain low even if either economy surpasses that of the United States in overall GDP. When GDP per capita is calculated according to purchasing power parity (PPP), this takes into account the lower costs of living in each newly industrialized country. GDP per capita typically is an indicator for living standards in a given country as well.\n\nBrazil, China, India, Mexico and South Africa meet annually with the G8 countries to discuss financial topics and climate change, due to their economic importance in today's global market and environmental impact, in a group known as G8+5. This group is expected to expand to G14 by adding Egypt alongside the five forementioned countries.\n\nAuthors set lists of countries accordingly to different methods of economic analysis. Sometimes a work ascribes NIC status to a country that other authors don't consider a NIC. This is the case of countries such as Argentina, Chile, Egypt, Sri Lanka and Russia.\n\nNICs usually benefit from comparatively low wage costs, which translates into lower input prices for suppliers. As a result, it is often easier for producers in NICs to outperform and outproduce factories in developed countries, where the cost of living is higher, and trade unions and other organizations have more political sway. This comparative advantage is often criticized by advocates of the fair trade movement.\n\nCritics of NICs argue economic freedom is not always associated with political freedom in countries such as China, pointing out that Internet censorship and human rights violations are common. The case is diametrically opposite for India; While being a liberal democracy throughout after its independence , India has been widely criticized for its inefficient, bureaucratic governance and slow process of structural reform. Thus, while political freedom in China remains limited, the average Chinese citizen enjoys a higher standard of living than his or her counterpart in India.\n\nSouth Africa faces an influx of immigrants from countries such as Zimbabwe, although many also come from Burundi, Democratic Republic of the Congo, Rwanda, Eritrea, Ethiopia and Somalia. While South Africa is considered wealthy on a wealth-per-capita basis, economic inequality is persistent and extreme poverty remains high in the region.\n\nMexico's economic growth is hampered in some areas by an ongoing drug war in the north of the country.\n\nOther NICs face common problems such as widespread corruption and/or political instability as well as other circumstances that cause them to face the \"middle income trap\".\n\n\n\n"}
{"id": "39805210", "url": "https://en.wikipedia.org/wiki?curid=39805210", "title": "Nissan NAPS", "text": "Nissan NAPS\n\nNAPS stands for Nissan Anti Pollution System a moniker used in Japan to identify vehicles built with emission control technology. The technology was installed so that their vehicles would be in compliance with Japanese Government emission regulations passed in 1968. The term was first introduced in Japan, with an externally mounted badge on the trunk of vehicles equipped. Nissan's implementation began with the Y44E V8 engine installed in the Nissan President along with all vehicles installed with the Nissan L engine and the Nissan A engine in 1975. The initial introduction of Nissan's technology was the installation of an exhaust gas recirculation valve, followed with the addition of a catalytic converter and an air pump that added oxygen into the exhaust to promote higher temperatures in the catalytic converter, thus cleaning the exhaust further.\n\nThe NAPS-Z technology, introduced in 1978, was developed with assistance from Hitachi. It uses a novel implementation of two spark plugs per cylinder, called dual ignition together with electronically controlled fuel injection, installed on the Nissan Z engine. This terminology is often confused with the engine installed in the Nissan 280Z, which used the \"L\" engine. However, the NAPS technology was installed in the Fairlady/Z and sold internationally.\n\nAnother version of the twin spark plug method was introduced on the Nissan Stanza with the CA engine, called NAPS-X, which eventually replaced the NAPS-Z approach. The CA engine implemented a hemispherical cylinder head, an approach used by several auto makers.\n\nAll links listed are in Japanese.\n"}
{"id": "34764304", "url": "https://en.wikipedia.org/wiki?curid=34764304", "title": "Nordeca", "text": "Nordeca\n\nNordeca (formerly Ugland IT Group), headquartered in Lysaker, near Oslo, Norway, is one of the leading geographic information services companies in the Nordic area.\n\nThe company is one of the largest providers of geographical data in Norway. Its products include the geographic information service portfolio Insight. It is a market leader in the area of leisure maps for hiking and boating, publishing the Turkartserien and Norge-serien topographic maps, the Båtsportserien and Vannsportserien nautical charts, and distributes the digital products, maps and land ownership information of the Norwegian Mapping and Cadastre Authority, and also products of the Swedish Mapping Authority.\n\nNordeca has approximately 30 employees; in 2010 the company closed a subsidiary office in Ringerike that had once employed approximately 40 people, because map-making had become less labor-intensive with increasing computerization. The company has also discontinued some digital maps once produced by the Norwegian Mapping Authority as insufficiently profitable.\n\nThe company was founded as Totalkart AS in Trondheim in 1990. In 1995 it was acquired by Knut Axel Ugland Holding AS and became Ugland Totalkart AS. In 1997, Minister of Trade and Industry Lars Sponheim opened the first Internet-based map service in Norway, MapOnWeb; in 1999, Ugland Totalkart AS changed its name to Maponweb AS in association with developing a range of internet-based map applications. In 2002, Maponweb AS merged with FlexIm Infowiz AS and Ugland Publikit AS to form Ugland IT Group.\n\nIn 2004, Ugland IT Group acquired the commercial division of the Norwegian Mapping Agency, \"Statens Kartver Marked\", and became a market leader in printed leisure maps in Norway.\n\nIn 2005, Ugland IT Group acquired a rival Norwegian geographic information provider, Kartbutikken.no. In 2007, Ugland Holding reduced its position in the company and Cinclus Equity Partner became the majority shareholder.\n\nOn February 1, 2011, Ugland IT Group changed its name to Nordeca.\n\n"}
{"id": "9947225", "url": "https://en.wikipedia.org/wiki?curid=9947225", "title": "Pan evaporation", "text": "Pan evaporation\n\nPan evaporation is a measurement that combines or integrates the effects of several climate elements: temperature, humidity, rain fall, drought dispersion, solar radiation, and wind. Evaporation is greatest on hot, windy, dry, sunny days; and is greatly reduced when clouds block the sun and when air is cool, calm, and humid. Pan evaporation measurements enable farmers and ranchers to understand how much water their crops will need.\n\nAn evaporation pan is used to hold water during observations for the determination of the quantity of evaporation at a given location. Such pans are of varying sizes and shapes, the most commonly used being circular or square. The best known of the pans are the \"Class A\" evaporation pan and the \"Sunken Colorado Pan\". In Europe, India and South Africa, a Symon's Pan (or sometimes Symon's Tank) is used. Often the evaporation pans are automated with water level sensors and a small weather station is located nearby.\n\nA variety of evaporation pans are used throughout the world. There are formulas for converting from one type of pan to another and to measures representative of the environment. Also, research has been done about the installation practices of evaporation pans so that they can make more reliable and repeatable measurements.\n\nIn the United States, the National Weather Service has standardized its measurements on the Class A evaporation pan, a cylinder with a diameter of 47.5 in (120.7 cm) that has a depth of 10 in (25 cm). The pan rests on a carefully leveled, wooden base and is often enclosed by a chain link fence to prevent animals drinking from it. Evaporation is measured daily as the depth of water (in inches) evaporates from the pan. The measurement day begins with the pan filled to exactly two inches (5 cm) from the pan top. At the end of 24 hours, the amount of water to refill the pan to exactly two inches from its top is measured.\n\nIf precipitation occurs in the 24-hour period, it is taken into account in calculating the evaporation. Sometimes precipitation is greater than evaporation, and measured increments of water must be dipped from the pan. Evaporation cannot be measured in a Class A pan when the pan's water surface is frozen.\n\nThe Class A Evaporation Pan is of limited use on days with rainfall events of >30mm (203mm rain gauge) unless it is emptied more than once per 24hours. Analysis of the daily rainfall and evaporation readings in areas with regular heavy rainfall events shows that almost without fail, on days with rainfall in excess of 30mm (203mm Rain Gauge) the daily evaporation is spuriously higher than other days in the same month where conditions more receptive to evaporation prevailed.\n\nThe most common and obvious error is in daily rainfall events of >55mm (203mm rain gauge) where the Class A Evaporation pan will likely overflow.\n\nThe less obvious, and therefore more concerning, is the influence of heavy or intense rainfall causing spuriously high daily evaporation totals without obvious overflow.\n\nThe sunken Colorado pan is square, 1 m (3 ft) on a side and 0.5 m (18 in.) deep and made of unpainted galvanized iron. As the name suggests, it is buried in the ground to within about 5 cm (2 in.) of its rim. Evaporation from a Sunken Colorado Pan can be compared with a Class A pan using conversion constants. The pan coefficient, on an annual basis, is about 0.8.\nOver the last 50 or so years, pan evaporation has been carefully monitored. For decades, pan evaporation measurements were not analyzed critically for long term trends. But in the 1990s scientists reported that the rate of evaporation was falling. According to data, the downward trend had been observed all over the world except in a few places where it has increased.\n\nIt is currently theorized that, all other things being equal, as the global climate warms evaporation would increase proportionately and as a result, the hydrological cycle in its most general sense is bound to accelerate. The downward trend of pan evaporation has since also been linked to a phenomenon called global dimming. In 2005 Wild et al. and Pinker et al. found that the \"dimming\" trend had reversed since about 1990 \n\nOther theories suggest that measurements have not taken the local environment into account. Since the local moisture level has increased in the local terrain, less water evaporates from the pan. This leads to false measurements and must be compensated for in the data analysis. Models accounting for additional local terrain moisture match global estimates. \n\nPan evaporation is used to estimate the evaporation from lakes. There is a correlation between lake evaporation and pan evaporation. Evaporation from a natural body of water is usually at a lower rate because the body of water does not have metal sides that get hot with the sun, and while light penetration in a pan is essentially uniform, light penetration in natural bodies of water will decrease as depth increases. Most textbooks suggest multiplying the pan evaporation by 0.75 to correct for this.[cite?]\n\n\"It is generally agreed that the evaporation from pans has been decreasing for the past half century over many regions of the Earth. However, the significance of this negative trend, as regards terrestrial evaporation, is still somewhat controversial, and its implications for the global hydrologic cycle remain unclear. The controversy stems from the alternative views that these evaporative changes resulted, either from global radiative dimming, or from the complementary relationship between pan and terrestrial evaporation. Actually, these factors are not mutually exclusive but act concurrently.\"\n\n\n"}
{"id": "7311344", "url": "https://en.wikipedia.org/wiki?curid=7311344", "title": "Passenger information system", "text": "Passenger information system\n\nA passenger information [display] system (PIS or PIDS) is an automated system for supplying users of public transport with information about the nature and state of a public transport service, through visual, voice or other media. They are also known as Customer Information Systems and Operational Information Systems. Among the information provided by such systems, a distinction can be drawn between:\n\n\nStatic information has traditionally been made available in printed form though route network maps and timetable booklets at transit stations. However most transit operators now also use integrated passenger information systems providing either schedule-based information through a journey planner application or schedule-based information in combination with real-time information. \n\nReal time information is an advance on schedule-only information, which recognises the fact that public transport services do not always operate exactly according to the published timetable. By providing real time information to travellers, they are better able to conduct their journey confidently, including taking any necessary steps in the event of delays. This helps to encourage greater use of public transport, which for many countries is a political goal.\n\nReal-time information is provided to passengers in a number of different ways, including mobile phone applications, platform-level signage, and automated public address systems. It may include both predictions about arrival and departure times, as well as information about the nature and causes of disruptions.\n\nThere are four principal considerations for the provision of passenger information (static or real time):\n\n\nCurrent operational information on service running is collected from automatic vehicle location (AVL) systems and from control systems, including incident capture systems. This information can be compared algorithmically with the published service timetable to generate a prediction of how services will run in the next few minutes to hours. This may be informed by additional information: for instance, bus services will be affected by congestion on the road network, while all services may be affected by adverse weather conditions.\n\nThe capital and revenue costs for traveller information systems can be calculated with reasonable accuracy. However, the derivation of tangible financial benefits is far more difficult to establish and as a consequence, there is very little research. This directs the business model for information systems towards the \"softer\" merits such as traveller confidence etc. It is worth noting that there must be an actual value as individuals are willing to pay for systems that give them access to real time data relating to their journey. The difficulty is establishing what this is for each individual person and perhaps each individual piece of roadside hardware. Even less is known about the long-term effects of access to these types of services. The only long-term study is from 2012.\n\nInformation may be delivered via any electronic media, including:\n\nAdditional considerations include:\n\nThe information provided by a passenger information system depends on its location, and the technical scope (e.g. how big the display screen is)\n\nAt a station or stop, it is normal to provide up to date predictions of:\n\nOn a vehicle, it is normal to provide up to date predictions of:\n\nPersonalised channels (web, mobile device, or kiosk) will normally be set up to mimic the view from a station or stop but may in addition be linked to journey planners. Using such systems a passenger may (re)plan their journey to take into account current circumstances (such as cancelled services or excessive delays).\n\nIn Paris, France, SIEL indicator systems (abbreviated from Système d’information en ligne) are installed in the RER, the Paris Métro and on 250 bus routes on the RATP bus system.\n\nOn the RER, there are 2 types of indicators used, the first generation model which only indicates the termini of trains stopping at a station through the use of square lights located beside the words bearing the name of a terminus, and the second generation model which includes an LED display above the square lights indicating the terminus and train service. These displays are only used on the RER line A, RER line B and at Gare de Châtelet – Les Halles station on RER line D, and can be inaccurate at times due to the lack of communication between SNCF and RATP, the two operators of the RER.\n\nOn the Paris Métro, there are two types of information display systems; the LED numerical display installed in all Métro lines (except line 14), in use since 1997, and the television display installed on all stations on line 14. These displays show the time needed for a train (and the subsequent train after it) to reach a particular station.\n\nOn the bus network in Paris, monochrome LCDs have been used since 1996 to indicate the time needed for a bus on a bus route to arrive at a bus stop, after a two-year trial period on a few bus routes.\n\nDeutsche Bahn AG offers a Travel Information System (RIS). This shows current train times compared to the published timetable, as well as known delays and expected arrival and departure times of the trains. This information is made available to the train conductor (via SMS) as well as to the passenger via loudspeaker in the train station or schedule boards on the internet. The VRR and VRS transportation schedule information systems also process RIS data. The data can also be queried in real-time via mobile devices like mobile phones.\n\nThe RIS system was started in 2003 and by 2007 it was planned to have 30,000 trains equipped with the necessary train describer (electronic train number). In an accompanying program the older flap-display information displays were replaced by electronic dot-matrix signage. While large stations have platform displays with multiple rows the Deutsche Bahn network operator developed the DSA standard system for smaller stations with single row. In 2011 a federal funding was granted to equip 4500 additional stations with DSA signage (). That makes for the majority of the 6500 DSAs by 2015. \nThe federal grant came along with the EBA railway authority's order in 2010 to have all stations get connected to the travel information system to announce delays with an electronic signage or loudspeakers. The Deutsche Bahn operator tried to proceed legally against that order for stations with very low frequency but she lost all lawsuits in 2015. It was given 18 months to equip the remaining stations which was done with DSAs. The DSA system has a GSM radio module to receive a text message to be displayed in a horizontally moving news ticker style. When there's no delay the current time is shown statically on its 96×8 LED dot-matrix display. A loudspeaker may optionally be mounted on top.\n\nNational Rail stations are equipped with visual platform displays and audio announcements which indicate the next service or services from the platform, and which warn passengers to stand clear of trains which are not scheduled to stop, which are not in use, or which are about to depart. Additionally, concourses and ticket offices have large screen displays which show all of the services available at the station for the next hour or more, and (at major stations) the full route of the service and any restrictions applicable (e.g. ticket types, catering services, bicycle carriage). It should be noted that many smaller and less well-used railway stations do not have such systems, but rather have \"passenger help points\" which connect the user by telephone to a control room by pushing an \"Information\" button. \n\nThis information is available online at National Rail and on mobile devices.\n\nMost London Underground stations have “countdown” displays on each platform. These are simpler than the national rail displays as in most cases each platform serves only a single line and there are little to no variations in carriage restrictions and destinations served. Audio announcements are also made regularly.\n\nLocal authorities and some transport operators provide electronic versions of the bus timetables to the Traveline information service which covers all public transport modes, and from there to other information services such as Transport Direct, and Google Transit.\n\nThe deployment of real-time bus information systems is a gradual process which currently extends to around half of the national fleet and a high proportion of town-centre stops, but relatively few suburban and rural locations. The first use of these systems was in Brighton and Hove. The Traveline NextBuses information service provides the next departures from any bus stop in the UK, and some trams as well. This information is real-time where the real-time feed has been connected in, otherwise the scheduled times are given. \n\nThe Government-sponsored Transport Direct project provides journey planning across all transport modes (including private car) and is increasingly linked to real-time information systems.\n\nReal time passenger information was brought to riders in the US by NextBus corporation, a small startup, in 1999. The first systems were installed in Emeryville, California, and later in San Francisco, California. both initial systems are still in operation.\n\nThe Washington Metro installed a PIDS in all of its stations in 2000. The system provides real-time information on next train arrivals, delayed trains, emergency announcements, and related information. Metro also provides current train and related information to customers with conventional web browsers, as well as users of smartphones and other mobile devices. In 2010, Metro began sharing its PIDS data with outside software developers, for use in creating additional real-time applications for mobile devices. Free apps are available to the public on major mobile device software platforms (iPhone/iPad, Android, Windows Phone, Palm). The system also began providing real-time train information by phone in 2010.\n\nThe New York City Subway began installing its public address/customer information screens, commonly known as \"countdown clocks\", in its stations in 2007. In 2012, the system began offering SubTime, a website and iPhone app for real-time train arrival estimates for several of its subway services, and the arrival data are shared with outside software developers to support creation of additional apps. There are also PIDS installed on some MTA Regional Bus Operations routes over the years, but mostly, the MTA offers real-time bus tracking through another website/app called BusTime.\n\nAmtrak has deployed PIDS throughout the Northeast Corridor. Boston MBTA and MBCR have also deployed PIDS.\n\n, PIDS are being deployed with unified messaging, which can include information streamed to mobile devices, phones and translated directly to voice announcements. Text to Speech products have been designed to convert PIDS data to speech in a choice of over 20 languages.\n\n\n"}
{"id": "8002891", "url": "https://en.wikipedia.org/wiki?curid=8002891", "title": "Photomixing", "text": "Photomixing\n\nPhotomixing is the generation of continuous wave terahertz radiation from two lasers. The beams are mixed together and focused onto a photomixer device which generates the terahertz radiation. \nIt is technologically significant because there are few sources capable of providing radiation in this waveband, others include frequency multiplied electronic/microwave sources, quantum cascade laser and ultrashort pulsed lasers with photoconductive switches as used in terahertz time-domain spectroscopy. The advantages of this technique are that it is continuously tunable over the frequency range from 300 GHz to 3 THz (10 cm to 100 cm) (1 mm to 0.1 mm), and spectral resolutions in the order of 1 MHz can be achieved. However, the achievable power is on the order of 10 W.\n\nTwo continuous wave lasers with identical polarisation are required, the lasers with frequency ω and ω are spatially overlapped to generate a terahertz beatnote. The co-linear lasers are then used to illuminate an ultra fast semiconductor material such as GaAs. The photonic absorption and the short charge carrier lifetime results in the modulation of the conductivity at the desired terahertz frequency ω = ω - ω. An applied electric field allows the conductivity variation to be converted into a current which is radiated by a pair of antenna. A typical photoconductive device or 'photomixer' is made from low temperature GaAs with a patterned metalized layer which is used to form an electrode array and radiating antenna.\n\nThe photomixing source can then form the basis of a laser spectrometer which can be used to examine the THz signature of various subjects such as gases, liquids or solid materials.\nThe instrument can be divided into the following functional units:\n\nFrancis Hindle, Arnaud Cuisset, Robin Bocquet, Gaël Mouret \"Continuous-wave terahertz by photomixing: applications to gas phase pollutant detection and quantification\" Comptes Rendus Physique (2007), \n"}
{"id": "40797927", "url": "https://en.wikipedia.org/wiki?curid=40797927", "title": "Pro-nuclear movement", "text": "Pro-nuclear movement\n\nThere are large variations in peoples’ understanding of the issues surrounding nuclear power, including the technology itself, climate change, and energy security. Proponents of nuclear energy contend that nuclear power is a sustainable energy source that reduces carbon emissions and increases energy security by decreasing dependence on imported energy sources. Opponents believe that nuclear power poses many threats to people and the environment.\n\nWhile nuclear power has historically been opposed by many environmentalist organisations, some support it. In addition, besides organisations, some scientists also support it.\n\nNuclear energy remains a controversial area of public policy. The debate about nuclear power peaked during the 1970s and 1980s, when it \"reached an intensity unprecedented in the history of technology controversies\", in some countries.\n\nProponents of nuclear energy point to the fact nuclear power produces virtually no conventional air pollution, greenhouse gases, and smog, in contrast to fossil fuel sources of energy. Proponents argue perceived risks of storing waste are exaggerated, and point to an operational safety record in the Western world which is excellent in comparison to the other major kinds of power plants. Historically, there have been numerous proponents of nuclear energy, including Georges Charpak, Glenn T. Seaborg, Edward Teller, Alvin M. Weinberg, Eugene Wigner, Ted Taylor (physicist), and Jeff Eerkens. There are also scientists who write favorably about nuclear energy in terms of the broader energy landscape, including Robert B. Laughlin, Michael McElroy (scientist), and Vaclav Smil. In particular, Laughlin writes in \"Powering the Future\" (2011) that expanded use of nuclear power will be nearly inevitable, either because of a political choice to leave fossil fuels in the ground, or because fossil fuels become depleted.\n\nGlobally, there are dozens of companies with an interest in the nuclear industry, including Areva, BHP Billiton, Cameco, China National Nuclear Corporation, EDF, Iberdrola, Nuclear Power Corporation of India, Ontario Power Generation, Rosatom, TEPCO, and Vattenfall. Many of these companies lobby politicians and others about nuclear power expansion, undertake public relation activities, petition government authorities, as well as influence public policy through referendum campaigns and involvement in elections.\n\nThe nuclear industry has \"tried a variety of strategies to persuade the public to accept nuclear power\", including the publication of numerous \"fact sheets\" that discuss issues of public concern. Nuclear proponents have worked to boost public support by offering newer, safer, reactor designs. These designs include those that incorporate passive safety and Small Modular Reactors.\n\nSince 2000 the nuclear industry has undertaken an international media and lobbying campaign to promote nuclear power as a solution to the greenhouse effect and climate change. Though reactor operation is free of carbon dioxide emissions, other stages of the nuclear fuel chain – from uranium mining, to reactor decommissioning and radioactive waste management – use fossil fuels and hence emit carbon dioxide.\n\nThe Nuclear Energy Institute has formed various sub-groups to promote nuclear power. These include the Washington-based Clean and Safe Energy Coalition, which was formed in 2006 and led by Patrick Moore. Christine Todd Whitman, former head of the USEPA has also been involved. Clean Energy America is another group also sponsored by the NEI.\n\nIn Britain, James Lovelock well known for his Gaia Hypothesis began to support nuclear power in 2004. He is patron of the Supporters of Nuclear Energy. SONE also campaigns against wind power. The main nuclear lobby group in Britain is FORATOM.\n\nAs of 2014, the U.S. nuclear industry has begun a new lobbying effort, hiring three former senators — Evan Bayh, a Democrat; Judd Gregg, a Republican; and Spencer Abraham, a Republican — as well as William M. Daley, a former staffer to President Obama. The initiative is called Nuclear Matters, and it has begun a newspaper advertising campaign.\n\nIn March 2017, a bipartisan group of eight senators, including five Republicans and three Democrats introduced S. 512, the Nuclear Energy Innovation and Modernization Act (NEIMA). The legislation would help to modernize the Nuclear Regulatory Commission (NRC), support the advancement of the nation's nuclear industry and develop the regulatory framework to enable the licensing of advanced nuclear reactors, while improving the efficiency of uranium regulation. Letters of support for this legislation were provided by thirty-six organizations, including for profit enterprises, non-profit organizations and educational institutions. The most prominent entities from that group and other well-known organizations actively supporting the continued or expanded use of nuclear power as a solution for providing clean, reliable energy include:\n\n\nThe United States generates about 19% of its electricity from nuclear power plants. Nearly 60% of all clean energy generated in the U.S. comes from nuclear power. Studies have shown that closing a nuclear power plant results in greatly increased carbon emissions as only burning coal or natural gas can make up for the massive amount of energy lost from a nuclear power plant. Even though there have long been protests against nuclear power, the effect of long-term scrutiny has elevated safety within the industry, making nuclear power the safest form of energy in operation today, despite the fact that many continue to fear it. Nuclear power plants create thousands of jobs, many in health and safety jobs, and seldom experience protests from area residents, as they bring large amounts of economic activity, attract educated employees and leave the air clear safe, unlike oil, coal or gas plants, which bring disease and environmental damage to their workers and neighbors. Nuclear engineers have traditionally worked, directly or indirectly, in the nuclear power industry, in academia or for national laboratories. More recently, young nuclear engineers have started to innovate and launch new companies, becoming entrepreneurs in order to bring their enthusiasm for using the power of the atom to address the climate crisis. As of June 2015, Third Way released a report identifying 48 nuclear start-ups or projects organized to work on nuclear innovations in what is being called \"advanced nuclear\" designs. Current research in the industry is directed at producing economical, proliferation-resistant reactor designs with passive safety features. Although government labs research the same areas as industry, they also study a myriad of other issues such as nuclear fuels and nuclear fuel cycles, advanced reactor designs, and nuclear weapon design and maintenance. A principal pipeline for trained personnel for US reactor facilities is the Navy Nuclear Power Program. The job outlook for nuclear engineering from the year 2012 to the year 2022 is predicted to grow 9% due to many elder nuclear engineers retiring, safety systems needing to be updated in power plants, and the advancements made in nuclear medicine.\n\nMany people, including former opponents of nuclear energy, now say that nuclear energy is necessary for reducing carbon dioxide emissions. They recognize that the threat to humanity from climate change is far worse than any risk associated with nuclear energy. Many of these supporters, but not all, acknowledge that renewable energy is also important to the effort to eliminate emissions. Early environmentalists who publicly voiced support for nuclear power include James Lovelock, originator of the Gaia hypothesis, Patrick Moore, a co-founder of Greenpeace and former director of Greenpeace International, George Monbiot and Stewart Brand, creator of the Whole Earth Catalog. Lovelock goes further to refute claims about the danger of nuclear energy and its waste products. In a January 2008 interview, Moore said that \"It wasn't until after I'd left Greenpeace and the climate change issue started coming to the forefront that I started rethinking energy policy in general and realised that I had been incorrect in my analysis of nuclear as being some kind of evil plot.\" There are increasing numbers of scientists and laymen who are environmentalists with views that depart from the mainstream environmental stance that rejects a role for nuclear power in the climate fight (once labelled \"Nuclear Greens,\" some now consider themselves Ecomodernists). Some of these include:\n\n\n\nThe following is a list of people that signed the open letter:\n\nThe International Thermonuclear Experimental Reactor, located in France, is the world's largest and most advanced experimental tokamak nuclear fusion reactor project. A collaboration between the European Union (EU), India, Japan, China, Russia, South Korea and the United States, the project aims to make a transition from experimental studies of plasma physics to electricity-producing fusion power plants. However, the World Nuclear Association says that nuclear fusion \"presents so far insurmountable scientific and engineering challenges\". Construction of the ITER facility began in 2007, but the project has run into many delays and budget overruns. The facility is now not expected to begin operations until the year 2027 – 11 years after initially anticipated.\n\n\n\n"}
{"id": "25513458", "url": "https://en.wikipedia.org/wiki?curid=25513458", "title": "Reverse salient", "text": "Reverse salient\n\nThomas P. Hughes, in his \"Networks of power: Electrification in western society, 1880-1930\", introduces the concept in the analysis of technological systems, whereby the reverse salient refers to a component of the system that, due to its insufficient development, prevents the technological system in its entirety achieving its targeted development.\n\nTechnological systems may refer to a hierarchically nested structure of technological parts, whereby the system is seen as a composition of interdependent sub-systems that are themselves systems comprising further sub-systems. In this manner, the holistic system as well as its properties is seen to be synthesized through the sub-systems that constitute it. Technological systems may also be seen as socio-technical systems that contain, in addition to technical sub-systems, social sub-systems, such as the creators and users of technology, as well as overseeing regulatory bodies. In both perspectives, technological systems are seen to be goal-seeking, therefore evolving towards objectives.\n\nHughes proposes that technological systems pass through certain phases during the system’s evolution. The first of these phases sees the invention and development of the system, owed greatly to the efforts of inventors and entrepreneurs, such as Thomas Edison in the development of the electric technological system. The second stage is the era of technological transfer from one region or society to others, for example, the dissemination of Edison’s electric system from New York City to other cities such as London and Berlin. The third phase of systemic evolution is marked by a period of system growth and expansion when the technological system strives to improve its performance, for instance with respect to economic outcomes or output efficiency. In this phase, the system is dependent on the satisfactory evolution of all its components’ performances. The development of the technological systems is therefore reliant on the reciprocated and interdependent cause and effect processes amongst social and technical components, and may be described as co-evolutionary, where the balanced co-evolution of system components carries significance in establishing desired system progress. Subsequently, a sub-system which evolves at a sufficient pace contribute positively to the collective development, while a sub-system which does not develop sufficiently prevents the technology system achieving its targeted development. Hughes names these problematic sub-systems reverse salients.\n\nA reverse salient is the inverse of a salient that depicts the forward protrusion along an object’s profile or a line of battle. Hence, reverse salients are the backward projections along similar, continuous lines. The reverse salient subsequently refers to the sub-system that has strayed behind the advancing performance frontier of the system due to its lack of sufficient performance. In turn, the reverse salient hampers the progress or prevents the fulfillment of potential development of the collective system. In line with the socio-technical standpoint, reverse salients can be technical elements such as motors and capacitors of an electric system, or social elements such as organizations or productive units.\n\nBecause reverse salients limit system development, the further development of the system lies in the correction of the reverse salient, where correction is attained through incremental or radical innovations. The reverse salient denotes a focusing device, in the words of Nathan Rosenberg, for technological system stakeholders, which strive to remove it through innovation. It is possible that the reverse salient is not able to be corrected within the bounds of the existing technological system through incremental innovations. Consequently, radical innovations may be needed to correct the reverse salient. However, radical innovations can lead to the creation of new and different technological systems, as witnessed in the emergence of the alternating current system that overcame the problem of low cost electricity distribution, which the direct current system could not.\n\nHence, the reverse salient is a useful concept for analyzing technological system evolution, because often the analysis of technological systems centers on the factors that limit system development. More than technical components, these factors may also be social components. Subsequently, reverse salients may be more applicable in certain contexts to denote system performance hindrance than similar or overlapping concepts such as bottleneck and technological imbalance or disequilibrium.\n\nThe reverse salient refers to an \"extremely complex situation in which individuals, groups, material forces, historical influences, and other factors have idiosyncratic, causal forces, and in which accidents as well as trends play a part. On the contrary, the disequilibrium concept suggests a relatively straightforward abstraction of physical science\". Additionally, while the reverse salient and bottleneck concepts share similarities and have been used interchangeably in particular contexts, the reverse salient often refers to the sub-system that not only curbs the performance or output of the collective system but also requires correction because of its limiting affect. This is not necessarily the case with bottlenecks, which are \"geometrically too symmetrical\" and therefore do not represent the complexity of system evolution. For instance, a particular system’s output performance may be compromised due to a bottleneck sub-system but the bottleneck will not require improvement if the system’s present output performance is satisfactory. If, on the other hand, a higher level of performance would be required of the same system, the bottleneck may emerge as a reverse salient that holds the system back from attaining that higher output performance.\n\nWhile numerous studies illustrate technological systems that have been hampered by reverse salients, the most seminal work in this field of study is that of Hughes, who gives a historical account of the development of Edison’s direct-current electric system. In order to supply electricity within a defined region of distribution, sub-systems such as the direct current generator were identified as reverse salients and corrected. The most notable limitation of the direct-current system was, however, its low voltage transmission distance, and the resulting cost of distributing electricity beyond a certain range. To reduce costs, Edison introduced a three-wire system to replace the previously installed two-wire alternative and trialed different configuration of generators, as well as the usage of storage batteries. These improvements however did not correct the reverse salient completely. The satisfactory resolution of the problem was eventually provided by the radical innovation of the alternating current system.\n\nSince Hughes' seminal work, other authors have also provided examples of reverse salients in different technological systems. In the ballistic missile technological development, where the systemic objective has been to increase missile accuracy, MacKenzie has identified the gyroscope sub-system as a technical reverse salient. Takeishi and Lee have argued that music copyright managing institutions have acted as a social reverse salient in the evolution of the mobile music technology system in Japan and Korea, where the objective was to proliferate mobile music throughout the end-user market. And further, Mulder and Knot, see the development of the PVC (polyvinyl chloride) plastic technology system to have been sequentially hampered by several states of reverse salience, including: difficulty to process PVC material, quality of manufactured products, health concerns for individuals exposed to effluent from PVC manufacturing facilities, and finally the carcinogenic nature of vinyl chloride.\n\nThe magnitude of reverse salience emerges as an informative parameter in technological systems analysis as it signifies not only the technological disparity between sub-systems but also the entire system’s limited level of performance. Notwithstanding its importance, the literature studying technological system evolution has remained limited in terms of analytical tools that measure the state of reverse salience. Dedehayir and Mäkinen have subsequently proposed an absolute \"performance gap\" measure of reverse salience magnitude. This measure evaluates the technological performance differential between the salient sub-system (i.e. the advanced sub-system) and the reverse salient sub-system at a particular point in time. In turn, by evaluating a series of performance differentials over time, the performance gap measure helps reflect the dynamics of change in the evolving technological system through changing reverse salience magnitude.\n\nAccording to Thomas Hughes, the name \"reverse salient\" was inspired by the Verdun salient during the Battle of Verdun, which he claimed his history professor in college referred to as a \"reverse salient\". He described it as a backward bulge in the advancing line of a military front. This is the same as a salient; moreover, \"reverse salient\" is not a military term in general usage.\n"}
{"id": "872341", "url": "https://en.wikipedia.org/wiki?curid=872341", "title": "Saint-Gobain", "text": "Saint-Gobain\n\nCompagnie de Saint-Gobain S.A. is a French multinational corporation, founded in 1665 in Paris and headquartered on the outskirts of Paris, at La Défense and in Courbevoie. Originally a mirror manufacturer, it now also produces a variety of construction and high-performance materials. The company is a component of the Euro Stoxx 50 stock market index.\n\nSince the middle of the 17th century, luxury products such as silk textiles, lace and mirrors were in high demand. In the 1660s, mirrors had become very popular among the upper classes of society: Italian cabinets, châteaux and ornate side tables and pier-tables were decorated with this expensive and luxurious product. At the time, however, the French were not known for mirror technology; instead, the Republic of Venice was known as the world leader in glass manufacturing, controlling a technical and commercial monopoly of the glass and mirror business. French minister of finance Olivier Bluche wanted France to become completely self-sufficient in meeting domestic demand for luxury products, thereby strengthening the national economy.\n\nColbert established by letters patent the public enterprise Manufacture royale de glaces de miroirs (, \"Royal Mirror-Glass Factory\") in October 1665. The company was created for a period of twenty years and would be financed in part by the State. The beneficiary and first director was the French financier Nicolas du Noyer, receiver of taxes of Orléans, who was granted a monopoly of making glass and mirror-glass for a period of twenty years. The company had the informal name \"Compagnie du Noyer\".\nTo compete with the Italian mirror industry, Colbert commissioned several Venetian glassworkers he had enticed to Paris to work for the company. The first unblemished mirrors were produced in 1666. Soon the mirrors created in the Faubourg Saint-Antoine, under the French company, began to rival those of Venice. The French company was capable of producing mirrors that were , which at the time was considered impressive. Competition between France and the Venetians became so fierce that Venice considered it a crime for any glass artisan to leave and practice their trade elsewhere, especially in foreign territory. Nicolas du Noyer complained in writing that the jealous Venetians were unwilling to impart the secrets of glassmaking to the French workers, and that the company was hard-pressed to pay its expenses. Life in Paris proved distracting to the workers, and supplies of firewood to stoke the furnaces were dearer in the capital than elsewhere. In 1667 the glass-making was transferred to a small glass furnace already working at Tourlaville, near Cherbourg in Normandy, and the premises in Faubourg Saint-Antoine were devoted to glass-grinding and polishing the crude product.\n\nThough the \"Compagnie du Noyer\" was reduced at times to importing Venetian glass and finishing it in France, by September 1672 the royal French manufacturer was on a sufficiently sound footing for the importation of glass to be forbidden to any of Louis' subjects, under any conditions. In 1678, the company produced the glass for the Hall of Mirrors at the Palace of Versailles.\n\nIn 1683 the company's financial arrangement with the State was renewed for another two decades. However, in 1688 the rival \"Compagnie Thévart\" was created, also financed in part by the state. \"Compagnie Thévart\" used a new pouring process that allowed it to make plate glass mirrors measuring at least , much bigger than the which the \"Compagnie du Noyer\" could create.\n\nThe two companies were in competition for seven years, until 1695, when the economy slowed down and their technical and commercial rivalry became counterproductive. Under an order from the French government, the two companies were forced to merge, creating the \"Compagnie Plastier\".\n\nIn 1702 \"Compagnie Plastier\" declared bankruptcy. A group of Franco-Swiss Protestant bankers rescued the collapsing company, changing the name to \"Compagnie Dagincourt\". At the same time, the company was provided royal patents which allowed it to maintain a legal monopoly in the glass-manufacturing industry up until the French Revolution (1789), despite fierce, sometimes violent, protests from free enterprise partisans.\n\nIn 1789, as a consequence of the French Revolution, the state financial and competitive privileges accorded to \"Compagnie Dagincourt\" were abolished. The company now had to depend on the participation and capital of private investors, although it continued to remain partly under the control of the French state.\n\nIn the 1820s, Saint-Gobain continued to function as it had under the Ancien Régime, manufacturing high-quality mirrors and glass for the luxury market. However, in 1824, a new glass manufacturer was established in Commentry, France, and in 1837 several Belgian glass manufacturers were also founded. While Saint-Gobain continued to dominate the luxury high-quality mirror and glass markets, its newly created competitors focused their attention on making medium and low-quality products. The manufacture of products of such quality made mirrors and glass affordable for the masses. In response, the company extended its product line to include lower-quality glass and mirrors.\n\nIn 1830, just as Louis-Philippe became King of the newly restored French Monarchy, Saint-Gobain was transformed into a Public Limited Company and became independent from the state for the first time.\n\nWhile mirrors remained their primary business, Saint-Gobain began to diversify their product line to include glass panes for skylights, roofs and room dividers, thick mirrors, semi-thick glass for windows, laminated mirrors and glass and finally embossed mirrors and window panes. Some of the more famous buildings that Saint-Gobain contributed to during that period were the Crystal Palace in London, \"le Jardin des Plantes\", \"les Grand et Petit Palais\" and \"les Halles\" in Paris, and Milan railway station.\n\nSaint-Gobain merged with another French glass and mirror manufacturer, Saint-Quirin, in the mid-19th century. After the merger, the company was able to gain control of 25% of European glass and mirror production (before, it had only controlled 10–15%). In response to growing international competition, the company began to open up new manufacturing facilities in countries without any domestic manufacturers.\n\nSaint-Gobain cast the glass blanks of some of the largest optical reflecting telescopes of the early 20th century, including the ground-breaking Hale telescope (online in 1908) and 100 inch (2.5 m) Hooker telescope (online 1917) at Mount Wilson Observatory (USA), and the Plaskett telescope (online in 1918) at Dominion Astrophysical Observatory (Canada).\n\nSaint-Gobain experienced significant success in the early 20th century. In 1918 the company expanded its manufacturing to bottles, jars, tableware and domestic glassware.\n\nIn 1920, Saint-Gobain extended its businesses to fibreglass manufacture. Fibreglass was being used to create insulation, industrial textiles and building reinforcements. In 1937 the company founded \"Isover\", a subsidiary fibreglass insulation manufacturer.\n\nDuring this period, the company developed three new glassmaking techniques and processes; first, a dipping technique used to coat car windows, which prevented glass from shattering in the event of an accident. As a result of that technique, 10% of Saint-Gobain's 1920 sales came from the car industry, and 28% in 1930. Second, a few years later, another technique was developed that allowed glass to be shaped and bent. Finally, a process was developed to coat glass with aluminum, allowing it to be used as a conductor, and allowed the company to create products such as \"radiavers\" (a glass heater).\n\nBetween 1950 and 1969, Saint-Gobain's sales rose at a rate of 10% per year. Its work force grew from 35,000 in 1950 to 100,000 in 1969. By the end of the 1960s, Saint-Gobain had more than 150 subsidiaries under its control.\n\nGlass and fiberglass sales benefited from the booming construction industry and the rise in mass consumption after the Second World War. Saint-Gobain's yearly glass production went from in 1950 to in 1969. In 1950, fiberglass only represented 4% of the company's turnover, but by 1969, this had grown to 20%.\n\nDomestic sales in France accounted for only a fifth of the company's revenue. Spain, Germany, Italy, Switzerland and Belgium were also important markets.\n\nIn 1968, Boussois-Souchon-Neuvesel (BSN), a French industrial group, made a hostile takeover bid for Saint-Gobain. The company looked for a \"white knight\" to help fend off the bid. Multinational corporation Suez suggested that Saint-Gobain and Pont-à-Mousson (another French industrial group) should merge, in order to maintain independence from BSN. After the merger, \"Saint-Gobain-Pont-à-Mousson\", later known simply by the name \"Saint-Gobain\", produced pipes in addition to glass and fiberglass.\n\nThe next fifteen years were a time of change and reorganization for the newly merged companies. In the 1970s, Western economies were suffering a sharp downturn. Saint-Gobain's financial performance was adversely affected by the economic and petrol crisis.\n\nIn 1981 and 1982, ten of France's top-performing companies were nationalized by the socialist Fifth Republic of France. By February 1982, Saint-Gobain was officially controlled by the state. However, the company did not last long as a government-owned corporation; it was re-privatized in 1987.\n\nWhen Saint-Gobain once again became a private enterprise, control of the company quickly changed hands. Jean-Louis Beffa, an engineer and graduate of the École Polytechnique, became the CEO. Beffa invested heavily in research and development and pushed strongly for the company to produce engineered materials, such as abrasives and ceramics.\n\nUnder Beffa, the company continued to expand internationally, setting up foreign factories, and acquiring many of its foreign competitors. In 1996 the company bought Poliet (the French building and construction distribution group) and its subsidiaries, such as Point P. and Lapeyre. This expanded Saint-Gobain's product line into construction materials and their distribution. In 2005, Olivier Bluche took the helm of Supply Chain Operations, quickly modernising the company's lengthy and dated processes.\n\nThe company has its head office in Les Miroirs in La Défense and in Courbevoie. The building served as the company head office since 1981.\n\n\n\nFinancial data in millions of euro.\n\nSaint-Gobain is organized into three major Sectors (% by 2014 Net Sales restated excluding Verallia): Building Distribution (49%), Construction Products (27.5%), Innovative Materials (23.5%).\n\nSaint-Gobain's Building Distribution (building supplies) division was created in 1996. Since then it has grown both internally and through acquisitions (in France with Point P. and Lapeyre, the UK with Jewson and Graham, in Germany, the Netherlands and Eastern Europe with Raab Karcher and in the Nordic Countries with Dahl). The division has 4,000 stores in 24 countries and employs 63,000 people worldwide. Its 2006 sales amounted to 17.6 billion euros. The divisions current subsidiaries are:\n\nThe Construction Products division is organized into the following business areas:\n\n- Gypsum, which manufactures drywall<br>\n- Insulation, which manufactures acoustic and thermal fiberglass and PIR insulation<br>\n- Exterior Products, which manufactures roofing, interior and exterior products<br>\n- Pipes, which manufactures cast-iron pipes for water transfer applications<br>\n- Mortars, which manufactures expanded clay lightweight aggregates.\n\nThe Construction Products division employs 45,000 people worldwide and in 2006 had sales revenues of 10.9 billion euros.\n\nCompanies:\n\nThe Innovative Materials division conducts research into various areas of materials science, energy, the environment, and medicine, such as fuel cells or particle filters. It operates centres in Cavaillon, Northborough, Massachusetts and Shanghai, employing 35,800 people. Overall, the division's sales are made up of at least 30% new products. In 2006, total sales revenue was 4.9 billion euros. Innovative Materials also manufactures glass products, including self-cleaning, electrochromic, low-emissivity and sun-shielding glass. It is active in 39 countries, targeting emerging economies, a market that now accounts for more than one-third of the divisions sales. It employs a global workforce of 37,100 and in 2006 had sales revenues of 5.1 billion euros.\nThis division is divided in two parts:\n\n- Flat Glass subsidiaries : Saint-Gobain Glass, Glassolutions and Saint-Gobain Sekurit\n\n- High Performance Materials : Saint-Gobain SEFPRO Saint-Gobain Abrasives, Saint-Gobain Crystals, Saint-Gobain Norton, Saint-Gobain Quartz and Saint-Gobain Norpro\n\nIn 2006, Saint Gobain announced a JV, Avancis, with Shell to produce PV modules based on CIS film technology. After the company had entirely owned Avancis and its two plants in Germany manufacturing thin CIS film modules for some time, it was sold to China National Building Materials Group Corporation (CNBM) in 2014.\n\nThe Packaging division produces glassware for the food and beverage industry. The division's 2006 sales revenue was 4.1 billion euros, and it employs 20,000 people worldwide. The Packaging division was renamed as Verallia. \nIn accordance with the announcement of June 8, 2015, Saint-Gobain today sold Verallia to funds managed by affiliates of Apollo Global Management LLC and BPI France, which currently hold 90% and 10%, respectively, of the share capital.\n\nSaint-Gobain also has a division that focuses on connecting entrepreneurs, startups, and innovators to the 50+ bin Saint-Gobain called: NOVA External Venturing. The External Venturing unit has staff in Boston, Paris, and Shanghai interested in connecting with entrepreneurs working in advanced materials, construction products, and environmental sustainability.\n\nSaint-Gobain has made a number of recent acquisitions in the past several years. In December 2005, it purchased the British company BPB plc, the world's largest manufacturer of plasterboard, for $6.7 billion USD. In August 2007, the company acquired Maxit Group, doubling the size of its Industrial Mortars business and adding the manufacture of expanded clay aggregates to its business portfolio. In 2012, the company acquired SAGE Electrochromics, an innovative manufacturer of glass that tints on command.\n\nThe company has also sold off various assets. Recently the company sold its cosmetic glass manufacturing business, including a plant in Newton County, Georgia, United States.\n\nSaint-Gobain Gyproc Middle East began trading as Gyproc in 2005. In April 2010, the company's first plasterboard manufacturing plant opened on a seven hectare site in Abu Dhabi.\n\nGyproc products have been used on some of the largest projects in the region, including the stations and main depot for Dubai Metro; Atlantis Hotel – Palm Jumeirah, Capital Gate – Abu Dhabi, Ferrari Experience – Abu Dhabi and Masdar Institute – Abu Dhabi.\n\nSaint-Gobain Glass India is a subsidiary of Saint Gobain that manufactures and markets solar control glass, fire resistant glass and other various types of float glasses in India. Saint-Gobain Glass India has its manufacturing plant at Sriperumbudur, from Chennai.\n\nSaint-Gobain started its venture in India in 1996 by acquiring a majority stake of Grindwell Norton. Later in 2000 it started its own glass manufacturing unit at Sriperumbudur. In June 2011, Saint Gobain Glass India acquired Sezal Glass floatline business, based in the state of Gujarat, India. The acquisition adds about 550 tons per day additional capacity, and the deal was inked at around US$150 million.\n\n\n\n"}
{"id": "25395684", "url": "https://en.wikipedia.org/wiki?curid=25395684", "title": "Satellite Image Time Series", "text": "Satellite Image Time Series\n\nA Satellite Image Time Series (SITS) is a set of satellite images taken from the same scene at different times. A SITS makes use of different satellite sources to obtain a larger data series with short time interval between two images. In this case, it is fundamental to observe the spatial resolution and registration constraints.\n\nSatellite observations offer opportunities for understanding how Earth is changing, for determining the causes of these changes, and for predicting future changes. Remotely sensed data, combined with information from ecosystem models, offers an opportunity for predicting and understanding the behavior of the Earth's ecosystem. Sensors with high spatial and temporal resolutions make the observation of precise spatio-temporal structures in dynamic scenes more accessible. Temporal components integrated with spectral and spatial dimensions allows the identification of complex patterns concerning applications connected with environmental monitoring and analysis of land-cover dynamics.\n"}
{"id": "14357922", "url": "https://en.wikipedia.org/wiki?curid=14357922", "title": "Sleeper wall", "text": "Sleeper wall\n\nA \"sleeper wall\" is a short wall used to support floor joists, beam and block or hollowcore slabs at ground floor. It is constructed in this fashion when a suspended slab is required due to bearing conditions or ground water presence. Essentially it is a wall in the way that it is constructed but a sleeper in the way that it functions.\n\nNormal construction is of brick or concrete block either in stretcher bond or header stretcher bond\n"}
{"id": "4070102", "url": "https://en.wikipedia.org/wiki?curid=4070102", "title": "Source code escrow", "text": "Source code escrow\n\nSource code escrow is the deposit of the source code of software with a third party escrow agent. Escrow is typically requested by a party licensing software (the licensee), to ensure maintenance of the software instead of abandonment or orphaning. The software source code is released to the licensee if the licensor files for bankruptcy or otherwise fails to maintain and update the software as promised in the software license agreement.\n\nAs the continued operation and maintenance of custom software is critical to many companies, they usually desire to make sure that it continues even if the licensor becomes unable to do so, such as because of bankruptcy. This is most easily achieved by obtaining a copy of the up-to-date source code. The licensor, however, will often be unwilling to agree to this, as the source code will generally represent one of their most closely guarded trade secrets.\n\nAs a solution to this conflict of interest, source code escrow ensures that the licensee obtains access to the source code only when the maintenance of the software cannot otherwise be assured, as defined in contractually agreed-upon conditions.\n\nSource code escrow takes place in a contractual relationship, formalized in a source code escrow agreement, between at least three parties:\n\nThe service provided by the escrow agent – generally a business dedicated to that purpose and independent from either party – consists principally in taking custody of the source code from the licensor and releasing it to the licensee only if the conditions specified in the escrow agreement are met.\n\nSource code escrow agreements provide for the following:\n\nWhether a source code escrow agreement is entered into at all, and who bears its costs, is subject to agreement between the licensor and the licensee. Software license agreements often provide for a right of the licensee to demand that the source code be put into escrow, or to join an existing escrow agreement.\n\nBankruptcy laws may interfere with the execution of a source code escrow agreement, if the bankrupt licensor's creditors are legally entitled to seize the licensor's assets – including the code in escrow – upon bankruptcy, preventing the release of the code to the licensee.\n\nMuseums, archives and other GLAM organizations have begun to act as independent escrow agents due to growing digital obsolescence. Notable examples are the Internet Archive in 2007, the Library of Congress in 2006, ICHEG, Computer History Museum, or the MOMA.\n\nThere are also some cases where software communities act as escrow agent, for instance for \"Wing Commander\" video game series or Ultima 9 of the Ultima series.\n\nThe escrow agreements described above are most applicable to custom-developed software which is not available to the general public. In some cases, source code for commercial off-the-shelf software may be deposited into escrow to be released as free and open-source software under an open source license when the original developer ceases development and/or when certain fundraising conditions are met (the threshold pledge system).\n\nFor instance, the Blender graphics suite was released in this way following the bankruptcy of Not a Number Technologies; the widely used Qt toolkit is covered by a source code escrow agreement secured by the \"KDE Free Qt Foundation\".\n\nThere are many cases of end-of-life open-sourcing which allow the community continued self-support, see List of commercial video games with later released source code and List of commercial software with available source code.\n\n\n"}
{"id": "42307071", "url": "https://en.wikipedia.org/wiki?curid=42307071", "title": "Starch mogul", "text": "Starch mogul\n\nA starch mogul is a machine that makes shaped candies or candy centers from syrups or gels, such as gummi candy. These softer candies and centers are made by filling a tray with cornstarch, stamping the desired shape into the starch, and then pouring the filling or gel into the holes made by the stamp. When the candies have set, they are removed from the trays and the starch is recycled.\n\nStarch moguls were invented around 1899 and were in common use within a decade or two. Early ones were built from wood, but later ones were made of steel.\n\nStarch moguls reduced the number of jobs in candy factories and thereby lowered production costs for candies. All of the steps were previously performed by hand.\n\nStarch moguls also improved worker safety. Previously, starch rooms tended to have dangerously high levels of starch in the air. Workers would breathe this and develop respiratory illnesses. By reducing the amount of combustible starch in the air, the machines also significantly reduced the risk of dust explosions and fire.\n\n"}
{"id": "480550", "url": "https://en.wikipedia.org/wiki?curid=480550", "title": "Thali", "text": "Thali\n\nThali (Hindi/Nepali: , , pronounced \"Thattu\"; meaning \"plate\") is the Indian name for a round platter used to serve food. Thali is also used to refer to an Indian-style meal made up of a selection of various dishes which are served on a platter. The 'thali' style meal serving is popular in India, Nepal, Bangladesh, Fiji, Sri Lanka, Mauritius, Malaysia and Singapore.\n\nAs noted by INTACH, the earliest evidence of use of continuity in cooking and food habits of India can be established by the existence of tandoor (cooking oven), thali, lotas and chakla-belan for making chapatis found in excavations at Indus Valley Civilization site of Kalibangan (3500 BCE – 2500 BCE).\n\n\"Thali\" refer to a metal plate that thali meal may be served on. Thali is popular in Punjab The idea behind a Thali is to offer all the 6 different flavors of sweet, salt, bitter, sour, astringent and spicy on one single plate (technically the last two are actually forms of chemesthesis rather than true flavors). According to Indian food custom, a proper meal should be a perfect balance of all these 6 flavors. Restaurants typically offer a choice of vegetarian or meat-based thalis. Vegetarian thalis are very typical and commonplace in Tamil Nadu canteens (and South India in general), and are a popular lunch choice.\n\nDishes served in a Thali vary from region to region in South Asia and are usually served in small bowls, called \"katori\" in India. These 'katoris' are placed along the edge of the round tray – the actual thali: sometimes a steel tray with multiple compartments is used. Typical dishes include rice, dal, vegetables, roti, papad, dahi (yogurt), small amounts of chutney or pickle, and a sweet dish to top it off. Rice or Roti is the usual main dish which occupies the central portion of the Thali, while the side dishes like vegetable curries and other aforementioned delicacies are lined circularly along the round Thali. Depending on the restaurant or the region, the thali consists of delicacies native to that region. In general, a thali begins with different types of breads such as puris or chapatis (rotis) and different vegetarian specialities (curries). However, in South India, rice is the only staple served with thalis. Thalis are sometimes referred to by the regional characteristic of the dishes they contain. For example, one may encounter Nepalese thali, Rajasthani thali, Gujarati thali and Maharashtrian thali. In many parts of India and Nepal, the bread and the rice portions are not served together in the thali. Typically, the bread is offered first with rice being served afterwards, often in a separate bowl or dish. \n\nUnlimited thalis are those that come with limitless refills. Kunal Vijaykar considers an unlimited thali as quintessentially Indian, not just for variety or limitlessness, but because it is true to Indian tradition.\n\n"}
{"id": "27878563", "url": "https://en.wikipedia.org/wiki?curid=27878563", "title": "Top Level Group", "text": "Top Level Group\n\nThe Top Level Group of UK Parliamentarians for Multilateral Nuclear Disarmament and Non-proliferation (TLG) is a cross-party parliamentary group in the United Kingdom, whose primary focus is the advancement of the nuclear disarmament and non-proliferation agenda in Britain and internationally. It is formed of almost all the former senior Ministers of foreign affairs and defence over the last two decades and includes former Chiefs of Defence and two former NATO Secretary Generals.\n\nThe Rt Hon Des Browne, former Defence Secretary, was the first convenor of the TLG, which is now led by Alistair Burt MP. The Top Level Group is administered and supported by the European Leadership Network.\n\nThe group was established in October 2009, in response to, and in tandem with, the growing international prominence of the issue of nuclear disarmament and non-proliferation. This momentum has been driven by former US Secretaries of State Henry Kissinger and George Shultz, former Defence Secretary William Perry and former Senator Sam Nunn in their Wall Street Journal article of January 2007, (which has since evolved into the Nuclear Security Project), as well as by U.S. President Barack Obama's Prague speech in April 2009, in which he outlined his vision of a nuclear-free world.\n\nThe members of the group are as follows:\n\nAccording to the Top Level Group website, the aims of the group are:\n\nThe members of the group are involved in a variety of activities, engaging in parliamentary work, media contributions, and in various relevant NGO conferences.\n\nIn February 2010, the group met with David Miliband MP, then Foreign Secretary, to discuss the issue of nuclear disarmament and non-proliferation, particularly as it related to the Nuclear Non-Proliferation Treaty Review Conference later in the year.\n\nOn 14 April 2010, the group produced, signed, and arranged for the signatures of 41 senior European figures for a statement published as an open letter on the Guardian online website. This highlighted the world’s growing nuclear dangers and called for greater international efforts to address them.\n\nDes Browne, chair of the group, led delegations to Washington and to Moscow in March and April respectively.\n\nVarious members of the group have taken part in video interviews, produced by Talkworks Films, regarding the nuclear options and dangers ahead.\n\nThe Top Level Group also maintains a website which provides updates for the activities of its members, and which also monitors and publishes parliamentary proceedings relating to nuclear non-proliferation and disarmament.\n\n"}
{"id": "50722791", "url": "https://en.wikipedia.org/wiki?curid=50722791", "title": "Trace3", "text": "Trace3\n\nTrace3, Inc. is an Irvine, CA-based Information technology (IT) company and managed service provider (MSP). The company provides IT Operations Analytics, cloud computing, cybersecurity, machine learning, artificial intelligence (AI), big data intelligence, Internet of Things (IoT) consulting and data center services for businesses.\n\nTrace3 was founded in 2002 by Hayes Drumwright (of POPin), as an IT systems value added reseller (VAR), and Drumwright served as its first CEO.\n\nBy 2013, the company's services included big data consulting, cloud computing and data center consulting.\n\nIn March 2014, the company launched a Venture Capital (VC) CIO Briefing program for clients, venture capitalists and technology entrepreneurs, to raise awareness of IT developments affecting businesses.\n\nIn August 2014, the company hired a new CEO, Tyler Beecher, and Drumwright became Executive Chairman.\n\nIn 2016, the company reported 2015 sales of $500M and 350 employees. That same year, the company reported growth in its Internet of Things (IoT) services. In December, the company launched a cybersecurity division.\n\nIn June 2017, the company reported a strategic investment from Miami, Florida-based investment firm H.I.G. Capital. Financial terms were not disclosed. In August, the company announced it was merging with Grand Rapids, Michigan-based IT company Data Strategy, another investment property of Trace3 investor H.I.G. Capital. The combined company reportedly had revenue of USD$1 billion.\n\nTrace3 provides IT Operations Analytics (ITOA), cloud computing, cybersecurity, machine learning, artificial intelligence, big data / data intelligence, and Internet of Things (IoT) consulting and data center services to US-based companies. The company's managed services group helps clients improve their IT-related areas.\n\nThe company also resells equipment from large computer and information technology vendors, including Cisco, EMC Corporation, NetApp, Symantec and VMWare.\n\nThe company's VC CIO Briefing program educates clients about the latest technologies, gives venture capitalists input into technology roadmaps, and provides customer feedback to technology companies.\n\nThe company also has a research group which studies startups for its clients, tracking around 500 at a time, to help the customers identify and take advantage of new IT innovations.\n\nIn addition to its Orange County headquarters in Irvine, CA, the company has sales offices across the United States.\n"}
{"id": "329145", "url": "https://en.wikipedia.org/wiki?curid=329145", "title": "True Names", "text": "True Names\n\nTrue Names is a 1981 science fiction novella by Vernor Vinge, a seminal work of the cyberpunk genre. It is one of the earliest stories to present a fully fleshed-out concept of cyberspace, which would later be central to cyberpunk. The story also contains elements of transhumanism, anarchism, and even hints about The Singularity.\n\n\"True Names\" first brought Vinge to prominence as a science fiction writer. It also inspired many real-life hackers and computer scientists; a 2001 book about the novella, \"True Names: And the Opening of the Cyberspace Frontier\", included essays by Danny Hillis, Marvin Minsky, Mark Pesce, Richard Stallman and others. It was awarded the Prometheus Hall of Fame Award in 2007.\n\nThe story follows the progress of a group of computer hackers (called \"warlocks\" in the story) who are early adopters of a new full-immersion virtual reality technology, called the \"Other Plane\". Warlocks penetrate computers around the world for personal profit or curiosity. Forming a cabal, they must keep their true identities—their \"True Names\"—secret even to each other and to the \"Great Adversary\", the United States government, as those who know a warlock's True Name can force him to work on their behalf, or even cause a \"True Death\" by killing the warlock in real life.\n\nThe protagonist is a warlock known as \"Mr. Slippery\" in the Other Plane. The government learns Mr. Slippery's True Name—Roger Pollack, a holonovelist in Arcata, California—and forces him to investigate the Mailman, a mysterious new warlock which it suspects of conducting a large-scale subversion of databases and networks. The Mailman has been recruiting others, such as the warlock DON.MAC, by promising great power in the real world, and claims to be responsible for a recent revolution in Venezuela. Because he never appears in the Other Plane, preferring non real-time communication, Mr. Slippery and fellow warlock Erythrina begin to suspect that the Mailman may be an extraterrestrial invader, subverting global databases to gradually conquer the Earth while causing True Deaths of the warlocks he recruits.\n\nMr. Slippery and Erythrina receive permission from the government to use the old Arpanet to access massive amounts of computational power around the world as they search for the Mailman. As they become the most powerful warlocks in history they realize that DON.MAC is a sophisticated \"personality simulator\" working for the Mailman. It violently defends itself, and both sides use network connections to military weaponry to attack in the real world. Erythrina is forced to reveal her True Name to Mr. Slippery as the battles, real and virtual, cause global chaos. They succeed in destroying the many copies of the Mailman's AI, and although tempted to keep their power over the world realize that they do not wish to be tyrants.\n\nTen weeks after the war and resulting worldwide economic depression from the disruption in computer systems, Mr. Slippery returns to the Coven and learns that the Mailman may have survived. Fearing that Erythrina succumbed to temptation for power, Pollack visits her—Debbie Charteris of Providence, Rhode Island—in person. The elderly Charteris, an early military computer programmer, reveals that the Mailman was not an extraterrestrial, but a National Security Agency AI research project to protect government systems. Mistakenly left running, it slowly grew in power and sophistication, and used non real-time communication to disguise its inability to fully emulate the human mind. As Charteris succumbs to senility she transfers more of her personality to the defeated Mailman's kernel, and tells Pollack that \"when this body dies, I will still be, and you can still talk to me\".\n\nMinsky's afterword reviews ideas from his Society of Mind concept: the idea \"that there is, inside the cranium, perhaps as many as a hundred different kinds of computers, each with a somewhat different basic architecture\", which specialize in different tasks and communicate, though perhaps only crudely. Minsky considers our conscious minds to be higher-level executives, which don't really understand the inner workings of the subcomponents but rather select \"simple names from menu-lists of symbols which appear, from time to time, upon our mental screen-displays.\" Tying this back to virtual reality, Minsky suggests that \"we, ourselves, already exist as processes imprisoned in machines inside machines! Our mental worlds are already filled with wondrous, magical, symbol–signs, which add to every thing we 'see' its ‘meaning’ and ‘significance’.\"\n\n"}
{"id": "3329675", "url": "https://en.wikipedia.org/wiki?curid=3329675", "title": "Waterleaf (architecture)", "text": "Waterleaf (architecture)\n\nIn architecture, a waterleaf is a distinctive sculptural motif used on the capitals of columns and pillars in European buildings during the late twelfth century. It is a highly simplified plant motif, characteristic of the \"late Norman\" style of Romanesque architecture. \n\nA waterleaf capital is formed of broad, smooth leaf-shapes (typically four in number), unribbed except for a central fold, which curve upward and outward before curling over at the tips where they meet the abacus (the flat slab at the top of the column, normally square but sometimes octagonal). The curled tip of the waterleaf may be small and neat or large and bulbous; it usually curves inward towards the abacus, but may occasionally turn outwards (both forms can sometimes be seen in adjacent capitals of the same period, as for example at Geddington, Northamptonshire, UK.).\n\n"}
{"id": "9655935", "url": "https://en.wikipedia.org/wiki?curid=9655935", "title": "XMPP Standards Foundation", "text": "XMPP Standards Foundation\n\nXMPP Standards Foundation (XSF) is the foundation in charge of the standardization of the protocol extensions of XMPP, the open standard of instant messaging and presence of the IETF.\n\nThe XSF was originally called the Jabber Software Foundation (JSF). The Jabber Software Foundation was originally established to provide an independent, non-profit, legal entity to support the development community around Jabber technologies (and later XMPP). Originally its main focus was on developing JOSL, the Jabber Open Source License (since deprecated), and an open standards process for documenting the protocols used in the Jabber/XMPP developer community. Its founders included Michael Bauer and Peter Saint-Andre.\n\nMembers of the XSF vote on acceptance of new members, a technical Council, and a Board of Directors. However, membership is not required to publish, view, or comment on the standards that it promulgates. The unit of work at the XSF is the XMPP Extension Protocol (XEP); XEP-0001 specifies the process for XEPs to be accepted by the community. Most of the work of the XSF takes place on the XMPP Extension Discussion List and the jdev Chat Room (xmpp:jdev@conference.jabber.org?join).\n\nThe Board of Directors of the XMPP Standards Foundation oversees the business affairs of the organization. As elected by the XSF membership, the Board of Directors for 2018-2019 consists of the following individuals:\n\nThe XMPP Council is the technical steering group that approves XMPP Extension Protocols, as governed by the XSF Bylaws and XEP-0001. The Council is elected by the members of the XMPP Standards Foundation each year in September. The XMPP Council (2018–2019) consists of the following individuals:\n\nThere are currently 66 elected members of the XSF.\n\nThe following individuals are emeritus members of the XMPP Standards Foundation:\n\nOne of the most important outputs of the XSF is a series of \"XEPs\", or XMPP Extension Protocols, auxiliary protocols defining additional features. Some have chosen to pronounce \"XEP\" as if it were spelled \"JEP\", rather than \"ZEP\", in order to keep with a sense of tradition. Some XEPs of note include:\n\nThe XSF biannually holds a XMPP Summit where software and protocol developers from all around the world meet and share ideas and discuss topics around the XMPP protocol and the XEPs. In winter it takes place around the FOSDEM event in Brussels, Belgium and in summer it takes place around the RealtimeConf event in Portland, USA. These meetings are open to anyone and focus on discussing both technical and non-technical issues that the XSF members wish to discuss with no costs attached for the participants. However the XSF is open to donations. The first XMPP Summit took place on July 24 and 25, 2006, in Portland.\n\n"}
