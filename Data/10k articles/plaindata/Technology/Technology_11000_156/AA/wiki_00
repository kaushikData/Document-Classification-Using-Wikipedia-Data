{"id": "30790895", "url": "https://en.wikipedia.org/wiki?curid=30790895", "title": "1,1'-Azobis-1,2,3-triazole", "text": "1,1'-Azobis-1,2,3-triazole\n\n1,1′-Azobis-1,2,3-triazole is a moderately explosive but comparatively stable chemical compound which contains a long continuous chain of nitrogen atoms, with an unbroken chain of eight nitrogen atoms cyclised into two 1,2,3-triazole rings. It is stable up to 194 °C. The compound exhibits cis–trans isomerism at the central azo group: the \"trans\" isomer is more stable and is yellow, while the \"cis\" isomer is less stable and is blue. The two rings are aromatic and form a conjugated system with the azo linkage. This chromophore allows the \"trans\" compound to be isomerised to the \"cis\" when treated with an appropriate wavelength of ultraviolet light. \nIn 2011 Azobis(tetrazole) was prepared by Klapötke and Piercey which has a ten-nitrogen chain. The record was later taken by a N11 chain compound synthesized by a group of Chinese researchers. A branched chain N11 system has also been reported as part of an unstable but highly nitrogen rich azidotetrazole derivative with formula | C=2 | N=14 .\n\n"}
{"id": "44778573", "url": "https://en.wikipedia.org/wiki?curid=44778573", "title": "Alffie", "text": "Alffie\n\nalffie is an Australian online training company headquartered in South Melbourne, Victoria. It currently provides nationally accredited vocational training in the areas of business, community services, hospitality, retail services and warehousing operations.\n\nFollowing the 2013 purchase of the online training company Leap eLearning (founded in 2010) by Tasmania-based face-to-face training company HR Company Pty Ltd (founded in 1997) the business then began trading as Leap Training. In 2014, former Australian rules footballer and Westpac executive Paul Licuria was appointed Chief Executive Officer.\n\nOn 9 July 2015 HR Company Pty Ltd was taken over by administrators and a majority of its staff where made redundant. On July 10, 2015 the Leap Training business was purchased by Training Online Australia Pty Ltd, who as an RTO continued operations under the Leap Training name until October 2015 when it was rebranded alffie.\n\nCertificates:\n\nIn addition to its accredited courses, the company also offers a number of non-accredited short courses.\n\n\n"}
{"id": "5157393", "url": "https://en.wikipedia.org/wiki?curid=5157393", "title": "Alkali-metal thermal to electric converter", "text": "Alkali-metal thermal to electric converter\n\nThe alkali-metal thermal-to-electric converter (AMTEC), originally called the sodium heat engine (SHE) was invented by Joseph T. Kummer and Neill Weber at Ford in 1966, and is described in US Patents , , and . It is a thermally regenerative electrochemical device for the direct conversion of heat to electrical energy. It is characterized by high potential efficiencies and no moving parts except the working fluid, which make it a candidate for space power applications.\n\nThis device accepts a heat input in a range 900–1300 K and produces direct current with predicted device efficiencies of 15–40%. In the AMTEC, sodium is driven around a closed thermodynamic cycle between a high-temperature heat reservoir and a cooler reservoir at the heat rejection temperature. The unique feature of the AMTEC cycle is that sodium ion conduction between a high-pressure or -activity region and a low-pressure or -activity region on either side of a highly ionically conducting refractory solid electrolyte is thermodynamically nearly equivalent to an isothermal expansion of sodium vapor between the same high and low pressures. Electrochemical oxidation of neutral sodium at the anode leads to sodium ions, which traverse the solid electrolyte, and electrons, which travel from the anode through an external circuit, where they perform electrical work, to the low-pressure cathode, where they recombine with the ions to produce low-pressure sodium gas. The sodium gas generated at the cathode then travels to a condenser at the heat-rejection temperature of perhaps 400–700 K, where liquid sodium reforms. The AMTEC thus is an electrochemical concentration cell, which converts the work generated by expansion of sodium vapor directly into electric power.\n\nThe converter is based on the electrolyte used in the sodium–sulfur battery, sodium beta″-alumina, a crystalline phase of somewhat variable composition containing aluminum oxide, AlO, and sodium oxide, NaO, in a nominal ratio of 5:1, and a small amount of the oxide of a small-cation metal, usually lithium or magnesium, which stabilizes the beta″ crystal structure. The sodium beta″-alumina solid electrolyte (BASE) ceramic is nearly insulating with respect to transport of electrons and is a thermodynamically stable phase in contact with both liquid sodium and sodium at low pressure.\n\nSingle-cell AMTECs with open voltages as high as 1.55 V and maximum power density as high as 0.50 W/cm at temperature of 1173 K (900 °C) have been obtained with long-term stable refractory metal electrodes.\n\nEfficiency of AMTEC cells has reached 16% in the laboratory. High-voltage multi-tube modules are predicted to have 20–25% efficiency, and power densities up to 0.2 kW/l appear to be achievable in the near future. Calculations show that replacing sodium with a potassium working fluid increases the peak efficiency from 28% to 31% at 1100 K with a 1 mm thick BASE tube.\n\nMost work on AMTECs has concerned sodium working fluid devices. Potassium AMTECs have been run with potassium beta″-alumina solid electrolyte ceramics and show improved power at lower operating temperatures compared to sodium AMTECs.\n\nA detailed quantitative model of the mass transport and intefacial kinetics behavior of AMTEC electrodes has been developed and used to fit and analyze the performance of a wide variety of electrodes, and to make predictions of the performance of optimized electrodes. The interfacial electrochemical kinetics can be further described quantitatively with a tunneling, diffusion, and desorption model. A reversible thermodynamic cycle for AMTEC shows that it is, at best, slightly less efficient than a Carnot cycle.\n\nAMTEC requires energy input at modest elevated temperatures and thus is easily adapted to any heat source, including radioisotope, concentrated solar, external combustion, or nuclear reactor. A solar thermal power conversion system based on an AMTEC has advantages over other technologies (including photovoltaic systems) in terms of the total power that can be achieved with such a system and the simplicity of the system (which includes the collector, energy storage (thermal storage with phase-change material) and power conversion in a compact unit). The overall system could achieve as high as 14 W/kg with present collector technology and future AMTEC conversion efficiencies. The energy storage system outperforms batteries, and the temperatures at which the system operates allows long life and reduced radiator size (heat-reject temperature of 600 K). Deep-space applications would use radioisotope thermoelectric generators; hybrid systems are in design.\n\nWhile space power systems are of intrinsic interest, terrestrial applications will offer large-scale applications for AMTEC systems. At the 25% efficiency projected for the device and projected costs of 350 USD/kW, AMTEC is expected to prove useful for a very wide variety of distributed generation applications including self-powered fans for high-efficiency furnaces and water heaters and recreational vehicle power supplies, cathodic protection of pipelines, remote telemetry from oil well sites are other areas where this type of electrical generation might be used. The potential to scavenge waste heat may allow integration of this technology into general residential and commercial cogeneration schemes, although costs per kilowatt-hour would have to drop substantially from current projections.\n"}
{"id": "4216735", "url": "https://en.wikipedia.org/wiki?curid=4216735", "title": "Analyser", "text": "Analyser\n\nAn analyser or analyzer (see spelling differences) is a person or device that analyses given data. It examines in detail the structure of the given data and tries to find patterns and relationships between parts of the data. An analyser can be a piece of hardware or a computer program running on a computer.\n\nAn analyser can also be an instrument or device which conducts chemical analysis (or similar analysis) on samples or sample streams. Such samples consist of some type of matter such as solid, liquid, or gas. Many analysers perform such analyses automatically or mostly automatically, such as autoanalysers. \n\nThe analysis can be done on samples which the operator brings to the analyser or the analyser can be connected to the source of the samples and the sampling be done automatically. The source of samples for automatic sampling is commonly some kind of process such as a chemical process. Analysers which are connected to a process, and conduct automatic sampling, can be called online (or on-line) analysers or sometimes inline (or in-line) analysers. For inline analysis, a sensor can be placed in a process vessel or stream of flowing material to conduct the analysis. Another method of online analysis is allowing a sample stream to flow from the process equipment into an analyser, sometimes conditioning the sample stream in between such as reducing pressure or changing the sample temperature. Such sampling is typically for fluids (either liquids or gases). If the sample stream is not substantially modified by the analyser, it can be returned to the process. Otherwise, the sample stream is not returned; for example, if any reagents have been added for the analysis. \n\nThe pressure of a sample stream can be lowered by a pressure reducing valve, particularly since many analysers are not designed to withstand high pressure. Such pressure reducing or similar valves may be used to control the flow rate to the online analyser. If the process pressure is insufficient to allow a sample stream to flow by itself to the analyser, a small pump may be used to move it there. The temperature of a hot sample may be lowered by use of an online sample cooler. The sampling and analysis can be done at the command of an operator, periodically (for example, every 15 minutes), or continuously - providing an analysis result vs. time graph on a chart recorder, computer, or other device. For periodic sampling, valves (or other devices) can be switched open to allow a fluid sample stream to flow to the analyser and shut when not sampling. \n\nSome methods of inline analysis are so simple, such as electrical conductivity or pH, the instruments are usually not even called analysers. Salinity determined from simple online analysis is often determined from a conductivity measurement where the output signal is calibrated in terms of salinity concentration (for example ppm of NaCl). Various types of other analyses can be devised. Physical properties can include electrical conductivity (or effectively electrical resistivity), refractive index, and radioactivity measurement. Rather simple processes which use inline electrical conductivity determination are water purification processes which test how effectively salts have been removed from the output water. Variations of electrical conductivity measurement of solutions include cation and anion conductivity. Chromatography such as ion chromatography or HPLC often tests the output stream continuously by determination of electrical conductivity, particularly cation or anion conductivity, refractive index, colorimetry or ultraviolet/visible absorbance at a certain wavelength. Inline sensors for dissolved oxygen in water are available. There are online and offline analysers available for various other types of analytes. Many of these add reagents to the samples or sample streams.\n\n"}
{"id": "8387723", "url": "https://en.wikipedia.org/wiki?curid=8387723", "title": "Asif Azam Siddiqi", "text": "Asif Azam Siddiqi\n\nAsif Azam Siddiqi is a Bangladeshi American space historian and a Guggenheim Fellowship winner. He is a professor of history at Fordham University. He specializes in the history of science and technology and modern Russian history. He has written several books on the history of space exploration.\n\nSiddiqi was born to Hafiz G. A. Siddiqi, Vice-Chancellor of North South University in Dhaka and Najma Siddiqi, a retired professor of philosophy at Jahangirnagar University. Siddiqi received his bachelor's and master's from Texas A&M University. He then completed his M.B.A. from the University of Massachusetts-Amherst and received his Ph.D. from Carnegie Mellon University in 2004 under a National Science Foundation Fellowship to study Cold War science and technologies.\n\nSiddiqi's first book \"Challenge to Apollo: The Soviet Union and the Space Race, 1945-1974\" is widely considered to be the best English-language history of the Soviet space program in print and was identified by the \"Wall Street Journal\" as \"one of the five best books\" on space exploration. This book was later published in paperback in two separate volumes, \"Sputnik and the Soviet Space Challenge\" and \"The Soviet Space Race with Apollo\".\n\nSiddiqi's major contribution to space history scholarship has been to apply academic training, theory, and methodology to the study of Soviet space program history. Siddiqi utilized newly available archival materials from Russia, published works such as memoirs, and other sources and essentially pioneered Soviet space history scholarship in the post Cold War-era. Even current Russian-based space history tends to rely heavily upon memoirs and as a result, Siddiqi is acknowledged by Russian space officials as one of the few people conducting original archival research on the subject worldwide. His articles have been published in the leading Russian space journal \"Novosti kosmonavtiki\" (News of Cosmonautics) as well as the official history journal of the Russian Academy of Sciences, \"Voprosy istorii estestvoznaniia i tekhniki\" (Problems in the History of Natural Sciences and Technology). He also regularly publishes pieces in the Moscow English language daily, Moscow Times.\n\nHe is a currently serving on the United States National Research Council's Committee on Human Spaceflight, tasked by Congress to evaluate and recommend options for the future of NASA's human spaceflight program.\n\nSiddiqi is also the editor of the series \"Rockets and People\" which are the four volume English-language translation of the memoirs of Boris Chertok, a leading designer who worked under Sergei Korolev. These volumes are being published by the NASA History Division.\n\nHe was featured in the NOVA WGBH-TV special \"Astrospies\" broadcast in 2008, and has been featured in various media as a specialist in the history of spaceflight.\n\nHe has received the American Historical Association's Fellowship in Aerospace history, the Eugene M. Emme Astronautical Literature Award, the History Manuscript Award from the American Institute of Aeronautics and Astronautics, and a National Science Foundation award for work on his Ph.D. dissertation.\n\nSiddiq's most recent book \"The Red Rockets' Glare: Spaceflight and the Soviet Imagination, 1857-1957\" (Cambridge, 2010) recovered the social and cultural roots of cosmic enthusiasm in the Russian context dating back to the 19th century. He has published widely in many different journals, including the \"Osiris\", \"Technology and Culture\", \"History and Technology\", \"Europe-Asia Studies\", \"Acta Astronautica\", Air & Space, \"Journal of the British Interplanetary Society\", and \"Spaceflight.\"\n\n"}
{"id": "48324366", "url": "https://en.wikipedia.org/wiki?curid=48324366", "title": "Batteroo Boost", "text": "Batteroo Boost\n\nThe Batteroo Boost (formerly known as the Batteriser ( )) is a line of products designed by Batteroo, Inc. that is claimed to significantly extend battery life by using a miniature boost voltage regulator. It was crowd-funded on Indiegogo. The company is based in Sunnyvale, California and founded by Bob Roohparvar, the previous VP of design in Broadcom and Frankie Roohparvar, the previous CTO of Sandisk in 2012.\n\nA patent was filed by Fariborz Frankie Roohparvar with the priority date of September 20, 2010. The Batteroo Boost is claimed to extend the life of both new and used batteries. Batteroo has said that Batterisers are non-toxic, reusable, and coated with a non-conductive coating to prevent any risk of shorts.They also claim that a built-in reverse polarity protection mechanism eliminates dangers of inserting a battery into the Batteriser the wrong way.\n\nCrowdfunding completed between July 2015 produced $394,459, while the initial goal was $30,000. During the crowdfunding Batteroo announced they would be producing Batteroo Boost for AA, AAA, C, and D batteries. Batteroo also received VC funding of at least $5,000,000 from SK Telecom and Forte Ventures, in July 2014. In August 2017, Batteroo launched a second crowd-funding campaign for a line of products for rechargeable batteries called Batteroo Reboost. In this crowd-funding campaign, they raised an additional $42,311.\n\nThe shipping date for the product has been delayed for various reasons, but photos from the manufacturing process have been made available. As of early May 2016, the company was months overdue to ship to its Indiegogo backers, with some backers accusing Batteroo of running a scam.\n\nSan Jose State University's Kiumars Parvin claimed, \"We tested the Batteriser sleeve in our lab and we confirmed that the Batteriser taps into 80 percent of energy that is usually thrown away.\" \n\nIn the test by UL (safety organization), a Garmin Golf GPS using Batteroo Boost was shown to have a lifespan of 10 hours and 12 minutes, in contrast to the 1 hour and 43 minutes of operating time without a Batteroo Boost. However, later the test was independently duplicated by TechnologyCatalyst to demonstrate that Garmin runs OK for over 17 hours on ordinary AA batteries, and the report by UL was based on the sloppy test design.\n\nPC World's Jon Phillips demoed the Batteroo Boost operating on 'dead' batteries in an Apple Inc. keyboard. The 'power meter' on the computer's screen showed the batteries as being dead without the Batteroo Boost, and as having 100% power remaining with the Batteriser. Brian Dipert at EDN called into question the strain on the keyboard being caused by the 'power meter,' and suggested that this test might not be representative of the Batteroo Boost's effectiveness in other applications.\n\nThe Batteriser's efficacy in consumer applications has been challenged by a number of sources.\n\nA source of contention surrounds the brownout voltages for battery-operated devices. David L. Jones in his EEVBlog used a programmable power supply to determine that nearly all devices function in some respect until around 1.1V, or roughly 80% of a battery's life due to the non-linear discharge curve of batteries. This stands in contrast to Batteroo's claim that using a Batteroo Boost will unlock the remaining 80% of power (from 1.3V downwards). Batteroo has counter-argued that the bench power supply test is flawed, because of the definitions used by Jones to define device functionality, the inherent differences between power supplies and batteries on the basis of Equivalent Series Resistance (ESR), and different measures of voltage (confusion between open circuit voltage and closed circuit voltage).\n\nA further source of controversy is whether or not the Batteroo Boost may shorten battery life in devices that undergo only intermittent use, because the Batteriser is always drawing power to boost the voltage, even when the device is idle.\n\nRoohparvar noted that many critics have drawn conclusions without actually having tried the product.\n\nThe first devices were delivered at the end of 2016. Frank Buss, and later on, Dave Jones, concluded in a first test that the device is not efficient when used in an electronically-unregulated toy train.\n\nDavid Jones on EEV Blog raised the concern that because the Batteroo Boost acts as a ground for the boost converter circuit, any nick in the insulation might result in a direct short, and potentially a fire.\n\nIn the wake of Dave Jones' video about Batteriser, his video was disliked by a torrent of IP addresses located in Vietnam. Other bloggers with Batteroo Boost-related videos experienced similar activity from addresses in Vietnam. The bloggers involved suspect that either a click farm in Vietnam was engaged to disrepute those attacking Batteroo Boost, or a single computer with many fake or stolen YouTube accounts utilized proxied IP addresses to cover its tracks. Due to the anonymous nature of the attacks, it is currently unknown who was responsible.\n\nOn July 25, 2016, Energizer Brands LLC filed a federal lawsuit, saying that the name Batteriser violates a variety of its trademarks on the word \"energizer\". The lawsuit said that \"... despite advertisements, solicitation, and pre-orders, Batteroo has not delivered a single Batteriser product to a consumer in the ordinary course of business.\" According to the lawsuit, the Trademark Trial and Appeal Board ruled June 27 in favor of Energizer and refused registration of the Batteriser and Batterise marks. According to Energizer, Batteroo also tried to falsely implicate Energizer in the product delays of Batteriser.\n"}
{"id": "39141747", "url": "https://en.wikipedia.org/wiki?curid=39141747", "title": "Bio fireplace", "text": "Bio fireplace\n\nA bio fireplace (also on bio-ethanol fireplace, ethanol fireplace) is a type of fireplace or furnace with combined zones generation of heat and technological process – combustion of fuel (denatured alcohol). This fireplace can be installed without a chimney and gives the real flame, not imitation. The main part of each bio fireplace is the burner. The burner is a metal container with various shape and dimensions. The burner is filled with fuel, usually it is bioethanol. Fuel is poured into the burner and lit with an extended candle lighter as suggested. Bio fireplaces should be operated with care, since bioethanol is highly flammable. Severe burn accidents can happen. Bioethanol-fueled decorative fireplaces are dangerous even by intended use. Gases from combustion, like carbon dioxide and nitrogen dioxide, volatile organic compounds, and particulate emissions are released into the room. Bio fireplace - irrespective of the type of fuel used are a source of fine and ultrafine particles and have a considerable influence on the quality of the indoor air if ventilation is not provided. It may generate odors.\n\nEthanol fireplaces are available in several different designs.There are manual ethanol fireplaces and automatic ethanol fireplaces. Automatic ethanol fireplace flames have no direct contact with fuel. Fuel is in stored in a reservoir and then heated until the alcohol vapors evaporate into the burner. These vapors are then ignited by a spark system in the unit. User can turn the flames on or off as well as regulate the flame size with a remote control, mobile app or smart home system. A microprocessor controls burning process, using numerous sensors to keep burning parameters stable. If the sensors detect any issues- such as an earth quake, low oxygen or excessively high temperatures- they will extinguish the flame. Wall mounted designs can be built into a drywall wall or be recessed into the wall. Free standing or stand alone ethanol fireplaces are portable, and can be used in any architectural setting. Table top ethanol fireplaces are the smallest versions of ethanol fireplaces and are often used as a decoration.\n\n"}
{"id": "27387674", "url": "https://en.wikipedia.org/wiki?curid=27387674", "title": "Blowmolding machine", "text": "Blowmolding machine\n\nA blowmolding machine is widely used in commercial beverage production. A blowmolding machine creates a plastic bottle according to a recipe, for example specifying the capacity of the bottle to be made. The machine consists of molds, a programmable logic controller, and mechanical and electronic instruments.\n\nFor more information about the process, refer to Blow molding.\n"}
{"id": "859798", "url": "https://en.wikipedia.org/wiki?curid=859798", "title": "Bottled gas", "text": "Bottled gas\n\nBottled gas is a term used for substances which are gaseous at standard temperature and pressure (STP) and have been compressed and stored in carbon steel, stainless steel, aluminum, or composite bottles known as gas cylinders.\n\nThere are four cases: either the substance remains a gas at standard temperature but increased pressure, the substance liquefies at standard temperature but increased pressure, the substance is dissolved in a solvent, or the substance is liquefied at reduced temperature and increased pressure. In the last case the bottle is constructed with an inner and outer shell separated by a vacuum (dewar flask) so that the low temperature can be maintained by evaporative cooling.\n\nThe substance remains a \"gas\" at \"standard temperature\" and \"increased pressure\", its critical temperature being below standard temperature. Examples include:\n\nThe substance \"liquefies\" at \"standard temperature\" but \"increased pressure\". Examples include:\n\nThe substance is \"dissolved\" at \"standard temperature\" in a solvent. Examples include:\n\nThe substance is \"liquefied\" at \"reduced temperature\" and \"increased pressure\". These are also referred to as cryogenic gases. Examples include:\n\nThe general rule is that one unit volume of liquid will expand to approximately 800 unit volumes of gas at Standard temperature and pressure with some variation due to intermolecular force and molecule size compared to an ideal gas. Normal high pressure gas cylinders or bottles will hold from 200 to 400 atmosphere (unit)s. The atmosphere units pressure held by the bottle is equivalent to the number of volumes of standard temperature and pressure of the gas held by the bottle for an ideal gas.\n\nBecause the contents are under high pressure and are sometimes hazardous, there are special safety regulations for handling bottled gases. These include chaining bottles to prevent falling and breaking, proper ventilation to prevent injury or death in case of leaks and signage to indicate the potential hazards.\n\nIn the United States, the Compressed Gas Association (CGA) sells a number of booklets and pamphlets on safe handling and use of bottled gases. (Members of the CGA can get the pamphlets for free.) The European Industrial Gases Association and the British Compressed Gases Association provide similar facilities in Europe and the United Kingdom.\n\nIn the United States, 'bottled gas' typically refers to liquefied petroleum gas. 'Bottled gas' is sometimes used in medical supply, especially for portable oxygen tanks. Packaged industrial gases are frequently called 'cylinder gas', though 'bottled gas' is sometimes used.\n\nThe United Kingdom and other parts of Europe more commonly refer to 'bottled gas' when discussing any usage whether industrial, medical or liquefied petroleum. However, in contrast, what the United States calls liquefied petroleum gas is known generically in the United Kingdom as 'LPG'; and it may be ordered using by one of several Trade names, or specifically as butane or propane depending on the required heat output.\n\nDifferent countries have different gas colour codes but attempts are being made to standardise the colours of cylinder shoulders:\n\nThe user should not rely on the colour of a cylinder to indicate what it contains. The label or decal should always be checked for product identification.\n\nThe colours below are specific shades, defined in the European Standard in terms of RAL coordinates. The requirements are based on a combination of a few named gases, otherwise on the primary hazard associated with the gas contents:\n\nDiving cylinders are left unpainted (for aluminium), or painted to prevent corrosion (for steel), often in bright colors, most often fluorescent yellow, to increase visibility. This should not be confused with industrial gases, where a yellow shoulder means chlorine.\n\n\n"}
{"id": "30146163", "url": "https://en.wikipedia.org/wiki?curid=30146163", "title": "Butter curler", "text": "Butter curler\n\nA butter curler is a kitchen tool designed to produce decorative butter shapes for use in food decoration. It can also be used to make chocolate and wax shavings. In typical use, the material to be cut is chilled slightly while the curler is dipped into hot water to ease the cutting.\n"}
{"id": "17577108", "url": "https://en.wikipedia.org/wiki?curid=17577108", "title": "CPU shielding", "text": "CPU shielding\n\nCPU shielding is a practice where on a multiprocessor system or on a CPU with multiple cores, real-time tasks can run on one CPU or core while non-real-time tasks run on another.\n\nThe operating system must be able to set a CPU affinity for both processes and interrupts.\n\nIn Linux in order to shield CPUs from individual interrupts being serviced on them you have to make sure that the following kernel configuration parameter is set:\n\n\n"}
{"id": "35635771", "url": "https://en.wikipedia.org/wiki?curid=35635771", "title": "Catalytic heater", "text": "Catalytic heater\n\nA catalytic heater is a type of heater which relies on catalyzed chemical reactions to break down molecules and produce heat. \n\nCatalytic heaters can be used wherever heat is required, producing heat at a lower, more controlled rate than unconstrained combustion. Examples of smaller and larger heaters are soldering irons, hand warmers (such as the Jon-E or Zippo) and space-heating appliances. Current small-scale space heaters are typically propane (LP) or butane fueled, whereas many older types used liquid fuel (naphtha, lighter fluid, Coleman camping fuel or \"white gas\" in the USA) or alcohol. Pocket-sized catalytic hand warmers have traditionally used naptha-type liquid fuel exclusively.\n\nCertain safety measures should be taken when using a catalytic heater. Catalytic heaters should be installed properly to prevent fuel leakage, in areas with good ventilation and away from combustibles.\n\n"}
{"id": "6987714", "url": "https://en.wikipedia.org/wiki?curid=6987714", "title": "Cebu IT Park", "text": "Cebu IT Park\n\nThe Cebu IT Park (formerly known as Asiatown IT Park) is a 27-hectare mixed use business park in Cebu City, Philippines, envisioned to attract locators in the information technology services. It is developed by \"Cebu Property Ventures and Development Corporation\", a subsidiary of Cebu Holdings, Inc.\n\nTenants include Cebu Bombardier, NEC, SPI Tech, 1&1 Internet Philippines, Inc., Aegis (now acquired by Teleperformance), Convergys (who later acquired both eTelecare and Stream), Qualfon, Promotional USB, Accenture, NCR, IBM, Microsoft, Xlibris/Author Solutions, JP Morgan Chase, and Epson. The main infrastructures found at the park are i1, i2, i3, E-BLOC, TGU Tower, PIPC 15, Skyrise 3, Skyrise 2, Skyrise 1, E-Office One, PIPC 11, PIPC 14, Globe IT, Aegis Tower, the CJRS and The Central Bloc.\n\nIn January 2010, IBM inaugurated its 2nd Global Delivery Center at TGU Tower. IBM established its initial presence in the Philippines in 1937. In 2007, IBM partnered with the Philippines Department of Science and Technology on the Philippine Intellectual Property Policy Strategy, Engineering Research & Development for Technology Program, and the National Technology Business Incubators Program. IBM Philippines Country General Manager James Velasquez said the company recognizes Cebu as the gateway both for its domestic clients in the Visayas and Mindanao, as well as overseas clients.\n\nIt was approved by the Philippine Economic Zone Authority (PEZA) board as an economic zone on April 6, 2000. On February 27, 2001, Presidential Proclamation No. 12 made it an Information Technology Special Economic Zone.\n\nConstruction on eOffice One, the first office modules in Cebu IT Park, began in 2001 and opened in 2002.\n\n\n"}
{"id": "20294329", "url": "https://en.wikipedia.org/wiki?curid=20294329", "title": "Center channel", "text": "Center channel\n\nCenter channel refers to an audio channel common to many surround sound formats. It is the channel that is mostly, or fully, dedicated to the reproduction of the dialogue of an audiovisual program. The speaker(s) connected to the center channel are placed in the center of and behind the perforated projection screen, to give the effect that sounds from the center channel are coming from the screen. In many home surround sound units, the center channel is positioned above or below the video screen.\n\nIn the post-production process of filmmaking and video production sound editing, dialogue can be mapped to other speakers when story action and direction require it, such as when the person talking is off-screen, but it is rare that there is vocal content that is completely absent from the center channel.\n\nIn material without accompanying visuals (e.g. music), the center channel simply reproduces sound intended to come from immediately in front of the listener, which usually includes the lead vocals, which are rarely panned hard left or right. \n\nThe center channel also anchors the sound field, eliminating phantom images such as those that plagued quadraphonic sound if the speakers were not precisely placed.\n\nThe center channel eliminates the need of creating a phantom center with left and right stereo speakers. The center channel provides image stabling effects and is considered the most important channel for film production.\n\nThe need for a center speaker to locate screen-centered sounds has been recognised since the Bell labs experiments in stereo sound from the 1930s, and multi-channel cinema sound systems, starting with the first commercial stereophonic film (Fantasia-1941) have always included one. Post-war stereo sound in theaters initially came from separate magnetic film reproducers synchronised to the picture, but in the 1950s systems using magnetic stripes on the film itself came into use. Cinemascope used four such tracks (left, center, right and surround), and the subsequent Todd-AO 70mm system used six (left, left-center, center, right-center and right, plus a single surround channel). Unfortunately these magnetic systems were not only very expensive, but were also unreliable and so were little used, the industry preferring to stay with the tried, tested (and cheap) mono optical track.\n\nDolby Stereo was introduced by Dolby Laboratories in 1975. It divided the existing soundtrack area of a 35mm film print into two, allowing a two-channel recording. Each of these two channels used Dolby A type noise reduction (later replaced by Dolby SR type). In addition a matrix, similar in principle to those used for the existing matrix-type quadraphonic systems, allowed the audio for left, center and right speakers, plus a single surround channel to be carried by the two tracks. Thus Dolby Stereo provided a similar stereo performance to that previously only available in the cinema by the magnetic tracks on 4-track Cinemascope or 6-track Todd-AO (70mm) formats, but at far lower cost. \n\nIn recent years digital multi-channel sound systems, such as Dolby Digital and DTS, have become available which provide 6 or 8 discrete audio channels providing for not only the usual three screen speakers but also 2 or 4 groups of surround speakers and a sub-woofer.\n\nMany of the home theater units have a \"center focus\" or \"dialog enhancement\" option that provides options for the dialogue reproduction, as well as the overall content mapped to the center and front channels. Common setting include modes that map dialogue strictly to the center channel (to the best ability of the decoder), modes that emphasize vocals for clear dialogue—and modes that mix the center and front channels, mapping some vocals to the front channels, and some non-vocal audio content to the center channel. It may also simply raise the volume level of the center channel. processing may include \"Dialog Control\", the ability to isolate and control dialog levels independent of other ambient noises.\n\n6.1 channel surround systems such as Dolby Pro Logic IIx, Dolby Digital EX and DTS-ES use a single rear surround channel in addition to the traditional left and right surround channels.\n"}
{"id": "25543430", "url": "https://en.wikipedia.org/wiki?curid=25543430", "title": "Centurion Reactor", "text": "Centurion Reactor\n\nThe term \"Centurion Reactor\" refers to a future class of commercial nuclear power reactors designed for, and licensed to operate for periods of time of one hundred years or longer - thus the term \"centurion\". There currently are no Centurion Reactors operating in the world. This article provides a brief overview of the Centurion Reactor concept, the technical challenges associated with achieving such longevity, and some of the business and societal issues surrounding deployment of Centurion Reactors.\n\nCommercial nuclear power plants in the United States are currently licensed, as stipulated by the Atomic Energy Act, for operating lifetimes of no more forty years. In October, 2009, ninety-three of the 104 operating nuclear power plants in the U.S. had either been issued, had applied for, or had indicated they would apply for operating license extensions of twenty years. Thus the majority of the U.S. commercial nuclear power fleet will operate for sixty years or possibly longer.\n\nWeinberg noted the \"trend toward nuclear reactor immortality\", and advocated that \"longevity\" be a critical design criterion in future nuclear power plants. More recently, Greene has elaborated on the challenges of simply extending plant lifetimes to 100 years. The limits to nuclear power plant lifetime will be determined by both technical and economic considerations, and achieving such extended lifetimes will require innovative business and financial arrangement.\n\nCurrent-generation commercial nuclear power plants (so-called \"Gen III\" plants) typically produce electricity at a busbar cost of 2-3 cents per kilowatt-hr after the initial capital cost of the plant is amortized. The typical amortization period for a commercial nuclear power plant is twenty years. Thus a Centurion Reactor could theoretically produce electricity at a cost of a few cents per kilowatt-hr for eighty years or longer after the initial plant investment is recovered. The press to extend the operating lifetime of commercial power plants is driven by fundamental investment economics, land use considerations, and social justice considerations.\n\nFrom the investment economics perspective, the allure of an 80-year profit stream to the plant owner/operator is a great attraction. Land-use considerations are also a motivating factor.\n\nA qualified and licensed nuclear power plant site is an extremely valuable asset due to the limited number of such sites available to energy produces. The value of a site is directly related to the amount of energy that can be produced on that site. Other considerations aside, the value of a specific site is directly related to the operating lifetime of the power plants on that site.\n\nFinally, inter-generational justice considerations motivate for maximizing the value \"bequeathed\" to following generations in exchange for the responsibilities associated with long-term protection and management of legacy nuclear wastes and spent nuclear fuel.\n\nThe technical challenges associated with achieving Centurion Reactors lie principally in the realm of materials science. Current Gen-III nuclear plant operating lifetimes appear to be limited primarily by long-term radiation-induced ageing phenomena in the reactor pressure vessel, primary coolant system piping, concrete containment structures, and cabling (particularly medium-voltage power cables). In-situ replacement of all of these components and structures is problematic - the reactor pressure vessel being of particular significance in this regard. Thus it is anticipated that substantial materials research and development will be required to open the door to this new class of nuclear power reactors.\n\n"}
{"id": "53464494", "url": "https://en.wikipedia.org/wiki?curid=53464494", "title": "Chronocinematograph", "text": "Chronocinematograph\n\nChronocinematograph is an astronomical instrument consisting of a film camera, chronometer and chronograph. The device records images using a more precise timetable for observing an eclipse. It was invented in 1927 by a Polish astronomer, mathematician and geodesist Tadeusz Banachiewicz for observing total solar eclipses. During the same year, Banachiewcz used his device for solar observations in Lapland (Sweden), then in USA (1932) and Greece, Japan and Siberia (1936).\n\nThe invention enhanced the precision for determining the time of an eclipse, due to more precisely timed photos of Baily's beads, and quantifying the duration of totality. This could not have been observed as closely as before due to the brightness of the sun.\n"}
{"id": "36130575", "url": "https://en.wikipedia.org/wiki?curid=36130575", "title": "Classes of computers", "text": "Classes of computers\n\nComputers can be classified, or typed, in many ways. Some common classifications of digital computers are summarized below. For others see .\n\nMicrocomputers became the most common type of computer in the late 20th century. The term “microcomputer” was introduced with the advent of systems based on single chip microprocessors. The best-known early system was the Altair 8800, introduced in 1975. The term \"microcomputer\" has practically become an anachronism.\n\nThese computers include:\n\n\nSmaller microcomputers are also called mobile devices:\n\n\nMinicomputers (colloquially, minis) are a class of multi-user computers that lie in the middle range of the computing spectrum, in between the smallest mainframe computers and the largest single-user systems (microcomputers or personal computers). The term superminicomputer or supermini was used to distinguish more powerful minicomputers that approached mainframes in capability. Superminis (such as the DEC VAX or Data General Eclipse MV/8000) were usually 32-bit at a time when most minicomputers (such as the PDP-11 or Data General Eclipse or IBM Series/1) were 16-bit. These traditional minicomputers in the last few decades of the 20th Century, found in small to medium-sized businesses, laboratories and embedded in (for example) hospital CAT scanners, often would be rack-mounted and connect to one or more terminals or tape/card readers, like mainframes and unlike most personal computers, but require less space and electrical power than a typical mainframe. The contemporary term for minicomputer is midrange computer, such as the higher-end SPARC, POWER and Itanium-based systems from Oracle Corporation, IBM and Hewlett-Packard, and the size is now typically smaller, such as a tower case.\n\nThe term mainframe computer was created to distinguish the traditional, large, institutional computer intended to service multiple users from the smaller, single user machines. These computers are capable of handling and processing very large amounts of data quickly. Mainframe computers are used in large institutions such as government, banks and large corporations.\nThey are measured in MIPS (million instructions per second) and can respond to hundreds of millions of users at a time.\n\nA Supercomputer is focused on performing tasks involving intense numerical calculations such as weather forecasting, fluid dynamics, nuclear simulations, theoretical astrophysics, and complex scientific computations. A supercomputer is a computer that is at the front-line of current processing capacity, particularly speed of calculation. The term supercomputer itself is rather fluid, and the speed of today's supercomputers tends to become typical of tomorrow's ordinary computer. Supercomputer processing speeds are measured in floating point operations per second, or FLOPS. An example of a floating point operation is the calculation of mathematical equations in real numbers. In terms of computational capability, memory size and speed, I/O technology, and topological issues such as bandwidth and latency, supercomputers are the most powerful, are very expensive, and not cost-effective just to perform batch or transaction processing. Transaction processing is handled by less powerful computers such as server computers or mainframes. They are mainly kept in a cool environment for proper functions.\n\nServer usually refers to a computer that is dedicated to providing one or more services. For example, a computer dedicated to a database may be called a \"database server\". \"File servers\" manage a large collection of computer files. \"Web servers\" process web pages and web applications. Many smaller servers are actually personal computers that have been dedicated to provide services for other computers.\nA server is expected to be reliable (e.g. error-correction of RAM; redundant cooling; self-monitoring, RAID), fit for running for several years, and giving useful diagnosis in case of an error. For even increased security, the server may be mirrored\n\nThese provide GUI sessions that can be used by client PCs that work someway like a remote control. Only the screen (and audio) output is shown on the client. The GUI applications run on the server, data (like in files) would be stored in the same LAN, thus avoiding problems, should a client PC be damaged or stolen.\n\nA server may run several virtual machines (VMs) for different activities, supplying the same environment to each VM as if it ran on dedicated hardware. Different operating systems (OS) can therefore be run at the same time. This technology approach needs special hardware support to be useful and was first the domain of mainframes and other large computers. Nowadays, most personal computers are equipped for this task, but for long-term operation or critical systems, specialized server hardware may be needed.\nAnother approach is to implement VMs on the operating system level, so all VMs run on the same OS instance (or incarnation), but are fundamentally separated to not interfere with each other.\n\nWorkstations are computers that are intended to serve one user and may contain special hardware enhancements not found on a personal computer. By the mid 1990s personal computers reached the processing capabilities of mini computers and workstations. Also, with the release of multi-tasking systems such as OS/2, Windows NT and Linux, the operating systems of personal computers could do the job of this class of machines.\n\nInformation appliances are computers specially designed to perform a specific \"user-friendly\" function—such as playing music, photography, or editing text. The term is most commonly applied to mobile devices, though there are also portable and desktop devices of this class.\n\nEmbedded computers are computers that are a part of a machine or device. Embedded computers generally execute a program that is stored in non-volatile memory and is only intended to operate a specific machine or device. Embedded computers are very common. The majority are microcontrollers. Embedded computers are typically required to operate continuously without being reset or rebooted, and once employed in their task the software usually cannot be modified. An automobile may contain a number of embedded computers; however, a washing machine or DVD player would contain only one microcontroller. Embedded computers are chosen to meet the requirements of the specific application, and most are slower and cheaper than CPUs found in a personal computer.\n\nPublic computers are open for public uses, possibly as an Interactive kiosk. There are many places one can use them, such as cybercafes, schools and libraries.\n\nThey are normally fire-walled and restricted to run only their pre-installed software. The operating system is difficult to change and/or resides on a file server. For example, \"thin client\" machines in educational establishments may be reset to their original state between classes. Public computers are not expected to keep an individual's data files.\n\nA personal computer has one user who may also be the owner (although the term has also come also mean any computer hardware somewhat like the original IBM PC, irrespective of how it is used). This user often may use all hardware resources, has complete access to any part of the computer and has rights to install/remove software. Personal computers normally store personal files, and often the owner/user is responsible for routine maintenance such as removing unwanted files and virus-scanning. Some computers in a business setting are for one user but are also served by staff with protocols to ensure proper maintenance.\n\nThese are computers where different people might log on at different times; unlike public computers, they would have usernames and passwords assigned on a long-term basis, with the files they see and the computer's settings adjusted to their particular account. Often the important data files will reside on a central file server, so a person could log onto different computers yet still see the same files. The computer (or workstation) might be a \"thin client\" or X terminal, otherwise it may have its own disk for some or all system files, but usually will need to be networked to the rest of the system for full functionality. Such systems normally require a system administrator to set up and maintain the hardware and software.\n\nComputers that are used just to display selected material (usually audio-visual, or simple slide shows) in a shop, meeting or trade show. These computers may have more capabilities than they are being used for; they are likely to have WiFi and so be capable of Internet access, but are rarely firewalled (but have restricted port access or monitored in some way). Such computers are used and maintained as appliances, and not normally used as the primary store for important files.\n\n\n\n"}
{"id": "3723337", "url": "https://en.wikipedia.org/wiki?curid=3723337", "title": "Coal pipeline", "text": "Coal pipeline\n\nCoal pipelines are pipelines used to transport coal from where it is mined to where it is consumed. For very short distances, large trucks are used to transport coal, but trains and barges are preferred for long distances. In some cases it is more economical to move the coal by pipeline than by train or barge. This can happen when there is no suitable railway or waterway to transport the coal, or when it must be moved very long distances.\n\nThere are two types of coal pipelines, slurry and log. Slurry pipelines use a slurry of water and pulverized coal. The ratio of coal to water is about 1 to 1 by weight. Coal log pipelines use coal that has been compressed into logs with a diameter 5 to 10% less than the diameter of the pipeline and a length about twice the diameter of the pipeline. The ratio of coal to water is about 3 or 4 to 1.\n\nCoal needs a very low moisture content before it can be burned efficiently, so the coal must be dried after it arrives at the coal power plant. Coal transported as slurry requires a lot of drying and electricity generation will be substantially less if it is not dried effectively. Coal logs do not require as much drying because they are tightly packed so they do not absorb much water, and any water originally in the coal is squeezed out during compression. To dry the coal, the water is evaporated or separated in a centrifuge.\n\nLarge coal power plants use large amounts of coal each day; enough to fill a hundred train coal cars carrying each. Water used to transport the coal is likewise significant, particularly in arid regions like the Southwestern United States. Such a power plant would use about with a coal slurry pipeline or about with a coal log pipeline. This amounts to about per year respectively.\nThe Mohave Generating Station in Laughlin, Nevada had the longest coal slurry pipeline in the world at . From 1969 until 2005, the pipeline used of water per year to carry about of coal to the plant from the Black Mesa Mine in the northeastern corner of Arizona. The plant was shut down on December 31, 2005 because the controversial coal and water supply terms were being renegotiated.\n\n\n"}
{"id": "16942788", "url": "https://en.wikipedia.org/wiki?curid=16942788", "title": "Cognos ReportNet", "text": "Cognos ReportNet\n\nCognos ReportNet (CRN) is a web-based software product for creating and managing ad hoc and custom-made reports. ReportNet is developed by the Ottawa-based company Cognos (formerly Cognos Incorporated), an IBM company. The web-based reporting tool was launched in September 2003. Since IBM's acquisition of Cognos, ReportNet has been renamed \"IBM Cognos ReportNet\" like all other Cognos products.\n\nReportNet uses web services standards such as XML and Simple Object Access Protocol and also supports dynamic HTML and Java. ReportNet is compatible with multiple databases including Oracle, SAP, Teradata, Microsoft SQL server, DB2 and Sybase. The product provides interface in over 10 languages, has Web Services architecture to meet the needs of multi-national, diversified enterprises and helps reduce total cost of ownership. Multiple versions of Cognos ReportNet have since been released by the company. Cognos ReportNet was awarded the Software and Information Industry Association (SIIA) 2005 Codie awards for the \"Best Business Intelligence or Knowledge Management Solution\" category. CRN's capabilities have been further used in IBM Cognos 8 BI (2005), the latest reporting tool. CRN comes with its own software development kit (SDK).\n\nEarly adopters of Cognos ReportNet for their corporate reporting needs included Bear Stearns, BMW and Alfred Publishing. Around this same time of launch, Cognos competitor Business Objects released version 6.1 of its enterprise reporting tool. Cognos ReportNet has been successful since its launch, raising revenues in 2004 from licensing fees. Subsequently, other major corporations like McDonald's adopted Cognos ReportNet.\n\nCognos rival Business Objects announced in 2005 that BusinessObjects XI significantly outperformed Cognos ReportNet in benchmark tests conducted by VeriTest, an independent software testing firm. The tests performed showed Cognos ReportNet performed poorly when processing styled reports, complex business reports and combination of both. The tests reported a massive 21 times higher report throughput for BusinessObjects XI than Cognos ReportNet at capacity loads. Cognos soon dismissed the claims by stating Business Objects dictated the environment and testing criteria and Cognos did not provide the software to participate in benchmark test. Cognos later performed their own test to demonstrate Cognos ReportNet capabilities.\n\n\n\n"}
{"id": "2024427", "url": "https://en.wikipedia.org/wiki?curid=2024427", "title": "Cohort analysis", "text": "Cohort analysis\n\nCohort analysis is a subset of behavioral analytics that takes the data from a given data set (e.g. an e-commerce platform, web application, or online game) and rather than looking at all users as one unit, it breaks them into related groups for analysis. These related groups, or cohorts, usually share common characteristics or experiences within a defined time-span. Cohort analysis allows a company to “see patterns clearly across the life-cycle of a customer (or user), rather than slicing across all customers blindly without accounting for the natural cycle that a customer undergoes.” By seeing these patterns of time, a company can adapt and tailor its service to those specific cohorts. While cohort analysis is sometimes associated with a cohort study, they are different and should not be viewed as one and the same. Cohort analysis is specifically the analysis of cohorts in regards to big data and business analytics, while in cohort study, data is broken down into similar groups.\n\nThe goal of a business analytic tool is to analyze and present actionable information. In order for a company to act on such information it must be relevant to the situation under analysis. A database full of thousands or even millions of entries of all user data makes it tough to gain actionable data, as that data spans many different categories and time periods. Actionable cohort analysis allows for the ability to drill down to the users of each specific cohort to gain a better understanding of their behaviors, such as if users checked out, and how much did they pay. In cohort analysis \"each new group [cohort] provides the opportunity to start with a fresh set of users,\" allowing the company to look at only the data that is relevant to the current query and act on it.\n\nIn eCommerce, a firm may only be interested in customers who signed up in the last two weeks and who made a purchase, which is an example of a specific cohort. A software developer may only care about the data from users who sign up after a certain upgrade, or who use certain features of the platform.\n\nAn example of cohort analysis of gamers on a certain platform: Expert gamers, cohort 1, will care more about advanced features and lag time compared to new sign-ups, cohort 2. With these two cohorts determined, and the analysis run, the gaming company would be presented with a visual representation of the data specific to the two cohorts. It could then see that a slight lag in load times has been translating into a significant loss of revenue from advanced gamers, while new sign-ups have not even noticed the lag. Had the company simply looked at its overall revenue reports for all customers, it would not have been able to see the differences between these two cohorts. Cohort analysis allows a company to pick up on patterns and trends and make the changes necessary to keep both advanced and new gamers happy.\n\n\"An actionable metric is one that ties specific and repeatable actions to observed results [like user registration, or checkout]. The opposite of actionable metrics are vanity metrics (like web hits or number of downloads) which only serve to document the current state of the product but offer no insight into how we got here or what to do next.\" Without actionable analytics the information that is being presented may not have any practical application, as the only data points represent vanity metrics that do not translate into any specific outcome. While it is useful for a company to know how many people are on their site, that metric is useless on its own. For it to be actionable it needs to relate a \"repeatable action to [an] observed result\".\n\nIn order to perform a proper cohort analysis, there are four main stages:\n\n\n\n"}
{"id": "22672945", "url": "https://en.wikipedia.org/wiki?curid=22672945", "title": "Cytel", "text": "Cytel\n\nCytel is a multinational statistical software developer and contract research organization, headquartered in Cambridge, Massachusetts, USA. Cytel provides clinical trial design, implementation services, and statistical products primarily for the biotech and pharmaceutical development markets.\n\nCytel specializes in adaptive trials – a type of randomized clinical trial that allows modifications of ongoing trials while aiming to preserve the statistical validity and integrity of the study. Based on either frequentist or Bayesian statistics, adaptive trial designs are now widely accepted by government regulatory agencies including the United States Food and Drug Administration (FDA), European Medicines Agency (EMA), and Medicines and Healthcare products Regulatory Agency (MHRA) in early and later stage clinical studies.\n\nAs of January 2014, the company claims 25 large biopharmaceutical companies as users of its software to design, simulate, and analyze trials.\n\nCompany founders Cyrus Mehta, Ph.D. and Nitin Patel, Ph.D. are amongst the pioneering statisticians credited for developing the underlying statistical methods behind so-called “flexible” designs: group sequential and adaptive trials.\n\nCytel statisticians have collectively published over 140 papers in peer-reviewed statistical and medical journals.\n\nCytel's Consulting arm focuses on optimizing the approach for biopharma clinical research development objectives. Functional elements their Strategic Consulting team claims to provide include:\n\n\nCytel's Clinical Research Services arm focuses on improving the probability of success for biopharma clinical research development efforts. Functional elements their Clinical Research Services team claim to provide include:\n\n\nEast\nClinical trial statistical software for the design, simulation and monitoring of adaptive, group sequential and fixed sample size trials. As of 2013, East 6.4 is in use at over 140 pharmaceutical and biotechnology companies, research centers and regulatory agencies including the FDA’s Center for Drug Evaluation and Research, Center for Biologics Evaluation and Research and Center for Devices and Radiological Health divisions.\n\nFirst introduced by the Cytel Software Corporation in 1995 as “East DOS”, the name is derived from the benefit of “Early stopping” a trial due to futility: a failure of the tested treatment to demonstrate significant improvement over an existing treatment and/or placebo.\n\nEnforesys\nIntroduced by Cytel in 2015, Enforesys is a feasibility study decision-making tool for predicting recruitment milestones. Enforsys utilizes historical study site-level data and simulation models to calculate a numerical probability of success for study enrollment strategies.\n\nCompass\nCompass is used by biostatisticians and clinicians to plan and design earlier stage adaptive clinical trials (traditionally known as phase 1 human tolerance and phase 2 dose-selection studies).\n\nCompass is the first commercially offered adaptive trial composition software with both frequentist and Bayesian methods. Other key capabilities include R code integration, trial simulation compute engines, plus various tables, charts and graphs to visualize and communicate trial design attributes.\n\nStatXact\nStatistical software based on the exact branch of statistics used for small-sample categorical and nonparametric data problem-solving. Utilized by statisticians and researchers in all fields of study, the StatXact software now has 150 different non-parametric statistical tests and procedures.\n\nInitially offered in 1989 as StatXact DOS, StatXact 8 was released in 2007. The StatXact PROCs variant integrates with the popular SAS statistical software.\n\nLogXact\nA logistic regression predictive modeling software package suited particularly to cases involving small samples and / or missing data. Logistic regression is used extensively in the medical and social sciences as well as marketing applications to predict subject behavior.\n\nFirst made available in 1996 under the name LogXact Turbo, LogXact was introduced in 2007 and is currently in its eleventh release. The LogXact PROCs variant integrates with the popular SAS statistical software.\n\nACES\nCytel's web-based Access Controlled Execution System. ACES simplifies compliance with the related FDA guidance and EMA guidelines by a secure means of communicating a clinical trial's interim analysis results and recommendations between the Data Monitoring Committee (DMC/DSMB), Independent Statistical Center and clinical team members. The validated system automatically creates an audit trial, allowing regulators to readily determine \"who saw what and when\".\n\nSiZ\nIn 2012, SiZ as a stand-alone product was deprecated, and its feature set was consolidated into the company's East product.\n\nUnited States\n\nEurope\n\nIndia\n\n"}
{"id": "19035720", "url": "https://en.wikipedia.org/wiki?curid=19035720", "title": "Dumbwaiter", "text": "Dumbwaiter\n\nA dumbwaiter is a small freight elevator or lift intended to carry objects rather than people. Dumbwaiters found within modern structures, including both commercial, public and private buildings, are often connected between multiple floors. When installed in restaurants, schools, hospitals, retirement homes or in private homes, the lifts generally terminate in a kitchen.\n\nThe term seems to have been popularized in the United States in the 1840s, after the model of earlier \"dumbwaiters\" now known as serving trays and lazy Susans. The mechanical dumbwaiter was invented by George W. Cannon, a New York City inventor. Cannon first filed for the patent of a brake system (US Patent no. 260776) that could be used for a dumbwaiter on January 6, 1883. Cannon later filed for the patent on the mechanical dumbwaiter (US Patent No. 361268) on February 17, 1887. Cannon reportedly generated a vast amount of royalties from the dumbwaiter patents until his death in 1897.\n\nA simple dumbwaiter is a movable frame in a shaft, dropped by a rope on a pulley, guided by rails; most dumbwaiters have a shaft, cart, and capacity smaller than those of passenger elevators, usually 45 to 450 kg (100 to 992 lbs.) Before electric motors were added in the 1920s, dumbwaiters were controlled manually by ropes on pulleys.\n\nEarly 20th-century building codes sometimes required fireproof dumbwaiter walls and self-closing fireproof doors and mention features such as buttons to control movement between floors and locks on doors preventing them from opening unless the cart is stopped at that floor.\nDumbwaiter Lifts in London were extremely popular in the houses of the rich and privileged. Maids would use them to deliver laundry to the laundry room from different rooms in the house. This obviated carrying handfuls of dirty washing through the house, saving time and preventing injury.\n\nA legal complaint about a Manhattan restaurant's dumbwaiter in 1915, which also mentions that food orders are shouted up and down the shaft, describes its operation and limitations as follows:\n[There is] ... great play between the cart of the dumb-waiter and the guides on which it runs, with the result that the running of the cart is accompanied by a loud noise. The rope which operates the cart of the dumb-waiter runs in a wheel with a very shallow groove, so that the rope is liable to and does at times slip off. ... The cart has no shock absorbers at the top, so that when it strikes the top of the shaft or wheel there is a loud report. ... [T]he ropes of the dumb-waiter strike such wall at frequent intervals with a loud report. ... [T]he dumb-waiter is often negligently operated, by running it faster than necessary, and by letting it go down with a sudden fall.\n\nMore recent dumbwaiters can be more sophisticated, using electric motors, automatic control systems, and custom freight containers of other kinds of elevators. Recently constructed book lifts in libraries and mail or other freight transports in office towers may be larger than many dumbwaiters in public restaurants and private homes, supporting loads as heavy as 450 kg (992 lbs).\n\nBuilding codes have regulated the construction and operation of dumbwaiters in parts of North America since the 19th century. Modern dumbwaiters in the United States and Canada must comply with American Society of Mechanical Engineers (ASME) codes and, therefore, have features similar to those of passenger elevators. The construction, operation and usage of dumbwaiters varies widely according to country.\n\nAfter defecting from the Soviet underground in 1938, Whittaker Chambers gave a last stash of stolen documents to his nephew-in-law, Nathan Levine, who hid them in a dumbwaiter on his mother's house in Brooklyn. A decade later, Chambers asked his nephew to retrieve them (which Chambers referred to as his \"life preserver\"). Handwritten and typewritten papers therein came from Alger Hiss and Harry Dexter White (and became known as the \"Baltimore Documents\"). Microfilm contained therein was subpoenaed and sensationalized (misnamed the \"Pumpkin Papers\" in the press) by Richard M Nixon for HUAC.\n\nHarold Pinter wrote a play in 1960 called \"The Dumb Waiter\", in which a dumbwaiter forms a key element.\n\nIn the 2005 movie \"\", Danny uses his home's dumbwaiter to hide from his brother, and later to move around the house without being seen by the Zorgons.\n\nIn Littlest Pet Shop, Blythe uses a dumbwaiter to get to the Littlest Pet Shop building and back (sometimes, the pets also use the dumbwaiter).\n\nIn the 2018 film \"\", Maisie Lockwood uses her family mansion's dumbwaiter to hide from the antagonist Eli Mills and later the hybrid dinosaur the \"Indoraptor\". She also uses it to travel to its subbasement laboratory, where she meets Owen Grady and Claire Dearing.\n"}
{"id": "58293115", "url": "https://en.wikipedia.org/wiki?curid=58293115", "title": "ELITE SHARP CTT", "text": "ELITE SHARP CTT\n\nThe Emergent Leader Immersive Training Environment Sexual Harassment/Assault Response and Prevention Command Team Trainer, or ELITE SHARP CTT, is a laptop-based training software application for junior officers as well as battalion and brigade commanders in the U.S. military to learn how to work together with Sexual Assault Response Coordinators (SARCs) and Victim Advocates (VAs) to properly approach sexual assault or harassment complaint in their ranks. It is an interactive avatar-based simulator designed in the format of a video game in order to more effectively convey the lessons and replace the traditional slide show-based training. The ELITE SHARP CTT program was developed by the Institute for Creative Technologies at the University of Southern California and the U.S. Army Research Laboratory (ARL).\n\nThe ELITE SHARP CTT software was based on the existing Emergent Leader Immersive Training Environment (ELITE) Lite platform, an avatar-based counseling tool that has helped train junior officers and non-commissioned officers on how to deal with situations ranging from disagreements with superiors to sexual harassment. Unlike the ELITE Lite program, which was aimed at the platoon level and below, the ELITE-SHARP CTT program was designed for command teams. According to the researchers, the development process took about 12 months and involved interviews with commanders who had sexual harassment and assault incidents occur within their commands in the past.\n\nOn September 10, 2015, a beta test was conducted by the U.S. Army SHARP Program Office, the U.S. Army SHARP Academy, the Army Research Lab, the Institute for Creative Technologies, and the National Simulation Center as part of the software validation process. The ELITE SHARP CTT program received final approval for use as an official Army training tool on March 21, 2016 and was released on April 2016.\n\nThe ELITE SHARP CTT program utilized a virtual human instructor to teach key concepts and provided animated vignettes that portray examples of good and bad responses to sexual harassment incidents. Practice exercises were also provided where the trainee could test what they learned in example scenarios.\n\nThe training application consisted of 13 scenarios that were presented in three phases: Up-front Instruction, Practice Environment, and an After Action Review. The practice exercises featured scenarios where the commander must interact with both the victim and the alleged perpetrator of the sexual harassment. The ELITE SHARP CTT program also included examples on how to address male sexual assault survivors.\n\nAccording to the 2016 U.S. Army Annual Report on Sexual Assault, company commanders and first sergeants demonstrated a 15% point increase in their knowledge on handling Sexual Harassment/Assault Response and Prevention incidents after completing the ELITE SHARP CTT training. While soldiers can download the training program for free on a website, efforts have been made to implement the game as part of the curriculum for schools in the U.S. Army.\n"}
{"id": "432640", "url": "https://en.wikipedia.org/wiki?curid=432640", "title": "Edit decision list", "text": "Edit decision list\n\nAn edit decision list or EDL is used in the post-production process of film editing and video editing. The list contains an ordered list of reel and timecode data representing where each video clip can be obtained in order to conform the final cut.\n\nEDLs are created by offline editing systems, or can be paper documents constructed by hand such as shot logging. These days, linear video editing systems have been superseded by non-linear editing (NLE) systems which can output EDLs electronically to allow autoconform on an online editing system – the recreation of an edited programme from the original sources (usually video tapes) and the editing decisions in the EDL.\n\nThey are also often used in the digital video editing world, so rather than referring to reels they can refer to sequences of images stored on disk.\n\nSome formats, such as CMX3600, can represent simple editing decisions only. Final Cut Pro XML, the Advanced Authoring Format (AAF), and AviSynth scripts are relatively advanced file formats that can contain sophisticated EDLs.\n\nLinear editing systems cannot dissolve between clips on the same video tape. Hence, one of these clips will need to be dubbed onto a new video tape. EDLs designate these occurrences by marking such dissolves' source reels as B-roll of \"b-reels\". For example, the EDL will change the 8th character of the reel name to the letter B.\n\nHowever, sometimes editors will (confusingly) use the letter B to designate time code breaks on a video tape. If there is broken time code on a video tape, there will be two (or more) instances of a particular time code on the video tape. When re-capturing, it can be ambiguous as to which timecode is the right one. The letter B may indicate that the right time code is from the second set of timecode on the video tape.\n\nEDL formats such as CMX, GVG, Sony, Final Cut Pro, and Avid are similar but can differ in small (but important) ways. Particular attention should be paid to reel naming convention. On the Avid, reel names can be up to 32 characters, but user should be aware that these EDLs don't adhere to online editing machine control specifications. These are used by systems that have modified the import/export code to handle file-based workflows as tape acquisition formats wane. On FCP, in CMX3600 format, only eight characters are allowed. Particular attention should be paid towards b-reels. If the EDL handles dissolves to the same reel, reel names should be limited to 7 characters since the 8th character may be replaced.\n\nEDLs can use either drop-frame (DF) or non drop-frame timecode (NDF), running at 24fps (non drop-frame only), 25fps (non drop-frame only), and 30fps (drop-frame and non drop-frame).\n\nOverall, EDLs are still commonly used as some systems do not support other more robust formats such as AAF and XML.\n\nAlmost any professional editing system and many others support some form of XML/EDL saving/processing.\n\nSome that make the list:\n\nProbably most of the above, plus any professional editing system, plus\n\n\n\n"}
{"id": "43948228", "url": "https://en.wikipedia.org/wiki?curid=43948228", "title": "FS Me", "text": "FS Me\n\nFS Me is a typeface/font that has been described as the first to have been designed in consultation with a group of people with learning disabilities and designed for use by this population. It was developed by Jason Smith in response to a commission from the charity Mencap in 2008. It was the only font featured in the D&AD Annual 2009 covering advertising and design work.\n"}
{"id": "2781797", "url": "https://en.wikipedia.org/wiki?curid=2781797", "title": "Fedwire", "text": "Fedwire\n\nFedwire (formerly known as the Federal Reserve Wire Network) is a real-time gross settlement funds transfer system operated by the United States Federal Reserve Banks that allows financial institutions to electronically transfer funds between its more than 9,289 participants (as of March 19, 2009). Transfers can only be initiated by the sending bank once they receive the proper wiring instructions from the receiving bank. These instructions include: the receiving bank's routing number, account number, name and dollar amount being transferred. This information is submitted to the Federal Reserve via the Fedwire system. Once the instructions are received and processed, the Fed will debit the funds from the sending bank's reserve account and credit the receiving bank's account. Wire transfers sent via Fedwire are completed in the same day, while some are completed instantly.\n\nIn conjunction with Clearing House Interbank Payments System (CHIPS), operated by The Clearing House Payments Company, a private company, Fedwire is the primary U.S. network for large-value or time-critical domestic and international payments, and it is designed to be highly resilient. In 2012, CHIPS was designated a systemically important financial market utility (SIFMU) under Title VIII of the Dodd–Frank Act, which means that CHIPS is subject to heightened regulatory scrutiny by the Federal Reserve Board.\n\nThe Fedwire system has grown since its inception, seeing growth in both number of transfers and total transaction dollar value of about 79% and 207% respectively between 1996 and 2016. In 2016, roughly 148.1 million transfers were valued at $766.7 trillion dollars. \n\nIn the early 1900s, settlement of interbank payments was often done by the physical delivery of cash or gold. By 1915, The Federal Reserve Banks began to move funds electronically. In 1918, the Banks established a proprietary telecommunications system to process funds transfers, connecting all 12 Reserve Banks, the Federal Reserve Board and the U.S. Treasury by telegraph using Morse code. Starting in the 1920s up until the 1970s, the system remained largely telegraphic; however as technology improved, they began to make the shift from telegraphy towards telex, then to computer operations and then to proprietary telecommunications networks.\n\nIn the early 1980s, Fedwire was taxed to its limit with the result that it was often subject to \"throttle\", which means that it took messages from the banks more slowly than its normal speed. From a user's point of view, throttle was like being put on hold every time one sent a message to the Fed. In 1983, the Fed made a major upgrade of the automated system it uses to support Fedwire. Because the major banks could not tolerate a long breakdown in their computer operations, the Fed designed its internal systems so that the maximum down time for a breakdown would be limited to a few minutes or a few hours at most. In an effort to improve operational efficiency, in the 1990s, the Reserve Banks consolidated most mainframe computer operations and centralized certain payment applications. More recently, the Reserve Banks have taken advantage of the flexibility and efficiency that Internet protocol (IP) and distributed processing technologies offer. These technologies have improved reliability and efficiency of Fedwire greatly. Today, three data processing centers support the Fedwire services. One site supports the primary processing environment with on-site backup. A second site serves an active, \"hot\" backup facility with on-site backup. A third site serves as a \"warm\" backup facility. The three data processing centers are located a considerable distance from one another (i.e., hundreds of miles) in order to mitigate the effects of natural disasters, power and telecommunication outages, and other wide-scale, regional disruptions. In addition, all three data processing centers have appropriate security and include various contingency features, such as redundant power feeds, environmental and emergency control systems, dual computer and network operations centers, and dual customer service centers.\n\nUntil 1981, Fedwire services were provided free and were available only to Federal Reserve member banks. The Depository Institutions Deregulation and Monetary Control Act of 1980 required most Federal Reserve Bank financial services to be priced, while giving nonmember depository institutions direct access to these priced services. Fees were now applicable to several services, including funds transfers and securities safekeeping. Banks are charged a gross transfer fee of $0.82 for every transaction, however there is a three-tiered discount schedule, which results in actual transaction fees costing between $0.034 and $0.82 per transaction depending on transaction volume.\n\nMore recently with the advancement in mobile technologies, many alternative modes of electronic funds transfers have emerged. These alternative modes are changing the way people make payments in that fewer and fewer people are using traditional banking methods to transfer money. Rather than transfer money from bank to bank, they are opting to transfer the funds directly to the other party via a mobile application. With fewer people using traditional banks, and fewer people transferring money from bank to bank, the volume of transactions going through Fedwire on a daily basis is also likely to decline. With this type of electronic funds transfers, corporations act much like the Fed by using their commercial bank accounts for processing and transferring payments between individuals. Many of these systems like PayPal, Venmo and Google Wallet are accessible on mobile devices and are much cheaper for consumers than a wire sent via the Fedwire system.\n\n\n"}
{"id": "21070916", "url": "https://en.wikipedia.org/wiki?curid=21070916", "title": "Filling carousel", "text": "Filling carousel\n\nA filling carousel is intended for filling liquefied petroleum gas (LPG) cylinders in large groups. It consist of a frame with running wheels, rails, a central column for LPG and air, and a driving unit the carousel frame around the central column. The speed of the carousel can be adapted to the various filling times and capacities. The dimension of the carousel is important to consider for the future filling capacity. The carousel frame chosen can be equipped with a number of filling scales covering the present demand and also for the future demand. Filling carousel can be provided with equipment for automatic introduction and automatic filling scales with ejection of cylinders.\n\nFrame sizes and approximate filling times for 12 kg cylinders with two-man operation:\n\nAutomatic inlet device controls the transfer of LPG cylinders from the inlet portion of the conveyor system into the carousel. It is an integral part of an automated cylinder filling operation.\n\nThe main purpose of the double chain conveyor is to carry the LPG cylinders to and from the filling carousel. It has a sturdy steel construction and is one of the most important components of an automated LPG filling operation. Good conveyor layout saves time, space and labor.\n\nA conveyor drive unit consists of an explosion-proof electric motor, a gear reducer and a chain-sprocket drive mechanism.\n\nA carousel drive unit consists of an explosion-proof electric motor, an hydraulic pump and an hydraulic motor to drive the rubber wheel that turns the carousel.\n\nThe ejection unit is designed to take the filled cylinder from the carousel. The ejection unit typically receives a signal from the corresponding filling head (filling head is up)indicating that the cylinder is filled and may be taken off the carousel. If the filling is not completed, \"filling head down\" tells the evacuation unit not to activate and the cylinder completes one more cycle on the carousel while being filled. On modern carousels these commands are electronic, but can also be mechanical by means of a dowel which operates a switch.\n\nThe pressure control valve is a very important instrument in cylinder filling. It allows LPG in pipeline to be at a predetermined pressure. When the pressure exceeds this value the valve opens and sends the flow to return line.\n"}
{"id": "18388052", "url": "https://en.wikipedia.org/wiki?curid=18388052", "title": "Fishing bait", "text": "Fishing bait\n\nFishing bait is any substance used to attract and catch fish, e.g. on the end of a fishing hook, or inside a fish trap. Traditionally, nightcrawlers, insects, and smaller bait fish have been used for this purpose. Fishermen have also begun using plastic bait and more recently, electronic lures, to attract fish.\n\nStudies show that natural baits like croaker and shrimp are more recognized by the fish and are more readily accepted. Which of the various techniques a fisher may choose is dictated mainly by the target species and by its habitat. Bait can be separated into two main categories: artificial baits and natural baits.\n\nUsing lures is a popular method for catching predatory fish. Lures are artificial baits designed to resemble the appearance and movement of prey, usually small fish. The lure may require a specialised presentation to impart an enticing action as, for example, in fly fishing. A common way to fish a soft plastic worm is the Texas rig.\n\nThe natural bait angler, with few exceptions, will use a common prey species of the fish as an attractant. The natural bait used may be alive or dead. Common natural baits include worms, leeches (notably bait-leech \"Nephelopsis obscura\"), minnows, frogs, salamanders, and insects. Natural baits are effective due to the lifelike texture, odour and colour of the bait presented. Cheese has been known to be a very successful bait due to its strong smell and light colours.\n\nThe common earthworm is a universal bait for fresh water angling. Grubs and maggots are also excellent bait when trout fishing. Grasshoppers, bees and even ants are also used as bait for trout in their season, although many anglers believe that trout or salmon and many other fresh water fish roe is superior to any other bait. In lakes in southern climates such as Florida, United States, fish such as bream will take bread bait. Bread bait is a small amount of bread, often moistened by saliva, balled up to a small size that is bite size to a small fish.\n\nMost common earthworm species, such as \"Lumbricus terrestris\", which can often be dug up in the garden, are eminently suitable for freshwater fishing. However, on a commercial scale they are not really candidates for worm farming for providing fishing bait. The greyish brown common earthworms are deep burrowing (\"anecic\") and do not readily breed in the shallow worm farm bins. The red compost worms, such as the well known red wiggler or the European nightcrawler, are better candidates, as they are epigeic, or surface dwellers. This is the reason that red worms are more usually available commercially for bait worms. Their natural home is just below the surface in rotting leaves, dung heaps and other plant litter. They are called detritivourous because they eat detritus (waste material).\n\nThe larger species, the European nightcrawler is much sought after for fishing bait as it tolerates near freezing water and is one of the few earthworms suitable for salt-water fishing. These worms can grow up to 7 inches (18 cm) in length, but usually are between 3 and 4 inches (7–10 cm) long. Worm farmers also offer other worm species for bait, depending on availability, which usually depends on the prevalent climatic conditions.\n\nThe capture, transportation and culture of bait fish can spread damaging organisms between ecosystems, endangering them. In 2007, several American states enacted regulations designed to slow the spread of fish diseases, including viral hemorrhagic septicemia, by bait fish. Because of the risk of transmitting \"Myxobolus cerebralis\" (whirling disease), trout and salmon should not be used as bait.\n\nAnglers may increase the possibility of contamination by emptying bait buckets into fishing venues and collecting or using bait improperly. The transportation of fish from one location to another can break the law and cause the introduction of fish alien to the ecosystem.\n\n\n"}
{"id": "4652142", "url": "https://en.wikipedia.org/wiki?curid=4652142", "title": "Fusible plug", "text": "Fusible plug\n\nA fusible plug is a threaded metal cylinder usually of bronze, brass or gunmetal, with a tapered hole drilled completely through its length. This hole is sealed with a metal of low melting point that flows away if a pre-determined, high temperature is reached. The initial use of the fusible plug was as a safety precaution against low water levels in steam engine boilers, but later applications extended its use to other closed vessels, such as air conditioning systems and tanks for transporting corrosive or liquefied petroleum gasses.\n\nA fusible plug operates as a safety valve when dangerous temperatures, rather than dangerous pressures, are reached in a closed vessel. In steam engines the fusible plug is screwed into the crown sheet (the top plate) of the firebox, typically extending about an inch (25mm) into the water space above it. Its purpose is to act as a last-resort safety device in the event of the water level falling dangerously low: when the top of the plug is out of the water it overheats, the low-melting-point core melts away and the resulting noisy release of steam into the firebox serves to warn the operators of the danger before the top of the firebox itself runs completely dry, which could result in catastrophic failure of the boiler. The temperature of the flue gases in a steam engine firebox can reach 1000 °F (550 °C), at which temperature copper, from which historically most fireboxes were made, softens to a state which can no longer sustain the boiler pressure and a severe explosion will result if water is not put into the boiler quickly and the fire removed or extinguished. The hole through the plug is too small to have any great effect in reducing the steam pressure and the small amount of water, if any, that passes through it is not expected to have any great impact in quenching the fire.\n\nThe device was invented in 1803 by Richard Trevithick, the proponent of high-pressure (as opposed to atmospheric) steam engines, in consequence of an explosion in one of his new boilers. His detractors were eager to denounce the whole concept of high-pressure steam, but Trevithick proved that the accident happened because his fireman had neglected to keep the boiler full of water. He publicised his invention widely, without patent, to counter these criticisms.\n\nExperiments conducted by the Franklin Institute, Boston, in the 1830s had initially cast doubt on the practice of adding water as soon as the escape of steam through the device was noted. A steam boiler was fitted with a small observation window of glass and heated beyond its normal operating temperature with the water level below the top of the firebox. When water was added it was found that the pressure rose suddenly and the observation glass shattered. The report concluded that the high temperature of the metal had vaporised the added water too quickly and that an explosion was the inevitable result. It was not until 1852 that this assumption was challenged: Thomas Redmond, one of the Institute's own inspectors, specifically ruled out this theory in his investigation into the boiler explosion on the steam ship \"Redstone\" on the Ohio River on 3 April that year. A 1907 investigation in Wales came to a similar conclusion: a steam locomotive belonging to the Rhymney Railway was inadvertently sent out with its safety valves wrongly assembled. The pressure in the boiler built up to the extent that the injectors failed; the crown sheet became uncovered, was weakened by the heat of the fire and violently blew apart. The investigation, led by Colonel Druitt of the Railway Inspectorate, dismissed the theory that the enginemen had succeeded in starting the injectors and that the sudden flood of cold water had caused such a generation of steam that the boiler burst. He quoted the results of experiments by the Manchester Steam Users' Association, a national boiler certification and insurance body, that proved that the weight of copper present (considered with its specific heat) was insufficient to generate enough steam to raise the boiler pressure at all. Indeed, the addition of cold water had caused the pressure to fall. From then on it was accepted that the correct action in the event of the operation of the fusible plug was to add water.\n\nThe original design was a simple solid plug filled with a slug of low-melting-point alloy. When this melts, it first melts as a narrow channel through the plug. Steam and water immediately begins to escape through this. The cored fusible plug was developed in the 1860s to give a wide opening as soon as the alloy softens. This version has a solid brass or bronze centre, soldered into place by a layer of the low-melting-point alloy. When overheated, the plug does not release any steam or water until the alloy melts sufficiently to release the centre plug. The plug now fails dramatically, opening its entire bore immediately. This full-bore jet is then more likely to be noticed.\n\nA drawback to the device was found on 7 March 1948, when the firebox crown sheet of \"Princess Alexandra\", a Coronation Pacific of the London, Midland and Scottish Railway, failed while hauling a passenger train from Glasgow to London. Enquiries established that both water gauges were defective and on a journey earlier that day one or both of the fusible plugs had melted, but this had gone unnoticed by the engine crew because of the strong draught carrying the escaping steam away from them.\n\nInvestigation showed the importance of the alloy on plug ageing. Alloys were initially favoured as they offered lower eutectic melting points than pure metals. It was found though that alloys aged poorly and could encourage the development of a matrix of oxides on the water surface of the plug, this matrix having a dangerously high melting point that made the plug inoperable. In 1888 the US Steamboat Inspection Service made a requirement that plugs were to be made of pure banca tin and replaced annually. This avoided lead and also zinc contamination. Zinc contamination was regarded as so serious a problem that the case of the plugs was also changed from brass (a copper-zinc alloy) to a zinc-free copper-tin bronze, to avoid the risk of zinc migrating from the housing into the alloy plug.\n\nIn the 1920s investigations by the U.S. Bureau of Standards, in conjunction with the Steamboat Inspection Service, found that in use encrustation and oxidation above the fusible core can increase melting point of the device and prevent it from working when needed: melting points in excess of 2000 °F (1100 °C) in used examples have been found. Typical current practice in locomotives requires new plugs to be inspected after \"15 to 30 working days (dependent upon water condition and use of locomotive) or at\nleast once every six months,\" depending on the boiler operating pressure and temperature.\n\nThe principle of the fusible plug is also applied to the transport of liquefied petroleum gases, where fusible plugs (or small, exposed patches of the containers' lining membrane) are designed to melt or become porous if too high a temperature is reached: a controlled release, at a typical temperature of 250 °F (120 °C), is preferable to an explosive release (a \"BLEVE\") at a higher temperature. Corrosive gas containers, such as those used for liquid chlorine, are fitted with one or more fusible plugs with an operating temperature of about 158 to 165 °F (70–74 °C).\n\nFusible plugs are common in aircraft wheels, typically in larger or high-performance aircraft. The very large thermal loads imposed by abnormal landing and braking conditions (and RTO notably) can cause already high pressure in the tyres to rise to the point that the tyre might burst, so fusible plugs are used as a relief mechanism. The vented gas may be directed to cool the braking surfaces.\n\nFusible plugs are sometimes fitted to the receivers of air compressors as a precaution against the ignition of any lubricating oil vapour that might be present. Should the action of the compressor heat the air above a safe temperature the core will melt and release the pressure.\n\nAutomobile air conditioning systems were commonly fitted with fusible plugs, operating at 100–110 °C, but from concerns about the environmental effects of any released refrigerant gas this function has been taken over by an electrical switch.\n\nA patented (Patent published 1867) type of fireproof safe uses a fusible plug to douse its contents with water if the external temperature gets too high.\n\n"}
{"id": "2451025", "url": "https://en.wikipedia.org/wiki?curid=2451025", "title": "Hand luggage", "text": "Hand luggage\n\nThe term hand luggage or cabin baggage (also commonly referred to as carry-on in North America) refers to the type of luggage that passengers are allowed to carry along in the passenger compartment of a vehicle instead of moving to the cargo compartment. Passengers are allowed to carry a limited number of smaller bags with them in the vehicle and contain valuables and items needed during the journey. There is normally storage space provided for hand luggage, either under seating, or in overhead lockers. Trains usually have luggage racks above the seats and may also (especially in the case of trains travelling longer distances) have luggage space between the backs of seats facing opposite directions, or in extra luggage racks, for example, at the ends of the carriage near the doors.\n\nHand baggage allowance is a topic frequently discussed in context of commercial air travel. On one hand, passengers may want to have more of their possessions at hand during flight, skip often time-consuming baggage claim process, and avoid the risk of having their checked baggage lost or damaged. On the other hand, safety concerns, takeoff weight limitations and financial incentives cause airlines to impose limits on how much and what can a passenger take into the cabin of aircraft.\n\nThe International Air Transport Association (IATA) sets guidelines for cabin baggage/hand luggage/carry-on luggage size. They are not mandatory, however, and individual airlines can and do vary their requirements. The IATA guideline at one time stated:\nCabin baggage should have a maximum length of 56 cm (22 inches), width of 45 cm (18 inches) and depth of 25 cm (10 inches) including all handles, side pockets, wheels etc.\nThe actual size and weight limits of cabin baggage can differ widely, in some cases they are dependent on the aircraft model being used, in other cases it depends on the booking class. Due to the lack of standardization a large number of different specifications were created by the airlines on the maximum permitted cabin luggage restrictions (see below). In 2015 the IATA made an effort to introduce a common smaller size for cabin luggage by introducing the \"IATA Cabin OK\" logo. Major airlines have expressed their interest to accept luggage of that size on their flights. This is specified as 55 x 35 x 20 cm (or 21.5 x 13.5 x 7.5 inches). There were news that the move was backed by nine airlines including Lufthansa, Emirates and Qatar Airlines. The new size restrictions were criticised widely with the introduction program to be put on hold a few days later. Consequently none of the mentioned airlines has introduced the new format (by April 2016).\n\nDimensions are sometimes listed as \"linear\", meaning the height, width, and length are not to exceed a certain total number.\n\nBusiness Class, First class passengers and holders of high level mileage club members are often allowed to carry on a second bag of the same size and weight, or a smaller size and weight.\n\nOn smaller sized aircraft, sometimes the hand baggage can be carried to the aircraft door, where it is collected by baggage handlers for stowing in the cargo area and returned to the passenger right after landing.\n\nFollowing the increase in restrictions imposed on flights from UK airports and to the USA after the events of August 2006 transatlantic aircraft plot, hand baggage on such flights was restricted to one cabin bag no bigger than 45 cm × 35 cm × 16 cm effective since 15 August. On 21 September 2006, the British Airports Authority advised that from the following day, the allowable size of the single item of hand baggage on outgoing flights from the UK would be increased to 56 cm × 45 cm × 25 cm (approx. 22 in × 17.75 in × 9.85 in), the IATA guideline size. Most UK airports remain to have a strict limit of one piece of cabin baggage per passenger, including business class (allowed two pieces of cabin baggage within Europe, excluding flights to and from the UK).\n\nA common regulation for cabin baggage restrictions was introduced on 6 November 2006 in European Union and in Switzerland, Norway and Iceland.\n\nThe United States Transportation Security Administration (TSA) has introduced a series of restrictions effective since 26 September 2006 under the name \"3:1:1\" for liquids.\n\nThe TSA has additional restrictions for security searches: for example, the baggage should not be locked (except with a special luggage locks that TSA staff can open), gifts should not be wrapped, and shoes may be required to be taken off during body search with the metal detector. Food items in the luggage may be mistaken for dangerous material triggering an intensive search.\n\n\n"}
{"id": "6739307", "url": "https://en.wikipedia.org/wiki?curid=6739307", "title": "Industrial finishing", "text": "Industrial finishing\n\nIndustrial finishing is any kind of secondary process done to any metal, plastic, or wood product used in a common market such as automotive, OEM, telecommunications or point-of-purchase. The most common commodity in the industrial finishing market is plastic parts. These can be injection molded, thermoformed, extruded or vacuum formed. Most parts are painted but can be pad printed or silkscreened.\n\nOne finishing process is vacuum metalising.\n\n"}
{"id": "910155", "url": "https://en.wikipedia.org/wiki?curid=910155", "title": "Jackhammer", "text": "Jackhammer\n\nA jackhammer (pneumatic drill or demolition hammer in British English) is a pneumatic or electro-mechanical tool that combines a hammer directly with a chisel. It was invented by William Mcreavy, who then sold the patent to Charles Brady King. Hand-held jackhammers are generally powered by compressed air, but some are also powered by electric motors. Larger jackhammers, such as rig mounted hammers used on construction machinery, are usually hydraulically powered. They are typically used to break up rock, pavement, and concrete.\n\nA jackhammer operates by driving an \"internal\" hammer up and down. The hammer is first driven down to strike the back and then back up to return the hammer to the original position to repeat the cycle. The effectiveness of the jackhammer is dependent on how much force is applied to the tool. It is generally used like a hammer to break the hard surface or rock in construction works and it is not considered under earth moving equipment, along with its accessories (i.e., pusher leg, lubricator). \n\nIn British English, electromechanical versions are colloquially known as \"Kangos\".\n\nThe first steam-powered drill was patented by Samuel Miller in 1806. This drill used steam only for raising the drill. Pneumatic drills were developed in response to the needs of mining, quarrying, excavating, and tunneling. A pneumatic drill was proposed by a C. Brunton in 1844. In 1846 a percussion drill that could be worked by steam or atmospheric pressure obtained from a vacuum was patented in Britain by Thomas Clarke, Mark Freeman and John Varley. The first American \"percussion drill\" was made in 1848 and patented in 1849 by Jonathan J. Couch of Philadelphia, Pennsylvania. In this drill, the drill bit passed through the piston of a steam engine. The piston snagged the drill bit and hurled it against the rock face. It was an experimental model. In 1849, Couch's assistant, Joseph W. Fowle, filed a patent caveat for a percussion drill of his own design. In Fowle’s drill, the drill bit was connected directly to the piston in the steam cylinder; specifically, the drill bit was connected to the piston’s crosshead. The drill also had a mechanism for turning the drill bit around its axis between strokes and for advancing the drill as the hole deepened. By 1850 or 1851, Fowle was using compressed air to drive his drill, making it the first true pneumatic drill.\n\nThe demand for pneumatic drills was driven especially by miners and tunnelers, because steam engines needed fires to operate and the ventilation in mines and tunnels was inadequate to vent the fires' fumes; there was also no way to convey steam over long distances, \"e.g.\", from the surface to the bottom of a mine; and mines and tunnels occasionally contained flammable explosive gases such as methane. By contrast, compressed air could be conveyed over long distances without loss of its energy, and after the compressed air had been used to power equipment, it could ventilate a mine or tunnel.\n\nIn Europe since the late 1840s, the king of Sardinia, Carlo Alberto, had been contemplating the excavation of a tunnel through Mount Fréjus to create a rail link between Italy and France, which would cross his realm. The need for a mechanical rock drill was obvious and this sparked research on pneumatic rock drills in Europe. A Frenchman, Cavé, designed, and in 1851 patented, a rock drill that used compressed air; but the air had to be admitted manually to the cylinder during each stroke, so it was not successful. In 1854, in England, Thomas Bartlett made and then patented (1855) a rock drill whose drill bit was connected directly to the piston of a steam engine. In 1855 Bartlett demonstrated his drill, powered by compressed air, to officials of the Mount Fréjus tunnel project. (In 1855, a German, Schumann, invented a similar pneumatic rock drill in Freiburg, Germany.) Bartlett’s drill was refined by the Savoy-born engineer Germain Sommeiller (1815-1871) and his colleagues, Grandis and Grattoni, by 1861. Thereafter, many inventors refined the pneumatic drill. Sommeiller took his drill to the lengthy Gotthard Pass Tunnel then being built to link railways between Switzerland and Italy under the Alps. Atlas Copco and Ingersoll Rand were two important drill companies in Europe and America respectively, each holding patents and dominating the industry. From this mining and railway tunnelling expanded.\n\nThe word \"jackhammer\" is used in North American English and Australia, while \"pneumatic drill\" is used colloquially elsewhere in the English speaking world, although strictly speaking a \"pneumatic drill\" refers to a pneumatically driven jackhammer.\n\nIn Britain, electromechanical versions are colloquially known as \"Kangos\". The term comes from the former British brand name now owned by Milwaukee tools.\n\nA full-sized portable jackhammer is impractical for use against walls and steep slopes, except by a very strong person, as the user would have to both support the weight of the tool, and push the tool back against the work after each blow. A technique developed by experienced workers is a two-man team to overcome this obstacle of gravity: one operates the hammer and the second assists by holding the hammer either on his shoulders or cradled in his arms. Both use their combined weight to push the bit into the workface. This method is commonly referred to as horizontal jackhammering.\n\nAnother method is overhead jackhammering, requiring strength conditioning and endurance to hold a smaller jackhammer, called a rivet buster, over one's head. To make overhead work safer a platform can be used. One such platform is a P.A.M. (Positioner Actuator Manipulator). This unit takes all the weight and vibration from the user.\n\nA pneumatic jackhammer, also known as a ' or ', is a jackhammer that uses compressed air as the power source. The air supply usually comes from a portable air compressor driven by a diesel engine. Reciprocating compressors were formerly used. The unit comprised a reciprocating compressor driven, through a centrifugal clutch, by a diesel engine. The engine's governor provided only two speeds:\n\n\nModern versions use rotary compressors and have more sophisticated variable governors. The unit is usually mounted on a trailer and sometimes includes an electrical generator to supply lights or electric power tools.\n\nAdditionally, some users of pneumatic jackhammers may use a pneumatic lubricator which is placed in series with the air hose powering the air hammer. This increases the life and performance of the jackhammer. Specific lubricant is filled in the pneumatic lubricator. Furthermore, air compressors typically incorporate moisture into the compressed air leading to freeze-ups of the jackhammer or air hammer in cold weather.\n\nAn electropneumatic hammer is often called a \"rotary hammer,\" because it has an electric motor, which rotates a crank. The hammer has two pistons—a drive piston and a free flight piston. The crank moves the drive piston back and forth in the same cylinder as the flight piston. The drive piston never touches the flight piston. Instead the drive piston compresses air in the cylinder, which then propels the flight piston against a striker, which contacts the drill bit.\n\nElectric powered tools come in a variety of sizes from about 12 lbs - 65 lbs. They require an external power source, but do not require a compressor. Although in the past these tools did not have the power of an air or pneumatic hammer, this is changing with newer brushless-motor tools coming close to the power of a pneumatic tool and in some cases even matching it. Electric powered tools are useful for locations where access to a compressor is limited or impractical, such as inside a building, in a crowded construction site, or in a remote location and it is not come under earth moving equipment or tool.\n\nElectropneumatic tools use a variety of chucks for attaching chisels, but the most common are SDS-max, 7/8\" Hex, TE-S, and 1-1/8\" Hex. The connection end size is also related to the breaking energy of the tool. For example, the Bosch and Hilti 12 lb tools both use SDS-max, while the Bosch, Hilti, and Makita 65 lbs tools all use 1-1/8\" Hex connection. See hammer drills for more on electropneumatic hammering.\n\nA hydraulic jackhammer, generally much larger than portable ones, may be fitted to mechanical excavators or backhoes and is widely used for roadwork, quarrying and general demolition or construction groundwork. These larger machine mounted breakers are known as Rig Mounted, or Machine Mounted Breakers. Such tools can also be used against vertical walls (or ceilings for that matter), since the vehicles involved are massive enough and powerful enough to exert the forces involved without needing the help of gravity in operating the tool. Pneumatic or hydraulic tools are particularly likely to be used in mines where there is an explosion risk (such as underground coal mines), since they lack any high-power electrical circuitry that might cause a triggering spark.\n\nHydraulic breakers usually use a hydraulic motor driving a sealed pneumatic hammer system, as a hydraulic hammer would develop a low strike speed and transfer unacceptable shock loads to the pump system.\n\nAdvances in technology have allowed for portable hydraulic breakers. The jackhammer is connected with hydraulic hoses to a portable hydraulic powerpack: either a petrol or diesel engine driving a hydraulic pump; or a mini-excavator or skid-steer via a power take-off driveshaft to the machine's hydraulic system. Hydraulic power sources are more efficient than air compressors, making the kit smaller, cheaper or more powerful than a comparable pneumatic version.\n\nBit types include\n\nChisels may be resharpened in a shop or with an angle grinder with grinding disc. After resharpening, they must then be heat treated to restore the integrity of the steel before use. As an alternative, Hilti also manufacturers self-sharpening polygon and flat chisels in SDS-max, TE-S, and 1-1/8\" Hex connection ends.\n\nThe sound of the hammer blows, combined with the explosive air exhaust, makes pneumatic jackhammers dangerously loud, emitting 100 decibels at two meters. Sound-blocking earmuffs must be worn by the operator to prevent a form of hearing damage, of which tinnitus is the main symptom. Most pneumatic jackhammers now have a silencer around the barrel of the tool.\n\nUse has been linked to Raynaud syndrome.\n\nProlonged exposure to the pronounced vibration conducted by the tool can lead to blood-circulation failures in the fingers, a condition known as vibration white finger. Applying athletic tape is not very effective in preventing white finger but seems to help alleviate some of its discomfort. Pneumatic drill usage can also lead to a predisposition for development of carpal tunnel syndrome.\n\nSome manufacturers of electro-pneumatic tools now offer vibration reduction systems to reduce the vibration felt by the operator. For example, the Hilti TE 3000-AVR has approximately the same impact energy of a 60 lb pneumatic hammer, but the vibration felt by the operator is significantly less (7 m/s). Other manufacturers such as Makita, DeWalt and Bosch also offer electric tools with vibration dampening.\n\nUsing a jackhammer to break up concrete pavement may expose the operator to hazardous dust containing respirable crystalline silica (RCS).\n\n"}
{"id": "21745175", "url": "https://en.wikipedia.org/wiki?curid=21745175", "title": "John V. Pavlik", "text": "John V. Pavlik\n\nJohn Vernon Pavlik is an American academic and author who publishes on the impact of technology on journalism, media, and society.\n\nHe is a professor in the Department of Journalism and Media Studies in the School of Communication and Information at Rutgers, the state university of New Jersey. He is former chair of the Department of Journalism and Media Studies at the School of Communication, Information and Library Studies at Rutgers University. While on leave from Rutgers in 2013 he served as the associate dean for research at Northwestern University in Qatar. He is also former chair of the editorial board for the journal of the National Academy of Television Arts and Sciences' \"Television Quarterly\" and a faculty associate at the Columbia Institute for Tele-Information.\n\nPavlik was a professor and executive director of the Center for New Media at the Columbia University Graduate School of Journalism.\nHe also previously served as the founding director of the School of Communication at San Diego State University.\n\nPavlik writes about the impact of new technology on journalism, media and society. His most recent book is \"Converging Media\", co-written with Shawn McIntosh (Oxford University Press 2013). Other works include \"Journalism and New Media\" (, Columbia University Press, 2001) and \"New Media Technology: Cultural and Commercial Perspectives\" (), now in its 2nd edition (Allyn & Bacon, 1998).\n\nPavlik has written more than a dozen computer software packages for education in journalism and communication. He holds a Ph.D. and M.A. in mass communication from the University of Minnesota and is graduate of the School of Journalism and Mass Communication at the University of Wisconsin–Madison.\nHe is co-developer with Prof. Steven Feiner of the Situated Documentary, a new form of storytelling using augmented reality. \n\nHe is married with two children and lives in New York City.\n\nHe developed the idea of the \"Three Stages of Online News Content,\" a concept which explains the source of online news content. That model contains three stages: \n"}
{"id": "22526294", "url": "https://en.wikipedia.org/wiki?curid=22526294", "title": "Labvantage", "text": "Labvantage\n\nLabVantage Solutions, Inc. is a laboratory information management system (LIMS) provider based in Somerset, New Jersey. Founded in 1981, LabVantage is the third largest LIMS provider in the world.\n\nOn June 30, 2009, LabVantage announced its intention to acquire the SQL*LIMS business from Applied BioSystems (subsidy of Life Technologies Corp.). The significance of this merger is that it heavily impacts the LIMS market. With LabVantage's most advanced technology and SQL*LIMS business's most experienced and globalized personnel, this combination thrusts the new LabVantage to become the number 1 LIMS provider in the industry. The purchase immediately opened LabVantage's market to all continents except Antarctica, giving the company the most global coverage in the LIMS industry throughout the world today. On August 10, 2009, LabVantage officially closed the transaction and completed the acquisition of SQL*LIMS business from Life Technologies Corp.\n\nPrior to the August 10, 2009 transaction, about half of the company's employees work in India, while 60 employees are in North America and 20 are in Europe.\n\nLabVantage's customers in the United States include Aventis, Pfizer, and Unilever's Best Foods (now called Hellmann's and Best Foods). In India, LabVantage provides services for GAIL, Indian Oil Corporation, and Reliance Industries.\n\n"}
{"id": "31638066", "url": "https://en.wikipedia.org/wiki?curid=31638066", "title": "Media controls", "text": "Media controls\n\nIn digital electronics, analogue electronics and entertainment, the user interface of media may include media controls or player controls, to enact and change or adjust the process of watching film or listening to audio. These widely known symbols can be found in a multitude of software products, exemplifying what is known as dominant design.\n\nThese are common icons on physical devices and application software. They are commonly found on portable media players, VCRs, DVD players, record players, remote controls, tape players and multimedia keyboards. Their application is described in ISO/IEC 18035.\n\nThe main symbols date back to the 1960s, with the Pause symbol having reportedly been invented at Ampex during that decade for use on reel-to-reel audio recorder controls, due to the difficulty of translating the word \"pause\" into some languages used in foreign markets. The Pause symbol was designed as a variation on the existing square Stop symbol and was intended to evoke the concept of an interruption or \"stutter stop\".\n\nIn popular culture, the play triangle is arguably the most widely used of the media control symbols. In many ways, the symbol has become synonymous with music culture and more broadly the digital download era. As such, there are now a multitude of items such as T-shirts, posters, and tattoos that feature this symbol. Similar cultural references can be observed with the Power symbol which is especially popular among video gamers and technology enthusiasts.\n\nMedia symbols can be found on an array of advertisements, from live music venues to streaming services. In 2012, Google rebranded its digital download store to Google Play, using a play symbol in its logo. The play symbol also serves as a logo for the Google-owned video sharing site YouTube since 2017.\n\nIn recent years, there has been a proliferation of devices that use media symbols in order to represent the Run/Stop/Pause functions. Likewise, user interface programing pertaining to these functions has also been influenced by that of media players. For example, some washers and dryers with a common illuminated play/pause button are programmed so that when the appliance is off, the play/pause light stays off. When the device is running the light stays on, and when the washer/dryer is in a paused state, the button flashes. This type of programing is similar to that of earlier CD players, which are also set to flash in this manner in the pause state.\n\nAside from appliances, there are many other instances when run/stop/pause functionality is needed and media symbols could theoretically be used instead of or in addition to words. In recent years, some exercise machine manufactures have chosen to do this. A notable difference in the use of these symbols on exercise equipment is that these machines usually contain an emergency stop button. Typically, this function is denoted with a red octagonal stop sign symbol, as opposed to a square, when the button may also be used for emergencies.\n\n"}
{"id": "904687", "url": "https://en.wikipedia.org/wiki?curid=904687", "title": "MoneyGram", "text": "MoneyGram\n\nMoneyGram International Inc. is a money transfer company based in the United States with headquarters in Dallas, Texas. It has an operations center in St. Louis Park, Minnesota and regional and local offices around the world. MoneyGram businesses are divided into two categories: Global Funds Transfers and Financial Paper Products. The company provides its service to individuals and businesses through a network of agents and financial institutions.\n\nMoneyGram is the second largest provider of money transfers in the world. The company operates in more than 200 countries with a global network of about 347,000 agent offices.\n\nMoneyGram International formed as a result of two businesses merging, Minneapolis-based Travelers Express and Denver-based Integrated Payment Systems Inc. MoneyGram was first established as a subsidiary of Integrated Payment Systems and afterwards became independent company before it was acquired by Travelers in 1998. In 2004, Travelers Express became what is known today as MoneyGram International.\n\nThe Minneapolis-based Travelers Express Co. Inc. was founded in 1940. In 1965, Travelers Express was acquired by The Greyhound Corporation (now known as Viad Corp) and became the nation's largest provider of money orders before initiating a company reorganization plan in 1993. \n\nMoneyGram was formed in 1988 as a subsidiary of Integrated Payment Systems Inc. Integrated Payment Systems was a subsidiary of First Data Corporation, which was itself a subsidiary of American Express. In 1992, First Data was spun off from American Express and publicly traded on the New York Stock Exchange. First Data Corporation later merged with First Financial, the owners of rival Western Union. In order to approve the merger, the Federal Trade Commission forced First Data to sell Integrated Payment Systems.\n\nThomas Cook Global Foreign Exchange, under the stewardship of John Bavister, launched a re-engineered money transfer service in 1994. Branded as MoneyGram, the venture saw the partnering of the global travel giant with First Data Corp.\n\nIn 1996, Integrated Payment Systems, the nation's second largest non-bank consumer money transfer business, became its own publicly traded company and was renamed MoneyGram Payment Systems Inc. In 1997, James F. Calvano, former president of Western Union, became MoneyGram Payment Systems CEO. By the late 1990s, MoneyGram Payment Systems had served customers at over 22,000 locations in 100 countries.\n\nMoneygram International Ltd. was established in 1997 by MoneyGram Payment Systems Inc and Thomas Cook, a year after the company had gone public. At the time when MoneyGram International was established, MoneyGram Payment Systems owned 51 percent of the company, while the other 49 percent was owned by the Thomas Cook Group.\n\nIn April 1998, Viad Corp acquired MoneyGram Payment Systems Inc. for $287 million. MoneyGram was then folded into Viad's Travelers Express in Minneapolis. I . November 2000 the Moneygram brand and business was sold to Travelex as part of its acquisition of Thomas Cook Financial Services for £400m. In 2003, Travelers Express gained full ownership of the MoneyGram network, including MoneyGram International. Later that year, Viad spun off Travelers Express as an independent company. In January 2004 and Travelers Express was renamed to MoneyGram International Inc. In June 2004, Viad sold MoneyGram and it became a publicly traded, individual entity.\n\nBy 2006, MoneyGram International had expanded internationally to include over 96,000 agents in regions such as the Asian-Pacific, Eastern Europe, and Central America. The company had also introduced additional services such as bill payment and online money transfers.\n\nDuring the financial crisis, MoneyGram's shares fell 96 percent from 2007 to 2009. It lost more than $1.6 billion from investments in securities backed by risky mortgages in 2008, and the losses led the company to sell a majority stake to Thomas H. Lee Partners and Goldman Sachs in exchange for a cash infusion. During the drop, U.S. Bancorp shifted its money transfer services to Western Union. The company began to see profitability again in 2009.\n\nAmid MoneyGram's turnaround, Pamela Patsley became the executive chairwoman of the company in January 2009 and was later named CEO in September of that year. In November 2010, MoneyGram officially relocated its global headquarters to the city of Dallas, Texas. The company continues to maintain global operations and information technology centers in Minneapolis, Minnesota.\n\nIn 2013 Moneygram began considering a sale. In 2014, Moneygram lost a relationship with Wal-Mart Stores Inc. and afterwards began restructuring to cut costs. From their peak in 2013 until late 2015, shares fell about 70%. Moneygram closed a Global Customer Care Operations center in Lakewood, CO resulting in over 500 layoffs. Furthermore, MoneyGram closed its 376-person Brooklyn Center operation in 2015. Moneygram has offshored numerous positions to Warsaw, Poland from its Colorado and Minnesota locations for additional cost cutting. In 2015, the company's agent network in Africa reached 25,000 locations, including an agreement with the Mauritius Post Office.\n\nBetween late October 2016 and January 2017, Moneygram's shares doubled in value. On January 26, 2017, Ant Financial Services Group announced a deal to acquire MoneyGram International for $880 million; the deal subsequently collapsed after it was rejected by the Committee on Foreign Investment in the United States.\n\nIn 2017, Moneygram announced that it uses Mitek's Mobile Verify to validate its customers’ identification. To complete the identity verification step in the money transfer process, Moneygram customers simply take a picture of their passport or other identity document using their mobile device camera.\n\n\n\nIn November 2012, MoneyGram International admitted to anti-money laundering and wire fraud violations. MoneyGram services were used by unrelated parties involved in mass marketing and consumer phishing scams that defrauded thousands of victims in the United States. As a part of the settlement, MoneyGram created a $100 million victim compensation fund. MoneyGram also retained a corporate monitor who will report regularly to the United States Department of Justice for a five-year trial period. If MoneyGram fulfills its obligations under the settlement, prosecutors will seek dismissal of the charges of aiding and abetting wire fraud. MoneyGram also terminated any agents complicit in the 2009 scams and invested more than $84 million in improvements to the company's consumer anti-fraud systems and consumer awareness education. In February 2015, MoneyGram assisted a Houston reporter in shutting down a fraud scam after discovering a scheme that utilized an account with the company.\n\nIn February 2016, MoneyGram agreed it would pay $13 million to end a probe stemming from customer complaints that scam artists duped them into wiring funds via the money transfer service. The settlement, with attorneys general in 49 states and Washington, D.C., includes $9 million for a nationwide fund that will facilitate the return of money to some MoneyGram customers and $4 million to cover states' costs and fees, according to numerous announcements by state attorneys general.\n\nMoneyGram launched the MoneyGram Foundation in 2013, which focuses on distributing grants internationally to support education. The MoneyGram Foundation distributed grants in 19 countries in its first year of operations. The Foundation gets the bulk of its funding from MoneyGram International, and builds on MoneyGram's previous Global Giving Program.\n\nThrough MoneyGram, Global Giving made a donation of $100,000 to World Vision International for education and school supplies, and another donation of $30,000 for the Girls Exploring Math and Science program in Dallas.\n\nMoneyGram participated in relief aid following the 2010 Haiti earthquake by reducing their fees to only $1 for any transactions to Haiti along with a $10,000 grant to Pan American Development Foundation and American Red Cross. In 2012, MoneyGram contributed to Hurricane Sandy relief efforts by pledging to donate $1 per transaction up to $200,000 to the American Red Cross.\n\nThe foundation has also contributed to other relief efforts following events such as Typhoon Haiyan in the Philippines. The company has also participated in the One Laptop per Child initiative and Habitat for Humanity through the MoneyGram Foundation.\n\n"}
{"id": "52174112", "url": "https://en.wikipedia.org/wiki?curid=52174112", "title": "Multibeam Corporation", "text": "Multibeam Corporation\n\nMultibeam is an American corporation that engages in the design of semiconductor processing equipment used in the fabrication of integrated circuits. Based in Santa Clara, in the Silicon Valley, Multibeam is led by Dr. David K. Lam, the founder and first CEO of Lam Research.\n\nMultibeam developed miniature, all-electrostatic columns for e-beam lithography. Arrays of e-beam columns operate simultaneously and in parallel to increase wafer processing speed. With 36 patents filed, Multibeam develops multicolumn e-beam systems and platforms for four major applications: Complementary E-Beam Lithography (CEBL), Direct Electron Writing (DEW), Direct Deposition/Etch (DDE), and E-Beam Inspection (EBI)\n\n"}
{"id": "10216271", "url": "https://en.wikipedia.org/wiki?curid=10216271", "title": "Multiphase flow meter", "text": "Multiphase flow meter\n\nA multiphase flow meter is a device used to measure the individual phase flow rates of constituent phases in a given flow (for example in oil and gas industry) where oil, water and gas mixtures are initially co-mingled together during the oil production processes.\n\nKnowledge of the individual fluid flow rates of a producing oil well is required to facilitate reservoir management, field development, operational control, flow assurance and production allocation.\n\nConventional solutions concerning two- and three-phase metering systems require expensive and cumbersome test separators, with associated high maintenance, and field personnel intervention. These conventional solutions do not lend themselves to continuous automated monitoring or metering. Moreover, with diminishing oil resources, oil companies are now frequently confronted with the need to recover hydrocarbons from marginally economical reservoirs. \nIn order to ensure economic viability of these accumulations, the wells may have to be completed subsea, or crude oil from several wells sent to a common production facility with excess processing capacity. The economic constraints on such developments do not lend themselves to the continued deployment of three-phase separators as the primary measurement devices. Consequently, viable alternatives to three-phase separators are essential. Industry’s response is the multiphase flow meter (MPFM).\n\nThe oil and gas industry began to be interested in developing MPFMs in the early 1980s, as measurement technology improved, and wellhead separators were costly. Depleting oil reserves, (More water and gas in the produced oil) along with smaller, deeper wells with higher water contents, saw the advent of increasingly frequent occurrences of multiphase flow where the single-phase meters were unable to provide accurate answers. After a lengthy gestation period, MPFMs capable of performing the required measurements became commercially available. Much of the early research was done at the Christian Michelsen research center in Bergen, Norway, and this work spawned a number of spin off companies in Norway leading to the Roxar / Emerson, Schlumberger, Framo, and MPM meters. ENI and Shell supported the development in Italy of the Pietro Fiorentini meter. Haimo introduced a meter with partial separation, making accurate measurement simpler, but at the expense of a physically larger device. Norway has remained a technology center for MPFM with the Norwegian Society for Oil and Gas Measurement (NFOGM) providing an academic and educational role. Since 1994, MPFM installation numbers have steadily increased as technology in the field has advanced, with substantial growth witnessed from 1999 onwards. A recent study estimated that there were approximately 2,700 MPFM applications including field allocation, production optimisation and mobile well testing in 2006.\n\nA number of factors have instigated the recent rapid uptake of multiphase measurement technology: improved meter performances, decreases in meter costs, more compact meters enabling deployment of mobile systems, the need for sub sea metering, increases in oil prices and a wider assortment of operators. As the initial interest in multiphase flow metering came from the offshore industry, most of the multiphase metering activity was concentrated in the North Sea. However, the present distribution of multiphase flow meters is much more diverse.\n\nMost modern meters combine a venturi flow rate meter, with a gamma densitometer, and some meters have additional measurements for water salinity. The meter measures the flow rates at line pressures, which are typically orders of magnitude greater than atmospheric pressure, but the meter must report the oil and gas volumes at standard (atmospheric) pressure and temperature. The meter must thus know the Pressure / Volume / Temperature properties of the oil, to add to the measured gas rate at line pressure the additional gas that would be liberated from the oil at atmospheric pressure, and also know the loss in oil volume from the release of that gas in conversion to standard conditions. With co-mingled flow from oil zones with differing PVT response, and different water salinities and hence densities, this PVT uncertainty may be the largest source of error in the measurement.\n\nThe introduction of the multi port selector valve (MSV) also facilitated the automation of the use of MPFM's, but this can also be achieved with conventional valving designs for well tests. MSV's are particularly suitable for onshore pad drilling,and where many nearby wells have similar pressures, and allow MPFM's to be shared between groups of wells. Sub Sea meters typically use conventional sub sea valve designs, to ensure maintainability.\n\nAlternatives to MPFM's include automated advance test separators. They separate gas from liquid for 2 Phase measurement. The main principle is to effectively separate the free gas from the liquid phase and to measure each phase independently, but with more automation than a conventional separator. With complete separation, the measurement equipment used in each phase can be utilized to their maximum effectiveness and potential.The separation of gas and liquid occurs in 2 stages. Stage 1, the liquid enters a vertical pipe at a downward tangential angle creating a cyclonic action in the pipe. This cyclonic action pushes the liquid towards the pipe wall and enables the majority of the gas to release to the center of the pipe and travel up to the gas run. The liquid with some remaining gas is carried into a secondary stage of separation.\n\nIn the 2nd stage of separation, the liquid with remaining gas flows along a horizontal section of pipe. The liquid level in the horizontal pipe is controlled in the middle of the pipe. The liquid level is controlled by a liquid level sensor in the horizontal pipe and a control valve in the gas run. As the liquid level rises in the horizontal pipe, the level sensor sends a signal to the control valve and which begins to pinch shut to create some slight back pressure to push the liquid level down. Conversely, as liquid level goes goes down, the control valve beings to open to relieve pressure to allow the liquid level to rise. In essence, the control valve modulates to maintain the liquid level in the middle of the horizontal pipe. With a large gas/liquid interface area, thin gas-bearing emulsion layer and quiescent flow in the horizontal pipe, all contribute to the final removal of free gas bubbles from the liquid stream. All of the free gas removed in the horizontal section joins the gas run through a connecting run and is measured in the gas leg. The liquid, now free of gas drops to a liquid run for measurement.\n\nIn the liquid run, Coriolis metering technology is typically employed for flow measurement. Water cut can be determined by either Net Oil Computer (density method) or through a separate water cut meter. The gas is typically measured with ultrasonic, vortex, or Coriolis technology. All technologies used in the Accuflow are already currently practiced and approved by all major oil companies. After measurement the gas and liquid streams are recombined and returned to the production line.\n\nBecause the multiphase stream is completely separated into liquid and gas stream prior to measurement, the systems can operate in all flow regimes. It is applicable to full range of gas fraction.\n\nMeasurement and interpretation of 2 and 3 phase multiphase flow can also be achieved by using alternative flow measurement technologies such as SONAR. SONAR meters apply the principles of underwater acoustics to measure flow regimes and; can be clamped on to wellheads and flow lines to measure the bulk (mean) fluid velocity of the total mixture which is then post-processed and analyzed along with wellbore compositional information and process conditions to infer the flow rates of each individual phase. This approached can be used in various applications such as black oil, gas condensate and wet gas.\n\nIndustry experts have forecast that MPFMs will become feasible on an installation per well basis when their capital cost falls to around US$40,000 – US$60,000. The cost of MPFMs today remains in the range of US$100,000 – US$500,000 (varying with onshore/offshore, topside/subsea, the physical dimensions of the meter and the number of units ordered). Installation of these MPFMs can cost up to 25% of the hardware cost and associated operating costs are estimated at between US$20,000 and $40,000 per year.\n\nA number of novel multiphase metering techniques, employing a variety of technologies, have been developed which eliminate the need for three-phase separator deployment. These MPFMs offer substantial economic and operating advantages over their phase separating predecessor. Nevertheless, it is still widely recognised that no single MPFM on the market can meet all multiphase metering requirements.\n\n"}
{"id": "1946437", "url": "https://en.wikipedia.org/wiki?curid=1946437", "title": "Muzzle (mouth guard)", "text": "Muzzle (mouth guard)\n\nA muzzle is a device that is placed over the snout of an animal to keep it from biting or otherwise opening its mouth.\n\nMuzzles can be primarily solid, with air holes to allow the animal to breathe, or formed from a set of straps that provides better air circulation and allow the animal to drink, and in some cases, eat. Leather, wire, plastic, and nylon are common materials for muzzles. The shape and construction of the muzzle might differ depending on whether the intent is to prevent an animal from biting or from eating, for example.\n\nMuzzles are sometimes used on untrained dogs, large or small, to prevent unwanted biting or scavenging. They can also be used on dogs who display aggression, whether motivated by excitement, fear or prey drive. They are usually made with a strong buckle or other fastening device to ensure that they do not come off accidentally. Muzzles are also used on dogs when there is a risk of them taking baits that have been laid for vermin. The most suitable materials for dog muzzles are leather, nylon, plastic and others. The most comfortable muzzles for dogs are those with wire cage construction. Muzzles of this kind give enough freedom for a dog to eat, drink and freely pant. The latter two are of vital importance, especially in hot weather.\n\nDog muzzles can be found in most pet supply stores.\n\nCertain muzzles with a wire-like structure are used on horses and related animals, usually to prevent biting or cribbing. Other types, known as \"grazing muzzles\", have a small opening in the center that allows limited intake of grass, and are used on obese horses or on those animals prone to laminitis or choke, to prevent them from eating too much or too fast.\n\nHorses trained for airscenting, for search and rescue or police work, often are worked in muzzles. This helps to keep them focused on their work, because they cannot easily snatch bites of grass.\n\nHorse muzzles may be purchased in tack shops or from equestrian supply companies, and are available as a halter attachment, or as a separate piece of equipment.\n\n\n"}
{"id": "4538803", "url": "https://en.wikipedia.org/wiki?curid=4538803", "title": "Ning Li (physicist)", "text": "Ning Li (physicist)\n\nNing Li is an American scientist known for her controversial claims about anti-gravity devices. She worked as a physicist at the Center for Space Plasma and Aeronomic Research, University of Alabama in Huntsville, in the 1990s. In 1999, she left the university to form a company, \"AC Gravity, LLC\", to continue anti-gravity research.\n\nIn a series of papers co-authored with fellow university physicist Douglas Torr and published between 1991 and 1993, she claimed a practical way to produce anti-gravity effects. She claimed that an anti-gravity effect could be produced by rotating ions creating a gravitomagnetic field perpendicular to their spin axis. In her theory, if a large number of ions could be aligned, (in a Bose–Einstein condensate) the resulting effect would be a very strong gravitomagnetic field producing a strong repulsive force. The alignment may be possible by trapping superconductor ions in a lattice structure in a high-temperature superconducting disc. Li claimed that experimental results confirmed her theories. Her claim of having functional anti-gravity devices was cited by the popular press and in popular science magazines with some enthusiasm at the time. However, in 1997 Li published a paper describing an experiment that showed the effect was very small, if it existed at all.\n\nLi is reported to have left the University of Alabama in 1999 to found the company \"AC Gravity LLC\". AC Gravity was awarded a U.S. DOD grant for $448,970 in 2001 to continue anti-gravity research. The grant period ended in 2002 but no results from this research were ever made public. Although no evidence exists that the company ever performed any other work, as of 2014, AC Gravity still remains listed as an \"existent\" business.\n\n\n"}
{"id": "57895391", "url": "https://en.wikipedia.org/wiki?curid=57895391", "title": "On-Demand Small Unmanned Aircraft System", "text": "On-Demand Small Unmanned Aircraft System\n\nThe On-Demand Small Unmanned Aircraft System (ODSUAS) is a custom-tailored, miniature 3D printed drone created by the U.S. Army Research Laboratory (ARL) and Georgia Technical Institute’s Aerospace Systems Design Laboratory. It was designed to provide flexible unmanned aircraft vehicle (UAV) support, where soldiers can input the specific requirements they need, such as size, weight, and endurance, into the mission-planning software before the drone is configured and 3D printed within 24 hours. The process allows the manufacturing of additional drone parts as needed, eliminating the need to carry spare parts for different configurations. While a 3D printer constructs the body of the UAV, the motors, sensors, cameras, propellers, and other apparatus would have to be obtained from a standing inventory. \n\nThe device can be tasked for missions involving perimeter surveillance, aerial defense, and reconnaissance. On December 2016, ARL researchers first tested it at Fort Benning, Georgia as part of the Army Expeditionary Warrior Experiments (AEWE) program. The researchers were given feedback s for possible improvements, such as improved agility, noise reduction, and heavier payload capacity. The ODSUAS was shown to demonstrate flight speeds of up to 55 miles per hour. \n\nARL researchers have stated that the manufacturing of 3D printed drones is only the first step of the project and that the overall goal is to leverage 3D printing as a way to produce mission-specific tools in a short amount of time.\n"}
{"id": "4250783", "url": "https://en.wikipedia.org/wiki?curid=4250783", "title": "Persona (user experience)", "text": "Persona (user experience)\n\nA persona, (also user persona, customer persona, buyer persona) in user-centered design and marketing is a fictional character created to represent a user type that might use a site, brand, or product in a similar way. Marketers may use personas together with market segmentation, where the qualitative personas are constructed to be representative of specific segments. The term persona is used widely in online and technology applications as well as in advertising, where other terms such as \"pen portraits\" may also be used.\n\nPersonas are useful in considering the goals, desires, and limitations of brand buyers and users in order to help to guide decisions about a service, product or interaction space such as features, interactions, and visual design of a website. Personas may also be used as part of a user-centered design process for designing software and are also considered a part of interaction design (IxD), having been used in industrial design and more recently for online marketing purposes.\n\nA user persona is a representation of the goals and behavior of a hypothesized group of users. In most cases, personas are synthesized from data collected from interviews with users . They are captured in 1–2-page descriptions that include behavior patterns, goals, skills, attitudes, and the environment, with a few fictional personal details to make the persona a realistic character. Personas are also widely used in sales, advertising, marketing and system design, Personas provide common behaviors, outlooks, and potential objections of people inherent to a given persona. \n\nWithin the software design domain, Alan Cooper, a noted pioneer software developer, proposed the concept of a \"user\" \"persona.\" Beginning in 1983, he started using a prototype of what the persona would become using data from informal interviews with seven to eight users. From 1995, he became engaged with how a specific rather than generalized user would use and interface with the software. The technique was popularized for the online business and technology community in his 1999 book \"The Inmates are Running the Asylum\". In this book, Cooper outlines the general characteristics, uses and best practices for creating personas, recommending that software be designed for single archetypal users.\n\nThe concept of understanding customer segments as communities with coherent identity was developed in 1993-4 by Angus Jenkinson and internationally adopted by OgilvyOne with clients using the name CustomerPrints as \"day-in-the-life archetype descriptions\". Creating imaginal or fictional characters to represent these customer segments or communities followed. Jenkinson's approach was to describe an imaginal character in their real interface, behavior and attitudes with the brand, and the idea was initially realized with Michael Jacobs in a series of studies. In 1997 the Ogilvy global knowledge management system, Truffles, described the concept as follows: \"Each strong brand has a tribe of people who share affinity with the brand’s values. This universe typically divides into a number of different communities within which there are the same or very similar buying behaviours, and whose personality and characteristics towards the brand (product or service) can be understood in terms of common values, attitudes and assumptions. CustomerPrints are descriptions that capture the living essence of these distinct groups of customers.\"\n\nAccording to Pruitt and Adlin, the use of personas offers several benefits in product development. Personas are said to be cognitively compelling because they put a personal human face on otherwise abstract data about customers. By thinking about the needs of a fictional persona, designers may be better able to infer what a real person might need. Such inference may assist with brainstorming, use case specification, and features definition. Pruitt and Adlin argue that personas are easy to communicate to engineering teams and thus allow engineers, developers, and others to absorb customer data in a palatable format. They present several examples of personas used for purposes of communication in various development projects.\n\nPersonas also help prevent some common design pitfalls which may otherwise be easy to fall into. The first is designing for what Cooper calls \"The Elastic User\" — by which he means that while making product decisions different stakeholders may define the 'user' according to their convenience. Defining personas helps the team have a shared understanding of the real users in terms of their goals, capabilities, and contexts. Personas also help prevent \"self-referential design\" when the designer or developer may unconsciously project their own mental models on the product design which may be very different from that of the target user population. Personas also provide a reality check by helping designers keep the focus of the design on cases that are most likely to be encountered for the target users and not on edge cases which usually won't happen for the target population. According to Cooper, edge cases which should naturally be handled properly should not become the design focus.\n\nThe benefits are summarized as:\n\nConcerning data, personas have typically been developed from surveys, focus groups, and other ethnographic data collection techniques. Most persona designers believe that personas should be based on ethnographic or other data collection approaches concerning customers and should not be based purely on the creator's imagination. The use of ethnographic research helps the creation of a number of archetype users that can be used to develop products that deliver positive user experiences. With the availability of large amounts of actual customer data, user data, social media data, and other web analytics data, personas derived from actual user/customer data.\n\nCriticism of personas falls into three general categories: analysis of the underlying logic, concerns about practical implementation, and empirical results.\n\nIn terms of scientific logic, it has been argued that because personas are fictional, they have no clear relationship to real customer data and therefore cannot be considered scientific. Chapman and Milham described the purported flaws in considering personas as a scientific research method. They argued that there is no procedure to work reliably from given data to specific personas, and thus such a process is not subject to the scientific method of reproducible research. However, in the age of abundant individual user data, there still is an apparent role for the use of personas, although the use may vary from their traditional employment. \n\nIn empirical results, the research to date has offered soft metrics for the success of personas, such as anecdotal feedback from stakeholders. Rönkkö has described how team politics and other organizational issues led to limitations of the personas method in one set of projects. Chapman, Love, Milham, Elrif, and Alford have demonstrated with survey data that descriptions with more than a few attributes (e.g., such as a persona) are likely to describe very few if any real people. They argued that personas cannot be assumed to be descriptive of actual customers.\n\nA study conducted by Long claimed support for Cooper, Pruitt \"et al.\" in the use of personas. In a partially controlled study, a group of students were asked to solve a design brief; two groups used personas while one group did not. The students who used personas were awarded higher course evaluations than the group who did not. Students who used personas were assessed as having produced designs with better usability attributes than students who did not use personas. The study also suggests that using personas may improve communication between design teams and facilitate user-focused design discussion. The study had several limitations: outcomes were assessed by a professor and students who were not blind to the hypothesis, students were assigned to groups in a non-random fashion, the findings were not replicated, and other contributing factors or expectation effects (e.g., the Hawthorne effect or Pygmalion effect) were not controlled for.\n\n\n"}
{"id": "204410", "url": "https://en.wikipedia.org/wiki?curid=204410", "title": "Photographic paper", "text": "Photographic paper\n\nPhotographic paper is a paper coated with a light-sensitive chemical formula, used for making photographic prints. When photographic paper is exposed to light, it captures a latent image that is then developed to form a visible image; with most papers the image density from exposure can be sufficient to not require further development, aside from fixing and clearing, though latent exposure is also usually present. The light-sensitive layer of the paper is called the emulsion. The most common chemistry was based on silver salts (the focus of this page) but other alternatives have also been used.\n\nThe print image is traditionally produced by interposing a photographic negative between the light source and the paper, either by direct contact with a large negative (forming a contact print) or by projecting the shadow of the negative onto the paper (producing an enlargement). The initial light exposure is carefully controlled to produce a gray scale image on the paper with appropriate contrast and gradation. Photographic paper may also be exposed to light using digital printers such as the LightJet, with a camera (to produce a photographic negative), by scanning a modulated light source over the paper, or by placing objects upon it (to produce a photogram).\n\nDespite the introduction of digital photography, photographic papers are still sold commercially. Photographic papers are manufactured in numerous standard sizes, paper weights and surface finishes. A range of emulsions are also available that differ in their light sensitivity, colour response and the warmth of the final image. Color papers are also available for making colour images.\n\nThe effect of light in darkening a prepared paper was discovered by Thomas Wedgwood in 1802. Photographic papers have been used since the beginning of all negative–positive photographic processes as developed and popularized by William Fox Talbot (Great Britain/1841-calotype).\n\nAfter the early days of photography, papers have been manufactured on a large scale with improved consistency and greater light sensitivity.\n\nPhotographic papers fall into one of three sub-categories:\n\nAll photographic papers consist of a light-sensitive emulsion, consisting of silver halide salts suspended in a colloidal material - usually gelatin- coated onto a paper, resin coated paper or polyester support. In black-and-white papers, the emulsion is normally sensitised to blue and green light, but is insensitive to wavelengths longer than 600 nm in order to facilitate handling under red or orange safelighting. In Chromogenic colour papers, the emulsion layers are sensitive to red, green and blue light, respectively producing cyan, magenta and yellow dye during processing.\n\nModern black-and-white papers are coated on a small range of bases; baryta-coated paper, resin-coated paper or polyester. In the past, linen has been used as a base material.\n\nFiber-based (FB or Baryta) photographic papers consist of a paper base coated with baryta. Tints are sometimes added to the baryta to add subtle colour to the final print; however most modern papers use optical brighteners to extend the paper's tonal range. Most fiber-based papers include a clear hardened gelatin layer above the emulsion which protects it from physical damage, especially during processing. This is called a supercoating. Papers without a supercoating are suitable for use with the bromoil process. Fiber-based papers are generally chosen as a medium for high-quality prints for exhibition, display and archiving purposes. These papers require careful processing and handling, especially when wet. However, they are easier to tone, hand-colour and retouch than resin-coated equivalents.\n\nThe paper base of resin-coated papers is sealed by two polyethylene layers, making it impenetrable to liquids. Since no chemicals or water are absorbed into the paper base, the time needed for processing, washing and drying durations are significantly reduced in comparison to fiber-based papers. Resin paper prints can be finished and dried within twenty to thirty minutes. Resin-coated papers have improved dimensional stability, and do not curl upon drying.\n\nThe term \"baryta\" derives from the name of a common barium sulfate-containing mineral, barite. However, the substance used to coat photographic papers is usually not pure barium sulfate, but a mixture of barium and strontium sulfates. The ratio of strontium to barium differs among commercial photographic papers, so chemical analysis can be used to identify the maker of the paper used to make a print and sometimes when the paper was made. The baryta layer has two functions 1) to brighten the image and 2) to prevent chemicals adsorbed on the fibers from infiltrating the gelatin layer. The brightening occurs because barium sulfate is in the form of a fine precipitate that scatters light back through the silver image layer. In the early days of photography, before baryta layers were used, impurities from the paper fibers could gradually diffuse into the silver layer and cause an uneven loss of sensitivity (before development) or mottle (unevenly discoluor) the silver image (after development).\n\nAll colour photographic materials available today are coated on either RC (resin coated) paper or on solid polyester. The photographic emulsion used for colour photographic materials consists of three colour emulsion layers (cyan, yellow, and magenta) along with other supporting layers. The colour layers are sensitised to their corresponding colours. Although it is commonly believed that the layers in negative papers are shielded against the intrusion of light of a different wavelength than the actual layer by colour filters which dissolve during processing, this is not so. The colour layers in negative papers are actually produced to have speeds which increase from cyan (red sensitive) to magenta (green sensitive) to yellow (blue sensitive), and thus when filtered during printing, the blue light is \"normalized\" so that there is no crosstalk. Therefore, the yellow (blue sensitive) layer is nearly ISO 100 while the cyan (red) layer is about ISO 25. After adding enough yellow filtration to make a neutral, the blue sensitivity of the slow cyan layer is \"lost\". \n\nIn negative-positive print systems, the blue sensitive layer is on the bottom, and the cyan layer is on the top. This is the reverse of the usual layer order in colour films.\n\nThe emulsion layers can include the colour dyes, as in Ilfochrome; or they can include colour couplers, which react with colour developers to produce colour dyes, as in type C prints or chromogenic negative–positive prints. Type R prints, which are no longer made, were positive–positive chromogenic prints.\n\nThe emulsion contains light sensitive silver halide crystals suspended in gelatin. Black-and-white papers typically use relatively insensitive emulsions composed of silver bromide, silver chloride or a combination of both. The silver halide used affects the paper's sensitivity and the image tone of the resulting print.\n\nPopular in the past, chloride papers are nowadays unusual; a single manufacturer produces this material. These insensitive papers are suitable for contact printing, and yield warm toned images by development. Chloride emulsions are also used for printing-out papers, or POP, which require no further development after exposure.\nContaining a blend of silver chloride and silver bromide salts, these emulsions produce papers sensitive enough to be used for enlarging. They produce warm-black to neutral image tones by development, which can be varied by using different developers.\n\nPapers with pure silver bromide emulsions are sensitive and produce neutral black or 'cold' blue-black image tones.\n\nFixed grade - or graded - black-and-white papers were historically available in a wide range of contrast total of buyers 12\ngrades, numbered 0 to 5, with 0 being the \"softest\", or least contrasty paper grade and 5 being the \"hardest\", or most contrasty paper grade. Low contrast negatives can be corrected by printing on a contrasty paper; conversely a very contrasty negative can be printed on a low contrast paper. Because of decreased demand, most extreme paper grades are now discontinued, and the few graded ranges still available include only middle contrast grades.\n\n\"Variable-contrast\" - or \"VC\" papers - account for the great majority of consumption of these papers in the 21st century. VC papers permit the selection of a wide range of contrast grades, in the case of the brand leader between 00 and 5. These papers are coated with a mixture of two or three emulsions, all of equal contrast and sensitivity to blue light. However, each emulsion is sensitised in different proportions to green light. Upon exposure to blue light, all emulsions act in an additive manner to produce a high contrast image. When exposed to green light alone, the emulsions produce a low contrast image because each is differently sensitised to green. By varying the ratio of blue to green light, the contrast of the print can be approximately continuously varied between these extremes, creating all contrast grades from 00 to 5.\n\nFilters in the enlarger's light path are a common method of achieving this control. Magenta filters absorb green and transmit blue and red, while yellow filters absorb blue and transmit green and red.\nThe contrast of photographic papers can also be controlled during processing or by the use of bleaches or toners.\n\nPanchromatic black-and-white photographic printing papers are sensitive to all wavelengths of visible light. They were designed for the printing of full-tone black-and-white images from colour negatives; this is not possible with conventional orthochromatic papers. Panchromatic papers can also be used to produce paper negatives in large-format cameras. These materials must be handled and developed in near-complete darkness. Kodak Panalure Select RC is an example of a panchromatic black-and-white paper; it was discontinued in 2005.\n\nNumerous photo sensitive papers that do not use silver chemistry exist. Most are hand made by enthusiasts but Cyanotype prints are made on what was commonly sold as blueprint paper. Certain precious metal including platinum and other chemistries have also been in common use at certain periods.\n\nThe longevity of any photographic print media will depend upon the processing, display and storage conditions of the print.\n\nFixing must convert all non-image silver into soluble silver compounds that can be removed by washing with water. Washing must remove these compounds and all residual fixing chemicals from the emulsion and paper base. A hypo-clearing solution, also referred to as Hypo Clearing Agent, HCA, or a washing aid, and which can consist of a 2% solution of sodium sulfite, can be used to shorten the effective washing time by displacing the thiosulfate fixer, and the byproducts of the process of fixation, that are bound to paper fibers.\n\nToners are sometimes used to convert the metallic silver into more stable compounds. Commonly used archival toners are: selenium, gold and sulfide.\n\nPrints on fiber-based papers that have been properly fixed and washed should last at least fifty years without fading. Some alternative non-silver processes - such as platinum prints - employ metals that are, if processed correctly, inherently more stable than gelatin-silver prints.\n\nFor colour images, Ilfochrome is often used because of its clarity and the stability of the colour dyes.\n\n"}
{"id": "1831174", "url": "https://en.wikipedia.org/wiki?curid=1831174", "title": "Plastic wrap", "text": "Plastic wrap\n\nPlastic wrap, cling film, shrink wrap, Saran wrap, cling wrap, food wrap, or pliofilm is a thin plastic film typically used for sealing food items in containers to keep them fresh over a longer period of time. Plastic wrap, typically sold on rolls in boxes with a cutting edge, clings to many smooth surfaces and can thus remain tight over the opening of a container without adhesive. Common plastic wrap is roughly 0.0005 inches (12.7 μm) thick. The trend has been to produce thinner plastic wrap, particularly for household use (where very little stretch is needed), so now the majority of brands on shelves around the world are 8, 9 or 10 μm thick.\n\nPlastic wrap was initially created from polyvinyl chloride (PVC), which remains the most common component globally. PVC allows permeability to water vapor and oxygen transmission, which can add to the duration of peak freshness of the food products wrapped. There are concerns about the transfer of plasticizers from PVC into food.\n\nA common, cheaper alternative to PVC is low-density polyethylene (LDPE). It is less adhesive than PVC, but this can be remedied by adding linear low-density polyethylene (LLDPE), which also increases the film's tensile strength.\n\nIn the US and Japan, plastic wrap is sometimes produced using polyvinylidene chloride (PVdC), though some brands, such as Saran wrap, have switched to other formulations due to environmental concerns.\n\nPlastic wrap is often used to wrap food to preserve its freshness and to prevent contact with insects and any other potential contaminants.\n\n\n"}
{"id": "30876930", "url": "https://en.wikipedia.org/wiki?curid=30876930", "title": "Powered exoskeleton", "text": "Powered exoskeleton\n\nPowered exoskeleton (also known as power armor, powered armor, powered suit, exoframe, hardsuit or exosuit) is a wearable mobile machine that is powered by a system of electric motors, pneumatics, levers, hydraulics, or a combination of technologies that allow for limb movement with increased strength and endurance,\n\nThe earliest known exoskeleton-like device was a set of walking, jumping and running assisted apparatus developed in 1890 by a Russian named Nicholas Yagn. The apparatus used energy stored in compressed gas bags to assist with movements, although it was passive and required human power. In 1917, United States inventor Leslie C. Kelley developed what he called a pedomotor, which operated on steam power with artificial ligaments acting in parallel to the wearer's movements. With the pedomotor, energy could be generated apart from the user.\n\nThe first true exoskeleton in the sense of being a mobile machine integrated with human movements was co-developed by General Electric and the United States Armed Forces in the 1960s. The suit was named Hardiman, and made lifting 110 kilograms (250 lb) feel like lifting 4.5 kilograms (10 lb). Powered by hydraulics and electricity, the suit allowed the wearer to amplify their strength by a factor of 25, so that lifting 25 kilograms was as easy as lifting one kilogram without the suit. A feature dubbed force feedback enabled the wearer to feel the forces and objects being manipulated.\n\nWhile the general idea sounded somewhat promising, the Hardiman had major limitations. It was impractical, due to its 680-kilogram (1,500 lb) weight. Another issue was that it is a master-slave system, where the operator is in a master suit, which, in turn, is inside the slave suit that responds to the master and handles the workload. This multiple physical layer type of operation may work fine, but responds slower than a single physical layer. When the goal is physical enhancement, response time matters. Its slow walking speed of 0.76 metres per second (2.5 ft/s) further limited practical uses. The project was not successful. Any attempt to use the full exoskeleton resulted in a violent uncontrolled motion, and as a result it was never tested with a human inside. Further research concentrated on one arm. Although it could lift its specified load of 340 kg (750 lb), it weighed three quarters of a ton, just over twice the liftable load. Without getting all the components to work together, the practical uses for the Hardiman project were limited.\n\nEarly active exoskeletons and humanoid robots were developed at the Mihajlo Pupin Institute in 1969, under the guidance of Prof. Miomir Vukobratović. Legged locomotion systems were developed first. Theories of these systems were developed in the same institute, in the frame of active exoskeletons. Active exoskeletons were predecessors of the modern high-performance humanoid robots. The present-day active exoskeletons are developed as the systems for enhancing capabilities of the natural human skeletal system. The most successful version of an active exoskeleton for rehabilitation of paraplegics and similar disabled persons, pneumatically powered and electronically programmed, was realized and tested at Belgrade Orthopedic Clinic in 1972. One specimen was delivered to the Central Institute for Traumatology and Orthopedy, Moscow, in the frame of the USSR-Yugoslav inter-state scientific cooperation. From 1991 the exoskeleton belongs to the basic fund of Polytechnic Museum (Moscow) and State Museum Fund of Russian Federation. It is displayed in the frame of the museum's exposition dedicated to the development of automation and cybernetics.\nLos Alamos Laboratories worked on an exoskeleton project in the 1960s called Project Pitman. In 1986, an exoskeleton prototype called the LIFESUIT was created by Monty Reed, a United States Army Ranger who had broken his back in a parachute accident. While recovering in the hospital, he read Robert Heinlein's science fiction novel, \"Starship Troopers\", and from Heinlein's description of Mobile Infantry Power Suits, he designed the LIFESUIT, and wrote letters to the military about his plans for the LIFESUIT. In 2001 LIFESUIT One (LSI) was built. In 2003 LS6 was able to record and play back a human gait. In 2005 LS12 was worn in a foot race known as the Saint Patrick's Day Dash in Seattle, Washington. Monty Reed and LIFESUIT XII set the Land Speed Distance Record for walking in robot suits. LS12 completed the 4.8-kilometre (3 mi) race in 90 minutes. The current LIFESUIT prototype 14 can walk 1.6 km (1 mi) on a full charge and lift 92 kg (203 lb) for the wearer.\n\nIn January 2007, \"Newsweek\" magazine reported that the Pentagon had granted development funds to a nanotechnologist, Ray Baughman of the University of Texas at Dallas, to develop military-grade artificial electroactive polymers. These electrically contractive fibers are intended to increase the strength-to-weight ratio of movement systems in military powered armor.\n\nOne of the main applications would be medical—improving the quality of life of persons who have, for example, lost the use of their legs, by providing assistive technology to enable system-assisted walking or restoration of other motor controls lost due to illness or accidental injury.\n\nAnother area of application could be medical care, nursing in particular. Faced with the impending shortage of medical professionals and the increasing number of people in elderly care, several teams of Japanese engineers have developed exoskeletons designed to help nurses lift and carry patients.\n\nExoskeletons can also be applied in the area of rehabilitation of stroke or spinal cord injury patients. Such exoskeletons are sometimes also called Step Rehabilitation Robots. An exoskeleton could reduce the number of therapists needed by allowing even the most impaired patient to be trained by one therapist, whereas several are currently needed. Also training would be more uniform, easier to analyze retrospectively and can be specifically customized for each patient. At this time there are several projects designing training aids for rehabilitation centers (LOPES exoskeleton, Lokomat, Modular robotic exoskeleton UniExo, CAPIO and the gait trainer, HAL 5.)\n\nRehabilitation exoskeletons can be configured such that they provide a minimal amount of assistance. In this way, they can electronically maximize the patient's efforts when possible and thus provide a more rigorous, targeted therapy session. Ekso Bionics of Richmond California has developed the Ekso GT, which incorporates this ability: \"The SmartAssist software allows physical therapists to vary the support of the device for each leg independently—from full power to free walking—and thereby meet the specific needs of patients. This capability enables the Ekso GT to rehabilitate a larger range of patients, from those too weak to walk to those who are nearly independent.\" The Ekso GT is also the first exoskeleton to be approved by the FDA for stroke patients.\n\nGerman Research Centre for Artificial Intelligence developed two general purpose powered exoskeletons CAPIO and VI-Bot. They also considered human force sensitivities in the design and operation phases. Teleoperation and power amplification were said to be the first applications, but after recent technological advances the range of application fields is said to have widened. Increasing recognition from the scientific community means that this technology is now employed in telemanipulation, man-amplification, neuromotor control research and rehabilitation, and to assist with impaired human motor control (\"Wearable Robots: Biomechatronic Exoskeletons\").\n\nThe medical field is another prime area for exoskeleton technology, where it can be used for enhanced precision during surgery, or as an assist to allow nurses to move heavy patients.\n\nThere are an increasing number of applications for an exoskeleton, such as decreased fatigue and increased productivity whilst unloading supplies or enabling a soldier to carry heavy objects (40–300 kg) while running or climbing stairs. Not only could a soldier potentially carry more weight, presumably, they could wield heavier armor and weapons while lowering their metabolic rate or maintaining the same rate with more carry capacity. Some models use a hydraulic system controlled by an on-board computer. They could be powered by an internal combustion engine, batteries, or potentially fuel cells.\n\nIn civilian areas, exoskeletons could be used to help firefighters and other rescue workers survive dangerous environments.\n\n\n\nEngineers of powered exoskeletons face a number of large technological challenges to build a suit that is capable of quick and agile movements, yet is also safe to operate without extensive training.\n\nOne of the biggest problems facing engineers and designers of powered exoskeletons is the power supply. There are currently few power sources of sufficient energy density to sustain a full-body powered exoskeleton for more than a few hours.\n\nNon-rechargeable primary cells tend to have more energy density and store it longer than rechargeable secondary cells, but then replacement cells must be transported into the field for use when the primary cells are depleted, of which may be a special and uncommon type. Rechargeable cells can be reused, but may require transporting a charging system into the field, which either must recharge rapidly or the depleted cells need to be able to be swapped out in the field, to be replaced with cells that have been slowly charging.\n\nFurther, chemical reactions can occur between substances used in rechargeable batteries, such as lithium, with atmospheric oxygen in the event of a battery being damaged, resulting in fire or explosion. Recent research by John Goodenough and a team at the University of Texas at Austin into glass battery technology is highly applicable to exoskeletal power research, as these batteries benefit from a solid-state electrolyte and improved energy density compared to traditional rechargeable cells.\n\nInternal combustion engine power supplies offer high energy output, but they also typically \"idle\", or continue to operate at a low power level sufficient to keep the engine running, when not actively in use, which continuously consumes fuel. Battery-based power sources are better at providing \"instantaneous\" and \"modulated\" power; stored chemical energy is conserved when load requirements cease. Engines that do not idle are possible, but require energy storage for a starting system capable of rapidly accelerating the engine to full operating speed, and the engine must be extremely reliable and never fail to begin running immediately.\n\nSmall and lightweight engines typically must operate at high speed to extract sufficient energy from a small engine cylinder volume, which both can be difficult to silence and induces vibrations into the overall system. Internal combustion engines can also get extremely hot, which may require additional weight from cooling systems or heat shielding.\n\nElectrochemical fuel cells such as solid oxide fuel cells (SOFC) are also being considered as a power source since they can produce instantaneous energy like batteries and conserve the fuel source when not needed. They can also easily be refueled in the field with liquid fuels such as methanol. However they require high temperatures to function; 600 °C is considered a low operating temperature for SOFCs.\n\nAs of 2015, most research designs are tethered to a much larger separate power source. For a powered exoskeleton that will not need to be used in completely standalone situations such as a battlefield soldier, this limitation may be acceptable, and the suit may be designed to be used with a permanent power umbilical. This is particularly useful in logistical support and some industrial areas.\n\nInitial exoskeleton experiments are commonly done using inexpensive and easy to mold materials, such as steel and aluminium. However, steel is heavy and the powered exoskeleton must work harder to overcome its own weight in order to assist the wearer, reducing efficiency. The aluminium alloys used are lightweight, but fail through fatigue quickly; it would be unacceptable for the exoskeleton to fail catastrophically in a high-load condition by \"folding up\" on itself and injuring the wearer.\n\nAs the design moves past the initial exploratory steps, the engineers move to progressively more expensive and strong, but lightweight materials, such as titanium, and use more complex component construction methods, such as molded carbon-fiber plates.\n\nThe powerful, but lightweight design issues are also true of the joint actuators. Standard hydraulic cylinders are powerful and capable of being precise, but they are also heavy due to the fluid-filled hoses and actuator cylinders, and the fluid has the potential to leak onto the user. Pneumatics are generally too unpredictable for precise movement since the compressed gas is springy, and the length of travel will vary with the gas compression and the reactive forces pushing against the actuator.\n\nPressurized hydraulic fluid leaks can be dangerous to humans. A jet squirting from a pinhole leak can penetrate skin at pressures as low as 100 PSI / 6.9 bar. If the injected fluid is not surgically removed, gangrene and poisoning can occur.\n\nGenerally electronic servomotors are more efficient and power-dense, utilizing high-gauss permanent magnets and step-down gearing to provide high torque and responsive movement in a small package. Geared servomotors can also utilize electronic braking to hold in a steady position while consuming minimal power.\n\nAdditionally, new series elastic actuators and other deformable actuators are being proposed for use in robotic exoskeletons based on the ideas of control of stiffness in human limbs.\nPneumatic artificial muscles are a new technology in pneumatic actuators. In this actuator, the volume of the cylinder changes, aiding performance according to thermodynamic principles.\n\nA similar artificial muscle is the air muscle, also known as the Braided Pneumatic Actuator, a lightweight and very flexible design that is more powerful than any other type of pneumatic actuator.\n\nMechanical advantage devices such as levers and pulleys are also being used as actuators, but it has not yet been proven that they can actually increase strength or reduce fatigue.\n\nFlexibility of the human anatomy is another design issue, which also affects the design of unpowered hard shell space suits. Several human joints such as the hips and shoulders are ball and socket joints, with the center of rotation inside the body. It is difficult for an exoskeleton to exactly match the motions of this ball joint using a series of external single-axis hinge points, limiting flexibility of the wearer.\n\nA separate exterior ball joint can be used alongside the shoulder or hip, but this then forms a series of parallel rods in combination with the wearer's bones. As the external ball joint is rotated through its range of motion, the positional length of the knee/elbow joint will lengthen and shorten, causing joint misalignment with the wearer's body. This slip in suit alignment with the wearer can be permitted, or the suit limbs can be designed to lengthen and shorten under power assist as the wearer moves, to keep the knee/elbow joints in alignment.\n\nA partial solution for more accurate free-axis movement is a hollow spherical ball joint that encloses the human joint, with the human joint as the center of rotation for the hollow sphere. Rotation around this joint may still be limited unless the spherical joint is composed of several plates that can either fan out or stack up onto themselves as the human ball joint moves through its full range of motion.\n\nSpinal flexibility is another challenge since the spine is effectively a stack of limited-motion ball joints. There is no simple combination of external single-axis hinges that can easily match the full range of motion of the human spine. A chain of external ball joints behind the spine can perform a close approximation, though it is again the parallel-bar length problem. Leaning forward from the waist, the suit shoulder joints would press down into the wearer's body. Leaning back from the waist, the suit shoulder joints would lift off the wearer's body. Again, this alignment slop with the wearer's body can be permitted, or the suit can be designed to rapidly lengthen or shorten the exoskeleton spine under power assist as the wearer moves.\n\nThe NASA Ames Research Center experimental AX-5 hard-shell space suit (1988), had a flexibility rating of 95%, compared to what movements are possible while not wearing the suit. It is composed of gasketed hard shell sections joined with free-rotating mechanical bearings that spin around as the person moves.\n\nHowever, the free-rotating hard sections have no limit on rotation and can potentially move outside the bounds of joint limits. It requires high precision manufacturing of the bearing surfaces to prevent binding, and the bearings may jam if exposed to lunar dust.\n\nControl and modulation of excessive and unwanted movement is a third large problem. It is not enough to build a simple single-speed assist motor, with forward/hold/reverse position controls and no on-board computer control. Such a mechanism can be too fast for the user's desired motion, with the assisted motion overshooting the desired position. If the wearer's body is enclosed with simple contact surfaces that trigger suit motion, the overshoot can result the wearer's body lagging behind the suit limb position, resulting in contact with a position sensor to move the exoskeleton in the opposite direction. This lagging of the wearer's body can lead to an uncontrolled high-speed oscillatory motion, and a powerful assist mechanism can batter or injure the operator unless shut down remotely. (An \"underdamped servo\" typically exhibits oscillations like this.)\n\nA single-speed assist mechanism which is slowed down to prevent oscillation is then restrictive on the agility of the wearer. Sudden unexpected movements such as tripping or being pushed over requires fast precise movements to recover and prevent falling over, but a slow assist mechanism may simply collapse and injure the user inside. (This is known as an \"overdamped servo\".)\n\nFast and accurate assistive positioning is typically done using a range of speeds controlled using computer position sensing of both the exoskeleton and the wearer, so that the assistive motion only moves as fast or as far as the motion of the wearer and does not overshoot or undershoot. (This is called a \"critically damped servo\".) This may involve rapidly accelerating and decelerating the motion of the suit to match the wearer, so that their limbs slightly press against the interior of the suit and then it moves out of the way to match the wearer's motion. The computer control also needs to be able to detect unwanted oscillatory motions and shut down in a safe manner if damage to the overall system occurs.\n\nA fourth issue is detection and prevention of invalid or unsafe motions, which is managed by an on-board real-time computational \"Self-Collision Detection System\".\n\nIt would be unacceptable for an exoskeleton to be able to move in a manner that exceeds the range of motion of the human body and tear muscle ligaments or dislocate joints. This problem can be partially solved using designed limits on hinge motion, such as not allowing the knee or elbow joints to flex backwards onto themselves.\n\nHowever, the wearer of a powered exoskeleton can additionally damage themselves or the suit by moving the hinge joints through a series of combined and otherwise valid movements which together cause the suit to collide with itself or the wearer.\n\nA powered exoskeleton would need to be able to computationally track limb positions and limit movement so that the wearer does not casually injure themselves through unintended assistive motions, such as when coughing, sneezing, when startled, or if experiencing a sudden uncontrolled seizure or muscle spasm.\n\nAn exoskeleton is typically constructed of very strong and hard materials, while the human body is much softer than the alloys and hard plastics used in the exoskeleton. Human skin is also covered with hair over the majority of the surface area. An exoskeleton typically cannot be worn directly in contact with bare skin due to the potential for skin and hair pinching where the exoskeleton plates and servos slide across each other. Instead the wearer may be enclosed in a heavy fabric suit to protect them from joint pinch hazards.\n\nCurrent exoskeleton joints themselves are also prone to environmental fouling from sand and grit, and may need protection from the elements to keep operating effectively. A traditional way of handling this is with seals and gaskets around rotating parts, but can also be accomplished by enclosing the exoskeleton mechanics in a tough fabric suit separate from the user, which functions as a protective \"skin\" for the exoskeleton. This enclosing suit around the exoskeleton can also protect the wearer from pinch hazards.\n\nMost exoskeletons pictured in this article typically show a fixed length distance between joints, but humans exhibit a wide range of physical size differences and skeletal bone lengths, so a one-size-fits-all fixed-size exoskeleton would not work. Although military use would generally use only larger adult sizes, civilian use may extend across all size ranges, including physically disabled babies and small children.\n\nThere are several possible solutions to this problem:\n\nA further difficulty is that not only is there variation in bone lengths, but also limb girth due to bone geometry, muscle build, fat, and any user clothing layering such as insulation for extreme cold or hot environments. An exoskeleton will generally need to fit the user's limb girth snugly so that their arms and legs are not loose inside and flopping around an oversized exoskeleton cavity, or so tight that the user's skin is lesioned from abrasion from a too-small exoskeleton cavity.\n\n\nAs of 2018, the U.S. Occupational Safety and Health Administration was not preparing any safety standards for exoskeletons. The International Organization for Standardization published a safety standard in 2014, and ASTM International was working on standards to be released beginning in 2019.\n\n\n"}
{"id": "842479", "url": "https://en.wikipedia.org/wiki?curid=842479", "title": "Prestel", "text": "Prestel\n\nPrestel (abbrev. from press telephone), the brand name for the UK Post Office's Viewdata technology, was an interactive videotex system developed during the late 1970s and commercially launched in 1979. It achieved a maximum of 90,000 subscribers in the UK and was eventually sold by BT in 1994.\n\nThe technology was a forerunner of on-line services today. Instead of a computer, a television set hooked to a dedicated terminal was used to receive information from a remote database via a telephone line. The service offered thousands of pages ranging from consumer information to financial data but with limited graphics.\n\nPrestel was created based on the work of Samuel Fedida at the then Post Office Research Station in Martlesham, Suffolk. In 1978, under the management of David Wood the software was developed by a team of programmers recruited from within the Post Office Data Processing Executive. As part of the privatisation of British Telecom, the team were moved into a \"Prestel Division\" of BT.\n\nPrestel databases is commonly referred to as the ‘tree structure’. The structure is shown pictorially as an inverted tree with the data considered as ‘leaves’ of the tree, accessed via ‘branches’ which serve as a means of classifying the information. There exists quite a lot of jargon regarding such structures but in order to appreciate the concept it is necessary to mention just the node, page and frame. Nodes are the junction pages in the tree at which a number of choices can be made leading to other nodes or to the information itself. Pages are the final levels in the tree and contain the actual data-these may be divided into frames which are really screenfuls of information.\n\nThe public Prestel database consisted of a set of individual frames, which were arranged in 24 lines of 40 characters each, similar to the display used by the Ceefax and ORACLE teletext services provided by the BBC and ITV television companies. Of these, the top line was reserved for the name of the Information Provider, the price and the page number, and the bottom line was reserved for system messages. Thus there remained 22 lines (of 40 characters each) in which the IP could present information to the end user.\n\nA page should be considered as a logical unit of data within the database and the frame a physical unit. Unfortunately the terms node, page and frame are often used synonymously which may lead to some confusion. To the user of course a node is the same as a page and they are both identified by a ‘page’ number. To access a particular item of information, a simple progression down through the nodes to the page is all that is required, and then the frames of that page can be stepped through. This is facilitated by each node displaying up to ten choices, one of which may be taken by the user responding with the appropriate digit from 0 to 9. This simple method of access may be thought of as a question and answer session: the computer displays a question ‘Which of the ten choices do you want to make?’ and the user replies with the appropriate digit. A choice of 9 at node 17 moves the user to page 179. The flexibility of this logical access method is increased firstly by allowing cross-referencing from one branch of the tree to another and secondly by providing a few simple commands available to the user for accessing certain pages directly.\nWhile this principle had considerable advantages in user simplicity and computer efficiency over the “keyword/thesaurus principle” used in many other systems, it has two very real disadvantages which have now been recognized: lack of flexibility; slowness.\n\nPage numbers were from one to nine digits in length, i.e., in the range 0 to 999999999 created in a tree like structure, whereby lower level pages could only exist if the higher numbered “parent” pages had already been created. Thus creating page 7471 required pages 747, 74 and 7 to exist, but generally the three digit node 747 would have been created in order to register the relevant main IP account. Single and double digit pages were special pages reserved by Prestel for general system information purposes, as were the 1nn-199nn sets of three digit nodes e.g. page 1a was the standard Prestel Main Index. Pages starting with a 9 were for system management functions, and were limited to three digits in length. e.g., page 92 showed details of the users Prestel bill, and page 910 gave IPs access to online editing facilities.\n\nAvailable characters consisted of upper and lower case alphanumeric characters as well as punctuation and simple arithmetic symbols, using a variant of ISO 646 and CCITT standard. This layout was later formalised in the 1981 CEPT videotex standard as the CEPT3 profile. By embedding cursor-control characters within the page data, it was also possible to encode simple animations by re-writing parts of the screen already displayed. These were termed \"dynamic frames\" and could not be created online using conventional editing terminals, but required specialist software and uploading via the \"bulk update\" facility. No timing options were available beyond that imposed by the available transmission speed, usually 1,200 baud.\n\nThe IP logo on line 1 occupied at least 43 bytes, depending on the number of control characters, so the space available for the IP's data is 877 characters at most. Lines could either occupy the full forty character positions, or be terminated early with a CR/LF sequence. Each control character took up two bytes, despite displaying as a single space, so the more complex a page, the less actual information could be presented. It was almost impossible, therefore, to display a right hand border to a page.\n\nRouting from page to page through the database was arranged by the use of numbered items on index pages, which used the space in the frame routing table to map the index links directly to other page numbers. Thus an index on page 747 might have links requiring the user to key 1 for “UK Flights”, key 2 for “Flights to Europe”, or key 3 for “Hotels” which represented links to page 74781, 74782, and 74791 respectively. The routing table for a particular frame only allowed specification of routes for digits 0-9, so double digit routes would typically be sent via an \"intermediate\" frame, usually a spare frame elsewhere in the IP's database, to which the first digit of all similarly numbered items would link. Since pressing a number would interrupt a page that was currently being displayed, the keying of a double digit route would not generally inconvenience the viewer with the display of the intermediate frame.\n\nPages did not scroll, but could effectively be extended by the use of frames, which required alphanumeric suffixes to be appended to the numeric page numbers. Thus keying page *7471# actually resulted in the display of frame 7471a which could be extended by use of follow-on frames 7471b, 7471c etc. each of which was accessed by repeated use of the \"#\" key. Because the Prestel system was originally designed to be operated solely by means of a simple numeric keypad it was not possible to access frames other than the top level frame directly (i.e., in this case pages other than \"7471a\").\n\nThis follow-on frame facility was exploited extensively by the implementation of \"telesoftware\" on Prestel whereby computer programmes, notably for the BBC Micro, were available for download from Prestel. Generally speaking, the first two or three frames acted as header pages. For example, one such programme was described on frame 70067a, and 70067b, while frame 70067c gave the number of subsequent frames containing the programme, and a crosscheck sum. Special software enabled this crosscheck sum to be compared with a value calculated from the result of downloading all the required frames in order to verify a successful download. The actual telesoftware programme was contained from frame 70067d onwards, in this case for a further ten frames. In the event that the check failed it was necessary to download the entire programme again starting from the beginning.\n\nEach frame had a single-character type code associated with it. Most frames would be \"i\" (for \"Information\" types) but other types included response frames, mailbox pages, or gateway pages. Special frame types could also be specified which caused the follow-on frame to be automatically displayed, with or without the usual clear-screen code, as soon as the current frame had finished being transmitted. These were mainly used by \"dynamic frames\", as it provided a mechanism to continue animations which would not otherwise fit within the number of characters available in a standard frame.\n\nThere were two levels of information provider (IP) – firstly a \"Main IP\" who rented pages from Post Office Telecommunications (PO)/British Telecom (BT) directly, and thus owned a three digit node or \"master page\" in the database. This required an ongoing investment, consisting of a minimal annual payment to become an information provider. The price of this basic package was £5,500 per annum in 1983, equivalent to around £25,000 as at end 2014. The charge includes: \nAdditional frames were available in batches of 500 for £500 per annum (over £2,300 as at end 2014) while Closed User Groups and Sub-IP Facility cost respectively £250 (over £1,100 as at end 2014).\n\nThose with smaller requirements or budget could rent pages from a main information provider rather than from the Post Office/British Telecom. The main IP had to pay an additional £250 to obtain the privilege but could then rent out individual pages at a market rate. Unlike the main IP, sub-IPs had to pay a per-minute charge for editing online, 8p per minute at Monday-Friday 8 am-6 pm or 8p per 4 minute in all other times for sub-IPs in 1983 (over 35p as at end 2014). Sub-IPs were restricted to pages under a 4 or more digit node within a Main IP's area, and could only edit existing pages. Sub-IP accounts were unable to create or delete pages or frames themselves.\n\nEditing of pages was possible in one of two ways, either directly by creating or amending pages using special editing keyboards whilst connected online to the main Update Computer, or by creating pages offline and updating them in bulk to the main Update Computer. Bulk update required that pages be created offline by the use of editing terminals which could store pages, or by micro-computers such as that provided by Apple or Acorn. The pages were then transmitted to the UDC online in bulk via a special dialup port and protocol, or sent via magnetic tape to the Update Centre (UDC) where they were uploaded by Network Operations (NOC) staff.\n\nUsing the online editor facility, IPs were also able to view information about their pages which was hidden from the ordinary end user, such as the time and date of the last update, whether the frame was in a Closed User Group (CUG), the price to view the frame (if any) and the \"frame count\" or number of times the frame had been accessed. The frame count was not accumulated over all IRC's but related only to the computer which was being viewed at the time so gaining national access counts was a manual exercise.\n\nIPs and sub-IPs accessed the Edit computer using their normal ID and password, but had a separate password to access the editing facility. Bulk uploads only required the edit password and the IPs account number.\n\nHaving logged on, each user was taken direct to their default main index page, known as the \"Welcome Page\". For standard users, this would be page 1a, the general top level index to the whole of Prestel. However, if a user signed up through, or later joined, products or services from major IPs, such as Club 403, Micronet 800, Prestel Travel, CitiService, etc., they would be given a different welcome page, so that after logon they were routed directly to 800a, 403a, 747a etc.\n\nFrom the Welcome Page it was possible for any user to find pages of information in several different ways, or a combination of them. Printed directories were available which gave the full page numbers corresponding to the items in an alphabetical index. Pages were accessed directly by keying \"*page number#\". Individual pages often had links to related pages which could be accessed by use of one or two digit routing codes. This feature was widely used on sets of index pages which were commonly grouped by subject heading, provided both by the Post Office/BT and by individual IPs. Because of the numerical limitation, it was often necessary to go through a series of index pages in order to reach the desired page. Extension frames which might be required to view further information on a topic could be only accessed by use of the \"#\" key. From 1987 onwards, it became possible to use access Prestel pages via use of special alphabetic codes, provided that the IP who owned the page set up a special keyword mapped onto that page. Thus, by keying *M NEWS#, it was possible for a user to route directly to page *40111# to obtain news about micro-computers.\n\nMany standard mailbox frames were available offering various designs for greetings cards or seasonal messages such as Valentine Cards. Messages could only occupy a single frame, so the main message text field could typically take up to a maximum of 100 words, depending upon how many other fields were required and what graphics were used on the frame. Mailbox frames were completed by entering relevant details and pressing the # key on each field. Completing the last, or only, of which lead to the request to “KEY 1 TO SEND KEY 2 NOT TO SEND”. Assuming all went well, this led to a subsequent final screen confirming successful dispatch, or if there were problems (such as a mistake in entering the Mailbox number) then an appropriate error frame was displayed. If it was desired to send the message to more than one recipient then it was necessary to re-key the message text into a fresh message frame, although some popular micro-computers of the time provided the facility to store the message so that it could be copied and pasted into a new message.\nSpecial commands were also available. For example, to facilitate movement around the database it was possible to step back through a maximum of 3 frames or pages by use of the special key combination \"*#\". In the event of corruption of a page in transmission it was possible to refresh the page by means of the code *00, which had the advantage of avoiding any page charge being raised again. Alternatively, if the user wished to update a page to see the latest information, for example of flight arrival times, the *09 command would retrieve the latest updates, at the same time re-billing any page display charge. If all else failed, a user could simply return to the first page which he saw after logging onto the system by use of the *0# combination, which brought up their default Main Index. Exceptionally, information could be hidden on a frame by an IP which could only be revealed by use of the ‘Reveal” key of the keypad (e.g., to show an answer to a quiz). The same 'Reveal' key was also used to hide the data once more.\n\nWith a view to supporting the planned major expansion program, a new Prestel infrastructure was designed around two different types of data center: Update Centre (UDC), where IPs could create, modify and delete their pages of information, and Information Retrieval Centre (IRC), which mirrored copy of the pages is provided to end-users. In practice there only ever was one Update Centre, and this always housed just one update computer, named \"Duke\", but within six months of public launch there were in addition two dedicated information retrieval computers.\nIn those early days of the public service all the live Prestel computers were located in St Alphage House, a 1960s office block on Fore Street in the City of London. At the time the National Operations Centre (NOC) was located in the same building on the same floor. The computers and the NOC were later moved to Baynard House, (on Queen Victoria Street, also in the City of London) which acted as a combined UDC and IRC. Both types of machine, together with other development hardware, remained in service there until 1994 when the Prestel service was sold by BT to a private company.\n\nEach IRC normally housed two information retrieval computers, although in some IRCs in London just a single machine was present. IRCs were generally located within major telephone exchanges, rather than in BT Data Processing Centres, in order to give room for the extensive communications requirements. Exchange buildings were ideally suited to housing the large numbers of rack mounted 1200/75 baud modems and associated cabling as well as the racks of 16-port Multi-Channel Asynchronous Communications Control Units (MCACCUs) or multiplexors from GEC which gave the modems logical access into the computers.\n\nIn the new infrastructure, IRCs were connected to the UDC in a star network configuration, originally via leased line permanent (not packet switched) connections, based on the X25 protocol, operating at 2.4 kilobits per second (kbit/s). By mid 1981, these private circuit links had been replaced with dedicated 4-wire X25 circuits over the new public Packet Switch Stream (PSS) network operating at 4.8 kbit/s.\n\nBy June 1980, there were four singleton retrieval computers in London, plus six other machines installed in pairs at IRC sites in Birmingham, Edinburgh and Manchester. Fully equipped IRC machines had a design capacity of 200 user ports each but these first ten machines were initially only capable of supporting approximately 1,000 users between them, expandable later to 2,000 users.\n\nBy September 1980, there were five IRC machines in London plus pairs of machines at Birmingham, Nottingham, Edinburgh, Glasgow, Manchester, Liverpool and Belfast offering a total of 914 user ports. Further IRC’s were planned at Luton, Reading, Sevenoaks, Brighton, Leeds, Newcastle, Cardiff, Bristol, Bournemouth, Chelmsford and Norwich by the end of 1980. In some of these locations where there was insufficient Prestel traffic to warrant siting an IRC computer, the plans were to site multiplex equipment in a suitable exchange building from where connections were made over X25 to the nearest proper IRC. As at the end of 1980, there was actually a total of 1500 live computer ports available and by July 1981, the number of IRC computers has been expended to 18, increasing the coverage of the telephone subscriber population from 30% to 62%.\n\nIn 1982, using the multiplexor technique described above, a virtual IRC was created in Boston, Massachusetts giving access to a machine in the UK known as Hogarth in order to provide Prestel services to subscribers from across the United States via the Telenet packet switching network.\n\nThe Prestel Mailbox service was originally launched on Enterprise computer to support messaging solely between users on that machine and by 1984, the facility had been rolled out nationwide. This required a further type of Prestel computer dedicated to the exchange of messages. The only example of this type, which became known as Pandora, was co-located with the UDC in Baynard House, London.\n\nOriginally Prestel IRC computers were directly dialled by means of an ordinary telephone number (e.g., the Enterprise computer in Croydon was accessed by dialling 01 686 0311. By 1984, the special short dialling codes 618 and 918 were in use in order to give access to the nearest IRC at local telephone call rates, at least across most parts of the UK.\n\nIn 1987, the entire local access network was being overhauled and shared with other Dialcom Group companies - users connecting and not automatically logging into Prestel would be greeted with a menu allowing access to Prestel, Telecom Gold, etc.\n\nPrestel computers were based on GEC 4000 series \" minicomputer \" with small differences in the accumulation according to the function of the machine. IRC main machines were originally GEC 4082 equipped with 384 Kbytes memory core store machines, six 70 Mbyte HDD and 100 ports for 1500 initial users. The network has since grown to the point that in June 1980 there are four stand-alone retrieval computers in the London area with six other computers installed in pairs in Birmingham, Edinburgh and Manchester. The ten computers can output to approximately 1000 user ports, expandable as required to 2000 when the number of subscribers goes up significantly. The GEC 4082 computer with 512 megabyte capacity will interconnect to the 10 and later to 20 retrieval computers to handle the data files. The initial data base consists of approximately 164,000 information pages (June, 1980) with planned update capacity of 260,000 pages. A page consists of a maximum of 960 data characters (5x7 bits each, suggesting approximately 35,000 bits per page).\n\nThis arrangement effectively limited the size of the public service database to around 250,000 frames so in order to cope with planned growth by 1981 the IRC machines had been expanded by the addition of two further data drives.\n\nEach IRC computer was configured with 208 ports and so was able to support 200 simultaneous Prestel users, the remaining 8 ports being used for test and control functions. Access for the ordinary user was provided via the duplex asynchronous interface provided by banks of GEC 16-port multi-channel asynchronous control units (MCACCU) known more simply as multiplexers. These devices in turn were accessed via banks of standard Post Office Modems No. 20 operating at 1200/75 bit/s, which were connected directly to the public switched telephone network (PSTN).\n\nBy 1981, this configuration had changed with memory doubled to 768 kbytes but with data discs reduced to six, corresponding to the number at the IRC machines and with just a single transaction disc.\n\nIn addition to the MCACCU units required to support 1200/75 dial up access, the Update Centre machines were also connected to special modems provided to support online bulk updating by IPs. Banks of 300/300 bit/s full duplex asynchronous V21 modems supported computer to computer links for the more sophisticated IP while 1200 bit/s half duplex V23 modems supported so called intelligent editing terminals (i.e. those capable of storing a number of frames offline before uploading to the UDC). In addition twin 9-track NRZI tape decks of 800 bytes/inch capacity were provided in order to support bulk offline updates.\n\nAlthough technically categorised as \"minicomputers\", these GEC machines were physically very large by today’s standards, each occupying several standard communications cabinets, each standing high by wide. The CDC 9762 hard disc drives were housed separately in large stand-alone units, each one about the size of a domestic washing machine. (See images in the photo of the GEC Computers' Development Centre). The 70 Mbyte capacity hard discs themselves were in fact removable units, each consisting of a stack of five 14 inch platters, standing high, that could be lifted in and out of the drive unit.\n\nThe GEC machines cost in excess of £200,000 each at GEC standard prices, in addition to which there were the costs of all the associated communications equipment. Putting together all of the computer and communications equipment required for a single IRC was a major undertaking and took some 15 months from order placement to commissioning.\n\nGEC 4000 series computers were capable of running a number of operating systems but in practice Prestel machines exclusively ran OS4000 which itself was developed by GEC. This in turn supported BABBAGE, the so-called high level assembler language in which all the Prestel software for both IRC and UDC machines (and later the messaging machine) was written.\n\nIn 1987, a Prestel Admin computer was introduced which supported the user registration process: the capture of user details from the paper Prestel Application Form (PAF), the transfer of data to the relevant Prestel computer, and the printing of the Welcome letter for users. This machine, also based upon GEC 4082 equipment, was the first to be equipped with 1 Mbyte of memory which was required to support the Rapport relational database. This product from Logica was an early example of deployment of a system written in a 4GL database language which supported all features of the Prestel Admin application.\n\nIn order to proactively manage the potentially large numbers of user connections to Prestel computers, special monitoring equipment was developed by Post Office Research & Development engineers. This was known by the acronym VAMPIRE, short for \"Viewdata Access Monitor and Priority Incident Reporting Equipment\" – a title which more or less describes its function. The device uses private circuits to connect modem ports on each computer or remote IRC multiplexor node, with a display on a television screen Prestel Prestel at the Regional Centre responsible for the administration of IRC. The VAMPIRE screen consisted of a matrix of small squares, so arranged that all ports for a single IRC computer could be displayed on a single television with each square representing the state of a port simply by means of the colour. Free ports were shown as green, occupied ports as yellow, incoming calls as pale blue and faulty ports as red, such that the state of a whole Prestel machine or concentrator node could be determined at a glance.\n\nIt was apparently planned to extend this facility via a system designated the \"Data Recording and Concentrator Unit for Line Applications\" known as DRACULA, which would generate a summary view so that the state of multiple computers could be displayed on a single screen. This device was never deployed since the number of VAMPIRE sets needed to monitor every Prestel computer and concentrator never got beyond a couple of dozen, spread over many Regional Prestel Centre offices.\n\nIn 1983, the Prestel messaging service known as “Prestel Mailbox” was launched, initially hosted on the computer known as \"Enterprise\", and later available from all IRC computers by means of a centralised messaging computer known as \"Pandora\". This facility extended the original day one concept of “Response Frames” whereby an end user could send a message back to the IP who owned the page via special pages, for example to order goods or services. The user’s name, address, telephone number, and date could be added automatically to the message when the IP set up the response frame by means of codes which triggered extraction of key data from the users account held on the IRC computer. Initially response frames were ingathered by an IP from each IRC individually, but later the facility to collect messages from all IRCs at the UDC from where they could be ingathered centrally was implemented, and with the introduction of Mailbox, they could be retrieved from any IRC.\n\nIn order to use the new Prestel Mailbox service, the user went to page *7# which gave access to a set of frames where new “free format” messages could be created, or pre-formatted messages filled out and stored messages could be retrieved, and other related facilities were hosted. Many standard mailbox frames were available offering various designs for greetings cards or seasonal messages such as Valentine Cards. In order to compose a new message, a blank message frame, which could also be accessed directly via *77#, was displayed with the sender’s mailbox number pre-filled, leaving space for the recipient’s mailbox number and the text of the message itself. Messages could only occupy a single frame, so the main message text field could typically take up to a maximum of 100 words, depending upon how many other fields were required and what graphics were used on the frame. Mailbox frames were completed by entering relevant details and pressing the # key on each field. Completing the last, or only, of which lead to the request to “KEY 1 TO SEND KEY 2 NOT TO SEND”. Assuming all went well, this led to a subsequent final screen confirming successful dispatch, or if there were problems (such as a mistake in entering the Mailbox number) then an appropriate error frame was displayed. If it was desired to send the message to more than one recipient then it was necessary to re-key the message text into a fresh message frame, although some popular micro-computers of the time provided the facility to store the message so that it could be copied and pasted into a new message.\n\nPrestel Mailbox numbers were generally based upon the last 9 digits of the user's telephone number, without spaces or punctuation. For example, the Prestel Mailbox number for Prestel Headquarters which had the telephone number 01-822 2211 would be simply 018222211, while that for a user in Manchester with telephone number 061-228 7878 would be 612287878. In keeping with the established telephone number practice, but unlike the convention with today's internet mailboxes, Prestel Mailbox numbers were published by default, and were available via the Prestel computers in a dedicated directory accessible from page *486#. On request, \"ex-directory\" mailbox numbers were available, usually employing a dummy telephone number format such as the series 01999nnnn, and later the series 01111nnnn.\n\nEvery time a user logged into Prestel, a Mailbox banner on their Welcome page, usually flashing, would alert them if they had any new messages waiting. Similarly, upon the user's request to sign off the system via *90#, a warning would appear if any new messages had arrived, with an option to read them, before the user was allowed to disconnect. Messages were retrieved from page *930#, where they were presented to the recipient in chronological order. After reading a new message, the user had to choose between deleting the message, or saving it, before the next message was presented. Initially only three messages could be saved at any one time, and these stored messages were accessible via page *931#.\n\nUse of the basic Mailbox service was free, that is to say there were no registration charges for owning a mailbox, or for sending new messages or for storing received messages, although even by 1984 only five messages could be saved once they had been read.\n\nBy 1984 the basic Mailbox service had been extended to give automatic access to the Telex service which at the time was still relatively common in business and was the standard way to reach remoter parts of the globe. Using a special \"Telex Link\" page, the message was composed in the usual way and then the destination country chosen and the Telex number entered before sending just like a standard message. Telex Link added the necessary Telex codes as required and tried to send the message as many times as required before positively confirming receipt by means of a special Mailbox message. Telexes could be sent to Prestel Mailbox users from a standard Telex terminal by using Telex Link number and inserting \"MBX\" and the relevant mailbox number as the first line of the telex message itself. The incoming telex message appeared to the Prestel recipient just as an ordinary Mailbox message but with the telex number inserted at the top of the frame.\n\nBecause of the charges inherent in use of the Telex service, messages sent via Prestel Telex Link were chargeable, in 1984 at the rate of 50p for destinations in the UK, £1.00 for Europe, £2.00 for North America, £3.00 for elsewhere and even £5.00 for sending to ships via INMARSAT. There was no charge to Prestel users for receiving Telex messages.\n\nIn the same year, when there were some 70,000 users registered, up to 100,000 mailboxes and telexes were sent each week via Prestel Mailbox.\n\nFrom July 1989, a new mailbox system was introduced which allowed for single messages of up to five frames in length, storing of messages prior to sending, sending to multiple recipients, either individually or via a mailing list, forwarding of messages, and requesting an acknowledgment of receipt. Whilst sending a simple mailbox using none of the new facilities remained free, all of the new options were charged at 1p per use per recipient. For the first time, the sending of spam was accounted for and permitted, albeit at 20p per recipient. In addition, the stored message facility was replaced by a summary page, which listed all the messages, both new and old, that were waiting. The user could then pick which message to view, rather than being required to read through them all in chronological order. As only the first 20 could be accessed, this effectively allowed for up to 19 messages to be stored while allowing the continued reception of new mail.\n\nWhile teletext services were provided free of charge, and were encoded as part of the regular television transmissions, Prestel data were transmitted via telephone lines to a set-top box terminal, computer, or dedicated terminal. While this enabled interactive services and a crude form of e-mail to be provided, gaining access to Prestel also involved purchasing a suitable terminal, and arranging with a Post Office engineer for the installation of a connection point known as a Jack 96A. (From the early 1980s, the \"New Plan\" sockets were fitted as standard on new lines and on any change of rented handset, and terminals or modems then required no special connections.)\n\nThereafter it was necessary to pay both a monthly subscription and the cost of local telephone calls. On top of this, some services (notably parts of Micronet 800) sold content on a paid-for basis. Each Prestel screen carried a price in pence in the top right-hand corner. Single screens could cost up to 99p.\n\nThe original idea was to persuade consumers to buy a modified television set with an inbuilt modem and a keypad remote control in order to access the service, but no more than a handful of models were ever marketed and they were prohibitively expensive. Eventually set-top boxes were made available, and some organisations made these available as part of their subscription, for example branded Tandata terminals were provided by the Nottingham Building Society for its customers, who could make financial transactions via Prestel.\n\nBecause the communication over telephone lines did not use any kind of error correction protocol, it was prone to interference from line noise which would result in garbled text. This was particularly problematic with early home modems which used acoustic couplers, because most home phones were hard-wired to the wall at that time.\n\nHowever, it was still an expensive proposition, and as a result, Prestel only ever gained a limited market penetration among private consumers, achieving a total of just 90,000 subscribers, with the largest user groups being Micronet 800 with 20,000 users and Prestel Travel with 6,500 subscribers. Micro Arts computer graphics Software and Magazine had 400 pages and interactive art software to download. This prefigured mixed media websites on the Internet.\n\nThe costs for businesses interested in publishing on Prestel were also expensive. This ensured that only the largest or most forward thinking companies were interested in the service.\n\nDuring the daytime, when business usage was high, there was a per-minute charge to use Prestel, but in the evenings and weekends, traditionally the quiet times, it was free apart from the telephone call. With Micronet being so popular, suddenly the quiet times became fairly busy.\n\nThe BT Prestel software development team developed a number of national variants of Prestel, all of which ran on GEC Computers. They were sold to the PTTs of other countries, including Australia, Austria, Belgium, Italy, Hungary, Hong Kong, Germany, Netherlands, New Zealand, Singapore and Yugoslavia. Italy was the largest system with 180,000 subscribers. The Singapore system had a notable technology difference in that pages were not returned over the modem connection, but were returned using teletext methods over one of four TV channels reserved specially for the purpose, which had all scan lines encoded in teletext format. This higher bandwidth enabled use of a feature called Picture Prestel which was used to carry significantly higher resolution pictures than were available on other Prestel systems. It was also demonstrated at the 1982 Worlds Fair in Knoxville, Tennessee, USA.\n\nThe original Prestel system, designed for cost effectiveness and simplicity, employed a rudimentary graphic capability known as serial mosaics. Through juxtaposition of the special mosaic characters, crude but recognizable graphic representations could be made on the screen. This graphic scheme had its limitations. To change colors between two mosaic graphic characters or between any two characters in general, a color change command was required. This command signal, however, physically occupied a blank space on the screen.The French sought to overcome this limitation when they joined the videotex world in the mid-1970s. They called their system Antiope. While based on the same mosaic graphics that were employed by the British, Antiope added a new feature, parallel attributes, or the ability to change the color from one cell to another without the need for a blank space.\nAt approximately the same time, the Canadians adapted standard computer graphic commands into a set of functions called alphageometrics. These alphageometric functions did away with the block mosaic graphics used by the British and French and replaced them with drawing instructions, such as: DRAW LINE, DRAW ARC, DRAW POLYGON, etc. Through use of these geometric commands much higher resolution could be achieved than with the mosaic commands. This alphageometric scheme was integrated into the Canadian videotex system which the Canadians referred to as \"Telidon\".\n\nIn contrast to the demise of the British system, the French equivalent of Prestel, Teletel/Minitel, received substantial public backing when millions of Minitel terminals were handed out free to telephone subscribers (causing Alcatel huge financial problems). As a consequence the Teletel network became very popular in France, and remained well used, with access later also possible over the Internet. After a short postponement, Minitel closed finally on 30 June 2012.\n\nIn 1979 The New Opportunity Press launched Careerdata, an interactive Graduate recruitment service devised and designed by Anthony Felix, the New Opportunity Press MD, and supported by GEC's Hirst Research centre in Wembley, London, who provided 12 terminals which were installed in the largest UK University Careers Advisory Services. This was the first commercial application on the new medium and was featured in the Prestel Road Show which toured the UK and some European centres. A closed access videotex system based on the Prestel model was developed by the travel industry, and continues to be almost universally used to this day by travel agents throughout the country: see Viewdata. The Prestel technology was also sold abroad to several countries, and in 1984 Prestel won a UK Queen's Award for Industry both for its innovative technology and use of British products (it largely ran on equipment provided by GEC Computers).\n\nIn 1979 Michael Aldrich developed an online shopping system, a type of e-commerce, using a modified domestic colour television equipped with the Prestel chip set and connected to a real-time transaction-processing computer via a domestic dial-up telelphone line. During the 1980s he sold these online shopping systems to large corporations mainly in the UK. All the terminals on these systems could also access the Prestel systems. Aldrich installed a travel industry system in Thomson Holidays in 1981.\n\nThe Prestel system was customised and resold by GEC Computers to several other countries, including: Austria, Australia, Germany, Hong Kong, Hungary, Italy, Malaysia, Netherlands, New Zealand, Singapore, and the former Yugoslavia.\nTelecom Australia re-branded their system Viatel, with the centre of operations in Windsor, Melbourne, Australia. During the Black Monday stock market crash the system's stock trading system was highly used. The system in Italy run by SIP was heavily used during the 1990 FIFA World Cup for reporting the match progress and scores. The Singapore system provided a much higher receive bandwidth than was available over dial-up modems at the time by broadcasting the return frames using the Teletext technique of embedding them in broadcast television signals. Four VHF TV channels were dedicated to this with all the scan lines used for Teletext encoding, which enabled the system to provide a feature called \"Picture Prestel\" to convey higher resolution images. The Yugoslav system was based in Zagreb, with additional IRCs located in Rijeka, Ljubljana, and Split.\n\nThe American Viewtron videotex service was modelled after Prestel.\n\nIn 1983 the UK's first online banking service opened with Homelink, which was a cooperation between the Nottingham Building Society and the Bank of Scotland.\n\n\n<div class=\"references - medium\">\n\n"}
{"id": "38383500", "url": "https://en.wikipedia.org/wiki?curid=38383500", "title": "RRT Global", "text": "RRT Global\n\nRRT Global is an international company that specializes in the development of technologies for oil refining process. The company's CEO, Douglas Harris, is a former vice president of TNK-BP. The company is a resident of the Energy Efficient Technologies cluster of the Skolkovo Foundation.\nRRT Global is the first company to implement the conversion of light gasoline fractions in a combined process.\n\nThe company was founded in St. Petersburg by chemical engineers Oleg Parputs () and Oleg Giiazov (), who had previously worked in an engineering company that served the oil refining sector. Startup financing was provided by Foresight Ventures, Bright Capital and the Skolkovo Foundation.\n\nThe first laboratory was established on the campus of the Saint Petersburg State Institute of Technology. The laboratory was unheated, which meant that researchers had to work there for 6–8 hours in padded jackets.\n\nThe company became a resident of the Energy Efficient Technologies cluster of the Skolkovo Foundation in 2011. Douglas Harris, the former Vice President of Refining at BP and TNK-BP, became the company’s CEO in the same year. The company has patented the PRIS technology in Russia, Europe and the United States.\n\nThe Prime Minister of the Russian Federation, Dmitry Medvedev, met with the company’s management team in October 2011, and visited the company’s laboratory in September 2012 during his official visit to Saint Petersburg.\n\nThe company was rated one of the Top 10 Startups of the Year in 2012 by the Russian Startup Rating.\n\nAfter conducting a company audit in 2012, PricewaterhouseCoopers awarded the company an AAA rating.\n\nIn 2015-2016 RRT Global formed technology alliances with American engineering company KBR and Russian IT company Yandex.\n\nThe company is headquartered in the United States. RRT Global has subsidiary operating in the Russian Federation\n\nRRT Global’s R&D center is located in St. Petersburg. The center includes a pilot plant park, laboratory facilities for studying catalytic systems, an analytical laboratory, and an administrative and logistics center.\n\nThe company’s senior management includes Dmitry Shalupkin (CTO), Douglas Harris (CEO), Oleg Giiazov (Director in Russia).\n\nOne of the company’s areas of focus is to improve the technology to obtain MSAT-2 gasoline components based on combining catalytic systems and refining in a single unit. The company is making extensive use of 3D printing for the production of certain equipment components.\n\nPRIS is a technology developed by the company for converting light gasoline fractions in a combined process. Papers on this technology have been published in the journal Chemical Engineering and Processing, as well as other specialized scientific journals. Worldwide Refinery Processing Review Included the technology in four of the State-of-the-Art commercial isomerization technologies, together with international providers of advanced technologies: UOP, Axens, GTC. Rossiyskaya Gazeta called the technology “revolutionary”.\n\nAccording to this technology, refining and catalytic systems are combined in a single unit, reducing capital and energy costs and reducing environmental pollution. A similar combination principle had previously been used primarily in the pharmaceutical industry. The PRIS technology allows the use of a low-octane straight-run gasoline fraction with a benzole-containing fraction as raw materials. The technology allows the production of high-octane gasoline components meeting the EURO 5 standard.\n\nIС7 is a technology developed by the company for isomerization product of oil refining, heptanes. Earlier heptanes not found practical applications and have been used as solvents. The technology helps to increase production of high-octane gasoline.\n\n\n"}
{"id": "49326296", "url": "https://en.wikipedia.org/wiki?curid=49326296", "title": "TechVibes", "text": "TechVibes\n\nTechvibes is an online technology hub for industry news, long read features, events and jobs. Techvibes was founded in 2002 by President and Editor-in-Chief Robert Lewis and in June 2016 it was acquired by technology consulting firm, Konrad Group.\n\nTechfest is a recruiting event for digital companies hosted by Techvibes. The first Techfest launched in Vancouver in 2013 and is now multiple North American cities including Toronto and Montreal. Past participating companies include Microsoft, Hootsuite, Shopify, Amazon and Wattpad.\n\nThe Canadian Startup Awards are an annual awards show “celebrating canada’s best and brightest in the tech space” that was launched by Techvibes in 2011. Notable past award recipients include Shopify, a publicly-traded company based in Ottawa, and Hootsuite, a private company also based in Vancouver.\n\n"}
{"id": "39425504", "url": "https://en.wikipedia.org/wiki?curid=39425504", "title": "Transition engineering", "text": "Transition engineering\n\nTransition Engineering is the professional engineering discipline that deals with the application of the principles of science to the design, innovation and adaptation of engineered systems that meet the needs of today without compromising the ecological, societal and economic systems on which future generations depend to meet their own needs. Just as safety considerations are incorporated into design parameters in all engineering fields, sustainability thinking is built into Transition Engineering. Transition engineering is emerging as a field to give engineers the tools necessary to address sustainability in design and management of engineered systems. Transition engineering is a cross-disciplinary field that addresses the issues of future resource availability and identifies, then realizes opportunities to increase resilience and adaptation.\n\nEngineering professions emerge when new technologies, new problems or new opportunities arise. This was the case when safety engineering grew in the early 1900s to combat the high workplace injury and fatality rates. In the 1960s, Environmental engineering emerged as a discipline to reduce industrial pollution and mitigate impacts on environmental health and water quality. Quality engineering came about with the increase in mass production techniques during WWII and the need to confirm the quality of the products.\n\nThere are two serious problems driving the emergence of Transition Engineering; the exponential growth in the concentration of carbon dioxide in Earth’s atmosphere and the lack of growth and imminent decline of conventional oil production known as peak oil. The concentration of carbon dioxide in the atmosphere recently exceeded 400ppm, a level that Earth has not known for 800,000 years. Transition engineering aims to take advantage of the current access to the remaining lower cost and higher EROI energy resources to re-develop all aspects of urban and industrial engineered systems to adapt as fossil fuel use is dramatically reduced.\n\nThe idea behind transition engineering has sprouted from many different roots, both technical and non-technical. The concept of sustainable development has been around since 1987 and the problem of sustainability was a driving force in the development of transition engineering. The Transition Town movement provided further inspiration as it showed that there were many groups of people around the world motivated to prepare for peak oil and climate change. Transition towns and ecovillages demonstrate the need for engineers to build systems that manage un-sustainable risks and provide people with sustainable options. Engineers are ethically required to \"hold paramount the safety, health and welfare of the public\" and answer society's need for sustainable development\n\nThe origins of safety engineering provided much of the inspiration for transition engineering. At the beginning of the 1900s, business owners viewed workplace safety as a wasted investment and politicians were slow to change. After the Triangle Shirtwaist Factory Fire in New York City killed 156 trapped workers, 62 engineers came together to investigate how to make the workplace a safer place to be. This eventually lead to the formation of the American Society of Safety Engineers.\n\nAs safety engineering manages the risks of unsafe conditions, transition engineering manages the risks of unsustainable conditions. To give engineers a better grasp of sustainability, transition engineering defines the problem as UN-sustainability. This is similar to the problem of un-safe conditions that is the purpose of safety engineering. We do not necessarily know what a perfectly safe system looks like, but we do know what unsafe systems look like and how to improve them; the same applies to unsustainability of systems. By reducing unsustainability issues we take steps in the right direction\n\nThe Transition Engineering method involves 7 steps to help engineers develop projects to deal with changing unsustainable activities. As a discipline, Transition Engineering recognizes that \"Business as Usual\" projections of future scenarios from past trends are not valid because the underlying conditions have changed sufficiently from the conditions of the past. For example, the projection of future oil supply in 2050 from data prior to 2005 would give an expectation of a 50% increase in demand over that time-frame. However, the actual production rate of conventional oil has not increased since 2005 and is projected to decline by more than 50% by 2050.\n\n\nGATE opened the first group in the UK in Feb 2014. GATE is a Professional Engineering Institution; a membership association and learned society, and comprises an emerging network of engineers and non-engineers that share the idea that engineers are responsible for changing engineered systems in order to adapt to reducing fossil fuel and other unsustainable resources. Transition Engineering is a change management discipline. Like Safety Engineering, Transition Engineering uses and audit and stock-take of current system design and operation to quantify the risks to essential activities and resources over a time-frame of study. The time-frame of study should be commensurate with the lifetime of the assets involved in the activity. An activity is anything that the engineered system supports, for example manufacturing, sewage treatment, mobility, or food preservation. Transition Engineering recognizes that the analytical methods of strategic analysis over a life-cycle time-frame are at odds with most economic analyses that discount values with time. The strategic analysis carried out by Transition Engineers seeks to avoid stranded investment by recognizing resource risks. A classic example of stranded investments is the North Atlantic Cod Fishery - where the largest number of bottom trawling ships (e.g. those ships responsible for destroying the Cod spawning beds) were manufactured in the year that the fish stocks collapsed.\nThe Global Association for Transition Engineering is registered charity number 1166048, registered with the UK Charity Commission on 14 March 2016. It is a \"Charitable Incorporated Organisation\" or CIO.\n\nEngineering design process<br>\nSafety Engineering<br>\nSustainable transport<br>\nSystems engineering<br>\nSusan Krumdieck\n\n"}
{"id": "2884010", "url": "https://en.wikipedia.org/wiki?curid=2884010", "title": "Turbine map", "text": "Turbine map\n\nEach turbine in a gas turbine engine has an operating map. Complete maps are either based on turbine rig test results or are predicted by a special computer program. Alternatively, the map of a similar turbine can be suitably scaled.\n\nA typical turbine map is shown on the right. In this particular case, lines of percent corrected speed (based on a reference value) are plotted against the x-axis which is pressure ratio, but deltaH/T (roughly proportional to temperature drop across the unit/component entry temperature) is also often used. The y-axis is some measure of flow, usually non-dimensional flow or, as in this case, corrected flow, but not actual flow. Sometimes the axes of a turbine map are transposed, to be consistent with those of a compressor map. As in this case, a companion plot, showing the variation of isentropic (i.e. adiabatic) or polytropic efficiency, is often also included.\n\nIn this example the turbine is a transonic unit, where the throat Mach number reaches sonic conditions and the turbine becomes truly choked. Consequently, there is virtually no variation in flow between the corrected speed lines at high pressure ratios.\n\nMost turbines however, are subsonic devices, the highest Mach number at the NGV throat being about 0.85. Under these conditions, there is a slight scatter in flow between the percent corrected speed lines in the 'choked' region of the map, where the flow for a given speed reaches a plateau.\n\nUnlike a compressor (or fan), surge (or stall) does not occur in a turbine. This is because the flow through the unit is all 'downhill', from high to low pressure. Consequently there is no surge line marked on a turbine map.\n\nWorking lines are difficult to see on a conventional turbine map because the speed lines bunch up. One trick is to replot the map, with the y-axis being the multiple of flow and corrected speed. This separates the speed lines, enabling working lines (and efficiency contours) to be cross-plotted and clearly seen.\n\nThe following discussion relates to the expansion system of a 2 spool, high bypass ratio, unmixed, turbofan.\n\nOn the RHS is a typical primary (i.e. hot) nozzle map (or characteristic). Its appearance is similar to that of a turbine map, but it lacks any (rotational) speed lines. Note that at high flight speeds (ignoring the change in altitude), the hot nozzle is usually in, or close to, a choking condition. This is because the ram rise in the air intake factors-up the nozzle pressure ratio. At static (e.g. SLS) conditions there is no ram rise, so the nozzle tends to operate unchoked (LHS of plot).\n\nThe low pressure turbine 'sees' the variation in flow capacity of the primary nozzle. A falling nozzle flow capacity tends to reduce the LP turbine pressure ratio (and deltaH/T). As the left hand map shows, initially the reduction in LP turbine deltaH/T has little effect upon the entry flow of the unit. Eventually, however, the LP turbine unchokes, causing the flow capacity of the LP turbine to start to decrease.\n\nAs long as the LP turbine remains choked, there is no significant change in HP turbine pressure ratio (or deltaH/T) and flow. Once, however, the LP turbine unchokes, the HP turbine deltaH/T starts to decrease. Eventually the HP turbine unchokes, causing its flow capacity to start to fall. Ground Idle is often reached shortly after HPT unchoke.\n"}
