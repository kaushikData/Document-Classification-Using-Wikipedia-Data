{"id": "41952259", "url": "https://en.wikipedia.org/wiki?curid=41952259", "title": "Adobe Marketing Cloud", "text": "Adobe Marketing Cloud\n\nAdobe Experience Cloud (AEC), formerly Adobe Marketing Cloud (AMC), is a collection of integrated online marketing and Web analytics products by Adobe Systems. \n\nAdobe Marketing Cloud includes a set of analytics, social, advertising, media optimization, targeting, Web experience management and content management products aimed at the advertising industry and hosted on Amazon Web Services. Like other Adobe Cloud services (e.g., Adobe Creative Cloud), the Adobe Marketing Cloud allows users with valid subscriptions to download the entire collection and use it directly on their computers with open access to online updates.\n\nThe Adobe Marketing Cloud collection was introduced to the public in October 2012 as Adobe began retiring the Omniture name it acquired in October 2009. Products of the defunct company were then integrated step by step into the new Cloud service which includes the following eight applications: Adobe Analytics, Adobe Target, Adobe Social, Adobe Experience Manager, Adobe Media Optimizer, Adobe Campaign, Audience Manager and Primetime. In November 2013, Adobe Systems introduced mobile features to its Marketing Cloud, making smartphones and other mobile devices new targets for analytics.\n\nOn September 15, 2009, Omniture, Inc. and Adobe Systems announced that Adobe would be acquiring Omniture, an online marketing and web analytics business unit in Orem, Utah. The deal of $1.8 billion, was completed on October 23, 2009, and is now joined by other Adobe acquisitions such as Day Software and Efficient Frontier, as the main components of Adobe's Digital Marketing Business Unit. Around 2012, Adobe withdrew the Omniture brand while its products were being integrated into the Adobe Marketing Cloud.\n\nOn May 21, 2018 Adobe announced the acquisition of Magento for 1.68$ billion. The addition of the Magento Commerce will enable commerce features to be integrated into the Adobe Experience Cloud. \n\nIn the same year, on September 20, 2018 Adobe acquired the marketing automation company Marketo.. The acquisition is expected to close in Q4 2018.\n"}
{"id": "55178664", "url": "https://en.wikipedia.org/wiki?curid=55178664", "title": "Agronic Oy", "text": "Agronic Oy\n\nAgronic Oy (Agronic Ltd.) is a Finnish manufacturer of agricultural equipment. The company is located in Haapavesi, Finland. Agronic Oy has a daughter company Prodevice Oy.\n\nAgronic Oy was established by Urpo Kuronen and Erkki Kivelä in 1993. The main products are round balers, bale wrappers, slurry tanks and umbilical slurry systems. Around 50% of products are exported to more than 20 countries all over the world. The company co-operates with Japanese company Takakita.\n\nIn 2011, Agronic was awarded the National Entrepreneur Prize. In 2013, the company became the biggest tax payer in Haapavesi.\n\n"}
{"id": "55140088", "url": "https://en.wikipedia.org/wiki?curid=55140088", "title": "Anant Bhardwaj", "text": "Anant Bhardwaj\n\nAnant Bhardwaj is an Indian computer scientist, software engineer, and Internet entrepreneur. He founded Instabase in 2015 and currently serves as its CEO.\n\nAnant Bhardwaj was born and raised in India. After completing his undergraduate degree in Computer Engineering at the University of Pune, he moved to the United States in 2010 to attend Stanford University where he earned a Masters in Computer Science.During his studies, he created VoiceX, an open-source platform designed to create an information ecosystem for people in developing world, with low-end feature phones that can't connect to the Internet, to generate, manage, retrieve, and search information. In 2012, he went to the Massachusetts Institute of Technology to pursue a Ph.D. where he was co-advised by Sam Madden and David Karger. He dropped out of the Ph.D. program in 2015 to start Instabase, where he serves as the CEO.\n"}
{"id": "11180423", "url": "https://en.wikipedia.org/wiki?curid=11180423", "title": "Andrea Wong", "text": "Andrea Wong\n\nAndrea Wong serves on the boards of Liberty Media Corporation, Liberty Interactive Corporation, Hudson's Bay Company, Hudson Pacific Properties and Social Capital Hedosophia Holdings. She is also a Governor of the British Film Institute, and a Trustee of the Royal Academy of Arts.\n\nWong was most recently President, International Production for Sony Pictures Television and President, International for Sony Pictures Entertainment based in London. She oversaw Sony Pictures Television's 18 overseas production companies, creating nearly 1,300 hours of entertainment around the world each year. Among her many achievements in this role, Wong brought \"The Crown\" to Sony, winner of Golden Globes for Best Drama Television Series and Best Performance by an Actress in a Television Series along with numerous other accolades. As President, International for Sony Pictures Entertainment, Wong guided the company on matters impacting international production and championed the studio's interests abroad.\n\nPreviously, Wong served as president and CEO of Lifetime Networks, where she oversaw the day-to-day operations of Lifetime Television, Lifetime Movie Network, Lifetime Real Women, and Lifetime Digital, including programming, marketing, advertising sales, affiliate sales, public affairs, business and legal affairs, strategic planning, operations and research. During her time there she saw \"Army Wives\" become Lifetime's top-rated original series ever and spearheaded Lifetime's acquisition of \"Project Runway\".\n\nPrior to that, Wong was executive vice president, alternative programming, specials and late night at ABC where she developed shows such as \"The Bachelor\", the U.S. version of \"Dancing With the Stars\" and the Emmy-award winning \"\".\n\nWong graduated MIT with a degree in electrical engineering and received an MBA from Stanford University.\nShe is a Henry Crown Fellow of the Aspen Institute, and serves on the Stanford Graduate School of Business Advisory Council.\n\n"}
{"id": "2892101", "url": "https://en.wikipedia.org/wiki?curid=2892101", "title": "Apple Remote", "text": "Apple Remote\n\nThe Apple Remote is a remote control device released in or after October 2005 by Apple Inc. for use with a number of its products which use infrared capabilities. The device was originally designed to interact with the Front Row media program on the iSight iMac G5 and is compatible with some later desktop and portable Macintosh computers. The first three generations of Apple TV used the Apple Remote as their primary control mechanism. It has now been replaced with the Siri Remote in the fourth generation. Prior to the Apple Remote, Apple produced several nameless IR remotes for products such as the Macintosh TV, TV tuner expansion boards, and the PowerCD drive.\n\nThe original Apple Remote was designed with six buttons and made of white plastic. Its shape and layout resembled the first-generation iPod Shuffle. A circular Play/Pause/Select button sat in the center of a larger four-button circle of (clockwise): Volume Up, Next/Fast-forward, Volume Down and Previous/Rewind. A separate Menu button was positioned below. The price was set at US$29.00.\n\nIn October 2009, the remote was redesigned as a thinner and longer aluminum version. The new remote was released along with the 27 inch aluminum iMacs and multi-touch Magic Mouse. The Play/Pause button was moved out of the center of the directional buttons and put beside the Menu button (under the directional buttons). The symbols for the Volume Up/Down and Next/Fast-forward buttons were replaced with small dots, to make it clear that the buttons were also used to move up, down, left, and right within menus. All of the buttons became black and embossed within aluminum. Along with the new design, the price was dropped to US$19.99. The newer design also underwent a slight revision with the navigation ring. In a small percentage of older remotes, this ring was flush with the curvature of the remote's aluminum body. The more-common revision is bulged slightly; presumably so users can find the ring more easily by touch.\n\nReplacement of the CR2032 battery in the original remote is done with a small pointed object such as a paper clip at the bottom right edge of the device, where the battery slides out on a tray. The newer version has the battery located behind a compartment in the middle of the device which is accessed by turning a coin in the compartment door's indent.\n\nThe Apple Remote's original function was to enable navigation in Front Row, which allows users to browse and play music, view videos (DVDs and downloaded files) and browse photos. Although Front Row was removed from OS X 10.7 and later, some Apple software still works with the remote. It can still be used to control presentations in Apple Keynote (on both Intel Macs & PowerPC Macs), picture slide shows in iPhoto and Aperture, DVD films via DVD Player, and to play video and audio in iTunes and QuickTime. Other software that is still compatible includes Elgato's EyeTV 3.5, and VLC media player. The remote can also be used to run presentations in Microsoft PowerPoint 2008 or in OpenOffice.org Impress.\n\nOther functions controlled by the remote can include putting a device into sleep mode, selecting a partition to boot from on startup, and ejecting optical disks. A device can be configured to respond only to a particular remote.\n\nAn iPod placed in a dock featuring an IR sensor can be used with the remote for music and media control. The remote's menu functionality does not work on the iPod. The Apple Remote can also be used to control the iPod Hi-Fi or third party devices tailored to it.\n\nStarting with Boot Camp 1.2, the remote has had some functionality when a user is running Windows. If iTunes is installed on the Windows partition, pressing the Menu button on the remote will load the program. The remote's media controls also support Windows Media Player, as well as system volume control. Other third party programs may also utilize the remote's capabilities; media applications such a foobar2000 and Media Player Classic allow users to control their functions via the remote. Applications must be in focus for the remote to control them. Boot Camp 5, the latest version of the software, also includes drivers for the remote control.\n\nApple offers a free 'Remote' app for iOS devices (available in the Apple App Store) which allows for wireless control of iTunes on Mac/Windows computers or the Apple TV.\n\nThe Siri Remote was launched with the 4th Generation Apple TV in 2015. It uses both IR and Bluetooth to communicate with the Apple TV. The remote has a glass trackpad, dual microphones, 5 buttons for Menu, Home, Siri and Play/Pause, Volume up and down as 1 button and is the size of 2 buttons. Additionally it has an accelerometer (IMU) and a gyroscope which allows the remote to be used as a gaming controller for tvOS apps and games. The remote (unlike previous generations) uses a built-in rechargeable Lithium Polymer Battery that is charged through the lightning port at the bottom of the remote. The Siri Remote is known as the Apple TV Remote in places that don't support Siri.\n\nEarlier models of the iMac (Polycarbonate iMac) featured a magnetic rest for the remote, which was later removed.\n\nUsing the Apple Remote with new MacBook Airs, Retina MacBooks or old Macs without a built-in IR Receiver requires a USB-based infrared receiver and additional software from a third party.\n\nUsing Remote Buddy (from IOSPIRIT GmbH) or mira (from Twisted Melon), it is possible to connect an external USB receiver such as the Windows Media Center Edition eHome receiver, and use the Apple Remote on these machines with full support for sleep, pairing, low battery detection and controlling a variety of Apple and third party software. In addition, Remote Buddy is able to emulate events of an Apple Remote on these systems, enabling users to use software written for the Apple Remote in exactly the same way as with Macs that have a built-in infrared receiver.\n\nFor the Apple computers without built-in infrared receiver, there is a miniature USB receiver, the \"SmartGUS\", which allows to give to iMac, MacBook and Mac Pro, the infrared functionality. In this case, all compatible software (iTunes, Keynote, PowerPoint, OpenOffice Impress, QuickTime Player, iPhoto, VLC, Kodi, Remote Buddy, Mira ...) can use the features of the Apple Remote.\n\nBecause many electrical appliances use infrared remote (IR) controls, concurrent use of the Apple Remote with other IR remotes may scramble communications and generate interference, preventing stable use. Remotes should be used individually to circumvent the problem.\n\nThe Apple Remote uses an NEC IR protocol which consists of a differential PPM encoding on a 1:3 duty cycle 38 kHz 950 nm infrared carrier. There are 32 bits of encoded data between the AGC leader and the stop bit:\nWhile the Apple Remote uses the NEC IR protocol for the timing, the 32-bit data package is in a different format. It consists of two 16 bit LSB words.\n\nThis is the internal page table (command page 0x00):\nThis is the command page table (command page 0x0e):\n\n\n"}
{"id": "36448616", "url": "https://en.wikipedia.org/wiki?curid=36448616", "title": "Association for Computers and the Humanities", "text": "Association for Computers and the Humanities\n\nThe Association for Computers and the Humanities (ACH) is the primary international professional society for digital humanities. ACH was founded in 1978. According to the official website, the organization \"support[s] and disseminate[s] research and cultivate[s] a vibrant professional community through conferences, publications, and outreach activities.\" ACH is based in the United States, and has an international membership. ACH is a founding member of the Alliance of Digital Humanities Organizations (ADHO), a co-originator of the Text Encoding Initiative, and a co-sponsor of an annual conference.\n\nACH has been a co-sponsor of the annual Digital Humanities conference (formerly ACH/ALLC, before that International Conference on Computing in the Humanities or ICCH) since 1989. From 2006, when ADHO was founded, the larger umbrella organization is the conference's official sponsor.\n\n\nACH is joined in ADHO by:\n\n\nOther related Organizations:\n\n\n"}
{"id": "253279", "url": "https://en.wikipedia.org/wiki?curid=253279", "title": "Automated Mathematician", "text": "Automated Mathematician\n\nThe Automated Mathematician (AM) is one of the earliest successful discovery systems. It was created by Douglas Lenat in Lisp, and in 1977 led to Lenat being awarded the IJCAI Computers and Thought Award.\n\nAM worked by generating and modifying short Lisp programs which were then interpreted as defining various mathematical concepts; for example, a program that tested equality between the length of two lists was considered to represent the concept of numerical equality, while a program that produced a list whose length was the product of the lengths of two other lists was interpreted as representing the concept of multiplication. The system had elaborate heuristics for choosing which programs to extend and modify, based on the experiences of working mathematicians in solving mathematical problems.\n\nLenat claimed that the system was composed of hundreds of data structures called \"concepts,\" together with hundreds of \"heuristic rules\" and a simple flow of control: \"AM repeatedly selects the top task from the agenda and tries to carry it out. This is the whole control structure!\" Yet the heuristic rules were not always represented as separate data structures; some had to be intertwined with the control flow logic. Some rules had preconditions that depended on the history, or otherwise could not be represented in the framework of the explicit rules.\n\nWhat's more, the published versions of the rules often involve vague terms that are not defined further, such as \"If two expressions are structurally similar, ...\" (Rule 218) or \"... replace the value obtained by some other (very similar) value...\" (Rule 129).\n\nAnother source of information is the user, via Rule 2: \"If the user has recently referred to X, then boost the priority of any tasks involving X.\" Thus, it appears quite possible that much of the real discovery work is buried in unexplained procedures.\n\nLenat claimed that the system had rediscovered both Goldbach's conjecture and the fundamental theorem of arithmetic. Later critics accused Lenat of over-interpreting the output of AM. In his paper \"Why AM and Eurisko appear to work\", Lenat conceded that any system that generated enough short Lisp programs would generate ones that could be interpreted by an external observer as representing equally sophisticated mathematical concepts. However, he argued that this property was in itself interesting—and that a promising direction for further research would be to look for other languages in which short random strings were likely to be useful.\n\nThis intuition was the basis of AM's successor Eurisko, which attempted to generalize the search for mathematical concepts to the search for useful heuristics.\n\n\n"}
{"id": "17501379", "url": "https://en.wikipedia.org/wiki?curid=17501379", "title": "Bounce keys", "text": "Bounce keys\n\nBounce keys is a feature of computer Desktop Environments. It is an accessibility feature to aid users who have physical disabilities. Bounce keys allows you to configure the computer to ignore rapid, repeated keypresses of the same key.\n\n"}
{"id": "2064142", "url": "https://en.wikipedia.org/wiki?curid=2064142", "title": "Chafing dish", "text": "Chafing dish\n\nA chafing dish (from the French \"chauffer\", \"to make warm\") is a kind of portable grate raised on a tripod, originally heated with charcoal in a brazier, and used for foods that require gentle cooking, away from the \"fierce\" heat of direct flames. The chafing dish could be used at table or provided with a cover for keeping food warm on a buffet. Double dishes that provide a protective water jacket are known as \"bains-marie\" and help keep delicate foods, such as fish, warm while preventing overcooking.\n\nThe Roman politician and writer Cicero described a \"kind of saucepan of Corinthian brass\", writing \"This simple and ingenious vessel possesses a double bottom, the upper one holds the light delicacies . . . and the fire is lit underneath\".\n\nFragments of ceramic chafing dishes are common in the archaeology of medieval city sites, such as York, England. Chafing dishes in the form of charcoal-burning braziers are familiar in 17th-century American inventories almost from the start. François Pierre La Varenne, \"Le Cuisinier françois\" (Paris, 1652) mentions the use of a \"réchaut\" in a recipe for \"champignons à l'olivier\". In describing the Velasquez genre painting (\"illustration\"), sometimes art historians not handy in the kitchen describe her as \"frying\" eggs in her earthenware dish. In 1520, Hernan Cortez reported to Charles V the manner in which Montezuma was served meals in Tenochtitlan:\nIn England silver braziers without handles, upon which a dish would be set, are mentioned in the reign of Queen Anne; wooden balls kept the heat of the charcoal in the pierced container from being transferred to the table surface. Dish-crosses and the chafing dish with a handle were introductions of the reign of George II. In the American colonies, \"One chafing dish\" was inventoried among the silver at Abraham de Peyster's death in New York, 1728, though only two colonial New York examples are known to survive.\n\nIn a light form and heated over a spirit lamp, a chafing dish could also be used for cooking various dainty dishes at table— of fish, cream, eggs or cheese— for which silver chafing dishes with fine heat-insulating wooden handles were made in the late 19th century, when \"chafing-dish suppers\" became fashionable, even in households where a kitchen maid prepared all the ingredients beforehand. Specialized chafing-dish cookbooks appeared from the 1880s. A book of chafing-dish recipes printed for the silversmiths, Gorham Manufacturing Co. in New York, (2nd edition, 1894), featured a brief history of chafing dishes, followed by proper instruction for use, suggesting its novelty. Fannie Farmer's \"Chafing Dish Possibilities\" was published in Boston in 1898.\nModern chafing dishes are made of light metal or ceramic casseroles with handles, sometimes covered with a Pyrex lid. Classic uses of a chafing-dish are in preparing Welsh rarebit or cheese fondue.\n\n\n"}
{"id": "408007", "url": "https://en.wikipedia.org/wiki?curid=408007", "title": "Comptometer", "text": "Comptometer\n\nThe comptometer was the first commercially successful key-driven mechanical calculator, patented in the United States by Dorr E. Felt in 1887.\n\nA key-driven calculator is extremely fast because each key adds or subtracts its value to the accumulator as soon as it is pressed and a skilled operator can enter all of the digits of a number simultaneously, using as many fingers as required, making them sometimes faster to use than electronic calculators. Consequently, in specialized applications, comptometers remained in use in limited numbers into the early 1990s, but with the exception of museum pieces, they have all now been superseded by electronic calculators and computers.\n\nManufactured without interruption from 1887 to the mid-1970s, it was constantly improved. The mechanical versions were made faster and more reliable, then a line of electro-mechanical models was added in the 1930s. Notably, it was the first mechanical calculator to receive an all-electronic calculator engine in 1961, with the ANITA Mark VII model released by Sumlock Comptometer. This created the link between the mechanical calculator industries and the electronic.\n\nAlthough the comptometer was primarily an adding machine, it could also do subtractions, multiplication and division. Its keyboard consisted of eight or more columns of nine keys each. Special comptometers with varying key arrays were produced for a variety of special purposes, including calculating currency exchanges, times and Imperial weights. The name comptometer was formerly in wide use as a generic name for this class of calculating machine.\n\nThe comptometer is the direct descendant of the key-driven machine of Thomas Hill patented in the United States in 1857 and of the Pascaline invented by Blaise Pascal in France in 1642. By just replacing the input wheels of the Pascaline by the columns of keys of Hill's machine, the comptometer was invented. Addition is performed exactly the same way, and both the Pascaline and the Comptometer make use of the 9's complement method for subtraction, but in the case of the comptometer it is the operator who must choose the right keys for the subtrahend (each key has its 9's complement written in miniature letter next to it).\n\nDorr Felt began his work on comptometer in 1882\nand started building first prototype during the American Thanksgiving holidays of 1884. Because of his limited amount of money, he used a macaroni box for the outside box, and skewers, staples and rubber bands for the mechanism inside. It was finished soon after New Year's Day, 1885. This prototype, called the macaroni box, is in the Smithsonian Institution in Washington, D.C., United States.\n\nShortly after, Robert Tarrant, the owner of a Chicago workshop, gave Mr. Felt a salary of $6 a week, a bench to work on and what would add up to $5,000 to build his first practical machine which he finished in the autumn of 1886.\n\nBy September 1887, eight production machines had been built.\n\nThe original comptometer design was patented by Felt, on July 19, 1887 and on October 11, 1887.\n\nTwo years later, on June 11, 1889, he was granted a patent for the Comptograph. A Comptograph is a comptometer with a printing mechanism making it more like a calculating machine (even though the keys are registered as they are typed and not when a handle is pulled) therefore slower and more complicated to operate. It was the first machine design to use individualized type impression which made its printed output very legible.\n\nThe first comptograph was sold to the Merchants & Manufacturers National Bank of Pittsburgh, PA. in December 1889. It was the first sale of a machine ever. This machine is now on display at the Smithsonian Institution in Washington, D.C..\n\nThe Felt & Tarrant Manufacturing Company built both comptometers and comptographs throughout the 1890s. In 1902, the Comptograph Company was set up to manufacture comptographs exclusively, but was shut down at the beginning of World War I.\n\nForty years later, in the mid-1950s, the Comptometer Corporation reused the name Comptograph for a line of 10 key printing machines.\n\nThe comptometer was the first machine in production to challenge the supremacy of the arithmometer and its clones; but not immediately, it took almost three years to sell the first hundred machines.\nFelt and Tarrant signed a partnership contract on November 28, 1887. The partnership was incorporated as Felt & Tarrant Manufacturing Company on January 25, 1889.\n\nIn 1902, Felt and Tarrant parted ways in creating a second company and decided to split the shares of both companies so that one would own the controlling interest of a company with 51% of the shares and the other would still share the profits with the remaining 49% of the shares. Mr. Felt became the majority owner of the Felt & Tarrant Manufacturing Company and Mr. Tarrant became the majority owner of the Comptograph Company. The Felt & Tarrant Manufacturing Company reabsorbed the Comptograph company at the beginning of World War I\n\nThe Felt & Tarrant Mfg Co became public in 1947 and changed its name to the Comptometer Corporation in 1957.\n\nIn 1960, the Bell Punch Company bought the British rights to the Comptometer design and trademark, and continued its development. In 1961, Sumlock, a division of the Bell Punch Company, was renamed The Sumlock Comptometer Ltd, and began marketing the first all-electronic desktop calculator, the ANITA Mark VII. The entire calculator division of the Bell Punch Company was bought by Rockwell International in 1973. They exited the calculator business in 1976 and shut down all operations.\n\nIn 1961, the Comptometer Corporation merged with the Victor Adding Machine Company, and the two became the Victor Comptometer Corporation. After barely surviving the microprocessor revolution, it is still doing business today as Victor Technology LLC.\n\nThe first machines were built using wooden boxes, which made them lighter but more fragile. They were called the \"woodies\", and about machines were built in between 1887 and 1903.\n\nTwo major improvements happened during the first years of production: the first was the insertion of carry inhibit push buttons for quicker operations of subtraction and the second was the color grouping of the columns of keys (three by three except for the first two columns that represent hundredths). Mr. Felt continually improved his machine with seven patents filed during the first ten years. These included the patents that he took for the Comptograph.\n\nEach row of keys is differentiated from the one above and the one below by a different tactile feel: the even rows have round and raised keys and the odd rows have flat and oblong keys. The keys of the first machines, with their metal rims, are similar to the typewriters keys of the same period. Plastic keys were also introduced very early on but their rows do not have that tactile difference.\n\nThe woodies had a three-part zeroing mechanism: a lever, a lever stop, and a knob. In order to reset the machine, first push the lever toward the stop, start rotating the knob and release the lever as soon as the result numbers start to move. Continue rotating the knob until all the result numbers are reset.\n\nAll the models, from this one on, have a metal casing, but the model A has a polished glass panel for the output display in the front. This model is produced from 1904 to 1906.\n\nDescribed in patents 762,520 and 762,521, it introduces a new carry mechanism that reduces the power needed to operate the keys to one fourth of its predecessor's. It also introduces the duplex feature that allows for keys to be pressed simultaneously. And finally it introduces a simplified clearing mechanism that only requires one lever, and one back and forth motion of it, to reset the machine. \"The machine gun of the office\" as it was called in some World War I advertisements, was starting to develop the form and mechanism that it would keep for the next forty years.\n\nThese models are built from 1907 to 1915. They all have the shoebox metal casing that will be used until the end of World War II.\n\nVery few machines of this type were built, but all were made between 1913 and 1915. This was a transition machine that introduced the \"Controlled-Key\" safeguard, which was part of an error detection mechanism that blocked most of the keyboard if a key was not pressed enough to add its total to the result. When this mechanism was activated, all the columns of keys were blocked except for the one where the error occurred. Therefore, the operator could find the column in question, reenter the number and unlock the keyboard by pressing the release key in order to resume the operations.\n\nEach key was supported by a metal plate that wrapped around it on three sides. This created a support that was used to block the key when the error detection mechanism was activated. The blocking mechanism was moved inside the box in the following models.\n\nThe production of this model started in 1915 and lasted five years. The error detection mechanism is moved inside the box and the release key is red. This machine is the last machine without the Comptometer logo inscribed on its front and back panels.\n\nManufactured from 1920 until the beginning of World War II, these models incorporate all of the improvements of the previous machines. The Comptometer logo is inscribed on their front and back panels and they have the red release key that was introduced with the previous two models.\n\nThe ST (SuperTotalizer) has two display output registers and two additional levers that allow for the creation of intermediate results. Each key pressed by the operator is added to the top display register creating an intermediate result which is added to the bottom display register when the operator activates the front right lever (it also resets it). The bottom display register accumulates the intermediate results until it is cleared by using the left lever.\n\nThese three models are the last of the shoebox design that was introduced in 1907 with the .\n\nThis electric model was introduced in the middle of the 1930s and had a very moderate success since the mechanical models were very easy to operate and less expensive to maintain.\n\nIntroduced just before World War II, it had a new box design which was more symmetrical and rounder in shape. The WM was manufactured during World War II with a less wasteful use of material for the mechanism.\n\nThe WM had many aluminum parts which were more prone to failure so the use of steel was \nresumed as soon as steel again became available. While the machine was lighter, it was also weaker.\n\nThe models become lighter with the use of aluminum and more refined inner mechanical designs. Both mechanical and electric models are offered. Their outside is similar to the model M machines. The model 3D11 succeeds to the model M and the electric model 992 succeeds to the model K.\nThe major external appearance change of the 3D11 & 992, the keys were two tones of green with a fuller depth almost twice the old original style. making identification very easy. Also the lock\nrelease key (red) was moved from the upper right corner of the keyboard to the center and just below the 'one' keys so it could be thumb operated. On model K electrics the motor was turned on with a switch and ran continuously during use. The 992 had a micro switch which started the motor at the start of any key depression and shut off after key action competed. This reduced the motor\nwear to almost nil.\n\nThe most important new designs of this period came from the Sumlock comptometer Ltd company that introduced the first all-electronic desktop calculators almost two years before the competition. They were the ANITA Mark VII, first introduced in 1961 and marketed in Continental Europe and the ANITA Mark VIII, a slightly improved design, introduced at the same time, marketed in Britain and the rest of the world. They both started shipping in 1962.\n\nThe last ANITA machine with a Comptometer keyboard was the \"ANITA mk 10\" introduced in 1965, still using cold-cathode switching tubes and that will be replaced in 1968 by the \"ANITA mk 11\", a 10 key machine.\n\nIncidentally, Sharp's first all transistor desktop calculator, the \"CS-10A COMPET\", introduced in the summer of 1964, also had a Comptometer type keyboard.\n\n\n"}
{"id": "456729", "url": "https://en.wikipedia.org/wiki?curid=456729", "title": "Computer-aided engineering", "text": "Computer-aided engineering\n\nComputer-aided engineering (CAE) is the broad usage of computer software to aid in engineering analysis tasks. It includes , , , durability and optimization.\n\nComputer Softwares used to analyse CAD geometry tools that have been developed to support these activities are considered CAE tools. CAE tools are being used, for example, to analyse the robustness and performance of components and assemblies. The term encompasses simulation, validation, and optimisation of products and manufacturing tools. In the future, CAE systems will be major providers of information to help support design teams in decision making. Computer-aided engineering is used in many fields such as automotive, aviation, space, and shipbuilding industries.\n\nIn regard to information networks, CAE systems are individually considered a single node on a total information network and each node may interact with other nodes on the network.\n\nCAE systems can provide support to businesses. This is achieved by the use of reference architectures and their ability to place information views on the business process. Reference architecture is the basis from which information model, especially product and manufacturing models.\n\nThe term CAE has also been used by some in the past to describe the use of computer technology within engineering in a broader sense than just engineering analysis. It was in this context that the term was coined by Jason Lemon, founder of SDRC in the late 1970s. This definition is however better known today by the terms CAx and PLM.\n\nCAE areas covered include:\n\nIn general, there are three phases in any computer-aided engineering task:\n\nThis cycle is iterated, often many times, either manually or with the use of commercial optimization software.\n\nCAE tools are very widely used in the automotive industry. In fact, their use has enabled the automakers to reduce product development cost and time while improving the safety, comfort, and durability of the vehicles they produce. The predictive capability of CAE tools has progressed to the point where much of the design verification is now done using computer simulations (diagnosis) rather than physical prototype testing. CAE dependability is based upon all proper assumptions as inputs and must identify critical inputs (BJ). Even though there have been many advances in CAE, and it is widely used in the engineering field, physical testing is still a must. It is used for verification and model updating, to accurately define loads and boundary conditions and for final prototype sign-off.\n\nEven though CAE has built a strong reputation as a verification, troubleshooting and analysis tool, there is still a perception that sufficiently accurate results come rather late in the design cycle to really drive the design. This can be expected to become a problem as modern products become ever more complex. They include smart systems, which leads to an increased need for multi-physics analysis including controls, and contain new lightweight materials, to which engineers are often less familiar. CAE software companies and manufacturers are constantly looking for tools and process improvements to change this situation. On the software side, they are constantly looking to develop more powerful solvers, better use computer resources and include engineering knowledge in pre- and post-processing. On the process side, they try to achieve a better alignment between 3D CAE, 1D System Simulation and physical testing. This should increase modeling realism and calculation speed. On top of that, they try to better integrate CAE in the overall product lifecycle management. In this way, they can connect product design with product use, which is an absolute must for smart products. Such an enhanced engineering process is also referred to as predictive engineering analytics.\n\n\n"}
{"id": "3115142", "url": "https://en.wikipedia.org/wiki?curid=3115142", "title": "Council for Responsible Genetics", "text": "Council for Responsible Genetics\n\nThe Council for Responsible Genetics (CRG) is a nonprofit NGO with a focus on biotechnology.\n\nThe Council for Responsible Genetics was founded in 1983 in Cambridge, Massachusetts.\n\nAn early voice concerned about the social and ethical implications of modern genetic technologies, CRG organized a 1985 Congressional Briefing and a 1986 panel of the American Association for the Advancement of Science, both focusing on the potential dangers of genetically engineered biological weapons. Francis Boyle was asked to draft legislation setting limits on the use of genetic engineering, leading to the Biological Weapons Anti-Terrorism Act of 1989.\n\nCRG was the first organization to compile documented cases of genetic discrimination, laying the intellectual groundwork for the Genetic Information Nondiscrimination Act of 2008 (GINA).\n\nThe organization created both a Genetic Bill of Rights and a Citizen's Guide to Genetically Modified Food. Also notable are CRG's support for the \"Safe Seeds Campaign\" (for avoiding gene flow from genetically engineered to non-GE seed) and the organization of a US conference on Forensic DNA Databanks and Racial Disparities in the Criminal Justice System. In 2010 CRG led a successful campaign to roll back a controversial student genetic testing program at the University of California, Berkeley. In 2011, CRG led a campaign to successfully enact [CalGINA] in California, which extended genetic privacy and nondiscrimination protections to life, disability and long term care insurance, mortgages, lending and other areas.\n\nCRG has issued five anthologies of commentaries:\nCRG \"fosters public debate about the social, ethical and environmental implications of genetic technologies.\" They list three central principles:\n\n\nIn 2007, CRG hosted a retreat to refresh the mission statement and determine goals for the future of the organization. The outcome was that CRG should:\n\nThe CRG publishes \"GeneWatch\", America's first and (according to CRG in 2009) only magazine dedicated to monitoring biotechnology's social, ethical and environmental consequences. The publication covers a broad spectrum of issues, from genetically modified food to biological weapons, genetic privacy and discrimination, reproductive technology, and human cloning. Established in 1983, the publication won the Utne Independent Press Award for General Excellence in the category of newsletters in 2006.\n\n\n\n\nA major source of CRG's funding is the Ford Foundation, which provided $420,000 in grants during 2005-2007.\n\n"}
{"id": "202120", "url": "https://en.wikipedia.org/wiki?curid=202120", "title": "Counterweight", "text": "Counterweight\n\nA counterweight is a weight that, by exerting an opposite force, provides balance and stability of a mechanical system. Its purpose is to make lifting the load more efficient, which saves energy and is less taxing on the lifting machine.\n\nCounterweights are often used in traction lifts (elevators), cranes and funfair rides. In these applications, the expected load multiplied by the distance that load will be spaced from the central support (called the \"tipping point\") must be equal to the counterweight's mass times its distance from the tipping point in order to prevent over-balancing either side. This distance times mass is called the load moment.\n\nA counterbalance is a weight or force that balances or offsets another as when two objects of equal weight, power, or influence are acting in opposition to each other. The objects are then said to be in counterbalance.\n\nTrebuchet: There are five major components of a trebuchet: beam, counterweight, frame, guide chute, and sling. After the counterweight drops from a platform on the frame, gravity pulls the counterweight and pivots the beam. Without the counterweight, the beam could not complete the arc that allows the sling to accurately release the projectile.\n\nCrankshaft: A countehirweight is also used in many rotating systems to reduce vibrations due to imbalances in the rotating assembly. A typical example is counterweights on crankshafts in piston engines.\n\nDesk lamp: Some balanced arm lamps work with a counterweight to keep the arm and lamp in the desired position.\nElevator: In traction (non-hydraulic) elevators, a heavy counterweight counterbalances the load of the elevator carriage, so the motor lifts much less of the carriage's weight (specifically, the counterweight is the weight of the carriage plus 40-50% of its rated capacity). The counterweight also increases the ascending acceleration force and decreases the descending acceleration force to reduce the amount of power needed by the motor. The elevator carriage and the counterweights both have wheel roller guides attached to them to prevent irregular movement and provide a smoother ride for the passengers.\n\nSpace elevator: A space elevator is a proposed structure designed to transport material from a celestial body's surface into space. Many variants have been proposed, but the concept most often refers to an elevator that reaches from the surface of the Earth to geostationary outer space, with a counterweight attached at its outer end.\n\nBy attaching a counterweight at the end, upward centrifugal force from the Earth's rotation ensures that the cable remains stretched taut, countering the gravitational pull on the lower sections and thereby allowing the elevator to remain upright.\n\nThe counterweight itself could assume one of several forms: \n\nMetronome: A wind-up mechanical metronome has an adjustable weight and spring mechanism that allows the speed to be adjusted by placement of the weight on the spindle. The tempo speed is decreased by moving the weight to a higher spindle marking or increased by moving it to a lower marking.\n\nCrane: The tower crane (see picture) is a modern form of balance crane that is fixed to the ground. A horizontal boom is balanced asymmetrically across the top of the tower. The long arm carries the lifting gear. The short arm is called the machinery arm; this holds the motors and electronics to operate the crane, as well as the concrete counterweights.<ref name=\"howstuffworks/crane\"></ref>\n\n"}
{"id": "13335403", "url": "https://en.wikipedia.org/wiki?curid=13335403", "title": "Cranmer Park", "text": "Cranmer Park\n\nCranmer Park is a Denver city park located in the Hilltop neighborhood off of Colorado Boulevard between East 1st and East 3rd Avenue. It is most famous for a large sundial, which does double duty as a climbing appliance for children.\n\nAn inscription at the base describes the axis of the gnomon as elevated 39°43' in the direction of polar north. The stone is perpendicular to the gnomon at 50°17', which makes it parallel to the equator. The south side of the stone is similarly marked for wintertime observation.\n\nA polar chart at the base of the sundial describes the zodiac and degrees of the sun's position, and how to set a clock based on the gnomon's shadow. For winter viewing, the chart continues on the south side of the stone.\n\nThe current sundial is the second one to exist at this location in the park. The first was donated in 1941 by longtime Manager of Denver Parks George E. Cranmer, for whom the park is named. It was destroyed by vandals who exploded dynamite under it in September 1965. The replacement sundial was installed in March, 1966 after a successful citywide fundraising effort led by the Denver Junior Chamber of Commerce.\n\n"}
{"id": "172809", "url": "https://en.wikipedia.org/wiki?curid=172809", "title": "DVD player", "text": "DVD player\n\nA DVD player is a device that plays DVD discs produced under both the DVD-Video and DVD-Audio technical standards, two different and incompatible standards. Some DVD players will also play audio CDs. DVD players are connected to a television to watch the DVD content, which could be a movie, a recorded TV show, or other content.\n\nThe first DVD player was created by Sony Corporation in Japan in collaboration with Pacific Digital Company from the United States in 1997. Some manufacturers originally announced that DVD players would be available as early as the middle of 1996. These predictions were too optimistic. Delivery was initially held up for \"political\" reasons of copy protection demanded by movie studios, but was later delayed by lack of movie titles. The first players appeared in Japan on November 1, 1996, followed by the United States on March 26, 1997 with distribution limited to only seven major cities for the first six months.\n\nPlayers slowly trickled into other regions around the world. Prices for the first players in 1997 started at $600 and could top out at prices over $1000. By the end of 2000, players were available for under $100 at discount retailers. In 2003 players became available for under $50. Six years after the initial launch, close to one thousand models of DVD players were available from over a hundred consumer electronics manufacturers.\n\nFujitsu released the first DVD-ROM-equipped computer on November 6th in GB. Toshiba released a DVD-ROM-equipped computer and a DVD-ROM drive in Japan in early 1997 (moved back from December which was moved back from November). DVD-ROM drives from Toshiba, Pioneer, Panasonic, Hitachi, and Sony began appearing in sample quantities as early as January 1997, but none were available before May. The first PC upgrade kits (a combination of DVD-ROM drive and hardware decoder card) became available from Creative Labs, Hi-Val, and Diamond Multimedia in April and May 1997. In 2014, every major PC manufacturer has models that include DVD-ROM drives.\n\nThe first DVD-Audio players were released in Japan by Pioneer in late 1999, but they did not play copy-protected discs. Matsushita (under the Panasonic and Technics labels) first released full-fledged players in July 2000 for $700 to $1,200. DVD-Audio players are now also made by Aiwa, Denon, JVC, Kenwood, Madrigal, Marantz, Nakamichi, Onkyo, Toshiba, Yamaha, and others. Sony released the first SACD players in May 1999 for $5,000. Pioneer's first DVD-Audio players released in late 1999 also played SACD. SACD players are now also made by Accuphase, Aiwa, Denon, Kenwood, Marantz, Philips, Sharp, and others.\n\nA DVD player has to complete these tasks:\n\nAdditionally, most DVD players allow users to play audio CDs (CD-DA, MP3, etc.) and Video CDs (VCD). A few include a home cinema decoder (i.e. Dolby Digital, Digital Theater Systems (DTS)). Some newer devices also play videos in the MPEG-4 ASP video compression format (such as DivX) popular in the Internet.\n\nMost hardware DVD players must be connected to a television; there are portable devices which have an attached LCD screen and stereo speakers. Portable DVD players are often used for long road trips and travel. They often have a plug for the 12 volt power jack in cars. Some models have two screens, so that two people in the back seat can both watch the movie. Other portable DVD players have a single screen that opens up like a laptop computer screen.\n\nDue to multiple audio (and video) output devices, there are many outputs on a DVD player, such as an RCA jack, component outputs, and an HDMI output. Consumers may become confused with how to connect a player to a TV or amplifier. Most systems include an optional digital audio connector for this task, which is then paired with a similar input on the amplifier. The physical connection is typically RCA connectors or TOSLINK, which transmits a S/PDIF stream carrying either uncompressed digital audio (PCM) or the original compressed audio data (Dolby Digital, DTS, MPEG audio) to be decoded by the audio equipment.\n\nVideo is another issue which continues to present most problems. Current players usually output analog video only, both composite video on an RCA jack as well as S-Video in the standard connector. However, neither connector was intended to be used for progressive video, so yet another set of connectors has started to appear, to carry a form of component video, which keeps the three components of the video, one luminance signal and two color difference signal, as stored on the DVD itself, on fully separate wires (whereas S-Video uses two wires, uniting and degrading the two color signals, and composite uses only one, uniting and degrading all three signals). The connectors are further confused by using a number of different physical connectors on different player models, RCA or BNC, as well as using VGA cables in a non-standard way (VGA is normally analog RGB—a different, incompatible form of component video). Even worse, there are often two sets of component outputs, one carrying interlaced video, and the other progressive, or an interlaced/progressive switch (either a physical switch or a menu setting).\n\nIn Europe (but not most other PAL areas), SCART connectors are generally used, which can carry composite and analog RGB interlaced video signals (RGB can be progressive, but not all DVD players and displays support this mode) or Y/C (S-Video), as well as analog two-channel sound and automatic 4:3 or 16:9 (widescreen) switching on a single convenient multi-wire cable. The analog RGB component signal offers video quality which is superior to S-Video and identical to YPbPr component video. However, analog RGB and S-Video signals can not be carried simultaneously, due to each using the same pins for different uses, and displays often must be manually configured as to the input signal, since no switching mode exists for S-Video. (A switching mode does exist to indicate whether composite or RGB is being used.) Some DVD players and set-top boxes offer YPbPr component video signals over the wires in the SCART connector intended for RGB, though this violates the official specification and manual configuration is again necessary. (Hypothetically, unlike RGB component, YPbPr component signals and S-Video Y/C signals could both be sent over the wire simultaneously, since they share the luminance (Y) component.)\n\nHDMI is a new digital connection for carrying high-definition video, similar to DVI. Along with video, HDMI also supports up to eight-channel digital audio. DVD players with connectors for high-definition video can upconvert the source to formats used for higher definition video (e.g., 720p, 1080i, 1080p, etc.), before outputting the signal. By no means, however, will the resulting signal be high-definition video; that is, aside from optional deinterlacing, upconverting generally consists of merely scaling the video's dimensions to match that of higher resolution formats, foregoing the scaling that would normally occur in the output device.\n\nSome DVD players include a USB video recorder. As well as such, there are also have DVD players with a USB port to be able to play digital media types as well as MP4, MP3, etc.\n\nWireless connections (bluetooth and/or wifi) are useful to manage (play/record) wirelessly content from or to other devices (i.e. cell phones).\n\n, retail prices for such a device, depending on its optional features (such as digital sound or video output), start between 30 and 80 USD/Euro. They are usually cheaper than VCRs.\n\n the largest producer of DVD players is China; in 2002 they produced 30 million players, more than 70% of the world output. These producers have to pay US$15–$20 per player in license fees, to the patent holders of the DVD technology (Sony, Philips, Toshiba and Time Warner) as well as for MPEG-2 licenses. To avoid these fees, China has developed the Enhanced Versatile Disc standard as an intended successor of DVD; , EVD players were only being sold in China.\n\nSoftware DVD players are programs that allow users to view DVD videos on a computer with a DVD-ROM drive. Some examples are the VLC media player, 5KPlayer and MPlayer (all free software), as well as WinDVD, TotalMedia Theatre, PowerDVD, Fluendo DVD Player and DVD Player.\n\nAmong others, there are variants & huge kinds of software DVD players as well as multimedia player software which has DVD video playback capability, whether its proprietary-type(as commercial software), freeware, shareware or just a free software are available in the market.\n\nThere are successors to the DVD player: the HD DVD player and the Blu-ray Disc player, utilizing two incompatible technologies that reproduce higher quality video images than standard DVD. On February 19, 2008, Toshiba, creator of the former technology announced it would cease production on all HD DVD products leaving Blu-ray as the high definition successor to DVD players. Also, upscaling and up-converting DVD units are available that connect to televisions via a high definition interface and increase the overall picture quality.\n\n\n"}
{"id": "11057200", "url": "https://en.wikipedia.org/wiki?curid=11057200", "title": "Digital Trends", "text": "Digital Trends\n\nDigital Trends is a technology news, lifestyle, and information website that publishes news, reviews, guides, how-to articles, descriptive videos and podcasts about technology and consumer electronics products. With offices in Portland, Oregon, and New York City, Digital Trends is operated by Designtechnica Corp., a media company that also publishes Digital Trends Español, a Spanish-language version of the site, and men's lifestyle site The Manual.\n\nThe site offers reviews and information on a wide array of products that have been shaped by technology. That includes consumer electronics products such as smartphones, video games and systems, laptops, PCs and peripherals, televisions, home theater systems, digital cameras, video cameras, tablets, and more.\n\nAccording to third-party web analytics provider SimilarWeb, the site receives over 40 million visits per month . Digital Trends editorial team is led by Editor-in-Chief Jeremy Kaplan and guided by Co-Founders Ian Bell and Dan Gaul.\n\nIan Bell and Dan Gaul founded Digital Trends in June 2006 in Lake Oswego, Oregon.\n\nIn May 2009, Digital Trends moved its headquarters from Lake Oswego into the US Bancorp Tower in Downtown Portland, Oregon. The company opened a second office in New York City in 2012. Digital Trends is a privately funded and owned corporation. Digital Trends en Español, a Spanish-language version of the site, was launched in December 2014, led by Editor-in-Chief Juan Garcia.\n\nDigitalTrends.com saw a surge in popularity in recent years; the site claimed a 100-percent increase in traffic in September 2015, reaching over 24 million unique readers globally and more than 13 million U.S. readers. It currently reaches approximately 30 million readers per month who view over 100 million pages.\n\nIn addition to growth, 2015 saw a series of changes for Digital Trends. The site expanded its awards program to include several international trade shows, including Mobile World Congress in Barcelona and IFA in Berlin. It also launched its first car of the year awards and Smart Home awards, underscoring the site's growing investment in these areas. The company also launched DT Design, an in-house creative ad agency, to focus on branded content and high-impact advertising units.\n\nIn late summer of 2016, Re/Code reported on a deal with Conde Nast to acquire Digital Trends for $120 million, noting that the site is expected to generate $30 million in revenue this year and around $6 million in profit. Bell denied that his company was in talks, but acknowledged that the company \"is periodically approached by would-be buyers.\" Media and marketing news site Digiday wrote about the deal as well, comparing the site's traffic to \"such properties as the Purch network, CNET and The Verge, and ahead of USA Today Tech, Yahoo Tech, and Business Insider’s Tech Insider.\"\n\n"}
{"id": "6059135", "url": "https://en.wikipedia.org/wiki?curid=6059135", "title": "Doppler echocardiography", "text": "Doppler echocardiography\n\nDoppler echocardiography is a procedure that uses Doppler ultrasonography to examine the heart. An echocardiogram uses high frequency sound waves to create an image of the heart while the use of Doppler technology allows determination of the speed and direction of blood flow by utilizing the Doppler effect.\n\nAn echocardiogram can, within certain limits, produce accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, any abnormal communications between the left and right side of the heart, any leaking of blood through the valves (valvular regurgitation), calculation of the cardiac output and calculation of E/A ratio (a measure of diastolic dysfunction). Contrast-enhanced ultrasound-using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.\n\nAn advantage of Doppler echocardiography is that it can be used to measure blood flow within the heart without invasive procedures such as cardiac catheterization. \n\nIn addition, with slightly different filter/gain settings, the method can measure tissue velocities by tissue Doppler echocardiography. The combination of flow and tissue velocities can be used for estimating left ventricular filling pressure, although only under certain conditions.\n\nAlthough \"Doppler\" has become synonymous with \"velocity measurement\" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift (when the received signal arrives). However, the calculation result will end up identical.\n\nThis procedure is frequently used to examine children's hearts for heart disease because there is no age or size requirement.\n\nUnlike 1D Doppler imaging, which can only provide one-dimensional velocity and has dependency on the beam to flow angle, 2D velocity estimation using Doppler ultrasound is able to generate velocity vectors with axial and lateral velocity components. 2D velocity is useful even if complex flow conditions such as stenosis and bifurcation exist. There are two major methods of 2D velocity estimation using ultrasound: Speckle tracking and crossed beam Vector Doppler, which are based on measuring the time shifts and phase shifts respectively.\n\nVector Doppler is a natural extension of the traditional 1D Doppler imaging based on phase shift. The phase shift is found by taking the autocorrelation between echoes from two consecutive firings. The main idea of Vector Doppler is to divide the transducer into three apertures: one at the center as the transmit aperture and two on each side as the receive apertures. The phase shifts measured from left and right apertures are combined to give the axial and lateral velocity components. The positions and the relative angles between apertures need to be tuned according to the depth of the vessel and the lateral position of the region of interest.\n\nSpeckle tracking, which is a well-established method in video compression and other applications, can be used to estimate blood flow in ultrasound systems. The basic idea of speckle tracking is to find the best match of a certain speckle from one frame within a search region in subsequent frames. The decorrelation between frames is one of the major factors degrading its performance. The decorrelation is mainly caused by the different velocity of pixels within a speckle, as they do not move as a block. This is less severe when measuring the flow at the center, where the changing rate of the velocity is the lowest. The flow at the center usually has the largest velocity magnitude, called \"peak velocity\". It is the most needed information in some cases, such as diagnosing stenosis.\nThere are mainly three methods of finding the best match: SAD (Sum of absolute difference), SSD (Sum of squared difference) and Cross correlation. Assume formula_1 is a pixel in the kernel and formula_2 is the mapped pixel shifted by formula_3 in the search region.\n\nSAD is calculated as:\nformula_4\n\nSSD is calculated as:\nformula_5\n\nNormalized cross correlation coefficient is calculated as:\nformula_6\n\nwhere formula_7 and formula_8 are the average values of formula_1 and formula_10 respectively.\nThe formula_3 pair that gives the lowest D for SAD and SSD, or the largest ρ for the cross correlation, is selected as the estimation of the movement. The velocity is then calculated as the movement divided by the time difference between the frames. Usually the median or average of multiple estimations is taken to give more accurate result.\n\nIn ultrasound systems, lateral resolution is usually much lower than the axial resolution. The poor lateral resolution in the B-mode image also results in poor lateral resolution in flow estimation. Therefore, sub pixel resolution is needed to improve the accuracy of the estimation in the lateral dimension. In the meantime, we could reduce the sampling frequency along the axial dimension to save computations and memories if the sub pixel movement is estimated accurately enough. There are generally two kinds of methods to obtain the sub pixel accuracy: interpolation methods, such as parabolic fit, and phase based methods in which the peak lag is found when the phase of the analytic cross correlation function crosses zero.\n\nAs shown in the right figure, parabolic fit can help find the real peak of the cross correlation function. The equation for parabolic fit in 1D is:\nformula_12\n\nwhere formula_13 is the cross correlation function and formula_14 is the originally found peak. formula_15 is then used to find the displacement of scatterers after interpolation. For the 2D scenario, this is done in both the axial and lateral dimensions. Some other techniques can be used to improve the accuracy and robustness of the interpolation method, including parabolic fit with bias compensation and matched filter interpolation.\n\nThe main idea of this method is to generate synthetic lateral phase and use it to find the phase that crosses zero at the peak lag.\n\nThe right figure illustrates the procedure of creating the synthetic lateral phase, as a first step. Basically, the lateral spectrum is split in two to generate two spectra with nonzero center frequencies. The cross correlation is done for both the up signal and down signal, creating formula_16 and formula_17 respectively. The lateral correlation function and axial correlation function are then calculated as follows:\nformula_18\n\nwhere formula_19 is the complex conjugate of formula_17.\n\nThey have the same magnitude, and the integer peak is found using traditional cross correlation methods. After the integer peak is located, a 3 by 3 region surrounding the peak is then extracted with its phase information. For both the lateral and axial dimensions, the zero crossings of a one-dimensional correlation function at the other dimension’s lags are found, and a linear least squares fitted line is created accordingly. The intersection of the two lines gives the estimate of the 2D displacement.\n\nBoth methods could be used for 2D Velocity Vector Imaging, but Speckle Tracking would be easier to extend to 3D. Also, in Vector Doppler, the depth and resolution of the region of interest are limited by the aperture size and the maximum angle between the transmit and receive apertures, while Speckle Tracking has the flexibility of alternating the size of the kernel and search region to adapt to different resolution requirement. However, vector Doppler is less computationally complex than speckle tracking.\n\nVelocity estimation from conventional Doppler requires knowledge of the beam-to-flow angle (inclination angle) to produce reasonable results for regular flows and does a poor job of estimating complex flow patterns, such as those due to stenosis and/or bifurcation. Volumetric flow estimation requires integrating velocity across the vessel cross-section, with assumptions about the vessel geometry, further complicating flow estimates. 2D Doppler data can be used to calculate the volumetric flow in certain integration planes. The integration plane is chosen to be perpendicular to the beam, and Doppler power (generated from power Doppler mode of Doppler ultrasound) can be used to differentiate between the components that are inside and outside the vessel. This method does not require prior knowledge of the Doppler angle, flow profile and vessel geometry.\n\nUntil recently, ultrasound images have been 2D views and have relied on highly trained specialists to properly orient the probe and select the position within the body to image with only few and complex visual cues. The complete measurement of 3D velocity vectors makes many post processing techniques possible. Not only is the volumetric flow across any plane measurable, but also, other physical information such as stress and pressure can be calculated based on the 3D velocity field. However, it is quite challenging to measure the complex blood flow to give velocity vectors, due to the fast acquisition rate and the massive computations needed for it. Plane wave technique is thus promising as it can generate very high frame rate.\n\n\n"}
{"id": "57793609", "url": "https://en.wikipedia.org/wiki?curid=57793609", "title": "Dr. T's Music Software", "text": "Dr. T's Music Software\n\nDr. T's Music Software was a software company based in Massachusetts. Development was started in 1984 by Emile Tobenfeld. The company operated until the mid-1990s, and developed music software for the Commodore 64, Commodore 128, Commodore Amiga, Macintosh and mainly the Atari ST.\n\n\nPatch editors for:\n\n\n"}
{"id": "27387633", "url": "https://en.wikipedia.org/wiki?curid=27387633", "title": "EN 10080", "text": "EN 10080\n\nThe EN 10080: Steel for the reinforcement of concrete is a European Standard. This standard is referenced by EN 1992.\n\n"}
{"id": "57392917", "url": "https://en.wikipedia.org/wiki?curid=57392917", "title": "EcoCash", "text": "EcoCash\n\nEcoCash, is a mobile phone-based money transfer, financing and microfinancing service, launched in 2011 by Econet Wireless, for its customers in Zimbabwe. The company's headquarters is in the Econet Wireless Building in Borrowdale, a suburb of Harare, the largest city and capital of Zimbabwe. \n\nEconet allows users to deposit, withdraw, transfer money and pay for goods and services, including utility bills, from a mobile handset. Users can also buy pre-paid airtime or data bundles for themselves or others. Users can also redeem stored mobile money for cash. A fee for each service is deducted directly from the account stored on the mobile phone and accessed using a PIN. Users can deposit and withdraw money, transfer money to other users, pay bills including water, electricity, cable, satellite and school fees, purchase airtime, and transfer money between the service and a regular bank account. The service can be used from branches of ZimPost.\n\nAs of November 2017, EcoCash was reported to have 6.7 million registered users, compared with 2 million conventional bank account holders in the country. It controlled 99.8 percent of the mobile money market in Zimbabwe at the time. During the first six years of existence, the service processed over $23 billion. In 2017, Zimbabwe's Gross Domestic Product was valued at US$7.5 billion.\n\n\n"}
{"id": "62379", "url": "https://en.wikipedia.org/wiki?curid=62379", "title": "End instrument", "text": "End instrument\n\nIn telecommunications an end instrument is a piece of equipment connected to the wires at the end of a telecommunications link. In telephony, this is usually a telephone connected by a local loop. End instruments that relate to data terminal equipment include printers, computers, barcode readers, automated teller machines (ATMs) and the console ports of routers.\n\n"}
{"id": "27514608", "url": "https://en.wikipedia.org/wiki?curid=27514608", "title": "Enginator", "text": "Enginator\n\nAn enginator is an internal combustion engine whose fuel supply comes directly from underground deposits or from waste gas produced from a land fill. Its fuel supply will last as long as the natural deposit.\n\nEnginators are currently built by the Waukesha Engines company and have been featured on the Modern Marvels Episode \"Horsepower\".\n\nOther references: DC, Darren J Cloete\n"}
{"id": "22688319", "url": "https://en.wikipedia.org/wiki?curid=22688319", "title": "Eubac", "text": "Eubac\n\neu.bac or European Building Automation and Controls Association is a Brussels-based industry platform supporting Home Controls, Building Automation and Energy Services for Buildings. It represents 25 companies. Eu.bac was founded in 2003 and allows only direct company membership. The association has offices in Brussels, London, Paris and Frankfurt.\n\nThe companies' activities are focused on energy controls, building automation components and systems that are used in homes and non-residential buildings. Besides efficient energy controls, Information and Communications Technology (ICT) is the core of automation within a building that enables monitoring, regulation and automation to reduce the energy consumption. Energy efficiency and comfort are the major targets of building automation and home controls.\n\nWithin eu.bac, eu.ESCO works in the field of energy services. Energy service companies (ESCOs) with offerings in building automation and controls are the major contributors to this group. eu.ESCO works with its own logo and identity. eu.bac is more in the field of hardware, software and systems for Building Automation.\n\nThe President of eu.bac is Jean-Yves Blanc (Schneider Electric). Vice Presidents are Ernst Malcherek (Honeywell) and Gerhard Glinzerer (Herz Armaturen).\n\nBAC Systems and devices must comply with the standards set out by CEN/TC 247 and ISO/TC 205 (European & International Standardization Groups).\n\nThe group devised the marking scheme \"eu.bac Certification for energy efficient products in the range of home controls and building automation\", e.g. Electronic Radiator Thermostats, Room Thermostats, Heating controllers (OTC), Individual Zone Controllers (IZC), etcetera... The certification Mark was requested in France by the Reglementation Thermique since RT 2005. The delivered Control Accuracy value CA is needed to calculate building energy demand.\n\nIn addition eu.bac set up an Energy Efficiency Label for Home Controls and Building Automation Products. This labeling scheme complements the European certification scheme and introduces a simple and market-oriented system.\n\nFor Building Automation and Controls Systems – BACS, eu.bac set up the Certification Scheme for Energy Efficiency Performance of BACS at delivery and system lifetime. Except for this system, no reliable standards to help building owners ensure that buildings (new or refurbished) have the best available BACS technology. No standards are available to help building owners ensure that their building keeps performing as well as when it was first commissioned. The closest match to these requirements is the EN 15232 standard (Energy performance of buildings – Impact of Building Automation, Controls and Building Management) which is the base of the eu.bac system audit. The System Certification Scheme of eu.bac addresses the principal difficulty in the BACS industry that systems may not be installed and used so as to achieve available efficiencies. It also addresses the tendency of systems to deteriorate without proper maintenance. \n\nThe recast of the Energy Performance of Buildings Directive (2002/91/EC) attempts to reduce EU energy consumption by 5–6% (60–80 Mtoe) by 2020. The scope of the Directive includes existing buildings undergoing renovation.\n\n"}
{"id": "12512020", "url": "https://en.wikipedia.org/wiki?curid=12512020", "title": "Federal Ministry for Economic Affairs and Energy", "text": "Federal Ministry for Economic Affairs and Energy\n\nThe Federal Ministry for Economic Affairs and Energy (), abbreviated BMWi, is a cabinet-level ministry of the Federal Republic of Germany. It was previously known as the \"Ministry of Economy\". It was recreated in 2005 as \"Ministry of Economics and Technology\" after it had previously been merged with other ministries to form the Federal Ministry for Economics and Labour between 2002 and 2005.\n\nThe historical predecessor of the current Federal Ministry for Economic Affairs and Energy was the \"Reichswirtschaftsamt\" (Reich Economic Office), founded in 1917. In 1919, this became the \"Reichswirtschaftsministerium\" (Reich Ministry of Economy), which existed until 1945.\n\nIn postwar occupied Germany, its functions were exercised by the Administrative Office of Economy () between 1946 and 1949. After the founding of the Federal Republic of Germany, the Federal Ministry of Economics () existed from 1949 to 1998. From May 1971 to December 1972, it was temporarily merged with the Federal Ministry of Finance, in the Federal Ministry of Economics and Finance. In 1998 the technology section of the Ministry of Research was added, making it the Federal Ministry of Economics and Technology. Between 2002 and 2005, it was merged with parts of the former Ministry of Labour and Social Affairs, in Federal Ministry for Economics and Labour. In the cabinet under Angela Merkel, the two parts were once again split up in 2005, so that there was, once again, a Federal Ministry of Economics and Technology.\n\nThe Ministry is organised into 9 departments and one central department.\n\nThe ministry is headquartered in Berlin.\n\nIn addition to its own operations, the Ministry also oversees the following agencies:\n\nPolitical Party:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "28569986", "url": "https://en.wikipedia.org/wiki?curid=28569986", "title": "Flexible carpooling", "text": "Flexible carpooling\n\nFlexible carpooling is carpooling that is not arranged ahead of time, but instead makes use of designated meeting places. It seeks to replicate the informal \"slug-lines\" that form in Washington DC, Houston, and San Francisco, by establishing more formal locations for travelers to form carpools without advance contact.\n\nThe essence of the systems is the use of a meeting-place to form carpools, without any advance contact between the participants. For people wishing to carpool, going to a meeting place is a very low-effort method for getting into a carpool, compared with any other system that involves contacting potential riders or drivers in advance, and arranging the trip. The key is that other people are also coming to the meeting place, and there need to be sufficient people traveling from any one meeting-place to the common destination so that the waiting time to form a carpool is acceptable.\n\nThe Transportation Research Board is carrying out research to determine the feasibility of 'flexible carpooling to transit stations'. Since fewer people are carpooling today than were thirty years ago, the federal government, as well as local governments and the Department of Defense, are trying to reverse the decline.\n\nIt is estimated that San Francisco saves almost 1 million gallons of gasoline and associated emissions each year because of the operation of the informal flexible carpooling system.\n\nFrom 1979 to 1980, Marin County, California, implemented a flexible carpooling system using as meeting points several major intersections near bus stops.\n\nIn 2009 the Washington State Legislature set aside $400,000 for a pilot project to test meeting-place based carpooling in the SR 520 corridor of Seattle. This will incorporate the Avego smartphone ridematching system.\n\nIn 2010 the Auckland Regional Transport Authority considered a flexible carpooling proposal by Trip Convergence Ltd.\n"}
{"id": "18182925", "url": "https://en.wikipedia.org/wiki?curid=18182925", "title": "Fluorine perchlorate", "text": "Fluorine perchlorate\n\nPerchloryl Hypofluorite is the rarely encountered chemical compound of fluorine, chlorine, and oxygen with the chemical formula or . It is an extremely unstable gas that explodes spontaneously and has a penetrating odor.\n\nOne synthesis uses fluorine and perchloric acid, though the action of ClF on water is another method.\n\nAnother method of synthesis involves the thermal decomposition of tetrafluoroammonium perchlorate, , which yields very pure that may be manipulated and frozen without explosions.\n\nFluorine perchlorate is not analogous to perchloric acid because the fluorine atom does not exist as a positive ion. It contains an oxygen atom in a rare oxidation state of 0, due to the electronegativity of oxygen, which is higher than that of chlorine but lower than that of fluorine.\n\nFClO has a very dangerous and unpredictable series of reactions associated with it, as a covalent perchlorate (chlorine in the +7 oxidation state) and a compound featuring a very sensitive O-F single bond. Small amounts of reducing agent, such as organic compounds, can trigger explosive detonation. Products of these decomposition reactions could include oxygen halides, interhalogen compounds, and other hazardous substances.\n\nAccidental synthesis is possible if precursors are carelessly mixed. Like similar covalent fluorides and perchlorates, it needs to be handled with extreme caution.\n\nFClO is a strong oxidant and it reacts with iodide ion:\n\nFClO can also react with tetrafluoroethylene:\n\nIt may be a radical addition reaction.\n\n"}
{"id": "4774599", "url": "https://en.wikipedia.org/wiki?curid=4774599", "title": "Food, Beverages and Catering Union", "text": "Food, Beverages and Catering Union\n\nThe Food, Beverages and Catering Union (, NGG) is a trade union in Germany. It has a membership of 205,900 and is one of eight industrial affiliates of the German Confederation of Trade Unions.\n\nToday, NGG mainly represents employees at major companies such as McDonald's, Nestlé and Unilever in Germany.\n\n\n"}
{"id": "2657811", "url": "https://en.wikipedia.org/wiki?curid=2657811", "title": "Front-end processor", "text": "Front-end processor\n\nA front end processor (FEP), or a communications processor, is a small-sized computer which interfaces to the host computer a number of networks, such as SNA, or a number of peripheral devices, such as terminals, disk units, printers and tape units. Data is transferred between the host computer and the front end processor using a high-speed parallel interface. The front end processor communicates with peripheral devices using slower serial interfaces, usually also through communication networks. The purpose is to off-load from the host computer the work of managing the peripheral devices, transmitting and receiving messages, packet assembly and disassembly, error detection, and error correction. Two examples are the IBM 3705 Communications Controller and the Burroughs Data Communications Processor.\n\nSometimes FEP is synonymous with a communications controller, although the latter is not necessarily as flexible. Early communications controllers such as the IBM 270x series were hard wired, but later units were programmable devices.\n\nFront end processor is also used in a more general sense in asymmetric multi-processor systems. The FEP is a processing device (usually a computer) which is closer to the input source than is the main processor. It performs some task such as telemetry control, data collection, reduction of raw sensor data, analysis of keyboard input, etc.\n\nFront-end processes relates to the software interface between the user (client) and the application processes (server) in the client/server architecture. The user enters input (data) into the front-end process where it is collected and processed in such a way that it conforms to what the receiving application (back end) on the server can accept and process. As an example, the user enters a URL into a GUI (front-end process) such as Microsoft Internet Explorer. The GUI then processes the URL in such a way that the user is able to reach or access the intended web pages on the web server (application server known as the “back end” process). Front-end processors or communications processors relates to efficient use of the host CPU by off-loading processing for peripheral control, as an example, to another device or controller.\n\nFEPs are responsible for linking client applications and their associated networks to host computer based applications. With the advent of the Internet and of IP as a universal protocol, it is often assumed that there is no longer any need for FEPs, which traditionally handled SNA traffic. This may be true where FEPs provide only straight connectivity (and assuming IP address never changes). However, FEPs also perform other vital functions, that are closely linked to transaction applications, including message and transaction switching, multiplexing, transaction security, Quality of Service guarantors, and end-to-end transaction management and reporting. The need for these functions is especially important in mission critical transaction environments such as banking, government, point-of-sale, security, and health care applications. In these environments, FEP functionality is more necessary than ever before.\n\nAlthough the IBM Corporation withdrew its 3745/3746 Communications Front End Processors from marketing in 2003, the company continues to maintain the estimated 20,000 installed front end processors. IBM also provides microcode enhancement features. Smaller companies have filled the void created by IBM's action, providing machines, features, parts and services worldwide.\n\n"}
{"id": "53198596", "url": "https://en.wikipedia.org/wiki?curid=53198596", "title": "GameFace Labs", "text": "GameFace Labs\n\nGameFace Labs is an American technology company that develops hardware and software solutions for the consumer virtual reality market, and was founded in 2013 by Edward Mason. The company's headquarters are in San Francisco, with international offices in London, United Kingdom.\n\nThe company’s first product is a standalone headset that features an onboard Nvidia Tegra SoC (system on a chip) and does not require a tethered connection to a computer or console in order to operate. GameFace Labs is a founding member of the Open Source Virtual Reality project (OSVR) and the Immersive Technology Alliance. In December 2015, GameFace Labs joined Valve’s OpenVR ecosystem.\n\nDuring Insomnia Gaming Festival 49 (2013), the company revealed its first prototype; a standalone headset featuring a 720p LCD display panel and an Nvidia Tegra 3 SoC. At CES 2014, the company showcased an updated prototype with a 1080p OLED display and a Tegra 4 SoC. \n\nIn June 2014, the company showcased the first iteration of its user interface, allowing users to launch applications inside a virtual environment, as well as invite friends from other platforms to enjoy and collaborate on content inside the same virtual space.\nIn June 2014, the company also demoed its Mk V prototype at E3 - running on a version of Nvidia’s Tegra K1 Soc. The Mk V headset was the first publicly demoed headset to include a 1440p display, which was supplied by Samsung.. The K1 prototype utilized Nvidia’s mobile Kepler architecture, which means it supported Android content that used DirectX 11 and OpenGL 4.4 and was also compatible with all Google Cardboard content.\n\nAt 2017’s E3 expo, GameFace Labs demoed its seventh standalone prototype: a multi-platform headset supporting Android / DayDream content, as well as full 6DoF positional tracking inside an Android environment. The prototype featured dual OLED display panels, low persistence display technology and support for 6DoF input control via a wireless IR beacon (in January 2018 the company revealed it had been developing an ARM-based implementation of Valve's generation 2 Lighthouse technology, using GameFace's custom firmware\n). The company also showed off an Alpha build of its developer platform, allowing users to share and download content up to 1000x faster than traditional downloading methods.\n\nIn May 2018, GameFace Labs began shipping developer kits powered by Nvidia's TX2 SoC. This makes the GameFace Labs developer kit the first consumer electronics device to use the TX2, with Magic Leap announcing the second device later in July. The GameFace Labs developer kit is the only system to pair the TX2 SoC with an Android Operating system. \n\nIn 2014, GameFace Labs was awarded ‘Hack of Honor’ at the annual Penguicon open source convention\n"}
{"id": "42702352", "url": "https://en.wikipedia.org/wiki?curid=42702352", "title": "History of the Giro d'Italia", "text": "History of the Giro d'Italia\n\nThe Giro d'Italia (; ) is an annual stage race bicycle race primarily held in Italy, while also occasionally passing through nearby countries. The race was first organized in 1909 to increase sales of the newspaper \"La Gazzetta dello Sport\"; however it is currently run by RCS Sport. The race has been held annually since its first edition in 1909, except when it was stopped for the two world wars. As the Giro gained prominence and popularity the race was lengthened, and the peloton expanded from primarily Italian participation to riders from all over the world.\n\nThe first edition of the race was won by Italian Luigi Ganna who had the fewest total points at the end of the race; the same format was used for the next two years and also resulted in an Italian cyclist winning. The 1912 Giro saw the general classification contested by teams, which Atala-Dunlop won by ten points. The following year they reverted to the original system, before switching to the aggregate time model in 1914. Alfredo Binda won five editions of the race over a period of nine years, before Gino Bartali and Fausto Coppi consistently asserted their superiority in the Giro d'Italia. Italians dominated the race for forty years before the first non-Italian, Hugo Koblet, in 1950.\n\nAfter Koblet became the first foreigner to win the Giro, the Italians won the majority of the races held until 1968. In 1968, the Belgian Eddy Merckx won his first Giro d'Italia, which were soon followed by four more victories. Bernard Hinault won the first of his three Giros d'Italia in 1980. Ireland's Stephen Roche won the 1987 race en route to winning the Triple Crown of Cycling. Spanish rider Miguel Indurain won two consecutive Giros d'Italia, in 1992 and 1993, with his victories being followed by wins by Evgeni Berzin, Tony Rominger, and Pavel Tonkov. The next ten editions of the race were won by Italian riders, including the likes of Marco Pantani, Paolo Savoldelli, and Gilberto Simoni.\n\nSpain's Alberto Contador won the 2008 Giro d'Italia en route to completing the rare Giro-Vuelta double for the season. The 2009 race celebrated the hundredth year since the first edition of the Giro d'Italia and was won by the Russian Denis Menchov. The following year saw Ivan Basso win his second overall race. Contador originally won the 2011 Giro d'Italia, but after a positive drug test result was found, his victory was stripped and given to Michele Scarponi. Vincenzo Nibali, the winner of the 2013 race, led the event from the race's eighth stage and on.\n\nThe idea of the holding a bicycle race that navigated around Italy was first suggested when \"La Gazzetta dello Sport\" editor Tullo Morgagni sent a telegram to both the paper's owner, Emilio Costamagna, and cycling editor, Armando Cougnet, stating the need for an Italian tour. At the time \"La Gazzetta\"'s rival, \"Corriere della Sera\" was planning on holding a bicycle race of its own, after the success they had gained from holding an automobile race. Morgagni then decided to try and hold their race before \"Corriere della Sera\" could hold theirs, but \"La Gazzetta\" lacked the money. However, after the success \"La Gazzetta\" had with creating the Giro di Lombardia and Milan–San Remo, the owner Costamagna decided to go through with the idea. Their bike race was announced on August 7, 1908 in the first page of that day's edition of \"La Gazzetta dello Sport\". The race was to be held in May of 1909. The idea of the race was inspired by the Tour de France and the success that \"L'Auto\" had gained from it.\n\nSince the organizers lacked the funds, 25,000 lira, needed to hold the race, they consulted Primo Bongrani, an accountant at the bank Cassa di Risparmio and friend of the three organizers. Bongrani proceeded to go around Italy asking for donations to help hold the race. Bongrani's efforts were largely successful, he had procured enough money to cover the operating costs. The money that was to be given out as prizes came from a casino in San Remo after Francesco Sghirla, a former \"Gazzetta\" employee, encouraged it to contribute to the race. Even \"Corriere\", \"La Gazzetta\"'s rival, gave 3,000 lire to the race's fund.\n\nOn 13 May 1909 at 02:53 am 127 riders started the first Giro d'Italia at Loreto Place in Milan. The race was split into eight stages covering . A total of 49 riders finished, with Italian Luigi Ganna winning. Ganna won three individual stages and the General Classification. Ganna received 5325 lira as a winner’s prize, with the last rider in the general classification receiving 300 lira. The Giro's director received only 150 lira a month, 150 lira fewer than the last-placed rider.\n\nDuring these years, and up until 1950, the winners of the Giro d'Italia were exclusively of Italian descent. The inaugural race was such a success that the organizers added two more stages and over to the race. In addition, the organizers restructured the point distribution to determine the overall leader; the stage winner would get one point for finishing first, second place got two points, and so on until the 51st and up finishers, who would just receive 51 points. The first non-Italian stage winner, Jean-Baptiste Dortignacq, came in the 1910 edition of the race; he won the second stage of the race. Carlo Galetti led from stage two until the finish. The 1911 Giro d'Italia was the first Giro to not have the start or finish of the race in Milan. The start and finish for the race was moved to the Italian capital Rome to celebrate Italy's 50th anniversary of unification. This race also saw the first foreign rider to lead the race, the Frenchmen Lucien Petit-Breton, as well as the first repeat winner of the race in Carlo Galetti.\n\nThe 1912 Giro d'Italia saw some big changes to how the general classification was to be run; the race was to be centered around the team instead of the individual, meaning that the leader of the race would be a single team. Teams were allowed only four riders for each squad and were awarded points towards the general classification based on stage placings. The changes to the general classification were met with strong opposition from the start. Fourteen teams lined up at the start in Milan. Atala-Dunlop, which consisted of Luigi Ganna, Carlo Galetti, Eberardo Pavesi, and Giovanni Micheletto, led the race in all but one stage. Ganna was the only member of Atala-Dunlop that didn't finish the race, as he dropped out during the fifth stage. Galetti became the first three-time winner of the Giro d'Italia.\nThe following year's race was the last Giro to be run with a points system. The race saw the first appearance of Costante Girardengo, who won the sixth stage, and would come to dominate the Giro in the future. Carlo Oriani — who had just gotten out of serving the Italian military in the Italo-Turkish War — won the event by six points over the second-place finisher, Eberardo Pavesi. The 1914 Giro d'Italia saw the calculation of the general classification shift from a points system to a time based system. In the new system, every rider's finishing time for each stage is totaled together and the rider with the lowest total time is the overall leader. Out of the eighty-one riders entered the race only eight of them finished. The grueling race was won by Alfonso Calzolari, who won by almost two hours over the second-place finisher. A Giro was planned for 1915, but the plans were scrapped when Italy entered World War I.\n\nAfter the Great War ended, the race resumed in 1919. This edition of the race navigated through the ruined parts of Northern Italy, which made it hard for the organizers and riders. Costante Girardengo — the eventual winner — became the first rider to lead the Giro from start to finish. En route to winning overall, Girardengo won seven of the ten stages that composed the event. In addition, the Belgian Marcel Buysse became the first foreigner to finish on the podium; Buysse finished third overall, over an hour slower than Girardengo. The following year's Giro began with close to fifty riders and finished with ten. The first stage of the 1920 Giro d'Italia briefly went into Switzerland, which was the first time that the Giro had ever left its home country of Italy. Girardengo came into the race the favorite, but injuries sustained from crashing in the second stage forced him to withdraw. During the second stage, Gaetano Belloni capitalized on Girardengo's, race leader Giuseppe Olivieri's, Carlo Galetti's withdrawal to win the second stage and take the lead. Belloni, known by many as \"the Eternal Second, went on to win the Giro and proved he could win a race.\n\nCostante Girardengo won the first four stages of the 1921 Giro d'Italia and was leader after the fourth day. During the fifth stage, Girardengo was involved in a crash, and when Gaetano Belloni saw this, he attacked. Girardengo chased for before quitting. Belloni was then the new leader after stage 5's conclusion. After the fifth stage, Giovanni Brunero was the only rider close to Belloni. Brunero attacked during the race's seventh stage to win the day and take the overall lead by fifty-two seconds, which proved enough to last until the race's conclusion in Milan. The 1922 Giro d'Italia saw some controversy amongst the general classification contenders. In the race's first stage, Giovanni Brunero received an illegal wheel change – Brunero took a wheel from his teammate Alfredo Sivocci – which he was penalized 25 minutes for. Costante Girardengo and Gaetono Belloni, along with their respective teams Maino and Bianchi, wanted Brunero to be expelled from the Giro for the incident. Both the Bianchi and Maino squads eventually withdrew from the Giro due to their outrage with the organizer's decision to only give Brunero a 25-minute penalty. Brunero went on to win the Giro d'Italia, the second one of his career.\n\nThe 1923 Giro d'Italia was dominated by Costante Girardengo, who won eight out of the ten race's stages. Despite the large amount of stage victories, Girardengo's margin of victory was thirty-seven seconds over the second-place finisher Giovanni Brunero. This was Girardengo's second career Giro victory. Girardengo, Brunero, and Gaetano Belloni didn't start the 1924 edition due to an argument over their compensation. Their decision not to participate gave the other general classification hopefuls a bigger chance to win the Giro. After the event's eighth leg that saw awful road and weather conditions, Giuseppe Enrici emerged as the overall leader and eventual winner. In addition, this Giro saw the first and only woman ever to compete in the race's history in Alfonsina Strada. She was eliminated from the race after the seventh stage; however, the organizers allowed her to continue the race thought she wouldn't be included in the general classification. Strada made it all the way to the event's finish in Milan and finished around twenty hours slower than the winner Enrici.\n\nThe 1925 Giro d'Italia saw the emergence of a new star, Alfredo Binda. Despite winning six legs of the event, Costante Girardengo did not win the race. Binda gained the lead after the fifth stage, after he and a few other general classification contenders attacked while Girardengo was repairing a flat tire. Binda won the Giro in his first attempt. Reigning champion Binda started off the following year's race with a crash in the first stage, which caused him to lose lots of time. Due to the crash, Binda worked as a domestique for his teammate Giovanni Brunero. With Binda's aid, Brunero won the whole event, while Binda won six stages.\n\nBefore the 1927 Giro d'Italia, the organizers made some changes: the stage winner now received a one-minute time bonus and stages were now occasionally run on consecutive days, where before they had at least one rest day before each stage. The event witnessed Alfredo Binda win twelve stages and lead the race from start to finish en route to his second overall victory. Binda's record of twelve stage wins in a single Giro still stands. Binda returned the next year to win six of the twelve stages, along with the race as a whole. He first captured the lead after the fourth stage, where he significantly distanced himself from his competitors. Binda became the second person to win three Giro d'Italias in their career. In addition, the event saw a record number of participants at 298 riders, of which 126 finished the race.\n\nIn 1929 Alfredo Binda won a record eight consecutive stages on the way to his third consecutive, and fourth career, Giro d'Italia victory. This Giro began in Rome, which was the second time that the Giro had not started in Milan in the history of its running. When Binda came to the finish in Milan, he was booed by some of the spectators. The next year, Binda was paid 22,500 lire — the same amount of money the winner of the Giro would get — not to participate in the Giro. Binda's absence left the field open for everyone else. The eventual winner, Luigi Marchisio, gained the lead after winning the third stage of the race. Marchisio held a slim fifty-two second lead for the last six stages of the race to the finish in Milan. Marchisio became the youngest rider to win the Giro at 21 years, 1 month, and 13 days; his record stood for ten years before being broken by Fausto Coppi.\nThe famed race general classification leader's pink jersey () was introduced in 1931. The color pink was chosen for the leader's jersey because La Gazzetta dello Sport printed its news on pink paper. The maglia rosa was first worn by Learco Guerra, who won the first stage of the race. Alfredo Binda returned to the Giro after his absence the year before, but withdrew while leading during the sixth stage. The eventual winner, Francesco Camusso, attacked during the eleventh stage to claim the lead of the race. Binda came into the 1932 Giro d'Italia in poor form, so he decided to work for his teammate Antonio Pesenti. During the seventh stage of the race, Pesenti gained the lead of the race by winning the stage by means of a solo attack and went on to win the event as a whole.\n\nThe mountains classification and the individual time trial were both introduced in the 1933 Giro d'Italia. The mountains classification awarded points to the riders who crossed certain mountains on the race route first and the rider with the most points was the leader. In addition, the organizers also expanded the total number of stages to seventeen, it had been around twelve the few preceding years. Alfredo Binda gained the lead after the second stage, but lost it to Jef Demuysere after the fifth stage. After the eighth stage, Binda regained the lead and had a six-minute advantage over Demuysere. Binda won the next three stages and went on to win the race as a whole. Along with winning the general classification, Binda won the inaugural mountains classification. By winning this edition of the Giro, Binda became the first five-time winner of the Giro d'Italia.\n\nLearco Guerra won ten of the seventeen stages that comprised the 1934 Giro d'Italia. After Alfredo Binda abandoned the race because of injuries sustained from being hit by a police motorcycle, Guerra's biggest competitor was Francesco Camusso. Camusso gained the lead after his stage thirteen performance. The race's fourteenth stage was a time trial and Camusso lost over four minutes to Guerra because of his poor performance on the day. The two riders battled all the way to Milan, but Guerra won by a margin of fifty-one seconds. The 1935 edition of the Giro saw some changes to how it was run: the organizers removed the time bonuses for winning stages and began to make use of half stages — which is when two legs are held in the same day. This race was the last time Alfredo Binda competed and the first time future great Gino Bartali participated. The eventual winner, Vasco Bergamaschi, originally came to the Giro to work for his teammate, the great Costante Girardengo. However, he gained the lead for a day after the first stage and regained it again after the sixth day of racing.\n\nDue to Italy's political stance at the time, the 1936 Giro d'Italia saw no foreign participation. The organizers of the race included the first uphill individual time trial in the Giro's history, which traveled up to the summit of Monte Terminillo. Gino Bartali took the lead of the race by attacking on the final climb of the hilly ninth stage and held that lead to the event's conclusion. In addition to the general classification, Bartali won his second consecutive mountains classification title. For the 1937 Giro d'Italia the organizers decided to include the Dolomites and a team time trial for the first time. The team time trial lasted and was won by Legnano, the team of the eventual winner Gino Bartali. Bartali displayed his dominance in the mountains and gained the lead after the uphill stage 8a time trial. Bartali carried the pink jersey all the way to Milan and won his second consecutive Giro d'Italia.\n\nGino Bartali, the winner of the past two editions, was ordered by the Italian government to race the Tour de France instead of the Giro in 1938. Giovanni Valetti took the lead of the 1938 Giro d'Italia after the mountainous ninth stage. Valetti had a lead of a minute and a half after that stage and built upon that as the Giro went on, finishing almost nine minutes ahead of the second place rider Ezio Cecchi. The 1939 Giro d'Italia was a battle between Gino Bartali and Giovanni Valetti. Valetti took the lead after the stage 9b individual time trial. Bartali became the leader after he had attacked on the Passo Rolle during the fifteenth stage. Bartali lost the lead to Valetti because he crashed and had several flat tires throughout the sixteenth stage. Valetti won his second consecutive Giro d'Italia, while Bartali left the Giro with his fourth mountains classification title.\n\nBartali came to the 1940 Giro d'Italia with a strong Legnano team and high ambitions to win the overall crown; however his hopes were derailed when he crashed in the race's second stage and lost time. Fausto Coppi was promoted to the new team leader after Bartali's misfortunes. Coppi donned the general classification leader's pink jersey after attacking on the Abetone in the race's eleventh stage and managed to keep the lead all the way to the event's finish in Milan. Coppi became the youngest rider to ever win the Giro at 20 years, 8 months and 25 days old, breaking the record that was held by Luigi Marchisio. World War II brought the Giro's annual running to a halt after the 1940 edition.\nBenito Mussolini, Italy's dictator at the time, tried to keep the bicycle races going while Italy was involved in the Second World War. The Giro consumed so much gasoline, food, and other supplies that it would hinder Italy's efforts towards the war, so the normal Giro was not run. The government created a new \"point series\" Giro that was composed of the major one day races that were run in Italy, where the riders would earn points based on their placing in each race. Some of the notable races that comprised this \"Giro\" were the Milan–San Remo and the Giro di Lombardia. The new \"point series\" Giro was first won by Gino Bartali in 1942. The 1943 edition of the government's Giro was interrupted after Allie forces landed in Sicily and was subsequently Mussolini deposed. With Mussolini's reign over, bicycle racing came to a complete stop in Italy.\n\nThe Giro resumed its annual running in 1946. The organizers added the \"maglia nera\" () for the last rider in the overall classification. Bartali and Coppi returned to the Giro now on separate teams. Coppi was put into difficulty in stage nine, where he lost a good deal of time. The twelfth stage was set to pass through Pieris on the way to Trieste; however, in Pieris there were some Yugoslavs who threw stones at some armed Italian guards. Gunfire erupted, the stage was cancelled, and close to twenty riders were escorted to Trieste. In addition, there were riots that took place in Trieste because both Yugoslavia and Italy claimed the city as part of their territory. Bartali gained the lead after the thirteenth stage of the race and went on to win the event without winning a stage.\n\nThe 1947 Giro d'Italia was the first Giro to have all competing riders be a part of a trade team, rather than some riders competing as independents. Fausto Coppi, Gino bartali, and Aldo Ronconi broke away during the fourth leg while climbing the Abetone and rode into Prato where Coppi won the stage and Bartali took the overall lead. Bartali held that lead until the sixteenth stage when his chain dropped on the climb of the Falzarego when Coppi saw this he attacked. On the descent of the Falzarego, the same misfortune struck Coppi, which allowed Bartali to rejoin him. Later, Coppi attacked on the Passo Pordoi and Bartali could not keep up. Coppi took the stage and the overall lead, which he held all the way to race's finish. Fiorenzo Magni won the 1948 Giro d'Italia by a margin of eleven seconds — the smallest margin of victory in the history of the race — over Ezio Cecchi. Magni gained significant time after being a part of a successful breakaway in the race's ninth day of racing. Magni's lead over race favorites Coppi and Bartali was near thirteen minutes. Cecchi briefly gained the lead of the race for two stages; however, Magni took the lead after the seventeenth stage which saw riders cross the Pordoi Pass. Coppi and his team suspected that Magni received help from the spectators, which the organizers gave Magni a two-minute penalty for. The penalty to Magni wasn't enough to prevent him from winning the race, but he did so by the slimmest of margins.\nAfter the ninth stage of the 1949 edition, Fausto Coppi was close to ten minutes behind the race leader Adolfo Leoni in the general classification. Coppi gained over nine minutes on Leoni during the tenth stage, but Leoni remained in the lead. The race's seventeenth leg went over five major climbs before finishing in Pinerolo. Coppi attacked off the start and was the first over all the climbs and the finish line in Pinerolo; his efforts gained him the race lead and a sizable lead over his rival Gino Bartali. Coppi went on to win the Giro, his third such victory. Coppi came into the 1950 Giro d'Italia as the favorite to win the general classification; however, bad luck struck as he broke his pelvis in the race's ninth day. In the eighth leg, Hugo Koblet attacked on the Pian delle Fugazze to win the stage and gain the overall lead. Koblet kept the lead all the way to the Giro's finish in Milan and in doing so he became the first foreigner to win the Giro d'Italia. In addition, Koblet won the mountains classification.\n\nAlmost three years after his first Giro d'Italia victory, Fiorenzo Magni won the Giro d'Italia again. Magni's major rival in the 1951 edition was the Belgian Rik Van Steenbergen who performed very well in the Dolomites. Magni took the lead after descending successfully down the final climb of the eighteenth stage and then protecting the lead all way to the race's finish. The 1952 Giro d'Italia featured one of the first deaths by a rider in the Giro in Orfeo Ponsin who died from complications after crashing into a tree on the descent of the Merluzza. Fausto Coppi took the race lead upon the tenth stage's finish, but the following day he attacked on the Passo Pordoi and rode the rest of the stage by himself to increase his lead. Coppi won two stages after gaining the overall lead, which further cemented his lead in the race.\n\nThe Swiss rider Hugo Koblet gained the lead of the 1953 Giro d'Italia after the race's stage eight individual time trial. Koblet defended the successfully lead from Fausto Coppi up until the twentieth stage that contained the Passo dello Stelvio. The night before the twentieth stage, Koblet overused amphetamines, which made him uneasy throughout the stage. Kolbet marked an attack on the ascent of the Stelvio while showing signs of weakness. Coppi heard of Koblet's drug use and attacked. Koblet was unable to match Coppi's move and Coppi went on to win the stage and take the race lead. The time gap proved to be enough and Coppi won a record tying five Giro d'Italia.\n\nThe tensions at the start of the race in 1954 were high because the organizers paid Fausto Coppi a large sum to participate in the race The transaction angered the peloton and led to the race being not highly contested; this was extremely evident on the twenty-first stage when the riders took over nine hours to complete the stage. Eventual winner Carlo Clerici attacked during the sixth leg and gained enough of a time advantage over the rest of the peloton and win the Giro d'Italia. Fiorenzo Magni led the majority of the first half of the 1955 Giro d'Italia before losing the lead to another rider. During the twentieth leg, Magni attacked with Fausto Coppi when his fellow competitors had stopped to change their tires. Magni's efforts gave him the race lead and ultiamtely his third Giro d'Italia victory.\n\nThe 1956 edition of the Giro was without much drama until the twenty-first stage that stretched from Merano to the summit finish on Monte Bondone, a mountain in the Dolomites. The stage was bitterly cold with temperatures near that forced over sixty riders to abandon the race including the race leader Pasquale Fornara. Charly Gaul attacked during the stage and won the stage, while also gaining enough time to take the lead. After the Gaul crossed the stage's finish, he was taken to the hospital because his jersey was stuck to his skin. Gaul won the race and became the first Luxembourgian rider to win the Giro d'Italia. Gaul returned to the race in 1957 and regained the after the sixteenth leg. During the eighteenth stage, Gaul stopped to urinate which led his fierce rival Louison Bobet to initiate an attack with Gastone Nencini and Miguel Poblet. Gaul lost to Nencini; however, Gaul took out his frustration by aiding Nencini with his bid to win just to spite Bobet. With Gaul's help, Nencini went on to win the event by nineteen seconds over Bobet.\n\nItalian Ercole Baldini took the lead of the 1958 Giro d'Italia after winning the mountainous, fifteenth stage to Bosco Chiesanuova. He won one more stage en route to his overall victory. The 1959 Giro d'Italia was headlined by the favorites Jacques Anquetil and Charly Gaul. Anquetil held the lead of the race going into the penultimate stage. Before the stage, Gaul stated openly that he was going to attack on the slopes of the Piccolo San Bernardo which caused Anquetil to mark him for most of the stage. Gaul stayed true to his word and attacked as the riders made their way up the Piccolo San Bernardo. Anquetil — who had eaten poorly during the stage — was unable to counter his later attacks. Gaul got away, won stage, and gained close to ten minutes on Anquetil that was sufficient enough to win him the event.\nJacques Anquetil captured the lead of the 1960 Giro d'Italia after dominating the lengthy stage 14 time trial from Seregno to Lecco. Anquetil's lead shrunk because of bike issues which prevented him from countering attacks while climbing the Gavia Pass during the penultimate stage. Anquetil retained a twenty-eight second lead that proved to be enough to last to the race's finish, making him the first French winner of the Giro d'Italia. Arnaldo Pambianco captured his lone Giro victory after his efforts in a breakaway on the 1961 edition's fourteenth stage gave him the race lead.\n\nThe 1962 edition of the Giro d’Italia was marred by severe weather conditions. The fourteenth stage was shortened following a violent storm which prevented the climbing of the last two scheduled mountain passes and the stage finished atop the Passo Rolle. Angelino Soler won the race's sixteenth stage, with Franco Balmamion finishing second on the stage and taking the race lead. Balmamion successfully defended the lead all the way to the race's finish in Milan. Balmamion repeated as champion the next year after gaining the lead after the nineteenth stage that contained six difficult climbs.\n\nJacques Anquetil took the race lead at the 1964 Giro d'Italia after the stage five individual time trial. Despite the best attempts of Franco Balmamion and Italo Zilioli, Anquetil managed to keep his lead until the race's conclusion to win his second Giro d'Italia. The 1965 Giro d'Italia saw the introduction of the \"Cima Coppi\" — in honor of Fausto Coppi — which is the title given to the highest mountain in each Giro and gives more points towards the mountains classification than any other climb. Vittorio Adorni won the stage 13 individual time trial and gained a significant time advantage over the rest of the riders. Adorni held on to the race lead for the rest of the race and won his first Giro d'Italia.\n\nIn 1966 the points classification was introduced, which was to be awarded the most consistent high finishing riders in the peloton, specifically the sprinters. Jacques Anquetil came to the race with aspirations to win, but after losing time early on, he decided to ride for his teammate Gianni Motta. With Anquetil's help, Motta rode well through the mountains, gained the lead after the fifteenth leg, and went on to win the event as a whole. In addition, Motta also won the inaugural points classification. During the 1967 edition's twentieth stage, Felice Gimondi attacked on the slopes of the Tonale and race leader Anquetil was not able to match his move. Upon the stage's conclusion, Gimondi had gained the race lead by over three minutes. Gimondi raced into Milan the next day and won his first Giro d'Italia victory.\n\nThe 1968 Giro d'Italia saw two important firsts: the first tests for drug use and the first prologue. A total of eight riders tested positive during the Giro. Belgian Eddy Merckx won his first Giro d'Italia after winning the twelfth stage's finish atop the Tre Cime di Lavaredo and also regaining the race lead. En route to the overall victory, Merckx won four stages. Merckx returned in 1969 and was leading the race after the sixteenth stage that ended in Savona. Merckx tested positive for a banned substance after the stage and was subsequently disqualified from the race; to this day Merckx still proclaims his innocence. Felice Gimondi took the lead after Merckx's dismissal and held it all the way to the race's conclusion.\n\nMerckx came back the following year to liking of his sponsor. Merckx took the lead after stage five and never relinquished it; he dominated the lengthy stage nine time trial. Merckx went on to win the Tour de France and in doing so became the third rider to win two Grand Tours in a single calendar year. In 1971, reigning champion Merckx decided to ride the Critérium du Dauphiné Libéré instead. Felice Gimondi lost substantial time early on in the race to put him out of contention, while fellow Italian and teammate Gianni Motta tested positive for banned substances and was dismissed from the Giro. Swedish cyclist Gösta Pettersson gained the lead after the race's eighteenth stage and held it all the way to the finish. Pettersson became the first Swedish cyclist to win a Grand Tour.\n\nMerckx returned to the Giro in 1972 and resumed his domination. He grabbed the lead after a long solo attack during the race's seventh stage and never let go of the lead. Merckx led the 1973 Giro d'Italia from start to finish; a feat that had not been done since Alfredo Binda did in 1927. José Manuel Fuente gained the lead early on in 1974 and held it all the way up to the Giro's fourteenth stage. Fuente had forgotten to eat properly during the fourteenth stage and suffered because of it; he lost over ten minutes to Merckx. Merckx would go on to win his fifth and final Giro d'Italia by just 12 seconds, joining the likes of Alfredo Binda and Fausto Coppi as the only five-time winners of the Giro d'Italia. Merckx success continued on into the season as he won the Tour de France and the men's road race at the World Championships and became the first rider to complete the Triple Crown of Cycling – which consists of winning two Grand Tours and the men's road race at the World Championships in one calendar year.\n\nWith the absence of Merckx from the 1975 edition due to illness, the competition increased between the other riders. Fausto Bertoglio and Francisco Galdós battled during the latter half of the race. The final stage of the race had a summit finish on the Passo dello Stelvio where Bertoglio fended off the attacks of Galdós to seal his overall victory. Johan de Muynck was in the lead of the 1976 Giro d'Italia when he crashed during the twentieth stage. Muynck's injuries prevented him from performing well in the next day's individual time trial. Felice Gimondi capitalized on Muynck's woes and took the lead on the final day of racing and went on to win his second Giro d'Italia. This was the last Giro that Merckx raced; he finished eighth overall.\nFreddy Maertens and Francesco Moser dominated the early portion of the 1977 Giro d'Italia. Belgian Michel Pollentier took the lead from Moser when the race hit the high mountains near the end of the race. Pollentier went on to win the penultimate stage en route to his lone Grand Tour victory of his career. Johan de Muynck first grabbed the lead of the 1978 Giro d'Italia after escaping during the third stage and soloing to victory. He then successfully defended his slim lead throughout the rest of the race and won the Giro. The 1979 edition featured less climbing than normal and a total of five time trials. Francesco Moser grabbed the early lead of the race after winning the first two time trials of the race. Giuseppe Saronni took the lead after the third time trial which ended in San Marino. Saronni then rode into Milan with over a two-minute lead over Moser to win the Giro.\n\nBernard Hinault's Giro d'Italia was in 1980. Up until the twentieth stage, the race was being dominated by the Italian competitors. During the twentieth stage, Hinault and teammate Jean-René Bernaudeau distanced themselves from the general classification contenders on the slopes of the Passo dello Stelvio and rode into Sondrio for the stage win. Bernaudeau won the stage, but Hinault took a sizable lead over the rest of the field – which he then held to the race's conclusion in Milan. The 1981 Giro d'Italia was hotly contested, with four riders being 30 seconds apart after twenty days of racing. Stage 20 saw the finish atop the Tre Cime di Lavaredo. Giovanni Battaglin took the lead by almost a minute over the second place rider after doing well on the climb of the Lavaredo. Battaglin won the Giro after putting in a solid performance in the race's final stage, an individual time trial.\n\nBernard Hinault returned to the Giro in 1982. Hinault dominated the race with stage wins in every time trial stage and stage wins atop the Campitello Matese and the Montecampione en route to the overall victory. Hinault would go on to win the Tour de France that year as well and complete the coveted Giro-Tour double. The 1983 Giro d'Italia featured few hard stages in the mountains and four time trials. The winner of the race, Giuseppe Saronni, gained the lead after the race's seventh stage that finished in Salerno. From there, Saronni won two more stages and successfully guarded his lead all the way to Milan to win his second Giro d'Italia.\n\nThe 1984 Giro d'Italia was a battle between Italian Francesco Moser and Frenchman Laurent Fignon. Moser was leading the race up until the mountainous stage twentieth stage that finished in Arabba. Fignon took the lead after riding into Arabba over two minutes ahead of Moser. Moser dashed through the course setting a blistering pace on the roads, he won the stage and the Giro due to his performance in the final stage. Bernard Hinault raced the Giro again in 1985. The race was led early on by Italian Roberto Visentini. However, after the stage twelve time trial, Hinault was in control of the race and went to win his third Giro d'Italia.\nGiuseppe Saronni led the 1986 Giro d'Italia for the majority of the race before losing it to Roberto Visentini in the Alps. Visentini then fought off attacks from the challengers in the Dolomites en route to his first Giro d'Italia general classification victory. The 1987 edition was highlighted by the controversy between 's two general classification riders Roberto Visentini and Stephen Roche. Roche led the race early on but lost the lead to Visentini after crashing during the thirteenth stage. Roche attacked on the race's mountainous fifteenth stage despite orders from Carrera team management not to. Roche took the lead and wound up winning the Giro. Roche's success would not stop there during the 1987 season, he would go on to win the Tour de France and the men's road race at the World Championships to complete the Triple Crown of Cycling.\n\nThe 1988 Giro d'Italia is remembered for the fourteenth stage that contained very poor weather throughout the stage and most notably on the slopes of the Passo di Gavia. Franco Chioccioli led the race at the start of the fabled fourteenth stage. On the slopes of the Gavia, Andrew Hampsten and Erik Breukink rode away from their fellow riders; Breukink would go on to win the stage, but Hampsten would take the overall lead. Hampsten went on to win the race and became the first non-European to win the Giro d'Italia.\n\nDutchman Erik Breukink gained the lead of the 1989 Giro d'Italia after winning the stage 10 individual time trial. Breukink lost the lead after the fourteenth stage that contained five major passes. The Frenchman Laurent Fignon took the lead of the race from Breukink and then held it all the way to the finish in Florence. This was also the year the intergiro classification was introduced to the Giro d'Italia – the calculation for the intergiro is similar to that of the general classification, in each stage there is a midway point that the riders pass through a point and where their time is stopped and then totaled up after each stage. Jure Pavlič was the first winner of the intergiro classification. Gianni Bugno dominated the 1990 edition after gaining the lead after the first stage. Bugno led the race from start to finish – a feat that had only been done three times before in the history of the Giro d'Italia.\nFranco Chioccioli reigned supreme at the 1991 Giro d'Italia. Chioccioli led the race for all but two stages. He cemented his lead and the eventual overall victory by winning the seventeenth stage that contained a summit finish on the Passo Pordoi and winning the penultimate stage which was an individual time trial. Miguel Indurain became the first Spanish rider to win the Giro d'Italia in 1992 Indurain first gained the lead of the race after the hilly third stage that led into Arrezo and then held it all the way to the finish in Milan. He separated himself from his competitors during the race's two individual time trials, both of which he won. Indurain would go on to ride the Tour de France in July and win it, and in doing so completed the rare Giro-Tour double.\n\nIndurain returned in 1993 to defend his crown. The only rider that could compete with Indurain was the Latvian Piotr Ugrumov, who attacked Indurain repeatedly throughout the race. Indurain won two stages – both time trials – en route to his second Giro d'Italia victory. He would go on to complete the Giro-Tour double for the second consecutive year, a feat which had never been accomplished before. The 1994 Giro d'Italia saw Russian Evgeni Berzin gain the overall lead after winning the fourth stage, featuring a summit finish on Campitello Matese. Berzin consolidated his lead with victories in the race's final two time trials en route to the overall victory. In doing so he spoiled Indurain's hopes for a three peat.\n\nTony Rominger came to the 1995 Giro d'Italia in great form. Rominger gained the lead after the stage two time trial and never gave it up. His opposition came from the returning champion Berzin and teammate Piotr Ugrumov who attacked each other repeatedly, which greatly hurt their chances. In addition to the general classification, Rominger also won the points and intergiro classifications. The 1996 Giro d'Italia celebrated the centenary of the founding of \"La Gazzetta dello Sport\" by holding the first three stages in the Greek capital of Athens. Eventual winner Pavel Tonkov first gained the race lead after the mountainous thirteenth stage that ended in Prato Nevoso. Tonkov lost his slim lead to the Spaniard Abraham Olano for a two-stage period, before regaining it after stage 21, which contained five climbs of high severity. Tonkov rode into Milan the next day winner of the Giro d'Italia.\n\nPavel Tonkov returned to the Giro in 1997 with ambitions of repeating as winner. Tonkov first led the race after winning the stage three time trial and up until the fourteenth stage's conclusion. During the fourteenth stage, Italian Ivan Gotti attacked and soloed his way to take the stage win in Breuil-Cervinia and the race lead. Gotti extended his lead after performing well in the mountainous nineteenth stage and went on to win the Giro three days later. Swiss rider Alex Zülle was the first rider to lead the 1998 Giro d'Italia and he led for the most part of the race. Zülle was leading the race as it entered the Dolomites. Italians Giuseppe Guerini and Marco Pantani were at the head of the race during the race's mountainous seventeenth stage; the two riders worked together to get to the stage finish in Selva di Val Gardena. Guerini won the stage while Pantani took the overall lead. Pantani went on to win the Giro and subsequently the 1998 Tour de France, thus completing the rare feat of winning the Giro d'Italia and the Tour de France in the same calendar year.\n\nPantani returned to the Giro in 1999 while in peak physical form. Pantani gained the lead after the race's fourteenth stage and as the race hit the high mountains, he extended his lead with three stage wins. On the morning of the twentieth stage, Pantani was dismissed from the Giro after having hematocrit levels above 50%. 1997 victor Ivan Gotti, who was second place at the time, subsequently took the lead and wound up winning the Giro for the second time in his career. Francesco Casagrande took the lead in the 2000 Giro d'Italia after a long solo attack during the race's ninth stage. Fatigue set in with Casagrande as the race wore on and on the penultimate stage he lost the lead, and ultimately the Giro, to Stefano Garzelli.\n\nDario Frigo took the lead in the 2001 Giro d'Italia after the race's fourth stage. Frigo defended the lead until the thirteenth stage, when the race went over some major passes in the Dolomites. During the thirteenth stage, Gilberto Simoni attacked and his labor bore fruits as he took the race leader's \"maglia rosa\" when the stage was over. Frigo gained some time back in the stage fifteen time trial, but it wasn't enough to overcome Simoni's lead. Simoni went on to win the Giro d'Italia by a wide margin after Frigo's withdrawal. Stefano Garzelli took the early lead after winning the 2002 Giro d'Italia's second stage, but soon tested positive for probenecid – a banned substance – and was forced to leave the Giro. In the final major mountain stage of the race, stage seventeen, Paolo Savoldelli attacked with around nine kilometers to go in the stage and managed to take the lead and go on to win the Giro.\nAlessandro Petacchi was the first rider to lead the 2003 Giro d'Italia after winning the opening stage. Petacchi lost the lead to Stefano Garzelli after he won the stage seven summit finish on the Monte Terminillo. Garzelli then lost the lead to Gilberto Simoni after the tenth stage. Simoni went on to win the Giro after expanding his lead through stage wins on the Monte Zoncolan and the Alpe di Pampeago The 2004 Giro d'Italia saw a battle between Damiano Cunego, Serhiy Honchar, and Gilberto Simoni. Simoni gained the lead after the third stage and held it to the seventh stage where he lost it to Cunego. Cunego held the lead until the lengthy stage twelve individual time trial when Yaroslav Popovych took the lead. Cunego regained the lead after the sixteenth stage and went on to win the race, while fellow Italian Alessandro Petacchi won nine out of the 21 stages.\n\nThe 2005 Giro d'Italia saw the race lead change hands multiple times within the first week of racing. Ivan Basso gained the lead after the eleventh stage, which finished in Zoldo Alto. Two days later, Paolo Savoldelli gained the lead after the thirteenth stage that finish in Urtijëi. Savoldelli went on to win his second Giro d'Italia while fending off the attacks of Gilberto Simoni and José Rujano. Ivan Basso won the 2006 Giro d'Italia in a convincing fashion. Basso gained the lead after winning the race's eighth stage that featured a summit finish on the Passo Lanciano. He won two more stages after taking the lead of the race en route to his overall victory.\n\nThe race leader's pink jersey changed hands five times in the first week of racing in the 2007 Giro d'Italia. Andrea Noè took the lead away from Marco Pinotti after the race's tenth stage. Noè lost the lead to Danilo Di Luca after he won the twelfth stage into Briançon. Di Luca was not seriously challenged after taking the race lead in stage 12, and comfortably won the Giro in Milan with a two-minute gap over Schleck in second.\n\nThe 2008 Giro d'Italia was led for many days by Giovanni Visconti who had gained the lead after participating in a breakaway. Eventual winner Alberto Contador first took the lead of the race after the second mountain stage, to Marmolada, by finishing nearly fifteen minutes ahead of previous race leader Gabriele Bosisio – who had just gained the lead the stage before. In the race's final week, Contador faced stern challenges from Riccò and defending Giro champion Danilo Di Luca; however, their efforts bore no fruits as Contador went on to win the race. Russian Denis Menchov won the 2009 centennial edition of the Giro, after having taken the lead in a long time trial in stage 12, and defended it vigorously against attacks from his closest challenger, Danilo Di Luca, during the mountain stages of the final week. Di Luca came in second, 41 seconds behind the winner, and won the points classification. Subsequent to the Giro, both he and third-place finisher Franco Pellizotti became embroiled in doping scandals, were given bans, and had their results stripped.\n\nThe 2010 Giro d'Italia saw the lead change hands eight times during the race. Spanish rider David Arroyo was leading the race as it headed into the final mountain stages of the race. Arroyo lost the race lead to Ivan Basso after the nineteenth stage where he lost over three minutes to Basso. Basso fended off attacks and performed adequately in the final time trial to secure his second Giro d'Italia victory. Alberto Contador returned to the Giro in 2011 and was seen as the favorite for the overall victory on what many saw as a very difficult course.\n\nOn stage 3 of the 2011 Giro d' Italia, Wouter Weylandt had a fatal crash. The next stage was neutralized, with his Leopard Trek team and friend Tyler Farrar allowed to pass the finish line first.\n\nContador assumed the race lead after winning the ninth stage to Mount Etna. Contador continued to increase his advantage by riding well in the remaining stages and winning the stage 16 individual time trial, which allowed him to win his second Giro d'Italia championship. Contador raced the 2011 Giro despite having an ongoing trial about his possible use of clenbuterol, a banned substance. On 6 February 2012 the Court of Arbitration for Sport decided that Contador should lose his 2010 Tour de France title and his results since that race, which included his Giro victory in May 2011, and receive a two-year ban. After Contador's conviction, runner up Michele Scarponi was then delegated the overall victory.\n\nThe 2012 Giro d'Italia saw a battle between Canadian Ryder Hesjedal and Spaniard Joaquim Rodríguez. Hesjedal first took the lead after finishing well on the seventh stage that featured a summit finish to Rocca di Cambio. Rodríguez snagged a narrow lead over Hesjedal after winning the tenth stage into Assisi. Hesjedal regained the lead after the mountainous fourteenth stage; however, Rodríguez took it back the next day. Rodríguez held that lead all the way to the final stage, which he came into with a 31-second buffer over Hesjedal. Hesjedal rode and manage to finish with a time 47 seconds better than Rodríguez, giving him the overall victory in the Giro. In 2013 Vincenzo Nibali took the lead after the race's eighth stage. Nibali would go on to win the race after he expanded his lead through performing well in the early mountain stages and winning both the stage 18 individual time trial and the penultimate stage of the race.\n\nTom Dumoulin won the 100th Giro in 2017, the first time ever that a Dutch cyclist won the Giro. He was also the first Dutchman to win the overall in a Grand Tour since Joop Zoetemelk won the 1980 Tour de France. A year later, Chris Froome won the 2018 Giro, becoming the first British rider to win the overall race.\n\n"}
{"id": "26186739", "url": "https://en.wikipedia.org/wiki?curid=26186739", "title": "Hytort process", "text": "Hytort process\n\nThe Hytort process is an above-ground shale oil extraction process developed by the Institute of Gas Technology. It is classified as a reactive fluid process, which produces shale oil by hydrogenation. \n\nThe Hytort process has advantages when processing oil shales containing less hydrogen, such as the eastern United States Devonian oil shales. In this process, oil shale is processed at controlled heating rates in a high-pressure hydrogen environment, which allows a carbon conversion rate of around 80%. Hydrogen reacts with coke precursors (a chemical structure in the oil shale that is prone to form char during retorting but has not yet done so). In the case of Eastern US Devonian shales, the reaction roughly doubles the yield of oil, depending on the characteristics of the oil shale and process.\n\nIn 1980, the HYCRUDE Corporation was established to commercialize the Hytort technology. The feasibility study was conducted by HYCRUDE Corporation, Phillips Petroleum Company, Bechtel Group and the Institute of Gas Technology.\n\n"}
{"id": "55564417", "url": "https://en.wikipedia.org/wiki?curid=55564417", "title": "ISmash", "text": "ISmash\n\niSmash is a high-street technology repair service, specialising in fixing smartphones, tablets and computers.\n\niSmash was founded by Irish entrepreneur, Julian Shovlin, in 2013 to provide a same-day service for common smartphone repairs. The idea originated from Shovlin's own negative experience having his phone repaired, and wanting a solution for fixing damaged technology. Whilst a Business and Economics student at Trinity College Dublin, Shovlin set up his first repair shop at age 19 and invested £15,000 of savings in the business. This was later followed by a further £900,000 investment of seed money by angel investors.\n\nThe first iSmash-branded store opened on the King’s Road, London in 2013. Since then, iSmash has opened 29 high street, shopping centre and train station locations across the UK, including London, Manchester, Bristol, Leeds, Brighton and Sheffield.\n\nA nationwide 50-store expansion is planned for 2018, which will include locations in Birmingham, Liverpool, Cardiff, Newcastle, Reading, Edinburgh and Glasgow.\n"}
{"id": "18788210", "url": "https://en.wikipedia.org/wiki?curid=18788210", "title": "International Committee for the History of Technology", "text": "International Committee for the History of Technology\n\nThe International Committee for the History of Technology (ICOHTEC) was founded at a meeting of the International Congress on the History of Science, Technology, and Medicine in Paris in the summer of 1968. Its founding was the brainchild of Melvin Kranzberg, Professor of the History of Technology at Case Western Reserve University in Cleveland, Ohio, in the United States. Kranzberg had already established the Society for the History of Technology (SHOT) and its journal, \"Technology and Culture\", but he felt that in the midst of Cold War tensions, there was a need for another forum that focused on crossing the artificial borders of the Iron Curtain. For Kranzberg, the History of Technology, a new discipline not yet burdened with ideological or political freight as were other branches of history, was an ideal arena where scholars from East and West could find common ground. Kranzberg was supported in this effort by Maurice Daumas from France, Eugene Olszewski from Poland, and S.J. Schuchardine from the USSR, among others. For the past several decades, ICOHTEC's principal activity has been an \"annual meeting\", where scholars from many countries and from many disclipines gather and share their work. Papers presented at the meetings are usually published in the Committee's annual journal \"ICON\".\n\nKranzberg’s vision was largely fulfilled in the meetings that followed, although exchanges among scholars were not always harmonious. Since the fall of the Berlin Wall in 1989 and the breakup of the Soviet Union in the years that followed, ICOHTEC lost its original reason for being. But it has thrived in the following years and continues to have an active international membership and hold well-attended and intellectually lively meetings. One reason is the perception that its sister society, SHOT, remains somewhat biased toward U.S. membership in spite of efforts to reach out more to an international audience.\n\n"}
{"id": "55837901", "url": "https://en.wikipedia.org/wiki?curid=55837901", "title": "Jerzy Sikorski", "text": "Jerzy Sikorski\n\nJerzy Sikorski (born July 25, 1935) is a Polish historian, Copernicologist, medievalist, museologist, author, publisher, journalist, and encyclopedist, who writes and publishes primarily in Polish. He is a resident of Olsztyn, Poland.\n\nJerzy Sikorski was born on July 25, 1935 in Vilnius (Polish: Wilno), to Anna Wołk-Lewanowicz (1914–1995) and Feliks Sikorski (1889–1980). Five years later (1940) Sikorski's sister Maria Danuta was born.\n\nIn October 1944, during World War II, Sikorski's mother Anna was arrested by the People's Commissariat for State Security along with 40 other people, when she was exposed as a courier between Vilnius' \"Kedyw\" and the general staff of the Polish Home Army. Early in January 1945 Sikorski's father Feliks was also arrested because he was a soldier of the 1st Polish Corps under the Polish general Józef Dowbor-Muśnicki in the \"Polish-Soviet War,\", but he was released after three months. Upon his release, Feliks made efforts to obtain Anna's release from prison, locate and take care of Jerzy and Maria, and reunite his family. Anna was released from prison camp in June 1946, and the family moved to Olsztyn. They chose Olsztyn based on the fact that most of the Vilnius board of education where Feliks once worked had been evacuated to Olsztyn, and Feliks could resume his position there.\n\nFollowing World War II, Sikorski's mother was prevented from teaching due to political persecution. After Sikorski graduated from high school in 1953, he was repeatedly denied entry to university in spite of passing exams in Gdańsk, Warsaw, Toruń, and Poznań. Each time, he received the characteristic preprinted postcard with \"not accepted due to lack of space\". On the advice of a Polish studies specialist teacher (Polish philologist), Janina Kirkicka, Sikorski was able to enroll in a two-year teacher's college in Olsztyn, but after two weeks the director Bolesław Wytrążek instructed Sikorski to leave the college, due the decision of the local Polish United Workers' Party (PZPR) in Olsztyn. Through , who was the director at the Museum of Warmia and Masuria located at Olsztyn Castle, Sikorski obtained employment (1954–1955) at the , which was under the patronage of the regional museum in Olsztyn. The work at Frombork’s museum and the person of the great Polish Astronomer Nicolas Copernicus, who spent most of his life in Frombork and Warmia region, made a lasting impression on Sikorski. As a result, Copernicus and \"Copernicana\" research became the dominant subject in Sikorski's lifetime work and in his professional affiliations with various research centers. Ultimately, it led to Sikorski's discovery and publication of Copernicus' resting place at Frombork’s cathedral in 1973, which was confirmed archaeologically in 2005.\n\nIn the fall of 1955 Sikorski enrolled at the Nicolaus Copernicus University in Toruń, where he chose to study under professor , who among other scholarly pursuits studied and researched the life and politics of Copernicus' maternal uncle, Bishop Lucas Watzenrode. Sikorski wrote his master’s thesis on Watzenrode, and subsequently wrote his doctoral dissertation \"Polish monarchy and Warmia at the end of the 15th century: Issues in systemic-law and politics\".\n\nUpon completing of his academic studies at the Nicolas Copernicus University in Toruń in 1960, Sikorski accepted a position at the Museum of Warmia at Lidzbark Warmiński Castle, where Copernicus spent at least seven years (1503–1509). Copernicus had strong ties to the city of Lidzbark, and had visited there at the invitation of his maternal uncle Bishop Lucas Watzenrode in 1495. Sikorski worked at the museum from 1961 to 1962.\n\nLidzbark Warmiński Castle was of substantial significance for Jerzy Sikorski's \"Copernicana\" studies in terms of the history of astronomy; as well as because the castle was a strongly fortified seat of the bishops of Warmia and the administration center of their lands from the second half of the 14th century. At that time, Lidzbark city had one of the largest populations in Warmia, similar to Braniewo. In Lidzbark, Sikorski researched historic documents pertaining to the history of Poland's Baltic seashore (Pomerania, Warmia, Mazury, Prussia), and its extant architectural relics, documents, and other materials relating to Nicolaus Copernicus. Following the death of bishop Watzenrode in Toruń on March 29, 1512, Copernicus only sporadically visited Lidzbark, either as an emissary of the Warmia Chapter, or as a personal medical doctor for the successive bishops: (d. 1523), Mauritius Ferber (Polish: Maurycy Ferber 1471–1537), and Johannes Dantiscus (Polish: Jan Dantyszek; 1485–1548).\n\nAn organized index of published books and articles by Sikorski is listed at his web portal, covering four areas of research: 1. Copernicana; 2. Cities and castles of Prussia; 3. Historic tradition of the region; and, 4. The History of Science.\n\nAn additional indexed list of Sikorski's complete articles and papers is available online at the Database of Articles (in Polish), including two English language papers: \"The Empirical Table of Olsztyn the Question of Nicolaus Copernicus' Scientific Workshop\", and \"The Practice of Bishops' Burials in Frombork Cathedral and the Question of the Grave of Nicolaus Copernicus' Uncle Łukasz Watzenrode\".\nIn 1973, in time for the 500th anniversary of Nicolas Copernicus' birth, Sikorski authored a popular monograph on the astronomer's life, work, and times. Three updated editions were published, in 1985, 1995, and 2011.\n\nDr Jerzy Sikorski is credited for history of science consultation in the opening titles to the Polish motion picture \"Kopernik\", released in Toruń, Poland, on February 14, 1973. The film was released on the occasion of the world and United Nations celebrations of the 500th anniversary of Copernicus’ birth that were organized by the International Union of History and Philosophy of Science.\n\nBetween 1966 and 2007 Jerzy Sikorski published a number of articles in \"Mazury-Warmian Communications\" (\"\"), on the history of Poland's ancestral Baltic seashore Pomerania, and the Polish regions of Warmia and Masuria. He also published articles on the life and activities of the Polish astronomer Nicolaus Copernicus. \n\nSikorski was a member of the editorial staff at \"Komunikaty Mazursko-Warmińskie\", and also a member of staff at \"Frombork's Commentaries\" (\"Komentarze Fromborskie\"), and also the substitute editor-in-chief of \"Olsztyn Yearly\" (\"\"),\n\nJerzy Sikorski published a number of short biographies of Warmian Chapter canons and members in the \"Polish Biographical Dictionary\", which is printed by the Polish Academy of Sciences. These included biographies of Preuck Jan (1575–1631), Preuck Jerzy (d. 1556), Reich Feliks (c. 1475–1539), Sculteti Aleksander (1485–1564), Sculteti Bernard (d. 1518), and Sculteti Jan (d. 1526).\n\nReviewing the document \"Locationes mansorum desertorum\" (\"Allocation of abandoned fiefs\"), which was written by Copernicus in the region of Olsztyn Castle, Olsztyn, Poland, Jerzy Sikorski discovered that, out of the 136 names of allocation of fiefs, 60 were Polish names (peasants had only first names) written in phonetically correct Polish. Sikorski also cites an extant document where Copernicus’ witness was Copernicus’ Polish servant: \"Wojciech Cebulski\". These documents indicate that Copernicus spoke and wrote Polish.\nIn a letter to king Sigismund I of Poland handwritten by Nicolas Copernicus in Olsztyn Castle, the administrator, chancellor, and commander in chief of the defense of Olsztyn Castle Copernicus and Warmia Canons together affirm their Polish nationality as subjects to King Sigismund I of Poland against the enemy, the German Teutonic Order. The writers say that they are willing to die defending Olsztyn Castle, Warmia, and Poland from the Teutonic Knights. Nicolas Copernicus is directly associated with Sigismund I of Poland in the wars against the Teutonic Order, the reform of royal mints and the minting of coins, in establishing modern market economy in 16th century Poland, in direct contacts with king's personal medical doctor, with Cracow (Kraków) and the Jagiellonian University, and with the Polish Roman Catholic Church in Cracow.\n\nSikorski wrote \"the intellectual adventure of my life were my discoveries of Copernicus’ Polish ancestry and nationality\".\n\nCopernicus was reportedly buried in Frombork Cathedral, but archaeologists searched there in vain for centuries for his remains. Efforts to locate the remains in 1581, 1626, 1802, 1909, 1939 and 2004 came to nought. In 1973 Dr Jerzy Sikorski published the location of Copernicus' resting place in his book, including the photo of the Altar of Saint Wacław, today Altar of Saint Cross, with a subscription (English translation of Polish) \"The remains of Mikołaj Kopernik rest unnamed next to this altar\".\n\nSikorski's analysis of recovered chapter documents guided the Polish archaeological searches of Frombork in 2004–2005. The Institute of Anthropology and Archaeology at the Pultusk Academy of Humanities of Aleksander Gieysztor, with Polish archaeologists under the direction of , commenced the search for Copernicus’ grave from 16–31 of August, 2004, in an area of 10 square meters, and this search was financially supported by the Archaeologic Foundation of Prof Konrad Jażdżewski in Lódz, Poland. In the second archaeological search conducted in August 2005, Copernicus’ skull and remains were discovered in a grave marked by archaeologists as 13/5. Forensic expert Capt. Dariusz Zajdel of the Polish Police Central Forensic Laboratory used the skull to reconstruct a face that closely resembled the features—including a broken nose and a scar above the left eye—on a Copernicus self-portrait. The expert also determined that the skull belonged to a man who had died around age 70—Copernicus's age at the time of his death.\n\nThe remains were genetically tested in Poland and Sweden and found to match hair samples taken from a book owned by Copernicus which was kept at the library of the University of Uppsala in Sweden.\n\nSikorski discovered the location of Copernicus' Canonic Curia outside the walls of Frombork (the \"Curiae extra muros\"). The building is not extant, having been burned by Teutonic Knights on February 1, 1520, but the foundation remains. \n\nSikorski searched the area nearby with instruments such as ground-penetrating radar in an effort to find Copernicus’ observatory \"pavimentum\", which was believed to have been nearby, but did not find it.\nSikorski was aided in the search by the notebook of Elias Olsen, who was sent by Tycho Brahe to Frombork in 1584 to use the still-extant \"pavimentum\" to obtain astronomic observations for comparison with those of Copernicus. the \"pavimentum\" has still not been located. Its foundations may have been destroyed during extensive ground works near Copernicus’ external curia. \n\nSikorski since 1973 researched and wrote on Copernicus' observatory in Olsztyn, which contains a still-extant plaster astronomical table that was used by Copernicus from 1516 to 1521.\n\nSikorski has been awarded Polish presidential medals for his professional work as a historian and Copernicologist: the Gold Cross of Merit (1975) and Knight's Cross of the Order of Polonia Restituta (1986).\n\nOn the fifteenth anniversary of the Warmia-Mazury Business Club, the President of Poland Lech Kaczyński gave presidential orders and awards to deserving club members. On March 28, 2008, the title \"Personality of the Year 2008 in Warmia and Mazury\" was awarded to Jerzy Sikorski for his research into the life and science of Nicolas Copernicus, and for his 1973 discovery of Copernicus's resting place.\n\n\n"}
{"id": "47487717", "url": "https://en.wikipedia.org/wiki?curid=47487717", "title": "Keystone Nano", "text": "Keystone Nano\n\nKeystone Nano, founded in 2005, is an American-based company based in Pennsylvania, that creates nanoscale products to diagnose and treat human disease and improve the quality of life.\n\nKeystone Nano Inc. and team has been granted the follow patents:\n\n\nIn January 2017, the FDA approved the investigational new drug application, NanoLiposome, to assess the product as a form of treatment for solid tumors. Phase 1 trials will take place at the University of Maryland, University of Virginia, and the Medical University of South Carolina.\n\nKeystone was recently approved to begin clinical trials to assess ceramide nanoliposome for possible use in treating cancer. It's known as an anticancer therapeutic agent, that targets tumors specifically. Once the compound penetrates the cellular lining of the tumor, and only then, it releases the chemotherapeutic cargo. This allows the compound to destroy cancer cells without impacting and healthy cells. \n"}
{"id": "45586559", "url": "https://en.wikipedia.org/wiki?curid=45586559", "title": "Khibiny (electronic countermeasures system)", "text": "Khibiny (electronic countermeasures system)\n\nKhibiny (L-175V) (Хибины) (Л-175В) is Soviet / Russian aircraft electronic countermeasures (ECM) system.\n\nThe system is designed for radio direction-finding and probing signal source irradiation allowing it to distort reflected signal parameters. This helps to\n\n\nThe first work related to the creation ECM \"Khibiny\" began in the Kaluga Research Institute of Radio Engineering (KRIRE; Russian:\" Калужский научно-исследовательский радиотехнический институт\") in 1977. The plan was to create a unified set of electronic countermeasures for all the armed forces, where the tasks for KRIRE was to develop SIGINT blocks of equipment \"Proran\" and radio jammer (RJ) \"Regatta\", which was successfully completed with protected of scientific research in 1980.\n\nIn 1982 KRIRE was entrusted with research and development activities, first with \"Proran\" and \"Regatta\", and after an ECM as a whole, which includes many developments on related topics (including the use of accumulated experience in the development of RJ \" Sorption \", which was planned to be installed on the Su-27 ). When started research and development with new ECM, later named \"Khibiny\", it was intended to unite all the units, ensuring that they work closely with the Jets avionics.\n\nThe first samples of \"Khibiny\" were far from ideal as to their weight and size parameters were not suitable for installation on the aircraft. To solve this problem KRIRE collaborated closely with Sukhoi, working under the direction of Rollan G. Martirosov. Collaborate on a plane integrated ECM (received code \"product L-175V\") bore little resemblance to the conventional practice in such cases. Typically, the manufacturer of equipment issues to aircraft design bureau specification for the placement of products that represent a complete set of design documents and makes recommendations on placement of its parts with the restrictions on the length of the connections between them. In this case, the equipment \"Khibiny\" was immediately inserted into the design of the aircraft under development. As a result of close co-operation by the end of the 1980s the first stage of R&D was completed. March 18, 2014 was adopted the fighter-bomber Su-34, equipped with electronic countermeasures complex L-175V \"Khibiny\". Upgraded Khibiny EW complexes join air units in course of modernization.\n"}
{"id": "7375432", "url": "https://en.wikipedia.org/wiki?curid=7375432", "title": "List of unmanned aerial vehicles", "text": "List of unmanned aerial vehicles\n\nThe following is a list of unmanned aerial vehicles developed and operated in various countries around the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndia is also planning to buy UAVs two troops (eight drones each) of IAI Heron from Israel.Under the Rs 1,200 crore contract with Israel Aerospace Industries (IAI), the Army will begin inducting these new Heron drones from January 2014. Also, India is developing UAVs that are capable of flying on solar power.\nDeveloped by DRDO (Defence Research and Development Organisation, New Delhi and Hindustan Aeronautics Limited, Bangalore)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4761239", "url": "https://en.wikipedia.org/wiki?curid=4761239", "title": "Macroelectronics", "text": "Macroelectronics\n\nFor over half of century, the technology of microelectronics has been advancing by miniaturization, leading to significant increases in computing power and continuous decreases in manufacturing cost. In parallel, remarkable progress on enlarging system scale in recent years gives rise to a nascent field known as macroelectronics, in which microelectronic devices are distributed yet integrated over large area substrates with sizes much bigger than semiconductor wafers.\n\nCurrently, the macroelectronics industry is dramatically growing in the similar way as the microelectronics was in early ’90s. The most visible example of macroelectronics at present is flat-panel displays, which have been rapidly replacing cathode-ray tubes as the monitors of choice for computers and televisions since 2000. The flat-panel displays have enabled applications unimaginable for cathode-ray tubes. For example, the Dolphins Stadium in Miami will soon have the world's largest high definition video display, about 15 m high and 42 m wide, comprising more than 4.6 million light-emitting diodes, showing image of more than 1.5 million pixels.\n\nWhile the commercial success of flat-panel display opens an era of large area electronics, other emerging applications, such as rollable display, printable thin film solar cell and electronic skin, demonstrate further desirable attributes for macroelectronic systems, including flexibility, portability and low-cost. To realize these attributes, a growing trend is to fabricate macroelectronic products directly on flexible substrates, such as polymers. The flat-panel displays currently available in market are fabricated on glass substrates and are fragile. A case in recent news is the cracking of the screens of the iPod nano, a music player that Apple Computer expects to be its best-selling portable device. By contrast, displays made on thin polymer substrates are rugged. Flexible displays of large areas will be lightweight and can be rolled up – they will be portable. For example, in Sept. 2005, Philips Polymer Vision has revealed the world's first prototype of a rollable electronic reader, which can unfold to a 5-inch display and roll back into a pocket-size (100×60×20 mm) device. Furthermore, such thin-film devices on flexible polymer substrates can lend themselves to low-cost fabrication process (i.e., roll-to-roll printing), resulting in lightweight, rugged and flexible macroelectronic products.\n\n"}
{"id": "9809315", "url": "https://en.wikipedia.org/wiki?curid=9809315", "title": "MicroImages, Inc.", "text": "MicroImages, Inc.\n\nMicroImages, Inc. is a software development company in Lincoln, Nebraska founded in 1986. MicroImages' software products include TNTmips and TNTlite for GIS and image processing. The TNT products also support web mapping applications. MicroImages serves professional clients in over 120 nations around the world.\n\nMicroImages was founded in 1983 by Dr. Lee D. Miller, and Michael J. Unverferth, with its first software product, the Map and Image Processing System (MIPS). MIPS had been developed in a university setting for image processing on microcomputers. Since 1986, MicroImages has released over 50 consecutive upgrades to MIPS, which evolved into a broad, integrated system that includes features for image processing, GIS, CAD, TIN, desktop cartography, spatial database management and visualization. In 1993, MIPS was renamed TNTmips®, rewritten with a standard X Window interface, and made available for all Windows and Mac computers.\n\nMicroImages operates solely for software development, manufacturing, distribution, and support. The company performs no service work, consulting, or data collection activities, since by so doing it would be in direct competition with its customers and resellers. MicroImages does not manufacture any computer equipment, although MicroImages does occasionally provide system integration assistance and other specialized consulting services.\n\nMicroImages Press, a division of MicroImages, provides educational and training materials to support the integration of image processing, GIS, CAD, TIN, desktop cartography, spatial database management, visualization, and related objectives with the TNT products. The Press currently maintains and publishes color illustrated materials that include a series of free Tutorial booklets, Technical Guides, and Quick Guides. Documentation is available on the MicroImages web site in PDF format.\n\nMicroImages, Inc. maintains an experienced staff of technical support specialists with computer science or related degrees. These professionals provide direct world-wide support to MicroImages' clients without cost, and also manage the TNT products releases. MicroImages' phones are answered directly by real people and not by recorded messages or selection menus. Hundreds of international FAXes and email requests for assistance are also answered weekly.\n\nAll MicroImages' independent Resellers actually use the TNT products. As a result, they have the experience to help you design your individual projects or a company or organization-wide geospatial solution.\n\nFrom their own experience in the use of these products these Resellers can answer your questions with regard to what the TNT products can, and cannot do for you in your language. In the case where they are unable to answer your questions they will communicate them to MicroImages to get your answers.\n\nYou can get the TNT products either from our consultants, resellers, or directly from MicroImages, Inc.\n\n\nMicroImages is located in downtown Lincoln, Nebraska U.S.A. (N 40° 48' 44\", W 96° 42' 07\"), a community of 200,000, the second largest city in the state. The company's modern offices can be accessed via the 11th floor of the Sharp Tower in the heart of Lincoln's downtown district within easy walking distance of the Nebraska State Capitol and the University of Nebraska. This building houses the software engineers, MicroImages Press, software support staff, production and shipping, training facilities, and other company activities.\n\n\nA free version of TNTmips, TNTedit, and TNTview is available TNTmips Free®, for students and learning professionals. TNTmips Free is now used in almost every nation in the world.\n\nThe TNT products are sold directly by MicroImages and by resellers around the world. The resellers provide another level of support and service for the MicroImages clients now operating in over 120 nations and students in almost every nation.\n\nThe TNT products have been translated into 24 international languages, including such diverse languages as Chinese, Japanese, Korean, Russian, French, Spanish, Arabic, and Turkish. The international font and language support in the TNT products lets users see the program interface and database tables in their own language, and produce professional maps with annotations, labels, and legends in any language.\n\nMicroImages, Inc. Company Information\n\nMicroImages X Server\n\nTNTproducts\n\n"}
{"id": "23400395", "url": "https://en.wikipedia.org/wiki?curid=23400395", "title": "Microbial electrolysis cell", "text": "Microbial electrolysis cell\n\nA microbial electrolysis cell (MEC) is a technology related to Microbial fuel cells (MFC). Whilst MFCs produce an electric current from the microbial decomposition of organic compounds, MECs partially reverse the process to generate hydrogen or methane from organic material by applying an electric current. The electric current would ideally be produced by a renewable source of power. The hydrogen or methane produced can be used to produce electricity by means of an additional PEM fuel cell or internal combustion engine.\n\nMEC systems are based on a number of components:\n\nMicroorganisms – are attached to the anode. The identity of the microorganisms determines the products and efficiency of the MEC.\n\nMaterials – The anode material in a MEC can be the same as an MFC, such as carbon cloth, carbon paper, graphite felt, graphite granules or graphite brushes. Platinum can be used as a catalyst to reduce the overpotential required for hydrogen production. The high cost of platinum is driving research into biocathodes as an alternative. Or as other alternative for catalyst, the stainless steel plates were used as cathode and anode materials. Other materials include membranes (although some MECs are membraneless), and tubing and gas collection systems.\n\nElectrogenic microorganisms consuming an energy source (such as acetic acid) release electrons and protons, creating an electrical potential of up to 0.3 volts. In a conventional MFC, this voltage is used to generate electrical power. In a MEC, an additional voltage is supplied to the cell from an outside source. The combined voltage is sufficient to reduce protons, producing hydrogen gas. As part of the energy for this reduction is derived from bacterial activity, the total electrical energy that has to be supplied is less than for electrolysis of water in the absence of microbes. Hydrogen production has reached up to 3.12 mH/md with an input voltage of 0.8 volts. The efficiency of hydrogen production depends on which organic substances are used. Lactic and acetic acid achieve 82% efficiency, while the values for unpretreated cellulose or glucose are close to 63%.\nThe efficiency of normal water electrolysis is 60 to 70 percent. As MEC’s convert unusable biomass into usable hydrogen, they can produce 144% more usable energy than they consume as electrical energy. Depending on the organisms present at the cathode, MECs can also produce methane by a related mechanism.\n\nCalculationsOverall hydrogen recovery was calculated as \"RH\" = \"C\"\"R\". The Coulombic efficiency is \"C\"=(\"n\"/\"n\"), where \"n\" is the moles of hydrogen that could be theoretically produced and \"n\" = \"C\"/(2\"F\") is the moles of hydrogen that could be produced from the measured current, \"C\" is the total coulombs calculated by integrating the current over time, \"F\" is Faraday's constant, and 2 is the moles of electrons per mole of hydrogen. The cathodic hydrogen recovery was calculated as \"R\" = \"n\"/\"n\", where \"n\" is the total moles of hydrogen produced. Hydrogen yield (\"Y\") was calculated as \"Y\" = \"n\" /\"n\", where \"n\" is substrate removal calculated on the basis of chemical oxygen demand (22).\n\nHydrogen and methane can both be used as alternatives to fossil fuels in internal combustion engines or for power generation. Like MFCs or bioethanol production plants, MECs have the potential to convert waste organic matter into a valuable energy source. Hydrogen can also be combined with the nitrogen in the air to produce ammonia, which can be used to make ammonium fertilizer. Ammonia has been proposed as a practical alternative to fossil fuel for internal combustion engines.\n\n\n"}
{"id": "10345589", "url": "https://en.wikipedia.org/wiki?curid=10345589", "title": "Ministry of Petroleum and Natural Gas", "text": "Ministry of Petroleum and Natural Gas\n\nThe Ministry of Petroleum and Natural Gas (MOP&NG) is a ministry of the Government of India. It is responsible for the exploration, production, refining, distribution, marketing, import, export, and conservation of petroleum, natural gas, petroleum products, and liquefied natural gas in India.\n\nThe ministry is headed by the Cabinet Minister Dharmendra Pradhan. K D Tripathi, IAS is the Secretary of the Ministry.\n\n\n\n\nThe ministry has administrative control over various public-sector undertakings of the Government of India.\n\n\n\nOn February 20, 2015, A group of seven Indian petroleum ministry and oil firm employees were arrested for allegedly stealing and leaking classified documents to energy companies in exchange for cash payments. The group stole, photocopied and leaked confidential oil ministry documents related to energy pricing and policy in exchange for undisclosed sums of money. They were apprehended when Delhi police received a tip that the men were trying to break into offices at the ministry.\n\n\n"}
{"id": "28306547", "url": "https://en.wikipedia.org/wiki?curid=28306547", "title": "NAS Award in Aeronautical Engineering", "text": "NAS Award in Aeronautical Engineering\n\nThe NAS Award in Aeronautical Engineering, also known as the J.C. Hunsaker Award in Aeronautical Engineering, is awarded by the U.S. National Academy of Sciences \"for excellence in the field of aeronautical engineering.\" Established by Jerome C. Hunsaker and his wife, it was first awarded in 1968. \n\nSource: National Academy of Sciences\n\n\nFor his distinguished leadership in commercial aviation as exemplified by the development and commercialization of the Boeing 777\n\n\nFor his service to the nation as a dedicated aeronautical engineer, a leader in the aerospace defense industry, a public servant, a civic leader, and a thought leader in the engineering profession.\n\n\nFor leadership in engineering design and construction of SpaceShipOne, Voyager, and other successful experimental aircraft.\n\n\nFor his pioneering contributions to the aerodynamic design of high performance aircraft.\n\n\nFor his significant and enduring contributions to education, aerodynamics, and aircraft design, including the optimal Sears-Haack body and the original Northrop flying wing.\n\n\nFor his major contributions to aeronautics, including his supersonic area rule, discovery of the value of wing sweep in attaining supersonic flight, and application of the Heaviside Calculus to flight dynamics.\n\n\nFor his bold and farsighted leadership, which has enabled this nation to maintain and enhance its preeminence in commercial aircraft.\n\n\nFor his distinguished and pioneering contributions in engineering design and development of advanced military and commercial aircraft.\n\n\nFor his vast contribution to the aviation world.\n\n\nFor his long continued contributions to aeronautical engineering.\n\n\n"}
{"id": "194031", "url": "https://en.wikipedia.org/wiki?curid=194031", "title": "Nuclear fuel cycle", "text": "Nuclear fuel cycle\n\nThe nuclear fuel cycle, also called nuclear fuel chain, is the progression of nuclear fuel through a series of differing stages. It consists of steps in the \"front end\", which are the preparation of the fuel, steps in the \"service period\" in which the fuel is used during reactor operation, and steps in the \"back end\", which are necessary to safely manage, contain, and either reprocess or dispose of spent nuclear fuel. If spent fuel is not reprocessed, the fuel cycle is referred to as an \"open fuel cycle\" (or a \"once-through fuel cycle\"); if the spent fuel is reprocessed, it is referred to as a \"closed fuel cycle\".\n\nNuclear power relies on fissionable material that can sustain a chain reaction with neutrons. Examples of such materials include uranium and plutonium. Most nuclear reactors use a moderator to lower the kinetic energy of the neutrons and increase the probability that fission will occur. This allows reactors to use material with far lower concentration of fissile isotopes than are needed for nuclear weapons. Graphite and heavy water are the most effective moderators, because they slow the neutrons through collisions without absorbing them. Reactors using heavy water or graphite as the moderator can operate using natural uranium.\n\nA light water reactor (LWR) uses water in the form that occurs in nature, and requires fuel enriched to higher concentrations of fissile isotopes. Typically, LWRs use uranium enriched to 3–5% content of the less common isotope U-235, the only fissile isotope that is found in significant quantity in nature. One alternative to this low-enriched uranium (LEU) fuel are mixed oxide (MOX) fuels produced by blending plutonium with natural or depleted uranium, and these fuels provide an avenue to utilize surplus weapons-grade plutonium. Another type of MOX fuel involves mixing LEU with thorium, which generates the fissile isotope U-233. Both plutonium and U-233 are produced from the absorption of neutrons by irradiating fertile materials in a reactor, in particular the common uranium isotope U-238 and thorium, respectively, and can be separated from spent uranium and thorium fuels in reprocessing plants.\n\nSome reactors do not use moderators to slow the neutrons. Like nuclear weapons, which also use unmoderated or \"fast\" neutrons, these fast-neutron reactors require much higher concentrations of fissile isotopes in order to sustain a chain reaction. They are also capable of breeding fissile isotopes from fertile materials; a breeder reactor is one that generates more fissile material in this way than it consumes.\n\nDuring the nuclear reaction inside a reactor, the fissile isotopes in nuclear fuel are consumed, producing more and more fission products, most of which are considered radioactive waste. The buildup of fission products and consumption of fissile isotopes eventually stop the nuclear reaction, causing the fuel to become a spent nuclear fuel. When 3% enriched LEU fuel is used, the spent fuel typically consists of roughly 1% U-235, 95% U-238, 1% plutonium and 3% fission products. Spent fuel and other high-level radioactive waste is extremely hazardous, although nuclear reactors produce relatively small volumes of waste compared to other power plants because of the high energy density of nuclear fuel. Safe management of these byproducts of nuclear power, including their storage and disposal, is a difficult problem for any country using nuclear power.\n\nA deposit of uranium, such as uraninite, discovered by geophysical techniques, is evaluated and sampled to determine the amounts of uranium materials that are extractable at specified costs from the deposit. Uranium reserves are the amounts of ore that are estimated to be recoverable at stated costs.\n\nNaturally occurring uranium consists primarily of two isotopes U-238 and U-235, with 99.28% of the metal being U-238 while 0.71% is U-235, and the remaining 0.01% is mostly U-234. The number in such names refers to the isotope's atomic mass number, which is the number of protons plus the number of neutrons in the atomic nucleus.\n\nThe atomic nucleus of U-235 will nearly always fission when struck by a free neutron, and the isotope is therefore said to be a \"fissile\" isotope. The nucleus of a U-238 atom on the other hand, rather than undergoing fission when struck by a free neutron, will nearly always absorb the neutron and yield an atom of the isotope U-239. This isotope then undergoes natural radioactive decay to yield Pu-239, which, like U-235, is a fissile isotope. The atoms of U-238 are said to be fertile, because, through neutron irradiation in the core, some eventually yield atoms of fissile Pu-239.\n\nUranium ore can be extracted through conventional mining in open pit and underground methods similar to those used for mining other metals. In-situ leach mining methods also are used to mine uranium in the United States. In this technology, uranium is leached from the in-place ore through an array of regularly spaced wells and is then recovered from the leach solution at a surface plant. Uranium ores in the United States typically range from about 0.05 to 0.3% uranium oxide (UO). Some uranium deposits developed in other countries are of higher grade and are also larger than deposits mined in the United States. Uranium is also present in very low-grade amounts (50 to 200 parts per million) in some domestic phosphate-bearing deposits of marine origin. Because very large quantities of phosphate-bearing rock are mined for the production of wet-process phosphoric acid used in high analysis fertilizers and other phosphate chemicals, at some phosphate processing plants the uranium, although present in very low concentrations, can be economically recovered from the process stream.\n\nMined uranium ores normally are processed by grinding the ore materials to a uniform particle size and then treating the ore to extract the uranium by chemical leaching. The milling process commonly yields dry powder-form material consisting of natural uranium, \"yellowcake\", which is sold on the uranium market as UO. Note that the material isn't always yellow.\n\nUsually milled Uranium oxide, UO (Triuranium octaoxide) is then processed into either of two substances depending on the intended use.\n\nFor use in most reactors UO is usually converted to Uranium hexafluoride (UF), the input stock for most commercial uranium enrichment facilities. A solid at room temperature, uranium hexafluoride becomes gaseous at 57 °C (134 °F). At this stage of the cycle the uranium hexafluoride conversion product still has the natural isotopic mix (99.28% of U-238 plus 0.71% of U-235).\n\nFor use in reactors such as CANDU which do not require enriched fuel, the UO may instead be converted to uranium dioxide (UO) which can be included in ceramic fuel elements.\n\nIn the current nuclear industry the volume of material converted directly to UO is typically quite small compared to that converted to UF.\n\nThe natural concentration (0.71%) of the fissionable isotope U-235 is less than that required to sustain a nuclear chain reaction in light water reactor cores. Accordingly UF produced from natural uranium sources must be enriched to a higher concentration of the fissionable isotope before being used as nuclear fuel in such reactors. The level of enrichment for a particular nuclear fuel order is specified by the customer according to the application they will use it for: light-water reactor fuel normally is enriched to 3.5% U-235, but uranium enriched to lower concentrations is also required. Enrichment is accomplished using any of several methods of isotope separation. Gaseous diffusion and gas centrifuge are the commonly used uranium enrichment methods, but new enrichment technologies are currently being developed.\n\nThe bulk (96%) of the byproduct from enrichment is depleted uranium (DU), which can be used for armor, kinetic energy penetrators, radiation shielding and ballast. As of 2008 there are vast quantities of depleted uranium in storage. The United States Department of Energy alone has 470,000 tonnes. About 95% of depleted uranium is stored as uranium hexafluoride (UF).\n\nFor use as nuclear fuel, enriched uranium hexafluoride is converted into uranium dioxide (UO) powder that is then processed into pellet form. The pellets are then fired in a high temperature sintering furnace to create hard, ceramic pellets of enriched uranium. The cylindrical pellets then undergo a grinding process to achieve a uniform pellet size. The pellets are stacked, according to each nuclear reactor core's design specifications, into tubes of corrosion-resistant metal alloy. The tubes are sealed to contain the fuel pellets: these tubes are called fuel rods. The finished fuel rods are grouped in special fuel assemblies that are then used to build up the nuclear fuel core of a power reactor.\n\nThe alloy used for the tubes depends on the design of the reactor. Stainless steel was used in the past, but most reactors now use a zirconium alloy. For the most common types of reactors, boiling water reactors (BWR) and pressurized water reactors (PWR), the tubes are assembled into bundles with the tubes spaced precise distances apart. These bundles are then given a unique identification number, which enables them to be tracked from manufacture through use and into disposal.\n\nTransport is an integral part of the nuclear fuel cycle. There are nuclear power reactors in operation in several countries but uranium mining is viable in only a few areas. Also, in the course of over forty years of operation by the nuclear industry, a number of specialized facilities have been developed in various locations around the world to provide fuel cycle services and there is a need to transport nuclear materials to and from these facilities. Most transports of nuclear fuel material occur between different stages of the cycle, but occasionally a material may be transported between similar facilities. With some exceptions, nuclear fuel cycle materials are transported in solid form, the exception being uranium hexafluoride (UF) which is considered a gas. Most of the material used in nuclear fuel is transported several times during the cycle. Transports are frequently international, and are often over large distances. Nuclear materials are generally transported by specialized transport companies.\n\nSince nuclear materials are radioactive, it is important to ensure that radiation exposure of those involved in the transport of such materials and of the general public along transport routes is limited. Packaging for nuclear materials includes, where appropriate, shielding to reduce potential radiation exposures. In the case of some materials, such as fresh uranium fuel assemblies, the radiation levels are negligible and no shielding is required. Other materials, such as spent fuel and high-level waste, are highly radioactive and require special handling. To limit the risk in transporting highly radioactive materials, containers known as spent nuclear fuel shipping casks are used which are designed to maintain integrity under normal transportation conditions and during hypothetical accident conditions.\n\nA nuclear reactor core is composed of a few hundred \"assemblies\", arranged in a regular array of cells, each cell being formed by a fuel or control rod surrounded, in most designs, by a moderator and coolant, which is water in most reactors.\n\nBecause of the fission process that consumes the fuels, the old fuel rods must be replaced periodically with fresh ones (this is called a (replacement) cycle). During a given replacement cycle only some of the assemblies (typically one-third) are replaced since fuel depletion occurs at different rates at different places within the reactor core. Furthermore, for efficiency reasons, it is not a good policy to put the new assemblies exactly at the location of the removed ones. Even bundles of the same age will have different burn-up levels due to their previous positions in the core. Thus the available bundles must be arranged in such a way that the yield is maximized, while safety limitations and operational constraints are satisfied. Consequently, reactor operators are faced with the so-called optimal fuel reloading problem, which consists of optimizing the rearrangement of all the assemblies, the old and fresh ones, while still maximizing the reactivity of the reactor core so as to maximise fuel burn-up and minimise fuel-cycle costs.\n\nThis is a discrete optimization problem, and computationally infeasible by current combinatorial methods, due to the huge number of permutations and the complexity of each computation. Many numerical methods have been proposed for solving it and many commercial software packages have been written to support fuel management. This is an ongoing issue in reactor operations as no definitive solution to this problem has been found. Operators use a combination of computational and empirical techniques to manage this problem.\n\nUsed nuclear fuel is studied in Post irradiation examination, where used fuel is examined to know more about the processes that occur in fuel during use, and how these might alter the outcome of an accident. For example, during normal use, the fuel expands due to thermal expansion, which can cause cracking. Most nuclear fuel is uranium dioxide, which is a cubic solid with a structure similar to that of calcium fluoride. In used fuel the solid state structure of most of the solid remains the same as that of pure cubic uranium dioxide. SIMFUEL is the name given to the simulated spent fuel which is made by mixing finely ground metal oxides, grinding as a slurry, spray drying it before heating in hydrogen/argon to 1700 C. In SIMFUEL, 4.1% of the volume of the solid was in the form of metal nanoparticles which are made of molybdenum, ruthenium, rhodium and palladium. Most of these metal particles are of the ε phase (hexagonal) of Mo-Ru-Rh-Pd alloy, while smaller amounts of the α (cubic) and σ (tetragonal) phases of these metals were found in the SIMFUEL. Also present within the SIMFUEL was a cubic perovskite phase which is a barium strontium zirconate (BaSrZrO).\n\nUranium dioxide is very insoluble in water, but after oxidation it can be converted to uranium trioxide or another uranium(VI) compound which is much more soluble. Uranium dioxide (UO) can be oxidised to an oxygen rich hyperstoichiometric oxide (UO) which can be further oxidised to UO, UO, UO and UO.2HO.\n\nBecause used fuel contains alpha emitters (plutonium and the minor actinides), the effect of adding an alpha emitter (Pu) to uranium dioxide on the leaching rate of the oxide has been investigated. For the crushed oxide, adding Pu tended to increase the rate of leaching, but the difference in the leaching rate between 0.1 and 10% Pu was very small.\n\nThe concentration of carbonate in the water which is in contact with the used fuel has a considerable effect on the rate of corrosion, because uranium(VI) forms soluble anionic carbonate complexes such as [UO(CO)] and [UO(CO)]. When carbonate ions are absent, and the water is not strongly acidic, the hexavalent uranium compounds which form on oxidation of uranium dioxide often form insoluble hydrated uranium trioxide phases.\n\nThin films of uranium dioxide can be deposited upon gold surfaces by ‘sputtering’ using uranium metal and an argon/oxygen gas mixture. These gold surfaces modified with uranium dioxide have been used for both cyclic voltammetry and AC impedance experiments, and these offer an insight into the likely leaching behaviour of uranium dioxide.\n\nThe study of the nuclear fuel cycle includes the study of the behaviour of nuclear materials both under normal conditions and under accident conditions. For example, there has been much work on how uranium dioxide based fuel interacts with the zirconium alloy tubing used to cover it. During use, the fuel swells due to thermal expansion and then starts to react with the surface of the zirconium alloy, forming a new layer which contains both fuel and zirconium (from the cladding). Then, on the fuel side of this mixed layer, there is a layer of fuel which has a higher caesium to uranium ratio than most of the fuel. This is because xenon isotopes are formed as fission products that diffuse out of the lattice of the fuel into voids such as the narrow gap between the fuel and the cladding. After diffusing into these voids, it decays to caesium isotopes. Because of the thermal gradient which exists in the fuel during use, the volatile fission products tend to be driven from the centre of the pellet to the rim area. Below is a graph of the temperature of uranium metal, uranium nitride and uranium dioxide as a function of distance from the centre of a 20 mm diameter pellet with a rim temperature of 200 C. The uranium dioxide (because of its poor thermal conductivity) will overheat at the centre of the pellet, while the other more thermally conductive forms of uranium remain below their melting points.\n\nThe nuclear chemistry associated with the nuclear fuel cycle can be divided into two main areas; one area is concerned with operation under the intended conditions while the other area is concerned with maloperation conditions where some alteration from the normal operating conditions has occurred or (\"more rarely\") an accident is occurring.\n\nThe releases of radioactivity from normal operations are the small planned releases from uranium ore processing, enrichment, power reactors, reprocessing plants and waste stores. These can be in different chemical/physical form from releases which could occur under accident conditions. In addition the isotope signature of a hypothetical accident may be very different from that of a planned normal operational discharge of radioactivity to the environment.\n\nJust because a radioisotope is released it does not mean it will enter a human and then cause harm. For instance, the migration of radioactivity can be altered by the binding of the radioisotope to the surfaces of soil particles. For example, caesium (Cs) binds tightly to clay minerals such as illite and montmorillonite, hence it remains in the upper layers of soil where it can be accessed by plants with shallow roots (such as grass). Hence grass and mushrooms can carry a considerable amount of Cs which can be transferred to humans through the food chain. But Cs is not able to migrate quickly through most soils and thus is unlikely to contaminate well water. Colloids of soil minerals can migrate through soil so simple binding of a metal to the surfaces of soil particles does not completely fix the metal.\n\nAccording to Jiří Hála's text book, the distribution coefficient K is the ratio of the soil's radioactivity (Bq g) to that of the soil water (Bq ml). If the radioisotope is tightly bound to the minerals in the soil, then less radioactivity can be absorbed by crops and grass growing on the soil.\n\n\nIn dairy farming, one of the best countermeasures against Cs is to mix up the soil by deeply ploughing the soil. This has the effect of putting the Cs out of reach of the shallow roots of the grass, hence the level of radioactivity in the grass will be lowered. Also after a nuclear war or serious accident, the removal of top few cm of soil and its burial in a shallow trench will reduce the long-term gamma dose to humans due to Cs, as the gamma photons will be attenuated by their passage through the soil.\n\nEven after the radioactive element arrives at the roots of the plant, the metal may be rejected by the biochemistry of the plant. The details of the uptake of Sr and Cs into sunflowers grown under hydroponic conditions has been reported. The caesium was found in the leaf veins, in the stem and in the apical leaves. It was found that 12% of the caesium entered the plant, and 20% of the strontium. This paper also reports details of the effect of potassium, ammonium and calcium ions on the uptake of the radioisotopes.\n\nIn livestock farming, an important countermeasure against Cs is to feed animals a small amount of Prussian blue. This iron potassium cyanide compound acts as an ion-exchanger. The cyanide is so tightly bonded to the iron that it is safe for a human to eat several grams of Prussian blue per day. The Prussian blue reduces the biological half-life (different from the nuclear half-life) of the caesium. The physical or nuclear half-life of Cs is about 30 years. This is a constant which can not be changed but the biological half-life is not a constant. It will change according to the nature and habits of the organism for which it is expressed. Caesium in humans normally has a biological half-life of between one and four months. An added advantage of the Prussian blue is that the caesium which is stripped from the animal in the droppings is in a form which is not available to plants. Hence it prevents the caesium from being recycled. The form of Prussian blue required for the treatment of humans or animals is a special grade. Attempts to use the pigment grade used in paints have not been successful. Note that a good source of data on the subject of caesium in Chernobyl fallout exists at (\"Ukrainian Research Institute for Agricultural Radiology\").\n\nThe IAEA assume that under normal operation the coolant of a water-cooled reactor will contain some radioactivity but during a reactor accident the coolant radioactivity level may rise. The IAEA states that under a series of different conditions different amounts of the core inventory can be released from the fuel, the four conditions the IAEA consider are \"normal operation\", a spike in coolant activity due to a sudden shutdown/loss of pressure (core remains covered with water), a cladding failure resulting in the release of the activity in the fuel/cladding gap (this could be due to the fuel being uncovered by the loss of water for 15–30 minutes where the cladding reached a temperature of 650-1250 C) or a melting of the core (the fuel will have to be uncovered for at least 30 minutes, and the cladding would reach a temperature in excess of 1650 C).\n\nBased upon the assumption that a Pressurized water reactor contains 300 tons of water, and that the activity of the fuel of a 1 GWe reactor is as the IAEA predicts, then the coolant activity after an accident such as the Three Mile Island accident (where a core is uncovered and then recovered with water) can be predicted.\n\nIt is normal to allow used fuel to stand after the irradiation to allow the short-lived and radiotoxic iodine isotopes to decay away. In one experiment in the USA, fresh fuel which had not been allowed to decay was reprocessed (the Green run ) to investigate the effects of a large iodine release from the reprocessing of short cooled fuel. It is normal in reprocessing plants to scrub the off gases from the dissolver to prevent the emission of iodine. In addition to the emission of iodine the noble gases and tritium are released from the fuel when it is dissolved. It has been proposed that by voloxidation (heating the fuel in a furnace under oxidizing conditions) the majority of the tritium can be recovered from the fuel.\n\nA paper was written on the radioactivity in oysters found in the Irish Sea. These were found by gamma spectroscopy to contain Ce, Ce, Ru, Ru, Cs, Zr and Nb. Additionally, a zinc activation product (Zn) was found, which is thought to be due to the corrosion of magnox fuel cladding in spent fuel pools. It is likely that the modern releases of all these isotopes from the Windscale event is smaller.\n\nSome reactor designs, such as RBMKs or CANDU reactors, can be refueled without being shut down. This is achieved through the use of many small pressure tubes to contain the fuel and coolant, as opposed to one large pressure vessel as in pressurized water reactor (PWR) or boiling water reactor (BWR) designs. Each tube can be individually isolated and refueled by an operator-controlled fueling machine, typically at a rate of up to 8 channels per day out of roughly 400 in CANDU reactors. On-load refueling allows for the optimal fuel reloading problem to be dealt with continuously, leading to more efficient use of fuel. This increase in efficiency is partially offset by the added complexity of having hundreds of pressure tubes and the fueling machines to service them.\n\nAfter its operating cycle, the reactor is shut down for refueling. The fuel discharged at that time (spent fuel) is stored either at the reactor site (commonly in a spent fuel pool) or potentially in a common facility away from reactor sites. If on-site pool storage capacity is exceeded, it may be desirable to store the now cooled aged fuel in modular dry storage facilities known as Independent Spent Fuel Storage Installations (ISFSI) at the reactor site or at a facility away from the site. The spent fuel rods are usually stored in water or boric acid, which provides both cooling (the spent fuel continues to generate decay heat as a result of residual radioactive decay) and shielding to protect the environment from residual ionizing radiation, although after at least a year of cooling they may be moved to dry cask storage.\n\nSpent fuel discharged from reactors contains appreciable quantities of fissile (U-235 and Pu-239), fertile (U-238), and other radioactive materials, including reaction poisons, which is why the fuel had to be removed. These fissile and fertile materials can be chemically separated and recovered from the spent fuel. The recovered uranium and plutonium can, if economic and institutional conditions permit, be recycled for use as nuclear fuel. This is currently not done for civilian spent nuclear fuel in the United States.\n\nMixed oxide, or MOX fuel, is a blend of reprocessed uranium and plutonium and depleted uranium which behaves similarly, although not identically, to the enriched uranium feed for which most nuclear reactors were designed. MOX fuel is an alternative to low-enriched uranium (LEU) fuel used in the light water reactors which predominate nuclear power generation.\n\nCurrently, plants in Europe are reprocessing spent fuel from utilities in Europe and Japan. Reprocessing of spent commercial-reactor nuclear fuel is currently not permitted in the United States due to the perceived danger of nuclear proliferation. However, the recently announced Global Nuclear Energy Partnership would see the U.S. form an international partnership to see spent nuclear fuel reprocessed in a way that renders the plutonium in it usable for nuclear fuel but not for nuclear weapons.\n\nAs an alternative to the disposal of the PUREX raffinate in glass or Synroc matrix, the most radiotoxic elements could be removed through advanced reprocessing. After separation, the minor actinides and some long-lived fission products could be converted to short-lived or stable isotopes by either neutron or photon irradiation. This is called transmutation. However, a strong and long-term international cooperation, many decades of research and huge investments remain necessary before to reach a mature industrial scale where the safety and the economical feasibility of partitioning and transmutation (P&T) could be demonstrated.\n\nA current concern in the nuclear power field is the safe disposal and isolation of either spent fuel from reactors or, if the reprocessing option is used, wastes from reprocessing plants. These materials must be isolated from the biosphere until the radioactivity contained in them has diminished to a safe level. In the U.S., under the Nuclear Waste Policy Act of 1982 as amended, the Department of Energy has responsibility for the development of the waste disposal system for spent nuclear fuel and high-level radioactive waste. Current plans call for the ultimate disposal of the wastes in solid form in a licensed deep, stable geologic structure called a deep geological repository. The Department of Energy chose Yucca Mountain as the location for the repository. However, its opening has been repeatedly delayed. Since 1999 thousands of nuclear waste shipments have been stored at the Waste Isolation Pilot Plant in New Mexico.\n\nFast-neutron reactors can fission all actinides, while the thorium fuel cycle produces low levels of transuranics. Unlike LWRs, in principle these fuel cycles could recycle their plutonium and minor actinides and leave only fission products and activation products as waste. The highly radioactive medium-lived fission products Cs-137 and Sr-90 diminish by a factor of 10 each century; while the long-lived fission products have relatively low radioactivity, often compared favorably to that of the original uranium ore.\n\nAlthough the most common terminology is \"fuel cycle,\" some argue that the term \"fuel chain\" is more accurate, because the spent fuel is never fully recycled. Spent fuel includes fission products, which generally must be treated as waste, as well as uranium, plutonium, and other transuranic elements. Where plutonium is recycled, it is normally reused once in light water reactors, although fast reactors could lead to more complete recycling of plutonium.\n\nNot a cycle \"per se\", fuel is used once and then sent to storage without further processing save additional packaging to provide for better isolation from the biosphere. This method is favored by six countries: the United States, Canada, Sweden, Finland, Spain and South Africa. Some countries, notably Finland, Sweden and Canada, have designed repositories to permit future recovery of the material should the need arise, while others plan for permanent sequestration in a geological repository like the Yucca Mountain nuclear waste repository in the United States.\n\nSeveral countries, including Japan, Switzerland, and previously Spain and Germany, are using or have used the reprocessing services offered by BNFL and COGEMA. Here, the fission products, minor actinides, activation products, and reprocessed uranium are separated from the reactor-grade plutonium, which can then be fabricated into MOX fuel. Because the proportion of the non-fissile even-mass isotopes of plutonium rises with each pass through the cycle, there are currently no plans to reuse plutonium from used MOX fuel for a third pass in a thermal reactor. However, if fast reactors become available, they may be able to burn these, or almost any other actinide isotopes.\n\nThe use of a medium-scale reprocessing facility onsite, and the use of pyroprocessing rather than the present day aqueous reprocessing, is claimed to considerably reduce the proliferation potential or possible diversion of fissile material as the processing facility is in-situ/integral. Similarly as plutonium is not separated on its own in the pyroprocessing cycle, rather all actinides are \"electro-won\" or \"refined\" from the spent fuel, the plutonium is never separated on its own, instead it comes over into the new fuel mixed with gamma and alpha emitting actinides, species that \"self-protect\" it in numerous possible thief scenarios.\n\nIt has been proposed that in addition to the use of plutonium, the minor actinides could be used in a critical power reactor. Tests are already being conducted in which americium is being used as a fuel.\n\nA number of reactor designs, like the Integral Fast Reactor, have been designed for this rather different fuel cycle. In principle, it should be possible to derive energy from the fission of any actinide nucleus. With a careful reactor design, all the actinides in the fuel can be consumed, leaving only lighter elements with short half-lives. Whereas this has been done in prototype plants, no such reactor has ever been operated on a large scale.\n\nIt so happens that the neutron cross-section of many actinides decreases with increasing neutron energy, but the ratio of fission to simple activation (neutron capture) changes in favour of fission as the neutron energy increases. Thus with a sufficiently high neutron energy, it should be possible to destroy even curium without the generation of the transcurium metals. This could be very desirable as it would make it significantly easier to reprocess and handle the actinide fuel.\n\nOne promising alternative from this perspective is an accelerator-driven sub-critical reactor / subcritical reactor. Here a beam of either protons (United States and European designs) or electrons (Japanese design) is directed into a target. In the case of protons, very fast neutrons will spall off the target, while in the case of the electrons, very high energy photons will be generated. These high-energy neutrons and photons will then be able to cause the fission of the heavy actinides.\n\nSuch reactors compare very well to other neutron sources in terms of neutron energy:\n\n\nAs an alternative, the curium-244, with a half-life of 18 years, could be left to decay into plutonium-240 before being used in fuel in a fast reactor.\n\nTo date the nature of the fuel (targets) for actinide transformation has not been chosen.\n\nIf actinides are transmuted in a Subcritical reactor, it is likely that the fuel will have to be able to tolerate more thermal cycles than conventional fuel. An accelerator-driven sub-critical reactor is unlikely to be able to maintain a constant operation period for equally long times as a critical reactor, and each time the accelerator stops then the fuel will cool down.\n\nOn the other hand, if actinides are destroyed using a fast reactor, such as an Integral Fast Reactor, then the fuel will most likely not be exposed to many more thermal cycles than in a normal power station.\n\nDepending on the matrix the process can generate more transuranics from the matrix. This could either be viewed as good (generate more fuel) or can be viewed as bad (generation of more \"radiotoxic\" transuranic elements). A series of different matrices exists which can control this production of heavy actinides.\n\nFissile nuclei, like Uranium-235, Plutonium-239 and Uranium-233 respond well to delayed neutrons and are thus important to keep a critical reactor stable, and this limits the amount of minor actinides that can be destroyed in a critical reactor. As a consequence, it is important that the chosen matrix allows the reactor to keep the ratio of fissile to non-fissile nuclei high, as this enables it to destroy the long-lived actinides safely. In contrast, the power output of a sub-critical reactor is limited by the intensity of the driving particle accelerator, and thus it need not contain any uranium or plutonium at all. In such a system, it may be preferable to have an inert matrix that doesn't produce additional long-lived isotopes.\n\nThe actinides will be mixed with a metal which will not form more actinides, for instance an alloy of actinides in a solid such as zirconia could be used.\n\nThorium will on neutron bombardment form uranium-233. U-233 is fissile, and has a larger fission cross section than both U-235 and U-238, and thus it is far less likely to produce higher actinides through neutron capture.\n\nIf the actinides are incorporated into a uranium-metal or uranium-oxide matrix, then the neutron capture of U-238 is likely to generate new plutonium-239. An advantage of mixing the actinides with uranium and plutonium is that the large fission cross sections of U-235 and Pu-239 for the less energetic delayed-neutrons could make the reaction stable enough to be carried out in a critical fast reactor, which is likely to be both cheaper and simpler than an accelerator driven system.\n\nIt is also possible to create a matrix made from a mix of the above-mentioned materials. This is most commonly done in fast reactors where one may wish to keep the breeding ratio of new fuel high enough to keep powering the reactor, but still low enough that the generated actinides can be safely destroyed without transporting them to another site. One way to do this is to use fuel where actinides and uranium is mixed with inert zirconium, producing fuel elements with the desired properties.\n\nIn the thorium fuel cycle thorium-232 absorbs a neutron in either a fast or thermal reactor. The thorium-233 beta decays to protactinium-233 and then to uranium-233, which in turn is used as fuel. Hence, like uranium-238, thorium-232 is a fertile material.\nAfter starting the reactor with existing U-233 or some other fissile material such as U-235 or Pu-239, a breeding cycle similar to but more efficient than that with U-238 and plutonium can be created. The Th-232 absorbs a neutron to become Th-233 which quickly decays to protactinium-233. Protactinium-233 in turn decays with a half-life of 27 days to U-233. In some molten salt reactor designs, the Pa-233 is extracted and protected from neutrons (which could transform it to Pa-234 and then to U-234), until it has decayed to U-233. This is done in order to improve the breeding ratio which is low compared to fast reactors.\n\nThorium is at least 4-5 times more abundant in nature than all of uranium isotopes combined; thorium is fairly evenly spread around Earth with a lot of countries\nhaving huge supplies of it; preparation of thorium fuel does not require difficult\n\nand expensive enrichment processes; the thorium fuel cycle creates mainly Uranium-233 contaminated with Uranium-232 which makes it harder to use in a normal, pre-assembled nuclear weapon which is stable over long periods of time (unfortunately drawbacks are much lower for immediate use weapons or where final assembly occurs just prior to usage time); elimination of at least the transuranic portion of the nuclear waste problem is possible in MSR and other breeder reactor designs.\n\nOne of the earliest efforts to use a thorium fuel cycle took place at Oak Ridge National Laboratory in the 1960s. An experimental reactor was built based on molten salt reactor technology to study the feasibility of such an approach, using thorium fluoride salt kept hot enough to be liquid, thus eliminating the need for fabricating fuel elements. This effort culminated in the Molten-Salt Reactor Experiment that used Th as the fertile material and U as the fissile fuel. Due to a lack of funding, the MSR program was discontinued in 1976.\n\nCurrently the only isotopes used as nuclear fuel are uranium-235 (U-235), uranium-238 (U-238) and plutonium-239, although the proposed thorium fuel cycle has advantages. Some modern reactors, with minor modifications, can use thorium. Thorium is approximately three times more abundant in the Earth's crust than uranium (and 550 times more abundant than uranium-235). However, there has been little exploration for thorium resources, and thus the proved resource is small. Thorium is more plentiful than uranium in some countries, notably India.\n\nHeavy water reactors and graphite-moderated reactors can use natural uranium, but the vast majority of the world's reactors require enriched uranium, in which the ratio of U-235 to U-238 is increased. In civilian reactors, the enrichment is increased to 3-5% U-235 and 95% U-238, but in naval reactors there is as much as 93% U-235.\n\nThe term \"nuclear fuel\" is not normally used in respect to fusion power, which fuses isotopes of hydrogen into helium to release energy.\n\n"}
{"id": "9480930", "url": "https://en.wikipedia.org/wiki?curid=9480930", "title": "PEL (Pakistan)", "text": "PEL (Pakistan)\n\nPak Elektron Limited (PEL) is a Pakistani engineering corporation which manufactures major home appliances and electrical equipment.\n\nPEL was founded in 1956 through technical collaboration with AEG. In 1978, PEL was acquired by Saigol Group and was taken public a decade later. Over the years, PEL has formed alliances with several international giants, including General Electric, Fujitsu and Hitachi. The company also became the sole distributor of LG Corporation's home appliances in 2009. PEL operates in two segments - power and appliances. The former includes manufacturing of transformers, grid stations and energy meters among other goods, while the latter division deals in making, assembling and distribution of home appliances like refrigerators and air conditioner.\n\nPEL’s Appliances Division is the flag carrier of the Saigol Group involved in home appliances manufacturing.\n\nPEL window-type air conditioners were introduced in 1981 in technical collaboration with General Corporation of Japan. Ever since their launch, PEL air conditioners have a leading position in the market. Recognizing the shift in consumers' preference from window-type to split-type air conditioners, PEL has focused its manufacturing efforts on split-type air conditioners.\n\nThe manufacturing of refrigerators started in 1986-87 in technical collaboration with M/s IAR-SILTAL of Italy. Like air conditioners, PEL's refrigerators are also in great demand. Today, PEL Crystal has 30% market share across Pakistan.\n\nPEL deep freezers were introduced in 1987 in technical collaboration with M/s Ariston of Italy.\n\nPEL's Power Division consists of three Main Products: (1) Energy Meters; (2) Transformers; and (3) Switch gears.\n\n"}
{"id": "35908017", "url": "https://en.wikipedia.org/wiki?curid=35908017", "title": "Phonehenge West", "text": "Phonehenge West\n\nPhonehenge West was a large folk art structure envisioned and constructed by Alan Kimble \"Kim\" Fahey, including a 70-foot tower made from reclaimed material and props from old movie sets. The structure rested on his 1.7-acre property in Acton, California, and the majority of the construction was done by Fahey himself over the course of 30 years. Fahey is a retired phone company technician; much of the material consisted of unclaimed telephone poles. The compound included 13 structures and was a representation of folk art.\n\nThe structure was destroyed by the county of Los Angeles in August 2011 for building code violations. Los Angeles County Superior Court judge Daviann L. Mitchell ordered Fahey to pay the $83,488 in demolition costs in addition to performing 63 days of community service, of which a minimum of five days had to be served at the L.A. County or Kern County morgue. During the demolition, which took three weeks, four truckloads of material were removed from the property. Fahey requested that the material be saved for reuse but all 53 tons of telephone poles and 280 tons of steel were too badly damaged by the end of the demolition operation. In December 2012, Fahey was sentenced to 539 days in jail for failure to pay.\n\nThe name Phonehenge West is a parody of Stonehenge, a prehistoric monument located in the English county of Wiltshire.\n\n"}
{"id": "36576130", "url": "https://en.wikipedia.org/wiki?curid=36576130", "title": "Samuel Garbett", "text": "Samuel Garbett\n\nSamuel Garbett (1717– 5 December 1803) was a prominent citizen of Birmingham England, during the industrial revolution, and a friend of Matthew Boulton. Historian Carl Chinn argues that he:\n\nGarbett's education extended:\n\nGarbett was employed by a London merchant named Hollis, as his agent for purchasing goods in Birmingham. In that role, he came:\n\nHe married Anne Clay (d. 1772) of Aston in August 1735.\n\nHe then made his fortune as a merchant in his own right, before entering partnership with Dr John Roebuck to set up a laboratory in Steelhouse Lane where precious metals were refined and assayed; a manufacturing centre for sulphuric acid in Prestonpans in 1749; and, with others, the Carron Iron Works, in Scotland, in 1760, in which the two Birmingham men each held a 25% share. He also chaired, from January 1788, a Birmingham committee against the slave trade.\n\nHis eldest child and only daughter Mary married Charles Gascoigne in 1759, and in 1765 Gascoigne became a partner in the Carron works, having been manager of Garbett's nearby turpentine factory, Garbett & Co., since 1763.\n\nGarbett was involved in the creation of Birmingham Assay Office in 1773, and was the first chairman of Birmingham's Commercial Committee, forerunner of successive Birmingham Chambers of Commerce, as was a member of the committee that raised funds to create Birmingham General Hospital.\n\nHe was declaredbankrupt in 1782. Boulton encouraged him to re-establish his business in Birmingham, which he did successfully.\n\nAt his death in 1803, his estate was over £12,000, albeit with some creditors not discharged. He was buried at St Philip's Church (later Birmingham's cathedral), where he had been a church warden. Matthew Boulton wrote of him:\n\nThroughout his life, Garbett played a prominent part in local politics and affairs, including police proposals and the development of Birmingham's canals. During the Birmingham riots of 1791, it was at his house in Newhall Street that the town and country gentry held their emergency meetings. His political lobbying in general, and correspondence with Shelburne in particular, make him a significant figure in national politics.\n\n"}
{"id": "468008", "url": "https://en.wikipedia.org/wiki?curid=468008", "title": "Sandra Magnus", "text": "Sandra Magnus\n\nSandra Hall Magnus (born October 30, 1964) is an American engineer and a former NASA astronaut. She returned to Earth with the crew of STS-119 \"Discovery\" on March 28, 2009, after having spent 134 days in orbit. She was assigned to the crew of STS-135, the final mission of the Space Shuttle. She is also a licensed amateur radio operator with the call sign KE5FYE.\n\nMagnus was born and raised in Belleville, Illinois. She earned degrees in physics and electrical engineering from the University of Missouri–Rolla (now known as the Missouri University of Science and Technology) before earning a PhD in materials science and engineering from the Georgia Institute of Technology in 1996. Research for her dissertation, entitled \"An Investigation of the relationship between the thermochemistry and emission behavior of thermionic cathodes based on the BaO-ScO-WO ternary system,\" was supported by a fellowship from the NASA Lewis Research Center.\n\nDuring the 1980s, Magnus worked on stealth aircraft design as an engineer for McDonnell Douglas. She worked on the propulsion system for the A-12 Avenger II until the project was canceled by the Navy in 1991.\n\nMagnus was selected as an astronaut candidate in 1996 and flew her first space mission, STS-112, in October 2002 as a mission specialist. The main objective of Space Shuttle \"Atlantis\"' mission was the installation of the S1 truss section on the International Space Station (ISS) and consumables delivery. Magnus operated the space station's robotic arm during the three spacewalks required to install and activate the S1 truss. The flight duration was 10 days 19 hours 58 minutes and 44 seconds.\n\nFrom January 29–31, 2006, together with Oleg Artemiev and Michael Barratt, Magnus took part in a two-day examination for the ability to survive in an uninhabited area in case of the Soyuz descent module making an emergency landing. She passed this examination in the forest near Moscow.\n\nFrom September 16–22, 2006, Magnus served as the commander of NASA's mission, an undersea expedition at the National Oceanic and Atmospheric Administration's Aquarius laboratory located off the coast of Florida. With fellow astronaut/aquanauts Timothy Kopra, Robert Behnken and Timothy Creamer, all of whom were training for possible assignment to missions to the International Space Station, Magnus imitated moonwalks, tested concepts for mobility using various spacesuit configurations and weights to simulate lunar gravity. Techniques for communication, navigation, geological sample retrieval, construction and using remote-controlled robots on the moon's surface also were tested. National Undersea Research Center support crew members Larry Ward and Roger Garcia provided engineering support inside the habitat.\n\nMagnus served as Flight Engineer on board the International Space Station as part of Expedition 18. Magnus was a Mission Specialist on STS-126 for the trip to the station, which launched on November 14, 2008. She served as Mission Specialist on STS-119 when it returned on March 28, 2009. She logged 133 days in orbit and received warm greetings from NASA on her return. Her replacement, JAXA astronaut Koichi Wakata, was launched aboard \"Discovery\" on March 15, 2009.\n\nOn September 14, 2010, NASA announced Magnus to be one of four astronauts assigned to the STS-135 \"launch on need\" crew that was, if needed, to fly a rescue mission for STS-134, which was originally the last scheduled shuttle flight. Other members assigned to that crew were commander Christopher Ferguson, pilot Douglas G. Hurley, and fellow mission specialist Rex J. Walheim. In January 2011, NASA added STS-135 to the manifest as the final space shuttle mission, scheduled to launch in July 2011; STS-134 was conducted successfully in May 2011, requiring no rescue flight. The mission launched successfully on 8 July 2011 and landed on July 21.\n\nDr. Magnus left NASA Astronaut corps to become the Executive Director of the American Institute of Aeronautics and Astronautics (AIAA) .\n\n"}
{"id": "1313007", "url": "https://en.wikipedia.org/wiki?curid=1313007", "title": "Semiconductor memory", "text": "Semiconductor memory\n\nSemiconductor memory is a digital electronic data storage device, often used as computer memory, implemented with semiconductor electronic devices on an integrated circuit (IC). There are many different types of implementations using various technologies.\n\nMost types of semiconductor memory have the property of random access, which means that it takes the same amount of time to access any memory location, so data can be efficiently accessed in any random order. This contrasts with data storage media such as hard disks and CDs which read and write data consecutively and therefore the data can only be accessed in the same sequence it was written. Semiconductor memory also has much faster access times than other types of data storage; a byte of data can be written to or read from semiconductor memory within a few nanoseconds, while access time for rotating storage such as hard disks is in the range of milliseconds. For these reasons it is used for main computer memory (primary storage), to hold data the computer is currently working on, among other uses.\n\nShift registers, processor registers, data buffers and other small digital registers that have no memory address decoding mechanism are not considered as memory although they also store digital data.\n\nIn a semiconductor memory chip, each bit of binary data is stored in a tiny circuit called a \"memory cell\" consisting of one to several transistors. The memory cells are laid out in rectangular arrays on the surface of the chip. The 1-bit memory cells are grouped in small units called \"words\" which are accessed together as a single memory address. Memory is manufactured in word length that is usually a power of two, typically \"N\"=1, 2, 4 or 8 bits.\n\nData is accessed by means of a binary number called a memory address applied to the chip's address pins, which specifies which word in the chip is to be accessed. If the memory address consists of \"M\" bits, the number of addresses on the chip is 2, each containing an \"N\" bit word. Consequently, the amount of data stored in each chip is \"N\"2 bits. The memory storage capacity for \"M\" number of address lines is given by 2, which is usually in power of two: 2, 4, 8, 16, 32, 64, 128, 256 and 512 and measured in kibibits, mebibits, gibibits or tebibits, etc. the largest semiconductor memory chips hold a few gibibits of data, but higher capacity memory is constantly being developed. By combining several integrated circuits, memory can be arranged into a larger word length and/or address space than what is offered by each chip, often but not necessarily a power of two.\n\nThe two basic operations performed by a memory chip are \"\"read\", in which the data contents of a memory word is read out (nondestructively), and \"write\"\" in which data is stored in a memory word, replacing any data that was previously stored there. To increase data rate, in some of the latest types of memory chips such as DDR SDRAM multiple words are accessed with each read or write operation.\n\nIn addition to standalone memory chips, blocks of semiconductor memory are integral parts of many computer and data processing integrated circuits. For example, the microprocessor chips that run computers contain cache memory to store instructions awaiting execution.\n\nRAM (\"Random-access memory\") has become a generic term for any semiconductor memory that can be written to, as well as read from, in contrast to ROM \"(below)\", which can only be read. All semiconductor memory, not just RAM, has the property of random access.\n\nVolatile memory loses its stored data when the power to the memory chip is turned off. However it can be faster and less expensive than non-volatile memory. This type is used for the main memory in most computers, since data is stored on the hard disk while the computer is off. Major types are:\n\nNonvolatile memory preserves the data stored in it during periods when the power to the chip is turned off. Therefore, it is used for the memory in portable devices, which don't have disks, and for removable memory cards among other uses. Major types are:\n"}
{"id": "3678878", "url": "https://en.wikipedia.org/wiki?curid=3678878", "title": "United States Army Research Laboratory", "text": "United States Army Research Laboratory\n\nThe Army Research Laboratory (ARL) is the U.S. Army's corporate research laboratory. ARL is headquartered at the Adelphi Laboratory Center (ALC) in Adelphi, Maryland. Its largest single site is at Aberdeen Proving Ground, Maryland. Other major ARL locations include Research Triangle Park, North Carolina, White Sands Missile Range, New Mexico, Orlando, Florida, and NASA's Glenn Research Center, Ohio and Langley Research Center, Virginia.\n\nIn addition to the Army Research Office, ARL has six technical directorates:\n\nBefore the forming of the ARL, the United States Army had research facilities dating back to 1820 when the laboratory at Watertown Arsenal, Massachusetts, studied pyrotechnics and waterproof paper cartridges. This facility would evolve into the Materials Technology Laboratory. Most pre-WWII military research occurred within the military by military personnel, but in 1945, the Army published a policy affirming the need for civilian scientific contributions in military planning and weapons production. Non-military involvement before this time was frequent; however, methods for contribution to warfare technology was on limited and incidental basis. \nOn June 11, 1946, a new research and development division of the War Department General Staff was created; however, due to internal forces within the military which supported the traditional technical service structure the division was closed. A variety of reorganizations took place over the next four decades, which put many organizations in command of Army research and development. Often commanders of these organizations were advocates of the reorganization, while some middle level management was opposed to the change.\n\nThe ARL represents the realization of a memorandum dated January 6, 1989 from the LABCOM Commander recommending integrating the corporate laboratories into a single entity. As part of the Base Realignment and Closure of 1989/1991, the consolidated research facilities would be located primarily at Adelphi Laboratory Center and Aberdeen Proving Ground. This would also relocate the majority of operations at MTL to APG. The Federal Advisory Commission reviewed and accepted the creation of ARL in 1992.\n\nThe \"Army Research Office\" (ARO), located in Research Triangle Park, funds extramural basic research, that is, research in outside academic and industrial organizations, providing grants to both single investigators and university-affiliated research centers, as well as outreach programs.\n\nThe \"Computational and Information Sciences Directorate\" (CISD) is the foremost U.S. Army organization for research and development of modern electronic systems. This research supports capabilities in the analysis, distribution, and assimilation of real or simulated digitized battlefield information. In addition to the digitized war initiatives, high performance computing research is performed at a variety of centers under the ARL. The use of supercomputers for mathematical simulations instead of mass fabrication can result in the conservation of human and physical resources. The directorate is a key player in the International Technology Alliance in conjunction with SEDD and HRED directorates.\n\nThe U.S. Army Research Laboratory's \"Human Research and Engineering Directorate\" (HRED) is the principal Army organization for research and development (R&D) in the human dimension. HRED conducts a broad-based program of scientific research and technology directed toward optimizing Soldier performance and Soldier-machine interactions to maximize battlefield effectiveness. HRED also executes an analysis mission that provides the Army with human factors leadership to ensure that Soldier performance requirements are adequately considered in technology development and system design. HRED coordinates technologies within the Army, other services and their laboratories, industry, and academia to leverage basic and applied research opportunities for the benefit of the Army.\n\n\"Sensors and Electron Devices Directorate\" (SEDD) is a group dedicated to produce equipment from tiny chips to fully integrated systems. SEDD also helps develop sensors and electronic devices which become an important part of modern warfare. Other components of this directorate include: multifunction radio frequency equipment, autonomous sensing, power generation and management, and signal processing algorithms. One of the best ways to protect the soldier is to develop autonomous sensing systems for all activities, from intelligence gathering to waging warfare.\n\nThe \"Survivability/Lethality Analysis Directorate\" (SLAD) is the primary center for expertise in survivability, lethality, and vulnerability of all army systems, across the full range of battlefield threats: ballistic, electronic warfare, information operations, and nuclear, biological, and chemical warfare. SLAD's mission is to assist technology and system developers in optimizing system designs and to provide analytical data to evaluators and decision makers in the Army and the DOD.\n\nThe U.S. Army Research Laboratory's \"Vehicle Technology Directorate\" (VTD) manages research and development of vehicle propulsion and structure. In addition, VTD conducts analytical and practical experiments in loads analysis, structural dynamics, aero-elasticity, structural acoustics, and vibration reduction. VTD makes ground combat vehicles lighter, more reliable, safer, and more fuel efficient. Air combat vehicles such as the helicopter are studied to decrease the vibrations caused by the rotors. Metallic materials are engineered to increase strength and decrease corrosion through the use of composites. The divisions of VTD are Loads & Dynamics, Structural Mechanics, Engine & Transmission Systems, Engine Components, and Unmanned Vehicles.\n\nThe U.S. Army Research Laboratory \"Weapons and Materials Research Directorate\" (WMRD) is the principal U.S. Army organization for research and development in weapons and materials technologies. The directorate is charged with making bullets pierce farther and ballistic vests stronger. Although these interests may seem opposed, the goal is to increase the survivability of the soldier. In the WMRD, traditional armaments are studied, as are advanced ballistic defense systems. This directorate is especially linked with VTD in order to produce safer vehicles, transports, and aircraft. One aspect of this directorate's analysis duty is to evaluate military technologies from an economical standpoint in order to reduce overall system costs.\n\nBeginning in 1996, the Army Research Laboratory entered into innovative cooperative agreements with industrial and academic partners to form \"federated laboratories,\" or \"FedLabs\", to perform research in broad areas important to the U.S. Army where the ARL could extract significant leverage from work being performed in the commercial and academic arenas. The first three FedLabs were in Advanced Displays, Advanced Sensors, and Telecommunications. Each FedLab was a large consortium of companies, universities, and non-profit organizations, with both an overall industrial leader and an ARL leader. The cooperative agreements forming the FedLabs were somewhat unusual in that the ARL was not a mere funder of research, but an active consortium participant. The first three FedLabs won a National Performance Award in 1999 from then-Vice President Al Gore.\n\nCollaborative Technology and Research Alliances is a term for partnerships between Army laboratories and centers, private industry and academia for performing research and technology development intended to benefit the US Army. The partnerships are funded by the US Army.\n\n\n\n"}
{"id": "24096397", "url": "https://en.wikipedia.org/wiki?curid=24096397", "title": "Wistron", "text": "Wistron\n\nWistron Corporation () is a major original design manufacturer in Taiwan. It was the manufacturing arm of Acer Inc. before being spun off in 2000.\n\nWistron employs over 80,000 people worldwide. As an ODM (original design manufacturer) Wistron designs and manufactures products for other companies to sell under their own brand name. Wistron focuses on ICT (information and communication technology) products, including notebook PCs, desktop PCs, servers, storage, LCD TVs, handheld devices, and devices and equipment for medical applications. Wistron provides a variety of technology services within the design, manufacturing, and after-sales service functions tailored to meet customers' specific requirements.\nIn July 2011, Wistron and Microsoft entered into an agreement that offered coverage under Microsoft's exclusive rights portfolio for Wistron products including tablets, mobile phones and other devices running the Chrome OS or Android platform. In September 2011, Wistron signed a patent licensing agreement with Intellectual Ventures.\n\n\n"}
