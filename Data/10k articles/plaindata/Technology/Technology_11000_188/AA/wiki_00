{"id": "15034446", "url": "https://en.wikipedia.org/wiki?curid=15034446", "title": "1980s oil glut", "text": "1980s oil glut\n\nThe 1980s oil glut was a serious surplus of crude oil caused by falling demand following the 1970s energy crisis. The world price of oil, which had peaked in 1980 at over US$35 per barrel (equivalent to $ per barrel in 2008 dollars, when adjusted for inflation), fell in 1986 from $27 to below $10 ($ to $ in 2008 dollars). The glut began in the early 1980s as a result of slowed economic activity in industrial countries (due to the crises of the 1970s, especially in 1973 and 1979) and the energy conservation spurred by high fuel prices. The inflation-adjusted real 2004 dollar value of oil fell from an average of $78.2 in 1981 to an average of $26.8 per barrel in 1986.\n\nIn June 1981, \"The New York Times\" stated an \"Oil glut! ... is here\" and \"Time Magazine\" stated: \"the world temporarily floats in a glut of oil\", though the next week an article in \"The New York Times\" warned that the word \"glut\" was misleading, and that in reality, while temporary surpluses had brought down prices somewhat, prices were still well above pre-energy crisis levels. This sentiment was echoed in November 1981, when the CEO of Exxon Corp also characterized the glut as a temporary surplus, and that the word \"glut\" was an example of \"our American penchant for exaggerated language\". He wrote that the main cause of the glut was declining consumption. In the United States, Europe and Japan, oil consumption had fallen 13% from 1979 to 1981, due to \"in part, in reaction to the very large increases in oil prices by the Organization of Petroleum Exporting Countries and other oil exporters\", continuing a trend begun during the 1973 price increases.\n\nAfter 1980, reduced demand and increased production produced a glut on the world market. The result was a six-year decline in the price of oil, which culminated by plunging more than half in 1986 alone.\n\nDuring the 1980s, reliance on Middle East production dwindled as commercial exploration developed major non-OPEC oilfields in Siberia, Alaska, the North Sea, and the Gulf of Mexico, and the Soviet Union became the world's largest producer of oil. Smaller non-OPEC producers including Brazil, Egypt, India, Malaysia, and Oman doubled their output between 1979 and 1985, to a total of 3 million barrels per day.\n\nIn April 1979, Jimmy Carter signed an executive order which was to remove price controls from petroleum products by October 1981, so that prices would be wholly determined by the free market. Carter's successor, Ronald Reagan signed an executive order on January 28, 1981, which enacted this reform immediately, allowing the free market to adjust oil prices in the US. This ended the withdrawal of old oil from the market and artificial scarcity, encouraging increased oil production. The US Oil Windfall profits tax was lowered in August 1981 and removed in 1988, ending disincentives to US oil producers. Additionally, Trans-Alaska Pipeline System began pumping oil in 1977. The Alaskan Prudhoe Bay Oil Field entered peak production, supplying 2 million bpd of crude oil in 1988, 25 percent of all U.S. oil production.\n\nPhillips Petroleum discovered oil in the Chalk Group at Ekofisk, in Norwegian waters in the central North Sea. Discoveries increased exponentially in the 1970s and 1980s, and new fields were developed throughout the continental shelf.\n\nFrom 1980 to 1986, OPEC decreased oil production several times and nearly in half, in an attempt to maintain oil's high prices. However, it failed to hold on to its preeminent position, and by 1981, its production was surpassed by non-OPEC countries. OPEC had seen its share of the world market drop to less than a third in 1985, from about half during the 1970s. In February 1982, the \"Boston Globe\" reported that OPEC's production, which had previously peaked in 1977, was at its lowest level since 1969. Non-OPEC nations were at that time supplying most of the West's imports.\n\nOPEC's membership began to have divided opinions over what actions to take. In September 1985, Saudi Arabia became fed up with de facto propping up prices by lowering its own production in the face of high output from elsewhere in OPEC. In 1985, daily output was around 3.5 million bpd, down from around 10 million in 1981. During this period, OPEC members were supposed to meet production quotas in order to maintain price stability; however, many countries inflated their reserves to achieve higher quotas, cheated, or outright refused to accord with the quotas. In 1985, the Saudis tired of this behavior and decided to punish the undisciplined OPEC countries. The Saudis abandoned their role as swing producer and began producing at full capacity, creating a \"huge surplus that angered many of their colleagues in OPEC\". High-cost oil production facilities became less or even not profitable. Oil prices as a result fell to as low as $7 per barrel.\n\nOPEC had relied on the price inelasticity of demand of oil to maintain high consumption, but underestimated the extent to which other sources of supply would become profitable as prices increased. Electricity generation from coal, nuclear power and natural gas; home heating from natural gas; and ethanol blended gasoline all reduced the demand for oil.\n\nNew passenger car fuel economy in the US rose from 17 mpg in 1978 to more than 22 mpg in 1982, an increase of more than 30 percent.\n\nThe US imported 28 percent of its oil in 1982 and 1983, down from 46.5 percent in 1977, due to lower consumption.\n\nThe 1986 oil price collapse benefited oil-consuming countries such as the United States, Japan, Europe, and Third World nations, but represented a serious loss in revenue for oil-producing countries in northern Europe, the Soviet Union, and OPEC.\n\nIn 1981, before the brunt of the glut, \"Time Magazine\" wrote that in general, \"A glut of crude causes tighter development budgets\" in some oil-exporting nations. \nMexico had an economic and debt crisis in 1982. The Venezuelan economy contracted and inflation levels (consumer price inflation) rose, remaining between 6 and 12% from 1982 to 1986. Even Saudi Arabian economic power was significantly weakened.\n\nIraq had fought a long and costly war against Iran, and had particularly weak revenues. It was upset by Kuwait contributing to the glut and allegedly pumping oil from the Rumaila field below their common border. Iraq invaded Kuwait in 1990, planning to increase reserves and revenues and cancel the debt, resulting in the first Gulf War.\n\nThe USSR had become a major oil producer before the glut. The drop of oil prices contributed to the nation's final collapse.\n\nIn the US, domestic exploration and the number of active drilling rigs were cut dramatically. In late 1985, there were nearly 2,300 rigs drilling wells in the USA; a year later, there were barely 1,000. The number of petroleum producers in United States decreased from 11,370 in 1985 to 5,231 in 1989, according to data from the Independent Petroleum Association of America. Oil producers held back on the search for new oilfields for fear of losing on their investments. In May 2007, companies like ExxonMobil were not making nearly the investment in finding new oil that they did in 1981.\n\nCanada responded to high energy prices in the 1970s with the National Energy Program (NEP) in 1980. This program was in place until 1985.\n\n\n\n"}
{"id": "57503654", "url": "https://en.wikipedia.org/wiki?curid=57503654", "title": "Brendan Blumer", "text": "Brendan Blumer\n\nBrendan Blumer (born August 8, 1986) is an American entrepreneur, executive, and investor. He is the CEO of Block.one, the tech company producing the EOS.IO distributed ledger software. He is based in Hong Kong.\n\nBlumer was born and raised in Cedar Rapids, Iowa. When he was 15 years old, he developed a website to sell virtual assets in the multiplayer online gaming space. His website, known as Gamecliff (stylized as \"GaMeCliff\"), displayed different characters, weapons, and houses for MMORPG games including EverQuest and World of Warcraft.\n\nIn 2005, Blumer's Gamecliff was acquired by IGE. The company moved to its new headquarters to Hong Kong to head Gamecliff's operations.\n\nIn 2007, Blumer founded The Accounts Network, a company that sold in-game MMORPG avatars.\n\nIn 2010, Blumer launched Okay.com, an enterprise data sharing platform for real estate brokers in Asia. The company later merged with the real estate company Asia Pacific Properties.\n\nHe founded ii5 in 2013, a startup dedicated to real-estate listings in India.\n\nIn 2016, Brendan Blumer met software programmer Daniel Larimer. In March 2017, Blumer and Larimer together formed Block.One, an open source software publishing company specializing in high performance blockchain technologies.\n\nIn May 2017, Blumer and Larimer announced their first project, EOS.IO. Critics have argued that Block.one's year long EOS initial coin offering (ICO) is unnecessarily aggressive in the amount of funds that it is seeking to raise for such a product.\n\nIn February 2018, he was recognized by \"Forbes\" as one of \"The Richest People In Cryptocurrency\".\n"}
{"id": "13686568", "url": "https://en.wikipedia.org/wiki?curid=13686568", "title": "Bronner Bros.", "text": "Bronner Bros.\n\nThe Bronner Bros. Enterprise is one of the largest private African American hair and skin care producers in the United States. Founded in 1947 by brothers Dr. Nathaniel H. Bronner, Sr. and Arthur E. Bronner, Sr., Bronner Bros. has over 300 full-time and part-time staff members. The company headquarters is located in Marietta, Georgia.\n\nNathaniel and Arthur Bronner, with the help of their sister, Emma Bronner, started Bronner Bros. in 1947 as a way to teach cosmetology at the local YMCA. About 300 people attended the first show but as the attendance grew, it was moved to the Royal Peacock Social Club and then to the Auburn Avenue Casino. In 1967, Bronner Bros. signed a contract with the new Hyatt Regency Hotel and the show was on that gas held there annually for the next 20 years. During this time, the Bronners secured a number of popular guest speakers, including Dr. Martin Luther King, Jr., Jackie Robinson, Dick Gregory and Dr. Benjamin Mays.\n\nWith Nathaniel Bronner, Sr's death in 1993, Bernard Bronner took over the company as president and CEO and the trade show has continued to grow by leaps and bounds. In 1996, all Bronner Bros. delegates traveled to Orlando, Florida for a large conference. The annual trade show is now held in the prestigious Georgia World Congress Center. As of 2015, the Bronner Bros. International Beauty Show was the largest gathering of multicultural beauty professionals in the U.S., pulling in 22,000 attendees and 300 exhibitors.\n\nMore and more Bronner family members are getting involved in the company. There were two brothers in the first generation of the business, six brothers in the second generation, the third generation now has 30 members.\n\nBernard Bronner was also a partner in Rainforest Films, which produced films targeting an urban audience and featuring African-American talent, until the company was formally dissolved in 2014. Rainforest's filmography includes \"Stomp the Yard\", \"The Gospel\", \"\", \"This Christmas\", and \"Takers\".\n\nBronner Bros. products are created primarily for the African-American population. Their main product lines include African Royale, BB, and NuExpressions. Bronner Bros. owns and operates two manufacturing facilities, a shipping facility, two beauty stores, a hair weaving studio and a Public Relations office.\n\nBronner Bros. has not been quiet when it comes to the company's Christian beliefs. One of their most successful organizations is The Word of Faith Family Worship Cathedral in Austell, Georgia, founded by Bishop Dale C. Bronner the son of Nathaniel Bronner (one of the Bronner Bros. founders). The ministry has more than 19,000 members and over five branch ministries which it has birthed.\n\nUnder the motto \"Reaching the lost, teaching the found,\" the center provides a number of ministries, including Christian education, Sunday school classes, singles ministry, marriage ministry, prison ministry and the Bronner Business Institute which provides training for those who want to become entrepreneurs. The ministry currently run a school for infant to preschool children called the Seeds of Excellence Christian Academy in Austell, Georgia, blocks away from the Cathedral. Word of Faith recently built a recreation center called The Family Life Center.\n\nThe Bronner family also founded The Ark of Salvation Worship Center in Atlanta, GA, which Rev. Nathaniel Bronner, Jr., Rev. Charles Bronner and Rev. James Bronner (The Brothers of the Word) lead. This ministry produces television broadcasts throughout the week as well as some leading online ministries.\n\nBronner Bros. was co-founded by Dr. Nathaniel H. Bronner, Sr. and Arthur E. Bronner, Sr. The business was passed to Nathaniel's six sons upon his passing in 1993. Nathaniel's sons in order of age include:\n\n\nIn 2004, Wal-Mart recognized Bronner Bros. as their top vendor of the year in their annual report. The same year, the largest beauty and barber retailing supplier in the United States, Sally Beauty Supply, also named the company their top vendor of the year. \n\n"}
{"id": "52167291", "url": "https://en.wikipedia.org/wiki?curid=52167291", "title": "CRON Systems", "text": "CRON Systems\n\nCRON Systems is a defence technology start up which builds Multi-sensor enabled intrusion detection systems. Founded by Tushar Chhabra and co-founders Saurav Agarwala, Tomer Katzenellenbogen it creates end-to-end encrypted solutions which are capable of distinguishing between intrusion by a human and an animal. Besides ground intrusion, CRON systems also builds solutions to detect drones, and is working on automated patrol vehicles .\n\nIn September 2016, Indian Border Security Force (BSF) installed several CRON laser walls at hostile areas along the border, to keep a vigil on intrusion attempts from across India's western border.\n\n"}
{"id": "56465", "url": "https://en.wikipedia.org/wiki?curid=56465", "title": "Cassava", "text": "Cassava\n\nManihot esculenta, commonly called cassava (), manioc, yuca, macaxeira, mandioca, aipim and Brazilian arrowroot, is a woody shrub native to South America of the spurge family, Euphorbiaceae. It is extensively cultivated as an annual crop in tropical and subtropical regions for its edible starchy tuberous root, a major source of carbohydrates. Though it is often called yuca in Spanish and in the United States, it differs from yucca, an unrelated fruit-bearing shrub in the family Asparagaceae. Cassava, when dried to a powdery (or pearly) extract, is called tapioca; its fried, granular form is named \"garri\".\n\nCassava is the third-largest source of food carbohydrates in the tropics, after rice and maize. Cassava is a major staple food in the developing world, providing a basic diet for over half a billion people. It is one of the most drought-tolerant crops, capable of growing on marginal soils. Nigeria is the world's largest producer of cassava, while Thailand is the largest exporter of dried cassava.\n\nCassava is classified as either sweet or bitter. Like other roots and tubers, both bitter and sweet varieties of cassava contain antinutritional factors and toxins, with the bitter varieties containing much larger amounts. It must be properly prepared before consumption, as improper preparation of cassava can leave enough residual cyanide to cause acute cyanide intoxication, goiters, and even ataxia, partial paralysis, or death. The more toxic varieties of cassava are a fall-back resource (a \"food security crop\") in times of famine or food insecurity in some places. Farmers often prefer the bitter varieties because they deter pests, animals, and thieves.\n\nThe cassava root is long and tapered, with a firm, homogeneous flesh encased in a detachable rind, about 1 mm thick, rough and brown on the outside. Commercial cultivars can be in diameter at the top, and around long. A woody vascular bundle runs along the root's axis. The flesh can be chalk-white or yellowish. Cassava roots are very rich in starch and contain small amounts of calcium (16 mg/100 g), phosphorus (27 mg/100 g), and vitamin C (20.6 mg/100 g). However, they are poor in protein and other nutrients. In contrast, cassava leaves are a good source of protein (rich in lysine), but deficient in the amino acid methionine and possibly tryptophan.\n\nWild populations of \"M. esculenta\" subspecies \"flabellifolia\", shown to be the progenitor of domesticated cassava, are centered in west-central Brazil, where it was likely first domesticated no more than 10,000 years BP. Forms of the modern domesticated species can also be found growing in the wild in the south of Brazil. By 4,600 BC, manioc (cassava) pollen appears in the Gulf of Mexico lowlands, at the San Andrés archaeological site. The oldest direct evidence of cassava cultivation comes from a 1,400-year-old Maya site, Joya de Cerén, in El Salvador. With its high food potential, it had become a staple food of the native populations of northern South America, southern Mesoamerica, and the Caribbean by the time of European contact in 1492. Cassava was a staple food of pre-Columbian peoples in the Americas and is often portrayed in indigenous art. The Moche people often depicted yuca in their ceramics.\n\nSpaniards in their early occupation of Caribbean islands did not want to eat cassava or maize, which they considered insubstantial, dangerous, and not nutritious. They much preferred foods from Spain, specifically wheat bread, olive oil, red wine, and meat, and considered maize and cassava damaging to Europeans. For these Christians in the New World, cassava was not suitable for communion since it could not undergo transubstantiation and become the body of Christ. \"Wheat flour was the symbol of Christianity itself\" and colonial-era catechisms stated explicitly that only wheat flour could be used.\n\nThe cultivation and consumption of cassava was nonetheless continued in both Portuguese and Spanish America. Mass production of cassava bread became the first Cuban industry established by the Spanish, Ships departing to Europe from Cuban ports such as Havana, Santiago, Bayamo, and Baracoa carried goods to Spain, but sailors needed to be provisioned for the voyage. The Spanish also needed to replenish their boats with dried meat, water, fruit, and large amounts of cassava bread. Sailors complained that it caused them digestive problems. Tropical Cuban weather was not suitable for wheat planting and cassava would not go stale as quickly as regular bread.\n\nCassava was introduced to Africa by Portuguese traders from Brazil in the 16th century. Around the same period, it was also introduced to Asia through Columbian Exchange by Portuguese and Spanish traders, planted in their colonies in Goa, Malacca, Eastern Indonesia, Timor and the Philippines. Maize and cassava are now important staple foods, replacing native African crops. Cassava has also become an important staple in Asia, extensively cultivated in Indonesia, Thailand and Vietnam. Cassava is sometimes described as the \"bread of the tropics\" but should not be confused with the tropical and equatorial bread tree \"(Encephalartos)\", the breadfruit \"(Artocarpus altilis)\" or the African breadfruit \"(Treculia africana)\".\n\nIn 2016, global production of cassava root was 277 million tonnes, with Nigeria as the world's largest producer having 21% of the world total (table). Other major growers were Thailand, Brazil, and Indonesia.\n\nCassava is one of the most drought-tolerant crops, can be successfully grown on marginal soils, and gives reasonable yields where many other crops do not grow well. Cassava is well adapted within latitudes 30° north and south of the equator, at elevations between sea level and above sea level, in equatorial temperatures, with rainfalls from to annually, and to poor soils with a pH ranging from acidic to alkaline. These conditions are common in certain parts of Africa and South America.\n\nCassava is a highly-productive crop when considering food calories produced per unit land area, per unit of time. Significantly higher than other staple crops, cassava can produce food calories at rates exceeding 250 kcal/hectare/day, as compared with 176 for rice, 110 for wheat and 200 for maize (corn).\n\nCassava, yams (\"Dioscorea\" spp.), and sweet potatoes (\"Ipomoea batatas\") are important sources of food in the tropics. The cassava plant gives the third-highest yield of carbohydrates per cultivated area among crop plants, after sugarcane and sugar beets. Cassava plays a particularly important role in agriculture in developing countries, especially in sub-Saharan Africa, because it does well on poor soils and with low rainfall, and because it is a perennial that can be harvested as required. Its wide harvesting window allows it to act as a famine reserve and is invaluable in managing labor schedules. It offers flexibility to resource-poor farmers because it serves as either a subsistence or a cash crop.\n\nWorldwide, 800 million people depend on cassava as their primary food staple. No continent depends as much on root and tuber crops in feeding its population as does Africa. In the humid and sub-humid areas of tropical Africa, it is either a primary staple food or a secondary costaple. In Ghana, for example, cassava and yams occupy an important position in the agricultural economy and contribute about 46 percent of the agricultural gross domestic product. Cassava accounts for a daily caloric intake of 30 percent in Ghana and is grown by nearly every farming family. The importance of cassava to many Africans is epitomised in the Ewe (a language spoken in Ghana, Togo and Benin) name for the plant, \"agbeli\", meaning \"there is life\".\n\nIn Tamil Nadu, India, there are many cassava processing factories alongside National Highway 68 between Thalaivasal and Attur. Cassava is widely cultivated and eaten as a staple food in Andhra Pradesh and in Kerala. In Assam it is an important source of carbohydrates especially for natives of hilly areas.\n\nIn the subtropical region of southern China, cassava is the fifth-largest crop in term of production, after rice, sweet potato, sugar cane, and maize. China is also the largest export market for cassava produced in Vietnam and Thailand. Over 60 percent of cassava production in China is concentrated in a single province, Guangxi, averaging over seven million tonnes annually.\n\nAlcoholic beverages made from cassava include cauim and tiquira (Brazil), kasiri (Guyana, Suriname), impala (Mozambique), masato (Peruvian Amazonia chicha), parakari or kari (Guyana), nihamanchi (South America) also known as nijimanche (Ecuador and Peru), ö döi (chicha de yuca, Ngäbe-Bugle, Panama), sakurá (Brazil, Suriname), tarul ko jaarh (Darjeeling, Sikkim, India).\n\nCassava-based dishes are widely consumed wherever the plant is cultivated; some have regional, national, or ethnic importance. Cassava must be cooked properly to detoxify it before it is eaten.\n\nCassava can be cooked in many ways. The root of the sweet variety has a delicate flavor and can replace potatoes. It is used in cholent in some households. It can be made into a flour that is used in breads, cakes and cookies. In Brazil, detoxified manioc is ground and cooked to a dry, often hard or crunchy meal known as \"farofa\" used as a condiment, toasted in butter, or eaten alone as a side dish.\n\nRaw cassava is 60% water, 38% carbohydrates, 1% protein, and has negligible fat (table). In a 100 gram amount, raw cassava provides 160 calories and contains 25% of the Daily Value (DV) for vitamin C, but otherwise has no micronutrients in significant content (no values above 10% DV; table). Cooked cassava starch has a digestibility of over 75%.\n\nCassava, like other foods, also has antinutritional and toxic factors. Of particular concern are the cyanogenic glucosides of cassava (linamarin and lotaustralin). On hydrolysis, these release hydrocyanic acid (HCN). The presence of cyanide in cassava is of concern for human and for animal consumption. The concentration of these antinutritional and unsafe glycosides varies considerably between varieties and also with climatic and cultural conditions. Selection of cassava species to be grown, therefore, is quite important. Once harvested, bitter cassava must be treated and prepared properly prior to human or animal consumption, while sweet cassava can be used after simply boiling.\n\nA comparative table shows that cassava is a good energy source. In its prepared forms in which its toxic or unpleasant components have been reduced to acceptable levels, it contains an extremely high proportion of starch. Compared to most staples however, cassava accordingly is a poorer dietary source of protein and most other essential nutrients. Though an important staple, its main value is as a component of a balanced diet.\n\nComparisons between the nutrient content of cassava and other major staple foods when raw, as shown in the table, must be interpreted with caution because most staples are not edible in such forms and many are indigestible, even dangerously poisonous or otherwise harmful. For consumption, each must be prepared and cooked as appropriate. Suitably cooked or otherwise prepared, the nutritional and antinutritional contents of each of these staples is widely different from that of raw form and depends on the methods of preparation such as soaking, fermentation, sprouting, boiling, or baking.\n\nIn many countries, significant research has begun to evaluate the use of cassava as an ethanol biofuel feedstock. Under the Development Plan for Renewable Energy in the Eleventh Five-Year Plan in the People's Republic of China, the target is to increase the production of ethanol fuel from nongrain feedstock to two million tonnes, and that of biodiesel to 200 thousand tonnes by 2010. This is equivalent to the replacement of 10 million tonnes of petroleum. As a result, cassava (tapioca) chips have gradually become a major source of ethanol production. On 22 December 2007, the largest cassava ethanol fuel production facility was completed in Beihai, with annual output of 200 thousand tons, which would need an average of 1.5 million tons of cassava. In November 2008, China-based Hainan Yedao Group invested US$51.5 million in a new biofuel facility that is expected to produce a year of bioethanol from cassava plants.\n\nCassava tubers and hay are used worldwide as animal feed. Cassava hay is harvested at a young growth stage (three to four months) when it reaches about above ground; it is then sun-dried for one to two days until its final dry matter content approaches 85 percent. Cassava hay contains high protein (20–27 percent crude protein) and condensed tannins (1.5–4 percent CP). It is valued as a good roughage source for ruminants such as cattle.\n\nManioc is also used in a number of commercially available laundry products, especially as starch for shirts and other garments. Using manioc starch diluted in water and spraying it over fabrics before ironing helps stiffen collars.\n\nAccording to the American Cancer Society, cassava is ineffective as an anti-cancer agent: \"there is no convincing scientific evidence that cassava or tapioca is effective in preventing or treating cancer\".\n\nCassava roots, peels and leaves should not be consumed raw because they contain two cyanogenic glucosides, linamarin and lotaustralin. These are decomposed by linamarase, a naturally occurring enzyme in cassava, liberating hydrogen cyanide (HCN). Cassava varieties are often categorized as either sweet or bitter, signifying the absence or presence of toxic levels of cyanogenic glucosides, respectively. The so-called sweet (actually not bitter) cultivars can produce as little as 20 milligrams of cyanide (CN) per kilogram of fresh roots, whereas bitter ones may produce more than 50 times as much (1 g/kg). Cassavas grown during drought are especially high in these toxins. A dose of 25 mg of pure cassava cyanogenic glucoside, which contains 2.5 mg of cyanide, is sufficient to kill a rat. Excess cyanide residue from improper preparation is known to cause acute cyanide intoxication, and goiters, and has been linked to ataxia (a neurological disorder affecting the ability to walk, also known as \"konzo\"). It has also been linked to tropical calcific pancreatitis in humans, leading to chronic pancreatitis.\n\nSymptoms of acute cyanide intoxication appear four or more hours after ingesting raw or poorly processed cassava: vertigo, vomiting, and collapse. In some cases, death may result within one or two hours. It can be treated easily with an injection of thiosulfate (which makes sulfur available for the patient's body to detoxify by converting the poisonous cyanide into thiocyanate).\n\n\"Chronic, low-level cyanide exposure is associated with the development of goiter and with tropical ataxic neuropathy, a nerve-damaging disorder that renders a person unsteady and uncoordinated. Severe cyanide poisoning, particularly during famines, is associated with outbreaks of a debilitating, irreversible paralytic disorder called konzo and, in some cases, death. The incidence of konzo and tropical ataxic neuropathy can be as high as three percent in some areas.\"\n\nDuring the shortages in Venezuela in the late-2010s, dozens of deaths were reported due to Venezuelans resorting to eating bitter cassava in order to curb starvation.\n\nSocieties that traditionally eat cassava generally understand that some processing (soaking, cooking, fermentation, etc.) is necessary to avoid getting sick. Brief soaking (four hours) of cassava is not sufficient, but soaking for 18–24 hours can remove up to half the level of cyanide. Drying may not be sufficient, either.\n\nFor some smaller-rooted, sweet varieties, cooking is sufficient to eliminate all toxicity. The cyanide is carried away in the processing water and the amounts produced in domestic consumption are too small to have environmental impact. The larger-rooted, bitter varieties used for production of flour or starch must be processed to remove the cyanogenic glucosides. The large roots are peeled and then ground into flour, which is then soaked in water, squeezed dry several times, and toasted. The starch grains that float to the surface during the soaking process are also used in cooking. The flour is used throughout South America and the Caribbean. Industrial production of cassava flour, even at the cottage level, may generate enough cyanide and cyanogenic glycosides in the effluents to have a severe environmental impact.\n\nA safe processing method known as the \"wetting method\" is to mix the cassava flour with water into a thick paste and then let it stand in the shade for five hours in a thin layer spread over a basket. In that time, about 83% of the cyanogenic glycosides are broken down by the linamarase; the resulting hydrogen cyanide escapes to the atmosphere, making the flour safe for consumption the same evening.\n\nThe traditional method used in West Africa is to peel the roots and put them into water for three days to ferment. The roots then are dried or cooked. In Nigeria and several other west African countries, including Ghana, Cameroon, Benin, Togo, Ivory Coast, and Burkina Faso, they are usually grated and lightly fried in palm oil to preserve them. The result is a foodstuff called \"gari\". Fermentation is also used in other places such as Indonesia (see Tapai). The fermentation process also reduces the level of antinutrients, making the cassava a more nutritious food. The reliance on cassava as a food source and the resulting exposure to the goitrogenic effects of thiocyanate has been responsible for the endemic goiters seen in the Akoko area of southwestern Nigeria.\n\nA project called \"BioCassava Plus\" uses bioengineering to grow cassava with lower cyanogenic glycosides combined with fortification of vitamin A, iron and protein to improve the nutrition of people in sub-Saharan Africa.\n\nCassava is harvested by hand by raising the lower part of the stem and pulling the roots out of the ground, then removing them from the base of the plant. The upper parts of the stems with the leaves are plucked off before harvest. Cassava is propagated by cutting the stem into sections of approximately 15 cm, these being planted prior to the wet season.\nCassava undergoes post-harvest physiological deterioration (PPD) once the tubers are separated from the main plant. The tubers, when damaged, normally respond with a healing mechanism. However, the same mechanism, which involves coumaric acids, starts about 15 minutes after damage, and fails to switch off in harvested tubers. It continues until the entire tuber is oxidized and blackened within two to three days after harvest, rendering it unpalatable and useless. PPD is related to the accumulation of reactive oxygen species (ROS) initiated by cyanide release during mechanical harvesting. Cassava shelf life may be increased up to three weeks by overexpressing a cyanide insensitive alternative oxidase, which suppressed ROS by 10-fold. PPD is one of the main obstacles preventing farmers from exporting cassavas abroad and generating income. Fresh cassava can be preserved like potato, using thiabendazole or bleach as a fungicide, then wrapping in plastic, coating in wax or freezing.\n\nWhile alternative methods for PPD control have been proposed, such as preventing ROS effects by use of plastic bags during storage and transport or coating the roots with wax, and freezing roots, such strategies have proved to be economically or technically impractical, leading to breeding of cassava varieties more tolerant to PPD and with improved durability after harvest. Plant breeding has resulted in different strategies for cassava tolerance to PPD. One was induced by mutagenic levels of gamma rays, which putatively silenced one of the genes involved in PPD genesis, while another was a group of high-carotene clones in which the antioxidant properties of carotenoids are postulated to protect the roots from PPD.\n\nA major cause of losses during cassava storage is infestation by insects. A wide range of species that feed directly on dried cassava chips have been reported as a major factor in spoiling stored cassava, with losses between 19% and 30% of the harvested produce. In Africa, a previous issue was the cassava mealybug (\"Phenacoccus manihoti\") and cassava green mite (\"Mononychellus tanajoa\"). These pests can cause up to 80 percent crop loss, which is extremely detrimental to the production of subsistence farmers. These pests were rampant in the 1970s and 1980s but were brought under control following the establishment of the \"Biological Control Centre for Africa\" of the International Institute of Tropical Agriculture (IITA) under the leadership of Hans Rudolf Herren. The Centre investigated biological control for cassava pests; two South American natural enemies \"Apoanagyrus lopezi\" (a parasitoid wasp) and \"Typhlodromalus aripo\" (a predatory mite) were found to effectively control the cassava mealybug and the cassava green mite, respectively.\n\nThe African cassava mosaic virus causes the leaves of the cassava plant to wither, limiting the growth of the root. An outbreak of the virus in Africa in the 1920s led to a major famine. The virus is spread by the whitefly and by the transplanting of diseased plants into new fields. Sometime in the late-1980s, a mutation occurred in Uganda that made the virus even more harmful, causing the complete loss of leaves. This mutated virus spread at a rate of per year, and as of 2005 was found throughout Uganda, Rwanda, Burundi, the Democratic Republic of the Congo and the Republic of the Congo.\n\nCassava brown streak virus disease has been identified as a major threat to cultivation worldwide.\n\nA wide range of plant parasitic nematodes have been reported associated with cassava worldwide. These include \"Pratylenchus brachyurus\", \"Rotylenchulus reniformis\", \"Helicotylenchus\" spp., \"Scutellonema\" spp. and \"Meloidogyne\" spp., of which \"Meloidogyne incognita\" and \"Meloidogyne javanica\" are the most widely reported and economically important. \"Meloidogyne\" spp. feeding produces physically damaging galls with eggs inside them. Galls later merge as the females grow and enlarge, and they interfere with water and nutrient supply. Cassava roots become tough with age and restrict the movement of the juveniles and the egg release. It is therefore possible that extensive galling can be observed even at low densities following infection. Other pest and diseases can gain entry through the physical damage caused by gall formation, leading to rots. They have not been shown to cause direct damage to the enlarged storage roots, but plants can have reduced height if there was loss of enlarged root weight.\n\nResearch on nematode pests of cassava is still in the early stages; results on the response of cassava is, therefore, not consistent, ranging from negligible to seriously damaging. Since nematodes have such a seemingly erratic distribution in cassava agricultural fields, it is not easy to clearly define the level of direct damage attributed to nematodes and thereafter quantify the success of a chosen management method.\n\nThe use of nematicides has been found to result in lower numbers of galls per feeder root compared to a control, coupled with a lower number of rots in the storage roots. The organophosphorus nematicide femaniphos, when used, did not affect crop growth and yield parameter variables measured at harvest. Nematicide use in cassava is neither practical nor sustainable; the use of tolerant and resistant cultivars is the most practical and sustainable management method.\n\n"}
{"id": "8219728", "url": "https://en.wikipedia.org/wiki?curid=8219728", "title": "Cintel", "text": "Cintel\n\nCintel was a British digital cinema company founded in 1927 by John Logie Baird and based in Ware, Hertfordshire. The early company was called \"Cinema Television Ltd\". Cinema Television was sold to J Arthur Rank Organization renamed Rank Cintel in 1958. It specialized in the design and manufacture of professional post-production equipment, for transcribing film into video or data formats. It was formerly part of the Rank Organisation. Along with a line of telecines, Rank Cintel made 3 tube RGB color video projectors in the 1960s.\n\nTheir main products were based on either cathode ray tube (CRT) or charge coupled device (CCD) technology and include, like the diTTo, diTTo Evolution & dataMill film scanners, Millennium II, Millennium HD & C-Reality & DSX telecines, imageMill 1 & 2 image processing system. The CRT tubes were made by Rank and Brimar. In September 2002 Cintel purchased ITK - Innovation TK Ltd. ITK held a number of patents for features used in Cintel products and also made the competitive unit the Millennium telecine. ITK founded in 1994, also made upgrade products include the TWiGi system, the SCAN’dAL, and the Y-Front.\n\nMany movies and TV shows for TV were transferred from film to TV on Cintel Telecines. Cintel saw reduced sales with the introduction of Spirit DataCine in 1996. The business was in administration until its announced liquidation. On July 24, 2012 Blackmagic Design acquired the assets of Cintel.\n\n\nInnovation TK Ltd, ITK, was founded in 1994, Innovation TK, the key engineer and manager was Stuart Hunt, who was design engineer for Cintel. Innovation TK designed and patented product to improve Cintel's flyingspot telecines, like the TWiGi system, the SCAN'dAL, and the Y-Front. Later TWiGi and SCAN'dAL are became standard features used on Cintel's URSA Diamond. Later ITK designed and built their own telecine the: Millennium Machine and Millennium Lite. In 2002 Cintel acquired Innovation TK, including the Millennium HD and Millennium Data Telecine.\n\n\n\n"}
{"id": "6968", "url": "https://en.wikipedia.org/wiki?curid=6968", "title": "Customer-relationship management", "text": "Customer-relationship management\n\nCustomer-relationship management (CRM) is an approach to manage a company's interaction with current and potential customers. It uses data analysis about customers' history with a company to improve business relationships with customers, specifically focusing on customer retention and ultimately driving sales growth.\n\nOne important aspect of the CRM approach is the systems of CRM that compile data from a range of different communication channels, including a company's website, telephone, email, live chat, marketing materials and more recently, social media. Through the CRM approach and the systems used to facilitate it, businesses learn more about their target audiences and how to best cater to their needs.\n\nThe concept of customer-relationship management started in the early 1970s, when customer satisfaction was evaluated using annual surveys or by front-line asking. At that time, businesses had to rely on standalone mainframe systems to automate sales, but the extent of technology allowed them to categorize customers in spreadsheets and lists. In 1982, Kate and Robert D. Kestnbaum introduced the concept of Database marketing, namely applying statistical methods to analyze and gather customer data. By 1986, Pat Sullivan and Mike Muhney released a customer evaluation system called ACT! based on the principle of digital rolodex, which offered a contact management service for the first time.\n\nThe trend was followed by numerous developers trying to maximize leads' potential, including Tom Siebel, who designed the first CRM product Siebel Systems in 1993. Nevertheless, customer relationship management popularized in 1997, due to the work of Siebel, Gartner, and IBM. Between 1997 and 2000, leading CRM products were enriched with enterprise resource planning functions, and shipping and marketing capabilities. Siebel introduced the first mobile CRM app called Siebel Sales Handheld in 1999. The idea of a cloud-hosted and moveable customer bases was soon adopted by other leading providers at the time, including PeopleSoft, Oracle, and SAP.\n\nThe first open-source CRM system was developed by SugarCRM in 2004. During this period, CRM was rapidly migrating to cloud, as a result of which it became accessible to sole entrepreneurs and small teams. This increase in accessibility generated a huge wave of price reduction. Around 2009, developers began considering the options to profit from social media's momentum, and designed tools to help companies become accessible on all users' favorite networks. Many startups at the time benefited from this trend to provide exclusively social CRM solutions, including Base and Nutshell. The same year, Gartner organized and held the first Customer Relationship Management Summit, and summarized the features systems should offer to be classified as CRM solutions. In 2013 and 2014, most of the popular CRM products were linked to business intelligence systems and communication software to improve corporate communication and end-users' experience. The leading trend is to replace standardized CRM solutions with industry-specific ones, or to make them customizable enough to meet the needs of every business.\n\nIn November 2016, \"Forrester\" released a report where it \"identified the nine most significant CRM suites from eight prominent vendors,\" among them companies such as Infor, Microsoft, and NetSuite.\n\nStrategic CRM is focused upon the development of a customer-centric business culture.\n\nThe primary goal of customer relationship management systems is to integrate and automate sales, marketing, and customer support. Therefore, these systems typically have a dashboard that gives an overall view of the three functions on a single customer view, a single page for each customer that a company may have. The dashboard may provide client information, past sales, previous marketing efforts, and more, summarizing all of the relationships between the customer and the firm. Operational CRM is made up of 3 main components: sales force automation, marketing automation, and service automation.\n\nThe role of analytical CRM systems is to analyze customer data collected through multiple sources and present it so that business managers can make more informed decisions. Analytical CRM systems use techniques such as data mining, correlation, and pattern recognition to analyze the customer data. These analytics help improve customer service by finding small problems which can be solved, perhaps, by marketing to different parts of a consumer audience differently. For example, through the analysis of a customer base's buying behavior, a company might see that this customer base has not been buying a lot of products recently. After scanning through this data, the company might think to market to this subset of consumers differently, in order to best communicate how this company's products might benefit this group specifically.\n\nThe third primary aim of CRM systems is to incorporate external stakeholders such as suppliers, vendors, and distributors, and share customer information across groups/departments and organisations. For example, feedback can be collected from technical support calls, which could help provide direction for marketing products and services to that particular customer in the future.\n\nA customer data platform (CDP) is a computer system used by marketing departments that assembles data about individual people from various sources into one database, with which other software systems can interact. As of February 2017 there were about twenty companies selling such systems and revenue for them was around US$300 million.\n\nThe main components of CRM are building and managing customer relationships through marketing, observing relationships as they mature through distinct phases, managing these relationships at each stage and recognizing that the distribution of value of a relationship to the firm is not homogenous. When building and managing customer relationships through marketing, firms might benefit from using a variety of tools to help organizational design, incentive schemes, customer structures, and more to optimize the reach of its marketing campaigns. Through the acknowledgement of the distinct phases of CRM, businesses will be able to benefit from seeing the interaction of multiple relationships as connected transactions. The final factor of CRM highlights the importance of CRM through accounting for the profitability of customer relationships. Through studying the particular spending habits of customers, a firm may be able to dedicate different resources and amounts of attention to different types of consumers.\n\nRelational Intelligence, or awareness of the variety of relationships a customer can have with a firm, is an important component to the main phases of CRM. Companies may be good at capturing demographic data, such as gender, age, income, and education, and connecting them with purchasing information to categorize customers into profitability tiers, but this is only a firm's mechanical view of customer relationships. This therefore is a sign that firms believe that customers are still resources that can be used for up-sell or cross-sell opportunities, rather than humans looking for interesting and personalized interactions.\n\nCRM systems include:\n\n\nCustomer satisfaction has important implications for the economic performance of firms because it has the ability to increase customer loyalty and usage behavior and reduce customer complaints and the likelihood of customer defection. The implementation of a CRM approach is likely to have an effect on customer satisfaction and customer knowledge for a variety of different reasons.\n\nFirstly, firms are able to customize their offerings for each customer. By accumulating information across customer interactions and processing this information to discover hidden patterns, CRM applications help firms customize their offerings to suit the individual tastes of their customers. This customization enhances the perceived quality of products and services from a customer's viewpoint, and because perceived quality is a determinant of customer satisfaction, it follows that CRM applications indirectly affect customer satisfaction. CRM applications also enable firms to provide timely, accurate processing of customer orders and requests and the ongoing management of customer accounts. For example, Piccoli and Applegate discuss how Wyndham uses IT tools to deliver a consistent service experience across its various properties to a customer. Both an improved ability to customize and a reduced variability of the consumption experience enhance perceived quality, which in turn positively affects customer satisfaction. Furthermore, CRM applications also help firms manage customer relationships more effectively across the stages of relationship initiation, maintenance, and termination.\n\nWith Customer relationship management systems customers are served better on day to day process and with more reliable information their demand of self service from companies will decrease. If there is less need to interact with the company for different problems, customer satisfaction level increases. These central benefits of CRM will be connected hypothetically to the three kinds of equity that are relationship, value and brand, and in the end to customer equity. Eight benefits were recognized to provide value drivers.\nIn 2012, after reviewing the previous studies, someone selected some of those benefits which are more significant in customer's satisfaction and summarized them into the following cases:\n\n\nResearch has found a 5% increase in customer retention boosts lifetime customer profits by 50% on average across multiple industries, as well as a boost of up to 90% within specific industries such as insurance. Companies that have mastered customer relationship strategies have the most successful CRM programs. For example, MBNA Europe has had a 75% annual profit growth since 1995. The firm heavily invests in screening potential cardholders. Once proper clients are identified, the firm retains 97% of its profitable customers. They implement CRM by marketing the right products to the right customers. The firm's customers' card usage is 52% above industry norm, and the average expenditure is 30% more per transaction. Also 10% of their account holders ask for more information on cross-sale products.\n\nAmazon has also seen great success through its customer proposition. The firm implemented personal greetings, collaborative filtering, and more for the customer. They also used CRM training for the employees to see up to 80% of customers repeat.\n\nCustomer or consumer profiles are the essence of the data that is collected alongside core data (name, address, company) and processed through customer analytics methods, essentially a type of profiling. A customer is abstracted to information that sums up consumption habits so far and projects them into the future so that they can be grouped for marketing and advertising purposes.\n\nConsultants, such as Bain & Company, argue that it is important for companies establishing strong CRM systems to improve their relational intelligence. According to this argument, a company must recognize that people have many different types of relationships with different brands. One research study analyzed relationships between consumers in China, Germany, Spain, and the United States, with over 200 brands in 11 industries including airlines, cars and media. This information is valuable as it provides demographic, behavioral, and value-based customer segmentation. These types of relationships can be both positive and negative. Some customers view themselves as friends of the brands, while others as enemies, and some are mixed with a love-hate relationship with the brand. Some relationships are distant, intimate or anything in between.\n\nManagers must understand the different reasons for the types of relationships, and provide the customer with what they are looking for. Companies can collect this information by using surveys, interviews, and more, with current customers. For example, Frito-Lay conducted many ethnographic interviews with customers to try and understand the relationships they wanted with the companies and the brands. They found that most customers were adults who used the product to feel more playful. They may have enjoyed the company's bright orange color, messiness and shape.\n\nCompanies must also improve their relational intelligence of their CRM systems. These days, companies store and receive huge amounts of data through emails, online chat sessions, phone calls, and more. Many companies do not properly make use of this great amount of data, however. All of these are signs of what types of relationships the customer wants with the firm, and therefore companies may consider investing more time and effort in building out their relational intelligence. Companies can use data mining technologies and web searches to understand relational signals. Social media such as Facebook, Twitter, blogs, etc. is also a very important factor in picking up and analyzing information. Understanding the customer and capturing this data allows companies to convert customer's signals into information and knowledge that the firm can use to understand a potential customer's desired relations with a brand.\n\nIt is also very important to analyze all of this information to determine which relationships prove the most valuable. This helps convert data into profits for the firm. Stronger bonds contribute to building market share. By managing different portfolios for different segments of the customer base, the firm can achieve strategic goals.\n\nMany firms have also implemented training programs to teach employees how to recognize and effectively create strong customer-brand relationships. For example, Harley Davidson sent its employees on the road with customers, who were motorcycle enthusiasts, to help solidify relationships. Other employees have also been trained in social psychology and the social sciences to help bolster strong customer relationships. Customer service representatives must be educated to value customer relationships, and trained to understand existing customer profiles. Even the finance and legal departments should understand how to manage and build relationships with customers.\n\nApplying new technologies while using CRM systems requires changes in infrastructure of the organization as well as deployment of new technologies such as business rules, databases and information technology.\n\nContact center CRM providers are popular for small and mid-market businesses. These systems codify the interactions between company and customers by using analytics and key performance indicators to give the users information on where to focus their marketing and customer service. This allows agents to have access to a caller's history to provide personalized customer communication. The intention is to maximize average revenue per user, decrease churn rate and decrease idle and unproductive contact with the customers.\n\nGrowing in popularity is the idea of gamifying, or using game design elements and game principles in a non-game environment such as customer service environments. The gamification of customer service environments includes providing elements found in games like rewards and bonus points to customer service representatives as a method of feedback for a job well done.\nGamification tools can motivate agents by tapping into their desire for rewards, recognition, achievements, and competition.\n\nContact-center automation, the practice of having an integrated system that coordinates contacts between an organization and the public, is designed to reduce the repetitive and tedious parts of a contact center agent's job. Automation prevents this by having pre-recorded audio messages that help customers solve their problems. For example, an automated contact center may be able to re-route a customer through a series of commands asking him or her to select a certain number in order to speak with a particular contact center agent who specializes in the field in which the customer has a question. Software tools can also integrate with the agent's desktop tools to handle customer questions and requests. This also saves time on behalf of the employees.\n\nSocial CRM involves the use of social media and technology to engage and learn from consumers. Because the public, especially young people, are increasingly using social networking sites, companies use these sites to draw attention to their products, services and brands, with the aim of building up customer relationships to increase demand.\n\nSome CRM systems integrate social media sites like Twitter, LinkedIn and Facebook to track and communicate with customers. These customers also share their own opinions and experiences with a company's products and services, giving these firms more insight. Therefore, these firms can both share their own opinions and also track the opinions of their customers.\n\nEnterprise feedback management software platforms, such as Confirmit, Medallia, and Satmetrix, combine internal survey data with trends identified through social media to allow businesses to make more accurate decisions on which products to supply.\n\nCRM systems can also include technologies that create geographic marketing campaigns. The systems take in information based on a customer's physical location and sometimes integrates it with popular location-based GPS applications. It can be used for networking or contact management as well to help increase sales based on location.\n\nDespite the general notion that CRM systems were created for the customer-centric businesses, they can also be applied to B2B environments to streamline and improve customer management conditions. For the best level of CRM operation in a B2B environment, the software must be personalized and delivered at individual levels.\n\nThe main differences between business-to-consumer (B2C) and business-to-business CRM systems concern aspects like sizing of contact databases and length of relationships. Business-to-business companies tend to have smaller contact databases than business-to-consumer, the volume of sales in business-to-business is relatively small. There are fewer figure propositions in business-to-business, but in some cases, they cost a lot more than business-to-consumer items and relationships in business-to-business environment are built over a longer period of time. Furthermore, business-to-business CRM must be easily integrated with products from other companies. Such integration enables the creation of forecasts about customer behavior based on their buying history, bills, business success, etc. An application for a business-to-business company must have a function to connect all the contacts, processes and deals among the customers segment and then prepare a paper. Automation of sales process is an important requirement for business-to-business products. It should effectively manage the deal and progress it through all the phases towards signing. Finally, a crucial point is personalization. It helps the business-to-business company to create and maintain strong and long-lasting relationship with the customer.\n\nThe overall CRM market grew by 12.3 percent in 2015. The following table lists the top vendors in 2012-2015 (figures in millions of US dollars) published in Gartner studies.\n\nThe four largest vendors with CRM system offerings are Salesforce, SAP, Oracle, and Microsoft, which represented 42 percent of the market in 2015. Other providers also are popular for small and mid market businesses. Splitting CRM providers into nine different categories (Enterprise CRM Suite, Midmarket CRM Suite, Small-Business CRM Suite, sales force automation, incentive management, marketing solutions, business intelligence, data quality, consultancies), each category has a different market leader. Additionally, applications often focus on professional fields such as healthcare, manufacturing, and other areas with branch-specific requirements.\n\nIn the Gartner CRM Summit 2010 challenges like \"system tries to capture data from social networking traffic like Twitter, handles Facebook page addresses or other online social networking sites\" were discussed and solutions were provided that would help in bringing more clientele. Many CRM vendors offer subscription-based web tools (cloud computing) and SaaS. Some CRM systems are equipped with mobile capabilities, making information accessible to remote sales staff. Salesforce.com was the first company to provide enterprise applications through a web browser, and has maintained its leadership position.\n\nTraditional providers have recently moved into the cloud-based market via acquisitions of smaller providers: Oracle purchased RightNow in October 2011 and SAP acquired SuccessFactors in December 2011.\n\nThe era of the \"social customer\" refers to the use of social media (Twitter, Facebook, LinkedIn, Google Plus, Pinterest, Instagram, Yelp, customer reviews in Amazon, etc.) by customers. CRM philosophy and strategy has shifted to encompass social networks and user communities.\n\nSales forces also play an important role in CRM, as maximizing sales effectiveness and increasing sales productivity is a driving force behind the adoption of CRM. Empowering sales managers was listed as one of the top 5 CRM trends in 2013.\n\nAnother related development is vendor relationship management (VRM), which provide tools and services that allow customers to manage their individual relationship with vendors. VRM development has grown out of efforts by ProjectVRM at Harvard's Berkman Center for Internet & Society and Identity Commons' Internet Identity Workshops, as well as by a growing number of startups and established companies. VRM was the subject of a cover story in the May 2010 issue of \"CRM\" Magazine.\n\nPharmaceutical companies were some of the first investors in sales force automation (SFA) and some are on their third- or fourth-generation implementations. However, until recently, the deployments did not extend beyond SFA—limiting their scope and interest to Gartner analysts.\n\nAnother trend worth noting is the rise of Customer Success as a discipline within companies. More and more companies establish Customer Success teams as separate from the traditional Sales team and task them with managing existing customer relations. This trend fuels demand for additional capabilities for more holistic understanding of the customer health, which is a limitation for many existing vendors in the space. As a result, a growing number of new entrants enter the market, while existing vendors add capabilities in this area to their suites. In 2017, artificial intelligence and predictive analytics were identified as the newest trends in CRM.\n\nCompanies face large challenges when trying to implement CRM systems. Consumer companies frequently manage their customer relationships haphazardly and unprofitably. They may not effectively or adequately use their connections with their customers, due to misunderstandings or misinterpretations of a CRM system's analysis. Clients who want to be treated more like a friend may be treated like just a party for exchange, rather than a unique individual, due to, occasionally, a lack of a bridge between the CRM data and the CRM analysis output. Many studies show that customers are frequently frustrated by a company's inability to meet their relationship expectations, and on the other side, companies do not always know how to translate the data they have gained from CRM software into a feasible action plan. In 2003, a Gartner report estimated that more than $2 billion had been spent on software that was not being used. According to CSO Insights, less than 40 percent of 1,275 participating companies had end-user adoption rates above 90 percent. Many corporations only use CRM systems on a partial or fragmented basis. In a 2007 survey from the UK, four-fifths of senior executives reported that their biggest challenge is getting their staff to use the systems they had installed. 43 percent of respondents said they use less than half the functionality of their existing systems. However, market research regarding consumers' preferences may increase the adoption of CRM among the developing countries' consumers.\n\nCollection of customer data such as personally identifiable information must strictly obey customer privacy laws, which often requires extra expenditures on legal support.\n\nPart of the paradox with CRM stems from the challenge of determining exactly what CRM is and what it can do for a company. The CRM paradox, also referred to as the \"dark side of CRM\", may entail favoritism and differential treatment of some customers.\n\nCRM technologies can easily become ineffective if there is no proper management, and they are not implemented correctly. The data sets must also be connected, distributed, and organized properly, so that the users can access the information that they need quickly and easily. Research studies also show that customers are increasingly becoming dissatisfied with contact center experiences due to lags and wait times. They also request and demand multiple channels of communications with a company, and these channels must transfer information seamlessly. Therefore, it is increasingly important for companies to deliver a cross-channel customer experience that can be both consistent as well as reliable.\n"}
{"id": "12060085", "url": "https://en.wikipedia.org/wiki?curid=12060085", "title": "Digital signal controller", "text": "Digital signal controller\n\nA digital signal controller (DSC) is a hybrid of microcontrollers and digital signal processors (DSPs). Like microcontrollers, DSCs have fast interrupt responses, offer control-oriented peripherals like PWMs and watchdog timers, and are usually programmed using the C programming language, although they can be programmed using the device's native assembly language. On the DSP side, they incorporate features found on most DSPs such as single-cycle multiply–accumulate (MAC) units, barrel shifters, and large accumulators. Not all vendors have adopted the term DSC. The term was first introduced by Microchip Technology in 2002 with the launch of their 6000 series DSCs and subsequently adopted by most, but not all DSC vendors. For example, Infineon and Renesas refer to their DSCs as microcontrollers.)\n\nDSCs are used in a wide range of applications, but the majority go into motor control, power conversion, and sensor processing applications. Currently DSCs are being marketed as green technologies for their potential to reduce power consumption in electric motors and power supplies.\n\nIn order of market share, the top three DSC vendors are Texas Instruments, Freescale, and Microchip Technology, according to market research firm Forward Concepts (2007). These three companies dominate the DSC market, with other vendors such as Infineon and Renesas taking a smaller slice of the pie.\n\nNOTE: Data is from 2012 (Microchip and TI) and table currently only includes offering from the top 3 DSC vendors.\"\n\nDSCs, like microcontrollers and DSPs, require software support. There are a growing number of software packages that offer the features required by both DSP applications and microcontroller applications. With a broader set of requirements, software solutions are more rare. They require: development tools, DSP libraries, optimization for DSP processing, fast interrupt handling, multi-threading, and a tiny footprint.\n"}
{"id": "703171", "url": "https://en.wikipedia.org/wiki?curid=703171", "title": "Drawing pin", "text": "Drawing pin\n\nA drawing pin (British English) or thumb tack (North American English) is a short nail or pin used to fasten items to a wall or board for display and intended to be inserted by hand, usually using the thumb. A variety of names are used to refer to different designs intended for various purposes. A push pin has a head that allows for easier removal. Map pin or map tack refers to push pins used to mark locations on a map or to hold the map in place. Thumb tack and push pin are both sometimes compounded (thumbtack or pushpin) or hyphenated (thumb-tack or push-pin). \n\nThumb tacks made of brass, tin or iron may be referred to as brass tacks, brass pins, tin tacks or iron tacks, respectively. These terms are particularly used in the idiomatic expression \"to come\" (or \"get\") \"down to brass\" (or otherwise) \"tacks\", meaning to consider basic facts of a situation.\n\nThe drawing pin was invented in name and as a mass-produced item in what is now the United States in the mid/late 1750s. It was first mentioned in the Oxford English Dictionary in 1759. It was said that the use of the newly invented drawing pin to attach notices to school house doors was making significant contribution to the whittling away of their gothic doors. Modern drawing pins were also found as standard in architects’ drawing boxes in the late 18th century.\n\nEdwin Moore patented the \"push-pin\" in the US in 1900 and founded the Moore Push-Pin Company. Moore described them as a pin with a handle. Later, in 1903, in Lychen, German clockmaker Johann Kirsten invented flat-headed pins for use with drawings.\n\nA drawing pin has two basic components: the head, often made of plastic, metal or wood, and the body, usually made of steel or brass. The head is wide to distribute the force of pushing the pin in, allowing only the hands to be used. Many head designs exist: flat, domed, spherical, cylindrical and a variety of novelty heads such as hearts or stars. Drawing pin heads also come in a variety of colours. These can be particularly useful to mark different locations on a map. Some drawing pin designs have the body cut out of the head and bent downward to produce a pin.\n\nDomed or gripped heads are sometimes preferred over flat heads as dropped flat-headed pins will point upward, posing a hazard. Drawing pins also pose a hazard of ingestion and choking, where they may do serious harm.\n"}
{"id": "38317591", "url": "https://en.wikipedia.org/wiki?curid=38317591", "title": "Energy balance (energy economics)", "text": "Energy balance (energy economics)\n\nEnergy balance, in terms of energy economics, is concerned with all processes within an organization that have a reference to energy. It derives from the ecobalance and has the ambition to analyze and verify the emergence, transformation and use of energy resources in an organization in detail. Energy balances serve as a major statistical data base for energy policy and energy management decisions. They contain important information such as the amount and composition of energy consumption, its changes or the transformation of energy.\n\nThe basic idea of a balance is that nothing can get lost or annihilated - this fits to the first law of thermodynamics, which assigns energy this property. But energy splits up during usage and its output does not have the same potential for the physical performance as before.\n\nFor this reason it is important to distinguish between input and output of energy usage. The input side can easily be measured with the help of the meter readings. But on the output side there may be effects that are difficulty predictable, such as heat, dust or noise. In this context it is very interesting, how much of the energy used has actually reached the intended use. Based on this calculation, improvement measures can be derived. A separation in energy sources and places of consumption is necessary. An Outline based on the cost centre of the organization is also possible.\n\n"}
{"id": "56906420", "url": "https://en.wikipedia.org/wiki?curid=56906420", "title": "Engineers Registration Board", "text": "Engineers Registration Board\n\nThe Engineers Registration Board (ERB), is a statutory authority established in 1969, under the \"Engineers Registration Act (ERA) Cap 271\", whose mission is to regulate and supervise the profession of engineering in Uganda.\n\n, the headquarters and offices of the ERB are temporarily housed at the offices of the Uganda Ministry of Works and Transport at Kyambogo, while efforts to secure a permanent location are ongoing, with the help of the Uganda Investment Authority. The geographical coordinate of the ERB headquarters are: 00°20'24.0\"N, 32°37'37.0\"E (Latitude:0.339999; Longitude:32.626949).\n\nUnder its mandate, the ERB is authorized to (a) register (b) de-register (c) restore registration (d) suspend registration (e) hold inquiries (f) hear appeals and (f) appear as respondents against a case brought against it in the High Court. It is also mandated to advise the government regarding the engineering sector.\n\nThe board is appointed by the Ugandan Minister of Works and Transport in consultation with the \"Uganda Institute of Professional Engineers\" (UIPE), the professional body of engineers in the country, who are guaranteed for positions on the board. \n\nThe 17th Board was named on 15 March 2018, by the Ugandan Minister of Works and Transport, Engineer Monica Azuba Ntege. The ERB members named are:\n\nRegistered engineers in Uganda, enjoy cross-border reciprocity of recognition of credentials in the countries of the countries of the East African Community (Burundi, Kenya, Rwanda, Tanzania and South Sudan).\n\n, there were 842 registered engineers, of whom 774 were Uganda nationals with full operational licences and 68 were foreigners with temporary registration. 720 of the registered engineers are concentrated in Kampala, with only 105 scattered across the remaining 120 districts.\n\nThe current board is haired by Engineer Michael Odongo, who id deputized by Engineer Henry Francis Okinyal.\n\nMinistry of Works and Transport (Uganda)\n\n"}
{"id": "8783558", "url": "https://en.wikipedia.org/wiki?curid=8783558", "title": "Environmental Science Services Administration", "text": "Environmental Science Services Administration\n\nThe Environmental Science Services Administration (ESSA) was a United States Federal executive agency created in 1965 as part of a reorganization of the United States Department of Commerce. Its mission was to unify and oversee the meteorological, climatological, hydrographic, and geodesic operations of the United States. It operated until 1970, when it was replaced by the new National Oceanic and Atmospheric Administration (NOAA).\n\nThe first U.S. Government organization with the word \"environment\" in its title, ESSA was the first such organization chartered to study the global natural environment as whole, bringing together the study of the oceans with that of the both the lower atmosphere and the ionosphere. This allowed the U.S Government for the first time to take a comprehensive approach to studying the oceans and the atmosphere, also bringing together various technologies – ships, aircraft, satellites, radar, and communications systems – that could operate together in gathering data for scientific study.\n\nIn May 1964, the U.S. Assistant Secretary of Commerce for Science and Technology, Dr. Herbert Holloman, established a special committee to review the environmental science service activities and responsibilities of the United States Department of Commerce. Committee members included the Director of the United States Weather Bureau, Dr. Robert M. White (1923–2015); the Director of the United States Coast and Geodetic Survey, Rear Admiral Henry Arnold Karo (1903–1986) of the United States Coast and Geodetic Survey Corps; the Director of the National Bureau of Standards, Allen V. Astin (1904–1984); and a panel of scientists from industry and academia. The committee's goal was to consider ways of improving the Department of Commerce's environmental science efforts by improving management efficiency and making the provision of environmental science services to the public more effective. The committee's work resulted in its recommendation that the Department of Commerce consolidate various scientific efforts scattered within and between the Weather Bureau, Coast and Geodetic Survey, and National Bureau of Standards by establishing a new parent agency – the Environmental Science Services Administration (ESSA) – which would coordinate the activities of the Weather Bureau and Coast and Geodetic Survey and bring at least some of their efforts, along with some of the work done in the National Bureau of Standards, together into new organizations that focused scientific and engineering mission support for shared areas of inquiry.\n\nIn a message to the United States Congress dated 13 May 1965 in which he formally proposed the creation of ESSA, U.S. President Lyndon Johnson described ESSA's mission in this way:\n\nThe new Administration will then provide a single national focus for our efforts to describe, understand, and predict the state of the oceans, the state of the lower and upper atmosphere, and the size and shape of the earth.\n\nThe Director of the Weather Bureau, Dr. Robert M. White, explained that the creation of ESSA:\n\nESSA was established on 13 July 1965 under the Department of Commerce's Reorganization Plan No. 2 of 1965. Its creation brought the Weather Bureau and the Coast and Geodetic Survey, as well as the Central Radio Propagation Laboratory that had been part of the National Bureau of Standards, together under a single parent scientific agency for the first time. Although the Weather Bureau and Coast and Geodetic Survey retained their independent identities under ESSA, the offices of Director of the Weather Bureau and Director and Deputy Director of the Coast and Geodetic Survey were abolished. These offices were replaced by a new Administrator and Deputy Administrator of ESSA.\n\nESSA was headquartered in Rockville, Maryland, with the ESSA Administrator as its senior executive. It consisted of five principal service and research elements, each of which reported directly to the ESSA Administrator: the Institutes for Environmental Research, reorganized in 1967 as the ESSA Research Laboratories; the Environmental Data Service; the United States Weather Bureau; the National Environmental Satellite Center; and the United States Coast and Geodetic Survey. Various other headquarters staff elements also reported directly to the Administrator, including the U.S. ESSA Commissioned Officer Corps (or \"ESSA Corps\").\n\nTo tackle scientific and technological problems related to understanding the global environment, ESSA created the Institutes for Environmental Research, based in Boulder, Colorado. The four institutes were:\n\n\nTo more precisely reflect the scope and mission of the individual elements of the institutes, ESSA reorganized them into the ESSA Research Laboratories in 1967. The ESSA Research Laboratories were made up of:\n\nUnder ESSA, the National Data Center was renamed the Environmental Data Service (EDS). In 1966, ESSA transferred the U.S. Coast and Geodetic Survey's Seismology Data Centers to Asheville, North Carolina, where they merged with the U.S. Weather Bureau's National Weather Records Center to create ESSA's Environmental Data Center.\n\nUnder the 1965 reorganization, the United States Weather Bureau became subordinate to ESSA. It retained its identity as the U.S. Weather Bureau while under ESSA. It was renamed the National Weather Service (NWS) in 1970.\n\nThe National Aeronautics and Space Administration (NASA) began weather satellite programs in 1958, and ESSA inherited these upon its creation in 1965. ESSA's National Environmental Satellite Center worked jointly with NASA to develop weather satellite capabilities. It managed the first operational U.S. polar orbiting weather satellite system, known as the Television Infrared Observation Satellite (TIROS) Program. These satellites, launched between 1960 and 1965 and known as TIROS 1 through 10, were the first generation of American weather satellites. These early satellites carried low-resolution television and infrared cameras. Designed mainly to test the feasibility of weather satellites, TIROS proved to be extremely successful. Four were still operating when ESSA was established.\n\nTIROS paved the way for the more advanced weather satellites of the TIROS Operational System (TOS). The ESSA National Environmental Satellite Center worked jointly with NASA to deploy the new TOS satellites, which constituted an operational experiment with early imaging and weather broadcast systems. Nine of ESSA's TOS satellites were launched between 1966 and 1969, each named \"ESSA\" followed by a number from 1 to 9, beginning with the launch of ESSA-1 on 3 February 1966. The last of these satellites was decommissioned in 1977, but ESSA's work with NASA laid the foundation for the deployment of the first geostationary weather satellites, the Synchronous Meteorological Satellites of 1974 and 1975.\n\nUnder the 1965 reorganization, the U.S. Coast and Geodetic Survey was subordinated to ESSA. While under ESSA, it retained its distinct identity and continued to carry out its responsibilities for coastal and oceanic hydrographic surveys, geodetic work in the interior of the United States and at sea, and other scientific work, such as in seismology. The Coast and Geodetic Survey also continued to operate its fleet of survey ships and research ships while subordinate to ESSA.\n\nIn the 1965 reorganization, the commissioned officers of the United States Coast and Geodetic Survey Corps, a component of the U.S. Coast and Geodetic Survey with a history dating back to 1917, were transferred to the control of the United States Secretary of Commerce. This created the United States Environmental Science Services Commissioned Officer Corps, known informally as the \"ESSA Corps,\" whose director reported directly to the ESSA Administrator. Like the Coast and Geodetic Survey Corps before it, the ESSA Corps was responsible for providing commissioned officers to operate the Coast and Geodetic Survey's ships, fly aircraft, support peacetime defense requirements and purely civilian scientific projects, and provide a ready source of technically skilled officers which could be incorporated into the United States armed forces in time of war, and was one of the seven uniformed services of the United States.\n\nRobert M. White (1923–2015) served as the Administrator of ESSA throughout its existence.\n\nOn the day ESSA and the ESSA Corps were created, Coast and Geodetic Survey Corps Rear Admiral Henry Arnold Karo (1903–1986) simultaneously became an ESSA Corps officer and was promoted to vice admiral to serve as ESSA's first deputy administrator. At the time the highest-ranking officer in the combined history of the Coast and Geodetic Survey Corps and ESSA Corps, Vice Admiral Karo served as Deputy Administrator of ESSA from 1965 to 1967. He was the only officer in the combined history of the Coast and Geodetic Survey Corps, ESSA Corps, and the ESSA Corps′ successor, the National Oceanic and Atmospheric Administration Commissioned Corps (NOAA Corps), to reach that rank until NOAA Corps Rear Admiral Michael S. Devany was promoted to vice admiral on 2 January 2014.\n\nThe first Director of the ESSA Corps was Rear Admiral James C. Tison, Jr. (1908–1991), who served in this capacity from 1965 to 1968. He was succeeded by the second and last Director of the ESSA Corps, Rear Admiral Don A. Jones (1912–2000), who served from 1968 to 1970.\n\nThe flag of the Environmental Science Services Administration was in essence the flag of the United States Coast and Geodetic Survey, modified by the addition of a blue circle to the center of the red triangle, within which was a stylized, diamond-shaped map of the world. Because the Coast and Geodetic Survey retained its identity after it was placed under ESSA in 1965, ships of the Survey's fleet continued to fly the Coast and Geodetic Survey flag as a distinguishing mark while the Survey was subordinate to ESSA.\n\nIn June 1966, the U.S. Congress passed the Marine Resources and Engineering Development Act, which declared that it was U.S. Government policy to:\n\n...develop, encourage, and maintain a coordinated, comprehensive, and long-range national program in marine science for the benefit of mankind, to assist in protection of health and property, enhancement of commerce, transportation, and national security, rehabilitation of our commercial fisheries, and increased utilization of these and other resources.\n\nThe act created a Commission on Marine Science, Engineering, and Resources – which came to be known informally as the \"Stratton Commission\" – and gave it the responsibility to review ongoing and planned U.S. Government marine science activities and recommend a national oceanographic program and a reorganization of the U.S. Government to carry out the program. President Lyndon Johnson appointed 15 members to the commission; Ford Foundation chairman Julius A. Stratton chaired it, and its members included attorney Leon Jaworski, Dean of the Graduate School of Oceanography at the University of Rhode Island John Knauss, ESSA Administrator Robert M. White, and other representatives of U.S. Government agencies, U.S. state governments, industry, academia, and other institutions with programs or interest in marine science and technology; it also included four U.S. Congressional advisors, including former U.S. Senator Warren G. Magnuson of Washington. The commission began its work in early 1967, and on 9 January 1969 it issued its final report, entitled \"Our Nation and the Sea: A Plan For National Action\". The Commission determined that \"because of the importance of the seas to this Nation and the world, our Federal organization of marine affairs must be put in order,\" and that fulfilling the U.S. ocean policy declared in the 1966 act and making \"full and wise use of the marine environment\" required the study of both the ocean and the atmosphere and their interactions with one another. Accordingly, it recommended the creation of an independent \"National Oceanic and Atmospheric Agency\" to administer the principal civil marine and atmospheric programs of the United States, and that the new agency be composed of the United States Coast Guard from the United States Department of Transportation; ESSA and its subordinates, the National Weather Service and U.S. Coast and Geodetic Survey, from the U.S. Department of Commerce; the Bureau of Commercial Fisheries and the functions of the Bureau of Sport Fisheres and Wildlife dealing with marine and migratory fishes from the United States Department of the Interior′s United States Fish and Wildlife Service; the National Sea Grant Program from the National Science Foundation; elements of the United States Lake Survey from the United States Department of the Army; and the National Oceanographic Data Center from the United States Department of the Navy.\n\nSoon after the Commission published the report, the U.S. Congress began to deliberate action on it, as did the Advisory Council on Executive Organization created by President Richard Nixon in 1969. Among the Advisory Council's proposals for reorganization of the executive branch of the United States Government was one that proposed the replacement of the U.S. Department of the Interior with a new U.S. Department of Natural Resources, and that this new department include a \"National Oceanic and Atmospheric Administration\" which combined ESSA with some elements of the Department of the Interior; the Nixon administration considered placing the new Administration within the Department of the Interior as an interim measure pending the creation of a new Department of Natural Resources. Noting that two-thirds of the new Administration would be made up of ESSA personnel and funding, United States Secretary of Commerce Maurice Stans (1908–1998) proposed instead that the new Administration become part of the Department of Commerce, where ESSA already was in place. Nixon decided to side with Stans, as well as to incorporate some of the Stratton Commission's and Advisory Council's recommendations, and in early July 1970 submitted Department of Commerce Reorganization Plan No. 4. It proposed the creation in 90 days within the Department of Commerce of the new National Oceanic and Atmospheric Administration (NOAA), consisting of ESSA; the Bureau of Commercial Fisheries and the marine sport fishing program of the Bureau of Sport Fisheries and Wildlife; the Office of Sea Grant Programs from the National Science Foundation; the mapping, charting, and research functions of the U.S. Army's U.S. Lake Survey; the U.S. Navy's National Oceanographic Data Center; the Marine Minerals Technology Center from the Department of the Interior's United States Bureau of Mines; the U.S. Navy's National Oceanographic Instrumentation Center; and the Department of Transportation's National Data Buoy Project, although it did not follow the Stratton Commission's recommendation to include the U.S. Coast Guard in NOAA.\n\nAccordingly, on 3 October 1970, ESSA was abolished as part of Reorganization Plan No. 4 of 1970, and it was replaced by NOAA. Under NOAA, the National Weather Service continued to operate as such, while the Coast and Geodetic Survey was disestablished and its functions were divided under various new NOAA offices. The Bureau of Commercial Fisheries of the United States Department of the Interior′s United States Fish and Wildlife Service was transferred to NOAA, and its fisheries science and oceanographic research ships joined the hydrographic survey ships of the former Coast and Geodetic Survey fleet to form the new NOAA fleet.\n\nIn the 1970 reorganization that created NOAA, the ESSA Corps was resubordinated to NOAA, becoming the National Oceanic and Atmospheric Administration Commissioned Officer Corps, known informally as the \"NOAA Corps.\" Like its predecessors, the Coast and Geodetic Survey Corps and ESSA Corps, the NOAA Corps became one of the seven uniformed services of the United States, and carries out responsibilities similar to those of the ESSA Corps.\n\nThe first U.S. Government organization to address environmental science and earth sciences holistically, ESSA pioneered the revolutionary organizational concept of uniting scientific and engineering activities that had been scattered among its subordinate agencies so as to establish unified mission support to meet environmental science and technology objectives. ESSA's successor, NOAA, continued and broadened the application of this organizational concept by adding marine life sciences to its portfolio of holistic study of the oceans and atmosphere alongside the earth sciences subordinated to ESSA. ESSA served as the prototype not only for NOAA but also for the United States Environmental Protection Agency, which was established two months after NOAA, on 2 December 1970.\n\nESSA's work in designing weather satellites and managing their missions was a major step forward both technologically and in terms of weather monitoring and prediction. It prompted further development of weather satellites in the exploration of their use, playing a major role in the development of modern weather satellites.\n\n\n"}
{"id": "49994138", "url": "https://en.wikipedia.org/wiki?curid=49994138", "title": "Express Transport", "text": "Express Transport\n\nExpress transport and its actors, the \"expressists\", consists in the delivery of small packages and parcel, in a limited duration from a point A (place of load decided by the carrier and not by the customer) to a point B (place of delivery).\n\nThe main characteristics of this type of transport are the speed of delivery as well as the ease of transport. It is rarely a \"door-to-door\" transport, contrary to the custom-made transport (or ad hoc). Most of the time, this type of transport begins in the warehouse up to the point of reception of the customer. Besides, this type of transport does not take into account the last kilometer, which is under the responsibility of the customer and not of the deliverer.\n\nExpress transport is an activity coming from the messaging market. With the technological and \nindustrial evolutions, customers requirements have developed the offer towards a faster delivery. So, the reduction of delivery deadlines has created this new type of market.\n\nExpress transport companies try to differ by their down-market positioning (it is the cheapest a customer can get with decent delivery deadlines) including a more and more wide side services range. These side services include for example information feedbacks, proofs of delivery, insurances, custom clearances, the specific packaging or the value-added services such as the IT maintenance.\n\nThe national express deliveries mean the domestic market, shipments achieved in unique country. In terms of guaranteed deadlines on the national express deliveries market, professionals distinguish two express categories of transport:\n\n\nThere are differences of delay for national express deliveries regarding the various operators. The delivery market is segmented according to these deadlines:\n\n\nThe express transport size includes monoparcel and multiparce deliveries with the express transport deadlines. We segmented the market of express transport according to the following categories of parcel:\n\n\nThe delivery of a parcel using express transport is divided into six phases, including the pick-up of the parcel at the customer’s place, grouping by road to the local recycling center, transport by plane or truck to the French main sorting center or European, sorting by destination and treatment of clearance services, the delivery of parcels by air or by truck center of local sorting and delivery by road to the recipient.\n\n\n\nTwo examples of express transport companies are FedEx and UPS.\n"}
{"id": "34162981", "url": "https://en.wikipedia.org/wiki?curid=34162981", "title": "Green Electronics Council", "text": "Green Electronics Council\n\nFounded 2005 the Green Electronics Council (GEC) promotes green computing and \"envisions a world where green electronics is a cornerstone of a healthy and vibrant world.\"\n\nGEC achieves this mission by supporting the production of consensus-based environmental leadership standards; by operating EPEAT, the definitive global rating system for greener electronics; and by convening global thought leaders in environmental design, strategy and marketing to envision more sustainable electronics design and delivery methods.\n\nThe Electronic Product Environmental Assessment Tool (EPEAT) system assists in the purchase of \"greener\" PCs and Displays, Imaging Equipment and Televisions. The EPEAT system evaluates electronics on more than 50 environmental criteria, some required and some optional, that measure a product's efficiency and sustainability attributes. Products are rated Gold, Silver or Bronze depending on how many optional criteria they meet. On 2007-01-24, President George W. Bush issued Executive Order 13423, which requires all United States Federal agencies to use EPEAT when purchasing computer systems. President Barack Obama issued a similar Executive Order in 2009. In 2012 EPEAT was launched in India. Since then a number of registered green devices has risen by 108%.\n\nIn partnership with the Yale Center for Green Chemistry and Engineering, in Sept. of 2008 GEC held a Forum for Sustainable Information and Communication Technologies (ICT) at Yale.\n\n Green Electronics Council hosts and presents annual Catalyst Award for \"practical projects whose impact can inspire further innovation in the electronics space.\" In the 2015 Dell received an award for their innovations in environmental safety of electronic production. During the 2014 calendar year Dell used 5000 tons of recycled plastic in its production of 34 products. Other notable nominees included Hewlett-Packard, Toshiba, and Arrow Electronics.\n\nEmerging Green Conference is an annual event organized by Green Electronics Council, where technology leaders meet to discuss \" advances, challenges and future of sustainable electronics.\" Latest gathering happened in September 2015 where over 30 companies and organizations attended a two-day event at The Nines Hotel in Portland, Oregon.\n\nKent Snyder, J.D. - Chairman - Attorney at Law, Snyder & Associates P.C.\n\nAlan Keith - Vice Chair - Vice President, Walt Disney Animation Studios\n\nCarl Smith - Treasurer - CEO, Call2Recycle\n\nChristine Ervin - Secretary - Principal, Christine Ervin/Company\n\nMike Biddle, PhD - President and Founder, MBA Polymers, Inc., USA\n\nAmy Knight - Director of CSR, Hasbro, Inc.\n\nPaul Anastas, PhD - Director, Center for Green Chemistry and Green Engineering\n\nKirsten Ritchie - Principal, Director of Sustainable Design, Gensler\n\nLiz Gasster - Vice President, Business Roundtable\n\n\n"}
{"id": "58019", "url": "https://en.wikipedia.org/wiki?curid=58019", "title": "Harvard architecture", "text": "Harvard architecture\n\nThe Harvard architecture is a computer architecture with physically separate storage and signal pathways for instructions and data. The term originated from the Harvard Mark I relay-based computer, which stored instructions on punched tape (24 bits wide) and data in electro-mechanical counters. These early machines had data storage entirely contained within the central processing unit, and provided no access to the instruction storage as data. Programs needed to be loaded by an operator; the processor could not initialize itself.\n\nToday, most processors implement such separate signal pathways for performance reasons, but actually implement a modified Harvard architecture, so they can support tasks like loading a program from disk storage as data and then executing it.\n\nIn a Harvard architecture, there is no need to make the two memories share characteristics. In particular, the word width, timing, implementation technology, and memory address structure can differ. In some systems, instructions for pre-programmed tasks can be stored in read-only memory while data memory generally requires read-write memory. In some systems, there is much more instruction memory than data memory so instruction addresses are wider than data addresses.\n\nIn a system with a pure von Neumann architecture, instructions and data are stored in the same memory, so instructions are fetched over the same data path used to fetch data. This means that a CPU cannot simultaneously read an instruction and read or write data from or to the memory. In a computer using the Harvard architecture, the CPU can both read an instruction and perform a data memory access at the same time, even without a cache. A Harvard architecture computer can thus be faster for a given circuit complexity because instruction fetches and data access do not contend for a single memory pathway.\n\nAlso, a Harvard architecture machine has distinct code and data address spaces: instruction address zero is not the same as data address zero. Instruction address zero might identify a twenty-four bit value, while data address zero might indicate an eight-bit byte that is not part of that twenty-four bit value.\n\nA modified Harvard architecture machine is very much like a Harvard architecture machine, but it relaxes the strict separation between instruction and data while still letting the CPU concurrently access two (or more) memory buses. The most common modification includes separate instruction and data caches backed by a common address space. While the CPU executes from cache, it acts as a pure Harvard machine. When accessing backing memory, it acts like a von Neumann machine (where code can be moved around like data, which is a powerful technique). This modification is widespread in modern processors, such as the ARM architecture, Power Architecture and x86 processors. It is sometimes loosely called a Harvard architecture, overlooking the fact that it is actually \"modified\".\n\nAnother modification provides a pathway between the instruction memory (such as ROM or flash memory) and the CPU to allow words from the instruction memory to be treated as read-only data. This technique is used in some microcontrollers, including the Atmel AVR. This allows constant data, such as text strings or function tables, to be accessed without first having to be copied into data memory, preserving scarce (and power-hungry) data memory for read/write variables. Special machine language instructions are provided to read data from the instruction memory, or the instruction memory can be accessed using a peripheral interface. (This is distinct from instructions which themselves embed constant data, although for individual constants the two mechanisms can substitute for each other.)\n\nIn recent years, the speed of the CPU has grown many times in comparison to the access speed of the main memory. Care needs to be taken to reduce the number of times main memory is accessed in order to maintain performance. If, for instance, every instruction run in the CPU requires an access to memory, the computer gains nothing for increased CPU speed—a problem referred to as being memory bound.\n\nIt is possible to make extremely fast memory, but this is only practical for small amounts of memory for cost, power and signal routing reasons. The solution is to provide a small amount of very fast memory known as a CPU cache which holds recently accessed data. As long as the data that the CPU needs are in the cache, the performance is much higher than it is when the CPU has to get the data from the main memory.\n\nModern high performance CPU chip designs incorporate aspects of both Harvard and von Neumann architecture. In particular, the \"split cache\" version of the modified Harvard architecture is very common. CPU cache memory is divided into an instruction cache and a data cache. Harvard architecture is used as the CPU accesses the cache. In the case of a cache miss, however, the data is retrieved from the main memory, which is not formally divided into separate instruction and data sections, although it may well have separate memory controllers used for concurrent access to RAM, ROM and (NOR) flash memory.\n\nThus, while a von Neumann architecture is visible in some contexts, such as when data and code come through the same memory controller, the hardware implementation gains the efficiencies of the Harvard architecture for cache accesses and at least some main memory accesses.\n\nIn addition, CPUs often have write buffers which let CPUs proceed after writes to non-cached regions. The von Neumann nature of memory is then visible when instructions are written as data by the CPU and software must ensure that the caches (data and instruction) and write buffer are synchronized before trying to execute those just-written instructions.\n\nThe principal advantage of the pure Harvard architecture—simultaneous access to more than one memory system—has been reduced by modified Harvard processors using modern CPU cache systems. Relatively pure Harvard architecture machines are used mostly in applications where trade-offs, like the cost and power savings from omitting caches, outweigh the programming penalties from featuring distinct code and data address spaces.\n\n\nEven in these cases, it is common to employ special instructions in order to access program memory as though it were data for read-only tables, or for reprogramming; those processors are modified Harvard architecture processors.\n\n"}
{"id": "44111946", "url": "https://en.wikipedia.org/wiki?curid=44111946", "title": "Helen Stone", "text": "Helen Stone\n\nHelen Stone is an English civil engineer and \nhas held the post of managing director of WS Atkins Structural Engineering, which she joined in 1972.\n\nShe attended the North London Collegiate School and after becoming inspired to become a civil engineer through, in her own words, \"a trip up the newly-opened M1 motorway when I was 10.\" She studied civil engineering at the University of Birmingham, obtained chartered status, and has worked on engineering projects including the Channel Tunnel, motorways, a theme park, aircraft hangar and oil refinery.\n\nIn 1991 she became only the third woman to become a fellow of the Institution of Civil Engineers and in 2002 she was elected a fellow of the Royal Academy of Engineering. She is known for representing British engineering overseas, and has a particular interest in representing the interests of women in engineering. According to the \"Daily Telegraph\", she found that in a wide range of countries, women were not being promoted to senior positions in engineering to the same extent as men, and now chairs the Diversity Panel of the Construction Industry Council. However, despite saying that \"I am conscious that I have a responsibility to break down inappropriate barriers which prevent women engineers from making progress,\" she is not in favour of positive discrimination, believing that it could lead to a lowering of standards.\n\nShe is the Chair of the Ethics and Standards Board of the APM Group, an accreditation group.\n"}
{"id": "4447365", "url": "https://en.wikipedia.org/wiki?curid=4447365", "title": "History of monorail", "text": "History of monorail\n\nThe term monorail or industrial monorail is used to describe any number of transport systems in which a chair or carrier is suspended from, or rides on, an overhead rail structure. Unlike the well-known duo-rail system, there are many rail-guided transport options which have been described as monorails, so that tracing the history presents a demarcation problem regarding what should be included and what should be omitted.\n\nCommon usage appears to define a monorail as any rail guided vehicle which does not employ the coning action of conventional adhesion railways to achieve directional stability. This would exclude rack railways and funicular railways.\n\nBearing in mind the pattern of development of conventional railways, different criteria and measures of effectiveness were relevant at different times, and alternative design solutions were proposed. Hence, a monorail of the early 19th Century bears little resemblance to current\ndesigns, and were optimised for different performance objectives, within different technological constraints.\n\nThe earliest patent for a vehicle designed to run on a single rail can be traced to UK patent No 4618 dated 22 November 1821. The inventor was Henry Robinson Palmer, commonly known by his alias as \"George Monorail\", who described it as 'a single line of rail, supported at such height from the ground as to allow the centre of\ngravity of the carriages to be below the upper surface of the rail'. The vehicles straddled the rail, rather like a pair of pannier baskets on a mule. Propulsion was by horse. A line was built in 1824 in the Deptford Dockyard in London, and in 1825, another line was built in Cheshunt, Hertfordshire. Dubbed the \"Cheshunt Railway\", this line made history as it was the world's first passenger-carrying monorail, and the first railway line to open in Hertfordshire. In 1826 a company was formed to construct a line between Barmen and Elberfeld in Germany, but construction never started.\n\nThroughout the 19th Century, the Palmer design was improved, with the addition of stabilising wheels and additional rails (rendering a misnomer of 'monorail'). In 1829, Maxwell Dick introduced 'safety rails' below the running rails to reduce the likelihood of derailment. He\nalso articulated the main advantage claimed for this class of vehicle: 'the pillars or supports to be of different heights as circumstances of the country may require'. In other words, the system was better suited for crossing rough terrain.\n\nIn 1868, William Thorold M.I.C.E. presented a paper proposing a monorail system that could be built at ground level in or alongside roads. The Patiala State Monorail Trainways and Kundala Valley Railway were built in India on the principle that he described.\n\nIn 1869, J Haddon built a monorail in Syria to replace a mule train in military use. This embodied lateral guide rails, but was basically a pannier design hauled by a locomotive having double vertical steam boilers.\n\nGeneral LeRoy Stone demonstrated in 1876 his Centennial Monorail for transporting passengers, but failed to exploit the concept also on the Bradford and Foster Brook Monorail, and this was closed after only one year due to a lethal boiler explosion in 1879. Around 1879, Joseph Stringfellow devised a similar \"one-rail\" system for possible use as a \"cheap railway\" in Australia.\n\nBy the end of the 19th Century, the main protagonists for the monorail where Charles Lartigue and F. B. Behr. Lartigue constructed Palmer monorails in Algeria to transport esparto grass, to replace mules and camels, although the motive power is recorded as 'animal'. He also demonstrated his ideas in Paris (1884), Westminster (1886), Tours (1889), St Petersburg (1894), Long Island (1894) and Brussels (1897). Behr proposed a high speed monorail between Liverpool and Manchester, but construction never started through lack of financial support.\n\nThe most famous Lartigue monorail was the Listowel and Ballybunion Railway, in Ireland, which stayed in service from 1888 until 1924. Part of this railway survives as a preserved railway and tourist attraction.\n\nThe last Lartigue design was built in 1924 between a magnesium mine at Crystal Hills, about 100 miles north of Los Angeles, and a railhead in Trona, California. This used petrol driven locomotives, and mounted the rail on a set of wooden A frames.\n\nIn 1886, the Enos Electric Company demonstrated a suspended monorail on the grounds of the Daft Electric Light Company in the Greenville section of Jersey City, New Jersey, which was closer in its appearance to more modern monorails, but the most famous suspended monorail of this era was Eugen Langen's 'Schwebebahn', or floating railway, of Wuppertal, which entered service in 1901, and is still in daily use.\n\nThe Wuppertal monorail follows the Wupper Valley where a conventional railway is quite impractical. The suspended monorail, like the Palmer monorail appears a potentially superior solution over rough and mountainous terrain, but since the majority of the track is over more \nfavourable territory, it only rarely offers an overall better solution. Short sections in mountainous areas, such as a system built for the Ria Copper Co. by Siemens in the Pyrenees, seem to be the niche for this type of monorail. This particular example used a form of regenerative braking, such that the electricity generated by the full descending trucks was sufficient to drive the empty trucks back up the mountain.\n\nIn 1890 the Boynton Bicycle Railroad was built in Long Island. Designed by Jose Ramon Villalon, who would later become one of Cuba's greatest statesmen, this railroad ran on a single rail at ground level, but with an overhead stabilising rail engaged by a pair of horizontally opposed wheels. The railway operated for only two years, but the design was adopted elsewhere.\n\nIn 1908, Elfric Wells Chalmers Kearney (1881–1960) designed a monorail having an overhead stabilizing rail with spring-loaded vertical stabilizing wheels, but although a car was built, it never saw service.\n\nFrom 1910–1914 a monorail system designed by Howard H Tunis was used on the Pelham Park and City Island Railroad in the Bronx, New York City. On the first trip of the monorail, the vehicle, with the inventor at the controls, slipped off the supporting lower rail. Though some of the New York newspapers erroneously reported that the accident was a major catastrophe, that is not so, and only one passenger on the car claimed a minor injury. The car was quickly repaired and the monorail operated safely on a regular basis from 1911 until 1917, when it was dismantled by the military at the outset of World War I, because the terminal near City Island was requisitioned.\nA propeller-driven suspended monorail, claiming the speed of aircraft with the safety and reliability of railways was designed by George Bennie in 1926, and named the 'Bennie Railplane'. A demonstrator was built near Glasgow in 1929, but the system did not progress further in the UK.\n\nRussia worked on a system similar to the Bennie Railplane in the 1930s and were even planning a 332-mile line through Turkestan with a top speed of 180 mph. Their system was unique in that it had two side by side cars suspended on one rail, and it could actually dismount from the rail to cross rivers as an amphibian and then on the other side remount the rail. And while elements of this system were tested in Moscow, the Russian government instead built a conventional rail system.\n\nPerhaps the only true monorail was the Gyro Monorail developed independently by Louis Brennan, August Scherl and Pyotr Shilovsky. This was a true single track train which used a gyroscope-based balancing system to remain upright. All were demonstrated by full-scale prototypes, but development was effectively stopped by the First World War. Brennan's design was given serious consideration for the North-West Frontier of India, and a Schilovski monorail was proposed by the USSR government between Leningrad and Tsarskoye Selo in 1921. Funds ran out shortly after construction began.\n\nIn the early 1930s, New York city considered a monorail system, which would have been the first in the US.\n\nThe vehicles referenced above (with the exception of the still in full service Wuppertal monorail) are now little more than historical curiosities. The advantage of the monorail of crossing rough mountainous terrain was relevant to the time of expansion of railway networks over virgin country, and in most cases the conventional railway proved the\nmore appropriate solution, except for a few niches. Monorail tracks were rarely longer than 60 miles, and usually considerably shorter. The motor road vehicle finally displaced the monorail from its few niche applications.\n\nWheel on steel characterised monorails of this early era, just as it does conventional railways, although some bicycle railways could react against the stabilizing rail to increase adhesion, improving acceleration, braking and hill climbing.\n\nThe development of automotive technology has given rise to a new class of monorail which owes little to the work of Palmer and Lartigue. These vehicles are suspended from or straddle concrete beams, and use pneumatic tyres to improve adhesion and reduce noise compared with wheel on steel. They have more in common with guided buses than conventional railways. The beam is less obtrusive than an overhead roadway or railway, and the modern designs may have a niche in dealing with right of way problems in congested city centres, at lower cost compared with tunneling.\n\n\n\n"}
{"id": "43638412", "url": "https://en.wikipedia.org/wiki?curid=43638412", "title": "Humans Need Not Apply", "text": "Humans Need Not Apply\n\nHumans Need Not Apply is a 2014 short Internet video, directed, produced, written and edited by CGP Grey. The video focuses on the future of the integration of automation into economics, as well as the impact of this integration to the worldwide workforce. It was released online as a YouTube video.\n\nThe video focuses on the topic of robots' rapidly increasing usefulness through human society, discussing how automation will lead to a future where human labor is no longer needed.\n\nEarly on, an analogy is made describing how humans once displaced horses from their jobs (by creating mechanical muscles such as automobiles), dismissing the argument that humans will always find new work, seeing as horses are not nearly as much used now. This analogy finishes by connecting the creation of mechanical minds, or \"brain labor\", will lead to robots ousting humans out of their occupations. Grey also discusses how economics is the force behind a future based upon automation. Grey concludes by stating that 45% of the workforce could be replaced by bots, a figure which is inclusive of professional, white-collar and low-skill occupations, and higher than the 25% unemployment figure of the Great Depression. To take one specific example, the video states that there are 3 million driving jobs in the United States and 70 million worldwide. Grey further states that even creative occupations are not secure, mentioning the bot-composed music in the background of his video.\n\nAdditionally, the viewer is reminded that the video is not discussing or portraying a future based upon science fiction, using examples such as Baxter, self-driving cars (referred to as autos in the video) and IBM's Watson.\n\nThe film was funded through Subbable, a crowdfunding website. Grey used this website as a means to support his projects before moving to Subbable's successor, Patreon.\n\n\"Humans Need Not Apply\" was covered by several publications, including \"Business Insider\", \"The Huffington Post\" and \"Forbes\". Coverage of the video complimented its presentation, calling the video \"well-produced\". These publications also praised its premise, calling it \"thought-provoking\", and \"compelling\", but also maintaining that the points and topics brought up in the video were \"terrifying\". Bruce Kasanoff of \"Forbes\" commented that the video was \"sobering,\" and \"suggests, in a convincing fashion, that many human jobs will disappear over the coming years, because automation will do them faster, better, and cheaper.\"\nAfter a few days of release, the video reached one million views. , \"Humans Need Not Apply\" has reached over 10 million views.\n\n\n"}
{"id": "6199604", "url": "https://en.wikipedia.org/wiki?curid=6199604", "title": "Hydrogen technologies", "text": "Hydrogen technologies\n\nHydrogen technologies are technologies that relate to the production and use of hydrogen. Hydrogen technologies are applicable for many uses.\n\nSome hydrogen technologies are carbon neutral and could have a role in preventing climate change and a possible future hydrogen economy. Hydrogen is a chemical widely used in various applications including ammonia production, oil refining and energy. Hydrogen is not a primary energy source, because it is not naturally occurring as a fuel. It is, however, widely regarded as an ideal energy storage medium, due to the ease with which electric power can convert water into its hydrogen and oxygen components through electrolysis and can be converted back to electrical power using a fuel cell. There are a wide number of different types of fuel and electrolysis cells.\n\nThe potential environmental impact depends primarily on the methods used to generate the hydrogen fuel.\n\n\n\n\nAudi:\n\nBMW:\n\nChrysler:\n\nDaimler:\n\nFiat:\n\nFord:\n\nForze Hydrogen-Electric Racing Team Delft\n\nGeneral Motors:\nHonda:\n\nHyundai:\n\nLotus Engineering:\n\nKia:\n\nMazda:\n\nMitsubishi:\n\nMorgan:\n\nNissan:\n\nPeugeot:\n\nRenault: \n\nRiversimple:\n\nRonn Motor Company:\n\nToyota:\n\nVolkswagen:\n\n\nPossible future aircraft using precooled jet engines include Reaction Engines Skylon and the Reaction Engines A2.\n\nThe following rockets were/are partially or completely propelled by hydrogen fuel:\n\n\n\n\n\n\n"}
{"id": "1201544", "url": "https://en.wikipedia.org/wiki?curid=1201544", "title": "Innovation, Science and Economic Development Canada", "text": "Innovation, Science and Economic Development Canada\n\nInnovation, Science and Economic Development Canada (), or ISED, formerly Industry Canada, is the department of the Government of Canada with a mandate of fostering a growing, competitive, and knowledge-based Canadian economy. ISED specifically supports Canadian innovation efforts, trade and investment, enterprise growth, and customized economic development in Canadian communities. \n\nISED has three core responsibilities. These responsibilities are to oversee Canadian companies, investment and growth; people, skills and communities; and science, technology, research and commercialization. It addresses these responsibilities by doing work in four areas. These areas are research and development; economic development; market integrity, regulation, and competition; and internal services. This work is done by distributing grants and contributions, providing programs and services, managing federal activities, and overseeing relevant regulation and legislation. \n\nIn 2018-19, ISED has emphasized the importance of women's entrepreneurship, innovation, and digital economy. ISED has also prioritized inclusivity, asserting that \"our economy should work for all Canadians\" \n\nIn order to fulfil this mandate among other responsibilities, ISED works in partnership with several organizations to address a broad and diverse range of economic variables across Canada. \nThese organizations focus on specific geographic regions or economic variables with the collective goal of strengthening the Canadian economy. \n\nThere are six regional development agencies under ISED's portfolio which provide tailored support suited to the strengths and needs of different areas of Canada. These agencies include: the Atlantic Canada Opportunities Agency, Canada Economic Development for Quebec Regions, Canadian Northern Economic Development Agency, Federal Economic Development Agency for Southern Ontario, Federal Economic Development Initiative for Northern Ontario, and Western Economic Diversification Canada. ISED is also associated with a number of special operating agencies, shared-governance corporations, departmental corporations, crown corporations, departmental agencies and a joint enterprise. \n\nThe Department of Trade and Commerce was created in statue on June 23rd, 1887 and proclaimed into force on December 3, 1892. In 1969 the Department of Trade and Commerce was replaced by the Department of Industry, Trade and Commerce. In 1990, Industry, Science and Technology replaced Industry, Trade and Commerce. This new department also absorbed the offices of the Minister of Regional Industrial Expansion and Minister of State for Science and Technology. This marked the inclusion of regional approaches and scientific emphasis in the development of Canadian industries. In 1993, the department expanded its portfolio further to include Consumer and Corporate Affairs. In March 1995, the department was renamed Industry Canada. \nUpon the November 2015 installation of the 29th Canadian Ministry led by Prime Minister Justin Trudeau, the position of Minister of Industry was re-named Minister of Innovation, Science, and Economic Development. Subsequently Industry Canada was re-named Innovation, Science and Economic Development Canada. The Department headquarters are located at the C.D. Howe Building at 235 Queen Street in Ottawa, Ontario.\n\n\nThe current general mandate for the Minister of Innovation, Science and Economic Development is to \"help Canadian businesses grow, innovate and export so that they can create good quality jobs and wealth for Canadians.\" The Minister's role is to work towards this mandate in collaboration with other Canadian governments, among other partners. In particular, the Minister of Innovation, Science and Economic Development works with the Minister of Science and the Minister of Small Business and Tourism. In this team, the Minister of Innovation, Science and Economic Development acts as a leader.\n\n\nThe current general goal for the Minister of Science is to \"support scientific research and the integration of scientific considerations in our investment and policy choices.\" The Minister must work with other Ministers to support science-based decision-making that furthers wider governmental economic and environmental mandates.\n\n\nISED oversees seventeen departments and agencies and is associated with an additional six organizations. Each of these organizations are related to one or more of the four focus areas of ISED: i) innovation in science and technology ii) trade and investment iii) growing small and medium-sized enterprises iv) economic growth of Canadian communities.\n\nISED oversees the following organizations: \n\n\nISED is associated with the following organizations: \n\n\nThe departmental legislation for ISED is the Department of Industry Act. The Act states that the Minister of ISED's objective is to use their role in order to \"strengthen the national economy and promote sustainable development.\" The Act also outlines a number of supporting objectives. The Minister must also use their position to support domestic trade and support a healthy marketplace through investment and technology. \n\nIn addition to the departmental legislation, ISED is also responsible for various legislation related to economic development, including: telecommunication legislation, marketplace and trade legislation, intellectual property legislation, consumer legislation, and other legislation governing general functions and agencies.\n\n\n"}
{"id": "5686380", "url": "https://en.wikipedia.org/wiki?curid=5686380", "title": "Integral windup", "text": "Integral windup\n\nIntegral windup, also known as integrator windup or reset windup, refers to the situation in a PID feedback controller where a large change in setpoint occurs (say a positive change) and the integral terms accumulates a significant error during the rise (windup), thus overshooting and continuing to increase as this accumulated error is unwound (offset by errors in the other direction). The specific problem is the excess overshooting.\n\nThis problem can be addressed by\n\nIntegral windup particularly occurs as a limitation of physical systems, compared with ideal systems, due to the ideal output being physically impossible (process : the output of the process being limited at the top or bottom of its scale, making the error constant). For example, the position of a valve cannot be any more open than fully open and also cannot be closed any more than fully closed. In this case, anti-windup can actually involve the integrator being turned off for periods of time until the response falls back into an acceptable range.\n\nThis usually occurs when the controller's output can no longer affect the controlled variable, or if the controller is part of a selection scheme and it is selected right.\n\nIntegral windup was more of a problem in analog controllers. Within modern Distributed Control Systems and Programmable Logic Controllers, it is much easier to prevent integral windup by either limiting the controller output, or by using external reset feedback, which is a means of feeding back the selected output to the integral circuit of all controllers in the selection scheme so that a closed loop is maintained.\n"}
{"id": "27784645", "url": "https://en.wikipedia.org/wiki?curid=27784645", "title": "Kadugodi", "text": "Kadugodi\n\nKadugodi (), also known as Kadugudi was founded by the great Cholas Dynasty. It is located in Whitefield, Bangalore in the state of Karnataka. Kadugodi comes from \"kadu\" & \"Gudi\" meaning \"temple in a forest\" in Kannada. It is believed that this whole area had thick vegetation consisting of soap-nut shrubs. The temple after which the place gets its name was also discovered by people venturing in to this kadu(forest) while gathering soap-nuts. This temple is of Anjaneya which is believed as udbhava murthi. Kadugudi is also referred to as \"Aranayapuri\" in temple related invites and publications.\n\nKadugodi has many temples and oldest among them are Anjaneya temple, kashi Vishwanatha temple, Ganesh temple, Eashwara temple and the grama devata temples of Maramma and Ganga parameshwari. All these temples are located in old kadugodi area starting from Bapuji circle up to Kashi vishwanatha temple.\n\nTwo major festivities of kadugudi is when Rathotsava(Charriot festival) are conducted for Eashwara and Rama deities. Lot of festivities are conducted before and after the actual rathotsava day.\n\nGoing by the street names of kadugudi one can deduce that this place must have been a well planned out settlement. There is a kumbhara beedi(potter street), there is Ganigara beedi(vegetable oil extractors), Angadi beedi(Market street), balajigara beedhi(mainly merchants dealing in bangles, flowers etc.).\n\nIn 1970s kadugodi saw its first extension towards west as Village Panchyat extension or simple referred to as new extension or VP extension. In 1980s saw another extension in eastern direction when a new extension was formed by free distribution of plots to economically deserving families. This was done by the Eashwara temple family and was named after Shree Shankar Dixit, this area now is popularly known as Shankara pura.\n\nKadugudi is known for its pleasant climate throughout the year. Its elevation is the highest among the major large cities of India.\n\nKadugudi has a tropical savanna climate (Köppen climate classification \"Aw\") with distinct wet and dry seasons. Due to its high elevation, Kadugudi usually enjoys a more moderate climate throughout the year, although occasional heat waves can make summer somewhat uncomfortable The coolest month is December with an average low temperature of and the hottest month is April with an average high temperature of . The highest temperature ever recorded in Kadugudi is (recorded in March 2012). However, the suburbs of Kadugudi recorded temperatures as high as . The lowest ever recorded is (recorded in January 2013).\n\nWinter temperatures rarely drop below , and summer temperatures seldom exceed . Kadugodi receives rainfall from both the northeast and the southwest monsoons and the wettest months are September, October and August, in that order. The summer heat is moderated by fairly frequent thunderstorms. The heaviest rainfall recorded in a 13-hour period in recorded on 17 October 2014\n\n\n"}
{"id": "50807470", "url": "https://en.wikipedia.org/wiki?curid=50807470", "title": "Katharine Stinson", "text": "Katharine Stinson\n\nKatharine Stinson was an American aeronautical engineer and the Federal Aviation Administration's first female engineer. Born in Fuquay-Varina, North Carolina 14 years after the Wright Brothers made their first flight on North Carolina's Outer Banks, Stinson loved airplanes. A ride from famed aviator Eddie Stinson (no relation) solidified her goal of learning to fly.\n\nWhile working as a mechanic's assistant at the Raleigh Municipal Airport at age 15, Stinson met Amelia Earhart. When Stinson told Earhart about her flight training, Earhart encouraged her to study engineering instead telling her that she would never make enough money as a pilot. Stinson took that advice, enrolling in physics classes in high school and applied to the engineering program at North Carolina State College.\n\nStinson's 1936 application to the engineering school was declined by Dean Wallace Riddick who met with her explaining that the college would not accept women as freshman. Stinson responded by accepting a scholarship to nearby Meredith College where she completed all 48 required credit hours in a single year. She enrolled in the engineering program the following fall as one of the few female students at the college and only female in the engineering department. She graduated in 1941 with a bachelor's degree in mechanical engineering with an aeronautical option, the first woman to do so at the college and one of just five women in the country that year to earn a degree in engineering or architecture.\n\nFollowing graduation, she was the first female engineer hired by the Civil Aviation Administration. During her 32-year career she specialized in aircraft safety developing standards for supersonic transports which were used to create the concorde and pioneering distribution of aircraft structural issues. She retired from the Federal Aviation Administration in 1974 as Technical Assistant to the Chief of Aircraft Engineering.\nDuring her career she was active in the and helped found the Society of Women Engineers where she served as president from 1953 to 1955. She also served on President Lyndon Johnson's Women's Advisory Committee on Aviation from 1964 to 1970, served as an officer in the Institute of Aeronautical Sciences. Stinson also belonged to the Soroptimist Club, serving president from 1970 to 1972.\n\nIn 1997 North Carolina State University named her a distinguished alumnus and named Katharine Stinson Drive on campus in her honor.\n"}
{"id": "56635199", "url": "https://en.wikipedia.org/wiki?curid=56635199", "title": "Let's Go (textbooks)", "text": "Let's Go (textbooks)\n\nLet's Go is a series of American-English based EFL (English as a foreign language) textbooks developed by Oxford University Press and first released in 1990. While having its origins in ESL teaching in the US, and then as an early EFL resource in Japan, the series is currently in general use for English-language learners in over 160 countries around the world. The series is now in its 4th edition, which was released in 2017, although the 3rd series is still in print.\n\nThe series was written by two (at that time) US-based EFL/ESL teachers and two Asia-based teachers. Ritsuko Nakata gained a BA from the University of California in Los Angeles, and has been involved in ELT for over 30 years, and is currently President of IIEEC, Teacher Training Center for English Teachers of Children, and President of AETC, The Association of English Teachers, based in Japan. Karen Frazier Tsai (cited as Karen Frazier) has 20 years experience of teaching ESL and has worked and travelled throughout Asia, Europe, and North America. Barbara Hoskins Sakamoto (cited as Barbara Hoskins) gained her MA in Teaching English as a Second Language from Northern Arizona University, and had been based in Japan since 1985. Carolyn Graham is the creator of Jazz Chants, which connect the rhythm of spoken American English to the beat of jazz.\n\nAccording to Nakata:In 1989, I was approached by the senior editor of Oxford University Press in New York asking me about what the Japanese market needed in terms of a new textbook. At the time, the only texts available were ESL texts that were written for students learning English in English speaking countries, so they were not appropriate for our Japanese students who were coming to class just once a week.According to Hoskins:\"Let’s Go\" has been a remarkably collaborative piece of publishing. It was one of Oxford’s first publishing projects which worked with authors living in different countries. When we first started, I lived in California, Karen Frazier (Tsai) lived in Taipei, Ritsuko Nakata was in Tokyo, and Carolyn Graham was in New York. Back then, we felt quite high tech with our desk top computers and fax machines–there was no internet or email yet!\n\nThe series is targeted towards an age range of 5–13 years of age (levels beginner to pre-intermediate). The 1st series (Books 1-6) was published in 1990, the 2nd in 2000, the 3rd in 2008, and the 4th in 2017. The lower level \"Let's Go Starter\" (by Nakata, Hoskins, and Frazier) was first released in 1997, before being replaced by \"Let's\" \"Begin\" in later series.\nAncillary publications include workbooks for each of the levels, \"Let's Go Phonics\" (by Jeffrrey Lehman), \"Let's Chant, Let's Sing\" (by Graham), and \"Let's Go Picture Dictionary\" (by Nakata, Frazier, and Hoskins). Available resources include teacher books, classroom CDs, teacher vocabulary cards and student vocabulary cards.\n\nThe series claims to improve student learning and classroom pedagogy in the following ways:\n\n"}
{"id": "34507803", "url": "https://en.wikipedia.org/wiki?curid=34507803", "title": "Line-of-sight (missile)", "text": "Line-of-sight (missile)\n\nIn missile guidance, Line of sight is the straight line between the missile and the target. At the end of the engagement the distance will be zero. See Command guidance#Command to Line-Of-Sight (CLOS).\n\nBy Pursuit guidance the missile is steered so that the velocity vector of the missile always points at the target, i.e. has the direction of the Line of sight.\n\n"}
{"id": "26178384", "url": "https://en.wikipedia.org/wiki?curid=26178384", "title": "List of United States mobile virtual network operators", "text": "List of United States mobile virtual network operators\n\nMobile virtual network operators (MVNOs) in the United States lease wireless telephone and data service from major carriers such as AT&T Mobility, Sprint Corporation, T-Mobile US, and Verizon Wireless, as well as the regional carrier the United States Cellular Corporation for resale.\n\nAs of 2016 MVNOs served about 36 million subscribers.\n\nThese providers offer services to individuals for personal use. Providers offering services to businesses are listed in a later section.\n\nMany of the Mobile Virtual Network Operators (MVNOs) are often partnered with one (and sometimes more) host carrier companies. Providers with multiple host networks only use one depending on the specific model phone and/or SIM card used, except Google Fi which combines all listed networks together. MVNOs will often push/favor a specific model phone because it's locked into the host carrier \"preferred network\" that gives that MVNO the best deal/rates. The acronym \"BYOD\" means \"Bring Your Own Device\".\n\nThe following is a list of the mobile virtual network operators offering government subsidized Lifeline mobile services in California and Minnesota. This list needs to be expanded for all other states. All 50 States run and operate their own Lifeline programs so company, MVNOs participation and plan offerings vary greatly from state to state.\n\n Does not officially support BYOD.\n\nThese providers offer services to individuals for personal use. Providers offering services to businesses are listed in a later section. All MVNO's listed below are BYOD, unless otherwise noted.\n\nThe following table lists operators focused on providing voice calls. Additional services may include SMS and basic low-speed data access.\n\nThese providers offer services to individuals for personal use. Providers offering services to businesses are listed in a later section. Unless specified otherwise, mobile broadband providers require the purchase of a dedicated mobile broadband modem.\n\n"}
{"id": "2159644", "url": "https://en.wikipedia.org/wiki?curid=2159644", "title": "Miller", "text": "Miller\n\nA miller is a person who operates a mill, a machine to grind a cereal crop to make flour. Milling is among the oldest of human occupations. \"Miller\", \"Milne\", and other variants are common surnames, as are their equivalents in other languages around the world (\"Melnyk\" in Ukrainian, \"Meunier\" in French, \"Müller\" or \"Mueller\" in German, \"Mulder\" and \"Molenaar\" in Dutch, \"Molnár\" in Hungarian, \"Molinero\" in Spanish, \"Molinaro\" or \"Molinari\" in Italian etc.). Milling existed in hunter-gatherer communities, and later millers were important to the development of agriculture.\n\nThe materials ground by millers are often foodstuffs and particularly grain. The physical grinding of the food allows for the easier digestion of its nutrients and saves wear on the teeth. Non-food substances needed in a fine, powdered form, such as building materials, may be processed by a miller.\n\nThe most basic tool for a miller was the quern-stone—simply a large, fixed stone as a base and another movable stone operated by hand, similar to a mortar and pestle. As technology and millstones (the bedstone and rynd) improved, more elaborate machines such as watermills and windmills were developed to do the grinding work. These mills harnessed available energy sources including animal, water, wind, and electrical power. Mills are some of the oldest factories in human history, so factories making other items are sometimes known as mills, for example, cotton mills and steel mills. These factory workers are also called millers.\n\nThe rynd in pre-reformation Scotland was often carved on millers' gravestones as a symbol of their trade.\n\nIn a traditional rural society, a miller is often wealthier than ordinary peasants, which can lead to jealousy and to millers being targeted in bread riots at times of famine. Conversely, millers might be in a stronger position vis-a-vis feudal land owners than are ordinary peasants.\n\nThe traditional carnival held annually in the city of Ivrea, Italy commemorates a spirited \"Mugnaia\" (miller's daughter) who supposedly refused to let a local duke exercise his right of the first night, and proceeded to chop the duke's head off and spark a revolution. Whatever the historical validity of the story, it is significant it was the daughter of a miller to whom folk tradition assigned this rebellious role.\n\nAs an important part of his job, the miller repeatedly takes into his hand samples of the ground meal coming out of the spout in order to feel the quality and character of the product. The miller rubs the grain between his thumb and forefinger. After years of doing this the miller's thumb changes shape and becomes broad and flattened. This is known as a \"miller's thumb\".\n\nSayings such as \"worth a millerˈs thumb\" and \"an honest miller hath a golden thumb\" refers to the profit the miller makes as a result of this skill.\n\nThe shape of a miller's thumb is said to have the appearance of the head of a fish. The European bullhead (\"Cottus gobio\"), a freshwater fish, is commonly called a miller's thumb for this reason.\n\n\nMiller (also known as Millar) is a common surname derived from the old english surname \"Milleiir\". The name, and its many other variants, can be found widely across the Europe in countries like the UK, Ireland, and many other countries across the world. \n"}
{"id": "12765929", "url": "https://en.wikipedia.org/wiki?curid=12765929", "title": "Mind control in popular culture", "text": "Mind control in popular culture\n\nMind control has proven a popular subject in fiction, featuring in books and films such as \"The Manchurian Candidate\" (1959; film adaptation 1962) and \"The IPCRESS File\" (1962; film 1965), both stories advancing the premise that controllers could hypnotize a person into murdering on command while retaining no memory of the killing. As a narrative device, mind control serves as a convenient means of introducing changes in the behavior of characters, and is used as a device for raising tension and audience uncertainty in the contexts of the Cold War and terrorism. Mind control has often been an important theme in science fiction and fantasy stories. Terry O'Brian comments: \"Mind control is such a powerful image that if hypnotism did not exist, then something similar would have to have been invented: the plot device is too useful for any writer to ignore. The fear of mind control is equally as powerful an image.\"\n\n\n\n\n\nThe TV series \"The Prisoner\" featured mind control as a recurring plot element.\n\nIn the Korean mini-series \"Winter Sonata\" the protagonist has his memory altered by a clinical psychiatrist at his mother's request which forms the crux of the plot as he struggles to overcome it.\n\nIn the movie \"Conspiracy Theory\", Mel Gibson plays Jerry Fletcher, a cab driver and a conspiracy theorist who accidentally hits a truth involving a secret government-funded mind control program, as it turns out Jerry himself is one of the subjects of the program.\n\nIn Judy Malloy's \"Revelations of Secret Surveillance\", a group of artists and writers struggle to understand and expose a covert system that utilizes psychodrama and brain scanning surveillance to interfere with the lives of artists, activists, and many other people.\n\nThe novel \"Trilby\" (1894) features the character Svengali, who hypnotizes the novel's heroine to enhance her singing performance. The character gained popularity as the stereotype of an evil hypnotist, and became the basis for feature films throughout the 20th century.\n\nIn Aldous Huxley's \"Brave New World\", a technique called hypnopaedia is used to condition children to be obedient citizens.\n\nAn adaptation of Kurt Vonnegut's \"Harrison Bergeron\", was made into a film \"Harrison Bergeron\", a 1995 production. Everyone but the elite had \"handicapping\" devices attached to their brains.\n\nMr. Big, one of the antagonists in the PBS Kids GO! series \"WordGirl\", frequently uses mind control to entice people to buy his products.\n\nQueen Chrysalis, a powerful antagonist of the Hub series \"\", has the gift of mind control via emotional manipulation and metamorphic abilities. In \"A Canterlot Wedding\", she uses this ability to disguise herself as Princess Cadence and bring the princess's fiancee Shining Armor under her spell, as well as manipulating Cadence's bridesmaids to become her servants. Starlight Glimmer, another antagonist, used a combination of a magic staff and more traditional forms of mind control (including solitary confinement, denial of care, and constantly broadcasting messages via loudspeaker) to create a cult of ponies obsessed with the idea of conformity.\n\nIn the \"Monkees\" episode \"The Frodis Caper\", an insane wizard captures a sentient potted plant from outer space and attempts world domination by broadcasting the plant's mind-control eye over television.\n\nIn the American soap opera \"Days of Our Lives\", several characters including John Black, Hope Brady, and Steve Johnson, were subjected to brainwashing and mind control by Stefano DiMera and other villains, in order to turn them into assassins and mob \"soldiers\".\n\nHypnotism has often been used by stage performers to induce volunteers do strange things, such as clucking like a chicken, for the entertainment of audiences. The British psychological illusionist Derren Brown performs more sophisticated mental tricks in his television programmes, \"Derren Brown: Mind Control\".\n\nThe late Russian psychic, Wolf Messing, was said to be able to hand somebody a blank piece of paper and make them see money or whatever he wanted them to see.\n\n"}
{"id": "8053931", "url": "https://en.wikipedia.org/wiki?curid=8053931", "title": "Mobile ticketing", "text": "Mobile ticketing\n\nMobile ticketing is the process whereby customers can order, pay for, obtain and/or validate tickets using mobile phones. Mobile tickets reduce the production and distribution costs connected with traditional paper-based ticketing channels and increase customer convenience by providing new and simple ways to purchase tickets.\n\nMobile tickets should not be confused with E-Tickets (electronic tickets) which are used by airlines since 1994, they can be sent by e-mail, printed and shown at the check-in desk at the airport to obtain a boarding pass.\n\nMany train and bus operators in Europe have created phone apps in which tickets can be bought and stored. These include but are not limited to SJ, DSB, NSB, DB and selected local transit authorities.\n\nPhilips and Sony developed near field communication (NFC) in 2002. It is build on the same basis as common contactless smartcards. Philips published an early paper on NFC in 2004. In 2004, the NFC Forum was established.\nNFC incorporated in a mobile phone allows all kind of novel contactless applications, mobile ticketing being an important one of them.\nMobile Tickets can be purchased via internet and will be downloaded in a few seconds to the mobile phone, be it in an sms with a 2-D barcode or to the connected NFC chip. In case of NFC at entrance the phone just has to be touched to the scanning device (in fact it makes contact within 10 cm). The GSM Association, GSMA, published a whitepaper on M-Ticketing in 2011.\nIt describes extensively the use and advantages of M-Ticketing, principally the use of NFC technology. They state that NFC is the best technology but \"it is expected however that M-Ticketing services using SMS and Bar Code implementations will be prevalent until the point that a critical mass of NFC enabled handsets is available.\"\n\n"}
{"id": "16015428", "url": "https://en.wikipedia.org/wiki?curid=16015428", "title": "Molecular conductance", "text": "Molecular conductance\n\nMolecular Conductance (formula_1), or the conductance of a single molecule, is a physical quantity in molecular electronics. Molecular conductance is dependent on the surrounding conditions (e.g. pH, temperature, pressure), as well as the properties of measuring device. Many experimental techniques have been developed in an attempt to measure this quantity directly, but theorists and experimentalists still face many challenges.\n\nRecently, a great deal of progress has been made in the development of reliable conductance-measuring techniques. These techniques can be divided into two categories: molecular film experiments, which measure groups of tens of molecules, and single-molecule-measuring experiments.\nMolecular film experiments generally consist of the sandwiching of a thin layer of molecules between two electrodes which are used to measure the conductance through the layer. Two of the most successful implementations of this concept have been the bulk electrode approach and in the use of nanoelectrodes. In the bulk electrode approach, a molecular film is typically immobilized onto one electrode and an upper electrode is brought into contact with it allowing for a measure of current flow as a function of applied bias voltage. The nanoelectrode class of experiments, in creatively utilizing equipment such as atomic force microscope tips and small-radius wires, are able to perform the same sorts of current versus applied bias measurements but on a much smaller number of molecules as compared to bulk electrode. For instance, the tip of an atomic force microscope can be used as a top electrode and, given the nano-scale radius of curvature of the tip, the number of molecules measured is drastically cut. The difficulties encountered in these experiments have come mainly in dealing with such thin layers of molecules which often results in problems with short-circuiting the electrodes.\n\nMore recently, single-molecule-measurement experiments have been developed that are bringing experimenters a better look at molecular conductance. These fall under the categories of scanning probe, which involves fixed electrode, and mechanically formed junction techniques. One example of a mechanically formed junction experiment involves using a movable electrode to make contact with and then pull away from an electrode surface coated with a single layer of molecules. As the electrode is removed from the surface the molecules that had bonded between the two electrodes begin to detach until eventually one molecule is connected. The atomic-level geometry of the tip-electrode contact has an effect on the conductance and can change from one run of the experiment to the next so a histogram approach is required. Forming a junction in which the precise contact geometry is known has been one of the main difficulties with this approach.\n\nAn important first step toward the goal of building electronic devices on the molecular level is the ability to measure and control the electric current through an individual molecule. Based on the anticipated continuation of Moore's Law, which is expected to carry the miniaturization of transistors on integrated circuits into the atomic scale within the next 10 to 20 years, this goal of single-molecule-level circuit design is likely to become widespread throughout the semiconductor industry.\n\nOther applications focus on the insight provided by these experiments in the area of charge transport, which is a recurrent phenomenon in many chemical and biological processes. This sort of insight gives researchers the ability to read the chemical information stored in a single molecule electronically, which can then be used in a wide variety of chemical and biosensor applications.\n"}
{"id": "50231947", "url": "https://en.wikipedia.org/wiki?curid=50231947", "title": "Mycroft (software)", "text": "Mycroft (software)\n\nMycroft is a free and open-source voice assistant for Linux-based operating systems that uses a natural language user interface. Its code was formerly copyleft, but is now under a permissive license.\n\nInspiration for Mycroft came when Ryan Sipes and Joshua Montgomery were visiting the Kansas City makerspace, where they came across a simple and basic intelligent virtual assistant project. They were interested in the technology, but did not like its inflexibility. Montgomery believes that the burgeoning industry of intelligent personal assistance poses privacy concerns for users and has promised that Mycroft will protect privacy through its open source machine learning platform. Mycroft has won several awards including the prestigious Techweek's KC Launch competition in 2016. Mycroft was part of the Sprint Accelerator 2016 class in Kansas City and joined 500 Startups Batch 20 in February 2017. The company accepted a strategic investment from Jaguar Land Rover during this same time period. To date, the company has raised more than $2.5 million from institutional investors and has opted to offer shares of the company to the public through Startengine, an equity crowdfunding platform. It is named after a fictional computer from 1966 science fiction novel \"The Moon Is a Harsh Mistress\".\n\nMycroft provides open source software for most parts of the voice stack. \n\nMycroft does Wake Word spotting, also called keyword spotting, through its \"Precise\" Wake Word engine. Prior to Precise becoming the default Wake Word engine, Mycroft employed PocketSphinx. Instead of being based on phoneme recognition, Precise uses a trained recurrent neural network to distinguish between sounds which are, and which aren't, Wake Words. \n\nMycroft is partnering with Mozilla's Common Voice Project to leverage their DeepSpeech speech to text software. \n\nMycroft uses an intent parser called Adapt to convert natural language into machine-readable data structures. Adapt undertakes intent parsing by matching specific keywords in an order within an utterance. They also have a parser, \"Padatious\". Padatious, in contrast, uses example-based inference to determine intent. \n\nFor speech synthesis Mycroft uses Mimic, which is based on the Festival Lite speech synthesis system. \n\nMycroft is designed to be modular, so users are able to change its components. For example, espeak can be used instead of Mimic.\n\nThe Mycroft project is also working on and selling smart speakers that run its software. All of its hardware is open-source, released under the CERN Open Hardware Licence.\n\nIts first hardware project was the Mark I, targeted primarily at developers. Its production was partially funded through a Kickstarter campaign, which finished successfully. Units started shipping out in April 2016.\n\nIts most recent hardware project is the Mark II, intended for general usage, not just for developers. Unlike the Mark I, the Mark II is equipped with a screen, being able to relay information both visually as well as acoustically. As with the Mark I, the Mark II's production will be partially funded through a Kickstarter campaign, which wrapped up in February 2018, hitting almost 8 times its original goal.\n\nMycroft announced that a third hardware project, Mark III, will be offered through Kickstarter, and that an entire product line of Mark I, II, and III will be released to stores by November, 2019.\n\nMycroft has undertaken several commercial collaborations. In May 2018, the company partnered with WorkAround, an impact sourcing provider who broker work opportunities for refugees, to undertake bulk machine learning training. In October 2018, Mycroft collaborated with disease surveillance and forecasting company, SickWeather, to identify the frequency of coughing on public transport, funded by the City of Kansas City, Missouri.\n\n"}
{"id": "21924889", "url": "https://en.wikipedia.org/wiki?curid=21924889", "title": "Nanopunk", "text": "Nanopunk\n\nNanopunk refers to an emerging subgenre of science fiction still very much in its infancy in comparison to its ancestor-genre cyberpunk and some of its other derivatives.\n\nThe genre is especially similar to biopunk, but describes a world where nanites are widely in use and nanotechnologies the predominant technological forces in society.\n\nCurrently the genre is mainly concerned with the artistic, psychological and especially societal impact of nanotechnology, rather than aspects of the technology which itself is still in its infancy. Unlike the cyberpunk which can be distinguished by a gritty and low-life yet technologically advanced character, nanopunk can have a darker dystopian character that might examine potential risks by nanotechnology as well a more optimistic outlook that might emphasize potential uses of nanotechnology.\n\n\n\n\n\n"}
{"id": "216192", "url": "https://en.wikipedia.org/wiki?curid=216192", "title": "New product development", "text": "New product development\n\nIn business and engineering, new product development (NPD) covers the complete process of bringing a new product to market. A central aspect of NPD is product design, along with various business considerations. New product development is described broadly as the transformation of a market opportunity into a product available for sale. The product can be tangible (something physical which one can touch) or intangible (like a service, experience, or belief), though sometimes services and other processes are distinguished from \"products.\" NPD requires an understanding of customer needs and wants, the competitive environment, and the nature of the market.\nCost, time and quality are the main variables that drive customer needs. Aiming at these three variables, companies develop continuous practices and strategies to better satisfy customer requirements and to increase their own market share by a regular development of new products. There are many uncertainties and challenges which companies must face throughout the process. The use of best practices and the elimination of barriers to communication are the main concerns for the management of the NPD .\n\nThe product development process typically consists of several activities that firms employ in the complex process of delivering new products to the market. A process management approach is used to provide a structure. Product development often overlaps much with the engineering design process, particularly if the new product being developed involves application of math and/or science. Every new product will pass through a series of stages/phases, including ideation among other aspects of design, as well as manufacturing and market introduction. In highly complex engineered products (e.g. aircraft, automotive, machinery), the NPD process can be likewise complex regarding management of personnel, milestones and deliverables. Such projects typically use an integrated product team approach. The process for managing large-scale complex engineering products is much slower (often 10-plus years) than that deployed for many types of consumer goods.\n\nThe product development process is articulated and broken down in many different ways, many of which often include the following phases/stages:\n\nThe front-end marketing phases have been very well researched, with valuable models proposed. Peter Koen et al. provides a five-step front-end activity called front-end innovation: opportunity identification, opportunity analysis, idea genesis, idea selection, and idea and technology development. He also includes an engine in the middle of the five front-end stages and the possible outside barriers that can influence the process outcome. The engine represents the management driving the activities described. The front end of the innovation is the greatest area of weakness in the NPD process. This is mainly because the FFE is often chaotic, unpredictable and unstructured.\nEngineering design is the process whereby a technical solution is developed iteratively to solve a given problem\nThe design stage is very important because at this stage most of the product life cycle costs are engaged. Previous research shows that 70–80% of the final product quality and 70% of the product entire life-cycle cost are determined in the product design phase, therefore the design-manufacturing interface represent the greatest opportunity for cost reduction.\nDesign projects last from a few weeks to three years with an average of one year. Design and Commercialization phases usually start a very early collaboration. When the concept design is finished it will be sent to manufacturing plant for prototyping, developing a Concurrent Engineering approach by implementing practices such as QFD, DFM/DFA and more.\nThe output of the design (engineering) is a set of product and process specifications – mostly in the form of drawings, and the output of manufacturing is the product ready for sale. Basically, the design team will develop drawings with technical specifications representing the future product, and will send it to the manufacturing plant to be executed. Solving product/process fit problems is of high priority in information communication design because 90% of the development effort must be scrapped if any changes are made after the release to manufacturing.\n\n\nConceptual models have been designed in order to facilitate a smooth process. The concept adopted by IDEO, a successful design and consulting firm, is one of the most researched processes in regard to new product development and is a five-step procedure. These steps are listed in chronological order:\nOne of the first developed models that today companies still use in the NPD process is the Booz, Allen and Hamilton (BAH) Model, published in 1982. This is the best known model because it underlies the NPD systems that have been put forward later. This model represents the foundation of all the other models that have been developed afterwards. Significant work has been conducted in order to propose better models, but in fact these models can be easily linked to BAH model. The seven steps of BAH model are: new product strategy, idea generation, screening and evaluation, business analysis, development, testing, and commercialization. \nA pioneer of NPD research in the consumers goods sector is Robert G. Cooper. Over the last two decades he conducted significant work in the area of NPD. The Stage-Gate model developed in the 1980s was proposed as a new tool for managing new products development processes. This was mainly applied to the consumers goods industry. The 2010 APQC benchmarking study reveals that 88% of U.S. businesses employ a stage-gate system to manage new products, from idea to launch. In return, the companies that adopt this system are reported to receive benefits such as improved teamwork, improved success rates, earlier detection of failure, a better launch, and even shorter cycle times – reduced by about 30%. These findings highlight the importance of the stage-gate model in the area of new product development. \nOver the last few years, the Lean Startup movement has grown in popularity, challenging many of the assumptions inherent in the stage-gate model.\n\nThere have been a number of approaches proposed for analyzing and responding to the marketing challenges of new product development. Two of these are \"the eight stages\" process of Peter Koen of the Stevens Institute of Technology, and a process known as \"the fuzzy front end.\"\n\n\"The Fuzzy Front End\" (FFE) is the messy \"getting started\" period of new product engineering development processes. It is also referred to as the \"Front End of Innovation\", or \"Idea Management\".\n\nIt is in the front end where the organization formulates a concept of the product to be developed and decides whether or not to invest resources in the further development of an idea. It is the phase between first consideration of an opportunity and when it is judged ready to enter the structured development process (Kim and Wilemon, 2007; Koen et al., 2001).\nIt includes all activities from the search for new opportunities through the formation of a germ of an idea to the development of a precise concept. The Fuzzy Front End phase ends when an organization approves and begins formal development of the concept.\n\nAlthough the Fuzzy Front End may not be an expensive part of product development, it can consume 50% of development time (see Chapter 3 of the Smith and Reinertsen reference below), and it is where major commitments are typically made involving time, money, and the product's nature, thus setting the course for the entire project and final end product. Consequently, this phase should be considered as an essential part of development rather than something that happens \"before development,\" and its cycle time should be included in the total development cycle time.\n\nKoen et al. (2001), distinguish five different front-end elements (not necessarily in a particular order):\n\nA universally acceptable definition for Fuzzy Front End or a dominant framework has not been developed so far.\nIn a glossary of PDMA, it is mentioned that the Fuzzy Front End generally consists of three tasks: strategic planning, idea generation, and pre-technical evaluation. These activities are often chaotic, unpredictable, and unstructured. In comparison, the subsequent new product development process is typically structured, predictable, and formal.\nThe term \"Fuzzy Front End \" was first popularized by Smith and Reinertsen (1991).\nR.G. Cooper (1988) it describes the early stages of NPPD as a four-step process in which ideas are generated (I), subjected to a preliminary technical and market assessment (II) and merged to coherent product concepts (III) which are finally judged for their fit with existing product strategies and portfolios (IV). \n\nOther authors have divided predevelopment product development activities differently. \n\nThe Stage-Gate model of NPD predevelopment activities are summarised in Phase zero and one, in respect to earlier definition of predevelopment activities:\n\n\nThese activities yield essential information to make a Go/No-Go to Development decision. These decisions represent the Gates in the Stage-Gate model.\n\nA conceptual model of Front-End Process was proposed which includes early phases of the innovation process. This model is structured in three phases and three gates:\n\n\nThe gates are:\n\n\nThe final gate leads to a dedicated new product development project. Many professionals and academics consider that the general features of Fuzzy Front End (fuzziness, ambiguity, and uncertainty) make it difficult to see the FFE as a structured process, but rather as a set of interdependent activities ( e.g. Kim and Wilemon, 2002). However, Husig et al., 2005 [10] argue that front-end not need to be fuzzy, but can be handled in a structured manner. In fact Carbone showed that when using the front end success factors in an integrated process, product success is increased. Peter Koen argues that in the FFE for incremental, platform and radical projects, three separate strategies and processes are typically involved. The traditional Stage Gate (TM) process was designed for incremental product development, namely for a single product. The FFE for developing a new platform must start out with a strategic vision of where the company wants to develop products and this will lead to a family of products. Projects for breakthrough products start out with a similar strategic vision, but are associated with technologies which require new discoveries.\n\nPredevelopment is the initial stage in NPD and consists of numerous activities, such as:\n\n\nEconomical analysis, benchmarking of competitive products and modeling and prototyping are also important activities during the front-end activities.\n\nThe outcomes of FFE are the:\n\n\nIncremental, platform and breakthrough products include:\n\n\n Companies must take a holistic approach to managing this process and must continue to innovate and develop new products if they want to grow and prosper.\n\n\n"}
{"id": "159284", "url": "https://en.wikipedia.org/wiki?curid=159284", "title": "Novartis", "text": "Novartis\n\nNovartis International AG is a Swiss multinational pharmaceutical company based in Basel, Switzerland. It is one of the largest pharmaceutical companies by both market capitalization and sales.\n\nNovartis manufactures the drugs clozapine (Clozaril), diclofenac (Voltaren), carbamazepine (Tegretol), valsartan (Diovan),\nimatinib mesylate (Gleevec/Glivec), ciclosporin (Neoral/Sandimmun), letrozole (Femara), methylphenidate (Ritalin), terbinafine (Lamisil), and others.\n\nIn 1996, Ciba-Geigy merged with Sandoz; the pharmaceutical and agrochemical divisions of both companies formed Novartis as an independent entity. Other Ciba-Geigy and Sandoz businesses were sold, or, like Ciba Specialty Chemicals, spun off as independent companies. The Sandoz brand disappeared for three years, but was revived in 2003 when Novartis consolidated its generic drugs businesses into a single subsidiary and named it Sandoz. Novartis divested its agrochemical and genetically modified crops business in 2000 with the spinout of Syngenta in partnership with AstraZeneca, which also divested its agrochemical business.\n\nNovartis is a full member of the European Federation of Pharmaceutical Industries and Associations (EFPIA), the International Federation of Pharmaceutical Manufacturers and Associations (IFPMA), and the Pharmaceutical Research and Manufacturers of America (PhRMA).\n\nNovartis AG is a publicly traded Swiss holding company that operates through the Novartis Group. Novartis AG owns, directly or indirectly, all companies worldwide that operate as subsidiaries of the Novartis Group.\n\nNovartis's businesses are divided into three operating divisions: Pharmaceuticals, Alcon (eye care) and Sandoz (generics). Novartis operates directly and through dozens of subsidiaries in countries around the world, each of which fall under one of the divisions, and that Novartis categorizes as fulfilling one or more of the following functions: \"Holding/Finance: the entity is a holding company and/or performs finance functions for the Group; Sales: the entity performs sales and marketing activities for the Group; Production: the entity performs manufacturing and/or production activities for the Group; and Research: the entity performs research and development activities for the Group.\"\n\nNovartis AG also holds 33.3% of the shares of Roche however, it does not exercise control over Roche. Novartis also owned 24.9% of Idenix Pharmaceuticals prior to its sale to Merck & Co, Inc. Novartis also has two significant license agreements with Genentech, a Roche subsidiary. One agreement is for Lucentis; the other is for Xolair, both of which Novartis markets outside the US.\n\nNovartis has established a multi-functional center in Hyderabad, India, in order to offshore several of its R&D, clinical development, medical writing and administrative functions. The global service centere began in 2001 with 17 people; Hyderabad was chosen from a shortlist of 23 cities, including Pune, Chennai and Gurgaon. The center supports the drug major’s operations in the pharmaceuticals (Novartis), eye care (Alcon) and generic drugs segments (Sandoz). This centre covers more than 870,000 square feet - large enough to house 8000 people.\n\nOverall, Novartis was the world's second largest pharmaceutical company in 2011. An IMS Health report ranked Novartis as the biggest pharma company in 2012.\n\nAlcon: Alcon was already the world's largest and most profitable eye care company when Novartis bought it, with 2009 annual sales of $6.5 billion and net income of $2 billion. At that time, Novartis stated that it believed the two companies could generate some $200 million of potential annual pre-tax cost synergies.\n\nSandoz: , Sandoz was the world's second-largest generic drug company, contributing US$1.09 billion to Novartis' operating profit on US$8.70 billion in revenue in 2012. Sandoz' biosimilars leads its field, getting the first biosimilar approvals in the EU.\n\nVaccines and Diagnostics Division: Novartis was considering selling this division off. While \"sales in the unit were up 14% for the first half of 2013, it reported an operating loss of $240 million in the first half of 2013 after a $250 million loss for all 2012... Vaccine revenue was $1.4 billion in 2012 and has been forecast to more than double to $3.14 billion by 2018.\"\n\nConsumer: Novartis is not a leader in the over-the-counter or animal health segments; its leading OTC brands are Excedrin and Theraflu, but sales have been slowed by problems at its key US manufacturing plant.\n\nIn 2012, Novartis ranked 7th on the Access to Medicine Index, which \"ranks companies on how readily they make their products available to the world’s poor.\" In 2010, Novartis was in the top three pharma companies (as it was in 2008).\n\nFor the fiscal year 2017, Novartis reported earnings of US$7.7 billion, with an annual revenue of US$50.1 billion, an increase of 1.4% over the previous fiscal cycle. Novartis shares traded at over $77 per share, and its market capitalization was valued at over US$197.7 billion in September 2018.\nNovartis was created in 1996 from the merger of Ciba-Geigy and Sandoz Laboratories, both Swiss companies with long histories. Ciba-Geigy was formed in 1970 by the merger of J. R. Geigy Ltd (founded in Basel in 1758) and CIBA (founded in Basel in 1859). Combining the histories of the merger partners, the company's effective history spans 250 years.\n\nIn 1859, Alexander Clavel (1805–1873) took up the production of fuchsine in his factory for silk-dyeing works in Basel. In 1864, a new site for the production of synthetic dyes was constructed, and in 1873, Clavel sold his dye factory to the new company Bindschedler and Busch. In 1884, Bindschedler and Busch was transformed into a joint-stock company with the name \"Gesellschaft für Chemische Industrie Basel\" (Company for Chemical Industry Basel). The acronym, CIBA, was adopted as the company's name in 1945.\n\nJohann Rudolf Geigy-Gemuseus (1733–1793) began trading in 1758 in \"materials, chemicals, dyes and drugs of all kinds\" in Basel, Switzerland. Johann Rudolf Geigy-Merian (1830–1917) and Johann Muller-Pack acquired a site in Basel in 1857, where they built a dyewood mill and a dye extraction plant. Two years later, they began the production of synthetic fuchsine. In 1901, they formed the public limited company Geigy and the name of the company was changed to J. R. Geigy Ltd in 1914. In 1925, J. R. Geigy Ltd. began producing textile auxiliaries, an activity which Ciba took up in 1928. In 1939, Geigy chemist Paul Hermann Müller discovered that DDT was effective against malaria-bearing insects. He received the 1948 Nobel Prize in Medicine for this work.\n\nCIBA and Geigy merged in 1971 to form Ciba‑Geigy Ltd. . In the United States, the Geigy staff relocated to join the CIBA staff at its American headquarters for research in Ardsley, New York. In 1980, Ciba-Geigy set up the company, Ciba Vision, to enter the contact lens market. In 1992 Ciba-Geigy agreed to pay New Jersey $62 million for illegal waste dumping.\n\nBefore the 1996 merger with Ciba-Geigy to form Novartis, Sandoz Pharmaceuticals (Sandoz AG) was a pharmaceutical company headquartered in Basel, Switzerland (as was Ciba-Geigy), and was best known for developing drugs such as Sandimmune for organ transplantation, the antipsychotic Clozaril, Mellaril Tablets and Serentil Tablets for treating psychiatric disorders, and Cafergot Tablets and Torecan Suppositories for treating migraine headaches.\n\nThe \"Chemiefirma Kern und Sandoz\" (\"Kern and Sandoz Chemistry Firm\") was founded in 1886 by Alfred Kern (1850–1893) and Edouard Sandoz (1853–1928). The first dyes manufactured by them were alizarinblue and auramine. After Kern's death, the partnership became the corporation \"Chemische Fabrik vormals Sandoz\" in 1895. The company began producing the fever-reducing drug antipyrin in the same year. In 1899, the company began producing the sugar substitute saccharin. Further pharmaceutical research began in 1917 under Arthur Stoll (1887–1971), who is the founder of Sandoz's pharmaceutical department in 1917. In 1918, Arthur Stoll isolated ergotamine from ergot; the substance was eventually used to treat migraine and headaches and was introduced under the trade name Gynergen in 1921.\n\nBetween the World Wars, Gynergen (1921) and Calcium-Sandoz (1929) were brought to market. Sandoz also produced chemicals for textiles, paper, and leather, beginning in 1929. In 1939, the company began producing agricultural chemicals.\n\nThe psychedelic effects of lysergic acid diethylamide (LSD) were discovered at the Sandoz laboratories in 1943 by Arthur Stoll and Albert Hofmann. Sandoz began clinical trials and marketed the substance, from 1947 through the mid-1960s, under the name \"Delysid\" as a psychiatric drug, thought useful for treating a wide variety of mental ailments, ranging from alcoholism to sexual deviancy. Sandoz suggested in its marketing literature that psychiatrists take LSD themselves, to gain a better subjective understanding of the schizophrenic experience, and many did exactly that and so did other scientific researchers. The Sandoz product received mass publicity as early as 1954, in a Time Magazine feature. Research on LSD peaked in the 1950s and early 1960s. Sandoz withdrew the drug from the market in the mid-1960s. The drug became a cultural novelty of the 1960s after psychologist Timothy Leary at Harvard University began to promote its use for recreational and spiritual experiences among the general public.\n\nSandoz opened its first foreign offices in 1964. In 1967, Sandoz merged with Wander AG (known for Ovomaltine and Isostar). Sandoz acquired the companies Delmark, Wasabröd (a Swedish manufacturer of crisp bread), and Gerber Products Company (a baby food company). On 1 November 1986, a fire broke out in a production plant storage room, which led to the Sandoz chemical spill and a large amount of pesticide being released into the upper Rhine river. This exposure killed many fish and other aquatic life. In 1995, Sandoz spun off its specialty chemicals business to form Clariant. In 1997, Clariant merged with the specialty chemicals business that was spun off from Hoechst AG in Germany.\n\nIn 1996 Ciba-Geigy merged with Sandoz, with the pharmaceutical and agrochemical divisions of both staying together to form Novartis. Other Ciba-Geigy and Sandoz businesses were spun off as independent companies. Notably, Ciba Specialty Chemicals was spun out as an independent company, and Sandoz's Master Builders Technologies, a producer of chemicals for the construction industry, (was sold off) to SKW Trostberg A.G., a subsidiary of the German energy company Viag, and its North American corn herbicide business (was sold off) to the German chemical maker BASF A.G.\"\n\nIn 1998, the company made headlines with its biotechnology licensing agreement with the University of California at Berkeley Department of Plant and Microbial Biology. Critics of the agreement expressed concern over prospects that the agreement would diminish academic objectivity, or lead to the commercialization of genetically modified plants. The agreement expired in 2003.\n\nIn 2000, Novartis and AstraZeneca combined their agrobusiness divisions to create a new company, Syngenta.\n\nIn 2003, Novartis organized all its generics businesses into one division, and merged some of its subsidiaries into one company, reusing the predecessor brand name of Sandoz.\n\nIn 2005, Novartis expanded its subsidiary Sandoz significantly through the US$8.29 billion acquisition of Hexal, one of Germany's leading generic drug companies, and Eon Labs, a fast-growing United States generic pharmaceutical company.\n\nIn 2006, Novartis acquired the California-based Chiron Corporation. Chiron had been divided into three units: Chiron Vaccines, Chiron Blood Testing, and Chiron BioPharmaceuticals. The biopharmaceutical unit was integrated into Novartis Pharmaceuticals, while the vaccines and blood testing units were made into a new Novartis Vaccines and Diagnostics division. Also in 2006, Sandoz became the first company to have a biosimilar drug approved in Europe with its recombinant human growth hormone drug.\n\nIn 2007, Novartis sold the Gerber Products Company to Nestlé as part of its continuing effort to shed old Sandoz and Ciba-Geigy businesses and focus on healthcare.\n\nIn 2009, Novartis reached an agreement to acquire an 85% stake in the Chinese vaccines company Zhejiang Tianyuan Bio-Pharmaceutical Co., Ltd. as part of a strategic initiative to build a vaccines industry leader in this country and expand the group's limited presence in this fast-growing market segment. This proposed acquisition will require government and regulatory approvals in China.\n\nIn 2010, Novartis offered to pay US $39.3 billion to fully acquire Alcon, the world's largest eye-care company, including a majority stake held by Nestlé. Novartis had bought 25% of Alcon in 2008. Novartis created a new division and called it Alcon, under which it placed its CIBA VISION subsidiary and Novartis Ophthalmics, which became the second-largest division of Novartis. The total cost for Alcon amounted to $60 billion.\nIn 2011, Novartis acquired the medical laboratory diagnostics company Genoptix to \"serve as a strong foundation for our (Novartis') individualized treatment programs\".\n\nIn 2012, the Company cut ~2000 positions in the United States, most in sales, in response to anticipated revenue downturns from the hypertension drug Diovan, which was losing patent protection, and the realization that the anticipated successor to Diovan, Rasilez, was failing in clinical trials. The 2012 personnel reductions follow ~2000 cut positions in Switzerland and the United States in 2011, ~1400 cut positions in the United States in 2010, and a reduction of \"thousands\" and several site closures in previous years. Also in 2012, Novartis became the biggest manufacturer of generic skin care medicine, after agreeing to buy Fougera Pharmaceuticals for $1.525 billion in cash.\n\nIn 2013, the Indian Supreme Court issued a decision rejecting Novartis' patent application in India on the final form of Gleevec, Novartis's cancer drug; the case caused great controversy. In 2013, Novartis was sued again by the US government, this time for allegedly bribing doctors for a decade so that their patients are steered towards the company's drugs.\n\nIn January 2014, Novartis announced plans to cut 500 jobs from its pharmaceuticals division. In February 2014, Novartis announced that it acquired CoStim Pharmaceuticals. In May 2014, Novartis bought the rights to market Ophthotech's Fovista (an anti-PDGF aptamer, also being investigated for use in combination with anti-VEGF treatments) outside the United States for up to $1 billion. Novartis will acquire exclusive rights to market the eye drug outside of America while retaining US marketing rights. The company agreed to pay Ophthotech $200 million upfront, and $130 million in milestone payments relating to Phase III trials. Ophthotech is also eligible to receive up to $300 million dependent upon future marketing approval milestones outside of America and up to $400 million relating to sales milestones. In September 2014, Ophthotech received its first $50 million phase III trial milestone payment from Novartis. In April 2014, Novartis announced that it would acquire GlaxoSmithKline's cancer drug business for $16 billion as well as selling its vaccines business to GlaxoSmithKline for $7.1 billion. In August 2014 \"Genetic Engineering & Biotechnology News\" reported that Novartis had acquired a 15% stake in Gamida Cell for $35 million, with the option to purchase the whole company for approximately $165 million. In October 2014, Novartis announced its intention to sell its influenza vaccine business (inclusive of its development pipeline), subject to regulatory approval, to CSL for $275 million.\n\nIn March 2015, the company announced BioPharma had completed its acquisition of two Phase III cancer-drug candidates; the MEK inhibitor binimetinib (MEK 162) and the BRAF inhibitor encorafenib (LGX818), for $85 million. Further, the company sold its RNAi portfolio to Arrowhead Research for $10 million and $25 million in stock. In June, the company announced it would acquire Spinifex Pharmaceuticals for more than $200 million. In August, the company acquired the remaining rights to the CD20 monoclonal antibody Ofatumumab from GlaxoSmithKline for up to $1 billion. In October the company acquired Admune Therapeutics for an undisclosed sum, as well as licensing PBF-509, an adenosine A2A receptor antagonist which is in Phase I clinical trials for non-small cell lung cancer, from Palobiofarma.\n\nIn November 2016, the company announced it would acquire Selexys Pharmaceuticals for $665 million. In December, the company acquired Encore Vision, gaining the company's principle compound, EV06, is a first-in-class topical therapy for presbyopia. In December Novartis acquired Ziarco Group Limited, bolstering its presence in eczema treatments.\n\nIn late October 2017, \"Reuters\" announced that Novartis would acquire Advanced Accelerator Applications for $3.9 billion, paying $41 per ordinary share and $82 per American depositary share representing a 47 percent premium.\n\nIn March 2018, GlaxoSmithKline announced that it has reached an agreement with Novartis to acquire Novartis’ 36.5% stake in their Consumer Healthcare Joint Venture for $13 billion (£9.2 billion). In April of the same year, the business utilised some of the proceeds from the aforementioned GlaxoSmithKline deal to acquire Avexis for $218 per share or $8.7 billion in total, gaining the lead compound AVXS-101 used to treat spinal muscular atrophy. In August 2018, Novartis signed a deal with Laekna-a Shanghai-based pharmaceutical company for its two clinical-stage cancer drugs. Novartis gave Laekna the exclusive international rights for the drugs that are oral pan-Akt kinase inhibitors namely; afuresertib (ASB138) and uprosertib (UPB795). In mid-October, the company announced it would acquire Endocyte Inc for $2.1 billion ($24 per share) merging it with a newly created subsidiary. Endocyte will bolster Novartis' offering in its radiopharmaceuticals business, with Endocyte's first in class candidate Lu-PSMA-617 being targeted against metastatic castration-resistant prostate cancer.\n\nThe following is an illustration of the company's major mergers and acquisitions and historical predecessors (this is not a comprehensive list):\n\nThe company's global research operations, called \"Novartis Institutes for BioMedical Research (NIBR)\" have their global headquarters in Cambridge, Massachusetts, United States. Two research institutes reside within NIBR that focus on diseases in the developing world: Novartis Institute for Tropical Diseases, which works on tuberculosis, dengue, and malaria, and Novartis Vaccines Institute for Global Health, which works on salmonella typhi (typhoid fever) and shigella.\n\nNovartis is also involved in publicly funded collaborative research projects, with other industrial and academic partners. One example in the area of non-clinical safety assessment is the InnoMed PredTox project. The company is expanding its activities in joint research projects within the framework of the Innovative Medicines Initiative of EFPIA and the European Commission.\n\nNovartis is working with Science 37 in order to allow video based telemedicine visits instead of physical traveling to clinics for patients. It is planning for ten clinical trials over three years using mobile technology to help free patients from burdensome hospital trips.\n\nAn ongoing Basel Campus Project has the aim to transform Novartis headquarters in Basel \"from an industrial complex to a place of innovation, knowledge, and encounter\". The pharmaceutical giant decided to transform the existing Sandoz office buildings and chemical factories of its headquarters in 2001.\n\nIn 1999, PWP Landscape Architecture won the competition for a landscape master plan that would transform a 51-acre site beside the Rhine River from a paved industrial landscape crisscrossed with train tracks into a modern—and largely pedestrian—research and administrative campus filled with outdoor art, trees, greens, and parks. The plan also dealt with an extensive network of existing underground infrastructure.\n\nThe buildings were gradually demolished and replaced with works by architects and artists of international stature. Frank Gehry, Rafael Moneo, and from SANAA, Kazuyo Sejima and Ryue Nishizawa were among the architects and Jenny Holzer and Richard Serra among the artists. Marked diversity of forms now dominates the campus. Novel features and technologies were introduced by Gehry to conform to the building standards of the Swiss government that prohibit air-conditioning, while still selecting a contemporary style of massive use of glass exteriors. One adaptation by the architect includes the integration of a building vent, teepee-style, through the roof, which creates a chimney effect that draws cool air in at the lower levels and vents warmer air.\n\nIn January 2009, the United States Department of Health and Human Services awarded Novartis a $486 million contract for construction of the first US plant to produce cell-based influenza vaccine, to be located in Holly Springs, North Carolina. The stated goal of this program is the capability of producing 150,000,000 doses of pandemic vaccine within six months of declaring a flu pandemic.\n\nIn April 2014, Novartis divested its consumer health section with $3,5 billion worth of assets into a new joint venture with GlaxoSmithkline, named GSK Consumer Healthcare, of which Novartis will hold a 36,5% stake. In March 2018, GSK announced that it has reached an agreement with Novartis to acquire Novartis’ 36.5% stake in their Consumer Healthcare Joint Venture for $13 billion (£9.2 billion).\n\n\n\n\nNovartis fought a seven-year, controversial battle to patent Gleevec in India, and took the case all the way to the Indian Supreme Court, where the patent application was finally rejected. The patent application at the center of the case was filed by Novartis in India in 1998, after India had agreed to enter the World Trade Organization and to abide by worldwide intellectual property standards under the TRIPS agreement. As part of this agreement, India made changes to its patent law; the biggest of which was that prior to these changes, patents on products were not allowed, while afterwards they were, albeit with restrictions. These changes came into effect in 2005, so Novartis' patent application waited in a \"mailbox\" with others until then, under procedures that India instituted to manage the transition. India also passed certain amendments to its patent law in 2005, just before the laws came into effect, which played a key role in the rejection of the patent application.\n\nThe patent application claimed the final form of Gleevec (the beta crystalline form of imatinib mesylate). In 1993, during the time India did not allow patents on products, Novartis had patented imatinib, with salts vaguely specified, in many countries but could not patent it in India. The key differences between the two patent applications, were that 1998 patent application specified the counterion (Gleevec is a specific salt - imatinib mesylate) while the 1993 patent application did not claim any specific salts nor did it mention mesylate, and the 1998 patent application specified the solid form of Gleevec - the way the individual molecules are packed together into a solid when the drug itself is manufactured (this is separate from processes by which the drug itself is formulated into pills or capsules) - while the 1993 patent application did not. The solid form of imatinib mesylate in Gleevec is beta crystalline.\n\nAs provided under the TRIPS agreement, Novartis applied for Exclusive Marketing Rights (EMR) for Gleevec from the Indian Patent Office and the EMR was granted in November 2003. Novartis made use of the EMR to obtain orders against some generic manufacturers who had already launched Gleevec in India. Novartis set the price of Gleevec at USD 2666 per patient per month; generic companies were selling their versions at USD 177 to 266 per patient per month. Novartis also initiated a program to assist patients who could not afford its version of the drug, concurrent with its product launch.\n\nWhen examination of Novartis' patent application began in 2005, it came under immediate attack from oppositions initiated by generic companies that were already selling Gleevec in India and by advocacy groups. The application was rejected by the patent office and by an appeal board. The key basis for the rejection was the part of Indian patent law that was created by amendment in 2005, describing the patentability of new uses for known drugs and modifications of known drugs. That section, Paragraph 3d, specified that such inventions are patentable only if \"they differ significantly in properties with regard to efficacy.\" At one point, Novartis went to court to try to invalidate Paragraph 3d; it argued that the provision was unconstitutionally vague and that it violated TRIPS. Novartis lost that case and did not appeal. Novartis did appeal the rejection by the patent office to India's Supreme Court, which took the case.\n\nThe Supreme Court case hinged on the interpretation of Paragraph 3d. The Supreme Court decided that the substance that Novartis sought to patent was indeed a modification of a known drug (the raw form of imatinib, which was publicly disclosed in the 1993 patent application and in scientific articles), that Novartis did not present evidence of a difference in therapeutic efficacy between the final form of Gleevec and the raw form of imatinib, and that therefore the patent application was properly rejected by the patent office and lower courts.\n\nAlthough the court ruled narrowly, and took care to note that the subject application was filed during a time of transition in Indian patent law, the decision generated widespread global news coverage and reignited debates on balancing public good with monopolistic pricing, innovation with affordability etc.\n\nHad Novartis won and had its patent issued, it could not have prevented generics companies in India from selling generic Gleevec, but it could have obliged them to pay a reasonable royalty under a grandfather clause included in India's patent law.\n\nIn reaction to the decision, Ranjit Shahani, vice-chairman and managing director of Novartis India Ltd was quoted as saying \"This ruling is a setback for patients that will hinder medical progress for diseases without effective treatment options.\" He also said that companies like Novartis would invest less money in research in India as a result of the ruling. Novartis also emphasised that it continues to be committed to good access to its drugs; according to Novartis, by 2013, \"95% of patients in India—roughly 16,000 people—receive Glivec free of charge... and it has provided more than $1.7 billion worth of Glivec to Indian patients in its support program since it was started...\"\n\nOn 17 May 2010, a jury in the United States District Court for the Southern District of New York awarded $3,367,250 in compensatory damages against Novartis, finding that the company had committed sexual discrimination against twelve female sales representatives and entry-level managers since 2002, in matters of pay, promotion, and treatment after learning that the employees were pregnant. Two months later the company settled with the remaining plaintiffs for $152.5 million plus attorney fees.\n\nIn September 2008, the US Food and Drug Administration (FDA) sent a notice to Novartis Pharmaceuticals regarding its advertising of Focalin XR, an ADHD drug, in which the company overstated its efficacy while marketing to the public and medical professionals.\n\nIn 2005, federal prosecutors opened an investigation into Novartis' marketing of several drugs: Trileptal, an antiseizure drug; three drugs for heart conditions - Diovan (the company’s top-selling product), Exforge, and Tekturna; Sandostatin, a drug to treat a growth hormone disorder; and Zelnorm, a drug for irritable bowel syndrome. In September, 2010, Novartis agreed to pay US$422.5 million in criminal and civil claims and to enter into a corporate integrity agreement with the US Office of the Inspector General. According to \"The New York Times\" \"Federal prosecutors accused Novartis of paying illegal kickbacks to health care professionals through speaker programs, advisory boards, entertainment, travel and meals. But aside from pleading guilty to one misdemeanor charge of mislabeling in an agreement that Novartis announced in February, the company denied wrongdoing.\" In the same New York Times article, Frank Lichtenberg, a Columbia professor who receives pharmaceutical financing for research on innovation in the industry, said off-label prescribing was encouraged by the American Medical Association and paid for by insurers, but off-label marketing was clearly illegal. \"So it’s not surprising that they would settle because they don’t have a legal leg to stand on.\"\n\nIn April 2013, federal prosecutors filed two lawsuits against Novartis under the False Claims Act for off-label marketing and kickbacks; in both suits, prosecutors are seeking treble damages. The first suit \"accused Novartis of inducing pharmacies to switch thousands of kidney transplant patients to its immunosuppressant drug Myfortic in exchange for kickbacks disguised as rebates and discounts\". In the second, the Justice Department joined a \"qui tam\", or whistleblower, lawsuit brought by a former sales rep over off-label marketing of three drugs: Lotrel and Valturna (both hypertension drugs), and the diabetes drug, Starlix. Twenty-seven states, the District of Columbia and Chicago and New York also joined.\n\nOutside the US, Novartis markets the drug ranibizumab (trade name Lucentis), which is a monoclonal antibody fragment derived from the same parent mouse antibody as bevacizumab (Avastin). Both Avastin and Lucentis were created by Genentech which is owned by Roche; Roche markets Avastin worldwide, and also markets Lucentis in the US. Lucentis has been approved worldwide as a treatment for wet macular degeneration and other retinal disorders; Avastin is used to treat certain cancers. Because the price of Lucentis is much higher than Avastin, many ophthalmologists began having compounding pharmacies formulate Avastin for administration to the eye, and began treating their patients with Avastin. In 2011, four trusts of the National Health Service in the UK issued policies approving use and payment for administering Avastin for macular degeneration, in order to save money, even though Avastin had not been approved for that indication. In April 2012, after failing to persuade the trusts that it was uncertain whether Avastin was as safe and effective as Lucentis, and in order to retain the market for Lucentis, Novartis announced it would sue the trusts. However, in July Novartis offered significant discounts (kept confidential) to the trusts, and the trusts agreed to change their policy, and in November, Novartis dropped the litigation.\n\nIn the summer of 2013, two Japanese universities retracted several publications of clinical trials that purported to show that Valsartan (branded as Diovan) had cardiovascular benefits, when it was found that statistical analysis had been manipulated, and that a Novartis employee had participated in the statistical analysis but had not disclosed his relationship with Novartis but only his affiliation with Osaka City University, where he was a lecturer.\n\nIn January 2018, Novartis began being investigated by Greek authorities over allegations of bribery towards public officials in the 2006-2015 period. Two former prime ministers, former ministers served in the ministries of health and economy, and bankers are included in the case, while the current manager of Novartis is banned from leaving the country. The minister's deputy described the allegations as \"the biggest scandal since the creation of the Greek state\", which caused \"annual state expenditure on medicine to explode\". Most of the ministers involved in the scandal have denied the allegations, calling the case \"political targeting\" and \"bullying\". Besides bribery that involves artificial increases in the price of several medicines, the case also involves money laundering, with suspicions of \"illegal funds of more than four billion euros ($4.2 billion)\" were involved.\n\nNovartis paid $1.2 million to Essential Consultants, an entity owned by Michael Cohen, following the 2017 inauguration of Donald Trump. Cohen was paid monthly, with each payment just under $100,000. Novartis claims it paid Cohen to help it understand and influence the new administration's approach to drug pricing and regulation.\n\nIn July 2018, the US Senate committee report \"White House Access for Sale\" revealed that Novartis Ag's relationship with Cohen was \"longer and more detailed\". Novartis initially stated that the relationship ceased a month after entering the USD 1.2 million contract with Cohen's consulting firm since the consultants were not able to provide the information the pharmaceutical company needed. Later, it became clear, however, that Joe Jimenez and Cohen communicated via email multiple times during 2017, which included ideas to lower drug prices to be discussed with the president. According to the report, several of the ideas appeared later in Trump's drug pricing plan, released in early 2018, in which pharmaceutical companies were protected from reduced revenues.\n\n"}
{"id": "3770363", "url": "https://en.wikipedia.org/wiki?curid=3770363", "title": "Nuclear pumped laser", "text": "Nuclear pumped laser\n\nA nuclear pumped laser is a laser pumped with the energy of fission fragments. The lasing medium is enclosed in a tube lined with uranium-235 and subjected to high neutron flux in a nuclear reactor core. The fission fragments of the uranium create excited plasma with inverse population of energy levels, which then lases. Other methods, e.g. the He-Ar laser, can use the He(n,p)H reaction, the transmutation of helium-3 in a neutron flux, as the energy source, or employing the energy of the alpha particles.\n\nThis technology may achieve high excitation rates with small laser volumes.\n\nSome example lasing media:\n\n\nResearch in nuclear pumped lasers started in the early 1970s when researchers were unable to produce a laser with a wavelength shorter than 110 nm with the end goal of creating an x-ray laser. When laser wavelengths become that short the laser requires a huge amount of energy which must also be delivered in an extremely short period of time. In 1975 it was estimated, by George Chapline and Lowell Wood from the Lawrence Livermore National Laboratory, that “pumping a 10-keV (0.12-nm) laser would require around a watt per atom” in a pulse that was “10 seconds x the square of the wavelength in angstroms.” As this problem was unsolvable with the materials at hand and a laser oscillator was not working, research moved to creating pumps that used excited plasma. Early attempts used high-powered lasers to excite the plasma to create an even more highly powered laser. Results using this method were unsatisfying, and fell short of the goal. Livermore scientists first suggested using a nuclear reaction as a power source in 1975. By 1980 Livermore considered both nuclear bombs and nuclear reactors as viable energy sources for an x-ray laser. \nOn November 14, 1980, the first successful test of the bomb-powered x-ray laser was conducted. The use of a bomb was initially supported over that of the reactor driven laser because it delivered a more intense beam. Livermore’s research was almost entirely devoted to missile defense using x-ray lasers. The idea was to mount a system of nuclear bombs in space where these bombs would each power approximately 50 lasers. Upon detonation these lasers would fire and theoretically destroy several dozen incoming nuclear missiles at once. Opponents of this plan found many faults in such an approach and questioned aspects such as the power, range, accuracy, politics, and cost of such deployments. In 1985 a test titled ‘Goldstone’ revealed the delivered power to be less than believed. Efforts to focus the laser also failed.\n\nFusion lasers (reactor driven lasers) started testing after the bomb-driven lasers proved successful. While prohibitively expensive (estimated at 30,000 dollars per test), research was easier in that tests could be performed several times a day and the equipment could be reused. In 1984, a test achieved wavelengths of less than 21 nm the closest to an official x-ray laser yet. (There are many definitions for an x-ray laser, some of which require a wavelength of less than 10 nm). The Livermore method was to remove the outer electrons in heavy atoms to create a “neon-like” substance. When presented at an American Physical Society meeting, the success of the test was shared by an experiment from Princeton University which was better in size, cost, measured wavelength, and amplification than Livermore’s test. Research has continued in the field of nuclear pumped lasers and it remains on the cutting edge of the field.\n\nAt least 3 uses for bomb pumped lasers have been proposed.\n\nLaser propulsion is an alternative method of propulsion ideal for launching objects into orbit, as this method requires less fuel, meaning less mass must be launched. A nuclear pumped laser is ideal for this operation. A launch using laser propulsion requires high intensity, short pulses, good quality, and a high power output. A nuclear pumped laser would theoretically be capable of meeting these requirements.\n\nThe characteristics of the nuclear pumped laser make it ideal for applications in deep-cut welding, cutting thick materials, the heat treating of metals, vapor deposition of ceramics, and the production of sub-micron sized particles.\nTitled Project Excalibur, the program was a part of President Reagan’s Strategic Defense Initiative. Livermore Laboratories conceived of the initial idea and Edward Teller developed and presented the idea to the president. Permission was granted to pursue the project though it has been reported Reagan was reluctant to incorporate nuclear devices in the nation’s plan against nuclear devices. While initial tests were promising, the results never reached acceptable levels. Later, lead scientists were accused of falsifying the reports. Project Excalibur was cancelled several years later.\n\n"}
{"id": "5716808", "url": "https://en.wikipedia.org/wiki?curid=5716808", "title": "Nudge (instant messaging)", "text": "Nudge (instant messaging)\n\nNudge, also known as buzz, is a feature of instant messaging software used to get the attention of another user, for example, by shaking the conversation window or playing a sound. The feature was first introduced in MSN Messenger 7.0, in 2005.\n\nXMPP extension protocol XEP-0224 calls this feature \"Attention\".\n\nIt is possible to change memory values of Windows Live Messenger to allow nudges to be sent unlimited with no time delay. This can be done manually with a memory editor such as Cheat Engine or with patching programs. Virtually all instant messaging programs that support MSNP (such as aMSN and Pidgin) allow this to be done without any extra hacks.\n\n"}
{"id": "49090", "url": "https://en.wikipedia.org/wiki?curid=49090", "title": "Ohm's law", "text": "Ohm's law\n\nOhm's law states that the current through a conductor between two points is directly proportional to the voltage across the two points. Introducing the constant of proportionality, the resistance, one arrives at the usual mathematical equation that describes this relationship:\n\nwhere is the current through the conductor in units of amperes, \"V\" is the voltage measured \"across\" the conductor in units of volts, and \"R\" is the resistance of the conductor in units of ohms. More specifically, Ohm's law states that the \"R\" in this relation is constant, independent of the current. Ohm's law is an empirical relation which accurately describes the conductivity of the vast majority of electrically conductive materials over many orders of magnitude of current. However some materials do not obey Ohm's law, these are called non-ohmic.\n\nThe law was named after the German physicist Georg Ohm, who, in a treatise published in 1827, described measurements of applied voltage and current through simple electrical circuits containing various lengths of wire. Ohm explained his experimental results by a slightly more complex equation than the modern form above (see History).\n\nIn physics, the term \"Ohm's law\" is also used to refer to various generalizations of the law; for example the vector form of the law used in electromagnetics and material science:\n\nwhere J is the current density at a given location in a resistive material, E is the electric field at that location, and \"σ\" (sigma) is a material-dependent parameter called the conductivity. This reformulation of Ohm's law is due to Gustav Kirchhoff.\n\nIn January 1781, before Georg Ohm's work, Henry Cavendish experimented with Leyden jars and glass tubes of varying diameter and length filled with salt solution. He measured the current by noting how strong a shock he felt as he completed the circuit with his body. Cavendish wrote that the \"velocity\" (current) varied directly as the \"degree of electrification\" (voltage). He did not communicate his results to other scientists at the time, and his results were unknown until Maxwell published them in 1879.\n\nFrancis Ronalds delineated “intensity” (voltage) and “quantity” (current) for the dry pile – a high voltage source – in 1814 using a gold-leaf electrometer. He found for a dry pile that the relationship between the two parameters was not proportional under certain meteorological conditions.\nOhm did his work on resistance in the years 1825 and 1826, and published his results in 1827 as the book \"Die galvanische Kette, mathematisch bearbeitet\" (\"The galvanic circuit investigated mathematically\"). He drew considerable inspiration from Fourier's work on heat conduction in the theoretical explanation of his work. For experiments, he initially used voltaic piles, but later used a thermocouple as this provided a more stable voltage source in terms of internal resistance and constant voltage. He used a galvanometer to measure current, and knew that the voltage between the thermocouple terminals was proportional to the junction temperature. He then added test wires of varying length, diameter, and material to complete the circuit. He found that his data could be modeled through the equation\nwhere \"x\" was the reading from the galvanometer, \"l\" was the length of the test conductor, \"a\" depended on the thermocouple junction temperature, and \"b\" was a constant of the entire setup. From this, Ohm determined his law of proportionality and published his results.\n\nIn modern notation we would write,\n\nwhere formula_5 is the open-circuit emf of the thermocouple, formula_6 is the internal resistance of the thermocouple and formula_7 is the resistance of the test wire. In terms of the length of the wire this becomes,\n\nwhere formula_9 is the resistance of the test wire per unit length. Thus, Ohm's coefficients are,\n\nOhm's law was probably the most important of the early quantitative descriptions of the physics of electricity. We consider it almost obvious today. When Ohm first published his work, this was not the case; critics reacted to his treatment of the subject with hostility. They called his work a \"web of naked fancies\" and the German Minister of Education proclaimed that \"a professor who preached such heresies was unworthy to teach science.\" The prevailing scientific philosophy in Germany at the time asserted that experiments need not be performed to develop an understanding of nature because nature is so well ordered, and that scientific truths may be deduced through reasoning alone. Also, Ohm's brother Martin, a mathematician, was battling the German educational system. These factors hindered the acceptance of Ohm's work, and his work did not become widely accepted until the 1840s. However, Ohm received recognition for his contributions to science well before he died.\n\nIn the 1850s, Ohm's law was known as such and was widely considered proved, and alternatives, such as \"Barlow's law\", were discredited, in terms of real applications to telegraph system design, as discussed by Samuel F. B. Morse in 1855.\n\nThe electron was discovered in 1897 by J. J. Thomson, and it was quickly realized that it is the particle (charge carrier) that carries electric currents in electric circuits. In 1900 the first (classical) model of electrical conduction, the Drude model, was proposed by Paul Drude, which finally gave a scientific explanation for Ohm's law. In this model, a solid conductor consists of a stationary lattice of atoms (ions), with conduction electrons moving randomly in it. A voltage across a conductor causes an electric field, which accelerates the electrons in the direction of the electric field, causing a drift of electrons which is the electric current. However the electrons collide with and scatter off of the atoms, which randomizes their motion, thus converting the kinetic energy added to the electron by the field to heat (thermal energy). Using statistical distributions, it can be shown that the average drift velocity of the electrons, and thus the current, is proportional to the electric field, and thus the voltage, over a wide range of voltages.\n\nThe development of quantum mechanics in the 1920s modified this picture somewhat, but in modern theories the average drift velocity of electrons can still be shown to be proportional to the electric field, thus deriving Ohm's law. In 1927 Arnold Sommerfeld applied the quantum Fermi-Dirac distribution of electron energies to the Drude model, resulting in the free electron model. A year later, Felix Bloch showed that electrons move in waves (Bloch waves) through a solid crystal lattice, so scattering off the lattice atoms as postulated in the Drude model is not a major process; the electrons scatter off impurity atoms and defects in the material. The final successor, the modern quantum band theory of solids, showed that the electrons in a solid cannot take on any energy as assumed in the Drude model but are restricted to energy bands, with gaps between them of energies that electrons are forbidden to have. The size of the band gap is a characteristic of a particular substance which has a great deal to do with its electrical resistivity, explaining why some substances are electrical conductors, some semiconductors, and some insulators.\n\nWhile the old term for electrical conductance, the mho (the inverse of the resistance unit ohm), is still used, a new name, the siemens, was adopted in 1971, honoring Ernst Werner von Siemens. The siemens is preferred in formal papers.\n\nIn the 1920s, it was discovered that the current through a practical resistor actually has statistical fluctuations, which depend on temperature, even when voltage and resistance are exactly constant; this fluctuation, now known as Johnson–Nyquist noise, is due to the discrete nature of charge. This thermal effect implies that measurements of current and voltage that are taken over sufficiently short periods of time will yield ratios of V/I that fluctuate from the value of R implied by the time average or ensemble average of the measured current; Ohm's law remains correct for the average current, in the case of ordinary resistive materials.\n\nOhm's work long preceded Maxwell's equations and any understanding of frequency-dependent effects in AC circuits. Modern developments in electromagnetic theory and circuit theory do not contradict Ohm's law when they are evaluated within the appropriate limits.\n\nOhm's law is an empirical law, a generalization from many experiments that have shown that current is approximately proportional to electric field for most materials. It is less fundamental than Maxwell's equations and is not always obeyed. Any given material will break down under a strong-enough electric field, and some materials of interest in electrical engineering are \"non-ohmic\" under weak fields.\n\nOhm's law has been observed on a wide range of length scales. In the early 20th century, it was thought that Ohm's law would fail at the atomic scale, but experiments have not borne out this expectation. As of 2012, researchers have demonstrated that Ohm's law works for silicon wires as small as four atoms wide and one atom high.\n\nThe dependence of the current density on the applied electric field is essentially quantum mechanical in nature; (see Classical and quantum conductivity.) A qualitative description leading to Ohm's law can be based upon classical mechanics using the Drude model developed by Paul Drude in 1900.\n\nThe Drude model treats electrons (or other charge carriers) like pinballs bouncing among the ions that make up the structure of the material. Electrons will be accelerated in the opposite direction to the electric field by the average electric field at their location. With each collision, though, the electron is deflected in a random direction with a velocity that is much larger than the velocity gained by the electric field. The net result is that electrons take a zigzag path due to the collisions, but generally drift in a direction opposing the electric field.\n\nThe drift velocity then determines the electric current density and its relationship to E and is independent of the collisions. Drude calculated the average drift velocity from p = −\"eEτ\" where p is the average momentum, −\"e\" is the charge of the electron and τ is the average time between the collisions. Since both the momentum and the current density are proportional to the drift velocity, the current density becomes proportional to the applied electric field; this leads to Ohm's law.\n\nA hydraulic analogy is sometimes used to describe Ohm's law. Water pressure, measured by pascals (or PSI), is the analog of voltage because establishing a water pressure difference between two points along a (horizontal) pipe causes water to flow. Water flow rate, as in liters per second, is the analog of current, as in coulombs per second. Finally, flow restrictors—such as apertures placed in pipes between points where the water pressure is measured—are the analog of resistors. We say that the rate of water flow through an aperture restrictor is proportional to the difference in water pressure across the restrictor. Similarly, the rate of flow of electrical charge, that is, the electric current, through an electrical resistor is proportional to the difference in voltage measured across the resistor.\n\nFlow and pressure variables can be calculated in fluid flow network with the use of the hydraulic ohm analogy. The method can be applied to both steady and transient flow situations. In the linear laminar flow region, Poiseuille's law describes the hydraulic resistance of a pipe, but in the turbulent flow region the pressure–flow relations become nonlinear.\n\nThe hydraulic analogy to Ohm's law has been used, for example, to approximate blood flow through the circulatory system.\n\nIn circuit analysis, three equivalent expressions of Ohm's law are used interchangeably:\n\nEach equation is quoted by some sources as the defining relationship of Ohm's law,\nor all three are quoted, or derived from a proportional form,\nor even just the two that do not correspond to Ohm's original statement may sometimes be given.\n\nThe interchangeability of the equation may be represented by a triangle, where V (voltage) is placed on the top section, the I (current) is placed to the left section, and the R (resistance) is placed to the right. The line that divides the left and right sections indicates multiplication, and the divider between the top and bottom sections indicates division (hence the division bar).\n\nResistors are circuit elements that impede the passage of electric charge in agreement with Ohm's law, and are designed to have a specific resistance value \"R\". In a schematic diagram the resistor is shown as a zig-zag symbol. An element (resistor or conductor) that behaves according to Ohm's law over some operating range is referred to as an \"ohmic device\" (or an \"ohmic resistor\") because Ohm's law and a single value for the resistance suffice to describe the behavior of the device over that range.\n\nOhm's law holds for circuits containing only resistive elements (no capacitances or inductances) for all forms of driving voltage or current, regardless of whether the driving voltage or current is constant (DC) or time-varying such as AC. At any instant of time Ohm's law is valid for such circuits.\n\nResistors which are in \"series\" or in \"parallel\" may be grouped together into a single \"equivalent resistance\" in order to apply Ohm's law in analyzing the circuit.\n\nWhen reactive elements such as capacitors, inductors, or transmission lines are involved in a circuit to which AC or time-varying voltage or current is applied, the relationship between voltage and current becomes the solution to a differential equation, so Ohm's law (as defined above) does not directly apply since that form contains only resistances having value R, not complex impedances which may contain capacitance (\"C\") or inductance (\"L\").\n\nEquations for time-invariant AC circuits take the same form as Ohm's law. However, the variables are generalized to complex numbers and the current and voltage waveforms are complex exponentials.\n\nIn this approach, a voltage or current waveform takes the form formula_12, where \"t\" is time, \"s\" is a complex parameter, and \"A\" is a complex scalar. In any linear time-invariant system, all of the currents and voltages can be expressed with the same \"s\" parameter as the input to the system, allowing the time-varying complex exponential term to be canceled out and the system described algebraically in terms of the complex scalars in the current and voltage waveforms.\n\nThe complex generalization of resistance is impedance, usually denoted \"Z\"; it can be shown that for an inductor,\n\nand for a capacitor,\n\nWe can now write,\n\nwhere V and I are the complex scalars in the voltage and current respectively and Z is the complex impedance.\n\nThis form of Ohm's law, with \"Z\" taking the place of \"R\", generalizes the simpler form. When \"Z\" is complex, only the real part is responsible for dissipating heat.\n\nIn the general AC circuit, \"Z\" varies strongly with the frequency parameter \"s\", and so also will the relationship between voltage and current.\n\nFor the common case of a steady sinusoid, the \"s\" parameter is taken to be formula_16, corresponding to a complex sinusoid formula_17. The real parts of such complex current and voltage waveforms describe the actual sinusoidal currents and voltages in a circuit, which can be in different phases due to the different complex scalars.\n\nOhm's law is one of the basic equations used in the analysis of electrical circuits. It applies to both metal conductors and circuit components (resistors) specifically made for this behaviour. Both are ubiquitous in electrical engineering. Materials and components that obey Ohm's law are described as \"ohmic\" which means they produce the same value for resistance (R = V/I) regardless of the value of V or I which is applied and whether the applied voltage or current is DC (direct current) of either positive or negative polarity or AC (alternating current).\n\nIn a true ohmic device, the same value of resistance will be calculated from R = V/I regardless of the value of the applied voltage V. That is, the ratio of V/I is constant, and when current is plotted as a function of voltage the curve is \"linear\" (a straight line). If voltage is forced to some value V, then that voltage V divided by measured current I will equal R. Or if the current is forced to some value I, then the measured voltage V divided by that current I is also R. Since the plot of I versus V is a straight line, then it is also true that for any set of two different voltages V and V applied across a given device of resistance R, producing currents I = V/R and I = V/R, that the ratio (V−V)/(I−I) is also a constant equal to R. The operator \"delta\" (Δ) is used to represent a difference in a quantity, so we can write ΔV = V−V and ΔI = I−I. Summarizing, for any truly ohmic device having resistance R, V/I = ΔV/ΔI = R for any applied voltage or current or for the difference between any set of applied voltages or currents.\n\nThere are, however, components of electrical circuits which do not obey Ohm's law; that is, their relationship between current and voltage (their I–V curve) is \"nonlinear\" (or non-ohmic). An example is the p-n junction diode (curve at right). As seen in the figure, the current does not increase linearly with applied voltage for a diode. One can determine a value of current (I) for a given value of applied voltage (V) from the curve, but not from Ohm's law, since the value of \"resistance\" is not constant as a function of applied voltage. Further, the current only increases significantly if the applied voltage is positive, not negative. The ratio \"V\"/\"I\" for some point along the nonlinear curve is sometimes called the \"static\", or \"chordal\", or DC, resistance, but as seen in the figure the value of total \"V\" over total \"I\" varies depending on the particular point along the nonlinear curve which is chosen. This means the \"DC resistance\" V/I at some point on the curve is not the same as what would be determined by applying an AC signal having peak amplitude ΔV volts or ΔI amps centered at that same point along the curve and measuring ΔV/ΔI. However, in some diode applications, the AC signal applied to the device is small and it is possible to analyze the circuit in terms of the \"dynamic\", \"small-signal\", or \"incremental\" resistance, defined as the one over the slope of the V–I curve at the average value (DC operating point) of the voltage (that is, one over the derivative of current with respect to voltage). For sufficiently small signals, the dynamic resistance allows the Ohm's law small signal resistance to be calculated as approximately one over the slope of a line drawn tangentially to the V-I curve at the DC operating point.\n\nOhm's law has sometimes been stated as, \"for a conductor in a given state, the electromotive force is proportional to the current produced.\" That is, that the resistance, the ratio of the applied electromotive force (or voltage) to the current, \"does not vary with the current strength .\" The qualifier \"in a given state\" is usually interpreted as meaning \"at a constant temperature,\" since the resistivity of materials is usually temperature dependent. Because the conduction of current is related to Joule heating of the conducting body, according to Joule's first law, the temperature of a conducting body may change when it carries a current. The dependence of resistance on temperature therefore makes resistance depend upon the current in a typical experimental setup, making the law in this form difficult to directly verify. Maxwell and others worked out several methods to test the law experimentally in 1876, controlling for heating effects.\n\nOhm's principle predicts the flow of electrical charge (i.e. current) in electrical conductors when subjected to the influence of voltage differences; Jean-Baptiste-Joseph Fourier's principle predicts the flow of heat in heat conductors when subjected to the influence of temperature differences.\n\nThe same equation describes both phenomena, the equation's variables taking on different meanings in the two cases. Specifically, solving a heat conduction (Fourier) problem with \"temperature\" (the driving \"force\") and \"flux of heat\" (the rate of flow of the driven \"quantity\", i.e. heat energy) variables also solves an analogous electrical conduction (Ohm) problem having \"electric potential\" (the driving \"force\") and \"electric current\" (the rate of flow of the driven \"quantity\", i.e. charge) variables.\n\nThe basis of Fourier's work was his clear conception and definition of thermal conductivity. He assumed that, all else being the same, the flux of heat is strictly proportional to the gradient of temperature. Although undoubtedly true for small temperature gradients, strictly proportional behavior will be lost when real materials (e.g. ones having a thermal conductivity that is a function of temperature) are subjected to large temperature gradients.\n\nA similar assumption is made in the statement of Ohm's law: other things being alike, the strength of the current at each point is proportional to the gradient of electric potential. The accuracy of the assumption that flow is proportional to the gradient is more readily tested, using modern measurement methods, for the electrical case than for the heat case.\n\nOhm's law, in the form above, is an extremely useful equation in the field of electrical/electronic engineering because it describes how voltage, current and resistance are interrelated on a \"macroscopic\" level, that is, commonly, as circuit elements in an electrical circuit. Physicists who study the electrical properties of matter at the microscopic level use a closely related and more general vector equation, sometimes also referred to as Ohm's law, having variables that are closely related to the V, I, and R scalar variables of Ohm's law, but which are each functions of position within the conductor. Physicists often use this continuum form of Ohm's Law:\n\nwhere \"E\" is the electric field vector with units of volts per meter (analogous to \"V\" of Ohm's law which has units of volts), \"J\" is the current density vector with units of amperes per unit area (analogous to \"I\" of Ohm's law which has units of amperes), and \"ρ\" (Greek \"rho\") is the resistivity with units of ohm·meters (analogous to \"R\" of Ohm's law which has units of ohms). The above equation is sometimes written as J = formula_19E where \"σ\" (Greek \"sigma\") is the conductivity which is the reciprocal of ρ.\nThe voltage between two points is defined as:\n\nwith formula_21 the element of path along the integration of electric field vector E. If the applied E field is uniform and oriented along the length of the conductor as shown in the figure, then defining the voltage V in the usual convention of being opposite in direction to the field (see figure), and with the understanding that the voltage V is measured differentially across the length of the conductor allowing us to drop the Δ symbol, the above vector equation reduces to the scalar equation:\n\nSince the E field is uniform in the direction of wire length, for a conductor having uniformly consistent resistivity ρ, the current density J will also be uniform in any cross-sectional area and oriented in the direction of wire length, so we may write:\n\nSubstituting the above 2 results (for \"E\" and \"J\" respectively) into the continuum form shown at the beginning of this section:\n\nThe electrical resistance of a uniform conductor is given in terms of resistivity by:\nwhere \"l\" is the length of the conductor in SI units of meters, \"a\" is the cross-sectional area (for a round wire \"a\" = \"πr\" if \"r\" is radius) in units of meters squared, and ρ is the resistivity in units of ohm·meters.\n\nAfter substitution of \"R\" from the above equation into the equation preceding it, the continuum form of Ohm's law for a uniform field (and uniform current density) oriented along the length of the conductor reduces to the more familiar form:\n\nA perfect crystal lattice, with low enough thermal motion and no deviations from periodic structure, would have no resistivity, but a real metal has crystallographic defects, impurities, multiple isotopes, and thermal motion of the atoms. Electrons scatter from all of these, resulting in resistance to their flow.\n\nThe more complex generalized forms of Ohm's law are important to condensed matter physics, which studies the properties of matter and, in particular, its electronic structure. In broad terms, they fall under the topic of constitutive equations and the theory of transport coefficients.\n\nIf an external B-field is present and the conductor is not at rest but moving at velocity v, then an extra term must be added to account for the current induced by the Lorentz force on the charge carriers.\n\nIn the rest frame of the moving conductor this term drops out because v= 0. There is no contradiction because the electric field in the rest frame differs from the E-field in the lab frame: E′ = E + v×B.\nElectric and magnetic fields are relative, see Lorentz transformation.\n\nIf the current J is alternating because the applied voltage or E-field varies in time, then reactance must be added to resistance to account for self-inductance, see electrical impedance. The reactance may be strong if the frequency is high or the conductor is coiled.\n\nIn a conductive fluid, such as a plasma, there is a similar effect. Consider a fluid moving with the velocity formula_28 in a magnetic field formula_29. The relative motion induces an electric field formula_30 which exerts electric force on the charged particles giving rise to an electric current formula_31. The equation of motion for the electron gas, with a number density formula_32, is written as\n\nwhere formula_34, formula_35 and formula_36 are the charge, mass and velocity of the electrons, respectively. Also, formula_37 is the frequency of collisions of the electrons with ions which have a velocity field formula_38. Since, the electron has a very small mass compared with that of ions, we can ignore the left hand side of the above equation to write\n\nwhere we have used the definition of the current density, and also put formula_40 which is the electrical conductivity. This equation can also be equivalently written as\n\nwhere formula_42 is the electrical resistivity. It is also common to write formula_43 instead of formula_44 which can be confusing since it is the same notation used for the magnetic diffusivity defined as formula_45.\n\n\n"}
{"id": "14963396", "url": "https://en.wikipedia.org/wiki?curid=14963396", "title": "ProjectExplorer", "text": "ProjectExplorer\n\nProjectExplorer is a documentary short film series. The films, directed and produced by ProjectExplorer's Founder, Jenny M Buccos, focus on histories and cultures of foreign places and people using interviews with subject experts, artists, and public figures including Archbishop Desmond Tutu, Dr. John Kani, Greg Marinovich, and Sipho “Hotstix” Mabuse. Produced for a child and young adult audience, segments in each series depict everyday life and the challenges and concerns of those living in the locations and regions featured. Each film is 2–4 minutes in length, with each series containing approximately 40 films.\n\nThe ProjectExplorer series is distributed internationally without charge via the web by ProjectExplorer, LTD. an American not-for-profit organization.\n\nThree series have been produced and distributed.\nIn fall 2009, ProjectExplorer's third series, Jordan, received a GOLD level Parents' Choice Award for excellence in web programming.\n\nThe first series was filmed in London, Stratford-upon-Avon, and New York City. The series includes more than 30 film segments.\n\nUnited Kingdom locations and individuals include:\n\nSegments filmed in New York City include:\n\nFilmed in Johannesburg, Cape Town, and KwaZulu Natal, the series contains over 40 film segments including:\n\nProminent South Africans featured in the series:\n\nFilmed in Johannesburg, Cape Town, and New York City, the series contains over 10 film segments.\n\nProminent South Africans featured in the series:\n\nFilmed in Amman, Petra, Umm Qais, Jerash, Madaba, Bethany, the Dead Sea, and New York City, the series contains more than 45 film segments.\n\nJordan series segments include:\n\n\n"}
{"id": "22222088", "url": "https://en.wikipedia.org/wiki?curid=22222088", "title": "Robert H. Brill", "text": "Robert H. Brill\n\nDr Robert Brill is in the field of archaeological science, best known for his work on the chemical analysis of ancient glass. Born in the United States of America in 1929, Brill attended West Side High School in Newark, New Jersey, before going on to study for his B.S. degree at Upsala College, also New Jersey (Brill 1993a, Brill 2006, Getty Conservation Institute 2009). Having completed his Ph.D in Physical Chemistry at Rutgers University in 1954, Brill was to return to Upsala College to teach chemistry himself until 1960 when he joined the staff of the Corning Museum of Glass as their second research scientist (Corning Museum of Glass, 2009)\n\nThroughout his lengthy career at Corning, where a four-year directorship punctuated his time as a research scientist, Brill was a forerunner in the scientific investigation of glass, glazes and colorants, developing and challenging the usefulness of emerging techniques. His pioneering work with the application of lead and oxygen isotope analysis in archaeology led him occasionally to add the investigation of metal objects to his portfolio so that, together, his published works number more than 160 (Brill and Wampler 1967). Perhaps the most famous of these is his \"Chemical Analyses of Early Glass\", a sum of his 39 years of work and now a seminal reference guide in the field (Brill 1999).\n\nBrill is a strong proponent of interdisciplinary cooperation as well as the collaboration between scientists across the world, and has served since 1982 on the International Commission on Glass. Within this he founded TC17, the technical committee for the Archaeometry of Glass, which lists among its aims the ‘promotion of collaboration among glass specialists in widely separated countries’ and the stimulation and encouragement of glass scientists ‘in developing countries’ (Archaeometry of Glass 2005). His internationalism is aptly demonstrated by his study of glasses from around the world, with his attentions most recently being focused on those from the Silk Road. Here, as with other areas of Brill's remarkable career, it seems he was attracted by the lack of previous study and the need for further development in the field. Seeing a disparity between contemporary knowledge of glasses from the western world and those from East Asia, Brill was keen to add insight to a hitherto unexploited field and, as such, has gone on to contribute a great deal to Silk Road studies (Brill 1993b).\n\nThe broad span of Brill's career allows this paper to provide only an abridged synopsis of his métier and published works to date. Focusing on Brill's achievements during the decades after he joined the Corning Museum in February 1960, it aims to highlight areas in which Brill pioneered new techniques and improved existing ones, offering summaries of major publications and proposing sources the interested reader may turn to for more information (Brill 1999).\n\nThe 1960s saw Brill beginning to develop the analytical techniques that would define the early years of his career at Corning, and yet the scope of his interest within glass remained vast. Indeed, 1961 saw Brill pen a letter to Nature with a colleague, that was a ‘bombshell’, according to Newton, in the field of glass-dating (1971, 3). Here Brill suggested that the rather enigmatic weathering crust found to form on buried glass objects over time could be used to date the object in a method rather similar to dendrochronology, using the separate layers of the shiny lamination (Brill 1961, Brill and Hood 1961, Newton 1971). Whilst in dendrochronology the tree-rings account simply for the tree's annual growth, in the weathering crust on glass Brill suggested the accumulation of a layer of laminate might respond to some kind of annual event of climatic change (Brill 1961). Unfortunately, despite the examples of the method's successful applications provided by Brill, such as the almost accurate count of 156 layers on a bottle-base from the York River submerged in 1781 and excavated in 1935, the technique largely failed to convince and did not see widespread adoption (Brill 1961, Newton 1971).\n\nThe most important of these techniques would prove to be Brill's pioneering application of lead isotope analysis, hitherto used only in geology, to archaeological objects. Brill first presented this idea at the 1965 Seminar in Examination of Works of Art, held at the Museum of Fine Arts Boston, but the first widely published account of the method seems to be Brill and Wampler's 1967 article in the \"American Journal of Archaeology\". Here, Brill and Wampler outlined how the technique could be used to provenance the lead contents of archaeological objects to lead ore sources around the world, based on the isotopic signature of various leads, which relates them to ‘ores occurring in different geographical areas’ (1967, 63). These different areas have different signatures because they are of varying geological age, something reflected by the individual lead isotopes which form only after the radioactive decay of uranium and thorium (Brill et al. 1965, Brill and Wampler 1967). While the lead isotope ratios used for provenancing are different, they are not unique: areas geologically similar will yield similar lead isotope signatures (Brill 1970). Furthermore, if leads were salvaged and mixed in ancient times, the isotope ratio will be compromised (Brill 1970). Aside from these two limitations, there is little else that could affect the lead isotope reading an object would yield. As such, Brill's method was greeted enthusiastically and he went on to develop the technique, as well as oxygen isotope analysis, in his 1970 publication. Here he demonstrated how the technique could be used both to classify early glasses and to a certain extent characterize the ingredients from which they were made (1970, 143).\n\nReturning to 1965, this year saw Brill launch another important innovation in glass analysis, the comparison of interlaboratory experiments in order to verify analytical results (Brill 1965). ‘Originally inspired by a plea from W E S Turner’, according to Freestone, Brill first mooted his idea at the \"VIIth International Congress on Glass\", in Brussels (Brill 1965a, I. Freestone, \"pers. comm.\" 2009). It wasn’t until the \"VIIIth International Congress on Glass\" in 1968, however, that Brill fully launched his concept of an ‘analytical round robin’, having distributed a number of reference glasses to be tested in different laboratories using a range of current techniques including X-ray fluorescence and neutron activation analysis (1968, 49). When discussing his motive for the experiment, Brill aptly stated: 'The truth is that the chemical analysis of glasses is a difficult undertaking and still remains in some senses an art' (1968, 49). By conducting the round robin experiment, Brill hoped the results gathered from different laboratories would help ‘correlate [...] earlier results’ and ‘calibrate future analyses in reference to one another’, as well as suggest which out of the analytical procedures used was the most accurate and effective (1968, 49). The results of the round robin were presented at the 'IXth International Congress on Glass' in 1971, and showed that, as Brill suspected, there was poor agreement between certain identified elements, and therefore these might be ‘troublesome’ generally across analyses (1971, 97). These included calcium, aluminium, lead and barium, among others (Brill 1971). Aside from their correctional potential, the results, from 45 different laboratories in 15 countries, also provided an enormous data set from which, Brill suggested, the participants could ‘evaluate their own methods and procedures against the findings of other analysts’ (1971, 97). At the time, Brill could hardly have suspected that the data would go on to have such great import, but Croegaard's generation of preferred glass compositions, from statistical analysis of the data, were used successfully by many people until Brill's own reference guide was published in 1999 (I. Freestone, 'pers. comm.', 2009).\n\nIt should not be thought that Brill spent the entire decade ensconced in the Corning laboratory; he made various forays to the Middle East, including accompanying Wertime's 1968 survey of the ancient technologies of Iran, alongside other great minds such as the noted ceramicist, Frederick Matson (UCL Institute for Archaeo-Metallurgical Studies 2007). In the years 1963-1964, the Corning Museum of Glass and the University of Missouri, following a long history of excavation at the necropolis of Beth She'Arim, conducted an examination of a huge slab of glass, some 2000 years old, that had been languishing in an ancient cistern (Brill and Wosinski 1965). Brill cannot recall who first suggested this slab, measuring 3.4m by 1.94m, could be made of glass, but the only way to test it was to drill a core through its 45 cm thickness and analyse it (Brill 1967, Brill and Wosinski 1965). On analysis of the core, Brill found that the glass was devitrified and stained, and not very homogenous, with a presence of wollastonite crystals throughout (1965, 219.2). Investigation of the manufacture technology required to produce the slab, suggested that in order to produce such a slab of glass, it would have been necessary to heat over eleven tons of batch material, and sustain it at around 1050˚C for between five and ten days (Brill 1967)! His initial interpretation was that the glass must have been heated either from above or from the sides using a kind of tank furnace; a hypothesis that was proven accurate when excavation underneath the slab suggested it had been melted \"in situ\", in a tank whose floor was a bed of limestone blocks with a thin parting layer of clay (Brill and Wosinski 1965, Brill 1967). Brill's interpretation, that the slab and its surroundings suggest ‘some early form of reverberatory furnace’ was the first suggestion of the use of tank furnaces in early glassmaking (1967, 92). The evidence at Beth She’arim encouraged further innovative thought because whilst the slab represented glass production on a grand scale, no associated evidence for glass working was found. Brill had already suspected that historical glassmaking occurred in two phases, the heavy ‘engineering’ stage when the glass is formed from the batch ingredients and the ‘crafting’ stage when the glass is formed into artefacts (Brill, pers. comm., 2009). These stages could occur in combination at one location, or at two differing locales, and the time span of production after the initial glass melt is highly flexible. For Brill, the idea of this ‘dual nature of all glassmaking’ was ‘crystallized’ at Beth She’Arim, where only the raw glass production was represented, and would be reinforced later by the contrasting evidence, where working was favoured over production, found at Jalame, as discussed below (Brill, pers. comm., 2009).\n\nAside from the aforementioned published results of his analytical round robin and his lead and oxygen isotope studies in the early 1970s, the decade saw Brill publish comparatively little, perhaps due to his post as director at The Corning Museum of Glass. Those publications he did pen are largely concerned with the development of lead isotope analysis and are listed in the further reading section. Alas, before Brill could be named Director, however, the museum was to be blighted by an enormous flood, ‘possibly the greatest single catastrophe borne by an American museum’ according to Buechner, Brill's successor in 1976 (1977, 7).\n\nThe flood was brought to Corning by Hurricane Agnes, a tropical storm that filled the Chemung River system to bursting until, on the morning of June 23, 1972, the river breached its banks and decimated the town (Martin and Edwards 1977). The Corning Glass Centre was under around twenty feet of water on the lower level's west side, while the museum itself was filled to a water-level of five feet and four inches (Martin and Edwards 1977). 528 of the museum's objects were damaged, the library's rare books were ruined and paper index systems, data and catalogues were all lost (Martin and Edwards 1977). In the wake of this destruction, Brill was named Director, so that his time holding this position, from 1972-1975, would be spent overseeing the complete restoration of the museum. Buechner praises how Brill 'painstakingly' prepared the insurance claim that would support the museum throughout the renovation process and facilitate the replacement of many wonderful objects (1977, 7). Under Brill's auspices, the Corning Museum of Glass was reopened just thirty-nine days after the event, on the 1st August, but it would be another four years before the collection and library were restored to their former glory (Buechner 1977).\n\nIn 1982, Brill joined the International Commission on Glass, ‘the world’s leading organization of glass scientists and technologists’ according to the Corning Museum (2009). The International Commission functions through various technical committees, among which Brill saw an opening for TC17, the committee for the Archaeometry of Glass, which he founded shortly after joining. The main purpose of TC17, whose members met for the first time in Beijing in 1984, is ‘to bring together glass scientists, archaeologists and museum curators to present and discuss the results of research on early glass and glassmaking and on the conservation of historical glass objects’, as expressed in their mission statement (Archaeometry of Glass 2005). Brill was to chair this committee until 2004 and received the W E S Turner Award from the International Commission of Glass on his departure, in recognition of his contribution as founder (Corning Museum of Glass 2009).\n\nOne of the on-running projects of the Corning Museum of Glass published the excavation report from their many field seasons at the ancient glass factory in Jalame, in Late Roman Palestine (Brill 1988, Schreurs and Brill 1984). Brill was called upon to conduct scientific investigations of the huge amount of material generated at the site, in order to exploit the full potential of the artefacts; after all, the site was being excavated specifically because of its role as a glass factory (Brill 1988). Of the vast quantity of glass fragments from Jalame, both vessel sherds and cullet, most were aqua and green and all were soda-lime-silica glasses melted in highly reducing conditions (Schreurs and Brill 1984). Where the melting conditions had been increasingly reducing, a ferri-sulfide chromophore complex was shown to have formed, thus changing the bluey-aqua colour of the glass to an olive, or even an amber shade (Schreurs and Brill 1984). Despite these colour variations, Brill's further chemical analysis showed the vessel glasses to be highly homogeneous in composition, apart from a clear divide where around 40 glasses demonstrated the intentional addition of manganese (Brill 1988). Brill conducted an investigation of the furnace at Jalame, nicknamed the Red Room, in which there was a mysterious absence of glass finds of any kind (Brill 1988). Whilst work at Beth She’Arim had eventually found there to be five firing chambers responsible for heating the one tank, the fragmentary remains at Jalame made it very difficult to interpret the furnace set-up, apart from the fact that they believed there to have been only one firing chamber (Brill 1988).\n\nIn the late eighties Brill was to contribute various studies to the Institute of Nautical Archaeology, following the excavation of a number of exciting shipwrecks including the \"Serçe Liman\", and the \"Ulu Burun\" (Barnes et al. 1986, Brill 1989). Here Brill's own technique of lead isotope analysis was to provide a means for provenancing items aboard ship, and thus determine the ship's origin and her ports-of-call. The excavators of the \"Serçe Liman\" wanted to know whether she was Byzantine or Islamic, a complicated question for lead isotope analysis as the lead ores of the Eastern Mediterranean share geographical characteristics and therefore overlap (Barnes et al. 1986). Using 900 lead net sinkers divided into six loose groupings, Brill found groups III, V and VI to be Byzantine, that is with ores found in modern-day Turkey (Barnes et al. 1986). Group I, however, was taken to be most indicative of the ship's origin; this group contained net sinkers, but also two ceramic glazes and three glass vessels, all sharing virtually identical lead ores with only one isotopic match, ‘an ore from Anguran, northwest of Tehran’ according to Barnes et al. (1986, 7).\n\nBrill's submissions to the \"XIVth International Congress on Glass\", which took place in New Delhi in 1986, can be seen to represent the origins of his work on the Great Silk Road, the impressive trade route carrying goods from the East through India to Europe. Here, chemical analysis of Early Indian glasses would help Brill not just to determine the ingredients and techniques of production, but ‘to make certain broad generalizations as to regions or periods of manufacture’, and therefore to follow an object's movement along the trade route (1987, 1). For the XIVth Congress, Brill conducted atomic absorption spectroscopy (AAS) and optical emission spectroscopy (OES) on samples of 38 glasses from India, and the success of his method was made clear when he was able to separate 21 samples away from those made in the Middle East and Europe (Brill 1987). Here the glasses were shown to have mixed alkali compositions, a feature that is ‘rare among glasses from more westerly sources’, and therefore Brill concluded that they had definitely been manufactured in India (1987, 4). Brill also collaborated with Mckinnon to conduct chemical analyses of some glass samples from Sumatra, Indonesia, the results of which would be the ‘first data of their kind from this island’ (1987, 1). The results of the study, which also used samples from Java, another important location for the Silk Road, were hoped by McKinnon and Brill to ‘stimulate a greater awareness of glass in the economy [...] of ancient Sumatra and further new lines of research in the archaeology of the region’ (1987, 1).\n\nThe beginning of the 1990s saw Brill accorded the Archaeological Institute of America's Pomerance Award for scientific contributions to archaeology; however the decade mostly reflects Brill's continuing dedication to Asian glasses and the study of the Silk Road (Archaeological Institute of America 2009). In \"Scientific Research in Early Chinese Glass\", Brill reflected that in comparison to the knowledge of glassmaking in the West, ‘little is known about Chinese glass and about the role it played in the overall unfolding of glass history on a worldwide basis’ (1991, vii). One reason for this is that glass was never produced in the East in such great quantities as it was in the West but also that archaeological Chinese glasses are often prone to problems (Brill 1991). The difficulties of analysing Chinese glasses were reflected later in the publication where, following the chemical investigation of 71 samples, Brill found that identifying the ‘basic formulation’, or ‘any of the primary batch materials’ of the glasses was still almost impossible (Brill et al. 1991). Brill had greater success in differentiating between Chinese glass samples when using lead isotope analysis, a method that has proven effective in the first instance of identifying Chinese glass as the leads used here are different from those anywhere else in the world (Brill, Barnes et al. 1991). Brill found his Chinese samples to fall into two distinct groups, possessing on one hand the highest, and on the other the lowest, lead isotope ratios he had \"ever\" encountered (Brill, Barnes et al. 1991). As such, he was able to show that despite the striking similarity in the glasses’ chemical composition and appearance, the ores from which their leads were sourced must have been from very geologically-different mines (Brill, Barnes et al. 1991).\n\nBrill conducted further investigations of ancient Asian glasses for the Nara Symposium on the Silk Road's maritime route in 1991, ‘to demonstrate [...] that chemical analyses can be useful for learning how glass was traded along the Desert, Steppe, and Maritime Routes of the Silk Road’ (1993a, 71), as well as providing a more technical discussion on glass and glassmaking in China for the Glass Art Society's Toledo Conference in 1993 (Brill 1993b). Further lead isotope analysis, this time on Chinese and central Asian pigments, was conducted with a larger team for the Getty's Conservation of Ancient Sites on the Silk Road, which saw Brill et al. launching studies that held incredible potential for understanding ‘chronological or stylistic differences among Buddhist cave paintings’, or ‘distinguish[ing] between original and repainted parts of individual works’ (1993, 371).\n\nIn 1999, Brill published the sum of 39 years worth of results from his chemical investigations at Corning in two volumes of reference material with a third forthcoming (Brill 1999). Brill was reluctant to publish the data without any accompanying interpretation, but he felt that the most important factor was to quickly release the material into a wider sphere, made ‘readily accessible to the scientific community’ (1999, 8). Of Corning's 10,000 research artefacts, the master catalogue contains 6,400 samples, an abbreviated catalogue, or AbbCat, of which is presented in the two volumes (1999, 11). Nineteen geographical, typological or chronological categories of glass samples are recorded, spanning Brill's various research projects and collaborations, from Egypt to the East (Brill 1999). It also records the results of oxygen isotope analyses, reminding us that Brill was ever one for the integration of different investigative methods.\n\nSince 2000, Dr Brill's interest in Silk Road studies and ancient glass compositions has continued, but his publication rate has slowed somewhat. His years of prolific publication, however, and his willingness to analyse glass from almost every situation have provided the archaeometry of glass with a bounty of reference material, as reflected by the \"Chemical Analyses of Early Glasses\". Despite his official retirement from the Corning Museum of Glass on May 31, 2008, he returned to the laboratory the next day and continues to work, showing no intention of enjoying a retirement proper any time soon (Brill, \"pers. comm.\", 2009).\n\n\n\n\n‎"}
{"id": "9771120", "url": "https://en.wikipedia.org/wiki?curid=9771120", "title": "SMS hubbing", "text": "SMS hubbing\n\nSMS hubbing is a new structure for the international flow of SMS between operators, reshaping the way that international mobile inter-operability works by implementing hubs to intermediate the SMS traffic and to offer a larger SMS coverage. \n\nThe GSM Association (GSMA) found in SMS Hubbing the solution to a problem that limits the continuing growth of international SMS, culminating to the development of the SMS Hubbing trials in 2006, part of the Open Connectivity project. This initiative created a new structure for international SMS interoperability, as well as developed standards and requirements that SMS hubs should follow.\n\nRegardless of the maturity of the operator or number of subscribers, each subscriber expects to be able to send an SMS to other subscribers, regardless of country and mobile network.\n\nThe lack of complete international SMS interoperability is caused by the way the GSM world is interconnected: each operator has a need to establish SMS interworking with all other mobile operators, meaning that international SMS can only transit from one operator to another if there is a bi-lateral roaming agreement in place.\n\nWhile SMS interoperability is limited to bi-lateral interworking / roaming agreements between operators, it is unlikely that full international SMS reach will be achieved by setting up more and more agreements, which are time consuming and costly to put in place. In addition to that, the revenue benefits of an extra interworking connection might not justify the investment required to set it up in the first place.\n\nSMS hubbing enables a broad international SMS coverage for mobile operators (“client operators”) through the connection to independent hubs, who have multiple agreements in place with other operators, therefore being able to route messages on behalf of the client operators.\n\nSMS Hubbing works with the same concept of voice connectivity model: rather than relying on costly and multiple individual agreements, voice traffic flows via telehouses, which are basically hubs for voice. In the same way, many operators connect to hubs to transit MMS messages, in an attempt to fix the many interoperability problems with this messaging technology. SMS Hubbing is about simplifying the SMS interworking system, by replacing much of the unproductive, identical investments in international agreements made traditionally by mobile operators. As well as this, SMS Hubbing is about providing a higher level of service to SMS, by introducing end-to-end quality of service through Service Level Agreements (SLAs).\n\nIt is clear that SMS Hubbing does not replace bilateral agreements. Every operator has the need to establish roaming agreements in order to provide subscribers with the possibility to roam outside their home network. Outside this frame of main roaming agreements that generate the majority of international traffic for voice and messaging, it makes sense for mobile operators to allocate SMS traffic that belongs to non-connected destinations to an SMS hub.\n\nWith the SMS Hubbing model, an operator looking to increase their international SMS coverage does not need to manage multiple bi-lateral agreements. Mobile operators can simplify this by connecting to a hub. The SMS Hubbing model reduces complexity for operators, as well the cost for SMS interworking agreements. Mobile subscribers also take advantage of an enlarged SMS reach, being able to send and receive message to all countries and networks.\n\nUbiquity and simplicity in increasing SMS coverage\n\nSMS Hubbing allows operators to manage a single legal, technical and billing relationship rather than hundreds of additional roaming agreements for SMS messaging only.\n\nMobile operators have been gradually outsourcing non-core functions to invest on areas that need a closer management of the operator, as well as focusing on areas that have a direct interface to customers. The hub concept follows this trend, removing a costly and complex area of technical interface and replacing it with a more efficient and effective outsourced solution.\n\nUnder the GSMA’s SMS Hubbing structure, a hub will negotiate a transit fee with an originating operator on a per-SMS basis for the use of the hub. As is the usual practice, the originating operator will also pay a termination fee to the terminating operator, also on a per-SMS basis.\n\nBeyond SMS, hubbing can be applied in different areas, such as MMS. Because of many interoperability problems, many operators generally connect to an MMS hub, which route the messages more efficiently on behalf of the operator.\n\n\n"}
{"id": "56797234", "url": "https://en.wikipedia.org/wiki?curid=56797234", "title": "SS Australasia", "text": "SS Australasia\n\nThe Australasia was a wooden hulled steamship that sank on October 18, 1896 in Lake Michigan near the town of Sevastopol, Door County, Wisconsin, United States, after burning off Cana Island. On July 3, 2013 the wreck of the \"Australasia\" was added to the National Register of Historic Places.\n\nThe \"Australasia\" (Official number 106302) was built in 1884 in West Bay City, Michigan by the shipyard owned by Captain James Davidson. She was built for the Davidson Steamship Company which was also owned by Captain Davidson. At a length of the \"Australasia\" was one of the largest wooden ships ever built; her beam was wide and her cargo hold was deep. She was powered by a fore and aft compound engine which was fueled by two coal burning Scotch marine boilers. She had a gross register tonnage of 1829.32 tons and a net register tonnage of 1539.20 tons.\n\nOn September 17, 1884 the \"Australasia\" was launched as hull number #9. At the time of her launch the \"Australasia\" was the largest wooden hulled ship in the world. Because of her enormous size the \"Australasia\" needed iron cross bracing, an iron keelson, iron plates, and several iron arches to increase her strength.\n\nShe was used to haul bulk cargoes such as iron ore, coal, grain and sometimes salt. She could carry these cargoes so efficiently that she earned a fortune for her owners at a time when small, less versatile wooden vessels were quickly being replaced by larger, and stronger iron or steel vessels. Just like all ships owned by Captain Davidson, the \"Australasia\" used to tow a wooden schooner barge.\n\nOn October 17, 1896, the \"Australasia\" was bound from Lake Erie to Milwaukee, Wisconsin, carrying 2,200 tons of soft coal. At around 6:00 p.m. near Baileys Harbor, the crew of the \"Australasia\" discovered \"a fire beneath the texas on the main deck\". They attempted to fight the blaze but failed. The crew abandoned the \"Australasia\" before she reached Jacksonport, Wisconsin. At 10:30 p.m., the \"Australasia\" was about four hours off Jacksonport when the tugboat \"John Leathem\" came upon the struggling steamer. The \"Leathem\" began towing the \"Australasia\" to shore, but the hawser connecting them kept burning through. At 9:00 a.m. on October 18, 1896, the crew of the \"Leathem\" gave up trying to salvage her and instead dragged her onto the beach in of water south of Cave Point. Her crew decided to scuttle her; they did this by ramming a hole in the \"Australasia\"&apos;s side with the \"Leathem\"&apos;s stem. She was left heading north by northwest. She burned until the night of October 18, 1896.\n\nThe \"Australasia\" was declared a total loss. Much of her cargo of soft coal and machinery was salvaged, however her hull was beyond repair and was abandoned. Today her lower hull lies mostly buried in sand 15 to 20 feet of water off Whitefish Dunes State Park. Because most of her hull remains buried in sand, there is the possibility that different hull sections may be uncovered which may reveal more significant information about her construction. Not a trace of her cargo is visible on the site of her wreck, but traces of coal is visible on a beach nearby. The wreck of the \"Australasia\" is rarely visited by divers which means that very little site disturbance to the site has occurred. Close by are the wrecks of several other ships including the early steel freighter \"Lakeland\", the large wooden bulk carrier \"Frank O'Connor, the wooden steamer \"Louisiana\" which was lost during the Great Lakes Storm of 1913, the schooner \"Christina Nilsson\" and the steamboat \"Joys\".\n"}
{"id": "4203230", "url": "https://en.wikipedia.org/wiki?curid=4203230", "title": "Scottish Campaign for Nuclear Disarmament", "text": "Scottish Campaign for Nuclear Disarmament\n\nThe Scottish Campaign for Nuclear Disarmament (Scottish CND) is the Scottish representative body of the Campaign for Nuclear Disarmament (CND). The Scottish CND campaigns for the abolition of British nuclear weapons to contribute to the global elimination of all nuclear weapons.\n\nThe organisation was founded in 1958, at the same time that the CND was founded, and was one of the core aspects of a CND revival during the 1970s.\n\nOn 17 November 2012, as part of a long history of supporting the Scottish independence movement, Scottish CND's Annual Conference passed a resolution, stating:\n\nConference urges all members to give priority to the campaign for a 'YES vote' in the 2014 Independence Referendum which will give the Scottish Government a mandate to negotiate a written constitution with a clause on No Nuclear Weapons in Scotland. Conference resolves that SCND affiliates to and promotes the “Yes” Campaign as the most immediate and effective way of getting rid of Trident.\n\nDuring May 2014, the Electoral Commission registered the organisation as a campaigning participant for a \"Yes\" vote in the September 2014 independence referendum and Chair Arthur West said that the registration process was a display of transparency regarding the CND's involvement with the campaign, further explaining: \"This decision was taken because our purpose as an organisation is to promote nuclear disarmament and we believe that independence offers the best opportunity for this.\" \n\nThe Scottish CND's office is in the city of Glasgow, and is the base for protest organising for cities such as Glasgow, Edinburgh and Faslane. As of June 2014, the Chair of the organisation is Arthur West, while the Co-ordinator is John Ainslie and the Assistant Co-ordinator is Flavia Tudoreanu.\n\nThe organisation has released numerous written resources to support its cause, including an April 2014 leaflet and poster, entitled \"No Nuclear Weapons Here\". The front of the leaflet reads \"Scotland no place for nuclear weapons\" underneath the title, while the back of the leaflet explains the situation in the UK, stating \"Nuclear disarmament begins at home\". The Scottish CND also provides people with the option to order free anti-nuclear stickers that are written in English, Scots and Gaelic languages.\n\nThe Her Majesty's Naval Base Clyde lies on the eastern coast of Scotland, 40 km (25 mi) west of Glasgow in the Faslane area. A nuclear submarine fleet is based at the site, facilitated by Prime Minister Clement Attlee's authorisation of a British nuclear weapons programme in 1947. A 1958 agreement between the UK and the United States (US) was followed by a 1962 US agreement, whereby it provided information about its submarine-launched missile system, \"Polaris\". The UK's first Polaris submarine, HMS Resolution, was launched in 1968 and the entire system was modified in the early 1970s to the British \"Chevaline\" system.\n\nThen, in 1980, the Thatcher Government purchased the new \"Trident\" missile system from the US to replace Chevaline and this was finalised in 1996. Submarines carrying Trident nuclear warheads are based at HMNB Clyde and the Scottish National Party affirmed that it would remove the submarines if independence was gained following the 2014 referendum. While experts suggested that the submarines could be relocated to a Devonport base in the English city of Plymouth, the Scottish CND advocates for the complete abolishment of the Trident warheads.\n\nIn January 2013, the Scottish CND released a report in which it stated that a much greater population would be put at risk if the weapons were transferred to Devonport. The report claimed that, in the event of an accident at Devonport, an estimated 800 people would be killed by leaking plutonium, while as many as 11,000 people could die from radiation poisoning If the weather was calm. Ainslie further explained to the media that an accident would mean \"a large proportion of the city would be abandoned for hundreds of years.\" A Ministry of Defence spokesperson responded to the discussion and report in January 2013, by stating:\nWe are therefore not making plans to move the nuclear deterrent from HM Naval Base Clyde, which supports 6,700 jobs, and where all of our submarines will be based from 2017 ... The government is committed to maintaining a continuous submarine-based nuclear deterrent and has begun the work of replacing our existing submarines.\n\nIn campaign material released in April 2014, the Scottish CND explained that \"All British nuclear weapons are in Scotland\" and \"a total of 120 nuclear warheads on Trident submarines\" are based at the HMNB Clyde in Faslane.\n\nFollowing the release of the \"Love\" album in 1987, Scottish band Aztec Camera was invited to perform at a benefit concert for the Scottish CND in the late 1980s. Frame explained in a television interview prior to the concert that he was merely the entertainment and would not deliver any speeches. \n\n\n"}
{"id": "12599589", "url": "https://en.wikipedia.org/wiki?curid=12599589", "title": "Solar Cookers International", "text": "Solar Cookers International\n\nSolar Cookers International (SCI) is a U.S.-based non-profit advocacy group in Sacramento, California, founded by a group of people in 1987 and incorporated on January 6, 1988. Solar Cookers International advocates for and provides education on solar cooking.\n\nSolar Cookers International won an Ashden Award in 2002 for their work with solar cookers in Kenya. In August 2006, SCI was the winner of the World Renewable Energy Award.\n\nSolar Cookers International was founded in 1987 as Solar Box Cookers International. Two American women described as American's serious solar cooker promoters in the 1970s, Barbara Kerr and Sherry Cole partnered with other supporters to form this organization.\n\nSCI produced and distributed manuals describing construction and use of solar box style cookers. They became advocates of how solar cooking could be incorporated into development and relief agency programs. SCI's role evolved into networking with other solar cooking organizations worldwide. They hosted forums for dialog including co-sponsoring three international solar cooking conferences with the University of the Pacific, US, in 1992, the National University of Costa Rica in 1994 and the deemed university, Coimbatore, India in 1997.\n\nSCI also administered a series of solar cooking field projects. Since 1995, SCI has managed or co-managed solar cooking projects in the Nyakach district, Kenya; in the Kakuma refugee camp, Kenya; in the Aisha refugee camp, Ethiopia; in various communities, Zimbabwe; and in Dadaab refugee camp, Kenya.\n\nSCI supported the development of the CooKit, a mass-producible, foldable solar cooker in the 1990s.\n\nSCI was instrumental in the formation of the Solar Cookers International Network, which is an association of approximately 500 non-governmental organizations and government agencies, manufacturers, and individuals promoting solar cooking throughout the world. The Solar Cookers International Network wiki includes over 1700 articles categorized by country, individual, NGO, manufacturer, and solar cooker designs. Also included is information regarding related technologies such as heat-retention cooking, water pasteurization, solar food processing, solar food drying, solar autoclaving, and solar canning. The Network maintains a wiki-based website] that allows the network members to share information and experiences at http://solarcooking.org.\n\nSCI has hosted regional and international solar cooking conferences, most recently the Solar Cookers International Conference held in Granada Spain in 2006. SCI publishes \"Solar Cooker Review\" three times a year.\n\nSCI developed the \"CooKit\" as an adaptation of a cooker designed by Dr. Roger Bernard in France. The cooker consists of a foil-lined cardboard reflector with a dark pot inside a plastic bag. This simple mechanism converts hundreds of watts of sunlight into heat and can cook one or two pots of food at a time.\n\nSCI has set up local production of this cooker in Nairobi, Kenya and in Sacramento, California. The CooKit is used in many solar cooking projects around the world.\n\nCardboard, aluminium foil, and plastic bags for well over 10,000 solar cookers have been donated to the Iridimi Refugee Camp and Touloum Refugee Camps in Chad by the combined efforts of the Jewish World Watch, the Dutch foundation KoZon, and SCI. The refugees construct the cookers themselves, using the donated supplies and locally purchased Arabic gum, and use them for midday and evening meals. The goal of this project was to reduce the Darfuri women's need to leave the relative safety of the camp to gather firewood, which exposed them to a high risk of being beaten, raped, kidnapped, or murdered. It has also significantly reduced the amount of time women spend tending open fires each day, with the results that they are healthier and they have more time to grow vegetables for their families and make handicrafts for export.\n\n"}
{"id": "12019770", "url": "https://en.wikipedia.org/wiki?curid=12019770", "title": "Steering damper", "text": "Steering damper\n\nA steering damper, or steering stabiliser is a damping device designed to inhibit an undesirable, uncontrolled movement or oscillation of a vehicle steering mechanism, a phenomenon known in motorcycling as wobble.\n\nSport bikes have a short wheelbase and an aggressive steering geometry to provide the ability to make very quick changes in direction. This has the harmful side-effect of making the bike less stable, more prone to feedback from uneven road surfaces, and more difficult to control. If the front wheel significantly deviates from the direction of travel when it touches down, it may cause an unwanted wobble. Steering dampers are factory installed on some high-end sport motorcycles and fitted to most contemporary racing bikes to counter these behaviours.\nSteering dampers are also mounted to off-road motorcycles such as motocross bikes. A damper helps keep the bike tracking straight over difficult terrain such as ruts, rocks, and sand, and also smooths out jolts through the handlebars at the end of jumps. They also reduce arm fatigue by reducing the effort to control the handlebars.\n\nOn motorcycles, one end of the damper is mounted to the steering yoke or triple tree, the other to the frame. Two main types are linear and rotary. Linear dampers resemble a telescoping shock absorber and operate in a similar manner. They can be aligned either longitudinally and to one side of the steering, or transversely across the bike. Rotary dampers resemble small boxes and operate via a rotating pivot. They are mounted coaxially with the steering axis and are typically located on top of the steering head. An electronically variable damper uses a rotary damper with hydraulic fluid that flows freely at low speeds, allowing easy turning, while restricting flow at higher speeds when more damping is necessary, as determined by the engine control unit.\n\nFor motorcycles with sidecars, especially for motorcycles which have been retrofitted with a sidecar and where the front wheel geometry, or trail, has not been adjusted for use with a sidecar, a steering damper is beneficial. This prevents low speed wobble which may occur in the lower speed range of about 13 to 20 mph. In older motorcycles adjustable friction dampers had been routinely installed. Hydraulically operated steering dampers may be retrofitted. In some jurisdictions, the installation and operation of a steering damper must be inspected by an expert or examiner and must be entered in the vehicle papers.\n\nSteering dampers have been available for bicycles as well. There is also a mechanism by the same name that consists only of a spring connected to the frame and the fork that merely provides a progressive torque to straighten the steering.\n"}
{"id": "45397464", "url": "https://en.wikipedia.org/wiki?curid=45397464", "title": "System Basis Chip", "text": "System Basis Chip\n\nA System Basis Chip (SBC) is an integrated circuit that includes various functions of automotive electronic control units (ECU) on a single die.\n\nIt typically includes a mixture between digital standard functionality like communication bus interfaces and analog or power functionality, denoted as smart power. Therefore SBCs are based on special smart power technology platforms.\n\nThe embedded functions may include:\n\nThe complexity range for SBCs starts with rather simple hardwired devices to configurable state-machine controlled devices, e.g. through a serial peripheral interface.\n\nVarious major automotive semiconductor manufacturers offer SBCs.\n"}
{"id": "4954764", "url": "https://en.wikipedia.org/wiki?curid=4954764", "title": "The Year in Industry", "text": "The Year in Industry\n\nThe Year in Industry (YinI) is a UK scheme, which organises gap year placements for pre-university and undergraduate students. Each year The Year in Industry places around 750 students in engineering, science, IT, and business. The Year in Industry is run by the not for shareholder profit Engineering Development Trust and is accredited by the Learning Grid.\n\nStudents submit a Curriculum vitae to The Year in Industry detailing what field they are interested in finding a placement. The Year in Industry then send individual students 'company CV's' in that field. Students can then elect to be put forward for that placement, and may be selected by the company for interview and ultimately the placement. Placements usually last around 12 months, during which in between two and four on-site visits are made by YinI to check up on the student.\n\nThe Year in Industry was set up in 1987 in the Bristol area and was originally called Pre-Formation of Undergraduate Engineers (PFUE). It has placed over 8500 students to date, in 2007 over 250 UK companies were involved in the scheme. The scheme has received a lot of praise from both universities and industry.\n\nPlacements usually last around 12 months and start between July and September, depending on the company and students requirements. Applications are free as The Year in Industry no longer charge an administrative fee to students. Students apply for placements through regional offices with the following process:\n\n\nThe Year in Industry is run by the non-profit organization Engineering Development Trust,\nan independent education charity whose aim is to involve young people in engineering, science and technology. The scheme is also part of the Royal Academy of Engineering's \"BEST\" programme, likewise this is aimed at encouraging young people to undertake careers in engineering.\n\nThe organisation is split up into twelve regional offices:\n\n\nEach year the Year in Industry runs a Contribution to Business award. This is aimed at providing students with the opportunity to demonstrate how they have made a difference to their company through their work placement. The award is open to all placement students, who submit a written application detailing the contributions they have made to their regional YinI office. This is supported by a statement written about the student by their line manager.\nAround eight people in each region are short-listed and are then required to give a short presentation to industry figures at the regional open day. The winner from each region then attends a national final to find an overall winner. In 2007 the winner of each regional final won a prize of £500. The overall winner will receive an award of £1000, plus three additional prizes are to be awarded of £500 each for innovation, communication skills and environment. The event in 2007 was held on Thursday, 6 September at The Royal College of Physicians, Regent's Park, London.\n\nStudents are expected to attend and ideally complete a Chartered Management Institute course or equivalent during the year; different regions undertake different courses due to differences in funding. Most regions undertake an 'Introduction to Management' certificate and a 'Certificate in Management', the latter being accredited to NVQ level 3 standard. Teaching comes in the form of residential and one-day-a-month workshops, usually taught by local colleges, totalling around 14 days of tuition. Other regions undertake a smaller Institute of Leadership & Management Level 3 Certificate in Leadership. This involves around 60 hours of tuition spread over five one-day-a-month workshops.\nThe course is organised by the Year in Industry and is paid for by the companies employing YinI students. Enrollment on this course gives YinI students membership to the NUS.\n\nAt the end of the year students attend a regional open day. Here they exhibit their work and get the opportunity to talk to industry professionals about their experiences. The judging of the regional Contribution to Business Award also takes places at the open day.\n\nIn 2007/8 academic year the Year in Industry launched its 'YinI Combo' placements which allowed students to work for part of the year before travelling.\n\nA voluntary maths course is offered by Loughborough University Best Maths Team. This course has been specially designed for engineering based students working in industry and aims to improve, and maintain students' maths skills over the course of the placement. The course is completed by the student through a series of distance learning modules and on-line tests.\n\nAt this moment in time the Year in Industry placements do not legally entitle participants to student council tax exemptions. Undergraduate students are entitled to the exemption since they remain members of their respective universities for the duration of the gap year, however, pre-university students with deferred entry to university do not qualify as students by council tax law. In the past some appeals with support from the Year in Industry have been successful.\n\n\n"}
{"id": "16888524", "url": "https://en.wikipedia.org/wiki?curid=16888524", "title": "Wired communication", "text": "Wired communication\n\nWired communication refers to the transmission of data over a wire-based communication technology. Examples include telephone networks, cable television or internet access, and fiber-optic communication. Also waveguide (electromagnetism), used for high-power applications, is considered as wired line. Local telephone networks often form the basis for wired communications that are used by both residential and business customers in the area. Most of the networks today rely on the use of fiber-optic communication technology as a means of providing clear signaling for both inbound and outbound transmissions. Fiber optics are capable of accommodating far more signals than the older copper wiring used in generations past, while still maintaining the integrity of the signal over longer distances.\n\nAlternatively, communication technologies that don't rely on wires to transmit information (voice or data) are considered wireless, and are generally considered to have higher latency and lower reliability.\n\nThe legal definition of most, if not all, wireless technologies today or \"apparatus, and services (among other things, the receipt, forwarding, and delivery of communications) incidental to such transmission\" are a wire communication as defined in the Communications act of 1934 in 47 U.S.C. §153 ¶(59). This makes everything online today and all wireless phones a use of wire communications by law whether a physical connection to wire is visible or is not. The Communications act of 1934 created the Federal Communications Commission to replace the Federal Radio Commission. If there were no real wired communications today, there would be no online and there would be no mobile phones or anything wireless except satellite communications.\n\nIn general, wired communications are considered to be the most stable of all types of communications services. They are relatively impervious to adverse weather conditions when compared to wireless solutions. With some forms of wired services, the strength and speed of the transmission is superior to other solutions, such as satellite or microwave transmissions. These characteristics have allowed wired communications to remain popular, even as wireless solutions have continued to advance. \n"}
{"id": "51145061", "url": "https://en.wikipedia.org/wiki?curid=51145061", "title": "Workplace respirator testing", "text": "Workplace respirator testing\n\nTo protect workers from air contaminants employers often used respirators. Reliable protection of workers' life and health requires that the effectiveness of the respirator must be consistent with the degree of air pollution. There are respiratory protective equipment RPE (or devices - RPD) of different designs, and their protective properties differ markedly. If employer wish to select the appropriate respirator, he or she need to know in advance what level of protection it will provide.\n\nInitially, the effectiveness of respirators were evaluated with their tests in the laboratories. But detection of cases of excessive exposure of harmful agents on those workers who properly used and maintained approved respirators (with high efficiency filters), has prompted experts to change their opinions. In the late 1960s, researchers found that real effectiveness of respirators in the workplaces were significantly lower than during tests in laboratories.\n\nTherefore, since the 1970s, specialists started RPD tests in workplaces (in industrialized countries). The results of these studies showed that the real effectiveness of respirators were much lower, than in the laboratories (in general). Specialists have begun to use the results of workplace testing for developing regulations (governing the selection RPD and organization of their usage) for limits of the safe allowable use of all RPD designs.\n\nThe invention of the first personal sampling pump (1958) made it possible to simultaneously measure the concentrations of air pollutions outside the respirator mask, and at the same time - pollution of inhaled air (under a facepiece). Comparison of these measurement results show the effectiveness of respiratory protective equipment. But until the 1970s experts mistakenly believed that the protective properties of the respirator under laboratory and in the production conditions are not significantly different. Measuring the effectiveness of respirators under production conditions were not performed; and limits of areas of safe use for different types of respirators were established on the basis of laboratory tests only.\n\nBut the results of the first workplace studies have shown that the effectiveness of respirators all designs - is very fickle, and strongly depend on the correctness of their use (continuous use in polluted atmosphere, etc...), and on the leakage of contaminated air under the mask through the gaps between it and the worker's face. It was found that respirators' efficiency in the workplaces were much lower, as compared to laboratory conditions. This has led to revise the boundaries of the application of RPE different designs, and prompted to develop requirements for the organization of their application, fixing them in the national legislation. The results of measurements forced to pay more attention to the technical methods of protection (sealing, ventilation, automation, changing technology, and others.).\n\nIf the facepiece of the respirator is tight-fitting one (elastomeric quarter or half masks, filtering facepieces, or full-face masks), efficiency decreases due to leakage of unfiltered air through the gaps between the mask and face. These gaps are formed due to the fact that employees must perform a variety of movements under production circumstances that testers in the lab do not, and the fact that even properly dressed masks may \"slip\" sometimes.\n\nIf respirator facepiece is loosely fitting one (hoods, helmets etc.) polluted air can also enter the breathing zone due to its injection (with moving stream of ambient air, unless it is motionless). But there are no significant drafts in the laboratory during RPD approval tests.\n\nA small number of testers can not simulate all the variety of faces of millions of workers (their shapes and sizes), and about 20 minutes at a certification lab test can not simulate all the variety of movements performed in the millions different workplaces. In addition, testers put on and wear masks more slow and accurate than the workers.\n\n(1974) The researchers studied the effectiveness of respirators used by miners. Scientists simultaneously measured dust concentrations with the personal samplers and two dust collectors - without mask and in the facepiece. Also, they measured the proportion of time, then the respirators were used by miners. For this purpose, two thermistor (one in the facepiece, the other on the belt) were used; over-heating PTC with expired air was a sign of wearing a mask. Because convenience of the respirators affect the timeliness of its usage, scientists have studied the opinion of the miners on the use of respirators. A detailed report was published before the publication of the article.\n\n(1974) The study found that respirators can be a good addition to the effective dedusting ventilation. The authors recommended to carry out medical examination of workers - in employment and periodically.\n\n(1975) Experts measured the concentration of dust under respiratory protective devices of workers-sandblasters, and outside - but not simultaneously. The measurement results showed that exposure to air pollution on employees exceeds the permissible value, and that the supply of clean air into the hood reduces adverse effects. Also, measurements have shown that there may be excessive exposure to workers during breaks (when the respirator may be withdrawn); and discovered the fault of some respirators. Authors recommended: to organize the correct application of RPD, to reduce the concentration of dust in the air, and apply the abrasive material with a lower silica content.\n\n(1976) The concentrations of sulfur dioxide were measured under the negative pressure filtering respirator with elastomeric half mask, and in the breathing zone (simultaneously); and the results were used to calculate the respirator protection factors PF (such as the concentration ratio, ambient to in-facepiece). Experts analyzed the results of only those measurements, during which the masks were not removed. They found a positive relationship between a comfortableness of respirators, and their effectiveness, as the workers tightened the straps of harness at the convenient masks much stronger.\n\n(1979) Efficiency of self contained breathing apparatus (SCBA) MSA has been studied when they were used by firefighters. This SCBA had had air supply into the mask with \"on demand\" mode (with negative pressure under the mask during inhalation). Evaluation of the effectiveness of the respirator was conducted by determining the content of carboxyhemoglobin in the blood immediately after the cessation of fire extinguishing. Carboxyhemoglobin formed due to inhalation of carbon monoxide. The biomonitoring results shown: (1) the intermittently usage of these respirators makes it totally ineffective; (2) even in continuous usage of such breathing apparatus does not provide effective protection. The results of this study (and other similar studies) led to restrict the use of respirators with fresh air supply \"on demand\"; and to prohibit their use in fighting fires. US and EU legislation obliges to use only self-contained breathing apparatus with \"pressure-demand\" mode of air supply (it provide positive pressure under the mask during inhalation) in extinguishing fires.\n\n(1980) Evaluating the effectiveness of RPE was performed with biomonitoring. Researchers measured a styrene concentration in the expired air and in the urine. Since the absorption of styrene through skin was small, respirators were able to provide reliable protection of workers.\n\n(1980) This study clearly showed that using respirators result (exposure to harmful substances on workers) is highly dependent on the organization of their application, and on training of the workers: \nEmployees use a respirator several times. The average protection factor of the respirator in one of the workers (who always used a respirator in a timely and accurately), proved to be 26 times greater than that of all other workers.\nThe authors have raised the question of the need to describe the effectiveness of the respirator using different terms. One term - for cases of timely application of the respirator, and the other - for use with interruptions (for description the real protection of employees).\n\n(1983) Employees used a respirator with forced air supply to the facepiece; expected protection factor = 1000 (and more). But measured protection factors were considerably lower: 4.5 - 62 times. This disparity has prompted experts to conduct additional research to discover the causes of differenses.\n\n(1983) Employees used a respirator (PAPR): air flow 184 l/min, the degree of air purification with filters 99.97%. But they are often discovered climbing of its face shield, and the minimum protection factors reaches 1.1, 1.2 ... . Some of respirators do not provide a snug fit to the face of the faces. Measurement results showed that the rest in the room with a clean air greatly reduces the impact of air pollution on the workers, and that respirators alone can not reliably protect them. It was also found that the calculation of the protection factors may give different results for different chemicals (for the same measurement carried out).\n\n(1984) Measured respirators' protection factors were very unstable. Scientists have proposed to establish the boundaries of the safe use of respirators based on the results obtained not in the laboratories, but in the workplaces, under application without interruption. This principle, with some variations, is used in developed countries today. Experts proposed to establish the expected protection factors so that the real effectiveness was greater than the expected effectiveness with a probability of 90% and in 95% of cases. The authors analyzed the results, and offered to reduce the permissible usage limits of the half-mask respirators with forced air supply (PAPR) from 500 to 50 PEL.\n\n(1984) Experts have studied the effectiveness of half mask respirators, used by employees with and without a beard under the exposure of coarse dust. The hair on the faces of the staff did not lead to a decrease in the effectiveness of respirators. This result can be explained by the fact that the dust was large, and it passes through the gap only partially. All Western manuals require workers shave face when using respirators with tight-fitting facepieces.\n(1984) This study was the third study (), which showed low efficiency respirators (Powered air-purifying respirator with loose-fitting facepiece, hood or helmet), which consistently provides reliable protection in the laboratory (PF>1000). The minimum protection factor of respirators, applied continuously, were as follows: for the 3M model - 28, and for the Racal model - 42.\n\nA significant difference between the real and laboratory efficiency prompted the National Institute for Occupational Safety and Health (NIOSH) released in 1982 two informational messages on respirators, warning consumers about the low effectiveness of RPD of the type that is expected to provide reliable protection. The experts were alarmed: they do not understand why (after the supplying more than 115 liters of clean air per minute under the facepiece) there are a lot of pollution in the inhaled air.\n\n\nThis study showed that laboratory tests can not be used as a reliable indicator of the effectiveness of respiratory protective equipment. Authors are encouraged to use the results of measurements of respirators' protection factors in the workplaces (with continuous use) to develop reasonable restrictions for different types of respiratory protective equipment.\n\nPublic discussion of the proposed terms were carried out in 1982-1986. As a result of this discussion, the experts gave definitions for the six respirator protection factors that can be measured in different conditions in the workplaces and in the laboratories.\nThese definitions began to be used officially, as well as in the preparation of articles about the trials of respirators.\n\nFor example, the assigned protection factor (APF), is the minimum PF, which RPD (of this type) must ensure if: the respirator will be used by trained and taught workers, after individual selection masks to face an employee; if it is to be used without interruption in the polluted atmosphere - in most cases (but not in all cases).\nThe experts recommended to develop APF values based on measurements of the protection factors in the workplace, or in considering the values of APF of similar respirators' types.\n\n\nOther workers also meet very strong volatility of efficiency (for example, the protection factor of 51 000 and 13 for the same worker).\n\nThe initial stage of research yielded the following results: experts developed a common terminology to describe the protective properties of the respirators; a methodology for measuring protection factors in different conditions of respirators' usage in the workplaces and laboratories; and the realization that national legislation should set limits for the use of all types of respirators based on their performance, measured not in the laboratory, but in the workplaces. The first studies also clearly confirmed that the use of respirators is the most unreliable method of protecting workers from all known methods. Therefore, respirators should be used only when other methods can not be used; or when other methods can not reduce the exposure of air pollution on people to an acceptable safe level.\n\nSince the effectiveness of respirators may vary depending on different circumstances related to the organization of their application, the RPE should be used as part of a respiratory protection program (a complex of measures aimed at eliminating the causes which may reduce the effectiveness of respiratory protection).\n\n(1984) Workers used disposable half-mask respirators, that protected them from the mercury vapor in the enterprise, produces chlorine. Range of measured protection factors - from 9 to 63. Perhaps the real effectiveness of respirators differ from measured - but take into account the deposition of harmful substances in the respiratory tract was too difficult.\n\n(1984) The study describes the measurement of the effectiveness of half-mask respirators, which has been used to protect against lead aerosols.\n\n(1985) Report of the exhibition and conference. The researchers presented their measurements of efficiency of respirators that have been used to protect against asbestos during removal of insulation and fire-retardant ceiling.\n\n(1986) The researchers studied the effectiveness of respirators and at the same time - the efficiency of the workers protection (by biomonitoring). They found a correlation between the respirator protection factors and lead concentrations in the blood. The authors noted that the violation of personal hygiene can lead to the entry into the body large amounts of lead (despite the use of effective respirators in the workplace).\n\n(1986) Respirators used for the protection of organic solvent vapors. In order to assess their effectiveness, the researchers used passive diffusion monitors. Exposure of pollution on the workers was excessive due to the fact that the masks used in the polluted atmosphere intermittently.\n\n(1986) Researchers have studied helmets with forced air supply (PAPR). These respirators protect workers from lead aerosol during battery production.\n\n(1986) Report of the exhibition and conference. The researchers reported the results of measurements of the protective properties of the hood with forced air supply. The respirators were used to protect workers against asbestos during brake manufacturing.\n\n(1987) The researchers studied the effectiveness of the filtering facepieces. They made a mistake: they measured the in-facepiece concentration by weighing, but the dust contains cement. Dust humidification increased its mass, and drying the in-facepiece filters can not remove moisture. The error was detected; and the planning of new studies in the majority of cases employ the chemical analysis of collected dust. The specialists began to point to what chemical element the measured protection factors were defined.\n\n(1987) The author has studied the effectiveness of the protection of workers who removed the old paint. Employees used a negative pressure filtering respirators with a full face mask; concentration of solvents under the mask was measured using a diffusion monitors. The author said that the high humidity of expired air were not interfere with the measurements.\n\n(1987) Report of the exhibition and conference. The report describes the measurement of the effectiveness of respirators that workers used to protect against aerosols of aluminum, titanium and silica during the polishing and grinding.\n\n(1987) Report of the exhibition and conference. The report described the measurement of protection factors airline respirator. The workers used RPD to protect against silica at shipyard.\n\n(1989) The workers used a helmet with forced air supply. This helmet was joined to a waterproof suit with the help of a zip fastener. This made it possible to obtain high protection factors (~ 350), and protect workers securely. The results of researches of the show, in order to ensure reliable protection of people with the help of a respirator, the employer shall pay attention to the organization and use of RPD, and to the planning of the work.\n\n(1989) Report of the exhibition and conference. The researchers studied the protective properties of negative pressure respirator with full face masks. These RPE used to protect employees from the lead in the enterprise, which was made this metal.\n\n(1989) Report of the exhibition and conference. The researchers studied the protective properties of SARs, which were used for protection against iron and quartz aerosols during abrasive machining of castings.\n\n(1990) Researchers measured protection factors of three models certified negative pressure filtering respirators with full face masks. Surprisingly, the values obtained were much lower than expected; and they are significantly smaller than the values previously obtained in the laboratories. Minimum PF values were: 11; 16; 17; 20; 26 ... (expected PF >900). For one of three models, there are no any measurements, when its protection factor exceeded 500 - et all.\n\n\n(1990) Measurement results showed that the effectiveness of the protection of employees of various specialties (using the same half mask respirators) may be different; and that the effectiveness of one and the same worker may differ by tens of times.\n\n(1990) Report of the exhibition and conference. The authors reported respirator protection factor measurements, that were applied for protection from aluminum dust in the production of this metal.\n\n(1990) Report of the exhibition and conference. Experts have reported PF of filtering facepiece respirators, used to protect against lead and zinc in brass casting.\n\n(1990) Report of the exhibition and conference. The researchers studied the protective properties of full face masks with forced air supply (PAPR). These RPE used to protect employees from the lead in the enterprise, which was made this metal.\n\n(1990) Report of the exhibition and conference. The researchers studied the protective properties of the helmet with forced air supply (Powered air-purifying respirator). These RPE used to protect employees from the steroids in the pharmaceutical industry.\n\n(1991) Measurement of protective properties of half mask respirators showed that they are ineffective, and that their protective properties are significantly higher in the laboratory than in the workplace.\n\n(1992) The authors made a review of the researches of RPE efficiency in the workplaces. This article shows a significant difference between the real and lab efficiencies, that forced to carry out researches of respirators in the workplaces, and develop adequate terminology. The article also describes the problems when evaluating the protective properties of the high efficiency RPE types: very low pollution of air under the facepiece preventing the accurate analysis; and it is difficult to find a workplace with a sufficiently high concentration of air pollution.\n\n(1992) Report of the exhibition and conference. Scientists studied respirators (filtering facepieces), used to protect against fume of iron, manganese, titanium and zinc during welding and abrasion on the shipyard.\n\n(1993) The respirators used by workers continuously. The effectiveness of half-mask with forced air supply (PAPR) is higher than one of negative pressure half-mask.\n\n(1993) The researchers studied the effectiveness of respirators to protect workers from styrene. The authors used two methods. They measured the concentrations of styrene under the mask and the outside it (for calculating the protection factor); and they conducted biomonitoring (determined concentration of phenylglyoxylic acid and mandelic acid in the urine). These substances are formed during the decomposition of styrene in humans. The average value of a respirator protection factors (calculated with the measured concentrations) was equal to 4; and the average intake of harmful substance in the body of the worker (measured using biomonitoring) decreased by 3 times. The authors recommended determine the impact of styrene on the workers by biomonitooring.\n\n(1993) Evaluating the effectiveness of a respirator was done using biomonitoring. The authors measured the concentrations of lead and zinc protoporphyrin in blood. The lead absorption by the human body increases these concentrations. The respirators usage reduced the lead ingress into the body of the employees. The researchers recommended to show the measurement results to employees to encourage the timely use of respirators, and personal hygiene.\n\n(1993) The authors studied the filtering facepieces respirators. The researchers found a positive correlation between respirators' protection factors (when RPE used without interruption) and the concentration of pollutants in the ambient air.\n\n(1993) Report of the exhibition and conference. The staff dismantled the old oven. They used a hose respirators (Supplied Air Respirator, SAR) for protection against quartz aerosol.\n\n(1993-1994) Experts have studied the protective properties of the different elastomeric respirators and filtering facepieces. The workers used these respirators in several companies producing paint, flame retardants and batteries.\n\n(1994) Report of the exhibition and conference. Experts measured protection coefficients of half mask respirators during cutting of old ships. The workers were exposed to the lead aerosol.\n\n(1995) The author carried out a statistical analysis of the results of all studies of half mask respirator (when they are used without interruption). He concluded that in most cases (it is - all the cases together) protection factors were greater than 10. But the expert did not realize just how fickle the protection coefficients for individual employees, and that the continuous use is not always possible.\n\n(1995) Research has shown that different types of respirators (quarter masks, half masks and PAPR with loose-fitting hood) provide a similar (low) degree of protection when its were used intermittently. The researchers interviewed the workers to determine deficiencies of RPE different designs, and gave advice on the selection of respirators for different types of work.\n\n(1996) Ventilation and use of respirators (without interruption) enables reliably protect workers.\n\n(1996) Studies have shown that the use of respirators (with technical means of collective protection) provides the desired reduction in the harmful effects on workers.\n\n(1996) The workers use the hoods with a continuous flow of a clean air under its. They purified the bridge from old paint. Exposure of lead on the employees exceeds the maximum permissible exposure.\n\n(1996) This article describes the reasons how ANSI has limited the use of respirators of different designs (Assigned PF). To develop values of APF ANSI used: (1) results of the measurements of RPD efficiency in the workplaces; (2) APF of respirators with similar design. Only in the absence of this information, the experts used the measurements of efficiency in the laboratory (than them imitate the real work). \n\n(1998) The workers use respirators with full face masks, air is supplied under a mask forcibly - from the block of filtration and purification. Components of respirators differ from the factory set (workers used the most appropriate mask, comfortable units clean and cheap filters - from different manufacturers). Of the 21 cases, the workers received adequate protection only in eight cases. The minimum protection factor (5) was significantly less than expected from the respirator of this design (1000). The authors recommended the use of working methods that create less dust concentration; train workers; and prohibit the use of RPE with non-approval configuration.\n\n(1998) The authors studied the respirator protection factors of the same type (half-mask), using different models. The effectiveness of different respirator models can differ significantly.\n\n(1999) Respirators used to protect workers against styrene. Scientists also examined the absorption of styrene through a skin during the study. Biological monitoring has shown that absorption through the skin is small; and respirators affect on the styrene exposure dose greater than the use of protective clothing.\n\n(1999) The researchers used an original method for measuring full facepiece performance. Determining the concentration of harmful substances under the mask can be very difficult, since the concentration can be very small. To solve this problem, the researchers attached to the mask hood under which supplied test gas (sulfur hexafluoride). Since workers do not travel long distances, the gas concentration was measured by stationary devices with long tubes. This method allows to accurately measure the gas leakage through the gaps between the mask and the face. They also used the standard EU methodology for determining leakage, which is used for respirator certification.\n\n(2000) The authors studied the effectiveness of respirators (they measured the styrene concentrations outside the mask, and under the mask); and the real effectiveness of the protection of workers (styrene concentration in the urine). It turned out that the effectiveness of the protection of workers (with impermanent usage of respirators) is significantly lower than the effectiveness of respirators themselves: the people's exposure decreased by only 5-60%, and exceeded the limit.\n\n(2000) The authors studied the filtering half-mask respirators. The workers used different models: part of respirators had forced supply of air from the cleaning unit, and a part was the usual negative pressure half masks. The results showed that if the workers use respirators intermittently, their efficiency are low, and significantly lower than expected: In the first type respirators 85-91% values of protection factors were lower than expected Assigned PF = 50; and the second type of RPE provide protection factor in 82-89% of cases lower than the expected APF = 10.\n\n(2000) Respirators field study has shown that the effectiveness of the protection of the shipyard workers from the styrene exposure may also depend on the concentration of pollutant in the dining room air. The use of respirators in the workplace without interruption reliably protect workers.\n\n(2000) Protection factors of filtering facepieces were measured. Workers used respirators intermittently. Because working conditions were improved (through the use of vacuum cleaners instead of dry sweeping the floor, ets), the intermittently usage of the respirators allowed protect workers to the extent necessary.\n\n(2001) The application the Powered Air Purifying Respirator (PAPR) allows reliably protect workers during grinding. Measured PF were >1000.\n\n(2001) Report of the exhibition and conference. The authors described the measurement of respirators' performance (PAPR with hood) in the enterprise, is made of nickel-cadmium batteries. The air was polluted by cadmium.\n\n(2002) Protection factors of half mask respirators that are not always used (because of high air temperature) have been very low. Half of the PF values were less than 2. The authors recommended to make general ventilation; and use PAPR (air blowing face may make use of such respirators more acceptable at a high temperature).\n\n(2002) The authors analyzed the expected values of protection factors and take into account the appearance of a mask during inspiration (at different flow of inhaled air). They found that efficiency of a Supplied Air Respirators (SAR) with a constant air feed mode can be lower than expected, and recommended to reduce Assigned PF from 100 to 40.\n\n(2002) Measuring the effectiveness of respirators, individually chosen for the workers, showed that they securely protect workers from welding fumes. The respirators were used without interruption.\n\n(2003) Experts studying how mask fit to the faces of the workers affect on the protection factors of the respirators. Protection Factor - a random variable, which depends on many factors and it is not predictable; however respirator masks that conform to the workers' faces, provide better protection on average. Similar results were also obtained in the laboratories. These results became the basis of the legal requirements: the employer is obliged to pick up the mask to the face of each employee individually, and to check compliance (with the form and size) with the instruments. One of the measurements showed that ordinary low-cost half mask can provide very high protection factor (230,000) sometimes. However, this protection is unreliable: the next measurement in the same working (which used the same mask) showed that the protection factor of only 19. The average value of the protection ratio for these two measurements was ~ 38. Other workers also meet any strong volatility of efficiency (for example, the protection factor of 51 000 and 13 for the same worker).\n\n(2004) The textbook states that the expected degree of protection is a such quantity, that the respirator will provide for a certain proportion of the workers, and with a certain probability. However, experts did not have enough results of PF measurements for the same worker repeatedly. Consequently, the expected effectiveness started (in practice) equal to the lower 95% confidence limit of the set values of measurement results. Later, in several studies, PF measurements were repeated to for the same worker, and specialists conducted statistical analysis. Nikas and Neyghauz tried to determine - then the expected PF will reliably protect not less than 95% of workers in more than 95% of cases of RPE usage. They took into account the volatility of PF of the individual worker and volatility of the average (mean) PF for the different workers. It turned out that then the expected PF (Assigned PF) = 10 efficiency respirators will be insufficient, and they are advised to reduce the Assigned PF to 5 for negative pressure half mask.\n\n(2004) After individual selection masks, checking their fit to the faces, and when they are used in a timely manner - the measured effectiveness of the workers protection has not been lower than expected.\n(2005) This article describes a unique portable device. The instrument determines the count concentration of aerosol particles outside the mask, and under the mask (during operation in real time). This instrument determines the concentration for 5 optical particle diameters ranges separately. The measurement results demonstrated that the effectiveness of the filtering facepiece (under a constant application) were very unstable; and that smaller particles leak under a mask better, than larger ones.\n\n(2005) Experts measured the protective properties of respirators that are used to protect against inhalation of fungal spores and bacteria. The result showed that the efficiency depends on the type of microorganism.\n\n(2007) Experts studied the protective properties of full face masks. The measurements showed that the workers were adequately protected. The duration of the measurements was 1–3 hours. In these circumstances, of the 52 measurements, in 2 cases, the workers had removed the mask (because they had something to say to each other) - and these results were not taken into account in the analysis. However, use of respirators intermittently drastically reduces their efficiency. This shows how important to correctly organize the use of respirators, and to provide workers with the intercom stations (if necessary).\n\n(2007) Respirators' protection factor was measured under the conditions of exposure of workers with xylene and ethylbenzene vapors. Also, the team members conducted biomonitoring of exposure. They measured the concentration methylhippuric acid in the urine. Scientists have found a correlation between the concentrations of xylene in air and the concentrations of methylhippuric acid in the urine; and calculated the proportion of solvent entering the body through the lungs and through the skin. It turned out that if the respirator protection factors was more than 17-25, more than half of xylene enters to the painters' body through unprotected skin. The authors recommended the use of more hygienic methods of painting, since the use of skin protection continuously in hot tropical conditions difficult to achieve.\n\n(2007) The authors of this study repeated mathematical modeling of the effectiveness of respirators, which had previously performed by Nikas and Neyghauz. The authors have complicated mathematical model, and take into account the results of new researches. Since the protection factors in the new studies were large, the author got the high anticipated effectiveness of half masks respirators - no less than 10 times.\n\n(2007) Respirators has been used continuously. The filtering half masks provide the required protection for steel plant employees.\n\n(2007) The workers used a high-quality half mask respirators; and they were trained. Mask was selected to employees individually and their compliance with the face were checked. Employees perform sedentary work, the air was contaminated with coarse dust. So, the lowest protection factor (PF=24) was much greater than expected (Assigned PF = 10). However, these conditions may not be available at other workplaces; and therefore the authors recommended do not change the Assigned PF for half mask respirator.\n\n(2008) PAPR protect workers securely. In all cases, the concentration of harmful substances (under a mask) was less than the threshold sensitivity of the analysis method used. The authors noted that the tests of high efficiency respirators require workplaces with high polluted air, and these places are hard to find.\n\n(2009) Workers used high quality masks - on time and correctly. So, in most cases pollution of inhaled air (under a mask) is below the threshold sensitivity of the analysis method used.\n\n(2010) Engineers used a special instrument for the study of respirators. Results showed that the protection efficiency of particles with a large optical diameter higher than that for particles with smaller optical diameter.\n\n(2012) The authors review work described research on the effectiveness of helmets with forced filtered air flow to the organs of respiration (model \"Airstreem\"), used in metallurgical enterprises in England.\n\n(2015) Experts studied the PAPR performance. Respirators provide reliable protection of workers from the nanoparticles.\n† - \"Only the published studies. Many studies have been conducted, but they were not published. However, their results were known and used.\"\n\n\nThe authors have shown that the average decrease dustiness of inhaled air in coal mining in the UK due to the use of respirators is 41% (1.7 times). Low efficiency respirators due to the fact that the miners used respirators intermittently or not used at all (in the conditions of effective ventilation and low dust concentration). They can not determine when the dust concentration is more than acceptable, and when it is necessary to use a respirator.\n\nThe handbook discusses the results of measurements of respirators' performance. The dust concentration was reduced by 92% with the use of half mask respirators while working longwall shearer; and decreased three times (in average). Helmet with forced filtering air supply (PAPR) reduces the concentration of dust in half. The expected decrease in dust concentration of the two types of respirators are 10 and 25 respectively. In subsequent publications of CDC (on the subject of reducing the dust content in underground mining) RPE are not mentioned at all.\n\nLiquidation of consequences of the Chernobyl accident required the reliable protection of people from radioactive aerosols. Even a small amount of radioactive substances can seriously harm human health if it enters into their bodies (because of the small distance from the tissues). \nAerosol particles deposited in the lungs, can remain there for many years, and it increases the risk of diseases. Eliminating the consequences of the accident were carried out the best experts, including members of the Kurchatov Institute. Only one of the manufacturers RPD sent to Chernobyl approximately 300,000 negative pressure filtering facepieces model \"Lepestok\" only in June 1986. These respirators are considered to be very effective (the declared protection factor of 200 for most common model \"Lepestok-200\").\nBut the application of this model on a large scale has revealed numerous cases of excessive exposure to air pollution on people. This has led experts to question the high efficiency of the respirator, and it is similar to the events in the US nuclear industry in the late 1960s. Doubts were strong; and they are forced to carry out an independent study of the effectiveness in a foreign laboratory. Specialists conducted parallel attempts to identify the reasons for the low efficiency of the respirator in Chernobyl.\n\nThe results of research in Chernobyl and in the United States were similar: the filter material is well caught a fine aerosol, and created a small resistance to the passage of air through it; but a lot of unfiltered air passes through the gap between the mask and face. Research has shown that one can expect the minimum values of the protection factor 2÷8, or even 1.5.\n\nRepresentatives of the institution in which the model of respirator was developed, given their very original interpretation of the results of independent (foreign) tests: \n... in 20% of cases the protection factor (fit factor) exceeded the declared value (200). ... Consequently, the respirator model provides the declared efficiency..\nIn addition, the respirator designers stated that experts from Kurchatov Institute conducted their measurements \"illiterate\".\n\nMany cases of excessive exposure to air pollution during operation at the Chernobyl plant did not lead to a change in assessments of the effectiveness of respirators - as happened in the United States before.\n\nField measurement results showed that the respirators are the most recent and the most unreliable means of protection.\nThe effectiveness of respiratory protective equipment is unstable and unpredictable. Respirators cannot substitute other measures, that reduce the impact of air pollution on the staff (sealing equipment, ventilation and so on), but only supplement them. Respirators are not convenient, they create discomfort and irritation, and prevent communication. The reduction of the field of view leads to an increase in the risk of accidents.\n\nRPE reinforce overheating at a high air temperature. These and other deficiencies often prevent use of respirators in the polluted atmosphere without interruptions. But if the RPD is not used, it becomes useless.\n\nWorkers using respirators partially lose performance. Industrial hygienists know many cases where harmful substances enter the body not through the respiratory system, but in other ways (through skin). Even the timely use of a respirator may not be sufficient for reliable protection of workers.\n\nIf the respiratory system is the main way of receipt of harmful substances in the body, and if the use of other means of protection does not allow to reduce the impact to an acceptable value, employee must use respirators. They should be selected taking into account their effectiveness (it depends on the respirator type); masks should be chosen for employees personally; and workers should be learned, and trained - in accordance with legal requirements. This reduces the risk of occupational diseases as much as possible.\n\nComparison of the results of tests of various types of respirators in the laboratories and in the workplaces showed that laboratory tests do not allow to properly assess the real effectiveness of respirators (even if they are applied without interruption). Therefore, legislation in industrialized countries establishes limitations on the use of all types of respirators, taking into account such differences, and taking into account the results of field trials. This field measurements revealed low efficiency of several types of respirators, and forced more strictly limit their use: for negative pressure air-purifying respirators with full face mask and high efficiency filters - from 500 PEL to 50 PEL (USA), from 900 OEL to 40 OEL (UK);\nfor Powered Air-Purifying Respirators with loose-fitting facepiece (hood or helmet) — from 1000 PEL to 25 PEL (USA), PAPR with half mask — from 500 PEL to 50 PEL (USA), for supplied Air Respirators with full face mask and continuous air supply mode - from 100 OEL to 40 OEL (UK); for SCBA and SAR with air supply on demand mode — from 100 PEL to 50 PEL (USA).\n\nThe results of numerous field tests and analysis, led to the restriction of application limits of filtering facepieces and negative pressure half mask respirators to 10 PEL in US.\n\nThe significant difference between the real and laboratory efficiency prompted the National Institute for Occupational Safety and Health to require the manufacturers of high-performance RPE perform its testing at the adequate workplaces (as a requirement for respirator certification in the United States).\n\nDue to the fact that respirators wearing cannot provide a reliable protection, National Institute for Occupational Safety and Health has developed a guide to reduce the dust concentration: in the underground coal mines; in the other mines; and other similar documents with specific recommendations.\n\n"}
