{"id": "56721694", "url": "https://en.wikipedia.org/wiki?curid=56721694", "title": "ASPIDER", "text": "ASPIDER\n\nASPIDER is the group name for a series of companies that are mostly based in Europe. The company name has evolved over the years as a result of acquisitions, mergers and restructuring. The company is an MVNE (Mobile Virtual Network Enabler), providing mobile services to companies that want to control their own network. Clients include enterprises, manufacturers, integrators, and the mobile operators themselves.\n\nFollowing the merger of ASPIDER and MEC Solutions in 2004, the company traded as ASPIDER Solutions. Following the acquisition of NGI in 2014 the company traded as ASPIDER-NGI. ASPIDER sold aspects of its M2M business in 2010 to Wyless (which was subsequently purchased by Kore Wireless).\n\nASPIDER is traditionally known as a Mobile Virtual Network Enabler (MVNE), delivering services to a variety of operators and MVNOs and MNOs (Postmobile, Vodafone and T-Mobile). More recently the company has been delivering services to Enterprises, Manufacturers and other MVNEs (SURFnet, Kore Wireless). In the past couple of years, ASPIDER has been promoting eSIM and Remote SIM Provisioning- the embedded SIM standard by GSMA and has announced partnerships with various industry players for the use of eSIM in IoT initiatives (IDEMIA, Oracle).\n\nASPIDER's promotion of eSIM is notable because, when the initial specifications were announced, they were limited mainly to use-cases around operator selection and Remote SIM provisioning. ASPIDER has been promoting additional use cases more for enterprise applications - like authentication and identity management. The company has been working with industry groups like BTG to get pilots off the ground with companies like SURFnet.\n\nASPIDER operates from a number of datacenters around the world and the largest of these is based in Malta. The Maltese operation includes a full core network that is interconnected to a number of operators, in particular with Vodafone Malta in order to launch a number of MVNOs in Malta. The core network components feature the usual manufacturers in Cisco and Oracle, but also smaller specialist vendors like Blueslice and BroadForward. ASPIDER has also acquired technologies from eSERVGlobal for a variety of messaging and communications components.\n\nThe company headquarters are in the Netherlands, with offices in Belgium, UK, USA, France, New Zealand and Malta. In previous years the company also had offices in Seattle (US), Ireland and Egypt.\n"}
{"id": "58439071", "url": "https://en.wikipedia.org/wiki?curid=58439071", "title": "Adolf Daimler", "text": "Adolf Daimler\n\nAdolf Daimler (8 September 1871 – 24 March 1913) was the son of German inventor and industrialist Gottlieb Daimler. A mechanical engineer by training, Adolf became managing director and co-owner of his father's firm Daimler-Motoren-Gesellschaft in 1900. Along with his brother Paul Daimler, Adolf is credited with developing the distinctive Mercedes 3-pointed star logo.\n\nAdolf Daimler was born on September 8, 1871, in Karlsruhe, then the capital of the Grand Duchy of Baden. He was the second son of Gottlieb Daimler and spent his childhood in Cologne when his father served as technical director at the Deutz AG gas engine factory. In 1882, the family moved to Bad Cannstatt, one of the outer city districts of Stuttgart, then the capital of the Kingdom of Württemberg. Adolf attended the nearby Royal Institute of Stuttgart (), where he graduated in 1891. After an apprenticeship at the Maschinenfabrik Esslingen company, he studied mechanical engineering at the University of Stuttgart from 1895 to 1898. During this time he married Marie Schuler.\n\nIn 1899 Adolf joined his father's company, Daimler-Motoren-Gesellschaft (DMG), which had been founded in 1890. There he helped develop the Mercedes brand. He and his brother Paul Daimler adapted a three-pointed star to represent the company and trademarked it in 1900. Their father had once used such a symbol to mark the family's house on a postcard depicting a view of the town of Deutz. Adolf became chief engineer in 1900 and deputy board member in 1904. In 1907, he was appointed Chief Operating Officer and appointed to the Board of Directors. After a long illness, Adolf died on March 24, 1913 in the university town of Tübingen. His funeral was held on March 26, 1913 at Uff Cemetery in Bad Cannstatt, where his father had already been laid to rest a few years earlier. His funeral was attended by a number of industrialist dignitaries, including Robert Bosch.\n\n"}
{"id": "3122904", "url": "https://en.wikipedia.org/wiki?curid=3122904", "title": "American Society for Engineering Education", "text": "American Society for Engineering Education\n\nThe American Society for Engineering Education (ASEE) is a non-profit member association, founded in 1893, dedicated to promoting and improving engineering and engineering technology education. The purpose of ASEE is the advancement of education in all of its functions which pertain to engineering and allied branches of science and technology, including the processes of teaching and learning, counseling, research, extension services and public relations.\n\nA full reading of the history of ASEE can be found in a 1993 centennial article in the Journal of Engineering Education.\n\nFounded initially as the Society for the Promotion of Engineering Education (SPEE) in 1893, the society was created at a time of great growth in American higher education. In 1862, Congress passed the Morrill Land-Grant Act, which provided money for states to establish public institutions of higher education. These institutions focused on providing practical skills, especially \"for the benefit of Agriculture and the Mechanic Arts\". As a result of increasingly available higher education, more Americans started entering the workforce with advanced training in applied fields of knowledge. However, they often lacked grounding in the science and engineering principles underlying this practical knowledge.\n\nAfter a generation of students had passed through these new public universities, professors of engineering began to question whether they should adopt a more rigorous approach to teaching the fundamentals of their field. Ultimately, they concluded that engineering curricula should stress fundamental scientific and mathematical principles, not hands-on apprenticeship experiences. To organize support for this approach to engineering education, SPEE was formed in the midst of the 1893 Chicago World’s Fair. Known as the World's Columbian Exposition, this event heralded the promise of science and engineering by introducing many Americans, for example, to the wonders of electricity. Emerging out of the Fair’s World Engineering Congress, SPEE members dedicated themselves to improving engineering education at the classroom level. Over its history, the society has put out several reports on the subject, such as the Mann Report (1907), the Wickenden Study (1920s), and the Grinter Report (1955).\n\nDuring World War II, the federal government started to place more emphasis on research, prompting SPEE to form the Engineering College Research Association (ECRA), which was more concerned with research than SPEE had ever been. The ECRA spoke for most engineering researchers, sought federal funds, and collected and published information on academic engineering research. After the war, the desire to integrate the less research-oriented SPEE with the ECRA resulted in the disbanding of SPEE and the formation of ASEE in 1946.\n\nASEE was a volunteer-run organization through the 1950s. In 1961, ASEE established a staff headquarters in Washington, DC, and undertook a more activist posture. However, through the 1960s, the Vietnam War and social unrest in general made the mood on many campuses anti-technology, anti-business, and anti-establishment. In the 1960s and 1970s, ASEE presidents Merritt Williams and George Hawkins reorganized ASEE to better represent its members and return its focus to teaching. As a result of this new focus, ASEE began to administer several teaching-related government contracts, including NASA's summer faculty fellowships and the Defense Department's Civil Defense Summer Institutes and Fellowships. Currently, ASEE administers over ten government contracts, including the prestigious National Science Foundation's Graduate Research Fellowship Program.\n\nAnother result of the renewed emphasis on teaching was ASEE’s initiative for recruiting minorities and women into engineering. ASEE created the Black Engineering College Development program which used industry funding to upgrade engineering faculty in traditionally black colleges and to develop public information on these schools. ASEE also received several grants in the 1970s to research the status of women and American Indians and develop programs to attract more of these students to enter engineering. Since then, ASEE has continued to release studies on the subject in its Journal of Engineering Education, and has created divisions specifically devoted to developing programs and research in this area.\n\nASEE produces many publications on the topic of engineering education, including the general-interest \"Prism\", a monthly magazine covering the pervasive role of engineering in the world, the journals \"Journal of Engineering Education\" and \"Advances in Engineering Education,\" peer-reviewed journals covering research in engineering education, \"Profiles of Engineering and Technology Colleges\", providing data on engineering colleges and universities, and the \"eGFI: Engineering, Go For It!\" magazine and associated website, designed to attract high school students and their parents and teachers to engineering.\n\nThe magazine reports about cutting-edge technology and other important trends in engineering education, including:\n\nThe \"Journal of Engineering Education\" is a peer-reviewed academic journal published quarterly in partnership with a global community of engineering education societies and associations. The journal is a founding member of the International Federation of Engineering Education Societies.\n\n\"Advances in Engineering Education\" covers engineering education practice, especially the creative use of multimedia.\n\nThis directory provides profiles of United States and Canadian schools offering undergraduate and graduate engineering, as well as engineering technology programs with the intent of preparing prospective students for their future education in engineering.\n\n\"Computers in Education\" () is an academic journal covering all aspects of computation in education. It is published by the Northeast Consortium for Engineering Education on behalf of the Computers in Education Division of the American Society for Engineering Education.\n\n\nASEE annually recognizes the outstanding accomplishments of engineering and engineering technology educators through the ASEE awards program. By their commitment to their profession, desire to further the Society's mission, and participation in civic and community affairs, ASEE award winners exemplify the best in engineering and engineering technology education.\n\n\nASEE and its members organize a number of conferences, meetings, and workshops, foremost among them the ASEE Annual Conference. Other events include regional member meetings, professional-interest focused conferences, and K-12 teacher training.\n\nASEE administers a number of fellowship and research opportunities with funding provided by federal agencies including the Department of Defense (DOD), NASA, and the National Science Foundation(NSF). These range from programs that provide summer internships for high school students to research programs for faculty members during the summer or while on sabbatical. Programs include undergraduate and graduate research support and postdoctoral research programs for recent PhDs at government and industrial research facilities. ASEE provides support tasks that include outreach and promotion activities, application processing support, application review activities, and administration of stipend and tuition payments for program participants.\n\n"}
{"id": "8773732", "url": "https://en.wikipedia.org/wiki?curid=8773732", "title": "Anacomp", "text": "Anacomp\n\nAnacomp, Inc., is an American company that specializes in computer services and document management. It was founded in Indianapolis, Indiana, in 1968 by Ronald D. Palamare, Robert R. Sadaka, and J. Melvin Ebbert, three professors at Purdue University. Their goal was to direct the power of the computer toward the disciplines of investment management, education, urban analysis, computer science and civic systems, but is now headquartered in Chantilly, Virginia. The name Anacomp is a combination of the words ANAlyze and COMPute. Since its inception, Anacomp has made many acquisitions and spin-offs and has entered and exited different lines of business.\n\n, Anacomp's business is primarily in the service and document management sectors. In 2010, Anacomp sold off its CaseLogistix product line to Thomson Reuters. CaseLogistix is a document review application used by law firms for document review and subsequent case preparation.\n\n\n"}
{"id": "4809505", "url": "https://en.wikipedia.org/wiki?curid=4809505", "title": "Argonide", "text": "Argonide\n\nArgonide is a Florida nanotechnology company. Their main product, NanoCeram, uses aluminum oxide nanofibers for water filtration. The company was formed in 1994 by American Fred Tepper, partly to provide employment for former Russian (government) scientists. The filter uses nanofibers originally developed at the Design Technology Center (DTC) in Tomsk, Russia.\n\nNanoCeram can be incorporated into glass and cellulose non-woven sheets, is an extremely effective filtration medium. The aluminum oxide fibers, which are only 2 nanometers wide, attract dirt, bacteria, viruses, and proteins using an electrostatic effect. NanoCeram can match the particle removal effectiveness of ultrafiltration, and it allows orders-of-magnitude higher flow rates at a given pressure difference or pressure drop.\n\nArgonide has been awarded a NASA Small Business Innovation Research (SBIR) contract to filter water aboard the Space Shuttles. In 2002 it won an R&D 100 Award, given annually to the top 100 most technologically significant new products by \"R&D Magazine\".\n\nArgonide also produces nanopowders and alloys.\n\n"}
{"id": "1005984", "url": "https://en.wikipedia.org/wiki?curid=1005984", "title": "Baby transport", "text": "Baby transport\n\nVarious methods of transporting children have been used in different cultures and times. These methods include baby carriages (prams in British English), infant car seats, portable bassinets (carrycots), strollers (pushchairs), slings, backpacks, baskets and bicycle carriers.\n\nThe large, heavy prams (short for perambulator), which had become popular during the Victorian era, were replaced by lighter designs during the latter half of the 1900s.\n\nInfant carrying likely emerged early in human evolution as the emergence of bipedalism would have necessitated some means of carrying babies who could no longer cling to their mothers and/or simply sit on top of their mother's back. On-the-body carriers are designed in various forms such as baby sling, backpack carriers, and soft front or hip carriers, with varying materials and degrees of rigidity, decoration, support and confinement of the child. Slings, soft front carriers, and \"baby carriages\" are typically used for infants who lack the ability to sit or to hold their head up. Frame backpack carriers (a modification of the frame backpack), hip carriers, slings, \"mei tai\"s and a variety of other soft carriers are used for older children.\n\nImages of children being carried in slings can be seen in Egyptian artwork dating back to the time of the Pharaohs, and have been used in many indigenous cultures. One of the earliest European artworks showing baby wearing is a fresco by Giotto painted in around 1306 AD, which depicts Mary carrying Jesus in a sling. Baby wearing in a sling was well known in Europe in medieval times, but was mainly seen as a practice of marginalised groups such as beggers and gypsies. A cradleboard is a Native American baby carrier used to keep babies secure and comfortable and at the same time allowing the mothers freedom to work and travel. The cradleboards were attached to the mother’s back straps from the shoulder or the head. For travel, cradleboards could be hung on a saddle or travois. Ethnographic tradition indicates that it was common practice to cradleboard newborn children until they were able to walk, although many mothers continued to swaddle their children well past the first birthday. Bound and wrapped on a cradleboard, a baby can feel safe and secure. Soft materials such as lichens, moss and shredded bark were used for cushioning and diapers. Cradleboards were either cut from flat pieces of wood or woven from flexible twigs like willow and hazel, and cushioned with soft, absorbent materials. The design of most cradleboards is a flat surface with the child wrapped tightly to it. It is usually only able to move its head.\n\nOn-the-body baby carrying started being known in western countries in the 1960s, with the advent of the structured soft pack in the mid-1960s. Around the same time, the frame backpack quickly became a popular way to carry older babies and toddlers. In the early 1970s, the wrap was reintroduced in Germany. The two ringed sling was invented by Rayner and Fonda Garner in 1981 and popularized by Dr William Sears starting in around 1985. In the early 1990s, the modern pouch carrier was created in Hawaii. While the Chinese \"mei tai\" has been around in one form or another for centuries, it did not become popular in the west until it was modernized with padding and other adjustments. It first became popular and well known in mid-2003.\n\nPortable cradles, including cradleboards, baskets, and bassinets, have been used by many cultures to carry young infants.\n\nWheeled devices are generally divided into prams, used for newborn babies in which the infant normally lies down facing the pusher, and the strollers, which are used for the small child up to about three years old in a sitting position facing forward.\n\nWilliam Kent developed an early stroller in 1733. In 1733, the Duke of Devonshire asked Kent to build a means of transport that would carry his children. Kent obliged by constructing a shell shaped basket on wheels that the children could sit in. This was richly decorated and meant to be pulled by a goat or small pony. Benjamin Potter Crandall sold baby carriages in the US in the 1830s which have been described as the \"first baby carriages manufactured in the US\" However, it has been argued that F.A. Whitney Carriage Company was the first. His son, Jesse Armour Crandall was issued a number of patents for improvements and additions to the standard models. These included adding a brake to carriages, a model which folded, designs for parasols and an umbrella hanger. By 1840, the baby carriage became extremely popular. Queen Victoria bought three carriages from Hitchings Baby Store.\n\nThe carriages of those days were built of wood or wicker and held together by expensive brass joints. These sometimes became heavily ornamented works of art. Models were also named after royalty, Princess and Duchess being popular names, as well as Balmoral and Windsor.\n\nIn June 1889, William H. Richardson patented his idea of the first reversible stroller. The bassinet was designed so it could face out or in towards the parent. He also made structural changes to the carriage. Until then the axis did not allow each wheel to move separately, Richardson’s design allowed this, which increased maneuverability of the carriages. As the 1920s began, prams were now available to all families and were becoming safer, with larger wheels, brakes, deeper prams, and lower, sturdier frames.\n\nIn 1965, Owen Maclaren, an aeronautical engineer, worked on complaints his daughter made about travelling from England to America with her heavy pram. Using his knowledge of aeroplanes, Maclaren designed a stroller with an aluminium frame and created the first true umbrella stroller. He then went on to found Maclaren, which manufactured and sold his new design. The design took off and soon “strollers” were easier to transport and used everywhere.\n\nIn the 1970s, however, the trend was more towards a more basic version, not fully sprung, and with a detachable body known as a \"carrycot\". Now, prams are very rarely used, being large and expensive when compared with \"buggies\" (see below). One of the longer lived and better known brands in the UK is Silver Cross, first manufactured in Hunslet, Leeds, in 1877, and later Guiseley from 1936 until 2002 when the factory closed. Silver Cross was then bought by the toy company David Halsall and Sons who relocated the head office to Skipton and expanded into a range of new, modern baby products including pushchairs and \"travel systems\". They continue to sell the traditional Silver Cross coach prams which are manufactured at a factory in Bingley in Yorkshire.\n\nSince the 1980s, the stroller industry has developed with new features, safer construction and more accessories.\n\nLarger and heavier prams, or perambulators had been used since their introduction in the Victorian era; prams were also used for infants, often sitting up. The term carrycot became more common in the UK after the introduction of lighter units with detachable baby carriers in the 1970s.\n\nAs they developed through the years suspension was added, making the ride smoother for both the baby and the person pushing it.\n\n'Strollers' (North American English) or 'push chairs/buggies' (English), are used for small children up to about three years old in a sitting position facing forward.\n\n\"Pushchair\" was the popularly used term in the UK between its invention and the early 1980s, when a more compact design known as a \"buggy\" became the trend, popularised by the conveniently collapsible aluminium framed Maclaren buggy designed and patented by the British aeronautical designer Owen Maclaren in 1965. \"Pushchair\" is the usual term in the UK, but is becoming increasingly replaced by buggy; in American English, buggy is synonymous with baby carriage. Newer versions can be configured to carry a baby lying down like a low pram and then be reconfigured to carry the child in the forward-facing position.\n\nA variety of twin pushchairs are manufactured, some designed for babies of a similar age (such as twins) and some for those with a small age gap. Triple pushchairs are a fairly recent addition, due to the number of multiple births being on the increase. Safety guidelines for standard pushchairs apply. Most triple buggies have a weight limit of 50 kg and recommended use for children up to the age of 4 years.\n\nA travel system is typically a set consisting of a chassis with a detachable baby seat and/or carrycot. Thus a travel system can be switched between a pushchair and a pram. Another benefit of a travel system is that the detached chassis (generally an umbrella closing chassis) when folded will usually be smaller than other types, to transport it in a car trunk or boot. Also, the baby seat will snap into a base meant to stay in an automobile, becoming a car seat. This allows undisturbed movement of the baby into or out of a car and a reduced chance of waking a sleeping baby.\n\nAnother modern design showcases a stroller that includes the possibility for the lower body to be elongated, thereby transforming the stroller into a kick scooter. Steering occurs by leaning towards either side. Depending on the model, it can be equipped with a foot- and/or handbrake. Speeds up to 10 mph can be reached. The first stroller of this kind was the so-called “Roller Buggy”, developed by industrial designer Valentin Vodev in 2005. In 2012 the manufacturer Quinny became interested in the concept and teamed up with a Belgian studio to design another model.\n\nThe modern infant car seat is a relative latecomer. It is used to carry a child within a car. Such car seats are required by law in many countries to safely transport young children.\n\nBicycles can be fitted with a bicycle trailer or a children's bicycle seat to carry small children. An older child can ride on a one-wheel trailer bike with an integrated seat and handle bars.\n\nA \"travel system\" includes a car seat base, an infant car seat, and a baby stroller. The car seat base is installed in a car. The infant car seat snaps into the car seat base when traveling with a baby. From the car, the infant car seat can be hand carried and snapped onto the stroller.\n\n\n"}
{"id": "297124", "url": "https://en.wikipedia.org/wiki?curid=297124", "title": "Breakthrough Propulsion Physics Program", "text": "Breakthrough Propulsion Physics Program\n\nThe Breakthrough Propulsion Physics Project (BPP) was a research project funded by NASA from 1996-2002 to study various proposals for revolutionary methods of spacecraft propulsion that would require breakthroughs in physics before they could be realized. The project ended in 2002, when the Advanced Space Transportation Program was reorganized and all speculative research (less than Technology Readiness Level 3) was cancelled.\nDuring its six years of operational funding, this program received a total investment of $1.2 million. \n\nThe Breakthrough Propulsion Physics project addressed a selection of “incremental and affordable” research questions towards the overall goal of propellantless propulsion, hyperfast travel, and breakthrough propulsion methods. It selected and funded five external projects, two in-house tasks and one\nminor grant.\nAt the end of the project, conclusions into fourteen topics, including these funded projects, were summarized by program manager Marc G. Millis. Of these, six research avenues were found to be nonviable, four were identified as opportunities for continued research, and four remain unresolved.\n\nOne in-house experiment tested the Schlicher thruster antenna, claimed by Schlicher to generate thrust. No thrust was observed.\n\nAnother experiment examined a gravity shielding mechanism claimed by Podkletnov and Nieminen. Experimental investigation on the BPPP and other experiments found no evidence of the effect.\n\nResearch on quantum tunneling was sponsored by the BPPP. It was concluded that this is not a mechanism for faster-than-light travel.\n\nOther approaches categorized as non-viable are oscillation thrusters and gyroscopic antigravity, Hooper antigravity coils, and coronal blowers.\n\nA theoretical examination of additional atomic energy levels (deep Dirac levels) was carried out. Some states were ruled out, but the problem remains unsolved.\n\nExperiments tested Woodward’s theory of inducing transient inertia by electromagnetic fields. The small effect could not be confirmed. Woodward continued refining the experiments and theory. Independent experiments also remained inconclusive.\n\nA possible torsion-like effect in the coupling between electromagnetism and spacetime, which may ultimately be useful for propulsion, was sought in experiments. The experiments were insufficient to resolve the question.\n\nOther theories listed in Millis' final assessment as unresolved are Abraham–Minkowski electromagnetic momentum, interpreting inertia and gravity quantum vacuum effects, and the Podkletnov force beam.\n\nOne of the eight tasks funded by the BPP program was to define a strategy towards space drives. \n\nAs a motivation, seven examples of hypothetical space drives were described at the onset of the project. These included the gravity-based pitch drive, bias drive, disjunction drive and diametric drive; the Alcubierre drive; and the vacuum energy based differential sail.\n\nThe project then considered the mechanisms behind these drives. At the end of the project, three mechanisms were identified as areas for future research. One considers the possibility of a reaction mass in seemingly empty space, for example in dark matter, dark energy, or zero-point energy. Another approach is to reconsider Mach's principle and Euclidean space. A third research avenue that might ultimately prove useful for spacecraft propulsion is the coupling of fundamental forces on sub-atomic scales.\n\nOne topic of investigations was the use of the zero-point energy field. As the Heisenberg uncertainty principle implies that there is no such thing as an exact amount of energy in an exact location, vacuum fluctuations are known to lead to discernible effects such as the Casimir effect. The differential sail is a speculative drive, based on the possibility of inducing differences in the pressure of vacuum fluctuations on either side of a sail-like structure — with the pressure being somehow reduced on the forward surface of the sail, but pushing as normal on the raft surface — and thus propel a vehicle forward.\n\nThe Casimir effect was investigated experimentally and analytically under the Breakthrough Propulsion Physics project. This included the construction of MicroElectroMechanical\n(MEM) rectangular Casimir cavities. Theoretical work showed that the effect could be used to create net forces, although the forces would be extremely small. At the conclusion of the project, the Casimir effect was categorized as an avenue for future research.\n\nAfter funding ended, program manager Marc G. Millis was supported by NASA to complete documentation of results. The book \"Frontiers of Propulsion Science\" was published by the AIAA in February 2009, providing a deeper explanation of several propulsion methods.\n\nFollowing program cancellation in 2002, Millis and others founded the Tau Zero Foundation.\n\n"}
{"id": "40121768", "url": "https://en.wikipedia.org/wiki?curid=40121768", "title": "Britain's Greatest Machines with Chris Barrie", "text": "Britain's Greatest Machines with Chris Barrie\n\nBritain's Greatest Machines with Chris Barrie is a documentary television series from National Geographic Channel. It is showing the technological progress of the 19th and 20th centuries from a British point of view. Chris Barrie is the host and is testing various means of transportation.\n\n"}
{"id": "27696778", "url": "https://en.wikipedia.org/wiki?curid=27696778", "title": "Bronchitis kettle", "text": "Bronchitis kettle\n\nThe bronchitis kettle, typified by a long spout, was used in the nineteenth and twentieth centuries to moisten the air for a sufferer of bronchitis, and was considered to make it easier to breathe for the patient. Sometimes menthol was added to the water to relieve congestion. The water was boiled on the fireplace in the room, or above a spirit lamp, or, in the twentieth century, by electricity. Sometimes the kettle was boiled within a tent placed around the patient.\n\n"}
{"id": "1665985", "url": "https://en.wikipedia.org/wiki?curid=1665985", "title": "Burr mill", "text": "Burr mill\n\nA burr mill, or burr grinder, is a mill used to grind hard, small food products between two revolving abrasive surfaces separated by a distance usually set by the user. When the two surfaces are set far apart, the resulting ground material is coarser, and when the two surfaces are set closer together, the resulting ground material is finer and smaller. Often, the device includes a revolving screw that pushes the food through. It may be powered electrically or manually.\n\nBurr mills do not heat the ground product by friction as much as do blade grinders (\"choppers\"), and produce particles of a uniform size determined by the separation between the grinding surfaces.\n\nFood burr mills are usually manufactured for a single purpose: coffee beans, dried peppercorns, coarse salt, spices, or poppy seeds, for example. Coffee mills are usually powered by electric motors; domestic pepper, salt, and spice mills, used to sprinkle a little seasoning on food, are usually operated manually, sometimes by a battery-powered motor.\n\nThe uniform particle size achieved using a burr grinder can be desirable for coffee preparation. However, some methods of brewing may be more tolerant of the range of particle sizes produced by a blade grinder; this may be the case for percolated coffee. Regardless, burr coffee grinders are more suited for keeping the flavor and aroma of the coffee beans intact. This is because burr mills produce less heat from friction compared to blade grinders; this is especially important for coffee aficionados looking to get the most flavor from the freshly ground beans, as heat from friction lessens or taints the natural flavor. Burr grinders create less friction and require lower motor speeds, which reduces potential flavor loss due to heat.\n\nBurr grinders obtain these lower speeds through two mechanisms. The lower cost models generally use a small electric motor to drive a series of reduction gears, while better constructed and more costly examples use a larger commercial motor and a belt, with no gear reduction to spin the burrs. The latter example is termed \"direct drive\". The reduction gear versions are noisier and usually do not have the lifespan of the direct drive units.\n\nElectrical powered burr grinders are available in many variations. Some grinders are \"stepped\" meaning that they are fixed by the factory into a set series of adjustments while \"stepless\" varieties use a worm drive or other mechanisms to offer an infinite number of adjustments within their grind range. Other variations include grinders that are equipped with dosers and others that are \"doserless\". Dosers function as a catch bin to collect the ground coffee and by using a lever, users are able to dose a set amount of coffee. Doserless versions remove the bin and dosing function, and the grinder outputs the ground coffee directly into an espresso machine portafilter or into another container. The doserless examples normally feature additional functions such as weight based or advance time based grinding in order for a barista to grind for an exact amount of grounds required for a specific espresso shot.\n\nManual coffee grinders have been largely supplanted by electrically powered ones where the object is simply to get the job done; manual grinders are used more for their appearance, and are often more costly than electric models. An exception is the manual Turkish coffee grinder; these are inexpensive, and can grind coffee to fine powder for Turkish coffee, unlike electric models.\n\nMany grinders are free-standing; some larger coffee grinders are fixed to a wall.\n\nManual burr grinders are turned by hand, rotating one grinding surface against the other. Coffee mills usually have a handle, providing leverage for the many turns required to grind enough coffee for a cup. The ground coffee is collected in a container which is part of the mill.\n\nSalt, pepper, and spice mills, essentially the same as coffee mills, usually do not have a handle, the entire top rotating instead. While this is less convenient, only a few turns are required to grind enough. The ground product falls directly onto the food being seasoned; the mill has no container. A few designs have abrasive surfaces which do not rotate; each squeeze of the handles moves one flat plate past another, then the plates are restored to their original position by a spring. Many hard spices are available in containers incorporating a simple cone burr grinder, intended to be discarded when empty.\n\nMost grinders can be adjusted to set the fineness of grind.\n\nManual mills can be used for grinding other food products than they are intended for, but mills designed for pepper grinding are inappropriate for producing finely-ground flour. Laura Ingalls Wilder's novel \"The Long Winter\" describes a family grinding wheat in a coffee mill to make flour during months of hardship.\n\nThe first coffee grinder was made by Richard Dearmann, an English blacksmith from Birmingham, in 1799. This grinder was widely distributed in the US, where Increase Wilson patented the first wall coffee grinder in 1818.\n\nPeugeot of France patented a pepper grinder in 1842. The mechanism of case-hardened steel cracked the peppercorns before the actual grinding process. The grooves on the Peugeot mechanism were individually cut into the metal and then case-hardened, making them very durable.\n\nThere are several types of materials used in pepper mills, each with its own particular advantages. Corrosion-resistant materials are used to grind salt.\n\nStainless steel: One of the most suitable and durable materials for grinding peppercorns and coffee beans. The male and female sections of the grinding mechanism are usually made from sintered metal. This material is preferred by professional chefs. The teeth of the grinder are machined to cut spice or beans. Stainless steel is not suggested for grinding salt.\n\nZinc alloy: Perhaps the most common mechanism found in pepper grinders, zinc alloy is composed of a mixture of metals, primarily zinc, often with chrome plating to resist corrosion. It is a good choice for grinding pepper but is not suitable for grinding salt.\n\nCarbon steel: An extremely hard metal, carbon steel provides the sharpest edges and most efficient grinding capability, and is preferred by professional chefs. Carbon steel is not suitable for grinding salt.\n\nCeramic: Ceramic is extremely hard and provides the best performance for multi-use grinding. It does not corrode and is suitable for grinding coffee beans, pepper, salt, and spices.\n\nAcrylic: Durable and low cost, acrylic is a non-corrosive material suitable for grinding salt and spices.\n\nElectric burr grinders are powered by electricity from a battery or mains supply. An electric motor drives the grinding elements against each other. Electric grinders grind faster than manual grinders with no effort, but friction and waste heat from the motor may heat the ground product slightly.\n\n"}
{"id": "53044507", "url": "https://en.wikipedia.org/wiki?curid=53044507", "title": "Callus shaver", "text": "Callus shaver\n\nA callus shaver (also called a credo knife) is a tool for the medical and cosmetic removal of a callus, mainly on the feet (see also pedicure), or hands. Technically speaking, its function is the abrasive treatment of hyperkeratotic skin lesions.\n\nThe callus shaver consists of a handle, which is usually arched, and a blade made of ceramic or metal. Because the shaver is very sharp, injuries can result from careless use. For this reason the shaver is illegal in some states in the USA.A cellulitis infection is possible if the skin is cut, especially if the shaver is shared with others and not properly sterilized. In very severe cases, amputation or death can result.\n\nTreatment with a callus shaver should involve the user in a longer-term care plan. Calluses are usually several skin layers deep, which means that treatment must be repeated several times on the affected area (usually the feet) if it is to remain callus free. However, there should be a period of a few days between the individual treatments, otherwise skin irritations and/or an even thicker callus may occur.\n\nThe callus shaver should not be confused with a foot file (foot rasp), which is used during the after-treatment of working with a callus shaver.\n\nThe callus shaver and other instruments date back to the instruments mentioned by the Frenchman Nicolas-Laurent LaForest in his 1782 book \"L’art de soigner les pieds\".\n"}
{"id": "716106", "url": "https://en.wikipedia.org/wiki?curid=716106", "title": "Capacitance Electronic Disc", "text": "Capacitance Electronic Disc\n\nThe Capacitance Electronic Disc (CED) is an analog video disc playback system developed by RCA, in which video and audio could be played back on a TV set using a special needle and high-density groove system similar to phonograph records.\n\nFirst conceived in 1964, the CED system was widely seen as a technological success which was able to increase the density of a long-playing record by two orders of magnitude. Despite this achievement, the CED system fell victim to poor planning, conflicts within RCA, and technical difficulties that slowed development and stalled production of the system for 17 years—until 1981, by which time it had already been made obsolete by laser videodisc (DiscoVision, later called LaserVision and LaserDisc) as well as Betamax and VHS video cassette formats. Sales for the system were nowhere near projected estimates. In 1984—before it was absorbed by General Electric—RCA discontinued player production, continuing software production until 1986, losing an estimated $600 million in the process. RCA had initially intended to release the SKT425 CED player with their high end Dimensia system in 1984, but cancelled CED player production just prior to the Dimensia system's release.\n\nThe format was commonly known as \"videodisc\", leading to much confusion with the contemporaneous LaserDisc format. LaserDiscs are read optically with a laser beam, whereas CED discs are read physically with a stylus (similar to a conventional gramophone record). The two systems are mutually incompatible.\n\nRCA used the brand \"SelectaVision\" for the CED system, a name also used for some early RCA brand VCRs, and other experimental projects at RCA.\n\nRCA began videodisc research in 1964, in an attempt to produce a phonograph-like method of reproducing video. Research and development was slow in the early years, as the development team originally comprised only four men, but by 1972, the CED team at RCA had produced a disc capable of holding ten minutes of color video (a portion of the \"Get Smart\" episode \"A Tale of Two Tails\", re-titled \"Lum Fong\").\n\nThe first CED prototype discs were multi-layered, consisting of a vinyl substrate, nickel conductive layer, glow-discharge insulating layer and silicone lubricant top layer. However, failure to fully solve the stylus and disc wear and complexity of manufacturing forced RCA to search for simpler solutions to the problem for constructing the disc. The final disc was crafted using PVC blended with carbon to allow the disc to be conductive. To preserve stylus and groove life, a thin layer of silicone was applied to the disc as a lubricant.\n\nCED videodiscs were originally meant to be sold in jackets and handled by hand similar to audio records, but during testing, it was shown that exposure to dust caused skipped grooves. It was learned that if dust was allowed to settle on the discs, then the dust would absorb moisture from the air and cement the dust particle to the disc surface, causing the stylus to jump back in a locked groove situation. Thus, an idea was developed in which the disc would be stored and handled in a caddy from which the CED would be extracted by the player so that exposure to dust would be minimized.\n\nAfter 17 years of research and development, the first CED player (model SFT100W) was released on March 22, 1981. A catalog of approximately 50 titles was released at the same time. The first title to be manufactured was \"Race for Your Life, Charlie Brown\". Fifteen months later, RCA released the SGT200 and SGT250 players, both with stereo sound while the SGT-250 was also the first CED player model to include a wireless remote control. Models with random access hit the market in 1983.\n\nSeveral problems doomed the new CED system before it was even introduced. From an early point in the development of the CED system, it was clear that VCRs and home videotape—with their longer storage capacity and recording capabilities—would pose a threat to the system. However, development pushed ahead. Once finally released, sales for the new CED players were slow; RCA had expected to sell 200,000 players by the beginning of 1982, but only about half that number had been sold, and there was little improvement in sales throughout 1982 and 1983.\nThe extremely long period of development—caused in part by political turmoil and a great deal of turnover in the high management of RCA—also contributed to the demise of the CED system. RCA had originally slated the videodisc system for a 1977 release. However, the discs were still not able to hold more than thirty minutes of video per side, and the nickel-like material used to make discs was not sturdy enough to put into manufacturing. Signal degradation was also an issue, as the handling of the discs was causing them to deteriorate more rapidly than expected, baffling engineers.\n\nRCA had hoped that by 1985 CED players would be in close to 50% of American homes, but the sales of players continued to drop. RCA attempted to cut the prices of CED players and offer special incentives to consumers such as rebates and free discs, but sales did not improve. RCA management realized that the system would never be profitable and announced the discontinuation of production of CED players on April 4, 1984. In an unexpected twist, demand for the videodiscs themselves became high immediately after the announcement, so RCA alerted customers that videodiscs would continue to be produced and new titles released for at least another three years after the discontinuation of players. Shortly after this announcement, however, the sale of discs declined sharply, prompting RCA to abandon disc production after only two years, in 1986.\nThe last titles released were \"The Jewel of the Nile\" by CBS/Fox Video, and \"Memories of VideoDisc\", a commemorative CED given to many RCA employees involved with the CED project, both in 1986.\n\nCEDs are conductive vinyl platters that are in diameter. To avoid metric names they are usually called \"12 inch discs\". A CED has a spiral groove on both sides. The groove is 657 nm wide and has a length of up to 12 miles (19 km). The discs rotate at a constant angular speed during playback (450 rpm for NTSC, 375 rpm for PAL) and each rotation contains 8 interlaced fields, or 4 full frames of video. These appear as spokes on the disc surface, with the gap between each field clearly visible under certain light. This meant that freeze frame was impossible on players without an expensive electronic frame store facility.\n\nA keel-shaped needle with a titanium electrode layer rides in the groove with extremely light tracking force (65 mg) and an electronic circuit is formed through the disc and stylus. Like an audio turntable, the stylus reads the disc, starting at the outer edge and going towards the center. The video and audio signals are stored on the Videodiscs in a composite analog signal which is encoded into vertical undulations in the bottom of the groove, somewhat like pits. These undulations have a shorter wavelength than the length of the stylus tip in the groove, and the stylus rides over them; the varying distance between the stylus tip and the conductive surface due to the depth of the undulations in the groove under the stylus directly controls the capacitance between the stylus and the conductive carbon-loaded PVC disc. This varying capacitance in turn alters the frequency of a resonant circuit, producing an FM electrical signal, which is then decoded into video and audio signals by the player's electronics.\n\nThe capacitive stylus pickup system which gives the CED its name can be contrasted with the technology of the conventional phonograph. Whereas the phonograph stylus physically vibrates with the variations in the record groove, and those vibrations are converted by a mechanical transducer (the phono pickup) to an electrical signal, the CED stylus normally does not vibrate and moves only to track the CED groove (and the disc surface—out-of-plane), while the signal from the stylus is natively obtained as an electrical signal. This more sophisticated system, combined with a high revolution rate, is necessary to enable the encoding of video signals with bandwidth of a few megahertz, compared to a maximum of 20 kilohertz for an audio-only signal—a difference of two orders of magnitude. Also, while the undulations in the bottom of the groove may be likened to pits, it is important to note that the spacing of vertical wave crests and troughs in a CED groove is continuously variable, as the CED is an analog medium. Usually, the term \"pits\", when used in the context of information media, refers to features with sharply defined edges and discrete lengths and depths, such as the pits on digital optical media such as CDs and DVDs.\n\nIn order to maintain an extremely light tracking force, the stylus arm is surrounded by coils, which sense deflection, and a circuit in the player responds to the signals from these coils by moving the stylus head carriage in steps as the groove pulls the stylus across the disc. Other coils are used to deflect the stylus, to finely adjust tracking. This system is very similar to—yet predates—the one used in Compact Disc players to follow the spiral optical track, where typically a servo motor moves the optical pickup in steps for coarse tracking and a set of coils shifts the laser lens for fine tracking, both guided by an optical sensing device, which is the analogue of CED stylus-deflection sensing coils. For the CED player, this tracking arrangement has the additional benefit that the stylus drag angle remains uniformly tangent to the groove, unlike the case for a phonograph tonearm, in which the stylus drag angle and consequently the stylus side force varies with the tonearm angle, which in turn depends on the radial position on the record of the stylus. Whereas for a phonograph, where the stylus has a pinpoint tip, linear tracking is merely ideal to reduce wear of records and styli and to maximize tracking stability, for a CED player linear tracking is a necessity for the keel-shaped stylus, which must always stay tangent to the groove. Furthermore, the achievement of an extremely light tracking force on the CED stylus enables the use of a fine groove pitch (i.e. fine spacing of adjacent revolutions of the spiral), necessary to provide a long playing time at the required high rotational speed, while also limiting the rate of disc and stylus wear.\n\nThe disc is stored inside a caddy, from which the player extracts it when it is loaded. The disc itself is surrounded by a \"spine\", a plastic ring (actually square on the outside edge) with a thick, straight rim-like edge, which extends outside of, and latches into, the caddy. When a person inserts a caddy containing a disc into the player, the player captures the spine, and both the disc and the spine are left in the player as the person pulls the caddy out. The inner edges of the opening of the caddy have felt strips designed to catch any dust or other debris that could be on the disc as it is extracted. Once the caddy has been withdrawn by the person, the player loads the disc onto the turntable, either manually with all SFT and most SGT prefix RCA players or automatically with the RCA SGT-250 and all other models and brands of players. When playback has been started, the player spins the disc up to speed while moving the pickup arm over the disc surface and lowering the stylus onto the beginning of the disc.\n\nWhen Stop is pressed, the stylus is lifted from the disc and returned to its parking location, and the disc and spine are lifted up again to align with the caddy slot. When ready, the slot is unlocked, and the caddy can be inserted and withdrawn by a person, now with the disc back inside.\n\nCED players, from an early point in their life, appealed to a lower-income market more than VHS, Betamax, and LaserDisc. The video quality (approximately 3 MHz of luma bandwidth for CED) was comparable to a VHS-SP or Betamax-II video, but sub-par compared to LaserDisc (about 5 MHz of luma bandwidth).\n\nCED players were intended to be \"low-cost\".\nBecause they have fewer precision parts than a VCR,\na CED player cost, at most, about half as much to manufacture.\nThe discs themselves could be inexpensively duplicated, stamped out on slightly-modified audio gramophone record presses.\n\nLike VCRs, CED videodisc players had features like rapid forward/reverse and visual search forward/reverse. They also had a pause feature, though it blanked the screen rather than displaying a still image; many players featured a \"page mode\", during which the current block of four successive frames would be repeatedly displayed.\n\nSince CEDs were a disc-based system, they did not require rewinding. Early discs were generally monaural, but later discs included stereo sound. (Monaural CED discs were packaged in white protective caddies, while stereo discs were packaged in blue protective caddies.) Other discs could be switched between two separate mono audio tracks, providing features such as bilingual audio capability.\n\nLike the LaserDisc and DVD, some CEDs feature random access and that users can quickly move to certain parts of the movie. Each side of a CED disc could be split into up to 63 \"chapters\", or bands. Two late RCA players (the SJT400 and SKT400) could access these bands in any given order. Unlike its laser-based counterparts, the chapters in a CED are based on minutes of the film, not scenes.\n\nNovelty discs and CED-based games were produced whereby accessing the chapters in a specified order would string together a different story each time. However, only a few were produced before the halt of CED player manufacturing.\n\nIn comparison with LaserDisc technology, CEDs suffered from the fact that they were a phonograph-related contact medium. RCA estimated that the number of times a CED could be played back, under ideal conditions, was 500. By comparison, a clean, laser rot-free LaserDisc could, at least in theory, be played an unlimited number of times (although, repeated handling might still result in damage). Since the CED system used a stylus to read the discs, it was necessary to regularly change the stylus in the player to avoid damage to the videodiscs.\n\nWorn and damaged discs also caused problems for consumers. When a disc began to wear, video and audio quality would severely decline, and the disc would begin to skip more. Several discs suffered from a condition called \"video virus\", where a CED would skip a great deal due to dust particles stuck in the grooves of the disc. However, playing the disc several times would generally solve this problem.\n\nUnlike VHS tapes, CEDs required a disc flip at some point during the course of almost all films, because only sixty minutes of video could be stored per side (75 mins on UK PAL discs due to the slower rotation speed). If a feature ran over two hours, it would be necessary to spread the feature over two discs. (In some cases, if a movie's theatrical running time was only slightly longer than two hours—from 120 minutes and a few seconds to 122 minutes—studios would often trim short scenes throughout the movie or employ time compression, speeding the extra run time out of the film, in order to avoid the expense of issuing two discs.) This problem was not unique to CEDs, as LaserDiscs presented the same difficulty, and some longer features, such as \"The Ten Commandments\" (1956), still required more than one tape or disc in the VHS, Beta, and LaserDisc formats. There were no two-disc UK PAL releases.\n\nLess significant disadvantages include lack of support for freeze-frame during pause, since CEDs scanned four frames in one rotation versus one frame per rotation on CAV LaserDisc, nor was computer technology advanced enough at the time to outfit the player with a framebuffer affordably. However, a \"page mode\" was available on many players that would allow those four frames to be repeated in an endless loop.\n\nCEDs were also larger than VHS tapes, thicker than LaserDiscs and considerably heavier due to the plastic caddies.\n\nCED players were manufactured by four companies—RCA, Hitachi, Sanyo, and Toshiba—but seven other companies marketed players manufactured by these companies.\n\nUpon release, 50 titles were available for the CED; along with RCA (which included the company's partnership with Columbia Pictures plus Paramount and Disney releases), CBS Video Enterprises (later CBS/FOX Video) produced the first 50 titles. Eventually, Disney, MGM, Paramount Pictures, MCA, Vestron Video, and other labels began to produce CED discs under their own home video labels, and did so until the end of disc manufacturing in 1986.\n\n\n\n"}
{"id": "471126", "url": "https://en.wikipedia.org/wiki?curid=471126", "title": "Clouds and the Earth's Radiant Energy System", "text": "Clouds and the Earth's Radiant Energy System\n\nClouds and the Earth's Radiant Energy System (CERES) is NASA climatological experiment from Earth orbit. The CERES are scientific satellite instruments, part of the NASA's Earth Observing System (EOS), designed to measure both solar-reflected and Earth-emitted radiation from the top of the atmosphere (TOA) to the Earth's surface. Cloud properties are determined using simultaneous measurements by other EOS instruments such as the Moderate Resolution Imaging Spectroradiometer (MODIS). Results from the CERES and other NASA missions, such as the Earth Radiation Budget Experiment (ERBE), could lead to a better understanding of the role of clouds and the energy cycle in global climate change.\n\nCERES experiment has four main objectives:\n\n\nEach CERES instrument is a radiometer which has three channels – a shortwave (SW) channel to measure reflected sunlight in 0.2–5 µm region, a channel to measure Earth-emitted thermal radiation in the 8–12 µm \"window\" or \"WN\" region, and a Total channel to measure entire spectrum of outgoing Earth's radiation (>0.2 µm). The CERES instrument was based on the successful Earth Radiation Budget Experiment which used three satellites to provide global energy budget measurements from 1984 to 1993.\n\nFor a climate data record (CDR) mission like CERES, accuracy is of high importance and achieved for pure infrared nighttime measurements by use of a ground laboratory SI traceable blackbody to determine total and WN channel radiometric gains. This however was not the case for CERES solar channels such as SW and solar portion of the Total telescope, which have no direct un-broken chain to SI traceability. This is because CERES solar responses were measured on ground using lamps whose output energy were estimated by a cryo-cavity reference detector, which used a silver Cassegrain telescope identical to CERES devices to match the satellite instrument field of view. The reflectivity of this telescope built and used since the mid-1990s was never actually measured, estimated only based on witness samples (see slide 9 of Priestley et al. (2014)). Such difficulties in ground calibration, combined with suspected on-ground contamination events have resulted in the need to make unexplained ground to flight changes in SW detector gains as big as 8%, simply to make the ERB data seem somewhat reasonable to climate science (note that CERES currently claims a one sigma SW absolute accuracy of 0.9%).\n\nCERES spatial resolution at nadir view (equivalent diameter of the footprint) is 10 km for CERES on TRMM, and 20 km for CERES on Terra and Aqua satellites. Perhaps of greater importance for missions such as CERES is calibration stability, or the ability to track and partition instrumental changes from Earth data so it tracks true climate change with confidence. CERES onboard calibration sources intended to achieve this for channels measuring reflected sunlight include solar diffusers and tungsten lamps. However the lamps have very little output in the important ultraviolet wavelength region where degradation is greatest and they have been seen to drift in energy by over 1.4% in ground tests, without a capability to monitor them on-orbit (Priestley et al. (2001)). The solar diffusers have also degraded greatly in orbit such that they have been declared unusable by Priestley et al. (2011). A pair of black body cavities that can be controlled at different temperatures are used for the Total and WN channels, but these have not been proved stable to better than 0.5%/decade. Cold space observations and internal calibration are performed during normal Earth scans.\n\nThe first CERES instrument Proto-Flight Module (PFM) was launched aboard the NASA Tropical Rainfall Measuring Mission (TRMM) in November 1997 from Japan. However, this instrument failed to operate after 8 months due to an on-board circuit failure.\n\nAn additional six CERES instruments were launched on the Earth Observing System and the Joint Polar Satellite System. The Terra satellite, launched in December 1999, carried two (Flight Module 1 (FM1) and FM2) and the Aqua satellite, launched in May 2002, carried two more (FM3 and FM4). A fifth instrument (FM5) was launched on the Suomi NPP satellite in October 2011 and a sixth (FM6) on NOAA-20 in November 2017. With the failure of the PFM on TRMM and the 2005 loss of the SW channel of FM4 on Aqua, there are five of the CERES Flight Modules that are fully operational as of 2017.\n\nThe measurements of the CERES instruments will be furthered by the Radiation Budget Instrument (RBI) to be launched on Joint Polar Satellite System-2 (JPSS-2) in 2021, JPSS-3 in 2026, and JPSS-4 in 2031. The Trump administration however seems set to cancel the RBI project, despite most of the money for it already having been spent.\n\nCERES operates in three scanning modes: across the satellite ground track (cross-track), along the direction of the satellite ground track (along-track), and in a Rotating Azimuth Plane (RAP). In RAP mode, the radiometers scan in elevation as they rotate in azimuth, thus acquiring radiance measurement from a wide range of viewing angles. Until February 2005, on Terra and Aqua satellites one of CERES instruments scanned in cross-track mode while the other was in RAP or along-track mode. The instrument operating in RAP scanning mode took two days of along-track data every month. However the multi-angular CERES data allowed to derive new models which account for anisotropy of the viewed scene, and allow TOA radiative flux retrieval with enhanced precision.\n\n\n"}
{"id": "5323", "url": "https://en.wikipedia.org/wiki?curid=5323", "title": "Computer science", "text": "Computer science\n\nComputer science is the theory, experimentation, and engineering that form the basis for the design and use of computers. It involves the study of algorithms that process, store, and communicate digital information. A computer scientist specializes in the theory of computation and the design of computational systems.\n\nIts fields can be divided into a variety of theoretical and practical disciplines. Some fields, such as computational complexity theory (which explores the fundamental properties of computational and intractable problems), are highly abstract, while fields such as computer graphics emphasize real-world visual applications. Other fields focus on challenges in implementing computation. For example, programming language theory considers various approaches to the description of computation, while the study of computer programming itself investigates various aspects of the use of programming languages and complex systems. Human–computer interaction considers the challenges in making computers and computations useful, usable, and universally accessible to humans.\n\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Further, algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.\n\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. He may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he released his simplified arithmometer, which was the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first \"automatic mechanical calculator\", his Difference Engine, in 1822, which eventually gave him the idea of the first \"programmable mechanical calculator\", his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first computer program. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".\n\nDuring the 1940s, as new and more powerful computing machines were developed, the term \"computer\" came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science degree program in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\n\nAlthough many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted among the greater academic population. It is the now well-known IBM brand that formed part of the computer science revolution during this time. IBM (short for International Business Machines) released the IBM 704 and later the IBM 709 computers, which were widely used during the exploration period of such devices. \"Still, working with the IBM [computer] was frustrating […] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to start the whole process over again\". During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were commonplace.\n\nTime has seen significant improvements in the usability and effectiveness of computing technology. Modern society has seen a significant shift in the users of computer technology, from usage only by experts and professionals, to a near-ubiquitous user base. Initially, computers were quite costly, and some degree of humanitarian aid was needed for efficient use—in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was needed for common usage.\nDespite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and society—in fact, along with electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the information revolution, seen as the third major leap in human technological progress after the Industrial Revolution (1750–1850 CE) and the Agricultural Revolution (8000–5000 BC).\n\nThese contributions include:\n\nAlthough first proposed in 1956, the term \"computer science\" appears in a 1959 article in \"Communications of the ACM\",\nin which Louis Fein argues for the creation of a \"Graduate School in Computer Sciences\" analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such programs, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term \"computing science\", to emphasize precisely that difference. Danish scientist Peter Naur suggested the term \"datalogy\", to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a distinct field of data analysis, including statistics and databases.\n\nAlso, in the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the \"Communications of the ACM\"—\"turingineer\", \"turologist\", \"flow-charts-man\", \"applied meta-mathematician\", and \"applied epistemologist\". Three months later in the same journal, \"comptologist\" was suggested, followed next year by \"hypologist\". The term \"computics\" has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. \"informatique\" (French), \"Informatik\" (German), \"informatica\" (Italian, Dutch), \"informática\" (Spanish, Portuguese), \"informatika\" (Slavic languages and Hungarian) or \"pliroforiki\" (\"πληροφορική\", which means informatics) in Greek. Similar words have also been adopted in the UK (as in \"the School of Informatics of the University of Edinburgh\").\n\"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"\n\nA folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, statistics, and logic.\n\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.\n\nThe relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.\n\nThe academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\n\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.\nCSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: \"theory of computation\", \"algorithms and data structures\", \"programming methodology and languages\", and \"computer elements and architecture\". In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\n\n\"Theoretical Computer Science\" is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies. All studies related to mathematical, logic and formal concepts and methods could be considered as theoretical computer science, provided that the motivation is clearly drawn from the field of computing.\n\nData structures and algorithms are the study of commonly used computational methods and their computational efficiency.\n\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be (efficiently) automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\n\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\n\nInformation theory is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\n\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. The field often involves disciplines of computer engineering and electrical engineering, selecting and interconnecting hardware components to create computers that meet functional, performance, and cost goals.\n\nComputer performance analysis is the study of work flowing through computers with the general goals of improving throughput, controlling response time, using resources efficiently, eliminating bottlenecks, and predicting performance under anticipated peak loads.\n\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. A distributed system extends the idea of concurrency onto multiple computers connected through a network. Computers within the same distributed system have their own private memory, and information is often exchanged among themselves to achieve a common goal.\n\nThis branch of computer science aims to manage networks between computers worldwide.\n\nComputer security is a branch of computer technology with an objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.\n\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages.\n\n\"Informatics practices\" (IP) is a field of software-related study. The course mainly covers coding, software and networking concepts. This field is comparable to computer science, but lacks the hardware concepts, while focusing more on networking concepts than computer science. This branch of information science and computer science mainly focuses on the study of information processing, particularly with respect to system integration and human interactions with machine and data.\n\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n\nResearch that develops theories, principles, and guidelines for user interface designers, so they can create satisfactory user experiences with desktop, laptop, and mobile devices.\n\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. In practical use, it is typically the application of computer simulation and other forms of computation to problems in various scientific disciplines.\n\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\n\nSoftware engineering is the study of designing, implementing, and modifying software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it doesn't just deal with the creation or manufacture of new software, but its internal maintenance and arrangement.\n\nThe philosopher of computing Bill Rapaport noted three \"Great Insights of Computer Science\":\n\n\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.\n\nProgramming languages can be leveraged as tools to accomplish a variety of tasks. Therefore, different programming paradigms exist. A few basic programming paradigms used throughout computer science are:\n\n\nMost programming languages offer support for multiple programming paradigms, making the distinction more a matter of style than of technical capabilities.\n\nSince computer science is a relatively new field, it is not as widely taught in schools and universities as other academic subjects. For example, in 2014, Code.org estimated that only 10 percent of high schools in the United States offered computer science education. A 2010 report by Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA) revealed that only 14 out of 50 states have adopted significant education standards for high school computer science. However, computer science education is growing. Some countries, such as Israel, New Zealand and South Korea, have already included computer science in their respective national secondary education curriculum. Several countries are following suit.\n\nIn most countries, there is a significant gender gap in computer science education. For example, in the US about 20% of computer science degrees in 2012 were conferred to women. This gender gap also exists in other Western countries. However, in some parts of the world, the gap is small or nonexistent. In 2011, approximately half of all computer science degrees in Malaysia were conferred to women. In 2001, women made up 54.5% of computer science graduates in Guyana.\n\n\nBibliography and academic search engines\nProfessional organizations\nMisc\n"}
{"id": "748852", "url": "https://en.wikipedia.org/wiki?curid=748852", "title": "Corpse paint", "text": "Corpse paint\n\nCorpse paint or corpsepaint is a style of black and white makeup used by black metal bands for concerts and band photos. The makeup is used to make the musicians appear inhuman, corpse-like, or demonic, and is perhaps \"the most identifiable aspect of the black metal aesthetic.\"\n\nCorpse paint typically involves making the face and neck white and making the area around the eyes and lips black. Musicians will often have a trademark style. Other colors are seldom used, yet there are notable exceptions, such as Attila Csihar's use of neon colors and the bands Satyricon and Dødheimsgard experimenting with color as well.\n\nOutside of black metal, face-painting has been used by a variety of other public figures such as shock rock artists (notably Arthur Brown, Alice Cooper, KISS, and members of the Misfits) and professional wrestlers (e.g. Sting and Vampiro).\n\nThe earliest rock groups to wear makeup similar to corpse paint included Screamin' Jay Hawkins and Arthur Brown in the 1960s; Secos & Molhados, Alice Cooper and KISS in the 1970s; and later that decade, punk rock bands like the Misfits and singer David Vanian of The Damned. On seeing shock rock pioneer Arthur Brown performing his US number two hit \"Fire\" in 1968, Alice Cooper states, \"Can you imagine the young Alice Cooper watching that with all his make-up and hellish performance? It was like all my Halloweens came at once!.\"\n\nIn the 1980s, Hellhammer and King Diamond of Mercyful Fate (who used corpse paint as early as 1978 in his band Black Rose) were among the early metal groups to use corpse paint. Per \"Dead\" Ohlin was the first to explicitly associate stylized face paint with an attempt to look like a corpse according to Mayhem drummer Jan Axel \"Hellhammer\" Blomberg. Other groups soon followed suit, including Hellhammer's later incarnation Celtic Frost. Brazilian band Sarcófago also pioneered the look, being dubbed by \"Metal Storm\" magazine as the first band with \"true\" corpse paint. However, Necrobutcher insists that his band Mayhem was the first to use corpse paint and credits the band's singer Per \"Dead\" Ohlin with coining the term. Early corpse paint was meant simply to highlight an individual's features and make them look \"dead.\"\n\nBands of the early Norwegian black metal scene used corpse paint extensively. Early vocalist of Mayhem Per \"Dead\" Ohlin started wearing it in the late 1980s. According to Necrobutcher, Mayhem's bass player: \"It wasn't anything to do with the way Kiss and Alice Cooper used make-up. Dead actually wanted to look like a corpse. He didn't do it to look cool.\" In the early 1990s, other Norwegian black metal bands followed suit and their style and sound was adopted by bands around the world. Eventually, some Norwegian bands—such as Emperor and Satyricon—stopped wearing corpse paint, often citing its loss of meaning or trendiness due to use by so many bands.\n\n\n"}
{"id": "38817016", "url": "https://en.wikipedia.org/wiki?curid=38817016", "title": "Cyanuric triazide", "text": "Cyanuric triazide\n\nCyanuric triazide (CN or (NCN)) is described as an environmentally friendly, low toxicity, and organic primary explosive with a detonation velocity of about 7,300 m·s, and ignition temperature at 205 °C. Primary research on this compound focuses on its use as a high energy density compound.\n\nCyanuric triazide is planar and has three-fold axis symmetry consisting of alternating carbon and nitrogen atoms, whose centers alternate 1.38 Å and 1.31 Å apart. The distance between the center of the ring and carbon atoms of each of the nitrogens is 1.30 Å and 1.39 Å. This fixing of the position of the bonds in the cynauric ring is mainly due to the unsymmetrical positions of the azide chains. Azide groups are linked to the carbon atoms on the cyanuric ring by single bonds with an interatomic distance of 1.38 Å, similar to the cyanuric ring itself.\n\nThis compound is purely synthetic and therefore does not exist in nature.\n\nCyanuric triazide can be synthesized via an alkyl elimination reaction using cyanuric trichloride with an excess of sodium azide refluxed in acetone solution. The white crystals can then be purified via crystallization in -20 °C toluene.\n\nThis white polycrystalline solid was found to be stable under standard conditions but is extremely shock sensitive causing it to violently decompose when ground with a mortar. Cyanuric triazide’s thermodynamic properties were studied using bomb calorimetry with H = 2235 kJ·mol under oxidizing conditions and H = 740 kJ·mol otherwise. This combustion enthalpy of 2234 kJ·mol is comparable to the military explosive RDX (CN)(NO)H but isn’t put into use due to its less than favorable stability. Melting point examination showed a sharp melting range to clear liquid at 94-95 °C, gas evolution at 155 °C, orange to brown solution discoloration at 170 °C, orange-brown solidification at 200 °C and rapid decomposition at 240 °C. The rapid decomposition at 240 °C results from the formation of elemental carbon as graphite and the formation of nitrogen gas.\n"}
{"id": "5954836", "url": "https://en.wikipedia.org/wiki?curid=5954836", "title": "Digital delay generator", "text": "Digital delay generator\n\nA digital delay generator (also known as digital-to-time converter) is a piece of electronic test equipment that provides precise delays for triggering, syncing, delaying and gating events. These generators are used in many types of experiments, controls and processes where electronic timing of a single event or multiple events to a common timing reference is needed. The digital delay generator may initiate a sequence of events or be triggered by an event. What differentiates it from ordinary electronic timing is the synchronicity of its outputs to each other and to the initiating event.\n\nA time-to-digital converter does the inverse function.\n\nThe digital delay generator is similar to a pulse generator in function but the timing resolution is much finer and the delay and width jitter much less.\n\nSome manufacturers, calling their units \"digital delay and pulse generators\", have added independent amplitude polarity and level control to each of their outputs in addition to both delay and width control. Now each channel provides its own delay, width and amplitude control, with the triggering synchronized to an external source or internal rep rate generator - like a general-purpose pulse generator.\n\nSome delay generators provide precise delays (edges) to trigger devices. Others provides precise delays and widths to also allow a gating function. Some delay generators provide a single channel of timing while others provide multiple channels of timing.\n\nDigital delay generator outputs are typically logic level, but some offer higher voltages to cope with electromagnetic interference environments. For very harsh environments, optical outputs and/or inputs, with fiber optic connectors, are also offered as options by some manufacturers. In general, a delay generator operates in a 50 Ω transmission line environment with the line terminated in its characteristic impedance to minimize reflections and timing ambiguities.\n\nHistorically, digital delay generators were single channel devices with delay-only (see DOT reference below). Now, multi-channel units with delay and gate from each channel are the norm. Some allow referencing to other channels and combining the timing of several channels onto one for more complex, multi-triggering applications. Multiple-lasers and detectors can be triggered and gated. (see second reference on \"Experimental study of laser ignition of a methane/air mixture by planar laser-induced fluorescence of OH.)\" Another example has a channel pumping a laser with a user-selected number of flash lamp pulses. Another channel may be used in Q-switching that laser. A third channel can then be used to trigger and gate a data acquisition or imaging system a distinct time after the laser fires. (see sensorsportal.com reference below)\n\nPulse selection or pulse picking of a single laser pulse from a stream of laser pulses generated via mode-locking is another useful feature of some delay generators. By using the mode-locked rate as an external clock to the digital delay generator one may adjust a delay and width to select a single pulse and synchronize other events to that single pulse.\n\nA delay generator can also be used to delay and gate high speed photodetectors in high speed imaging applications. (see reference on high speed photography below)\n\nDigital delay generators are usually the heart of the timing for larger systems and experiments. Users generally create a GUI, graphical user interface to provide a single control to the entire system or experiment. Digital delay generator manufacturers have added remote programming schemes that facilitate the creation of such GUIs. Industry standards such as GPIB, RS232, USB and ethernet are available from a variety of manufacturers.\n\nExperimental fluid dynamics uses digital delay generators in its investigations of fluid flow. The field of PIV, particle image velocimetry, encompasses several subsets which would use digital delay generators as the main component of its timing where multiple lasers may be triggered. Multiple channels may trigger multiple lasers. One is also able to multiplex the timing of several channels onto one channel in order to trigger or even gate the same device multiple times. A single channel may trigger a laser or gate a camera with its multiple, multiplexed pulses. Another useful setup is to have one channel drive flash lamps a preset number of times, followed by a single Q-switch, followed by a delay and gate for the data acquisition or imaging system.\n\nNegative delay is available with digital delay generators that can select some other channel as a reference. This would be useful for applications where an event must occur in advance of the reference. An example would be to allow for the opening of a shutter prior to the reference.\n\nA digital delay generator has been used in mass spectrometry.\n\nA new development are digital delay generators that have gating and external triggering, dual or multi-trigger capabilities. The gate allows the user to enable outputs and/or triggers with an electronic signal. Some units have gate or trigger capabilities using a single or separate connectors. Dual or multi-trigger digital delay generators have several input triggers. These triggers can be selectively used to trigger any or all channels.\n\nThe multi-trigger versions have programmable logic controller type functionality for incorporating interlocks, latches, dynamic delay adjustment and trigger noise suppression. Triggers are formed by logically combining the various inputs and outputs in And, Or, Xor and Negated forms.\n\nLIDAR applications use digital delay generators. A channel is used to trigger a laser. A second channel is used to provide a delayed gate for the data acquisition system. Gating allows regions of interest to be processed and stored while ignoring the bulk of unwanted data.\n\nDual-trigger digital delay generators provide two independently triggered digital delay generators in one package. Since benchtop digital delay generators are now multi-channel, it is possible to have two or more input triggers and select the channels which respond to each of the triggers. An interesting concept to provide dual-trigger capability converts an instrument that has separate trigger and gate inputs to allow the gate to operate as a second trigger.\n\nA key issue in the design of DDGs is to generate triggered delays having crystal-oscillator precision but that are not quantized to the edges of the reference oscillator. There are a number of techniques used in digital delay generation.\n\n\n\n\n"}
{"id": "16575805", "url": "https://en.wikipedia.org/wiki?curid=16575805", "title": "Diversified Project Management", "text": "Diversified Project Management\n\nFounded in 1989, Diversified Project Management is a New England-based owners’ representative consulting company headquartered in Newton, Massachusetts. With additional offices in Hartford, Connecticut, and Stamford, Connecticut, Diversified Project Management specializes in programming, project management, construction administration, FF&E coordination and move-planning services for corporate, education, health care, manufacturing and biotechnology construction, renovation and relocation projects.\n\n\nDiversified Project Management established its Boston office in 1989, with the mission of providing objective tenant representation, project programming, construction administration, facilities management and move coordination services. Diversified Project Management later founded its Hartford office in 1996 to better serve the Hartford, New Haven and Western Massachusetts markets. In 2007, Diversified Project Management added its Stamford location, serving Fairfield and Westchester counties in Connecticut and New York. Diversified Project Management has since opened an office in Providence, Rhode Island.\n\nDiversified provided move management services for the convert|170000|sqft|m2|sing=on Life Sciences Facility at Brown University. This research center houses the departments of molecular biology, cellular biology and biochemistry and neuroscience. With 60 new laboratories, the project modernized the departments and expanded laboratory and research capacity at Brown by 50 percent. It was designed to encourage scientific collaboration among faculty; staff and students by uniting complementary research under one roof.\n\nDiversified managed the corporate relocation project for CVS into their Support Center Expansion space in Woonsocket, Rhode Island. The multi-phased relocation management was broken into three projects focusing on moving employees into the new space, relocation into the vacated and renovated existing space and bringing staff in from various CVS facilities in the surrounding areas. Over 1,000 staff members were moved in the process. Along with master timeline generation and overall move management services, DPM oversaw the furniture knockdown/reconfiguration and installation of new systems.\n\nDiversified was hired to centralize Partners HealthCare's finance department, by consolidating seven offices into of space on two-and-a-half floors at the Schrafft Center, in Charlestown, Massachusetts. The project scope included programming and move management for approximately 800 people.\n\nMRO Software retained Diversified for the renovation of of office space and relocation of over 300 employees in Bedford, Massachusetts. Diversified's responsibilities included overall project management, design development coordination, general contractor selection and administration, value engineering, voice and data coordination, furniture selection and management and move planning. Diversified acted as MRO's representative.\n\nDiversified was retained to manage Acme Packet's office and lab build out and relocation. Acme Packet relocated from their single, floor office space in Woburn to a new two-story state-of-the-art building in Burlington, where they nearly doubled in size to of space as the sole tenant. The move involved 84 staff members, 60 workstations, manufacturing areas and a test lab.\n\nJewish Family & Children's Services\nDiversified was retained to provide project management for Jewish Family & Children's Services relocation to a building in Waltham, Massachusetts. The move involved 120 employees from two separate locations in Newton and Boston.\n\nEisai Research Institute\nDiversified managed the relocation for 120 employees into a laboratory and office in Andover, Massachusetts. The project included renovating and expanding building four on the Andover campus to include more office space and the addition of an elevator and stairway. Simultaneously, building five on the campus was demolished to make way for the construction of a new office and laboratory building. A connector was built between building four and the new building to facilitate easy access between the buildings for company personnel.\n\nDiversified managed the relocation of the United States Coast Guard from its facility located at the University of Connecticut campus at Avery Point in Groton, Connecticut. Diversified oversaw the entire project from the evaluation and selection of a new property to the coordination of the move into the new space. The office facility is designed for 170 employees and 10-percent of the real estate was designed as research lab space.\n\n"}
{"id": "3979717", "url": "https://en.wikipedia.org/wiki?curid=3979717", "title": "Ducks demo", "text": "Ducks demo\n\nThe Ducks demo was a tech demo that demonstrated the capabilities of the PlayStation 2 at E3 2000 and the PlayStation 3 at E3 2005. In the PlayStation 2 demo, only one duck is shown interacting with the water in the bathtub. In the PlayStation 3 demo, there are many ducks interacting with each other and their environment, as a visual representation of the leap in processing abilities from the PlayStation 2 Sony executives promise the PlayStation 3 will deliver. The Ducks demo was the basis for \"Super Rub 'a' Dub\" which is a downloadable title for the PS3 produced by SCEE.\n\nIn the demo, Phil Harrison shows the crowd a small bathtub with some rubber ducks in it. He uses these to demonstrate the real-time water ripples and dynamics. He then uses two toy pirate ships to demonstrate cloth dynamics. To show the Cell's processing power, he adds a huge amount of ducks to the tub (all reacting as they would in real-life, due to the advanced physics engine). Phil comments dryly that \"this is only possible with the use of LOD technology, for those of you that don't know, LOD stands for \"Lots of Ducks\"\n\nPhil Harrison then invites the creator of EyeToy to show the new technology the PlayStation 3 can use. Plugging a standard PlayStation 2 EyeToy into the PlayStation 3, he picks up two ordinary cups and proceeds to move the cups in a scooping motion, which scoops up the water pictured on screen. (animated and reacting as real water would).\n\nThe ducks demo was the basis for \"Super Rub 'a' Dub\" which is a downloadable title for the PS3 produced by SCEE.\n\n"}
{"id": "24824635", "url": "https://en.wikipedia.org/wiki?curid=24824635", "title": "Electrochemical hydrogen compressor", "text": "Electrochemical hydrogen compressor\n\nAn electrochemical hydrogen compressor is a hydrogen compressor where hydrogen is supplied to the anode, and compressed hydrogen is collected at the cathode with an exergy efficiency up to and even beyond 80% for pressures up to 10,000 psi or 700 bars.\n\nA multi-stage electrochemical hydrogen compressor incorporates membrane-electrode-assemblies (MEAs) separated by proton exchange membranes (PEMs) in series to reach higher pressures, when a current is passed through the MEA protons and electrons are generated at the anode. The protons are electrochemically driven across the membrane to the cathode, after which they combine with the rerouted electrons to form hydrogen, which is fed to the hydrogen compressor to be oxidized at the anode of each cell to form protons and electrons.\n\nThis type of compressor has no moving parts and is compact. With electrochemical compression of hydrogen a pressure of 14500 psi (1000 bar) is achieved, this world record was set by HyET from the Netherlands in 2011. Water vapor partial pressure, current density, operating temperature and hydrogen back diffusion due to the pressure gradient have an effect on the maximum output pressure.\n\nElectrochemical hydrogen compressors have been proposed for use in hydrogen refueling stations to pressurize hydrogen gas for storage. They have also been applied into novel refrigeration systems to pressurize hydrogen for absorption into metal hydrides or to pressurize other working fluids (such as refrigerants) as demonstrated by winners of the global GE's Ecomagination awards for 2011. These electrochemical compressors are noiseless, scalable, modular and highly efficient without the use of CFC's.\n\n"}
{"id": "319906", "url": "https://en.wikipedia.org/wiki?curid=319906", "title": "Fictional technology", "text": "Fictional technology\n\nFictional technology is technology that does not exist. It may be an idea or design that has not yet been developed, or it may be a fictional device used in a novel.\n\nTechnical innovations which represent progressive developments within a field for competitive advantage.\n\nSeeks to identify if a prospective technology can be designed in detail, and simulated, even if it cannot be built yet - this is often a prerequisite to venture capital funding, or investigation in weapons research.\n\nOften emphasizes a speculative potential of a specific technology in order to stimulate investment in it, or a counter-technology. This is a common motivation in any society dominated by a military-industrial complex. \"See also militarism, technological escalation, arms race.\"\n\nEmphasizes some amazing potential of some technology that is \"under development\" (usually without any specific timelines) by a company that is seeking simply to present itself as being competent with technology. \"See also vaporware, persuasion technology.\"\n\nMany works of science fiction are centered around the use of fictional future innovations and technologies and their potential uses. This can sometimes result in inventors using these fictional technologies as inspiration for real-life devices and other emerging technologies.\n\nFantasy genres like steampunk and dieselpunk explore the consequences of more advanced technology being developed earlier in history, while not necessarily entering into the realm of science fiction. Magic powered technology, colloquially known as \"magitech\", is also common in fantasy media, where it can be used as a substitute for modern technology while still giving the setting a fantasy atmosphere. Well-known examples are human-created golems and artificially levitating airships.\n"}
{"id": "18740332", "url": "https://en.wikipedia.org/wiki?curid=18740332", "title": "Field mill (carriage)", "text": "Field mill (carriage)\n\nA field mill, also known as a camp mill, was a premodern vehicle which acted as a mobile mill used for grinding grains, which had the very practical use of feeding a moving army. \nIn the \"Yezhongji\" (鄴中記) ('Record of Affairs at the Capital of the Later Zhao Dynasty') by Lu Hui, covering the history of the Later Zhao (319–351 AD) court in China, the text describes various mechanical devices used, including the wheeled odometer for measuring distance and the south-pointing chariot for indicating cardinal direction. Two engineers in particular, the Palace Officer Xie Fei and Director of Imperial Workshops Wei Mengbian, were known for their designs and worked at the court of Shi Hu (r. 334–349). The two had crafted a four-wheeled carriage about 6 m (20 ft) long with water-spouting dragons hanging over a large golden Buddhist statue that had a mechanical wooden statue of a Daoist continually rubbing his front. Other mechanical figures included ten Daoists dressed in monastic robes who continually rotated around the Buddha while periodically bowing, saluting, and throwing incense into a censer. All of these mechanical figures were driven only by the movement of the carriage; once the carriage halted, the figures stopped moving and the water stopped spouting from the artificial dragons.\n\nXie and Wei created a similar device operated by wheel motion called the field mill, although it served a more practical purpose than the theatrical display of moving statues and water-spouting dragons. The \"Yezhongji\" states that the two devised a \"pounding cart\" or \"pounding wagon\" which had figurine statues armed with real tilt hammers who pounded and hulled rice only when the cart moved. In addition to this they had a \"mill cart\" (field mill or camp mill) which had rotating millstones mounted on their frames, which would rotate and grind wheat as the cart moved forward. Just like the carriage with mechanical figures mentioned above, when the carriage stopped, the devices associated with them halted.\n\nUse of the field mill in China seems to have died out in use after the Later Zhao, since it was no longer mentioned in Chinese texts until Ming Dynasty. \n\nThe Italian military engineer Pompeo Targone, who was most notably involved in the Siege of La Rochelle (1627-1628) in western France, invented the field mill in Europe by 1580. As shown in the Italian Vittorio Zonca's engineering treatise of 1607, two mills mounted to a wagon are rotated by a horse whim and gearing while in a stationary position at military camp or near billets. \n\nIn the \"Yuanxi Qiqi Tushuo Luzui\" ('Collected Diagrams and Explanations of the Wonderful Machines of the Far West') compiled and translated in 1627 by German Jesuit Johann Schreck (1576–1630) and Ming Dynasty Chinese author Wang Zheng (王徵 1571–1644), a field mill is shown amongst other devices. In this picture, two mills are operated by the gearing of a rotating bar and a whippletree harnessed to a single horse, unlike the two horses seen in Zonca's illustration.\n\n\n"}
{"id": "49550577", "url": "https://en.wikipedia.org/wiki?curid=49550577", "title": "Frontiers of Architectural Research", "text": "Frontiers of Architectural Research\n\nFrontiers of Architectural Research is a quarterly peer-reviewed open access academic journal covering the field of architecture, including architectural design and theory, architectural science and technology, urban planning, landscape architecture, existing building renovation and architectural heritage conservation. It is published by Elsevier on behalf of Higher Education Press. The journal was established in 2012 and the editor-in-chief is Jianguo Wang (Southeast University (China)).\n\nThe journal is abstracted and indexed in the Chinese Science Citation Database, Scopus, and the Emerging Sources Citation Index.\n"}
{"id": "41502420", "url": "https://en.wikipedia.org/wiki?curid=41502420", "title": "GCFLearnFree.org", "text": "GCFLearnFree.org\n\nGCFLearnFree.org is a free online educational website focusing on technology, job training, reading, and math skills. The site is a program of Goodwill Industries of Eastern NC Inc. (GIENC) and the Goodwill Community Foundation Inc. (GCF). All content is created in Raleigh, North Carolina.\nGCFLearnFree.org has 16 employees responsible for content development and web development, and 9 online instructors who facilitate accredited classes by grading work and responding to learners' questions.\n\nGCFLearnFree.org was created in July 2000 by GIENC president Dennis McLain as an online training program with both English and Spanish language lessons. The program is funded through revenue generated from the value of donated items to Goodwill Community Foundation (GCF) throughout eastern North Carolina.\n\nThe GCFLearnFree.org programs are designed around self-paced instruction. The website offers dozens of free, self-paced tutorials in technology, Microsoft Office, work and career, reading, math, and everyday life.\nAll tutorials can be accessed with no registration required, but users can also create a free GCFLearnFree.org account to track their learning history and create transcripts of completed tutorials. Tutorials use video, sound, art, storytelling, and text.\nGCFLearnFree.org's self-paced tutorials are part of the free resources available at DigitalLiteracy.gov, an Obama administration initiative focused on digital literacy. GCFLearnFree.org is also an Everyoneon.org partner.\nOrganizations that use GCFLearnFree.org include K-12 schools, colleges, homeschool groups, nonprofit organizations, libraries, businesses, and career centers.\n\nGCFLearnFree.org offers free online classes that offer certificates of completion and continuing education units (CEUs). Online classes require registration and submission of assignments, and they are supported by online instructors. The online class program is accredited by the International Association for Continuing Education and Training (IACET).\n\nGCFLearnFree.org won the 2012 MERLOT Classics Award for its Word 2010 tutorial.\nGreatNonprofits, a provider of user reviews about nonprofit organizations, named GCFLearnFree.org a 2014 Top-Rated Nonprofit based on reviews submitted by people who volunteer for or use the website. However, calling GCFLearnFree.org a \"nonprofit\" is not exactly accurate because the Employer Identification Number listed for GCFLearnFree.org by GreatNonprofits is registered with the Internal Revenue Service to Goodwill Community Foundation Inc. and not to GCFLearnFree.org, and the name \"GCFLearnFree.org\" does not appear in other major nonprofit databases such as GuideStar and Charity Navigator but \"Goodwill Community Foundation Inc.\" does.\n"}
{"id": "27117682", "url": "https://en.wikipedia.org/wiki?curid=27117682", "title": "GPU switching", "text": "GPU switching\n\nGPU switching is a mechanism used on computers with multiple graphic controllers. This mechanism allows the user to either maximize the graphic performance or prolong battery life by switching between the graphic cards. It's mostly used on gaming laptops which usually have an integrated graphic device and a discrete video card. \n\nMost computers using this feature contain integrated graphics processors and dedicated graphics cards that applies to the following categories.\n\nAlso known as: \"Integrated graphics\", \"shared graphics solutions\", \"integrated graphics processors\" (IGP) or \"unified memory architecture\" (UMA). This kind of graphics processors usually have much fewer processing units and share the same memory with the CPU.\nSometimes the graphics processors are integrated onto a motherboard. It is commonly known as: \"on-board graphics.\" A motherboard with on-board graphics processors doesn't require a discrete graphics card or a CPU with graphics processors to operate.\n\nAlso known as: \"discrete graphics cards\". Unlike integrated graphics, dedicated graphics cards have much more processing units and have its own RAM with much higher memory bandwidth.\n\nIn some cases, a dedicated graphics chip can be integrated onto the motherboards, B150-GP104 for example. Regardless of the fact that the graphics chip is integrated, it is still counted as a dedicated graphics cards system because the graphics chip is integrated with its own memory.\n\nMost computers have a motherboard that uses a Southbridge and Northbridge structure. The Northbridge is one of the core logic chipset that handles communications among the CPU, GPU, RAM and the Southbridge. The discrete graphics card is usually installed onto the graphics card slot and the integrated graphics is integrated onto the CPU or occasionally onto the Northbridge. The Northbridge is the most responsible for switching between GPUs. The way how it works usually has the following process: \n\nThe Southbridge is also named as I/O Controller Hub (ICH). It handles all of a computer's I/O functions, such as receiving the keyboard input and outputting the data onto the screen. The way how it usually works usually has two steps:\nThe reason why the second step can be optional is that sometimes the rendered the data is outputted directly from the discrete graphics card which is located on the graphics card slot so there is no need to output the data through the Southbridge.\n\nGPU switching is mostly used for saving energy by switching between graphic cards. The dedicated graphics cards consume much more power than integrated graphics but also provides higher 3D performances, which is needed for a better gaming and CAD experience. Following is a list of the TDPs of the most popular CPU with integrated graphics and dedicated graphics cards.\nThe dedicated graphics cards exhibit much higher power consumption than the integrated graphics on both platforms. Disabling them when no heavy graphics processing is needed can significantly lower the power consumption.\n\nNvidia Optimus™ is a computer GPU switching technology created by Nvidia that can dynamically and seamlessly switch between two graphic cards based on running programs.\n\nAMD Enduro™ is a collective brand developed by AMD that features many new technologies that can significantly save power. It was previously named as: \"PowerXpress\" and \"Dynamic Switchable Graphics (DSG).\" This technology implements a sophisticated system to predict the potential usage need for graphics cards and switch between graphics cards based on predicted need. This technology also introduces a new power control plan that allows the discrete graphics cards consume no energy when idling.\n\nIn personal computers, the IGP (integrated graphics processors) are mostly manufactured by Intel and AMD and are integrated onto their CPUs. They are commonly known as:\n\nThe most popular dedicated graphics cards are manufactured by AMD and Nvidia. They are commonly known as:\n\nMost common operating systems have built-in support for this feature. However, the users may download the updated drivers from Nvidia or AMD for better experience.\n\nWindows 7 has built-in support for this feature. The system automatically switches between GPUs depending on the program that's running. However, the user may switch the GPUs manually through device manager or power manager.\n\nIn the Linux systems, a patch named codice_1 has been added to the Linux kernel since version 2.6.34 in order to deal with multiple GPU. Here, the switch requires a restart of the X Window System to be taken into account.\n\nUbuntu Control Center allows the user to access codice_1 functionality through a GUI.\n\nMac OS has built-in support for this feature since v10.5 Leopard. Since OS X Mountain Lion, Apple has integrated the GPU monitor into the Activity Monitor.\n\n\n\n"}
{"id": "3046529", "url": "https://en.wikipedia.org/wiki?curid=3046529", "title": "Gunpowder Incident", "text": "Gunpowder Incident\n\nThe Gunpowder Incident (or Gunpowder Affair) was a conflict early in the American Revolutionary War between Lord Dunmore, the Royal Governor of the Colony of Virginia, and militia led by Patrick Henry. On April 20, 1775, one day after the Battles of Lexington and Concord (and well before news of that event reached Virginia), Lord Dunmore ordered the removal of the gunpowder from the magazine in Williamsburg, Virginia to a Royal Navy ship.\n\nThis action sparked local unrest, and militia companies began mustering throughout the colony. Patrick Henry led a small militia force toward Williamsburg to force return of the gunpowder to the colony's control. The matter was resolved without conflict when a payment of £330 was made to Henry. Dunmore, fearing for his personal safety, later retreated to a naval vessel, ending royal control of the colony.\n\nMilitary tensions began to rise in the British colonies of North America in 1774 when a series of legislative acts by the British Parliament known as the Intolerable Acts began to be implemented in the colonies. The colonies, in solidarity with the Province of Massachusetts Bay, which had been singled out for punishment by those acts in the wake of the Boston Tea Party, had organized a Congress to meet in September 1774. During the meeting of the First Continental Congress word arrived of a militia uprising in Massachusetts that became known as the Powder Alarm. In early September, General Thomas Gage, the royal governor of Massachusetts, had removed gunpowder from a powder magazine in Charlestown (in a location now in Somerville), and militia from all over New England had flocked to the area in response to false rumors that violence had been involved. One consequence of this action was that the Congress called for the colonies to organize militia companies for their defense. Another was that Lord Dartmouth, the Secretary of State for the Colonies, advised the colonial governors to secure their military supplies, and prohibited importation of further supplies of powder.\n\nIn early 1775, Virginians began to organize militia companies and seek out military supplies (weapons, ammunition, and gunpowder) to arm and equip them. Lord Dunmore, Virginia's royal governor, saw this rising unrest in his colony and sought to deprive Virginia militia of these supplies. It was not until after Patrick Henry's \"Give me liberty or give me death\" speech at the Second Virginia Convention on March 23 that Dunmore \"[thought] it prudent to remove some Gunpowder which was in a Magazine in this place.\" Although British Army troops had been withdrawn from Virginia in the wake of the Powder Alarm, there were several Royal Navy ships in the Virginia waters of Chesapeake Bay. On April 19, Lord Dunmore quietly brought a company of British sailors into Williamsburg and quartered them in the governor's mansion. Dunmore then ordered Captain Henry Collins, commander of HMS \"Magdalen\", to remove the gunpowder from the magazine in Williamsburg.\n\nOn the night of April 20, Royal Navy sailors went to the Williamsburg powder magazine, loaded fifteen half barrels of powder into the governor's wagon, and transported it to the eastern end of the Quarterpath Road to be loaded aboard the \"Magdalen\" in the James River. The act was discovered by townsfolk while underway, and they sounded an alarm. Local militia rallied to the scene, and riders spread word of the incident across the colony. Dunmore had as a precaution armed his servants with muskets, and it was only the calming words of Patriot leaders, including the Speaker of the House of Burgesses, Peyton Randolph, that prevented the assembling crowd from storming Dunmore's mansion. The city council demanded the return of the powder, claiming it was the property of the colony and not the Crown. Dunmore demurred, stating that he was moving the powder as protection against its seizure during a rumored slave uprising, and would eventually return it. This seemed to satisfy the assembled crowd, and it dispersed peacefully.\n\nUnrest however persisted in Williamsburg and spread throughout the countryside. After a second crowd was convinced to disperse by Patriot leaders, Dunmore reacted angrily, warning on April 22 that if attacked, he would \"declare Freedom to the Slaves, and reduce the City of Williamsburg to Ashes.\" He also told a Williamsburg alderman that he had \"once fought for the Virginians\" but \"By God, I would let them see that I could fight against them.\"\nBy April 29, militia mobilizing in the countryside had learned of the battles at Lexington and Concord. Nearly 700 men mustered at Fredericksburg, and decided to send a messenger to Williamsburg to assess the situation before marching on the capital. Peyton Randolph advised against violence, and George Washington, a longtime leader of the Virginia militia, concurred. In response to their advice, the Fredericksburg militia voted by a narrow margin not to march. However, militia from other parts of the colony did march to Williamsburg. The Hanover County militia, led by Patrick Henry, voted on May 2 to march on Williamsburg. Henry dispatched a small company to the home of Richard Corbin, who was the Deputy Collector of the Royal Revenue in Virginia, in a bid to force him to pay for the powder from Crown revenue in his possession; the remainder of the Hanover County militia, numbering about 150, marched toward Williamsburg, arriving about away on May 3. That day Dunmore's family escaped Williamsburg to Porto Bello, Lord Dunmore's hunting lodge on the York River, and from there to , lying at anchor in the York River.\n\nCorbin was not at home—he was in Williamsburg, meeting with Dunmore. Henry was advised by Carter Braxton, Corbin's son-in-law and a Patriot member of the House of Burgesses, not to enter the city, while Braxton rode into the city and negotiated a payment. The next day, May 4, Henry received a bill of exchange for £330 signed by a wealthy plantation owner, as payment for the powder (he refused the offer of payment from Crown accounts). Henry then departed to take his place as a member of Virginia's delegation to the Second Continental Congress, promising to deliver the money to \"the Virginia Delegates at the General congress\". On May 6 Dunmore issued a proclamation charging Henry with extortion of the £330, and forbidding the citizenry to assist Henry in any way. Henry was offered protection by several counties, and was escorted by several companies of militia to the Maryland border as he made his way to Philadelphia.\n\nThe incident burnished Henry's reputation while worsening Dunmore's popularity. Although his family briefly returned to Williamsburg on May 12 as a sign of good faith, relations between Dunmore and the House of Burgesses continued to deteriorate. On June 8, Dunmore and his family fled the Governor's Palace in the middle of the night and took up residence aboard \"Fowey\". The Burgesses had been deliberating the Conciliatory Resolution, a proposal that was an attempt by the North Ministry to divide the colonies. In the wake of Dunmore's flight, the Burgesses rejected the proposal.\n\nDunmore continued to make vigorous attempts to regain control of the colony, but after a decisive defeat of British forces at Great Bridge in December, he was reduced to raiding operations and eventually abandoned the colony for good in August 1776. Virginia's government was first taken over by a Committee of Safety, chosen by the Third Virginia Convention in July 1775; Patrick Henry became the independent state's first governor in July 1776.\n\n\n\n"}
{"id": "4178640", "url": "https://en.wikipedia.org/wiki?curid=4178640", "title": "Hazchem", "text": "Hazchem\n\nHazchem (hazardous chemicals)() is a warning plate system used in Australia, Malaysia, New Zealand, India and the United Kingdom for vehicles transporting hazardous substances, and on storage facilities. The top-left section of the plate gives the Emergency Action Code (EAC) telling the fire brigade what actions to take if there's an accident or fire. The middle-left section containing a 4 digit number gives the UN Substance Identification Number describing the material. The lower-left section gives the telephone number that should be called if special advice is needed. The warning symbol in the top right indicates the general hazard class of the material. The bottom-right of the plate carries a company logo or name.\n\nThere is also a standard null Hazchem plate to indicate the transport of non-hazardous substances. The null plate does not include an EAC or substance identification.\n\nThe National Chemical Emergency Centre (NCEC) in the United Kingdom provides a Free Online Hazchem Guide.\n\nThe Emergency Action Code (EAC) is a three character code displayed on all dangerous goods classed carriers, and provides a quick assessment to first responders and emergency responders (i.e. fire fighters and police) of what actions to take should the carrier carrying such goods become involved in an incident (traffic collision, for example). EAC's are characterised by a single number (1 to 4) and either one or two letters (depending on the hazard).\n\nNCEC was commissioned by the Department for Communities and Local Government (CLG) to edit the EAC List 2013 publication, outlining the application of Hazchem Emergency Actions Codes (EACs) in Britain for 2013. The Dangerous Goods Emergency Action Code (EAC) List is reviewed every two years and is an essential compliance document for all emergency services, local government and for those who may control the planning for, and prevention of, emergencies involving dangerous goods. The current EAC List is 20013. NCEC has been at the heart of the UK EAC system since its inception in the early 1970s, publishing the list on behalf of the UK Government until 1996 and resuming its management in 2008.\n\nThe printed version of the book can be purchased from TSO directly () or downloaded as a PDF file from NCEC’s website.\n \n\nThis number is indicative of what type of fire suppressant should be used to suppress a fire from igniting or extinguish a fire caused by the chemical. The system is designed to rank fire suppression methods in order of usability. For example, a chemical marked with the number 2 or Fog can be attacked with methods 3 (Foam) or 4 (Dry Agent) but not with 1 (Jets). The \"Dry Agent\" method must be used for chemicals that have an undesirable reaction with water and must not be allowed to come in contact with water, therefore 4 is the highest ranking suppression method as all of the other methods use water.\n\nEach EAC contains at least one letter, which determines which category the chemical falls under, and which also highlights the violence of the chemical (i.e. likelihood to spontaneously combust, explode etc.), what personal protective equipment to use while working around the chemical and what action to take when disposing of the chemical.\n\nEach category is assigned a letter to determine what actions are required when handling, containing and disposing of the chemical in question. Eight 'major categories' exist which are commonly denoted by a black letter on a white background. Four subcategories exist which specifically deal with what type of personal protective equipment responders must wear when handling the emergency, denoted by a white letter on a black background. In Australia with the update of the Australian Dangerous Goods Code volume 7 as of 2010, the white letter on a black background has been removed, making BA a requirement at all large incidents regardless of whether the substance is involved in a fire.\n\nIf a category is classed as violent, this means that the chemical can be violently or explosively reactive, either with the atmosphere or water, or both (which could be marked by the Dangerous when Wet symbol).\n\nProtection is divided up into three categories of personal protective equipment, \"Full\", \"BA\" and \"BA for fire only\". Full denotes that full personal protective equipment provisions must be used around and in contact with the chemical, which will usually include a portable breathing apparatus and water tight and chemical proof suit. \"BA\" (acronym for breathing apparatus) specifies that a portable breathing apparatus must be used at all times in and around the chemical, and \"BA for fire only\" specifies that a breathing apparatus is not necessary for short exposure periods to the chemical but is required if the chemical is on alight. \"BA for fire only\" is denoted within the emergency action code as a white letter on a black background, while a black letter on a white background denotes breathing apparatus at all times. <ins>When changing the background colour is not possible (such as with handwriting), the use of brackets means the same as a black background. \"3[Y]E\" means the same as a white letter on a black background.</ins>\n\nSubstance control specifies what to do with the chemical in the event of a spill, either \"dilute\" or \"contain\". Dilute means that the chemical may be washed down the drain with large quantities of water. \"Contain\" requires that the spillage must not come in contact with drains or water courses.\n\nIn the event of a chemical incident, the EAC may specify that an evacuation may be necessary as the chemical poses a public hazard which may extend beyond the immediate vicinity. If evacuation is not possible, advice to stay in doors and secure all points of ventilation may be necessary. This condition is denoted by an E at the end of any emergency action code. It is an optional letter, depending on the nature of the chemical.\n\nA very commonly displayed example is 3YE on petrol tankers. This means that a fire must be fought using foam or dry agent (if a small fire), that it can react violently and is explosive, that fire fighters must wear a portable breathing apparatus at all times, or if a white on black Y, only if there is a fire, and that the run-off needs to be contained. It also indicates to the incident controller that evacuation of the surrounding area may be necessary.\n\nExample:\n\nThere are three substances to be carried as a multi-load, having emergency action codes of 3Y, •2S and 4WE.\n\n1st Character (Number):\nThe first character of the EAC for each of the three substances is 3, 2 and 4. The highest number must be taken as the first character of the code for the multi-load and therefore the first character will be 4. The bullet in •2S is not assigned to the mixed load because other EACs do not include a bullet.\n\n2nd Character (Letter):\nThe second character for the EAC for each of the three substances is Y, S and W. Taking the Y along the top row of the chart and the S along the left hand column, the intersection is at Y and therefore the character for the first two substances would be Y. This resultant character (Y) is then taken along the top row and the character for the third substance (W) is taken along the left hand column. The intersection point is now W. The second character of the code for the three substances is therefore W.\n\nLetter ‘E’:\nThe third substance has an ‘E’ as a third character and therefore the multi-load must also have an ‘E’.\n\nThe resultant Hazchem Code for the three substances carried as a multi-load will therefore be 4WE.\n\n\n"}
{"id": "50388323", "url": "https://en.wikipedia.org/wiki?curid=50388323", "title": "Holly Brockwell", "text": "Holly Brockwell\n\nHolly Brockwell, born is an English technology journalist, copywriter and the founder of the tech site \"Gadgette\". She has also written columns for \"Gizmodo\", the BBC, \"The Guardian\" and other media outlets.\n\nBrockwell is from Nottingham and currently lives in London. She originally hoped to study computer science at university, but instead took a degree in English and became an advertising copywriter and an assistant reviewer and then an editor at \"ShinyShiny\", a women's technology website.\n\nIn 2012 Brockwell requested the ability to be sterilized from the United Kingdom's National Health Service. She was denied due to her age, which according to the NHS is common practice because surgeons prefer to wait until prospective patients are at least 30 years of age and have had children. At the end of March 2016, after turning 30, she was granted permission for the procedure.\n\nIn 2012 Brockwell blogged about an advertising agency and its campaign about benefit schemes for their staff which sparked controversy and negative publicity. The agency accused Brockwell of libel but later apologised.\n\nThe next year Brockwell sparked controversy again after writing about a 2013 Hyundai advert that was later withdrawn.\n\nIn 2015 Brockwell started her own women’s tech site called \"Gadgette\" in order to tackle the apparent condescending and discriminating attitudes towards women in the technology industry. The launch of the website faced backlash, according to an op-ed by Brockwell.\n\nIn the summer of 2017 she was the first journalist hired by Jimmy Wales for his new commercial news venture, Wikitribune. Her last day with that publication was November 17, 2017.\n\nIn 2015 she won the SheSays Award for The Drum Woman of the Year.\n\n"}
{"id": "30543568", "url": "https://en.wikipedia.org/wiki?curid=30543568", "title": "Institute of Quarrying", "text": "Institute of Quarrying\n\nThe Institute of Quarrying is the international professional body for quarrying, construction materials and the related extractive and processing industries. The Institute's long-term objective is to promote progressive improvements in all aspects of operational performance of the extractives industry through education and training. The Institute has been supporting the extractives industry and associated sectors since 1917.\n\nThe Institute was founded on 19 October 1917 from a meeting of “The Association of Quarry Managers” in Caernarfon in North Wales. Gradually expanding over the years, IQ now has affiliate organisations in Australia, New Zealand, Malaysia, Southern Africa and Hong Kong.\n\nIn September 2012 the Institute moved to its new premises at McPherson House, named after its founder Simon McPherson, in Chilwell, Nottingham.\n\nThe largest membership group remains in the UK, where the Institute was founded in 1917. Australia constitutes the largest group in the Pacific region and close ties are maintained with their neighbours in New Zealand and Malaysia. To the north, members are based in Hong Kong, operating both in the territory and China. The Institute's activities in Southern Africa are centred on South Africa which provides support for members in other countries of the region.\n\nIt has thirteen regional branches in the UK.\n\n\nIt regulates the quarrying industry, providing training and consultation for standards in the industry, similar to other engineering professional bodies.\n\n\n"}
{"id": "25288012", "url": "https://en.wikipedia.org/wiki?curid=25288012", "title": "Jayrun Water Clock", "text": "Jayrun Water Clock\n\nThe Jayrun Water Clock, a water clock built by the Muslim engineer Muhammad al-Sa'ati, was positioned at the gate of Damascus, Syria, at the exit of the Umayyad Mosque in the 12th century during the reign of Nur ad-Din Zangi.\n\nThere is a full description of the clock in the treatise \"Ktab 'Amal al-sa'at wa-l-amal biha\" (\"On the Construction of Clocks and their Use\") written by Ridwan b. al Saati in 1203. This treatise describes the reconstruction by Ridwan of the water clock which was built by his father, Muhammad al-Saati, in the reign of Nur al-Din Mahmud b. Zanki in Damascus (reigned 1154–74). \"The clock-face consisted of wall of timber about 4.23 metres wide and 2.78 metres high. In this screen was a row of doors, at either end of which was the figure of a falcon. During the day a small crescent moved at constant speed in front of the doors and at every hour a door rotated to reveal a different colour, the falcons leant forward, discharged pellets on to cymbals and resumed their upright positions. Above the doors a zodiac circle rotated at constant speed. Above this was a semicircle of twelve circular holes. During the night one of these holes became fully illuminated every hour. The clock was operated by the \"Archimedes\" water machinery and the motion transmitted to the activating mechanisms by pulley and rope systems.\"\n\nIn 1203, al-Sa'ati's son Ridwan wrote a treatise on how he had been tasked to repair the clock his father had built, after others had failed.\n\nA full-size reconstruction of the clock can be seen in the Nationaal Beiaard- en Natuurmuseum Asten in the Netherlands.\n\n\n"}
{"id": "5345493", "url": "https://en.wikipedia.org/wiki?curid=5345493", "title": "MDDL", "text": "MDDL\n\nMDDL, the Market Data Definition Language, is an XML-based messaging format for exchanging information related to\n\nMDDL was developed by FISD (Financial Information Services Division) of SIIA (Software & Information Industry Association). The initiative for the use of XML in market data exchange was started in the year 2000 and has been gaining industry-wide acceptance.\n\n"}
{"id": "23998046", "url": "https://en.wikipedia.org/wiki?curid=23998046", "title": "Marshall Rose", "text": "Marshall Rose\n\nMarshall T. Rose (born 1961) is a network protocol and software engineer, author, and speaker who has contributed to the Internet Engineering Task Force (IETF), the Internet, and Internet and network applications. More specifically, he has specialized in network management, distributed systems management, applications management, email, the ISO Development Environment (ISODE), and service-oriented architecture (SOA).\n\nRose holds a Ph.D. in Information and Computer Science from the University of California, Irvine and is former area director for network management of the IETF.\n\nRose is presently; the Principal Engineer at Brave (web browser).\n\nRose's work on behalf of the Internet Engineering Task Force has included:\n\nRose has written the following published books:\n"}
{"id": "25235659", "url": "https://en.wikipedia.org/wiki?curid=25235659", "title": "Nitin Pradhan", "text": "Nitin Pradhan\n\nNitin Pradhan (;) was the Departmental Chief Information Officer (CIO) for the US Department of Transportation (DOT) as part of the Obama Administration from July 6, 2009 to August 31, 2012. After leaving US DOT, Pradhan established and led the nation's first Federal Technology Accelerator and Partner Consortium called Public Private Innovations and later cofounded GOVonomy, an emerging products technology marketplace for the public sector as well as ScaleUP USA, a Digital Business Growth Accelerator.\n\nPrior to joining DOT, Pradhan was an IT Executive at Fairfax County Public Schools (FCPS), the 12th largest school district in the USA. Earlier, Pradhan was the Managing Director of Virginia’s Center for Innovative Technology (CIT). He was also the co-founder and former CEO of a wireless startup.\n\nBorn and brought up in Pune, India, Pradhan attended Loyola High School (Pune). After high school, Pradhan attended the Faculty of Technology and Engineering, Maharaja Sayajirao University of Baroda (MSU), India where he completed his bachelor's degree in engineering. He followed this degree with master's in marketing management from Institute of Management Development and Research, Pune, India. Pradhan came to Washington, DC on a graduate fellowship from the Kogod School of Business at The American University (AU), to study for his second master's degree in accounting.\n\nPradhan’s IT philosophy focuses on people first, innovation, agility and driving business value. He believes that technology is first about people. Technology based Innovation is his second theme. He believes that Chief Information Officers (CIOs) have a dual role to play as Chief Innovation Officers. This aspect focuses on creating an innovation life cycle within organizations consisting of ideation and crowd sourcing tools like DOT’s IdeaHub, a process for selection of best ideas, a cloud based agility platform for quick deployment of “apps” that drive business value, an Amazon.com-like IT business catalogue, with a one-stop shop for basic, premium, and fee-for-service IT offerings, and an IT Vital Signs dashboard that measures progress.\n\nFinally, he has promoted the concept of new IT—IT 2.0, as he calls it, is based on immediately providing significant public value and business value, and Everything as a Service (EAAS).\n\nHe is a strong proponent of using technology to deliver business results, for example; using emerging innovations like Intelligent Transportation Systems and DOT’s Connected Vehicles Program designed to leverage vehicle-to-vehicle (V2V), driverless cars, and vehicle-to-infrastructure (V2I) wireless communication in order to make driving safer by making cars, trucks, buses and other vehicles aware of the vehicles around them, even if the drivers aren't.\n\nPradhan also supported initiatives launched in the DC metro region in the mid-nineties; including Potomac Knowledge Way and Netpreneur, established by the Morino Institute, and the creation and expansion of the Northern Virginia Technology Council (NVTC) a membership and trade association for the technology community in Northern Virginia, now the largest technology council in the nation, currently serving about 1,000 organizations. After CIT, Pradhan co-founded a wireless startup and was the CEO in early 2000.\n\nIn 2009, Pradhan joined DOT as its CIO. Pradhan was the chief advisor to the Secretary of Transportation, Ray LaHood relating to information technology. In his role as the Departmental CIO, Pradhan provided IT vision, strategy, planning and oversight for DOT's more than $3.0 billion IT portfolio, the 6th largest in the Federal Government. Pradhan's focus at DOT was on using technology to drive mission and business value, IT portfolio optimization, streamlining technology services, creating an IT business catalogue and Online distribution(app store), holistic cyber security and public private partnerships like the Digital Transportation Ecosystem (DTE). Some of the DOT technology initiatives included:\n\n\nIn September 2012, Pradhan established the nation’s first Federal Technology Accelerator and Partner Consortium called Public Private Innovations (PPI). The goal of PPI is to drive public value through private growth by researching and analyzing government problems and matching them with business technology solutions; nurturing, adapting, and deploying technology platforms, products, and services for the federal marketplace and starting, building and growing government practices for new or existing IT contractors and technology suppliers.\n\nIn, March 2013, Pradhan cofounded GOVonomy.com, an emerging products technology marketplace focused on the public sector. GOVonomy’s goal is to drive increased public value by regularly introducing cost-effective, cutting-edge, targeted technologies that specifically address the challenges and opportunities facing the public sector. GOVonomy connects public organizations with new technology products/services from growth companies and helps arrange strategic discussions, demonstrations and pilots for increased understanding, education, purchasing integration of the emerging technology products for the public sector. \n\nFor his work at the Department of Transportation, Pradhan and the DOT were awarded:\n\n"}
{"id": "34005648", "url": "https://en.wikipedia.org/wiki?curid=34005648", "title": "Oyster Fly Rods", "text": "Oyster Fly Rods\n\nOyster Fly Rods is a bamboo fly rod making business in Blue Ridge, Georgia. It belongs to famous fly fisherman Bill Oyster, who makes \"every one\" of the forty custom rods produced each year by hand, as well as a few standardized rods. He described the process as: \"I split each twelve-foot length of cane into six strips... Then, depending on the length, number of pieces, and action wanted, I plane each strip of cane to thousandths of an inch. I also engrave all the nickel and silver hardware, which takes as long as the actual creation of the rod.\" The rods have fast action, include distinctive engravings, \"elegant\" rattan grips, and are \"remarkably fishable\". Oyster Fly Rods was a winner in the \"Sporting\" category of \"Garden & Gun\" magazine's \"Made in the South\" 2010 competition.\nOyster moved the business from Gainesville, Georgia to Blue Ridge in 2009. He has fished around the U.S. and in Argentina. The custom rods sell for as much as $10,000 each and \"at least a couple have gone into the hands of former President Jimmy Carter.\" Oyster also offers rod making classes.\n\n"}
{"id": "39528724", "url": "https://en.wikipedia.org/wiki?curid=39528724", "title": "PowerLinks Media", "text": "PowerLinks Media\n\nPowerLinks Media is an advertising technology platform that consolidates the management of content recommendation and discovery and the delivery of In-Content and Native advertising. PowerLinks’ technology uses 1st and 3rd party data to deliver personalized and targeted content and advertising to digital audiences. The company was established in Manchester, England in 2011, with additional offices in London and New York City. \n\nIts clients include agencies, marketers (Universal McCann, GroupM etc.) and publishers who serve customers like Microsoft, Hewlett-Packard, IBM and EE among others.\n\nPowerLinks was founded in September 2011 by Kevin Flood and Mike Harty. Flood and Harty first established the social shopping and product discovery website Shopow shortly after graduating from University and progressed to co-found PowerLinks.\nDavid Bell (Chairman Emeritus, Interpublic Group of Companies; independent director of Warnaco Group, Primedia, the Freedom Group and Lighting Science Group), Michael Bungey (Previous CEO of Cordiant Communications Group), and Nelson “Skip” Riddle (CEO, Riddle International) act as the Board of Advisors to PowerLinks Media.\n\nPowerLinks offers technology products and services that are sold primarily to advertising agencies and media companies to allow publisher clients to traffic, target, deliver, and report on their content and In-Content and Native advertising campaigns. The company's main product line is its proprietary Content Recommendation and In-Content and Native advertising Server, which is designed for digital publishers.\n\nThe PowerLinks Ad Server is intended to automate and enhance the administration of content recommendation and In-feed and In-Content advertising inventory management for publishers. In conjunction with the PowerLinks exchange, the PowerLinks system is intended to increase the purchasing efficiency for advertisers whilst minimizing unsold inventory for publishers. The PowerLinks Advertising Exchange (released Q2 2013) connects media buyers and sellers on an advertising exchange much like a traditional Stock exchange.\n\nThe PowerLinks content recommendation technology is intended to help internet publishers to increase Web traffic of their websites. It achieves this by presenting them with links to articles and other related content. The content recommendation system uses Behavioural targeting and contextual targeting to recommend interesting articles, blog posts or videos to a reader, rather than relying on a more basic 'related items' widget.\n\nPowerLinks operates as a Software as a service product which is intended to enable publishers to bring their In-Feed and In-Content advertising in line with their regular display advertising\n"}
{"id": "7338650", "url": "https://en.wikipedia.org/wiki?curid=7338650", "title": "Process variable", "text": "Process variable\n\nA process variable, process value or process parameter is the current measured value of a particular part of a process which is being monitored or controlled. An example of this would be the temperature of a furnace. The current temperature is called the process variable, while the desired temperature is known as the set-point. The set point is usually abbreviated to SP, and the process value is usually abbreviated to PV.\n\nMeasurement of process variables are essential in control systems to controlling a process. The value of the process variable is continuously monitored so that control may be exerted. \n\nFour commonly measured variables which affect chemical and physical processes are: pressure, temperature, level and flow. but there are in fact a large number of measurement quantities which for international purposes use the International System of Units (SI)\n\nThe SP-PV error is used to exert control on a process so that the value of PV equals the value of the SP. A classic use of this is in the \nPID controller.\n"}
{"id": "14233462", "url": "https://en.wikipedia.org/wiki?curid=14233462", "title": "RTI Surgical", "text": "RTI Surgical\n\nRTI Surgical is a medical technology company that was founded in 1992 as Pioneer Surgical Technology in Marquette, Michigan, United States. The firm was Michigan's largest private medical technology company. The company was founded by Matthew Songer, and its first major product was the Songer Cable, used in spine surgeries. The Songer Cable was featured in the book, \"Contemporary Management of Spinal Cord Injuries: From Impact to Rehabilitation.\"\n\nPioneer Surgical has since developed products in the spine, biologics, orthopedics, and cardio-thoracic markets. Pioneer has facilities throughout the U.S. and Europe and employs nearly 300 people. In 2007, Pioneer acquired Angstrom Medica, a Massachusetts-based medical company focused on nanotechnology and Encelle, A North Carolina-based company researching and developing tissue regeneration products. Pioneer was acquired in July 2013 by RTI Biologics, and the firm was then renamed RTI Surgical.\n\n"}
{"id": "1041641", "url": "https://en.wikipedia.org/wiki?curid=1041641", "title": "Radiation hardening", "text": "Radiation hardening\n\nRadiation hardening is the act of making electronic components and systems resistant to damage or malfunctions caused by ionizing radiation (particle radiation and high-energy electromagnetic radiation), such as those encountered in outer space and high-altitude flight, around nuclear reactors and particle accelerators, or during nuclear accidents or nuclear warfare.\n\nMost semiconductor electronic components are susceptible to radiation damage; radiation-hardened components are based on their non-hardened equivalents, with some design and manufacturing variations that reduce the susceptibility to radiation damage. Due to the extensive development and testing required to produce a radiation-tolerant design of a microelectronic chip, radiation-hardened chips tend to lag behind the most recent developments.\n\nRadiation-hardened products are typically tested to one or more resultant effects tests, including total ionizing dose (TID), enhanced low dose rate effects (ELDRS), neutron and proton displacement damage, and single event effects (SEE, SET, SEL and SEB).\n\nEnvironments with high levels of ionizing radiation create special design challenges. A single charged particle can knock thousands of electrons loose, causing electronic noise and signal spikes. In the case of digital circuits, this can cause results which are inaccurate or unintelligible. This is a particularly serious problem in the design of satellites, spacecraft, military aircraft, nuclear power stations, and nuclear weapons. In order to ensure the proper operation of such systems, manufacturers of integrated circuits and sensors intended for the military or aerospace markets employ various methods of radiation hardening. The resulting systems are said to be rad(iation)-hardened, rad-hard, or (within context) hardened.\n\nTypical sources of exposure of electronics to ionizing radiation are the Van Allen radiation belts for satellites, nuclear reactors in power plants for sensors and control circuits, particle accelerators for control electronics particularly particle detector devices, residual radiation from isotopes in chip packaging materials, cosmic radiation for spacecraft and high-altitude aircraft, and nuclear explosions for potentially all military and civilian electronics.\n\n\nTwo fundamental damage mechanisms take place:\n\nLattice displacement is caused by neutrons, protons, alpha particles, heavy ions, and very high energy gamma photons. They change the arrangement of the atoms in the crystal lattice, creating lasting damage, and increasing the number of recombination centers, depleting the minority carriers and worsening the analog properties of the affected semiconductor junctions. Counterintuitively, higher doses over short time cause partial annealing (\"healing\") of the damaged lattice, leading to a lower degree of damage than with the same doses delivered in low intensity over a long time (LDR or Low Dose Rate). This type of problem is particularly significant in bipolar transistors, which are dependent on minority carriers in their base regions; increased losses caused by recombination cause loss of the transistor gain (see \"neutron effects\"). Components certified as ELDRS (Enhanced Low Dose Rate Sensitive) free, do not show damages with fluxes below 0.01 rad(Si)/s = 36 rad(Si)/h.\n\nIonization effects are caused by charged particles, including the ones with energy too low to cause lattice effects. The ionization effects are usually transient, creating glitches and soft errors, but can lead to destruction of the device if they trigger other damage mechanisms (e.g. a latchup). Photocurrent caused by ultraviolet and x-ray radiation may belong to this category as well. Gradual accumulation of holes in the oxide layer in MOSFET transistors leads to worsening of their performance, up to device failure when the dose is high enough (see \"total ionizing dose effects\").\n\nThe effects can vary wildly depending on all the parameters – type of radiation, total dose and radiation flux, combination of types of radiation, and even the kind of device load (operating frequency, operating voltage, actual state of the transistor during the instant it is struck by the particle) – which makes thorough testing difficult, time consuming, and requiring many test samples.\n\nThe \"end-user\" effects can be characterized in several groups,\nA neutron interacting with the semiconductor lattice will displace its atoms. This leads to an increase in the count of recombination centers and deep-level defects, reducing the lifetime of minority carriers, thus affecting bipolar devices more than CMOS ones. Bipolar devices on silicon tend to show changes in electrical parameters at levels of 10 to 10 neutrons/cm², CMOS devices aren't affected until 10 neutrons/cm². The sensitivity of the devices may increase together with increasing level of integration and decreasing size of individual structures. There is also a risk of induced radioactivity caused by neutron activation, which is a major source of noise in high energy astrophysics instruments. Induced radiation, together with residual radiation from impurities in used materials, can cause all sorts of single-event problems during the device's lifetime. GaAs LEDs, common in optocouplers, are very sensitive to neutrons. The lattice damage influences the frequency of crystal oscillators. Kinetic energy effects (namely lattice displacement) of charged particles belong here too.\n\nThe cumulative damage of the semiconductor lattice (\"lattice displacement\" damage) caused by ionizing radiation over the exposition time. It is measured in rads and causes slow gradual degradation of the device's performance. A total dose greater than 5000 rads delivered to silicon-based devices in seconds to minutes will cause long-term degradation. In CMOS devices, the radiation creates electron–hole pairs in the gate insulation layers, which cause photocurrents during their recombination, and the holes trapped in the lattice defects in the insulator create a persistent gate biasing and influence the transistors' threshold voltage, making the N-type MOSFET transistors easier and the P-type ones more difficult to switch on. The accumulated charge can be high enough to keep the transistors permanently open (or closed), leading to device failure. Some self-healing takes place over time, but this effect is not too significant. This effect is the same as hot carrier degradation in high-integration high-speed electronics. Crystal oscillators are somewhat sensitive to radiation doses, which alter their frequency. The sensitivity can be greatly reduced by using swept quartz. Natural quartz crystals are especially sensitive. Radiation performance curves for TID testing may be generated for all resultant effects testing procedures. These curves show performance trends throughout the TID test process and are included in the radiation test report.\n\nThe short-time high-intensity pulse of radiation, typically occurring during a nuclear explosion. The high radiation flux creates photocurrents in the entire body of the semiconductor, causing transistors to randomly open, changing logical states of flip-flops and memory cells. Permanent damage may occur if the duration of the pulse is too long, or if the pulse causes junction damage or a latchup. Latchups are commonly caused by the x-rays and gamma radiation flash of a nuclear explosion. Crystal oscillators may stop oscillating for the duration of the flash due to prompt photoconductivity induced in quartz.\n\nSGEMP are caused by the radiation flash traveling through the equipment and causing local ionization and electric currents in the material of the chips, circuit boards, electrical cables and cases.\n\nSingle-event effects (SEE), mostly affecting only digital devices, were not studied extensively until relatively recently. When a high-energy particle travels through a semiconductor, it leaves an ionized track behind. This ionization may cause a highly localized effect similar to the transient dose one - a benign glitch in output, a less benign bit flip in memory or a register or, especially in high-power transistors, a destructive latchup and burnout. Single event effects have importance for electronics in satellites, aircraft, and other civilian and military aerospace applications. Sometimes, in circuits not involving latches, it is helpful to introduce RC time constant circuits that slow down the circuit's reaction time beyond the duration of an SEE.\n\nSET happens when the charge collected from an ionization event discharges in the form of a spurious signal traveling through the circuit. This is de facto the effect of an electrostatic discharge. Soft error, reversible.\n\nSingle-event upsets (SEU) or transient radiation effects in electronics are state changes of memory or register bits caused by a single ion interacting with the chip. They do not cause lasting damage to the device, but may cause lasting problems to a system which cannot recover from such an error. Soft error, reversible. In very sensitive devices, a single ion can cause a multiple-bit upset (MBU) in several adjacent memory cells. SEUs can become Single-event functional interrupts (SEFI) when they upset control circuits, such as state machines, placing the device into an undefined state, a test mode, or a halt, which would then need a reset or a power cycle to recover.\n\nSEL can occur in any chip with a parasitic PNPN structure. A heavy ion or a high-energy proton passing through one of the two inner-transistor junctions can turn on the thyristor-like structure, which then stays \"shorted\" (an effect known as latchup) until the device is power-cycled. As the effect can happen between the power source and substrate, destructively high current can be involved and the part may fail. Hard error, irreversible. Bulk CMOS devices are most susceptible.\n\nSingle-event snapback is similar to SEL but not requiring the PNPN structure, can be induced in N-channel MOS transistors switching large currents, when an ion hits near the drain junction and causes avalanche multiplication of the charge carriers. The transistor then opens and stays opened. Hard error, irreversible.\n\nSEB may occur in power MOSFETs when the substrate right under the source region gets forward-biased and the drain-source voltage is higher than the breakdown voltage of the parasitic structures. The resulting high current and local overheating then may destroy the device. Hard error, irreversible.\n\nSEGR was observed in power MOSFETs when a heavy ion hits the gate region while a high voltage is applied to the gate. A local breakdown then happens in the insulating layer of silicon dioxide, causing local overheat and destruction (looking like a microscopic explosion) of the gate region. It can occur even in EEPROM cells during write or erase, when the cells are subjected to a comparatively high voltage. Hard error, irreversible.\n\nWhile proton beams are widely used for SEE testing due to availability, at lower energies proton irradiation can often underestimate SEE susceptibility. Furthermore, proton beams expose devices to risk of total ionizing dose (TID) failure which can cloud proton testing results or result in pre-mature device failure. White neutron beams — ostensibly the most representative SEE test method — are usually derived from solid target-based sources, resulting in flux non-uniformity and small beam areas. White neutron beams also have some measure of uncertainty in their energy spectrum, often with high thermal neutron content.\n\nThe disadvantages of both proton and spallation neutron sources can be avoided by using mono-energetic 14 MeV neutrons for SEE testing. A potential concern is that mono-energetic neutron-induced single event effects will not accurately represent the real-world effects of broad-spectrum atmospheric neutrons. However, recent studies have indicated that, to the contrary, mono-energetic neutrons—particularly 14 MeV neutrons—can be used to quite accurately understand SEE cross-sections in modern microelectronics.\n\nA particular study of interest, performed in 2010 by Normand and Dominik, powerfully demonstrates the effectiveness of 14 MeV neutrons.\n\nHardened chips are often manufactured on insulating substrates instead of the usual semiconductor wafers. Silicon on insulator (SOI) and sapphire (SOS) are commonly used. While normal commercial-grade chips can withstand between 50 and 100 gray (5 and 10 krad), space-grade SOI and SOS chips can survive doses many orders of magnitude greater. At one time many 4000 series chips were available in radiation-hardened versions (RadHard).\n\nBipolar integrated circuits generally have higher radiation tolerance than CMOS circuits. The low-power Schottky (LS) 5400 series can withstand 1000 krad, and many ECL devices can withstand 10 000 krad.\n\nMagnetoresistive RAM, or MRAM, is considered a likely candidate to provide radiation hardened, rewritable, non-volatile conductor memory. Physical principles and early tests suggest that MRAM is not susceptible to ionization-induced data loss.\n\nShielding the package against radioactivity, to reduce exposure of the bare device.\n\nCapacitor-based DRAM is often replaced by more rugged (but larger, and more expensive) SRAM.\n\nChoice of substrate with wide band gap, which gives it higher tolerance to deep-level defects; e.g. silicon carbide or gallium nitride.\n\nShielding the chips themselves by use of depleted boron (consisting only of isotope boron-11) in the borophosphosilicate glass passivation layer protecting the chips, as boron-10 readily captures neutrons and undergoes alpha decay (see soft error).\n\nError correcting memory uses additional parity bits to check for and possibly correct corrupted data. Since radiation effects damage the memory content even when the system is not accessing the RAM, a \"scrubber\" circuit must continuously sweep the RAM; reading out the data, checking the parity for data errors, then writing back any corrections to the RAM.\n\nRedundant elements can be used at the system level. Three separate microprocessor boards may independently compute an answer to a calculation and compare their answers. Any system that produces a minority result will recalculate. Logic may be added such that if repeated errors occur from the same system, that board is shut down.\n\nRedundant elements may be used at the circuit level. A single bit may be replaced with three bits and separate \"voting logic\" for each bit to continuously determine its result. This increases area of a chip design by a factor of 5, so must be reserved for smaller designs. But it has the secondary advantage of also being \"fail-safe\" in real time. In the event of a single-bit failure (which may be unrelated to radiation), the voting logic will continue to produce the correct result without resorting to a watchdog timer. System level voting between three separate processor systems will generally need to use some circuit-level voting logic to perform the votes between the three processor systems. \n\nHardened latches may be used.\n\nA watchdog timer will perform a hard reset of a system unless some sequence is performed that generally indicates the system is alive, such as a write operation from an onboard processor. During normal operation, software schedules a write to the watchdog timer at regular intervals to prevent the timer from running out. If radiation causes the processor to operate incorrectly, it is unlikely the software will work correctly enough to clear the watchdog timer. The watchdog eventually times out and forces a hard reset to the system. This is considered a last resort to other methods of radiation hardening.\n\nRadiation-hardened and radiation tolerant components are often used in military and space applications. These applications may include point-of-Load (POL) applications, satellite system power supply, step down switching regulator, microprocessor, FPGAs), FPGA power source, high efficiency low voltage subsystem power supply\n\nIn telecommunication, the term \"nuclear hardness\" has the following meanings:\n1) an expression of the extent to which the performance of a system, facility, or device is expected to degrade in a given nuclear environment, 2) the physical attributes of a system or electronic component that will allow survival in an environment that includes nuclear radiation and electromagnetic pulses (EMP).\n\n\n\n\n\n"}
{"id": "6187740", "url": "https://en.wikipedia.org/wiki?curid=6187740", "title": "Renewable fuels", "text": "Renewable fuels\n\nRenewable fuels are fuels produced from renewable resources. Examples include: biofuels (e.g. Vegetable oil used as fuel, ethanol, methanol from clean energy and carbon dioxide or biomass, and biodiesel) and Hydrogen fuel (when produced with renewable processes). This is in contrast to non-renewable fuels such as natural gas, LPG (propane), petroleum and other fossil fuels and nuclear energy. Renewable fuels can include fuels that are synthesized from renewable energy sources, such as wind and solar. Renewable fuels have gained in popularity due to their sustainability, low contributions to the carbon cycle, and in some cases lower amounts of greenhouse gases. The geo-political ramifications of these fuels are also of interest, particularly to industrialized economies which desire independence from Middle Eastern oil.\n\nThe International Energy Agency's \"World Energy Outlook 2006\" concludes that rising oil demand, if left unchecked,\nwould accentuate the consuming countries' vulnerability to a severe supply disruption and resulting price shock. Renewable biofuels for transport represent a key source of diversification from petroleum products. Biofuels from grain and beet in temperate regions have a part to play, but they are relatively expensive and their energy efficiency and CO2 savings benefits, are variable. Biofuels from sugar cane and other highly productive tropical crops are much more competitive and beneficial. But all first generation biofuels ultimately compete with food production for land, water, and other resources. Greater efforts are required to develop and commercialize second generation biofuel technologies, such as biorefineries and ligno-cellulosics, enabling the flexible production of biofuels and other products from non-edible plant materials.\n\nHubbert's peak oil theory suggests that petroleum is a finite resource that is rapidly depleting. Of the worldwide total remaining petroleum reserves of approximately (about one half of the original virgin reserves) and a worldwide usage rate of per year, only about 50 years worth of petroleum is predicted to remain at the current depletion rate. Petroleum is imperative for the following industries: fuel (home heating, jet fuel, gasoline, diesel, etc.) transportation, agriculture, pharmaceutical, plastics/resins, man-made fibers, synthetic rubber, and explosives. If the modern world remains reliant on petroleum as a source of energy, the price of crude oil could increase markedly, destabilizing economies worldwide. Consequently, renewable fuel drivers include: high oil prices, imbalance of trade, instability in oil exporting regions of the world, the Energy Policy Act of 2005, the potential for windfall profits for American farmers and industries, avoidance of economic depression, avoidance of scarcity of products due to a volatile ‘peak oil’ scenario expected to begin as early as 2021, (though peak oil is not a new idea) and a slowing of global warming that may usher in unprecedented climate change.\n\nFurthermore, the global debate on climate change, along with regional geopolitical instabilities have challenged nations to act to develop both alternative and carbon-neutral sources of energy. Renewable fuels are therefore becoming attractive to many governments, who are beginning to see sustainable energy independence as a valuable asset.\n\nOn December 19, 2007, President Bush signed into law the Energy Independence and Security Act, establishing a requirement that at least of renewable fuel be used in the marketplace by 2022.\n\nAccording to the International Energy Agency (IEA), cellulosic ethanol commercialization could allow ethanol fuels to play a much larger role in the future than previously thought. Cellulosic ethanol can be made from plant matter composed primarily of inedible cellulose fibers that form the stems and branches of most plants. Dedicated energy crops, such as switchgrass, are also promising cellulose sources that can be produced in many regions of the United States.\n\nBiofuel is a type of fuel whose energy is derived from biological carbon fixation. Biofuels include fuels derived from biomass conversion, as well as solid biomass, liquid fuels and various biogases. Although fossil fuels have their origin in ancient carbon fixation, they are not considered biofuels by the generally accepted definition because they contain carbon that has been \"out\" of the carbon cycle for a very long time. Biofuels are gaining increased public and scientific attention, driven by factors such as oil price spikes, the need for increased energy security, concern over greenhouse gas emissions from fossil fuels, and support from government subsidies.\n\nBioethanol is an alcohol made by fermentation, mostly from carbohydrates produced in sugar or starch crops such as corn or sugar cane. Cellulosic biomass, derived from non-food sources such as trees and grasses, is also being developed as a feedstock for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the USA and in Brazil. Current plant design does not provide for converting the lignin portion of plant raw materials to fuel components by fermentation.\n\nBiodiesel is made from vegetable oils and animal fats. Biodiesel can be used as a fuel for vehicles in its pure form, but it is usually used as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe.\n\nIn 2010 worldwide biofuel production reached 105 billion liters (28 billion gallons US), up 17% from 2009, and biofuels provided 2.7% of the world's fuels for road transport, a contribution largely made up of ethanol and biodiesel. Global ethanol fuel production reached 86 billion liters (23 billion gallons US) in 2010, with the United States and Brazil as the world's top producers, accounting together for 90% of global production. The world's largest biodiesel producer is the European Union, accounting for 53% of all biodiesel production in 2010. As of 2011, mandates for blending biofuels exist in 31 countries at the national level and in 29 states/provinces. According to the International Energy Agency, biofuels have the potential to meet more than a quarter of world demand for transportation fuels by 2050.\n\nPyrolysis oil is another type of fuel derived from the lignocellulosic fraction of biomass. By rapidly heating biomass in the absence of oxygen (pyrolysis), a liquid crude can be formed that can be further processed into a usable bio-oil. As opposed to other biofuels, pyrolysis oils use the non-edible fraction of biomass and can occur on the order of milliseconds and without the need for large fermentation reactors.\n\nHydrogen fuel refers to the use of hydrogen gas (H) as an energy carrier. Broadly speaking, the production of renewable hydrogen fuel can be divided into two general categories: biologically derived production, and chemical production. This is an area of current research, and new developments and technologies are causing this field to evolve rapidly.\n\nThe biological production of hydrogen fuel has been a topic of research since at least the 1970s. Hydrogen gas can be produced from biomass sources like agricultural and forest residues, consumer waste and other specific agricultural crops. Specifically, hydrogen fuel is produced by a process called gasification, where biomass is processed into a combustible gas and then burned, or by pyrolysis, a related process which can lead to hydrogen gas suitable for fuel-cell applications. One continuing subject of research regards the production of unwanted co-products in both of these processes. The presence of other contaminant gases often depend on specific composition of the biomass source, which can be difficult to control. Another source for biological production of hydrogen fuel is algae. In the late 1990s it was discovered that if algae are deprived of sulfur they will switch from the production of oxygen, as in normal photosynthesis, to the production of hydrogen. Experimental algae farms are attempting to make algae an economically feasible energy source.\n\nThere are also several physico-chemical methods for producing hydrogen; most of these methods require electrolysis of water. When this process draws its power from renewable energy sources like wind turbines or photovoltaic cells, the production requires little consumption of non-renewable resources. Hydrogen fuel, when produced by renewable sources of energy like wind or solar power, is a renewable fuel.\n\nPEF is a partial replacement for fossil fuels in cement kilns. It has significant calorific value and can be used as a fuel substitute for coal and gas in high-combustion facilities. PEF facilities typically divert waste from landfill, reducing demand for non-renewable coal and reducing waste to landfill.\n\n"}
{"id": "23151867", "url": "https://en.wikipedia.org/wiki?curid=23151867", "title": "SlideIT", "text": "SlideIT\n\nSlideIT is a text input method for touchscreen devices developed by Dasur Ltd based on pattern recognition.\n\nOn Android, it works up to and including Lollipop, version 5 of Android.\n\nSlideIT allows a user to enter a word by sliding a finger or stylus from letter to letter on a virtual keyboard, lifting only between words. SlideIT uses advanced pattern recognition and error-correction algorithms together with a complicated language model and word database to ensure maximum accuracy of the text in output, while allowing the user a lot of freedom and inaccuracy in entering text. This allows for text input at much higher rates than normal mobile phone and other small touch-screen keyboards.\n\nDasur is a technology company that specializes in complex mathematical applications. Over the last 20 years, Dasur has developed a powerful pattern recognition engine which can be the basis for many applications. Based on this engine, Dasur has developed this generation of text input solutions for mobile phones and PDAs.\n\nThe first version was released on 2007 as two separate keyboards, ThumbKey and SlideIT. ThumbKey's technology brought the touchscreen method with the option of using only one hand. Dasur's ThumbKey used an algorithm that allowed users to just press in the general vicinity of the required key for the intended word to appear on the screen. The algorithm processed the typed letters and its disambiguation feature suggested the intended word and also displayed additional possibilities next to the keyboard.\n\nIn 2009 the two keyboards were merged into one product branded SlideIT which offers an 'ABC' mode with the ThumbKey unique algorithm and the 'Slide' mode with the sliding algorithm. At the end of 2010 Dasur released a new version for Symbian and in early 2011 a version 3.0 for the Android was released.\n\nThe keyboard can be customized with a primary and several secondary languages, and the user can rotate between them at any time by pressing a button. In addition, Slide it supports a third tier of languages accessible by long-pressing the language button. \n\nIn effect, this multi language support is a key differentiator to other keyboards for users regularly switching between three, four or more languages. Rotate between the two or three most used languages, have the other ones quickly available.\n\nThe average computer user, using a standard full-size QWERTY keyboard, types 33 words per minute. PDAs and smartphones allow much slower speeds. Sliding style keyboards for touch screens can increases typing speed by 30-40% and yields potential typing speeds of more than 50 words per minute – matching the average speed of a professional typist. A comparable sliding style keyboard, Swype, as of late 2010, holds the world speed record for text input into a mobile phone.\n\nSlideIT can be run on any Android phone or tablet computer, Windows Mobile and Symbian S60 v5. SlideIT for iPhone and Windows Tablet PC will be available soon.\n\n\n"}
{"id": "37292180", "url": "https://en.wikipedia.org/wiki?curid=37292180", "title": "Smart intelligent aircraft structure", "text": "Smart intelligent aircraft structure\n\nThe term \"smart structures\" is commonly used for structures which have the ability to adapt to environmental conditions according to the design requirements. As a rule, the adjustments are designed and performed in order to increase the efficiency or safety of the structure. Combining \"smart structures\" with the \"sophistication\" achieved in materials science, information technology, measurement science, sensors, actuators, signal processing, nanotechnology, cybernetics, artificial intelligence, and biomimetics, one can talk about Smart Intelligent Structures. In other words, structures which are able to sense their environment, self-diagnose their condition and adapt in such a way so as to make the design more useful and efficient.\n\nThe concept of Smart Intelligent Aircraft Structures offers significant improvements in aircraft total weight, manufacturing cost and, above all, operational cost by an integration of system tasks into the load carrying structure. It also helps to improve the aircraft’s life cycle and reduce its maintenance. Individual morphing concepts also have the ability to decrease airframe generated noise and hence reduce the effect of air traffic noise near airports. Furthermore, cruise drag reductions have a positive effect on fuel consumption and required take-off fuel load.\n\nFixed geometry wings are optimized for a single design point, identified through altitude, Mach number, weight, etc. Their development is always a compromise between design and off-design points, referred to a typical mission. This is emphasised for civil aircraft where flight profiles are almost standard. Nevertheless, it may occur to fly at high speeds and low altitude with light weight over a short stretch or to fly at low speeds and high altitude with maximum load for a longer range. The lift coefficient would then range between 0.08 and 0.4, with the aircraft experiencing up to 30% weight reduction as the fuel is consumed. These changes could be compensated by wing camber variations, to pursue optimal geometry for any flight condition, thus improving aerodynamic and structural performance\n\nExisting aircraft cannot change shape without aerodynamic gaps, something that can be solved with Smart Intelligent Structures. By ensuring the detailed consideration of structural needs throughout the entire lifetime of an aircraft and focusing on the structural integration of needed past capabilities, Smart Intelligent Aircraft Structures will allow aircraft designers to seriously consider conformal morphing technologies. The reduced drag during take-off, cruise and landing for future and ecologically improved civil aircraft wings can be achieved through naturally laminar wing technology, by incorporating a gapless and deformable leading edge device with lift providing capability. Such a morphing structure typically consists of a flexible outer skin and an internal driving mechanism (Figure 1).\nCurrent aircraft designs already employ winglets aimed at increasing the cruise flight efficiency by induced drag reduction. Smart intelligent Structures propose a state of the art technology that incorporates a wingtip active trailing edge, which could be a means of reducing winglet and wing loads at key flight conditions.\n\nAnother component of an \"intelligent\" aircraft structure is the ability to sense and diagnose potential threats to its structural integrity. This differs from conventional non-destructive testing (NDT) by the fact that Structural Health Monitoring (SHM) uses sensors that are permanently bonded or embedded in the structure. Composite materials, which are highly susceptible to hidden internal flaws which may occur during manufacturing and processing of the material or while the structure is subjected to service loads, require a substantial amount of inspection and defect monitoring at regular intervals. Thus, the increasing use of composite materials for aircraft primary structure aircraft components increases substantially their life cycle cost. According to some estimates, over 25% of the life cycle cost of an aircraft or aerospace structure, which includes pre-production, production, and post-production costs, can be attributed to operation and support, involving inspection and maintenance. With sensing technology reducing in cost, size and weight, and sensor signal processing power continuously increasing, a variety of approaches have been developed allowing integration of such sensing options onto or into structural components.\n\nAlthough available in principle, none of these SHM technologies have currently achieved a sufficient level of maturity such that SHM could be reliably applied to real engineering structures. A real reduction of life cycle costs related to maintenance and inspections can only be achieved by SHM systems designed as \"fail-safe\" components and included within a damage tolerance assessment scenario, able to reduce the inspection times (or their intervals) by investigating the structure quickly and reliably and avoiding the time consuming disassembly of structural parts.\n\nThe advantages of carbon fibre reinforced polymers (CFRPs) over metallic materials in terms of specific stiffness and strength are well known. In the last few years, there has been a sharp increase in the demand for composite materials with integrated multifunctional capabilities for use in aeronautical structures.\n\nHowever, a major drawback with CFRPs for primary structural applications is their low toughness and damage tolerance. Epoxy resins are brittle and have poor impact strength and resistance to crack propagation, resulting in unsatisfactory levels of robustness and reliability. This results in designs with large margins of safety and complex inspection operations. In addition, by increasing the relative fraction of composite components within new aircraft, challenges regarding electrical conductivity have arisen such as lightning strike protection, static discharge, electrical bonding and grounding, interference shielding and current return through the structure. These drawbacks can be solved by the use of emerging technologies such as nanocomposites, which combine mechanical, electrical and thermal properties.\n\nNanoparticle reinforced resins have been found to offer two distinct advantages over current resin systems. First of all, they are able to provide an increase in fracture toughness of up to 50% for older liquid resin infusion (LRI) resins and 30% in more advanced systems. Secondly, percolated nanoparticles drastically improve resin conductivity, turning it from a perfect isolator into a semiconductor. While improved damage tolerance properties could directly lead to structural weight savings, the exploitation of electrical properties could also enable a simpler, and hence cheaper, Electrical Structure Network (ESN).\n\nDeveloping these technologies for future A/C, there is currently (2011 – 2015) a running project, partially funded by the European Commission, called \"SARISTU\" (Smart Intelligent Aircraft Structures) with a total budget of €51,000,000. This initiative is coordinated by Airbus and brings together 64 partners from 16 European countries. SARISTU focuses on the cost reduction of air travel through a variety of individual applications as well as their combination. Specifically, the integration of different conformal morphing concepts in a laminar wing is intended to improve aircraft performance through a 6% drag reduction, with a positive effect on fuel consumption and required take-off fuel load. A side effect will be a decrease of up to 6 dB(A) of the airframe generated noise, thus reducing the impact of air traffic noise in the vicinity of airports. Recent calculations and Computational Fluid Dynamics Analysis indicate that the target is likely to be exceeded but will still need to be offset against a possible weight penalty.\n\nAnother expected outcome is to limit the integration cost of Structural Health Monitoring (SHM) systems by moving the system integration as far forward in the manufacturing chain as possible. In this manner, SHM integration becomes a feasible concept to enable in-service inspection cost reductions of up to 1%. Structural Health Monitoring related trials indicate that specific aircraft inspections may gain higher benefits than originally anticipated.\n\nFinally, the incorporation of Carbon Nanotubes into aeronautical resins is expected to enable weight savings of up to 3% when compared to the unmodified skin/stringer/frame system, while a combination of technologies is expected to decrease Electrical Structure Network installation costs by up to 15%.\n\n"}
{"id": "34504569", "url": "https://en.wikipedia.org/wiki?curid=34504569", "title": "Solar fuel", "text": "Solar fuel\n\nA solar fuel is a synthetic chemical fuel produced directly/indirectly from solar energy sunlight/solar heat through photochemical/photobiological (i.e., artificial photosynthesis, experimental as of 2013), thermochemical(i.e. through the use of solar heat supplied by concentrated solar thermal energy to drive a chemical reaction), and electrochemical reaction. Light is used as an energy source, with solar energy being transduced to chemical energy, typically by reducing protons to hydrogen, or carbon dioxide to organic compounds. A solar fuel can be produced and stored for later usage, when sunlight is not available, making it an alternative to fossil fuels. Diverse photocatalysts are being developed to carry these reactions in a sustainable, environmentally friendly way.\n\nThe world's dependence on the declining reserves of fossil fuels poses not only environmental problems but also geopolitical ones. Solar fuels, in particular hydrogen, are viewed as an alternative source of energy for replacing fossil fuels especially where storage is essential. Electricity can be produced directly from sunlight through photovoltaics, but this form of energy is rather inefficient to store compared to hydrogen. A solar fuel can be produced when and where sunlight is available, and stored and transported for later usage.\n\nThe most widely researched solar fuels are hydrogen and products of carbon dioxide reduction.\n\nSolar fuels can be produced via direct or indirect processes. Direct processes harness the energy in sunlight to produce a fuel without intermediary energy conversions. In contrast, indirect processes have solar energy converted to another form of energy first (such as biomass or electricity) that can then be used to produce a fuel. Indirect processes have been easier to implement but have the disadvantage of being less efficient than, e.g., water splitting for the production of hydrogen, since energy is wasted in the intermediary conversion.\n\nHydrogen can be produced by electrolysis. To use sunlight in this process, a photoelectrochemical cell can be used, where one photosensitized electrode converts light into an electric current that is then used for water splitting. One such type of cell is the dye-sensitized solar cell. This is an indirect process, since it produces electricity that then is used to form hydrogen. The other major indirect process using sunlight is conversion of biomass to biofuel using photosynthetic organisms; however, most of the energy harvested by photosynthesis is used in life-sustaining processes and therefore lost for energy use.\n\nA direct process can use a catalyst that reduces protons to molecular hydrogen upon electrons from an excited photosensitizer. Several such catalysts have been developed as proof of concept, but not yet scaled up for commercial use; nevertheless, their relative simplicity gives the advantage of potential lower cost and increased energy conversion efficiency. One such proof of concept is the \"artificial leaf\" developed by Nocera and coworkers: a combination of metal oxide-based catalysts and a semiconductor solar cell produces hydrogen upon illumination, with oxygen as the only byproduct.\n\nHydrogen can also be produced from some photosynthetic microorganisms (microalgae and cyanobacteria) using photobioreactors. Some of these organisms produce hydrogen upon switching culture conditions; for example, \"Chlamydomonas reinhardtii\" produces hydrogen anaerobically under sulfur deprivation, that is, when cells are moved from one growth medium to another that does not contain sulfur, and are grown without access to atmospheric oxygen. Another approach was to abolish activity of the hydrogen-oxidizing (uptake) hydrogenase enzyme in the diazotrophic cyanobacterium \"Nostoc punctiforme\", so that it would not consume hydrogen that is naturally produced by the nitrogenase enzyme in nitrogen-fixing conditions. This \"N. punctiforme\" mutant could then produce hydrogen when illuminated with visible light.\n\nCarbon dioxide (CO) can be reduced to carbon monoxide (CO) and other more reduced compounds, such as methane, using the appropriate photocatalysts. One early example was the use of Tris(bipyridine)ruthenium(II) chloride (Ru(bipy)Cl) and cobalt chloride (CoCl) for CO reduction to CO. Many compounds that do similar reactions have since been developed, but they generally perform poorly with atmospheric concentrations of CO, requiring further concentration. The simplest product from CO reduction is carbon monoxide (CO), but for fuel development, further reduction is needed, and a key step also needing further development is the transfer of hydride anions to CO.\n\nAlso in this case, the use of microorganisms has been explored. Using genetic engineering and synthetic biology techniques, parts of or whole biofuel-producing metabolic pathways can be introduced in photosynthetic organisms. One example is the production of 1-butanol in \"Synechococcus elongatus\" using enzymes from \"Clostridium acetobutylicum\", \"Escherichia coli\" and \"Treponema denticola\". One example of a large-scale research facility exploring this type of biofuel production is the AlgaePARC in the Wageningen University and Research Centre, Netherlands.\n\n\n"}
{"id": "52624552", "url": "https://en.wikipedia.org/wiki?curid=52624552", "title": "The Sustainable City", "text": "The Sustainable City\n\nThe Sustainable City is a 46 hectare property development in Dubai, United Arab Emirates. Situated on the Al Qudra road, it is the first net zero energy development in the Emirate of Dubai. The development includes 500 villas, 89 apartments and a mixed use area consisting of offices, retail, healthcare facilities, a nursery and food and beverage outlets. Phase 2 of the development will include a hotel, school and innovation centre. \n\nThe City was developed by Dubai-based Diamond Developers, whose Chief Executive Officer, Faris Saeed, has stated that much of his inspiration for the development came from UC Davis West Village.\n\nKey elements of the City include:\n\n\nApart from periphery roads and car parking areas, the development is a car-free site.\n\nThe parking areas are topped by solar shading featuring solar panels that are connected to an electrical grid to supply energy into different sections of the city.\n\nPanels are also placed on the roofs of all of the houses.\n\nThe construction waste is reused to furniture the public spaces.\n\nThe townhouses have UV reflective paint to reduce the thermal heat gain inside the houses.\n\n\n"}
{"id": "8269245", "url": "https://en.wikipedia.org/wiki?curid=8269245", "title": "Time-domain reflectometry", "text": "Time-domain reflectometry\n\nTime-domain reflectometry or TDR is a measurement technique used to determine the characteristics of electrical lines by observing reflected waveforms. Time-domain transmissometry (TDT) is an analogous technique that measures the transmitted (rather than reflected) impulse. Together, they provide a powerful means of analysing electrical or optical transmission media such as coaxial cable and optical fiber. \n\nVariations of TDR exist. For example, spread-spectrum time-domain reflectometry (SSTDR) is used to detect intermittent faults in complex and high-noise systems such as aircraft wiring. Coherent optical time domain reflectometry (COTDR) is another variant, used in optical systems, in which the returned signal is mixed with a local oscillator and then filtered to reduce noise.\n\nThe impedance of the discontinuity can be determined from the amplitude of the reflected signal. The distance to the reflecting impedance can also be determined from the time that a pulse takes to return. The limitation of this method is the minimum system rise time. The total rise time consists of the combined rise time of the driving pulse and that of the oscilloscope or sampler that monitors the reflections.\n\nThe TDR analysis begins with the propagation of a step or impulse of energy into a system and the subsequent observation of the energy reflected by the system. By analyzing the magnitude, duration and shape of the reflected waveform, the nature of the impedance variation in the transmission system can be determined.\n\nIf a pure resistive load is placed on the output of the reflectometer and a step signal is applied, a step signal is observed on the display, and its height is a function of the resistance. The magnitude of the step produced by the resistive load may be expressed as a fraction of the input signal as given by:\n\nwhere formula_2 is the characteristic impedance of the transmission line.\n\nFor reactive loads, the observed waveform depends upon the time constant formed by the load and the characteristic impedance of the line.\n\n"}
{"id": "19424663", "url": "https://en.wikipedia.org/wiki?curid=19424663", "title": "UniPro protocol stack", "text": "UniPro protocol stack\n\nIn mobile-telephone technology, the UniPro protocol stack follows the architecture of the classical OSI Reference Model. In UniPro, the OSI Physical Layer is split into two sublayers: Layer 1 (the actual physical layer) and Layer 1.5 (the PHY Adapter layer) which abstracts from differences between alternative Layer 1 technologies. The actual physical layer is a separate specification as the various PHY options are reused in other MIPI Alliance specifications.\n\nThe UniPro specification itself covers Layers 1.5, 2, 3, 4 and the DME (Device Management Entity). The Application Layer (LA) is out of scope because different uses of UniPro will require different LA protocols. The Physical Layer (L1) is covered in separate MIPI specifications in order to allow the PHY to be reused by other (less generic) protocols if needed.\n\nOSI Layers 5 (Session) and 6 (Presentation) are, where applicable, counted as part of the Application Layer.\n\nVersions 1.0 and 1.1 of UniPro use MIPI's D-PHY technology for the off-chip Physical Layer. This PHY allows inter-chip communication. Data rates of the D-PHY are variable, but are in the range of 500-1000 Mbit/s (lower speeds are supported, but at decreased power efficiency). The D-PHY was named after the Roman number for 500 (\"D\").\n\nThe D-PHY uses differential signaling to convey PHY symbols over micro-stripline wiring. A second differential signal pair is used to transmit the associated clock signal from the source to the destination. The D-PHY technology thus uses a total of 2 clock wires per direction plus 2 signal wires per lane and per direction. For example, a D-PHY might use 2 wires for the clock and 4 wires (2 lanes) for the data in the forward direction, but 2 wires for the clock and 6 wires (3 lanes) for the data in the reverse direction. Data traffic in the forward and reverse directions are totally independent at this level of the protocol stack.\n\nIn UniPro, the D-PHY is used in a mode (called \"8b9b\" encoding) which conveys 8-bit bytes as 9-bit symbols. The UniPro protocol uses this to represent special control symbols (outside the usual 0 to 255 values). The PHY itself uses this to represent certain special symbols that have meaning to the PHY itself (e.g. IDLE symbols). Note that the ratio 8:9 can cause some confusion when specifying the data rate of the D-PHY: a PHY implementation running with a 450 MHz clock frequency is often rated as a 900 Mbit/s PHY, while only 800 Mbit/s is then available for the UniPro stack.\n\nThe D-PHY also supports a Low-Power Data Transmission (LPDT) mode and various other low-power modes for use when no data needs to be sent.\n\nVersions 1.4 and beyond of UniPro support both the D-PHY as well as M-PHY technology. The M-PHY technology is still in draft status, but supports high-speed data rates starting at about 1000 Mbit/s (the M-PHY was named after the Roman number for 1000). In addition to higher speeds, the M-PHY will use fewer signal wires because the clock signal is embedded with the data through the use of industry-standard 8b10b encoding. Again, a PHY capable of transmitting user data at 1000 Mbit/s is typically specified as being in 1250 Mbit/s mode due to the 8b10b encoding.\n\nThe D- and M-PHY are expected to co-exist for several years. D-PHY is a less complex technology, M-PHY provides higher bandwidths with fewer signal wires, and C-PHY provides low-power.\n\nIt is worth noting that UniPro supports the power efficient low speed communication modes provided by both the D-PHY (10 Mbit/s) and M-PHY (3 Mbit/sec up to 500 Mbit/s). In these modes, power consumption roughly scales with the amount of data that is sent.\nFurthermore, both PHY technologies provide additional power saving modes because they were optimized for use in battery-powered devices.\n\nArchitecturally, the PHY Adapter layer serves to hide the differences between the different PHY options (D- and M-PHY). This abstraction thus mainly gives architectural flexibility. Abstracted PHY details include the various power states and employed symbol encoding schemes.\n\nL1.5 thus has its own (conceptual) symbol encoding consisting of 17-bit symbols. These 17-bit symbols never show up on the wires, because they are first converted by L1.5 to a pair of PHY symbols. The extra 17th control bit indicates special control symbols which are used by the protocol (L1.5 and L2) itself. In the figures, the control bits are shown in \"L1.5 red\" as a reminder that they are defined in- and used by protocol Layer 1.5.\n\nThe main feature that L1.5 offers users is to allow the bandwidth of a UniPro link to be increased by using 2, 3 or 4 lanes when a single lane does not provide enough bandwidth. To the user, such a multi-lane link simply looks like a faster physical layer because the symbols are sent across 2, 3 or 4 lanes. Applications that require higher bandwidth in one direction but require less bandwidth in the opposite direction, can have different numbers of lanes per direction.\n\nStarting in UniPro v1.4, L1.5 automatically discovers the number of usable M-PHY lanes for each direction of the link. This involves a simple discovery protocol within L1.5 that is executed on initialization. The protocol transmits test data on each available outbound lane, and receives information back from the peer entity about which data on which lane actually made it to the other end of the link. The mechanism also supports transparent remapping of the lanes to give circuit board designers flexibility in how the lanes are physically wired.\n\nStarting in UniPro v1.4, L1.5 has a built in protocol called PACP (PA Control Protocol) that allows L1.5 to communicate with its peer L1.5 entity at the other end of an M-PHY-based link. Its main usage is to provide a simple and reliable way for a controller at one end of the link to change the power modes of both the forward and reverse directions of the link. This means that a controller situated at one end of the link can change the power mode of both link directions in a single atomic operation. The intricate steps required for doing this in a fully reliable way are handled transparently within L1.5.\n\nIn addition to the L1.5 link power management the PACP is also used to access control and status parameters of the peer UniPro device.\n\nThe mechanisms in L1.5 guarantee the following to upper layer protocols:\n\nThe main task of UniPro's Data Link layer (L2) is to allow reliable communication between two adjacent nodes in the network - despite occasional bit errors at the Physical layer or potential link congestion if the receiver cannot absorb the data fast enough.\n\nL2 clusters 17-bit UniPro L1.5 symbols into packet-like data frames (the term packet is reserved for L3). These data frames start with a 17-bit start-of-frame control symbol, followed by up to 288 bytes of data (144 data symbols) and followed by an end-of-frame control symbol and a checksum.\n\nNote that two or more of the 288 bytes are used by higher layers of the UniPro protocol. The maximum frame size of 288 payload bytes per frame was chosen to ensure that the entire protocol stack could easily transmit 256 bytes of application data in a single chunk. Payloads consisting of odd numbers of bytes are supported by padding the frame to an even number of bytes and inserting a corresponding flag in the trailer.\n\nIn addition to data frames which contain user data, L2 also transmits and receives control frames. The control frames can be distinguished from data frames by three bits in the first symbol. There are two types of control frames:\nNote that these L2 types of control frames are sent autonomously by L2.\n\nHigh speed communication at low power levels can lead to occasional errors in the received data. The Data Link layer contains a protocol to automatically acknowledge correctly received data frames (using AFC control frames) and to actively signal errors that can be detected at L2 (using NAC control frames). The most likely cause of an error at L2 is that a data frame was corrupted at the electrical level (noise, EMI). This results in an incorrect data or control frame checksum at the receiver side and will lead to its automatic retransmission. Note that data frames are acknowledged (AFC) or negatively acknowledged (NAC). Corrupt control frames are detected by timers that monitor expected or required responses.\n\nA bandwidth of 1 Gbit/s and a bit-error rate of 10 at a speed of 1 gigabit/s would imply an error every 1000 seconds or once every 1000th transmitted Gbit. Layer 2 thus automatically corrects these errors at the cost of marginal loss of bandwidth and at the cost of buffer space needed in L2 to store copies of transmitted data frames for possible retransmission or \"replay\".\n\nAnother feature of L2 is the ability for an L2 transmitter to know whether there is buffer space for the data frame at the receiving end. This again relies on L2 control frames (AFC) which allow a receiver to tell the peer's transmitter how much buffer space is available. This allows the receiver to pause the transmitter if needed, thus avoiding receive buffer overflow. Control frames are unaffected by L2 flow control: they can be sent at any time and the L2 receiver is expected to process these at the speed at which they arrive.\n\nUniPro currently supports two priority levels for data frames called Traffic Class 0 (TC0) and Traffic Class 1 (TC1). TC1 has higher priority than TC0. This means that if an L2 transmitter has a mix of TC0 and TC1 data frames to send, the TC1 data frames will be sent first. Assuming that most data traffic uses TC0 and that the network has congestion, this helps ensure that TC1 data frames arrive at their destination faster than TC0 data frames (analogous to emergency vehicles and normal road traffic). Furthermore, L2 can even interrupt or \"preempt\" an outgoing TC0 data frame to transmit a TC1 data frame. Additional arbitration rules apply to control frames: in essence these receive higher priority than data frames because they are small and essential for keeping traffic flowing.\n\nIn a multi-hop network, the arbitration is done within every L2 transmitter at every hop. The Traffic Class assigned to data does not normally change as data progresses through the network. It is up to the applications to decide how to use the priority system.\n\nIn UniPro version 1.1, an option was introduced to allow simple endpoint devices to implement only one of the two Traffic Classes if they choose to. This can be useful when device designers are more concerned with implementation cost than with control over frame arbitration. The connected L2 peer device detects such devices during the link initialization phase and can avoid using the missing Traffic Class.\n\nThe various L2 mechanisms provide a number of guarantees to higher layer protocols:\nThus individual links autonomously provide reliable data transfer. This is different from, for example, the widely used TCP protocol that detects errors at the endpoints and relies on end-to-end retransmission in case of corrupted or missing data.\n\nThe network layer is intended to route packets through the network toward their destination. Switches within a multi-hop network use this address to decide in which direction to route individual packets. To enable this, a header containing a 7-bit destination address is added by L3 to all L2 data frames. In the example shown in the figure, this allows Device #3 to not only communicate with Device #1, #2 and #5, but also enables it to communicate with Devices #4 and #6.\n\nVersion 1.4 of the UniPro spec does not specify the details of a switch, but does specify enough to allow a device to work in a future networked environment.\n\nAlthough the role of the L3 address is the same as the IP address in packets on the Internet, a UniPro DeviceID address is only 7 bits long. A network can thus have up to 128 different UniPro devices. Note that, as far as UniPro is concerned, all UniPro devices are created equal: unlike PCI Express or USB, any device can take the initiative to communicate with any other device. This makes UniPro a true network rather than a bus with one master.\n\nThe diagram shows an example of an L3 packet which starts at the first L2 payload byte of an L2 frame and ends at the last L2 payload byte of an L2 frame. For simplicity and efficiency, only a single L3 packet can be carried by one L2 frame. This implies that, in UniPro, the concepts of an L2 Frame, an L3 Packet and an L4 Segment (see below) are so closely aligned that they are almost synonyms. The distinction (and \"coloring\") is however still made to ensure that the specification can be described in a strictly layered fashion.\n\nUniPro short-header packets use a single header byte for L3 information. It includes the 7-bit L3 destination address. The remaining bit indicates the short-header packet format. For short-header packets, the L3 source address is not included in the header because it is assumed that the two communicating devices have exchanged such information beforehand (connection-oriented communication).\n\nLong-header packets are intended to be introduced in a future version of the UniPro specification, so their format is undefined (except for one bit) in the current UniPro v1.4 specification. However, UniPro v1.4 defines a hook that allows long-header packets to be received or transmitted by a UniPro v1.4 conformant-device assuming the latter can be upgraded via software. The \"long-header trap\" mechanism of UniPro v1.4 simply passes the payload of a received L2 data frame (being the L3 packet with its header and payload) to the L3 extension (e.g. software) for processing. The mechanism can also accept L2 frame payload from the L3 extension for transmission. This mechanism aims to allow UniPro v1.4 devices to be able to be upgraded in order to support protocols that require the as-yet undefined long-header packets.\n\nAlthough details of switches are still out of scope in the UniPro v1.4 spec, L3 allows UniPro v1.0/v1.1/v1.4 devices to serve as endpoints on a network. It therefore guarantees a number of properties to higher layer protocols:\n\nThe features of UniPro's Transport layer are not especially complex, because basic communication services have already been taken care of by lower protocol layers. L4 is essentially about enabling multiple devices on the network or even multiple clients within these devices to share the network in a controlled manner. L4's features tend to be roughly comparable to features found in computer networking (e.g. TCP and UDP) but that are less commonly encountered in local busses like PCI Express, USB or on-chip busses.\n\nUniPro's L4 also has special significance because it is the top protocol layer in the UniPro specification. Applications are required to use L4's top interface to interact with UniPro and are not expected to bypass L4 to directly access lower layers. Note that the interface at the top of L4 provided for transmitting or receiving data is defined at the behavioral or functional level. This high level of abstraction avoids restricting implementation options. Thus, although the specification contains an annex with a signal-level interface as a non-normative example, a UniPro implementation is not required to have any specific set of hardware signals or software function calls at its topmost interface.\n\nUniPro's Transport layer can be seen as providing an extra level of addressing within a UniPro device. This\nThese points are explained in more detail below.\n\nAn L4 segment, is essentially the payload of an L3 packet. The L4 header, in its short form, consists of just a single byte.\nThe main field in the short L4 header is a 5-bit \"CPort\" identifier which can be seen as a sub-address within a UniPro device and is somewhat analogous to the port numbers used in TCP or UDP. Thus every segment (with a short header) is addressed to a specific CPort of specific UniPro device.\n\nA single bit in the segment header also allows segments to be defined with long segment headers. UniPro v1.4 does not define the structure of such segment formats (except for this single bit). Long header segments may be generated via the long header trap described in the L3 section.\n\nUniPro calls a pair of CPorts that communicate with each other a Connection (hence the C in CPort). Setting up a connection means that one CPort has been initialized to create segments which are addressed to a specific L4 CPort of a specific L3 DeviceID using a particular L2 Traffic Class. Because UniPro connections are bidirectional, the destination CPort is also configured to allow data to be sent back to the source CPort.\n\nIn UniPro 1.0/1.1 connection setup is implementation specific.\n\nIn UniPro v1.4 connection setup is assumed to be relatively static: the parameters of the paired CPorts are configured by setting the corresponding connection Attributes in the local and peer devices using the DME. This will be supplemented by a dynamic connection management protocol in a future version of UniPro.\n\nCPorts also contain state variables that can be used to track how much buffer space the peer or connected CPort has. This is used to prevent the situation whereby a CPort sends segments to a CPort which has insufficient buffer space to hold the data, thus leading to stalled data traffic. Unless resolved fast, this traffic jam at the destination quickly grows into a network-wide gridlock. This is highly undesirable as it can greatly affect network performance for all users or, worse, can lead to deadlock situations. The described L4 mechanism is known as end-to-end flow control (E2E FC) because it involves the endpoints of a connection.\n\nL4 flow control is complementary to L2 flow control. Both work by having the transmitter pause until it knows there is sufficient buffer space at the receiver. But L4 flow control works between a pair of CPorts (potentially multiple hops apart) and aims to isolate connections from one another (\"virtual wire\" analogy). In contrast, L2 flow control is per-hop and avoids basic loss of data due to lack of receiver buffer space.\n\nE2E FC is only possible for connection-oriented communication, but at present UniPro's L4 does not support alternative options. E2E FC is enabled by default but can, however, be disabled. This is not generally recommended.\n\nUniPro provides \"safety net\" mechanisms that mandate that a CPort absorbs all data sent to it without stalling. If a stall is detected anyway, the endpoint discards the incoming data arriving at that CPort in order to maintain data flow on the network. This can be seen as a form of graceful degradation at the system level: if one connection on the network cannot keep up with the speed of the received data, other devices and other connections are unaffected.\n\nUniPro L4 allows a connection between a pair of CPorts to convey a stream of so-called messages (each consisting of a series of bytes) rather than a single stream of bytes. Message boundaries are triggered by the application-level protocol using UniPro and are signaled via a bit in the segment header. This End-of-Message bit indicates that the last byte in the L4 segment is the last byte of the application-level message.\n\nUniPro needs to be told by the application where or when to insert message boundaries into the byte stream: the boundaries have no special meaning for UniPro itself and are provided as a service to build higher-layer protocols on top of UniPro. Messages can be used to indicate (e.g. via an interrupt) to the application that a unit of data is complete and can thus be processed. Messages can also be useful as a robust and efficient mechanism to implement resynchronization points in some applications.\n\nUniPro v1.4 introduces the notion of message fragment, a fragment being a portion of a message passed between the application and the CPort. This option can be useful when specifying Applications on top of UniPro that need to interrupt the Message creation based on information from the UniPro stack, e.g., incoming Messages, or backpressure.\n\nThe mechanisms in L4 provide a number of guarantees to upper layer protocols:\n\nThe DME (Device Management Entity) controls the layers in the UniPro stack. It provides access to control and status parameters in all layers, manages the power mode transitions of the Link and handles the boot-up, hibernate and reset of the stack. Furthermore, it provides means to control the peer UniPro stack on the Link.\n"}
{"id": "11173100", "url": "https://en.wikipedia.org/wiki?curid=11173100", "title": "Vaseline", "text": "Vaseline\n\nVaseline is an American brand of petroleum jelly-based products owned by British-Dutch company Unilever. Products include plain petroleum jelly and a selection of skin creams, soaps, lotions, cleansers, and deodorants.\n\nIn many languages, the word \"Vaseline\" is used as generic for petroleum jelly; in Portuguese and some Spanish-speaking countries, the Unilever products are called Vasenol.\n\nThe first known reference to the name Vaseline was by the inventor of petroleum jelly, Robert Chesebrough in his U.S. patent for the process of making petroleum jelly (U.S. Patent 127,568) in 1872. \"I, Robert Chesebrough, have invented a new and useful product from petroleum which I have named Vaseline...\"\n\nThe name \"vaseline\" is said by the manufacturer to be derived from German \"Wasser\" \"water\" + Greek έλαιον (\"elaion\") \"olive oil\".\n\nIn 1859, Chesebrough went to the oil fields in Titusville, Pennsylvania, and learned of a residue called \"rod wax\" that had to be periodically removed from oil rig pumps. The oil workers had been using the substance to heal cuts and burns. Chesebrough took samples of the rod wax back to Brooklyn, extracted the usable petroleum jelly, and began manufacturing the medicinal product he called Vaseline.\nVaseline was made by the Chesebrough Manufacturing Company until the company was purchased by Unilever in 1987.\n\nWhile Vaseline can be used as a lubricant, it can also be used as a moisture insulator for local skin conditions characterized by tissue dehydration.\n\nVaseline has been reported to be highly-refined, triple-purified and regarded as non-carcinogenic.\n\n"}
{"id": "32432422", "url": "https://en.wikipedia.org/wiki?curid=32432422", "title": "WISTA", "text": "WISTA\n\nThe WISTA Science and Technology Park in Berlin-Adlershof was founded in 1991 after the dissolution of the Academy of Sciences of the German Democratic Republic at the same place. Today it covers an area of making it the largest science park in Germany.\n\nThe existence of technology centers at Adlershof dates back well into the late 19th and early 20th centuries - the Berlin–Görlitz railway had reached the Berlin suburb of Adlershof in 1867 and the population increased from 743 in 1875 to 5591 in 1895 following the industrialization in the area. The decision to build the Teltow Canal (1900 - 1906) furthered the development of businesses in the area between the old town (in the north) and the canal (in the south). In 1909 the Johannisthal Air Field opened in the west of the area and a number of institutions settled in Adlershof, including the German Research Institute for Aviation (today German Aerospace Center). During the wars the area was used as a technology development center for aviation and radio telecommunications - for some time the large 36 meter radio telescope was an icon of the area.\n\nAfter the total destruction of Berlin 1945 the new Eastern Government decided to move the German Academy of Sciences to Adlershof. In 1950 to 1952 the media centers of the Deutscher Fernsehfunk television broadcaster were built in Adlershof. The aviation field was closed down (as air transport had been moved to Berlin-Schönefeld Airport a few miles away) and the guards regiment Feliks Dzierzynski (12,000 soldiers) occupied the military buildings.\n\nAfter the German reunification almost all institutions at Adlershof were closed down during 1990/1991. The infrastructure was old and not up to date with the technology level of West Germany however the available area had the best premises to build a modern one. Berlin did found the Redevelopment Company Adlershof Ltd (Entwicklungsgesellschaft Adlershof mbH - EGA) in 1991 and it was renamed to WISTA Management Ltd (WISTA-Management GmbH) in 1994 as the plans for a new science and technology park took shape. Since then, it helps developing the cluster partly like a business incubator, with network management, communication and marketing, acceleration of special fields of technology and acquisition of projects, investments and companies.\n\nThe name WISTA is not officially explained as an abbreviation although at the time the full name was \"WISTA - Wissenschafts- und Wirtschaftsstandort Adlershof\" (WISTA - Science and Business Location Adlershof) so that WI-ST-A points to a previous project name \"WIssenschaftsSTandort Adlershof\" (Science Location Adlershof) while much of the press refers to the area as \"Wissenschaftsstadt Adlershof\" (Science City Adlershof) leading to the same abbreviation. In fact WISTA was included in the Expo 2000 fairs under the name of \"Berlin Adlershof – Stadt für Wissenschaft, Wirtschaft und Medien\" (Berlin Adlershof - City of Science, Business and Media) corroborating the correspondence.\n\nAt the end of 2007 the redevelopment legislation for Adlershof were lifted. In 2008 the area advanced to the north-west building the Hermann Dörner Avenue (Hermann-Dörner-Allee). In 2011 the tram line was extended to run right through the science and technology park. Today the BESSY synchrotron is something of an icon for the area. There is still room for extension of the science park using the industrial area along the earlier airfield in the west and the earlier shunting station in the north. Developments plans for a housing project west of the earlier airfield (today a grassland recreational area) have been scrapped with new homes being single and duplex houses.\n\nThe complete area of the WISTA development area encompasses . The Science City as the core area covered . The Media City stretches for as of 2001.\n\nIn 2009 there were 819 companies and other institutions at Adlershof with whole turnover of 1.789 billion Euro employing 13,981 people.\n\n\n"}
{"id": "9363980", "url": "https://en.wikipedia.org/wiki?curid=9363980", "title": "Western Union splice", "text": "Western Union splice\n\nThe Western Union or Lineman splice was developed during the introduction of the telegraph to mechanically and electrically connect wires that were subject to loading stress. The wrapping pattern is designed to cause the termination to tighten as the conductors pull against each other. This type of splice is more suited to solid, rather than stranded conductors.\n\nThe Western Union Splice is made by twisting two ends of a wire together, traditionally counterclockwise, 3/4 of a turn each, finger tight. Then, usually using needle-nose pliers, the ends are twisted at least five more turns, tightly. The cut off ends are pushed close to the center wire.\n\n\"Short tie\" and \"long tie\" variations exist, mainly for purposes of coating the connection with solder. The longer version may aid in solder flow.\n\nNASA tests on 22 and 16 AWG wire showed that the Western Union Splice when soldered is very strong and is stronger than the wire alone if done properly.\n\n\n"}
{"id": "29895247", "url": "https://en.wikipedia.org/wiki?curid=29895247", "title": "Wolffia arrhiza", "text": "Wolffia arrhiza\n\nWolffia arrhiza is a species of flowering plant known by the common names spotless watermeal and rootless duckweed, belonging to the Araceae, a family rich in water-loving species, such as \"Arum\" and \"Pistia\". It is the smallest vascular plant on Earth. It is native to Europe, Africa, and parts of Asia, and it is present in other parts of the world as a naturalized species. It is an aquatic plant which grows in quiet water bodies such as ponds. The green part of the plant, the frond, is a sphere measuring about 1 mm wide, but with a flat top that floats at the water's surface. It has a few parallel rows of stomata. There is no root. The plant produces a minute flower fully equipped with one stamen and one pistil. It often multiplies by vegetative reproduction, however, with the rounded part budding off into a new individual. In cooler conditions the plant becomes dormant and sinks to the bed of the water body to overwinter as a turion. The plant is a mixotroph which can produce its own energy by photosynthesis or absorb it from the environment in the form of dissolved carbon.\n\nThis tiny plant is a nutritious food. Its green part is about 40% protein by dry weight and its turion is about 40% starch. It contains many amino acids important to the human diet, relatively large amounts of dietary minerals and trace elements such as calcium, magnesium, and zinc, and vitamin B. It has long been used as a cheap food source in Burma, Laos, and Thailand, where it is known as \"khai-nam\" (\"eggs of the water\"). The plant is prolific in its reproduction, growing in floating mats that can be harvested every 3 to 4 days; it has been shown to double its population in less than four days \"in vitro\".\n\nIt is also useful as a form of agricultural and municipal water treatment. It is placed in effluent from black tiger shrimp farms to absorb and metabolize pollutants. The plants grow quickly and take up large amounts of nitrogen and phosphorus from the water. The plants that grow in the wastewater can then be used as feed for animals, such as carp, Nile tilapia, and chickens.\n\n"}
