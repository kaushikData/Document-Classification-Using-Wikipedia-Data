{"id": "207601", "url": "https://en.wikipedia.org/wiki?curid=207601", "title": "Adsorption", "text": "Adsorption\n\nAdsorption is the adhesion of atoms, ions or molecules from a gas, liquid or dissolved solid to a surface. This process creates a film of the \"adsorbate\" on the surface of the \"adsorbent\". This process differs from absorption, in which a fluid (the \"absorbate\") is dissolved by or permeates a liquid or solid (the \"absorbent\"), respectively. Adsorption is a \"surface phenomenon\", while absorption involves the whole volume of the material. The term \"sorption\" encompasses both processes, while \"desorption\" is the reverse of it.\n\nSimilar to surface tension, adsorption is a consequence of surface energy. In a bulk material, all the bonding requirements (be they ionic, covalent or metallic) of the constituent atoms of the material are filled by other atoms in the material. However, atoms on the surface of the adsorbent are not wholly surrounded by other adsorbent atoms and therefore can attract adsorbates. The exact nature of the bonding depends on the details of the species involved, but the adsorption process is generally classified as physisorption (characteristic of weak van der Waals forces) or chemisorption (characteristic of covalent bonding). It may also occur due to electrostatic attraction.\n\nAdsorption is present in many natural, physical, biological and chemical systems and is widely used in industrial applications such as heterogeneous catalysts, activated charcoal, capturing and using waste heat to provide cold water for air conditioning and other process requirements (adsorption chillers), synthetic resins, increasing storage capacity of carbide-derived carbons and water purification. Adsorption, ion exchange and chromatography are sorption processes in which certain adsorbates are selectively transferred from the fluid phase to the surface of insoluble, rigid particles suspended in a vessel or packed in a column. Pharmaceutical industry applications, which use adsorption as a means to prolong neurological exposure to specific drugs or parts thereof, are lesser known.\n\nThe word \"adsorption\" was coined in 1881 by German physicist Heinrich Kayser (1853 –1940).\n\nAdsorption is usually described through isotherms, that is, the amount of adsorbate on the adsorbent as a function of its pressure (if gas) or concentration (if liquid) at constant temperature. The quantity adsorbed is nearly always normalized by the mass of the adsorbent to allow comparison of different materials. To date, 15 different isotherm models were developed.\n\nThe first mathematical fit to an isotherm was published by Freundlich and Kuster (1906) and is a purely empirical formula for gaseous adsorbates:\n\nwhere formula_2 is the mass of adsorbate adsorbed, formula_3 is the mass of the adsorbent, formula_4 is the pressure of adsorbate (this can be changed to concentration if investigating solution rather than gas), and formula_5 and formula_6 are empirical constants for each adsorbent–adsorbate pair at a given temperature. The function is not adequate at very high pressure because in reality formula_7 has an asymptotic maximum as pressure increases without bound. As the temperature increases, the constants formula_5 and formula_6 change to reflect the empirical observation that the quantity adsorbed rises more slowly and higher pressures are required to saturate the surface.\n\nIrving Langmuir was the first to derive a scientifically based adsorption isotherm in 1918. The model applies to gases adsorbed on solid surfaces. It is a semi-empirical isotherm with a kinetic basis and was derived based on statistical thermodynamics. It is the most common isotherm equation to use due to its simplicity and its ability to fit a variety of adsorption data. It is based on four assumptions:\n\nThese four assumptions are seldom all true: there are always imperfections on the surface, adsorbed molecules are not necessarily inert, and the mechanism is clearly not the same for the very first molecules to adsorb to a surface as for the last. The fourth condition is the most troublesome, as frequently more molecules will adsorb to the monolayer; this problem is addressed by the BET isotherm for relatively flat (non-microporous) surfaces. The Langmuir isotherm is nonetheless the first choice for most models of adsorption and has many applications in surface kinetics (usually called Langmuir–Hinshelwood kinetics) and thermodynamics.\n\nLangmuir suggested that adsorption takes place through this mechanism: formula_10, where \"A\" is a gas molecule, and \"S\" is an adsorption site. The direct and inverse rate constants are \"k\" and \"k\". If we define surface coverage, formula_11, as the fraction of the adsorption sites occupied, in the equilibrium we have:\n\nor\n\nwhere formula_4 is the partial pressure of the gas or the molar concentration of the solution.\nFor very low pressures formula_15, and for high pressures formula_16.\n\nThe value of formula_11 is difficult to measure experimentally; usually, the adsorbate is a gas and the quantity adsorbed is given in moles, grams, or gas volumes at standard temperature and pressure (STP) per gram of adsorbent. If we call \"v\" the STP volume of adsorbate required to form a monolayer on the adsorbent (per gram of adsorbent), then formula_18, and we obtain an expression for a straight line:\n\nThrough its slope and \"y\" intercept we can obtain \"v\" and \"K\", which are constants for each adsorbent–adsorbate pair at a given temperature. \"v\" is related to the number of adsorption sites through the ideal gas law. If we assume that the number of sites is just the whole area of the solid divided into the cross section of the adsorbate molecules, we can easily calculate the surface area of the adsorbent.\nThe surface area of an adsorbent depends on its structure: the more pores it has, the greater the area, which has a big influence on reactions on surfaces.\n\nIf more than one gas adsorbs on the surface, we define formula_20 as the fraction of empty sites, and we have:\n\nAlso, we can define formula_22 as the fraction of the sites occupied by the \"j\"-th gas:\n\nwhere \"i\" is each one of the gases that adsorb.\n\nOften molecules do form multilayers, that is, some are adsorbed on already adsorbed molecules, and the Langmuir isotherm is not valid. In 1938 Stephen Brunauer, Paul Emmett, and Edward Teller developed a model isotherm that takes that possibility into account. Their theory is called BET theory, after the initials in their last names. They modified Langmuir's mechanism as follows:\n\nThe derivation of the formula is more complicated than Langmuir's (see links for complete derivation). We obtain:\n\nwhere \"x\" is the pressure divided by the vapor pressure for the adsorbate at that temperature (usually denoted formula_25), \"v\" is the STP volume of adsorbed adsorbate, \"v\" is the STP volume of the amount of adsorbate required to form a monolayer, and \"c\" is the equilibrium constant \"K\" we used in Langmuir isotherm multiplied by the vapor pressure of the adsorbate. The key assumption used in deriving the BET equation that the successive heats of adsorption for all layers except the first are equal to the heat of condensation of the adsorbate.\n\nThe Langmuir isotherm is usually better for chemisorption, and the BET isotherm works better for physisorption for non-microporous surfaces.\n\nIn other instances, molecular interactions between gas molecules previously adsorbed on a solid surface form significant interactions with gas molecules in the gaseous phases. Hence, adsorption of gas molecules to the surface is more likely to occur around gas molecules that are already present on the solid surface, rendering the Langmuir adsorption isotherm ineffective for the purposes of modelling. This effect was studied in a system where nitrogen was the adsorbate and tungsten was the adsorbent by Paul Kisliuk (1922–2008) in 1957. To compensate for the increased probability of adsorption occurring around molecules present on the substrate surface, Kisliuk developed the precursor state theory, whereby molecules would enter a precursor state at the interface between the solid adsorbent and adsorbate in the gaseous phase. From here, adsorbate molecules would either adsorb to the adsorbent or desorb into the gaseous phase. The probability of adsorption occurring from the precursor state is dependent on the adsorbate’s proximity to other adsorbate molecules that have already been adsorbed. If the adsorbate molecule in the precursor state is in close proximity to an adsorbate molecule that has already formed on the surface, it has a sticking probability reflected by the size of the S constant and will either be adsorbed from the precursor state at a rate of \"k\" or will desorb into the gaseous phase at a rate of \"k\". If an adsorbate molecule enters the precursor state at a location that is remote from any other previously adsorbed adsorbate molecules, the sticking probability is reflected by the size of the S constant.\n\nThese factors were included as part of a single constant termed a \"sticking coefficient\", \"k\", described below:\n\nAs S is dictated by factors that are taken into account by the Langmuir model, S can be assumed to be the adsorption rate constant. However, the rate constant for the Kisliuk model (\"R\"’) is different from that of the Langmuir model, as \"R\"’ is used to represent the impact of diffusion on monolayer formation and is proportional to the square root of the system’s diffusion coefficient. The Kisliuk adsorption isotherm is written as follows, where Θ is fractional coverage of the adsorbent with adsorbate, and \"t\" is immersion time:\n\nSolving for Θ yields:\n\nAdsorption constants are equilibrium constants, therefore they obey the van 't Hoff equation:\n\nAs can be seen in the formula, the variation of \"K\" must be isosteric, that is, at constant coverage.\nIf we start from the BET isotherm and assume that the entropy change is the same for liquefaction and adsorption, we obtain\nthat is to say, adsorption is more exothermic than liquefaction.\n\nAdsorbents are used usually in the form of spherical pellets, rods, moldings, or monoliths with a hydrodynamic radius between 0.25 and 5 mm. They must have high abrasion resistance, high thermal stability and small pore diameters, which results in higher exposed surface area and hence high capacity for adsorption. The adsorbents must also have a distinct pore structure that enables fast transport of the gaseous vapors.\n\nMost industrial adsorbents fall into one of three classes:\n\nSilica gel is a chemically inert, nontoxic, polar and dimensionally stable (< ) amorphous form of SiO. It is prepared by the reaction between sodium silicate and acetic acid, which is followed by a series of after-treatment processes such as aging, pickling, etc. These after-treatment methods results in various pore size distributions.\n\nSilica is used for drying of process air (e.g. oxygen, natural gas) and adsorption of heavy (polar) hydrocarbons from natural gas.\n\nZeolites are natural or synthetic crystalline aluminosilicates, which have a repeating pore network and release water at high temperature. Zeolites are polar in nature.\n\nThey are manufactured by hydrothermal synthesis of sodium aluminosilicate or another silica source in an autoclave followed by ion exchange with certain cations (Na, Li, Ca, K, NH). The channel diameter of zeolite cages usually ranges from 2 to 9 Å. The ion exchange process is followed by drying of the crystals, which can be pelletized with a binder to form macroporous pellets.\n\nZeolites are applied in drying of process air, CO removal from natural gas, CO removal from reforming gas, air separation, catalytic cracking, and catalytic synthesis and reforming.\n\nNon-polar (siliceous) zeolites are synthesized from aluminum-free silica sources or by dealumination of aluminum-containing zeolites. The dealumination process is done by treating the zeolite with steam at elevated temperatures, typically greater than . This high temperature heat treatment breaks the aluminum-oxygen bonds and the aluminum atom is expelled from the zeolite framework.\n\nActivated carbon is a highly porous, amorphous solid consisting of microcrystallites with a graphite lattice, usually prepared in small pellets or a powder. It is non-polar and cheap. One of its main drawbacks is that it reacts with oxygen at moderate temperatures (over 300 °C).\n\nActivated carbon can be manufactured from carbonaceous material, including coal (bituminous, subbituminous, and lignite), peat, wood, or nutshells (e.g., coconut). The manufacturing process consists of two phases, carbonization and activation. The carbonization process includes drying and then heating to separate by-products, including tars and other hydrocarbons from the raw material, as well as to drive off any gases generated. The process is completed by heating the material over in an oxygen-free atmosphere that cannot support combustion. The carbonized particles are then \"activated\" by exposing them to an oxidizing agent, usually steam or carbon dioxide at high temperature. This agent burns off the pore blocking structures created during the carbonization phase and so, they develop a porous, three-dimensional graphite lattice structure. The size of the pores developed during activation is a function of the time that they spend in this stage. Longer exposure times result in larger pore sizes. The most popular aqueous phase carbons are bituminous based because of their hardness, abrasion resistance, pore size distribution, and low cost, but their effectiveness needs to be tested in each application to determine the optimal product.\n\nActivated carbon is used for adsorption of organic substances and non-polar adsorbates and it is also usually used for waste gas (and waste water) treatment. It is the most widely used adsorbent since most of its chemical (e.g. surface groups) and physical properties (e.g. pore size distribution and surface area) can be tuned according to what is needed. Its usefulness also derives from its large micropore (and sometimes mesopore) volume and the resulting high surface area.\n\nTypical adsorbents used in carbon capture and storage are zeolites and MOFs. The customization of adsorbents makes them an attractive alternative to absorption. Because adsorbents can be regenerated by temperature or pressure swing, this step can be less energy intensive than absorption regeneration methods. Major problems that are present with adsorption cost in carbon capture are: regenerating the adsorbent, mass ratio, solvent/MOF, cost of adsorbent, production of the adsorbent, lifetime of adsorbent.\n\nProtein adsorption is a process that has a fundamental role in the field of biomaterials. Indeed, biomaterial surfaces in contact with biological media, such as blood or serum, are immediately coated by proteins. Therefore, living cells do not interact directly with the biomaterial surface, but with the adsorbed proteins layer. This protein layer mediates the interaction between biomaterials and cells, translating biomaterial physical and chemical properties into a \"biological language\". In fact, cell membrane receptors bind to protein layer bioactive sites and these receptor-protein binding events are transduced, through the cell membrane, in a manner that stimulates specific intracellular processes that then determine cell adhesion, shape, growth and differentiation. Protein adsorption is influenced by many surface properties such as surface wettability, surface chemical composition and surface nanometre-scale morphology.\nSurfactant adsorption is a similar phenomenon, but utilising surfactant molecules in the place of proteins.\n\nCombining an adsorbent with a refrigerant, adsorption chillers use heat to provide a cooling effect. This heat, in the form of hot water, may come from any number of industrial sources including waste heat from industrial processes, prime heat from solar thermal installations or from the exhaust or water jacket heat of a piston engine or turbine.\n\nAlthough there are similarities between adsorption chillers and absorption refrigeration, the former is based on the interaction between gases and solids. The adsorption chamber of the chiller is filled with a solid material (for example zeolite, silica gel, alumina, active carbon or certain types of metal salts), which in its neutral state has adsorbed the refrigerant. When heated, the solid desorbs (releases) refrigerant vapour, which subsequently is cooled and liquefied. This liquid refrigerant then provides a cooling effect at the evaporator from its enthalpy of vaporization. In the final stage the refrigerant vapour is (re)adsorbed into the solid. As an adsorption chiller requires no compressor, it is relatively quiet.\n\nPortal site mediated adsorption is a model for site-selective activated gas adsorption in metallic catalytic systems that contain a variety of different adsorption sites. In such systems, low-coordination \"edge and corner\" defect-like sites can exhibit significantly lower adsorption enthalpies than high-coordination (basal plane) sites. As a result, these sites can serve as \"portals\" for very rapid adsorption to the rest of the surface. The phenomenon relies on the common \"spillover\" effect (described below), where certain adsorbed species exhibit high mobility on some surfaces. The model explains seemingly inconsistent observations of gas adsorption thermodynamics and kinetics in catalytic systems where surfaces can exist in a range of coordination structures, and it has been successfully applied to bimetallic catalytic systems where synergistic activity is observed.\n\nIn contrast to pure spillover, portal site adsorption refers to surface diffusion to adjacent adsorption sites, not to non-adsorptive support surfaces.\n\nThe model appears to have been first proposed for carbon monoxide on silica-supported platinum by Brandt \"et al.\" (1993). A similar, but independent model was developed by King and co-workers to describe hydrogen adsorption on silica-supported alkali promoted ruthenium, silver-ruthenium and copper-ruthenium bimetallic catalysts. The same group applied the model to CO hydrogenation (Fischer–Tropsch synthesis). Zupanc \"et al.\" (2002) subsequently confirmed the same model for hydrogen adsorption on magnesia-supported caesium-ruthenium bimetallic catalysts. Trens \"et al.\" (2009) have similarly described CO surface diffusion on carbon-supported Pt particles of varying morphology.\n\nIn the case catalytic or adsorbent systems where a metal species is dispersed upon a support (or carrier) material (often quasi-inert oxides, such as alumina or silica), it is possible for an adsorptive species to indirectly adsorb to the support surface under conditions where such adsorption is thermodynamically unfavorable. The presence of the metal serves as a lower-energy pathway for gaseous species to first adsorb to the metal and then diffuse on the support surface. This is possible because the adsorbed species attains a lower energy state once it has adsorbed to the metal, thus lowering the activation barrier between the gas phase species and the support-adsorbed species.\n\nHydrogen spillover is the most common example of an adsorptive spillover. In the case of hydrogen, adsorption is most often accompanied with dissociation of molecular hydrogen (H) to atomic hydrogen (H), followed by spillover of the hydrogen atoms present.\n\nThe spillover effect has been used to explain many observations in heterogeneous catalysis and adsorption.\n\nAdsorption of molecules onto polymer surfaces is central to a number of applications, including development of non-stick coatings and in various biomedical devices. Polymers may also be adsorbed to surfaces through polyelectrolyte adsorption.\n\nAdsorption is the first step in the viral life cycle. The next steps are penetration, uncoating, synthesis (transcription if needed, and translation), and release. The virus replication cycle, in this respect, is similar for all types of viruses. Factors such as transcription may or may not be needed if the virus is able to integrate its genomic information in the cell's nucleus, or if the virus can replicate itself directly within the cell's cytoplasm.\n\nThe game of Tetris is a puzzle game in which blocks of 4 are adsorbed onto a surface during game play. Scientists have used Tetris blocks \"as a proxy for molecules with a complex shape\" and their \"adsorption on a flat surface\" for studying the thermodynamics of nanoparticles.\n\n\n"}
{"id": "40991492", "url": "https://en.wikipedia.org/wiki?curid=40991492", "title": "Ammonia fuming", "text": "Ammonia fuming\n\nAmmonia fuming is a wood finishing process that darkens wood and brings out the grain pattern. It consists of exposing the wood to fumes from a strong aqueous solution of ammonium hydroxide which reacts with the tannins in the wood. The process works best on white oak because of the high tannin content of this wood. Fumed oak is also called smoked oak. Other species may also be fumed, but usually will not darken as much as white oak. The introduction of the process is usually associated with the American furniture maker Gustav Stickley at the beginning of the 20th century, but fuming was certainly known in Europe before this.\n\nThe wood to be fumed is placed in a sealed chamber with all the surfaces to be fumed exposed to freely circulating air. A large shallow container of ammonium hydroxide solution is placed on the floor of the chamber and the chamber is sealed. If the chamber is large or the fuming is to be done for a long time, more than one container may be provided, or the ammonia may be replenished during the process. The fuming time depends on the amount of darkening required, the size of the chamber, and the strength of the ammonia used. It is usual to oil the wood after fuming to fully bring out the effect.\n\nFuming has an advantage over staining because it does not obscure the grain, it merely darkens it. Unlike staining, there is no possibility of blotches or runs. Fuming is also colourfast. Fuming has the disadvantage that it is not a precise process. Different batches of wood will react to fuming differently. For this reason, wood to be fumed for a project is often harvested from the same tree. Boards from the same tree, and different regions of the same board, can have a noticeably different colour. Where a consistent colour is important, staining or dyeing may be better options.\n\nFuming has some safety issues. The solution of ammonium hydroxide used is much stronger (26% to 30%) than in household ammonia and is corrosive. The fuming must be done in an enclosed sealed chamber. Ammonia splashes can burn skin and the fumes can cause burns to eyes and lungs. Operators need to wear gas masks, gloves and eye protection.\n\nThe darkening of the colour relies on the ammonia reacting with tannins in the wood. The process is usually applied to white oak, as this wood has a high tannin content. Red oak may turn greenish rather than deep brown. Other species may not darken as noticeably as white oak, depending on the tannin content. The effect of fuming can be enhanced in non-tannic woods by applying a coat of tannic acid to the surface before fuming.\n\nFuming was an accidental discovery in England after it was noticed that oak boards stored in a stable had darkened. This was caused by the ammonia fumes from the horse urine reacting with the wood. At the end of the 19th and beginning of the 20th centuries fuming became popular with furniture makers in the Arts and Crafts movement. The technique was introduced to the US by Gustav Stickley in 1901 and a manufacturing technique was perfected in the mission style furniture line of the Stickley family business. Stickley also described a method of fuming the wooden architecture of an entire room by placing bowls of ammonia in the room and sealing it. This method was not very practical and quite dangerous for the person placing the ammonia without personal protective equipment available to modern workers. Stickley was quickly followed by other American Arts and Crafts furniture makers such as Charles Limbert and the Roycroft community.\n\n"}
{"id": "50453100", "url": "https://en.wikipedia.org/wiki?curid=50453100", "title": "Benguela-Belize Lobito-Tomboco Platform", "text": "Benguela-Belize Lobito-Tomboco Platform\n\nBenguela-Belize Lobito-Tomboco is a offshore compliant tower oil platform off the coast of Angola, in the lower Congo basin. It is owned and run by the Chevron Corporation.\n\n"}
{"id": "5520834", "url": "https://en.wikipedia.org/wiki?curid=5520834", "title": "Book of Ingenious Devices", "text": "Book of Ingenious Devices\n\nThe Book of Ingenious Devices (Arabic: كتاب الحيل \"Kitab al-Hiyal\", Persian: كتاب ترفندها \"Ketab tarfandha\", literally: \"The Book of Tricks\") was a large illustrated work on mechanical devices, including automata, published in 850 by the three Iraqi brothers of Persian descent, known as the Banu Musa (Ahmad, Muhammad and Hasan bin Musa ibn Shakir) working at the House of Wisdom (\"Bayt al-Hikma\") in Baghdad, Iraq, under the Abbasid Caliphate. The book described about one hundred devices and how to use them.\n\nThe book was commissioned by the Abbasid Caliph of Baghdad, also made by Al-Jazari, Abu Jafar al-Ma'mun ibn Harun (786–833), who instructed the Banu Musa to acquire all of the Hellenistic texts that had been preserved by monasteries and by scholars during the decline and fall of Roman civilization. The Banū Mūsā brothers invented a number of automata (automatic machines) and mechanical devices, and they described a hundred such devices in their \"Book of Ingenious Devices\".\n\nSome of the devices described in the \"Book of Ingenious Devices\" were inspired by the works of Hero of Alexandria and Philo of Byzantium, as well as ancient Persian, Chinese and Indian engineering. Many of the other devices described in the book, however, were original inventions by the Banu Musa brothers. While they took Greek works as a starting point, the Banu Musa went \"well beyond anything achieved by Hero or Philo.\" Their preoccupation with automatic controls distinguishes them from their Greek predecessors, including the Banu Musa's \"use of self-operating valves, timing devices, delay systems and other concepts of great ingenuity.\" Many of their innovations involved subtle combinations of pneumatics and aerostatics. The closest modern parallel to their work lies in control engineering and pneumatic instrumentation.\n\nIn turn, the Banu Musa's work was later cited as an influence on the work of Al-Jazari, who produced a similarly titled book in 1206. Given that the \"Book of Ingenious Devices\" was widely circulated across the Muslim world, some of its ideas may have also reached Europe through Islamic Spain, such as the use of automatic controls in later European machines or the use of conical valves in the work of Leonardo da Vinci.\n\nThe Banu Musa brothers described a number of early automatic controls. Two-step level controls for fluids, an early form of discontinuous variable structure controls, was developed by the Banu Musa brothers. They also described an early feedback controller. Donald Routledge Hill wrote the following on the automatic controls underlying the mechanical trick devices described in the book:\n\nThe Banu Musa also developed an early fail-safe system for use in their trick devices, as described by Hill:\n\nThe non-manual crank appears in several of the hydraulic devices described by the Banū Mūsā brothers in their \"Book of Ingenious Devices\". These automatically operated cranks appear in several devices, two of which contain an action which approximates to that of a crankshaft, anticipating Al-Jazari's invention by several centuries and its first appearance in Europe by over five centuries. However, the automatic crank described by the Banu Musa would not have allowed a full rotation, but only a small modification was required to convert it to a crankshaft.\n\nA mechanism developed by the Banu Musa, of particular importance for future developments, was the conical valve, which was used in a variety of different applications. This includes using conical valves as \"in-line\" components in flow systems, which was the first known use of conical valves as automatic controllers. Some of the other valves they described include:\n\n\nThe double-concentric siphon and the funnel with bent end for pouring in different liquids, neither of which appear in any earlier Greek works, were also original inventions by the Banu Musa brothers. Some of the other mechanisms they described include a float chamber and an early differential pressure sensor.\n\nThe book describes the construction of various automatic fountains, an aspect that was largely neglected in earlier Greek treatises on technology. In one of these fountains, the \"water issues from the fountainhead in the shape of a shield, or like a lily-of-the-valley,\" i.e. \"the shapes are discharged alternately—either a sheet of water concave downwards, or a spray.\" Another fountain \"discharges a shield or a single jet,\" while a variation of this features double-action alternation, i.e. has two fountainheads, with one discharging a single jet and the other a shield, and the two alternating repeatedly. Another variation features one main fountainhead and two or more subsidiary ones, such that when the main one ejects a single jet, the subsidiaries eject shields, with the two alternating.\n\nThe Banu Musa brothers also described the earliest known wind-powered fountain, which is described as, \"operated by wind or water, it discharges a single jet or a lily-of-the-valley.\" A variation of this fountain incorporates a worm-and-pinion gear, while another variation features double-action alternation. The book also describes a fountain with variable discharge. The book also describes fountains that change shapes at intervals.\n\nThe Banu Musa invented an early mechanical musical instrument, in this case a hydropowered organ which played interchangeable cylinders automatically. According to Charles B. Fowler, this \"cylinder with raised pins on the surface remained the basic device to produce and reproduce music mechanically until the second half of the nineteenth century.\"\n\nThe Banu Musa also invented an automatic flute player which may have been the first programmable machine. The flute sounds were produced through hot steam and the user could adjust the device to various patterns so that they could get various sounds from it.\n\nThe mechanical grab, specifically the clamshell grab, is an original invention by the Banu Musa brothers that does not appear in any earlier Greek works. The grab they described was used to extract objects from underwater, and recover objects from the beds of streams.\n\nThe Banu Musa also invented an early gas mask, for protecting workers in polluted wells. They also described bellows that could remove foul air from wells. They explained that these instruments allow a worker to \"descend into any well he wishes for a while and he will not fear it, nor will it harm him, if God wills may he be exalted.\"\n\nThe book describes a dispenser for hot and cold water, where the two outlets alternate, one discharging cold water and the other hot, then vice versa repeatedly. It also describes a vessel with a basin by its side where, when cold water is poured into the top of the vessel, it discharges from the mouth of a figure into the basin; when hot water or another liquid is poured into the basin, the same quantity of cold water is discharged from the mouth of the figure.\n\nThe book also describes a boiler with a tap to access hot water. The water is heated through cold water being poured into a pipe which leads to a tank at the bottom of the boiler, where the water is heated with fire. A person can then access hot water from the boiler through a tap.\n\nSome of the other devices the Banu Musa described in their book include:\n\n\n\n\n"}
{"id": "2293147", "url": "https://en.wikipedia.org/wiki?curid=2293147", "title": "Bottle scraper", "text": "Bottle scraper\n\nThe bottle scraper (known as both a flessenschraper (bottle scraper) and flessenlikker (bottle licker) in Dutch) is a Dutch kitchen tool similar to a small spatula. It is designed to scrape the contents of long bottles that would be impossible to reach with other kitchen tools. Although the tool is sold in Norway and has even been described in some accounts as having originated there, it is cited as a quintessentially Dutch tool as well as an example of Dutch thrift.\n\nThe scraper is made of a long shaft, frequently around in length. On one side is a small flexible rubber spatula head roughly across set perpendicular to the shaft. The head is flexible and usually has a rounded half-circle shape one side useful for scraping round bottles and jars and a flat side with two right angles useful for scraping out cartons. The head is flexible so that it can be pushed into and pulled out of bottles whose mouth is smaller than the fully expanded head of the scraper but larger than the shaft.\n\nIt is a common tool in kitchens in the Netherlands and available in many kitchen stores and supermarkets — although this may vary by region. It is almost entirely unused and unknown outside of that country.\n\nHistorically, vla (a popular dairy product with the consistency similar to custard or yoghurt) was sold in long glass bottles. Vla was frequently delivered by milkmen in bottles similar to the long, tapered-neck milk bottles used in many places in the world. However, while the use of the milk bottle streamlined production and distribution practices, the shape of the container made it hard to extract the last bits with spoons, spatulas, or other kitchen utensils. The Dutch answer was the bottle scraper which was perfectly suited to answer this problem.\n\n\"Vla\" is now primarily sold in cartons which reduces the need for a bottle scraper or make the design less efficient (e.g., bottles have rounded walls while cartons frequently have square bases with 90 degree angles). Modern bottle scrapers include a spatula-side with right angles useful for scraping \"vla\" out of these containers as well. Additionally, the scraper has come to occupy an important role in Dutch kitchens in variety of other purposes being used for scraping the contents of many other bottles and jars (e.g., peanut butter, Nutella, etc.). Much of the Dutch population is familiar with the bottle scraper and skilled in its use.\n\nThere is an element of the technical and engineering community that sees the bottle scraper as a monument to a wrong type of technical solution. This group argues that the best solution to the problem of remaining vla in bottles was to put vla in different containers preventing the problem from ever occurring, rather than inventing another tool and then educating a population in its use.\n\n"}
{"id": "3933", "url": "https://en.wikipedia.org/wiki?curid=3933", "title": "Braille", "text": "Braille\n\nBraille (; ) is a tactile writing system used by people who are visually impaired. It is traditionally written with embossed paper. Braille users can read computer screens and other electronic supports using refreshable braille displays. They can write braille with the original slate and stylus or type it on a braille writer, such as a portable braille notetaker or computer that prints with a braille embosser.\n\nBraille is named after its creator, Louis Braille, a Frenchman who lost his sight as a result of a childhood accident. In 1824, at the age of fifteen, he developed a code for the French alphabet as an improvement on night writing. He published his system, which subsequently included musical notation, in 1829. The second revision, published in 1837, was the first small binary form of writing developed in the modern era.\n\nThese characters have rectangular blocks called \"cells\" that have tiny bumps called \"raised dots\". The number and arrangement of these dots distinguish one character from another. Since the various braille alphabets originated as transcription codes for printed writing, the mappings (sets of character designations) vary from language to language, and even within one; in English Braille there are three levels of encoding: Grade 1 – a letter-by-letter transcription used for basic literacy; Grade 2 – an addition of abbreviations and contractions; and Grade 3 – various non-standardized personal stenography.\n\nBraille cells are not the only thing to appear in braille text. There may be embossed illustrations and graphs, with the lines either solid or made of series of dots, arrows, bullets that are larger than braille dots, etc. A full braille cell includes six raised dots arranged in two columns, each column having three dots. The dot positions are identified by numbers from one to six. There are 64 possible solutions using zero or more dots. A cell can be used to represent a letter, number, punctuation mark, or even a word.\n\nIn the face of screen reader software, braille usage has declined. However, because it teaches spelling and punctuation, braille education remains important for developing reading skills among blind and visually impaired children, and braille literacy correlates with higher employment rates.\n\nBraille was based on a tactile military code called night writing, developed by Charles Barbier in response to Napoleon's demand for a means for soldiers to communicate silently at night and without a light source. In Barbier's system, sets of 12 embossed dots encoded 36 different sounds. It proved to be too difficult for soldiers to recognize by touch and was rejected by the military. In 1821 Barbier visited the Royal Institute for the Blind in Paris, where he met Louis Braille. Braille identified two major defects of the code: first, by representing only sounds, the code was unable to render the orthography of the words; second, the human finger could not encompass the whole 12-dot symbol without moving, and so could not move rapidly from one symbol to another. Braille's solution was to use 6-dot cells and to assign a specific pattern to each letter of the alphabet.\nAt first, Braille was a one-to-one transliteration of French orthography, but soon various abbreviations, contractions, and even logograms were developed, creating a system much more like shorthand. The expanded English system, called Grade-2 Braille, was complete by 1905. For blind readers, Braille is an independent writing system, rather than a code of printed orthography.\n\nBraille is derived from the Latin alphabet, albeit indirectly. In Braille's original system, the dot patterns were assigned to letters according to their position within the alphabetic order of the French alphabet, with accented letters and \"w\" sorted at the end.\n\nThe first ten letters of the alphabet, \"a–j,\" use the upper four dot positions: (black dots in the table below). These stand for the ten digits \"1–9\" and \"0\" in a system parallel to Hebrew gematria and Greek isopsephy. (Though the dots are assigned in no obvious order, the cells with the fewest dots are assigned to the first three letters (and lowest digits), \"abc = 123\" (), and to the three vowels in this part of the alphabet, \"aei\" (), whereas the even digits, \"4, 6, 8, 0\" (), are corners/right angles.)\n\nThe next ten letters, \"k–t,\" are identical to \"a–j,\" respectively, apart from the addition of a dot at position 3 (red dots in the table): :\n\nThe next ten letters (the next \"decade\") are the same again, but with dots also at positions both 3 and 6 (green dots). Here \"w\" was left out as not being a part of the official French alphabet at the time of Braille's life; the French braille order is \"u v x y z ç é à è ù\" ().\n\nThe next ten, ending in \"w\", are the same again, except that for this series position 6 (purple dot) is used without position 3. These are \"â ê î ô û ë ï ü ö w\" ().\n\nThe \"a–j\" series lowered by one dot space () are used for punctuation. Letters \"a\" and \"c\" , which only use dots in the top row, were lowered two places for the apostrophe and hyphen: . (These are the decade diacritics, at left in the table below, of the second and third decade.)\n\nIn addition, there are ten patterns that are based on the first two letters () shifted to the right; these were assigned to non-French letters (\"ì ä ò\" ), or serve non-letter functions: (superscript; in English the accent mark), (currency prefix), (capital, in English the decimal point), (number sign), (emphasis mark), (symbol prefix).\n\nOriginally there had been nine decades. The fifth through ninth used dashes as well as dots, but proved to be impractical and were soon abandoned. These could be replaced with what we now know as the number sign (), though that only caught on for the digits (old 5th decade → modern 1st decade). The dash occupying the top row of the original sixth decade was simply dropped, producing the modern fifth decade. (See 1829 braille.)\n\nHistorically, there have been three principles in assigning the values of a linear script (print) to Braille: Using Louis Braille's original French letter values; reassigning the braille letters according to the sort order of the print alphabet being transcribed; and reassigning the letters to improve the efficiency of writing in braille.\n\nUnder international consensus, most braille alphabets follow the French sorting order for the 26 letters of the basic Latin alphabet, and there have been attempts at unifying the letters beyond these 26 (see international braille), though differences remain, for example in German Braille and the contractions of English Braille. This unification avoids the chaos of each nation reordering the braille code to match the sorting order of its print alphabet, as happened in Algerian Braille, where braille codes were numerically reassigned to match the order of the Arabic alphabet and bear little relation to the values used in other countries (compare modern Arabic Braille, which uses the French sorting order), and as happened in an early American version of English Braille, where the letters \"w, x, y, z\" were reassigned to match English alphabetical order. A convention sometimes seen for letters beyond the basic 26 is to exploit the physical symmetry of braille patterns iconically, for example, by assigning a reversed \"n\" to \"ñ\" or an inverted \"s\" to \"sh\". (See Hungarian Braille and Bharati Braille, which do this to some extent.)\n\nA third principle was to assign braille codes according to frequency, with the simplest patterns (quickest ones to write with a stylus) assigned to the most frequent letters of the alphabet. Such frequency-based alphabets were used in Germany and the United States in the 19th century (see American Braille), but with the invention of the braille typewriter their advantage disappeared, and none are attested in modern use – they had the disadvantage that the resulting small number of dots in a text interfered with following the alignment of the letters, and consequently made texts more difficult to read than Braille's more arbitrary letter-assignment. Finally, there are braille scripts which don't order the codes numerically at all, such as Japanese Braille and Korean Braille, which are based on more abstract principles of syllable composition.\n\nAcademic texts are sometimes written in a script of eight dots per cell rather than six, enabling them to encode a greater number of symbols. (See Gardner–Salinas braille codes.) Luxembourgish Braille has adopted eight-dot cells for general use; for example, it adds a dot below each letter to derive its capital variant.\n\nBraille was the first writing system with binary encoding. The system as devised by Braille consists of two parts:\n\nWithin an individual cell, the dot positions are arranged in two columns of three positions. A raised dot can appear in any of the six positions, producing sixty-four (2) possible patterns, including one in which there are no raised dots. For reference purposes, a pattern is commonly described by listing the positions where dots are raised, the positions being universally numbered, from top to bottom, as 1 to 3 on the left and 4 to 6 on the right. For example, dot pattern 1-3-4 describe a cell with three dots raised, at the top and bottom in the left column and at the top of the right column: that is, the letter \"m\". The lines of horizontal Braille text are separated by a space, much like visible printed text, so that the dots of one line can be differentiated from the braille text above and below. Different assignments of braille codes (or code pages) are used to map the character sets of different printed scripts to the six-bit cells. Braille assignments have also been created for mathematical and musical notation. However, because the six-dot braille cell allows only 64 (2) patterns, including space, the characters of a braille script commonly have multiple values, depending on their context. That is, character mapping between print and braille is not one-to-one. For example, the character corresponds in print to both the letter \"d\" and the digit \"4\".\n\nIn addition to simple encoding, many braille alphabets use contractions to reduce the size of braille texts and to increase reading speed. (See Contracted braille)\n\nBraille may be produced by hand using a slate and stylus in which each dot is created from the back of the page, writing in mirror image, or it may be produced on a braille typewriter or Perkins Brailler, or an electronic Brailler or eBrailler. Because braille letters cannot be effectively erased and written over if an error is made, an error is overwritten with all six dots (). \"Interpoint\" refers to braille printing that is offset, so that the paper can be embossed on both sides, with the dots on one side appearing between the divots that form the dots on the other (see the photo in the box at the top of this article for an example).\nUsing a computer or other electronic device, Braille may be produced with a braille embosser (printer) or a refreshable braille display (screen).\n\nBraille has been extended to an 8-dot code, particularly for use with braille embossers and refreshable braille displays. In 8-dot braille the additional dots are added at the bottom of the cell, giving a matrix 4 dots high by 2 dots wide. The additional dots are given the numbers 7 (for the lower-left dot) and 8 (for the lower-right dot). Eight-dot braille has the advantages that the case of an individual letter is directly coded in the cell containing the letter and that all the printable ASCII characters can be represented in a single cell. All 256 (2) possible combinations of 8 dots are encoded by the Unicode standard. Braille with six dots is frequently stored as Braille ASCII.\n\nThe first 25 braille letters, up through the first half of the 3rd decade, transcribe \"a–z\" (skipping \"w\"). In English Braille, the rest of that decade is rounded out with the ligatures \"and, for, of, the,\" and \"with\". Omitting dot 3 from these forms the 4th decade, the ligatures \"ch, gh, sh, th, wh, ed, er, ou, ow\" and the letter \"w\".\n\nVarious formatting marks affect the values of the letters that follow them. They have no direct equivalent in print. The most important in English Braille are:\nThat is, is read as capital 'A', and as the digit '1'.\n\nBasic punctuation marks in English Braille include:\n\nPunctuation varies from language to language. For example, French Braille uses for its question mark and swaps the quotation marks and parentheses (to and ); it uses the period () for the decimal point, as in print, and the decimal point () to mark capitalization.\n\nBraille contractions are words and affixes that are shortened so that they take up fewer cells. In English Braille, for example, the word \"afternoon\" is written with just three letters, , much like stenoscript. There are also several abbreviation marks that create what are effectively logograms. The most common of these is dot 5, which combines with the first letter of words. With the letter \"m\", the resulting word is \"mother\". There are also ligatures (\"contracted\" letters), which are single letters in braille but correspond to more than one letter in print. The letter \"and\", for example, is used to write words with the sequence \"a-n-d\" in them, such as \"hand\".\n\nMost braille embossers support between 34 and 40 cells per line, and 25 lines per page.\n\nA manually operated Perkins braille typewriter supports a maximum of 42 cells per line (its margins are adjustable), and typical paper allows 25 lines per page.\n\nA large interlining Stainsby has 36 cells per line and 18 lines per page.\n\nAn A4-sized Marburg braille frame, which allows interpoint braille (dots on both sides of the page, offset so they do not interfere with each other) has 30 cells per line and 27 lines per page.\n\nIn 1951 David Abraham, a woodworking teacher at the Perkins School for the Blind produced the first Braille typewriter, the Perkins Brailler, a typewriter with six keys that allow to write in Braille alphabet on a regular white page. In 1991 Ernest Bate developed the Mountbatten Brailler, an electronic machine used to type braille on braille paper, giving it a number of additional features such as word processing, audio feedback and embossing. This version was improved in 2008 with a quiet writer that had an erase key. \n\nBraille printers or embosser were produced in 1950s.\nIn 1960 Robert Mann, a teacher in MIT, wrote DOTSYS, a software that allowed automatic braille translation, and another group created an embossing device called \"M.I.T. Braillemboss.\". The Mitre Corporation team of Robert Gildea, Jonathan Millen, Reid Gerhart and Joseph Sullivan (now president of Duxbury Systems) developed DOTSYS III, the first braille translator written in a portable programming language. DOTSYS III was developed for the Atlanta Public Schools as a public domain program.\n\nIn 2011 David S. Morgan produced the first SMART Brailler machine, with added text to speech function and allowed digital capture of data entered.\n\nBraille is usually read in printed forms such on paper books written in braille, braille public signals and also on Braille e-books .Currently more than 1 % off all printed books have being translated into braille.\n\nChildren who are blind not only have the education disadvantage of not being able to see – they also miss out on fundamental parts of early and advanced education if not provided with the necessary tools. Children who are blind or visually impaired can begin learning pre-braille skills from a very young age to become fluent braille readers as they get older.\n\nIn 1960, 50% of legally blind, school-age children were able to read braille in the U.S. According to the 2015 \"Annual Report\" from the American Printing House for the Blind, there were 61,739 legally blind students registered in the U.S. Of these, 8.6% (5,333) were registered as braille readers, 31% (19,109) as visual readers, 9.4% (5,795) as auditory readers, 17% (10,470) as pre-readers, and 34% (21,032) as non-readers.\n\nThere are numerous causes for the decline in braille usage, including school budget constraints, technology advancement, and different philosophical views over how blind children should be educated.\n\nA key turning point for braille literacy was the passage of the Rehabilitation Act of 1973, an act of Congress that moved thousands of children from specialized schools for the blind into mainstream public schools. Because only a small percentage of public schools could afford to train and hire braille-qualified teachers, braille literacy has declined since the law took effect. Braille literacy rates have improved slightly since the bill was passed, in part because of pressure from consumers and advocacy groups that has led 27 states to pass legislation mandating that children who are legally blind be given the opportunity to learn braille.\n\nIn 1998 there were 57,425 legally blind students registered in the United States, but only 10% (5,461) of them used braille as their primary reading medium.\n\nEarly Braille education is crucial to literacy for a blind or low-vision child. A study conducted in the state of Washington found that people who learned braille at an early age did just as well, if not better than their sighted peers in several areas, including vocabulary and comprehension. In the preliminary adult study, while evaluating the correlation between adult literacy skills and employment, it was found that 44% of the participants who had learned to read in braille were unemployed, compared to the 77% unemployment rate of those who had learned to read using print. Currently, among the estimated 85,000 blind adults in the United States, 90% of those who are braille-literate are employed. Among adults who do not know braille, only 33% are employed. Statistically, history has proven that braille reading proficiency provides an essential skill set that allows blind or low-vision children to compete with their sighted peers in a school environment and later in life as they enter the workforce.\n\nThough braille is thought to be the main way blind people read and write, in Britain (for example) out of the reported two million blind and low vision population, it is estimated that only around 15–20 thousand people use braille. Younger people are turning to electronic text on computers with screen reader software instead, a more portable communication method that they can use with their friends. A debate has started on how to make braille more attractive and for more teachers to be available to teach it.\n\nAlthough it is possible to transcribe print by simply substituting the equivalent braille character for its printed equivalent, in English such a character-by-character transcription (known as \"uncontracted braille\") is only used by beginners.\n\nBraille characters are much larger than their printed equivalents, and the standard 11\" by 11.5\" (28 cm × 30 cm) page has room for only 25 lines of 43 characters. To reduce space and increase reading speed, most braille alphabets and orthographies use ligatures, abbreviations, and contractions. Virtually all English Braille books are transcribed in this \"contracted braille,\" which adds an additional layer of complexity to English orthography: The Library of Congress's \"Instruction Manual for Braille Transcribing\" runs to over 300 pages and braille transcribers must pass certification tests.\n\nFully contracted braille is known as \"Grade 2 Braille\". There is an intermediate form between Computer Braille—one-for-one identity with print—and Grade 2, which is called Grade 1 Braille. In Grade 1, the capital sign and Number sign are used, and most punctuation marks are shown using their Grade 2 values.\n\nThe system of contractions in English Braille begins with a set of 23 words which are contracted to single characters. Thus the word \"but\" is contracted to the single letter \"b,\" \"can\" to \"c\", \"do\" to \"d\", and so on. Even this simple rule creates issues requiring special cases; for example, \"d\" is, specifically, an abbreviation of the verb \"do;\" the noun \"do\" representing the note of the musical scale is a different word, and must be spelled out.\n\nPortions of words may be contracted, and many rules govern this process. For example, the character with dots 2-3-5 (the letter \"f\" lowered in the Braille cell) stands for \"ff\" when used in the middle of a word. At the beginning of a word, this same character stands for the word \"to\"; the character is written in braille with no space following it. (This contraction was removed in the Unified English Braille Code.) At the end of a word, the same character represents an exclamation point.\n\nSome contractions are more similar than their print equivalents. For example, the contraction , meaning 'letter', differs from , meaning 'little', only in adding one dot to the second : \"little\", \"letter\". This causes greater confusion between the braille spellings of these words and can hinder the learning process of contracted braille.\n\nThe contraction rules take into account the linguistic structure of the word; thus, contractions are generally not to be used when their use would alter the usual braille form of a base word to which a prefix or suffix has been added. Some portions of the transcription rules are not fully codified and rely on the judgment of the transcriber. Thus, when the contraction rules permit the same word in more than one way, preference is given to \"the contraction that more nearly approximates correct pronunciation.\"\n\n\"Grade 3 Braille\" is a variety of non-standardized systems that include many additional shorthand-like contractions. They are not used for publication, but by individuals for their personal convenience.\n\nWhen people produce braille, this is called braille transcription. When computer software produces braille, this is called braille\ntranslation. Braille translation software exists to handle most of the common languages of the world, and many technical areas,\nsuch as mathematics (mathematical notation), for example WIMATS, music (musical notation), and tactile graphics.\n\nSince Braille is one of the few writing systems where tactile perception is used, as opposed to visual perception, a braille reader must develop new skills. One skill important for Braille readers is the ability to create smooth and even pressures when running one's fingers along the words. There are many different styles and techniques used for the understanding and development of braille, even though a study by B. F. Holland suggests that there is no specific technique that is superior to any other.\n\nAnother study by Lowenfield & Abel shows that braille could be read \"the fastest and best... by students who read using the index fingers of both hands\". Another important reading skill emphasized in this study is to finish reading the end of a line with the right hand and to find the beginning of the next line with the left hand simultaneously. One final conclusion drawn by both Lowenfield and Abel is that children have difficulty using both hands independently where the right hand is the dominant hand. But this hand preference does not correlate to other activities.\n\nWhen Braille was first adapted to languages other than French, many schemes were adopted, including mapping the native alphabet to the alphabetical order of French – e.g. in English W, which was not in the French alphabet at the time, is mapped to braille X, X to Y, Y to Z, and Z to the first French-accented letter – or completely rearranging the alphabet such that common letters are represented by the simplest braille patterns. Consequently, mutual intelligibility was greatly hindered by this state of affairs. In 1878, the International Congress on Work for the Blind, held in Paris, proposed an international braille standard, where braille codes for different languages and scripts would be based, not on the order of a particular alphabet, but on phonetic correspondence and transliteration to Latin.\n\nThis unified braille has been applied to the languages of India and Africa, Arabic, Vietnamese, Hebrew, Russian, and Armenian, as well as nearly all Latin-script languages. Greek, for example, \"gamma\" is written as Latin \"g\", despite the fact that it has the alphabetic position of \"c\"; Hebrew \"bet\", the second letter of the alphabet and cognate with the Latin letter \"b\", is sometimes pronounced /b/ and sometimes /v/, and is written \"b\" or \"v\" accordingly; Russian \"ts\" is written as \"c\", which is the usual letter for /ts/ in those Slavic languages that use the Latin alphabet; and Arabic \"f\" is written as \"f\", despite being historically \"p\", and occurring in that part of the Arabic alphabet (between historic \"o\" and \"q\").\n\nOther systems for assigning values to braille patterns are also followed beside the simple mapping of the alphabetical order onto the original French order. Some braille alphabets start with unified braille, and then diverge significantly based on the phonology of the target languages, while others diverge even further.\n\nIn the various Chinese systems, traditional braille values are used for initial consonants and the simple vowels. In both Mandarin and Cantonese Braille, however, characters have different readings depending on whether they are placed in syllable-initial (onset) or syllable-final (rime) position. For instance, the cell for Latin \"k\", , represents Cantonese \"k\" (\"g\" in Yale and other modern romanizations) when initial, but \"aak\" when final, while Latin \"j\", , represents Cantonese initial \"j\" but final \"oei\".\n\nNovel systems of braille mapping include Korean, which adopts separate syllable-initial and syllable-final forms for its consonants, explicitly grouping braille cells into syllabic groups in the same way as hangul. Japanese, meanwhile, combines independent vowel dot patterns and modifier consonant dot patterns into a single braille cell – an abugida representation of each Japanese mora.\n\nThe current series of Canadian banknotes has a tactile feature consisting of raised dots that indicate the denomination, allowing bills to be easily identified by blind or low vision people. It does not use standard braille; rather, the feature uses a system developed in consultation with blind and low vision Canadians after research indicated that braille was not sufficiently robust and that not all potential users read braille. Mexican bank notes, Australian bank notes, Indian rupee notes, Israeli new shekel notes and Russian ruble notes also have special raised symbols to make them identifiable by persons who are blind or low vision. \n\nIn India there are instances where the parliament acts have been published in braille, such as \"The Right to Information Act\".\n\nIn the United States, the Americans with Disabilities Act of 1990 requires various building signage to be in braille.\n\nIn the United Kingdom, it is required that medicines have the name of the medicine in Braille on the labeling.\n\nAustralia also recently introduced the tactile feature onto their five-dollar banknote\n\nU.K. September 2017 – On the front of the £10 polymer note (the side with raised print), there are two clusters of raised dots in the top left hand corner. This tactile feature helps blind and partially sighted people identify the value of the note.\n\nBraille was added to the Unicode Standard in September 1999 with the release of version 3.0.\n\nMost braille embossers and refreshable braille displays do not support Unicode, using instead 6-dot braille ASCII. Some embossers have proprietary control codes for 8-dot braille or for full graphics mode, where dots may be placed anywhere on the page without leaving any space between braille cells so that continuous lines can be drawn in diagrams, but these are rarely used and are not standard.\n\nThe Unicode standard encodes 8-dot braille glyphs according to their binary appearance, rather than following their assigned numeric order. Dot 1 corresponds to the least significant bit of the low byte of the Unicode scalar value, and dot 8 to the high bit of that byte.\n\nThe Unicode block for braille is U+2800 ... U+28FF:\n\nEvery year on 4 January, World Braille Day is observed internationally to commemorate the birth of Louis Braille and to recognize his efforts, but the event is not considered a public holiday.\n\n"}
{"id": "2215734", "url": "https://en.wikipedia.org/wiki?curid=2215734", "title": "Changing table", "text": "Changing table\n\nA changing table is a small raised platform designed to allow a person to change someone's diaper.\n\nTypically made of wood (such as oak or pine), it is often part of a nursery set. The topmost surface is used to rest the person being changed, during the changing process while there are usually shelves or drawers to store necessary supplies such as diapers, baby wipes, baby powder and sometimes clothing as a dresser would have.\n\nMany public restrooms have public tables available should a diaper change be required in a public place. They are typically made of hard plastic and rest on hinges so they can be folded into the wall when not in use. They are usually not enclosed in a stall.\n\nThese became popular in the 1990s. Originally they were mainly found in women's restrooms. Through the lobbying efforts of Eric Letts, a founder of the Fair Parenting Project, it became commonplace to find them in men's rooms across Canada and the United States.\n\nIn 2016, President Obama signed the BABIES Act into law, requiring changing tables in all publicly accessible, federal buildings in the United States.\n"}
{"id": "40863", "url": "https://en.wikipedia.org/wiki?curid=40863", "title": "Channel service unit", "text": "Channel service unit\n\nIn telecommunications, a channel service unit (CSU) is a line Bridging device for use with T-carrier that:\n\nCSUs can be categorized by the class of service they support (DS1, DS3, DDS, etc.) and by the capabilities within that class. For example, basic DS1 (T1) CSUs support loopback of each interface and will produce Alarm indication signal to the provider's NIU (Network Interface Device) in the event of loss of signal from the customer-premises equipment (CPE). More advanced units will include internal monitors of the performance of the carrier in both directions and may have test pattern generation and monitor capabilities.\n\nCSUs are required by PSTN providers at digital interfaces that terminate in a DSU on the customer side. They are not used when the service terminates in a modem, such as the DSL family of service. The maintenance capabilities of the CSU provide important guidance as to whether the provider needs to dispatch a repairman to the customer location.\n\n"}
{"id": "6526471", "url": "https://en.wikipedia.org/wiki?curid=6526471", "title": "Cheryl Arrowsmith", "text": "Cheryl Arrowsmith\n\nCheryl H. Arrowsmith is a Canadian structural biologist and is the Chief Scientist at the Toronto laboratory of the Structural Genomics Consortium. Her contributions to protein structural biology includes the use of NMR and X-ray crystallography to pursue structures of proteins on a proteome wide scale.\n\nShe received her Ph.D. in chemistry at the University of Toronto in 1987 and post-doctoral training at Stanford University working with Professor Oleg Jardetzky. One of her areas of interest is the tumour suppressor p53 and related proteins.\n\nHer current research is to determine the 3-dimensional structures of human proteins of therapeutic relevance by structural proteomics. She made significant contributions to epigenetic signaling with her highest cited paper is \"Epigenetic protein families: a new frontier for drug discovery\" cited at 802 times, according to Google Scholar.\n\nArrowsmith was named a Fellow of the American Association for the Advancement of Science (AAAS) in 2015.\n"}
{"id": "27905358", "url": "https://en.wikipedia.org/wiki?curid=27905358", "title": "Cisco Cius", "text": "Cisco Cius\n\nThe Cisco Cius is a business-oriented, Android-based tablet computer from Cisco Systems. It was announced on June 29 at Cisco Live 2010 and discontinued on May 24, 2012, although it will still be offered on a limited basis.\n\n\nThere will also be an optional HD media station that will support USB, 10/100/1000 Ethernet, and a handset option.\n\n\n"}
{"id": "43612515", "url": "https://en.wikipedia.org/wiki?curid=43612515", "title": "Cloud-based design and manufacturing", "text": "Cloud-based design and manufacturing\n\nCloud-based design and manufacturing (CBDM) refers to a service-oriented networked product development model in which service consumers are able to configure products or services and reconfigure manufacturing systems through Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), Hardware-as-a-Service (HaaS), and Software-as-a-Service (SaaS).\nAdapted from the original cloud computing paradigm and introduced into the realm of computer-aided product development, Cloud-Based Design and Manufacturing is gaining significant momentum and attention from both academia and industry.\n\nCloud-based design and manufacturing includes two aspects: cloud-based design and cloud-based manufacturing. Another related concept is cloud manufacturing that is more general and popular.\n\nCloud-Based Design (CBD) refers to a networked design model that leverages cloud computing, service-oriented architecture (SOA), Web 2.0 (e.g., social network sites), and semantic web technologies to support cloud-based engineering design services in distributed and collaborative environments.\n\nCloud-Based Manufacturing (CBM) refers to a networked manufacturing model that exploits on-demand access to a shared collection of diversified and distributed manufacturing resources to form temporary, reconfigurable production lines which enhance efficiency, reduce product lifecycle costs, and allow for optimal resource allocation in response to variable-demand customer generated tasking.\n\nThe enabling technologies for Cloud-Based Design and Manufacturing include cloud computing, Web 2.0, Internet of Things (IoT), and service-oriented architecture (SOA).\n\nThe term cloud-based design and manufacturing (CBDM) was initially coined by Dazhong Wu, David Rosen, and Dirk Schaefer at Georgia Tech in 2012 for the purpose of articulating a new paradigm for digital manufacturing and design innovation in distributed and collaborative settings. The main objective of CBDM is to further reduce time and cost associated with\nmaintaining information and communication technology (ICT) infrastructures for design and\nmanufacturing, enhancing digital manufacturing and design innovation in distributed and collaborative environments, and adapting to rapidly changing\nmarket demands.\n\nIn 2014, the same research group also published the worldwide first two books on the subjects of Cloud-Based Design and Manufacturing (CBDM) and Social Product Development (SPD) with Springer, edited by Dirk Schaefer.\n\nCBDM exhibits the following key characteristics:\n\nCBDM differs from traditional collaborative and distributed design and manufacturing systems such as web-based systems and agent-based systems from a number of perspectives, including (1) computing architecture, (2) data storage, (3) sourcing process, (4) information and communication technology infrastructure, (5) business model, (6) programming model, and (7) communication.\n\n\nSimilar to cloud computing, CBDM services can be categorized into four major deployment models: the public cloud, private cloud, hybrid cloud, and community cloud.\n\n\n"}
{"id": "1963417", "url": "https://en.wikipedia.org/wiki?curid=1963417", "title": "Collaborative real-time editor", "text": "Collaborative real-time editor\n\nA collaborative editor is a form of collaborative software application that allows several people to edit a computer file using different computers, a practice called collaborative editing. There are two types of collaborative editing: real-time and non-real-time. In real-time collaborative editing , users can edit the same file simultaneously, whereas in Non-real-time collaborative editing, the users do not edit the same file at the same time (similar to revision control systems). Collaborative real-time editors generally permit both the above modes of editing in any given instance.\n\nThe first instance of a collaborative real-time editor was demonstrated by Douglas Engelbart in 1968, in The Mother of All Demos. Widely available implementations of the concept took decades to appear.\n\nInstant Update was released for the classic Mac OS in 1991 from ON Technology. Later, a version for Microsoft Windows was released as well, allowing real-time collaboration across these two operating systems. Instant Update relied on a work group server to coordinate documents updated in real time on multiple clients.\n\nThe Web 2.0 phenomenon has caused an explosion of interest in browser-based document editing tools. In particular, a product called Writely saw explosive user growth and was bought by Google in March 2006 (what became known as Google Docs and later renamed to Google Drive). It provided simultaneous edits on the entirety of a document, though changes from other users were only reflected after the client program polling the server (every half-minute or so). Another early web-based solution was JotSpotLive, in which line-by-line simultaneous editing was available in near-realtime. However, after Google's purchase of parent company JotSpot in November 2006, the site was closed. Google Sites was launched in February 2007 as a refactoring of JotSpot, but it lacks the multi-user real-time abilities of JotLive. The Synchroedit (rich text) and MobWrite (plain text) projects are two, more recent, open source attempts to fill the in gap real-time browser-based collaborative editing, though still unable to achieve true real-time performance, especially on a large scale architecture.\n\nIn 2009, Google started beta testing Google Wave, a real-time collaboration environment which Google hoped would eventually displace email and instant messaging. EtherPad was acquired by Google, which allocated the EtherPad team to work within the Wave project. However, Google announced in August 2010 on its blog that it had decided to stop developing Wave as a standalone project, due to insufficient user adoption. After Google released the abandoned EtherPad source code as open source in December 2009, the community took over its development and produced a complete rewrite named Etherpad lite, which is written entirely in JavaScript and built on top of node.js.\n\nThe complexity of real-time collaborative editing solutions stems from communication latency. In theory, if communication were instantaneous, then creating a real-time collaborative editor would be no more difficult than creating a single-user editor, because a document could be edited using an algorithm similar to the following:\n\n\nHowever, the speed of communication is limited by network latency. This creates a fundamental dilemma: users need their own edits incorporated into the document instantly, but if they are incorporated instantly, then because of communication latency, their edits must necessarily be inserted into different versions of the document.\n\nAn example illustrates this problem. Suppose Bob and Alice start with a document containing the word \"Mary\". Bob deletes 'M', then inserts 'H', to change the word into \"Hary\". Alice, before she receives either edit from Bob, deletes 'r', then deletes 'a', to change it into \"My\". Both Bob and Alice will then receive edits that were applied to versions of the document that \"never existed\" on their own machines.\n\nThus, the challenge of real-time collaborative editing is to figure out exactly how to apply edits from remote users, which were originally created in versions of the document that never existed locally, and which may conflict with the user's own local edits.\n\nThe most sophisticated solutions solve this problem in a way that does not require a server, does not use locking (all users can freely edit all parts of a document at the same time), and supports any number of users (limited only by the resources of the computers). UNA and SubEthaEdit are examples of two programs that take this approach.\n\nWhile these sophisticated approaches enable the best user experience, a basic collaborative editor can also be created in a client–server model. In a client–server scenario, one of the editor instances is assigned the role of collaboration server when the document is opened. This server ensures that other editors are kept in sync by determining network latency and acting as a time synchronization server. The server receives timestamped notifications of changes made to the document by other users. It determines how those changes should affect its local copy, and broadcasts its changes to the collaboration pool. In some models, the changes are not reflected on a client until an official response is returned from the server, even if those changes were made locally.\n\nThis approach, while significantly less powerful, allows for basic collaboration at a relatively low cost. This makes it preferable in situations where processing resources are limited. NetSketch is an example of a program that uses this model.\n\nIn the past, Microsoft and IBM have worked to add collaboration facilities to their existing architectures. Although marketed as real-time collaboration, these 'workspace' approaches require either document locking (so only one person can edit it at a time), or 'reconciliation' of conflicting changes, which is generally found by users to be unsatisfactory.\n\n"}
{"id": "53992606", "url": "https://en.wikipedia.org/wiki?curid=53992606", "title": "Common Electrical I/O", "text": "Common Electrical I/O\n\nThe Common Electrical I/O (CEI) refers to a series of influential Interoperability Agreements (IAs) that have been published by the Optical Internetworking Forum (OIF). CEI defines the electrical and jitter requirements for 3.125, 6, 11, 25-28, and 56 Gbit/s electrical interfaces.\n\nThe Common Electrical I/O (CEI) Interoperability Agreement published by the OIF defines the electrical and jitter requirements for 3.125, 6, 11, 25-28, and 56 Gbit/s SerDes interfaces. This CEI specification has defined SerDes interfaces for the industry since 2004, and it has been highly influential. The development of electrical interfaces at the OIF began with SPI-3 in 2000, and the first differential interface was published in 2003. The seventh generation electrical interface, CEI-56G, defines five reaches of 56 Gb/s interfaces. The OIF began work on its eighth generation with its CEI-112G project. CEI has influenced or has been adopted or adapted in many other serial interface standards by many different standards organizations over its long lifetime. SerDes interfaces have been developed based on CEI for most ASIC and FPGA products.\n\nThroughout the 2000's, the OIF produced an important series of interfaces that influenced the development of multiple generations of devices. Beginning with the donation of the PL-3 interface by PMC-Sierra in 2000, the OIF produced the System Packet Interface (SPI) family of packet interfaces. SPI-3 and SPI-4.2 defined two generations of devices before they were supplanted by the closely related Interlaken standard in the SPI-5 generation in 2006.\n\nThe OIF also defined the SerDes Framer Interface (SFI) family of specifications in parallel with SPI. As a part of the SPI-5 and SFI-5 development, a common electrical interface was developed termed SxI-5. SxI-5 abstracted the electrical I/O interface away from the individual SPI and SFI documents. This abstraction laid the groundwork for the highly successful CEI family of Interoperability Agreements and was incorporated in the original release of CEI 1.0 a generation later.\n\nTwo earlier generations in this development path were defined by some of the same individuals at the ATM Forum in 1994 and 1995. These specifications were called UTOPIA Level 1 and 2. These operated at 25 Mbit/s (0.025 Gbit/s) and 50 Mbit/s per wire single ended and were used in OC-3 (155 Mbit/s) applications. PL-3 was a packet extension of the cells carried by those earlier interfaces.\n\nCompliant implementations to the draft CEI-56G IAs were demonstrated in the OIF booth at the Optical Fiber Conference in 2015, 2016 and 2017.\n"}
{"id": "8302614", "url": "https://en.wikipedia.org/wiki?curid=8302614", "title": "Craniofacial prosthesis", "text": "Craniofacial prosthesis\n\nCraniofacial prostheses are prostheses made by individuals trained in anaplastology or maxillofacial prosthodontics who medically help rehabilitate those with facial defects caused by disease (mostly progressed forms of skin cancer, and head and neck cancer), trauma (outer ear trauma, eye trauma) or birth defects (microtia, anophthalmia). They have the ability to replace almost any part of the face, but most commonly the ear, nose or eye/eyelids. An ocular prosthesis and hair prosthesis can also be classified as craniofacial prostheses. Prostheses are held in place either by biocompatible drying adhesives, osseointegrated implants, magnets, or another mechanical means (although rare) such as glasses or straps. Prostheses are designed to be as similar as possible to the natural anatomy of each individual. Their purpose is to cover, protect, and disguise facial disfigurements or underdevelopments.\n\nWhen surgical reconstruction is not ideal, craniofacial prosthetics are favored when they can better restore the form and function of the absent facial feature. Craniofacial prosthetics are not wholly considered cosmetic because they replace the physical form and functional mechanics of the absent anatomy and serve a significant role in the emotional stability and rehabilitation of those suffering from facial defects.\n\n\n"}
{"id": "2658254", "url": "https://en.wikipedia.org/wiki?curid=2658254", "title": "Creamery", "text": "Creamery\n\nIn a dairy, the creamery is the location of cream processing. Cream is separated from whole milk; pasteurization is done to the skimmed milk and cream separately. Whole milk for sale has had some cream returned to the skimmed milk.\n\nThe creamery is the source of butter from a dairy. Cream is an emulsion of fat-in-water; the process of churning causes a phase inversion to butter which is an emulsion of water-in-fat. Excess liquid as buttermilk is drained off in the process. Modern creameries are automatically controlled industries, but the traditional creamery needed skilled workers. Traditional tools included the butter churn and Scotch hands.\n\nThe term \"creamery\" is sometimes used in retail trade as a place to buy milk products such as yogurt and ice cream. Under the banner of a creamery one might find a store also stocking pies and cakes or even a coffeehouse with confectionery.\n\n"}
{"id": "766619", "url": "https://en.wikipedia.org/wiki?curid=766619", "title": "Cyclonic separation", "text": "Cyclonic separation\n\nA cyclonic separation is a method of removing particulates from an air, gas or liquid stream, without the use of filters, through vortex separation. When removing particulate matter from liquid, a hydrocyclone is used; while from gas, a gas cyclone is used. Rotational effects and gravity are used to separate mixtures of solids and fluids. The method can also be used to separate fine droplets of liquid from a gaseous stream.\n\nA high speed rotating (air)flow is established within a cylindrical or conical container called a cyclone. Air flows in a helical pattern, beginning at the top (wide end) of the cyclone and ending at the bottom (narrow) end before exiting the cyclone in a straight stream through the center of the cyclone and out the top. Larger (denser) particles in the rotating stream have too much inertia to follow the tight curve of the stream, and thus strike the outside wall, then fall to the bottom of the cyclone where they can be removed. In a conical system, as the rotating flow moves towards the narrow end of the cyclone, the rotational radius of the stream is reduced, thus separating smaller and smaller particles. The cyclone geometry, together with volumetric flow rate, defines the \"cut point\" of the cyclone. This is the size of particle that will be removed from the stream with a 50% efficiency. Particles larger than the cut point will be removed with a greater efficiency, and smaller particles with a lower efficiency as they separate with more difficulty or can be subject to re-entrainment when the air vortex reverses direction to move in direction of the outlet.\n\nAn alternative cyclone design uses a secondary air flow within the cyclone to keep the collected particles from striking the walls, to protect them from abrasion. The primary air flow containing the particulates enters from the bottom of the cyclone and is forced into spiral rotation by stationary spinner vanes. The secondary air flow enters from the top of the cyclone and moves downward toward the bottom, intercepting the particulate from the primary air. The secondary air flow also allows the collector to optionally be mounted horizontally, because it pushes the particulate toward the collection area, and does not rely solely on gravity to perform this function.\n\nLarge scale cyclones are used in sawmills to remove sawdust from extracted air. Cyclones are also used in oil refineries to separate oils and gases, and in the cement industry as components of kiln preheaters. Cyclones are increasingly used in the household, as the core technology in bagless types of portable vacuum cleaners and central vacuum cleaners. Cyclones are also used in industrial and professional kitchen ventilation for separating the grease from the exhaust air in extraction hoods. Smaller cyclones are used to separate airborne particles for analysis. Some are small enough to be worn clipped to clothing, and are used to separate respirable particles for later analysis.\n\nSimilar separators are used in the oil refining industry (e.g. for Fluid catalytic cracking) to achieve fast separation of the catalyst particles from the reacting gases and vapors.\n\nJames Dyson has become a billionaire from developing and marketing bagless vacuum cleaners based on cyclonic separation of dust, initially inspired by seeing a sawdust separator at a sawmill. \n\nAnalogous devices for separating particles or solids from liquids are called hydrocyclones or hydroclones. These may be used to separate solid waste from water in wastewater and sewage treatment.\n\nAs the cyclone is essentially a two phase particle-fluid system, fluid mechanics and particle transport equations can be used to describe the behaviour of a cyclone. The air in a cyclone is initially introduced tangentially into the cyclone with an inlet velocity formula_1. Assuming that the particle is spherical, a simple analysis to calculate critical separation particle sizes can be established.\n\nIf one considers an isolated particle circling in the upper cylindrical component of the cyclone at a rotational radius of formula_2 from the cyclone's central axis, the particle is therefore subjected to drag, centrifugal, and buoyant forces. Given that the fluid velocity is moving in a spiral the gas velocity can be broken into two component velocities: a tangential component, formula_3, and an outward radial velocity component formula_4. Assuming Stokes' law, the drag force in the outward radial direction that is opposing the outward velocity on any particle in the inlet stream is:\n\nUsing formula_6 as the particle's density, the centrifugal component in the outward radial direction is:\n\nThe buoyant force component is in the inward radial direction. It is in the opposite direction to the particle's centrifugal force because it is on a volume of fluid that is missing compared to the surrounding fluid. Using formula_9 for the density of the fluid, the buoyant force is:\n\nIn this case, formula_12 is equal to the volume of the particle (as opposed to the velocity). Determining the outward radial motion of each particle is found by setting Newton's second law of motion equal to the sum of these forces:\n\nTo simplify this, we can assume the particle under consideration has reached \"terminal velocity\", i.e., that its acceleration formula_14 is zero. This occurs when the radial velocity has caused enough drag force to counter the centrifugal and buoyancy forces. This simplification changes our equation to:\n\nformula_15\n\nWhich expands to:\n\nSolving for formula_4 we have\n\nNotice that if the density of the fluid is greater than the density of the particle, the motion is (-), toward the center of rotation and if the particle is denser than the fluid, the motion is (+), away from the center. In most cases, this solution is used as guidance in designing a separator, while actual performance is evaluated and modified empirically.\n\nIn non-equilibrium conditions when radial acceleration is not zero, the general equation from above must be solved. Rearranging terms we obtain\n\nSince formula_4 is distance per time, this is a 2nd order differential equation of the form formula_21.\n\nExperimentally it is found that the velocity component of rotational flow is proportional to formula_22, therefore:\n\nThis means that the established feed velocity controls the vortex rate inside the cyclone, and the velocity at an arbitrary radius is therefore:\n\nSubsequently, given a value for formula_3, possibly based upon the injection angle, and a cutoff radius, a characteristic particle filtering radius can be estimated, above which particles will be removed from the gas stream.\n\nThe above equations are limited in many regards. For example, the geometry of the separator is not considered, the particles are assumed to achieve a steady state and the effect of the vortex inversion at the base of the cyclone is also ignored, all behaviours which are unlikely to be achieved in a cyclone at real operating conditions.\n\nMore complete models exist, as many authors have studied the behaviour of cyclone separators. Numerical modelling using computational fluid dynamics has also been used extensively in the study of cyclonic behaviour. A major limitation of any fluid mechanics model for cyclone separators is the inability to predict the agglomeration of fine particles with larger particles, which has a great impact on cyclone collection efficiency.\n\n"}
{"id": "7474213", "url": "https://en.wikipedia.org/wiki?curid=7474213", "title": "DataHand", "text": "DataHand\n\nThe DataHand keyboard was introduced in 1995 by DataHand Systems, Inc. It was invented by Dale J. Retter and was produced by Industrial Innovations as early as 1992. The keyboard consists of two completely separate \"keyboards\", one for the left hand and one for the right, that are molded to rest the user's hands on. This allows the user to place each hand wherever it is most comfortable to them. Each finger activates five buttons, the four compass directions as well as down. The thumbs also have five buttons, one inside and two outside as well as up and down. The button modules in which the fingers rest are adjustable to best fit the user's hands—each side can be independently moved up and down, towards the palm or farther away.\nThis ergonomic layout allows for all typing to occur without any wrist motion, as well as without any finger extension. The keyboard layout is initially similar to a QWERTY keyboard, but the middle two columns of keys (i.e. H,Y,G...) have been delegated to sideways finger movements, and all of the keys outside of the main three rows are accessed through two additional modes, including a mode for mousing. There are three primary modes all together: letters, number and symbols, and function / mouse mode. Some practice is required. However, eventual typing speedups are possible.\n\nAlso of note is the button design—instead of being spring-loaded, the buttons are held in place with magnets and are activated using optical sensors. This was done in order to dramatically reduce the finger workload while optimizing tactile feedback.\n\nThis unconventional keyboard was seen in the Jodie Foster movie \"Contact\" (1997) as the pilot's controls for the futuristic spaceship; and the spy movie \"Stormbreaker\" (2006). The Industrial Innovations version was featured on the television series \"Mighty Morphin Power Rangers\".\n\nAfter the initial prototype was released in 1995, DataHand has released the Professional and Professional II with new bodies. The Professional II also has extended programming capabilities over the Professional, being able to record macros of keystrokes for convenient use.\n\nDataHand Systems, Inc. announced in early 2008 that it was ceasing to market and sell its keyboards. The company web site states that due to supplier issues, the company will not sell the DataHand keyboard \"until a new manufacturer can be identified\". However, the company plans a final, limited production run to satisfy existing customers. In January 2009, the company's website started taking orders for a \"limited number of new DataHand Pro II units\".\n\n\n"}
{"id": "286078", "url": "https://en.wikipedia.org/wiki?curid=286078", "title": "Digital intermediate", "text": "Digital intermediate\n\nDigital intermediate (typically abbreviated to DI) is a motion picture finishing process which classically involves digitizing a motion picture and manipulating the color and other image characteristics. \n\nIt often replaces or augments the photochemical timing process and is usually the final creative adjustment to a movie before distribution in theaters. It is distinguished from the telecine process in which film is scanned and color is manipulated early in the process to facilitate editing. However the lines between telecine and DI are continually blurred and are often executed on the same hardware by colorists of the same background. These two steps are typically part of the overall color management process in a motion picture at different points in time. A digital intermediate is also customarily done at higher resolution and with greater color fidelity than telecine transfers.\n\nAlthough originally used to describe a process that started with film scanning and ended with film recording, digital intermediate is also used to describe color correction and color grading and even final mastering when a digital camera is used as the image source and/or when the final movie is not output to film. This is due to recent advances in digital cinematography and digital projection technologies that strive to match film origination and film projection.\n\nIn traditional photochemical film finishing, an intermediate is produced by exposing film to the original camera negative. The intermediate is then used to mass-produce the films that get distributed to theaters. Color grading is done by varying the amount of red, green, and blue light used to expose the intermediate. This seeks to be able to replace or augment the photochemical approach to creating this intermediate.\n\nThe digital intermediate process uses digital tools to color grade, which allows for much finer control of individual colors and areas of the image, and allows for the adjustment of image structure (grain, sharpness, etc.). The intermediate for film reproduction can then be produced by means of a film recorder. The physical intermediate film that is a result of the recording process is sometimes also called a digital intermediate, and is usually recorded to internegative (IN) stock, which is inherently finer-grain than camera negative (OCN).\n\nOne of the key technical achievements that made the transition to DI possible was the use of the 3D look-up tables (aka \"3D LUTs\"), which could be used to mimic how the digital image would look once it was printed onto release print stock. This removed a large amount of skilled guesswork from the film-making process, and allowed greater freedom in the colour grading process while reducing risk.\n\nThe digital master is often used as a source for a DCI-compliant distribution of the motion picture for digital projection. For archival purposes, the digital master created during the Digital Intermediate process can still be recorded to very stable high dynamic range yellow-cyan-magenta (YCM) separations on black-and-white film with an expected 100-year or longer life. This archival format, long used in the industry prior to the invention of DI, still provides an archival medium that is independent of changes in digital data recording technologies and file formats that might otherwise render digitally archived material unreadable in the long term.\n\nTelecine tools to electronically capture film images are nearly as old as broadcast television, but the resulting images were widely considered unsuitable for exposing back onto film for theatrical distribution. Film scanners and recorders with quality sufficient to produce images that could be inter-cut with regular film began appearing in the 1970s, with significant improvements in the late 1980s and early 1990s. During this time, digitally processing an entire feature-length film was impractical because the scanners and recorders were extremely slow and the image files were too large compared to computing power available. Instead, individual shots or short sequences were processed for special visual effects.\n\nIn 1992, Visual Effects Supervisor/Producer Chris F. Woods broke through several \"techno-barriers\" in creating a digital studio to produce the visual effects for the 1993 release \"Super Mario Bros.\" It was the first feature film project to digitally scan a large number of VFX plates (over 700) at 2K resolution. It was also the first film scanned and recorded at Kodak's just launched Cinesite facility in Hollywood. This project based studio was the first feature film to use Discreet Logic's (now Autodesk) Flame and Inferno systems, which enjoyed early dominance as high resolution / high performance digital compositing systems.\n\nDigital film compositing for visual effects was immediately embraced, while optical printer use for VFX declined just as quickly. Chris Watts further revolutionized the process on the 1998 feature film \"Pleasantville\", becoming the first visual effects supervisor for New Line Cinema to scan, process, and record the majority of a feature length, live-action, Hollywood film digitally. The first Hollywood film to utilize a digital intermediate process from beginning to end was \"O Brother, Where Art Thou?\" in 2000 and in Europe it was \"Chicken Run\" released that same year.\n\nThe process rapidly caught on in the mid-2000s. Around 50% of Hollywood films went through a digital intermediate in 2005, increasing to around 70% by mid-2007. This is due not only to the extra creative options the process affords film makers but also the need for high-quality scanning and color adjustments to produce movies for digital cinema.\n\n\n\n"}
{"id": "205126", "url": "https://en.wikipedia.org/wiki?curid=205126", "title": "Drill", "text": "Drill\n\nA drill is a tool primarily used for making round holes or driving fasteners. It is fitted with a bit, either a drill or driver, depending on application, secured by a chuck. Some powered drills also include a hammer function.\n\nDrills vary widely in speed, power, and size. They are characteristically corded electrically driven devices, with hand operated types dramatically decreasing in popularity and cordless battery powered ones proliferating.\n\nDrills are commonly used in woodworking, metalworking, machine tool fabrication, construction and utility projects. Specially designed versions are made for medicine, Space, and miniature applications.\n\nAround 35,000 BC, Homo sapiens discovered the benefits of the application of rotary tools. This would have rudimentarily consisted of a pointed rock being spun between the hands to bore a hole through another material. This led to the hand drill, a smooth stick, that was sometimes attached to flint point, and was rubbed between the palms. This was used by many ancient civilizations around the world including the Mayans. The earliest perforated artifacts, such as bone, ivory, shells, and antlers found, are from the Upper Paleolithic era.\n\nBow drill (strap-drills) are the first machine drills, as they convert a back and forth motion to a rotary motion, and they can be traced back to around 10,000 years ago. It was discovered that tying a cord around a stick, and then attaching the ends of the string to the ends of a stick(a bow), allowed a user to drill quicker and more efficiently. Mainly used to create fire, bow-drills were also used in ancient woodwork, stonework, and dentistry. Archaeologists discovered a Neolithic grave yard in Mehrgrath, Pakistan dating from the time of the Harappans, around 7,500–9,000 years ago, containing 9 adult bodies with a total of 11 teeth that had been drilled. There are hieroglyphs depicting Egyptian carpenters and bead makers in a tomb at Thebes using bow-drills. The earliest evidence of these tools being used in Egypt dates back to around 2500 BCE. The usage of bow-drills was widely spread through Europe, Africa, Asia, and North America, during ancient times and is still used today. Over the years many slight variations of bow and strap drills have developed for the various uses of either boring through materials or lighting fires.\n\nThe core drill was developed in ancient Egypt by 3000 BC(5016 years before now). The pump drill was invented during Roman times. It consists of a vertical spindle aligned by a piece of horizontal wood and a flywheel to maintain accuracy and momentum.\n\nThe hollow-borer tip, first used around the 13th century, consisted of a stick with a tubular shaped piece of metal on the end, such as copper. This allowed a hole to be drilled while only actually grinding the outer section of it. This completely separates the inner stone or wood from the rest, allowing the drill to pulverize less material to create a similarly sized hole.\n\nWhile the pump-drill and the bow-drill were used in Western Civilization to bore smaller holes for a larger part of human history, the Auger was used to drill larger holes starting sometime between Roman and Medieval ages. The auger allowed for more torque for larger holes. It is uncertain when the Brace and Bit was invented; however, the earliest picture found so far dates from the 15th century. It is a type of hand crank drill that consists of two parts as seen in the picture. The brace, on the upper half, is where the user holds and turns it and on the lower part is the bit. The bit is interchangeable as bits wear down. The auger uses a rotating helical screw similar to the Archimedean screw-shaped bit that is common today. The gimlet is also worth mentioning as it is a scaled down version of an auger.\n\nIn the East, churn drills were invented as early as 221 BC during the Chinese Qin Dynasty, capable of reaching a depth of 1500 m. Churn drills in ancient China were built of wood and labor-intensive, but were able to go through solid rock. The churn drill appears in Europe during the 12th century. In 1835 Isaac Singer is reported to have built a steam powered churn drill based on the method the Chinese used. Also worth briefly discussing are the early drill presses; they were machine tools that derived from bow-drills but were powered by windmills or water wheels. Drill presses consisted of the powered drills that could be raised or lowered into a material, allowing for less force by the user.\n\nThe next great advancement in drilling technology, the electric motor, led to the invention of the electric drill. It is credited to Arthur James Arnot and William Blanch Brain of Melbourne, Australia who patented the electric drill in 1889. In 1895, the first portable handheld drill was created by brothers Wilhem & Carl Fein of Stuttgart, Germany. In 1917 the first trigger-switch, pistol-grip portable drill was patented by Black & Decker. This was the start of the modern drill era. Over the last century the electric drill has been created in a variety of types and multiple sizes for an assortment of specific uses.\n\nThere are many types of drills: some are powered manually, others use electricity (electric drill) or compressed air (\"pneumatic drill\") as the motive power, and a minority are driven by an internal combustion engine (for example, earth drilling augers). Drills with a percussive action (hammer drills) are mostly used in hard materials such as masonry (brick, concrete and stone) or rock. Drilling rigs are used to bore holes in the earth to obtain water or oil. Oil wells, water wells, or holes for geothermal heating are created with large drilling rigs. Some types of hand-held drills are also used to drive screws and other fasteners. Some small appliances that have no motor of their own may be drill-powered, such as small pumps, grinders, etc.\n\n\nThe most common form of corded drill is the pistol-grip. A specialty form is a right-angle drill, used to drill in tight spaces. Power in corded drills is typically supplied by a universal motor, used for its high power to weight ratio.\n\nFor much of the 20th century, many attachments could commonly be purchased to convert corded electric hand drills into a range of other power tools, such as orbital sanders and power saws, more cheaply than purchasing dedocated versions of those tools. As the prices of power tools and suitable electric motors have fallen such attachments have become much less common. A similar practice is currently employed for cordless tools where the battery, the most expensive component, is shared between various compatible devices.\nA cordless drill is an electric drill which uses rechargeable batteries. These drills are available with similar features to an AC mains-powered drill. These are the most common type of drill. They are available in the hammer drill configuration and most have a clutch, which aids in driving screws into various substrates while not damaging them. Also available are right angle drills, which allow a worker to drive screws in a tight space. While 21st century battery innovations allow significantly more drilling, large diameter holes (typically or larger) may drain current cordless drills quickly.\n\nFor continuous use, a worker will have one or more spare battery packs charging while drilling, and quickly swap them instead of having to wait an hour or more for recharging, although there are now Rapid Charge Batteries that can charge in 10–15 minutes.\n\nEarly cordless drills used interchangeable 7.2 V battery packs. Over the years battery voltages have increased, with 18 V drills being most common, but higher voltages are available, such as 24 V, 28 V, and 36 V. This allows these tools to produce as much torque as some corded drills.\n\nCommon battery types of are nickel-cadmium (NiCd) batteries and lithium-ion batteries, with each holding about half the market share. NiCd batteries have been around longer, so they are less expensive (their main advantage), but have more disadvantages compared to lithium-ion batteries. NiCd disadvantages are limited life, self-discharging, environment problems upon disposal, and eventually internally short circuiting due to dendrite growth. Lithium-ion batteries are becoming more common because of their short charging time, longer life, absence of memory effect, and low weight. Instead of charging a tool for an hour to get 20 minutes of use, 20 minutes of charge can run the tool for an hour. Lithium-ion batteries also hold a charge for a significantly longer time than nickel-cadmium batteries, about two years if not used, vs. 1 to 4 months for a nickel-cadmium battery.\n\nThe hammer drill is similar to a standard electric drill, with the exception that it is provided with a hammer action for drilling masonry. The hammer action may be engaged or disengaged as required. Most electric hammer drills are rated (input power) at between 600 and 1100 watts. The efficiency is usually 50-60% i.e. 1000 watts of input is converted into 500-600 watts of output (rotation of the drill and hammering action).\n\nThe hammer action is provided by two cam plates that make the chuck rapidly pulse forward and backward as the drill spins on its axis. This pulsing (hammering) action is measured in Blows Per Minute (BPM) with 10,000 or more BPMs being common. Because the combined mass of the chuck and bit is comparable to that of the body of the drill, the energy transfer is inefficient and can sometimes make it difficult for larger bits to penetrate harder materials such as poured concrete. A standard hammer drill accepts 6 mm (1/4 inch) and 13 mm (1/2 inch) drill bits. The operator experiences considerable vibration, and the cams are generally made from hardened steel to avoid them wearing out quickly. In practice, drills are restricted to standard masonry bits up to 13 mm (1/2 inch) in diameter. A typical application for a hammer drill is installing electrical boxes, conduit straps or shelves in concrete.\n\nThe rotary hammer (also known as a rotary hammer drill, roto hammer drill or masonry drill) combines a primary dedicated hammer mechanism with a separate rotation mechanism, and is used for more substantial material such as masonry or concrete. Generally, standard chucks and drills are inadequate and chucks such as SDS and carbide drills that have been designed to withstand the percussive forces are used. A rotary hammer uses SDS or Spline Shank bits. These heavy bits are adept at pulverising the masonry and drill into this hard material with relative ease. Some styles of this tool are intended for masonry drilling only and the hammer action cannot be disengaged. Other styles allow the drill to be used without the hammer action for normal drilling, or hammering to be used without rotation for chiselling. In 1813 Richard Trevithick designed a steam-driven rotary drill, also the first drill to be powered by steam.\n\nIn contrast to the cam-type hammer drill, a rotary/pneumatic hammer drill accelerates only the bit. This is accomplished through a piston design, rather than a spinning cam. Rotary hammers have much less vibration and penetrate most building materials. They can also be used as \"drill only\" or as \"hammer only\" which extends their usefulness for tasks such as chipping brick or concrete. Hole drilling progress is greatly superior to cam-type hammer drills, and these drills are generally used for holes of 19 mm (3/4 inch) or greater in size. A typical application for a rotary hammer drill is boring large holes for lag bolts in foundations, or installing large lead anchors in concrete for handrails or benches.\n\nA drill press (also known as a pedestal drill, pillar drill, or bench drill) is a style of drill that may be mounted on a stand or bolted to the floor or workbench. Portable models are made, some including a magnetic base. Major components include a base, column (or pillar), adjustable table, spindle, chuck, and drill head, usually driven by an electric motor. The head typically has a set of three handles radiating from a central hub that are turned to move the spindle and chuck vertically. A drill press is typically measured by its \"swing\", calculated as twice the distance from the center of the chuck to the closest edge of the column. Thus, a tool with 4\" between chuck center and column edge is described as an 8\" drill press.\n\nA drill press has a number of advantages over a hand-held drill:\n\nFor most drill presses—especially those meant for woodworking or home use—speed change is achieved by manually moving a belt across a stepped pulley arrangement. Some drill presses add a third stepped pulley to increase the number of available speeds. Modern drill presses can, however, use a variable-speed motor in conjunction with the stepped-pulley system. Medium-duty drill presses such as those used in machine shop (tool room) applications are equipped with a continuously variable transmission. This mechanism is based on variable-diameter pulleys driving a wide, heavy-duty belt. This gives a wide speed range as well as the ability to change speed while the machine is running. Heavy-duty drill presses used for metalworking are usually of the gear-head type described below.\n\nDrill presses are often used for miscellaneous workshop tasks other than drilling holes. This includes sanding, honing, and polishing. These tasks can be performed by mounting sanding drums, honing wheels and various other rotating accessories in the chuck. This can be unsafe in some cases, as the chuck arbor, which may be retained in the spindle solely by the friction of a taper fit, may dislodge during operation if the side loads are too high.\n\nA geared head drill press transmits power from the motor to the spindle through spur gearing inside the machine's head, eliminating a flexible drive belt. This assures a positive drive at all times and minimizes maintenance. Gear head drills are intended for metalworking applications where the drilling forces are higher and the desired speed (RPM) is lower than that used for woodworking.\n\nLevers attached to one side of the head are used to select different gear ratios to change the spindle speed, usually in conjunction with a two- or three-speed motor (this varies with the material). Most machines of this type are designed to be operated on three-phase electric power and are generally of more rugged construction than equivalently sized belt-driven units. Virtually all examples have geared racks for adjusting the table and head position on the column.\n\nGeared head drill presses are commonly found in tool rooms and other commercial environments where a heavy duty machine capable of production drilling and quick setup changes is required. In most cases, the spindle is machined to accept Morse taper tooling for greater flexibility. Larger geared head drill presses are frequently fitted with power feed on the quill mechanism, with an arrangement to disengage the feed when a certain drill depth has been achieved or in the event of excessive travel. Some gear-head drill presses have the ability to perform tapping operations without the need for an external tapping attachment. This feature is commonplace on larger gear head drill presses. A clutch mechanism drives the tap into the part under power and then backs it out of the threaded hole once the proper depth is reached. Coolant systems are also common on these machines to prolong tool life under production conditions.\n\nA radial arm drill press is a large geared head drill press in which the head can be moved along an arm that radiates from the machine's column. As it is possible to swing the arm relative to the machine's base, a radial arm drill press is able to operate over a large area without having to reposition the workpiece. This saves considerable time because it is much faster to reposition the drill head than it is to unclamp, move, and then re-clamp the workpiece to the table. The size of work that can be handled may be considerable, as the arm can swing out of the way of the table, allowing an overhead crane or derrick to place a bulky workpiece on the table or base. A vise may be used with a radial arm drill press, but more often the workpiece is secured directly to the table or base, or is held in a fixture. Power spindle feed is nearly universal with these machines and coolant systems are common. Larger size machines often have power feed motors for elevating or moving the arm. The biggest radial arm drill presses are able to drill holes as large as four inches (101.6 millimeters) diameter in solid steel or cast iron. Radial arm drills are specified by the diameter of the column and the length of the arm. The length of the arm is usually the same as the maximum throat distance. The Radial Arm Drill pictured in this article is a 9-inch column x 3-foot arm. The maximum throat distance of this drill would be approximately 36\", giving a swing of 72\" (6 feet).\n\nA magnetic drill is a portable machine for drilling holes in large and heavy workpieces which are difficult to move or bring to a stationary conventional drilling machine. It has a magnetic base and drills holes with the help of cutting tools like annular cutters (broach cutters) or with twist drill bits. There are various types depending on their operations and specializations, like magnetic drilling cum tapping machines, cordless, pneumatic, compact horizontal, automatic feed, cross table base etc.\n\nMill drills are a lighter alternative to a milling machine. They combine a drill press (belt driven) with the X/Y coordinate abilities of the milling machine's table and a locking collet that ensures that the cutting tool will not fall from the spindle when lateral forces are experienced against the bit. Although they are light in construction, they have the advantages of being space-saving and versatile as well as inexpensive, being suitable for light machining that may otherwise not be affordable.\n\nDrills are used in surgery to remove or create holes in bone; specialties that use them include dentistry, orthopedic surgery and neurosurgery. The development of surgical drill technology has followed that of industrial drilling, including transitions to the use of lasers, endoscopy, use of advanced imaging technologies to guide drilling, and robotic drills.\n\nDrills are often used simply as motors to drive a variety of applications, in much the same way that tractors with generic PTOs are used to power ploughs, mowers, trailers, etc.\n\nAccessories available for drills include:\n\n\nDrilling capacity indicates the maximum diameter a given power drill or drill press can produce in a certain material. It is essentially a proxy for the continuous torque the machine is capable of producing. Typically a given drill will have its capacity specified for different materials, i.e., 10mm for steel, 25mm for wood, etc.\n\nFor example, the maximum recommended capacities for the DeWalt DCD790 cordless drill for specific drill bit types and materials are as follows:\n\n\n"}
{"id": "28937856", "url": "https://en.wikipedia.org/wiki?curid=28937856", "title": "Dyson–Harrop satellite", "text": "Dyson–Harrop satellite\n\nA Dyson–Harrop satellite is a hypothetical megastructure intended for power generation using the solar wind. It is inspired by the Dyson sphere, but much harder to detect from another star system.\n\nThe concept for the so-called Dyson–Harrop satellite begins with a long metal wire loop pointed at the sun. This wire is charged to generate a cylindrical magnetic field that snags the electrons that make up half the solar wind. These electrons get funnelled into a metal spherical receiver to produce a current, which generates the wire's magnetic field – making the system self-sustaining. Any current not needed for the magnetic field powers an infrared laser trained on satellite dishes back on Earth, designed to collect the energy. Earth's air does not absorb infra-red light, so the system would be highly efficient. Back on the satellite, the current has been drained of its electrical energy by the laser – the electrons fall onto a ring-shaped sail, where incoming sunlight can excite them enough to keep the satellite in orbit around the sun.\n\nA relatively small Dyson–Harrop satellite using a 1-centimetre-wide copper wire 300 metres long, a receiver 2 metres wide and a sail 10 metres in diameter, sitting at roughly the same distance from the sun as the Earth, could generate 1.7 megawatts of power – enough for about 1000 family homes in the US. Larger sizes could produce far greater amounts of power, even exceeding the current usage of Earth. Satellites could be placed anywhere in the solar system, and networks of satellites could combine to generate terawatts of power.\n"}
{"id": "33821764", "url": "https://en.wikipedia.org/wiki?curid=33821764", "title": "Ecompass", "text": "Ecompass\n\nAn eCompass is a tilt compensated electronic compass utilizing an accelerometer and a magnetometer. A number of manufacturers, including Motorola, make ecompasses. eCompasses are also found in some laptops such as the HP Spectre\n\nE Compass is a sensor which is present in todays smartphones the e compass sensor acts as a real compass and helpful in some games also."}
{"id": "6596725", "url": "https://en.wikipedia.org/wiki?curid=6596725", "title": "Equatorium", "text": "Equatorium\n\nAn equatorium (plural, equatoria) is an astronomical calculating instrument. It can be used for finding the positions of the Moon, Sun, and planets without calculation, using a geometrical model to represent the position of a given celestial body.\n\nThe earliest extant record of a solar equatorium, that is, one to find the position of the sun, is found in Proclus's fifth-century work \"Hypostasis\", where he gives instructions on how to construct one in wood or bronze.\nAlthough planetary equatoria were also probably made by the ancient Greeks, the first surviving description of one is from the \"Libros del saber de astronomia\" (\"Books of the knowledge of astronomy\"), a Castilian compilation of astronomical works collected under the patronage of Alfonso X of Castile in the thirteenth century, which includes translations of two eleventh century Arabic texts on equatoria by Ibn al‐Samḥ and al-Zarqālī. \"Theorica Planetarum\" (c. 1261-1264) by Campanus of Novara describes the construction of an equatorium, the earliest known description in Latin Europe.\n\nRichard of Wallingford (1292–1336) is known to have built a sophisticated equatorium named \"Albion \" in 1326. It could calculate lunar, solar and planetary\nlongitudes. Unlike most equatoria, the \"Albion\" could also predict eclipses.\nThe device is described in a manuscript and in drawings by the Abbot. It consisted of several rotating disks, showing the courses of the sun, moon and stars. These disks were operated manually. It was not a clockwork mechanism.\n\nThe inventor of the Equatorium, Al-Zarqali, was an Arab Muslim instrument maker, mathematician, and leading astronomer at the time. Al-Zarqali based the equatorium off the Universal Astrolabe, yet made it more accurate and specialized. He was the first to show clearly the motion of the solar apogee, when the sun is furthest from earth, is 12.0 seconds per year (the actual value is 11.8 seconds per year). Al-Zarqali also corrected Ptolemy’s work, calculating that the length of the Mediterranean Sea was at 42°, while Ptolemy said it was 62°. His work was even quoted multiple times in Nicolaus Copernicus's “De Revolutionibus Orbium Celestium” where he discussed the revolution of the celestial orbs. Even though Al-Zaqali’s work was based on the belief that the earth was at the center of the universe, many of his primary concepts still apply to the modern world of astronomy.\n\nThe model of Ptolemy that was used by Al-Zarqali to create the equatorium put the earth at the center of the universe. The Equatorium was based on the notion that the stars were on a large sphere, where the earth is right in the middle of it held in place by air. Ptolemy theorized that all of the stars moved around the earth every single day, and the motion of the sun, moon, and planets surrounded it. He believed that each planet moved on a small sphere or circle, called an epicycle, that moved on a larger sphere or circle, called a “deferent” which contains the earth at the center. The epicycle and deferent are the basis of what the equatorium was built on.\nThe history of the equatorium does not just end after the 11th Century but it inspired a more diverse invention called “The Albion”. The Albion is an astronomical instrument invented by Richard of Wallingford at the beginning of the 14th Century. It has various functional uses such as that of the equatorium for planetary and conjunction computations. It can calculate when eclipses will occur. The Albion is made up of 18 different scales which makes it extremely complex in comparison to the equatorium. The history of this instrument is still disputed to this day, as the only Albion from the past is both unnamed and unmarked.\n\nIn the early 1000 CE (11th Century), the Equatorium was invented by Abū Ishāq Ibrāhīm Al-Zarqālī. To understand where the roots of the Equatorium began, look back to what inspired it: the Astrolabe. The history of the Astrolabe dates back to roughly 220 BC and was invented by Hipparchus. The difference between the two instruments is that the Astrolabe measures the time and position of the sun and stars at a specific location in time. There are also specialized astrolabes for land and for boats. The Astrolabe works by adjusting the moveable components (labels) to a specific date and time. Afterward, line up the astrolabe with the horizon by holding it up and setting one end of the label to where one end is at eye level and the opposite end is pointed towards whichever celestial structure you want to view. In contrast, the equatorium is a rarer astronomical instrument. It is used to calculate the past or future positions of the planets and celestial bodies according to the planetary theory of Ptolemy. What is similar about these tools is that they are both calculating devices that simplify and efficiently make geometrical calculations.\n\nThe equatorium can further be specialized depending on the epicycle. There are three possible epicycles that can be adjusted to serve for planetary positions in three groups: the moon, the stars, and the sun. The sun was considered a planet in the Ptolemaic system, hence why the equatorium could be used to determine its position. Through the use of Ptolemy’s model, astronomers were able to make a single instrument with various capabilities that catered to the belief that the solar system had the earth at the center. In fact, specialized equatorium’s had astrological aspects of medicine, as the orientation of planets gave insight to zodiac signs which helped some doctors cater medical treatments to patients.\n\nAt least 15 minutes was needed to calculate the planetary position with the use of a table for each celestial body. A horoscope of that era would have required the positions of seven astronomical objects, requiring nearly two hours of manual calculation time. \n\n\n"}
{"id": "3133405", "url": "https://en.wikipedia.org/wiki?curid=3133405", "title": "Flow battery", "text": "Flow battery\n\nA flow battery, or redox flow battery (after \"reduction–oxidation\"), is a type of electrochemical cell where chemical energy is provided by two chemical components dissolved in liquids contained within the system and separated by a membrane. Ion exchange (accompanied by flow of electric current) occurs through the membrane while both liquids circulate in their own respective space. Cell voltage is chemically determined by the Nernst equation and ranges, in practical applications, from 1.0 to 2.2 volts.\n\nA flow battery may be used like a fuel cell (where the spent fuel is extracted and new fuel is added to the system) or like a rechargeable battery (where an electric power source drives regeneration of the fuel). While it has technical advantages over conventional rechargeables, such as potentially separable liquid tanks and near unlimited longevity, current implementations are comparatively less powerful and require more sophisticated electronics.\n\nThe energy capacity is a function of the electrolyte volume (amount of liquid electrolyte), and the power is a function of the surface area of the electrodes.\n\nA flow battery is a rechargeable fuel cell in which an electrolyte containing one or more dissolved electroactive elements flows through an electrochemical cell that reversibly converts chemical energy directly to electricity (electroactive elements are \"elements in solution that can take part in an electrode reaction or that can be adsorbed on the electrode\"). Additional electrolyte is stored externally, generally in tanks, and is usually pumped through the cell (or cells) of the reactor, although gravity feed systems are also known. Flow batteries can be rapidly \"recharged\" by replacing the electrolyte liquid (in a similar way to refilling fuel tanks for internal combustion engines) while simultaneously recovering the spent material for re-energization. Many flow batteries use carbon felt electrodes due to its low cost and adequate electrical conductivity, although these electrodes somewhat limit the charge / discharge power due to their low inherent activity towards many redox couples.\n\nIn other words, a flow battery is just like an electrochemical cell, with the exception that the ionic solution (electrolyte) is not stored in the cell around the electrodes. Rather, the ionic solution is stored outside of the cell, and can be fed into the cell in order to generate electricity. The total amount of electricity that can be generated depends on the size of the storage tanks.\n\nFlow batteries are governed by the design principles established by electrochemical engineering.\n\nVarious types of flow cells (batteries) have been developed, including redox, hybrid and membraneless. The fundamental difference between conventional batteries and flow cells is that energy is stored not as the electrode material in conventional batteries but as the electrolyte in flow cells.\n\nThe redox (reduction–oxidation) cell is a reversible cell in which electrochemical components are dissolved in the electrolyte. Redox flow batteries are rechargeable (secondary cells). Because they employ heterogeneous electron transfer rather than solid-state diffusion or intercalation they are more appropriately called fuel cells rather than batteries. In industrial practice, fuel cells are usually, and unnecessarily, considered to be primary cells, such as the / system. The unitized regenerative fuel cell on NASA's Helios Prototype is another reversible fuel cell. The European Patent Organisation classifies redox flow cells (H01M8/18C4) as a sub-class of regenerative fuel cells (H01M8/18). Examples of redox flow batteries are the Vanadium redox flow battery, polysulfide bromide battery (Regenesys), and uranium redox flow battery. Redox fuel cells are less common commercially although many systems have been proposed.\n\nA prototype zinc-polyiodide flow battery has been demonstrated with an energy density of 167 Wh/l (watt-hours per liter). Older zinc-bromide cells reach 70 Wh/l. For comparison, lithium iron phosphate batteries store 233 Wh/l. The zinc-polyiodide battery is claimed to be safer than other flow batteries given its absence of acidic electrolytes, nonflammability and operating range of that does not require extensive cooling circuitry, which would add weight and occupy space. One unresolved issue is zinc build-up on the negative electrode that permeated the membrane, reducing efficiency. Because of the Zn dendrite formation, the Zn-halide batteries cannot operate at high current density (> 20 mA/cm) and thus have limited power density. Adding alcohol to the electrolyte of the ZnI battery can slightly control the problem.\n\nWhen the battery is fully discharged, both tanks hold the same electrolyte solution: a mixture of positively charged zinc ions () and negatively charged iodide ion, I-. When charged, one tank holds another negative ion, polyiodide, I3-. The battery produces power by pumping liquid from external tanks into the battery's stack area where the liquids are mixed. Inside the stack, zinc ions pass through a selective membrane and change into metallic zinc on the stack's negative side. To further increase the energy density of the zinc-iodide flow battery, bromide ions (Br–) are used as the complexing agent to stabilize the free iodine, forming iodine-bromide ions (I2Br–) as a means to free up iodide ions for charge storage.\n\nTraditional flow battery chemistries have both low specific energy (which makes them too heavy for fully electric vehicles) and low specific power (which makes them too expensive for stationary energy storage). However a high power of 1.4 W/cm2 was demonstrated for hydrogen-bromine flow batteries, and a high specific energy (530 Wh/kg at the tank level) was shown for hydrogen-bromate flow batteries\n\nOne system uses organic polymers and a saline solution with a cellulose membrane. The prototype withstood 10000 charging cycles while retaining substantial capacity. The energy density was 10 Wh/l. Current density reached 100 milliamperes/cm2.\n\nThe hybrid flow battery uses one or more electroactive components deposited as a solid layer. In this case, the electrochemical cell contains one battery electrode and one fuel cell electrode. This type is limited in energy by the surface area of the electrode. Hybrid flow batteries include the zinc-bromine, zinc–cerium, lead–acid, and iron-salt flow batteries. Weng et al. reported a Vanadium-Metal hydride rechargeable hybrid flow battery with an experimental OCV of 1.93 V and operating voltage of 1.70 V, very high values among rechargeable flow batteries with aqueous electrolytes. This hybrid battery consists of a graphite felt positive electrode operating in a mixed solution of VOSO and HSO, and a metal hydride negative electrode in KOH aqueous solution. The two electrolytes of different pH are separated by a bipolar membrane. The system demonstrated good reversibility and high efficiencies in coulomb (95%), energy (84%), and voltage (88%). They reported further improvements of this new redox couple with achievements of increased current density, operation of larger 100 cm electrodes, and the operation of 10 large cells in series. Preliminary data using a fluctuating simulated power input tested the viability toward kWh scale storage. Recently, a high energy density Mn(VI)/Mn(VII)-Zn hybrid flow battery has been proposed.\n\nA membraneless battery relies on laminar flow in which two liquids are pumped through a channel. They undergo electrochemical reactions to store or release energy. The solutions stream through in parallel, with little mixing. The flow naturally separates the liquids, eliminating the need for a membrane.\n\nMembranes are often the most costly component and the least reliable components of batteries, as they can corrode with repeated exposure to certain reactants. The absence of a membrane enabled the use of a liquid bromine solution and hydrogen. This combination is problematic when membranes are used, because they form hydrobromic acid that can destroy the membrane. Both materials are available at low cost.\n\nThe design uses a small channel between two electrodes. Liquid bromine flows through the channel over a graphite cathode and hydrobromic acid flows under a porous anode. At the same time, hydrogen gas flows across the anode. The chemical reaction can be reversed to recharge the battery—a first for any membraneless design.\nOne such membraneless flow battery published in August 2013 produced a maximum power density of 7950 W/m, three times as much power as other membraneless systems— and an order of magnitude higher than lithium-ion batteries.\n\nRecently, a macroscale membraneless redox flow battery capable of recharging and recirculation of the same electrolyte streams for multiple cycles has been demonstrated. The battery is based on immiscible organic catholyte and aqueous anolyte liquids, which exhibits high capacity retention and columbic efficiency during cycling.\n\nPrimus Power has developed patented technology in its zinc bromine flow battery, a type of redox flow battery, to eliminate the membrane or separator, which reduces costs and failure rates. The Primus Power membraneless redox flow battery is working in installations in the United States and Asia with a second generation product announced 21 February 2017.\n\nCompared to traditional aqueous inorganic redox flow batteries such as vanadium redox flow batteries and Zn-Br2 batteries, which have been developed for decades, organic redox flow batteries emerged in 2009 and hold great promise to overcome major drawbacks preventing economical and extensive deployment of traditional inorganic redox flow batteries. The primary merit of organic redox flow batteries lies in the tunable redox properties of the redox-active components.\n\nOrganic redox flow batteries could be further classified into two categories: Aqueous Organic Redox Flow Batteries (AORFBs) and Non-aqueous Organic Redox Flow Batteries (NAORFBs). AORFBs use water as solvent for electrolyte materials while NAORFBs employ organic solvents to dissolve redox active materials. Depending on using one or two organic redox active electrolytes as anode and/or cathode, AORFBs and NAORFBs can be further divided into total organic systems and hybrid organic systems that use inorganic materials for anode or cathode. The proof of concept of AORFBs was demonstrated before NAORFBs. In larger-scale energy storage, AORFBs hold much greater potential than NAORFBs because of the former's lower cost, higher current and power performance, as well as safety advantages of aqueous over non-aqueous electrolytes. NAORFBs may find their place in limited special applications for their higher energy density than AORFBs though more challenges are to be overcome in terms of safety, cost of organic solvents, radical induced side reactions, electrolyte crossover and limited life time. The content below mainly covers the representative studies on AORFBs.\n\nQuinones are the basis of some AORFBs. In one study, 1,2-dihydrobenzoquinone-3,5-disulfonic acid (BQDS) and 1,4-dihydrobenzoquinone-2-sulfonic acid (BQS) were employed as cathodes, and conventional Pb/PbSO was the anolyte in an acid AORFB. These first AORFBs are hybrid systems as they use organic redox active materials only for the cathode side. The quinones accepts two units of electrical charge, compared with one in conventional catholyte, implying that such a battery could store twice as much energy in a given volume.\n\n9,10-Anthraquinone-2,7-disulphonic acid (AQDS), also a quinone, has been evaluated as well. AQDS undergoes rapid, reversible two-electron/two-proton reduction on a glassy carbon electrode in sulphuric acid. An aqueous flow battery with inexpensive carbon electrodes, combining the quinone/hydroquinone couple with the / redox couple, yields a peak galvanic power density exceeding 6,000 W/m at 13,000 A/m. Cycling showed >99 % storage capacity retention per cycle. Volumetric energy density was over 20 Wh/l. Anthraquinone-2-sulfonic acid and anthraquinone-2,6-disulfonic acid on the negative side and 1,2-dihydrobenzoquinone- 3,5-disulfonic acid on the positive side avoids the use of hazardous Br. The battery was claimed to last for 1,000 cycles without degradation although no official data were published. While this total organic system appears robust, it has a low cell voltage (ca. 0.55 V) and a low energy density (< 4 Wh/L).\n\nHydrobromic acid used as an electrolyte has been replaced with a far less toxic alkaline solution (1M KOH) and ferrocyanide. The higher pH is less corrosive, allowing the use of inexpensive polymer tanks. The increased electrical resistance in the membrane was compensated by increasing the voltage. The cell voltage was 1.2 V. The cell's efficiency exceeded 99%, while round-trip efficiency measured 84%. The battery has an expected lifetime of at least 1,000 cycles. Its theoretic energy density was 19 Wh per liter. Ferrocyanide's chemically stability in high pH KOH solution without forming Fe(OH)2 or Fe(OH)3 needs to be verified before scale-up.\n\nAnother organic AORFB has been demonstrated methyl viologen as anolyte and 4-hydroxy-2,2,6,6-tetramethylpiperidin-1-oxyl as catholyte, plus sodium chloride and a low-cost anion exchange membrane to enable charging and discharging. This MV/TEMPO system has the highest cell voltage, 1.25 V, and, possibly, lowest capital cost ($180/kWh) reported for AORFBs. The water-based liquid electrolytes were designed as a drop-in replacement for current systems without replacing existing infrastructure. A 600-milliwatt test battery was stable for 100 cycles with nearly 100 percent efficiency at current densities ranging from 20 to 100 mA per square centimeter, with optimal performance rated at 40-50 mA, at which about 70 percent of the battery's original voltage was retained. The significance of the research is that neutral AORFBs can be more environmentally friendly than acid or alkaline AORFBs while showing electrochemical performance comparable to corrosive acidic or alkaline RFBs. The MV/TEMPO AORFB has an energy density of 8.4 Wh/L with the limitation on the TEMPO side. The next step is to identify a high capacity catholyte to match MV (ca. 3.5 M solubility in water, 93.8 Ah/L).\n\nOne flow-battery concept is based on redox active, organic polymers employs viologen and TEMPO with dialysis membranes. The polymer-based redox-flow battery (pRFB) uses functionalized macromolecules (similar to acrylic glass or Styrofoam) being dissolved in water as active material for the anode as well as the cathode. Thereby, metals and strongly corrosive electrolytes – like vanadium salts in sulfuric acid – are avoided and simple dialysis membranes can be employed. The membrane, which separates the cathode and the anode of the flow cell, works like a strainer and is produced much more easily and at lower cost than conventional ion-selective membranes. It retains the big “spaghetti”-like polymer molecules, while allowing the small counterions to pass. The concept may solve the high cost of traditional Nafion membrane but the design and synthesis of redox active polymer with high solubility in water is not trivial.\n\nProton flow batteries (PFB) integrate a metal hydride storage electrode into a reversible proton exchange membrane (PEM) fuel cell. During charging, PFB combines hydrogen ions produced from splitting water with electrons and metal particles in one electrode of a fuel cell. The energy is stored in the form of a solid-state metal hydride. Discharge produces electricity and water when the process is reversed and the protons are combined with ambient oxygen. Metals less expensive than lithium can be used and provide greater energy density than lithium cells.\n\nLithium–sulfur system arranged in a network of nanoparticles eliminates the requirement that charge moves in and out of particles that are in direct contact with a conducting plate. Instead, the nanoparticle network allows electricity to flow throughout the liquid. This allows more energy to be extracted.\n\nIn a semi-solid flow cell, the positive and negative electrodes are composed of particles suspended in a carrier liquid. The positive and negative suspensions are stored in separate tanks and pumped through separate pipes into a stack of adjacent reaction chambers, where they are separated by a barrier such as a thin, porous membrane. The approach combines the basic structure of aqueous-flow batteries, which use electrode material suspended in a liquid electrolyte, with the chemistry of lithium-ion batteries in both carbon-free suspensions and slurries with conductive carbon network. The carbon free semi-solid redox flow battery is also sometimes referred to as Solid Dispersion Redox Flow Battery. Dissolving a material changes its chemical behavior significantly. However, suspending bits of solid material preserves the solid's characteristics. The result is a viscous suspension that flows like molasses.\n\nA wide range of chemistries have been tried for flow batteries.\n\nRedox flow batteries, and to a lesser extent hybrid flow batteries, have the advantages of flexible layout (due to separation of the power and energy components), long cycle life (because there are no solid-to-solid phase transitions), quick response times, no need for \"equalisation\" charging (the overcharging of a battery to ensure all cells have an equal charge) and no harmful emissions. Some types also offer easy state-of-charge determination (through voltage dependence on charge), low maintenance and tolerance to overcharge/overdischarge. Compared to solid-state rechargeable batteries such as Li ion, RFBs, and ARFBs in particular, can operate at much higher current and power densities. These technical merits make redox flow batteries a well-suited option for large-scale energy storage.\n\nOn the negative side, the energy densities vary considerably but are, in general, lower compared to portable batteries, such as the Li-ion.\n\nAlso, compared to nonreversible fuel cells or electrolyzers using similar electrolytic chemistries, flow batteries generally have somewhat lower efficiency.\n\nThe development and economy in going from laboratory to industry-scale is an ongoing process. Component cost is one of several aspects of this process. A sulfur-oxygen-salt chemistry has been demonstrated in laboratory.\n\nFlow batteries are normally considered for relatively large (1 kWh – 10 MWh) stationary applications. These are for: \n\n\n"}
{"id": "3788318", "url": "https://en.wikipedia.org/wiki?curid=3788318", "title": "Fringe shift", "text": "Fringe shift\n\nIn interferometry experiments such as the Michelson–Morley experiment, a fringe shift is the behavior of a pattern of “fringes” when the phase relationship between the component sources change.\n\nA fringe pattern can be created in a number of ways but the stable fringe pattern found in the Michelson type interferometers is caused by the separation of the original source into two separate beams and then recombining them at differing angles of incidence on a viewing surface.\n\nThe interaction of the waves on a viewing surface alternates between constructive interference and destructive interference causing alternating lines of dark and light. In the example of a Michelson Interferometer, a single fringe represents one wavelength of the source light and is measured from the center of one bright line to the center of the next. The physical width of a fringe is governed by the difference in the angles of incidence of the component beams of light, but regardless of a fringe's physical width, it still represents a single wavelength of light.\n\nIn the 1887 Michelson–Morley experiment, the speed that the two beams traveled down the perpendicular arms was expected to be altered by an apparent aether wind caused by the Earth's motion through the luminiferous aether. This time difference was calculated to result in a phase shift of 0.4 wavelengths. This means that as the interferometer's arms were spun to face into and against the ether wind, the vertical fringe lines should have moved across the viewer 0.4 fringe widths left and right for a total of 0.8 fringes from maximum to minimum. Michelson reported that only between one-sixth and one-quarter of the expected reading was found.\n"}
{"id": "25254226", "url": "https://en.wikipedia.org/wiki?curid=25254226", "title": "Furnace roller", "text": "Furnace roller\n\nA furnace roller can be used in many applications, for example in steel manufacturing.\n\nOne common problem is heat loss due to water cooling of roller i.e. cooling of furnace => Higher power needed.\n\nNew technology has open up an opportunity to save money and down time, usage of material that can withstand high temperature oxidation.\n\n"}
{"id": "56435915", "url": "https://en.wikipedia.org/wiki?curid=56435915", "title": "Giant Powder Company", "text": "Giant Powder Company\n\nThe Giant Powder Company was an explosives manufacturing company which operated from the mid 19th century through the first half of the 20th century, located in the San Francisco Bay Area of California. The Giant Powder Company was the first company in the United States to produce dynamite under an exclusive license from Alfred Nobel.\n\nThe company was incorporated in August 1867 by Julius Bandmann of San Francisco for the express purpose of manufacturing Nobel's newly-patented explosive in the United States. Bandmann immediately began construction of his factory in what was then the remote southern part of San Francisco, now Glen Canyon Park in the Glen Park neighborhood of the city. The facility was ready by early 1868, with production commencing in March. The location of this factory is listed as California Historical Landmark number 1002.\n\nOn November 26, 1869, a terrific explosion destroyed the Giant dynamite factory, killing two and injuring nine people. A new facility was subsequently built at another site located in the western part of San Francisco, among the sand dunes and scrub that later became part of the Sunset District (in the vicinity of today's Kirkham, Ortega, 20th, and 32nd Avenues), but another accident destroyed that plant as well.\n\nThe public outcry that ensued from these two accidents prompted the Giant Powder Company to move across the bay to a more remote site in what was then West Berkeley (now Albany), between Fleming Point and Cerrito de San Antonio. The railroad station for the facility was named \"Nobel\". The Judson Manufacturing Co., whose founder and CEO Egbert Judson acquired an interest in Giant, established its chemical works adjacent to the Giant plant to supply it with the acids for manufacturing dynamite. Production was successful here for more than a decade before yet another accidental explosion occurred on July 9, 1892. The blast was widely felt, shattering windows for miles around, including those on the campus of the University of California. Several workers at the Giant plant were killed. The facility was entirely destroyed.\n\nThe Giant Powder Company moved once again, this time to a remote site near Point Pinole, northwest of the city of San Pablo.\n\nIn 1915, the Giant Powder Company was acquired by the Atlas Powder Company. Atlas, as well as the Hercules Powder Company, had been formed in 1912 as part of the settlement of the court-ordered breakup of the DuPont Corporation's explosives monopoly. The new management implemented more rigorous safety measures.\n\nGiant's production facility remained at Point Pinole for decades without any further serious accidents, although there were a few incidents, producing a wide variety of explosives for commercial and military uses until 1960. The area where explosives were manufactured was named \"Nitro\" while the nearby company town was called \"Giant\". The area is still shown on maps as \"Giant\", and a principal thoroughfare through the area is called \"Giant Highway\". The site of the Giant Powder Company at Point Pinole Regional Shoreline is a California Historical Landmark, number 1002-1, marked with a monument and plaque.\n\nOn May 31, 1961, shortly after the Atlas Powder Company closed its Giant facility, it changed its name to Atlas Chemical Industries, Inc. as it started to move away from producing explosives. On July 21, 1971, Atlas was purchased by Imperial Chemical Industries Limited (U.K.) and became its American affiliate under the name ICI Americas Inc.\n\n\n\n"}
{"id": "7626130", "url": "https://en.wikipedia.org/wiki?curid=7626130", "title": "Grammar Girl's Quick and Dirty Tips for Better Writing", "text": "Grammar Girl's Quick and Dirty Tips for Better Writing\n\nGrammar Girl's Quick and Dirty Tips for Better Writing is an educational podcast produced by Mignon Fogarty that was launched in July 2006 and the title of a print book by Fogarty that was released in July 2008. The podcast, which has ranked as high as #2 on iTunes, offers short one-topic English grammar lessons at no charge to subscribers hoping to improve their writing skills.\n\nThe print book offers advice similar to that found in the podcast and reached number nine on the New York Times best-seller list for paperback advice books.\n\nThe Grammar Girl podcast had been downloaded over seven million times by December 2007. According to a Podtrac survey conducted in October 2006, approximately 50% of listeners are male and the other 50% are female. Listeners are encouraged to submit grammar questions by e-mail and voicemail. The majority of episodes are built around these submissions.\n\nThe podcast has won three awards: the 2007 Podcast Award for Best Education Podcast, the 2007 Podcast Peer Award for Favorite Audio Program, and the 2006 Podcast Peer Award for Best Education Podcast.\n\nThe Grammar Girl podcast was the subject of an article in the \"Wall Street Journal\" (November 4–5, 2006), recommended by the German newspaper Bild.de (December 1, 2006), profiled on CNN.com (January 23, 2007), and positively reviewed by the Podcasting Tricks website (November 30, 2006).\n\nIn their end-of-year review, iTunes staff listed the Grammar Girl podcast as a favorite for 2006. The show was also listed as an iTunes People's Choice podcast for 2006, ranking 29th in number of new subscribers for the year. In 2013, iTunes listed Grammar Girl as one of 12 \"Best Classic Podcasts\". \n\nMignon Fogarty appeared on the March 26, 2007 episode of \"The Oprah Winfrey Show\" as a grammar expert. She was on the show to answer a viewer question about the use of possessive apostrophes. The viewer thought a previous show should have been titled \"Oprah's and Gayle's Big Adventure\", but Fogarty confirmed that \"Oprah and Gayle's Big Adventure\" was a correct use of compound possession. She went on to discuss several other common grammar errors, including \"affect vs. effect\" and \"who vs. whom\".\n\nThe Grammar Girl print book reached number nine on the August 17, 2008 New York Times best-seller list for paperback advice books.\n\n"}
{"id": "2991090", "url": "https://en.wikipedia.org/wiki?curid=2991090", "title": "Hodgkin–Huxley model", "text": "Hodgkin–Huxley model\n\nThe Hodgkin–Huxley model, or conductance-based model, is a mathematical model that describes how action potentials in neurons are initiated and propagated. It is a set of nonlinear differential equations that approximates the electrical characteristics of excitable cells such as neurons and cardiac myocytes. It is a continuous time model, unlike, for example, the Rulkov map.\n\nAlan Lloyd Hodgkin and Andrew Fielding Huxley described the model in 1952 to explain the ionic mechanisms underlying the initiation and propagation of action potentials in the squid giant axon. They received the 1963 Nobel Prize in Physiology or Medicine for this work.\n\nThe typical Hodgkin–Huxley model treats each component of an excitable cell as an electrical element (as shown in the figure). The lipid bilayer is represented as a capacitance (C). Voltage-gated ion channels are represented by electrical conductances (\"g\", where \"n\" is the specific ion channel) that depend on both voltage and time. Leak channels are represented by linear conductances (\"g\"). The electrochemical gradients driving the flow of ions are represented by voltage sources (\"E\") whose voltages are determined by the ratio of the intra- and extracellular concentrations of the ionic species of interest. Finally, ion pumps are represented by current sources (\"I\"). The membrane potential is denoted by \"V\".\n\nMathematically, the current flowing through the lipid bilayer is written as\n"}
{"id": "59067911", "url": "https://en.wikipedia.org/wiki?curid=59067911", "title": "Hybrid Electric Road Train", "text": "Hybrid Electric Road Train\n\nThe Hybrid Electric Road Train (HERT) is a hybrid trackless train developed by the Department of Science and Technology of the Philippines for public transport.\n\nThe Hybrid Electric Road Train (HERT) was developed by the Metals Industry Research and Development Center (MIRDC) of the Department of Science and Technology (DOST) and is part of the science agency's Advanced Transport Program under the Makina at Teknolohiya Para Sa Bayan (MakiBayan; ) Program. It was designed by Filipino engineers using locally available parts. It was meant to be a means for an alternative mass transport system in metropolitan areas experiencing heavy traffic congestion. The HERT consists of five interlinked coaches with air-conditioning and runs on a combination of diesel fuel and electrical power via a 260-battery generator.\n\nDescribed as a \"road train\", the Hybrid Electric Road Train operates on roads and does not run on railways. It measures long, has a maximum speed of . Four of the train's coaches are meant for transporting passengers while a lone coach hosts the engine. Each coach has a capacity of 60 people for a total capacity of 240 passengers per trip. The HERT devises a regenerative brake.\n\nThe Hybrid Electric Road Train was officially launched by the DOST on August 22, 2014. and had its \"soft launching\" at the Clark Freeport Zone on June 25, 2015 at the Clark Freeport Parade Ground. where it was tested.\n\nIn May 2015, the representatives of local government of Cebu City and the DOST signed a memorandum of understanding to work on the integration of the HERT into the city's public transport system. It was also demonstrated during the opening day of the National Science and Technology Week in Pasay in July 2015.\n\nBy 2016, the train with modifications has already been adopted by the Clark Development Corporation to serve employees of its 1,000 business locators at the Clark Freeport and Special Economic Zone. On February 5, 2016, the train was demonstrated as part of EDSA Evolution, a road-sharing project by the Bayanihan sa Daan Movement and various government agencies. As part of the demonstration, passengers were transported from the SM Mall of Asia in Pasay to the Museo Pambata at the Luneta Park in Manila.\n\nThe HERT had its maiden demonstration run in General Santos on November 15, 2017 as a prelude to a trial run of the vehicle for as long as five months.\n"}
{"id": "45381972", "url": "https://en.wikipedia.org/wiki?curid=45381972", "title": "IMC Content Studio", "text": "IMC Content Studio\n\nIMC Content Studio is an educational authoring system, especially the production of content for e-learning areas such as MOOCs. The program is developed and distributed by IMC AG, a German company based in Saarbrücken. The software is used by college and university teachers in adult education and continuing education, in particular at occupational training institutions such as Corporate Universities.\n\nAs is common with authoring systems, the users are usually not software experts. Therefore, authoring software programs are designed to be intuitive and easily understood. The interface of IMC Content Studio resembles the design of Microsoft products. One example is the ribbon, used in Microsoft Office since 2007 and Windows 8. The program allows the import of Microsoft Power Point templates and slides.\n\nBeside producing content for e-learning, the software can be used to design graphic novels, interactive books and presentations. In addition to visual content, the program offers the option of producing audio-content via speech synthesis.\n\nThe software is based on HTML 5, which allows it to run on personal computers and also on tablets and smartphones.\n\nThe program is compliant with the SCORM standard drawn up by the Pentagon authority ADL for e-learning, as well as with the newer standard PENS of the AICC Committee, jointly developed by the European and US aviation industry.\n\nThe software interface is available in more than 30 languages.\n\nThe usage range varies from basic training, often with multiple choice tests, up to complex MOOCs at Corporate Universities.\n\n"}
{"id": "643965", "url": "https://en.wikipedia.org/wiki?curid=643965", "title": "Kyota Sugimoto", "text": "Kyota Sugimoto\n\nKyota Sugimoto was born in Okayama prefecture in 1882. Because of his desire to become a specialist in communication technology, he entered the Training Institute for Communication Technology in Osaka, and completed his studies at the training institute in 1900. At that time, typewriters were already commonly used in Europe and America, but no practical type of typewriter had been developed yet for the Japanese language, which would make it possible to write Japanese (Kanji) without using a pen. Because a typewriter which could be used to type Japanese would thus be very useful if it could be used with the large number of Japanese characters (unlike the 25-30 or so letters of e.g. various European language alphabets), people were hoping that such a typewriter would be invented.\n\nAfter he completed his studies at the training institute, Kyota Sugimoto started working in the letterpress technology field, and then turned his attention to development of a typewriter for text in Japanese. At that time, typewriters which could be used to write Japanese utilized characters arranged either on a cylindrical surface or on an arc-shaped surface, but either way only a few characters were available. In order to really adapt typewriters to kanji, which has a huge number of characters, Kyota Sugimoto carefully considered the nature of this writing system, including the frequency of use of characters used in public documents. The 2,400 characters chosen as a result were arranged by classification on a character carriage, and the chosen character was raised by a type bar that could move forward and backward and left and right. The character was then typed against a cylindrical paper supporter. He obtained the patent rights to the Japanese typewriter that he invented, first in Japan (1915, Patent N° 27877 ) and somewhat later in the USA (1917, Patent N° 1245633 ).\n\nThis invention amounted to an epoch making design on which current typewriters for Japanese are based, contributing greatly to efficient processing of documents and creation of different types of documents. In year 1953 he was decorated with the Blue Ribbon Award, and in year 1965 he received the Small Asahi Ribbon Award. He died in year 1972.\n\nUntil the popularization of word processor technology, the Japanese typewriter contributed greatly to increased efficiency of document preparation at Japanese companies and government Offices.\n"}
{"id": "168435", "url": "https://en.wikipedia.org/wiki?curid=168435", "title": "Light meter", "text": "Light meter\n\nA light meter is a device used to measure the amount of light. In photography, a light meter is often used to determine the proper exposure for a photograph. Typically a light meter will include either digital or analog electronic circuit, which allows the photographer to determine which shutter speed and f-number should be selected for an optimum exposure, given a certain lighting situation and film speed.\n\nLight meters are also used in the fields of cinematography and scenic design, in order to determine the optimum light level for a scene. They are used in the general field of architectural lighting design to verify proper installation and performance of a building lighting system, and in assessing the light levels for growing plants. \n\nThe earliest type of light meters were called \"extinction meters\" and contained a numbered or lettered row of neutral density filters of increasing density. The photographer would position the meter in front of his subject and note the filter with the greatest density that still allowed incident light to pass through. The letter or number corresponding to the filter was used as an index into a chart of appropriate aperture and shutter speed combinations for a given film speed.\n\nExtinction meters suffered from the problem that they depended on the light sensitivity of the human eye (which can vary from person to person) and subjective interpretation.\n\nLater meters removed the human element and relied on technologies incorporating selenium, CdS, and silicon photodetectors.\n\nSelenium and silicon light meters use sensors that are photovoltaic: they \"generate\" a voltage proportional to light exposure. Selenium sensors generate enough voltage for direct connection to a meter; they need no battery to operate and this made them very convenient in completely mechanical cameras. Selenium sensors however cannot measure low light accurately (ordinary lightbulbs can take them close to their limits) and are altogether unable to measure very low light, such as candlelight, moonlight, starlight etc. Silicon sensors need an amplification circuit and require a power source such as batteries to operate. CdS light meters use a photoresistor sensor whose electrical resistance changes proportionately to light exposure. These also require a battery to operate. Most modern light meters use silicon or CdS sensors. They indicate the exposure either with a needle galvanometer or on an LCD screen.\n\nMany modern consumer still and video cameras include a built-in meter that measures a scene-wide light level and are able to make an approximate measure of appropriate exposure based on that. Photographers working with controlled lighting and cinematographers use handheld light meters to precisely measure the light falling on various parts of their subjects and use suitable lighting to produce the desired exposure levels.\n\nThere are two general types of light meters: reflected-light and incident-light. Reflected-light meters measure the light \"reflected by the scene\" to be photographed. All in-camera meters are reflected-light meters. Reflected-light meters are calibrated to show the appropriate exposure for “average” scenes. An unusual scene with a preponderance of light colors or specular highlights would have a higher reflectance; a reflected-light meter taking a reading would incorrectly compensate for the difference in reflectance and lead to underexposure. Badly underexposed sunset photos are common exactly because of this effect: the brightness of the setting sun fools the camera's light meter and, unless the in-camera logic or the photographer take care to compensate, the picture will be grossly underexposed and dull.\n\nThis pitfall (but not in the setting-sun case) is avoided by incident-light meters which measure the amount of light \"falling on the subject\" using an integrating sphere (usually, a translucent hemispherical plastic dome is used to approximate this) placed on top of the light sensor. Because the incident-light reading is independent of the subject's reflectance, it is less likely to lead to incorrect exposures for subjects with unusual average reflectance. Taking an incident-light reading requires placing the meter at the subject's position and pointing it in the general direction of the camera, something not always achievable in practice, e.g., in landscape photography where the subject distance approaches infinity.\n\nAnother way to avoid under- or over-exposure for subjects with unusual reflectance is to use a spot meter: a reflected-light meter that measures light in a very tight cone, typically with a one degree circular angle of view. An experienced photographer can take multiple readings over the shadows, midrange and highlights of the scene to determine optimal exposure, using systems like the Zone System.\n\nMany modern cameras include sophisticated multi-segment metering systems that measure the luminance of different parts of the scene to determine the optimal exposure. When using a film whose spectral sensitivity is not a good match to that of the light meter, for example orthochromatic black-and-white or infrared film, the meter may require special filters and re-calibration to match the sensitivity of the film.\n\nThere are other types of specialized photographic light meters. Flash meters are used in flash photography to verify correct exposure. Color meters are used where high fidelity in color reproduction is required. Densitometers are used in photographic reproduction.\n\nIn most cases, an incident-light meter will cause a medium tone to be\nrecorded as a medium tone, and a reflected-light meter will cause\n\"whatever is metered\" to be recorded as a medium tone. What\nconstitutes a “medium tone” depends on meter calibration and\nseveral other factors, including film processing or digital image conversion.\n\nMeter calibration establishes the relationship between subject lighting and recommended camera settings. The calibration of photographic light meters is\ncovered by .\n\nFor reflected-light meters, camera settings are related to ISO speed and\nsubject luminance by the reflected-light exposure equation:\n\nformula_1\n\nwhere\n\n\nFor incident-light meters, camera settings are related to ISO speed and\nsubject illuminance by the incident-light exposure equation:\n\nwhere\n\n\nDetermination of calibration constants has been largely subjective;\nThe constants formula_6 and formula_9 shall be chosen by\nstatistical analysis of the results of a large number of tests carried out\nto determine the acceptability to a large number of observers, of a number\nof photographs, for which the exposure was known, obtained under various\nconditions of subject manner and over a range of luminances.\nIn practice, the variation of the calibration constants among manufacturers\nis considerably less than this statement might imply, and values have\nchanged little since the early 1970s.\n\nof 10.6 to 13.4 with\nluminance in cd/m². Two values for formula_6 are in common\nuse: 12.5 (Canon, Nikon, and\nSekonic) and 14 (Minolta, Kenko, and Pentax); the\ndifference between the two values is approximately 1/6\nEV.\n\nThe earliest calibration standards were developed for use with\nwide-angle averaging reflected-light meters\n(Jones and Condit 1941). Although wide-angle average\nmetering has largely given way to other metering sensitivity patterns\n(e.g., spot, center-weighted, and multi-segment), the values for\nformula_6 determined for wide-angle averaging meters have remained.\n\nThe incident-light calibration constant depends on the type of light\nreceptor. Two receptor types are common: flat (cosine-responding) and\nhemispherical (cardioid-responding). With a flat receptor,\nformula_9 of 240 to 400 with\nilluminance in lux; a value of 250 is commonly used. A flat receptor\ntypically is used for measurement of lighting ratios, for measurement of\nilluminance, and occasionally, for determining exposure for a flat subject.\n\nFor determining practical photographic exposure, a hemispherical receptor has\nproven more effective. Don Norwood, inventor of incident-light exposure\nmeter with a hemispherical receptor, thought that a sphere was a reasonable\nrepresentation of a photographic subject. According to his patent\n(Norwood 1938), the objective was\nto provide an exposure meter which is substantially uniformly\nresponsive to light incident upon the photographic subject from practically\nall directions which would result in the reflection of light to the camera\nor other photographic register.\nand the meter provided for \"measurement of the effective illumination obtaining at the position\nof the subject.\"\n\nWith a hemispherical receptor, \nrecommends a range for\nformula_9 of 320 to 540 with illuminance in lux; in practice, values\ntypically are between 320 (Minolta) and 340 (Sekonic). The relative\nresponses of flat and hemispherical receptors depend upon the number and type\nof light sources; when each receptor is pointed at a small light\nsource, a hemispherical receptor with formula_9 = 330 will indicate an\nexposure approximately 0.40 step greater than that indicated by a flat\nreceptor with formula_9 = 250. With a slightly revised definition of illuminance,\nmeasurements with a hemispherical receptor indicate “effective scene\nilluminance.”\n\nIt is commonly stated that reflected-light meters are calibrated to an 18%\nreflectance, but the calibration has nothing to do with\nreflectance, as should be evident from the exposure formulas. However,\nsome notion of reflectance is implied by a comparison of incident- and\nreflected-light meter calibration.\n\nCombining the reflected-light and incident-light exposure equations and rearranging gives\n\nReflectance formula_20 is defined as\n\nA uniform perfect diffuser (one following Lambert's cosine law)\nof luminance formula_4 emits a flux density of\nformula_23formula_4; reflectance then is\n\nIlluminance is measured with a flat receptor. It is straightforward to compare an incident-light measurement using a flat receptor with a\nreflected-light measurement of a uniformly illuminated flat surface of\nconstant reflectance. Using values of 12.5 for formula_6 and 250 for\nformula_9 gives\n\nWith a formula_6 of 14, the reflectance would be 17.6%, close to that of a standard 18% neutral test card. In theory, an incident-light\nmeasurement should agree with a reflected-light measurement of a test card\nof suitable reflectance that is perpendicular to the direction to the\nmeter. However, a test card seldom is a uniform diffuser, so incident- and\nreflected-light measurements might differ slightly.\n\nIn a typical scene, many elements are not flat and are at various\norientations to the camera, so that for practical photography, a\nhemispherical receptor usually has proven more effective for determining\nexposure. Using values of 12.5 for\nformula_6 and 330 for formula_9 gives\n\nWith a slightly revised definition of reflectance, this result can be taken\nas indicating that the average scene reflectance is approximately 12%. A\ntypical scene includes shaded areas as well as areas that receive direct\nillumination, and a wide-angle averaging reflected-light meter responds to\nthese differences in illumination as well as differing reflectances of\nvarious scene elements. Average scene reflectance then would be\n\nwhere “effective scene illuminance” is that measured by a meter\nwith a hemispherical receptor.\n\nto be measured by aiming the receptor at a transilluminated diffuse surface, and for incident-light calibration to be measured by aiming the receptor at a point source in a darkened room. For a perfectly diffusing test card and perfectly diffusing flat receptor, the comparison between a reflected-light measurement and an incident-light measurement is valid for any position of\nthe light source. However, the response of a hemispherical receptor to an off-axis light source is approximately that of a cardioid rather than a cosine, so the 12% “reflectance” determined for an incident-light meter with a hemispherical receptor is valid only when the\nlight source is on the receptor axis.\n\nCalibration of cameras with internal meters is covered by\nnonetheless, many manufacturers specify (though seldom state) exposure\ncalibration in terms of formula_6, and many calibration instruments\n(e.g., Kyoritsu-Arrowin multi-function camera testers ) use the specified\nformula_6 to set the test parameters.\n\nIf a scene differs considerably from a statistically average scene, a\nwide-angle averaging reflected-light measurement may not indicate the\ncorrect exposure. To simulate an average scene, a substitute measurement\nsometimes is made of a neutral test card, or \"gray card\".\n\nAt best, a flat card is an approximation to a three-dimensional scene,\nand measurement of a test card may lead to underexposure unless adjustment\nis made. The instructions for a Kodak neutral test card recommend that\nthe indicated exposure be increased by ½ step for a frontlighted scene in\nsunlight. The instructions also recommend that the test card be held\nvertically and faced in a direction midway between the Sun and the camera;\nsimilar directions are also given in the \"Kodak Professional Photoguide\".\nThe combination of exposure increase and the card orientation gives\nrecommended exposures that are reasonably close to those given by an\nincident-light meter with a hemispherical receptor when metering with an\noff-axis light source.\n\nIn practice, additional complications may arise. Many neutral test cards\nare far from perfectly diffuse reflectors, and specular reflections can\ncause increased reflected-light meter readings that, if followed, would\nresult in underexposure. It is possible that the neutral test card\ninstructions include a correction for specular reflections.\n\nLight meters or light detectors are also used in illumination. Their purpose is to measure the illumination level in the interior and to switch off or reduce the output level of luminaires. This can greatly reduce the energy burden of the building by significantly increasing the efficiency of its lighting system. It is therefore recommended to use light meters in lighting systems, especially in rooms where one cannot expect users to pay attention to manually switching off the lights. Examples include hallways, stairs, and big halls.\n\nThere are, however, significant obstacles to overcome in order to achieve a successful implementation of light meters in lighting systems, of which user acceptance is by far the most formidable. Unexpected or too frequent switching and too bright or too dark rooms are very annoying and disturbing for users of the rooms. Therefore, different switching algorithms have been developed:\n\n\n\n"}
{"id": "1532037", "url": "https://en.wikipedia.org/wiki?curid=1532037", "title": "Materials recovery facility", "text": "Materials recovery facility\n\nA materials recovery facility, materials reclamation facility, materials recycling facility or Multi re-use facility (MRF, pronounced \"murf\") is a specialized plant that receives, separates and prepares recyclable materials for marketing to end-user manufacturers. Generally, there are two different types: clean and dirty materials recovery facilities.\n\nA \"clean MRF\" accepts recyclable comingled materials that have already been separated at the source from municipal solid waste generated by either residential or commercial sources. There are a variety of clean MRFs. The most common are single stream where all recyclable material is mixed, or dual stream MRFs, where source-separated recyclables are delivered in a mixed container stream (typically glass, ferrous metal, aluminum and other non-ferrous metals, PET [No.1] and HDPE [No.2] plastics) and a mixed paper stream including corrugated cardboard boxes, newspapers, magazines, office paper and junk mail. Material is sorted to specifications, then baled, shredded, crushed, compacted, or otherwise prepared for shipment to market.\n\nA mixed-waste processing system, sometimes referred to as a dirty MRF, accepts a mixed solid waste stream and then proceeds to separate out designated recyclable materials through a combination of manual and mechanical sorting. The sorted recyclable materials may undergo further processing required to meet technical specifications established by end-markets while the balance of the mixed waste stream is sent to a disposal facility such as a landfill. Today, MWPFs are attracting renewed interest as a way to address low participation rates for source-separated recycling collection systems and prepare fuel products and/or feedstocks for conversion technologies. MWPFs can give communities the opportunity to recycle at much higher rates than has been demonstrated by curbside or other waste collection systems. Advances in technology make today’s MWPF different and, in many respects better, than older versions. \n\nThe percentage of residuals (unrecoverable recyclable or non-program materials) from a properly operated clean MRF supported by an effective public outreach and education program should not exceed 10% by weight of the total delivered stream and in many cases it can be significantly below 5%. A dirty MRF recovers between 5% and 45% of the incoming material as recyclables, then the remainder is landfilled or otherwise disposed. A dirty MRF can be capable of higher recovery rates than a clean MRF, since it ensures that 100% of the waste stream is subjected to the sorting process, and can target a greater number of materials for recovery than can usually be accommodated by sorting at the source. However, the dirty MRF process results in greater contamination of recyclables, especially of paper. Furthermore, a facility that accepts mixed solid waste is usually more challenging and more expensive to site. Operational costs can be higher because it is more labor-intensive.\n\nAround 2004, new mechanical biological treatment technologies were beginning to utilise \"wet MRFs\". These combine a dirty MRF with water, which acts to densify, separate and clean the output streams. It also hydrocrushes and dissolves biodegradable organics in solution to make them suitable for anaerobic digestion.\n\nIn the United States, modern MRF's began in the 1970s. Peter Karter established Resource Recovery Systems, Inc. in Branford, Connecticut, the \"first materials recovery facility (MRF)\" in the US. \n\n\n"}
{"id": "42117977", "url": "https://en.wikipedia.org/wiki?curid=42117977", "title": "Measuring moisture content using time-domain reflectometry", "text": "Measuring moisture content using time-domain reflectometry\n\nTime-domain reflectometry or TDR is a measurement technique which correlates the frequency-dependent electric and dielectric properties of materials such as soil, agrarian products, snow, wood or concrete to their moisture content.\n\nMeasurement usually involves inserting a sensor into the substance to be tested and then applying either Standard Waveform Analysis to determine the average moisture content along the sensor or Profile Analysis to provide moisture content at discrete points along the sensor. A spatial location can be achieved by appropriate installation of several sensors.\n\nIn the waveform analysis a sensor (usually a probe) is placed in the material to be tested. The sensor contains a waveguide consisting of two or three parallel wires which is connected via a coaxial cable to a voltage pulse generator which sends precisely defined voltage pulses into the sensor. As the pulse travels along the waveguide its progress varies depending on the moisture content of the material being examined. When the pulse reaches the end of the waveguide it is reflected. This reflection is visualised in a TDR waveform using an oscilloscope connected to the sensor. The intensity of the pulse in the probe is measured and related to moisture content, with higher voltage indicating an increase of moisture. By comparing the measured reflection to the initial pulse the average moisture content and relative permitivity of the sample can be calculated by using an equivalent circuit as a reference.\n\nStandard waveform analysis can be used either manually (hand held instruments) or automatically for monitoring moisture content in several areas such as hydrology, agriculture and construction.\n\nStandard Waveform Analysis is unable to provide a spatial moisture profile. More sophisticated methods such as Profile Analysis are required. This method uses a variety of techniques to add spatial information to the measurement results.\n\n\n\n\nProfile analysis allows fully automatic measurement and monitoring of spatial moisture content and thus a leak monitoring of building foundations, landfill barriers and geological repositories in salt mines.\n\n\n"}
{"id": "17130797", "url": "https://en.wikipedia.org/wiki?curid=17130797", "title": "Microchannel (microtechnology)", "text": "Microchannel (microtechnology)\n\nMicrochannel in microtechnology is a channel with a hydraulic diameter below 1 mm. Microchannels are used in fluid control (see Microfluidics) and heat transfer (see Micro heat exchanger). The concept of the microchannel was proposed for the first time by Tuckerman and Pease. They suggested an effective method for designing microchannels in the laminar and fully developed flow.\n\n"}
{"id": "32068219", "url": "https://en.wikipedia.org/wiki?curid=32068219", "title": "Nottingham Asphalt Tester", "text": "Nottingham Asphalt Tester\n\nThe Nottingham Asphalt Tester (NAT) is equipment used for rapid determination of modulus, permanent deformation and fatigue of bituminous mixtures using cylindrical specimens that are cored from the highway or prepared in laboratory. These mechanical properties are essential to people involved in the production of roads and the development of materials used in road construction. NAT's are used across the world by materials testing laboratories, universities, oil companies, regional laboratories, contractors and consulting engineers.\n\nThe NAT was invented in the 1980s at the University of Nottingham by Keith Cooper, who later founded Cooper Research Technology Ltd.\n"}
{"id": "20832779", "url": "https://en.wikipedia.org/wiki?curid=20832779", "title": "Oil Shale (journal)", "text": "Oil Shale (journal)\n\nOil Shale is a quarterly peer-reviewed scientific journal covering research in petrology, especially concerning oil shale. The journal covers geology, mining, formation, composition, methods of processing, combustion, economics, and environmental protection related to oil shale. It is abstracted and indexed in the Science Citation Index. The editor-in-chief is Anto Raukas.\n\nThe plan for publishing an oil shale journal arose in 1983 in the Estonian Academy of Sciences and the journal was established in 1984 as the journal of the Academy of Sciences of the USSR and the Estonian Academy of Sciences. Publishing was financed by the Academy of Sciences of the USSR and administered by the Institute of Chemistry of the Estonian Academy of Sciences. It was published by the publishing house Perioodika in Tallinn.\n\nAt first, the journal was published in Russian under the name \"Горючие Сланцы\" (translit. Goryutchie Slantsy). However, there was the permission from Soviet authorities to use the English-written subtitle \"Oil Shale\" and to add English summaries to Russian papers. The first editor-in-chief was Ilmar Öpik. The financing by the Academy of Sciences of the USSR ended in the beginning of 1990s after Estonia regained independence. As of result, a new editorial board was selected, which decided to reform the journal into an international English-language journal. The publishing was mainly sponsored by oil shale companies and private persons.\n\nIn 1994, the journal was indexed in ISI products and since then published by the Estonian Academy Publishers. Since 1998, the publishing is financed through Estonia's state budget.\n"}
{"id": "22860835", "url": "https://en.wikipedia.org/wiki?curid=22860835", "title": "PRIMA (Indonesia)", "text": "PRIMA (Indonesia)\n\nPRIMA is one of the interbank networks in Indonesia. PRIMA is owned by PT Rintis Sejahtera. PT Rintis Sejahtera is a Satellite Communication services provider that transmitting digital information within the region and around the world. Before the creation of ATM PRIMA, this network was known as ATM BCA network that worked as the ATM network for Bank Central Asia.\n\n\n\n"}
{"id": "203505", "url": "https://en.wikipedia.org/wiki?curid=203505", "title": "Pictorialism", "text": "Pictorialism\n\nPictorialism is the name given to an international style and aesthetic movement that dominated photography during the later 19th and early 20th centuries. There is no standard definition of the term, but in general it refers to a style in which the photographer has somehow manipulated what would otherwise be a straightforward photograph as a means of \"creating\" an image rather than simply recording it. Typically, a pictorial photograph appears to lack a sharp focus (some more so than others), is printed in one or more colors other than black-and-white (ranging from warm brown to deep blue) and may have visible brush strokes or other manipulation of the surface. For the pictorialist, a photograph, like a painting, drawing or engraving, was a way of projecting an emotional intent into the viewer's realm of imagination.\n\nPictorialism as a movement thrived from about 1885 to 1915, although it was still being promoted by some as late as the 1940s. It began in response to claims that a photograph was nothing more than a simple record of reality, and transformed into an international movement to advance the status of all photography as a true art form. For more than three decades painters, photographers and art critics debated opposing artistic philosophies, ultimately culminating in the acquisition of photographs by several major art museums.\n\nPictorialism gradually declined in popularity after 1920, although it did not fade out of popularity until the end of World War II. During this period the new style of photographic Modernism came into vogue, and the public's interest shifted to more sharply focused images. Several important 20th-century photographers began their careers in a pictorialist style but transitioned into sharply focused photography by the 1930s.\n\nPhotography as a technical process involving the development of film and prints in a darkroom originated in the early 19th century, with the forerunners of traditional photographic prints coming into prominence around 1838 to 1840. Not long after the new medium was established, photographers, painters and others began to argue about the relationship between the scientific and artistic aspects of the medium. As early as 1853, English painter William John Newton proposed that the camera could produce artistic results if the photographer would keep an image slightly out of focus. Others vehemently believed that a photograph was equivalent to the visual record of a chemistry experiment. Photography historian Naomi Rosenblum points out that \"the dual character of the medium — its capacity to produce both art and document — [was] demonstrated soon after its discovery ... Nevertheless, a good part of the nineteenth century was spent debating which of these directions was the medium's true function.\"\n\nThese debates reached their peak during the late nineteenth and early twentieth centuries, culminating in the creation of a movement that is usually characterized as a particular style of photography: pictorialism. This style is defined first by a distinctly personal expression that emphasizes photography's ability to create visual beauty rather than simply record facts. However, recently historians have recognized that pictorialism is more than just a visual style. It evolved in direct context with the changing social and cultural attitudes of the time, and, as such, it should not be characterized simply as a visual trend. One writer has noted that pictorialism was \"simultaneously a movement, a philosophy, an aesthetic and a style.\"\n\nContrary to what some histories of photography portray, pictorialism did not come about as the result of a linear evolution of artistic sensibilities; rather, it was formed through \"an intricate, divergent, often passionately conflicting barrage of strategies.\" While photographers and others debated whether photography could be art, the advent of photography directly affected the roles and livelihoods of many traditional artists. Prior to the development of photography, a painted miniature portrait was the most common means of recording a person's likeness. Thousands of painters were engaged in this art form. But photography quickly negated the need for and interest in miniature portraits. One example of this effect was seen at the annual exhibition of the Royal Academy in London; in 1830 more than 300 miniature paintings were exhibited, but by 1870 only 33 were on display. Photography had taken over for one type of art form, but the question of whether photography itself could be artistic had not been resolved.\n\nSome painters soon adopted photography as a tool to help them record a model's pose, a landscape scene or other elements to include in their art. It's known that many of the great 19th-century painters, including Delacroix, Courbet, Manet, Degas, Cézanne, and Gauguin, took photographs themselves, used photographs by others and incorporated images from photographs into their work. While heated debates about the relationship between photography and art continued in print and in lecture halls, the distinction between a photographic image and a painting became more and more difficult to discern. As photography continued to develop, the interactions between painting and photography became increasingly reciprocal. More than a few pictorial photographers, including Alvin Langdon Coburn, Edward Steichen, Gertrude Käsebier, Oscar Gustave Rejlander, and Sarah Choate Sears, were originally trained as painters or took up painting in addition to their photographic skills.\n\nIt was during this same period that cultures and societies around the world were being affected by a rapid increase in intercontinental travel and commerce. Books and magazines published on one continent could be exported and sold on another with increasing ease, and the development of reliable mail services facilitated individual exchanges of ideas, techniques and, most importantly for photography, actual prints. These developments led to pictorialism being \"a more international movement in photography than almost any other photographic genre.\" Camera clubs in the U.S., England, France, Germany, Austria, Japan and other countries regularly lent works to each other's exhibitions, exchanged technical information and published essays and critical commentaries in one another's journals. Led by The Linked Ring in England, the Photo-Secession in the U.S., and the Photo-Club de Paris in France, first hundreds and then thousands of photographers passionately pursued common interests in this multi-dimensional movement. Within the span of little more than a decade, notable pictorial photographers were found in Western and Eastern Europe, North America, Asia and Australia.\n\nFor the first forty years after a practical process of capturing and reproducing images was invented, photography remained the domain of a highly dedicated group of individuals who had expert knowledge of and skills in science, mechanics and art. To make a photograph, a person had to learn a great deal about chemistry, optics, light, the mechanics of cameras and how these factors combine to properly render a scene. It was not something that one learned easily or engaged in lightly, and, as such, it was limited to a relatively small group of academics, scientists and professional photographers.\nAll of that changed in a few years' time span. In 1888 George Eastman introduced the first handheld amateur camera, the Kodak camera. It was marketed with the slogan \"You press the button, we do the rest.\" The camera was pre-loaded with a roll of film that produced about 100 2.5\" round pictures exposures, and it could easily be carried and handheld during its operation. After all of the shots on the film were exposed, the whole camera was returned to the Kodak company in Rochester, New York, where the film was developed, prints were made, and new photographic film was placed inside. Then the camera and prints were returned to the customer, who was ready to take more pictures.\n\nThe impact of this change was enormous. Suddenly almost anyone could take a photograph, and within the span of a few years photography became one of the biggest fads in the world. Photography collector Michael Wilson observed \"Thousands of commercial photographers and a hundred times as many amateurs were producing millions of photographs annually ... The decline in the quality of professional work and the deluge of snapshots (a term borrowed from hunting, meaning to get off a quick shot without taking the time to aim) resulted in a world awash with technically good but aesthetically indifferent photographs.\"\n\nConcurrent with this change was the development of national and international commercial enterprises to meet the new demand for cameras, films and prints. At the 1893 World Columbian Exposition in Chicago, which attracted more than 27 million people, photography for amateurs was marketed at an unprecedented scale. There were multiple large exhibits displaying photographs from around the world, many camera and darkroom equipment manufacturers showing and selling their latest goods, dozens of portrait studios and even on-the-spot documentation of the Exposition itself. Suddenly photography and photographers were household commodities.\n\nMany serious photographers were appalled. Their craft, and to some their art, was being co-opted by a newly engaged, uncontrolled and mostly untalented citizenry. The debate about art and photography intensified around the argument that if anyone could take a photograph then photography could not possibly be called art. Some of the most passionate defenders of photography as art pointed out that photography should not and cannot be seen as an \"either/or\" medium ‒ some photographs are indeed simple records of reality, but with the right elements some are indeed works of art. William Howe Downs, art critic for the \"Boston Evening Transcript\", summed up this position in 1900 by saying \"Art is not so much a matter of methods and processes as it is an affair of temperament, of taste and of sentiment ... In the hands of the artist, the photograph becomes a work of art ... In a word, photography is what the photographer makes it ‒ an art or a trade.\"\n\nAll of these elements ‒ the debates over photography and art, the impacts of Kodak cameras, and the changing social and cultural values of the times ‒ combined to set the stage for an evolution in how art and photography, independently and together, would appear at the turn of the century. The course that drove pictorialism was set almost as soon as photographic processes were established, but it wasn't until the last decade of the 19th century that an international pictorialist movement came together.\n\nIn 1869 English photographer Henry Peach Robinson published a book entitled \"Pictorial Effect in Photography: Being Hints On Composition And Chiaroscuro For Photographers.\" This is the first common use of the term \"pictorial\" referring to photography in the context of a certain stylistic element - chiaroscuro ‒ an Italian term used by painters and art historians that refers to the use of dramatic lighting and shading to convey an expressive mood. In his book Robinson promoted what he called \"combination printing\", a method he had devised nearly 20 years earlier by combining individual elements from separate images into a new single image by manipulating multiple negatives or prints. Robinson thus considered that he had created \"art\" through photography, since it was only through his direct intervention that the final image came about. Robinson continued to expand on the meaning of the term throughout his life.\n\nOther photographers and art critics, including Oscar Rejlander, Marcus Aurelius Root, John Ruskin, echoed these ideas. One of the primary forces behind the rise of pictorialism was the belief that straight photography was purely representational ‒ that it showed reality without the filter of artistic interpretation. It was for, all intents and purposes, a simple record of the visual facts, lacking artistic intent or merit. Robinson and others felt strongly that the \"usually accepted limitations of photography had to be overcome if an equality of status was to be achieved.\n\nRobert Demachy later summarized this concept in an article entitled \"What Difference Is There Between a Good Photograph and an Artistic Photograph?\". He wrote \"We must realize that, on undertaking pictorial photography, we have, unwittingly perhaps, bound ourselves to the strict observance of rules hundreds of years more ancient than the oldest formulae of our chemical craft. We have slipped into the Temple of Art by a back door, and found ourselves amongst the crowd of adepts.\"\n\nOne of the challenges in promoting photography as art was that there were many different opinions about how art should look. After the Third Philadelphia Salon 1900, which showcased dozens of pictorial photographers, one critic wondered \"whether the idea of art in anything like the true sense had ever been heard or thought by the great majority of exhibitors.\"\n\nWhile some photographers saw themselves becoming true artists by emulating painting, at least one school of painting directly inspired photographers. During the 1880s, when debates over art and photography were becoming commonplace, a style of painting known as Tonalism first appeared. Within a few years it became a significant artistic influence on the development of pictorialism. Painters such as James McNeill Whistler, George Inness, Ralph Albert Blakelock and Arnold Böcklin saw the interpretation of the experience of nature, as contrasted with simply recording an image of nature, as the artist's highest duty. To these artists it was essential that their paintings convey an emotional response to the viewer, which was elicited through an emphasis on the atmospheric elements in the picture and by the use of \"vague shapes and subdued tonalities ... [to convey] a sense of elegiac melancholy.\"\n\nApplying this same sensibility to photography, Alfred Stieglitz later stated it this way: \"Atmosphere is the medium through which we see all things. In order, therefore, to see them in their true value on a photograph, as we do in Nature, atmosphere must be there. Atmosphere softens all lines; it graduates the transition from light to shade; it is essential to the reproduction of the sense of distance. That dimness of outline which is characteristic for distant objects is due to atmosphere. Now, what atmosphere is to Nature, tone is to a picture.\"\n\nPaul L. Anderson, a prolific contemporary promoter of pictorialism, advised his readers that true art photography conveyed \"suggestion and mystery\", in which \"mystery consists in affording an opportunity for the exercise of the imagination, whereas suggestion involves stimulating the imagination by direct or indirect means.\" Science, pictorialists contended, might answer a demand for truthful information, but art must respond to the human need for stimulation of the senses. This could only be done by creating a mark of individuality for each image and, ideally, each print.\n\nFor pictorialists, true individuality was expressed through the creation of a unique print, considered by many to be the epitome of artistic photography. By manipulating the appearance of images through what some called \"ennobling processes\", such as gum or bromoil printing, pictorialists were able to create unique photographs that were sometimes mistaken for drawings or lithographs.\n\nMany of the strongest voices that championed pictorialism at its beginning were a new generation of amateur photographers. In contrast to its meaning today, the word \"amateur\" held a different connotation in the discussions of that time. Rather than suggesting an inexperienced novice, the word characterized someone who strived for artistic excellence and a freedom from rigid academic influence. An amateur was seen as someone who could break the rules because he or she was not bound by the then rigid rules set forth by long-established photography organizations like the Royal Photographic Society. An article in the British journal \"Amateur Photographer\" stated \"photography is an art ‒ perhaps the only one in which the amateur soon equals, and frequently excels, the professional in proficiency.\" This attitude prevailed in many countries around the world. At the 1893 Hamburg International Photographic Exhibition in Germany, only the work of amateurs was allowed. Alfred Lichtwark, then director of the Kunsthalle Hamburg believed \"the only good portraiture in any medium was being done by amateurs photographers, who had the economic freedom and time to experiment.\"\n\nIn 1948, S.D.Jouhar defined a Pictorial photograph as \"mainly an aesthetic symbolic record of a scene \"plus\" the artist's personal comment and interpretation, capable of transmitting an emotional response to the mind of a receptive spectator. It should show originality, imagination, unity of purpose, a quality of repose, and have an infinite quality about it\"\n\nOver the years other names were given to pictorialism, including \"art photography\" and \"Camerawork\" (both by Alfred Stieglitz), \"Impressionist photography\" (by George Davision), \"new vision (\"Neue Vision\"), and finally \"subjective photography\" (\"Subjektive Fotographie\") in Germany after the 1940s. In Spain pictorial photographers were sometimes called \"interventionists\" (\"intervencionistas\"), although the style itself was not known as \"interventionism\".\n\nThe evolution of pictorialism from the 19th century well into the 1940s was both slow and determined. From its roots in Europe it spread to the U.S. and the rest of the world in several semi-distinct stages. Prior to 1890 pictorialism emerged through advocates who were mainly in England, Germany, Austria and France. During the 1890s the center shifted to New York and Stieglitz's multi-faceted efforts. By 1900 pictorialism had reached countries around the world, and major exhibitions of pictorial photography were held in dozens of cities.\n\nA culminating moment for pictorialism and for photography in general occurred in 1910, when the Albright Gallery in Buffalo bought 15 photographs from Stieglitz' 291 Gallery. This was the first time photography was officially recognized as an art form worthy of a museum collection, and it signaled a definite shift in many photographers' thinking. Stieglitz, who had worked so long for this moment, responded by indicating he was already thinking of a new vision beyond pictorialism. He wrote, \"It is high time that the stupidity and sham in pictorial photography be struck a solarplexus blow ... Claims of art won't do. Let the photographer make a perfect photograph. And if he happens to be a lover of perfection and a seer, the resulting photograph will be straight and beautiful - a true photograph.\"\n\nSoon after Stieglitz began to direct his attention more to modern painting and sculpture, and Clarence H. White and others took over the leadership of a new generation of photographers. As the harsh realities of World War I affected people around the world, the public's taste for the art of the past began to change. Developed countries of the world focused on more and more in the industry and growth, and art reflected this change by featuring hard-edged images of new buildings, airplanes and industrial landscapes.\n\nPictorialism had, for the most part, run its course, but in the early part of the 21st century it is now more popular than ever . Adolf Fassbender, a 20th-century photographer who continued to make pictorial photographs well into the 1960s, believed that pictorialism is eternal because it is based upon beauty first. He wrote \"There is no solution in trying to eradicate pictorialism for one would then have to destroy idealism, sentiment and all sense of art and beauty. There will always be pictorialism.\"\n\nOne of the primary catalysts of pictorialism in Australia was John Kauffmann (1864–1942), who studied photographic chemistry and printing in London, Zurich and Vienna between 1889 and 1897. When he returned to his home country in 1897, he greatly influenced his colleagues by exhibiting what one newspaper called photographs that could be \"mistaken for works of art.\" Over the next decade a core of photographer artists, including Harold Cazneaux, Frank Hurley, Cecil Bostock, Henri Mallard, Rose Simmonds and Olive Cotton, exhibited pictorial works at salons and exhibitions across the country and published their photos in the \"Australian Photographic Journal\" and the \"Australasian Photo-Review\".\n\nIn 1891 the Club der Amateur Photographen in Wien (Vienna Amateur Photographers' Club) held the first International Exhibition of Photography in Vienna. The Club, founded by Carl Sma, Federico Mallmann and Charles Scolik, was founded to foster relationships with photographic groups in other countries. After Alfred Buschbeck became head of the club in 1893, it simplified its name to Wiener Camera-Klub (Vienna Camera Club) and began publishing a lavish magazine called \"Wiener Photographische Blätter\" that continued until 1898. It regularly featured articles from influential foreign photographers such as Alfred Stieglitz and Robert Demachy.\n\nAs in other countries, opposing viewpoints engaged a wider range of photographers in defining what pictorialism meant. , and Heinrich Kühn formed an organization called Das Kleeblatt (The Trilfolium) expressly to increase the exchange of information with other organizations in other countries, especially, France, Germany and the United States. Initially a small, informal group, Das Kleeblatt increased it influence in the Wiener Camera-Klub through its international connections, and several other organizations promoting pictorialism were created in other cities throughout the region. As in other countries, interest in pictorialism faded after World War I, and eventually most of the Austrian organization slipped into obscurity during the 1920s.\n\nPictorialism in Canada initially centered on Sidney Carter (1880–1956), the first of his countrymen to be elected to the Photo-Secession. This inspired him to bring together a group of pictorial photographers in Toronto, the Studio Club in Toronto, with Harold Mortimer-Lamb (1872–1970) and fellow Secessionist Percy Hodgins. In 1907 Carter organized Canada's first major exhibition of pictorial photography at Montreal's Art Association. Carter and fellow photographer Arthur Goss attempted to introduce pictorialist principles to the members of the Toronto Camera Club, although their efforts were met with some resistance.\n\nAs early as 1853 amateur photographer William J. Newton proposed the idea that \"a 'natural object', such as a tree, should be photographed in accordance 'the acknowledged principles of fine art'\". From there other early photographers, including Henry Peach Robinson and Peter Henry Emerson, continued to promote the concept of photography as art. In 1892 Robinson, along with George Davison and Alfred Maskell, established the first organization devoted specifically to the ideal of photography as art ‒ The Linked Ring. They invited like-minded photographers, including Frank Sutcliffe, Frederick H. Evans, Alvin Langdon Coburn, Frederick Hollyer, James Craig Annan, Alfred Horsley Hinton and others, to join them. Soon The Linked Ring was at the forefront of the movement to have photography regarded as an art form.\n\nAfter The Linked Ring invited a select group of Americans as members, debates broke out about the goals and purpose of the club. When more American than British members were shown at their annual exhibit in 1908, a motion was introduced to disband the organization. By 1910 The Linked Ring has dissolved, and its members went their own way.\n\nPictorialism in France is dominated by two names, Constant Puyo and Robert Demachy. They are the most famous members of the Photo-Club de Paris, a separate organization from the Société française de photographie. They are particularly well known for their use of pigment processes, especially gum bichromate. In 1906, they published a book on the subject, \"Les Procédés d'art en Photographie\". Both of them also wrote many articles for the \"Bulletin du Photo-Club de Paris\" (1891-1902) and \"La Revue de Photographie\" (1903-1908), a magazine which quickly became the most influential French publication dealing with artistic photography during the early 20th century.\n\nThe brothers Theodor and Oskar Hofmeister of Hamburg were among the first to advocate for photography as art in their country. At meetings of the Society for the Promotion of Amateur Photography (Gesellschaft zur Förderung der Amateur-Photographie), other photographers, including Heinrich Beck, George Einbeck, and Otto Scharf, advanced the cause of pictorialism. The Homeisters, along with Heinrich Kühn, later formed The Presidium (Das Praesidium), whose members were instrumental in major exhibitions at the Kunsthalle in Hamburg. Nowadays Karl Maria Udo Remmes represents the style of pictorialism in the field of theatrical backstage photography.\n\nIn 1889 photographers Ogawa Kazumasa, W. K. Burton, Kajima Seibei and several others formed the Nihon Shashin-kai (Japan Photographic Society) in order to promote \"geijutsu shashin\" (art photography) in that country. Acceptance of this new style was slow at first, but in 1893 Burton coordinated a major invitational exhibition known as \"Gaikoku Shashin-ga Tenrain-kai\" or the \"Foreign Photographic Art Exhibition\". The 296 works that were shown came from members of the London Camera Club, including important photographs by Peter Henry Emerson and George Davison. The breadth and depth of this exhibition had a tremendous impact on Japanese photographers, and it \"galvanized the discourse of art photography throughout the country.\" After the exhibition ended Burton and Kajima founded a new organization, the Dai Nihon Shashin Hinpyō-kai (Greater Japan Photography Critique Society) to advance their particular viewpoints on art photography.\n\nIn 1904 a new magazine called \"Shashin Geppo\" (\"Monthly Photo Journal\") was started, and for many years it was the centerpiece for the advancement of and debates about pictorialism. The meaning and direction of art photography as championed by Ogawa and others was challenged in the new journal by photographers Tarō Saitō and Haruki Egashira, who, along with Tetsusuke Akiyama and Seiichi Katō, formed a new group known as Yūtsuzu-sha. This new group promoted their own concepts of what they called \"the inner truth\" of art photography. For the next decade many photographers aligned themselves with one of these two organizations.\n\nIn the 1920s new organizations were formed that bridged the transition between pictorialism and modernism. Most prominently among these was the Shashin Geijustu-sha (Photographic Art Society) formed by Shinzō Fukuhara and his brother Rosō Fukuhara. They promoted the concept of \"hikari to sono kaichō\" (light with its harmony) that rejected an overt manipulation of an image in favor of soft-focused images using silver gelatin printing.\n\nThe first generation of Dutch pictorialists, including Bram Loman, Chris Schuver and Carl Emile Mögle, began working around 1890. They initially focused on naturalistic themes and favored platinum printing. Although initially there was no Dutch equivalent of The Linked Ring or Photo-Secession, several smaller organizations collaborated to produce the First International Salon for Art Photography in 1904. Three years later Adriann Boer, Ernest Loeb, Johan Huijsen and others founded the Dutch Club for Art Photography (Nederlandsche Club voor Foto-Kunst), which amassed an important collection of pictorial photography now housed at Leiden University. A second generation of Dutch pictorialists included Henri Berssenbrugge, Bernard Eilers and Berend Zweers.\n\nPictorialism spread to Russia first through European magazines and was championed by photography pioneers Evgeny Vishnyakov in Russia and Jan Bulhak from Poland. Soon after a new generation of pictorialists became active. These included Aleksei Mazuin, Sergei Lobovikov, Piotr Klepikov, Vassily Ulitin, Nikolay Andreyev, Nikolai Svishchov-Paola, Leonid Shokin and Alexander Grinberg. In 1894 the Russian Photographic Society was established in Moscow, but differences of opinion among the members led to the establishment of a second organization, the Moscow Society of Art Photography. Both were the primary promoters of pictorialsim in Russia for many years.\n\nThe main centers of pictorial photography in Spain were Madrid and Barcelona. Leading the movement in Madrid was Antonio Cánovas, who founded the Real Sociedad Fotográfica de Madrid and edited the magazine \"La Fotografía\". Cánovas claimed to be the first to introduce artistic photography to Spain, but throughout his career he remained rooted in the allegorical style of the early English pictorialists like Robinson. He refused to use any surface manipulation in his prints, saying that style \"is not, cannot be and will never be photography.\". Other influential photographers in the country were Carlos Iñigo, Manual Renon, Joan Vilatobà and a person known only as the Conde de la Ventosa. Unlike the rest of Europe, pictorialism remained popular in Spain throughout the 1920s and 1930s, and Ventosa was the most prolific pictorialist of that period. Unfortunately very few original prints remain from any of these photographers; most of their images are now known only from magazine reproductions.\n\nOne of the key figures in establishing both the definition and direction of pictorialism was American Alfred Stieglitz, who began as an amateur but quickly made the promotion of pictorialism his profession and obsession. Through his writings, his organizing and his personal efforts to advance and promote pictorial photographers, Stieglitz was a dominant figure in pictorialism from its beginnings to its end. Following in the footsteps of German photographers, in 1892 Stieglitz established a group he called the Photo-Secession in New York. Stieglitz hand-picked the members of the group, and he tightly controlled what it did and when it did it. By selecting photographers whose vision was aligned with his, including Gertrude Käsebier, Eva Watson-Schütze, Alvin Langdon Coburn, Edward Steichen, and Joseph Keiley, Stieglitz built a circle of friends who had enormous individual and collective influence over the movement to have photography accepted as art. Stieglitz also continually promoted pictorialism through two publications he edited, Camera Notes and Camera Work and by establishing and running a gallery in New York that for many years exhibited only pictorial photographers (the Little Galleries of the Photo-Secession).\n\nWhile much initially centered on Stieglitz, pictorialism in the U.S. was not limited to New York. In Boston F. Holland Day was one of the most prolific and noted pictorialists of his time. Clarence H. White, who produced extraordinary pictorial photographs while in Ohio, went on to teach a whole new generation of photographers. On the West Coast the California Camera Club and Southern California Camera Club included prominent pictorialists Annie Brigman, Arnold Genthe, Adelaide Hanscom Leeson, Emily Pitchford and William E. Dassonville. Later on, the Seattle Camera Club was started by a group of Japanese-American pictorialists, including Dr. Kyo Koike, Frank Asakichi Kunishige and Iwao Matsushita (prominent members later included Ella E. McBride and Soichi Sunami).\n\nPictorial photographers began by taking an ordinary glass-plate or film negative. Some adjusted the focus of the scene or used a special lens to produce a softer image, but for the most part the printing process controlled the final appearance of the photograph. Pictorialists used a variety of papers and chemical processes to produce particular effects, and some then manipulated the tones and surface of prints with brushes, ink or pigments. The following is a list of the most commonly used pictorial processes. More details about these processes may be found in Crawford (pp 85‒95) and in Daum (pp 332‒334). Unless otherwise noted, the descriptions below are summarized from these two books.\n\n\nFollowing are two lists of prominent photographers who engaged in pictorialism during their careers. The first list includes photographers who were predominantly pictorialists for all or almost all of their careers (generally those active from 1880 to 1920). The second list includes 20th-century photographers who used a pictorial style early in the careers but who are more well known for pure or straight photography.\n\n\n\n\n"}
{"id": "25100", "url": "https://en.wikipedia.org/wiki?curid=25100", "title": "Pitot tube", "text": "Pitot tube\n\nA pitot ( ) tube, also known as pitot probe, is a flow measurement device used to measure fluid flow velocity. The pitot tube was invented by the French engineer Henri Pitot in the early 18th century and was modified to its modern form in the mid-19th century by French scientist Henry Darcy. It is widely used to determine the airspeed of an aircraft, water speed of a boat, and to measure liquid, air and gas flow velocities in certain industrial applications.\n\nThe basic pitot tube consists of a tube pointing directly into the fluid flow. As this tube contains fluid, a pressure can be measured; the moving fluid is brought to rest (stagnates) as there is no outlet to allow flow to continue. This pressure is the stagnation pressure of the fluid, also known as the total pressure or (particularly in aviation) the pitot pressure.\n\nThe measured stagnation pressure cannot itself be used to determine the fluid flow velocity (airspeed in aviation). However, Bernoulli's equation states:\n\nWhich can also be written\n\nSolving that for flow velocity gives\n\nwhere\n\nNOTE: The above equation applies only to fluids that can be treated as incompressible. Liquids are treated as incompressible under almost all conditions. Gases under certain conditions can be approximated as incompressible. See Compressibility.\n\nThe dynamic pressure, then, is the difference between the stagnation pressure and the static pressure. The dynamic pressure is then determined using a diaphragm inside an enclosed container. If the air on one side of the diaphragm is at the static pressure, and the other at the stagnation pressure, then the deflection of the diaphragm is proportional to the dynamic pressure.\n\nIn aircraft, the static pressure is generally measured using the static ports on the side of the fuselage. The dynamic pressure measured can be used to determine the indicated airspeed of the aircraft. The diaphragm arrangement described above is typically contained within the airspeed indicator, which converts the dynamic pressure to an airspeed reading by means of mechanical levers.\n\nInstead of separate pitot and static ports, a pitot-static tube (also called a Prandtl tube) may be employed, which has a second tube coaxial with the pitot tube with holes on the sides, outside the direct airflow, to measure the static pressure.\n\nIf a liquid column manometer is used to measure the pressure difference formula_7, \nwhere\n\nTherefore,\n\nA pitot-static system is a system of pressure-sensitive instruments that is most often used in aviation to determine an aircraft's airspeed, Mach number, altitude, and altitude trend. A pitot-static system generally consists of a pitot tube, a static port, and the pitot-static instruments. Errors in pitot-static system readings can be extremely dangerous as the information obtained from the pitot static system, such as airspeed, is potentially safety-critical.\n\nSeveral commercial airline incidents and accidents have been traced to a failure of the pitot-static system. Examples include Austral Líneas Aéreas Flight 2553, Northwest Airlines Flight 6231, Birgenair Flight 301 and one of the two X-31s. The French air safety authority BEA said that pitot tube icing was a contributing factor in the crash of Air France Flight 447 into the Atlantic Ocean. In 2008 Air Caraïbes reported two incidents of pitot tube icing malfunctions on its A330s.\n\nBirgenair Flight 301 had a fatal pitot tube failure which investigators suspected was due to insects creating a nest inside the pitot tube; the prime suspect is the black and yellow mud dauber wasp.\n\nAeroperú Flight 603 had a pitot-static system failure due to the cleaning crew leaving the static port blocked with tape.\n\nIn industry, the flow velocities being measured are often those flowing in ducts and tubing where measurements by an anemometer would be difficult to obtain. In these kinds of measurements, the most practical instrument to use is the pitot tube. The pitot tube can be inserted through a small hole in the duct with the pitot connected to a U-tube water gauge or some other differential pressure gauge for determining the flow velocity inside the ducted wind tunnel. One use of this technique is to determine the volume of air that is being delivered to a conditioned space.\n\nThe fluid flow rate in a duct can then be estimated from:\n\nIn aviation, airspeed is typically measured in knots.\n\nIn weather stations with high wind speeds, the pitot tube is modified to create a special type of anemometer called pitot tube static anemometer.\n\n\n\nNotes\nBibliography\n\n"}
{"id": "355413", "url": "https://en.wikipedia.org/wiki?curid=355413", "title": "Player Piano (novel)", "text": "Player Piano (novel)\n\nPlayer Piano is the first novel of American writer Kurt Vonnegut, published in 1952. It depicts a dystopia of automation, describing the negative impact it can have on quality of life. The story takes place in a near-future society that is almost totally mechanized, eliminating the need for human laborers. The widespread mechanization creates conflict between the wealthy upper class, the engineers and managers, who keep society running, and the lower class, whose skills and purpose in society have been replaced by machines. The book uses irony and sentimentality, which were to become hallmarks developed further in Vonnegut's later works.\n\n\"Player Piano\" is set in the near future, after a third world war. While most Americans were fighting overseas, the nation's managers and engineers faced a depleted workforce and responded by developing ingenious automated systems that allowed the factories to operate with only a few workers. The novel begins ten years after the war, when most factory workers have been replaced by machines. The bifurcation of the population is represented by the division of Ilium, New York into \"The Homestead,\" where every person not a manager or an engineer lives, and the other side of the river, where all the engineers and the managers live.\n\n\"Player Piano\" develops two parallel plotlines that converge only briefly and then insubstantially, at the beginning and the end of the novel. The more prominent plotline follows the protagonist, Dr. Paul Proteus (referred to as Paul), an intelligent, 35-year-old factory manager of Ilium Works. The secondary plotline follows the American tour of the Shah of Bratpuhr, a spiritual leader of six million residents in a distant, underdeveloped nation.\n\nThe purpose of the two plotlines is to give two perspectives of the system: one from an insider who is emblematic of the system, and one from an outsider who is looking in it. Paul, for all intents and purposes, is the living embodiment of what a man within the system should strive to be, and the Shah is a visitor from a very different culture and so applies a very different context to whatever he sees on his tour.\n\nThe main plotline follows Paul's development from an uncritical cog in the system to one of its outspoken critics. Paul's father, George, was the first \"National, Industrial, Commercial Communications, Foodstuffs, and Resources Director.\" George had almost complete control over the nation's economy and was more powerful than the President of the United States. Paul has inherited his father's reputation and social status but harbors a vague dissatisfaction with the industrial system and his contribution to society. His struggle with that unnameable distress is heightened when Ed Finnerty, an old friend whom Paul has always held in high regard, informs him he has quit his important engineering job in Washington, DC. Paul and Finnerty visit a bar in the \"Homestead\" section of town, where workers who have been displaced by machines live out their meaningless lives in mass-produced houses. There, they meet an Episcopal minister, Lasher, with an M.A. in anthropology, who puts into words the unfairness of the system that the two engineers have only vaguely sensed. They soon learn that Lasher is the leader of a rebel group known as the \"Ghost Shirt Society,\" and Finnerty instantly takes up with him. Paul is not bold enough to make a clean break, as Finnerty has done, until his superiors ask him to betray Finnerty and Lasher. However, Paul secretly purchases a rundown farm, managed by an elderly heir of the prior owners. Paul's intention is to start a new life by living off the land with his wife, Anita, but Anita is disgusted by Paul's wishes to change their lifestyle radically. Paul and Anita's relationship is one of emotional distance and personal disagreements. She and Paul had married quickly when it seemed that she was pregnant, but it turned out that Anita was barren and that it was just a hysterical pregnancy. \"Of all the people on the north side of the river, Anita was the only one whose contempt for those in Homestead was laced with active hatred... If Paul were ever moved to be extremely cruel to her, the cruelest thing he could do... would be to point out to her why she hated [Homesteaders] as she did: if he hadn't married her, this was where she'd be, what she'd be.\"\n\nShe temporarily convinces Paul to stay in his position, and to continue to compete with two other engineers, Dr. Shepherd and Dr. Garth, for a more prominent position in Pittsburgh, Pennsylvania.\n\nAfter rumors of Paul's disloyalty to the system and suspicious activity during the hosting of \"the Meadows,\" an annual competition for high class engineers, begin circulating, Paul determines that with or without Anita, he must work with his friend Finnerty, among others, to stop the socioeconomic \"system\" of having machines replace humans. He quits his job and is captured by the \"Ghost Shirt Society\" in which he is made the public figurehead of the organization although the position is merely nominal. By his father's success, Paul's name is famous among the citizens and so the organization intends to use his name to its advantage by making him the false 'leader' to gain publicity.\n\nPaul is arrested and put on public trial but is freed as the Ghost Shirt Society and the general population begin to riot, destroying the automated factories. The mob, once unleashed, goes farther than the Ghost Shirt leaders had planned, destroying both food production plants and the superfluous plants. Despite the brief and impressive success of the rebellion, the military quickly surrounds the town, and the citizenry, used to the comforts of the system, begin to rebuild the machines of their own volition. Paul, Finnerty, Lasher, and other members of the Ghost Shirt Society acknowledge that at least they had tried to stop the government's system before they surrender themselves to the military.\n\nThe automation of industry and the effect that it has on society are the predominant themes of \"Player Piano\". It is \"a novel about people and machines, and machines frequently got the best of it, as machines will.\" More specifically, it delves into a theme to which Vonnegut returns, \"a problem whose queasy horrors will eventually be made world-wide by the sophistication of machines. The problem is this: How to love people who have no use.\" Unlike much dystopian fiction, the novel's society was created by indifference, both of the populace and the technology that replaced it. As such, it is the sense of purposelessness of those living in a capitalistic society that has outgrown a need for them that must be rectified.\n\nMankind's blind faith in technology and its usually-disastrous effect on society as well as the dehumanization of the poor or oppressed later became common themes throughout Vonnegut's work. Throughout his life, Vonnegut continued to believe the novel's themes were of relevance to society, writing, for example, in 1983 that the novel was becoming \"more timely with each passing day.\"\n\n\"Player Piano\" displays the beginnings of the idiosyncratic style that Vonnegut developed and employed throughout much of his career. It has early inklings of the hallmark Vonnegutian flair of using meta-fiction, such as when a writer's wife describes her husband's dilemma to the Shah of Bratpuhr in the back of the limousine: that the writer's \"anti-machine\" novel cannot get a passing \"readability quotient\" under the reading machine's scoring algorithm. However, the fourth wall does not get broken, as in later writings. His style of self-contained chapters \"of no more than five hundred words, often as few as fifty,\" which would come to define his writing, had yet to be developed.\n\nIn a 1973 interview Vonnegut discussed his inspiration to write the book: I was working for General Electric at the time, right after World War II, and I saw a milling machine for cutting the rotors on jet engines, gas turbines. This was a very expensive thing for a machinist to do, to cut what is essentially one of those Brâncuși forms. So they had a computer-operated milling machine built to cut the blades, and I was fascinated by that. This was in 1949 and the guys who were working on it were foreseeing all sorts of machines being run by little boxes and punched cards. \"Player Piano\" was my response to the implications of having everything run by little boxes. The idea of doing that, you know, made sense, perfect sense. To have a little clicking box make all the decisions wasn't a vicious thing to do. But it was too bad for the human beings who got their dignity from their jobs. In the same interview he acknowledges that he \"cheerfully ripped off the plot of \"Brave New World\", whose plot had been cheerfully ripped off from Yevgeny Zamyatin's \"We\".\"\n\nA player piano is a modified piano that \"plays itself.\" The piano keys move according to a pattern of holes punched in an unwinding scroll. Unlike a music synthesizer, the instrument actually produces the sound itself, with the keys moving up and down, driving hammers that strike the strings. Like its counterpart, a player piano can be played by hand as well. When a scroll is run through the instrument, the movement of its keys produce the illusion that an invisible performer is playing the instrument. Vonnegut uses the player piano as a metaphor to represent how even the most simple of activities, such as teaching oneself how to play the piano in one's spare time, has been replaced by machines instead of people. Early in the book, Paul Proteus's friend and future member of the Ghost Shirt Society, Ed Finnerty, is shown manually playing a player piano, suggesting the idea of humans reclaiming their animus from the machines.\n\nThis satirical take on industrialization and the rhetoric of General Electric and the big corporations, which discussed arguments very topical in the postwar United States, was instead advertised by the publisher with the more innocuous and marketable label of \"science fiction,\" a genre that was booming in mass popular culture in the 1950s. Vonnegut, surprised by that reception, wrote, \"I learned from reviewers that I was a science-fiction author. I didn't know that.\" He was distressed because he felt that science fiction was shoved in a drawer which \"many serious critics regularly mistake... for a urinal\" because \"[t]he feeling persists that no one can simultaneously be a respectable writer and understand how a refrigerator works.\"\n\n\"Player Piano\" was later released in paperback by Bantam Books in 1954 under the title Utopia 14 in an effort to drive sales with readers of science fiction. Paul Proteus' trial was dramatized in the 1972 TV movie \"Between Time and Timbuktu\", which presented elements from various works by Vonnegut.\n\nIn the Italian translation, \"Player Piano\" is rendered as \"Piano meccanico\", a double-entendre, which, without any other words in the phrase, can mean either \"player piano\" or \"mechanical plan.\"\n\nReviewing the novel for a genre science fiction audience, Groff Conklin declared it \"a biting, vividly alive and very effectively understated anti-Utopia.\" Boucher and McComas named it to their \"year's best\" list, describing it as \"Human, satirical, and exciting;... by far the most successful of the recent attempts to graft science fiction onto the serious 'straight' novel.\" They praised Vonnegut for \"blending skilfully a psychological study of the persistent human problems in a mechanistically 'ideal' society, a vigorous melodramatic story-line, and a sharp Voltairean satire.\n\n\"Player Piano\" was nominated for the International Fantasy Award in 1953.\n\n"}
{"id": "8099541", "url": "https://en.wikipedia.org/wiki?curid=8099541", "title": "Pogo pin", "text": "Pogo pin\n\nA Pogo pin is a device used in electronics to establish a (usually temporary) connection between two printed circuit boards. Named by analogy with the pogo stick toy, the pogo pin usually takes the form of a slender cylinder containing a sharp, spring-loaded pin. Pressed between two electronic circuits, the sharp points at each end of the pogo pin make secure contacts with the two circuits and thereby connect them together.\n\nAlthough often used as a generic name, Pogo is a registered trademark of Everett Charles Technologies (ECT). ECT and its subsidiaries have been manufacturing Pogo pins for over forty years.\n\nPogo pins are usually arranged in a dense array, connecting together many individual nodes of the two circuit boards. They are very commonly found in automatic test equipment in the form of a bed of nails, where they facilitate the rapid, reliable connection of the devices under test (DUTs). In one extremely high-density configuration, the array takes the form of a ring containing hundreds or thousands of individual pogo pins; this device is sometimes referred to as a pogo tower.\n\nThey can also be used to establish more permanent connections, for example, in the Cray 2 superomputer.\n\nWhen used in the highest-performance applications, pogo pins must be very carefully designed to allow not only high reliability across many mating/unmating cycles but also high-fidelity transmission of the electrical signals. The pins themselves must be hard, yet plated with a substance (such as gold) that provides for reliable contact. Within the body of the pogo pin, the pins must make good electrical contact with the body lest the spring carry the signal (along with the undesirable inductance that the spring represents). The design of pogo pins to be used in matched-impedance circuits is especially challenging; to maintain the correct characteristic impedance, pogo pins are sometimes arranged with one signal-carrying pin surrounded by four, five, or six grounded pins.\n\nPogo pins and Pogo pin cables are also widely used for in-circuit programming for microcontrollers.\n\n"}
{"id": "30863172", "url": "https://en.wikipedia.org/wiki?curid=30863172", "title": "Programming interview", "text": "Programming interview\n\nA programming interview is a technical job interview in the software industry or in information technology (IT) departments of major corporations to test candidates' technical knowledge, coding ability, problem solving skills, and creativity about computers. Candidates usually have a degree in computer science, information science, computer engineering or electrical engineering, and are asked to solve programming problems, algorithms, puzzles, and/or computer related technical questions as used in Microsoft interviews.\n\nA screening interview is usually undertaken by human resources or personnel staff, or by a third-party recruiter. The aim of the screening interview, which can occur by telephone or in person, is to check a candidate's technical relevance against the job and whether they meet minimum qualification requirements so that interviewers can determine if a candidate has the basic skills to perform the work. Candidates will be asked questions oriented around their experience and training, though there may be some questions designed to reveal other issues such as deceptiveness. However, their main interest is gathering resumes and initial impressions before making decisions on whether they will move to the next step. This phase can also be aided by software tools for assessment of programming proficiency.\n\nAn on-site interviews consist mostly of a variety of technical questions: problems requiring a candidate to implement a simple program or function, questions that test knowledge of computers, languages, and programming; and mathematics and logic puzzles. On-site interviews usually last either a half day or a full day, and typically consist of three to six interviews of 30 to 60 minutes each.\n\nIf the interviewing position has specific programming language requirements, the candidate is expected to know those languages and solve the questions with it. If the interview is for general programming or development position, a thorough knowledge of one mainstream language such as C, C++, C#, or Java may be enough to get by. An interviewer may also allow use of other popular languages, such as JavaScript, PHP, or Perl. If a choice of languages is given, it is best to choose the one the interviewee is most familiar with. Interviewers are less likely to allow using less-mainstream languages such as Lisp, Tcl, Prolog, COBOL, or Fortran, however, if a candidate is especially expert at one of these, it is definitely good to tell the interviewer.\n\nThe dress code for an on-site programming interview is usually reflected by what other people wear at the company. These days, most companies in the software industry allow business casual at work. Therefore, unless it is asked or the interviewing position has a significant business or consulting aspect whereby formal dress will be needed, a suit may be overkill for a programming interview. It is advised to dress professionally and appear clean and tidy since it gives the first impression of the candidates. If the information about the dress code of a target company is given or can be found, wear accordingly. Women are advised to avoid heavy jewelry.\n\n"}
{"id": "9875359", "url": "https://en.wikipedia.org/wiki?curid=9875359", "title": "Prym", "text": "Prym\n\nThe \"William Prym Holding GmbH\" is the oldest family business in Germany. The holding company is located in Stolberg (Rhineland).\n\nFor twelve generations the Prym family has been operating from the city of Stolberg, Germany, making and marketing semi-finished copper and brass products as well as haberdashery. In the 17th and 18th century they contributed substantially to establishing the local brass industry and its development towards international reputation. Johann (c.1340-c.1420), whose name is documented in a medieval register in Aachen, is referred to as the oldest ancestor. In the middle of the 15th century Wilhelm, a goldsmith, is mentioned as an administrator of an Aachen citizen. He is considered the founder of the family tradition in metal manufacturing and trade.\n\nDue to the important zinc ore deposits of the region, the conditions for brass production were ideal at the time. Mainly Protestant families established through their skill and diligence the rapidly growing fame of brass and copper products from Aachen. At the\nbeginning of the 17th century, the loss of the guild rights in Catholic Aachen forced the now homeless Protestant brass manufacturers to move. Eight families came to nearby Stolberg, a small settlement in the dukedom of Jülich-Cleve-Berg, the brass manufacturer Christian (1614–83) and his family being among them. In 1642 he settled down at Dollartshammer, the head office of the family company which is at the same location to this day. As his sisters Maria (bef.1610- 53) and Sara (1615–1696), he also married into the Peltzer family, one of the richest brass manufacturers of the region. Another sister, Catharina (1610–81), married Jeremias Hoesch (1610–53) in 1637.\n\nIn the 19th century the Pryms produced the first finished products made not only of brass, but also of steel and iron in addition to the rolled material and wires made of the traditional copper alloys. Under the management of William (1811–83) the company entered a new era of success. His son Heinrich August (1843–1927) did his apprenticeship in Birmingham and used the knowledge acquired there to introduce mechanical manufacturing of metal haberdashery in Germany.\n\nHans Friedrich (1875–1965), son of August, expanded the Austrian production and sales branches he had been leading since 1908, to a dominating position in the Austrian monarchy and the crown lands. In this period, in 1903, he made the ingenious improvement to the press fastener, already invented in 1885.\n\n”Prym” and ”Prym’s Zukunft” (Prym’s future) became the earliest brand names of the 20th century, the stag – featured in the family’s coat of arms – with a needle in his antlers one of the most famous trademarks. In the following years, Hans’ great commitment was to the export business. Thus, not long after World War One, he established a business in Schweidnitz (Silesia), a branch in Berlin for sales in the East, and a subsidiary in the USA. In 1932, he modernised the brass rolling mill in Stolberg established prior to the turn of the century, despite the general lack of orders. A decision that turned out to be economically extremely successful. During World War Two, the company produced turbine blades and war related materials in addition to its rolled materials, wires and haberdashery; however, Hans never became a member of the National Socialist Party.\n\nFollowing World War Two, the considerable possessions of the family and the company in Berlin, Eastern Germany, Poland and Belgium were lost. Despite initial problems, in the years following the war, Hans managed to expand the zip fastener business with the newly developed plastic zip, ”Prymalon”, as an addition to the traditional products. In 1960, after prolonged negotiations, he was successful in buying back all Austrian properties and rights, which was of essential importance, particularly regarding the ”Prym” brand name. His marriage to Russian-born Olga Gütschow (1884–1975) in 1903 produced six sons, four of whom worked in the company. From 1928-72, Hans-August (1904–82) was head of ”William Prym of America, Inc.”, while Axel (1907–89) and Dieter (born 1917) held leading positions in the Stolberg head office which was expanding towards the Far East and Central America. Following the withdrawal of the British group, ”Coats Viyella” (1976–93, with a 25% participation), the descendants of Heinrich Augusts once again became sole owners of the family company, probably the oldest in Germany.\n\nAfter the strategically important sale of Prymetall (a leading manufacturer of semi-finished products of copper and copper alloys), the company as a holding is divided into three independent sectors with numerous subsidiaries both in Germany and abroad: Prym Consumer (sewing and needlework accessories), Prym Fashion (fastening systems and accessories), and Prymtec (contact and electronic components). Until today members of the Prym family are still involved in the management of the business. The family stands for continuity in the development of this group of companies.\n\nIn 2007 Prym was fined €40M by the European Commission for its part in operating a price fixing cartel, along with YKK and Coats plc. Prym's fine was much less than the other companies' as it alerted the Commission to the wrongdoing.\n\n\n\n\n"}
{"id": "53180443", "url": "https://en.wikipedia.org/wiki?curid=53180443", "title": "Susan Horn", "text": "Susan Horn\n\nSusan Helen Dadakis Horn is an American biostatistician. She is the senior scientist at the Institute for Clinical Outcomes Research, a professor at the University of Utah School of Medicine in the Health Services Innovation and Research Program, and an affiliate faculty member at Weill Cornell Graduate School of Medical Sciences. She is known for her work in developing computational statistical models for clinicians to use in-practice to improve therapy results.\n\nSusan D. Horn graduated from Cornell University in 1964 with a Bachelor of Arts in mathematics, after which she completed her PhD in statistics from Stanford University in 1968. From 1968 to 1992, she was a professor at Johns Hopkins University where she conducted research, taught mathematics and health services courses, and directed the Robert Wood Johnson Foundation Program for Faculty Fellowships in Health Care Finance.\n\nShe became a Fellow of the American Statistical Association in 1978.\n\nSusan D. Horn is married to Roger Horn, an American mathematician and fellow professor at the University of Utah. They have three children. Her daughter Ceres was killed in the 1987 Maryland train collision while returning to Princeton University from the family home in Baltimore for her freshman year fall term final exams. Roger submitted a testimony on the crash to the US Senate Subcommittee on Transportation.\n"}
{"id": "27439063", "url": "https://en.wikipedia.org/wiki?curid=27439063", "title": "Thunder Horse PDQ", "text": "Thunder Horse PDQ\n\nThunder Horse PDQ is a BP plc and ExxonMobil joint venture semi-submersible oil platform on location over the Mississippi Canyon Thunder Horse oil field (Block 778/822), in deepwater Gulf of Mexico, southeast of New Orleans, moored in waters of . The \"PDQ\" identifies the platform as being a Production and oil Drilling facility with crew Quarters.\n\n\"Thunder Horse PDQ\" is the largest offshore installation of its kind in the world. The vessel's hull is of GVA design. The hull was built by Daewoo Shipbuilding & Marine Engineering (DSME) in Okpo, South Korea, then loaded aboard the heavy lift ship and transported to Kiewit Offshore Services in Ingleside, Texas where it was integrated with its topsides modules that were built in Morgan City, La.. The journey around the Cape of Good Hope took nine weeks (63 days), from 23 July to 23 September 2004.\n\n\"Thunder Horse PDQ\" was evacuated with the approach of Hurricane Dennis in July 2005. After the hurricane passed, the platform fell into a 20 degree list and was in danger of foundering.\n\nThe platform was designed for a 100-year event, and inspection teams found no hull damage and no leaks through its hull. Rather, an incorrectly plumbed 6-inch length of pipe allowed water to flow freely among several ballast tanks that set forth a chain of events causing the platform to tip into the water. The platform was fully righted about a week after \"Dennis\", delaying commercial production initially scheduled for late 2005. During repairs, it was discovered that the underwater manifold was severely cracked due to poorly welded pipes.\n\nThe platform took a nearly-direct hit six weeks later from Hurricane Katrina, but was undamaged.\n\n\n"}
{"id": "58900", "url": "https://en.wikipedia.org/wiki?curid=58900", "title": "Unmanned aerial vehicle", "text": "Unmanned aerial vehicle\n\nAn unmanned aerial vehicle (UAV), commonly known as a drone, is an aircraft without a human pilot aboard. UAVs are a component of an unmanned aircraft system (UAS); which include a UAV, a ground-based controller, and a system of communications between the two. The flight of UAVs may operate with various degrees of autonomy: either under remote control by a human operator or autonomously by onboard computers.<ref name=\"ICAO's circular 328 AN/190\"></ref>\n\nCompared to manned aircraft, UAVs were originally used for missions too \"dull, dirty or dangerous\" for humans. While they originated mostly in military applications, their use is rapidly expanding to commercial, scientific, recreational, agricultural, and other applications, such as policing, peacekeeping, and surveillance, product deliveries, aerial photography, agriculture, smuggling, and drone racing. Civilian UAVs now vastly outnumber military UAVs, with estimates of over a million sold by 2015, so they can be seen as an early commercial application of autonomous things, to be followed by the autonomous car and home robots.\n\nMultiple terms are used for unmanned aerial vehicles, which generally refer to the same concept.\n\nThe term drone, more widely used by the public, was coined in reference to the early remotely-flown target aircraft used for practice firing of a battleship's guns, and the term was first used with the 1920s Fairey Queen and 1930's de Havilland Queen Bee target aircraft. These two were followed in service by the similarly-named Airspeed Queen Wasp and Miles Queen Martinet, before ultimate replacement by the GAF Jindivik.\n\nThe term \"unmanned aircraft system\" (UAS) was adopted by the United States Department of Defense (DoD) and the United States Federal Aviation Administration in 2005 according to their Unmanned Aircraft System Roadmap 2005–2030. The International Civil Aviation Organization (ICAO) and the British Civil Aviation Authority adopted this term, also used in the European Union's Single-European-Sky (SES) Air-Traffic-Management (ATM) Research (SESAR Joint Undertaking) roadmap for 2020. This term emphasizes the importance of elements other than the aircraft. It includes elements such as ground control stations, data links and other support equipment. A similar term is an \"unmanned-aircraft vehicle system\" (UAVS), \"remotely piloted aerial vehicle\" (RPAV), \"remotely piloted aircraft system\" (RPAS). Many similar terms are in use.\n\nA UAV is defined as a \"powered, aerial vehicle that does not carry a human operator, uses aerodynamic forces to provide vehicle lift, can fly autonomously or be piloted remotely, can be expendable or recoverable, and can carry a lethal or nonlethal payload\". Therefore, missiles are not considered UAVs because the vehicle itself is a weapon that is not reused, though it is also unmanned and in some cases remotely guided.\n\nThe relation of UAVs to remote controlled model aircraft is unclear. UAVs may or may not include model aircraft. Some jurisdictions base their definition on size or weight; however, the US Federal Aviation Administration defines any unmanned flying craft as a UAV regardless of size. For recreational uses, a drone (as opposed to a UAV) is a model aircraft that has first-person video, autonomous capabilities, or both.\n\nThe earliest recorded use of an unmanned aerial vehicle for warfighting occurred on July 1849, serving as a balloon carrier (the precursor to the aircraft carrier) in the first offensive use of air power in naval aviation. Austrian forces besieging Venice attempted to launch some 200 incendiary balloons at besieged city. The balloons were launched mainly from land, however some were also launched from the Austrian ship SMS Vulcano.At least one bomb fell in the city, however due to the wind changing after launch, most of the balloons missed their target and some drifted back over Austrian lines and the launching ship \"Vulcano\".\n\nUAV innovations started in the early 1900s and originally focused on providing practice targets for training military personnel. UAV development continued during World War I, when the Dayton-Wright Airplane Company invented a pilotless aerial torpedo that would explode at a preset time.\n\nThe earliest attempt at a powered UAV was A. M. Low's \"Aerial Target\" in 1916. Nikola Tesla described a fleet of unmanned aerial combat vehicles in 1915. Advances followed during and after World War I, including the Hewitt-Sperry Automatic Airplane. This developments also inspired the development of the Kettering Bug by Charles Kettering from Dayton, Ohio. This was initially meant as an unmanned plane that would carry an explosive payload to a predetermined target. The first scaled remote piloted vehicle was developed by film star and model-airplane enthusiast Reginald Denny in 1935. More emerged during World War II – used both to train antiaircraft gunners and to fly attack missions. Nazi Germany produced and used various UAV aircraft during the war. Jet engines entered service after World War II in vehicles such as the Australian GAF Jindivik, and Teledyne Ryan Firebee I of 1951, while companies like Beechcraft offered their Model 1001 for the U.S. Navy in 1955. Nevertheless, they were little more than remote-controlled airplanes until the Vietnam War.\n\nIn 1959, the U.S. Air Force, concerned about losing pilots over hostile territory, began planning for the use of unmanned aircraft. Planning intensified after the Soviet Union shot down a U-2 in 1960. Within days, a highly classified UAV program started under the code name of \"Red Wagon\". The August 1964 clash in the Tonkin Gulf between naval units of the U.S. and North Vietnamese Navy initiated America's highly classified UAVs (Ryan Model 147, Ryan AQM-91 Firefly, Lockheed D-21) into their first combat missions of the Vietnam War. When the Chinese government showed photographs of downed U.S. UAVs via \"Wide World Photos\", the official U.S. response was \"no comment\".\n\nThe War of Attrition (1967–1970) featured the introduction of UAVs with reconnaissance cameras into combat in the Middle East.\n\nIn the 1973 Yom Kippur War Israel used UAVs as decoys to spur opposing forces into wasting expensive anti-aircraft missiles.\n\nIn 1973 the U.S. military officially confirmed that they had been using UAVs in Southeast Asia (Vietnam). Over 5,000 U.S. airmen had been killed and over 1,000 more were missing or captured. The USAF 100th Strategic Reconnaissance Wing flew about 3,435 UAV missions during the war at a cost of about 554 UAVs lost to all causes. In the words of USAF General George S. Brown, Commander, Air Force Systems Command, in 1972, \"The only reason we need (UAVs) is that we don't want to needlessly expend the man in the cockpit.\" Later that year, General John C. Meyer, Commander in Chief, Strategic Air Command, stated, \"we let the drone do the high-risk flying ... the loss rate is high, but we are willing to risk more of them ... they save lives!\"\n\nDuring the 1973 Yom Kippur War, Soviet-supplied surface-to-air missile batteries in Egypt and Syria caused heavy damage to Israeli fighter jets. As a result, Israel developed the first UAV with real-time surveillance. The images and radar decoys provided by these UAVs helped Israel to completely neutralize the Syrian air defenses at the start of the 1982 Lebanon War, resulting in no pilots downed. The first time UAVs were used as proof-of-concept of super-agility post-stall controlled flight in combat-flight simulations involved tailless, stealth technology-based, three-dimensional thrust vectoring flight control, jet-steering UAVs in Israel in 1987.\n\nWith the maturing and miniaturization of applicable technologies in the 1980s and 1990s, interest in UAVs grew within the higher echelons of the U.S. military. In the 1990s, the U.S. DoD gave a contract to AAI Corporation along with Israeli company Malat. The U.S. Navy bought the AAI Pioneer UAV that AAI and Malat developed jointly. Many of these UAVs saw service in the 1991 Gulf War. UAVs demonstrated the possibility of cheaper, more capable fighting machines, deployable without risk to aircrews. Initial generations primarily involved surveillance aircraft, but some carried armaments, such as the General Atomics MQ-1 Predator, that launched AGM-114 Hellfire air-to-ground missiles.\n\nCAPECON was a European Union project to develop UAVs, running from 1 May 2002 to 31 December 2005.\n\nAs of 2012, the USAF employed 7,494 UAVs – almost one in three USAF aircraft. The Central Intelligence Agency also operated UAVs.\n\nIn 2013 at least 50 countries used UAVs. China, Iran, Israel, Pakistan, and others designed and built their own varieties.\n\nUAVs typically fall into one of six functional categories (although multi-role airframe platforms are becoming more prevalent):\nThe U.S. Military UAV tier system is used by military planners to designate the various individual aircraft elements in an overall usage plan.\n\nVehicles can be categorised in terms of range/altitude. The following has been advanced as relevant at industry events such as ParcAberporth Unmanned Systems forum:\nOther categories include:\n\nClassifications according to aircraft weight are quite simpler:\n\nManned and unmanned aircraft of the same type generally have recognizably similar physical components. The main exceptions are the cockpit and environmental control system or life support systems. Some UAVs carry payloads (such as a camera) that weigh considerably less than an adult human, and as a result can be considerably smaller. Though they carry heavy payloads, weaponized military UAVs are lighter than their manned counterparts with comparable armaments.\n\nSmall civilian UAVs have no life-critical systems, and can thus be built out of lighter but less sturdy materials and shapes, and can use less robustly tested electronic control systems. For small UAVs, the quadcopter design has become popular, though this layout is rarely used for manned aircraft. Miniaturization means that less-powerful propulsion technologies can be used that are not feasible for manned aircraft, such as small electric motors and batteries.\n\nControl systems for UAVs are often different than manned craft. For remote human control, a camera and video link almost always replace the cockpit windows; radio-transmitted digital commands replace physical cockpit controls. Autopilot software is used on both manned and unmanned aircraft, with varying feature sets.\n\nThe primary difference for planes is the absence of the cockpit area and its windows. Tailless quadcopters are a common form factor for rotary wing UAVs while tailed mono- and bi-copters are common for manned platforms.\n\nSmall UAVs mostly use lithium-polymer batteries (Li-Po), while larger vehicles rely on conventional airplane engines. Scale or size of aircraft is not the defining or limiting characteristic of energy supply for a UAV. At present, the energy density of Li-Po is far less than gasoline. The record of travel for a UAV (built from balsa wood and mylar skin) across the North Atlantic Ocean is held by a gasoline model airplane or UAV. Manard Hill in \"in 2003 when one of his creations flew 1,882 miles across the Atlantic Ocean on less than a gallon of fuel\" holds this record. See: Electric power is used as less work is required for a flight and electric motors are quieter. Also, properly designed, the thrust to weight ratio for an electric or gasoline motor driving a propeller can hover or climb vertically. Botmite airplane is an example of an electric UAV which can climb vertically.\n\nBattery elimination circuitry (BEC) is used to centralize power distribution and often harbors a microcontroller unit (MCU). Costlier switching BECs diminish heating on the platform.\n\nUAV computing capability followed the advances of computing technology, beginning with analog controls and evolving into microcontrollers, then system-on-a-chip (SOC) and single-board computers (SBC).\n\nSystem hardware for small UAVs is often called the flight controller (FC), flight controller board (FCB) or autopilot.\n\nPosition and movement sensors give information about the aircraft state. Exteroceptive sensors deal with external information like distance measurements, while exproprioceptive ones correlate internal and external states.\n\nNon-cooperative sensors are able to detect targets autonomously so they are used for separation assurance and collision avoidance.\n\nDegrees of freedom (DOF) refers to both the amount and quality of sensors on-board: 6 DOF implies 3-axis gyroscopes and accelerometers (a typical inertial measurement unit – IMU), 9 DOF refers to an IMU plus a compass, 10 DOF adds a barometer and 11 DOF usually adds a GPS receiver.\n\nUAV actuators include digital electronic speed controllers (which control the RPM of the motors) linked to motors/engines and propellers, servomotors (for planes and helicopters mostly), weapons, payload actuators, LEDs and speakers.\n\nUAV software called the flight stack or autopilot. UAVs are real-time systems that require rapid response to changing sensor data. Examples include Raspberry Pis, Beagleboards, etc. shielded with NavIO, PXFMini, etc. or designed from scratch such as Nuttx, preemptive-RT Linux, Xenomai, Orocos-Robot Operating System or DDS-ROS 2.0.\n\nCivil-use open-source stacks include:\n\nUAVs employ open-loop, closed-loop or hybrid control architectures.\n\nUAVs can be programmed to perform aggressive manœuvres or landing/perching on inclined surfaces, and then to climb toward better communication spots. Some UAVs can control flight with varying flight modelisation, such as VTOL designs.\n\nUAVs can also implement perching on a flat vertical surface.\n\nMost UAVs use a radio for remote control and exchange of video and other data. Early UAVs had only narrowband uplink. Downlinks came later. These bi-directional narrowband radio links carried command and control (C&C) and telemetry data about the status of aircraft systems to the remote operator. For very long range flights, military UAVs also use satellite receivers as part of satellite navigation systems. In cases when video transmission was required, the UAVs will implement a separate analog video radio link. \n\nIn the most modern UAV applications, video transmission is required. So instead of having 2 separate links for C&C, telemetry and video traffic, a broadband link is used to carry all types of data on the a single radio link. These broadband links can leverage quality of service techniques to optimize the C&C traffic for low latency. Usually these broadband links carry TCP/IP traffic that can be routed over the Internet. \n\nThe radio signal from the operator side can be issued from either:\n\nICAO classifies unmanned aircraft as either remotely piloted aircraft or fully autonomous. Actual UAVs may offer intermediate degrees of autonomy. E.g., a vehicle that is remotely piloted in most contexts may have an autonomous return-to-base operation.\n\nBasic autonomy comes from proprioceptive sensors. Advanced autonomy calls for situational awareness, knowledge about the environment surrounding the aircraft from exterioceptive sensors: sensor fusion integrates information from multiple sensors.\n\nOne way to achieve autonomous control employs multiple control-loop layers, as in hierarchical control systems. As of 2016 the low-layer loops (i.e. for flight control) tick as fast as 32,000 times per second, while higher-level loops may cycle once per second. The principle is to decompose the aircraft's behavior into manageable \"chunks\", or states, with known transitions. Hierarchical control system types range from simple scripts to finite state machines, behavior trees and hierarchical task planners. The most common control mechanism used in these layers is the PID controller which can be used to achieve hover for a quadcopter by using data from the IMU to calculate precise inputs for the electronic speed controllers and motors.\n\nExamples of mid-layer algorithms:\nEvolved UAV hierarchical task planners use methods like state tree searches or genetic algorithms.\n\nUAV manufacturers often build in specific autonomous operations, such as:\n\nFull autonomy is available for specific tasks, such as airborne refueling or ground-based battery switching; but higher-level tasks call for greater computing, sensing and actuating capabilities. One approach to quantifying autonomous capabilities is based on OODA terminology, as suggested by a 2002 US Air Force Research Laboratory, and used in the table below:\n\nReactive autonomy, such as collective flight, real-time collision avoidance, wall following and corridor centring, relies on telecommunication and situational awareness provided by range sensors: optic flow, lidars (light radars), radars, sonars.\n\nMost range sensors analyze electromagnetic radiation, reflected off the environment and coming to the sensor. The cameras (for visual flow) act as simple receivers. Lidars, radars and sonars (with sound mechanical waves) emit and receive waves, measuring the round-trip transit time. UAV cameras do not require emitting power, reducing total consumption.\n\nRadars and sonars are mostly used for military applications.\n\nReactive autonomy has in some forms already reached consumer markets: it may be widely available in less than a decade.\n\nSLAM combines odometry and external data to represent the world and the position of the UAV in it in three dimensions. High-altitude outdoor navigation does not require large vertical fields-of-view and can rely on GPS coordinates (which makes it simple mapping rather than SLAM).\n\nTwo related research fields are photogrammetry and LIDAR, especially in low-altitude and indoor 3D environments.\n\nRobot swarming refers to networks of agents able to dynamically reconfigure as elements leave or enter the network. They provide greater flexibility than multi-agent cooperation. Swarming may open the path to data fusion. Some bio-inspired flight swarms use steering behaviors and flocking.\n\nIn the military sector, American Predators and Reapers are made for counterterrorism operations and in war zones in which the enemy lacks sufficient firepower to shoot them down. They are not designed to withstand antiaircraft defenses or air-to-air combat. In September 2013, the chief of the US Air Combat Command stated that current UAVs were \"useless in a contested environment\" unless manned aircraft were there to protect them.[167] A 2012 Congressional Research Service (CRS) report speculated that in the future, UAVs may be able to perform tasks beyond intelligence, surveillance, reconnaissance and strikes; the CRS report listed air-to-air combat (\"a more difficult future task\") as possible future undertakings.[168] The Department of Defense's Unmanned Systems Integrated Roadmap FY2013-2038 foresees a more important place for UAVs in combat.[169] Issues include extended capabilities, human-UAV interaction, managing increased information flux, increased autonomy and developing UAV-specific munitions.[169] DARPA's project of systems of systems, or General Atomics work may augur future warfare scenarios, the latter disclosing Avenger swarms equipped with High Energy Liquid Laser Area Defense System (HELLADS).\n\nCognitive radio technology may have UAV applications.\n\nUAVs may exploit distributed neural networks.\n\nThe global military UAV market is dominated by companies based in the United States and Israel. By sale numbers, The US held over 60% military-market share in 2017. Four of top five military UAV manufactures are American including General Atomics, Lockheed Martin, Northrop Grumman and Boeing, followed by the Chinese company CASC. Israel companies mainly focus on small surveillance UAV system and by quantity of drones, Israel exported 60.7% (2014) of UAV on the market while the United States export 23.9% (2014); top importers of military UAV are The United Kingdom (33.9%) and India (13.2%). United States alone operated over 9,000 military UAVs in 2014. General Atomics is the dominant manufacturer with the Global Hawk and Predator/Mariner systems product-line.\n\nThe civilian drone market is dominated by Chinese companies. Chinese drone manufacturer DJI alone has 75% of civilian-market share in 2017 with $11 billion forecast global sales in 2020. Followed by French company Parrot with $110m and US company 3DRobotics with $21.6m in 2014. As of March 2018, more than one million UAVs (878,000 hobbyist and 122,000 commercial) were registered with the U.S. FAA. 2018 NPD point to consumers increasingly purchasing drones with more advanced features with 33 percent growth in both the $500+ and $1000+ market segments.\n\nCivilian UAV market is relatively new compared to military. Companies are emerging in both developed and developing nations at the same time. Many early stage startups have received support and funding from investors like in United States and by government agencies as the case in India. Some universities offer research and training programs or degrees. Private entities also provide online and in-person training programs for both recreational and commercial UAV use.\n\nConsumer drones are also widely used by military organizations worldwide because of the cost-effective nature of consumer product. In 2018, Israeli military started to use DJI Mavic and Matrice series of UAV for light reconnaissance mission since the civilian drones are easier to use and have higher reliability. DJI drones is also the most widely used commercial unmanned aerial system that the US Army has employed.\n\nLighted drones are beginning to be used in nighttime displays for artistic and advertising purposes.\n\nThe AIA reports large cargo and passengers drones should be certified and introduced over the next 20 years.\nSensor-carrying large drones are expected from 2018; short-haul, low altitude freighters outside cities from 2025; long-haul cargo flights by the mid-2030s and then passenger flights by 2040.\nSpending should rise from a few hundred million dollars on research and development in 2018 to $4 billion by 2028 and $30 billion by 2036.\n\nFlapping-wing ornithopters, imitating birds or insects, are a research field in microUAVs. Their inherent stealth recommends them for spy missions.\n\nThe Nano Hummingbird is commercially available, while sub-1g microUAVs inspired by flies, albeit using a power tether, can \"land\" on vertical surfaces.\n\nOther projects include unmanned \"beetles\" and other insects.\n\nResearch is exploring miniature optic-flow sensors, called ocellis, mimicking the compound insect eyes formed from multiple facets, which can transmit data to neuromorphic chips able to treat optic flow as well as light intensity discrepancies.\n\nUAV endurance is not constrained by the physiological capabilities of a human pilot.\n\nBecause of their small size, low weight, low vibration and high power to weight ratio, Wankel rotary engines are used in many large UAVs. Their engine rotors cannot seize; the engine is not susceptible to shock-cooling during descent and it does not require an enriched fuel mixture for cooling at high power. These attributes reduce fuel usage, increasing range or payload.\n\nProper drone cooling is essential for long-term drone endurance. Overheating and subsequent engine failure is the most common cause of drone failure.\n\nHydrogen fuel cells, using hydrogen power, may be able to extend the endurance of small UAVs, up to several hours.\n\nMicro air vehicles endurance is so far best achieved with flapping-wing UAVs, followed by planes and multirotors standing last, due to lower Reynolds number.\n\nSolar-electric UAVs, a concept originally championed by the AstroFlight Sunrise in 1974, have achieved flight times of several weeks.\n\nSolar-powered atmospheric satellites (\"atmosats\") designed for operating at altitudes exceeding 20 km (12 miles, or 60,000 feet) for as long as five years could potentially perform duties more economically and with more versatility than low earth orbit satellites. Likely applications include weather monitoring, disaster recovery, earth imaging and communications.\n\nElectric UAVs powered by microwave power transmission or laser power beaming are other potential endurance solutions.\n\nAnother application for a high endurance UAV would be to \"stare\" at a battlefield for a long interval (ARGUS-IS, Gorgon Stare, Integrated Sensor Is Structure) to record events that could then be played backwards to track battlefield activities.\n\nReliability improvements target all aspects of UAV systems, using resilience engineering and fault tolerance techniques.\n\nIndividual reliability covers robustness of flight controllers, to ensure safety without excessive redundancy to minimize cost and weight. Besides, dynamic assessment of flight envelope allows damage-resilient UAVs, using non-linear analysis with ad-hoc designed loops or neural networks. UAV software liability is bending toward the design and certifications of manned avionics software.\n\nSwarm resilience involves maintaining operational capabilities and reconfiguring tasks given unit failures.\n\nThere are numerous civilian, commercial, military, and aerospace applications for UAVs. These include:\n\n\nUAVs are being developed and deployed by many countries around the world. Due to their wide proliferation, no comprehensive list of UAV systems exists.\n\nThe export of UAVs or technology capable of carrying a 500 kg payload at least 300 km is restricted in many countries by the Missile Technology Control Regime.\n\nUAVs can threaten airspace security in numerous ways, including unintentional collisions or other interference with other aircraft, deliberate attacks or by distracting pilots or flight controllers. The first incident of a drone-airplane collision occurred in mid-October 2017 in Quebec City, Canada. The first recorded instance of a drone collision with a hot air balloon occurred on 10 August 2018 in Driggs, Idaho, United States; although there was no significant damage to the balloon nor any injuries to its 3 occupants, the balloon pilot reported the incident to the NTSB, stating that \"I hope this incident helps create a conversation of respect for nature, the airspace, and rules and regulations.”\n\nUAVs could be loaded with dangerous payloads, and crashed into vulnerable targets. Payloads could include explosives, chemical, radiologial or biological hazards. UAVs with generally non-lethal payloads could possibly be hacked and put to malicious purposes. Anti-UAV systems are being developed by states to counter this threat. This is, however, proving difficult. As Dr J. Rogers stated in an interview to A&T \"There is a big debate out there at the moment about what the best way is to counter these small UAVs, whether they are used by hobbyists causing a bit of a nuisance or in a more sinister manner by a terrorist actor.”\n\nBy 2017, drones were being used to drop contraband into prisons.\n\nThe interest in UAVs cyber security has been raised greatly after the Predator UAV video stream hijacking incident in 2009, where Islamic militants used cheap, off-the-shelf equipment to stream video feeds from a UAV. Another risk is the possibility of hijacking or jamming a UAV in flight. Several security researchers have made public some vulnerabilities in commercial UAVs, in some cases even providing full source code or tools to reproduce their attacks. At a workshop on UAVs and privacy in October 2016, researchers from the Federal Trade Commission showed they were able to hack into three different consumer quadcopters and noted that UAV manufacturers can make their UAVs more secure by the basic security measures of encrypting the Wi-Fi signal and adding password protection.\n\nIn the United States, flying close to a wildfire is punishable by a maximum $25,000 fine. Nonetheless, in 2014 and 2015, firefighting air support in California was hindered on several occasions, including at the Lake Fire and the North Fire. In response, California legislators introduced a bill that would allow firefighters to disable UAVs which invaded restricted airspace. The FAA later required registration of most UAVs.\n\nThe use of UAVs is also being investigated to help detect and fight wildfires, whether through observation or launching pyrotechnic devices to start backfires.\n\nEthical concerns and UAV-related accidents have driven nations to regulate the use of UAVs.\n\nThe Irish Aviation Authority (IAA) requires all UAVs over 1 kg to be registered with UAVs weighing 4 kg or more requiring a license to be issued by the IAA.\n\n, the Dutch police are testing trained bald eagles to intercept offending UAVs.\n\nIn 2016 Transport Canada proposed the implementation of new regulations that would require all UAVs over 250 grams to be registered and insured and that operators would be required to be a minimum age and pass an exam in order to get a license. These regulations are expected to be introduced in 2018.\n\nIn April 2014, the South African Civil Aviation Authority announced that it would clamp down on the illegal flying of UAVs in South African airspace. \"Hobby drones\" with a weight of less than 7 kg at altitudes up to 500m with restricted visual line-of-sight below the height of the highest obstacle within 300m of the UAV are allowed. No license is required for such vehicles.\n\nIn order to fly a drone in Dubai, citizens have to obtain a no objection certificate from Dubai Civil Aviation Authority (DCAA). This certificate can be obtained online.\n\nThe ENAC (Ente Nazionale per l'Aviazione Civile), that is, the Italian Civil Aviation Authority for technical regulation, certification, supervision and control in the field of civil aviation, issued on May 31, 2016 a very detailed regulation for all UAV, determining which types of vehicles can be used, where, for which purposes, and who can control them. The regulation deals with the usage of UAV for either commercial and recreational use. The last version was published on December 22, 2016.\n\nIn 2015, Civil Aviation Bureau in Japan announced that \"UA/Drone” (refers to any airplane, rotorcraft, glider or airship which cannot accommodate any person on board and can be remotely or automatically piloted) should (A) not fly near or above airports, (B) not fly over 150 meter above ground/water surface, (C) not fly over urban area and suburb (so only rural area is allowed.) UA/drone should be operated manually and at Visual Line of Sight (VLOS) and so on. UA/drone should not fly near any important buildings or facilities of the country including nuclear facilities. UA/drone must follow the Japan Radio Act exactly.\n\nFrom 21 December 2015 all hobby type UAVs between 250 grams and 25 kilograms needed to be registered with FAA no later than 19 February 2016.\n\nThe new FAA UAV registration process includes requirements for:\n\nOn May 19, 2017, in the case \"Taylor v. Huerta\", the U.S. Court of Appeals for the District of Columbia Circuit held that the FAA's 2015 drone registration rules were in violation of the 2012 FAA Modernization and Reform Act. Under the court's holding, although commercial drone operators are required to register, recreational operators are not. On May 25, 2017, one week after the \"Taylor\" decision, Senator Diane Feinstein introduced S. 1272, the Drone Federalism Act of 2017, in Congress.\n\nOn 21 June 2016 the Federal Aviation Administration announced regulations for commercial operation of small UAS craft (sUAS), those between 0.55 and 55 pounds (about 250 gm to 25 kg) including payload. The rules, which exclude hobbyists, require the presence at all operations of a licensed Remote Pilot in Command. Certification of this position, available to any citizen at least 16 years of age, is obtained solely by passing a written test and then submitting an application. For those holding a sport pilot license or higher, and with a current flight review, a rule-specific exam can be taken at no charge online at the faasafety.gov website. Other applicants must take a more comprehensive examination at an aeronautical testing center. All licensees are required to take a review course every two years. At this time no ratings for heavier UAS are available.\n\nCommercial operation is restricted to daylight, line-of-sight, under 100 mph, under 400 feet, and Class G airspace only, and may not fly over people or be operated from a moving vehicle. Some organizations have obtained a waiver or Certificate of Authorization that allows them to exceed these rules. For example, CNN has obtained a waiver for UAVs modified for injury prevention to fly over people, and other waivers allow night flying with special lighting, or non-line-of-sight operations for agriculture or railroad track inspection.\n\nPrevious to this announcement, any commercial use required a full pilot's license and an FAA waiver, of which hundreds had been granted.\n\nThe use of UAVs for law-enforcement purposes is regulated at a state level. \n\nIn Oregon, law enforcement is allowed to operate non-weaponized drones without a warrant if there is enough reason to believe that the current environment poses imminent danger to which the drone can acquire information or assist individuals. Otherwise, a warrant, with a maximum period of 30 days of interaction, must be acquired. \n\nAs of December 20, 2017, UAVs of 250g or less are not controlled by the CAA guidances that include maintaining 50 meters from persons, animals or property.\n\nThe UAV must still not go higher than 400 ft with a single pilot or 1000 ft with a pilot and spotter; however as with UAVs above 300g, if within 400 ft of a structure, one is allowed to go 400 ft higher than the structure.\n\n\n\n"}
{"id": "12297096", "url": "https://en.wikipedia.org/wiki?curid=12297096", "title": "W46", "text": "W46\n\nTowards the end of 1955, consideration was given to using the physics package of the TX-46 aerial bomb as a warhead for the USAF Snark intercontinental cruise missile. Consideration to use one of the Army's Redstone MRBM was also given. The XW-46/Redstone was canceled in favor of the Titan II/W-53 combination in April 1958.\n"}
{"id": "222333", "url": "https://en.wikipedia.org/wiki?curid=222333", "title": "Western Digital", "text": "Western Digital\n\nWestern Digital Corporation (abbreviated WDC, commonly known as Western Digital and abbreviated WD) is an American computer hard disk drive manufacturer and data storage company. It designs, manufactures and sells data technology products, including storage devices, data center systems and cloud storage services.\n\nWestern Digital Corporation has a long history in the electronics industry as an integrated circuit maker and a storage products company. It is also one of the larger computer hard disk drive manufacturers, along with its primary competitor Seagate Technology.\n\nIn October 2009, Western Digital announced the shipment of first 3 TB internal hard disk drive, which has 750 GB-per-platter density with SATA interface.\n\nIn March 2011, Western Digital agreed to acquire the storage unit of Hitachi, HGST, for about $4.3 billion of which $3.5 billion was paid in cash and the rest with 25 million shares of Western Digital.\n\nIn March 2012, Western Digital completed the acquisition of HGST and became the largest traditional hard drive manufacturer in the world; to address the requirements of regulatory agencies, in May 2012 Western Digital divested assets to manufacture and sell certain 3.5-inch hard drives for the desktop and consumer electronics markets to Toshiba.\n\nIn February 2014, Western Digital announced a new \"Purple\" line of hard disk drives for use in video surveillance systems, with capacities from 1 to 4 TB. They feature internal optimizations for applications that involve near-constant disk writing, and \"AllFrame\" technology which is designed to reduce write errors.\nIn April 2017, Western Digital moved its headquarters from Irvine, California to San Jose, California. In December 2017 Western Digital reached an agreement with Toshiba about the sale of the jointly owned NAND production facility in Japan. In May 2018 Toshiba reached an agreement with the Bain consortium about the sale of that chip unit.\n\nIn May 2016, Western Digital acquired SanDisk for US$19 billion. In the summer of 2017, Western Digital licensed the Fusion-io/SanDisk ION Accelerator software to One Stop Systems. In August 2017, Western Digital acquired Upthere, which offers apps that sync files and photos across devices, with the intention to continue building out the service. In September 2017, Western Digital acquired Tegile Systems, maker of flash memory storage arrays.\n\nIn July 2018, Western Digital announced their plan to close their hard disk production facility in Kuala Lumpur. The company ranked 158th on the 2018 Fortune 500 of the largest United States corporations by revenue.\n\nSelf-encrypting Western Digital hard drives have been reported to have severe faults and to be easy to decrypt.\n\nWestern Digital was the last manufacturer of parallel ATA hard disk drives for laptops (2.5-inch form factor) and desktop PCs (3.5-inch form factor), producing them until December 2013. Furthermore, they were the only manufacturer that have 250 GB and 320 GB in 2.5-inch form factor.\n\nWestern Digital sells data center software and system solutions. Including an enterprise-class Ultrastar product line. In October 2017, Western Digital shipped the world’s first 14 TB HDD, the helium-filled HGST Ultrastar Hs14.\n\nWestern Digital sells consumer storage products under the WD brand, with product families called My Passport, My Book, WD TV, and My Cloud. While traditionally these products have used HDDs, Western Digital has started to offer SSD versions, such as the My Passport SSD, its first portable SSD. In September 2015, Western Digital released My Cloud OS 3, a platform that enables connected HDDs to sync between PCs and mobile devices.\n\nWestern Digital also sells consumer data technology products under the SanDisk, and G-Technology product brands, as well as cloud storage services under the Upthere brand. Through Western Digital’s acquisition of Upthere, the company offers personal cloud storage through the Upthere Home app and UpOS operating system.\n\nUnder the SanDisk brand, Western Digital offers mobile storage products, cards and readers, USB flash drives, SSDs and MP3 players. Most of Western Digital’s consumer flash memory products are offered through SanDisk. The SanDisk iXpand product family, including the iXpand Flash Drive and iXpand Base, is made specifically for use with the Apple iPhone and iPad. The 400GB SanDisk Ultra microSDXC UHS-I card was designed primarily for use in Android smartphones that include an expansion slot.\n\nUnder the G-Technology brand, Western Digital offers HDD, SSD, platforms and systems products designed specifically for creative professionals.\n\nFormer offerings include Western Digital Media Center and over-the-top set-top boxes.\n\nWestern Digital Capital is Western Digital's investment arm. It has contributed funding for data technology companies such as Elastifile and Avere Systems.\n\nLawsuits have been filed against various manufacturers including Western Digital, related to the claimed capacity of their drives. The drives are labelled using the convention of 10 (1,000) bytes to the kilobyte, resulting in a perceived capacity shortfall when reported by most operating systems, which tend to use 2 (1,024) bytes to the kilobyte.\n\nWhile Western Digital maintained that they used \"the indisputably correct industry standard for measuring and describing storage capacity\", and that they \"cannot be expected to reform the software industry\", they agreed to settle in March 2006, with a $30 refund to affected customers in the form of backup and recovery software of the same value.\n\n"}
{"id": "13274760", "url": "https://en.wikipedia.org/wiki?curid=13274760", "title": "Young Engineers' Satellite 2", "text": "Young Engineers' Satellite 2\n\nThe Young Engineers' Satellite 2 (YES2) is a 36 kg student-built tether satellite that is part of ESA's Foton-M3 microgravity mission. The launch of the Russian Foton-M3 occurred on September 14, 2007 at 13:00 (CEST) by a Soyuz-U launcher. The project was carried out by Delta-Utec SRC and supervised by the ESA Education Office and was nearly entirely designed and build by students and young engineers.\n\nThe YES2 deployment took place Sept. 25, 2007. The mission objective was to deploy a 30 km long and 0.5 mm thin tether (made of Dyneema) in two controlled stages, in order to release a small, spherical, lightweight reentry capsule called Fotino into a predetermined trajectory to a landing area in Kazakhstan. The scientific objectives of the mission have been achieved. The YES2 featured the first multi-stage tether deployment. It could be reconstructed within about 20 m accuracy for the first stage (3400 m) and 100–150 m for the 31.7 km deployment as a whole. The first stage was deployed accurately (about 10–20 m error), the second stage overdeployed by 1.7 km. Fotino released as planned during a swing of the tethered system through the vertical (as seen from Foton). The tether properties, deployment dynamics and tether deployer system performance could be evaluated. The tether deployer performed nominally. However, due to an electrical fault, the on-board computer failed to register the final length correctly and only a partial deployment was initially reported based on telemetry available in real-time. Initial deployment friction was found to exceed the nominal range, revealed by post-mission testing to be most likely due to a thermomechanical settling of the tether spool Some weeks after mission completion, analysis of the full data set confirmed that the tether deployed to its full length of 31.7 km. No signal was ever received from the \"Fotino\" re-entry capsule after separation, and it was lost. YES2 established a new world record as the longest artificial structure in space and was later included in the Guinness Book of Records Edition 2009.\n\nMost of the work done in this ambitious project (like design, manufacturing and integration) was done by students and young engineers. In total some 450 students participated.\n\nSoon after the beginning of the project, four \"Centres of Expertise\" were created. These were universities which were responsible for parts of the satellite or subsystems. The centres were: Samara State Aerospace University, Russia (mission analysis, GPS); University of Modena and Reggio Emilia, Italy (re-entry capsule); Hochschule Niederrhein in Krefeld, Germany (tether); University of Patras, Greece (mechanical and thermal). Coordination and system engineering was carried out by prime contractor Delta-Utec SRC from the Netherlands.\n\nTowards the end of the project, in the manufacturing and integration phase, the work concentrated on the Delta-Utec office in Leiden and ESA's ESTEC in Noordwijk, where the satellite was built and tested.\n\nThe test program included:\n\nThe satellite was handed over to ESA at the beginning of May 2007 and was shipped to Samara (Russia) soon after, where YES2 was mated to Foton-M3 for the first time for test purposes. Afterwards YES2 and Foton were separated again and brought to Baikonur (Kazakhstan) by train where the whole satellite was completely integrated and mated with the launcher, a Soyuz-U rocket. Foton-M3 and YES2 finally launched on 14 September 2007 at 13:00 (CEST) from the Gagarin launch pad at Baikonur Cosmodrome.\n\nThe main contribution of the project has been the demonstration of a complex controlled deployment in two stages. Post-flight, several independent sources of deployment data were collected, including deployment length and rate measurements from YES2 itself as well as highly precise triaxial accelerometer data from a separate experiment on the Foton carrier spacecraft. These data confirmed that the deployment did progress mostly successfully, in particular the critical first stage and stage transition and the tether deployer performed nominally. The data that has been recovered has helped to understand the deployer performance and tether dynamics in yet unseen detail, including explicit signatures of sound waves, transverse waves and spring-mass motion. The small reentry capsule Fotino, intended to demonstrate the SpaceMail concept, was not successfully recovered. Calculations based on YES2 sensor data indicate that the landing site should be in or near the Aral Sea. Alternatively, the capsule, experimental in itself, may have burnt up or crashlanded.\n\nThere are three main components of the experiment:\n\n\nDuring the flight, the FLOYD mechanism ejected the other two components. There was then to be a controlled deployment of a 30 km long tether. Orbital dynamics caused the Fotino capsule to be positioned in front of the mother spacecraft. By bringing the deployment to a halt, a pendulum-like swing was induced. When the capsule and tether swung through the local vertical, the tether was cut. Since the capsule was then going too slowly to stay in orbit, it has entered a trajectory to re-enter the atmosphere from an altitude of about 250 km, protected by a heat shield made of novel materials. Once it reached an altitude of 5 km, a parachute was intended to deploy to ensure a soft landing on the steppes of Kazakhstan.\n\nSince data from the Fotino was not downlinked and the Fotino capsule itself was not recovered, it is not known how well the capsule has survived the entry.\n\n\n"}
