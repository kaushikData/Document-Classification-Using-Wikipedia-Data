{"id": "33102937", "url": "https://en.wikipedia.org/wiki?curid=33102937", "title": "Activating function", "text": "Activating function\n\nThe activating function is a mathematical formalism that is used to approximate the influence of an extracellular field on an axon or neurons. It was developed by Frank Rattay and is a useful tool to approximate the influence of functional electrical stimulation (FES) or neuromodulation techniques on target neurons. It points out locations of high hyperpolarization and depolarization caused by the electrical field acting upon the nerve fiber. As a rule of thumb, the activating function is proportional to the second-order spatial derivative of the extracellular potential along the axon.\n\nIn a compartment model of an axon, the activating function of compartment n, formula_1, is derived from the driving term of the external potential, or the equivalent injected current\n\nformula_2,\n\nwhere formula_3 is the membrane capacity, formula_4 the extracellular voltage outside compartment formula_5 relative to the ground and formula_6 the axonal resistance of compartment formula_5.\n\nThe activating function represents the rate of membrane potential change if the neuron is in resting state before the stimulation. Its physical dimensions are V/s or mV/ms. In other words, it represents the slope of the membrane voltage at the beginning of the stimulation.\n\nFollowing McNeal's simplifications for long fibers of an ideal internode membrane, with both membrane capacity and conductance assumed to be 0 the differential equation determining the membrane potential formula_8 for each node is:\n\nformula_9,\n\nwhere formula_10 is the constant fiber diameter, formula_11 the node-to-node distance, formula_12 the node length formula_13 the axomplasmatic resistivity, formula_3 the capacity and formula_15 the ionic currents. From this the activating function follows as:\n\nformula_16.\n\nIn this case the activating function is proportional to the second order spatial difference of the extracellular potential along the fibers. If formula_17 and formula_18 then:\n\nformula_19.\n\nThus formula_20 is proportional to the second order spatial differential along the fiber.\n\nPositive values of formula_20 suggest a depolarization of the membrane potential and negative values a hyperpolarization of the membrane potential.\n"}
{"id": "4226856", "url": "https://en.wikipedia.org/wiki?curid=4226856", "title": "Amadeus IT Group", "text": "Amadeus IT Group\n\nAmadeus IT Group is a major Spanish IT provider for the global travel and tourism industry.\n\nThe company is structured around two areas: its global distribution system and its IT Solutions business area. Acting as an international network, Amadeus provides search, pricing, booking, ticketing and other processing services in real-time to travel providers and travel agencies through its Amadeus CRS distribution business area. Through its IT Solutions business area, it also offers travel companies software systems which automate processes such as reservations, inventory management and departure control.\n\nThe group, which processed 850 million billable travel transactions in 2010, services customers including airlines, hotels, tour operators, insurers, car rental and railway companies, ferry and cruise lines, travel agencies and individual travellers directly.\n\nThe parent company of Amadeus IT Group, holding over 99.7% of the firm, is Amadeus IT Holding S.A. It is listed on the Spanish stock exchanges as of 29 April 2010 and trades under the symbol AMS. For the year ended 31 December 2012, the company reported revenues of €2.910 billion and EBITDA of €1.108 billion.\n\nAmadeus has central sites in Madrid, Spain (corporate headquarters and marketing), Sophia Antipolis, France (product development), Breda, Netherlands (PMS development), Erding, Germany (data processing centre) and Bangalore, India (software lab) as well as regional offices in Boston, Bangkok, Buenos Aires, Dubai, Miami, Istanbul, Singapore, and Sydney. At market level, Amadeus maintains customer operations through 173 local Amadeus Commercial Organisations (ACOs) covering 195 countries. The Amadeus group employs 14,200 employees worldwide, and listed in Forbes' list of \"The World's Largest Public Companies\" as No. 985.\n\nAmadeus was originally created as a neutral global distribution system (GDS) by Air France, Iberia, Lufthansa and SAS in 1987 in order to connect providers' content with travel agencies and consumers in real time. The creation of Amadeus was intended to offer a European alternative to Sabre, an American GDS. The first Amadeus system was built from core reservation system code coming from System One, an American GDS that competed with Sabre but went bankrupt, and a copy of the Air France pricing engine. These systems were respectively running under IBM TPF and Unisys. At the beginning of Amadeus, the Amadeus systems were functionally dedicated to airline reservation and centered on the PNR (Passenger Name Record), the passenger's travel file. Throughout the years, the PNR was opened up to additional travel industries (hotels, rail, cars, cruises, ferries, insurance, etc.).\n\nAlthough established initially as a private partnership, Amadeus went public in October 1999, becoming listed on the Paris, Frankfurt and Madrid stock exchanges. Progressively and in line with industry evolution, Amadeus diversified its operations by focusing on information technologies (IT) to deliver services spanning beyond sales and reservation functionalities, centred on streamlining the operational and distribution requirements of its customer base.\n\nIn 2000, Amadeus received an ISO 9001:2000 quality certification – the first GDS company to do so. Since 2004, the company has invested €1 billion in R&D and Amadeus's technology has increasingly embraced open systems which provide clients with more flexibility and features, as well as other benefits. , 85% of its addsoftware portfolio was open system based and it expects by the end of 2016 to have fully migrated away from mainframe-based TPF software.\n\nIn 2005, Amadeus was delisted from the Paris, Frankfurt and Madrid stock exchanges when BC Partners and Cinven bought their stake from three of the four founding airlines and the rest of the capital floated from institutional and minority shareholders. The transition from distribution system to technology provider was reflected by the change in its corporate name in 2006, when the company name was changed to Amadeus IT Group. In 2009, Amadeus invested about €257 million in R&D. Amadeus is again listed on the Spanish Stock Exchanges as of 29 April 2010 (AMS). Throughout the years, Amadeus acquired: \nIn September 2014, Air France sold a 3% stake in the firm for $438 million.\n\nAmadeus has its own data centre in Erding, Germany. In 2010, the Erding complex processed ½ billion transactions per day, and handled, on average, user data queries per second, with a system response time of less than 3 milliseconds and an average system uptime of 99.99%.\n\nAmadeus' global operations comprise not only the main site in Erding, Germany, but also two strategic operation centres in Miami, United States and Sydney, Australia, and local competency centres in Germany, Thailand, India, Poland, Colombia, Ukraine and the United Kingdom.\n\nAmadeus CRS is the largest GDS provider in the worldwide travel and tourism industry, with an estimated market share of 37% in 2009. As of December 2010, over travel agencies worldwide use the Amadeus system and airline sales offices use it as their internal sales and reservations system. Amadeus gives access to bookable content from 435 airlines (including 60 low-cost carriers), 29 car rental companies (representing car rental locations), 51 cruise lines and ferry operators, 280 hotel chains and hotels, 200 tour operators, 103 rail operators and 116 travel insurance companies.\n\nThe principal service of this business area is the Amadeus Altéa Customer Management System (CMS), a software suite which addresses airlines' sales and reservations, inventory management and departure control.\n\nUnlike the carriers’ legacy IT systems, the Altéa platform is based on a common technical infrastructure and software. With Altéa, airlines outsource their IT operations onto a community platform which allows them to share information with both airline alliance and code-share partners.\n\nThe Altéa suite presently consists of four main modules: Altéa Reservation, providing booking, pricing and ticketing management through a single interface; Altéa Inventory, providing schedule and seat capacity management on a flight-by-flight basis; Altéa Departure Control, a departure control system software package; and Altéa e-commerce, a software suite for airline e-commerce sales and support.\n\nIn 2009, the number of passengers boarded by airlines using Amadeus Altéa was 238 million. Amadeus is extending its IT solutions business with the ongoing development of similar systems for rail companies, hotel chains, airport operators and ground handling companies.\n\nAccording to a May 2015 investigation, Amadeus has contributed to the Docker open source software project.\n\nThe business model of Amadeus is booking fee or transaction based, which means that a fee is taken for each confirmed net booking made in the Amadeus CRS. \n\nIn late 1990s, a business division specialized in e-commerce was created.\n\nIn 2000, Amadeus was awarded the development of two new operational applications for British Airways and Qantas: the inventory management and the departure control systems. These products were outside of the core expertise domain of Amadeus and were built with the expertise of the airlines.\n\nIn March 2015, Amadeus announced that Blacklane, a Berlin-based professional driver service available worldwide, would become their first fully integrated taxi and transfer solution provider.\n"}
{"id": "7242241", "url": "https://en.wikipedia.org/wiki?curid=7242241", "title": "Authorize.Net", "text": "Authorize.Net\n\nAuthorize.Net is a United States-based payment gateway service provider allowing merchants to accept credit card and electronic check payments through their website and over an Internet Protocol (IP) connection. Founded in 1996, Authorize.Net is now a subsidiary of Visa Inc. Its service permits customers to enter credit card and shipping information directly onto a web page, in contrast to some alternatives that require the customer to sign up for a payment service before performing a transaction.\n\nAuthorize.Net was founded in 1996, in Utah, by Jeff Knowles. Its primary market is small- to medium-sized businesses. As of 2004 it had about 90,000 customers.\n\nAuthorize.Net was one of several companies acquired by Go2Net, a company backed by Microsoft founder Paul Allen, in 1999, for 90.5 million in cash and stock. Go2Net was acquired by InfoSpace in 2000 for about 4 billion; Authorize.Net was acquired by Lightbridge in 2004 for 82 million and then by CyberSource in 2007.\n\nVisa Inc. acquired CyberSource in 2010 for 2 billion. Visa has maintained Authorize.Net and Cybersource as separate services, with Authorize.Net concentrating on small- to medium-sized businesses, and Cybersource concentrating on international and large-scale payment processing. At the time of the 2010 acquisition, the company's CEO identified three priorities: expanding the ecommerce market, enhancing fraud detection and prevention, and improving data security. As of 2014, along with parent CyberSource, it had about 450,000 customers. \n\nIn September 2004, Authorize.Net's servers were hit by a Distributed Denial of Service (DDoS) attack. The DDoS attack lasted for over one week and caused a virtual shut down of the payment gateway's service. The attackers demanded money from Authorize.net in exchange for stopping the attack.\n\nOn July 2, 2009, 11pm, the entire web infrastructure for Authorize.Net (main website, merchant gateway website, etc.) went offline and stayed down all morning July 3, 2009. None of the over 200,000 merchants who use Authorize.Net payment gateway were able to process credit cards. Authorize.Net's phone numbers were closed July 3 because of the July 4th holiday as previously announced on their website (though the website was down at the time). Other companies that have nearby offices have reported to the media that there was a fire. Authorize.net started a Twitter account that morning, but did not update their phones to give notice to customers until July 5 when they reopened phones.\n\nAuthorize.Net processes card and ACH payments for companies from small and medium-sized merchants. It offers fraud protection services, recurring billing subscriptions, and simple checkout options. In addition to major credit cards, it permits payments using PayPal, Apple Pay, and Visa Checkout, and it can integrate with Quickbooks. For developers, it provides an application programming interface (API) and software development kits for Android and iOS. Its Virtual Terminal and Invoice features can process manual payments. It also offers recurring billing and a plugin for the integration with Authorize.Net, and technical support is available for merchants.\n\nAuthorize.Net has the most customers of any payment processor, and has been described as one of the more senior players in the payment processing industry, retaining a \"decent portion\" of the industry's market share.\n\n"}
{"id": "40003849", "url": "https://en.wikipedia.org/wiki?curid=40003849", "title": "Bronchial blocker", "text": "Bronchial blocker\n\nAn bronchial blocker (also called endobronchial blocker) is a device which can be inserted down a tracheal tube after tracheal intubation so as to block off the right or left main bronchus of the lungs in order to be able to achieve a controlled one sided ventilation of the lungs in thoracic surgery. The lung tissue distal to the obstruction will collapse, thus allowing the surgeon's view and access to relevant structures within the thoracic cavity.\n\nBronchial blockers are used to achieve lung separation and one lung ventilation as an alternative to double-lumen endotrachealtubes (DLT) and are the method of choice in children and paediatric patients for whom even the smallest DLTs might be too big.\n\nMade by Fuji Systems, Tokyo, Japan, is a tracheal tube with a second lumen that contains a coaxial, balloon tipped catheter which can be advanced under fiber optic bronchoscopy and blocked in either bronchus.\n\nProduced by Cook Critical Care, Bloomington, USA, is a catheter with a balloon tip and inner lumen which contains a flexible wire which is coupled to a fiber optical bronchoscope to guide the device into the desired bronchus.\n\nBy Cook Critical Care, is a catheter shaft with a distal soft nylon flexible tip and balloon which can be deflected by 90° to guide the device into either bronchus.\n\nBy Smith Medical, Rosmalen, NL, has a preformed angulation at the distal tip to aid placement in the desired bronchus.\n\nBy Teleflex Inc., USA, a Y-shaped bronchial blocker with two distal extensions to be placed in both main stem bronchi.\n\n"}
{"id": "6234521", "url": "https://en.wikipedia.org/wiki?curid=6234521", "title": "Building performance", "text": "Building performance\n\nBuilding performance is an attribute of a building that expresses how well that building carries out its functions. It may also relate to the performance of the building construction process. Categories of building performance are quality (how well the building fulfils its functions), resource saving (how much resource is needed to fulfil its functions) and workload capacity (how much the building can do). The performance of a building depends on the response of the building to an external load or shock. Building performance plays an important role in architecture, building services engineering, building regulation, architectural engineering and construction management. Prominent building performance aspects are energy efficiency, thermal comfort, indoor air quality and daylighting.\n\nBuilding performance has been of interest to humans from the very first shelters built to protect us from the weather, natural enemies and other dangers. Initially design and performance were managed by craftsmen who combined expertise in both domains. More formal approaches to building performance appeared in the 1970s and 1980s, with seminal works being the book on \"Building Performance\" and \"CIB Report 64\". Further progress on building performance studies took place in parallel with the development of building science as a discipline, and with the introduction of personal computing (especially computer simulation) in the field; for a good overview of the role of simulation in building design see the chapter by Augenbroe. A more general overview that also includes physical measurement, expert judgement and stakeholder evaluation is presented in the book \"Building Performance Analysis\".\n\nAlthough many buildings in the U.S., Canada, U.K., and elsewhere claim to be “green,” “low energy,” or “high performance,” it is rarely clear on what evidence or data these claims are based. Such claims cannot be credible without standardized performance measurement protocols that are applied consistently. If claims of superior building performance are to be believed, it is essential that a common set of measurements be used and the results reported against meaningful benchmarks. Such protocols are also needed to give usable feedback to building designers and operators when measured performance does not match design intent. \nThis article describes ASHRAE’s Performance Measurement Protocols for Commercial Buildings (PMP), which provides a standardized, consistent set of protocols, for a range of cost/accuracy, to facilitate the appropriate comparison of measured energy, water, and indoor environmental quality (thermal comfort, indoor air quality [IAQ], lighting, and acoustics) performance of commercial buildings, while maintaining acceptable levels of building service for the occupants. Benchmarks are included in the protocols to facilitate comparison to peer buildings or for self-reference over time (often before and after retrofit). A recent article describing just the acoustic performance measurement protocols in the PMP has been published in the ASHRAE Journal. \nThe PMP is a collaborative effort of ASHRAE, the U.S. Green Building Council (USGBC), and the Chartered Institution of Building Service Engineers (CIBSE). It began with a detailed evaluation of literature related to measured building performance that included databases, measurement techniques, M&V protocols, and available instrumentation. A project committee representing several ASHRAE Technical Committees (TCs 7.6, 7.9, 4.7, and others) developed the content.\n\nThe protocols identify what to measure, how it is to be measured (instrumentation and spatial resolution), and how often it is to be measured for inclusion in the building’s operation and maintenance plan. For each of the six measure categories (energy, water, thermal comfort, IAQ, lighting, and acoustics), protocols are developed at three levels: low, medium and high cost and accuracy, providing a range of choices for levels of effort, detail, and rigor to characterize the building stock, and comparison to appropriate benchmarks.\n\nFor each measure category and each level, the following characteristics are described: \nIn the PMP, first the six measure categories are addressed at the basic level. This is followed by a presentation of the intermediate level protocols and then the advanced level protocols. However, here we will primarily discuss the basic level protocols, with only a brief description of higher levels; details of the intermediate and advanced levels are described in the PMP. \n\nRepresentative measures at each level were tested by applying them to the newly renovated ASHRAE Headquarters Building in Atlanta (facing page). Measurements were taken by the commissioning authority for the renovation during 2009 and 2010, after the renovated building was reoccupied and was undergoing post-occupancy commissioning. Occupant surveys were taken before and after the renovation to evaluate indoor environmental quality.\n\n"}
{"id": "17065840", "url": "https://en.wikipedia.org/wiki?curid=17065840", "title": "Consciousness Industry", "text": "Consciousness Industry\n\nThe Consciousness Industry is a term coined by author and theorist Hans Magnus Enzensberger, which identifies the mechanisms through which the human mind is reproduced as a social product. Foremost among these mechanisms are the institutions of mass media and education. According to Enzensberger, the mind industry does not produce anything specific; rather, its main business is to perpetuate the existing order of man's domination over man.\n\nHans Haacke elaborates on the consciousness industry as it applies to the arts in a wider system of production, distribution, and consumption. Haacke specifically implicates museums as manufacturers of aesthetic perception that fail to acknowledge their intellectual, political, and moral authority: \"rather than sponsoring intelligent, critical awareness, museums thus tend to foster appeasement.\"\n\n\n\nMuseums: Managers of Consciousness \n"}
{"id": "5629357", "url": "https://en.wikipedia.org/wiki?curid=5629357", "title": "DNA field-effect transistor", "text": "DNA field-effect transistor\n\nA DNA field-effect transistor (DNAFET) is a field-effect transistor which uses the field-effect due to the partial charges of DNA molecules to function as a biosensor. The structure of DNAFETs is similar to that of MOSFETs with the exception of the gate structure which, in DNAFETs, is replaced by a layer of immobilized ssDNA (single-stranded DNA) molecules which act as surface receptors. When complementary DNA strands hybridize to the receptors, the charge distribution near the surface changes, which in turn modulates current transport through the semiconductor transducer.\n\nArrays of DNAFETs can be used for detecting single nucleotide polymorphisms (causing many hereditary diseases) and for DNA sequencing. Their main advantage compared to optical detection methods in common use today is that they do not require labeling of molecules. Furthermore, they work continuously and (near) real-time. DNAFETs are highly selective since only specific binding modulates charge transport.\n\n"}
{"id": "8488200", "url": "https://en.wikipedia.org/wiki?curid=8488200", "title": "Damp (mining)", "text": "Damp (mining)\n\nHistorically, gases (other than breathable air) in coal mines in Britain were collectively known as \"damps\". This comes from the Middle Low German word \"dampf\" (meaning \"vapour\"), and was in use by 1480.\n\nDamps included:\n\nThe term \"damp\" also gives rise to damp sheet, a heavy curtain used to direct air currents and prevent the buildup of dangerous gases.\n\n\n"}
{"id": "2791910", "url": "https://en.wikipedia.org/wiki?curid=2791910", "title": "Deely bobber", "text": "Deely bobber\n\nA deely bobber (also deeley bobber or deeley boppers) is a novelty item of headgear comprising a headband to which are affixed two springy protrusions resembling the antennae of insects or of stereotypical little green men. These \"antennae\" may be topped with simple plastic shapes or more elaborate and fanciful decorations, such as mini pom poms or light emitting diodes. The name \"deely bobber\" is a genericized trademark; other names include deely-boppers, bonce boppers, or space boppers. In June 1982, a \"New York Times\" headline called them Martian antennae.\n\nStephen Askin invented the original deely bobber in 1981, inspired by the \"Killer Bees\" costumes on \"Saturday Night Live\". Askin was a serial entrepreneur who had sold dartboards depicting Ayatollah Khomeini during the Iran hostage crisis of 1980. Askin made prototype Deely Bobbers in his kitchen and test-marketed them at the Los Angeles Street Fair of summer 1981, selling 800 at US$5 each. He sold the invention to the Ace Novelty Co. of Bellevue, Washington, which launched it in January 1982 at the California Gift Fair. The name \"Deely Bobber\" was suggested by the wife of John Minkove, an Ace marketer; it had been her schoolfriend's placeholder name for \"thingamajig\". It was previously a brand of toy block sold 1969–1973. Deely boppers began retailing in April 1982 at US$3. They quickly became a fad in the United States, before reaching the United Kingdom in July. At the 1982 World's Fair in Knoxville, Tennessee, 10,000 a day were sold; total sales by August were estimated at 2 million, with Askin getting 5% of the wholesale price. Imitations costing $1–2 undercut the original, though Askin applied for a patent. The original decorations for the antennae were polystyrene shapes covered in sparkles: spheres, stars, hearts. Flashing lights were added to cash in on the hit movie \"E.T. the Extra-Terrestrial\", with seasonal themes for later holidays.\n"}
{"id": "38689183", "url": "https://en.wikipedia.org/wiki?curid=38689183", "title": "Denis Browne bar", "text": "Denis Browne bar\n\nThe Denis Browne bar, also known as the Denis Browne splint or foot abduction orthosis, is a medical device used in the treatment of club foot. The device is named after Sir Denis Browne (1892-1967), an Australian-born surgeon at Great Ormond Street Hospital in London who was considered the father of pediatric surgery in the United Kingdom. Browne first described the device in 1934. The bar may be used as part of the Ponseti method, a series of nonsurgical techniques to address club foot.\n\n"}
{"id": "14353326", "url": "https://en.wikipedia.org/wiki?curid=14353326", "title": "Dress form", "text": "Dress form\n\nA dress form is a three-dimensional model of the torso used for fitting clothing that is being designed or sewed. When making a piece of clothing, it can be put on the dress form so one can see the fit and drape of the garment as it would appear on a body, and make adjustments or alterations. Dress forms come in all sizes and shapes for almost every article of clothing that can be made. Dress forms in standard clothing sizes are used to make patterns, while adjustable dress forms allow garments to be tailored to fit a specific individual.\n\nThis is often colloquially referred to as a \"Judy\" for the female form and a \"James\" for the male.\n\n"}
{"id": "286256", "url": "https://en.wikipedia.org/wiki?curid=286256", "title": "Drop (telecommunication)", "text": "Drop (telecommunication)\n\nIn a communications network, a drop is the portion of a device directly connected to the internal station facilities, such as toward a telephone switchboard, toward a switching center, or toward a telephone exchange. A drop can also be a wire or cable from a pole or cable terminus to a building, in which case it may be referred to as a downlead. These cables may be reinforced to withstand the tension (due to gravity and weather) of an aerial drop (i.e., hanging in air), as in \"messenger\" type RG-6 coaxial cable, which is reinforced with a steel messenger wire along its length.\n\n"}
{"id": "7544715", "url": "https://en.wikipedia.org/wiki?curid=7544715", "title": "Electronic centralised aircraft monitor", "text": "Electronic centralised aircraft monitor\n\nAn electronic centralised aircraft monitor (ECAM) is a system that monitors aircraft functions and relays them to the pilots. It also produces messages detailing failures and in certain cases, lists procedures to undertake to correct the problem.\n\nECAM is similar to another system, known as Engine Indicating and Crew Alerting System (EICAS), used by Boeing and Embraer, which displays data concerning aircraft systems and also failures. Airbus developed ECAM, such that it not only provided the features of EICAS, but also displayed corrective action to be taken by the pilot, as well as system limitations after the failures. Using a colour-coded scheme the pilots can instantly assess the situation and decide on the actions to be taken. It was designed to ease pilot stress in abnormal and emergency situations, by designing a paperless cockpit in which all the procedures are instantly available.\n\nECAM is actually a series of systems designed to work in unison to display information to the pilots in a quick and effective manner. Sensors placed throughout the aircraft, monitoring key parameters, feed their data into two System Data Acquisition Concentrator (SDACs) which in turn process the data and feed it to two Flight Warning Computers (FWCs). The FWCs check for discrepancies in the data and then display the data on the ECAM displays through the three Display Management Computers (DMCs). In the event of a fault the FWCs generate the appropriate warning messages and sounds. More vital systems are routed directly through the FWCs such that failures in them can still be detected even with the loss of both SDACs. The whole system can continue to operate even with a failure of one SDAC and one FWC.\n\nFailures are classed by importance ranging from level 1 failures to level 3 failures. In the event of simultaneous failures the most critical failure is displayed first. The warning hierarchy is as follows:\n\n\nIn addition to the three failure levels are following status messages:\n\n\n"}
{"id": "37752399", "url": "https://en.wikipedia.org/wiki?curid=37752399", "title": "Embedded instrumentation", "text": "Embedded instrumentation\n\nIn the electronics industry, embedded instrumentation refers to the integration of test and measurement instrumentation into semiconductor chips (or integrated circuit devices). Embedded instrumentation differs from embedded system, which are electronic systems or subsystems that usually comprise the control portion of a larger electronic system. Instrumentation embedded into chips (embedded instrumentation) is employed in a variety of electronic test applications, including validating and testing chips themselves, validating, testing and debugging the circuit boards where these chips are deployed, and troubleshooting systems once they have been installed in the field.\n\nA working group of the IEEE (Institute of Electrical and Electronic Engineers) that is developing a standard for accessing embedded instruments (the IEEE 1687 Internal JTAG standard) defines embedded instrumentation as follows:\n\n\"Any logic structure within a device whose purpose is Design for Test (DFT), Design-for-Debug (DFD), Design-for-Yield (DFY), Test… There exists the widespread use of embedded instrumentation (such as BIST (built-in self-test) Engines, Complex I/O Characterization and Calibration, Embedded Timing Instrumentation, etc.). \"\n\nDating back to as early as the 1990s, the electronics industry recognized that design validation, test and debug would be seriously impeded in the near future. Initially, the impetus behind this recognition and the subsequent development of solutions was the emergence of new semiconductor chip packages such as the ball grid array (BGA) which placed the device’s pins beneath the silicon die, making them inaccessible to physical contact with an instrument’s or a test system’s metal probes. At that time, most test instruments, such as the oscilloscope and logic analyzers in design, and in-circuit test (ICT) in volume manufacturing were external to the chips and circuit boards. They relied upon placing a probe on a chip or a circuit board to obtain test data. To overcome the disappearing access for test probes, instrumentation technology began to be embedded into semiconductors and onto printed circuit boards.\n\nIn recent years, this situation has been exacerbated by increasingly high-speed serial inter-chip connections (interconnects or buses) on circuit boards as well as by even more complex chip packaging technologies like system-on-a-chip (SOC), system-in-package (SIP) and package-on-package (POP). These and other developments are making instrumentation embedded into chips a necessity for design validation, test and debug processes.\n\nAlthough it was not referred to as an embedded instrument at the time of its development, the IEEE 1149.1 Boundary Scan Standard can be seen as the first enabling technology for embedded instrumentation. (Boundary scan is also referred to as JTAG, after the Joint Test Action Group which initially undertook its development before it came under the aegis of a working group of the IEEE. ‘JTAG’ is often used to designate the access port on a chip which conforms to the boundary-scan standard.) Some would consider the boundary-scan test process as a form of embedded instrumentation. Boundary scan involves embedding an instrumentation infrastructure into chips and onto circuit boards. This infrastructure can be utilized during design debug and first prototype board bring-up, volume manufacturing and field service to test and diagnose the structural integrity of electrical connections on circuit boards. In addition, the boundary scan infrastructure can also be applied to the programming of devices such as memories, complex programmable logic devices (CPLDs) and Field-programmable gate arrays (FPGAs) after they have been soldered to a circuit board.\n\nIn the intervening years since the development of boundary-scan standard as a structural test technology, the embedded boundary-scan infrastructure in chips and on circuit boards has been appropriated for a number of related applications, including as an access method to the instrumentation inserted in chips. A later standard in the boundary-scan family, the IEEE 1149.6 Boundary-Scan Standard for Advanced Digital Networks, utilizes the 1149.1 boundary-scan embedded instrumentation infrastructure but expands the types of chip-to-chip interconnects that can be tested. Whereas the 1149.1 standard defines a methodology for testing DC-coupled interconnects, the 1149.6 version of the boundary-scan standard extends the methodology to testing high-speed AC coupled and/or differential interconnects.\n\nAnother addition to the boundary-scan family of standards has been IEEE 1149.7, which defines a reduced pin-count interface and provides for enhanced software debug. In addition, IEEE 1149.7 is expected to be used in the testing of complex chips with multiple die stacked in one package.\n\nA working group of the IEEE has also undertaken the development of a standard that specifically addresses embedded instrumentation. The official name of this standard is the IEEE 1687 Standard for Access and Control of Instrumentation Embedded within a Semiconductor Device, but it is commonly referred to as the Internal JTAG (IJTAG) standard. The objective of the working group has been to define a technology for automating, accessing and analyzing the output of embedded instruments. Standardizing the interface to embedded instruments means that these instruments could come from any number of sources, but user interaction would be simplified since this would be based on an industry-accepted standard. The actual embedded instruments could be developed by any of several different types of groups, including chip suppliers, third-party providers, chip design tool vendors or in-house design groups. The availability of chips that conform to the IEEE 1687 standard would encourage the development of standards-based tools for interacting with and utilizing the instrumentation embedded in the chips. The IEEE 1687 standard is now a ratified standard.\n\nEmbedded instrumentation can perform certain functions that external test and measurement technologies have difficulty with because they are external to the chip or circuit board being tested. Moreover, embedded instrumentation is often more efficient and adaptable, since it is software-based, unlike external hardware testers. In addition, embedded instrumentation is simply better suited to much of the computer and communications technologies that are emerging today.\n\nSome of the more traditional test and measurement technologies are only able to measure performance and data flow at the input/output (I/O) point on a chip. The real challenge is for the instrument to gain visibility into the processing that is going on inside the core silicon itself. To do this, embedded instruments are needed between the I/O point on the perimeter of the chip and the processing core.\n\nThe following are some of the difficulties that traditional test and measurement instruments are running into as chips, circuit boards and systems continue to become faster, smaller and more complex.\n\nThe applications for embedded instrumentation are extensive. At the level of circuit boards, two of the most prominent applications are design validation and non-intrusive board test.\n\nInstruments are being embedded into chips and utilized to validate circuit board designs. This sort of design validation can make use of a variety of embedded instruments such as bit error rate test (BERT) engines, BIST) for logic devices, margining engines, memory BIST, memory test, random pattern generators and a complete logic analyzer. Deployed in design validation applications, these embedded instruments may function inside the chip or across on-board chip-to-chip interconnects to validate the performance and functionality of a circuit board design before it moves into volume production.\n\nNon-intrusive board test (NBT) employs embedded instrumentation to perform structural and electrical tests on circuit boards. In addition to boundary scan, other types of NBT methods include processor-controlled test (PCT) and FPGA-controlled test (FCT). See below for more on these methodologies.\n\nAs mentioned above, the IEEE 1149.1 Boundary-Scan Standard could be seen as the first enabler of embedded instrumentation and, as such, the first embedded instrumentation methodology. In addition to providing the infrastructure for accessing and operating embedded instruments, tests that utilize the boundary scan infrastructure can be applied to circuit boards to identify structural defects such as shorts and opens. Several other methodologies also apply tests that are initiated by embedded instruments. \n\n"}
{"id": "77687", "url": "https://en.wikipedia.org/wiki?curid=77687", "title": "Emergency Broadcast System", "text": "Emergency Broadcast System\n\nThe Emergency Broadcast System (EBS), occasionally called the Emergency Broadcasting System and sometimes called the Emergency Action Notification System (EANS), was an emergency warning system used in the United States. It replaced the previous CONELRAD system and was used from 1963 to 1997, at which point it was replaced by the Emergency Alert System.\n\n\"The system was established to provide the President of the United States with an expeditious method of communicating with the American public in the event of war, threat of war, or grave national crisis.\" The Emergency Broadcast System replaced CONELRAD on August 5, 1963. In later years, it was expanded for use during peacetime emergencies at the state and local levels.\n\nAlthough the system was never used for a national emergency, it was activated more than 20,000 times between 1976 and 1996 to broadcast civil emergency messages and warnings of severe weather hazards.\n\nAn order to activate the EBS at the national level would have originated with the President and been relayed via the White House Communications Agency duty officer to one of two origination points – either the Aerospace Defense Command (ADC) or the Federal Preparedness Agency (FPA) – as the system stood in 1978. Participating telecommunications common carriers, radio and television networks, the Associated Press, and United Press International would receive and authenticate (by means of code words) an Emergency Action Notification (EAN) via an EAN teletypewriter network designed specifically for this purpose. These recipients would relay the EAN to their subscribers and affiliates.\n\nThe release of the EAN by the Aerospace Defense Command or the Federal Preparedness Agency would initiate a process by which the common carriers would link otherwise independent networks such as ABC, CBS, and NBC into a single national network from which even independent stations could receive programming. \"Broadcast stations would have used the two-tone Attention Signal on their assigned broadcast frequency to alert other broadcast stations to stand by for a message from the President.\" The transmission of programming on a broadcast station's assigned frequency, and the fact that television networks/stations and FM radio stations could participate, distinguished EBS from CONELRAD. EBS radio stations would not necessarily transmit on 640 or 1240 on the AM dial, and FM radio and television would carry the same audio program as AM radio stations did.\n\nActual activations originated with a primary station, which would transmit the . The Attention Signal most commonly associated with the system was a combination of the sine waves of 853 and 960 Hzsuited to attention due to its unpleasantness. Decoders at relay stations would sound an alarm, alerting station personnel to the incoming message. Then, each relay station would broadcast the alert tone and rebroadcast the emergency message from the primary station. The Attention Signal was developed in the mid-1960s.\n\nA nationwide activation of the EBS was called an Emergency Action Notification (EAN), and was the only activation that stations were not allowed to ignore; the Federal Communications Commission made local civil emergencies, weather advisories optional (except for stations that agreed to be the \"primary\" source of such messages).\n\nTo activate the EAN protocol, the Associated Press and United Press International wire services would notify stations with a special message. It began with a full line of X's, and a bell inside the Teletype machine would sound ten times. To avoid abuse and mistakes, the message included a confirmation password which changed daily. Stations that subscribed to one of the wire services were not required to activate the EBS if the activation message did not have proper confirmation.\n\nA properly authenticated Emergency Action Notification was incorrectly sent to United States broadcast stations at 9:33 a.m. Eastern Standard Time on February 20, 1971. At the usual time a weekly EAN test is performed, NORAD teletype operator W. S. Eberhardt had three tapes in front of him: a test tape, and two tapes indicating a real emergency, instructing the use of EAN Message #1, and #2, respectively. He inadvertently used the wrong tape, with codeword \"HATEFULNESS\". This message ordered stations to cease regular programming immediately, and begin an Emergency Action Notification using Message #1. Message 1 states that regular programming has been interrupted at the request of the United States government, but is not specific about the cause. A cancellation message was sent at 9:59 a.m. EST, but it used an incorrect codeword. A cancellation message with the \"correct\" codeword, \"IMPISH\", was not sent until 10:13 a.m. EST After 40 minutes and six incorrect or improperly formatted cancellation messages, the accidental activation was officially terminated.\n\nThis false alarm demonstrated major flaws in the practical implementation of an EAN. Over 2,500 radio and television stations received the notification. Some stations ignored it (convinced it was false because it came at the time of a scheduled test) and continued with regular programming. Others cancelled the EAN prematurely, with or without an any coded indication that the alert was erroneous. Still other stations did not have EAN procedure documents readily accessible to them, so they had no indication of what to do at all. It is estimated that only 20% of the stations that received the activation followed the procedures completely. Several stations went off the air, as they were instructed to do. Recordings from stations that did not (and were not supposed to according to EAN procedures) include one from WOWO in Fort Wayne, Indiana, for which a recording of the EAN activation exists. Another recording of the EAN activation on WCCO in Minneapolis/Saint Paul, Minnesota can be heard on RadioTapes.com.\n\nThis false alarm was sufficiently disruptive to move the FCC to temporarily suspend use and testing of Emergency Action Notifications (EANs) by codeword effective February 25, 1971. In the meantime, a national EBS activation (actual or test) would be routed through news service broadcast desks, then authenticated with the White House communications center, introducing a delay of approximately one minute. Numerous investigations were launched, and several changes were made to the EBS. Among them, EAN Message #2, which contains specific language indicating an imminent attack, was eliminated. Another change was moving the tapes for genuine alerts away from the broadcasting machines to prevent them being mistaken for the weekly test tapes. After numerous safeguards were put in place, the FCC voted to resume automatic national activation of the EBS using EANs in mid-December, 1972, almost 20 months after they were suspended.\n\nThough it was never used, the FCC's EBS plan involved detailed procedures for stations to follow during an EAN. It included precise scripts that announcers were to read at the outset of the emergency, as well as whenever detailed information was scarce. Among other things, citizens were instructed not to use the telephone, but rather continue listening to broadcast stations for information.\n\nThe initial scripted announcement was: \"We interrupt this program. This is a national emergency. The President of the United States or his designated representative will appear shortly over the Emergency Broadcast System.\"\n\nAs official information began to emerge from various sources, non-primary stations were to broadcast it according to the following priority list:\n\nA presidential message was always required to be aired live during an EAN. For other information, stations were to follow the priority list to decide what should be disseminated first. Lower priority official programming was to be recorded for the earliest available rebroadcast.\n\nParticipation in EAN emergency broadcasting was done with the \"voluntary cooperation\" of each station (as noted in the classic test announcement). Stations that were not prepared to be part of the national EBS network were classified as \"non-participating\" by the FCC. During an EAN, a non-participating station was required to advise listeners/viewers to tune elsewhere to find emergency bulletins. The station's transmitter would then be turned off. Non-participating stations had to remain off the air until the EAN was terminated. Under no circumstances could any broadcast station continue with normal programming during a national emergency.\n\nUntil the system was superseded, radio and television stations were required to perform a \"Weekly Transmission Test Of The Attention Signal and Test Script\" at random days and times between 8:30 a.m. and local sunset. Stations were required to perform the test at least once a week, and were only exempt from doing so if they had activated the EBS for a state or local emergency, or participated in a coordinated state or local EBS test during the past week. Additionally, stations were required to log tests they received from each station they monitored for EBS messages. This served as an additional check, as these stations could expect to hear a weekly test from each source. Failure to receive a signal at least once a week meant that either the monitored station was having a problem transmitting the alert signal, or the monitoring station was having a problem receiving it.\n\nEarly on, tests and activations were initiated in a similar way to CONELRAD tests. Primary stations would turn their transmitters off for five seconds, back on for five seconds, off for five seconds more, then would go back on air and transmit a 1000 Hz tone for 15 seconds to alert secondary stations. Television stations adhere to similar rules, but stations switch only their sound carriers off. This quick off-and-on became known to broadcast engineers as the \"EBS Stress Test\", as older transmitters would sometimes fail after the quick cycling on and off. This became unnecessary as broadcast technology advanced and the two-tone alarm was developed.\n\nBeginning in 1976, the old CONELRAD signaling method (the \"EBS Stress Test\") was scrapped in favor of the following procedure:\n\n1. Normal programming was suspended, though tests were typically done during commercial breaks for continuity reasons. Television stations would transmit a video slide such as the one illustrated at the beginning of the article; numerous designs were available over the years. One of the following announcements was transmitted:\n\n\nAlternatively, the name \"Emergency Broadcasting System\" could be used.\n\n2. The Attention Signal was transmitted from the EBS encoder for 20 to 25 seconds. At the special request of the FCC, however, this step was occasionally (though rarely) skipped. In mid-1995, a new rule was put in place that gave stations the option to transmit the attention signal for anywhere from eight to 25 seconds.\n\n3. The announcement written below (depending on the variation) was transmitted. The first part read:\n\nThere were a number of variations for the second half of the statement. During the system's early days, stations other than the designated primary station for an operational area were required to shut down in the event of an emergency (reminiscent of the CONELRAD days), and the message was a variation of:\n\nBy the early 1980s, it was easier for stations to record and relay messages from a primary station, and the risk of hostile bombers using broadcast signals to navigate lessened due to the development of ICBMs. As a result, the requirement to shut down during an activation of the system was dropped, and the message became:\n\nSome stations also listed the types of emergencies that the EBS would be activated for (i.e., tornado warning, flash flood warning, hurricane warning, or earthquake); the test conducted by WXYZ-TV made explicit reference to an attack on the United States as being a possible scenario for EBS activation. In the late 1980s, several television stations in the Los Angeles area had specific test scripts that emphasized earthquake preparedness.\n\nAs the EBS was about to be replaced by its successor, the Emergency Alert System, some stations used the following message:\n\n4. The test concluded with one of the following phrases:\n\nThese variations were heard in different parts of the country throughout the years depending on FCC regulations at the time, local preferences, and whether the specific station performing the test was a primary EBS station or not. The announcement text was mandated by the FCC. Stations had the option of either reading the test script live, or using recorded versions. WHEN radio in Syracuse, New York and WGR radio in Buffalo, New York both had a sung version of the most common script. There was also a version done by Los Angeles-based Cheap Radio Thrills, as well as another by the comedy team of Bob and Ray. The FCC declared it illegal to sing the test message, or read it as a joke. However, it was acceptable to read it in another language (for example, French or Spanish), if a station broadcast in a language other than English. Copies of the warning message script had a note saying that it was acceptable to broadcast in any other language, so long as it was broadcast in English as well. Usually the post-test recorded announcement began with the phrase, \"This has been a test of the Emergency Broadcast System...\", followed by the here-in-above stated recitations.\n\nThe purpose of the test was to allow the FCC and broadcasters to verify that EBS tone transmitters and decoders were functioning properly. In addition to the weekly test, test activations of the entire system were conducted periodically for many years. These tests showed that about 80% of broadcast outlets nationwide would carry emergency programming within a period of five minutes when the system was activated.\n\nThe weekly broadcasts of the EBS attention signal and test script made it a significant part of the American cultural fabric of its time, and became the subject of a great number of jokes and skits, such as the sung versions of the test script in the late 1970s. In addition, many people have testified to being frightened by the test patterns and attention signal as children.\n\nAlthough intended for the President to communicate with the American people in the event of a national emergency, many critics questioned whether the EBS would work in an actual emergency scenario. Curt Beckmann of WCCO-AM expressed his doubts about the system's effectiveness in a 1984 interview:\n\n\n"}
{"id": "52826729", "url": "https://en.wikipedia.org/wiki?curid=52826729", "title": "EuroSpec", "text": "EuroSpec\n\nEuroSpec, abbreviation for European Specification for Railway Vehicles, is an initiative of several European railway companies with the aim to develop common, explicit technical specifications for train systems.\n\nThe program includes among other items toilet systems, air conditioning and sliding steps.\n\nThe Eurospec consortium is composed of six European railway companies and the association of railway companies in the United Kingdom of Great Britain and Northern Ireland.\n\nEuroSpec consortium shall publish the specifications in English only. \n\n"}
{"id": "36787184", "url": "https://en.wikipedia.org/wiki?curid=36787184", "title": "FlowTex", "text": "FlowTex\n\nDuring the mid-1980s the company FlowTex was established by Mandred Schmider and Klaus Kleiser, two businessmen based in the South-Western state of Baden-Wuerttemberg, with the purpose of constructing and operating machines for horizontal drilling. The business idea was profitable and successful as it allowed for the installation of cables and pipes without the need to dig up roads, which would have caused considerable traffic chaos and noise pollution. By the year 2000, the two men had sold more than 3,000 machines, each at around 1.5 million Deutsche Marks.\n\nWhat started as a legal business with a viable model soon turned into large-scale fraud; in reality, FlowTex had only produced 181 machines that were sold multiple times, with the certificates and identification plates manipulated according to the scam. At one point, the same machine was paraded at various different fake construction sites to potential investors during the same inspection; it later emerged that they had moved the machine from one location to another during lunch breaks. Over a period of ten years or so, they secured loans worth more than two billion Euros for non-existent drilling systems and the scam is widely tipped as Germany’s largest ever case of white-collar crime.\n\nThe scam was aided by a network of co-conspirators that included family, friends and employees of the firm, but ultimately it was Schmider and Kleiser who paid the largest price for their crimes; they were both arrested in February 2000 on suspicion of fraud and tax evasion, four years after the authorities were allegedly first tipped off about the fraudulent business practices. This apparent lack of determination to follow up on the leads was allegedly partly due to concerns that such investigations would jeopardise jobs and compromise leading politicians who had courted the FlowTex directors as model businessmen. Also suspicious was the fact that the tax inspector assigned to FlowTex was a tennis partner of one of the FlowTex directors. However, to date there have been no indications that the FlowTex management or any politicians tried to directly influence the proceedings.\n"}
{"id": "50602564", "url": "https://en.wikipedia.org/wiki?curid=50602564", "title": "Freedom Scientific", "text": "Freedom Scientific\n\nFreedom Scientific is a company that makes accessibility products for computer users with low-vision, blindness, and learning disabilities. The company is a subsidiary of VFO Group. The software they create enables screen magnification, screen reading and use of refreshable Braille displays with modern computers. The company also offers services including training and accessibility consulting, including in-person services in major cities throughout the United States and other countries.\n\nThe company is based in Saint Petersburg, Florida, with European headquarters in Barendrecht, Netherlands.\n\nFormer motorcycle racer Ted Henter developed the JAWS screen reader after he himself became blind due to complications from a car accident.\n\nHenter and Bill Joyce founded Henter-Joyce in 1987 in St. Petersburg, which produced an MS-DOS version of JAWS and later a Microsoft Windows version. Henter-Joyce merged with Arkenstone and Blazie Engineering in 2000 to form Freedom Scientific.\n\n"}
{"id": "11077711", "url": "https://en.wikipedia.org/wiki?curid=11077711", "title": "Freenex", "text": "Freenex\n\nFreenex Co, Ltd. is a Korea's group of navigation system in electronics and automotive company. headquartered in Gil-dong Gangdong-gu Seoul, Korea. established in 2002. The Freenex's companies that develops consumer and aviation technologies navigation for the Global Positioning System. Freenex also creates OEM products for BMW, Hyundai Autonet, WIA brand navigation automotive markets and product use for Vitas.\n\nThrough the automotive navigation system technology display position DVD and DMB components and systems are produced, television; navigated teletext; and digital maps. and naviogation only maker of automotive company as primary competitor in Hyundai Autonet and Garmin. The Freenex CEO is Lee Woo Yeol (이우열).\n\n\n\n\n"}
{"id": "1079466", "url": "https://en.wikipedia.org/wiki?curid=1079466", "title": "Förster resonance energy transfer", "text": "Förster resonance energy transfer\n\nFörster resonance energy transfer (FRET), fluorescence resonance energy transfer (FRET), resonance energy transfer (RET) or electronic energy transfer (EET) is a mechanism describing energy transfer between two light-sensitive molecules (chromophores). A donor chromophore, initially in its electronic excited state, may transfer energy to an acceptor chromophore through nonradiative dipole–dipole coupling. The efficiency of this energy transfer is inversely proportional to the sixth power of the distance between donor and acceptor, making FRET extremely sensitive to small changes in distance.\n\nMeasurements of FRET efficiency can be used to determine if two fluorophores are within a certain distance of each other. Such measurements are used as a research tool in fields including biology and chemistry.\n\nFRET is analogous to near-field communication, in that the radius of interaction is much smaller than the wavelength of light emitted. In the near-field region, the excited chromophore emits a virtual photon that is instantly absorbed by a receiving chromophore. These virtual photons are undetectable, since their existence violates the conservation of energy and momentum, and hence FRET is known as a \"radiationless\" mechanism. Quantum electrodynamical calculations have been used to determine that radiationless (FRET) and radiative energy transfer are the short- and long-range asymptotes of a single unified mechanism.\n\nFörster resonance energy transfer is named after the German scientist Theodor Förster. When both chromophores are fluorescent, the term \"fluorescence resonance energy transfer\" is often used instead, although the energy is not actually transferred by fluorescence. In order to avoid an erroneous interpretation of the phenomenon that is always a nonradiative transfer of energy (even when occurring between two fluorescent chromophores), the name \"Förster resonance energy transfer\" is preferred to \"fluorescence resonance energy transfer\"; however, the latter enjoys common usage in scientific literature. It should also be noted that FRET is not restricted to fluorescence. It can occur in connection with phosphorescence as well.\n\nThe FRET efficiency (formula_1) is the quantum yield of the energy transfer transition, i.e. the fraction of energy transfer event occurring per donor excitation event:\n\nwhere formula_3 is the rate of energy transfer, formula_4 the radiative decay rate, and the formula_5 are the rate constants of any other de-excitation pathways.\n\nThe FRET efficiency depends on many physical parameters that can be grouped as 1) the distance between the donor and the acceptor (typically in the range of 1–10 nm), 2) the spectral overlap of the donor emission spectrum and the acceptor absorption spectrum, and 3) the relative orientation of the donor emission dipole moment and the acceptor absorption dipole moment.\n\nformula_1 depends on the donor-to-acceptor separation distance formula_7 with an inverse 6th-power law due to the dipole-dipole coupling mechanism:\nwith formula_9 being the Förster distance of this pair of donor and acceptor, i.e. the distance at which the energy transfer efficiency is 50%.\nThe Förster distance depends on the overlap integral of the donor emission spectrum with the acceptor absorption spectrum and their mutual molecular orientation as expressed by the following equation:\nwhere formula_11 is the fluorescence quantum yield of the donor in the absence of the acceptor, formula_12 is the dipole orientation factor, formula_13 is the refractive index of the medium, formula_14 is Avogadro's number, and formula_15 is the spectral overlap integral calculated as\nwhere formula_17 is the donor emission spectrum, formula_18 is the donor emission spectrum normalized to an area of 1, and formula_19 is the acceptor molar extinction coefficient normally obtained from an absorption spectrum.\nThe orientation factor \"κ\" is given by\nwhere formula_21 denotes the normalized transition dipole moment of the respective fluorophore, and formula_22 denotes the normalized inter-fluorophore displacement.\nformula_12 = 2/3 is often assumed. This value is obtained when both dyes are freely rotating and can be considered to be isotropically oriented during the excited state lifetime. If either dye is fixed or not free to rotate, then formula_12 = 2/3 will not be a valid assumption. In most cases, however, even modest reorientation of the dyes results in enough orientational averaging that formula_12 = 2/3 does not result in a large error in the estimated energy transfer distance due to the sixth-power dependence of formula_9 on formula_12. Even when formula_12 is quite different from 2/3, the error can be associated with a shift in formula_9, and thus determinations of changes in relative distance for a particular system are still valid. Fluorescent proteins do not reorient on a timescale that is faster than their fluorescence lifetime. In this case 0 ≤ formula_12 ≤ 4.\n\nThe FRET efficiency relates to the quantum yield and the fluorescence lifetime of the donor molecule as follows:\nwhere formula_32 and formula_33 are the donor fluorescence lifetimes in the presence and absence of an acceptor respectively, or as\nwhere formula_35 and formula_36 are the donor fluorescence intensities with and without an acceptor respectively.\n\nThe inverse sixth-power distance dependence of Förster resonance energy transfer was experimentally confirmed by Wilchek, Edelhoch and Brand using tryptophyl peptides. Stryer, Haugland and Yguerabide also experimentally demonstrated the theoretical dependence of Förster resonance energy transfer on the overlap integral by using a fused indolosteroid as a donor and a ketone as an acceptor. However, a lot of contradictions of special experiments with the theory was observed. The reason is that the theory has approximate character and gives overestimated distances of 50–100 ångströms.\n\nIn fluorescence microscopy, fluorescence confocal laser scanning microscopy, as well as in molecular biology, FRET is a useful tool to quantify molecular dynamics in biophysics and biochemistry, such as protein-protein interactions, protein–DNA interactions, and protein conformational changes. For monitoring the complex formation between two molecules, one of them is labeled with a donor and the other with an acceptor. The FRET efficiency is measured and used to identify interactions between the labeled complexes. There are several ways of measuring the FRET efficiency by monitoring changes in the fluorescence emitted by the donor or the acceptor.\n\nOne method of measuring FRET efficiency is to measure the variation in acceptor emission intensity. When the donor and acceptor are in proximity (1–10 nm) due to the interaction of the two molecules, the acceptor emission will increase because of the intermolecular FRET from the donor to the acceptor. For monitoring protein conformational changes, the target protein is labeled with a donor and an acceptor at two loci. When a twist or bend of the protein brings the change in the distance or relative orientation of the donor and acceptor, FRET change is observed. If a molecular interaction or a protein conformational change is dependent on ligand binding, this FRET technique is applicable to fluorescent indicators for the ligand detection.\n\nFRET efficiencies can also be inferred from the photobleaching rates of the donor in the presence and absence of an acceptor. This method can be performed on most fluorescence microscopes; one simply shines the excitation light (of a frequency that will excite the donor but not the acceptor significantly) on specimens with and without the acceptor fluorophore and monitors the donor fluorescence (typically separated from acceptor fluorescence using a bandpass filter) over time. The timescale is that of photobleaching, which is seconds to minutes, with fluorescence in each curve being given by\n\nwhere formula_38 is the photobleaching decay time constant and depends on whether the acceptor is present or not. Since photobleaching consists in the permanent inactivation of excited fluorophores, resonance energy transfer from an excited donor to an acceptor fluorophore prevents the photobleaching of that donor fluorophore, and thus high FRET efficiency leads to a longer photobleaching decay time constant:\n\nwhere formula_40 and formula_38 are the photobleaching decay time constants of the donor in the presence and in the absence of the acceptor respectively. (Notice that the fraction is the reciprocal of that used for lifetime measurements).\n\nThis technique was introduced by Jovin in 1989. Its use of an entire curve of points to extract the time constants can give it accuracy advantages over the other methods. Also, the fact that time measurements are over seconds rather than nanoseconds makes it easier than fluorescence lifetime measurements, and because photobleaching decay rates do not generally depend on donor concentration (unless acceptor saturation is an issue), the careful control of concentrations needed for intensity measurements is not needed. It is, however, important to keep the illumination the same for the with- and without-acceptor measurements, as photobleaching increases markedly with more intense incident light.\n\nFRET efficiency can also be determined from the change in the fluorescence lifetime of the donor. The lifetime of the donor will decrease in the presence of the acceptor. Lifetime measurements of the FRET-donor are used in fluorescence-lifetime imaging microscopy (FLIM).\n\nOne common pair fluorophores for biological use is a cyan fluorescent protein (CFP) – yellow fluorescent protein (YFP) pair. Both are color variants of green fluorescent protein (GFP). Labeling with organic fluorescent dyes requires purification, chemical modification, and intracellular injection of a host protein. GFP variants can be attached to a host protein by genetic engineering which can be more convenient. Additionally, a fusion of CFP and YFP linked by a protease cleavage sequence can be used as a cleavage assay.\n\nA limitation of FRET is the requirement for external illumination to initiate the fluorescence transfer, which can lead to background noise in the results from direct excitation of the acceptor or to photobleaching. To avoid this drawback, bioluminescence resonance energy transfer (or BRET) has been developed. This technique uses a bioluminescent luciferase (typically the luciferase from \"Renilla reniformis\") rather than CFP to produce an initial photon emission compatible with YFP. One drawback of BRET is the requirement to generate at least one fusion protein encoding luciferase, though some applications of FRET can be implemented with antibody-conjugated fluorophores.\n\nBRET has also been implemented using a different luciferase enzyme, engineered from the deep-sea shrimp \"Oplophorus gracilirostris\". This luciferase is smaller (19 kD) and brighter than the more commonly used luciferase from \"Renilla reniformis\". Promega has developed this luciferase variant under the product name NanoLuc.\n\nIn general, \"FRET\" refers to situations where the donor and acceptor proteins (or \"fluorophores\") are of two different types. In many biological situations, however, researchers might need to examine the interactions between two, or more, proteins of the same type—or indeed the same protein with itself, for example if the protein folds or forms part of a polymer chain of proteins or for other questions of quantification in biological cells.\n\nObviously, spectral differences will not be the tool used to detect and measure FRET, as both the acceptor and donor protein emit light with the same wavelengths. Yet researchers can detect differences in the polarisation between the light which excites the fluorophores and the light which is emitted, in a technique called FRET anisotropy imaging; the level of quantified anisotropy (difference in polarisation between the excitation and emission beams) then becomes an indicative guide to how many FRET events have happened.\n\nFRET has been used to measure distance and detect molecular interactions in a number of systems and has applications in biology and chemistry. FRET can be used to measure distances between domains in a single protein and therefore to provide information about protein conformation. FRET can also detect interaction between proteins. Applied in vivo, FRET has been used to detect the location and interactions of genes and cellular structures including integrins and membrane proteins. FRET can be used to obtain information about metabolic or signaling pathways. FRET is also used to study lipid rafts in cell membranes.\n\nFRET and BRET are also the common tools in the study of biochemical reaction kinetics and molecular motors.\n\nThe applications of fluorescence resonance energy transfer (FRET) have expanded tremendously in the last 25 years, and the technique has become a staple technique in many biological and biophysical fields. FRET can be used as spectroscopic ruler in various areas such as structural elucidation of biological molecules and their interactions in vitro assays, in vivo monitoring in cellular research, nucleic acid analysis, signal transduction, light harvesting and metallic nanomaterial etc. Based on the mechanism of FRET a variety of novel chemical sensors and biosensors have been developed.\n\nIn the field of chemistry, this technique is growing very fast for monitoring the nanoparticle formation. Similarly to see the protein interaction and other many more applications.\n\nA different, but related, mechanism is Dexter electron transfer.\n\nAn alternative method to detecting protein–protein proximity is the bimolecular fluorescence complementation (BiFC), where two parts of a fluorescent protein are each fused to other proteins. When these two parts meet, they form a fluorophore on a timescale of minutes or hours.\n\n"}
{"id": "36559918", "url": "https://en.wikipedia.org/wiki?curid=36559918", "title": "Gradient oven tester", "text": "Gradient oven tester\n\nA gradient oven tester is a testing instrument which simulates the conditions of a production oven in the lab. It is used to test the baking and drying properties of liquid/powder coatings, resins, plastics, etc. and can determine how these materials will react when placed under extreme stress from heat, but more importantly can determine the temperature needed for drying as well as how long it will take for these materials to dry and harden once applied. The usage of the oven guarantees that these things are known prior to production which aids product consistency.\n\nThe gradient oven tester has a heating bank which has 45 heating elements each of which contain a Pt-100 temperature probe. Each element is individually insulated which allows for the setting of different temperatures in two adjoining elements. \n\nSpecifications; \n\nISO 9000\n"}
{"id": "1626186", "url": "https://en.wikipedia.org/wiki?curid=1626186", "title": "Herbert Callen", "text": "Herbert Callen\n\nHerbert Bernard Callen (1919 – May 22, 1993) was an American physicist best known as the author of the textbook \"Thermodynamics and an Introduction to Thermostatistics\", the most frequently cited thermodynamic reference in physics research literature. During World War II he was also called upon to undertake theoretical studies of the principles underpinning the effort to create the atom bomb.\n\nA native of Philadelphia, Herbert Callen received his Bachelor of Science degree from Temple University. He married in 1945, as the war and his work for the Manhattan Project were coming to an end, and subsequently studied for a PhD in physics at the Massachusetts Institute of Technology (MIT), receiving his degree in 1947. His graduate advisor was the eminent physicist László Tisza.\n\nIn 1948, Callen joined the faculty of the University of Pennsylvania Department of Physics and, in 1984, received the Elliott Cresson Medal from the Franklin Institute. He was also the recipient of a Guggenheim Fellowship for the academic year 1972/1973, and his distinguished career in physics was capped by induction into the National Academy of Sciences in 1990. Specialists consider that his most lasting contribution to physics remains the proof of Nyquist's fluctuation-dissipation theorem, an extremely general result describing how a system's response to perturbations relates to its behavior at equilibrium; as well as his classic text on Thermodynamics, which was published in two editions, translated into many languages, and continues to be used at the graduate and undergraduate level in Thermodynamics courses around the world.\n\nAfter battling Alzheimer's disease for eleven years, Herbert Callen died in the Philadelphia suburb of Merion at the age of 73. He and his wife, Sara Smith, had two children, Jed and Jill.\n\n"}
{"id": "41602531", "url": "https://en.wikipedia.org/wiki?curid=41602531", "title": "Horizon Discovery", "text": "Horizon Discovery\n\nHorizon Discovery Group plc (LSE: HZD) (\"Horizon\"), is a world-leading gene editing company that designs and engineers genetically modified cells and then applies them in research and clinical applications that advance human health.\n\nHorizon builds human disease models and reagents derived from genetically-engineered cells that its customers use, or that are deployed on their behalf to: gain knowledge of the genetic drivers of disease; develop novel drugs or cell therapies targeted at these genetic drivers; and develop companion diagnostics that predict patient response in the clinic. In this way they apply genetic knowledge to provide biological insights that translate to improved research and development outcomes for drug developers and treatment regimens that better serve patients.\n\nHorizon has an international customer base of over 1,400 unique organisations across more than 50 countries, including major pharmaceutical, biotechnology and diagnostic companies as well as leading academic research centers. The Group supplies its products and services into a range of markets, estimated to be worth over £29 billion in 2015.\n\nHorizon is headquartered in Cambridge, UK, and is listed on the London Stock Exchange’s AIM market under the ticker “HZD”.\n\nGene editing is the process by which specific changes are made to the sequence of a gene within the context of a host cell. By editing the code of a patient-derived cell to introduce or repair a genetic change believed to drive disease, a patient’s disease can be reproduced in a laboratory setting, letting researchers ask important biological questions of potential drugs or cell therapies earlier in the drug discovery process.\n\nTremendous research efforts have gone into identifying and refining the tools used to generate engineered human cell lines. These fall into two categories based on their biological approach:\nEach technology has its own features and strengths and it is these differences that make each best suited for addressing different gene-editing challenges.\n\nHorizon has taken a ‘technology agnostic’ approach, developing deep experience and taking multiple licenses for rAAV, CRISPR and ZFNs, letting the company choose the right approach for any project.\nThrough its gene editing platform, Horizon is able to alter genes in virtually any human or mammalian cell line. Based on this, Horizon now offers over 23,000 cell line pairs that accurately model the mutations found in genetically based diseases.\n\nThese ‘patients-in-a-test-tube’ are being used directly by customers, or by Horizon on their behalf, to identify the effect of individual or compound genetic mutations on drug activity, patient responsiveness, and resistance, which may lead to the successful prediction of which patient sub-groups will respond to currently available and future drug treatments.\n\nOnce built, engineered cells can also act as product manufacturing engines, yielding related cell and reagent products that can be used by customers as research tools or molecular diagnostic reference standards or as a means to generate advanced \"in vivo\" models, or even as factories for the manufacturing of protein-based therapeutics.\n\nIn this way, the cells power Horizon’s entire business of Products, Services and high value Research Biotech initiatives.\n\nHorizon operates through three business units; Products, Services and Research Biotech. Products and Services are offered under the following brands: \n\n"}
{"id": "34025898", "url": "https://en.wikipedia.org/wiki?curid=34025898", "title": "Infini (CRS)", "text": "Infini (CRS)\n\nInfini is a computer reservations system based in Japan which provides its services in the Japanese market. It was created in 1990 by All Nippon Airways and Abacus (GDS) who currently own 100% of Infini. It utilises technology from Sabre Holdings to provide booking and ticketing capabilities to travel agencies in Japan.\n\n\n"}
{"id": "13917034", "url": "https://en.wikipedia.org/wiki?curid=13917034", "title": "Kitchenette", "text": "Kitchenette\n\nA kitchenette is a small cooking area, which usually has a fridge and a microwave, but may have other appliances. In some motel and hotel rooms, small apartments, college dormitories, or office buildings, a kitchenette usually consists of a small refrigerator, a microwave oven or hotplate, and, less frequently, a sink. New York City building code defines a kitchenette as a kitchen of less than 7.4 m (80 ft) of floor space.\n\nKitchenettes are a common feature in hotel and motel guest rooms and often contain a coffeemaker, a bar refrigerator, commonly called a mini-bar. Some hotel kitchenettes have provisioned refrigerators that have an interior sensor feature used by management to monitor guest use of the refrigerator's contents and thus charge for the consumables, which typically include soda, beer, and liquor.\n\nIn British English, the term kitchenette also refers to a small secondary kitchen in a house. Often it is found on the same floor as the children's bedrooms, and used by a nanny or au pair to prepare meals for children; the same feature can be found in hotels such as some in London.\nThe word \"kitchenette\" was also used to refer to a type of small apartment prevalent in African American communities in Chicago and New York City during the mid-twentieth century. Landlords often divided single-family homes or large apartment units into smaller units to house more families. Living conditions in these kitchenettes were often wretched; the author Richard Wright described them as \"our prison, our death sentence without a trial\".\n\nIn Brazil, a kitchenette (spelled \"quitinete\" in Brazilian Portuguese) is a very small apartment. It is composed of one room, one bathroom, and a kitchen, which is often in the same space as the room. It corresponds to the studio apartment in American culture (or a bedsit in the UK and Ireland).\n\n"}
{"id": "15699387", "url": "https://en.wikipedia.org/wiki?curid=15699387", "title": "Kongsberg Spacetec", "text": "Kongsberg Spacetec\n\nKongsberg Spacetec AS or KSPT, prior to 1994 Spacetec A/S, is a supplier of ground stations based in Tromsø, Norway. It is owned by the Kongsberg Group and is part of its Kongsberg Defence & Aerospace division. The company is co-located with Kongsberg Satellite Services (KSAT) and Tromsø Satellite Station (TSS).\n\nThe company was established by some of the most experienced employees of Tromsø Satellite Station, which had been operating since 1967. They originally established Drive Electronics in 1982, but it was bankrupt two years later. The company was re-established as Spacetec, which was registered on 11 December 1984. The company was established with a share capital of NOK 5 million or NOK 1,000 per share. Along with Norsk Data, Computas, Informasjonskontroll and Noratom, Spacetec established the joint venture Norspace to act as supplier to the European Space Agency. By 1986, the company had eighteen employees. It received a state subsidy of NOK 6.25 million to finance its expansion.\n\nThis resulted in a contract to deliver a ground station system to Esrange in Kiruna, Sweden. The Norwegian Defence Research Establishment developed an application of synthetic-aperture radar for satellites at this was to be commercialized by Spacetec and Norsk Data. In July 1988 the company signed a contract to deliver equipment worth NOK 26 million to TSS in partnership with Norsk Data. This involved Spacetec designing a downlink and image processing software for ERS-1 which needed to be capable of handling 100 megabits per second. The new ground station was to be completed in 1989 and operational by April 1990. Also in 1988, Norspace signed an agreement to deliver telemetry components for ESA's Columbus module for the International Space Station. In December Spacetec signed an agreement with ESA to develop a system to transfer satellite data from magnetic tapes to optical discs.\n\nDuring 1989, Spacetec participated in a cooperation with TSS and the University of Tromsø in developing technology to send satellite images to customers in the course of minutes, rather than hours and days, through the use of broadband. After ten years of development, the Norwegian Defence Research Establishment launched its CESAR supercomputer, which was tailor-made for analysis of SAR images and had been developed in cooperation with Spacetec. It allowed for the analysis of a surface area in eight minutes. In November 1990, Spacetec signed an agreement with ESA to develop and supply a simulator to test and verify the launching ramp of the Ariane 5 rockets. Spacetec cooperated with the Norwegian Meteorological Institute to develop a processing system for the latter to utilize satellite data for meteorology. With two years of development, Spacetec planned to sell the technology to other users. In 1993 Spacetec signed an agreement with ESA to deliver telemetry systems to ERS-2.\n\nIn 1991 the company had 38 employees, of which 30 were engineers. It had a revenue of NOK 28 million and a net income of NOK 2.9 million. One sixth of the company was owned by various employees, while the remaining was owned by various industrial companies in Tromsø. The largest were Odd Berg Gruppen (29 percent) and Sparebanken Nord-Norge (18 percent). Norsk Forsvarsteknologi (NFT) made an unsuccessful attempt to purchase Spacetec in December 1993, with a price of NOK 1300 per share. Negotiations with the main shareholders followed and by March NFT had bought the company at a price of NOK 1,550 per share, NOK 7.75 million in total.\n\nIn August 1994, Spacetect signed a contract for parts of a new ground station in Singapore. In September 1995, Spacetec started a cooperation with the French company Thomson to develop systems for the European Organisation for the Exploitation of Meteorological Satellites. In November, a subsidiary was established which would work with processing medical X-ray images. Spacetec participated with technology for the Solar and Heliospheric Observatory.\n\nIn 1996, the Norwegian Space Center started negotiations with NASA to provide a ground station for the Earth Observing System (EOS) in Longyearbyen. Svalbard Satellite Station was established in 1997 and was owned by Kongsberg/Lockheed Martin Space Data Services, a joint venture between Kongsberg Spacetec and Lockheed Martin. That year Kongsberg Spacetec had a revenue of NOK 42 million and they announced that they would focus on receiving contracts in Asia and South America. Throughout the 1990s, Kongsberg Spacetec had a fairly stable revenue and an annual profit of between NOK 2 and 3 million. In 1997, Kongsberg moved ten employees working on a radar system to Tromsø and co-located them with Spacetec. Spacetec experienced a major revenue increase from 1999 to 2002, to more than NOK 60 million. However, they were not able to increase their profits. After three years development of new weather satellite technology, Spacetec won a contract worth NOK 50 million in September 2002 for Meteosat 8.\n\nIn 2002, ownership and operations of the facility were consolidated and taken over by the newly created Kongsberg Satellite Services. Lockheed Martin was no longer interested in owning a share of the facility, and sold their shares. NSC and Kongsberg merged their interests in the new company, which also took over Tromsø Satellite Station. By 2004, six antennas, between in diameter, had been installed. In April 2005 the company was for the first time in its history forced to lay off employees. The company cited a combination of reduced activity in ESA, a delay in the Meteosat program and a reduction in discounted wage taxes. Nine people were laid off. Following this, there surfaced that there periodically had been poor cooperation between Spacetec and KSAT, as the latter had been in a period with rapid growth without the other following suit. Spacetec had its break-through with NASA in September 2006 when it signed an agreement to deliver twenty-two ground stations to Goddard Space Flight Center in a contract worth NOK 19 million.\n\nSpacetec signed its first contract with a Russian organization in April 2007, when a contract for a ground station for Meteosat was signed with Hydrometeorological Centre of Russia with options for a further two stations. Tromsø Centre for Remote Technology was established in February 2008 as a cooperation between Spacetec, KSAT, UiT and the Northern Research Institute. The goal of the project was to create closer ties between research and commercial activity within satellite communication technology.\n\n\n\n"}
{"id": "16243930", "url": "https://en.wikipedia.org/wiki?curid=16243930", "title": "Learn.com", "text": "Learn.com\n\nLearn.com was a software company headquartered in Sunrise, Florida. Learn.com provided on-demand learning management and talent management software, and e-learning courses.\n\nLearn.com was founded by Jim Riley and Patrick Toomey in January 1999 as a website that allowed anyone to create and publish e-learning courses or e-learning sites (called LearnCenters). Learn.com became an early example of a website containing open content.\n\nIn September 2000, Learn.com introduced the first commercial version of its LearnCenter LMS, with assistance of co-founder JW Ray.\n\nIn June 2001, Learn.com client ECOT became the first electronic charter school in the nation to graduate students.\n\nIn September 2002, Learn.com acquired Learn2 Corporation, a provider of e-learning content.\n\nIn June 2004, Learn.com acquired Mentor Communications, Inc.\n\nIn December 2005, Learn.com introduced LearnCenter X, the HCM industry's first integrated Talent management suite.\n\nIn September 2007, Learn.com introduced its WebRoom web conferencing product.\n\nIn June 2009, Learn.com introduced its Learn.com Personal Edition (LPE), a website that allows anyone to take courses and learn/improve skills or create and publish their own courses.\n\nIn October 2010, Learn.com was acquired by Taleo Corporation (NASDAQ: TLEO), a leader in the on-demand Talent Management market. In April 2012, Taleo was in turn acquired by Oracle Corporation and Learn.com technology became the foundation for the Oracle Learn Cloud product.\n\nIn November 2009, announced that Learn.com was the first winner in their newly created Best Talent Management System (TMS) category. Learn.com also won top honors for its Learning Management System (LMS) in this readers' choice award and continued the trend of Best Enterprise LMS recognition for a record fourth year in a row as the company had won top honors in 2006 from Training Magazine.\n\nIn January 2008, Elearning! Magazine announced that their readers had voted the Learn.com LearnCenter platform as the Best LMS for 2007. Elearning! Magazine voters also gave top honors to Learn.com's Information Technology (IT) and Soft Skills content libraries. Learn.com LearnCenter was voted one of the Best Enterprise LMS for 2007 and 2008.\n\n\n"}
{"id": "40888522", "url": "https://en.wikipedia.org/wiki?curid=40888522", "title": "List of Razer products", "text": "List of Razer products\n\nThis is a list of various Razer gaming hardware products. Individual products may have their own article, please see the navbox at the bottom if a product is not listed here.\n"}
{"id": "5024249", "url": "https://en.wikipedia.org/wiki?curid=5024249", "title": "List of inventions and innovations of indigenous Americans", "text": "List of inventions and innovations of indigenous Americans\n\nThis is an alphabetic list of achievements in science and technology made by Indigenous peoples of the Americas during the 15,000 years that they have inhabited the Americas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "48735497", "url": "https://en.wikipedia.org/wiki?curid=48735497", "title": "List of referred Indian Standard Codes for civil engineers", "text": "List of referred Indian Standard Codes for civil engineers\n\nA large number of Indian Standard (IS) codes are available that are meant for virtually every aspect of civil engineering one can think of. During one’s professional life one normally uses only a handful of them depending on the nature of work they are involved in. Civil engineers engaged in construction activities of large projects usually have to refer to a good number of IS codes as such projects entail use of lots of assorted construction materials in many varieties of structures such as buildings, roads, steel structures, all sorts of foundations and what not.\n\nA list of these codes can come in handy not only for them but also for construction-newbies, students, etc. The list provided below may not be a comprehensive one, yet it definitely includes some IS codes quite frequently used (while a few of them occasionally) by construction engineers. The description of the codes in the list may not be exactly the same as that written on the covers of the codes. Readers may add more such codes to this list and also point out slips if found in the given list.\n\nIndian standard codes are list of codes used for civil engineers in India for the purpose of design and analysis of civil engineering structures like buildings, dams, roads, railways, airports and many more\n\n\n1. Specification for 33 Grade ordinary portland cement IS 269 - 2015\n\n2. Specification for Rapid hardening portland cement IS 8041 - 1990\n\n3. Specification for portland Pozzolona cement IS 1489 (part 1&2) 1991\n\n4. Methods of physical test for hydraulic cement IS 14032 - 1988\n\n5. Method of chemical analysis of hydraulic cement IS 4032 - 1985\n\n6. Method of sampling for hydraulic cement IS 3535 - 1986\n\n7. standard sand testing of cement IS 650 - 1991\n\n8. Specification for 43 Grade OPC… IS 8112 - 2013\n\n9. Specification for 53 Grade OPC… IS 12269-1987\n\n1 Specification for coarse & fine aggregate IS 383-1970\n\n2 Methods of test for aggregate for concrete particle size and shape IS 2386 (Part I) 1963\n\n3 Methods of test for aggregate for concrete estimation of deleterious materials and organic impurities. IS 2386 (Part II) 1963\n\n4 Methods of test for aggregate for specific gravity, density, voids, absorption & bulking IS 2386 (Part III) 1963\n\n5 Methods of test for aggregate for Mechanical properties. IS 2386 (Part IV) 1963\n\n6 Methods of test for aggregate Soundness IS 2386 (Part V) 1963\n\n7 Methods of test for aggregate measuring mortar making properties of fine aggregates.IS 2386 (Part VI) 1963\n\n8 Methods of test for aggregate for alkali-aggregate reactivity IS 2386 (Part VII) 1963\n\n9 Methods of test for aggregate for petrographic examination IS 2386 (Part VIII) 1963\n\n1 Method of sampling of clay building bricks IS 5454 - 1978\n\n2 Method of test for burnt-clay building bricks. IS 3495 (Parts I TO iv) 1976\n\n3 Common burnt clay building bricks. IS 1077 - 1992\n\n1 Specification for sand for masonry mortars. IS 2116 - 1980\n\n2 Code of practice for preparation and use of masonry mortar IS 2250 - 1981\n\n1 Specification for coarse and fine aggregate. IS 383 - 1970\n\n2 Specification for compressive strength, Flexural strength IS 516 - 1959\n\n3 Code of Practices for Plain & reinforced concrete etc. IS 456 – 2000\n\n4 Methods of sampling and analysis of concrete IS 1199 – 1959\n\n5 Recommended Guide Lines for Concrete Mix Design IS 10262 – 1982\n\n1 Standard test method for water retention & daylight reflection test on concrete. ASTM-C-156809\n\n2 The standard method of test for the effect of organic materials in fine aggregate on strength of mortar. ASTM-C. 87-69\n\n3 Standard specification for liquid membranes forming compounds. ASTM C. 309-89\n\n1 Code of practice for the provision of water stops. IS 12200 – 1987\n\n2 Procedure for Testing Parts of IS 8543-19\n\n3 Standard Test Methods for Tensile Properties of Plastics. ASTM : D 638-1991\n\n4 Standard Test Methods for Thermoplastic Elastomers-Tension. ASTM : D 412-1992\n\n1 Specifications for HYSD bars. IS 1786 – 1985\n\n2 Specification for Mild Steel and Medium Tensile steel bars. IS 432 (P II) 1966\n\n3 Method for Tensile testing of steel wires. IS 5121 – 1972\n\n4 Hard drawn steel wire for concrete reinforcement. IS 1566 – 1982\n\n5 Method for Tensile testing of Steel products IS 1608 – 1972\n\n6 Code of practice for bending & fixing of bars for concrete reinforcement IS 2502 - 1963\n\n1 Specifications for pre cast concrete pipes. IS 458 – 1988\n\n2 Methods of Tests for concrete pipes. IS 3597 – 1985\n\n1 Preparation of dry sample (soil) IS:2720 (Part .I) 1983\n\n2 Determination of water content (moisture content) IS:2720 (Part .III) 1980 Sect/1\n\n3 Determination of specific gravity of fine-grained soil IS: 2720 (Part. III) 1980 Sect/2\n\n4 Determination of specific gravity of fine, medium & coarse-grained soil. IS: 2720 (Part. III) 1980 Sect/2\n\n5 Grain size analysis IS:2720 (Part.4) 1985\n\n6 Determination of Liquid and plastic limit IS:2720 (Part.5) 1985\n\n7 Determination of shrinkage factors IS: 2720 (Part. VI) 1987 \n8 Determination of water content - dry density relation using light compaction. IS: 2720 (Part. VII) 1980\n\n9 Determination of water content - dry density relation using heavy compaction.IS:2720 (Part.8) 1983\n\n10 Determination of water content - dry density relation using constant wt. soil method. IS:2720 (Part IX) 1971\n\n11 Determination of Unconfined compressive strength IS: 2720 (Part. X) 1991\n\n12 Determination of shear strength parameters(tri-axial) with out measurement of pore pressure parameters(Tri-axial compaction) IS:2720(Part. XI) 1971\n\n13 Determination of shear strength parameters (Tri-axial compaction)IS: 2720 (Part. XII) 1981\n\n14 Direct shear test IS: 2720 (Part. XIII) 1986\n\n15 Determination of Density Index (R.D) of cohesion less soil.IS:2720 (Part.14) 1983\n\n16 Determination of consolidation properties IS:2720 (Part.15) 1986\n\n17 Determination of permeability IS:2720 (Part.17) 1986\n\n18 Determination of dry density of soils, in place by the sand replacement method. IS:2720 (Part.28) 1974\n\n19 Determination of dry density of soils, in place by the core-cutter method.IS:2720 (Part.29) 1975\n\n20 Laboratory vane shear test. IS:2720 (Part.30) 1980\n\n21 Determination of the density in place by the ring and water replacement method.IS:2720 (Part.33) 1971\n\n22 Determination of free swell index of soils IS: 2720 (Part. XI) 1977\n\n23 Measurement of swelling pressure of soils. IS: 2720 (Part. XII) 1978\n\n24 Classification and identification of soils for General Engineering purposes.IS:1498 1970\n\nhttp://apwsipnsp.gov.in/APWSIP/Downloads/PIP%20Final/QC&QA_Manual_%20WB%2028.12.2009%20as%20corrected%20by%20R.K.Malhotra%20final/List%20of%20Indian%20Standards.pdf\n\n"}
{"id": "37515751", "url": "https://en.wikipedia.org/wiki?curid=37515751", "title": "Ministry of Mining and Energy (Serbia)", "text": "Ministry of Mining and Energy (Serbia)\n\nThe Ministry of Mining and Energy () is the ministry in the Government of Serbia which is in the charge of mining and energy. The current minister is Aleksandar Antić, in office since 27 April 2014.\n\nThe Ministry of Mining and Energy was established on 11 February 1991. The Ministry was abolished in 2011, when it was merged into Ministries of Infrastructure (Energy department) and Environment (Mining department). In 2012, it was reestablished as Energy department was split from the Infrastructure Ministry, and Environment department of reorganized former Environment Ministry was split. In 2014, the ministry in its original form with departments of Mining and Energy was established.\n\nThere is agency that operate within the scope of the Ministry:\n\nPolitical Party:\n\n\n"}
{"id": "28836625", "url": "https://en.wikipedia.org/wiki?curid=28836625", "title": "Mobile enterprise asset management", "text": "Mobile enterprise asset management\n\nMobile enterprise asset management (or mobile EAM) refers to the mobile extension of work processes for maintenance, operations and repair of corporate or public-entity physical assets, equipment, buildings and grounds. It involves management of work orders (planned, break/fix or service requests) via communication between a mobilized workforce and computer systems to maintain an organization’s facilities, structures and other assets.\n\nThe idea behind mobile EAM as a business practice is that it enables remote workers – employees who spend part or all of their time away from a central office – access to data from the organization’s computer application software for enterprise asset management (commonly referred to as an enterprise system, EAM system or backend system), typically using a handheld or other mobile computer. This is to distinguish from the term mobile asset management, which refers more broadly to the actual tools, instruments and containers organizations use to track and secure equipment and other such assets frequently on the move.\n\nIn the mobile EAM process, the organization eliminates a need for paper forms or other data reporting and communication methods (push-to-talk and radio) to move work order information to and from the point where the work is being performed.\n\nWhile enterprise asset management encompasses the management of an organization’s entire asset portfolio across processes including equipment addition/ reduction, replacement, over-hauling, redundancy setup and maintenance budgets, mobile enterprise asset management is focused, by definition, strictly on the wireless automation of asset management data for such processes.\n\nWhen viewed and used on a handheld device, mobile work order applications provide details such as location, stepwise job plans, safety alerts, lock-outs and prior work history on the asset, giving a maintenance technician or other remote worker more detailed asset information as well as the ability to transmit work data to the organization’s enterprise system when completed – through a wireless network, docking station or other synchronization method.\n\nUsing computer software to achieve standard mobile EAM practices, organizations often report such advantages as an increase in timely, accurate data flow between their remote workers and central management such as planners and schedulers, which thereby improves capital and labor allocation decision processes (including an ability to schedule more planned/preventive maintenance work). \nWith the proliferation of smartphone and other mobile computing technologies, asset managers can expect an ever more tech-savvy workforce, lower costs in mobile devices and a higher propensity of feature-rich, workflow-specific mobile applications.\n\nNearly all challenges in mobile EAM practices can be traced to two factors: time and labor resources (including IT or information technology management) and investment costs.\n\nDeveloping and implementing a mobile application architecture on the enterprise scale is not an easy undertaking by any means, as mobile applications are faced with a diversity of device operating systems, output media (voice and data) and connectivity methods, contrasting with a PC (personal computer) environment where in most cases software requires relatively few, if major, updates and lower upfront costs. Organizations looking to implement mobile EAM applications often seek the help of technology consulting firms and spend months researching, planning and selecting an implementation strategy.\n\nThe use of mobile enterprise application platforms (MEAPs), designed around service-oriented architecture principles for multiple systems integration and custom modification, and other forms of wireless computing technology for mobile EAM solutions is growing rapidly, particularly in industries where physical assets form a significant cost proportion of organizations’ total assets. These industries can include:\n\nIn such high-value asset scenarios, the asset lifecycle improvements introduced by the increase in enterprise data flow of mobile EAM processes can bring significant savings, particularly when part of an enterprise-wide capital and labor management strategy that integrates multiple systems in an enterprise architecture (EAM system, mobile EAM application, labor dispatch/scheduling software, GIS, etc.).\n\nIn a 2009 study, market analyst Gartner, Inc. forecasted, “for the MEAP and packaged mobile application market…we now expect market growth annually of 15 to 20% through 2013.” Gartner attributes this anticipated growth to enterprises’ increasing willingness (and ability) to extend decision-relevant information to employees, who are themselves increasingly mobile.\n\nFor EAM practices as a whole, this means that an increasing proportion of organizations in capital-intensive industry sectors (such as those above) are adopting mobile technology as an integral part of their enterprise asset management strategy – corresponding with an enterprise-wide emphasis on whole life planning, life cycle costing, planned and proactive maintenance and other industry best practices.\n\n"}
{"id": "9415650", "url": "https://en.wikipedia.org/wiki?curid=9415650", "title": "Moving floor", "text": "Moving floor\n\nMoving floor is a hydraulically-driven moving-floor conveyance system for moving bulk material or palletized product, which can be used in a warehouse, loading dock or semi-trailer. It automates and facilitates loading and unloading of palletized goods by eliminating the need for a forklift to enter the trailer. In a truck-based application, the system can quickly unload loose material without having to tip the trailer or tilt the floor as with other dumping systems. In a bulk material application such as a waste facility, these systems can reduce double handling by allowing any vehicle to deliver material to the conveying floor and move heavy bulk materials to subsequent stages of a process. For bagged waste, the system can also be combined with bag openers.\n\nThe moving floor is divided into three sets of narrow floor slats, with every third slat connected together, hydraulically powered to move forward and backward either in unison, or alternately. When all three sets move in unison, the load is moved upon them in the direction the operator wishes. Slat retraction (during which the load does not move) is accomplished by moving only one set of slats at a time. (The friction of the load on the two stationary sets of slats keeps the load from moving while a single set of slats alternately slides past.)\n\nOptionally, the semi-trailer may include a movable front wall with a rubber flap at the bottom extending onto the floor, or simply a movable flap or tarp at the front of the trailer bed on which the material was loaded. During unloading of loose material, either of these will ensure that nothing is left behind, almost or completely eliminating having to sweep the floor.\n\nIt takes about 5 to 15 minutes to unload a full 13.6-metre trailer, taking less manpower, equipment and time than without the system. The operator can enter a narrow low gate and dump the load inside a building. With a conventional tipper (or dump truck), that is often not possible. It is also possible to handle a full-width or full-length pallets, without opening the sides of the trailer.\n\nA similar system is designed specifically for moving pallets. It uses only two (rather than three) sets of slats; where one set raises the load just enough for the second set to retract. After the load-raiser lowers and retracts, then both sets move together to actually move the load. Load capacity is 30 tons and floor speed is up to 12 feet/minute.\n\nMoving walkway\n"}
{"id": "8648480", "url": "https://en.wikipedia.org/wiki?curid=8648480", "title": "Nonel", "text": "Nonel\n\nNonel is a shock tube detonator designed to initiate explosions, generally for the purpose of demolition of buildings and for use in the blasting of rock in mines and quarries. Instead of electric wires, a hollow plastic tube delivers the firing impulse to the detonator, making it immune to most of the hazards associated with stray electric current. It consists of a small diameter, three-layer plastic tube coated on the innermost wall with a reactive explosive compound, which, when ignited, propagates a low energy signal, similar to a dust explosion. The reaction travels at approximately 6,500 ft/s (2,000 m/s) along the length of the tubing with minimal disturbance outside of the tube. The design of nonel detonators incorporates patented technology, including the Cushion Disk (CD) and Delay Ignition Buffer (DIB) to provide reliability and accuracy in all blasting applications.\n\nNonel was invented by the Swedish company Nitro Nobel in the 1960s and 1970s, under the leadership of Per-Anders Persson, and launched to the demolitions market in 1973. (Nitro Nobel became a part of Dyno Nobel after being sold to Norwegian Dyno Industrier AS in 1986.) Nonel is a contraction of \"non electric\".\n\n"}
{"id": "15444460", "url": "https://en.wikipedia.org/wiki?curid=15444460", "title": "Open-circuit time constant method", "text": "Open-circuit time constant method\n\nThe open-circuit time constant method is an approximate analysis technique used in electronic circuit design to determine the corner frequency of complex circuits. It also is known as the zero-value time constant technique. The method provides a quick evaluation, and identifies the largest contributions to time constants as a guide to the circuit improvements.\n\nThe basis of the method is the approximation that the corner frequency of the amplifier is determined by the term in the denominator of its transfer function that is linear in frequency. This approximation can be extremely inaccurate in some cases where a zero in the numerator is near in frequency.\n\nThe method also uses a simplified method for finding the term linear in frequency based upon summing the RC-products for each capacitor in the circuit, where the resistor R for a selected capacitor is the resistance found by inserting a test source at its site and setting all other capacitors to zero. Hence the name \"zero-value time constant technique\".\n\nFigure 1 shows a simple RC low-pass filter. Its transfer function is found using Kirchhoff's current law as follows. At the output,\n\nwhere \"V\" is the voltage at the top of capacitor \"C\". At the center node:\n\nCombining these relations the transfer function is found to be:\n\nThe linear term in \"j\"ω in this transfer function can be derived by the following method, which is an application of the open-circuit time constant method to this example.\n\nIn effect, it is as though each capacitor charges and discharges through the resistance found in the circuit when the other capacitor is an open circuit.\n\nThe open circuit time constant procedure provides the linear term in \"j\"ω regardless of how complex the RC network becomes. For a complex circuit, the procedure consists of following the above rules, going through all the capacitors in the circuit. A more general derivation is found in Gray and Meyer.\n\nSo far the result is general, but an approximation is introduced to make use of this result: the assumption is made that this linear term in \"j\"ω determines the corner frequency of the circuit.\n\nThat assumption can be examined more closely using the example of Figure 1: suppose the time constants of this circuit are τ and τ; that is:\n\nComparing the coefficients of the linear and quadratic terms in \"j\"ω, there results:\n\nOne of the two time constants will be the longest; let it be τ. Suppose for the moment that it is much larger than the other, τ » τ. In this case, the approximations hold that:\n\nand\n\nIn other words, substituting the RC-values:\n\nand\n\nwhere ( ^ ) denotes the approximate result. As an aside, notice that the circuit time constants both involve both capacitors; in other words, in general the circuit time constants are not decided by any single capacitor. Using these results, it is easy to explore how well the corner frequency (the 3 dB frequency) is given by\n\nas the parameters vary. Also, the exact transfer function can be compared with the approximate one, that is,\n\nOf course agreement is good when the assumption τ » τ is accurate.\n\nFigure 2 illustrates the approximation. The x-axis is the ratio τ / τ on a logarithmic scale. An increase in this variable means the higher pole is further above the corner frequency. The y-axis is the ratio of the OCTC (open-circuit time constant) estimate to the true time constant. For the lowest pole use curve T_1; this curve refers to the corner frequency; and for the higher pole use curve T_2. The worst agreement is for τ = τ. In this case τ = 2 τ and the corner frequency is a factor of 2 too small. The higher pole is a factor of 2 too high (its time constant is half of the real value).\n\nIn all cases, the estimated corner frequency is closer than a factor of two from the real one, and always is \"conservative\" that is, lower than the real corner, so the actual circuit will behave better than predicted. However, the higher pole always is \"optimistic\", that is, predicts the high pole at a higher frequency than really is the case. To use these estimates for step response predictions, which depend upon the ratio of the two pole frequencies (see article on pole splitting for an example), Figure 2 suggests a fairly large ratio of τ / τ is needed for accuracy because the errors in τ and τ reinforce each other in the ratio τ / τ.\n\nThe open-circuit time constant method focuses upon the corner frequency alone, but as seen above, estimates for higher poles also are possible.\n\nApplication of the open-circuit time constant method to a number of single transistor amplifier stages can be found in Pittet and Kandaswamy.\n"}
{"id": "15361791", "url": "https://en.wikipedia.org/wiki?curid=15361791", "title": "Oscilloscope", "text": "Oscilloscope\n\nAn oscilloscope, previously called an oscillograph, and informally known as a scope or o-scope, CRO (for cathode-ray oscilloscope), or DSO (for the more modern digital storage oscilloscope), is a type of electronic test instrument that allows observation of varying signal voltages, usually as a two-dimensional plot of one or more signals as a function of time. Other signals (such as sound or vibration) can be converted to voltages and displayed.\n\nOscilloscopes are used to observe the change of an electrical signal over time, such that voltage and time describe a shape which is continuously graphed against a calibrated scale. The observed waveform can be analyzed for such properties as amplitude, frequency, rise time, time interval, distortion and others. Modern digital instruments may calculate and display these properties directly. Originally, calculation of these values required manually measuring the waveform against the scales built into the screen of the instrument.\n\nThe oscilloscope can be adjusted so that repetitive signals can be observed as a continuous shape on the screen. A storage oscilloscope allows single events to be captured by the instrument and displayed for a relatively long time, allowing observation of events too fast to be directly perceptible.\n\nOscilloscopes are used in the sciences, medicine, engineering, automotive and the telecommunications industry. General-purpose instruments are used for maintenance of electronic equipment and laboratory work. Special-purpose oscilloscopes may be used for such purposes as analyzing an automotive ignition system or to display the waveform of the heartbeat as an electrocardiogram.\n\nEarly oscilloscopes used cathode ray tubes (CRTs) as their display element (hence they were commonly referred to as CROs) and linear amplifiers for signal processing. Storage oscilloscopes used special storage CRTs to maintain a steady display of a single brief signal. CROs were later largely superseded by digital storage oscilloscopes (DSOs) with thin panel displays, fast analog-to-digital converters and digital signal processors. DSOs without integrated displays (sometimes known as digitisers) are available at lower cost and use a general-purpose digital computer to process and display waveforms.\n\nThe basic oscilloscope, as shown in the illustration, is typically divided into four sections: the display, vertical controls, horizontal controls and trigger controls. The display is usually a CRT (historically) or LCD panel which is laid out with both horizontal and vertical reference lines referred to as the graticule. CRT displays are additionally equipped with three controls: focus, intensity, and beam finder.\n\nThe vertical section controls the amplitude of the displayed signal. This section carries a Volts-per-Division (Volts/Div) selector knob, an AC/DC/Ground selector switch and the vertical (primary) input for the instrument. Additionally, this section is typically equipped with the vertical beam position knob.\n\nThe horizontal section controls the time base or \"sweep\" of the instrument. The primary control is the Seconds-per-Division (Sec/Div) selector switch. Also included is a horizontal input for plotting dual X-Y axis signals. The horizontal beam position knob is generally located in this section.\n\nThe trigger section controls the start event of the sweep. The trigger can be set to automatically restart after each sweep or it can be configured to respond to an internal or external event. The principal controls of this section will be the source and coupling selector switches. An external trigger input (EXT Input) and level adjustment will also be included.\n\nIn addition to the basic instrument, most oscilloscopes are supplied with a probe as shown. The probe will connect to any input on the instrument and typically has a resistor of ten times the oscilloscope's input impedance. This results in a .1 (‑10X) attenuation factor which helps to isolate the capacitive load presented by the probe cable from the signal being measured. Some probes have a switch allowing the operator to bypass the resistor when appropriate.\n\nMost modern oscilloscopes are lightweight, portable instruments that are compact enough to be easily carried by a single person. In addition to the portable units, the market offers a number of miniature battery-powered instruments for field service applications. Laboratory grade oscilloscopes, especially older units which use vacuum tubes, are generally bench-top devices or may be mounted into dedicated carts. Special-purpose oscilloscopes may be rack-mounted or permanently mounted into a custom instrument housing.\n\nThe signal to be measured is fed to one of the input connectors, which is usually a coaxial connector such as a BNC or UHF type. Binding posts or banana plugs may be used for lower frequencies.\nIf the signal source has its own coaxial connector, then a simple coaxial cable is used; otherwise, a specialized cable called a \"scope probe\", supplied with the oscilloscope, is used. In general, for routine use, an open wire test lead for connecting to the point being observed is not satisfactory, and a probe is generally necessary.\nGeneral-purpose oscilloscopes usually present an input impedance of 1 megohm in parallel with a small but known capacitance such as 20 picofarads. This allows the use of standard oscilloscope probes. Scopes for use with very high frequencies may have 50‑ohm inputs, which must be either connected directly to a 50‑ohm signal source or used with Z or active probes.\n\nLess-frequently-used inputs include one (or two) for triggering the sweep, horizontal deflection for X‑Y mode displays, and trace brightening/darkening, sometimes called \"z'‑axis inputs.\n\nOpen wire test leads (flying leads) are likely to pick up interference, so they are not suitable for low level signals. Furthermore, the leads have a high inductance, so they are not suitable for high frequencies. Using a shielded cable (i.e., coaxial cable) is better for low level signals. Coaxial cable also has lower inductance, but it has higher capacitance: a typical 50 ohm cable has about 90 pF per meter. Consequently, a one-meter direct (1X) coaxial probe will load a circuit with a capacitance of about 110 pF and a resistance of 1 megohm.\n\nTo minimize loading, attenuator probes (e.g., 10X probes) are used. A typical probe uses a 9 megohm series resistor shunted by a low-value capacitor to make an RC compensated divider with the cable capacitance and scope input. The RC time constants are adjusted to match. For example, the 9 megohm series resistor is shunted by a 12.2 pF capacitor for a time constant of 110 microseconds. The cable capacitance of 90 pF in parallel with the scope input of 20 pF and 1 megohm (total capacitance 110 pF) also gives a time constant of 110 microseconds. In practice, there will be an adjustment so the operator can precisely match the low frequency time constant (called compensating the probe). Matching the time constants makes the attenuation independent of frequency. At low frequencies (where the resistance of \"R\" is much less than the reactance of \"C\"), the circuit looks like a resistive divider; at high frequencies (resistance much greater than reactance), the circuit looks like a capacitive divider.\n\nThe result is a frequency compensated probe for modest frequencies that presents a load of about 10 megohms shunted by 12 pF. Although such a probe is an improvement, it does not work when the time scale shrinks to several cable transit times (transit time is typically 5 ns). In that time frame, the cable looks like its characteristic impedance, and there will be reflections from the transmission line mismatch at the scope input and the probe that causes ringing. The modern scope probe uses lossy low capacitance transmission lines and sophisticated frequency shaping networks to make the 10X probe perform well at several hundred megahertz. Consequently, there are other adjustments for completing the compensation.\n\nProbes with 10:1 attenuation are by far the most common; for large signals (and slightly-less capacitive loading), 100:1 probes may be used. There are also probes that contain switches to select 10:1 or direct (1:1) ratios, but this setting has significant capacitance (tens of pF) at the probe tip, because the whole cable's capacitance is now directly connected.\n\nMost oscilloscopes allow for probe attenuation factors, displaying the effective sensitivity at the probe tip. Historically, some auto-sensing circuitry used indicator lamps behind translucent windows in the panel to illuminate different parts of the sensitivity scale. To do so, the probe connectors (modified BNCs) had an extra contact to define the probe's attenuation. (A certain value of resistor, connected to ground, \"encodes\" the attenuation.) Because probes wear out, and because the auto-sensing circuitry is not compatible between different makes of oscilloscope, auto-sensing probe scaling is not fool proof. Likewise, manually setting the probe attenuation is prone to user error and it is a common mistake to have the probe scaling set incorrectly; the resultant reading will then be wrong by a factor of 10.\n\nThere are special high voltage probes which also form compensated attenuators with the oscilloscope input; the probe body is physically large, and some require partly filling a canister surrounding the series resistor with volatile liquid fluorocarbon to displace air. At the oscilloscope end is a box with several waveform-trimming adjustments. For safety, a barrier disc keeps one's fingers distant from the point being examined. Maximum voltage is in the low tens of kV. (Observing a high voltage ramp can create a staircase waveform with steps at different points every repetition, until the probe tip is in contact. Until then, a tiny arc charges the probe tip, and its capacitance holds the voltage (open circuit). As the voltage continues to climb, another tiny arc charges the tip further.)\n\nThere are also current probes, with cores that surround the conductor carrying current to be examined. One type has a hole for the conductor, and requires that the wire be passed through the hole; they are for semi-permanent or permanent mounting. However, other types, for testing, have a two-part core that permit them to be placed around a wire. Inside the probe, a coil wound around the core provides a current into an appropriate load, and the voltage across that load is proportional to current. However, this type of probe can sense AC, only.\n\nA more-sophisticated probe includes a magnetic flux sensor (Hall effect sensor) in the magnetic circuit. The probe connects to an amplifier, which feeds (low frequency) current into the coil to cancel the sensed field; the magnitude of that current provides the low-frequency part of the current waveform, right down to DC. The coil still picks up high frequencies. There is a combining network akin to a loudspeaker crossover network.\n\nThis control adjusts CRT focus to obtain the sharpest, most-detailed trace. In practice, focus needs to be adjusted slightly when observing quite-different signals, which means that it needs to be an external control. The control varies the voltage applied to a focusing anode within the CRT. Flat-panel displays do not need focus adjustments and therefore do not include this control. \n\nThis adjusts trace brightness. Slow traces on CRT oscilloscopes need less, and fast ones, especially if not often repeated, require more brightness. On flat panels, however, trace brightness is essentially independent of sweep speed, because the internal signal processing effectively synthesizes the display from the digitized data.\n\nCan also be called \"Shape\" or \"spot shape\". Adjusts the relative voltages on two of the CRT anodes such that a displayed spot changes from elliptical in one plane through a circular spot to an ellipse at 90 degrees to the first. This control may be absent from simpler oscilloscope designs or may even be an internal control. It is not necessary with flat panel displays.\n\nModern oscilloscopes have direct-coupled deflection amplifiers, which means the trace could be deflected off-screen. They also might have their beam blanked without the operator knowing it. To help in restoring a visible display, the beam finder circuit overrides any blanking and limits the beam deflected to the visible portion of the screen. Beam-finder circuits often distort the trace while activated.\n\nThe graticule is a grid of squares that serve as reference marks for measuring the displayed trace. These markings, whether located directly on the screen or on a removable plastic filter, usually consist of a 1 cm grid with closer tick marks (often at 2 mm) on the centre vertical and horizontal axis. One expects to see ten major divisions across the screen; the number of vertical major divisions varies. Comparing the grid markings with the waveform permits one to measure both voltage (vertical axis) and time (horizontal axis). Frequency can also be determined by measuring the waveform period and calculating its reciprocal.\n\nOn old and lower-cost CRT oscilloscopes the graticule is a sheet of plastic, often with light-diffusing markings and concealed lamps at the edge of the graticule. The lamps had a brightness control. Higher-cost instruments have the graticule marked on the inside face of the CRT, to eliminate parallax errors; better ones also had adjustable edge illumination with diffusing markings. (Diffusing markings appear bright.) Digital oscilloscopes, however, generate the graticule markings on the display in the same way as the trace.\n\nExternal graticules also protect the glass face of the CRT from accidental impact. Some CRT oscilloscopes with internal graticules have an unmarked tinted sheet plastic light filter to enhance trace contrast; this also serves to protect the faceplate of the CRT.\n\nAccuracy and resolution of measurements using a graticule is relatively limited; better instruments sometimes have movable bright markers on the trace that permit internal circuits to make more refined measurements.\n\nBoth calibrated vertical sensitivity and calibrated horizontal time are set in 1 - 2 - 5 - 10 steps. This leads, however, to some awkward interpretations of minor divisions.\n\nDigital oscilloscopes generate the graticule digitally, which means that the scale can vary, and accuracy of readings is much improved.\n\n These select the horizontal speed of the CRT's spot as it creates the trace; this process is commonly referred to as the sweep. In all but the least-costly modern oscilloscopes, the sweep speed is selectable and calibrated in units of time per major graticule division. Quite a wide range of sweep speeds is generally provided, from seconds to as fast as picoseconds (in the fastest) per division. Usually, a continuously-variable control (often a knob in front of the calibrated selector knob) offers uncalibrated speeds, typically slower than calibrated. This control provides a range somewhat greater than that of consecutive calibrated steps, making any speed available between the extremes.\n\nFound on some better analog oscilloscopes, this varies the time (holdoff) during which the sweep circuit ignores triggers. It provides a stable display of some repetitive events in which some triggers would create confusing displays. It is usually set to minimum, because a longer time decreases the number of sweeps per second, resulting in a dimmer trace. See Holdoff for a more detailed description.\n\nTo accommodate a wide range of input amplitudes, a switch selects calibrated sensitivity of the vertical deflection. Another control, often in front of the calibrated-selector knob, offers a continuously-variable sensitivity over a limited range from calibrated to less-sensitive settings.\n\nOften the observed signal is offset by a steady component, and only the changes are of interest. A switch (AC position) connects a capacitor in series with the input that passes only the changes (provided that they are not too slow -- \"slow\" would mean visible). However, when the signal has a fixed offset of interest, or changes quite slowly, the input is connected directly (DC switch position). Most oscilloscopes offer the DC input option. For convenience, to see where zero volts input currently shows on the screen, many oscilloscopes have a third switch position (GND) that disconnects the input and grounds it. Often, in this case, the user centers the trace with the Vertical Position control.\n\nBetter oscilloscopes have a polarity selector. Normally, a positive input moves the trace upward, but this permits inverting—positive deflects the trace downward.\n\nThis control is found only on more elaborate oscilloscopes; it offers adjustable sensitivity for external horizontal inputs. It is only active when the instrument is in X-Y mode, that is, when the internal horizontal sweep is not in use.\n\n The vertical position control moves the whole displayed trace up and down. It is used to set the no-input trace exactly on the center line of the graticule, but also permits offsetting vertically by a limited amount. With direct coupling, adjustment of this control can compensate for a limited DC component of an input.\n\n The horizontal position control moves the display sidewise. It usually sets the left end of the trace at the left edge of the graticule, but it can displace the whole trace when desired. This control also moves the X-Y mode traces sidewise in some instruments, and can compensate for a limited DC component as for vertical position.\n\n\"* (Please see Dual and Multiple-trace Oscilloscopes, below.)\"\n\nDual-trace oscilloscopes have a mode switch to select either channel alone, both channels, or (in some) an X‑Y display, which uses the second channel for X deflection. When both channels are displayed, the type of channel switching can be selected on some oscilloscopes; on others, the type depends upon timebase setting. If manually selectable, channel switching can be free-running (asynchronous), or between consecutive sweeps. Some Philips dual-trace analog oscilloscopes had a fast analog multiplier, and provided a display of the product of the input channels.\n\nMultiple-trace oscilloscopes have a switch for each channel to enable or disable display of that trace's signal.\n\n\"* (Please see Delayed Sweep, below.)\"\n\nThese include controls for the delayed-sweep timebase, which is calibrated, and often also variable. The slowest speed is several steps faster than the slowest main sweep speed, although the fastest is generally the same. A calibrated multiturn delay time control offers wide range, high resolution delay settings; it spans the full duration of the main sweep, and its reading corresponds to graticule divisions (but with much finer precision). Its accuracy is also superior to that of the display.\n\nA switch selects display modes: Main sweep only, with a brightened region showing when the delayed sweep is advancing, delayed sweep only, or (on some) a combination mode.\n\nGood CRT oscilloscopes include a delayed-sweep intensity control, to allow for the dimmer trace of a much-faster delayed sweep that nevertheless occurs only once per main sweep. Such oscilloscopes also are likely to have a trace separation control for multiplexed display of both the main and delayed sweeps together.\n\n\"* (Please see Triggered Sweep, below.)\"\n\nA switch selects the Trigger Source. It can be an external input, one of the vertical channels of a dual or multiple-trace oscilloscope, or the AC line (mains) frequency. Another switch enables or disables Auto trigger mode, or selects single sweep, if provided in the oscilloscope. Either a spring-return switch position or a pushbutton arms single sweeps.\n\nA Level control varies the voltage on the waveform which generates a trigger, and the Slope switch selects positive-going or negative-going polarity at the selected trigger level.\n\nTo display events with unchanging or slowly (visibly) changing waveforms, but occurring at times that may not be evenly spaced, modern oscilloscopes have triggered sweeps. Compared to simpler oscilloscopes with sweep oscillators that are always running, triggered-sweep oscilloscopes are markedly more versatile.\n\nA triggered sweep starts at a selected point on the signal, providing a stable display. In this way, triggering allows the display of periodic signals such as sine waves and square waves, as well as nonperiodic signals such as single pulses, or pulses that do not recur at a fixed rate.\n\nWith triggered sweeps, the scope will blank the beam and start to reset the sweep circuit each time the beam reaches the extreme right side of the screen. For a period of time, called \"holdoff\", (extendable by a front-panel control on some better oscilloscopes), the sweep circuit resets completely and ignores triggers. Once holdoff expires, the next trigger starts a sweep. The trigger event is usually the input waveform reaching some user-specified threshold voltage (trigger level) in the specified direction (going positive or going negative—trigger polarity).\n\nIn some cases, variable holdoff time can be really useful to make the sweep ignore interfering triggers that occur before the events to be observed. In the case of repetitive, but complex waveforms, variable holdoff can create a stable display that cannot otherwise be achieved.\n\n\"Trigger holdoff\" defines a certain period following a trigger during which the scope will not trigger again. This makes it easier to establish a stable view of a waveform with multiple edges which would otherwise cause another trigger.\n\nImagine the following repeating waveform:\n\nThe green line is the waveform, the red vertical partial line represents the location of the trigger, and the yellow line represents the trigger level. If the scope was simply set to trigger on every rising edge, this waveform would cause three triggers for each cycle:\n\nAssuming the signal is fairly high frequency, the scope would probably look something like this:\n\nExcept that on the scope, each trigger would be the same channel, and so would be the same color.\n\nIt is desired to set the scope to only trigger on one edge per cycle, so it is necessary to set the holdoff to be slightly less than the period of the waveform. That will prevent it from triggering more than once per cycle, but still allow it to trigger on the first edge of the next cycle.\n\nTriggered sweeps can display a blank screen if there are no triggers. To avoid this, these sweeps include a timing circuit that generates free-running triggers so a trace is always visible. Once triggers arrive, the timer stops providing pseudo-triggers. Automatic sweep mode can be de-selected when observing low repetition rates.\n\nIf the input signal is periodic, the sweep repetition rate can be adjusted to display a few cycles of the waveform. Early (tube) oscilloscopes and lowest-cost oscilloscopes have sweep oscillators that run continuously, and are uncalibrated. Such oscilloscopes are very simple, comparatively inexpensive, and were useful in radio servicing and some TV servicing. Measuring voltage or time is possible, but only with extra equipment, and is quite inconvenient. They are primarily qualitative instruments.\n\nThey have a few (widely spaced) frequency ranges, and relatively wide-range continuous frequency control within a given range. In use, the sweep frequency is set to slightly lower than some submultiple of the input frequency, to display typically at least two cycles of the input signal (so all details are visible). A very simple control feeds an adjustable amount of the vertical signal (or possibly, a related external signal) to the sweep oscillator. The signal triggers beam blanking and a sweep retrace sooner than it would occur free-running, and the display becomes stable.\n\nSome oscilloscopes offer these—the sweep circuit is manually armed (typically by a pushbutton or equivalent) \"Armed\" means it's ready to respond to a trigger. Once the sweep is complete, it resets, and will not sweep until re-armed. This mode, combined with an oscilloscope camera, captures single-shot events.\n\nTypes of trigger include:\n\nSome recent designs of oscilloscopes include more sophisticated triggering schemes; these are described toward the end of this article.\n\nMore sophisticated analog oscilloscopes contain a second timebase for a delayed sweep. A delayed sweep provides a very detailed look at some small selected portion of the main timebase. The main timebase serves as a controllable delay, after which the delayed timebase starts. This can start when the delay expires, or can be triggered (only) after the delay expires. Ordinarily, the delayed timebase is set for a faster sweep, sometimes much faster, such as 1000:1. At extreme ratios, jitter in the delays on consecutive main sweeps degrades the display, but delayed-sweep triggers can overcome that.\n\nThe display shows the vertical signal in one of several modes: the main timebase, or the delayed timebase only, or a combination thereof. When the delayed sweep is active, the main sweep trace brightens while the delayed sweep is advancing. In one combination mode, provided only on some oscilloscopes, the trace changes from the main sweep to the delayed sweep once the delayed sweep starts, although less of the delayed fast sweep is visible for longer delays. Another combination mode multiplexes (alternates) the main and delayed sweeps so that both appear at once; a trace separation control displaces them.\n\nDSOs allow waveforms to be displayed in this way, without offering a delayed timebase as such.\n\nOscilloscopes with two vertical inputs, referred to as dual-trace oscilloscopes, are extremely useful and commonplace.\nUsing a single-beam CRT, they multiplex the inputs, usually switching between them fast enough to display two traces apparently at once. Less common are oscilloscopes with more traces; four inputs are common among these, but a few (Kikusui, for one) offered a display of the sweep trigger signal if desired. Some multi-trace oscilloscopes use the external trigger input as an optional vertical input, and some have third and fourth channels with only minimal controls. In all cases, the inputs, when independently displayed, are time-multiplexed, but dual-trace oscilloscopes often can add their inputs to display a real-time analog sum. (Inverting one channel provides a difference, provided that neither channel is overloaded. This difference mode can provide a moderate-performance differential input.)\n\nSwitching channels can be asynchronous, that is, free-running, with trace blanking while switching, or after each horizontal sweep is complete. Asynchronous switching is usually designated \"Chopped\", while sweep-synchronized is designated \"Alt[ernate]\". A given channel is alternately connected and disconnected, leading to the term \"chopped\". Multi-trace oscilloscopes also switch channels either in chopped or alternate modes.\n\nIn general, chopped mode is better for slower sweeps. It is possible for the internal chopping rate to be a multiple of the sweep repetition rate, creating blanks in the traces, but in practice this is rarely a problem; the gaps in one trace are overwritten by traces of the following sweep. A few oscilloscopes had a modulated chopping rate to avoid this occasional problem. Alternate mode, however, is better for faster sweeps.\n\nTrue dual-beam CRT oscilloscopes did exist, but were not common. One type (Cossor, U.K.) had a beam-splitter plate in its CRT, and single-ended deflection following the splitter. Others had two complete electron guns, requiring tight control of axial (rotational) mechanical alignment in manufacturing the CRT. Beam-splitter types had horizontal deflection common to both vertical channels, but dual-gun oscilloscopes could have separate time bases, or use one time base for both channels. Multiple-gun CRTs (up to ten guns) were made in past decades. With ten guns, the envelope (bulb) was cylindrical throughout its length. (Also see \"CRT Invention\" in Oscilloscope history.)\n\nIn an analog oscilloscope, the vertical amplifier acquires the signal[s] to be displayed. In better oscilloscopes, it delays them by a fraction of a microsecond, and provides a signal large enough to deflect the CRT's beam. That deflection is at least somewhat beyond the edges of the graticule, and more typically some distance off-screen. The amplifier has to have low distortion to display its input accurately (it must be linear), and it has to recover quickly from overloads. As well, its time-domain response has to represent transients accurately—minimal overshoot, rounding, and tilt of a flat pulse top.\n\nA vertical input goes to a frequency-compensated step attenuator to reduce large signals to prevent overload. The attenuator feeds a low-level stage (or a few), which in turn feed gain stages (and a delay-line driver if there is a delay). Following are more gain stages, up to the final output stage which develops a large signal swing (tens of volts, sometimes over 100 volts) for CRT electrostatic deflection.\n\nIn dual and multiple-trace oscilloscopes, an internal electronic switch selects the relatively low-level output of one channel's amplifiers and sends it to the following stages of the vertical amplifier, which is only a single channel, so to speak, from that point on.\n\nIn free-running (\"chopped\") mode, the oscillator (which may be simply a different operating mode of the switch driver) blanks the beam before switching, and unblanks it only after the switching transients have settled.\n\nPart way through the amplifier is a feed to the sweep trigger circuits, for internal triggering from the signal. This feed would be from an individual channel's amplifier in a dual or multi-trace oscilloscope, the channel depending upon the setting of the trigger source selector.\n\nThis feed precedes the delay (if there is one), which allows the sweep circuit to unblank the CRT and start the forward sweep, so the CRT can show the triggering event. High-quality analog delays add a modest cost to an oscilloscope, and are omitted in oscilloscopes that are cost-sensitive.\n\nThe delay, itself, comes from a special cable with a pair of conductors wound around a flexible, magnetically soft core. The coiling provides distributed inductance, while a conductive layer close to the wires provides distributed capacitance. The combination is a wideband transmission line with considerable delay per unit length. Both ends of the delay cable require matched impedances to avoid reflections.\n\nMost modern oscilloscopes have several inputs for voltages, and thus can be used to plot one varying voltage versus another. This is especially useful for graphing I-V curves (current versus voltage characteristics) for components such as diodes, as well Lissajous patterns. Lissajous figures are an example of how an oscilloscope can be used to track phase differences between multiple input signals. This is very frequently used in broadcast engineering to plot the left and right stereophonic channels, to ensure that the stereo generator is calibrated properly. Historically, stable Lissajous figures were used to show that two sine waves had a relatively simple frequency relationship, a numerically-small ratio. They also indicated phase difference between two sine waves of the same frequency.\n\nThe X-Y mode also allows the oscilloscope to be used as a vector monitor to display images or user interfaces. Many early games, such as Tennis for Two, used an oscilloscope as an output device.\n\nComplete loss of signal in an X-Y CRT display means that the beam strikes a small spot, which risks burning the phosphor. Older phosphors burned more easily. Some dedicated X-Y displays reduce beam current greatly, or blank the display entirely, if there are no inputs present.\n\nAs with all practical instruments, oscilloscopes do not respond equally to all possible input frequencies. The range of frequencies an oscilloscope can usefully display is referred to as its bandwidth. Bandwidth applies primarily to the Y-axis, although the X-axis sweeps have to be fast enough to show the highest-frequency waveforms.\n\nThe bandwidth is defined as the frequency at which the sensitivity is 0.707 of that at DC or the lowest AC frequency\n(a drop of 3 dB). The oscilloscope's response will drop off rapidly as the input frequency is raised above that point. Within the stated bandwidth the response will not necessarily be exactly uniform (or \"flat\"), but should always fall within a +0 to -3 dB range. One source states that there is a noticeable effect on the accuracy of voltage measurements at only 20 percent of the stated bandwidth. Some oscilloscopes' specifications do include a narrower tolerance range within the stated bandwidth.\n\nProbes also have bandwidth limits and must be chosen and used to properly handle the frequencies of interest. To achieve the flattest response, most probes must be \"compensated\" (an adjustment performed using a test signal from the oscilloscope) to allow for the reactance of the probe's cable.\n\nAnother related specification is rise time. This is the duration of the fastest pulse that can be resolved by the scope. It is related to the bandwidth approximately by:\n\nBandwidth in Hz x rise time in seconds = 0.35 \n\nFor example, an oscilloscope intended to resolve pulses with a rise time of 1 nanosecond would have a bandwidth of 350 MHz.\n\nIn analog instruments, the bandwidth of the oscilloscope is limited by the vertical amplifiers and the CRT or other display subsystem. In digital instruments, the sampling rate of the analog to digital converter (ADC) is a factor, but the stated analog bandwidth (and therefore the overall bandwidth of the instrument) is usually less than the ADC's Nyquist frequency. This is due to limitations in the analog signal amplifier, deliberate design of the anti-aliasing filter that precedes the ADC, or both.\n\nFor a digital oscilloscope, a rule of thumb is that the continuous sampling rate should be ten times the highest frequency desired to resolve; for example a 20 megasample/second rate would be applicable for measuring signals up to about 2 megahertz. This allows the anti-aliasing filter to be designed with a 3 dB down point of 2 MHz and an effective cutoff at 10 MHz (the Nyquist frequency), avoiding the artifacts of a very steep (\"brick-wall\") filter.\n\nA sampling oscilloscope can display signals of considerably higher frequency than the sampling rate if the signals are exactly, or nearly, repetitive. It does this by taking one sample from each successive repetition of the input waveform, each sample being at an increased time interval from the trigger event. The waveform is then displayed from these collected samples. This mechanism is referred to as \"equivalent-time sampling\". Some oscilloscopes can operate in either this mode or in the more traditional \"real-time\" mode at the operator's choice.\n\nSome oscilloscopes have \"cursors\", which are lines that can be moved about the screen to measure the time interval between two points, or the difference between two voltages. A few older oscilloscopes simply brightened the trace at movable locations. These cursors are more accurate than visual estimates referring to graticule lines.\n\nBetter quality general purpose oscilloscopes include a calibration signal for setting up the compensation of test probes; this is (often) a 1 kHz square-wave signal of a definite peak-to-peak voltage available at a test terminal on the front panel. Some better oscilloscopes also have a squared-off loop for checking and adjusting current probes.\n\nSometimes the event that the user wants to see may only happen occasionally.\nTo catch these events, some oscilloscopes, known as \"storage scopes\", preserve the most recent sweep on the screen. This was originally achieved by using a special CRT, a \"storage tube\", which would retain the image of even a very brief event for a long time.\n\nSome digital oscilloscopes can sweep at speeds as slow as once per hour, emulating a strip chart recorder.\nThat is, the signal scrolls across the screen from right to left. Most oscilloscopes with this facility switch from a sweep to a strip-chart mode at about one sweep per ten seconds. This is because otherwise, the scope looks broken: it's collecting data, but the dot cannot be seen.\n\nIn current oscilloscopes, digital signal sampling is more often used for all but the simplest models. Samples feed fast analog-to-digital converters, following which all signal processing (and storage) is digital.\n\nMany oscilloscopes have different plug-in modules for different purposes, e.g., high-sensitivity amplifiers of relatively narrow bandwidth, differential amplifiers, amplifiers with four or more channels, sampling plugins for repetitive signals of very high frequency, and special-purpose plugins, including audio/ultrasonic spectrum analyzers, and stable-offset-voltage direct-coupled channels with relatively high gain.\n\nOne of the most frequent uses of scopes is troubleshooting malfunctioning electronic equipment. One of the advantages of a scope is that it can graphically show signals: where a voltmeter may show a totally unexpected voltage, a scope may reveal that the circuit is oscillating. In other cases the precise shape or timing of a pulse is important.\n\nIn a piece of electronic equipment, for example, the connections between stages (e.g. electronic mixers, electronic oscillators, amplifiers) may be 'probed' for the expected signal, using the scope as a simple signal tracer. If the expected signal is absent or incorrect, some preceding stage of the electronics is not operating correctly. Since most failures occur because of a single faulty component, each measurement can prove that half of the stages of a complex piece of equipment either work, or probably did not cause the fault.\n\nOnce the faulty stage is found, further probing can usually tell a skilled technician exactly which component has failed. Once the component is replaced, the unit can be restored to service, or at least the next fault can be isolated. This sort of troubleshooting is typical of radio and TV receivers, as well as audio amplifiers, but can apply to quite-different devices such as electronic motor drives.\n\nAnother use is to check newly designed circuitry. Very often a newly designed circuit will misbehave because of design errors, bad voltage levels, electrical noise etc. Digital electronics usually operate from a clock, so a dual-trace scope which shows both the clock signal and a test signal dependent upon the clock is useful. Storage scopes are helpful for \"capturing\" rare electronic events that cause defective operation.\n\nFirst appearing in the 1970s for ignition system analysis, automotive oscilloscopes are becoming an important workshop tool for testing sensors and output signals on electronic engine management systems, braking and stability systems. Some oscilloscopes can trigger and decode serial bus messages, such as the CAN bus commonly used in automotive applications. \n\nFor work at high frequencies and with fast digital signals, the bandwidth of the vertical amplifiers and sampling rate must be high enough. For general-purpose use, a bandwidth of at least 100 MHz is usually satisfactory. A much lower bandwidth is sufficient for audio-frequency applications only.\nA useful sweep range is from one second to 100 nanoseconds, with appropriate triggering and (for analog instruments) sweep delay. A well-designed, stable trigger circuit is required for a steady display. The chief benefit of a quality oscilloscope is the quality of the trigger circuit.\n\nKey selection criteria of a DSO (apart from input bandwidth) are the sample memory depth and sample rate. Early DSOs in the mid- to late 1990s only had a few KB of sample memory per channel. This is adequate for basic waveform display, but does not allow detailed examination of the waveform or inspection of long data packets for example. Even entry-level (<$500) modern DSOs now have 1 MB or more of sample memory per channel, and this has become the expected minimum in any modern DSO. Often this sample memory is shared between channels, and can sometimes only be fully available at lower sample rates. At the highest sample rates, the memory may be limited to a few tens of KB.\nAny modern \"real-time\" sample rate DSO will have typically 5–10 times the input bandwidth in sample rate. So a 100 MHz bandwidth DSO would have 500 Ms/s – 1 Gs/s sample rate. The theoretical minimum sample rate required, using SinX/x interpolation, is 2.5 times the bandwidth.\n\nAnalog oscilloscopes have been almost totally displaced by digital storage scopes except for use exclusively at lower frequencies. Greatly increased sample rates have largely eliminated the display of incorrect signals, known as \"aliasing\", that was sometimes present in the first generation of digital scopes. The problem can still occur when, for example, viewing a short section of a repetitive waveform that repeats at intervals thousands of times longer than the section viewed (for example a short synchronization pulse at the beginning of a particular television line), with an oscilloscope that cannot store the extremely large number of samples between one instance of the short section and the next.\n\nThe used test equipment market, particularly on-line auction venues, typically has a wide selection of older analog scopes available. However it is becoming more difficult to obtain replacement parts for these instruments, and repair services are generally unavailable from the original manufacturer. Used instruments are usually out of calibration, and recalibration by companies with the equipment and expertise usually costs more than the second-hand value of the instrument.\n\n, a 350 MHz bandwidth (BW), 2.5 gigasamples per second (GS/s), dual-channel digital storage scope costs about US$7000 new.\n\nOn the lowest end, an inexpensive hobby-grade single-channel DSO could be purchased for under $90 as of June 2011. These often have limited bandwidth and other facilities, but fulfill the basic functions of an oscilloscope.\n\nMany oscilloscopes today provide one or more external interfaces to allow remote instrument control by external software. These interfaces (or buses) include GPIB, Ethernet, serial port, and USB.\n\nThe following section is a brief summary of various types and models available. For a detailed discussion, refer to the other article.\n\nThe earliest and simplest type of oscilloscope consisted of a cathode ray tube, a vertical amplifier, a timebase, a horizontal amplifier and a power supply. These are now called \"analog\" scopes to distinguish them from the \"digital\" scopes that became common in the 1990s and 2000s.\n\nAnalog scopes do not necessarily include a calibrated reference grid for size measurement of waves, and they may not display waves in the traditional sense of a line segment sweeping from left to right. Instead, they could be used for signal analysis by feeding a reference signal into one axis and the signal to measure into the other axis. For an oscillating reference and measurement signal, this results in a complex looping pattern referred to as a Lissajous curve. The shape of the curve can be interpreted to identify properties of the measurement signal in relation to the reference signal, and is useful across a wide range of oscillation frequencies.\n\nThe dual-beam analog oscilloscope can display two signals simultaneously. A special dual-beam CRT generates and deflects two separate beams. Although multi-trace analog oscilloscopes can simulate a dual-beam display with chop and alternate sweeps, those features do not provide simultaneous displays. (Real time digital oscilloscopes offer the same benefits of a dual-beam oscilloscope, but they do not require a dual-beam display.)\nThe disadvantages of the dual trace oscilloscope are that it cannot switch quickly between the traces and it cannot capture two fast transient events. In order to avoid this problems a dual beam oscilloscope is used.\n\nTrace storage is an extra feature available on some analog scopes; they used direct-view storage CRTs. Storage allows the trace pattern that normally decays in a fraction of a second to remain on the screen for several minutes or longer. An electrical circuit can then be deliberately activated to store and erase the trace on the screen.\n\nWhile analog devices make use of continually varying voltages, digital devices employ binary numbers which correspond to samples of the voltage. In the case of digital oscilloscopes, an analog-to-digital converter (ADC) is used to change the measured voltages into digital information.\n\nThe digital storage oscilloscope, or DSO for short, is now the preferred type for most industrial applications, although simple analog CROs are still used by hobbyists. It replaces the electrostatic storage method used in analog storage scopes with digital memory, which can store data as long as required without degradation and with uniform brightness. It also allows complex processing of the signal by high-speed digital signal processing circuits.\n\nA standard DSO is limited to capturing signals with a bandwidth of less than half the sampling rate of the ADC (called the \"Nyquist limit\"). There is a variation of the DSO called the \"digital sampling oscilloscope\" that can exceed this limit for certain types of signal, such as high-speed communications signals, where the waveform consists of repeating pulses. This type of DSO deliberately samples at a much lower frequency than the Nyquist limit and then uses signal processing to reconstruct a composite view of a typical pulse. A similar technique, with analog rather than digital samples, was used before the digital era in \"analog sampling oscilloscopes.\"\n\nA digital phosphor oscilloscope (DPO) uses color information to convey information about a signal. It may, for example, display infrequent signal data in blue to make it stand out. In a conventional analog scope, such a rare trace may not be visible.\n\nA mixed-signal oscilloscope (or MSO) has two kinds of inputs, a small number of analog channels (typically two or four), and a larger number of digital channels(typically sixteen). It provides the ability to accurately time-correlate analog and digital channels, thus offering a distinct advantage over a separate oscilloscope and logic analyser. Typically, digital channels may be grouped and displayed as a bus with each bus value displayed at the bottom of the display in hex or binary. On most MSOs, the trigger can be set across both analog and digital channels.\n\nIn a mixed-domain oscilloscope (MDO) you have an additional RF input port that goes into a spectrum analyzer part. It links those traditionally separate instruments, so that you can e.g. time correlate events in the time domain (like a specific serial data package) with events happening in the frequency domain (like RF transmissions).\n\nHandheld oscilloscopes are useful for many test and field service applications. Today, a hand held oscilloscope is usually a digital sampling oscilloscope, using a liquid crystal display.\n\nMany hand-held and bench oscilloscopes have the ground reference voltage common to all input channels. If more than one measurement channel is used at the same time, all the input signals must have the same voltage reference, and the shared default reference is the \"earth\". If there is no differential preamplifier or external signal isolator, this traditional desktop oscilloscope is not suitable for floating measurements. (Occasionally an oscilloscope user will break the ground pin in the power supply cord of a bench-top oscilloscope in an attempt to isolate the signal common from the earth ground. This practice is unreliable since the entire stray capacitance of the instrument cabinet will be connected into the circuit. Since it is also a hazard to break a safety ground connection, instruction manuals strongly advise against this practice.)\n\nSome models of oscilloscope have isolated inputs, where the signal reference level terminals are not connected together. Each input channel can be used to make a \"floating\" measurement with an independent signal reference level. Measurements can be made without tying one side of the oscilloscope input to the circuit signal common or ground reference.\n\nThe isolation available is categorized as shown below:\n\nA new type of oscilloscope is emerging that consists of a specialized signal acquisition board (which can be an external USB or parallel port device, or an internal add-on PCI or ISA card). The user interface and signal processing software runs on the user's computer, rather than on an embedded computer as in the case of a conventional DSO.\n\nA large number of instruments used in a variety of technical fields are really oscilloscopes with inputs, calibration, controls, display calibration, etc., specialized and optimized for a particular application. Examples of such oscilloscope-based instruments include waveform monitors for analyzing video levels in television productions and medical devices such as vital function monitors and electrocardiogram and electroencephalogram instruments. In automobile repair, an ignition analyzer is used to show the spark waveforms for each cylinder. All of these are essentially oscilloscopes, performing the basic task of showing the changes in one or more input signals over time in an \"X\"‑\"Y\" display.\n\nOther instruments convert the results of their measurements to a repetitive electrical signal, and incorporate an oscilloscope as a display element. Such complex measurement systems include spectrum analyzers, transistor analyzers, and time domain reflectometers (TDRs). Unlike an oscilloscope, these instruments automatically generate stimulus or sweep a measurement parameter.\n\nThe Braun tube was known in 1897, and in 1899 Jonathan Zenneck equipped it with beam-forming plates and a magnetic field for sweeping the trace. Early cathode ray tubes had been applied experimentally to laboratory measurements as early as the 1920s, but suffered from poor stability of the vacuum and the cathode emitters. V. K. Zworykin described a permanently sealed, high-vacuum cathode ray tube with a thermionic emitter in 1931. This stable and reproducible component allowed General Radio to manufacture an oscilloscope that was usable outside a laboratory setting.\nAfter World War II surplus electronic parts became the basis of revival of Heathkit Corporation, and a $50 oscilloscope kit made from such parts was a first market success.\n\n\n\n"}
{"id": "652623", "url": "https://en.wikipedia.org/wiki?curid=652623", "title": "Otto Robert Frisch", "text": "Otto Robert Frisch\n\nOtto Robert Frisch FRS (1 October 1904 – 22 September 1979) was an Austrian physicist who worked on nuclear physics. With Lise Meitner he advanced the first theoretical explanation of nuclear fission (coining the term) and first experimentally detected the fission by-products. Later, with his collaborator Rudolf Peierls he designed the first theoretical mechanism for the detonation of an atomic bomb in 1940.\n\nFrisch was born in Vienna in 1904, the son of Justinian Frisch, a painter, and Auguste Meitner Frisch, a concert pianist. He himself was talented at both but also shared his aunt Lise Meitner's love of physics and commenced a period of study at the University of Vienna, graduating in 1926 with some work on the effect of the newly discovered electron on salts. \n\nAfter some years working in relatively obscure laboratories in Germany, Frisch obtained a position in Hamburg under the Nobel Prize-winning scientist Otto Stern. Here he produced work on the diffraction of atoms (using crystal surfaces) and also proved that the magnetic moment of the proton was much larger than had been previously supposed.\n\nThe accession of Adolf Hitler to the chancellorship of Germany in 1933 caused Otto Robert Frisch to make the decision to move to London, where he joined the staff at Birkbeck College and worked with the physicist Patrick Maynard Stuart Blackett on cloud chamber technology and artificial radioactivity. He followed this with a five-year stint in Copenhagen with Niels Bohr where he increasingly specialised in nuclear physics, particularly in neutron physics.\n\nDuring the Christmas holiday in 1938 he visited his aunt Lise Meitner in Kungälv. While there she received the news that Otto Hahn and Fritz Strassmann in Berlin had discovered that the collision of a neutron with a uranium nucleus produced the element barium as one of its byproducts. Hahn, in a letter to Meitner, called this new reaction a \"bursting\" of the uranium nucleus. Frisch and Meitner hypothesized that the uranium nucleus had split in two, explained the process, estimated the energy released, and Frisch coined the term fission to describe it. \n\nPolitical restraints of the Nazi era forced the team of Hahn and that of Frisch and Meitner (both of whom were Jewish) to publish separately. Hahn's paper described the experiment and the finding of the barium byproduct. Meitner's and Frisch's paper explained the physics behind the phenomenon. \n\nFrisch went back to Copenhagen, where he was quickly able to isolate the pieces produced by fission reactions. As Frisch himself later recalls, a fundamental idea of the direct experimental proof of the nuclear fission was suggested to him by George Placzek. Many feel that Meitner and Frisch deserved Nobel Prize recognition for their contributions to understanding fission.\n\nIn mid-1939 Frisch left Denmark for what he anticipated would be a short trip to Birmingham, but the outbreak of World War II precluded his return. With war on his mind, he and the physicist Rudolf Peierls produced the Frisch–Peierls memorandum at the University of Birmingham, which was the first document to set out a process by which an atomic explosion could be generated. Their process would use separated uranium-235, which would require a fairly small critical mass and could be made to achieve criticality using conventional explosives to create an immensely powerful detonation. The memorandum went on to predict the effects of such an explosion—from the initial blast to the resulting fallout. This memorandum was the basis of British work on building an atomic device (the Tube Alloys project) and also that of the Manhattan Project on which Frisch worked as part of the British delegation. Frisch and Rudolf Peierls worked together in the Physics Department at the University of Birmingham 1939–40. He went to America in 1943 having been hurriedly made a British citizen.\n\nIn 1944 at Los Alamos, one of Frisch's tasks as the leader of the Critical Assemblies group was to accurately determine the exact amount of enriched uranium which would be required to create the critical mass, the mass of uranium which would sustain a nuclear chain reaction. He did this by stacking several dozen 3 cm bars of enriched uranium hydride at a time and measuring rising neutron activity as the critical mass was approached. The hydrogen in the metal bars increased the time that the reaction required to accelerate. One day Frisch almost caused a runaway reaction by leaning over the stack, which he termed the \"Lady Godiva assembly\". His body reflected neutrons back into the stack. Out of the corner of his eye he saw that the red lamps that flickered intermittently when neutrons were being emitted, were 'glowing continuously'. Realizing what was happening, Frisch quickly scattered the bars with his hand. Later he calculated that the radiation dose was \"quite harmless\" but that if he \"had hesitated for another two seconds before removing the material ... the dose would have been fatal\". \"In two seconds he received, by the generous standards of the time, a full day's permissible dose of neutron radiation.\" In this way his experiments determined the exact masses of uranium required to fire the Little Boy bomb over Hiroshima.\n\nHe also designed the \"dragon's tail\" or \"guillotine\" experiment in which a uranium slug was dropped through a hole in larger fixed mass of uranium, reaching just above critical mass (0.1%) for a fraction of a second. At the meeting to approve the experiment, Richard Feynman, commenting on the transient danger involved, said it was \"just like tickling the tail of a sleeping dragon.\" In the period of about 3 milliseconds, the temperature rose at a rate of 2000 °C per sec and over 10 excess neutrons were emitted.\n\nIn 1946 he returned to England to take up the post of head of the nuclear physics division of the Atomic Energy Research Establishment at Harwell, though he also spent much of the next thirty years teaching at Cambridge where he was Jacksonian Professor of Natural Philosophy and a fellow of Trinity College.\n\nBefore he retired he designed a device, SWEEPNIK, that used a laser and computer to measure tracks in bubble chambers. Seeing that this had wider applications, he helped found a company, Laser-Scan Limited, now known as 1Spatial, to exploit the idea.\n\nHe retired from the chair in 1972 as required by University regulations. He died on 22 September 1979 and was cremated on 5 October at Cambridge City Crematorium. His son, Tony Frisch, is also a physicist.\n\n\n"}
{"id": "57406245", "url": "https://en.wikipedia.org/wiki?curid=57406245", "title": "Peptide receptor radionuclide therapy", "text": "Peptide receptor radionuclide therapy\n\nPeptide receptor radionuclide therapy (PRRT) is a type of unsealed source radiotherapy, using a radiopharmaceutical which targets peptide receptors to deliver localised treatment, typically for neuroendocrine tumours (NETs).\nA key advantage of PRRT over other methods of radiotherapy is the ability to target delivery of therapeutic radionuclides directly to the tumour or target site. This works because some tumours have an abundance (overexpression) of peptide receptors, compared to normal tissue. A radioactive substance can be combined with a relevant peptide (or its analogue) so that it preferentially binds to the tumour. With a gamma emitter as the radionuclide, the technique can be used for imaging with a gamma camera or PET scanner to locate tumours. When paired with alpha or beta emitters, therapy can be achieved, as in PRRT.\n\nThe current generation of PRRT targets somatostatin receptors, with a range of analogue materials such as octreotide and other DOTA compounds. These are combined with indium-111, lutetium-177 or yttrium-90 for treatment. In is primarily used for imaging alone, however in addition to its gamma emmission there are also auger electrons emitted, which can have a therapeutic effect in high doses.\nPRRT radiopharmaceuticals are constructed with three components; the radionuclide, chelator, and somatostatin analogue (peptide). The radionuclide delivers the actual therapeutic effect, (or photons for imaging). The chelator is the essential link between the radionuclide and peptide. For Lu and Y this is typically DOTA (tetracarboxylic acid, and its variants) and DTPA (pentetic acid) for In. Other chelators known as NOTA (triazacyclononane triacetic acid) and HYNIC (hydrazinonicotinamide) have also been experimented with, albeit more for imaging applications. The somatostatin analogue affects biodistribution of the radionuclide, and therefore how effectively any treatment effect can be targeted. Changes affect which somatostatin receptor is most strongly targeted. For example, DOTA-lanreotide (DOTALAN) has a lower affinity for receptor 2 and a higher affinity for receptor 5 compared to DOTA-octreotide (DOTATOC).\n\nThe body of research on the effectiveness of current PRRT is promising, but limited. Complete or partial treatment response has been seen in 20-30% of patients in trials treated with Lu-DOTATATE or Y-DOTATOC, the most widely used PRRT drugs. When it comes to comparing these two PRRT, Y-labeled and Lu-labeled PRRTs, it appears that Y-labeled is more effective for larger tumors, while Lu-labeled is better for smaller and primary tumors. The lack of ɤ-emission with Y-labeled PPRTs is also an important difference between Lu peptides and Y peptide. In particular, with Y-labeled PRRT it becomes difficult to set up a dose of radiations specific to the patient's needs. In most cases PRRT is used for cancers of the gastroenteropancreatic and bronchial tracts, and in some cases phaeochromocytoma, paraganglioma, neuroblastoma or medullary thyroid carcinoma. Various approaches to approve effectiveness and limit side effects are being investigated, including radiosensitising drugs, fractionation regimes and new radionuclides. Alpha emitters, which have much shorter ranges in tissue (limiting the effect on nearby healthy tissue), such as bismuth-213 or actinium-225 labeled DOTATOC are of particular interest.\n\nTherapeutic PRRT treatments typically involve several gigabecquerels (GBq) of activity. Several radiopharmaceuticals allow simultaneous imaging and therapy, enabling precise dosimetric estimates to be made. For example, the bremsstrahlung emission from Y and gamma emissions from Lu can be detected by a gamma camera. In other cases, imaging can be performed by labelling a suitable radionuclide to the same peptide as used for therapy. Radionuclides that can be used for imaging include gallium-68, technetium-99m and fluorine-18.\n\nCurrently used peptides can result in high kidney doses, as the radiopharmaceutical is retained for relatively long periods. Renal protection is therefore used in some cases, taking the form of alternative substances that reduce the uptake of the kidneys.\n\nPRRT is not yet widely available, with various radiopharmaceuticals at different stages of clinical trials. The cost of small volume production of the relevant radionuclides is high. The cost of Lutathera, a commercial Lu-DOTATATE product, has been quoted by the manufacturer as £71,500 (€80,000 or $94,000 in July 2018) for 4 administrations of 7.4 GBq.\n\nLu-DOTATATE (international nonproprietary name: lutetium (lu) oxodotreotide) was approved by the FDA in early 2018, for treatment of gastroenteropancreatic neuroendocrine tumors (GEP-NETs).\n\nMarketing authorisation for Lu-DOTATATE was granted by the European Medicines Agency on 26 September 2017. Y-DOTATOC (international nonproprietary name: yttrium (Y) edotreotide) is designated as an orphan drug but has not yet received marketing authorisation.\n\nIn guidance published in August 2018, Lutetium (177Lu) oxodotreotide was recommended by NICE for treating unresectable or metastatic neuroendocrine tumours.\n\n"}
{"id": "168313", "url": "https://en.wikipedia.org/wiki?curid=168313", "title": "Pictogram", "text": "Pictogram\n\nA pictogram, also called a pictogramme, pictograph, or simply picto, and in computer usage an icon, is an ideogram that conveys its meaning through its pictorial resemblance to a physical object. Pictographs are often used in writing and graphic systems in which the characters are to a considerable extent pictorial in appearance. A pictogram may also be used in subjects such as leisure, tourism, and geography.\n\nPictography is a form of writing which uses representational, pictorial drawings, similarly to cuneiform and, to some extent, hieroglyphic writing, which also uses drawings as phonetic letters or determinative rhymes. Some pictograms, such as Hazards pictograms, are elements of formal languages.\n\nPictograph has a rather different meaning in the field of prehistoric art, including recent art by traditional societies and then means art painted on rock surfaces, as opposed to petroglyphs; the latter are carved or incised. Such images may or may not be considered pictograms in the general sense.\n\nEarly written symbols were based on pictographs (pictures which resemble what they signify) and ideograms (symbols which represent ideas). Ancient Sumerian, Egyptian, and Chinese civilizations began to adapt such symbols to represent concepts, developing them into logographic writing systems. Pictographs are still in use as the main medium of written communication in some non-literate cultures in Africa, the Americas, and Oceania. Pictographs are often used as simple, pictorial, representational symbols by most contemporary cultures.\n\nPictographs can be considered an art form, or can be considered a written language and are designated as such in Pre-Columbian art, Native American art, Ancient Mesopotamia and Painting in the Americas before Colonization. One example of many is the Rock art of the Chumash people, part of the .\nIn 2011, UNESCO's World Heritage List added \"Petroglyph Complexes of the Mongolian Altai, Mongolia\" to celebrate the importance of the pictograms engraved in rocks.\n\nSome scientists in the field of neuropsychiatry and neuropsychology, such as Prof. Dr. Mario Christian Meyer, are studying the symbolic meaning of indigenous pictograms and petroglyphs, aiming to create new ways of communication between native people and modern scientists to safeguard and valorize their cultural diversity.\n\nAn early modern example of the extensive use of pictographs may be seen in the map in the London suburban timetables of the London and North Eastern Railway, 1936-1947, designed by George Dow, in which a variety of pictographs was used to indicate facilities available at or near each station. Pictographs remain in common use today, serving as pictorial, representational signs, instructions, or statistical diagrams. Because of their graphical nature and fairly realistic style, they are widely used to indicate public toilets, or places such as airports and train stations. Because they are a concise way to communicate a concept to people who speak many different languages, pictograms are also used extensively at the Olympics, and are redesigned for each set of games.\n\nPictographic writing as a modernist poetic technique is credited to Ezra Pound, though French surrealists accurately credit the Pacific Northwest American Indians of Alaska who introduced writing, via totem poles, to North America.\n\nContemporary artist Xu Bing created Book from the Ground, a universal language made up of pictograms collected from around the world. A Book from the Ground chat program has been exhibited in museums and galleries internationally.\n\nPictograms are used in many areas of modern life for commodity purposes, often as a formal language (see the In mathematics section).\n\nIn statistics, pictograms are charts in which icons represent numbers to make it more interesting and easier to understand. A key is often included to indicate what each icon represents. All icons must be of the same size, but a fraction of an icon can be used to show the respective fraction of that amount.\n\nFor example, the following table:\n\ncan be graphed as follows:\n\nKey: = 10 letters\n\nAs the values are rounded to the nearest 5 letters, the second icon on Tuesday is the left half of the original.\n\nPictographs can often transcend languages in that they can communicate to speakers of a number of tongues and language families equally effectively, even if the languages and cultures are completely different. This is why road signs and similar pictographic material are often applied as global standards expected to be understood by nearly all.\n\nA standard set of pictographs was defined in the international standard \"ISO 7001: Public Information Symbols\". Other common sets of pictographs are the laundry symbols used on clothing tags and the chemical hazard symbols as standardized by the GHS system.\n\nPictograms have been popularized in use on the web and in software, better known as \"icons\" displayed on a computer screen in order to help user navigate a computer system or mobile device.\n\n"}
{"id": "1919071", "url": "https://en.wikipedia.org/wiki?curid=1919071", "title": "Porringer", "text": "Porringer\n\nA porringer is a shallow bowl, between 4 and 6 inches (100 to 150mm) in diameter, and 1½\" to 3\" (40 to 80mm) deep; the form originated in the medieval period in Europe and was made in wood, ceramic, pewter and silver. They had flat, horizontal handles. Colonial porringers tended to have one handle, whereas European ones tended to have two handles on opposite sides, on which the owner's initials were sometimes engraved, and they occasionally came with a lid. Porringers resembled the smaller quaich, a Scottish drinking vessel.\n\nOne can discern authentic pewter porringers in much the same way that silver can be authenticated from the touch marks that were stamped either into the bowl of the porringer or on its base. Wooden porringers are occasionally found from excavations; e.g. 16th-century example from Southwark and 11th century from Winchester.\n\nThe most famous colonial porringers are probably those made by Paul Revere.\n\nIn more modern times, some manufacturers of porringers have produced them without handles. These types of porringers appear to be deep bowls, with the sides being nearly totally flat. Porringers are also used less and less, as a bowl will suffice for most people; porringers, however, are still circulated, mainly as a Christening-gift.\n\nA second, modern usage, for the term porringer is a double saucepan similar to a bain-marie used for cooking porridge. The porridge is cooked gently in the inner saucepan, heated by steam from boiling water in the outer saucepan. This ensures the porridge does not burn and allows a longer cooking time so that the oats can absorb the water or milk in which they are cooked more completely. Also the porridge does not need stirring during the cooking process, which means the oats maintain their structural integrity and the porridge has a better mouthfeel and texture. The lower heat may also degrade less of the beta-glucan in the oats, which gives oats their cholesterol-lowering properties.\n\nPorringers were also made out of red earthenware clay in a type of pottery that is called \"redware\" today but called \"earthen\" during colonial and Early America. These would have the typical strap or pulled handle that is familiar on mugs and cups today. \n\nSome collectors or materials historians also call what resembles the pewter porringer a \"bleeding cup\".\n\n\n"}
{"id": "769984", "url": "https://en.wikipedia.org/wiki?curid=769984", "title": "Project MKNAOMI", "text": "Project MKNAOMI\n\nMKNAOMI was the code name for a joint Department of Defense/CIA research program lasting from the 1950s through the 1970s. Unclassified information about the MKNAOMI program and the related Special Operations Division is scarce. It is generally reported to be a successor to the MKULTRA project and to have focused on biological projects including biological warfare agents—specifically, to store materials that could either incapacitate or kill a test subject and to develop devices for the diffusion of such materials.\n\nDuring the first twenty years of its establishment, the CIA engaged in various projects designed to increase U.S. biological and chemical warfare capabilities. Project MKNAOMI was initiated to provide the CIA with a covert support base to meet its top-secret operational requirements. The purpose was to establish a robust arsenal within the CIA's Technical Services Division (TSD) of various lethal and incapacitating materials. This would enable the TSD to serve as a highly maintained center for the circulation of biological and chemical materials.\n\nSurveillance, testing, upgrading, and the evaluation of special materials and items were also provided by MKNAOMI so as to ensure that no defects and unwanted contingencies emerged during operational conditions. For these purposes the U.S. Army's Special Operations Command (SOC) was assigned to assist the CIA in the development, testing, and maintenance procedures for the biological agents and delivery systems (1952). Both the CIA and SOC also modified guns that fired special darts coated with biological agents and various poisonous pills. The darts would incapacitate guard dogs, infiltrate the area that the dogs were guarding, and then awaken the dogs upon exiting the facility. In addition, the SOC was designated to research the potentials for using biological agents against other animals and crops.\n\nA 1967 CIA memo which was uncovered by the Church Committee was evidence of at least three covert techniques for attacking and poisoning crops that had been examined under field conditions. On November 25, 1969, President Richard Nixon abolished any military practice involving biological weapons and Project MKNAOMI was dissolved. On February 14, 1970, a presidential order was given to outlaw all stockpiles of bacteriological weapons and nonliving toxins. However, despite this presidential order, a CIA scientist was able to acquire an estimated 11 grams of deadly shellfish toxin from SOC personnel at Fort Detrick. The toxin was then stored in a CIA laboratory where it remained undetected for over five years.\n\n\n"}
{"id": "1373098", "url": "https://en.wikipedia.org/wiki?curid=1373098", "title": "Robert Scoble", "text": "Robert Scoble\n\nRobert Scoble (born January 18, 1965) is an American blogger, technical evangelist, and author. Scoble is best known for his blog, \"Scobleizer\", which came to prominence during his tenure as a technology evangelist at Microsoft. He later worked for Fast Company as a video blogger, and then Rackspace and the Rackspace-sponsored community site \"Building 43\" promoting breakthrough technology and startups. \n\nScoble was born in New Jersey in 1965, and grew up about a kilometer from Apple Computer’s head office in Silicon Valley.\n\nIn 1993 he dropped out without finishing his degree in journalism from San Jose State University’s School of Journalism and Mass Communications.\n\nIn June 2003 Scoble accepted a position at Microsoft. \"The Economist\" described Scoble’s influence in its February 15, 2005 edition:\nOn June 10, 2006, Scoble announced he was leaving Microsoft to join Podtech.net as vice president of media development with a higher salary accompanied by \"a quite aggressive stock option\" offer that would have made him wealthy if his new company had succeeded. According to Alexa Internet that day had the biggest traffic to his blog and PodTech over their lifetime. June 28, 2006 was his last day at Microsoft.\n\nOn December 11, 2007, while taking part in a panel discussion at the LeWeb3 Conference, Scoble inadvertently leaked news (by loading up a post on TechCrunch) that he would be leaving PodTech on January 14, 2008, and was likely to join Fast Company. He acknowledged the news on his blog on December 12 but stated that he had not yet signed on with Fast Company. He did a video interview about his plans and leased studio space from Revision3.\n\nOn March 3, 2008, Scoble launched FastCompany.tv with two shows: FastCompany Live and ScobleizerTV. He characterizes the first as \"a show done totally on cell phones.\" The second is similar to his previous show on PodTech, only with better equipment and a camera operator. The show is recorded with two cameras in 720p HD.\n\nOn March 14, 2009, Scoble announced via his blog and on the Gillmor Gang that he was joining Rackspace. As part of his work there, he teamed up with the company to develop Building 43, a new content and social networking website aimed to help grow new startups and promote groundbreaking technology. In 2012, Building 43 was re-branded as \"Small Teams, Big Impact\". Scoble’s mission remains to find and report bleeding edge technology.\n\nScoble left Rackspace to join UploadVR in 2016 as an entrepreneur in residence. \n\nOn October 20, 2017, the news outlet BuzzFeed published a story that alleges that in 2010, Scoble sexually harassed Michelle Greer, his Rackspace coworker, and Quinn Norton, a technology journalist, whose account was corroborated by multiple witnesses. Scoble apologized after the BuzzFeed article was published, saying that he has been working towards making amends ever since becoming sober two years ago. However, several women countered this claim, reporting that he made inappropriate advances during the time period he claimed to be sober.. Days later, he deleted his apology, and proclaimed his innocence in a blog post that also announced his new company, LightPitch.\n\nOne immediate reaction was by the VR/AR Association (VRARA), stating: \"In light of recent news and allegations of sexual harassment by Robert Scoble, the VR/AR Association has formally removed Robert from our Board of Advisors. Our organization does not condone harassment of any kind, and feel that this is the best course of action.\"\n\nScoble resigned from the Transformation Group, his AR consulting firm, which he co-founded in late 2016.\n\nIn November, 2013, Scoble was co-keynote speaker with Shel Israel at the 2013 Telstra Australian Digital Summit. Scoble and Israel talked to their book titled \"Age of Context: Mobile, Sensors, Data and the Future of Privacy\".\n\nOn April 1, 2008, \"The Register\" ran an April fool’s spoof claiming Robert Scoble was actually an IBM bot.\n\nOn November 14, 2007, he was a contestant on a game show at NewTeeVee Live featuring other internet celebrities such as Veronica Belmont, Casey McKinnon, Cali Lewis, Kevin Rose, Justin Kan, and others.\n\nOn November 6, 2006, Scoble appeared as a panelist on a Chinese Software Professionals Association event called \"The New Age of Influence: The Impact of Social Computing on Media and Marketing\".\n\nIn September 2008, Follow cost, a website which calculated how annoying it would be to follow anyone on Twitter, invented the milliscoble unit of measurement defined as: \"1/1000 of the average daily Twitter status updates by Robert Scoble as of 10:09 CST September 25, 2008.\" At that time, Scoble was averaging 21.21 tweets per day, so a milliscoble is 0.02121 tweets per day. A person with a milliscoble rating of 1000 will be as annoying to follow as Scoble.\n\nHe is married to Maryam Ghaemmaghami Scoble. Although he considers himself an agnostic, he converted to Islam at the time of the marriage. He has three sons, one with autism.\n\n\n"}
{"id": "458260", "url": "https://en.wikipedia.org/wiki?curid=458260", "title": "Sellotape", "text": "Sellotape\n\nSellotape () is a British brand of transparent, polypropylene-based, pressure-sensitive tape, and is the leading brand in the United Kingdom. Sellotape is generally used for joining, sealing, attaching and mending.\n\nThe term has become a genericised trademark in Britain and a number of other countries where it is sold. The term is also used much in the same way that Scotch Tape came to be used in Canada and the United States in referring to any brand of clear adhesive tape.\n\nThe name Sellotape was derived from \"cellophane\", at that time a trademarked name, with the \"C\" changed to \"S\" so that the new name could be trademarked.\n\nThe tape was originally manufactured in 1937 by Colin Kinninmonth and George Grey, in Acton, West London. From the 1960s to 1980s, the Sellotape company was part of Dickinson Robinson Group, a British packaging and paper conglomerate. In the 1960s a small group of chemists (based in Wallington), led by a man named Alan Robinson, undertook a development of the stickiness in this product. In 2002, the company was bought by Henkel Consumer Adhesives.\n\nSellotape Industrial was bought by Scapa Group plc in 1997, and their products continue to be manufactured at its factory in Dunstable, Bedfordshire.\n\nThe Sellotape brand now covers a variety of tape products, and the word is frequently used to refer to other adhesive tapes in many countries due to its market exposure. As an example of a genericized trademark, it has an entry in the Oxford English Dictionary.\n\nThe tape can be used to repair tears in paper, or to attach pieces of paper or cardboard together for modelling. On fragile paper surfaces the tape can only be used once, as removing it will either tear the paper or remove the top layer of rough cardboard; on smooth painted surfaces it can generally be removed without leaving any trace, though sometimes the adhesive can remain on the surface. It does not affix items to such surfaces permanently.\n\n\n"}
{"id": "2623096", "url": "https://en.wikipedia.org/wiki?curid=2623096", "title": "Spin valve", "text": "Spin valve\n\nA spin valve is a device, consisting of two or more conducting magnetic materials, whose electrical resistance can change between two values depending on the relative alignment of the magnetization in the layers. The resistance change is a result of the giant magnetoresistive effect. The magnetic layers of the device align \"up\" or \"down\" depending on an external magnetic field. In the simplest case, a spin valve consists of a non-magnetic material sandwiched between two ferromagnets, one of which is fixed (pinned) by an antiferromagnet which acts to raise its magnetic coercivity and behaves as a \"hard\" layer, while the other is free (unpinned) and behaves as a \"soft\" layer. Due to the difference in coercivity, the soft layer changes polarity at lower applied magnetic field strength than the hard one. Upon application of a magnetic field of appropriate strength, the soft layer switches polarity, producing two distinct states: a parallel, low-resistance state, and an antiparallel, high-resistance state.\n\nSpin valves work because of a quantum property of electrons (and other particles) called spin. Due to a split in the density of states of electrons at the Fermi energy in ferromagnets, there is a net spin polarisation. An electric current passing through a ferromagnet therefore carries both charge and a spin component. In comparison, a normal metal has an equal number of electrons with up and down spins so, in equilibrium situations, such materials can sustain a charge current with a zero net spin component. However, by passing a current from a ferromagnet into a normal metal it is possible for spin to be transferred. A normal metal can thus transfer spin between separate ferromagnets, subject to a long enough spin diffusion length.\n\nSpin transmission depends on the alignment of magnetic moments in the ferromagnets. If a current is passing into a ferromagnet whose majority spin is spin up, for example, then electrons with spin up will pass through relatively unhindered, while electrons with spin down will either 'reflect' or spin flip scatter to spin up upon encountering the ferromagnet to find an empty energy state in the new material. Thus if both the fixed and free layers are polarised in the same direction, the device has relatively low electrical resistance, whereas if the applied magnetic field is reversed and the free layer's polarity also reverses, then the device has a higher resistance due to the extra energy required for spin flip scattering.\n\nAn antiferromagnetic layer is required to pin one of the ferromagnetic layers (i.e., make it fixed or magnetically hard). This results from a large negative exchange coupling energy between ferromagnets and antiferromagnets in contact.\n\nThe non-magnetic layer is required to decouple the two ferromagnetic layers so that at least one of them remains free (magnetically soft).\n\nThe basic operating principles of a pseudo spin valve are identical to that of an ordinary spin valve, but instead of changing the magnetic coercivity of the different ferromagnetic layers by pinning one with an antiferromagnetic layer, the two layers are made of different ferromagnets with different coercivities e.g., NiFe and Co. Note that coercivities are largely an extrinsic property of materials and thus determined by processing conditions.\n\nSpin valves are used in magnetic sensors and hard disk read heads. They are also used in magnetic random access memories (MRAM).\n\n"}
{"id": "1657892", "url": "https://en.wikipedia.org/wiki?curid=1657892", "title": "Tannerite", "text": "Tannerite\n\nTannerite is a brand of binary explosive targets used for firearms practice and sold in kit form. The targets comprise a combination of oxidizers and a fuel – primarily aluminum powder – that is supplied as two separate components that are mixed by the user. The combination is relatively stable when subjected to forces less severe than a high-velocity bullet impact. A hammer blow, being dropped, or impact from a low-velocity bullet or shotgun blast will not initiate a reaction. It is also designed to be non-flammable (the reaction cannot be triggered by a burning fuse or electricity), although its explosion can cause other flammable material to ignite.\n\nBecause it is sold as two separate components, it can be transported and sold in many places without the legal restrictions that would otherwise apply to explosives. The term \"tannerite\" is often used to refer to the mixture itself, and other reactive targets and combination explosives are often generically referred to as tannerite.\n\nTannerite brand targets explode when shot by a high-velocity bullet. Low-velocity bullets and shotgun ammunition will not initiate a reaction.\n\nThe explosive reaction, once initiated, occurs at a very high velocity, producing a large vapor cloud and a loud report. It is marketed as a target designation that is useful for long-range target practice: the shooter does not need to walk down-range to see if the target has been hit, as the target will react and serve as a highly visible indicator.\n\nBinary explosives like Tannerite are also used in some business applications, including commercial blasting, product testing, and special effects.\n\nFor safety reasons, Tannerite Sports recommends using no more than of the mixed composition at once, and will sell its largest targets with a size of to professionals only.\n\nTannerite targets are sold in pre-sized quantities. The package includes a first, target container and a second container. An oxidizer composition is contained within one of the containers and a catalyst composition is contained within the other container. \n\nThe product, developed by Daniel Jeremy Tanner, and initially formulated in 1996, consists of two components: a fuel mixed with a catalyst or sensitizer, and a bulk material or oxidizer. The fuel/catalyst mixture is 90% 600-mesh dark flake aluminum powder, combined with the catalyst that is a mixture of 5% 325-mesh titanium sponge and 5% 200-mesh zirconium hydride (with another patent document listing 5% zirconium hydroxide). The oxidizer is a mixture of 85% 200-mesh ammonium nitrate and 15% ammonium perchlorate. The patents on these formulations were applied for on August 20, 2001.\n\nIn the United States, the Bureau of Alcohol, Tobacco, Firearms and Explosives advises: \"Persons manufacturing explosives for their own personal, non-business use only (e.g., personal target practice) are not required to have a Federal explosives license or permit.\" However, \"persons falling into certain categories are prohibited from possessing explosive materials\". Those prohibited from possessing explosives include most non-citizens, unlawful drug users and addicts, those convicted or indicted for serious crimes, fugitives, and those who have been officially declared mentally defective or have been committed to a mental institution. Restrictions imposed at the state and local level also apply. In California in particular, a permit may be required to use or possess the product.\n\nVarious regulations also govern the storage of unmixed explosives. As oxidizers and combustibles, the unmixed components still have some shipping restrictions in the United States.\n\nA Minnesota man was fined $2,583 and sentenced to three years' probation on charges of detonating an explosive device and unlawful possession of components for explosives after he detonated of Tannerite inside the bed of a dump truck by shooting it with a rifle chambered in .50 BMG from away on January 14, 2008, in Red Wing, Minnesota. The man was on probation when he mixed and shot the Tannerite and was not allowed to possess firearms or explosives. \n\nA 20-year-old man in Busti, New York, shot of Tannerite on January 13, 2013, that sent a particularly \"loud boom\" through much of southern Chautauqua County, New York, and extending as far south as Pennsylvania, at least 3 miles away. Multiple other sounds of explosions were also reported in the incident. The explosive noise caused numerous phone calls to the Chautauqua County Sheriff's Office, the New York State Police, and other law enforcement in the area.\n\nThe September 2016 New York and New Jersey bombings involved improvised explosive devices that contained \"a compound similar to a commercial explosive known as Tannerite\", set off by a small charge of unstable hexamethylene triperoxide diamine, which served as a detonator for the highly stable ammonal-type secondary charge.\n\nOn April 23, 2017, Dennis Dickey, an off-duty U.S. Border Patrol agent, shot a Tannerite target in a gender reveal celebration on state trust land south of Tucson, Arizona, which accidentally ignited the nearby dry brush and started a fire known as the Sawmill Fire. At the time, winds were gusting up to per hour and the National Weather Service had issued a fire watch in the area. By the time the wildfire was mostly contained one week later, it had jumped over the Santa Rita Mountains and crossed Arizona Highway 83, spreading into the historic Empire Ranch and the surrounding Las Cienegas National Conservation Area. The estimated damage caused by the blaze was $8.19 million. Dickey pleaded guilty in September 2018 to a misdemeanor violation of U.S. Forest Service regulations and was sentenced to five years' probation. He also was ordered to pay restitution, with an initial payment of $100,000 (taken from his retirement fund) and monthly payments of $500 per month thereafter for 20 years unless his income changes significantly. The payments will total $220,000 over the 20 years, after which the case will return to a judge to make a decision about future restitution. The eventual restitution payments could hypothetically be up to $8,188,069.\n"}
{"id": "43562149", "url": "https://en.wikipedia.org/wiki?curid=43562149", "title": "Telus Garden", "text": "Telus Garden\n\nTelus Garden is a 1,000,000 square foot office mixed-use redevelopment, located in Vancouver, BC, Canada. The two new buildings will incorporate office, retail and residential space. Of the 488,000 square feet of office space, approximately 212,000 square feet will be for Telus. \n\nIn August 2018, Telus and Westbank (the joint building owners) announced the building's sale to an unnamed party. Telus will continue to lease space in the building for its head office.\n\n\n\n\n"}
{"id": "2582573", "url": "https://en.wikipedia.org/wiki?curid=2582573", "title": "Tetrasulfur tetranitride", "text": "Tetrasulfur tetranitride\n\nTetrasulfur tetranitride is an inorganic compound with the formula SN. This gold-poppy coloured solid is the most important binary sulfur nitride, which are compounds that contain only the elements sulfur and nitrogen. It is a precursor to many S-N compounds and has attracted wide interest for its unusual structure and bonding.\n\nNitrogen and sulfur have similar electronegativities. When the properties of atoms are so highly similar, they often form extensive families of covalently bonded structures and compounds. Indeed, a large number of S-N and S-NH compounds are known with SN as their parent.\n\nSN adopts an unusual “extreme cradle” structure, with D point group symmetry. It can be viewed as a derivative of a hypothetical eight-membered ring of alternating sulfur and nitrogen atoms. The pairs of sulfur atoms across the ring are separated by 2.586 Å, resulting in a cage-like structure as determined by single crystal X-Ray diffraction. The nature of the \"transannular\" S–S interactions remains a matter of investigation because it is significantly shorter than the sum of the van der Waal's distances but has been explained in the context of molecular orbital theory. The bonding in SN is considered to be delocalized, which is indicated by the fact that the bond distances between neighboring sulfur and nitrogen atoms are nearly identical. SN has been shown to co-crystallize with benzene and the C molecule.\n\nSN is stable to air. It is, however, unstable in the thermodynamic sense with a positive enthalpy of formation of +460 kJ mol. This endothermic enthalpy of formation originates in the difference in energy of SN compared to its highly stable decomposition products:\nBecause one of its decomposition products is a gas, SN can be used as an explosive. Purer samples tend to be more explosive. Small samples can be detonated by striking with a hammer. SN is thermochromic, changing from pale yellow below −30 °C to orange at room temperature to deep red above 100 °C.\n\n was first prepared in 1835 by M. Gregory by the reaction of disulfur dichloride with ammonia, a process that has been optimized:\nCoproducts of this reaction include heptasulfur imide (SNH) and elemental sulfur. A related synthesis employs sulfur monochloride and NHCl instead:\n\nAn alternative synthesis entails the use of [(MeSi)N]S as a precursor with pre-formed S–N bonds. [(MeSi)N]S is prepared by the reaction of lithium bis(trimethylsilyl)amide and SCl.\n\nThe [((CH)Si)N]S reacts with the combination of SCl and SOCl to form SN, trimethylsilyl chloride, and sulfur dioxide:\n\nSN serves as a Lewis base by binding through nitrogen to strongly Lewis acidic compounds such as SbCl and SO. The cage is distorted in these adducts.\n\nThe reaction of [PtCl(PMePh)] with SN is reported to form a complex where a sulfur forms a dative bond to the metal. This compound upon standing is isomerised to a complex in which a nitrogen atom forms the additional bond to the metal centre.\n\nIt is protonated by HBF to form a tetrafluoroborate salt:\n\nThe soft Lewis acid CuCl forms a coordination polymer:\n\nDilute NaOH hydrolyzes SN as follows, yielding thiosulfate and trithionate:\n\nMore concentrated base yields sulfite:\n\nSN reacts with metal complexes. The cage remains intact in some cases but in other cases, it is degraded. SN reacts with Vaska's complex ([Ir(Cl)(CO)(PPh)] in an oxidative addition reaction to form a six coordinate iridium complex where the SN binds through two sulfur atoms and one nitrogen atom.\n\nMany S-N compounds are prepared from SN. Reaction with piperidine generates [SN]:\n\nA related cation is also known, i.e. [SN]. Treatment with tetramethylammonium azide produces the heterocycle [SN]:\n\nCyclo-[SN] has 10 pi-electrons: 2e/S plus 1e/N plus 1e for the negative charge.\n\nIn an apparently related reaction, the use of PPNN gives a salt containing the blue [NS] anion:\nThe anion NS has a chain structure described using the resonance [S=S=N–S–S] ↔ [S–S–N=S=S].\n\nSN reacts with electron-poor alkynes.\n\nChlorination of SN gives thiazyl chloride.\n\nPassing gaseous SN over silver metal yields the low temperature superconductor polythiazyl or polysulfurnitride (transition temperature (0.26±0.03) K), often simply called \"(SN)\". In the conversion, the silver first becomes sulfided, and the resulting AgS catalyzes the conversion of the SN into the four-membered ring SN, which readily polymerizes.\n\nThe selenium compound SeN is known and has been the subject of some research. In addition, adducts of aluminium chloride with SeN have been isolated; this is formed from SeN.\n\nSN is shock-sensitive. Purer samples are more shock-sensitive than those contaminated with elemental sulfur.\n"}
{"id": "695713", "url": "https://en.wikipedia.org/wiki?curid=695713", "title": "Toothpick", "text": "Toothpick\n\nA toothpick is a small thin stick of wood, plastic, bamboo, metal, bone or other substance with at least one and sometimes two sharp ends to insert between teeth to remove detritus, usually after a meal. Toothpicks are also used for festive occasions to hold or spear small appetizers (like cheese cubes or olives) or as a cocktail stick, and can be decorated with plastic frills or small paper umbrellas or flags.\n\nKnown in all cultures, the toothpick is not just the oldest instrument for dental cleaning, but predates the arrival of early modern humans, for the skulls of Neanderthals, as well as Homo sapiens, show clear signs of teeth picked with a tool. Toothpicks made of bronze have been found as burial objects in prehistoric graves in Northern Italy and in the East Alps. In 1986, researchers in Florida discovered the 7500-year-old remains of ancient Native Americans and discovered small grooves between many of the molar teeth. One of the researchers, Justin Martin of Concordia University Wisconsin, said \"The enamel on teeth is quite tough, so they must have used the probes quite rigorously to make the grooves.\"\n\nThere are delicate, artistic examples made of silver in antiquity, as well as from mastic wood with the Romans.\n\nIn the 17th century, toothpicks were luxury objects and like jewelry, were artfully stylized using precious metal and set with expensive stones.\n\nThe first toothpick-manufacturing machine was developed in 1869, by Marc Signorello. Another was patented in 1872, by Silas Noble and J. P. Cooley.\n\nAmerican wooden toothpicks are cut from birch wood. Logs are first spiral cut into thin sheets, which are then cut, chopped, milled and bleached (to lighten) into the individual toothpicks. Plastic toothpicks, also called dental pics, are still made in America in Georgia, by Armond's Manufacturing. The Mayo Clinic recommends using a dental pic in lieu of a wooden toothpick to clean one's teeth, as they clean more effectively and one does not risk injuring the gums.\nNowadays other means of dental hygiene are preferred such as dental floss and toothbrushes.\n\nIn September 2012, Ed Cahill set a world record in Ireland by placing 3,107 toothpicks in his beard in just under three hours.\n\n\n"}
{"id": "14318149", "url": "https://en.wikipedia.org/wiki?curid=14318149", "title": "Vertebral fixation", "text": "Vertebral fixation\n\nVertebral fixation (also known as \"spinal fixation\") is a orthopedic surgical procedure in which two or more vertebrae are anchored to each other through a synthetic \"vertebral fixation device\", with the aim of reducing vertebral mobility and thus avoiding possible damage to the spinal cord and/or spinal roots.\n\nA vertebral fixation procedure may be indicated in cases of vertebral fracture, vertebral deformity, or degenerative vertebral disorders (such as spondylolisthesis).\n\nThe device used to achieve vertebral fixation is usually a permanent rigid or semi-rigid prosthesis made of titanium; examples include rods, plates, screws, and various combinations thereof. A less common alternative is the use of a resorbable fixation device, composed of a bio-resorbable material.\n\nThe medical community uses several different techniques for stabilizing the posterior region of the spine. The most radical of these techniques is spinal fusion. In recent years/decades spinal surgeons have begun to rely more heavily on mechanical implants, which provide increased stability without so severely limiting the recipient’s range of motion. A number of devices have been developed that allow the recipients near natural range of motion while still providing some support. In many cases the support offered by such devices is insufficient, leaving the physician with few other choices than spinal fusion. \n\nA spinal fixation device stabilizes an area of the posterior spine while allowing for a significant range of motion and limiting the compression of the affected vertebrae. The device consists of two or more arm assemblies (lateral) connected by one or more telescopic assemblies (vertical). Each arm assembly is composed of a central portion, which connects to the telescopic assembly or assemblies. Left and right arms attach to the corresponding side of the central portion of the arm assembly. Each arm section is directly connected to its individual pedicle by means of pedicle fasteners. \n\nMore information about this specific spinal fixation device can be found in The United States Patent Service’s November 13, 2007 publication of new patents. This patent can currently (September 23, 2008) be found on The U.S. Patent Website.\n\n"}
{"id": "33421030", "url": "https://en.wikipedia.org/wiki?curid=33421030", "title": "Watap", "text": "Watap\n\nWatap, watape, wattap, or wadab ( or ) is the thread and cordage used by the Native Americans and First Nations peoples of Canada to sew together sheets and panels of birchbark. The word itself comes from the Algonquian language family, but watap cordage was used and sewn by all of the people who lived where the paper birch tree grows. The cordage was usually manufactured from the roots of various species of conifers, such as the white spruce, black spruce, or Northern whitecedar, but could originate from a variety of species that sprouted root fibers with sufficient tensile strength for the required purpose. In a typical manufacturing process, the roots would be debarked, subjected to a lengthy soaking process, and then steamed or boiled to render them pliable for sewing. The roots could be left whole and used as cords, or divided into smaller fibers for twine.\n\nSewn birchbark panels were employed by the native North Americans of the Upper Great Lakes for a wide variety of purposes; the best-known, and one which required among the highest degree of craftsmanship, was the manufacture of light canoes. Panels of bark, sewn together with watap and caulked with tree resin, could be used to create a vessel that would resist leakage to the point of being almost waterproof. The watap could also be used as part of the joinery for the structural elements of the canoe.\n\nWatap-sewn bark sheets and panels could also be used to make vessels and utensils for food storage and other household use. Examples of this packaging were called \"wiigwaasi-makakoon\", and the watap stitchery was often used as an element in the decoration and unique identity of the package. The peoples initiated into the heritage of the Midewiwin, or \"Great Medicine Society\", kept records and aids to memory through birchbark scrolls sewn together with watap.\nThe word \"watap\" entered European languages through Canadian French, and ultimately derives from the Cree word \"watapiy\". The wide use of the term in Cree indicates the importance of birchbark craft to that nation and the widespread presence of the paper birch and spruce in their historical homeland. The Anishinaabe peoples also extensively harvested and manufactured watap.\n\n\"Watap Lake\", also called \"Watape Lake\", and the adjacent \"Watap Portage\" are key elements in the Grand Portage from Lake Superior to the interior of North America. The lake and portage were of such historical importance that the boundary between Canada and the United States of America follows the length of the lake and portage today.\n\n\"Watab River\" in Stearns County, Minnesota, served as a boundary separating the Dakota peoples from the Ojibwa people per the 1825 First Treaty of Prairie du Chien.\n\n"}
