{"id": "8668103", "url": "https://en.wikipedia.org/wiki?curid=8668103", "title": "Animal transporter", "text": "Animal transporter\n\nAnimal transporters are used to transport livestock or non-livestock animals over long distances. They could be specially-modified vehicles, trailers, ships or aircraft containers. While some animal transporters like horse trailers only carry a few animals, modern ships engaged in live export can carry tens of thousands.\n\nThe Animal Transportation Association campaigns for humane transporting of animals as do many other animal welfare organisations.\n\n"}
{"id": "5920338", "url": "https://en.wikipedia.org/wiki?curid=5920338", "title": "Atomistix", "text": "Atomistix\n\nAtomistix A/S was a software company developing tools for atomic scale modelling. It was headquartered in Copenhagen, Denmark, with a subsidiary for Asia Pacific in Singapore and for the Americas in California. In September 2008 Atomistix A/S went bankrupt, but in December 2008 the newly founded company QuantumWise announced that they had acquired all assets from the Atomistix estate and would continue the development and marketing of the products Atomistix ToolKit and Atomistix Virtual NanoLab.\n\nThe company was founded in October 2003 by Dr. Kurt Stokbro, Dr. Jeremy Taylor and Dr. Thomas Magnussen. Dr. Stokbro and Dr. Taylor are co-authors on the article [1] introducing the electron transport method and program TranSIESTA (based on the SIESTA program [2]) for academic research. This method, and methods used in Dr. Taylor Ph.D. research, was the starting point for Atomistix first product, TranSIESTA-C. The C refers to the program being written in the C programming language as opposed to Fortran in which TranSIESTA was written. This code had been completely reengineered and further developed into the commercial products marketed by the company today.\nSince the very beginning the company had been working in close collaboration with the Nano-Science Center at the Niels Bohr Institute of Copenhagen University, to enhance the product development, and had instituted cooperations with leading nanotechnology centers, experts and private companies around the world.\n\nThe management team consisted of:\n\nAtomistix A/S provided the following products:\n\n\nLegacy Products:\n\nThese products represented a package of integrated software modules for quantum chemistry modelling, providing a user-friendly graphical interface interaction to complex computational methods.\n\nFrom the usability point of view, the setup of the computation is done through Atomistix Virtual NanoLab, a metaphoric interface, mimicking \"in silico\" the approach of an experiment in a real laboratory. The underlying computational engine is the Atomistix ToolKit (ATK) which is the next generation of TranSIESTA-C. ATK also has a Python based interface NanoLanguage and a text file interface.\n\nThe methods used in the software products are based primarily on density functional theory (DFT) and non-equilibrium Green's function (NEGF) techniques and also all the underlying quantum mechanics.\n\n\n\n"}
{"id": "197418", "url": "https://en.wikipedia.org/wiki?curid=197418", "title": "Autographic film", "text": "Autographic film\n\nThe autographic system for roll film was launched by Kodak in 1914, and allowed the photographer to add written information on the film at the time of exposure. \n\nThe system was patented by Henry Jacques Gaisman, inventor and safety razor manufacturer. George Eastman purchased the rights for US$300,000. It consisted of a tissue-like carbon paper sandwiched between the film and the paper backing. Text was entered using a metal stylus, and would appear in the margin of the processed print. The system was never very popular, and was discontinued in 1932.\n\nKodak's autographic films had \"A\" as the first part of the film size designation. Thus, standard 122 film would be labeled \"122\" and autographic 122 would be \"A122\". Autographic roll film sizes were A116, A118, A120, A122, A123, A126, A127, and A130. The autographic feature was marketed as having no extra charge. In 1915, Kodak also sold upgrade autographic backs for their existing cameras.\n\n\n"}
{"id": "26391839", "url": "https://en.wikipedia.org/wiki?curid=26391839", "title": "Bacon and Hams", "text": "Bacon and Hams\n\nBacon and Hams is a 1917 book by George J. Nicholls, a member of the Institute of Certificated Grocers. The book details the then-modern bacon and ham industry beginning with the use of the pig breeds, meat processing and the distribution and pricing of cuts with a focus on the United Kingdom. The meat processing aspects focus on the popular Wiltshire cut of the time, but also includes American cuts as well. The book was described, with approbation, by the Saskatchewan Overseas Livestock Marketing Commission, as an \"admirable and important treatise\". Despite having entered the public domain, the book is rare and collectible and generated interest for its \"unparalleled\" anatomical details of pigs found in its fold-out pages.\n\nA modern review called it a \"paean to the pig\".\n\nAccording to the title page of the book, George J. Nicholls was the director of George Bowles, Nicholls & Company. He was also a Trustee, Member of Council, Chairman of Finance Committee, and honorary Examiner to the Institute of Certificated Grocers. He was also a member of the Home and Foreign Produce Exchange and Chairman of Committee of the Wholesale Produce Merchants' Association in London. Nicholls followed in the footsteps of his father in the provision trade and hoped that two of his three sons would continue the family tradition.\nThe book features a studio photograph of its author in fancy dress as a side of bacon.\n\nThe term bacon in this book does not refer to the sliced and pre-packaged bacon that is ubiquitous in American markets, but instead the cured whole side of the pig. Furthermore, the differences between bacon in the United States and in the United Kingdom are quite pronounced, and the book refers only to cuts common in the United Kingdom. The cut that is used to produce \"American bacon\" is a cut referred to as \"streaky bacon\" in the United Kingdom. Most bacon consumed in the United Kingdom is known as back bacon and consists of both pork belly and pork loin in a single cut. The Wiltshire cut was a way to divide up the meat, and was marketed to consumers in still further cuts.\n\nThis book's target audience is not the common consumer of pig meat, but the grocer who would sell the products to the masses. The goal of the book was to aid in the proper handling, selling and profiting from the business of selling pork. Furthermore, Nicholls noted that this book would come to fill a need for the students who would be taking the Institute of Certificated Grocer exams.\n\nThe book is divided into nine chapters with seven appendices. The first chapter details the pig and its use in antiquity before summarizing the nutrient components of pork in comparison to beef. The second chapter opens with the defining characteristics of a good bacon pig. The six principal breeds of the United Kingdom at the time were the Large White Yorkshire, the Middle White, the Tamworth, the Berkshire, the Lincolnshire Curly-Coated and the Large Black. The Large White Yorkshire breed is traced to Robert Bakewell and highlights the importance of the breed for its quick maturity, rapid fattening and providing a long side. Though also credited for the breeding of the Small White pig, Bakewell was secretive in his work and evidence of his pig breeding (as a whole) cannot be confirmed. Nicholls describes the other breeds and provides information on the number of pigs, the state of the industry and the classification of pigs for the market.\n\nThe third chapter details the industry outside the United Kingdom. The fourth chapter discusses the current practices of the bacon factory, including the stages in which the pigs are received, killed, branded and processed. The usage of the entire carcass is covered, from the blood to the fat and hair of the pig. Chapter five details the distribution and wholesale centers of the industry and the terms and regulations used. Chapter six details the selection and grading of the cuts, beginning with the most popular Wiltshire cut. Chapter seven and eight details the retail distribution of the bacon, and dividing the Wiltshire cut into different cuts and pricing. Chapter nine concludes with the retail distribution of the American and Canadian cuts. The book includes fold-out anatomical charts that were popular during the time.\n\n\"Bacon and Hams\" was first published in 1917 by the Institute of Certificated Grocers and printed by Richard Clay & Sons of London. A second edition was published in 1924. The book was referred to with approbation by the Saskatchewan Overseas Livestock Marketing Commission which described it as an \"admirable and important treatise\". Though the book has entered the public domain, it is rare and collectible. The website \"Cooking Issues\" featured the obscure book, creating an interactive Adobe Flash animation of Nicholls's \"unparalleled fold-out pig\".\n"}
{"id": "892714", "url": "https://en.wikipedia.org/wiki?curid=892714", "title": "Blanket", "text": "Blanket\n\nA blanket is a piece of soft cloth large enough either to cover or to enfold a great portion of the user's body, usually when sleeping or otherwise at rest, thereby trapping radiant bodily heat that otherwise would be lost through convection, and so keeping the body warm. \n\nThe term arose from the generalization of a specific fabric called Blanket fabric, a heavily napped woolen weave pioneered by Thomas Blanket (Blanquette), a Flemish weaver who lived in Bristol, England in the 14th century.\nEarlier usage of the term is possible through its derivation from the French word for white, blanc.\n\nMany types of blanket material, such as wool, are used because they are thicker and have more substantial fabric to them, but cotton can also be used for light blankets. Wool blankets are warmer and also relatively slow to burn compared to cotton. The most common types of blankets are woven acrylic, knitted polyester, mink, cotton, fleece and wool. Blankets also come with exotic crafting and exotic material such as crocheted afghan or a silk covering. The term blanket is often interchanged with comforter, quilt, and duvet, as they all have similar uses.\n\nBlankets have been used by militaries for many centuries. Most militaries have blankets as compulsory for sleeping quarters in preference to duvets. Militaries are some of the biggest single consumers of woolen blankets. Military blankets tend to be coarse grey with a high level of microns, usually over 20. Suppliers include J. E. Ashworth & Sons and Faribault Woolen Mills who made half of all blankets in America at one time.\n\nThrow blankets are smaller blankets, often in decorative colors and patterns, that can be used for extra warmth and decoration on the outside of bed. Blankets are sometimes used as comfort objects by small children. \n\nBlankets may be spread on the ground for a picnic or where people want to sit in a grassy or muddy area without soiling their clothing. Temporary blankets have been designed for this purpose.\n"}
{"id": "685810", "url": "https://en.wikipedia.org/wiki?curid=685810", "title": "Boiling liquid expanding vapor explosion", "text": "Boiling liquid expanding vapor explosion\n\nA boiling liquid expanding vapor explosion (BLEVE, ) is an explosion caused by the rupture of a vessel containing a pressurized liquid that has reached temperatures above its boiling point.\n\nThere are three characteristics of liquids which are relevant to the discussion of a BLEVE:\n\n\nTypically, a BLEVE starts with a container of liquid which is held above its normal, atmospheric-pressure boiling temperature. Many substances normally stored as liquids, such as CO, propane, and other similar industrial gases have boiling temperatures, at atmospheric pressure, far below room temperature. In the case of water, a BLEVE could occur if a pressurized chamber of water is heated far beyond the standard . That container, because the boiling water pressurizes it, is capable of holding \"liquid\" water at very high temperatures.\n\nIf the pressurized vessel, containing liquid at high temperature (which may be room temperature, depending on the substance) ruptures, the pressure which prevents the liquid from boiling is lost. If the rupture is catastrophic, where the vessel is immediately incapable of holding any pressure at all, then there suddenly exists a large mass of liquid which is at very high temperature and very low pressure. This causes a portion of the liquid to \"instantaneously\" boil, which in turn causes an extremely rapid expansion. Depending on temperatures, pressures and the substance involved, that expansion may be so rapid that it can be classified as an explosion, fully capable of inflicting severe damage on its surroundings.\n\nFor example, a tank of pressurized liquid water held at might be pressurized to above atmospheric (\"gauge\") pressure. If the tank containing the water were to rupture, there would for a brief moment exist a volume of liquid water which would be at:\nAt atmospheric pressure the boiling point of water is - liquid water at atmospheric pressure does not exist at temperatures higher than . At that moment, the water would boil and turn to vapor explosively, and the liquid water turned to gas would take up significantly more volume (~1,600-fold) than it did as liquid, causing a vapor explosion. Such explosions can happen when the superheated water of a steam engine escapes through a crack in a boiler, causing a boiler explosion.\n\nA BLEVE need not be a chemical explosion—nor does there need to be a fire—however if a flammable substance is subject to a BLEVE it may also be subject to intense heating, either from an external source of heat which may have caused the vessel to rupture in the first place or from an internal source of localized heating such as skin friction. This heating can cause a flammable substance to ignite, adding a secondary explosion caused by the primary BLEVE. While blast effects of any BLEVE can be devastating, a flammable substance such as propane can add significantly to the danger.\nWhile the term BLEVE is most often used to describe the results of a container of flammable liquid rupturing due to fire, a BLEVE can occur even with a non-flammable substance such as water, liquid nitrogen, liquid helium or other refrigerants or cryogens, and therefore is not usually considered a type of chemical explosion. Note that in the case of liquefied gasses, BLEVEs can also be hazardous because of rapid cooling due to the absorption of the enthalpy of vaporization (e.g. frostbites), or because of possible asphyxiation if a large volume of gas is produced and not rapidly dispersed (e.g. inside a building, or in a trough in the case of heavier-than-air gasses), or because of the toxicity of the gasses produced. \n\nBLEVEs can be caused by an external fire near the storage vessel causing heating of the contents and pressure build-up. While tanks are often designed to withstand great pressure, constant heating can cause the metal to weaken and eventually fail. If the tank is being heated in an area where there is no liquid, it may rupture faster without the liquid to absorb the heat. Gas containers are usually equipped with relief valves that vent off excess pressure, but the tank can still fail if the pressure is not released quickly enough. Relief valves are sized to release pressure fast enough to prevent the pressure from increasing beyond the strength of the vessel, but not so fast as to be the cause of an explosion. An appropriately sized relief valve will allow the liquid inside to boil slowly, maintaining a constant pressure in the vessel until all the liquid has boiled and the vessel empties.\n\nIf the substance involved is flammable, it is likely that the resulting cloud of the substance will ignite after the BLEVE has occurred, forming a fireball and possibly a fuel-air explosion, also termed a vapor cloud explosion (VCE). If the materials are toxic, a large area will be contaminated.\n\nThe term \"BLEVE\" was coined by three researchers at Factory Mutual, in the analysis of an accident there in 1957 involving a chemical reactor vessel. Anthony Santos was one of the Factory Mutual chemical engineers who coined the term.\n\nOn August 18, 1959, the Kansas City Fire Department suffered its second largest loss of life in the line of duty, when a 25,000 gallon (95,000 liter) gasoline tank exploded during a fire on Southwest Boulevard, killing five firefighters. \n\nLater incidents have included:\n\n\nSome fire mitigation measures are listed under liquefied petroleum gas.\n\n\n"}
{"id": "7147618", "url": "https://en.wikipedia.org/wiki?curid=7147618", "title": "CALS Table Model", "text": "CALS Table Model\n\nThe CALS Table Model is a standard for representing tables in SGML/XML. It was developed as part of the CALS DOD initiative.\n\nThe CALS Table Model was developed by the CALS Industry Steering Group \"Electronic Publishing Committee\" (EPC).\n\nThe EPC subcommittee, of which Harvey Bingham was co-chair and a major contributor, designed the CALS Table Model in 1989-1990. The EPC was made up of industry and military service representatives. Some represented traditional military document printing agencies. Others represented electronic publishing organizations. SGML itself was new. At that time, the CALS intent for all their technical manuals was to use that DTD to achieve system-neutral interchange of content and structure.\n\nIts basis was a minimal description and example of a table from the prior Mil-M-38784B specification for producing technical manuals. The incomplete specification of the semantics associated with the table model allowed too much freedom for vendor interpretation, and resulted in problems with interchange. SGML-Open (now OASIS) surveyed the implementing vendors to identify differences, as the initial step toward reaching a common interpretation. The next step was an updated CALS Table Model DTD and semantics. Both are now available from \n\"OASIS\".\n\nAs implementations of the CALS Table Model were developed, a number of ambiguities and omissions were detected and reported to the EPC committee. The differences in interpretation had led to serious interoperability problems. To resolve these differences, \nOASIS identified a subset of the full CALS table model that had a high probability of successful interoperability among the OASIS vendor products. This subset is \nthe Exchange Table Model DTD.\n\n<table frame=\"none\">\n<tgroup cols=\"2\" colsep=\"0\">\n<colspec colnum=\"1\" colname=\"col1\" colwidth=\"32mm\"/>\n<colspec colnum=\"2\" colname=\"col2\" colwidth=\"132mm\"/>\n<thead>\n<row>\n<entry valign=\"top\"/>\n<entry valign=\"top\">(IUPAC) name</entry></row></thead>\n<tbody>\n<row rowsep=\"0\">\n<entry>pyro-EGTA</entry>\n<entry>2,2',2\",2\"'-(2,2'-(1,2-phenylene bis(oxy))bis(ethane-2,1-diyl)) bis(azanetriyl)tetraacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>EGTA</entry>\n<entry>ethylene glycol-bis(2-aminoethylether)-N,N,N',N'-tetraacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>EDTA</entry>\n<entry>2,2',2\",2\"'-(ethane-1,2-diyldinitrilo)tetraacetic acid (ethylenediamine tetraacetic acid)</entry></row>\n<row rowsep=\"0\">\n<entry>AATA</entry>\n<entry>2,2'-(2-(2-(2-(bis(carboxymethyl)amino)ethoxy)ethoxy) phenylazanediyl)diacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>APTRA</entry>\n<entry>2-carboxymethoxy-aniline-N,N-diacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>BAPTA</entry>\n<entry>1,2-bis(-2-aminophenoxy)ethane- N,N,N',N'-tetraacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>HIDA</entry>\n<entry>N-(2-hydroxyethyl)iminodiacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>Carboxyglutamate</entry>\n<entry>3-Aminopropane-1,1,3-tricarboxylic acid</entry></row></tbody></tgroup>\n</table>\nOASIS is the \"Organization for the Advancement of Structured Information Standards\", a global consortium that develops data representation standards for use in computer software.\n\n"}
{"id": "2270364", "url": "https://en.wikipedia.org/wiki?curid=2270364", "title": "Changing bag", "text": "Changing bag\n\nA changing bag is a photographic bag specifically designed to be light-proof while in use. It is required for certain applications involving photosensitive materials when a darkroom is not available, like in the field. Common usages include removing film from its canister to put it into a developing tank, or loading and unloading sheet film holders. They are also commonly found on the set of a film, where the clapper loader may need one if shooting on location or far away from a darkroom.\n\nIt is handy to use when a darkroom is not available as is often the case in field shooting. It is also used in commercial photo processing labs, often to change paper.\n\nA changing bag has two sleeves at one end for both the user's arms, and a zipper (often more than one, for double layered changing bags) to insert the tools and film needed. There are several sizes available, from smaller ones for many still photography applications to larger bags used in large-format still photography or film making, which may need to hold both a magazine and a can of film stock which each have a 1000-foot capacity. Larger changing bag sizes are also available as \"changing tents\", where the top of the bag can be held in a dome-like configuration through the use of two curved rods.\n"}
{"id": "22469701", "url": "https://en.wikipedia.org/wiki?curid=22469701", "title": "Chief Technology Officer of the United States", "text": "Chief Technology Officer of the United States\n\nThe United States Chief Technology Officer (US CTO), also formally an Assistant to the President, is in the Office of Science and Technology Policy. This position was created within the Office of Science and Technology Policy by President Barack Obama. The U.S. CTO helps the President and their team harness the power of data, innovation and technology on behalf of the American People. The team works closely with others both across and outside government on a broad range of work to upgrade government capability including using applied technology to help create jobs, creating paths to improve government services with lower costs, higher quality and increased transparency, helping upgrade agencies to use open data and expanding their data science capabilities, reduce the costs of health care and criminal justice, increase access to broadband, bring technical talent into government for policy and modern operations input, improve community innovation engagement by agencies working on local challenges, and help keep the nation secure. \n\nAneesh Chopra was named by President Obama as the nation's first CTO in April 2009, and confirmed by the Senate on August 7, 2009. Chopra resigned effective February 8, 2012 and was succeeded by Todd Park, formerly the CTO of the department of Health and Human Services. On September 4, 2014 Megan Smith was named as the CTO.\n"}
{"id": "3098243", "url": "https://en.wikipedia.org/wiki?curid=3098243", "title": "Clarence Hiskey", "text": "Clarence Hiskey\n\nClarence Francis Hiskey (1912–1998), born Clarence Szczechowski, was a Soviet espionage agent in the United States. He became active in the Communist Party USA (CPUSA) when he attended graduate school at the University of Wisconsin. He became a professor of chemistry at the University of Tennessee, Columbia University and Brooklyn Polytechnic Institute. For a time, Hiskey worked at the Tennessee Valley Authority and the University of Chicago Metallurgical Laboratory, part of the Manhattan Project. He was the father of Nicholas Sand.\n\nHiskey joined the Chicago Metallurgical Laboratory in September 1943. \nIn May 1944, a message sent by New York KGB to Moscow Venona project was intercepted and decrypted. The message contained information reporting that Bernard Schuster, member of the CPUSA secret apparatus, working for Soviet intelligence, had traveled to Chicago on the KGB's instructions. The message recorded Schuster's description of those he had come in contact with, including Rose Olsen, and stating Olsen had been meeting with Hiskey on the instructions of the organization. In July, it appears Joseph Katz had been assigned to the Hiskey case.\n\nOn 28 April 1944, Army counter-intelligence (G-2) observed a meeting between Clarence Hiskey and Soviet Military Intelligence (GRU) officer Arthur Adams. Hiskey was removed from the Manhattan Project by drafting him into the Army, and stationing him in Canada for the duration of the conflict. While en route, Army counter-intelligence officers secretly searched Hiskey's luggage and found seven pages of classified notes taken from the Chicago Metallurgical Lab. When the officers subsequently performed a follow up search, the notes were no longer with Hiskey.\n\nIn 1948, the House Un-American Activities Committee (HUAC) established that Hiskey was an active member of the CPUSA and had attempted to recruit other scientists to pass secret atomic data to Soviet intelligence. Congressional investigators concluded: \nIt became obvious that Hiskey had for some time been supplying Adams with secret information regarding atomic research. Immediately after seeing Adams, Hiskey flew to Cleveland, Ohio, where he contacted John Hitchcock Chapin. Chapin, through the urging of Clarence Hiskey, agreed to take over Hiskey's contacts with Adams.\n\nChapin admitted to investigators that Hiskey had told him that Adams was indeed a Soviet agent. Edward Manning was another Chicago Met Lab employee Hiskey attempted to recruit.\n\nIn testimony before HUAC and Senate Internal Security Subcommittee, Hiskey repeatedly refused to answer questions about his Communist associations and espionage, and in 1950, he was cited for contempt of Congress. Hiskey resigned his position as associate professor of analytical chemistry on the faculty of Brooklyn Polytechnic Institute and joined the International Biotechnical Corporation, later becoming director of analytical research for Endo Laboratories.\n\nIn June 1953, Hiskey was subpoenaed to testify before the Senate Subcommittee on Investigations. In a closed door session, Hiskey was interrogated by Sen. Joseph McCarthy:\n\nThen after some discussion of the Fifth Amendment,\n\nThe proceeding closed with,\n\nThe subcommittee did not call Hiskey to testify in public.\n\nThe recommendations on May 27, 1954 of the Personnel Security Board of the U.S. Atomic Energy Commission investigation into J. Robert Oppenheimer, director of the Manhattan Project at Los Alamos, stated Oppenheimer had been found in the company of \"Joseph W. Weinberg and Clarence Hiskey, who were alleged to be members of the Communist Party and to have engaged in espionage on behalf of the Soviet Union.\" Oppenheimer's security clearance was revoked the following month.\n\nWith the collapse of the Soviet Union, KGB Archives were made accessible to historian Allen Weinstein and a former KGB officer Alexander Vassiliev. The identification of Hiskey as a Soviet agent cover named RAMSAY which occurs in the Venona papers, corroborated Hiskey's covert relationship with Soviet intelligence.\n\n\n"}
{"id": "49997022", "url": "https://en.wikipedia.org/wiki?curid=49997022", "title": "Cyber manufacturing", "text": "Cyber manufacturing\n\nCybermanufacturing is a concept derived from cyber-physical systems (CPS) that refers to a modern manufacturing system that offers an information-transparent environment to facilitate asset management, provide reconfigurability, and maintain productivity. Compared with conventional experience-based management systems, cybermanufacturing provides an evidence-based environment to keep equipment users aware of networked asset status, and transfer raw data into possible risks and actionable information. Driving technologies include design of cyber-physical systems, combination of engineering domain knowledge and computer sciences, as well as information technologies. Among them, mobile applications for manufacturing is an area of specific interest to industries and academia.\n\nThe idea of cyber manufacturing stems from the fact that Internet-enabled services have added business value in economic sectors such as retail, music, consumer products, transportation, and healthcare. However, compared to existing Internet-enabled sectors, manufacturing assets are less connected and less accessible in real-time. Besides, current manufacturing enterprises make decisions following a top-down approach: from overall equipment effectiveness to assignment of production requirements, without considering the condition of machines. This will usually lead to inconsistency in operation management due to lack of linkage between factories, possible overstock in spare part inventory, as well as unexpected machine downtime. \nSuch situation calls for connectivity between machines as a foundation, and analytics on top of that as a necessity to translate raw data into information that actually facilitates user decision making. Expected functionalities of cybermanufacturing systems include machine connectivity and data acquisition, machine health prognostics, fleet-based asset management, and manufacturing reconfigurability.\n\nSeveral technologies are involved in developing cyber-manufacturing solutions. The following is a short description of these technologies and their involvement in cyber-manufacturing. \n\nIn 2013 the Office of Naval Research in the US Military has issued a proposal solicitation subjected for cyber-manufacturing. Later in 2015, the US National Science Foundation (NSF) has awarded a research project for developing cyber-manufacturing system to the Center for Intelligent Maintenance Systems (IMS) at the University of Cincinnati.\n"}
{"id": "22569896", "url": "https://en.wikipedia.org/wiki?curid=22569896", "title": "Danish modern", "text": "Danish modern\n\nDanish modern is a style of minimalist furniture and housewares from Denmark associated with the Danish design movement. In the 1920s, Kaare Klint embraced the principles of Bauhaus modernism in furniture design, creating clean, pure lines based on an understanding of classical furniture craftsmanship coupled with careful research into materials, proportions and the requirements of the human body. With designers such as Arne Jacobsen and Hans Wegner and associated cabinetmakers, Danish furniture thrived from the 1940s through the 1960s. Adopting mass-production techniques and concentrating on form rather than just function, Finn Juhl contributed to the style's success. Danish housewares adopting a similar minimalist design such as cutlery and trays of teak and stainless steel and dinnerware such as those produced in Denmark for Dansk in its early years, expanded the Danish modern aesthetic beyond furniture.\n\nBetween the two world wars, Kaare Klint exerted a strong influence on Danish furniture making. Appointed head of the Furniture Department at the Architecture School of the Royal Danish Academy of Fine Arts, he encouraged his students to take an analytical approach, adapting design to modern-day needs. Adopting the Functionalist trend of abandoning ornamentation in favour of form, he nonetheless maintained the warmth and beauty inherent in traditional Danish cabinet making, as well as high-quality craftsmanship and materials.\n\nThe development of modern Danish furniture owes much to the collaboration between architects and cabinetmakers. Cabinetmaker A. J. Iversen, who had successfully exhibited furniture from designs by architect Kay Gottlob at the Paris World Exhibition in 1925, was instrumental in fostering further partnerships. In 1927, with a view to encouraging innovation and stimulating public interest, the Danish Cabinetmakers Guild organized a furniture exhibition in Copenhagen which was to be held every year until 1967. It fostered collaboration between cabinetmakers and designers, creating a number of lasting partnerships including those between Rudolph Rasmussen and Kaare Klint, A. J. Iversen and Ole Wanscher, and Erhard Rasmussen and Børge Mogensen. From 1933, collaboration was reinforced as a result of the annual competition for new types of furniture, arranged each year prior to the exhibition. \nIn the postwar years, Danish designers and architects believed that design could be used to improve people's lives. Particular attention was given to creating affordable furniture and household objects that were both functional and elegant. Fruitful cooperation ensued, combining Danish craftsmanship with innovative design. Initially, the furniture was handmade, but recognizing that their work would sell better if prices were reduced, the designers soon turned to factory production. Interest in Danish Modern in the United States began when Edgar Kaufmann, Jr. from the Museum of Modern Art purchased some items for the Fallingwater home designed by Frank Lloyd Wright. This ultimately led to mass-production in the United States, too.\n\nThe scarcity of materials after the Second World War encouraged the use of plywood. In the late 1940s, the development of new techniques led to the mass production of bent plywood designs by Hans Wegner and Børge Mogensen, both of whom produced chairs with a teak plywood seat and back on a beech frame. In 1951, Arne Jacobsen went even further with his sculptural Ant Chair with a one-piece plywood seat and back, bent in both directions. Collapsible chairs dating from the 1930s include Kaare Klint's Safari Chair and propeller stools which were also developed by Poul Kjærholm and Jørgen Gammelgaard.\n\nFinn Juhl's home in Charlottenlund, just north of Copenhagen, has been preserved as he left it with the furniture he designed. Other major contributors to Danish Modern include Mogens Koch, Verner Panton, Jørn Utzon, Hans J. Wegner and Grete Jalk. Examples of their work can be seen at Designmuseum Danmark in Copenhagen. Of particular note are Mogensen's Sleigh Chair, Jacobsen's Swan and Juhl's sculptural wood-frame seats. One of Wegner's works was used by Nixon and Kennedy in a 1960 televised debate and is now known simply as The Chair.\n\nAs a result of the furniture school he founded at the Royal Academy in 1924, Klint had a strong influence on Danish furniture, shaping designers such as Kjærholm and Mogensen. His carefully researched designs are based on functionality, proportions in line with the human body, craftsmanship and the use of high quality materials. Notable examples of his work include the Propeller Stool (1927), the Safari Chair and the Deck Chair (both 1933), and the Church Chair (1936).\n\nPoul Henningsen, an architect, with a strong belief in the functionalist way of thinking, was an important participant in the Danish Modern school, not for furniture but for lighting design. His attempt to prevent the blinding glare from the electric lamp bulb succeeded in 1926 with a three-shade lamp, known as the PH lamp. The curvature of the shades allowed his hanging lamp to illuminate both the table and the rest of the room. He went on to design many similar lamps, some with frosted glass, including desk lamps, chandeliers and wall-mounted fixtures. Though he died in 1967, many of his designs have remained popular to this day.\n\nIn addition to his architectural work, Lassen was also a keen furniture designer. Influenced both by Le Corbusier and Ludwig Mies van der Rohe, he developed a unique approach to Functionalism. As a result of his fine craftsmanship and his search for simplicity, his steel-based furniture from the 1930s added a new dimension to the modernist movement. His later designs in wood still form part of classical Danish Modern, especially his three-legged stool and folding Egyptian coffee table (1940) originally produced by A. J. Iversen.\n\nGraduating from the Royal Academy in 1924, Jacobsen quickly demonstrated his mastery of both architecture and furniture design. With the completion of his Royal Hotel in Copenhagen and all its internal fittings and furniture in 1960, his talents became widely recognized, especially as a result of the chairs called the Egg and the Swan, now international icons. His stackable, three-legged Ant Chair (1952) with a one-piece plywood seat and back and its four-legged counterpart, the 7 Chair (1955), were particularly popular with worldwide sales in the millions.\n\nInspired by Kaare Klint under whom he had studied, Wanscher later followed in his footsteps as professor of the Royal Academy's furniture school. Particularly interested in 18th-century English furniture and in early Egyptian furniture, one of his most successful pieces was his delicately designed Egyptian Stool (1960) crafted from luxurious materials. Another successful item was his Colonial Chair in Brazilian rosewood. He was awarded the Grand Prix for furniture at Milan's triennale in 1960.\n\nThough he studied architecture at the Royal Academy, Juhl was a self-taught designer as far as furniture was concerned. In the late 1930s, he created furniture for himself but from 1945 he became recognized for his expressively sculptural designs, placing emphasis on form rather than function, so breaking tradition with the Klint school. His successful interior design work at the UN Headquarters in New York spread the notion of Danish Modern far and wide, paving the way for the international participation of his Danish colleagues. Two key pieces of furniture, in which the seat and backrest are separated from the wooden frame, are his 45-Chair, with its elegant armrests, and his Chieftain Chair (1949).\n\nAfter studying under Kaare Klint at the Copenhagen School of Arts and Crafts and at the Royal Danish Academy of Fine Arts, Mogensen adopted Klint's approach to simple, functional furniture design. Taking an almost scientific approach to an item's functionality, most of his furniture is characterized by strong, simple lines and was designed for industrial production. Notable items include his oak-framed Hunting Chair (1950) with a strong leather back and seat, his light, open Spokeback Sofa (1945), and the low robust Spanish Chair (1959).\n\n\nBest known as ‘the Master of the Chairs,' Wegner created fascinating furniture with clean, organic and aesthetic lines, balanced by a minimalist and composed aspect. He was a modernist with emphasis on the practicality and elegance of each piece he crafted. He believed the versatility and usability of his designs were as vital for him as the looks of them.After graduating in architecture in 1938, he worked in Arne Jacobsen and Eric Møller's office before establishing his own office in 1943. Striving for functionality as well as beauty, he became the most prolific Danish designer, producing over 500 different chairs. His Round Chair (technically Model 500) in 1949 was called \"the world's most beautiful chair\" before being labelled simply \"The Chair\" after Nixon and Kennedy used it in a 1960 televised debate. His Wishbone Chair, also 1949, with a Y-shaped back split and a curved back, was inspired by a Chinese child's chair he had seen. A work of simplicity and comfort, it is still made today by the Danish firm Carl Hansen & Son. Wegner's designs can now be found in several of the world's top design museums including New York's Museum of Modern Art.\n\nAfter training as a cabinetmaker, she studied at the Danish Design School in 1946, while receiving additional instruction from Kaare Klint at the Royal Academy's Furniture School. Inspired by Alvar Aalto's laminated bent-plywood furniture and Charles Eames' moulded plywood designs, she began to develop her own boldly curved models in the 1950s. In 1963, she won a Daily Mirror competition with her \"He Chair\" and \"She Chair\". With the help of furniture manufacturer Poul Jeppesen, she went on to design simpler models with clear, comfortable lines, which became popular both in Denmark and the United States thanks to their competitive prices. Jalk also edited the Danish design magazine \"Mobilia\" and compiled an authoritative four-volume work on Danish furniture.\n\nOn graduating from the Royal Academy in 1951, Panton worked briefly with Arne Jacobsen. During the 1960s, he designed furniture, lamps and textiles with an imaginative combination of innovative materials, playful shapes and bold colours. Among his earliest designs were the Bachelor Chair and Tivoli Chair (1955), both produced by Fritz Hansen, but he is remembered above all for his Panton Chair (1960), the world's first one-piece moulded plastic chair. Sometimes referred to as a pop artist, unlike the majority of his colleagues, he continued to be successful in the 1970s, not only with furniture but with interior designs including lighting.\n\nIn addition to an academic career at the School of Arts and Crafts and at the Institute of Design at the Royal Academy, Kjærholm always took full account of the importance of place a piece of furniture had in surrounding architectural space. Functionality took second place to his artistic approach which was centred on elegantly clean lines and attention to detail. Unlike many of his contemporaries, he worked essentially with steel, combining it with wood, leather, cane or marble. Kjærhom developed a close understanding with the cabinetmaker E. Kold Christensen who produced most of his designs. Today a wide selection of his furniture is produced by Fritz Hansen. Kjærholm's work can be seen in New York's Museum of Modern Art and the Victoria and Albert Museum in London.\n\nOften credited with having introduced Danish Modern design to America, Risom was a graduate of Copenhagen School of Industrial Arts and Design. He emigrated to the United States in 1939 to study American design, working first as a textile designer and later as a freelance furniture designer. In 1941 he joined Hans Knoll at the Hans Knoll Furniture Company, and together they toured the country promoting Risom's designs. A true minimalist, Risom worked mainly in wood because it was cheap, and one of his most successful pieces, Knoll Chair #654 (which is still being manufactured) was made with a seat of nylon webbing that had been discarded by the army.\n\nMany other designers and cabinetmakers contributed to the Danish modern scene. Several worked in partnerships, including:\n\n\nA number of cabinetmakers also developed skills in design. They include:\n\n\nSeveral other individuals made important contributions:\n\n\nA number of Danish textile designers worked closely with furniture designers to help shape the look of Danish modernism, for example by creating textiles for cushions, sofas, and beds. These include Lis Ahlmann and Vibeke Klint, among others.\n\nFrom the beginning of the 1950s, American manufacturers obtained licenses for the mass production of Danish designs while maintaining high standards of craftsmanship. Later, the designs were altered to suit American tastes and American parts were introduced to reduce costs. When Sears and Woolworth's entered the market, the Danes countered by producing new designs based on new materials. Sales peaked around 1963, but when American manufacturers introduced moulded plastic and wood-grained Formica as cheaper substitutes, they started to decline in favour of Mediterranean designs which became popular in 1966. There has however been a resurgence of interest in recent years. While the mass-produced works of Wegner, Juhl and Jacobsen are still in demand, collectors are increasingly turning to limited production items from these and the other designers. In the United States, while prices have increased, they are still at reasonable levels compared to similar items of new furniture. Licensed manufacturers have started reissuing key designs, while others have used Danish Modern for inspiration.\n\nEmploying some 15,000 people, each year Denmark's 400 furniture companies produce goods worth around DKK 13 billion (€1.75 billion). A highly productive sector, over 80% of the furniture produced is sold abroad making furniture Denmark's fifth most-important export industry. Most of the items produced are for the home, but many are designed for the workplace. In addition to its classic designs, Danish designer furniture benefits from a new generation of innovative players. As a result, Denmark has maintained its place as the world's leading furniture producer in relation to the country's population.\n\nA number of firms continue to be active in producing both classic Danish Modern designs and in introducing variants designed by a new generation of artists. They include Republic of Fritz Hansen, Fredericia Furniture, Carl Hansen & Søn and Normann Copenhagen, all of whom exhibited at the 2011 \"Salone Internazionale del Mobile\" in Milan. Other significant producers include PP Møbler, Kjærholm Production and One Collection, formerly known as Hansen & Sørensen.\n\nInnovative design work is also encouraged by the Wilhelm Hansen Foundation with the annual Finn Juhl Prize which is awarded to designers, manufacturers or writers who have made a special contribution to the field of furniture design, especially chairs.\n\n\n\n\n"}
{"id": "34158626", "url": "https://en.wikipedia.org/wiki?curid=34158626", "title": "David Manoukian", "text": "David Manoukian\n\nDavid J.L. Manoukian (January 10, 1975) is a French-Belgian-Armenian businessman. He is the founder, chairman, and chief executive officer of the luxury social network service The-Sphere.com. David Manoukian is the co-founder and chief executive officer of Realty Capital Partners which is a Company that invests in luxury real estate within niche markets such as Courchevel, Megève or Saint-Tropez.\n\nAfter studying in Le Rosey in Switzerland and then the US in Boston, David Manoukian got his Bachelor of Business Administration from the Institut supérieur de gestion in Paris in 1997.\nHe went on to PricewaterhouseCoopers, the auditing and consulting firm where he worked first in the Audit division and then in the Corporate-Finance division in charge of mergers and acquisitions for 3 years.\n\nIn 2000, he joined the family group Alain Manoukian as the International President and actively participated in the brand’s expansion in Europe, Russia, the Middle East and Asia. It was also with the family company that he developed France’s first textile E-commerce website. Before moving on to other horizons he oversaw the French ready-to-wear brand’s acquisition by the American brand BCBG Max Azria and its subsequent growth in the United States.\nWith his family in 2007, he turned his attention to urban commercial real estate development projects as well as high-luxury residential real estate.\nIn 2008, he created a multimedia division exclusively dedicated to the Web and the first project was The Sphere. \n\nDavid Manoukian is married to Chantal Manoukian; a Lebanese-Armenian woman who is a lawyer who took her bar exam in Paris. They have two boys Raffi and Azad.\n\n"}
{"id": "15367013", "url": "https://en.wikipedia.org/wiki?curid=15367013", "title": "Dharmacon, Inc.", "text": "Dharmacon, Inc.\n\nDharmacon Inc., now known as Dharmacon, a Horizon Discovery Group company, was founded in 1995 by Stephen Scaringe as Dharmacon Research to develop and commercialize a new technology for RNA oligonucleotide synthesis. The original Dharmacon focus and vision was to develop 2'-ACE RNA technology as the standard for RNA synthesis and to advance RNA oligo-dependent applications and technologies. When RNA interference (RNAi) emerged in the late 1990s, Dharmacon was poised to provide RNAi-related products to the multitude of academic and industry researchers. Dharmacon has become an important resource for those investigating the mechanisms of siRNA (small interfering RNA)-induced gene knockdown and applying the specificity and potency of RNAi to human biotherapeutics. Dharmacon's expertise in bioinformatics, RNA biology, and synthesis chemistry has allowed it to develop a complete line of products for the RNAi researcher.\n\nIn November 2002, Dharmacon Research, Inc. officially changed its name to Dharmacon, Inc., to reflect the increasing capabilities and future growth potential of the company, as it had advanced beyond research and development and into whole solutions for RNA oligo-dependent applications and technologies. In March 2004, Dharmacon became a wholly owned subsidiary of Fisher Scientific International, Inc. In November 2006, Fisher Scientific International, Inc. merged with Thermo Electron Corporation to become Thermo Fisher Scientific.\n\nIn October 2012 the websites for the companies formerly known as Dharmacon, Open Biosystems, Fermentas, Finnzymes, and ABgene were merged, creating the new Thermo Scientific Life Sciences Research eCommerce site.\n\nIn April 2013, Thermo Fisher Scientific reached an agreement to buy Life Technologies Corporation. The acquisition of Life Technologies Corporation received conditional approval from the European Commission in November 2013. The conditional requirement was the divestment of Thermo Fisher Scientific's cell culture (sera and media), magnetic beads and Dharmacon gene modulation businesses for antitrust reasons.\n\nIn early January 2014, GE Healthcare reached an agreement with Thermo Fisher Scientific to acquire the cell culture, magnetic bead and Dharmacon gene modulation businesses for 1.05 billion USD. The acquisition of Dharmacon is viewed as complementary to GE Healthcare's drug discovery research technologies.\n\nIn the fall of 2014, GE Healthcare announced it had reached a licensing agreement with the Broad Institute of MIT and Harvard granting access to CRISPR-Cas9 intellectual property. Under the agreement, GE Healthcare is able to incorporate the patented technologies, and develop and launch additional complementary gene editing reagents. Then, in October 2014, GE Healthcare Dharmacon launched its first CRISPR-Cas9 genome engineering products under the “Edit-R” brand. In July 2015, GE Healthcare Dharmacon launched synthetic crRNA and lentiviral sgRNA pre-designed with a functionally validated algorithm against entire human, mouse and rat genomes.\n\nIn July 2017, UK firm Horizon Discovery reached an agreement to acquire Dharmacon from GE Healthcare for a total consideration of $85 million. The acquisition is expected to complement Horizon's portfolio of gene editing products and engineered cell lines.\n\n\n\n"}
{"id": "19777162", "url": "https://en.wikipedia.org/wiki?curid=19777162", "title": "Electrical resistance heating remediation", "text": "Electrical resistance heating remediation\n\nElectrical Resistance Heating (ERH) is an intensive in situ environmental remediation method that uses the flow of alternating current electricity to heat soil and groundwater and evaporate contaminants. Electric current is passed through a targeted soil volume between subsurface electrode elements. The resistance to electrical flow that exists in the soil causes the formation of heat; resulting in an increase in temperature until the boiling point of water at depth is reached. After reaching this temperature, further energy input causes a phase change, forming steam and removing volatile contaminants. ERH is typically more cost effective when used for treating contaminant source areas...\n\nThree-phase heating (see Technology below) was originally created to enhance oil recovery. This design was patented in 1976 by Bill Pritchett of ARCO. The patent has expired and is now available for public use.\n\nSix-phase heating (see Technology below) was created and patented for the US Department of Energy (DOE) in the 1980s for use on DOE sites as well as commercial applications.\n\nElectrical resistance heating is used by the environmental restoration industry for remediation of contaminated soil and groundwater. ERH consists of constructing electrodes in the ground, applying alternating current (AC) electricity to the electrodes and heating the subsurface to temperatures that promote the evaporation of contaminants. Volatilized contaminants are captured by a subsurface vapor recovery system and conveyed to the surface along with recovered air and steam. Similar to Soil vapor extraction, the air, steam and volatilized contaminants are then treated at the surface to separate water, air and the contaminants. Treatment of the various streams depends on local regulations and the amount of contaminant.\n\nSome low volatility organic contaminants have a short hydrolysis half life. For contaminants like these, i.e. 1,1,2,2-Tetrachloroethane and 1,1,1-trichloroethane, hydrolysis can be the primary form of remediation. As the subsurface is heated the hydrolysis half life of the contaminant will decrease as described by the Arrhenius equation. This results in a rapid degradation of the contaminant. The hydrolysis by-product may be remediated by conventional ERH, however the majority of the mass of the primary contaminant will not be recovered but rather will degrade to a by-product.\n\nThere are predominantly two electrical load arrangements for ERH: three-phase and six-phase. Three-phase heating consists of electrodes in a repeating triangular or delta pattern. Adjacent electrodes are of a different electrical phase so electricity conducts between them as shown in Figure 1. The contaminated area is depicted by the green shape while the electrodes are depicted by the numbered circles.\n\nSix-phase heating consists of six electrodes in a hexagonal pattern with a neutral electrode in the center of the array. The six-phase arrays are outlined in blue in Figure 2 below. Once again the contaminated area is depicted by the green shape while the electrodes are depicted by the numbered circles. In a six-phase heating pattern there can be hot spots and cold spots depending on the phases that are next to each other. For this reason, six-phase heating typically works best on small circular areas that are less than 65 feet in diameter. \n\nERH is typically most effective on volatile organic compounds (VOCs). The chlorinated compounds perchloroethylene, trichloroethylene, and cis- or trans- 1,2-dichloroethylene are contaminants that are easily remediated with ERH. The table shows contaminants that can be remediated with ERH along with their respective boiling points. Less volatile contaminants like xylene or diesel can also be remediated with ERH but energy requirements increase as the volatility decreases.\n\nElectrode spacing and operating time can be adjusted to balance the overall remediation cost with the desired cleanup time. A typical remediation may consist of electrodes spaced 15 to 20 feet apart with operating times usually less than a year. The design and cost of an ERH remediation system depends on a number of factors, primarily the volume of soil/groundwater to be treated, the type of contamination, and the treatment goals. The physical and chemical properties of the target compounds are governed by laws that make heated remediations advantageous over most conventional methods. The electrical energy usage required for heating the subsurface and volatilizing the contaminants can account for 5 to 40% of the overall remediation cost.\n\nThere are several laws that govern an ERH remediation. Dalton’s law governs the boiling point of a relatively insoluble contaminant. Raoult’s law governs the boiling point of mutually soluble co-contaminants and Henry’s law governs the ratio of the contaminant in the vapor phase to the contaminant in the liquid phase.\n\nFor mutually insoluble compounds, Dalton's law states that the partial pressure of a non aqueous phase liquid (NAPL) is equal to its vapor pressure, and that the NAPL in contact with water will boil when the vapor pressure of water plus the vapor pressure of the VOC is equal to ambient pressure. When a VOC-steam bubble is formed the composition of the bubble is proportional to the composite’s respective vapor pressures.\nFor mutually soluble compounds, Raoult's law states that the partial pressure of a compound is equal to its vapor pressure times its mole fraction. This means that mutually soluble contaminants will volatilize slower than if there was only one compound present. \nHenry's law describes the tendency of a compound to join air in the vapor phase or dissolve in water. The Henry’s Law constant, sometimes called coefficient, is specific to each compound and depends on the system temperature. The constant is used to predict the amount of contaminant what will remain in the vapor phase (or transfer to the liquid phase), upon exiting the condenser.\n\nSignificant ERH technological advancements have occurred over the last five years. Three areas of focus have been: bedrock remediation, 1,4-dioxane and other emerging contaminants, and controlled low temperature heat to enhance other remedial or natural processes.\n\nERH has been used for over 15 years for treatment of unconsolidated soils in both the vadose and saturated zones. Recent advancements and results show that ERH can be an effective treatment method for bedrock. At an ERH site, the primary electrical current path is on the thin layer of water immediately adjacent to the soil or rock grains. Little current is carried by the water in the pore volume. It is not the pore fluid that dominates the electrical conductivity; it is the grain wetting fluid that dominates the electrical conductivity. Sedimentary rock will typically possess the thin layer of water required for current flow. This means ERH can effectively be used for treatment of sedimentary bedrock, which typically has significant primary porosity. \n1,4-dioxane is a recently-identified contaminant of concern. The regulatory criteria for 1,4-dioxane is constantly changing as more is learned about this contaminant. 1,4-dioxane has a high solubility in water and a low Henry's Law constant which combine to present complex challenges associated with remediation. At ambient conditions, the physical properties of 1,4-dioxane indicate air stripping is not an efficient treatment mechanism. Recent ERH remediation results indicate that ERH creates conditions favorable for treatment. ERH remediation involves steam stripping, which historically had not been investigated for 1,4-dioxane. At ERH sites, steam stripping was observed to effectively transfer 1,4-dioxane to the vapor phase for subsequent treatment. 99.8% reductions (or greater) in 1,4-dioxane concentrations in groundwater have been documented on recent ERH remediation. Monitoring of the above grade treatment streams indicates that 95% of 1,4-dioxane remained in the vapor stream after removal from the subsurface. Furthermore, granular activated carbon has proven to be an effective 1,4-dioxane vapor treatment method.\nVolatilization is the primary removal mechanism on most ERH sites. However, ERH can also be used to enhance other processes, some naturally occurring, to reduce the cost for treatment of a plume. ERH can be used to provide controlled low temperature heating for projects with remediation processes that do not involve steam stripping. \"Low temperature heating\" refers to the targeting of a subsurface temperature that is less than the boiling point of water. Examples of low temperature ERH include heat-enhanced bioremediation, heating the subsurface to temperatures above the solubility of dissolved gasses to induce VOC stripping (most notably carbon dioxide ebullition), heat enhanced in situ chemical oxidation (especially for persulfate activation), and heat-enhanced reduction (such as with iron-catalyzed reactions). ERH low-temperature heating can also be used to hydrolyze chlorinated alkanes in-situ at sub-boiling temperatures where hydrochloric acid released during hydrolysis further reacts with subsurface carbonates and bicarbonates to produce carbon dioxide for subsurface stripping of VOCs.\n\nUsing low temperature heating coupled with bioremediation, chemical oxidation, or dechlorination will result in increased reaction rates. This can significantly reduce the time required for these remediation processes as compared to a remediation at ambient temperature. In addition, a low temperature option does not require the use of the above grade treatment system for recovered vapors, as boiling temperatures will not be reached. This means less above grade infrastructure and lower overall cost.\n\nWhen heat is combined with multi-phase extraction, the elevated temperatures will reduce the viscosity and surface tension of the recovered fluids which makes removal faster and easier. This is the original purpose for the development of ERH - to enhance oil recovery (see above).\n\n\n\n"}
{"id": "4793304", "url": "https://en.wikipedia.org/wiki?curid=4793304", "title": "Firepower", "text": "Firepower\n\nFirepower is the military capability to direct force at an enemy. (It is not to be confused with the concept of rate of fire, which describes the cycling of the firing mechanism in a weapon system.) Firepower involves the whole range of potential weapons. The concept is generally taught as one of the three key principles of modern warfare wherein the enemy forces are destroyed or have their will to fight negated by sufficient and preferably overwhelming use of force as a result of combat operations.\n\nThrough the ages firepower has come to mean offensive power applied from a distance, thus involving ranged weapons as opposed to one-on-one close quarters combat. \"Firepower\" is thus something employed to keep enemy forces at a range where they can be defeated in detail or sapped of the will to continue. In the field of naval artillery, the weight of a broadside was long used as a figure of merit of a warship's firepower.\n\nThe earliest forms of warfare that might be called \"firepower\" were the slingers of ancient armies (a notable example being the biblical story of David), and archers. Eventually, the feared Huns employed the composite bow and light cavalry tactics to shower arrows on the enemy forces, a tactic that also appeared in a less mobile form in Britain, with its famed longbowmen, used during the various Anglo-French conflicts collectively known as the Hundred Years' War during the Middle Ages. The Battle of Crécy is often thought of as the beginning of the \"age of firepower\" in the west, where missile weapons enabled a small force to defeat a numerically superior enemy without the need for single combat. Firepower was later used to dramatic effect in a similar fashion during the Battle of Agincourt.\n\nFirepower of military units large and small has steadily increased since the introduction of firearms, with technical improvements that have, with some exceptions, diminished the effectiveness of fortification. Such improvements made close order formation useless for middle to late 19th century infantry, and the use of machine guns early in the 20th stymied frontal assaults. Military uniforms changed from gaudy to drab, making soldiers less visible to the increasing firepower. At sea, improved naval artillery ended the use of prize crews, and naval aviation brought an end to heavily armored battleships. \n\nThe use of firepower in achieving military objectives became one of several conflicting schools of military thought, or doctrines. The Battle of Vimy Ridge used massed artillery to help win an Allied victory, but dramatic improvements in siege weapon technology had also gone hand in hand with small scale infantry tactics. Operation Desert Storm also relied on massed firepower as did the 2003 Invasion of Iraq, but firepower was integrated with advances in small-unit training.\n\nSmall arms, such as the M249 SAW, have been employed on a squad level to provide an overwhelming volume of fire in relatively close quarters situations (within 100-300 yds). The idea is that a large volume of accurate suppressive fire will immobilize the enemy, degrading their ability to perform. In addition, grenade launchers such as the M79, and particularly those that can be underslung on an assault rifle, such as the M203 or M320, are used to provide units with a disproportionate amount of firepower. These weapons are useful in situations where a unit is outnumbered and needs to respond immediately with fire superiority, such as in an ambush by forces not similarly equipped.\n\n"}
{"id": "4649290", "url": "https://en.wikipedia.org/wiki?curid=4649290", "title": "Fluorosilicate glass", "text": "Fluorosilicate glass\n\nFluorosilicate glass (FSG) is a glass material composed primarily of fluorine, silicon and oxygen. It has a number of uses in industry and manufacturing, especially in semiconductor fabrication where it forms an insulating dielectric. The related fluorosilicate glass-ceramics have good mechanical and chemical properties.\n\nFluorosilicate glass has a low-k dielectric and is used in between copper metal layers during silicon integrated circuit fabrication process. It is widely used by semiconductor foundries on geometries sub 0.25μ.\nFluorosilicate glass is effectively a fluorine-containing silicon dioxide (k=3.5, while k of undoped silicon dioxide is 3.9).\nFluorosilicate glass is used by IBM. Intel started using Cu metal layers and FSG on its 1.2 GHz Pentium processor at 130 nm CMOS. TSMC (Taiwan Semiconductor Manufacturing Company) brought in FSG and copper together in the Altera APEX.\n\nFluorosilicate glass-ceramics are crystalline or semi-crystalline solids formed by careful cooling of molten fluorosilicate glass. They have good mechanical properties.\n\nPotassium fluororichterite based materials are composed from tiny interlocked rod-shaped amphibole crystals; they have good resistance to chemicals and can be used in microwave ovens. Richterite glass-ceramics are used for high-performance tableware.\n\nFluorosilicate glass-ceramics with sheet structure, derived from mica, are strong and machinable. They find a number of uses and can be used in high vacuum and as dielectrics and precision ceramic components. A number of mica and mica-fluoroapatite glass-ceramics were studied as biomaterials.\n\n"}
{"id": "47230181", "url": "https://en.wikipedia.org/wiki?curid=47230181", "title": "Hat box", "text": "Hat box\n\nA hat box (also commonly hatbox and sometimes hat bucket or hat tin)and occasionally referred to as a bandbox, is a container for storing and transporting headgear, protecting it from damage and dust. A more generic term for a box used to carry garments, including headgear, is a bandbox. Typically, a hat box is deep and round in shape, although it may also be boxlike and used as an item of luggage for transporting a variety of hats.\n\nHat boxes may be made of a range of materials, including cardboard, leather or metal. They may include straps or a carrying handle for transportation. More luxurious models may be padded and lined in materials such as silk in order to protect the headgear.\n\nThe hat box became a popular item in the 19th century – matching the popularity of hats for both day and evening wear – and accessories were produced to assist with both storage and cleaning. While milliners often packaged designs they sold in cardboard hat boxes, more robust designs were produced for travelling. Some designs were made to store more than one hat – including designs that could store both a daytime top hat and a collapsible version for evenings, known as a gibus. They might also include storage space for items such as a hat brush.\n\nDesigns became quite large during the Victorian era. A letter to \"The Times\" in 1844 warned travellers that Blackwall Railway's porters had charged a 1d to carry a hat box onto the train and a further 6d for transporting it to the London terminus, with the traveller himself paying only 4d for the journey. He recommended that travellers with luggage should go by steamboat.\n\nWhile traditionally hat boxes are circular or square in shape, some versions may follow the shape of the hat. New York Historical Society archives include a crescent shaped cardboard design thought to be from the early to mid 19th century and attributed to the New York City hatmaker Elisha Bloomer; Canadian archives include a tin design curved to match the tricorne-style military hat worn by Isaac Brock and dating from 1812.\n\n"}
{"id": "27582686", "url": "https://en.wikipedia.org/wiki?curid=27582686", "title": "Impinj", "text": "Impinj\n\nImpinj, Inc. is a manufacturer of radio-frequency identification (RFID) devices and software. The company was founded in 2000 and is headquartered in Seattle, Washington. The company was started based on the research done at the California Institute of Technology by Carver Mead and Chris Diorio. Impinj currently produces EPC Class 1, Gen 2 passive UHF RFID chips, RFID readers, RFID reader chips, and RFID antennas, and software applications for encoding chips, and gathering business intelligence on RFID systems.\n\nImpinj was founded in 2000 based on the research of Carver Mead and his student Chris Diorio. The name Impinj stands for \"Impact-ionized Hot-electron Injection\".\nIn 2006, Impinj became the first company to introduce products based on the EPCglobal UHF Gen 2 standard.\nBear Stearns reported in 2006 that Wal-Mart issued contracts to Impinj and Alien Technology, including them as significant suppliers for a total 15,000 RFID readers needed for Wal-Mart stores and distribution centers. The report invited speculation but was not confirmed.\n\nIn the same year, Impinj created new partnerships in Asia. In February, Impinj signed an Original Equipment Manufacturer agreement with the Hong Kong based company Convergence Systems Limited (subsidiary of the Chung Nam Group of Companies).\nIn December, Impinj partnered with Korean company LS Industrial Systems (part of LS Group) to create RFID solutions targeting the Korean market.\n\nIn June 2008, Impinj sold its non-volatile memory business to Virage Logic.\nAlso in 2008, Impinj acquired the Intel RFID division, including an Intel-developed RFID reader chip. Impinj renamed the chip Indy R1000.\nIn 2009, Coca-Cola unveiled their Freestyle soda machine that gives users one hundred different possible drink combinations. The Freestyle soda machine uses Impinj Monza tag chips and Indy reader chips to determine user preferences and to monitor the dispensers.\nMexico has certified the Impinj Speedway reader to be used by state agencies in the electronic vehicle registration initiative beginning in Mexico in July 2010.\nIn 2005, Impinj began working with Intel to develop RFID chips that would allow for \"Processor Secured Storage.\"\nImpinj created two new chips for the project: Monza X-2K Dura and Monza X-8K Dura,\nwhich allow for increased theft deterrence and wireless configuration of electronic devices. The chips will be used in Intel's Microsoft Windows 8-based processors for tablet computers, which will be released in the second half of 2012.\n\nIntroduced in April 2005, Monza tag chips were the first UHF Gen 2 RFID tag chips.\nUpon their introduction, it was announced that Impinj would be selling 50 million Monza tag chips that year. These 50 million chips were sold to nine different companies, including competitors Alien Technology and Texas Instruments. In 2010, Impinj introduced its Monza 4 tag chips with increased read and write capabilities and more memory options.\nIn April 2011, Impinj released their new Monza 5 chips which are designed to speed item-level encoding, with fewer errors. The Monza 5 can boost encoding speeds by up to 220 percent compared with other RFID technology on the market.\nAnnounced in April 2012, Impinj's Monza X tag chips are intended for such applications as theft deterrence and wireless device configuration. When embedded in an electronic device (such as a laptop PC), the device processor or an RFID reader can write to or read data from that device through a Monza X chip, even when the device is powered off.\nImpinj developed the Monza X chips through a partnership with Intel that began in 2005.\nImpinj created the Monza X-2K and Monza X-8K Dura chips with lockable memory blocks and two independent antennas, which allow the chips to be read by both near field and long range readers.\nIntel will be using the Monza X chips in Microsoft Windows 8-based processors for tablet computers, to be released in the second half of 2012.\n\nSpeedway is a registered trademark of Impinj. Speedway products include Speedway Revolution RFID Reader and Speedway xPortal RFID reader.\n\nThe Speedway RFID reader was first introduced in 2005 as the first RFID reader sold by Impinj. Designed to meet the RFID Gen 2 standards, Speedway was one of Impinj's GrandPrix products alongside Monza.\n\nThe Speedway Revolution RFID reader was introduced in 2009. The Speedway Revolution is 80% smaller than the original Speedway RFID reader, measuring 6.75 x 5.5 x 1 inches.\nThe Speedway Revolution introduced Autopilot technology, which enables the reader to reconfigure itself as the environment shifts.\n\nIntroduced in 2010, the Speedway xPortal is a RFID fixed reader that combined the Speedway Revolution with Dual-Linear Phased Array technology, with a smaller design than previous reader portals. Whereas previous portal readers weighed about 150 lbs, the Speedway xPortal weighs 6.5 lbs and measures 40.5 x 8.72 x 2 inches.\n\nIn 2008, Impinj acquired the Intel RFID division, including an Intel-developed RFID reader chip which Impinj renamed Indy R1000. By combining many electrical components on one microchip, RFID reader chips can minimize size and costs of RFID readers. As of 2008, 40 to 50 manufacturers had developed readers using the R1000 chip.\nIn 2009, Impinj unveiled the Indy R2000 reader chip, with increased performance designed for use in high-end readers for more challenging applications. In 2010, Impinj further expander their reader chip portfolio by introducing the Indy R500 reader chip, a lower cost chip for applications that don't require high performance.\n\nIn 2011, Impinj announced its STP Source Tagging Platform, a combination of a reader and firmware designed for mass encoding of RFID tags. The platform consists of two systems: bulk encoding for tags already attached to items, or in-line encoding before tags are applied to products. The STP platform is capable of encoding 1100 tags per minute in the bulk system, and up to 1750 tags per minute using the in-line system. In 2012, Impinj announced a version 2 release of the STP platform that will enable brand owners and service bureaus to achieve encoding speeds up to 7,500 tags per minute.\n\nImpinj's Store Performance Simulator (SPS), released in June 2012, is a \"Web-based analysis tool\"\ndesigned to show retailers how RFID can increase the accuracy of their inventory and positively impact profitability.\nA retailer can use SPS's 25 inputs to reflect their particular store and simulate various \"what-if\" scenarios,\nrather than running unfeasible real-world tests.\n\n"}
{"id": "36366491", "url": "https://en.wikipedia.org/wiki?curid=36366491", "title": "Integrated standby instrument system", "text": "Integrated standby instrument system\n\nAn integrated standby instrument system (ISIS) is an electronic aircraft instrument intended to serve as backup in case of failures in a glass cockpit instrument system, and thus is designed to operate as reliably and independently as possible from the aircraft's main instrument system, with provisions for backup power, and embedded sensors when possible.\n\nAn ISIS is designed to combine the functions of separate equivalent mechanical instruments that had previously been included as backup in such cockpits, including altimeter, airspeed indicator, and attitude indicator. More sophisticated integrated standby systems can include additional functions, such as the Rockwell Collins Pro Line 21 flight deck fitted to the Cessna Citation XLS+, which features a standby navigation display and engine gauges.\n"}
{"id": "11648922", "url": "https://en.wikipedia.org/wiki?curid=11648922", "title": "King of Shaves", "text": "King of Shaves\n\nThe King of Shaves Company, Ltd. is a British toiletries company headquartered in Beaconsfield, Buckinghamshire, England. The King of Shaves brand was founded in 1993 by Will King. The company was demerged from Knowledge & Merchandising Inc. Ltd. on 1 June 2009. It is owned by The King of Shaves Holding Company Ltd.\n\nWill King founded Knowledge & Merchandising Inc., Ltd. (KMI) the company which owned the King of Shaves brand in 1993.\n\nIn 2009, The King of Shaves Company Ltd. was demerged from KMI, and the company appointed Atul Sharma as CFO and Chris Outram as Non-Executive Chairman of both The King of Shaves Company Ltd. and KMI (now KMI Brands Ltd).\n\nSince 2009, King of Shaves has used a business model similar to that used by Coca-Cola, with a central office controlling research and development, whilst production and point of sale is localised to a specific market. The success of this model has varied. Though in 2009 the company's merchandise was stocked in at least 7 separate countries, from 2007 the turnover went from £13.9 million, to £25 million in 2008, to £10 million by 2012.\n\nOn 22 June 2009, the company announced a \"Shaving Bond\" issue of up to five thousand £1,000 non-transferable and non-convertible bonds, which was reported in the press and described by Will King as being an \"innovative way to potentially raise money from up to 5,000 brand enthusiasts\". The issue was overseen by FSA regulated accountancy firm BDO Stoy Hayward with legal advice provided by Memery Crystal LLP.\n\nIn October 2011 The King of Shaves Company signed a multi-year agreement with Spectrum Brands Holdings Inc, owner of Remington branded electrical grooming products, to exclusively distribute King of Shaves products in the USA and Canada. The distribution agreement was dissolved in 2012 after 'breaches of contract' by Spectrum Brands\n\nOn 27 October 2014 it was announced that Will King would be stepping down as CEO, with Andy Hill appointed as the new CEO.\n\nKing of Shaves launched a 4-blade razor (call the Azor) in the UK in late 2008. They launched a 5-blade razor (called the Azor 5) in the UK in early 2011. The cartridges featured blades manufactured by Kai Industries in Japan (a shareholder in King of Shaves). The Azor 5 was discontinued in early 2014 and was replaced by the Hyperglide system razor. All the razors have been designed and manufactured in the UK, with cartridges developed in collaboration with Kai.\n\nIn March 2008 KMI acquired the issued share capital of Floraroma Ltd., which owns women's toiletries brands including Phil Smith, Delicious Beauty, Dead Sea Source, Little Me, Derma-Mum and Floracologie. In October 2009, KMI acquired the Naked skincare brand.\n\nIn 2009 the company offered savings bonds with 6%pa interest rate to consumers. These savings bonds rely on the company's growth in order to fund the returns. This scheme was not covered by the Financial Services Compensation Scheme and at the time of application consumers were unable to get reports on how well the company was performing.\n\nIn 2012 a fresh batch of bonds were released to existing Bondholders only, though the ability to redeem them has been withheld for a number of years, with no change to their status with regulators.\n\nKing of Shaves has sponsored athletes throughout Great Britain. During 2006 they sponsored John Terry and P1 Offshore Powerboat. In later years, they sponsored both Chrissy Palmer for MRF Formula Ford races, but also Jordan King, son of Justin King (former CEO of Sainsbury's), during his F3 races.\n\n2012 saw the company sponsor James Ellington the \"eBay\" athlete and back the Welsh Extreme 40 catamaran.\n\n"}
{"id": "488640", "url": "https://en.wikipedia.org/wiki?curid=488640", "title": "List of engineering societies", "text": "List of engineering societies\n\nAn engineering society is a professional organization for engineers of various disciplines. Some are umbrella type organizations which accept many different disciplines, while others are discipline-specific. Many award professional designations, such as European Engineer, professional engineer, chartered engineer, incorporated engineer or similar. There are also many student-run engineering societies, commonly at universities or technical colleges.\n\n\n\nIn Canada, the term \"engineering society\" sometimes refers to organizations of engineering students as opposed to professional societies of engineers. The Canadian Federation of Engineering Students, whose membership consists of most of the engineering student societies from across Canada (see below), is the national association of undergraduate engineering student societies in Canada.\n\nCanada also has many traditions related to the calling of an engineer.\n\nThe Engineering Institute of Canada (French: l'Institut Canadien des ingénieurs) has the following member societies:\n\n\n\n\n\n\n\n\n\n\nIn the Philippines, the Professional Regulation Commission is a three-man commission attached to the office of the president of the Philippines. Its mandate is to regulate and supervise the practice of the professionals (except lawyers) who constitute the highly skilled manpower of the country. As the agency-in-charge of the professional sector, the PRC plays a strategic role in developing the corps of professionals for industry, commerce, governance and the economy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the United Kingdom, the Engineering Council is the regulatory body for the engineering profession. The Engineering Council was incorporated by Royal Charter in 1981 and controls the award of chartered engineer, incorporated engineer, engineering technician and information and communications technology technician titles, through licences issued to thirty six recognised Institutions. There are also 19 professional affiliate institutions, not licensed, but with close associations to the Engineering Council.\n\nThe Royal Academy of Engineering is the national academy for engineering, analogous to the Royal Society in science and the British Academy in arts and humanities.\n\n\n\n\n\n\n\n\n\n"}
{"id": "19528", "url": "https://en.wikipedia.org/wiki?curid=19528", "title": "Mechanical engineering", "text": "Mechanical engineering\n\nMechanical engineering is the discipline that applies engineering, physics, engineering mathematics, and materials science principles to design, analyze, manufacture, and maintain mechanical systems. It is one of the oldest and broadest of the engineering disciplines.\n\nThe mechanical engineering field requires an understanding of core areas including mechanics, dynamics, thermodynamics, materials science, structural analysis, and electricity. In addition to these core principles, mechanical engineers use tools such as computer-aided design (CAD), computer-aided manufacturing (CAM), and product life cycle management to design and analyze manufacturing plants, industrial equipment and machinery, heating and cooling systems, transport systems, aircraft, watercraft, robotics, medical devices, weapons, and others. It is the branch of engineering that involves the design, production, and operation of machinery.\n\nMechanical engineering emerged as a field during the Industrial Revolution in Europe in the 18th century; however, its development can be traced back several thousand years around the world. In the 19th century, developments in physics led to the development of mechanical engineering science. The field has continually evolved to incorporate advancements; today mechanical engineers are pursuing developments in such areas as composites, mechatronics, and nanotechnology. It also overlaps with aerospace engineering, metallurgical engineering, civil engineering, electrical engineering, manufacturing engineering, chemical engineering, industrial engineering, and other engineering disciplines to varying amounts. Mechanical engineers may also work in the field of biomedical engineering, specifically with biomechanics, transport phenomena, biomechatronics, bionanotechnology, and modeling of biological systems.\n\nThe application of mechanical engineering can be seen in the archives of various ancient and medieval societies. In ancient Greece, the works of Archimedes (287–212 BC) influenced mechanics in the Western tradition and Heron of Alexandria (c. 10–70 AD) created the first steam engine (Aeolipile). In China, Zhang Heng (78–139 AD) improved a water clock and invented a seismometer, and Ma Jun (200–265 AD) invented a chariot with differential gears. The medieval Chinese horologist and engineer Su Song (1020–1101 AD) incorporated an escapement mechanism into his astronomical clock tower two centuries before escapement devices were found in medieval European clocks. He also invented the world's first known endless power-transmitting chain drive.\n\nDuring the Islamic Golden Age (7th to 15th century), Muslim inventors made remarkable contributions in the field of mechanical technology. Al-Jazari, who was one of them, wrote his famous \"Book of Knowledge of Ingenious Mechanical Devices\" in 1206 and presented many mechanical designs. Al-Jazari is also the first known person to create devices such as the crankshaft and camshaft, which now form the basics of many mechanisms.\n\nDuring the 17th century, important breakthroughs in the foundations of mechanical engineering occurred in England. Sir Isaac Newton formulated Newton's Laws of Motion and developed Calculus, the mathematical basis of physics. Newton was reluctant to publish his works for years, but he was finally persuaded to do so by his colleagues, such as Sir Edmond Halley, much to the benefit of all mankind. Gottfried Wilhelm Leibniz is also credited with creating Calculus during this time period.\n\nDuring the early 19th century industrial revolution, machine tools were developed in England, Germany, and Scotland. This allowed mechanical engineering to develop as a separate field within engineering. They brought with them manufacturing machines and the engines to power them. The first British professional society of mechanical engineers was formed in 1847 Institution of Mechanical Engineers, thirty years after the civil engineers formed the first such professional society Institution of Civil Engineers. On the European continent, Johann von Zimmermann (1820–1901) founded the first factory for grinding machines in Chemnitz, Germany in 1848.\n\nIn the United States, the American Society of Mechanical Engineers (ASME) was formed in 1880, becoming the third such professional engineering society, after the American Society of Civil Engineers (1852) and the American Institute of Mining Engineers (1871). The first schools in the United States to offer an engineering education were the United States Military Academy in 1817, an institution now known as Norwich University in 1819, and Rensselaer Polytechnic Institute in 1825. Education in mechanical engineering has historically been based on a strong foundation in mathematics and science.\n\nDegrees in mechanical engineering are offered at various universities worldwide. Mechanical engineering programs typically take four to five years of study and result in a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Science Engineering (B.Sc.Eng.), Bachelor of Technology (B.Tech.), Bachelor of Mechanical Engineering (B.M.E.), or Bachelor of Applied Science (B.A.Sc.) degree, in or with emphasis in mechanical engineering. In Spain, Portugal and most of South America, where neither B.Sc. nor B.Tech. programs have been adopted, the formal name for the degree is \"Mechanical Engineer\", and the course work is based on five or six years of training. In Italy the course work is based on five years of education, and training, but in order to qualify as an Engineer one has to pass a state exam at the end of the course. In Greece, the coursework is based on a five-year curriculum and the requirement of a 'Diploma' Thesis, which upon completion a 'Diploma' is awarded rather than a B.Sc.\n\nIn Australia, mechanical engineering degrees are awarded as Bachelor of Engineering (Mechanical) or similar nomenclature although there are an increasing number of specialisations. The degree takes four years of full-time study to achieve. To ensure quality in engineering degrees, Engineers Australia accredits engineering degrees awarded by Australian universities in accordance with the global Washington Accord. Before the degree can be awarded, the student must complete at least 3 months of on the job work experience in an engineering firm. Similar systems are also present in South Africa and are overseen by the Engineering Council of South Africa (ECSA).\n\nIn the United States, most undergraduate mechanical engineering programs are accredited by the Accreditation Board for Engineering and Technology (ABET) to ensure similar course requirements and standards among universities. The ABET web site lists 302 accredited mechanical engineering programs as of 11 March 2014. Mechanical engineering programs in Canada are accredited by the Canadian Engineering Accreditation Board (CEAB), and most other countries offering engineering degrees have similar accreditation societies.\n\nIn India, to become an engineer, one needs to have an engineering degree like a B.Tech or B.E, have a diploma in engineering, or by completing a course in an engineering trade like fitter from the Industrial Training Institute (ITIs) to receive a \"ITI Trade Certificate\" and also pass the All India Trade Test (AITT) with an engineering trade conducted by the National Council of Vocational Training (NCVT) by which one is awarded a \"National Trade Certificate\". A similar system is used in Nepal.\n\nSome mechanical engineers go on to pursue a postgraduate degree such as a Master of Engineering, Master of Technology, Master of Science, Master of Engineering Management (M.Eng.Mgt. or M.E.M.), a Doctor of Philosophy in engineering (Eng.D. or Ph.D.) or an engineer's degree. The master's and engineer's degrees may or may not include research. The Doctor of Philosophy includes a significant research component and is often viewed as the entry point to academia. The Engineer's degree exists at a few institutions at an intermediate level between the master's degree and the doctorate.\n\nStandards set by each country's accreditation society are intended to provide uniformity in fundamental subject material, promote competence among graduating engineers, and to maintain confidence in the engineering profession as a whole. Engineering programs in the U.S., for example, are required by ABET to show that their students can \"work professionally in both thermal and mechanical systems areas.\" The specific courses required to graduate, however, may differ from program to program. Universities and Institutes of technology will often combine multiple subjects into a single class or split a subject into multiple classes, depending on the faculty available and the university's major area(s) of research.\n\nThe fundamental subjects of mechanical engineering usually include:\n\nMechanical engineers are also expected to understand and be able to apply basic concepts from chemistry, physics, chemical engineering, civil engineering, and electrical engineering. All mechanical engineering programs include multiple semesters of mathematical classes including calculus, and advanced mathematical concepts including differential equations, partial differential equations, linear algebra, abstract algebra, and differential geometry, among others.\n\nIn addition to the core mechanical engineering curriculum, many mechanical engineering programs offer more specialized programs and classes, such as control systems, robotics, transport and logistics, cryogenics, fuel technology, automotive engineering, biomechanics, vibration, optics and others, if a separate department does not exist for these subjects.\n\nMost mechanical engineering programs also require varying amounts of research or community projects to gain practical problem-solving experience. In the United States it is common for mechanical engineering students to complete one or more internships while studying, though this is not typically mandated by the university. Cooperative education is another option. Future work skills research puts demand on study components that feed student's creativity and innovation.\n\nEngineers may seek license by a state, provincial, or national government. The purpose of this process is to ensure that engineers possess the necessary technical knowledge, real-world experience, and knowledge of the local legal system to practice engineering at a professional level. Once certified, the engineer is given the title of Professional Engineer (in the United States, Canada, Japan, South Korea, Bangladesh and South Africa), Chartered Engineer (in the United Kingdom, Ireland, India and Zimbabwe), \"Chartered Professional Engineer\" (in Australia and New Zealand) or \"European Engineer\" (much of the European Union).\n\nIn the U.S., to become a licensed Professional Engineer (PE), an engineer must pass the comprehensive FE (Fundamentals of Engineering) exam, work a minimum of 4 years as an \"Engineering Intern (EI)\" or \"Engineer-in-Training (EIT)\", and pass the \"Principles and Practice\" or PE (Practicing Engineer or Professional Engineer) exams. The requirements and steps of this process are set forth by the National Council of Examiners for Engineering and Surveying (NCEES), a composed of engineering and land surveying licensing boards representing all U.S. states and territories.\n\nIn the UK, current graduates require a BEng plus an appropriate master's degree or an integrated MEng degree, a minimum of 4 years post graduate on the job competency development, and a peer reviewed project report in the candidates specialty area in order to become a Chartered Mechanical Engineer (CEng, MIMechE) through the Institution of Mechanical Engineers. CEng MIMechE can also be obtained via an examination route administered by the City and Guilds of London Institute.\n\nIn most developed countries, certain engineering tasks, such as the design of bridges, electric power plants, and chemical plants, must be approved by a professional engineer or a chartered engineer. \"Only a licensed engineer, for instance, may prepare, sign, seal and submit engineering plans and drawings to a public authority for approval, or to seal engineering work for public and private clients.\" This requirement can be written into state and provincial legislation, such as in the Canadian provinces, for example the Ontario or Quebec's Engineer Act.\n\nIn other countries, such as Australia, and the UK, no such legislation exists; however, practically all certifying bodies maintain a code of ethics independent of legislation, that they expect all members to abide by or risk expulsion.\n\nMechanical engineers research, design, develop, build, and test mechanical and thermal devices, including tools, engines, and machines.\n\nMechanical engineers typically do the following:\n\nMechanical engineers design and oversee the manufacturing of many products ranging from medical devices to new batteries. They also design power-producing machines such as electric generators, internal combustion engines, and steam and gas turbines as well as power-using machines, such as refrigeration and air-conditioning systems.\n\nLike other engineers, mechanical engineers use computers to help create and analyze designs, run simulations and test how a machine is likely to work.\n\nThe total number of engineers employed in the U.S. in 2015 was roughly 1.6 million. Of these, 278,340 were mechanical engineers (17.28%), the largest discipline by size. In 2012, the median annual income of mechanical engineers in the U.S. workforce was $80,580. The median income was highest when working for the government ($92,030), and lowest in education ($57,090). In 2014, the total number of mechanical engineering jobs was projected to grow 5% over the next decade. As of 2009, the average starting salary was $58,800 with a bachelor's degree.\n\nMany mechanical engineering companies, especially those in industrialized nations, have begun to incorporate computer-aided engineering (CAE) programs into their existing design and analysis processes, including 2D and 3D solid modeling computer-aided design (CAD). This method has many benefits, including easier and more exhaustive visualization of products, the ability to create virtual assemblies of parts, and the ease of use in designing mating interfaces and tolerances.\n\nOther CAE programs commonly used by mechanical engineers include product lifecycle management (PLM) tools and analysis tools used to perform complex simulations. Analysis tools may be used to predict product response to expected loads, including fatigue life and manufacturability. These tools include finite element analysis (FEA), computational fluid dynamics (CFD), and computer-aided manufacturing (CAM).\n\nUsing CAE programs, a mechanical design team can quickly and cheaply iterate the design process to develop a product that better meets cost, performance, and other constraints. No physical prototype need be created until the design nears completion, allowing hundreds or thousands of designs to be evaluated, instead of a relative few. In addition, CAE analysis programs can model complicated physical phenomena which cannot be solved by hand, such as viscoelasticity, complex contact between mating parts, or non-Newtonian flows.\n\nAs mechanical engineering begins to merge with other disciplines, as seen in mechatronics, multidisciplinary design optimization (MDO) is being used with other CAE programs to automate and improve the iterative design process. MDO tools wrap around existing CAE processes, allowing product evaluation to continue even after the analyst goes home for the day. They also utilize sophisticated optimization algorithms to more intelligently explore possible designs, often finding better, innovative solutions to difficult multidisciplinary design problems.\n\nThe field of mechanical engineering can be thought of as a collection of many mechanical engineering science disciplines. Several of these subdisciplines which are typically taught at the undergraduate level are listed below, with a brief explanation and the most common application of each. Some of these subdisciplines are unique to mechanical engineering, while others are a combination of mechanical engineering and one or more other disciplines. Most work that a mechanical engineer does uses skills and techniques from several of these subdisciplines, as well as specialized subdisciplines. Specialized subdisciplines, as used in this article, are more likely to be the subject of graduate studies or on-the-job training than undergraduate research. Several specialized subdisciplines are discussed in this section.\n\nMechanics is, in the most general sense, the study of forces and their effect upon matter. Typically, engineering mechanics is used to analyze and predict the acceleration and deformation (both elastic and plastic) of objects under known forces (also called loads) or stresses. Subdisciplines of mechanics include\n\nMechanical engineers typically use mechanics in the design or analysis phases of engineering. If the engineering project were the design of a vehicle, statics might be employed to design the frame of the vehicle, in order to evaluate where the stresses will be most intense. Dynamics might be used when designing the car's engine, to evaluate the forces in the pistons and cams as the engine cycles. Mechanics of materials might be used to choose appropriate materials for the frame and engine. Fluid mechanics might be used to design a ventilation system for the vehicle (see HVAC), or to design the intake system for the engine.\n\nMechatronics is a combination of mechanics and electronics. It is an interdisciplinary branch of mechanical engineering, electrical engineering and software engineering that is concerned with integrating electrical and mechanical engineering to create hybrid systems. In this way, machines can be automated through the use of electric motors, servo-mechanisms, and other electrical systems in conjunction with special software. A common example of a mechatronics system is a CD-ROM drive. Mechanical systems open and close the drive, spin the CD and move the laser, while an optical system reads the data on the CD and converts it to bits. Integrated software controls the process and communicates the contents of the CD to the computer.\n\nRobotics is the application of mechatronics to create robots, which are often used in industry to perform tasks that are dangerous, unpleasant, or repetitive. These robots may be of any shape and size, but all are preprogrammed and interact physically with the world. To create a robot, an engineer typically employs kinematics (to determine the robot's range of motion) and mechanics (to determine the stresses within the robot).\n\nRobots are used extensively in industrial engineering. They allow businesses to save money on labor, perform tasks that are either too dangerous or too precise for humans to perform them economically, and to ensure better quality. Many companies employ assembly lines of robots, especially in Automotive Industries and some factories are so robotized that they can run by themselves. Outside the factory, robots have been employed in bomb disposal, space exploration, and many other fields. Robots are also sold for various residential applications, from recreation to domestic applications.\n\nStructural analysis is the branch of mechanical engineering (and also civil engineering) devoted to examining why and how objects fail and to fix the objects and their performance. Structural failures occur in two general modes: static failure, and fatigue failure. \"Static structural failure\" occurs when, upon being loaded (having a force applied) the object being analyzed either breaks or is deformed plastically, depending on the criterion for failure. \"Fatigue failure\" occurs when an object fails after a number of repeated loading and unloading cycles. Fatigue failure occurs because of imperfections in the object: a microscopic crack on the surface of the object, for instance, will grow slightly with each cycle (propagation) until the crack is large enough to cause ultimate failure.\n\nFailure is not simply defined as when a part breaks, however; it is defined as when a part does not operate as intended. Some systems, such as the perforated top sections of some plastic bags, are designed to break. If these systems do not break, failure analysis might be employed to determine the cause.\n\nStructural analysis is often used by mechanical engineers after a failure has occurred, or when designing to prevent failure. Engineers often use online documents and books such as those published by ASM to aid them in determining the type of failure and possible causes.\n\nOnce theory is applied to a mechanical design, physical testing is often performed to verify calculated results. Structural analysis may be used in an office when designing parts, in the field to analyze failed parts, or in laboratories where parts might undergo controlled failure tests.\n\nThermodynamics is an applied science used in several branches of engineering, including mechanical and chemical engineering. At its simplest, thermodynamics is the study of energy, its use and transformation through a system. Typically, engineering thermodynamics is concerned with changing energy from one form to another. As an example, automotive engines convert chemical energy (enthalpy) from the fuel into heat, and then into mechanical work that eventually turns the wheels.\n\nThermodynamics principles are used by mechanical engineers in the fields of heat transfer, thermofluids, and energy conversion. Mechanical engineers use thermo-science to design engines and power plants, heating, ventilation, and air-conditioning (HVAC) systems, heat exchangers, heat sinks, radiators, refrigeration, insulation, and others.\n\nDrafting or technical drawing is the means by which mechanical engineers design products and create instructions for manufacturing parts. A technical drawing can be a computer model or hand-drawn schematic showing all the dimensions necessary to manufacture a part, as well as assembly notes, a list of required materials, and other pertinent information. A U.S. mechanical engineer or skilled worker who creates technical drawings may be referred to as a drafter or draftsman. Drafting has historically been a two-dimensional process, but computer-aided design (CAD) programs now allow the designer to create in three dimensions.\n\nInstructions for manufacturing a part must be fed to the necessary machinery, either manually, through programmed instructions, or through the use of a computer-aided manufacturing (CAM) or combined CAD/CAM program. Optionally, an engineer may also manually manufacture a part using the technical drawings. However, with the advent of computer numerically controlled (CNC) manufacturing, parts can now be fabricated without the need for constant technician input. Manually manufactured parts generally consist spray coatings, surface finishes, and other processes that cannot economically or practically be done by a machine.\n\nDrafting is used in nearly every subdiscipline of mechanical engineering, and by many other branches of engineering and architecture. Three-dimensional models created using CAD software are also commonly used in finite element analysis (FEA) and computational fluid dynamics (CFD).\n\nMechanical engineers are constantly pushing the boundaries of what is physically possible in order to produce safer, cheaper, and more efficient machines and mechanical systems. Some technologies at the cutting edge of mechanical engineering are listed below (see also exploratory engineering).\n\nMicron-scale mechanical components such as springs, gears, fluidic and heat transfer devices are fabricated from a variety of substrate materials such as silicon, glass and polymers like SU8. Examples of MEMS components are the accelerometers that are used as car airbag sensors, modern cell phones, gyroscopes for precise positioning and microfluidic devices used in biomedical applications.\n\nFriction stir welding, a new type of welding, was discovered in 1991 by The Welding Institute (TWI). The innovative steady state (non-fusion) welding technique joins materials previously un-weldable, including several aluminum alloys. It plays an important role in the future construction of airplanes, potentially replacing rivets. Current uses of this technology to date include welding the seams of the aluminum main Space Shuttle external tank, Orion Crew Vehicle test article, Boeing Delta II and Delta IV Expendable Launch Vehicles and the SpaceX Falcon 1 rocket, armor plating for amphibious assault ships, and welding the wings and fuselage panels of the new Eclipse 500 aircraft from Eclipse Aviation among an increasingly growing pool of uses.\n\nComposites or composite materials are a combination of materials which provide different physical characteristics than either material separately. Composite material research within mechanical engineering typically focuses on designing (and, subsequently, finding applications for) stronger or more rigid materials while attempting to reduce weight, susceptibility to corrosion, and other undesirable factors. Carbon fiber reinforced composites, for instance, have been used in such diverse applications as spacecraft and fishing rods.\n\nMechatronics is the synergistic combination of mechanical engineering, electronic engineering, and software engineering. The discipline of mechatronics began as a way to combine mechanical principles with electrical engineering. Mechatronic concepts are used in the majority of electro-mechanical systems. Typical electro-mechanical sensors used in mechatronics are strain gauges, thermocouples, and pressure transducers.\n\nAt the smallest scales, mechanical engineering becomes nanotechnology—one speculative goal of which is to create a molecular assembler to build molecules and materials via mechanosynthesis. For now that goal remains within exploratory engineering. Areas of current mechanical engineering research in nanotechnology include nanofilters, nanofilms, and nanostructures, among others.\nFinite Element Analysis is a computational tool used to estimate stress, strain, and deflection of solid bodies. It uses a mesh setup with user-defined sizes to measure physical quantities at a node. The more nodes there are, the higher the precision. This field is not new, as the basis of Finite Element Analysis (FEA) or Finite Element Method (FEM) dates back to 1941. But the evolution of computers has made FEA/FEM a viable option for analysis of structural problems. Many commercial codes such as NASTRAN, ANSYS, and ABAQUS are widely used in industry for research and the design of components. Some 3D modeling and CAD software packages have added FEA modules. In the recent times, cloud simulation platforms like SimScale are becoming more common.\n\nOther techniques such as finite difference method (FDM) and finite-volume method (FVM) are employed to solve problems relating heat and mass transfer, fluid flows, fluid surface interaction, etc.\n\nBiomechanics is the application of mechanical principles to biological systems, such as humans, animals, plants, organs, and cells. Biomechanics also aids in creating prosthetic limbs and artificial organs for humans. Biomechanics is closely related to engineering, because it often uses traditional engineering sciences to analyze biological systems. Some simple applications of Newtonian mechanics and/or materials sciences can supply correct approximations to the mechanics of many biological systems.\n\nIn the past decade, reverse engineering of materials found in nature such as bone matter has gained funding in academia. The structure of bone matter is optimized for its purpose of bearing a large amount of compressive stress per unit weight. The goal is to replace crude steel with bio-material for structural design.\n\nOver the past decade the Finite element method (FEM) has also entered the Biomedical sector highlighting further engineering aspects of Biomechanics. FEM has since then established itself as an alternative to in vivo surgical assessment and gained the wide acceptance of academia. The main advantage of Computational Biomechanics lies in its ability to determine the endo-anatomical response of an anatomy, without being subject to ethical restrictions. This has led FE modelling to the point of becoming ubiquitous in several fields of Biomechanics while several projects have even adopted an open source philosophy (e.g. BioSpine).\n\nComputational fluid dynamics, usually abbreviated as CFD, is a branch of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. With high-speed supercomputers, better solutions can be achieved. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as transonic or turbulent flows. Initial validation of such software is performed using a wind tunnel with the final validation coming in full-scale testing, e.g. flight tests.\n\nAcoustical engineering is one of many other sub-disciplines of mechanical engineering and is the application of acoustics. Acoustical engineering is the study of Sound and Vibration. These engineers work effectively to reduce noise pollution in mechanical devices and in buildings by soundproofing or removing sources of unwanted noise. The study of acoustics can range from designing a more efficient hearing aid, microphone, headphone, or recording studio to enhancing the sound quality of an orchestra hall. Acoustical engineering also deals with the vibration of different mechanical systems.\n\nManufacturing engineering, aerospace engineering and automotive engineering are grouped with mechanical engineering at times. A bachelor's degree in these areas will typically have a difference of a few specialized classes.\n\n\n\n\n\n\n"}
{"id": "335668", "url": "https://en.wikipedia.org/wiki?curid=335668", "title": "Medical equipment", "text": "Medical equipment\n\nMedical equipment (also known as armamentarium) is designed to aid in the diagnosis, monitoring or treatment of medical conditions.\n\nThere are several basic types:\n\nA biomedical equipment technician (BMET) is a vital component of the healthcare delivery system. Employed primarily by hospitals, BMETs are the people responsible for maintaining a facility's medical equipment. BMET mainly act as an interface between doctor and equipment.\n"}
{"id": "49862426", "url": "https://en.wikipedia.org/wiki?curid=49862426", "title": "Memetic Computing Society", "text": "Memetic Computing Society\n\nThe Memetic Computing Society is a society focusing on research in the area of memetic algorithms and evolutionary computation. The society is located in Singapore.\n\nThe Memetic Computing Society supports the following conferences.\n\n\n\n"}
{"id": "53912255", "url": "https://en.wikipedia.org/wiki?curid=53912255", "title": "Monjolo", "text": "Monjolo\n\nA Monjolo is a primitive hydraulic machine, used for the processing and grinding of grains. It was introduced in Brazil by the Portuguese during the colonial period.\n\nIt can be used to peel and grind dry beans, resulting in a thicker flour.\n\nIt is formed by a wooden beam suspended so that the part that supports the pestle is larger than the other, which ends with a trough. A spout fills the trough with the water, thus raising the pestle. When the trough is full, it lowers the trough, and when the trough spills the water, the beam falls, causing the pestle to hit the mortar. As such, the monjolo is an important tool for agricultural facilitation.\n\nIt is common for rural people to seek to live near a river or stream as a source of water. The monjolo is considered one of the most useful machines for planters of multiple crops.\n\n\n"}
{"id": "34335144", "url": "https://en.wikipedia.org/wiki?curid=34335144", "title": "NanoWorld", "text": "NanoWorld\n"}
{"id": "4653948", "url": "https://en.wikipedia.org/wiki?curid=4653948", "title": "Nanochemistry", "text": "Nanochemistry\n\nNanochemistry is the combination of chemistry and nanoscience. Nanochemistry is associated with synthesis of building blocks which are dependent on size, surface, shape and defect properties. Nanochemistry is being used in chemical, materials and physical, science as well as engineering, biological and medical applications. Nanochemistry and other nanoscience fields have the same core concepts but the usages of those concepts are different.\n\nThe nano prefix was given to nanochemistry when scientists observed the odd changes on materials when they were in nanometer-scale size. Several chemical modification on nanometer scaled structures, approves effects of being size dependent.\n\nNanochemistry can be characterized by concepts of size, shape, self-assembly, defects and bio-nano; So the synthesis of any new nano-construct is associated with all these concepts. Nano-construct synthesis is dependent on how the surface, size and shape will lead to self-assembly of the building blocks into the functional structures; they probably have functional defects and might be useful for electronic, photonic, medical or bioanalytical problems.\n\nSilica, gold, polydimethylsiloxane, cadmium selenide, iron oxide and carbon are materials that show the transformative power of nanochemistry. Nanochemistry can make the most effective contrast agent of MRI out of iron oxide (rust) which has the ability of detecting cancers and even killing them at their initial stages. Silica (glass) can be used to bend or stop light in its tracks. Developing countries also use silicone to make the circuits for the fluids to attain developed world's pathogen detection abilities. Carbon has been used in different shapes and forms and it will become a better choice for electronic materials.\n\nOverall, nanochemistry is not related to the atomic structure of compounds. Rather, it is about different ways to transform materials into solutions to solve problems. Chemistry mainly deals with degrees of freedom of atoms in the periodic table however nanochemistry brought other degrees of freedom that controls material's behaviors.\n\nNanochemical methods can be used to create carbon nanomaterials such as carbon nanotubes (CNT), graphene and fullerenes which have gained attention in recent years due to their remarkable mechanical and electrical properties.\n\nNanotopography refers to the specific surface features which appear on the nanoscale. In industry, applications of nanotopography typically encompass electrics and artificially produced surface features. However, natural surface features are also included in this definition, such as molecular-level cell interactions and the textured organs of animals and plants. These nanotopographical features in nature serve distinctive purposes that aid in regulation and function of the biotic organism, as nanotopographical features are extremely sensitive in cells.\n\nNanolithography is the process by which nanotopographical etchings are artificially produced on a surface. Many practical applications make use of nanolithography, including semiconductor chips in computers. There are many types of nanolithography, which include:\n\n\nEach nanolithography technique has varying factors of resolution, time consumption, and cost. There are three basic methods used by nanolithography. One involves using a resist material which acts as a \"mask\" to cover and protect the areas of the surface that are intended to be smooth. The uncovered portions can now be etched away, with the protective material acting as a stencil. The second method involves directly carving the desired pattern. Etching may involve using a beam of quantum particles, such as electrons or light, or chemical methods such as oxidation or SAM's (self-assembled monolayers). The third method places the desired pattern directly on the surface, producing a final product that is ultimately a few nanometers thicker than the original surface. In order to visualize the surface to be fabricated, the surface must be visualized by a nano-resolution microscope, which include the scanning probe microscope (SPM) and the atomic force microscope (AFM). Both microscopes can also be engaged in processing the final product.\n\nOne of the methods of nanolithography is use of self-assembled monolayers (SAM) which develops soft methodology. SAMs are long chain alkanethiolates that are self-assembled on gold surfaces making a well-ordered monolayer films. The advantage of this method is to create a high quality structure with lateral dimensions of 5 nm to 500 nm. In this methodology a patterned elastomer made of polydimethylsiloxane (PDMS) as a mask is usually used. In order to make a PDMS stamp, the first step is to coat a thin layer of photoresist onto a silicon wafer. The next step is to expose the layer with UV light, and the exposed photoresist is washed away with developer. In order to reduce the thickness of the prepolymer, the patterned master is treated with perfluoroalkyltrichlorosilane. These PDMS elastomers are used to print micron and submicron design chemical inks on both planar and curved surfaces for different purposes.\n\nOne highly researched application of nanochemistry is medicine. A simple skin-care product using the technology of nanochemistry is sunscreen. Sunscreen contains nanoparticles of zinc oxide and titanium dioxide. These nanochemicals protect the skin against harmful UV light by absorbing or reflecting the light and prevent the skin from retaining full damage by photoexcitation of electrons in the nanoparticle. Effectively, the excitation of the particle blocks skin cells from DNA damage.\n\nEmerging methods of drug delivery involving nanotechnological methods can be advantageous by improving increased bodily response, specific targeting, and efficient, non-toxic metabolism. Many nanotechnological methods and materials can be functionalized for drug delivery. Ideal materials employ a controlled-activation nanomaterial to carry a drug cargo into the body. Mesoporous silica nanoparticles (MSN) have been increasing in research popularity due to its large surface area and flexibility for various individual modifications while demonstrating high resolution performance under imaging techniques. Activation methods greatly vary across nanoscale drug delivery molecules, but the most commonly used activation method uses specific wavelengths of light to release the cargo. Nanovalve-controlled cargo release uses low intensity light and plasmonic heating to release the cargo in a variation of MSN containing gold molecules. The two-photon activated photo-transducer (2-NPT) uses near IR wavelengths of light to induce breaking of a disulfide bond to release the cargo. Recently, nanodiamonds have demonstrated potential in drug delivery due to non-toxicity, spontaneous absorption through the skin, and ability to enter the blood-brain barrier.\n\nBecause cells are very sensitive to nanotopographical features, optimization of surfaces in tissue engineering has pushed the frontiers towards implantation. Under the appropriate conditions, a carefully crafted 3-dimensional scaffold is used to direct cell seeds towards artificial organ growth. The 3-D scaffold incorporates various nanoscale factors that control the environment for optimal and appropriate functionality. The scaffold is an analog of the \"in vivo\" extracellular matrix \"in vitro\", allowing for successful artificial organ growth by providing the necessary, complex biological factors \"in vitro\". Additional advantages include the possibility of cell expression manipulation, adhesion, and drug delivery.\n\nFor abrasions and wounds, nanochemistry has demonstrated applications in improving the healing process. Electrospinning is a polymerization method used biologically in tissue engineering, but can be functionalized for wound dressing as well as drug delivery. This produces nanofibers which encourage cell proliferation, antibacterial properties, and controlled environment. These properties have been created in macroscale; however, nanoscale versions may show improved efficiency due to nanotopographical features. Targeted interfaces between nanofibers and wounds have higher surface area interactions and are advantageously \"in vivo\".\n\nThere is evidence certain nanoparticles of silver are useful to inhibit some viruses and bacteria.\n\nNew developments in nanochemistry provide a variety of nanostructure materials with significant properties that are highly controlable. Some of the application of these nanostructure materials include SAMs and lithography, use of nanowires in sensors, and nanoenzymes.\n\nScientist also devised a large number of nanowire compositions with controlled length, diameter, doping, and surface structure by using vapor and solution phase strategies. These oriented single crystals are being used in semiconductor nanowire devices such as diodes, transistors, logic circuits, lasers and sensors. Since nanowires have one dimensional structure meaning large surface to volume ratio, the diffusion resistance decreases. In addition, their efficiency in electron transport which is due to the quantum confinement effect, make their electrical properties be influenced by minor perturbation. Therefore, use of these nanowires in nanosensor elements increases the sensitivity in electrode response. As mentioned above, one dimensionality and chemical flexibility of the semiconductor nanowires make them applicable in nanolasers. Peidong Yang and his co-workers have done some research on room-temperature ultraviolet nanowire nanolasers in which the significant properties of these nanolasers have been mentioned. They have concluded that using short wavelength nanolasers have applications in different fields such as optical computing, information storage, and microanalysis.\n\nNanostructure materials mainly used in nanoparticle-based enzymes have drawn attraction due to the specific properties they show. Very small size of these nanoenzymes (or nanozymes) (1–100 nm) have provided them unique optical, magnetic, electronic, and catalytic properties. Moreover, the control of surface functionality of nano particles and predictable nanostructure of these small sized enzymes have made them to create a complex structure on their surface which in turn meet the needs of specific applications\n\nFluorescent nanoparticles have broad applications, but their use into macroscopic arrays allows them to be used efficiently in applications of plasmonics, photonics and quantum communications that makes them highly sought after. While there are many methods in assembling nanoparticles array, especially gold nanoparticles, they tend to be weakly bonded to their substrate so it can’t be used for wet chemistry processing steps or lithography. Nanodiamonds allow for a greater variability in access that can subsequently be used to couple plasmonic waveguides to realize quantum plasmonics circuitry.\n\nNanodiamonds can be synthesized by employing nanoscale carbonaceous seeds that are fabricated by a single step using a mask-free electron beam induced position technique to add amine groups to self-assemble nanodiamonds into arrays. The presence of dangling bonds at the nanodiamond surface allows them to be functionalized with a variety of ligands. The surfaces of these nanodiamonds are terminated with carboxylic acid groups, enabling their attachment to amine-terminated surfaces through carbodiimide coupling chemistry. This process gives a high yield do that this method relies on covalent bonding between the amine and carboxyl functional groups on amorphous carbon and nanodiamond surfaces in the presence of EDC. Thus unlike gold nanoparticle they can withstand processing and treatment, for many device applications.\n\nFluorescent properties in nanodiamonds arise from the presence of nitrogen vacancy (NV) centers, nitrogen atom next to a vacancy. NV centres can be created by irradiating nanodiamond with high-energy particles (electrons, protons, helium ions), followed by vacuum-annealing at 600–800 °C. Irradiation forms vaccines in the diamond structure while vacuum-annealing migrates these vacancies, which will get trapped by nitrogen atoms within the nanodiamond. This process produces two types of NV centers. Two types of NV centers are formed—neutral (NV0) and negatively charged (NV–)—and these have different emission spectra. The NV– centre is of particular interest because it has an \"S\" = 1 spin ground state that can be spin-polarized by optical pumping and manipulated using electron paramagnetic resonance. Fluorescent nanodiamonds combine the advantages of semiconductor quantum dots (small size, high photostability, bright multicolor fluorescence) with biocompatibility, non-toxicity and rich surface chemistry, which means that they have the potential to revolutionize \"in vivo\" imaging application.\n\nNanodiamonds have the ability to self-assemble and a wide range of small molecules, proteins antibodies, therapeutics and nucleic acids can bind to its surface allow for drug delivery, protein-mimicking and surgical implants. Other potential biomedical applications are the use of nanodiamonds as a support for solid-phase peptide synthesis and as sorbents for detoxification and separation and fluorescent nanodiamonds for biomedical imaging. Nanodiamonds are capable of biocompatibility, the ability to carry a broad range of therapeutics, dispersibility in water and scalability and thee potential for targeted therapy all properties needed for a drug delivery platform. The small size, stable core, rich surface chemistry, ability to self-assemble and low cytotoxicity of nanodiamonds have led to suggestions that they could be used to mimic globular proteins. Nanodiamonds have been mostly studied as potential injectable therapeutic agents for generalized drug delivery, but it has also been shown that films of parylene nanodiamond composites can be used for localized sustained release of drugs over periods ranging from two days to one month.\n\nMonodispurse, nanometer-size clusters (also known as nanoclusters) are synthetically grown crystals whose size and structure influence their properties through the effects of quantum confinement. One method of growing these crystals is through inverse micellar cages in non aqueous solvents. Research conducted on the optical properties of MoS nanoclusters compared them to their bulk crystal counterparts and analyzed their absorbance spectra. The analysis reveals that size dependence of the absorbance spectrum by bulk crystals is continuous, whereas the absorbance spectrum of nanoclusters takes on discrete energy levels. This indicates a shift from solid-like to molecular-like behavior which occurs at a reported cluster size of 4.5 – 3.0 nm.\n\nInterest in the magnetic properties of nanoclusters exists due to their potential use in magnetic recording, magnetic fluids, permanent magnets, and catalysis. Analysis of Fe clusters shows behavior consistent with ferromagnetic or superparamagnetic behavior due to strong magnetic interactions within clusters.\n\nDielectric properties of nanoclusters are also a subject interest due to their possible applications in catalysis, photocatalysis, microcapacitors, microelectronics, and nonlinear optics.\n\nThere are several researchers in nanochemistry that have been credited with development of the field. Geoffrey A. Ozin, from the University of Toronto, is known as one of the \"founding fathers of Nanochemistry\" due to his four and a half decades of research on this subject. This research includes the study of Matrix isolation laser Raman spectroscopy, naked metal clusters chemistry and photochemistry, nanoporous materials, hybrid nanomaterials, mesoscopic materials, and ultrathin inorganic nanowires.\n\nAnother chemist who is also viewed as one of nanochemistry's pioneers is Charles M. Lieber at Harvard University. He is known for his contributions in the development of nano-scale technologies, particularly in the field of biology and medicine. The technologies include nanowires, a new class of quasi-one dimensional materials that have demonstrated superior electrical, optical, mechanical, and thermal properties and can be used potentially as biological sensors. Research under Lieber has delved into the use of nanowires for the purpose of mapping brain activity.\n\nShimon Weiss, a professor at the University of California, Los Angeles, is known for his research of fluorescent semiconductior nanocrystals, a subclass of quantum dots, for the purpose of biological labeling. Paul Alivisatos, from the University of California Berkley, is also notable for his research on the fabrication and use of nanocrystals. This research has the potential to develop insight into the mechanisms of small scale particles such as the process of nucleation, cation exchange, and branching. A notable application of these crystals is the development of quantum dots.\n\nPeidong Yang, another researcher from the University of California, Berkley, is also notable for his contributions to the development of 1-dimensional nanostructures. Currently, the Yang group has active research projects in the areas of nanowire photonics, nanowire-based solar cells, nanowires for solar to fuel conversion, nanowire thermoelectrics, nanowire-cell interface, nanocrystal catalysis, nanotube nanofluidics, and plasmonics.\n\n"}
{"id": "22102", "url": "https://en.wikipedia.org/wiki?curid=22102", "title": "Naval mine", "text": "Naval mine\n\nA naval mine is a self-contained explosive device placed in water to damage or destroy surface ships or submarines. Unlike depth charges, mines are deposited and left to wait until they are triggered by the approach of, or contact with, any vessel. Naval mines can be used offensively, to hamper enemy shipping movements or lock vessels into a harbour; or defensively, to protect friendly vessels and create \"safe\" zones.\n\nMines can be laid in many ways: by purpose-built minelayers, refitted ships, submarines, or aircraft—and even by dropping them into a harbour by hand. They can be inexpensive: some variants can cost as little as US$2000, though more sophisticated mines can cost millions of dollars, be equipped with several kinds of sensors, and deliver a warhead by rocket or torpedo.\n\nTheir flexibility and cost-effectiveness make mines attractive to the less powerful belligerent in asymmetric warfare. The cost of producing and laying a mine is usually between 0.5% and 10% of the cost of removing it, and it can take up to 200 times as long to clear a minefield as to lay it. Parts of some World War II naval minefields still exist because they are too extensive and expensive to clear. It is possible for some of these 1940s-era mines to remain dangerous for many years to come.\n\nMines have been employed as offensive or defensive weapons in rivers, lakes, estuaries, seas, and oceans, but they can also be used as tools of psychological warfare. Offensive mines are placed in enemy waters, outside harbours and across important shipping routes with the aim of sinking both merchant and military vessels. Defensive minefields safeguard key stretches of coast from enemy ships and submarines, forcing them into more easily defended areas, or keeping them away from sensitive ones.\n\nMinefields designed for psychological effect are usually placed on trade routes and are used to stop shipping from reaching an enemy nation. They are often spread thinly, to create an impression of minefields existing across large areas. A single mine inserted strategically on a shipping route can stop maritime movements for days while the entire area is swept.\n\nInternational law requires nations to declare when they mine an area, to make it easier for civil shipping to avoid the mines. The warnings do not have to be specific; for example, during World War II, Britain declared simply that it had mined the English Channel, North Sea and French coast.\n\nPrecursors to naval mines were first invented by Han Chinese innovators of Imperial China and were described in thorough detail by the early Ming dynasty artillery officer Jiao Yu, in his 14th century military treatise known as the \"Huolongjing\". Chinese records tell of naval explosives in the 16th century, used to fight against Japanese pirates (\"wokou\"). This kind of naval mine was loaded in a wooden box, sealed with putty. General Qi Jiguang made several timed, drifting explosives, to harass Japanese pirate ships. The \"Tiangong Kaiwu\" (\"The Exploitation of the Works of Nature\") treatise, written by Song Yingxing in 1637 AD, describes naval mines with a rip cord pulled by hidden ambushers located on the nearby shore who rotated a steel wheellock flint mechanism to produce sparks and ignite the fuse of the naval mine. Although this is the rotating steel wheellock's first use in naval mines, Jiao Yu had described their use for land mines back in the 14th century.\n\nThe first plan for a sea mine in the West was by Ralph Rabbards, who presented his design to Queen Elizabeth I of England in 1574. The Dutch inventor Cornelius Drebbel was employed in the Office of Ordnance by King Charles I of England to make weapons, including a \"floating petard\" which proved a failure. Weapons of this type were apparently tried by the English at the Siege of La Rochelle in 1627.\nAmerican David Bushnell developed the first American naval mine for use against the British in the American War of Independence. It was a watertight keg filled with gunpowder that was floated toward the enemy, detonated by a sparking mechanism if it struck a ship. It was used on the Delaware River as a drift mine.\n\nIn 1812 Russian engineer Pavel Shilling exploded an underwater mine using an electrical circuit. In 1854, during the unsuccessful attempt of the Anglo-French fleet to seize the Kronstadt fortress, British steamships HMS \"Merlin\" (9 June 1855, the first successful mining in history), HMS \"Vulture\" and HMS \"Firefly\" suffered damage due to the underwater explosions of Russian naval mines. Russian naval specialists set more than 1500 naval mines, or \"infernal machines\", designed by Moritz von Jacobi and by Immanuel Nobel,\nin the Gulf of Finland during the Crimean War of 1853-1856. The mining of \"Vulcan\" led to the world's first minesweeping operation.\nDuring the next 72 hours, 33 mines were swept.\n\nThe Jacobi mine was designed by German-born, Russian engineer Jacobi, in 1853. The mine was tied to the sea bottom by an anchor. A cable connected it to a galvanic cell which powered it from the shore, the power of its explosive charge was equal to of black powder. In the summer of 1853, the production of the mine was approved by the Committee for Mines of the Ministry of War of the Russian Empire. In 1854, 60 Jacobi mines were laid in the vicinity of the Forts Pavel and Alexander (Kronstadt), to deter the British Baltic Fleet from attacking them. It gradually phased out its direct competitor the Nobel mine on the insistence of Admiral Fyodor Litke. The Nobel mines were bought from Swedish industrialist Immanuel Nobel who had entered into collusion with Russian head of navy Alexander Sergeyevich Menshikov. Despite their high cost (100 Russian rubles) the Nobel mines proved to be faulty, exploding while being laid, failing to explode or detaching from their wires and drifting uncontrollably, at least 70 of them were subsequently disarmed by the British. In 1855, 301 more Jacobi mines were laid around Krostadt and Lisy Nos. British ships did not dare to approach them.\n\nIn the 19th century, mines were called torpedoes, a name probably conferred by Robert Fulton after the torpedo fish, which gives powerful electric shocks. A spar torpedo was a mine attached to a long pole and detonated when the ship carrying it rammed another one and withdrew a safe distance. The submarine used one to sink on 17 February 1864. A Harvey torpedo was a type of floating mine towed alongside a ship and was briefly in service in the Royal Navy in the 1870s. Other \"torpedoes\" were attached to ships or propelled themselves. One such weapon called the Whitehead torpedo after its inventor, caused the word \"torpedo\" to apply to self-propelled underwater missiles as well as to static devices. These mobile devices were also known as \"fish torpedos\".\n\nThe American Civil War of 1861-1865 also saw the successful use of mines. The first ship sunk by a mine, , foundered in 1862 in the Yazoo River. Rear Admiral David Farragut's famous/apocryphal command during the Battle of Mobile Bay in 1864, \"Damn the torpedoes, full speed ahead!\" refers to a minefield laid at Mobile, Alabama.\n\nAfter 1865 the United States adopted the mine as its primary weapon for coastal defense. In the decade following 1868, Major Henry Larcom Abbot carried out a lengthy set of experiments to design and test moored mines that could be exploded on contact or be detonated at will as enemy shipping passed near them. This initial development of mines in the United States took place under the purview of the U.S. Army Corps of Engineers, which trained officers and men in their use at the Engineer School of Application at Willets Point, New York (later named Fort Totten). In 1901 underwater minefields became the responsibility of the US Army's Artillery Corps, and in 1907 this was a founding responsibility of the United States Army Coast Artillery Corps.\n\nThe Imperial Russian Navy, a pioneer in mine warfare, successfully deployed mines against the Ottoman Navy during both the Crimean War and the Russo-Turkish War (1877-1878).\n\nDuring the Battle of Tamsui (1884), in the Keelung Campaign of the Sino-French War, Chinese forces in Taiwan under Liu Mingchuan took measures to reinforce Tamsui against the French; they planted nine torpedo mines in the river and blocked the entrance.\n\nDuring the Boxer Rebellion, Imperial Chinese forces deployed a command-detonated mine field at the mouth of the Peiho river before the Dagu forts, to prevent the western Allied forces from sending ships to attack.\n\nThe next major use of mines was during the Russo-Japanese War of 1904–1905. Two mines blew up when the struck them near Port Arthur, sending the holed vessel to the bottom and killing the fleet commander, Admiral Stepan Makarov, and most of his crew in the process. The toll inflicted by mines was not confined to the Russians, however. The Japanese Navy lost two battleships, four cruisers, two destroyers and a torpedo-boat to offensively laid mines during the war. Most famously, on May 15, 1904, the Russian minelayer \"Amur\" planted a 50-mine minefield off Port Arthur and succeeded in sinking the Japanese battleships and .\n\nFollowing the end of the Russo-Japanese War, several nations attempted to have mines banned as weapons of war at the Hague Peace Conference (1907).\n\nMany early mines were fragile and dangerous to handle, as they contained glass containers filled with nitroglycerin or mechanical devices that activated a blast upon tipping. Several mine-laying ships were destroyed when their cargo exploded.\n\nBeginning around the start of the 20th century, submarine mines played a major role in the defense of U.S. harbors against enemy attack as part of the Endicott and Taft Programs. The mines employed were controlled mines, anchored to the bottoms of the harbors and detonated under control from large mine casemates on shore.\n\nDuring World War I, mines were used extensively to defend coasts, coastal shipping, ports and naval bases around the globe. The Germans laid mines in shipping lanes to sink merchant and naval vessels serving Britain. The Allies targeted the German U-boats in the Strait of Dover and the Hebrides. In an attempt to seal up the northern exits of the North Sea, the Allies developed the North Sea Mine Barrage. During a period of five months from June 1918 almost 70,000 mines were laid spanning the North Sea's northern exits. The total number of mines laid in the North Sea, the British East Coast, Straits of Dover, and Heligoland Bight is estimated at 190,000 and the total number during the whole of WWI was 235,000 sea mines. Clearing the barrage after the war took 82 ships and five months, working around the clock. It was also during World War I, that the naval mine sunk its largest vessel ever, the British hospital ship, HMHS \"Britannic\", which was the sister ship of the \"Titanic\".\n\nDuring World War II, the U-boat fleet, which dominated much of the battle of the Atlantic, was small at the beginning of the war and much of the early action by German forces involved mining convoy routes and ports around Britain. German submarines also operated in the Mediterranean Sea, in the Caribbean Sea, and along the U.S. coast.\n\nInitially, contact mines—requiring a ship to physically strike a mine to detonate it—were employed, usually tethered at the end of a cable just below the surface of the water. Contact mines usually blew a hole in ships' hulls. By the beginning of World War II, most nations had developed mines that could be dropped from aircraft and floated on the surface, making it possible to lay them in enemy harbours. The use of dredging and nets was effective against this type of mine, but this consumed valuable time and resources, and required harbours to be closed.\n\nLater, some ships survived mine blasts, limping into port with buckled plates and broken backs. This appeared to be due to a new type of mine, detecting ships by their proximity to the mine (an influence mine) and detonating at a distance, causing damage with the shock wave of the explosion. Ships that had successfully run the gauntlet of the Atlantic crossing were sometimes destroyed entering freshly cleared British harbours. More shipping was being lost than could be replaced, and Churchill ordered the intact recovery of one of these new mines to be of the highest priority.\n\nThe British experienced a stroke of luck in November 1939 when a German mine was dropped from an aircraft onto the mud flats off Shoeburyness during low tide. Additionally, the land belonged to the army and a base with men and workshops was at hand. Experts were dispatched from HMS Vernon to investigate the mine. They had some idea that the mines could use magnetic sensors, so everyone removed all metal, including their buttons, and made tools of non-magnetic brass. They disarmed the mine and rushed it to labs at HMS Vernon, where scientists discovered a new type of arming mechanism. A large ferrous object passing through the Earth's magnetic field will concentrate the field through it; the mine's detector was designed to trigger as a ship passed over, when its magnetic field was concentrated as measured by the mine. The mechanism had an adjustable sensitivity, calibrated in milligauss. (As it turned out, the German firing mechanism was overly sensitive, making sweeping easier.) The U.S. began adding delay counters to their magnetic mines in June 1945.\nFrom this data, methods were developed to clear the mines. Early methods included the use of large electromagnets dragged behind ships or below low-flying aircraft (a number of older bombers like the Vickers Wellington were used for this). Both of these methods had the disadvantage of \"sweeping\" only a small strip. A better solution was found in the \"Double-L Sweep\" using electrical cables dragged behind ships that passed large pulses of current through the seawater. This induced a large magnetic field and swept the entire area between the two ships. The older methods continued to be used in smaller areas. The Suez Canal continued to be swept by aircraft, for instance. Wartime Japanese sweep methods, by contrast, never advanced much past 1930s standards, and failed entirely to keep up with new American mines, clearing no more than 15% of all the mines laid in Japan's coastal waters. Moreover, IJN's minesweeping force was far too small with 350 ships, and 20,000 men.\n\nWhile these methods were useful for clearing mines from local ports, they were of little or no use for enemy-controlled areas. These were typically visited by warships, and the majority of the fleet then underwent a massive degaussing process, where their hulls had a slight \"south\" bias induced into them which offset the concentration effect almost to zero.\n\nInitially, major warships and large troopships had a copper \"degaussing coil\" fitted around the perimeter of the hull, energized by the ship's electrical system whenever in suspected magnetic-mined waters. Some of the first to be so fitted were the carrier HMS \"Ark Royal\" and the liners and . This was felt to be impracticable for the myriad of smaller warships and merchant vessels, mainly because the ships lacked the generating capacity to energise such a coil. It was found that \"wiping\" a current-carrying cable up and down a ship's hull temporarily cancelled the ships' magnetic signature sufficiently to nullify the threat. This started in late 1939, and by 1940 merchant vessels and the smaller British warships were largely immune for a few months at a time until they once again built up a field. Many of the boats that sailed to Dunkirk were degaussed in a marathon four-day effort by degaussing stations.\n\nThe Allies deployed acoustic mines, against which even wooden-hulled ships (in particular minesweepers) remained vulnerable. Japan developed sonic generators to sweep these; the gear was not ready by war's end. The primary method Japan used was small air-delivered bombs. This was profligate and ineffectual; used against acoustic mines at Penang, 200 bombs were needed to detonate just 13 mines.\n\nThe Germans had also developed a pressure-activated mine and planned to deploy it as well, but they saved it for later use when it became clear the British had defeated the magnetic system. The U.S. also deployed these, adding \"counters\" which would allow a variable number of ships to pass unharmed before detonating. This made them a great deal harder to sweep. Japan's antiquated sweep methods, lifting mines in nets, accidentally proved useful against these mines; it remained too slow and hazardous to be truly effective, especially in light of the high numbers being laid.\n\nMining campaigns could have devastating consequences. The U.S. effort against Japan, for instance, closed major ports, such as Hiroshima, for days, and by the end of the Pacific War had cut the amount of freight passing through Kobe–Yokohama by 90%.\n\nWhen the war ended, more than 25,000 U.S.-laid mines were still in place, and the Navy proved unable to sweep them all, limiting efforts to critical areas. After sweeping for almost a year, in May 1946, the Navy abandoned the effort with 13,000 mines still unswept. Over the next thirty years, more than 500 minesweepers (of a variety of types) were damaged or sunk clearing them.\n\nSince World War II, mines have damaged 14 United States Navy ships, whereas air and missile attacks have damaged four. During the Korean War, mines laid by North Korean forces caused 70% of the casualties suffered by U.S. naval vessels and caused 4 sinkings.\n\nDuring the Iran–Iraq War from 1980 to 1988, the belligerents mined several areas of the Persian Gulf and nearby waters. On 24 July 1987, the supertanker \"SS\" Bridgeton was mined by Iran near Farsi Island. On 14 April 1988, struck an Iranian M-08/39 mine in the central Persian Gulf shipping lane, wounding 10 sailors.\n\nIn the summer of 1984, magnetic sea mines damaged at least 19 ships in the Red Sea. The U.S. concluded Libya was probably responsible for the minelaying. In response the U.S., Britain, France, and three other nations launched Operation Intense Look, a minesweeping operation in the Red Sea involving more than 46 ships.\n\nOn the orders of the Reagan administration, the CIA mined Nicaragua's Sandino port in 1984 in support of the Contra guerrilla group. A Soviet tanker was among the ships damaged by these mines. In 1986, in the case of \"Nicaragua v. United States\", the International Court of Justice ruled that this mining was a violation of international law.\n\nDuring the Gulf War, Iraqi naval mines severely damaged and . When the war concluded, eight countries conducted clearance operations.\n\nNaval mines may be classified into three major groups; contact, remote and influence mines.\n\nThe earliest mines were usually of this type. They are still used today, as they are extremely low cost compared to any other anti-ship weapon and are effective, both as a psychological weapon and as a method to sink enemy ships. Contact mines need to be touched by the target before they detonate, limiting the damage to the direct effects of the explosion and usually affecting only the vessel that triggers them.\n\nEarly mines had mechanical mechanisms to detonate them, but these were superseded in the 1870s by the \"Hertz horn\" (or \"chemical horn\"), which was found to work reliably even after the mine had been in the sea for several years. The mine's upper half is studded with hollow lead protuberances, each containing a glass vial filled with sulfuric acid. When a ship's hull crushes the metal horn, it cracks the vial inside it, allowing the acid to run down a tube and into a lead–acid battery which until then contained no acid electrolyte. This energizes the battery, which detonates the explosive.\n\nEarlier forms of the detonator employed a vial of sulfuric acid surrounded by a mixture of potassium perchlorate and sugar. When the vial was crushed, the acid ignited the perchlorate-sugar mix, and the resulting flame ignited the gunpowder charge.\n\nDuring the initial period of World War I, the Royal Navy used contact mines in the English Channel and later in large areas of the North Sea to hinder patrols by German submarines. Later, the American antenna mine was widely used because submarines could be at any depth from the surface to the seabed. This type of mine had a copper wire attached to a buoy that floated above the explosive charge which was weighted to the seabed with a steel cable. If a submarine's steel hull touched the copper wire, the slight voltage change caused by contact between two dissimilar metals was amplified and detonated the explosives.\n\nLimpet mines are a special form of contact mine that are manually attached to the target by magnets and remain in place. They are named because of the similarity to the limpet, a mollusk.\n\nGenerally, this mine type is set to float just below the surface of the water or as deep as five meters. A steel cable connecting the mine to an anchor on the seabed prevents it from drifting away. The explosive and detonating mechanism is contained in a buoyant metal or plastic shell. The depth below the surface at which the mine floats can be set so that only deep draft vessels such as aircraft carriers, battleships or large cargo ships are at risk, saving the mine from being used on a less valuable target. In littoral waters it is important to ensure that the mine does not become visible when the sea level falls at low tide, so the cable length is adjusted to take account of tides. During WWII there were mines that could be moored in 300m-deep water (Example: The U.S. Mark 6).\n\nFloating mines typically have a mass of around 200 kg, including 80 kg of explosives e.g. TNT, minol or amatol.\n\nDrifting mines were occasionally used during World War I and World War II. However, they were more feared than effective. Sometimes floating mines break from their moorings and become drifting mines; modern mines are designed to deactivate in this event. After several years at sea, the deactivation mechanism might not function as intended and the mines may remain live. Admiral Jellicoe's British fleet did not pursue and destroy the outnumbered German High Seas Fleet when it turned away at the Battle of Jutland because he thought they were leading him into a trap: he believed it possible that the Germans were either leaving floating mines in their wake, or were drawing him towards submarines, although neither of these was the case.\n\nAfter World War I the drifting contact mine was banned, but was occasionally used during World War II. The drifting mines were much harder to remove than tethered mines after the war, and they caused about the same damage to both sides.\n\nChurchill promoted \"Operation Royal Marine\" in 1940 and again in 1944 where floating mines were put into the Rhine in France to float down the river, becoming active after a time calculated to be long enough to reach German territory.\n\nFrequently used in combination with coastal artillery and hydrophones, controlled mines (or command detonation mines) can be in place in peacetime, which is a huge advantage in blocking important shipping routes. The mines can usually be turned into \"normal\" mines with a switch (which prevents the enemy from simply capturing the controlling station and deactivating the mines), detonated on a signal or be allowed to detonate on their own. The earliest ones were developed around 1812 by Robert Fulton. The first remotely controlled mines were moored mines used in the American Civil War, detonated electrically from shore. They were considered superior to contact mines because they did not put friendly shipping at risk.\n\nModern examples usually weigh , including of explosives (TNT or hexatonal).\n\nThese mines are triggered by the influence of a ship or submarine, rather than direct contact. Such mines incorporate electronic sensors designed to detect the presence of a vessel and detonate when it comes within the blast range of the warhead. The fuzes on such mines may incorporate one or more of the following sensors: magnetic, passive acoustic or water pressure displacement caused by the proximity of a vessel.\n\nFirst used during WWI, their use became more general in WWII. The sophistication of influence mine fuzes has increased considerably over the years as first transistors and then microprocessors have been incorporated into designs. Simple magnetic sensors have been superseded by total-field magnetometers. Whereas early magnetic mine fuzes would respond only to changes in a single component of a target vessel's magnetic field, a total field magnetometer responds to changes in the magnitude of the total background field (thus enabling it to better detect even degaussed ships). Similarly, the original broadband hydrophones of 1940s acoustic mines (which operate on the integrated volume of all frequencies) have been replaced by narrow-band sensors which are much more sensitive and selective. Mines can now be programmed to listen for highly specific acoustic signatures (e.g. a gas turbine powerplant or cavitation sounds from a particular design of propeller) and ignore all others. The sophistication of modern electronic mine fuzes incorporating these digital signal processing capabilities makes it much more difficult to detonate the mine with electronic countermeasures because several sensors working together (e.g. magnetic, passive acoustic and water pressure) allow it to ignore signals which are not recognised as being the unique signature of an intended target vessel.\n\nModern influence mines such as the BAE Stonefish are computerised, with all the programmability this implies, such as the ability to quickly load new acoustic signatures into fuzes, or program them to detect a single, highly distinctive target signature. In this way, a mine with a passive acoustic fuze can be programmed to ignore all friendly vessels and small enemy vessels, only detonating when a very large enemy target passes over it. Alternatively, the mine can be programmed specifically to ignore all surface vessels regardless of size and exclusively target submarines.\n\nEven as far back as WWII it was possible to incorporate a \"ship counter\" function in mine fuzes. This might set the mine to ignore the first two ships passing over it (which could be minesweepers deliberately trying to trigger mines) but detonate when the third ship passes overhead, which could be a high-value target such as an aircraft carrier or oil tanker. Even though modern mines are generally powered by a long life lithium battery, it is important to conserve power because they may need to remain active for months or even years. For this reason, most influence mines are designed to remain in a semi-dormant state until an unpowered (e.g. deflection of a mu-metal needle) or low-powered sensor detects the possible presence of a vessel, at which point the mine fuze powers up fully and the passive acoustic sensors will begin to operate for some minutes. It is possible to program computerised mines to delay activation for days or weeks after being laid. Similarly, they can be programmed to self-destruct or render themselves safe after a preset period of time. Generally, the more sophisticated the mine design, the more likely it is to have some form of anti-handling device to hinder clearance by divers or remotely piloted submersibles.\n\nThe moored mine is the backbone of modern mine systems. They are deployed where water is too deep for bottom mines. They can use several kinds of instruments to detect an enemy, usually a combination of acoustic, magnetic and pressure sensors, or more sophisticated optical shadows or electro potential sensors. These cost many times more than contact mines. Moored mines are effective against most kinds of ships. As they are cheaper than other anti-ship weapons they can be deployed in large numbers, making them useful area denial or \"channelizing\" weapons.\nMoored mines usually have lifetimes of more than 10 years, and some almost unlimited. These mines usually weigh , including of explosives (RDX). In excess of of explosives the mine becomes inefficient, as it becomes too large to handle and the extra explosives add little to the mine's effectiveness.\n\nBottom mines are used when the water is no more than deep or when mining for submarines down to around . They are much harder to detect and sweep, and can carry a much larger warhead than a moored mine. Bottom mines commonly utilize multiple types of sensors, which are less sensitive to sweeping.\n\nThese mines usually weigh between , including between of explosives.\n\nSeveral specialized mines have been developed for other purposes than the common minefield.\n\nThe bouquet mine is a single anchor attached to several floating mines. It is designed so that when one mine is swept or detonated, another takes its place. It is a very sensitive construction and lacks reliability.\n\nThe anti-sweep mine is a very small mine (40 kg warhead) with as small a floating device as possible. When the wire of a mine sweep hits the mine, it sinks, letting the sweep wire drag along the anchoring wire of the mine until the sweep hits the mine. That detonates the mine and cuts the sweeping wire. They are very cheap and usually used in combination with other mines in a minefield to make sweeping more difficult. One type is the Mark 23 used by the United States during World War II.\n\nThe mine is hydrostatically controlled to maintain a pre-set depth below the water's surface independently of the rise and fall of the tide.\n\nThe ascending mine is a floating distance mine that may cut its mooring or in some other way float higher when it detects a target. It lets a single floating mine cover a much larger depth range.\n\nThese are mines containing a moving weapon as a warhead, either a torpedo or a rocket.\n\nRocket mine: a Russian invention, the rocket mine is a bottom distance mine that fires a homing high-speed rocket (not torpedo) upwards towards the target. It is intended to allow a bottom mine to attack surface ships as well as submarines from a greater depth. One type is the Te-1 rocket propelled mine.\n\nTorpedo mine: the torpedo mine is a self-propelled variety, able to lie in wait for a target and then pursue it e.g. the Mark 60 CAPTOR. Generally, torpedo mines incorporate computerised acoustic and magnetic fuzes. The U.S. Mark 24 \"mine\", code-named Fido, was actually an ASW homing torpedo. The mine designation was disinformation to conceal its function.\n\nThe mine is propelled to its intended position by propulsion equipment such as a torpedo. After reaching its destination, it sinks to the seabed and operates like a standard mine. It differs from the homing mine in that its mobile stage is before it lays in wait, rather than as part of the attacking phase.\n\nOne such design is the Mk 67 submarine launched mobile mine (which is based on a Mark 37 torpedo) are capable of travelling as far as 10 miles through or into a channel, harbor, shallow water area and other zones which would normally be inaccessible to craft laying the device. After reaching the target area they sink to the sea bed and act like conventionally laid influence mines.\n\nDuring the Cold War a test was conducted with naval mine fitted with tactical nuclear warheads for the \"Baker\" shot of Operation Crossroads. This weapon was experimental and never went into production. There have been some reports that North Korea may be developing a nuclear mine The Seabed Arms Control Treaty prohibits the placement of nuclear weapons on the seabed beyond a 12-mile coast zone.\n\nThis comprises two moored, floating contact mines which are tethered together by a length of steel cable or chain. Typically, each mine is situated approximately away from its neighbour, and each floats a few metres below the surface of the ocean. When the target ship hits the steel cable, the mines on either side are drawn down the side of the ship's hull, exploding on contact. In this manner it is almost impossible for target ships to pass safely between two individually moored mines. Daisy-chained mines are a very simple concept which was used during World War II.\n\nPlastic drums filled with sand or concrete are periodically rolled off the side of ships as real mines are laid in large mine-fields. These inexpensive false targets (designed to be of a similar shape and size as genuine mines) are intended to slow down the process of mine clearance: a mine-hunter is forced to investigate each suspicious sonar contact on the sea bed, whether it is real or not. Often a maker of naval mines will provide both training and dummy versions of their mines.\n\nHistorically several methods were used to lay mines. During WWI and WWII, the Germans used U-boats to lay mines around the UK. In WWII, aircraft came into favour for mine laying with one of the largest examples being the mining of the Japanese sea routes in Operation Starvation.\n\nLaying a minefield is a relatively fast process with specialized ships, which is today the most common method. These minelayers can carry several thousand mines and manoeuvre with high precision. The mines are dropped at predefined intervals into the water behind the ship. Each mine is recorded for later clearing, but it is not unusual for these records to be lost together with the ships. Therefore, many countries demand that all mining operations be planned on land and records kept so that the mines can later be recovered more easily.\n\nOther methods to lay minefields include:\n\n\nIn some cases, mines are automatically activated upon contact with the water. In others, a safety lanyard is pulled (one end attached to the rail of a ship, aircraft or torpedo tube) which starts an automatic timer countdown before the arming process is complete. Typically, the automatic safety-arming process takes some minutes to complete. This allows the people laying the mines sufficient time to move out of its activation and blast zones.\n\nIn the 1930s, Germany had experimented with the laying of mines by aircraft. It became a crucial element in their overall mining strategy. Aircraft had the advantage of speed, and they would never get caught in their own minefields. German mines held a large explosive charge. From April to June 1940, the Luftwaffe laid 1,000 mines in British waters. Soviet ports were mined, as was the Arctic convoy route to Murmansk. The Heinkel He 115 could carry two medium or one large mine while the Heinkel He 59, Dornier Do 18, Junkers Ju 88 and Heinkel He 111 could carry more.\n\nThe USSR was relatively ineffective in its use of naval mines in WWII in comparison with its record in previous wars. Small mines were developed for use in rivers and lakes, and special mines for shallow water. A very large chemical mine was designed to sink through ice with the aid of a melting compound. Special aerial mine designs finally arrived in 1943–1944, the AMD-500 and AMD-1000. Various Soviet Naval Aviation torpedo bombers were pressed into the role of aerial mining in the Baltic Sea and the Black Sea, including Ilyushin DB-3s, Il-4s and Lend Lease Douglas Boston IIIs.\n\nIn September 1939, the UK announced the placement of extensive defensive minefields in waters surrounding the Home Islands. Offensive aerial mining operations began in April 1940 when 38 mines were laid at each of these locations: the Elbe River, the port of Lübeck and the German naval base at Kiel. In the next 20 months, mines delivered by aircraft sank or damaged 164 Axis ships with the loss of 94 aircraft. By comparison, direct aerial attacks on Axis shipping had sunk or damaged 105 vessels at a cost of 373 aircraft lost. The advantage of aerial mining became clear, and the UK prepared for it. A total of 48,000 aerial mines were laid by the Royal Air Force (RAF) in the European Theatre during World War II.\n\nThe United States' early aerial mining efforts used smaller aircraft unable to carry many mines. Using Grumman TBF Avenger torpedo bombers, the US Navy mounted a direct aerial mining attack on enemy shipping in Palau on 30 March 1944 in concert with simultaneous conventional bombing and strafing attacks. The dropping of 78 mines stopped 32 Japanese ships from escaping Koror harbor; the combined operation sank or damaged 36 ships. Two Avengers were lost, and their crews were recovered. The mines brought port usage to a halt for 20 days; further mine laying in the area contributed to the Japanese abandoning Palau as a base.\n\nAs early as 1942, American mining experts such as Naval Ordnance Laboratory scientist Dr. Ellis A. Johnson, CDR USNR, suggested massive aerial mining operations against Japan's \"outer zone\" (Korea and northern China) as well as the \"inner zone\", their home islands. First, aerial mines would have to be developed further and manufactured in large numbers. Second, laying the mines would require a sizable air group. The US Army Air Forces had the carrying capacity but considered mining to be the navy's job. The US Navy lacked suitable aircraft. Johnson set about convincing General Curtis LeMay of the efficacy of heavy bombers laying aerial mines.\n\nIn the meantime, B-24 Liberators, PBY Catalinas and other bomber aircraft took part in localized mining operations in the Southwest Pacific and the China Burma India (CBI) theaters, beginning with a successful attack on the Yangon River in February 1943. Aerial minelaying operations involved a coalition of British, Australian and American aircrews, with the RAF and the Royal Australian Air Force (RAAF) carrying out 60% of the sorties and the USAAF and US Navy covering 40%. Both British and American mines were used. Japanese merchant shipping suffered tremendous losses, while Japanese mine sweeping forces were spread too thin attending to far-flung ports and extensive coastlines. Admiral Thomas C. Kinkaid, who directed nearly all RAAF mining operations in CBI, heartily endorsed aerial mining, writing in July 1944 that \"aerial mining operations were of the order of 100 times as destructive to the enemy as an equal number of bombing missions against land targets.\"\n\nIn March 1945, Operation Starvation began in earnest, using 160 of LeMay's B-29 Superfortress bombers to attack Japan's inner zone. Almost half of the mines were the US-built Mark 25 model, carrying 1250 lbs of explosives and weighing about 2,000 lbs. Other mines used included the smaller 1,000 lb Mark 26. Fifteen B-29s were lost while 293 Japanese merchant ships were sunk or damaged. Twelve thousand aerial mines were laid, a significant barrier to Japan's access to outside resources. Prince Fumimaro Konoe said after the war that the aerial mining by B-29s had been \"equally as effective as the B-29 attacks on Japanese industry at the closing stages of the war when all food supplies and critical material were prevented from reaching the Japanese home islands.\" The United States Strategic Bombing Survey (Pacific War) concluded that it would have been more efficient to combine the United States's effective anti-shipping submarine effort with land- and carrier-based air power to strike harder against merchant shipping and begin a more extensive aerial mining campaign earlier in the war. Survey analysts projected that this would have starved Japan, forcing an earlier end to the war. After the war, Dr. Johnson looked at the Japan inner zone shipping results, comparing the total economic cost of submarine-delivered mines versus air-dropped mines and found that, though 1 in 12 submarine mines connected with the enemy as opposed to 1 in 21 for aircraft mines, the aerial mining operation was about ten times less expensive per enemy ton sunk.\n\nBetween 600,000 and 1,000,000 naval mines of all types were laid in WWII. Advancing military forces worked to clear mines from newly-taken areas, but extensive minefields remained in place after the war. Air-dropped mines had an additional problem for mine sweeping operations: they were not meticulously charted. In Japan, much of the B-29 mine-laying work had been performed at high altitude, with the drifting on the wind of mines carried by parachute adding a randomizing factor to their placement. Generalized danger areas were identified, with only the quantity of mines given in detail. Mines used in Operation Starvation were supposed to be self-sterilizing, but the circuit did not always work. Clearing the mines from Japanese waters took so many years that the task was eventually given to the Japan Maritime Self-Defense Force.\n\nFor the purpose of clearing all types of naval mines, the Royal Navy employed German crews and minesweepers from June 1945 to January 1948, organised in the German Mine Sweeping Administration (GMSA), which consisted of 27,000 members of the former \"Kriegsmarine\" and 300 vessels. Mine clearing was not always successful: a number of ships were damaged or sunk by mines after the war. Two such examples were the liberty ships \"Pierre Gibault\" which was scrapped after hitting a mine in a previously cleared area off the Greek island of Kythira in June 1945, and \"Nathaniel Bacon\" which hit a minefield off Civitavecchia, Italy in December 1945, caught fire, was beached, and broke in two.\n\nThe damage that may be caused by a mine depends on the \"shock factor value\", a combination of the initial strength of the explosion and of the distance between the target and the detonation. When taken in reference to ship hull plating, the term \"Hull Shock Factor\" (HSF) is used, while keel damage is termed \"Keel Shock Factor\" (KSF). If the explosion is directly underneath the keel, then HSF is equal to KSF, but explosions that are not directly underneath the ship will have a lower value of KSF.\n\nUsually only created by contact mines, direct damage is a hole blown in the ship. Among the crew, fragmentation wounds are the most common form of damage. Flooding typically occurs in one or two main watertight compartments, which can sink smaller ships or disable larger ones. Contact mine damage often occurs at or close to the waterline near the bow, but depending on circumstances a ship could be hit anywhere on its outer hull surface (the mine attack being a good example of a contact mine detonating amidships and underneath the ship).\n\nThe bubble jet effect occurs when a mine or torpedo detonates in the water a short distance away from the targeted ship. The explosion creates a bubble in the water, and due to the difference in pressure, the bubble will collapse from the bottom. The bubble is buoyant, and so it rises towards the surface. If the bubble reaches the surface as it collapses, it can create a pillar of water that can go over a hundred meters into the air (a \"columnar plume\"). If conditions are right and the bubble collapses onto the ship's hull, the damage to the ship can be extremely serious; the collapsing bubble forms a high-energy jet that can break a metre-wide hole straight through the ship, flooding one or more compartments, and is capable of breaking smaller ships apart. The crew in the areas hit by the pillar are usually killed instantly. Other damage is usually limited.\n\nThe Baengnyeong incident, in which the ROKS \"Cheonan\" broke in half and sank off the coast South Korea in 2010, was caused by the bubble jet effect, according to an international investigation.\n\nIf the mine detonates at a distance from the ship, the change in water pressure causes the ship to resonate. This is frequently the most deadly type of explosion, if it is strong enough. The whole ship is dangerously shaken and everything on board is tossed around. Engines rip from their beds, cables from their holders, etc.. A badly shaken ship usually sinks quickly, with hundreds, or even thousands of small leaks all over the ship and no way to power the pumps. The crew fare no better, as the violent shaking tosses them around. This shaking is powerful enough to cause disabling injury to knees and other joints in the body, particularly if the affected person stands on surfaces connected directly to the hull (such as steel decks).\n\nThe resulting gas cavitation and shock-front-differential over the width of the human body is sufficient to stun or kill divers.\n\nWeapons are frequently a few steps ahead of countermeasures, and mines are no exception. In this field the British, with their large seagoing navy, have had the bulk of world experience, and most anti-mine developments, such as degaussing and the double-L sweep, were British inventions. When on operational missions, such as the recent invasion of Iraq, the US still relies on British and Canadian minesweeping services. The US has worked on some innovative mine-hunting countermeasures, such as the use of military dolphins to detect and flag mines. However, they are of questionable effectiveness. Mines in nearshore environments remain a particular challenge. They are small and as technology has developed they can have anechoic coatings, be non-metallic, and oddly shaped to resist detection. Further, oceanic conditions and the sea bottoms of the area of operations can degrade sweeping and hunting efforts. Mining countermeasures are far more expensive and time-consuming than mining operations, and that gap is only growing with new technologies.\n\nShips can be designed to be difficult for mines to detect, to avoid detonating them. This is especially true for minesweepers and mine hunters that work in minefields, where a minimal signature outweighs the need for armour and speed. These ships have hulls of glass fibre or wood instead of steel to avoid magnetic signatures. These ships may use special propulsion systems, with low magnetic electric motors, to reduce magnetic signature, and Voith-Schneider propellers, to limit the acoustic signature. They are built with hulls that produce a minimal pressure signature. These measures create other problems. They are expensive, slow, and vulnerable to enemy fire. Many modern ships have a mine-warning sonar—a simple sonar looking forward and warning the crew if it detects possible mines ahead. It is only effective when the ship is moving slowly.<br> \n\nA steel-hulled ship can be \"degaussed\" (more correctly, de-oerstedted or depermed) using a special degaussing station that contains many large coils and induces a magnetic field in the hull with alternating current to demagnetize the hull. This is a rather problematic solution, as magnetic compasses need recalibration and all metal objects must be kept in exactly the same place. Ships slowly regain their magnetic field as they travel through the Earth's magnetic field, so the process has to be repeated every six months.\n\nA simpler variation of this technique, called \"wiping\", was developed by Charles F. Goodeve which saved time and resources.\n\nBetween 1941 and 1943 the US Naval Gun factory (a division of the Naval Ordnance Laboratory) in Washington, D.C., built physical models of all US naval ships. Three kinds of steel were used in shipbuilding: mild steel for bulkheads, a mixture of mild steel and high tensile steel for the hull, and special treatment steel for armor plate. The models were placed within coils which could simulate the Earth's magnetic field at any location. The magnetic signatures were measured with degaussing coils. The objective was to reduce the vertical component of the combination of the Earth's field and the ship's field at the usual depth of German mines. From the measurements, coils were placed and coil currents determined to minimize the chance of detonation for any ship at any heading at any latitude.\n\nSome ships are built with magnetic inductors, large coils placed along the ship to counter the ship's magnetic field. Using magnetic probes in strategic parts of the ship, the strength of the current in the coils can be adjusted to minimize the total magnetic field. This is a heavy and clumsy solution, suited only to small-to-medium-sized ships. Boats typically lack the generators and space for the solution, while the amount of power needed to overcome the magnetic field of a large ship is impractical.\n\nActive countermeasures are ways to clear a path through a minefield or remove it completely. This is one of the most important tasks of any mine warfare flotilla.\n\nA sweep is either a contact sweep, a wire dragged through the water by one or two ships to cut the mooring wire of floating mines, or a distance sweep that mimics a ship to detonate the mines. The sweeps are dragged by minesweepers, either purpose-built military ships or converted trawlers. Each run covers between one and two hundred meters, and the ships must move slowly in a straight line, making them vulnerable to enemy fire. This was exploited by the Turkish army in the Battle of Gallipoli in 1915, when mobile howitzer batteries prevented the British and French from clearing a way through minefields.\n\nIf a contact sweep hits a mine, the wire of the sweep rubs against the mooring wire until it is cut. Sometimes \"cutters\", explosive devices to cut the mine's wire, are used to lessen the strain on the sweeping wire. Mines cut free are recorded and collected for research or shot with a deck gun.\n\nMinesweepers protect themselves with an oropesa or paravane instead of a second minesweeper. These are torpedo-shaped towed bodies, similar in shape to a Harvey Torpedo, that are streamed from the sweeping vessel thus keeping the sweep at a determined depth and position. Some large warships were routinely equipped with paravane sweeps near the bows in case they inadvertently sailed into minefields—the mine would be deflected towards the paravane by the wire instead of towards the ship by its wake. More recently, heavy-lift helicopters have dragged minesweeping sleds, as in the 1991 Persian Gulf War.\n\nThe distance sweep mimics the sound and magnetism of a ship and is pulled behind the sweeper. It has floating coils and large underwater \"drums\". It is the only sweep effective against bottom mines.\n\nDuring WWII, RAF Coastal Command used Vickers Wellington bombers Wellington DW.Mk I fitted with degaussing coils to trigger magnetic mines.\n\nModern influence mines are designed to discriminate against false inputs and are, therefore, much harder to sweep. They often contain inherent anti-sweeping mechanisms. For example, they may be programmed to respond to the unique noise of a particular ship-type, its associated magnetic signature and the typical pressure displacement of such a vessel. As a result, a mine-sweeper must accurately mimic the required target signature to trigger detonation. The task is complicated by the fact that an influence mine may have one or more of a hundred different potential target signatures programmed into it.\n\nAnother anti-sweeping mechanism is a ship-counter in the mine fuze. When enabled, this allows detonation only after the mine fuze has been triggered a pre-set number of times. To further complicate matters, influence mines may be programmed to arm themselves (or disarm automatically—known as \"self-sterilization\") after a pre-set time. During the pre-set arming delay (which could last days or even weeks) the mine would remain dormant and ignore any target stimulus, whether genuine or false.\n\nWhen influence mines are laid in an ocean minefield, they may have various combinations of fuze settings configured. For example, some mines (with the acoustic sensor enabled) may become active within three hours of being laid, others (with the acoustic and magnetic sensors enabled) may become active after two weeks but have the ship-counter mechanism set to ignore the first two trigger events, and still others in the same minefield (with the magnetic and pressure sensors enabled) may not become armed until three weeks have passed. Groups of mines within this mine-field may have different target signatures which may or may not overlap. The fuzes on influence mines allow many different permutations, which complicates the clearance process.\n\nMines with ship-counters, arming delays and highly specific target signatures in mine fuzes can falsely convince a belligerent that a particular area is clear of mines or has been swept effectively because a succession of vessels have already passed through safely.\n\nAs naval mines have become more sophisticated, and able to discriminate between targets, so they have become more difficult to deal with by conventional sweeping. This has given rise to the practice of mine-hunting.\nMine hunting is very different from sweeping, although some minehunters can do both tasks. Minehunting pays little attention to the nature of the mine itself. Nor does the method change much. At the current state of the art, Minehunting remains the best way to deal with influence mines proving to be both safer and more effective than sweeping. Specialized high-frequency sonars and high fidelity sidescaning sonar are used for mine location. Mines are hunted using sonar, then inspected and destroyed either by divers or ROVs (remote controlled unmanned mini-submarines). It is slow, but also the most reliable way to remove mines. Minehunting started during the Second World War, but it was only after the war that it became truly effective.\n\nSea mammals (mainly the Bottlenose Dolphin) have been trained to hunt and mark mines, most famously by the U.S. Navy Marine Mammal Program. Mine-clearance dolphins were deployed in the Persian Gulf during the Iraq War in 2003. The US Navy claims that these dolphins were effective in helping to clear more than 100 antiship mines and underwater booby traps from Umm Qasr Port.\n\nFrench naval officer Jacques Yves Cousteau's Undersea Research Group was once involved in mine-hunting operations: They removed or detonated a variety of German mines, but one particularly defusion-resistant batch—equipped with acutely sensitive pressure, magnetic, and acoustic sensors and wired together so that one explosion would trigger the rest—was simply left undisturbed for years until corrosion would (hopefully) disable the mines.\n\nA more drastic method is simply to run a ship through the minefield, letting other ships safely follow the same path. An early example of this was Farragut's actions at Mobile Bay during the American Civil War. However, as mine warfare became more developed this method became uneconomical.\nThis method was revived by the German \"Kriegsmarine\" during WWII. Left with a surfeit of idle ships due to the Allied blockade, the \"Kriegsmarine\" introduced a ship known as \"Sperrbrecher\" (\"block breaker\"). Typically an old cargo ship, loaded with cargo that made her less vulnerable to sinking (wood for example), the \"Sperrbrecher\" was run ahead of the ship to be protected, detonating any mines that might be in their path. The use of \"Sperrbrecher\" obviated the need to continuous and painstaking sweeping, but the cost was high. Over half the 100 or so ships used as \"Sperrbrecher\" were sunk during the war. Alternatively, a shallow draught vessel can be steamed through the minefield at high speed to generate a pressure wave sufficient to trigger mines, with the minesweeper moving fast enough to be sufficiently clear of the pressure wave so that triggered mines do not destroy the ship itself. These techniques are the only publicly known to be employed way to sweep pressure mines. The technique can be simply countered by use of a ship-counter, set to allow a certain number of passes before the mine is actually triggered. Modern doctrine calls for ground mines to be hunted rather than swept. A new system is being introduced for sweeping pressure mines, however counters are going to remain a problem.\n\nAn updated form of this method is the use of small unmanned ROVs (such as the \"Seehund\" drone) that simulate the acoustic and magnetic signatures of larger ships and are built to survive exploding mines. Repeated sweeps would be required in case one or more of the mines had its \"ship counter\" facility enabled i.e. were programmed to ignore the first 2, 3, or even 6 target activations.\n\nThe United States Navy MK56 ASW mine (the oldest still in use by the United States) was developed in 1966. More advanced mines include the MK60 CAPTOR (short for \"encapsulated torpedo\"), the MK62 and MK63 Quickstrike and the MK67 SLMM (Submarine Launched Mobile Mine). Today, most U.S. naval mines are delivered by aircraft.\n\nMK67 SLMM Submarine Launched Mobile Mine<br>\nThe SLMM was developed by the United States as a submarine deployed mine for use in areas inaccessible for other mine deployment techniques or for covert mining of hostile environments. The SLMM is a shallow-water mine and is basically a modified Mark 37 torpedo.\n\nGeneral characteristics\n\nMK65 Quickstrike<br>\nThe Quickstrike is a family of shallow-water aircraft-laid mines used by the United States, primarily against surface craft. The MK65 is a 2,000-lb (900 kg) dedicated, purpose-built mine. However, other Quickstrike versions (MK62, MK63, and MK64) are converted general-purpose bombs. These latter three mines are actually a single type of electronic fuze fitted to Mk82, Mk83 and Mk84 air-dropped bombs. Because this latter type of Quickstrike fuze only takes up a small amount of storage space compared to a dedicated sea mine, the air-dropped bomb casings have dual purpose i.e. can be fitted with conventional contact fuzes and dropped on land targets, or have a Quickstrike fuze fitted which converts them into sea mines.\n\nGeneral characteristics\n\n\nMK56<br>\nGeneral characteristics\n\n\nAccording to a statement made to the UK Parliament in 2002:\n\n...the Royal Navy does not have any mine stocks and has not had since 1992. Notwithstanding this, the United Kingdom retains the capability to lay mines and continues research into mine exploitation. Practice mines, used for exercises, continue to be laid in order to retain the necessary skills.\n\nHowever, a British company (BAE Systems) does manufacture the Stonefish influence mine for export to friendly countries such as Australia, which has both war stock and training versions of Stonefish, in addition to stocks of smaller Italian MN103 Manta mines. The computerised fuze on a Stonefish mine contains acoustic, magnetic and water pressure displacement target detection sensors. Stonefish can be deployed by fixed-wing aircraft, helicopters, surface vessels and submarines. An optional kit is available to allow Stonefish to be air-dropped, comprising an aerodynamic tail-fin section and parachute pack to retard the weapon's descent. The operating depth of Stonefish ranges between 30 and 200 metres. The mine weighs 990 kilograms and contains a 600 kilogram aluminised PBX explosive warhead.\n\nMine warfare remains the most cost-effective of asymmetrical naval warfare. Mines are relatively cheap and being small they are easy to deploy. Indeed, with some kinds of mines, trucks and rafts will suffice. At present there are more than 300 different mines available. Some 50 countries currently have mining ability. The number of naval mine producing countries has increased by 75% since 1988. It is also noted that these mines are of an increasing sophistication while even the older type mines present a significant problem. It has been noted that mine warfare may become an issue with terrorist organizations. Mining busy shipping straits and mining shipping harbors remain some of the most serious threats.\n\n\n\nSources\n\n\n"}
{"id": "9951247", "url": "https://en.wikipedia.org/wiki?curid=9951247", "title": "ORFS", "text": "ORFS\n\nORFS stands for \"Output RF Spectrum\", where 'RF' stands for Radio Frequency.\n\nThe acronym ORFS is used in the context of mobile communication systems, e.g., GSM. It stands for the relationship between (a) the frequency offset from the carrier and (b) the power, measured in a specific bandwidth and time, produced by the mobile station due to effects in modulation and power ramping and switching. ORFS measurements are defined and required in order to prove conformance by various institutions, e.g., the U.S. Federal Communications Commission (FCC) or ETSI.\n\nORFS - O-Ring Face Seal: Hydraulic sealing system for hoses and fittings.\n"}
{"id": "7528474", "url": "https://en.wikipedia.org/wiki?curid=7528474", "title": "Outline of vehicles", "text": "Outline of vehicles\n\nThe following outline is provided as an overview of and topical guide to vehicles:\n\nVehicle – non-living means of transportation. Vehicles are most often man-made, although some other means of transportation which are not made by man can also be called vehicles; examples include icebergs and floating tree trunks.\n\n\nAircraft\n\nLand vehicle\n\nWatercraft\n\n\n\n\n"}
{"id": "9756269", "url": "https://en.wikipedia.org/wiki?curid=9756269", "title": "Plectron", "text": "Plectron\n\nA Plectron is a specialized VHF/UHF single-channel, emergency alerting radio receiver, used to activate emergency response personnel, and disaster warning systems. Manufactured from the late 1950s, through the late 1990s, by the now defunct Plectron Corporation in Overton, Nebraska, hundreds of thousands of these radios were placed in homes of first responders across all of North America. This included ambulance crews, full-time and volunteer firefighters, off-duty specialized police response teams, Civil Defense members, and search and rescue teams.\n\nA Plectron's main feature (distinguishing it from a regular squelched radio) was its selective de-squelching. It would only sound when a correct pair of tones were broadcast - allowing many agencies (with different tones) to share the same frequency. Some Plectron Models had a set of \"Built-in Re-chargeable Batteries\", for this made the Plectron Portable. The Motorola MINITOR pager also uses this function and has largely replaced the Plectron.\n\n\n\n"}
{"id": "22695976", "url": "https://en.wikipedia.org/wiki?curid=22695976", "title": "Point-to-point laser technology", "text": "Point-to-point laser technology\n\nMost commonly used for as-built and existing conditions documentation or converting the built environment into a digital format.\n\nPPLT has many benefits in creating BIM or (CAD) models. Entering data directly into a CAD- or BIM-enabled work station allows a user or 'surveyor' to capture and confirm a building's geometry on site. This effectively builds a digital model of a building while it is being measured enabling not only speed but accuracy. Additionally, building in real time can eliminate the need for revisits and also minimizes the need for future interpretation and manipulation of measurements and data by a CAD operator.\n\nSince PPLT mainly deals with building surveying there are a few associations that are building surveying specific. There is the Association of Professional Building Surveyors (APBS) in the United States and the Royal Institution of Chartered Surveyors (RICS) in Britain. Whereas building surveying is offered at the curriculum level in Europe it still is in its nascent stages in the United States.\n\nBoth the APBS and RICS offer certification methods and programs. The APBS has the professional building surveyor (PBS) certifications.\n"}
{"id": "23716", "url": "https://en.wikipedia.org/wiki?curid=23716", "title": "Programmer", "text": "Programmer\n\nA programmer, developer, dev, coder, or software engineer is a person who creates computer software. The term \"computer programmer\" can refer to a specialist in one area of computers or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst.\n\nA programmer's primary computer language (Assembly, COBOL, C, C++, C#, Java, Lisp, Python, etc.) is often prefixed to these titles, and those who work in a web environment often prefix their titles with \"web\".\n\nA range of occupations, including: software developer, web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, or software analyst, while they do involve programming, also require a range of other skills. The use of the simple term \"programmer\" for these positions is sometimes considered an insulting or derogatory simplification.\n\nBritish countess and mathematician Ada Lovelace is often considered the first computer programmer, as she was the first to publish an algorithm intended for implementation on Charles Babbage's analytical engine, in October 1842, intended for the calculation of Bernoulli numbers. Because Babbage's machine was never completed to a functioning standard in her time, she never saw this algorithm run.\n\nThe first person to run a program on a functioning modern electronically based computer was computer scientist Konrad Zuse, in 1941.\n\nThe ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.\n\nInternational Programmers' Day is celebrated annually on 7 January. In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had also been an unofficial international holiday before that.\n\nThe word \"software\" was first used as early as 1953, but did not appear in print until the 1960s. Before this time, computers were programmed either by customers, or the few commercial computer vendors of the time, such as UNIVAC and IBM. The first company founded to provide software products and services was Computer Usage Company in 1955.\n\nThe software industry expanded in the early 1960s, almost immediately after computers were first sold in mass-produced quantities. Universities, government, and business customers created a demand for software. Many of these programs were written in-house by full-time staff programmers. Some were distributed freely between users of a particular machine for no charge. Others were done on a commercial basis, and other firms such as Computer Sciences Corporation (founded in 1959) started to grow. The computer/hardware makers started bundling operating systems, system software and programming environments with their machines.\n\nThe industry expanded greatly with the rise of the personal computer (\"PC\") in the mid-1970s, which brought computing to the desktop of the office worker. In the following years, it also created a growing market for games, applications, and utilities. DOS, Microsoft's first operating system product, was the dominant operating system at the time.\n\nIn the early years of the 21st century, another successful business model has arisen for hosted software, called software-as-a-service, or SaaS; this was at least the third time this model had been attempted. From the point of view of producers of some proprietary software, SaaS reduces the concerns about unauthorized copying, since it can only be accessed through the Web, and by definition, no client software is loaded onto the end user's PC. By 2014, the role of cloud developer had been defined; in this context, one definition of a \"developer\" in general was published:\n\nComputer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.\n\nProgrammers work in many settings, including corporate information technology (\"IT\") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some authorities disagree on the grounds that only careers with legal licensing requirements count as a profession).\n\nProgrammers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer’s supervision.\n\nProgrammers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as \"Java programmers\", or by the type of function they perform or environment in which they work: for example, \"database programmers\", \"mainframe programmers\", or Web developers.\n\nWhen making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps.\n\nProgrammers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called \"maintenance programming\". Programmers may contribute to user guides and online help, or they may work with technical writers to do such work.\n\nComputer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.\n\nA software developer needs to have deep technical expertise with certain aspects of computing. Some positions will require a degree in a relevant field such as computer science, information technology, engineering, programming, or any other IT related post graduate studies. An ideal software developer is a self-motivated professional carrying a dynamic hands-on experience on key languages of programming such as C++, C#, PHP, Java, C, Javascript, VB, Oracle, UML, Linux, Python, UNIX, XML, HTTP, Smalltalk, or other software testing tools.\n\nAccording to developer Eric Sink, the differences between system design, software development, and programming are more apparent. Already in the current market place there can be found a segregation between programmers and developers, in that one who implements is not the same as the one who designs the class structure or hierarchy. Even more so that developers become software architects or systems architects, those who design the multi-leveled architecture or component interactions of a large software system.\n\nProgrammers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.\n\nIn some organizations, particularly small ones, people commonly known as \"programmer analysts\" are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.\n\nIn addition, the rise of the Internet has made web development a huge part of the programming field. Currently more software applications are web applications that can be used by anyone with a web browser. Examples of such applications include the Google search service, the Outlook.com e-mail service, and the Flickr photo-sharing service.\n\nProgramming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.\n\nAccording to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey. The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.\n\nComputer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.\n\nLarge companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and avoid previously employer paid training using industry specific technologies not covered in most accredited degree programs. Other reasons for employers claiming skill shortages is the result of their own cost saving combining of several disparate skill sets previously held by several specialized programmers into fewer generalized multifaceted positions that are unlikely to have enough \"qualified\" candidates with the desired experience.\n\nEnrollment in computer-related degrees in US has dropped recently due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers. This situation has resulted in confusion about whether the US economy is entering a \"post-information age\" and the nature of US comparative advantages. Most academic institutions have an Institutional research office that keep past statistics of degrees conferred which show several dips and rises in Computer Science degrees over the past 30 years. The overall trend shows a slightly overall decline in growth (especially when compared to other STEM degree growth) since certain peaks of 1986, 1992, 2002, and 2008 showing periods of flat growth or even declines. In addition the U.S. Bureau of Labor Statistics Occupational Outlook 2016-26 is -7% (a decline in their words) for Computer Programmers because Computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower.\n\n\n"}
{"id": "57627814", "url": "https://en.wikipedia.org/wiki?curid=57627814", "title": "Prosthetic testicle", "text": "Prosthetic testicle\n\nA prosthetic testicle is an artificial replacement for a testicle that has been lost either through surgery or injury. They are manufactured from silicone rubber filled with a salt solution.\n\n"}
{"id": "25521", "url": "https://en.wikipedia.org/wiki?curid=25521", "title": "Relay league", "text": "Relay league\n\nA relay league is a chain of message forwarding stations in a system of optical telegraphs, radio telegraph stations, or riding couriers.\n\nAn interesting description of these early 19th century methods and its evolution into the electrical telegraph networks of the mid-to-late 19th century is found in \"The Victorian Internet\", a book by Tom Standage .\n\nRadio amateurs have been early in arranging relay leagues, as is reflected in the name of the organization of American Radio Relay League (ARRL), http://www.arrl.org/.\n\nRadio amateur message relay operations were originally conducted using Morse code in the first two decades of the 20th century using spark-gap transmitters. As vacuum tubes became affordable operations shifted to more efficient manual telegraphy transmitters, referred to as CW (Continuous wave). Messages were relayed station to station typically covering four or more re-transmission cycles to cover the continental United States, in an organized system of amateur radio networks. After World War II, voice and radioteletype implementations of the message relay system were employed.\n\n"}
{"id": "10029655", "url": "https://en.wikipedia.org/wiki?curid=10029655", "title": "Responsive architecture", "text": "Responsive architecture\n\nResponsive architecture is an evolving field of architectural practice and research. Responsive architectures are those that measure actual environmental conditions (via sensors) to enable buildings to adapt their form, shape, color or character responsively (via actuators).\n\nResponsive architectures aim to refine and extend the discipline of architecture by improving the energy performance of buildings with responsive technologies (sensors / control systems / actuators) while also producing buildings that reflect the technological and cultural conditions of our time.\n\nResponsive architectures distinguish themselves from other forms of interactive design by incorporating intelligent and responsive technologies into the core elements of a building's fabric. For example: by incorporating responsive technologies into the structural systems of buildings architects have the ability to tie the shape of a building directly to its environment. This enables architects to reconsider the way they design and construct space while striving to advance the discipline rather than applying patchworks of intelligent technologies to an existing vision of \"building\".\n\nThe common definition of responsive architecture, as described by many authors, is a class of architecture or building that demonstrates an ability to alter its form, to continually reflect the environmental conditions that surround it.\n\nThe term \"responsive architecture\" was introduced by Nicholas Negroponte, who first conceived of it during the late 1960s when spatial design problems were being explored by applying cybernetics to architecture. Negroponte proposes that responsive architecture is the natural product of the integration of computing power into built spaces and structures, and that better performing, more rational buildings are the result. Negroponte also extends this mixture to include the concepts of recognition, intention, contextual variation, and meaning into computing and its successful (ubiquitous) integration into architecture. This cross-fertilization of ideas lasted for about eight years. Several important theories resulted from these efforts, but today Nicholas Negroponte’s contributions are the most obvious to architecture. His work moved the field of architecture in a technical, functional, and actuated direction.\n\nSince Negroponte’s contribution, new works of responsive architecture have also emerged, but as aesthetic creations—rather than functional ones. The works of Diller & Scofidio (Blur), dECOi (Aegis Hypo-Surface), and NOX (The Freshwater Pavilion, NL) are all classifiable as types of responsive architecture. Each of these works monitors fluctuations in the environment and alters its form in response to these changes. The Blur project by Diller & Scofidio relies upon the responsive characteristics of a cloud to change its form while blowing in the wind. In the work of dECOi, responsiveness is enabled by a programmable façade, and finally in the work of NOX, a programmable audio–visual interior.\n\nAll of these works depend upon the abilities of computers to continuously calculate and join digital models that are programmable, to the real world and the events that shape it.\n\nFinally an account of the development of the use of responsive systems and their history in respect to recent architectural theory can be found in Tristan d'Estree Sterk's recent opening keynote address (ACADIA 2009) entitled \"Thoughts for Gen X— Speculating about the Rise of Continuous Measurement in Architecture\" \n\nWhile a considerable amount of time and effort has been spent on intelligent homes in recent years, the emphasis here has been mainly on developing computerized systems and electronics to adapt the interior of the building or its rooms to the needs of residents. Research in the area of responsive architecture has had far more to do with the building structure itself, its ability to adapt to changing weather conditions and to take account of light, heat and cold. This could theoretically be achieved by designing structures consisting of rods and strings which would bend in response to wind, distributing the load in much the same way as a tree. Similarly, windows would respond to light, opening and closing to provide the best lighting and heating conditions inside the building.\n\nThis line of research, known as actuated tensegrity, relies on changes in structures controlled by actuators which in turn are driven by computerized interpreters of the real world conditions.\n\nClimate adaptive building shells (CABS) can be identified as a sub-domain of responsive architecture, with special emphasis on dynamic features in facades and roofs. CABS can repeatedly and reversibly change some of its functions, features or behavior over time in response to changing performance requirements and variable boundary conditions, with the aim of improving overall building performance.\n\nTristan d'Estree Sterk of The Bureau For Responsive Architecture and The School of the Art Institute of Chicago and Robert Skelton of UCSD in San Diego are working together on actuated tensegrity, experimenting with pneumatically controlled rods and wires which change the shape of a building in response to sensors both outside and inside the structure. Their goal is to limit and reduce the impact of buildings on natural environments.\n\nMIT's Kinetic Design Group has been developing the concept of \"intelligent kinetic systems\" which are defined as \"architectural spaces and objects that can physically re-configure themselves to meet changing needs.\" They draw on structural engineering, embedded computation and adaptable architecture. The objective is to demonstrate that energy use and the environmental quality of buildings could be rendered more efficient and affordable by making use of a combination of these technologies.\n\nDaniel Grünkranz of The University of Applied Arts in Vienna is currently undertaking PhD research in the field of Phenomenology as it applies to Responsive Architectures and Technologies.\n\nDepicted Left: A full scale actuated tensegrity prototype built from cast aluminium, stainless steel components and pneumatic muscles (pneumatic muscles provided by Shadow Robotics UK) by Tristan d'Estree Sterk and The Office for Robotic Architectural Media (2003). These types of structural systems use variable and controllable rigidity to provide architects and engineers with systems that have a controllable shape. As a form of ultra-lightweight structure these systems offer a primary method for reducing the embodied energy used in construction processes.\n\n\n\n"}
{"id": "23993297", "url": "https://en.wikipedia.org/wiki?curid=23993297", "title": "SRTM Water Body Data", "text": "SRTM Water Body Data\n\nThe SRTM Water Body Data (SWBD) is a geographical dataset (2003) encoding high-resolution worldwide coastline outlines in a vector format, published by NASA and designed for use in geographic information systems and mapping applications. It was created by BAE Systems ADR for the US National Geospatial-Intelligence Agency (NGA) as a complementary product during editing of the digital elevation model database of the Shuttle Radar Topography Mission (SRTM). SWBD data covers the Earth's surface between 56° southern latitude and 60° northern latitude. It is distributed in ESRI shapefile format, divided into 12,229 files, each covering one 1°-by-1° tile of the Earth's surface.\n\nSWBD data is in the public domain and is made available online for free download by NASA.\n\n\n"}
{"id": "2256597", "url": "https://en.wikipedia.org/wiki?curid=2256597", "title": "Safe-life design", "text": "Safe-life design\n\nIn safe-life design products are designed to survive a specific design life with a chosen reserve.\n\nSafe life is particularly relevant to metal aircraft, where airframe components are subjected to varying loads over the lifetime of the aircraft which makes them susceptible to metal fatigue. In certain areas such as in wing or tail components structural failure in flight would be catastrophic.\n\nThe safe-life design technique is employed in critical systems which are either very difficult to repair or may cause severe damage to life and property. These systems are designed to work for years without requirement of any repairs.\n\nThe drawback is that products designed with a safe-life approach are over-built or allocated more resources than they are expected to need, which may be uneconomical.\nIn order to maintain the designed safety, they will have to be replaced after the design life has expired, while they may still have a considerable life ahead of them. To counter these drawbacks, alternative design philosophies like fail-safe design and fault-tolerant design were developed.\n\n"}
{"id": "1984238", "url": "https://en.wikipedia.org/wiki?curid=1984238", "title": "Safety barrier", "text": "Safety barrier\n\nA safety barrier is a component which prevents passage into a dangerous area, commonly used to mitigate risk. Safety barriers may be hard barriers physically restricting passage or soft barriers that control circuits based on the presence of foreign bodies.\n\nA safety barrier is a component which prevents passage into a dangerous area. It is commonly used to mitigate risk in the Hazard-Barrier-Target model, as studied in safety science. Work Safe Victoria (an Australian organization) defines a Safety Barrier as a device that:\n\nHard barriers are fixed or removable guards which prevent entry. There include fences, traffic barriers, and crush barriers.\n\nSoft barriers are devices such as light curtains. They detect the presence of a foreign body and are tied into the control circuit to stop the machine.\n\nHard barriers are fixed into the ground as a bollard or gate, removable on stands, or clipped to a structure. Regardless of the barrier type, the components and overall assembly will be similar.\n\nThe material of rails used to stop the flow of objects into the protected area must be durable and strong enough to withstand any impacts related to the environment in which it is installed; for example, a guard rail on the side of the road must be able to contain and safely stop or drastically slow down a vehicle weighing up to 40 tons (cable barriers are most effective against tractor trailers), whereas a pedestrian safety barrier only needs to be tall enough, designed with a lattice effect to keep people from climbing through it, or highly visible when being used to display caution.\n\nSafety barriers vary depending on their application. In an industrial setting, a safety barrier may be a fence or window, designed to keep the operator away from moving parts or other hazards. Railings should be closed cornered with rounded or flared edges to prevent any further damage to personnel or machinery in the event of a collision. Safety barriers can also be used to protect the corners of fixtures, support columns, exposed pipes or cables, or designated areas on the work floor.\n\nIn an automotive setting, the barrier may be a traffic barrier or Jersey barrier, separating lanes/directions of traffic flow. In crowd control, terraces use crush barriers to reduce the risk of human crush in large crowds.\n"}
{"id": "578150", "url": "https://en.wikipedia.org/wiki?curid=578150", "title": "Standard hydrogen electrode", "text": "Standard hydrogen electrode\n\nThe Standard hydrogen electrode (abbreviated SHE), is a redox electrode which forms the basis of the thermodynamic scale of oxidation-reduction potentials. Its absolute electrode potential is estimated to be at 25 °C, but to form a basis for comparison with all other electrode reactions, hydrogen's standard electrode potential (\"E\") is declared to be zero volts only at 298K. Potentials of any other electrodes are compared with that of the standard hydrogen electrode at the same temperature.\n\nHydrogen electrode is based on the redox half cell:\n\nThis redox reaction occurs at a platinized platinum electrode.\nThe electrode is dipped in an acidic solution and pure hydrogen gas is bubbled through it. The concentration of both the reduced form and oxidised form is maintained at unity. That implies that the pressure of hydrogen gas is 1 bar (100 kPa) and the activity of hydrogen ions in the solution is unity. The activity of hydrogen ions is their effective concentration, which is equal to the formal concentration times the activity coefficient. These unit-less activity coefficients are close to 1.00 for very dilute water solutions, but usually lower for more concentrated solutions. \nThe Nernst equation should be written as:\n\nwhere:\n\nDuring the early development of electrochemistry, researchers used the normal hydrogen electrode as their standard for zero potential. This was convenient because it could \"actually be constructed\" by \"[immersing] a platinum electrode into a solution of 1 N strong acid and [bubbling] hydrogen gas through the solution at about 1 atm pressure\". However, this electrode/solution interface was later changed. What replaced it was a theoretical electrode/solution interface, where the concentration of H was 1 M, but the H ions were assumed to have no interaction with other ions (a condition not physically attainable at those concentrations). To differentiate this new standard from the previous one it was given the name 'Standard Hydrogen Electrode'.\nIn summary,\n\nThe choice of platinum for the hydrogen electrode is due to several factors:\nThe surface of platinum is platinized (i.e., covered with platinum black) to:\n\nOther metals can be used for building electrodes with a similar function such as the palladium-hydrogen electrode.\n\nBecause of the high adsorption activity of the platinized platinum electrode, it's very important to protect electrode surface and solution from the presence of organic substances as well as from atmospheric oxygen. Inorganic ions that can reduce to a lower valency state at the electrode also have to be avoided (e.g., Fe, ). A number of organic substances are also reduced by hydrogen at a platinum surface, and these also have to be avoided.\n\nCations that can reduce and deposit on the platinum can be source of interference: silver, mercury, copper, lead, cadmium and thallium.\n\nSubstances that can inactivate (\"poison\") the catalytic sites include arsenic, sulfides and other sulfur compounds, colloidal substances, alkaloids, and material found in living systems.\n\nThe standard redox potential of the deuterium couple is slightly different from that of the proton couple (ca. −0.0044 V vs SHE). Various values in this range have been obtained: −0.0061 V, −0.00431 V, −0.0074 V.\n\nAlso difference occur when hydrogen deuteride is used instead of hydrogen in the electrode.\n\nThe scheme of the standard hydrogen electrode:\n\n\n"}
{"id": "57547773", "url": "https://en.wikipedia.org/wiki?curid=57547773", "title": "Swissa", "text": "Swissa\n\nSwissa was a popular typewriter brand of the post-war era. The Swissa was made by the company Birchmeier`s Söhne in Murgenthal (Aargau) and was based on the technical developments of the Patria typewriters. The Swissa Typewriters received much attention due to their product design.\n\nSwissa`s predecessor - the Patria - was invented by Otto Haas and produced in Switzerland beginning in 1936. \nThe Company Patria started as a ski and ski poles manufacturer in 1908. It branched out into producing typewriters in 1936 initially at its factory in Büttenbergweg 5, Pierterlen, Switzerland (the factory building is still standing, as of May 2018). \nThe first typewriter was a portable typewriter designed by Carl Winterling (of the Archo typewriter Company) based on the idea of engineer Otto Haas.\n\nIn 1933, Otto Haas filed a patent for the Patria Typewriter. In it, Otto Haas described his thoughts on his concept for a truly portable/compact typewriter. “In order to fully meet the requirements connected with the construction of an easily portable or travelling typewriter it is necessary to pay regard to the aforementioned conditions in a far greater measure than has heretofore been done in practice. For the purpose in view the dimensions of the typewriter and its parts must be kept so small, that it may conveniently be carried in a small travelling bag, and, accordingly, the weight of the typewriter must be quite considerably reduced, as compared with herefore constructed portable typewriters”.\n\nIn 1936 the Patria typewriter was shown at the Swiss tradeshow „Basler Mustermesse” \nIn the same year, the production of the portable typewriter began in Pieterlen near Biel (Katon Bern). For this purpose the \" A.G.für Schreibmaschinen-Fabrikation, Pieterlen, Schweiz” whose name translates to “Factory for Typewriters, Pieterlen, Switzerland” had been founded. Investors were the industrial families Chessex and Lutz.\n\nThe Engineer Otto Haas had settled in Teufenthal (Aargau) and brought his machine to production maturity together with Ch.Werner, Josef Brönner and Hermann Wasem.\n\nIn 1938/39 the corporation was dissolved, but seamlessly continued by the same family Lutz as \"Patria-Schreibmaschinen GmbH\" in Zurich.\nThe first Patria typewriter was only produced for three years up to 1938 when it moved its factory from Pieterlen to Zurich.\nHence, not many Patria typewriters were labelled \"A.G. für Schreibmaschinen-Fabrikation - Pieterlen (Schweiz)\" on the front of the typewriter - most are labelled \"Patria Schreibmaschinen Gesellschaft (Zürich)\" as the Zurich factory produced these typewriters up till 1943.\nAfter 1943 the Patria was produced by August Birchmeier`s sons, Murgenthal, Canton of Aargau, Switzerland.\nThe first Patria models were known to be sturdy and solid and were even used by the Swiss Army.\n\nIn 1944, the architect, artist and designer Max Bill (1908-1994) was commissioned to redesign the Patria and designed an external form that became the archetype of the modern portable typewriter par excellence.\n\nAugust Birchmeier's sons continued to manufacture after World War II and launched the machine in 1950 under the name “SWISSA”. The continuity is obvious: The \"Patria\" designed by Max Bill is identical to the \"SWISSA\" except for the brand name.\n\nAugust Birchmeier`s Sons (August Birchmeier`s Söhne) was founded in 1908 by the eponymous founder as stamping tools factory August Birchmeier in Murgenthal, Canton of Aargau (from 1911 in the commercial register). \nIn 1922 August Birchmeier died but his widow continued the workshop. \nIn 1935 the sons took over the business: \"August Birchmeier's sons\" was founded. \nFrom 1931 to 1936 the company grew with the production of furniture fittings and locks. In the post-war years Birchmeier`s began the production of a portable typewriters.\n1952/52 August Birchmeier`s sons set up a second plant in Wolfwil [Canton of Solothurn] (distance only 5 kilometers from Murgenthal).\n\nIn 1963 the toolmaking shop was relocated to the new factory. Around 1965 the typewriter SWISSA was produced in Murgenthal and furniture fittings and stamped products and other metal goods in Wolfwil.\n\nThe typewriter factory remained operational for 30 years until the late 1970s. On 31.5.1995 the company \"August Birchmeier Sons\" was deleted from the Swiss Commercial Register.\n\nIn 1950, the name changes from \"Patria\" to \"Swissa Piccolo\". All other models of the \"Birchmeier Patria\" now run under the brand name Swissa.\nThe following models were produced by August Birchmeier`s Sons until 1980 when production ceased:\n\n\nThe Swissa was sold all over Europe. However, a whole family of portable typewriters common throughout Europe had, as its base, the Patria of Switzerland. This machine's basic design was eventually produced via licensing agreements in England, France, Germany and Spain, that includes \nJapy acquires a license to manufacture the Swiss Patria typewriter. The result is the Japy, model P6, which was produced from 1937 to 1948. The P6 was mechanically a \"Patria\", but had its own design. \nFrom 1949 to 1959, the second design of the Patria typewriter, now SWISSA piccolo, which was designed by Max Bill, was made by Japy. The machines were called \"P68\" or \"Japy Personelle\".\nIn the 1960`s the JAPY SCRIPT was adopted, which was a later restyling, if only in the change to two-tone flat enamel paint, as opposed to single-color crinkle finish paint.\n\nThe post-war Oliver Portable and Oliver Courier, were based perfectly on the original Patria design. This was the third Oliver portable typewriter design—the first was the Europa, the next the similar Italian-made SIM.\n\nThe spanish Patria typewriter came on the market in 1947. “Spanish and Swiss technology and experience united have produced this gem of the mechanographic art for you,” states some early advertising. The Patria was recommended to “all those who need to bring work home, but do not wish to give their place of residence the sobriety of an office.” The Patria was distributed throughout Spain by Guillermo Trúniger, S.A., with a good deal of success. It was exported in small quantities as early as 1950 (one Patria sold in Canada and marked “Eaton’s” appears to be a Spanish product).\n"}
{"id": "113017", "url": "https://en.wikipedia.org/wiki?curid=113017", "title": "The Inquirer", "text": "The Inquirer\n\nThe Inquirer is a British technology tabloid website founded by Mike Magee after his departure from \"The Register\" (of which he was one of the founding members) in 2001. In 2006 the site was acquired by Dutch publisher \"Verenigde Nederlandse Uitgeverijen\" (VNU). Mike Magee later left The Inquirer in February 2008 to work on the \"IT Examiner\".\n\nHistorically, the magazine was entirely Internet-based with its journalists living all over the world and filing copy online, though in recent years it has been edited from Incisive Media's offices in London.\n\nAlthough traditionally a 'red top', under \"Incisive Media\" it has put more weight behind its journalism, reducing the number of jibes at companies, and moved instead towards sponsored online debates in association with high-profile organisations, most recently, Intel.\n\nIn 2006 \"The Inquirer\" reported laptop battery problems that affected Dell, Sony and Apple as of September 2006, with rumours of problems at Toshiba and Lenovo. In June 2006, \"The Inquirer\" published photographs of a Dell notebook PC bursting into flames at a conference in Japan; \"The New York Times\", and others, reprinted \"The Inquirer\"'s photographs. \"The Inquirer\" was also the first publication to report Dell's subsequent decision to recall faulty batteries, according to \"BusinessWeek\".\n\n\"The Inquirer\"\"s successful reporting of the story relied on information supplied by readers and later by a confidential source at Dell. \"I attribute being on top of the story to old-fashioned print journalism standards — cultivating, and, if you'll excuse the pun, not burning such contacts,\" \"The Inquirer\"<nowiki>'</nowiki>s founder, Mike Magee, told BusinessWeek.\n\nIn July 2006, \"The Inquirer\" posted images to show cheating by NVIDIA Windows device drivers in \"Rydermark 2006\". The images were alleged to be fake by a number of sources. \"The Inquirer\" denied any wrongdoing and quoted the maker of Rydermark calling the allegations against them \"irresponsible\". About 8 months after the original Rydermark article, \"The Inquirer\" ran another article claiming that Rydermark was still being developed, but was near release. In response, one of its critics offered $1,000 to a charity of the Rydermark articles author's choosing if he could produce (breaching his NDA) a version of Rydermark that showed the alleged screenshots in full-motion video before a set deadline (which gave the author 10 and a half hours, beginning at 6:30PM UK time). No one produced the program before the deadline passed.\n\nIndependent verification that RyderMark was genuine, first appeared in TweakTown in May 2007. RyderMark developer Ajith Ram denied ever sending the Inquirer NVIDIA cheating allegations.\n\nOn 24 July 2006, \"The Inquirer\" wrote that, in response to AMD's announced intent to purchase ATI, \"ATI had its chipset license pulled, or at least not renewed by Intel.\" ATI responded by stating that its license had not been revoked and that they continue to ship Intel chipsets under license. On 23 August 2006, ATI showed its chipset roadmap to motherboard vendors which showed that next-generation chipsets for the Intel platform are cancelled. On 1 March 2007, AMD said that they would continue developing chipsets for Intel platforms.\n\nOn 3 October 2014, \"The Inquirer\" reported on the privacy policy for the Technical Preview the upcoming \"Microsoft\" \"Windows 10\" operating system. In the report, it pointed out that the permissions included the ability for Microsoft to monitor individual keystrokes as well as file content from users. The story was picked up by news media around the world causing Microsoft to admit that monitoring was a necessary part of the process, but denying use of a keylogger.\n\n\"Ed Bott\", writing for \"ZDNet\" accused the site of being \"a tech tabloid known for its breathless headlines and factually challenged prose\" and said of writer Chris Merriman, \"there’s little evidence that the author has enough background in computer science or security to tell a keylogger from a key lime pie.\"\n"}
{"id": "18004739", "url": "https://en.wikipedia.org/wiki?curid=18004739", "title": "The New Age of Innovation", "text": "The New Age of Innovation\n\nThe New Age of Innovation: Driving Cocreated Value Through Global Networks () is a book by University of Michigan Ross School of Business Professors C. K. Prahalad and M. S. Krishnan published in April 2008. The book outlines a new strategic path for companies in the 21st century.\n\nIn the first chapter, Prahalad and Krishnan outline their central thesis; that there are new managerial demands in business, requiring new sources of value creation. They argue that these demands have created an N=1 and R=G environment, where companies need to customize their product for each customer by gaining access to a new array of suppliers.\n\nThe book argues that the old sources of competitive advantage -technology, labor, and capital – are fading and that new sources are emerging. Prahalad and Krishnan suggest an internal capacity to reconfigure resources in real time by focusing on clearly documented, transparent, and resilient business processes (the link between strategy, business models and operations) has become a strong differentiator. They also argue that a focus on co-creation, by developing an R=G supply network and emphasizing analytics which identify trends and unique opportunities can create a strong competitive advantage. The technical architecture required to develop these flexible and resilient business processes and strong analytics capabilities is outlined in the book. \n\nThe last four chapters describe how to implement these new strategies. Recognizing that many companies have fragmented and archaic systems, the book describes typical problems that occur when migrating to an N=1 and R=G friendly system. Prahalad and Krishnan emphasize the importance of a social architecture with strong linkages between managers and the technical architecture. They also outline the necessity for companies to recruit new skills from around the world and use globalization to its advantage.\n\nThe book argues that \"the industrial system as we know it has been morphing for some time. Now it may have reached an inflection point.” Prahalad and Krishnan outline the approach that firms in the new economy must take to survive and become successful. The previous choice between low cost and differentiation is rarely the chief strategic choice anymore, firms must achieve what Prahalad and Krishnan call N=1 and R=G.\n\nN=1 requires companies to focus on the importance of individual customer experiences and tailor their product accordingly. It requires that companies have resilient, dynamic and flexible business processes. They also emphasize the importance of strong analytics which allow management to discover trends and unique opportunities and enable the company to engage in product co-creation with their consumer base.\n\nR=G advises firms take a horizontal approach to supply rather than vertical integration. The focus is on obtaining access, rather than ownership, to resources from an array of suppliers both inside and outside the firm. R=G provides the best opportunity for firms to leverage the necessary resources to co-create a personalized experience for each customer. R=G is often mistaken as a suggestion to outsource, however this is often not the case, as there are many notable instances where it makes sense for companies to leverage local resources to fulfill a personalized demand model.\n\n\n"}
{"id": "52381", "url": "https://en.wikipedia.org/wiki?curid=52381", "title": "Thermite", "text": "Thermite\n\nThermite () is a pyrotechnic composition of metal powder, which serves as fuel, and metal oxide. When ignited by heat, thermite undergoes an exothermic reduction-oxidation (redox) reaction. Most varieties are not explosive, but can create brief bursts of heat and high temperature in a small area. Its form of action is similar to that of other fuel-oxidizer mixtures, such as black powder.\n\nThermites have diverse compositions. Fuels include aluminium, magnesium, titanium, zinc, silicon, and boron. Aluminium is common because of its high boiling point and low cost. Oxidizers include bismuth(III) oxide, boron(III) oxide, silicon(IV) oxide, chromium(III) oxide, manganese(IV) oxide, iron(III) oxide, iron(II,III) oxide, copper(II) oxide, and lead(II,IV) oxide.\n\nThe reaction, also called the Goldschmidt process, is used for thermite welding, often used to join rail tracks. Thermites have also been used in metal refining, disabling munitions, and in incendiary weapons. Some thermite-like mixtures are used as pyrotechnic initiators in fireworks.\n\nIn the following example, elemental aluminium reduces the oxide of another metal, in this common example iron oxide, because aluminium forms stronger and more stable bonds with oxygen than iron:\n\nThe products are aluminium oxide, elemental iron, and a large amount of heat. The reactants are commonly powdered and mixed with a binder to keep the material solid and prevent separation.\n\nOther metal oxides can be used, such as chromium oxide, to generate the given metal in its elemental form. For example, a copper thermite reaction using copper oxide and elemental aluminium can be used for creating electric joints in a process called cadwelding, that produces elemental copper (it may react violently):\n\nThermites with nanosized particles are described by a variety of terms, such as metastable intermolecular composites, super-thermite, nano-thermite, and nanocomposite energetic materials.\n\nThe thermite (\"thermit\") reaction was discovered in 1893 and patented in 1895 by German chemist Hans Goldschmidt. Consequently, the reaction is sometimes called the \"Goldschmidt reaction\" or \"Goldschmidt process\". Goldschmidt was originally interested in producing very pure metals by avoiding the use of carbon in smelting, but he soon discovered the value of thermite in welding.\n\nThe first commercial application of thermite was the welding of tram tracks in Essen in 1899.\n\nRed iron(III) oxide (FeO, commonly known as rust) is the most common iron oxide used in thermite. Magnetite also works. Other oxides are occasionally used, such as MnO in manganese thermite, CrO in chromium thermite, quartz in silicon thermite, or copper(II) oxide in copper thermite, but only for specialized purposes. All of these examples use aluminium as the reactive metal. Fluoropolymers can be used in special formulations, Teflon with magnesium or aluminium being a relatively common example. Magnesium/teflon/viton is another pyrolant of this type.\n\nCombinations of dry ice (frozen carbon dioxide) and reducing agents such as magnesium, aluminium and boron follow the same chemical reaction as with traditional thermite mixtures, producing metal oxides and carbon. Despite the very cold temperature of a dry ice thermite mixture, such a system is capable of being ignited with a flame. When the ingredients are finely divided, confined in a pipe and armed like a traditional explosive, this cryo-thermite is detonatable and a portion of the carbon liberated in the reaction emerges in the form of diamond.\n\nIn principle, any reactive metal could be used instead of aluminium. This is rarely done, because the properties of aluminium are nearly ideal for this reaction:\n\n\nAlthough the reactants are stable at room temperature, they burn with an extremely intense exothermic reaction when they are heated to ignition temperature. The products emerge as liquids due to the high temperatures reached (up to 2500 °C with iron(III) oxide)—although the actual temperature reached depends on how quickly heat can escape to the surrounding environment. Thermite contains its own supply of oxygen and does not require any external source of air. Consequently, it cannot be smothered, and may ignite in any environment given sufficient initial heat. It burns well while wet, and cannot be easily extinguished with water—though enough water to remove sufficient heat may stop the reaction. Small amounts of water boil before reaching the reaction. Even so, thermite is used for welding underwater.\n\nThe thermites are characterized by almost complete absence of gas production during burning, high reaction temperature, and production of molten slag. The fuel should have high heat of combustion and produce oxides with low melting point and high boiling point. The oxidizer should contain at least 25% oxygen, have high density, low heat of formation, and produce metal with low melting and high boiling point (so the energy released is not consumed in evaporation of reaction products). Organic binders can be added to the composition to improve its mechanical properties, however they tend to produce endothermic decomposition products, causing some loss of reaction heat and production of gases.\n\nThe temperature achieved during the reaction determines the outcome. In an ideal case, the reaction produces a well-separated melt of metal and slag. For this, the temperature must be high enough to melt both reaction products, the resulting metal and the fuel oxide. Too low a temperature produces a mixture of sintered metal and slag; too high a temperature (above the boiling point of any reactant or product) leads to rapid production of gas, dispersing the burning reaction mixture, sometimes with effects similar to a low-yield explosion. In compositions intended for production of metal by aluminothermic reaction, these effects can be counteracted. Too low a reaction temperature (e.g., when producing silicon from sand) can be boosted with addition of a suitable oxidizer (e.g., sulfur in aluminium-sulfur-sand compositions); too high a temperature can be reduced by using a suitable coolant and/or slag flux. The flux often used in amateur compositions is calcium fluoride, as it reacts only minimally, has relatively low melting point, low melt viscosity at high temperatures (therefore increasing fluidity of the slag) and forms a eutectic with alumina. Too much of flux however dilutes the reactants to the point of not being able to sustain combustion. The type of metal oxide also has dramatic influence to the amount of energy produced; the higher the oxide, the higher the amount of energy produced. A good example is the difference between manganese(IV) oxide and manganese(II) oxide, where the former produces too high temperature and the latter is barely able to sustain combustion; to achieve good results a mixture with proper ratio of both oxides should be used.\n\nThe reaction rate can be also tuned with particle sizes; coarser particles burn slower than finer particles. The effect is more pronounced with the particles requiring being heated to higher temperature to start reacting. This effect is pushed to the extreme with nano-thermites.\n\nThe temperature achieved in the reaction in adiabatic conditions, when no heat is lost to the environment, can be estimated using the Hess's law – by calculating the energy produced by the reaction itself (subtracting the enthalpy of the reactants from the enthalpy of the products) and subtracting the energy consumed to heating the products (from their specific heat, when the materials only change their temperature, and their enthalpy of fusion and eventually enthalpy of vaporization, when the materials melt or boil). In real conditions, the reaction loses heat to the environment, the achieved temperature is therefore somewhat lower. The heat transfer rate is finite, so the faster the reaction is, the closer to adiabatic condition it runs and the higher is the achieved temperature.\n\nThe most common composition is the iron thermite. The oxidizer used is usually either iron(III) oxide or iron(II,III) oxide. The former produces more heat. The latter is easier to ignite, likely due to the crystal structure of the oxide. Addition of copper or manganese oxides can significantly improve the ease of ignition.\n\nThe original mixture, as invented, used iron oxide in the form of mill scale. The composition was very difficult to ignite.\n\nCopper thermite can be prepared using either copper(I) oxide (CuO, red) or copper(II) oxide (CuO, black). The burn rate tends to be very fast and the melting point of copper is relatively low so the reaction produces a significant amount of molten copper in a very short time. Copper(II) thermite reactions can be so fast that copper thermite can be considered a type of flash powder. An explosion can occur and send a spray of copper drops to considerable distance.\n\nCopper(I) thermite has industrial uses in e.g., welding of thick copper conductors (\"cadwelding\"). This kind of welding is being evaluated also for cable splicing on the US Navy fleet, for use in high-current systems, e.g., electric propulsion.\n\nThermate composition is a thermite one enriched with a salt-based oxidizer (usually nitrates, e.g., barium nitrate, or peroxides). In contrast with thermites, thermates burn with evolution of flame and gases. The presence of the oxidizer makes the mixture easier to ignite and improves penetration of target by the burning composition, as the evolved gas is projecting the molten slag and providing mechanical agitation. This mechanism makes thermate more suitable than thermite for incendiary purposes and for emergency destruction of sensitive equipment (e.g., cryptographic devices), as thermite's effect is more localized.\n\nMetals can burn under the right conditions, similar to the combustion process of wood or gasoline. In fact, rust is the result of oxidation of steel or iron at very slow rates. A thermite reaction is a process in which the correct mixture of metallic fuels combine and ignite. Ignition itself requires extremely high temperatures.\n\nIgnition of a thermite reaction normally requires a sparkler or easily obtainable magnesium ribbon, but may require persistent efforts, as ignition can be unreliable and unpredictable. These temperatures cannot be reached with conventional black powder fuses, nitrocellulose rods, detonators, pyrotechnic initiators, or other common igniting substances. Even when the thermite is hot enough to glow bright red, it doesn't ignite, as it must be at or near white-hot to initiate the reaction. It is possible to start the reaction using a propane torch if done correctly.\n\nOften, strips of magnesium metal are used as fuses. Because metals burn without releasing cooling gases, they can potentially burn at extremely high temperatures. Reactive metals such as magnesium can easily reach temperatures sufficiently high for thermite ignition. Magnesium ignition remains popular among amateur thermite users, mainly because it can be easily obtained.\n\nThe reaction between potassium permanganate and glycerol or ethylene glycol is used as an alternative to the magnesium method. When these two substances mix, a spontaneous reaction begins, slowly increasing the temperature of the mixture until it produces flames. The heat released by the oxidation of glycerine is sufficient to initiate a thermite reaction.\n\nApart from magnesium ignition, some amateurs also choose to use sparklers to ignite the thermite mixture. These reach the necessary temperatures and provide enough time before the burning point reaches the sample. This can be a dangerous method, as the iron sparks, like the magnesium strips, burn at thousands of degrees and can ignite the thermite even though the sparkler itself is not in contact with it. This is especially dangerous with finely powdered thermite.\n\nSimilarly, finely powdered thermite can be ignited by a flint spark lighter, as the sparks are burning metal (in this case, the highly reactive rare-earth metals lanthanum and cerium). Therefore, it is unsafe to strike a lighter close to thermite.\n\nThermite reactions have many uses. Thermite is not an explosive; instead it operates by exposing a very small area to extremely high temperatures. Intense heat focused on a small spot can be used to cut through metal or weld metal components together both by melting metal from the components, and by injecting molten metal from the thermite reaction itself.\n\nThermite may be used for repair by the welding in-place of thick steel sections such as locomotive axle-frames where the repair can take place without removing the part from its installed location.\n\nThermite can be used for quickly cutting or welding steel such as rail tracks, without requiring complex or heavy equipment. However, defects such as slag inclusions and voids (holes) are often present in such welded junctions and great care is needed to operate the process successfully. The numerical analysis of thermite welding of rails has been approached similar to casting cooling analysis. Both this finite element analysis and experimental analysis of thermite rail welds has shown that weld gap is the most influential parameter affecting defect formation. Increasing weld gap has been shown to reduce shrinkage cavity formation and cold lap welding defects, and increasing preheat and thermite temperature further reduces these defects. However, reducing these defects promotes a second form of defect: microporosity. Care must also be taken to ensure that the rails remain straight, without resulting in dipped joints, which can cause wear on high speed and heavy axle load lines.\n\nA thermite reaction, when used to purify the ores of some metals, is called the , or aluminothermic reaction. An adaptation of the reaction, used to obtain pure uranium, was developed as part of the Manhattan Project at Ames Laboratory under the direction of Frank Spedding. It is sometimes called the Ames process.\n\nCopper thermite is used for welding together thick copper wires for the purpose of electrical connections. It is used extensively by the electrical utilities and telecommunications industries (exothermic welded connections).\n\nThermite hand grenades and charges are typically used by armed forces in both an anti-materiel role and in the partial destruction of equipment; the latter being common when time is not available for safer or more thorough methods. For example, thermite can be used for the emergency destruction of cryptographic equipment when there is a danger that it might be captured by enemy troops. Because standard iron-thermite is difficult to ignite, burns with practically no flame and has a small radius of action, standard thermite is rarely used on its own as an incendiary composition. In general, an increase in the volume of gaseous reaction products of a thermite blend increases the heat transfer rate (and therefore damage) of that particular thermite blend. It is usually used with other ingredients that increase its incendiary effects. Thermate-TH3 is a mixture of thermite and pyrotechnic additives that have been found superior to standard thermite for incendiary purposes. Its composition by weight is generally about 68.7% thermite, 29.0% barium nitrate, 2.0% sulfur, and 0.3% of a binder (such as PBAN). The addition of barium nitrate to thermite increases its thermal effect, produces a larger flame, and significantly reduces the ignition temperature. Although the primary purpose of Thermate-TH3 by the armed forces is as an incendiary anti-materiel weapon, it also has uses in welding together metal components.\n\nA classic military use for thermite is disabling artillery pieces, and it has been used for this purpose since World War II, such as at Pointe du Hoc, Normandy. Thermite can permanently disable artillery pieces without the use of explosive charges, and therefore thermite can be used when silence is necessary to an operation. This can be done by inserting one or more armed thermite grenades into the breech and then quickly closing it; this welds the breech shut and makes loading the weapon impossible. Alternatively, a thermite grenade discharged inside the barrel of the gun fouls the barrel, making the weapon dangerous to fire. Thermite can also weld the traversing and elevation mechanism of the weapon, making it impossible to aim properly.\n\nDuring World War II, both German and Allied incendiary bombs used thermite mixtures. Incendiary bombs usually consisted of dozens of thin thermite-filled canisters (bomblets) ignited by a magnesium fuse. Incendiary bombs created massive damage in many cities due to fires started by the thermite. Cities that primarily consisted of wooden buildings were especially susceptible. These incendiary bombs were utilized primarily during nighttime air raids. Bombsights could not be used at night, creating the need to use munitions that could destroy targets without the need for precision placement. In recent times, thermite was used by Russia (under Putin's administration) in the Syrian Civil War. In mid 2016, Russian Television by mistake showed video of RBK-500 cluster bombs loaded with ZAB-2.5SM thermite incendiaries mounted on Su-34 bomber aircraft. In the video, the defense minister of Russia can be seen supervising the attacks on civilian areas.\n\nThermite usage is hazardous due to the extremely high temperatures produced and the extreme difficulty in smothering a reaction once initiated. Small streams of molten iron released in the reaction can travel considerable distances and may melt through metal containers, igniting their contents. Additionally, flammable metals with relatively low boiling points such as zinc (with a boiling point of 907 °C, which is about 1,370 °C below the temperature at which thermite burns) could potentially spray superheated boiling metal violently into the air if near a thermite reaction.\n\nIf, for some reason, thermite is contaminated with organics, hydrated oxides and other compounds able to produce gases upon heating or reaction with thermite components, the reaction products may be sprayed. Moreover, if the thermite mixture contains enough empty spaces with air and burns fast enough, the super-heated air also may cause the mixture to spray. For this reason it is preferable to use relatively crude powders, so the reaction rate is moderate and hot gases could escape the reaction zone.\n\nPreheating of thermite before ignition can easily be done accidentally, for example by pouring a new pile of thermite over a hot, recently ignited pile of thermite slag. When ignited, preheated thermite can burn almost instantaneously, releasing light and heat energy at a much higher rate than normal and causing burns and eye damage at what would normally be a reasonably safe distance.\n\nThe thermite reaction can take place accidentally in industrial locations where workers use abrasive grinding and cutting wheels with ferrous metals. Using aluminium in this situation produces a mixture of oxides that can explode violently.\n\nMixing water with thermite or pouring water onto burning thermite can cause a steam explosion, spraying hot fragments in all directions.\n\nThermite's main ingredients were also utilized for their individual qualities, specifically reflectivity and heat insulation, in a paint coating or dope for the German zeppelin \"Hindenburg\", possibly contributing to its fiery destruction. This was a theory put forward by the former NASA scientist Addison Bain, and later tested in small scale by the scientific reality-TV show \"MythBusters\" with semi-inconclusive results (it was proven not to be the fault of the thermite reaction alone, but instead conjectured to be a combination of that and the burning of hydrogen gas that filled the body of the \"Hindenburg\"). The \"MythBusters\" program also tested the veracity of a video found on the Internet, whereby a quantity of thermite in a metal bucket was ignited while sitting atop several blocks of ice, causing a sudden explosion. They were able to confirm the results, finding huge chunks of ice as far as 50m from the point of explosion. Co-host Jamie Hyneman conjectured that this was due to the thermite mixture aerosolizing, perhaps in a cloud of steam, causing it to burn even faster. Hyneman also voiced skepticism about another theory explaining the phenomenon: that the reaction somehow separated the hydrogen and oxygen in the ice and then ignited them. This explanation claims that the explosion is due to the reaction of high temperature molten aluminium with water. Aluminium reacts violently with water or steam at high temperatures, releasing hydrogen and oxidizing in the process. The speed of that reaction and the ignition of the resulting hydrogen can easily account for the explosion verified.<ref name=\"Aluminium Times Vol 11 n 3 Jul/Aug 2009\"></ref> This process is akin to the explosive reaction caused by dropping metallic potassium into water.\n\n\n"}
{"id": "24561471", "url": "https://en.wikipedia.org/wiki?curid=24561471", "title": "Union process", "text": "Union process\n\nThe Union process was an above ground shale oil extraction technology for production of shale oil, a type of synthetic crude oil. The process used a vertical retort where heating causes decomposition of oil shale into shale oil, oil shale gas and spent residue. The particularity of this process is that oil shale in the retort moves from the bottom upward to the top, countercurrent to the descending hot gases, by a mechanism known as a rock pump. The process technology was invented by the American oil company Unocal Corporation in late 1940s and was developed through several decades. The largest oil shale retort ever built was the Union B type retort.\n\nUnion Oil Company of California (Unocal) started its oil shale activities in 1920s. In 1921, it acquired an oil shale tract in the Parachute Creek area of Colorado, southern Piceance Basin. The development of the Union process began in the late 1940s, when the Union A retort was designed. This technology was tested between 1954 and 1958 at the company-owned tract in the Parachute Creek. During these tests, up to 1,200 tonne per day of oil shale was processed, resulting of shale oil, which was refined at a Colorado refinery. More than of gasoline and fuels were produced. This production was finally shut down in 1961 due to cost.\n\nIn 1974, the Union B process, evolved from the Union A process, was developed. In 1976, Union announced its plans to build a Union B demonstration plant. Construction started in 1981 at Long Ridge in Garfield County, Colorado, and the plant was started its operations in 1986. It was closed in 1991 after production of shale oil.\n\nThe Union process can be operated in two different combustion modes, which are direct and indirect. The Union A (direct) process is similar to the gas combustion retort technology, classified as an internal combustion method, while the Union B (indirect) process is classified as an externally generated hot gas method.\n\nThe Union retort is a vertical shaft retort. The main difference to other vertical shaft retorts such as Kiviter, Petrosix, Paraho and Fushun is that crushed oil shale is fed through the bottom of the retort rather than the top. Lumps of oil shale in size of are moved upwards through the retort by a solids pump (known as a \"rock pump\"). Hot gases, generated by internal combustion or circulated through the top of the retort, decompose the oil shale while descending. The pyrolysis occurs at the temperature of to . Condensed shale oil and gases are removed from the retort at the bottom. Part of the gases is recirculated for pyrolysis and fueling combustion, while other part could be used as product gas. The spent shale is removed from the top of the retort. After cooling with a water, it is conveyed to the waste disposal.\n\nThe Union retort design has several advantages. The reducing atmosphere in the retort allows the removal of sulfur and nitrogen compounds through the formation of hydrogen sulfide and ammonia. Oil vapors are cooled by the raw oil, thus minimizing polymer formation among the hydrocarbon fractions.\n\n"}
{"id": "26398761", "url": "https://en.wikipedia.org/wiki?curid=26398761", "title": "Whitebox Geospatial Analysis Tools", "text": "Whitebox Geospatial Analysis Tools\n\nWhitebox Geospatial Analysis Tools (GAT) is an open-source and cross-platform Geographic information system (GIS) and remote sensing software package that is distributed under the GNU General Public License. It has been developed by the members of the University of Guelph Centre for Hydrogeomatics and is intended for advanced geospatial analysis and data visualization in research and education settings. The package features a friendly graphical user interface (GUI) with help and documentation built into the dialog boxes for each of the more than 410 analysis tools. Users are also able to access extensive off-line and online help resources. The Whitebox GAT project started as a replacement for the Terrain Analysis System (TAS), a geospatial analysis software package written by John Lindsay. The current release support raster and vector (shapefile) data structures. There are also extensive functionality for processing laser scanner (LiDAR) data contained with LAS files.\n\nWhitebox GAT is extendible. Users are able to create and add custom tools or plugins using any JVM language. The software also allows scripting using the programming languages Groovy, JavaScript, and Python.\n\nWhitebox GAT contains more than 385 tools to perform spatial analysis on raster data sets. The following is an incomplete list of some of the more commonly used tools:\n\n\nThe Whitebox GAT project has adopted a novel approach for linking the software's development and user communities, known as software transparency, or open-access software (considered an extension of open-source software). The philosophy of transparency in software states that the user 1) has the right to view the underlying workings of a tool or operation, and 2) should be able to access this information in a way that reduces, or ideally eliminates, any barriers to viewing and interpreting it. This concept was developed as a response to the fact that the code base of many open-source projects can be so massive and its organization so complex that individual users often find the task of interpreting the underlying code too daunting when they are interested in a small portion of the overall code base, e.g. if the user would like to know how a particular tool or algorithm operates. Furthermore, when the software's source code is written in an unfamiliar programming language, the task of interpreting the code is made even more difficult. For some open-source projects, these characteristics can create a divide between the development and user communities, often restricting future development to a few individuals that have been involved in the project during the earliest periods of development. The View Code button that is present on all Whitebox GAT tools is the embodiment of this software-transparency philosophy by pointing the user to the specific region of the source-code that is relevant to a particular tool, also allowing for code conversion to other programming languages. The Whitebox GAT logo is also representative of the open and transparent characteristic of the software, being a transparent glass cube, open on one face.\n\n"}
{"id": "17885106", "url": "https://en.wikipedia.org/wiki?curid=17885106", "title": "Wireless engineering", "text": "Wireless engineering\n\nWireless Engineering is the branch of engineering which addresses the design, application, and research of wireless communication systems and technologies.\n\nWireless engineering is an engineering subject dealing with engineering problems using wireless technology such as radio communications and radar, but it is more general than the conventional radio engineering. It may include using other techniques such as acoustic, infrared, and optical technologies.\n\nWireless technologies have skyrocketed since their late 19th Century advancements. With the invention of the FM Radio in 1935, wireless communications have become a concentrated focus of both private and government sectors.\n\nAuburn University's Samuel Ginn College of Engineering was the first in the United States to offer a formalized undergraduate degree in such a field. The program was initiated by Samuel Ginn, an Auburn Alumni, in 2001.\n\nMacquarie University in Sydney, was the first University to offer Wireless Engineering in Australia. The university works closely with nearby industries in research and teaching development in wireless engineering.\n\nUniversiti Teknikal Malaysia Melaka in Malacca, was the first University to offer Wireless Communication Engineering in Malaysia.\n\nWireless engineering contains a wide spectrum of application, most notably cellular networks. The recent popularity of cellular networks has created a vast career demand with a large repository. The popularity has also sparked many wireless innovations, such as increased network capacity, 3G, cryptology and network security technologies.\n"}
{"id": "27343633", "url": "https://en.wikipedia.org/wiki?curid=27343633", "title": "Yamar Electronics", "text": "Yamar Electronics\n\nYamar specializes in semiconductor smart transceivers for communication over noisy AC and DC battery power lines. The DC-BUS technology enables the transfer of both power and data over a single line utilizing advanced digital communication techniques, tailored for overcoming the hostile and noisy environment encountered in power lines, \nYamar was founded in 1994 by Yair Maryanka with the vision to use the direct current power line for data transfer eliminating additional wires, reducing its cost and weight, and increasing reliability of cables, which is a strategic goal in many industries. The DC-BUS technology and products. are used in automotive, avionics, industrial, and other applications worldwide allowing use of existing data bus protocols such as CAN, LIN/UART.\n"}
