{"id": "11878024", "url": "https://en.wikipedia.org/wiki?curid=11878024", "title": "2,5-Dimethylfuran", "text": "2,5-Dimethylfuran\n\n2,5-Dimethylfuran is a heterocyclic compound with the formula (CH)CHO. Although often abbreviated DMF, it should not be confused with dimethylformamide. A derivative of furan, this simple compound is a potential biofuel, being derivable from cellulose.\n\nFructose can be converted into 2,5-dimethylfuran in a catalytic biomass-to-liquid process.Production of DMF from 5-HMF is gaining lot of interest. The conversion of fructose to DMF proceeds via hydroxymethylfurfural.\n\nFructose is obtainable from glucose, a building block in cellulose.\n\nDMF has a number of attractions as a biofuel. It has an energy density 40% greater than that of ethanol, making it comparable to gasoline (petrol). It is also chemically stable and, being insoluble in water, does not absorb moisture from the atmosphere. Evaporating dimethylfuran during the production process also requires around one third less energy than the evaporation of ethanol, although it has a boiling point some 14 °C higher, at 92 °C, compared to 78 °C for ethanol.\n\nThe ability to efficiently and rapidly produce dimethylfuran from fructose, found in fruit and some root vegetables, or from glucose, which can be derived from starch and cellulose - all widely available in nature - adds to the attraction of dimethylfuran, although safety issues must be examined. Bioethanol and biodiesel are currently the leading liquid biofuels.\n\nThe stoichiometric air/fuel ratio of dimethylfuran is 10.72, compared to ethanol at 8.95 and gasoline at 14.56. This means that burning dimethylfuran requires approximately 33% less air than the same quantity of gasoline, but approximately 20% more air than the same quantity of ethanol.\n\nThe calorific value of liquid dimethylfuran is 33.7 MJ/kg, compared to 26.9 MJ/kg for ethanol and 43.2 MJ/kg for gasoline. The research octane number (RON) of dimethylfuran is 119. The latent heat of vaporization at 20 °C is 31.91 kJ/mol. Recent tests in a single-cylinder gasoline engine found that the thermal efficiency of burning dimethylfuran is similar to that of gasoline.\n\n2,5-Dimethylfuran serves as a scavenger for singlet oxygen, a property which has been exploited for the determination of singlet oxygen in natural waters. The mechanism involves a Diels-Alder reaction followed by hydrolysis, ultimately leading to diacetylethylene and hydrogen peroxide as products. More recently, furfuryl alcohol has been used for the same purpose.\nThis compound has also been proposed as an internal standard for NMR spectroscopy. 2,5-Dimethylfuran has singlets in its H NMR spectrum at δ 2.2 and 5.8; the singlets give reliable integrations, while the positions of the peaks do not interfere with many analytes. The compound also has an appropriate boiling point of 92 °C which prevents evaporative losses, yet is easily removed.\n\n2,5-Dimethylfuran forms upon thermal degradation of some sugars and has been identified in trace amounts as a component of caramelized sugars.\n\n2,5-Dimethylfuran plays a role in the mechanism for the neurotoxicity of hexane in humans. Together with hexane-2,5-dione and 4,5-dihydroxy-2-hexanone, it is one of the main metabolites of hexane.\n\n2,5-Dimethylfuran has been identified as one of the components of cigar smoke with low cilatoxicity (ability to adversely affect the cilia in the respiratory tract that are responsible for removing foreign particles). Its blood concentration can be used as a biomarker for smoking.\n\nComparison of Safety Data Sheets shows that human handling of 2,5-dimethylfuran is approximately as hazardous as handling gasoline.\n"}
{"id": "18806192", "url": "https://en.wikipedia.org/wiki?curid=18806192", "title": "20-pair colour code (Australia)", "text": "20-pair colour code (Australia)\n\nThe 20-pair colour code is a colour code used in Australia to identify individual conductors in a kind of electrical telecommunication wiring for indoor use, known as twisted pair cables. The colours are applied to the insulation that covers each conductor. The first colour is chosen from one group of five colours.\n\nThe combinations are also shown in the table below showing the colour for each wire (\"1\" and \"2\") and the pair number.\n\nNote that there is no colour called \"Grey\". The correct colour terminology is \"Slate\". \n\n"}
{"id": "47433257", "url": "https://en.wikipedia.org/wiki?curid=47433257", "title": "5-Tiles", "text": "5-Tiles\n\n5-Tiles is a virtual keyboard for mobile devices with touchscreen that run the Android operating system. It is characterized by needing a small amount of space leaving as much space as possible to the software that needs the keyboard. As the name indicates there are exactly five keys on the keyboard on one line of keys. Characters are typed by tap and swipe gestures.\n\nThe five keys of this keyboard all have different colors to be easily distinguishable. You have the following options to type a character:\nThe concept of this keyboard was invented in 2004. The co-founder of the company creating this keyboard, Michal Kubacki, first didn't have skills in programming. He learned some skills that allowed him to write basic software for testing purposes and to further develop the keyboard. He got in touch with people who helped him creating the product as a virtual keyboard for Android devices. An early version was downloaded thousands of times. At the 2013 Droidcon Demo Camp the company won the second place. It was also awarded in other competitions.\n"}
{"id": "7019701", "url": "https://en.wikipedia.org/wiki?curid=7019701", "title": "Air data inertial reference unit", "text": "Air data inertial reference unit\n\nAn air data inertial reference unit (ADIRU) is a key component of the integrated air data inertial reference system (ADIRS), which supplies air data (airspeed, angle of attack and altitude) and inertial reference (position and attitude) information to the pilots' electronic flight instrument system displays as well as other systems on the aircraft such as the engines, autopilot, aircraft flight control system and landing gear systems. An ADIRU acts as a single, fault tolerant source of navigational data for both pilots of an aircraft. It may be complemented by a secondary attitude air data reference unit (SAARU), as in the Boeing 777 design.\n\nThis device is used on various military aircraft as well as civilian airliners starting with the Airbus A320 and Boeing 777.\n\nAn ADIRS consists of up to three fault tolerant ADIRUs located in the aircraft electronic rack, an associated control and display unit (CDU) in the cockpit and remotely mounted air data modules (ADMs). The No 3 ADIRU is a redundant unit that may be selected to supply data to either the commander's or the co-pilot's displays in the event of a partial or complete failure of either the No 1 or No 2 ADIRU. There is no cross-channel redundancy between the Nos 1 and 2 ADIRUs, as No 3 ADIRU is the only alternate source of air and inertial reference data. An inertial reference (IR) fault in ADIRU No 1 or 2 will cause a loss of attitude and navigation information on their associated primary flight display (PFD) and navigation display (ND) screens. An air data reference (ADR) fault will cause the loss of airspeed and altitude information on the affected display. In either case the information can only be restored by selecting the No 3 ADIRU.\n\nEach ADIRU comprises an ADR and an inertial reference (IR) component.\n\nThe air data reference (ADR) component of an ADIRU provides airspeed, Mach number, angle of attack, temperature and barometric altitude data. Ram air pressure and static pressures used in calculating airspeed are measured by small ADMs located as close as possible to the respective pitot and static pressure sensors. ADMs transmit their pressures to the ADIRUs through ARINC 429 data buses.\n\nThe IR component of an ADIRU gives attitude, flight path vector, ground speed and positional data. The ring laser gyroscope is a core enabling technology in the system, and is used together with accelerometers, GPS and other sensors to provide raw data. The primary benefits of a ring laser over older mechanical gyroscopes are that there are no moving parts, it is rugged and lightweight, frictionless and does not resist a change in precession.\n\nAnalysis of complex systems is itself so difficult as to be subject to errors in the certification process. Complex interactions between flight computers and ADIRU's can lead to counter-intuitive behaviour for the crew in the event of a failure. In the case of Qantas Flight 72, the captain switched the source of IR data from ADIRU1 to ADIRU3 following a failure of ADIRU1; however ADIRU1 continued to supply ADR data to the captain's primary flight display. In addition, the master flight control computer (PRIM1) was switched from PRIM1 to PRIM2, then PRIM2 back to PRIM1, thereby creating a situation of uncertainty for the crew who did not know which redundant systems they were relying upon.\n\nReliance on redundancy of aircraft systems and can also lead to delays in executing needed repairs as airline operators rely on the redundancy to keep the aircraft system working without having to repair faults immediately.\n\nOn May 3, 2000, the FAA issued airworthiness directive 2000-07-27, addressing dual critical failures during flight, attributed to power supply issues affecting early Honeywell HG2030 and HG2050 ADIRU ring laser gyros used on several Boeing 737, 757, Airbus A319, A320, A321, A330, and A340 models.\n\nOn 27 January 2004 the FAA issued airworthiness directive 2003-26-03 (later superseded by AD 2008-17-12) which called for modification to the mounting of ADIRU3 in Airbus A320 family aircraft to prevent failure and loss of critical attitude and airspeed data.\n\nOn 25 June 2005, an Alitalia Airbus A320-200 registered as I-BIKE departed Milan with a defective ADIRU as permitted by the Minimum Equipment List. While approaching London Heathrow Airport during deteriorating weather another ADIRU failed, leaving only one operable. In the subsequent confusion the third was inadvertently reset, losing its reference heading and disabling several automatic functions. The crew was able to effect a safe landing after declaring a Pan-pan.\n\nOn 1 August 2005 a serious incident involving Malaysia Airlines Flight 124, occurred when a Boeing 777-2H6ER (9M-MRG) flying from Perth to Kuala Lumpur–International also involved an ADIRU fault resulting in uncommanded manoeuvres by the aircraft acting on false indications. In that incident the incorrect data impacted all planes of movement while the aircraft was climbing through . The aircraft pitched up and climbed to around , with the stall warning activated. The pilots recovered the aircraft with the autopilot disengaged and requested a return to Perth. During the return to Perth, both the left and right autopilots were briefly activated by the crew, but in both instances the aircraft pitched down and banked to the right. The aircraft was flown manually for the remainder of the flight and landed safely in Perth. There were no injuries and no damage to the aircraft. The ATSB found that the main probable cause of this incident was a latent software error which allowed the ADIRU to use data from a failed accelerometer.\n\nThe US Federal Aviation Administration issued Emergency Airworthiness Directive (AD) 2005-18-51 requiring all 777 operators to install upgraded software to resolve the error.\n\nOn 12 September 2006, Qantas Flight 68, Airbus A330 registration VH-QPA, from Hong Kong to Perth exhibited ADIRU problems but without causing any disruption to the flight. At and estimated position north of Learmonth, Western Australia, \"NAV IR1 FAULT\" then, 30 minutes later, \"NAV ADR 1 FAULT\" notifications were received on the ECAM identifying navigation system faults in Inertial Reference Unit 1, then in ADR 1 respectively. The crew reported to the later Qantas Flight 72 investigation involving the same airframe and ADIRU that they had received numerous warning and caution messages which changed too quickly to be dealt with. While investigating the problem, the crew noticed a weak and intermittent \"ADR 1 FAULT\" light and elected to switch off ADR 1, after which they experienced no further problems. There was no impact on the flight controls throughout the event. The ADIRU manufacturer's recommended maintenance procedures were carried out after the flight and system testing found no further fault.\n\nOn 7 February 2008, a similar aircraft (VH-EBC) operated by Qantas subsidiary Jetstar Airways was involved in a similar occurrence while conducting the JQ7 service from Sydney to Ho Chi Minh City, Vietnam. In this event - which occurred east of Learmonth - many of the same errors occurred in the ADIRU unit. The crew followed the relevant procedure applicable at the time and the flight continued without problems.\n\nOn 6 August 2008, the FAA issued airworthiness directive 2008-17-12 expanding on the requirements of the earlier AD 2003-26-03 which had been determined to be an insufficient remedy. In some cases it called for replacement of ADIRUs with newer models, but allowed 46 months from October 2008 to implement the directive.\n\nThe ATSB has yet to confirm if this event is related to the other Airbus A330 ADIRU occurrences.\n\nOn 7 October 2008, Qantas Flight 72, using the same aircraft involved in the Flight 68 incident, departed Singapore for Perth. Some time into the flight, while cruising at 37,000 ft, a failure in the No.1 ADIRU led to the autopilot automatically disengaging followed by two sudden uncommanded pitch down manoeuvres, according to the Australian Transport Safety Bureau (ATSB). The accident injured up to 74 passengers and crew, ranging from minor to serious injuries. The aircraft was able to make an emergency landing without further injuries. The aircraft was equipped with a Northrop Grumman made ADIRS, which investigators sent to the manufacturer for further testing.\n\nOn 27 December 2008, Qantas Flight 71 from Perth to Singapore, a different Qantas A330-300 with registration VH-QPG was involved in an incident at 36,000 feet approximately north-west of Perth and south of Learmonth Airport at 1729 WST. The autopilot disconnected and the crew received an alert indicating a problem with ADIRU Number 1.\n\nOn 15 January 2009, the European Aviation Safety Agency issued Emergency Airworthiness Directive No 2009-0012-E to address the above A330 and A340 Northrop-Grumman ADIRU problem of incorrectly responding to a defective inertial reference. In the event of a NAV IR fault the directed crew response is now to \"select OFF the relevant IR, select OFF the relevant ADR, and then turn the IR rotary mode selector to the OFF position.\" The effect is to ensure that the faulted IR is powered off so that it no longer can send erroneous data to other systems.\n\nOn 1 June 2009, Air France Flight 447, an Airbus A330 en route from Rio de Janeiro to Paris, crashed in the Atlantic Ocean after transmitting automated messages indicating faults with various equipment, including the ADIRU. While examining possibly related events of weather-related loss of ADIRS, the NTSB decided to investigate two similar cases on cruising A330s. On a 21 May 2009 Miami-Sao Paulo TAM Flight 8091 registered as PT-MVB, and on a 23 June 2009 Hong Kong-Tokyo Northwest Airlines Flight 8 registered as N805NW each saw sudden loss of airspeed data at cruise altitude and consequent loss of ADIRS control.\n\n\n"}
{"id": "8189114", "url": "https://en.wikipedia.org/wiki?curid=8189114", "title": "Anti-hijack system", "text": "Anti-hijack system\n\nAn anti-hijack system is an electronic system fitted to motor vehicles to deter criminals from hijacking them. Although these types of systems are becoming more common on newer cars, they have not caused a decrease in insurance premiums as they are not as widely known as other more common anti-theft systems such as alarms or steering locks. It can also be a part of an alarm or immobiliser system. An approved anti-hijacking system will achieve a safe, quick shutdown of the vehicle it is attached to. There are also mechanical anti-hijack devices. \n\nThere are three basic principles on which the systems work.\n\nA lockout system is armed when the driver turns the ignition key to the \"on\" position and carries out a specified action, usually flicking a hidden switch or depressing the brake pedal twice. It is activated when the vehicle drops below a certain speed or becomes stationary, and will cause all of the vehicles doors to automatically lock, to prevent against thieves stealing the vehicle when it is stopped, for example at a traffic light or pedestrian crossing.\n\nA transponder system is a system which is always armed until a device, usually a small RFID transponder, enters the vehicle's transmitter radius. Since the device is carried by the driver, usually in their wallet or pocket, if the driver leaves the immediate vicinity of the vehicle, so will the transponder, causing the system to assume the vehicle has been hijacked and disable it. \n\nAs the transponder itself is concealed, the thief would not be aware that such a system is active on a vehicle until they had ejected the driver and moved the vehicle out of range of the driver (usually only a couple of meters). This is probably the most common anti-hijack system, and a central locking system that uses the same concept was demonstrated by Jeremy Clarkson on an old episode of the BBC Top Gear program where he teased a butler by asking him to put his bags in a Mercedes-Benz S600 but didn't give him the RFID transponder. The butler was confused when the S600 doors wouldn't open when he tried, but when Jeremy approached with the transponder in his pocket, the system acknowledged this and unlocked the car, allowing Jeremy to simply pull the door handle to gain entry to the vehicle.\n\nA microswitch system is always armed and is usually activated if one of the vehicle doors is opened and closed again while the vehicle's engine is running. Once the system has been activated, the driver will have a set time limit to disarm it by entering a code before the vehicle takes measures. \n\nIf the system is not disarmed in the time window, it will warn the driver by sounding the vehicle's horn once every 10 seconds for 30 seconds, at which point the system will start sounding the horn at much shorter intervals and will usually activate the vehicle's hazard lights. \n\nAt this point the immobiliser circuit will also start rapidly pulsing for 40 seconds, completely disabling the engine and eventually bringing the vehicle to a stop. If the thief switches the ignition to the \"off\" position and back to the \"on\" position again, the horn will restart and operate constantly and the hazard lights will flash for 60 seconds. \n\nThe immobiliser circuit will close for 15 seconds and will rapidly pulse for 15 seconds before re-opening the circuit, allowing the vehicle to be driven to a safe location before once again being immobilised. The hazard lights will continue to flash, and on every subsequent attempt to start the vehicle will cause the horn to operate for 30 seconds, but the immobilizer circuit will not open, so the vehicle will not start and the hazard lights will keep flashing until the vehicle's battery is drained or the system is disarmed.\n\nBlocks steering shaft rotation and prevent rotation of the steering wheel. When set mechanical anti-hijack device the wheels do not turn and the car can travel only straight forward and backward. Or in the side back and forth, before installing the mechanical anti-hijack device lock if the wheels turn. To go by car to the established mechanical anti-hijack device is not possible.\n\n\nInformation and photo, mechanicals anti-hijack devices on cars\n"}
{"id": "20842790", "url": "https://en.wikipedia.org/wiki?curid=20842790", "title": "Aquifer storage and recovery", "text": "Aquifer storage and recovery\n\nAquifer storage and recovery (ASR) is the direct injection of surface water supplies such as potable water, reclaimed water, or river water into an aquifer for later recovery and use. ASR has been done for municipal, industry and agriculture use.\n\nThe first ASR well with a downhole control valve was installed in Highlands Ranch, CO in 1992 for Centennial Water and Sanitation District. Since then, over 40 ASR wells have been installed for many different municipalities. These wells range in depths from to below ground surface, with injection rates commonly between and per minute (gpm) per well.\n\nThe use of ASR in Florida has been examined to determine potential benefits for the Everglades and other Florida water systems under the Comprehensive Everglades Restoration Plan (CERP). An estimate of 333 ASR wells would be implemented as part of CERP and used to store, treat and supply excess surface water to the Everglades and other systems of water during dry periods. \n\nDoubts remain about the benefit of introducing ASR in such large capacity to Florida due to a predominantly karst geography. Current or potential problems include: (1) poor recovery due to mixing of the injected fresh water with the existing brackish to saline water in the aquifer; (2) pre-existing quality of water introduced to ASR; and (3) potential risk of resulting water quality due to mixing of injected freshwater and existing aquifer.\n\nThe first agriculture ASR wells were put into service in Oregon in the autumn of 2006 and have injected well over of water during the winter and spring flood flow times using artificial recharge (AR) of flood water as their water source. This shallow recharged water is then recovered as potable water and injected into the deep basalt aquifer.\n\nDuring the injection process, electrical energy can be generated by the head pressure of the water flowing back into the aquifer. This stored water is recovered during late summer and early autumn for irrigation needs.\n\nBoth of these well types use a down-hole control valve. ASR can also be used to re-inject water used by HVAC systems to maintain the ground water levels and store the thermal differences from summertime cooling for winter time heating. Industry can also capture cold winter waters and store it for summertime use and avoid the need for cooling water in the summer for industrial processes. This may also free up short supplies of summer time water for other beneficial uses. This reinjection process may also avoid the cost of surface disposal and avoid the increased thermal load to the rivers and streams during the summer air conditioning season.\n\nThe Texas cities of El Paso, Kerrville and San Antonio use ASR, providing water resources to these water-vulnerable communities. A University of Florida report ranked daily per-capita water availability for 225 large urban areas across the U.S. The study weighed fresh water available to cities from naturally occurring and constructed sources such as reservoirs, aquifers and imports. Of the cities reviewed, San Antonio ranked last, or most vulnerable, and El Paso ranked as 10th-worst, though other Texas cities made the list.\n\nSan Antonio stores drinking water in its Carrizo ASR facility, which contains more than 91,000 acre-feet of water and has a maximum capacity of 120,000 acre-feet.\n\nA 2010 Texas Water Development Board (TWDB) survey of Texas water utilities found four primary objections to ASR in other parts of Texas: legal and physical limitations, the quality of the recovered water, cost-effectiveness and the potential for other pumpers to capture the utility’s stored water.\n\nIn South Australia, the City of Salisbury in northern Adelaide has since 1994 played a pioneering role in establishing the viability of the capture and treatment of urban stormwater runoff within artificial wetlands, and injecting the treated water into Tertiary aquifers in winter, for later use by industry, and for irrigation of city parks and school playing fields in summer. In 2009 the capacity of the Salisbury ASR schemes was around 5 gigalitres per annum (expected to rise to 14 GL by 2014) and this success has led to other local government areas across Adelaide undertaking similar ASR projects.\n\n\n"}
{"id": "5389424", "url": "https://en.wikipedia.org/wiki?curid=5389424", "title": "Arduino", "text": "Arduino\n\nArduino is an open-source hardware and software company, project and user community that designs and manufactures single-board microcontrollers and microcontroller kits for building digital devices and interactive objects that can sense and control objects in the physical and digital world. Its products are licensed under the GNU Lesser General Public License (LGPL) or the GNU General Public License (GPL), permitting the manufacture of Arduino boards and software distribution by anyone. Arduino boards are available commercially in preassembled form or as do-it-yourself (DIY) kits.\n\nArduino board designs use a variety of microprocessors and controllers. The boards are equipped with sets of digital and analog input/output (I/O) pins that may be interfaced to various expansion boards or breadboards (\"shields\") and other circuits. The boards feature serial communications interfaces, including Universal Serial Bus (USB) on some models, which are also used for loading programs from personal computers. The microcontrollers are typically programmed using a dialect of features from the programming languages C and C++. In addition to using traditional compiler toolchains, the Arduino project provides an integrated development environment (IDE) based on the Processing language project.\n\nThe Arduino project started in 2003 as a program for students at the Interaction Design Institute Ivrea in Ivrea, Italy, aiming to provide a low-cost and easy way for novices and professionals to create devices that interact with their environment using sensors and actuators. Common examples of such devices intended for beginner hobbyists include simple robots, thermostats and motion detectors.\n\nThe name \"Arduino\" comes from a bar in Ivrea, Italy, where some of the founders of the project used to meet. The bar was named after Arduin of Ivrea, who was the margrave of the March of Ivrea and King of Italy from 1002 to 1014.\n\nThe Arduino project started at the Interaction Design Institute Ivrea (IDII) in Ivrea, Italy. At that time, the students used a BASIC Stamp microcontroller at a cost of $50, a considerable expense for many students. In 2003 Hernando Barragán created the development platform \"Wiring\" as a Master's thesis project at IDII, under the supervision of Massimo Banzi and Casey Reas. Casey Reas is known for co-creating, with Ben Fry, the Processing development platform. The project goal was to create simple, low cost tools for creating digital projects by non-engineers. The Wiring platform consisted of a printed circuit board (PCB) with an ATmega168 microcontroller, an IDE based on Processing and library functions to easily program the microcontroller.\nIn 2003, Massimo Banzi, with David Mellis, another IDII student, and David Cuartielles, added support for the cheaper ATmega8 microcontroller to Wiring. But instead of continuing the work on Wiring, they forked the project and renamed it \"Arduino\".\n\nThe initial Arduino core team consisted of Massimo Banzi, David Cuartielles, Tom Igoe, Gianluca Martino, and David Mellis, but Barragán was not invited to participate.\n\nFollowing the completion of the Wiring platform, lighter and less expensive versions were distributed in the open-source community.\n\nIt was estimated in mid-2011 that over 300,000 official Arduinos had been commercially produced, and in 2013 that 700,000 official boards were in users' hands.\n\nIn October 2016, Federico Musto, Arduino's former CEO, secured a 50% ownership of the company. In April 2017, Wired reported that Musto had \"fabricated his academic record... On his company's website, personal LinkedIn accounts, and even on Italian business documents, Musto was until recently listed as holding a PhD from the Massachusetts Institute of Technology. In some cases, his biography also claimed an MBA from New York University.\" Wired reported that neither University had any record of Musto's attendance, and Musto later admitted in an interview with Wired that he had never earned those degrees.\n\nAround that same time, Massimo Banzi announced that the Arduino Foundation would be \"a new beginning for Arduino.\" But a year later, the Foundation still hasn't been established, and the state of the project remains unclear.\n\nThe controversy surrounding Musto continued when, in July 2017, he reportedly pulled many Open source licenses, schematics, and code from the Arduino website, prompting scrutiny and outcry.\n\nIn October 2017, Arduino announced its partnership with ARM Holdings (ARM). The announcement said, in part, \"ARM recognized independence as a core value of Arduino ... without any lock-in with the ARM architecture.” Arduino intends to continue to work with all technology vendors and architectures.\n\nIn early 2008, the five cofounders of the Arduino project created a company, Arduino LLC, to hold the trademarks associated with Arduino. The manufacture and sale of the boards was to be done by external companies, and Arduino LLC would get a royalty from them. The founding bylaws of Arduino LLC specified that each of the five founders transfer ownership of the Arduino brand to the newly formed company.\n\nAt the end of 2008, Gianluca Martino's company, Smart Projects, registered the Arduino trademark in Italy and kept this a secret from the other cofounders for about two years. This was revealed when the Arduino company tried to register the trademark in other areas of the world (they originally registered only in the US), and discovered that it was already registered in Italy. Negotiations with Gianluca and his firm to bring the trademark under control of the original Arduino company failed. In 2014, Smart Projects began refusing to pay royalties. They then appointed a new CEO, Federico Musto, who renamed the company \"Arduino SRL\" and created the website \"arduino.org\", copying the graphics and layout of the original \"arduino.cc\". This resulted in a rift in the Arduino development team.\n\nIn January 2015, Arduino LLC filed a lawsuit against Arduino SRL.\n\nIn May 2015, Arduino LLC created the worldwide trademark Genuino, used as brand name outside the United States.\n\nAt the World Maker Faire in New York on October 1, 2016, Arduino LLC co-founder and CEO Massimo Banzi and Arduino SRL CEO Federico Musto announced the merger of the two companies.\n\nBy 2017 Arduino AG owned many Arduino trademarks. In July 2017 BCMI, founded by Massimo Banzi, David Cuartielles, David Mellis and Tom Igoe, acquired Arduino AG and all the Arduino trademarks. Fabio Violante is the new CEO replacing Federico Musto, who no longer works for Arduino AG.\n\nArduino is open-source hardware. The hardware reference designs are distributed under a Creative Commons Attribution Share-Alike 2.5 license and are available on the Arduino website. Layout and production files for some versions of the hardware are also available.\n\nAlthough the hardware and software designs are freely available under copyleft licenses, the developers have requested the name \"Arduino\" to be exclusive to the official product and not be used for derived works without permission. The official policy document on use of the Arduino name emphasizes that the project is open to incorporating work by others into the official product. Several Arduino-compatible products commercially released have avoided the project name by using various names ending in \"-duino\".\n\nMost Arduino boards consist of an Atmel 8-bit AVR microcontroller (ATmega8, ATmega168, ATmega328, ATmega1280, ATmega2560) with varying amounts of flash memory, pins, and features. The 32-bit Arduino Due, based on the Atmel SAM3X8E was introduced in 2012. The boards use single or double-row pins or female headers that facilitate connections for programming and incorporation into other circuits. These may connect with add-on modules termed \"shields\". Multiple and possibly stacked shields may be individually addressable via an I²C serial bus. Most boards include a 5 V linear regulator and a 16 MHz crystal oscillator or ceramic resonator. Some designs, such as the LilyPad, run at 8 MHz and dispense with the onboard voltage regulator due to specific form-factor restrictions.\n\nArduino microcontrollers are pre-programmed with a boot loader that simplifies uploading of programs to the on-chip flash memory. The default bootloader of the Arduino UNO is the optiboot bootloader. Boards are loaded with program code via a serial connection to another computer. Some serial Arduino boards contain a level shifter circuit to convert between RS-232 logic levels and transistor–transistor logic (TTL) level signals. Current Arduino boards are programmed via Universal Serial Bus (USB), implemented using USB-to-serial adapter chips such as the FTDI FT232. Some boards, such as later-model Uno boards, substitute the FTDI chip with a separate AVR chip containing USB-to-serial firmware, which is reprogrammable via its own ICSP header. Other variants, such as the Arduino Mini and the unofficial Boarduino, use a detachable USB-to-serial adapter board or cable, Bluetooth or other methods. When used with traditional microcontroller tools, instead of the Arduino IDE, standard AVR in-system programming (ISP) programming is used.\n\nThe Arduino board exposes most of the microcontroller's I/O pins for use by other circuits. The \"Diecimila\", \"Duemilanove\", and current \"Uno\" provide 14 digital I/O pins, six of which can produce pulse-width modulated signals, and six analog inputs, which can also be used as six digital I/O pins. These pins are on the top of the board, via female 0.1-inch (2.54 mm) headers. Several plug-in application shields are also commercially available. The Arduino Nano, and Arduino-compatible Bare Bones Board and Boarduino boards may provide male header pins on the underside of the board that can plug into solderless breadboards.\n\nMany Arduino-compatible and Arduino-derived boards exist. Some are functionally equivalent to an Arduino and can be used interchangeably. Many enhance the basic Arduino by adding output drivers, often for use in school-level education, to simplify making buggies and small robots. Others are electrically equivalent but change the form factor, sometimes retaining compatibility with shields, sometimes not. Some variants use different processors, of varying compatibility.\n\nThe original Arduino hardware was produced by the Italian company Smart Projects. Some Arduino-branded boards have been designed by the American companies SparkFun Electronics and Adafruit Industries. , 17 versions of the Arduino hardware have been commercially produced.\n\nArduino and Arduino-compatible boards use printed circuit expansion boards called \"shields\", which plug into the normally supplied Arduino pin headers. Shields can provide motor controls for 3D printing and other applications, Global Positioning System (GPS), Ethernet, liquid crystal display (LCD), or breadboarding (prototyping). Several shields can also be made do it yourself (DIY).\n\nA program for Arduino hardware may be written in any programming language with compilers that produce binary machine code for the target processor. Atmel provides a development environment for their 8-bit AVR and 32-bit ARM Cortex-M based microcontrollers: AVR Studio (older) and Atmel Studio (newer).\n\nThe Arduino integrated development environment (IDE) is a cross-platform application (for Windows, macOS, Linux) that is written in the programming language Java. It originated from the IDE for the languages \"Processing\" and \"Wiring\". It includes a code editor with features such as text cutting and pasting, searching and replacing text, automatic indenting, brace matching, and syntax highlighting, and provides simple \"one-click\" mechanisms to compile and upload programs to an Arduino board. It also contains a message area, a text console, a toolbar with buttons for common functions and a hierarchy of operation menus. The source code for the IDE is released under the GNU General Public License, version 2.\n\nThe Arduino IDE supports the languages C and C++ using special rules of code structuring. The Arduino IDE supplies a software library from the Wiring project, which provides many common input and output procedures. User-written code only requires two basic functions, for starting the sketch and the main program loop, that are compiled and linked with a program stub \"main()\" into an executable cyclic executive program with the GNU toolchain, also included with the IDE distribution. The Arduino IDE employs the program \"avrdude\" to convert the executable code into a text file in hexadecimal encoding that is loaded into the Arduino board by a loader program in the board's firmware.\n\nA program written with the Arduino IDE is called a \"sketch\". Sketches are saved on the development computer as text files with the file extension .ino. Arduino Software (IDE) pre-1.0 saved sketches with the extension .pde.\n\nA minimal Arduino C/C++ program consist of only two functions:\n\n\nMost Arduino boards contain a light-emitting diode (LED) and a current limiting resistor connected between pin 13 and ground, which is a convenient feature for many tests and program functions. A typical program used by beginners, akin to Hello, World!, is \"blink\", which repeatedly blinks the on-board LED integrated into the Arduino board. This program uses the functions , , and , which are provided by the internal libraries included in the IDE environment. This program is usually loaded into a new Arduino board by the manufacturer.\n\nThe open-source nature of the Arduino project has facilitated the publication of many free software libraries that other developers use to augment their projects.\n\n\nThe Arduino project received an honorary mention in the Digital Communities category at the 2006 Prix Ars Electronica.\n\n\n\n"}
{"id": "265952", "url": "https://en.wikipedia.org/wiki?curid=265952", "title": "Brain in a vat", "text": "Brain in a vat\n\nIn philosophy, the brain in a vat (BIV; alternately known as brain in a jar) is a scenario used in a variety of thought experiments intended to draw out certain features of human conceptions of knowledge, reality, truth, mind, consciousness, and meaning. It is an updated version of René Descartes's evil demon thought experiment originated by Gilbert Harman. Common to many science fiction stories, it outlines a scenario in which a mad scientist, machine, or other entity might remove a person's brain from the body, suspend it in a vat of life-sustaining liquid, and connect its neurons by wires to a supercomputer which would provide it with electrical impulses identical to those the brain normally receives. According to such stories, the computer would then be simulating reality (including appropriate responses to the brain's own output) and the \"disembodied\" brain would continue to have perfectly normal conscious experiences, such as those of a person with an embodied brain, without these being related to objects or events in the real world.\n\nThe simplest use of brain-in-a-vat scenarios is as an argument for philosophical skepticism and solipsism. A simple version of this runs as follows: Since the brain in a vat gives and receives exactly the same impulses as it would if it were in a skull, and since these are its only way of interacting with its environment, then it is not possible to tell, \"from the perspective of that brain\", whether it is in a skull or a vat. Yet in the first case most of the person's beliefs may be true (if they believe, say, that they are walking down the street, or eating ice-cream); in the latter case their beliefs are false. Since the argument says one cannot know whether one is a brain in a vat, then one cannot know whether most of one's beliefs might be completely false. Since, in principle, it is impossible to rule out oneself being a brain in a vat, there cannot be good grounds for believing any of the things one believes; a skeptical argument would contend that one certainly cannot \"know\" them, raising issues with the definition of knowledge.\n\nThe brain in a vat is a contemporary version of the argument given in Hindu Maya illusion, Plato's Allegory of the Cave, Zhuangzi's \"Zhuangzi dreamed he was a butterfly\", and the evil demon in René Descartes' \"Meditations on First Philosophy\".\n\nBrain-in-a-vat scenarios—or closely related scenarios in which the protagonist is in a virtual reality simulation and unaware of this fact—have also been used for purposes other than skeptical arguments. For example, Vincent Conitzer uses such a scenario to illuminate further facts—facts that do not follow logically from the physical facts—about qualia (what it is \"like\" to have specific experiences), indexicality (what time it is \"now\" and who \"I\" am), and personal identity. Specifically, imagine a person in the real world who is observing a simulated world on a screen, from the perspective of one of the simulated agents in it. The person observing knows that besides the code responsible for the physics of the simulation, there must be \"additional\" code that determines in which colors the simulation is displayed on the screen, and which agent's perspective is displayed. (These questions are related to the inverted spectrum scenario and whether there are further facts about personal identity.) That is, the person can conclude that the facts about the physics of the simulation (which are completely captured by the code governing the physics) do not fully determine his experience by themselves. But then, Conitzer argues, imagine someone who has become so engrossed in a VR simulation that he has \"forgotten\" that it is a simulation he is watching. Could he not still reach the same conclusion? And if so, can we not conclude the same in our own daily lives?\n\nWhile the disembodied brain (the brain in a vat) can be seen as a helpful thought experiment, there are several philosophical debates surrounding the plausibility of the thought experiment. If these debates conclude that the thought experiment is implausible, a possible consequence would be that we are no closer to knowledge, truth, consciousness, representation, etc. than we were prior to the experiment.\n\nOne argument against the BIV thought experiment derives from the idea that the BIV is not and cannot be biologically similar to that of an embodied brain (that is, a brain found in a person). Since the BIV is \"dis\"embodied, it follows that it does not have a similar biology to that of an embodied brain. That is, the BIV lacks the connections from the body to the brain, which renders the BIV neither neuroanatomically nor neurophysiologically similar to that of an embodied brain. If this is the case, we cannot say that it is even possible for the BIV to have similar experiences to the embodied brain, since the brains are not equal.\n\nA second argument deals directly with the stimuli coming into the brain. This is often referred to as the account from externalism or ultra-externalism. In the BIV, the brain receives stimuli from a machine. In an embodied brain, however, the brain receives the stimuli from the sensors found in the body (via touching, tasting, smelling, etc.) which receive their input from the external environment. This argument oftentimes leads to the conclusion that there is a difference between what the BIV is representing and what the embodied brain is representing. This debate has been hashed out, but remains unresolved, by several philosophers including Uriah Kriegel, Colin McGinn, and Robert Rupert. This debate has ramifications for philosophy of mind discussions on (but not limited to) representation, consciousness, content, cognition, and embodied cognition.\n\n\n\n"}
{"id": "5320", "url": "https://en.wikipedia.org/wiki?curid=5320", "title": "Carbon nanotube", "text": "Carbon nanotube\n\nCarbon nanotubes (CNTs) are allotropes of carbon with a cylindrical nanostructure. These cylindrical carbon molecules have unusual properties, which are valuable for nanotechnology, electronics, optics and other fields of materials science and technology. Owing to the material's exceptional strength and stiffness, nanotubes have been constructed with length-to-diameter ratio of up to 132,000,000:1, significantly larger than for any other material.\n\nIn addition, owing to their extraordinary thermal conductivity, mechanical, and electrical properties, carbon nanotubes find applications as additives to various structural materials. For instance, nanotubes form a tiny portion of the material(s) in some (primarily carbon fiber) baseball bats, golf clubs, car parts or damascus steel.\n\nNanotubes are members of the fullerene structural family. Their name is derived from their long, hollow structure with the walls formed by one-atom-thick sheets of carbon, called graphene. These sheets are rolled at specific and discrete (\"chiral\") angles, and the combination of the rolling angle and radius decides the nanotube properties; for example, whether the individual nanotube shell is a metal or semiconductor. Nanotubes are categorized as single-walled nanotubes (SWNTs) and multi-walled nanotubes (MWNTs). Individual nanotubes naturally align themselves into \"ropes\" held together by van der Waals forces, more specifically, pi-stacking.\n\nApplied quantum chemistry, specifically, orbital hybridization best describes chemical bonding in nanotubes. The chemical bonding of nanotubes involves entirely \"sp\"-hybrid carbon atoms. These bonds, which are similar to those of graphite and stronger than those found in alkanes and diamond (which employ \"sp\"-hybrid carbon atoms), provide nanotubes with their unique strength.\nThere is no consensus on some terms describing carbon nanotubes in scientific literature: both \"-wall\" and \"-walled\" are being used in combination with \"single\", \"double\", \"triple\" or \"multi\", and the letter C is often omitted in the abbreviation; for example, multi-walled carbon nanotube (MWNT).\n\nwhere \"a\" = 0.246 nm.\n\nSWNTs are an important variety of carbon nanotube because most of their properties change significantly with the (\"n\",\"m\") values, and this dependence is non-monotonic (see Kataura plot). In particular, their band gap can vary from zero to about 2 eV and their electrical conductivity can show metallic or semiconducting behavior. Single-walled nanotubes are likely candidates for miniaturizing electronics. The most basic building block of these systems is the electric wire, and SWNTs with diameters of an order of a nanometer can be excellent conductors. One useful application of SWNTs is in the development of the first intermolecular field-effect transistors (FET). The first intermolecular logic gate using SWCNT FETs was made in 2001. A logic gate requires both a p-FET and an n-FET. Because SWNTs are p-FETs when exposed to oxygen and n-FETs otherwise, it is possible to expose half of an SWNT to oxygen and protect the other half from it. The resulting SWNT acts as a \"not\" logic gate with both p and n-type FETs in the same molecule.\n\nPrices for single-walled nanotubes declined from around $1500 per gram as of 2000 to retail prices of around $50 per gram of as-produced 40–60% by weight SWNTs as of March 2010. As of 2016 the retail price of as-produced 75% by weight SWNTs were $2 per gram, cheap enough for widespread use. SWNTs are forecast to make a large impact in electronics applications by 2020 according to \"The Global Market for Carbon Nanotubes\" report.\n\nMulti-walled nanotubes (MWNTs) consist of multiple rolled layers (concentric tubes) of graphene. There are two models that can be used to describe the structures of multi-walled nanotubes. In the \"Russian Doll\" model, sheets of graphite are arranged in concentric cylinders, e.g., a (0,8) single-walled nanotube (SWNT) within a larger (0,17) single-walled nanotube. In the \"Parchment\" model, a single sheet of graphite is rolled in around itself, resembling a scroll of parchment or a rolled newspaper. The interlayer distance in multi-walled nanotubes is close to the distance between graphene layers in graphite, approximately 3.4 Å. The Russian Doll structure is observed more commonly. Its individual shells can be described as SWNTs, which can be metallic or semiconducting. Because of statistical probability and restrictions on the relative diameters of the individual tubes, one of the shells, and thus the whole MWNT, is usually a zero-gap metal.\n\nDouble-walled carbon nanotubes (DWNTs) form a special class of nanotubes because their morphology and properties are similar to those of SWNTs but they are more resistant to chemicals. This is especially important when it is necessary to graft chemical functions to the surface of the nanotubes (functionalization) to add properties to the CNT. Covalent functionalization of SWNTs will break some C=C double bonds, leaving \"holes\" in the structure on the nanotube, and thus modifying both its mechanical and electrical properties. In the case of DWNTs, only the outer wall is modified. DWNT synthesis on the gram-scale was first proposed in 2003 by the CCVD technique, from the selective reduction of oxide solutions in methane and hydrogen.\n\nThe telescopic motion ability of inner shells and their unique mechanical properties will permit the use of multi-walled nanotubes as main movable arms in coming nanomechanical devices. Retraction force that occurs to telescopic motion caused by the Lennard-Jones interaction between shells and its value is about 1.5 nN.\n\n Junctions between 2 or more nanotubes have been widely discussed theoretically. Such junctions are quite frequently observed in samples prepared by arc discharge as well as by chemical vapor deposition. The electronic properties of such junctions were first considered theoretically by Lambin et al., who pointed out that a connection between metallic tube and a semiconducting one would represent a nanoscale heterojunction. Such a junction could therefore form a component of a nanotube-based electronic circuit. The adjacent image shows a junction between two multiwalled nanotubes.\nJunctions between nanotubes and graphene have been considered theoretically, but not widely studied experimentally. Such junctions form the basis of pillared graphene, in which parallel graphene sheets are separated by short nanotubes. Pillared graphene represents a class of three-dimensional carbon nanotube architectures.\nRecently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (>100 nm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricate macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.\n\nCarbon nanobuds are a newly created material combining two previously discovered allotropes of carbon: carbon nanotubes and fullerenes. In this new material, fullerene-like \"buds\" are covalently bonded to the outer sidewalls of the underlying carbon nanotube. This hybrid material has useful properties of both fullerenes and carbon nanotubes. In particular, they have been found to be exceptionally good field emitters. In composite materials, the attached fullerene molecules may function as molecular anchors preventing slipping of the nanotubes, thus improving the composite’s mechanical properties.\n\nA carbon peapod is a novel hybrid carbon material which traps fullerene inside a carbon nanotube. It can possess interesting magnetic properties with heating and irradiation. It can also be applied as an oscillator during theoretical investigations and predictions.\n\nIn theory, a nanotorus is a carbon nanotube bent into a torus (doughnut shape). Nanotori are predicted to have many unique properties, such as magnetic moments 1000 times larger than previously expected for certain specific radii. Properties such as magnetic moment, thermal stability, etc. vary widely depending on radius of the torus and radius of the tube.\n\nGraphenated carbon nanotubes are a relatively new hybrid that combines graphitic foliates grown along the sidewalls of multiwalled or bamboo style CNTs. The foliate density can vary as a function of deposition conditions (e.g. temperature and time) with their structure ranging from few layers of graphene (< 10) to thicker, more graphite-like. The fundamental advantage of an integrated graphene-CNT structure is the high surface area three-dimensional framework of the CNTs coupled with the high edge density of graphene. Depositing a high density of graphene foliates along the length of aligned CNTs can significantly increase the total charge capacity per unit of nominal area as compared to other carbon nanostructures.\n\nCup-stacked carbon nanotubes (CSCNTs) differ from other quasi-1D carbon structures, which normally behave as quasi-metallic conductors of electrons. CSCNTs exhibit semiconducting behaviors due to the stacking microstructure of graphene layers.\n\nThe observation of the \"longest\" carbon nanotubes grown so far are over 1/2 m (550 mm long) was reported in 2013. These nanotubes were grown on silicon substrates using an improved chemical vapor deposition (CVD) method and represent electrically uniform arrays of single-walled carbon nanotubes.\n\nThe \"shortest\" carbon nanotube is the organic compound cycloparaphenylene, which was synthesized in 2008.\n\nThe \"thinnest\" carbon nanotube is the armchair (2,2) CNT with a diameter of 0.3 nm. This nanotube was grown inside a multi-walled carbon nanotube. Assigning of carbon nanotube type was done by a combination of high-resolution transmission electron microscopy (HRTEM), Raman spectroscopy and density functional theory (DFT) calculations.\n\nThe \"thinnest freestanding\" single-walled carbon nanotube is about 0.43 nm in diameter. Researchers suggested that it can be either (5,1) or (4,2) SWCNT, but the exact type of carbon nanotube remains questionable. (3,3), (4,3) and (5,1) carbon nanotubes (all about 0.4 nm in diameter) were unambiguously identified using aberration-corrected high-resolution transmission electron microscopy inside double-walled CNTs.\n\nThe \"highest density\" of CNTs was achieved in 2013, grown on a conductive titanium-coated copper surface that was coated with co-catalysts cobalt and molybdenum at lower than typical temperatures of 450 °C. The tubes averaged a height of 380 nm and a mass density of 1.6 g cm. The material showed ohmic conductivity (lowest resistance ∼22 kΩ).\n\nCarbon nanotubes are the strongest and stiffest materials yet discovered in terms of tensile strength and elastic modulus respectively. This strength results from the covalent sp bonds formed between the individual carbon atoms. In 2000, a multi-walled carbon nanotube was tested to have a tensile strength of . (For illustration, this translates into the ability to endure tension of a weight equivalent to on a cable with cross-section of .) Further studies, such as one conducted in 2008, revealed that individual CNT shells have strengths of up to ≈, which is in agreement with quantum/atomistic models. Since carbon nanotubes have a low density for a solid of 1.3 to 1.4 g/cm, its specific strength of up to 48,000 kN·m·kg is the best of known materials, compared to high-carbon steel's 154 kN·m·kg.\n\nAlthough the strength of individual CNT shells is extremely high, weak shear interactions between adjacent shells and tubes lead to significant reduction in the effective strength of multi-walled carbon nanotubes and carbon nanotube bundles down to only a few GPa. This limitation has been recently addressed by applying high-energy electron irradiation, which crosslinks inner shells and tubes, and effectively increases the strength of these materials to ≈60 GPa for multi-walled carbon nanotubes and ≈17 GPa for double-walled carbon nanotube bundles. CNTs are not nearly as strong under compression. Because of their hollow structure and high aspect ratio, they tend to undergo buckling when placed under compressive, torsional, or bending stress.\n\nOn the other hand, there was evidence that in the radial direction they are rather soft. The first transmission electron microscope observation of radial elasticity suggested that even the van der Waals forces can deform two adjacent nanotubes. Later, nanoindentations with atomic force microscope were performed by several groups to quantitatively measure radial elasticity of multiwalled carbon nanotubes and tapping/contact mode atomic force microscopy was also performed on single-walled carbon nanotubes. Young's modulus of on the order of several GPa showed that CNTs are in fact very soft in the radial direction.\n\nUnlike graphene, which is a two-dimensional semimetal, carbon nanotubes are either metallic or semiconducting along the tubular axis. For a given (\"n\",\"m\") nanotube, if \"n\" = \"m\", the nanotube is metallic; if \"n\" − \"m\" is a multiple of 3 and n ≠ m and nm ≠ 0, then the nanotube is quasi-metallic with a very small band gap, otherwise the nanotube is a moderate semiconductor.\nThus all armchair (\"n\" = \"m\") nanotubes are metallic, and nanotubes (6,4), (9,1), etc. are semiconducting.\nCarbon nanotubes are not semimetallic because the degenerate point (that point where the π [bonding] band meets the π* [anti-bonding] band, at which the energy goes to zero) is slightly shifted away from the \"K\" point in the Brillouin zone due to the curvature of the tube surface, causing hybridization between the σ* and π* anti-bonding bands, modifying the band dispersion.\n\nThe rule regarding metallic versus semiconductor behavior has exceptions, because curvature effects in small diameter tubes can strongly influence electrical properties. Thus, a (5,0) SWCNT that should be semiconducting in fact is metallic according to the calculations. Likewise, zigzag and chiral SWCNTs with small diameters that should be metallic have a finite gap (armchair nanotubes remain metallic). In theory, metallic nanotubes can carry an electric current density of 4 × 10 A/cm, which is more than 1,000 times greater than those of metals such as copper, where for copper interconnects current densities are limited by electromigration. Carbon nanotubes are thus being explored as interconnects, conductivity enhancing components in composite materials and many groups are attempting to commercialize highly conducting electrical wire assembled from individual carbon nanotubes. There are significant challenges to be overcome, however, such as undesired current saturation under voltage, the much more resistive nanotube-to-nanotube junctions and impurities, all of which lower the electrical conductivity of the macroscopic nanotube wires by orders of magnitude, as compared to the conductivity of the individual nanotubes.\n\nBecause of its nanoscale cross-section, electrons propagate only along the tube's axis. As a result, carbon nanotubes are frequently referred to as one-dimensional conductors. The maximum electrical conductance of a single-walled carbon nanotube is 2\"G\", where \"G\" = 2\"e\"/\"h\" is the conductance of a single ballistic quantum channel.\n\nDue to the role of the π-electron system in determining the electronic properties of graphene, doping in carbon nanotubes differs from that of bulk crystalline semiconductors from the same group of the periodic table (e.g. silicon). Graphitic substitution of carbon atoms in the nanotube wall by boron or nitrogen dopants leads to p-type and n-type behavior, respectively, as would be expected in silicon. However, some non-substitutional (intercalated or adsorbed) dopants introduced into a carbon nanotube, such as alkali metals as well as electron-rich metallocenes, result in n-type conduction because they donate electrons to the π-electron system of the nanotube. By contrast, π-electron acceptors such as FeCl or electron-deficient metallocenes function as p-type dopants since they draw π-electrons away from the top of the valence band.\n\nIntrinsic superconductivity has been reported, although other experiments found no evidence of this, leaving the claim a subject of debate.\n\nCarbon nanotubes have useful absorption, photoluminescence (fluorescence), and Raman spectroscopy properties. Spectroscopic methods offer the possibility of quick and non-destructive characterization of relatively large amounts of carbon nanotubes. There is a strong demand for such characterization from the industrial point of view: numerous parameters of the nanotube synthesis can be changed, intentionally or unintentionally, to alter the nanotube quality. As shown below, optical absorption, photoluminescence and Raman spectroscopies allow quick and reliable characterization of this \"nanotube quality\" in terms of non-tubular carbon content, structure (chirality) of the produced nanotubes, and structural defects. Those features determine nearly any other properties such as optical, mechanical, and electrical properties.\n\nCarbon nanotubes are unique \"one-dimensional systems\" which can be envisioned as rolled single sheets of graphite (or more precisely graphene). This rolling can be done at different angles and curvatures resulting in different nanotube properties. The diameter typically varies in the range 0.4–40 nm (i.e. \"only\" ~100 times), but the length can vary ~100,000,000,000 times, from 0.14 nm to 55.5 cm. The nanotube aspect ratio, or the length-to-diameter ratio, can be as high as 132,000,000:1, which is unequalled by any other material. Consequently, all the properties of the carbon nanotubes relative to those of typical semiconductors are extremely anisotropic (directionally dependent) and tunable.\n\nWhereas mechanical, electrical and electrochemical (supercapacitor) properties of the carbon nanotubes are well established and have immediate applications, the practical use of optical properties is yet unclear. The aforementioned tunability of properties is potentially useful in optics and photonics. In particular, light-emitting diodes (LEDs) and photo-detectors based on a single nanotube have been produced in the lab. Their unique feature is not the efficiency, which is yet relatively low, but the narrow selectivity in the wavelength of emission and detection of light and the possibility of its fine tuning through the nanotube structure. In addition, bolometer and optoelectronic memory devices have been realised on ensembles of single-walled carbon nanotubes.\n\nCrystallographic defects also affect the tube's electrical properties. A common result is lowered conductivity through the defective region of the tube. A defect in armchair-type tubes (which can conduct electricity) can cause the surrounding region to become semiconducting, and single monatomic vacancies induce magnetic properties.\n\nAll nanotubes are expected to be very good thermal conductors along the tube, exhibiting a property known as \"ballistic conduction\", but good insulators lateral to the tube axis. Measurements show that an individual SWNT has a room-temperature thermal conductivity along its axis of about 3500 W·m·K; compare this to copper, a metal well known for its good thermal conductivity, which transmits 385 W·m·K. An individual SWNT has a room-temperature thermal conductivity across its axis (in the radial direction) of about 1.52 W·m·K, which is about as thermally conductive as soil. Macroscopic assemblies of nanotubes such as films or fibres have reached up to 1500 W·m·K so far. The temperature stability of carbon nanotubes is estimated to be up to 2800 °C in vacuum and about 750 °C in air.\n\nCrystallographic defects strongly affect the tube's thermal properties. Such defects lead to phonon scattering, which in turn increases the relaxation rate of the phonons. This reduces the mean free path and reduces the thermal conductivity of nanotube structures. Phonon transport simulations indicate that substitutional defects such as nitrogen or boron will primarily lead to scattering of high-frequency optical phonons. However, larger-scale defects such as Stone Wales defects cause phonon scattering over a wide range of frequencies, leading to a greater reduction in thermal conductivity.\n\nTechniques have been developed to produce nanotubes in sizable quantities, including arc discharge, laser ablation, chemical vapor deposition (CVD) and high-pressure carbon monoxide disproportionation (HiPCO). Among these arc discharge, laser ablation, chemical vapor deposition (CVD) are batch by batch process and HiPCO is gas phase continuous process. Most of these processes take place in a vacuum or with process gases. The CVD growth method is popular, as it yields high quantity and has a degree of control over diameter, length and morphology. Using particulate catalysts, large quantities of nanotubes can be synthesized by these methods, but achieving the repeatability becomes a major problem with CVD growth. The HiPCO process advances in catalysis and continuous growth are making CNTs more commercially viable. The HiPCO process helps in producing high purity single walled carbon nanotubes in higher quantity. The HiPCO reactor operates at high temperature 900-1100 °C and high pressure ~30-50 bar. It uses carbon monoxide as the carbon source and Nickel/ iron penta carbonyl as catalyst. These catalyst acts as the nucleation site for the nanotubes to grow.\n\nVertically aligned carbon nanotube arrays are also grown by thermal chemical vapor deposition. A substrate (quartz, silicon, stainless steel, etc.) is coated with a catalytic metal (Fe, Co, Ni) layer. Typically that layer is iron, and is deposited via sputtering to a thickness of 1–5 nm. A 10–50 nm underlayer of alumina is often also put down on the substrate first. This imparts controllable wetting and good interfacial properties.\nWhen the substrate is heated to the growth temperature (~700 °C), the continuous iron film breaks up into small islands… each island then nucleates a carbon nanotube. The sputtered thickness controls the island size, and this in turn determines the nanotube diameter. Thinner iron layers drive down the diameter of the islands, and they drive down the diameter of the nanotubes grown. The amount of time that the metal island can sit at the growth temperature is limited, as they are mobile, and can merge into larger (but fewer) islands. Annealing at the growth temperature reduces the site density (number of CNT/mm) while increasing the catalyst diameter.\n\nThe as-prepared carbon nanotubes always have impurities such as other forms of carbon (amorphous carbon, fullerene, etc.) and non-carbonaceous impurities (metal pes used for catalyst). These impurities need to be removed to make use of the carbon nanotubes in applications.\n\nThere are many metrology standards and reference materials available for carbon nanotubes.\n\nFor single-wall carbon nanotubes, ISO/TS 10868 describes a measurement method for the diameter, purity, and fraction of metallic nanotubes through optical absorption spectroscopy, while ISO/TS 10797 and ISO/TS 10798 establish methods to characterize the morphology and elemental composition of single-wall carbon nanotubes, using transmission electron microscopy and scanning electron microscopy respectively, coupled with energy dispersive X-ray spectrometry analysis.\n\nNIST SRM 2483 is a soot of single-wall carbon nanotubes used as a reference material for elemental analysis, and was characterized using thermogravimetric analysis, prompt gamma activation analysis, induced neutron activation analysis, inductively coupled plasma mass spectroscopy, resonant Raman scattering, UV-visible-near infrared fluorescence spectroscopy and absorption spectroscopy, scanning electron microscopy, and transmission electron microscopy. The Canadian National Research Council also offers a certified reference material SWCNT-1 for elemental analysis using neutron activation analysis and inductively coupled plasma mass spectroscopy. NIST RM 8281 is a mixture of three lengths of single-wall carbon nanotube.\n\nFor multiwall carbon nanotubes, ISO/TR 10929 identifies the basic properties and the content of impurities, while ISO/TS 11888 describes morphology using scanning electron microscopy, transmission electron microscopy, viscometry, and light scattering analysis. ISO/TS 10798 is also valid for multiwall carbon nanotubes.\n\nCarbon nanotubes can be functionalized to attain desired properties that can be used in a wide variety of applications. The two main methods of carbon nanotube functionalization are covalent and non-covalent modifications. Because of their apparent hydrophobic nature, carbon nanotubes tend to agglomerate hindering their dispersion in solvents or viscous polymer melts. The resulting nanotube bundles or aggregates reduce the mechanical performance of the final composite. The surface of the carbon nanotubes can be modified to reduce the hydrophobicity and improve interfacial adhesion to a bulk polymer through chemical attachment.\n\nAlso surface of carbon nanotubes can be fluorinated or halofluorinated by CVD-method with fluorocarbons, hydro- or halofluorocarbons by heating while in contact of such carbon material with fluoroorganic substance to form partially fluorinated carbons (so called Fluocar materials) with grafted (halo)fluoroalkyl functionality.\n\nCurrent use and application of nanotubes has mostly been limited to the use of bulk nanotubes, which is a mass of rather unorganized fragments of nanotubes. Bulk nanotube materials may never achieve a tensile strength similar to that of individual tubes, but such composites may, nevertheless, yield strengths sufficient for many applications. Bulk carbon nanotubes have already been used as composite fibers in polymers to improve the mechanical, thermal and electrical properties of the bulk product.\n\nOther current applications include:\nCurrent research for modern applications include:\n\nThe strength and flexibility of carbon nanotubes makes them of potential use in controlling other nanoscale structures, which suggests they will have an important role in nanotechnology engineering. The highest tensile strength of an individual multi-walled carbon nanotube has been tested to be 63 GPa. Carbon nanotubes were found in Damascus steel from the 17th century, possibly helping to account for the legendary strength of the swords made of it. Recently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (>1mm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricated macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.\n\nCNTs are potential candidates for future via and wire material in nano-scale VLSI circuits. Eliminating electromigration reliability concerns that plague today's Cu interconnects, isolated (single and multi-wall) CNTs can carry current densities in excess of 1000 MA/sq-cm without electromigration damage.\n\nLarge quantities of pure CNTs can be made into a freestanding sheet or film by surface-engineered tape-casting (SETC) fabrication technique which is a scalable method to fabricate flexible and foldable sheets with superior properties. Another reported form factor is CNT fiber (a.k.a. filament) by wet spinning. The fiber is either directly spun from the synthesis pot or spun from pre-made dissolved CNTs. Individual fibers can be turned into a yarn. Apart from its strength and flexibility, the main advantage is making an electrically conducting yarn. The electronic properties of individual CNT fibers (i.e. bundle of individual CNT) are governed by the two-dimensional structure of CNTs. The fibers were measured to have a resistivity only one order of magnitude higher than metallic conductors at 300K. By further optimizing the CNTs and CNT fibers, CNT fibers with improved electrical properties could be developed.\n\nCNT-based yarns are suitable for applications in energy and electrochemical water treatment when coated with an ion-exchange membrane. Also, CNT-based yarns could replace copper as a winding material. Pyrhönen et al. (2015) have built a motor using CNT winding.\n\nThe National Institute for Occupational Safety and Health (NIOSH) is the leading United States federal agency conducting research and providing guidance on the occupational safety and health implications and applications of nanotechnology. Early scientific studies have indicated that some of these nanoscale particles may pose a greater health risk than the larger bulk form of these materials. In 2013, NIOSH published a Current Intelligence Bulletin detailing the potential hazards and recommended exposure limit for carbon nanotubes and fibers.\n\nAs of October 2016, single wall carbon nanotubes have been registered through the European Union's Registration, Evaluation, Authorization and Restriction of Chemicals (REACH) regulations, based on evaluation of the potentially hazardous properties of SWCNT. Based on this registration, SWCNT commercialization is allowed in the EU up to 10 metric tons. Currently, the type of SWCNT registered through REACH is limited to the specific type of single wall carbon nanotubes manufactured by OCSiAl, which submitted the application.\n\nThe true identity of the discoverers of carbon nanotubes is a subject of some controversy. A 2006 editorial written by Marc Monthioux and Vladimir Kuznetsov in the journal \"Carbon\" described the interesting and often-misstated origin of the carbon nanotube. A large percentage of academic and popular literature attributes the discovery of hollow, nanometer-size tubes composed of graphitic carbon to Sumio Iijima of NEC in 1991. He published a paper describing his discovery which initiated a flurry of excitement and could be credited by inspiring the many scientists now studying applications of carbon nanotubes. Though Iijima has been given much of the credit for discovering carbon nanotubes, it turns out that the timeline of carbon nanotubes goes back much further than 1991.\n\nIn 1952, L. V. Radushkevich and V. M. Lukyanovich published clear images of 50 nanometer diameter tubes made of carbon in the Soviet \"Journal of Physical Chemistry\". This discovery was largely unnoticed, as the article was published in Russian, and Western scientists' access to Soviet press was limited during the Cold War. Monthioux and Kuznetsov mentioned in their \"Carbon\" editorial: \n\nIn 1976, Morinobu Endo of CNRS observed hollow tubes of rolled up graphite sheets synthesised by a chemical vapour-growth technique. The first specimens observed would later come to be known as single-walled carbon nanotubes (SWNTs). Endo, in his early review of vapor-phase-grown carbon fibers (VPCF), also reminded us that he had observed a hollow tube, linearly extended with parallel carbon layer faces near the fiber core. This appears to be the observation of multi-walled carbon nanotubes at the center of the fiber. The mass-produced MWCNTs today are strongly related to the VPGCF developed by Endo. In fact, they call it the “Endo-process”, out of respect for his early work and patents.\n\nIn 1979, John Abrahamson presented evidence of carbon nanotubes at the 14th Biennial Conference of Carbon at Pennsylvania State University. The conference paper described carbon nanotubes as carbon fibers that were produced on carbon anodes during arc discharge. A characterization of these fibers was given as well as hypotheses for their growth in a nitrogen atmosphere at low pressures.\n\nIn 1981, a group of Soviet scientists published the results of chemical and structural characterization of carbon nanoparticles produced by a thermocatalytical disproportionation of carbon monoxide. Using TEM images and XRD patterns, the authors suggested that their “carbon multi-layer tubular crystals” were formed by rolling graphene layers into cylinders. They speculated that by rolling graphene layers into a cylinder, many different arrangements of graphene hexagonal nets are possible. They suggested two possibilities of such arrangements: circular arrangement (armchair nanotube) and a spiral, helical arrangement (chiral tube).\n\nIn 1987, Howard G. Tennent of Hyperion Catalysis was issued a U.S. patent for the production of \"cylindrical discrete carbon fibrils\" with a \"constant diameter between about 3.5 and about 70 nanometers..., length 10 times the diameter, and an outer region of multiple essentially continuous layers of ordered carbon atoms and a distinct inner core...\"\n\nIijima's discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods in 1991 and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, then they would exhibit remarkable conducting properties helped create the initial buzz that is now associated with carbon nanotubes. Nanotube research accelerated greatly following the independent discoveries by Bethune at IBM and Iijima at NEC of \"single-walled\" carbon nanotubes and methods to specifically produce them by adding transition-metal catalysts to the carbon in an arc discharge. The arc discharge technique was well-known to produce the famed Buckminster fullerene on a preparative scale, and these results appeared to extend the run of accidental discoveries relating to fullerenes. The discovery of nanotubes remains a contentious issue. Many believe that Iijima's report in 1991 is of particular importance because it brought carbon nanotubes into the awareness of the scientific community as a whole.\n\n\"This article incorporates public domain text from National Institute of Environmental Health Sciences (NIEHS) as quoted.\"\n"}
{"id": "12520645", "url": "https://en.wikipedia.org/wiki?curid=12520645", "title": "Chicago fitting", "text": "Chicago fitting\n\nA Chicago fitting (also called a Duck's foot fitting due to its shape) is a one quarter turn fitting used for attaching hoses or piping together. Chicago fittings are used on both low to medium pressure gas and fluid lines. The advantages of the Chicago fitting are that it can be used in a wide range of industries and that there is no male or female fitting; both fittings are identical.\n\nGladhand connector\n\n"}
{"id": "6345215", "url": "https://en.wikipedia.org/wiki?curid=6345215", "title": "Codex Digital", "text": "Codex Digital\n\nCodex Digital creates digital production workflow tools for motion pictures, commercials, independent films, and TV productions. \n\nCodex products include recorders and media processing systems that transfer digital files and images from the camera to post-production, and tools for colour dynamics, dailies creation, archiving, review, and digital asset management.\n\nCodex is based out of London, UK, with offices in Los Angeles, CA, and Wellington, NZ.\n\nCodex recorders are high-resolution media recording systems, designed to capture pictures and sound from digital cinematography cameras. The first cameras Codex supported were the ARRI Alexa, the Sony CineAlta series, the Panavision Genesis and the Arriflex D-21. They recorded twin 4:4:4 dual-link HD-SDI inputs for A & B camera or stereoscopic 3D work at up to 16-bits colour depth.\n\nCodex products used a touchscreen interface and removable \"data packs\" containing up to 10TB of raid array disk storage. Interfaces for digital cinematography cameras include single and dual-link HD-SDI and Infiniband. Codex uses what they call a \"Virtual File System\" or in technical terms, it acts as a file server. When accessed via a conventional Ethernet network, the captured material can be viewed in a number of resolutions and formats, such as QuickTime, MXF, AVI, WAV and JPEG.\n\nThe Codex Studio recorder was introduced in 2005 and was used as the capture device for Dalsa cameras.\n2007 saw the introduction of the Codex portable recording system. With a design based on the larger Codex studio recorder, this unit was a compact, battery-powered variant which claims \"visually lossless\" recording.\n\n2010 saw the introduction of the Codex onboard recording system. Based on the larger Codex portable recorder, this is another compact, battery-powered variant which offers uncompressed and wavelet-based recording. The recorder mounts directly on the camera and weighs in at 2.5 kg.\n\nThe Codex recording systems have been used on past projects, such as Tim Burton's \"Alice in Wonderland,\" Michael Apted's \"The Chronicles of Narnia: The Voyage of the Dawn Treader\", Joseph Kosinski's \"Tron Legacy\" and Roland Emmerich's \"Anonymous.\"\n\n\n"}
{"id": "25174653", "url": "https://en.wikipedia.org/wiki?curid=25174653", "title": "Cubicle curtain", "text": "Cubicle curtain\n\nA cubicle curtain or hospital curtain is a dividing cloth used in a medical treatment facility that provides a private enclosure for one or more patients. The curtain is usually made from inherently flame retardant (IFR) fabric, and is suspended from a supporting structure or ceiling track.\n\nHealthcare cubicle curtains are constructed from different woven fabrics stitched together with the top portion of open mesh of the curtain as required by the National Fire Protection Association (NFPA-701 large & small scale). The suggested fire prevention and regulatory codes are followed by local, state and Federal fire marshals with Healthcare patient safety in mind. The suggestion for mesh is seventy-percent (70%) open allowing for ceiling sprinkler head water penetration in event of fire. The vertical length requirement for mesh size is determined by the number of horizontal inches of the curtain ceiling track from the sprinkler head. Example: If ceiling track is six (6\") inches from ceiling sprinkler head, the required vertical length of the mesh is approximately six (6\") inches. The maximum vertical mesh length required is eighteen inches (18\") regardless of additional distance from the ceiling sprinkler head. The code for the vertical mesh requirement is found under National Fire Protection Agency code NFPA-701/large and small scale (NFPA-701). The lower portion of the cubicle curtain is opaque to ensure patient privacy. Cubicle curtain fabrics with an antimicrobial finish are not required by code but are becoming more common in medical treatment facilities to reduce the spread of germs within a facility. All cubicle curtains are required to be fire-retardant by NFPA code. The fire-retardant properties can be topically applied and/or inherent to a wide range of synthetic fibers used for cubicle curtains.\n\nCubicle curtain design underwent a period of rapid growth in the 1990s. Instead of traditional solids and tone-on-tones, a broader range of subtle colors, muted tones, and soft hues became available along with different textures and more elaborate patterns. Nature themed cubicle curtains are popular as well as customizable options.\n\nCubicle curtains have been known to cause HAI’s (Hospital Acquired Infections). Studies have found methicillin‐resistant Staphylococcus aureus (MRSA) on cubicle curtains in hospitals. Since the exposure of this phenomenon, some medical treatment facilities have begun to use anti-microbial curtains in an effort to impede the spread of HAI's.\n\nCubicle curtains can be used in hospital curtains and must be complaint to National Health Standard in the United Kingdom for infection control and flame retardant to British Standard BS5867 for drapery material.\n\nFitting room cubicle curtains are used for privacy whilst in retail premises for whilst customers are dressing into clothes. In the United Kingdom these need to be certified to BS5867 for use in commercial environments.\n"}
{"id": "889576", "url": "https://en.wikipedia.org/wiki?curid=889576", "title": "Curb weight", "text": "Curb weight\n\nCurb weight (American English) or kerb weight (British English) is the total weight of a vehicle with standard equipment and hardpoints, all necessary operating consumables such as motor oil, transmission oil, coolant, air conditioning refrigerant, and sometimes a full tank of fuel, while not loaded with either passengers, cargo, or weaponry.\n\nThis definition may differ from definitions used by governmental regulatory agencies or other organizations. For example, many European Union manufacturers include the weight of a driver to follow European Directive 95/48/EC. Organizations may also define curb weight with fixed levels of fuel and other variables to equalize the value for the comparison of different vehicles.\n\nThe United States Environmental Protection Agency regulations define curb weight as follows: Curb weight means the actual or the manufacturer’s estimated weight of the vehicle in operational status with all standard equipment, and weight of fuel at nominal tank capacity, and the weight of optional equipment computed in accordance with §86.1832–01; incomplete light-duty trucks shall have the curb weight specified by the manufacturer.\n\nUnladen mass depends on the manufacturer and can be the same as curb weight, however, it is often the total mass of the car without a driver, fluid or any additional equipment.\n\n"}
{"id": "4354735", "url": "https://en.wikipedia.org/wiki?curid=4354735", "title": "Direct digital control", "text": "Direct digital control\n\nDirect Digital control (DDC) is the automated control of a condition or process by a digital device (computer). DDC takes a centralized network-oriented approach. All instrumentation is gathered by various analog and digital converters which use the network to transport these signals to the central controller. The centralized computer then follows all of its production rules (which may incorporate sense points anywhere in the structure) and causes actions to be sent via the same network to valves, actuators, and other HVAC components that can be adjusted.\n\nCentral controllers and most terminal unit controllers are programmable, meaning the direct digital control program code may be customized for the intended use. The program features include time schedules, setpoints, controllers, logic, timers, trend logs, and alarms.\n\nThe unit controllers typically have analog and digital inputs, that allow measurement of the variable (temperature, humidity, or pressure) and analog and digital outputs for control of the medium (hot/cold water and/or steam). Digital inputs are typically (dry) contacts from a control device, and analog inputs are typically a voltage or current measurement from a variable (temperature, humidity, velocity, or pressure) sensing device. Digital outputs are typically relay contacts used to start and stop equipment, and analog outputs are typically voltage or current signals to control the movement of the medium (air/water/steam) control devices. Usually abbreviated as \"DDC\".\n\nA very early example of a DDC system meeting the above requirements was completed by the Australian business Midac in 1981-1982 using R-Tec Australian designed hardware. The system installed at the University of Melbourne used a serial communications network, connecting campus buildings back to a control room \"front end\" system in the basement of the Old Geology building. Each remote or Satellite Intelligence Unit (SIU) ran 2 Z80 microprocessors whilst the front end ran eleven Z80's in a Parallel Processing configuration with paged common memory. The z80 microprocessors shared the load by passing tasks to each other via the common memory and the communications network. This was possibly the first successful implementation of a distributed processing direct digital control.\n\nWhen DDC controllers are networked together they can share information through a data bus. The control system may speak 'proprietary' or 'open protocol' language to communicate on the data bus. Examples of open protocol language are BACnet (Building Automation Control Network), LonWorks(Echelon), Modbus TCP and KNX.\n\nWhen different DDC data networks are linked together they can be controlled from a shared platform. This platform can then share information from one language to another. For example, a LON controller could share a temperature value with a BACnet controller. The integration platform can not only make information shareable, but can interact with all the devices.\n\nMost of the integration platforms are either a PC or a network applliance. In many cases, the HMI (human machine interface) or SCADA (Supervisory Control And Data Acquisition) are part of it. Integration platform examples, to name only a few, are the Tridium Niagara AX, Trend Controls,TAC Vista, CAN2GO and the Unified Architecture i.e. OPC (Open Connectivity) server technology used when direct connectivity is not possible.\n\nDDC is often used to control the HVAC (heating, ventilating, and air conditioning) devices such as valves via microprocessors using software to perform the control logic. Such systems receive analog and digital inputs from the sensors and devices installed in the HVAC system and, according to the control logic, provide analog or digital outputs to control the HVAC system devices.\n\nThese systems may be mated with a software package that graphically allows operators to monitor, control, alarm and diagnose building equipment remotely.\n\n\n"}
{"id": "10240256", "url": "https://en.wikipedia.org/wiki?curid=10240256", "title": "Dog (engineering)", "text": "Dog (engineering)\n\nIn engineering, a dog is a tool or part of a tool that prevents movement or imparts movement by offering physical obstruction or engagement of some kind. It may hold another object in place by blocking it, clamping it, or otherwise obstructing its movement. Or it may couple various parts together so that they move in unison – the primary example of this being a flexible drive to mate two shafts in order to transmit torque. Some devices use dog clutches to lock together two spinning components. In a manual transmission, the dog clutches, or \"dogs\" lock the selected gear to the shaft it rotates on. Unless the dog is engaged, the gear will simply freewheel on the shaft.\n\nThis word usage is a metaphor derived from the idea of a dog (animal) biting and holding on, the \"dog\" name derived from the basic idea of how a dog jaw locks on, by the movement of the jaw, or by the presence of many teeth. In engineering the \"dog\" device has some special engineering work when making it – it is not a simple part to make as it is not a simple bar or pipe, and the metal used in its construction is likely to be special rather than regular steel.\n\nThere is potential for confusion as \"dog tensioners\" are levers that are named due to the shape of the lever appearing as a dog leg, as the lever is in a pantograph arrangement, or \"dog trailers\", which are named due to the use of multiple trailers for transporting animal cages.\n\nAlthough not seen on all chainsaws, when present chainsaw dogs are mounted where the bar meets the power head. Chainsaw dogs provide stability and serve as a sort of fulcrum for swinging the bar through the item being cut.\nFunctional exterior window shutters (which can be swung shut whenever storms approach in order to protect the window glass from impact by wind-blown debris) are held open during pleasant weather by wrought-iron or cast-iron dogs, which are called \"shutter dogs\".\n\nA bench dog is an accessory used on a woodworking workbench to allow clamping of wooden items whilst being worked.\n\nLadder dogs are the parts of a ladder that hold the ladder at a certain height, and articulate against pawls to allow adjustment.\n\nThe doors that allow passage through bulkheads between compartments inside a ship can be closed during emergencies to seal off one compartment from another, thus sequestering water leaks, fires, or waves of air pressure and preventing them from compromising the rest of the ship's interior. The objects that are wedged against the door to hold it closed against the water or air pressure are an example of dogs. To \"dog the hatches\" means to close the hatches and dog them down (fasten them closed).\n\nAndirons, which hold up the firewood in a fireplace, are sometimes called \"dogs\", \"firedogs\", or \"dog irons\".\n\nThe clutch that mates the engine to the transmission in a modern manual-shift automobile is a \"friction clutch\" whose disc and pressure plate are smooth; they lock up simply through friction. However, some kinds of clutches (including those inside an automatic transmission) may lock up via the engagement of dogs, rather than only through friction. These clutches are called \"dog clutches\" and the dogs used within them are called \"clutch dogs\".\n\nThe \"lathe dog\" (or \"lathe carrier\") is essentially analogous to a clutch dog. It is used to provide positive drive to a workpiece turning between centers on a lathe. Without the dog, the cutting tool would tend to \"catch\", e.g., stop the workpiece from turning while the headstock center continued to rotate, possibly causing damage to the workpiece, or the lathe.\n\nThe feed dogs of a sewing machine feed the fabric in a linear stepping motion past the needle.\n\nIn carpentry log dogs were used to repair timber frame joints. In hewing (shaping with an axe) timbers or sawing with some types of water powered sawmills log dogs are used to hold the timber in place.\n"}
{"id": "3461033", "url": "https://en.wikipedia.org/wiki?curid=3461033", "title": "Furring", "text": "Furring\n\nIn construction, furring (furring strips) are thin strips of wood or other material to level or raise surfaces of another material to prevent dampness, to make space for insulation, or to level and resurface ceilings or walls. Furring refers to the process of installing the strips and to the strips themselves. \"Firring\" is a U.K. term for wood strips which are usually 50 mm wide, tapered and fixed above wood roof joists to provide drainage falls below roof boarding. Furring strips themselves are typically referred to as \"battens\" in the U.K. and sometimes the material is called \"strapping\" in the U.S.\n\nWood furring strips typically measure 1 x 2 or 1 x 3 inches. They can be laid out perpendicular to studs or joists and nailed to them, or set vertically against an existing wall surface. The spacing between the strips depends on the type of finishing material. Wider spacing is typically used behind the heavy boards that support ceramic tiles. Closely spaced strips are needed for thin panelling or plaster. The use of strips with plaster, however, is called either lath and plaster or wattle and daub.\n\nMetal furring strips are used for commercial projects, or in towns where fire-proof supporting elements are required by the local building code. Often called \"hat channels\" to describe the profile (cross section), they consist of two flanges on each side of a trapezoid shape, 7/8 in thick.\n\nFurring is also used to support roof materials and may be seen under barn and shed roofs but is now used less often, replaced by labor-saving plywood. Drywall has become the most common interior wall finishing material and does not need furring due to its strength. Furring is still used in remodeling work to fill out uneven sections for resurfacing, or to add room for insulation.\n\nFurring is a type of ship rebuilding method indicative of the late 16th century and early 17th century England. It was adopted as a remedial process to solve crank ships that were built too narrow. When a ship could not bear sail, was too narrow or her bearing laid too low, a second layer of frames was attached to the first to make her broader and lay her bearing higher. This was done by ripping the planks off and applying the second frames on top of the original frames, and then adding the planks back on. \"They commonly fur some two or three strakes under water and as much above, according as the ship requires, more or less.\" (Mainwaring, 153)\n\nAlthough this appeared to have fixed the problems with \"makeshift corrections\" due to miscalculation, it was seen as a poor remedy and at times was used as a black listing method among shipwrights during the end of the 16th century and beginning of the 17th century England (ex. Phineas Pett and the \"Prince Royal\").\n\nBecause it was seen as such an incompetent occurrence to miscalculate while building, even the writers of the time had plenty to say about its uncertainties: \"I think in all the world there are not so many ships furred as are in England, and it is a pity that there is no order taken either for the punishing of those who build such ships or the preventing of it, for it is an infinite loss to the owners and an utter spoiling and disgrace to all ships that are so handled (Mainwaring, 153)\".\n\nThe only archaeological evidence of furring found to date is on the Princes Channel Wreck.\n\n\n"}
{"id": "28105403", "url": "https://en.wikipedia.org/wiki?curid=28105403", "title": "GeoLearning", "text": "GeoLearning\n\nGeoLearning, Inc., was a US company headquartered in West Des Moines, Iowa, offering on-demand software as a service (SaaS) learning management system (LMS) solutions for corporate internal training, talent management and external initiatives.\n\nFounded in 1997 by Frank A. Russell, president and CEO, and his wife Linda in the basement of their home, GeoLearning grew to employ 280 people. The company provided services to more than 800 organizations, including more than six million users from 160 countries around the world in 16 languages, in the field of on-demand learning and performance management platforms and training services. GeoLearning’s approach allowed users to work with a number of the company’s partner organizations.\n\nIn February 2008, GeoLearning secured a growth equity investment of $31 million from Fidelity Ventures. The funds, which represented the first institutional investment for the company, were utilized to accelerate the company’s sales and marketing efforts, continued building of its international presence and channel partner network, and potential acquisitions.\nOn January 5, 2011, Geolearning was acquired by SumTotal.\n\nGeoLearning has been recognized with several awards, including ranking on the Inc. 500/5000 list of America's fastest-growing private companies for seven consecutive years (2004–2010), six years on the Software 500 list (2005–2010), \"Training Product of the Year\" from HR Executive Magazine, several American Business Awards (including \"Most Innovative Company\"), and TrainingIndustry.com's list of the \"Top 20 Companies in the Training Outsourcing Industry\" for six successive years (2004–09).\n\nIn 2010, GeoLearning was named a finalist for “Business Innovation of the Year” from the American Business Awards for its Extended Enterprise solution, winner of “Software Company of the Year” at the Prometheus Awards from the Technology Association of Iowa (TAI), finalist for “Best Corporate Learning Solution” at the Codie Awards from the Software & Information Industry Association and named to the “Baker’s Dozen: Top Learning Providers” from Human Resources Outsourcing Today magazine.\n\nFrank A. Russell was named CEO of the Year at the 2009 Prometheus Awards sponsored by the TAI. He was voted to Training Industry's 2007 list of the \"Most Influential Training Professionals\" by his peers in the industry and was the 2006 Entrepreneur of the Year by the Des Moines Business Record. In 2005, Frank received the Ernst & Young Entrepreneur of the Year award in the Business Services category for the Central Midwest Region and was named Small Business Person of the Year for the State of Iowa by the U.S. Small Business Administration.\n"}
{"id": "49274271", "url": "https://en.wikipedia.org/wiki?curid=49274271", "title": "Geocoder (Ruby)", "text": "Geocoder (Ruby)\n\nGeocoder (Ruby) is a geocoding library for Ruby. Geocoding helps to enhance webpages by presenting location relevant information to the user. When used with Rails, Geocoder adds geocoding functionality such as finding coordinates with street addresses or vice versa in addition to distance calculations for ActiveRecord objects. Since the functionality does not rely on proprietary database functions, finding different geocoded objects in an area works out-of-the-box for databases like MySQL, PostgreSQL and SQLite.\n\nGeocoder has been fully tested with Ruby 1.8.7, 1.9.2, and JRuby 1.5.3.\n\nGeocoder is compatible with Rails 3, but there is only limited functionality with Rails 2.\n\nThe Prerequisites to installing Geocoder are Ruby and RubyGems.\n\nGeocoder gem can be installed with the following command:\nOr, if you're using Bundler for Rails, you may add this to your Gemfile:\nand run at the command prompt:\nIt can be used as a plugin with rails too:\nIn order to use Geocoder with objects, the project must be set up as follows:\n\nIn order to use Geocoding with ActiveRecord objects, they must have two additional attributes, latitude and longitude coordinates. When stored in the table they should be called \"latitude\" and \"longitude\" but they may be changed as explained below. When using reverse geocoding (translating a user's location coordinates into a physical address), the model must implement a method that returns an address. The address may be a single attribute; however, it can also be a method which returns a string assembled from different attributes such as city, state, and country.\n\nWhen using Mongoid, the model only needs to add the address, latitude and longitudes as fields. The model must also include \"Geocoder::Model::Mongoid\" before making any calls to the \"geocoded_by:\" method\".\" \n\nIn the rails model, Geocoder must be told which method returns the object's full address:\n\nFor reverse geocoding, Geocoder must know which method returns latitude and longitude coordinates. If :address option is not provided, it fetches the address automatically in the address attribute. Else, it fetches the address into the location attribute like the example given below.\n\nForward and Reverse Geocoding on the same model is possible.\n\nIn order to use different names for latitude and longitude in the model, the following change may be done when implementing \"geocoded_by:\"\n\nAdditionally, the address method may return any string that would be used to search Google Maps. Any of the following examples will work:\n\nBy default, Geocoder makes use of Google's geocoding API to retrieve addresses and coordinates. Currently, these address geocoding services are supported:\n\nHere are some examples to demonstrate Geocoder functionality:\n\nFinds hotels near Raleigh.\n\nFinds the distance from \"@restaurant\" to the Empire State Building.\n\n"}
{"id": "27313582", "url": "https://en.wikipedia.org/wiki?curid=27313582", "title": "Handover", "text": "Handover\n\nIn cellular telecommunications, the terms handover or handoff refer to the process of transferring an ongoing call or data session from one channel connected to the core network to another channel. In satellite communications it is the process of transferring satellite control responsibility from one earth station to another without loss or interruption of service.\n\nAmerican English uses the term \"handoff\", and this is most commonly used within some American organizations such as 3GPP2 and in American originated technologies such as CDMA2000. In British English the term \"handover\" is more common, and is used within international and European organisations such as ITU-T, IETF, ETSI and 3GPP, and standardised within European originated standards such as GSM and UMTS. The term handover is more common than handoff in academic research publications and literature, while handoff is slightly more common within the IEEE and ANSI organisations.\n\nIn telecommunications there may be different reasons why a handover might be conducted:\n\nThe most basic form of handover is when a phone call in progress is redirected from its current cell (called \"source\") to a new cell (called \"target\"). In terrestrial networks the source and the target cells may be served from two different cell sites or from one and the same cell site (in the latter case the two cells are usually referred to as two \"sectors\" on that cell site). Such a handover, in which the source and the target are different cells (even if they are on the same cell site) is called \"inter-cell\" handover. The purpose of inter-cell handover is to maintain the call as the subscriber is moving out of the area covered by the source cell and entering the area of the target cell.\n\nA special case is possible, in which the source and the target are one and the same cell and only the used channel is changed during the handover. Such a handover, in which the cell is not changed, is called \"intra-cell\" handover. The purpose of intra-cell handover is to change one channel, which may be interfered or fading with a new clearer or less fading channel.\n\nIn addition to the above classification of \"inter-cell\" and \"intra-cell\" classification of handovers, they also can be divided into hard and soft handovers:\nHandover can also be classified on the basis of handover techniques used. Broadly they can be classified into three types:\n\nAn advantage of the hard handover is that at any moment in time one call uses only one channel. The hard handover event is indeed very short and usually is not perceptible by the user. In the old analog systems it could be heard as a click or a very short beep; in digital systems it is unnoticeable. Another advantage of the hard handover is that the phone's hardware does not need to be capable of receiving two or more channels in parallel, which makes it cheaper and simpler. A disadvantage is that if a handover fails the call may be temporarily disrupted or even terminated abnormally. Technologies which use hard handovers, usually have procedures which can re-establish the connection to the source cell if the connection to the target cell cannot be made. However re-establishing this connection may not always be possible (in which case the call will be terminated) and even when possible the procedure may cause a temporary interruption to the call.\n\nOne advantage of the soft handovers is that the connection to the source cell is broken only when a reliable connection to the target cell has been established and therefore the chances that the call will be terminated abnormally due to failed handovers are lower. However, by far a bigger advantage comes from the mere fact that simultaneously channels in multiple cells are maintained and the call could only fail if all of the channels are interfered or fade at the same time. Fading and interference in different channels are unrelated and therefore the probability of them taking place at the same moment in all channels is very low. Thus the reliability of the connection becomes higher when the call is in a soft handover. Because in a cellular network the majority of the handovers occur in places of poor coverage, where calls would frequently become unreliable when their channel is interfered or fading, soft handovers bring a significant improvement to the reliability of the calls in these places by making the interference or the fading in a single channel not critical. This advantage comes at the cost of more complex hardware in the phone, which must be capable of processing several channels in parallel. Another price to pay for soft handovers is use of several channels in the network to support just a single call. This reduces the number of remaining free channels and thus reduces the capacity of the network. By adjusting the duration of soft handovers and the size of the areas in which they occur, the network engineers can balance the benefit of extra call reliability against the price of reduced capacity.\n\nWhile theoretically speaking soft handovers are possible in any technology, analog or digital, the cost of implementing them for analog technologies is prohibitively high and none of the technologies that were commercially successful in the past (e.g. AMPS, TACS, NMT, etc.) had this feature. Of the digital technologies, those based on FDMA also face a higher cost for the phones (due to the need to have multiple parallel radio-frequency modules) and those based on TDMA or a combination of TDMA/FDMA, in principle, allow not so expensive implementation of soft handovers. However, none of the 2G (second-generation) technologies have this feature (e.g. GSM, D-AMPS/IS-136, etc.). On the other hand, all CDMA based technologies, 2G and 3G (third-generation), have soft handovers. On one hand, this is facilitated by the possibility to design not so expensive phone hardware supporting soft handovers for CDMA and on the other hand, this is necessitated by the fact that without soft handovers CDMA networks may suffer from substantial interference arising due to the so-called \"near-far\" effect..\n\nIn all current commercial technologies based on FDMA or on a combination of TDMA/FDMA (e.g. GSM, AMPS, IS-136/DAMPS, etc.) changing the channel during a hard handover is realised by changing the pair of used transmit/receive frequencies.\n\nFor the practical realisation of handovers in a cellular network each cell is assigned a list of potential target cells, which can be used for handing over calls from this source cell to them. These potential target cells are called \"neighbors\" and the list is called \"neighbor list\". Creating such a list for a given cell is not trivial and specialized computer tools are used. They implement different algorithms and may use for input data from field measurements or computer predictions of radio wave propagation in the areas covered by the cells.\n\nDuring a call one or more parameters of the signal in the channel in the source cell are monitored and assessed in order to decide when a handover may be necessary. The downlink (forward link) and/or uplink (reverse link) directions may be monitored. The handover may be requested by the phone or by the base station (BTS) of its source cell and, in some systems, by a BTS of a neighboring cell. The phone and the BTSes of the neighboring cells monitor each other's signals and the best target candidates are selected among the neighboring cells. In some systems, mainly based on CDMA, a target candidate may be selected among the cells which are not in the neighbor list. This is done in an effort to reduce the probability of interference due to the aforementioned near-far effect.\n\nIn analog systems the parameters used as criteria for requesting a hard handover are usually the \"received signal power\" and the \"received signal-to-noise ratio\" (the latter may be estimated in an analog system by inserting additional tones, with frequencies just outside the captured voice-frequency band at the transmitter and assessing the form of these tones at the receiver). In non-CDMA 2G digital systems the criteria for requesting hard handover may be based on estimates of the received signal power, bit error rate (BER) and block error/erasure rate (BLER), received quality of speech (RxQual), distance between the phone and the BTS (estimated from the radio signal propagation delay) and others. In CDMA systems, 2G and 3G, the most common criterion for requesting a handover is Ec/Io ratio measured in the pilot channel (CPICH) and/or RSCP.\n\nIn CDMA systems, when the phone in soft or softer handover is connected to several cells simultaneously, it processes the received in parallel signals using a rake receiver. Each signal is processed by a module called \"rake finger\". A usual design of a rake receiver in mobile phones includes three or more rake fingers used in soft handover state for processing signals from as many cells and one additional finger used to search for signals from other cells. The set of cells, whose signals are used during a soft handover, is referred to as the \"active set\". If the search finger finds a sufficiently-strong signal (in terms of high Ec/Io or RSCP) from a new cell this cell is added to the active set. The cells in the neighbour list (called in CDMA \"neighbouring set\") are checked more frequently than the rest and thus a handover with a neighbouring cell is more likely, however a handover with others cells outside the neighbor list is also allowed (unlike in GSM, IS-136/DAMPS, AMPS, NMT, etc.).\n\nThere are occurrences where a handoff is unsuccessful. Much research has been dedicated to this problem. The source of the problem was discovered in the late 1980s. Because frequencies cannot be reused in adjacent cells, when a user moves from one cell to another, a new frequency must be allocated for the call. If a user moves into a cell when all available channels are in use, the user’s call must be terminated. Also, there is the problem of signal interference where adjacent cells overpower each other resulting in receiver desensitization.\n\nThere are also inter-technology handovers where a call's connection is transferred from one access technology to another, e.g. a call being transferred from GSM to UMTS or from CDMA IS-95 to cdma2000.\n\nThe 3GPP UMA/GAN standard enables GSM/UMTS handoff to Wi-Fi and vice versa.\n\nDifferent systems have different methods for handling and managing handoff request. Some systems handle handoff in same way as they handle new originating call. In such system the probability that the handoff will not be served is equal to blocking probability of new originating call. But if the call is terminated abruptly in the middle of conversation then it is more annoying than the new originating call being blocked. So in order to avoid this abrupt termination of ongoing call handoff request should be given priority to new call this is called as handoff prioritization.\n\nThere are two techniques for this:\n\n\n\n\n"}
{"id": "7468574", "url": "https://en.wikipedia.org/wiki?curid=7468574", "title": "IFT Research &amp; Development Award", "text": "IFT Research &amp; Development Award\n\nThe IFT Research & Development Award has been awarded since 1997. It has been awarded by the Institute of Food Technologists (IFT) to scientists who have made recent and significant research and development contributions to the understanding of food science, food technology, or nutrition.\n\nAward winners receive a USD 3000 honorarium and a plaque from IFT.\n\n"}
{"id": "925497", "url": "https://en.wikipedia.org/wiki?curid=925497", "title": "Land Warrior", "text": "Land Warrior\n\nLand Warrior was a United States Army program, cancelled in 2007, but restarted in 2008, that used a combination of commercial, off-the-shelf technology (COTS) and current-issue military gear and equipment designed to:\n\nWhile technology had long been a primary focus of the U.S. Armed Forces, very little of it had actually been adopted by the U.S. Army infantry soldier. With growing concerns of urban warfare and dismounted infantry actions, the U.S. Army recognized the need to upgrade an individual infantryman. The Land Warrior program drew upon many wearable computer concepts, and maximized existing technologies to correct most infantry soldier limitations in the short term.\n\nThe SI (Stryker Interoperable) version of the system completed U.S. Army testing as of November 2004. Due to limited resources, and issues with the overall weight of the system, Land Warrior was cancelled by the Army in February 2007, but restarted in July 2007. Despite the initial system's cancellation the 4th Stryker Brigade Combat Team (SBCT) was deployed to Iraq as part of the spring 2007 \"surge\" of U.S. forces, and used the Land Warrior, on which they had trained for the previous few years.\n\nThe systems and technology of the Land Warrior program were to be rolled into the Future Force Warrior program, and the Army has developed the Nett Warrior system to supersede Land Warrior as its next soldier network program.\n\nInternationally, there are several similar development programs, these include IdZ (Germany), FIST (UK), Félin (France), Land 125 (Australia), MARKUS (Sweden), Soldato Futuro (Italy), IMESS (Switzerland), Projekt TYTAN (Poland), FINSAS (India) and ACMS (Singapore), Ratnik (Russia), SARV (Iran).\n\nThe original Land Warrior program, by other name, was undertaken by General Electric in Moorestown, New Jersey approximately 1989, as a prototype having intent to eventually reduce size and weight in future phases. Then in the mid-1990s, the name Land Warrior was initially handled by a division of Hughes Aerospace, which was subsequently acquired by Raytheon. (The soldier radio component of Land Warrior was to be supplied by the Integrated Information Systems division of Motorola.)\n\nEarly demonstration versions of the LW system used software written in the Ada programming language running on a Unix platform. In January 1999, in an attempt to reduce development costs and accelerate the program, the development work was transitioned to a multi-company team that had been organized by Exponent (NASDAQ: EXPO), an engineering firm with headquarters in Silicon Valley.\n\nAn intensive redesign of the system ensued, and both the embedded firmware and the application software were rewritten from scratch. Many of the COTS hardware components were purchased (literally \"off the shelf\") at Fry's Electronics, the Silicon Valley retail chain. Approximately 100 proof-of-concept Land Warrior units were built and successfully demonstrated in September 2000 by a U.S. Army platoon that was air-dropped into a large war-fighting exercise at Fort Polk, Louisiana.\n\nThese initial prototype units, designated Land Warrior v0.6, were built around a PC/104 computer platform running Microsoft Windows. The system used the CAN-bus protocol on the wired PAN (personal area network). The communications subsystem was built using Windows CE running on a StrongARM platform, and the wireless network protocol was IEEE 802.11. During the Fort Polk exercise, preliminary interoperability with traditional military radio networks was also demonstrated for LW v0.6, using a two-way, SINCGARS-compatible gateway radio.\n\nThe success of the Fort Polk exercise reinvigorated the program, and further funding was allocated for the next phase of LW development. A \"Land Warrior Consortium\" was formed by several of the contracting firms, with the goal of designing and building the first field-able LW system, designated LW v1.0, later LW-IC (Land Warrior – Initial Capability). The basic Windows and WinCE platforms were retained, and a new hybrid PAN was designed, which drew upon both USB and FireWire protocols. A modified version of the IEEE 802.11 protocol was adopted, which added various enhancements for COMSEC and information security, mobile ad hoc network (MANET) capabilities, and support for multi-hop packet routing.\n\nIn 2003, a variant of the LW-IC system was developed to incorporate features of the CombatID System (CIDS) – a form of IFF (identification friend or foe) that is designed to reduce the potential for friendly fire incidents. This version, designated LW-CIDS, was successfully demonstrated in interoperational tests with several other CIDS-equipped units at Moffett Field, California.\n\nAs the Land Warrior program matured, it became clear that its successful deployment would hinge significantly upon the key factor of batteries. The need to continuously resupply (or recharge) LW batteries was proving to be a major logistical challenge. This was one of the driving factors behind the decision to move away from an earlier plan to initially equip airborne Army units, as in the Fort Polk exercise, and to focus instead upon those using Stryker ground vehicle systems. This latter approach would enable more LW batteries to be distributed and/or recharged as needed.\n\nThe contract for development of the Land Warrior – Stryker Interoperable (LW-SI) version of the system was awarded in 2003 to an industry team that was led by General Dynamics and included most of the existing Land Warrior Consortium companies. At about the same time, further development of the existing LW-IC system was halted and the manufacturing plans for it were shelved indefinitely. The Land Warrior Consortium was formally disbanded and work got under way on the newly focused LW-SI program.\n\nIn September 2006, the 4th Battalion, 9th Infantry Regiment trained with and evaluated the LW-SI system. The system successfully completed the assessment, which was based on Joint Capabilities Integration and Development System (JCIDS) guidance, and received testimonials from the unit. However, funding for further system development under the Land Warrior program was suspended in February 2007, although the 4-9 Infantry was deployed to Iraq and used the LW-SI system extensively. The Land Warrior program was then re-instated. This was the final trial phase before the key decision is made on the overall future of wearable soldier systems, including Future Force Warrior. The decision was made in 2011 to continue the program.\n\nLand Warrior has three priority objectives:\n\nLand Warrior has seven main subsystems:\n\nLater features of the Land Warrior system included:\n\nThe original system was built around the M16 rifle or M4 carbine, both with modular rail mounts to allow customization as needed for each mission. It included the weapon itself, plus components such as a daylight video sight, thermal weapons sight and MFL (Multi-Function Laser). The MFL provided range and direction information, as well as IR, visible, and MILES lasers, while the cameras provided a video feed and thermographic capabilities, plus allowing a soldier to shoot around corners or behind cover without actually exposing himself to enemy fire. This is highly effective for confirming kills without exposing one's position.\n\nThe Helmet Subsystem (HSS) combined a lightweight advanced helmet with a computer and OLED display that provided various information from digital maps and troop locations down to his weapon-mounted video camera. This is what would have allowed the soldier to see (and fire) around corners. The HSS also incorporated a microphone as well as a headset.\n\nThe Interceptor Body Armor system and Modular Lightweight Load-carrying Equipment (MOLLE) load-bearing system currently in service with the U.S. Army today are partially a result of the Land Warrior program.\n\nThe Computer Subsystem (CSS) provided the processing power and storage capacity for the system. The CSS is based around an ARM XScale processor. The CSS connects to each one of the LRUs as well as to the batteries.\n\nThe Navigation Subsystem (NSS) provided positional information, it integrates a GPS receiver and a Dead Reckoning Module (DRM) that maintains accurate location when GPS signal becomes unavailable.\n\nThe Communication Network Radio Subsystem (CNRS) provided communications capabilities for the Land Warrior. The CNRS is based on EPLRS.\n\nLand Warrior's software system was powered by a variant of the Linux operating system and has a modular, open architecture for further improvement. Reliability in recent testing at Fort Benning has been extremely high. The Land Warrior software suite contains six main software packages for weapon sights and for data.\n\nA key capability of the Land Warrior development effort was the interoperability with the Stryker family of combat vehicles, attained through a Stryker Vehicle Integration Kit or VIK.\n\nLand Warriors mounted in the Stryker vehicle were to be provided voice, data and power connectivity through an umbilical connection, as an extension of the individual soldier's Personal Area Network. The mounted Land Warriors were to have the capability of communicating voice and data with each other, the vehicle crew, Land Warriors external to the vehicle and other Land Warrior units mounted in other Stryker vehicles. Communications with the vehicle crew were to be achieved by interfacing to the Vehicle Intercom System.\n\nDismounted Land Warriors, when within communications range of their Stryker vehicle, were to have the same voice connectivity and Army Battle Command System interoperability as the mounted Land Warrior.\n\nThe Vehicle Integration Kit (VIK) was being developed in a spiral fashion to help manage development risk. The first spiral produced early VIK prototypes used to validate physical integration approaches in the various vehicle configurations. It also demonstrated Land Warrior soldier voice connectivity with the Stryker crew-members in and around the vehicle through the Vehicle Intercom System.\n\nThere were plans to integrate the Boomerang Mobile Shooter Detection System onto the Stryker vehicle as part of the Land Warrior system.\n\nLand Warrior was the first U.S. Army soldier network system to be used in combat since the networked soldier concept began in 1989. 229 Land Warrior ensembles were deployed by the 4th Battalion, 9th Infantry Regiment to Iraq from May 2007 to June 2008. A Stryker Brigade later deployed with the system to Afghanistan, and Land Warrior remained in use until spring 2012. The Army built upon Land Warrior with the Ground Soldier System (GSS) as its successor, an advanced dismounted soldier integrated situational awareness system that entered technology development in February 2009. GSS Increment 1 was renamed Nett Warrior in June 2010 after Medal of Honor recipient Robert B. Nett (although the term \"Nett\" has frequently been misinterpreted as a reflection of its tactical networking features). Nett Warrior was first demonstrated in spring 2011, which then was essentially the 10-pound Land Warrior ensemble with some enhanced additional software. Later iterations focused on a hand-held solution that integrated a commercial hand-held screen device with the Rifleman radio, simplifying the system and decreasing weight to . Nett Warrior utilizes enhanced smartphones, using different models over the course of its development including the Motorola Atrix, Samsung Galaxy Note I, and the Samsung Galaxy Note 2.\n\n\n9. The Land Warrior Soldier System: A Case Study for the Acquisition of\nSoldier Systems, NAVAL POSTGRADUATE SCHOOL MONTEREY, CALIFORNIA, MBA PROFESSIONAL REPORT, http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ada493630\n\n"}
{"id": "62389", "url": "https://en.wikipedia.org/wiki?curid=62389", "title": "Langmuir probe", "text": "Langmuir probe\n\nA Langmuir probe is a device used to determine the electron temperature, electron density, and electric potential of a plasma. It works by inserting one or more electrodes into a plasma, with a constant or time-varying electric potential between the various electrodes or between them and the surrounding vessel. The measured currents and potentials in this system allow the determination of the physical properties of the plasma.\n\nThe beginning of Langmuir probe theory is the \"I-V\" characteristic of the Debye sheath, that is, the current density flowing to a surface in a plasma as a function of the voltage drop across the sheath. The analysis presented here indicates how the electron temperature, electron density, and plasma potential can be derived from the \"I-V\" characteristic. In some situations a more detailed analysis can yield information on the ion density (formula_1), the ion temperature formula_2, or the electron energy distribution function (EEDF) or formula_3.\n\nConsider first a surface biased to a large negative voltage. If the voltage is large enough, essentially all electrons (and any negative ions) will be repelled. The ion velocity will satisfy the Bohm sheath criterion, which is, strictly speaking, an inequality, but which is usually marginally fulfilled. The Bohm criterion in its marginal form says that the ion velocity at the sheath edge is simply the sound speed given by\n\nformula_4.\n\nThe ion temperature term is often neglected, which is justified if the ions are cold. Even if the ions are known to be warm, the ion temperature is usually not known, so it is usually assumed to be simply equal to the electron temperature. In that case, consideration of finite ion temperature only results in a small numerical factor. \"Z\" is the (average) charge state of the ions, and formula_5 is the adiabatic coefficient for the ions. The proper choice of formula_5 is a matter of some contention. Most analyses use formula_7, corresponding to isothermal ions, but some kinetic theory suggests that formula_8, corresponding to one degree of freedom is more appropriate. For formula_9 and formula_10, using the larger value results in the conclusion that the density is formula_11 times smaller. Uncertainties of this magnitude arise several places in the analysis of Langmuir probe data and are very difficult to resolve.\n\nThe charge density of the ions depends on the charge state \"Z\", but quasineutrality allows one to write it simply in terms of the electron density as formula_12.\n\nUsing these results we have the current density to the surface due to the ions. The current density at large negative voltages is due solely to the ions and, except for possible sheath expansion effects, does not depend on the bias voltage, so it is\nreferred to as the ion saturation current density and is given by\n\nformula_13 where formula_14 is the charge of an electron, formula_15 is the number density of electrons, and formula_16 is as defined above.\n\nThe plasma parameters, in particular, the density, are those at the sheath edge.\n\nAs the voltage of the Debye sheath is reduced, the more energetic electrons are able to overcome the potential barrier of the electrostatic sheath. We can model the electrons at the sheath edge with a Maxwell–Boltzmann distribution, i.e.,\n\nformula_17,\n\nexcept that the high energy tail moving away from the surface is missing, because only the lower energy electrons moving toward the surface are reflected. The higher energy electrons overcome the sheath potential and are absorbed. The mean velocity of the electrons which are able to overcome the voltage of the sheath is\n\nformula_18,\n\nwhere the cut-off velocity for the upper integral is\n\nformula_19.\n\nformula_20 is the voltage across the Debye sheath, that is, the potential at the sheath edge minus the potential of the surface. For a large voltage compared to the electron temperature, the result is\n\nformula_21.\n\nWith this expression, we can write the electron contribution to the current to the probe in terms of the ion saturation current as\n\nformula_22,\n\nvalid as long as the electron current is not more than two or three times the ion current.\n\nThe total current, of course, is the sum of the ion and electron currents:\n\nformula_23.\n\nWe are using the convention that current \"from\" the surface into the plasma is positive. An interesting and practical question is the potential of a surface to which no net current flows. It is easily seen from the above equation that\n\nformula_24.\n\nIf we introduce the ion reduced mass formula_25, we can write\n\nformula_26\n\nSince the floating potential is the experimentally accessible quantity, the current (below electron saturation) is usually written as \n\nformula_27.\n\nWhen the electrode potential is equal to or greater than the plasma potential, then there is no longer a sheath to reflect electrons, and the electron current saturates. Using the Boltzmann expression for the mean electron velocity given above with formula_28 and setting the ion current to zero, the electron saturation current density would be\n\nformula_29\n\nAlthough this is the expression usually given in theoretical discussions of Langmuir probes, the derivation is not rigorous and the experimental basis is weak. The theory of double layers typically employs an expression analogous to the Bohm criterion, but with the roles of electrons and ions reversed, namely\n\nformula_30\n\nwhere the numerical value was found by taking \"T\"=\"T\" and γ=γ.\n\nIn practice, it is often difficult and usually considered uninformative to measure the electron saturation current experimentally. When it is measured, it is found to be highly variable and generally much lower (a factor of three or more) than the value given above. Often a clear saturation is not seen at all. Understanding electron saturation is one of the most important outstanding problems of Langmuir probe theory.\n\nThe Debye sheath theory explains the basic behavior of Langmuir probes, but is not complete. Merely inserting an object like a probe into a plasma changes the density, temperature, and potential at the sheath edge and perhaps everywhere. Changing the voltage on the probe will also, in general, change various plasma parameters. Such effects are less well understood than sheath physics, but they can at least in some cases be roughly accounted.\n\nThe Bohm criterion requires the ions to enter the Debye sheath at the sound speed. The potential drop that accelerates them to this speed is called the pre-sheath. It has a spatial scale that depends on the physics of the ion source but which is large compared to the Debye length and often of the order of the plasma dimensions. The magnitude of the potential drop is equal to (at least)\n\nformula_31\n\nThe acceleration of the ions also entails a decrease in the density, usually by a factor of about 2 depending on the details.\n\nCollisions between ions and electrons will also affect the \"I-V\" characteristic of a Langmuir probe. When an electrode is biased to any voltage other than the floating potential, the current it draws must pass through the plasma, which has a finite resistivity. The resistivity and current path can be calculated with relative ease in an unmagnetized plasma. In a magnetized plasma, the problem is much more difficult. In either case, the effect is to add a voltage drop proportional to the current drawn, which shears the characteristic. The deviation from an exponential function is usually not possible to observe directly, so that the flattening of the characteristic is usually misinterpreted as a larger plasma temperature. Looking at it from the other side, any measured \"I-V\" characteristic can be interpreted as a hot plasma, where most of the voltage is dropped in the Debye sheath, or as a cold plasma, where most of the voltage is dropped in the bulk plasma. Without quantitative modeling of the bulk resistivity, Langmuir probes can only give an upper limit on the electron temperature.\n\nIt is not enough to know the current \"density\" as a function of bias voltage since it is the \"absolute\" current which is measured. In an unmagnetized plasma, the current-collecting area is usually taken to be the exposed surface area of the electrode. In a magnetized plasma, the projected area is taken, that is, the area of the electrode as viewed along the magnetic field. If the electrode is not shadowed by a wall or other nearby object, then the area must be doubled to account for current coming along the field from both sides. If the electrode dimensions are not small in comparison to the Debye length, then the size of the electrode is effectively increased in all directions by the sheath thickness. In a magnetized plasma, the electrode is sometimes assumed to be increased in a similar way by the ion Larmor radius.\n\nThe finite Larmor radius allows some ions to reach the electrode that would have otherwise gone past it. The details of the effect have not been calculated in a fully self-consistent way.\n\nIf we refer to the probe area including these effects as formula_32 (which may be a function of the bias voltage) and make the assumptions\nand ignore the effects of\nthen the \"I-V\" characteristic becomes\n\nformula_37,\n\nwhere\n\nformula_38.\n\nThe theory of Langmuir probes is much more complex when the plasma is magnetized. The simplest extension of the unmagnetized case is simply to use the projected area rather than the surface area of the electrode. For a long cylinder far from other surfaces, this reduces the effective area by a factor of π/2 = 1.57. As mentioned before, it might be necessary to increase the radius by about the thermal ion Larmor radius, but not above the effective area for the unmagnetized case.\n\nThe use of the projected area seems to be closely tied with the existence of a magnetic sheath. Its scale is the ion Larmor radius at the sound speed, which is normally between the scales of the Debye sheath and the pre-sheath. The Bohm criterion for ions entering the magnetic sheath applies to the motion along the field, while at the entrance to the Debye sheath it applies to the motion normal to the surface. This results in a reduction of the density by the sine of the angle between the field and the surface. The associated increase in the Debye length must be taken into account when considering ion non-saturation due to sheath effects.\n\nEspecially interesting and difficult to understand is the role of cross-field currents. Naively, one would expect the current to be parallel to the magnetic field along a flux tube. In many geometries, this flux tube will end at a surface in a distant part of the device, and this spot should itself exhibit an \"I-V\" characteristic. The net result would be the measurement of a double-probe characteristic; in other words, electron saturation current equal to the ion saturation current.\n\nWhen this picture is considered in detail, it is seen that the flux tube must charge up and the surrounding plasma must spin around it. The current into or out of the flux tube must be associated with a force that slows down this spinning. Candidate forces are viscosity, friction with neutrals, and inertial forces associated with plasma flows, either steady or fluctuating. It is not known which force is strongest in practice, and in fact it is generally difficult to find any force that is powerful enough to explain the characteristics actually measured.\n\nIt is also likely that the magnetic field plays a decisive role in determining the level of electron saturation, but no quantitative theory is as yet available.\n\nOnce one has a theory of the \"I-V\" characteristic of an electrode, one can proceed to measure it and then fit the data with the theoretical curve to extract the plasma parameters. The straightforward way to do this is to sweep the voltage on a single electrode, but, for a number of reasons, configurations using multiple electrodes or exploring only a part of the characteristic are used in practice.\n\nThe most straightforward way to measure the \"I-V\" characteristic of a plasma is with a single probe, consisting of one electrode biased with a voltage ramp relative to the vessel. The advantages are simplicity of the electrode and redundancy of information, i.e. one can check whether the \"I-V\" characteristic has the expected form. Potentially additional information can be extracted from details of the characteristic. The disadvantages are more complex biasing and measurement electronics and a poor time resolution. If fluctuations are present (as they always are) and the sweep is slower than the fluctuation frequency (as it usually is), then the \"I-V\" is the \"average\" current as a function of voltage, which may result in systematic errors if it is analyzed as though it were an instantaneous \"I-V\". The ideal situation is to sweep the voltage at a frequency above the fluctuation frequency but still below the ion cyclotron frequency. This, however, requires sophisticated electronics and a great deal of care.\n\nAn electrode can be biased relative to a second electrode, rather than to the ground. The theory is similar to that of a single probe, except that the current is limited to the ion saturation current for both positive and negative voltages. In particular, if formula_39 is the voltage applied between two identical electrodes, the current is given by;\n\nformula_40,\n\nwhich can be rewritten using formula_41 as a hyperbolic tangent:\n\nformula_42.\n\nOne advantage of the double probe is that neither electrode is ever very far above floating, so the theoretical uncertainties at large electron currents are avoided. If it is desired to sample more of the exponential electron portion of the characteristic, an asymmetric double probe may be used, with one electrode larger than the other. If the ratio of the collection areas is larger than the square root of the ion to electron mass ratio, then this arrangement is equivalent to the single tip probe. If the ratio of collection areas is not that big, then the characteristic will be in-between the symmetric double tip configuration and the single-tip configuration. If formula_43 is the area of the larger tip then:\n\nformula_44\n\nAnother advantage is that there is no reference to the vessel, so it is to some extent immune to the disturbances in a radio frequency plasma. On the other hand, it shares the limitations of a single probe concerning complicated electronics and poor time resolution. In addition, the second electrode not only complicates the system, but it makes it susceptible to disturbance by gradients in the plasma.\n\nAn elegant electrode configuration is the triple probe, consisting of two electrodes biased with a fixed voltage and a third which is floating. The bias voltage is chosen to be a few times the electron temperature so that the negative electrode draws the ion saturation current, which, like the floating potential, is directly measured. A common rule of thumb for this voltage bias is 3/e times the expected electron temperature. Because the biased tip configuration is floating, the positive probe can draw at most an electron current only equal in magnitude and opposite in polarity to the ion saturation current drawn by the negative probe, given by :\n\nformula_45\n\nand as before the floating tip draws effectively no current:\n\nformula_46.\n\nAssuming that: \n1.) The electron energy distribution in the plasma is Maxwellian,\n2.) The mean free path of the electrons is greater than the ion sheath about the tips and larger than the probe radius, and\n3.) the probe sheath sizes are much smaller than the probe separation,\nthen the current to any probe can be considered composed of two partsthe high energy tail of the Maxwellian electron distribution, and the ion saturation current:\n\nformula_47\n\nwhere the current \"I\" is thermal current. Specifically,\n\nformula_48,\n\nwhere \"S\" is surface area, \"J\" is electron current density, and \"n\" is electron density.\n\nAssuming that the ion and electron saturation current is the same for each probe, then the formulas for current to each of the probe tips take the form\n\nformula_49\n\nformula_50\n\nformula_51.\n\nIt is then simple to show\n\nformula_52\n\nbut the relations from above specifying that \"I=-I\" and \"I\"=0 give\n\nformula_53,\n\na transcendental equation in terms of applied and measured voltages and the unknown \"T\" that in the limit \"qV = q(V-V) » k T\", becomes\n\nformula_54.\n\nThat is, the voltage difference between the positive and floating electrodes is proportional to the electron temperature. (This was especially important in the sixties and seventies before sophisticated data processing became widely available.)\n\nMore sophisticated analysis of triple probe data can take into account such factors as incomplete saturation, non-saturation, unequal areas.\n\nTriple probes have the advantage of simple biasing electronics (no sweeping required), simple data analysis, excellent time resolution, and insensitivity to potential fluctuations (whether imposed by an rf source or inherent fluctuations). Like double probes, they are sensitive to gradients in plasma parameters.\n\nArrangements with four (tetra probe) or five (penta probe) have sometimes been used, but the advantage over triple probes has never been entirely convincing. The spacing between probes must be larger than the Debye length of the plasma to prevent an overlapping Debye sheath.\n\nA pin-plate probe consists of a small electrode directly in front of a large electrode, the idea being that the voltage sweep of the large probe can perturb the plasma potential at the sheath edge and thereby aggravate the difficulty of interpreting the \"I-V\" characteristic. The floating potential of the small electrode can be used to correct for changes in potential at the sheath edge of the large probe. Experimental results from this arrangement look promising, but experimental complexity and residual difficulties in the interpretation have prevented this configuration from becoming standard.\n\nVarious geometries have been proposed for use as ion temperature probes, for example, two cylindrical tips that rotate past each other in a magnetized plasma. Since shadowing effects depend on the ion Larmor radius, the results can be interpreted in terms of ion temperature. The ion temperature is an important quantity that is very difficult to measure. Unfortunately, it is also very difficult to analyze such probes in a fully self-consistent way.\n\nEmissive probes use an electrode heated either electrically or by the exposure to the plasma. When the electrode is biased more positive than the plasma potential, the emitted electrons are pulled back to the surface so the \"I\"-\"V\" characteristic is hardly changed. As soon as the electrode is biased negative with respect to the plasma potential, the emitted electrons are repelled and contribute a large negative current. The onset of this current or, more sensitively, the onset of a discrepancy between the characteristics of an unheated and a heated electrode, is a sensitive indicator of the plasma potential.\n\nTo measure fluctuations in plasma parameters, arrays of electrodes are used, usually onebut occasionally two-dimensional. A typical array has a spacing of 1 mm and a total of 16 or 32 electrodes. A simpler arrangement to measure fluctuations is a negatively biased electrode flanked by two floating electrodes. The ion-saturation current is taken as a surrogate for the density and the floating potential as a surrogate for the plasma potential. This allows a rough measurement of the turbulent particle flux\n\nformula_55\n\nMost often, the Langmuir probe is a small size electrode inserted in plasma and connected through an external (with respect to plasma) electric circuit with the electrode of a large surface area contacting with the same plasma (very often it is metallic wall of a chamber containing plasma) to obtain the probe I-V characteristic formula_56. The characteristic formula_56 is measured by sweeping the voltage formula_58 of scanning generator (inserted in the probe circuit) with simultaneous measuring of the probe current.\n\nRelations between the probe I-V characteristic and parameters of isotropic plasma were found by the Irving Langmuir and they can be derived most elementary for the planar probe of a large surface area formula_59 (ignoring the edge effects problem). Let us choose the point formula_60 in plasma at the distance formula_61 from the probe surface where electric field of the probe is negligible while each electron of plasma passing this point could reach the probe surface without collisions with plasma components: formula_62, formula_63 is the Debye length and formula_64 is the electron free path calculated for its total cross section with plasma components. In the vicinity of the point formula_60 we can imagine a small element of the surface area formula_66 parallel to the probe surface. The elementary current formula_67 of plasma electrons passing throughout formula_66 in a direction of the probe surface can be written in the form\n\nformula_69, (1)\n\nwhere formula_70 is a scalar of the electron thermal velocity vector formula_71,\n\nformula_72, (2)\n\nformula_73 is the element of the solid angle with its relative value formula_74, formula_75 is the angle between perpendicular to the probe surface recalled from the point formula_60 and the radius-vector of the electron thermal velocity formula_71 forming a spherical layer of thickness formula_78 in velocity space, and formula_79 is the electron distribution function normalized to unity\n\nformula_80. (3)\n\nTaking into account uniform conditions along the probe surface (boundaries are excluded), formula_81, we can take double integral with respect to the angle formula_82, and with respect to the velocity formula_83, from the expression (1), after substitution Eq. (2) in it, to calculate a total electron current on the probe\n\nformula_84. (4)\n\nwhereformula_58 is the probe potential with respect to the potential of plasma formula_86, formula_87 is the lowest electron velocity value at which the electron still could reach the probe surface charged to the potential formula_58, formula_89 is the upper limit of the angle formula_75 at which the electron having initial velocity formula_70 can still reach the probe surface with a zero-value of its velocity at this surface. That means the value formula_89 is defined by the condition\n\nformula_93. (5)\n\nDeriving the value formula_89 from Eq. (5) and substituting it in Eq. (4), we can obtain the probe I-V characteristic (neglecting the ion current) in the range of the probe potential formula_95 in the form\n\nformula_96. (6)\n\nDifferentiating Eq. (6) twice with respect to the potential formula_58, one can find the expression describing the second derivative of the probe I-V characteristic (obtained firstly by M. J. Druyvestein \n\nformula_98 (7)\n\ndefining the electron distribution function over velocity formula_99 in the evident form. M. J. Druyvestein has shown in particular that Eqs. (6) and (7) are valid for description of operation of the probe of any arbitrary convex geometrical shape. \n\nformula_100, (8)\n\nwhere formula_101 is the most probable velocity, in Eq. (6) we obtain the expression\n\nformula_102. (9)\n\nFrom which the very useful in practice relation follows\n\nformula_103. (10)\n\nallowing one to derive the electron energy formula_104 (for Maxwellian distribution function only!) by a slope of the probe I-V characteristic in a semilogarithmic scale. \nThus in plasmas with isotropic electron distributions, the electron current formula_105 on a surface formula_106 of the cylindrical Langmuir probe at plasma potential formula_86 is defined by the average electron thermal velocity formula_108 and can be written down as equation (see Eqs. (6), (9) at formula_86)\n\nformula_110, (11)\n\nwhere formula_111 is the electron concentration, formula_112 is the probe radius, and formula_113 is its length. \nIt is obvious that if plasma electrons form an electron wind (flow) across the cylindrical probe axis with a velocity formula_114, the expression \nformula_115 (12)\n\nholds true. \nIn plasmas produced by gas-discharge arc sources as well as inductively coupled sources, the electron wind can develop the Mach number formula_116 . Here the parameter formula_117 is introduced along with the Mach number for simplification of mathematical expressions. Note that formula_118, whereformula_119 is the most probable velocity for the Maxwellian distribution function, so that formula_120 . Thus the general case where formula_121 is of the theoretical and practical interest. \nCorresponding physical and mathematical considerations presented in Refs. [9,10] has shown that at the Maxwellian distribution function of the electrons in a reference system moving with the velocity formula_122 across axis of the cylindrical probe set at plasma potential formula_86, the electron current on the probe can be written down in the form\n\nformula_124, (13)\n\nwhere formula_125 and formula_126 are Bessel functions of imaginary arguments and Eq. (13) is reduced to Eq. (11) atformula_127 being reduced to Eq. (12) at formula_128 . \nThe second derivative of the probe I-V characteristic formula_129 with respect to the probe potential formula_58 can be presented in this case in the form (see Fig. 3)\n\nformula_131, (14)\n\nwhere\n\nformula_132 (15)\n\nand the electron energy formula_133 is expressed in eV.\n\nAll parameters of the electron population: formula_111, formula_135, formula_136 and formula_119 in plasma can be derived from the experimental probe I-V characteristic second derivative formula_129 by its least square best fitting with the theoretical curve expressed by Eq. (14). For detail and for problem of the general case of none-Maxwellian electron distribution functions see. \n\nFor laboratory and technical plasmas, the electrodes are most commonly tungsten or tantalum wires several thousandths of an inch thick, because they have a high melting point but can be made small enough not to perturb the plasma. Although the melting point is somewhat lower, molybdenum is sometimes used because it is easier to machine and solder than tungsten. For fusion plasmas, graphite electrodes with dimensions from 1 to 10 mm are usually used because they can withstand the highest power loads (also sublimating at high temperatures rather than melting), and result in reduced bremsstrahlung radiation (with respect to metals) due to the low atomic number of carbon. The electrode surface exposed to the plasma must be defined, e.g. by insulating all but the tip of a wire electrode. If there can be significant deposition of conducting materials (metals or graphite), then the insulator should be separated from the electrode by a to prevent short-circuiting.\n\nIn a magnetized plasma, it appears to be best to choose a probe size a few times larger than the ion Larmor radius. A point of contention is whether it is better to use proud probes, where the angle between the magnetic field and the surface is at least 15°, or flush-mounted probes, which are embedded in the plasma-facing components and generally have an angle of 1 to 5 °. Many plasma physicists feel more comfortable with proud probes, which have a longer tradition and possibly are less perturbed by electron saturation effects, although this is disputed. Flush-mounted probes, on the other hand, being part of the wall, are less perturbative. Knowledge of the field angle is necessary with proud probes to determine the fluxes to the wall, whereas it is necessary with flush-mounted probes to determine the density.\n\nIn very hot and dense plasmas, as found in fusion research, it is often necessary to limit the thermal load to the probe by limiting the exposure time. A reciprocating probe is mounted on an arm that is moved into and back out of the plasma, usually in about one second by means of either a pneumatic drive or an electromagnetic drive using the ambient magnetic field. Pop-up probes are similar, but the electrodes rest behind a shield and are only moved the few millimeters necessary to bring them into the plasma near the wall.\n\nA Langmuir probe can be purchased off the shelf for on the order of 15,000 U.S. dollars, or they can be built by an experienced researcher or technician. When working at frequencies under 100 MHz, it is advisable to use blocking filters, and take necessary grounding precautions.\n\nIn low temperature plasmas, in which the probe does not get hot, surface contamination may become an issue. This effect can cause hysteresis in the I-V curve and may limit the current collected by the probe. A heating mechanism or a glow discharge plasma may be used to clean the probe and prevent misleading results.\n\n\n\n"}
{"id": "52935507", "url": "https://en.wikipedia.org/wiki?curid=52935507", "title": "List of Instax cameras and printers", "text": "List of Instax cameras and printers\n\nThis is a list of cameras and printers designed for use with Fujifilm's Instax series of instant film.\n"}
{"id": "1062244", "url": "https://en.wikipedia.org/wiki?curid=1062244", "title": "List of cartridges by caliber", "text": "List of cartridges by caliber\n\nCalibers in the size range of (mm, inches): \n\nBy name\n"}
{"id": "1173559", "url": "https://en.wikipedia.org/wiki?curid=1173559", "title": "Living street", "text": "Living street\n\nA living street is a street designed primarily with the interests of pedestrians and cyclists in mind and as a social space where people can meet and where children may also be able to play legally and safely. These roads are still available for use by motor vehicles, however their design aims to reduce both the speed and dominance of motorised transport. This is often achieved using the shared space approach, with greatly reduced demarcations between vehicle traffic and pedestrians. Vehicle parking may also be restricted to designated bays. It became popular during the 1970s in the Netherlands, which is why the Dutch word for a living street (woonerf) is often used as a synonym.\n\nCountry-specific living street implementations include: home zone (United Kingdom), residential zone (, Russia), shared zone (Australia/New Zealand), woonerf (Netherlands, Flanders, South Africa, Namibia) and \"zone résidentielle\" (France).\n\nLegislation was introduced in the United Kingdom with the Highway Act 1835 which banned the playing of football and games on the highway. In 1859 a total of 44 children were sent to prison for failure to pay fines for playing in the street in London and Middlesex, rising to 2,000 young people under the age of seventeen by 1935.\n\nAs the level of fast motorised traffic increased during the 20th century it became apparent that the social and recreational functions of the street were being severely impaired by the volume, speed and dominance of vehicular traffic.\n\nThe woonerf movement originated in the Netherlands in the 1970s as a way of re-balancing the relationship between people and the movement of vehicles.\n\nThese streets are often built at the same grade as sidewalks, without curbs. Cars are limited to a speed that does not disrupt other uses of the streets (usually defined to be pedestrian speed), or through traffic is eliminated using bollards or circuitous one-way operation. To make this lower speed natural, the street is normally set up so that a car cannot drive in a straight line for significant distances, for example by placing planters at the edge of the street, alternating the side of the street the parking is on, or curving the street itself. Other traffic calming measures are also used. \n\nHowever, early methods of traffic calming such as speed humps are now avoided in favor of methods which make slower speeds more natural to drivers, rather than an obvious imposition. Implementations of living streets that fail to address motor vehicle speed and volume usually result in domination of the street by motor vehicles and the marginalisation of walking and cycling. Furthermore, the elimination of a clearly defined boundary between vehicles and pedestrians can negatively affect walkability for those with sensory disabilities, most notably blindness.\n\n\n"}
{"id": "26071502", "url": "https://en.wikipedia.org/wiki?curid=26071502", "title": "Micro-sustainability", "text": "Micro-sustainability\n\nMicro-sustainability focuses on the small environmental actions that collectively result in a large environmental impact. Micro-sustainability centers on individual efforts, behavior modification and creating attitudinal changes, which result in an environmentally conscious individual or community. Micro-sustainability encourages sustainable changes through \"change agents\", which are individuals that are encouraged; and therefore, foster positive environmental action inside their sphere of influence. Examples of micro-sustainability include recycling, power saving by turning off unused lights, programming thermostats for efficient use of energy, reducing water usage, changing driving habits or patterns in order to use less gasoline or modifying buying habits to reduce waste and consumption. The focus is on individual actions, rather than organizational practices. These narrow, small ticket, community level actions have immediate local benefits. If widely imitated, they have a cumulative, broader impact.\n\nThe remaining large-scale plans for sustainability, categorized under the term \"macro-sustainability\", are in most cases addressed by governments, multi-national corporations or companies. They combat global issues including climate change, and reliance upon petroleum-based energy sources. Businesses primarily focus on the business case and return on investment of changes such as their source of energy or the way they transport or manufacture products. Governments confront these larger issues through increased regulation, subsidies, and investment in new technologies and energy sources.\n\n\n"}
{"id": "35697992", "url": "https://en.wikipedia.org/wiki?curid=35697992", "title": "Moment-resisting frame", "text": "Moment-resisting frame\n\nMoment-resisting frame is a rectilinear assemblage of beams and columns, with the beams rigidly connected to the columns.\n\nResistance to lateral forces is provided primarily by rigid frame action – that is, by the development of bending moment and shear force in the frame members and joints. By virtue of the rigid beam–column connections, a moment frame cannot displace laterally without bending the beams or columns depending on the geometry of the connection. The bending rigidity and strength of the frame members is therefore the primary source of lateral stiffness and strength for the entire frame.\n\nThese connections between beams and columns, are formed through the use of rigid moment connections. A Moment Connection is a joint that allows the transfer of bending moment forces between a column and beam (or any other two members). This is different to shear or pinned connections that prevent a moment-resisting frame to occur. An examples of this is pinned truss structures.\n\nThe 1994 Northridge earthquake revealed a common flaw in steel-frame construction — poorly welded moment connections — and building codes were revised to strengthen them.\n\nSteel moment-resisting frames have been in use for more than one hundred years, dating to the earliest use of structural steel in building construction. Steel building construction with the frame carrying the vertical loads initiated with the Home Insurance Building in Chicago, a 10-story structure constructed in 1884 with a height of 138 ft, often credited with being the first skyscraper. This and other tall buildings in Chicago spawned an entire generation of tall buildings, constructed with load bearing steel frames supporting concrete floors and non-load bearing, unreinforced masonry infill walls at their perimeters. Framing in these early structures typically utilized \"H\" shapes built up from plates, and \"L\" and \"Z\" sections.\n"}
{"id": "2995702", "url": "https://en.wikipedia.org/wiki?curid=2995702", "title": "Multimedia Broadcast Multicast Service", "text": "Multimedia Broadcast Multicast Service\n\nMultimedia Broadcast Multicast Services (MBMS) is a point-to-multipoint interface specification for existing and upcoming 3GPP cellular networks, which is designed to provide efficient delivery of broadcast and multicast services, both within a cell as well as within the core network. For broadcast transmission across multiple cells, it defines transmission via single-frequency network configurations. The specification is referred to as Evolved Multimedia Broadcast Multicast Services (eMBMS) when transmissions are delivered through an LTE (Long Term Evolution) network. eMBMS is also known as LTE Broadcast.\n\nTarget applications include mobile TV and radio broadcasting, live streaming video services, as well as file delivery and emergency alerts.\n\nQuestions remain whether the technology is an optimization tool for the operator or if an operator can generate new revenues with it. Several studies have been published on the domain identifying both cost savings and new revenues.\n\nIn 2013, Verizon announced that it would launch eMBMS services in 2014, over its nationwide (United States) LTE networks. AT&T subsequently announced plans to use the 700 MHz Lower D and E Block licenses it acquired in 2011 from Qualcomm for an LTE Broadcast service.\n\nSeveral major operators worldwide have been lining-up to deploy and test the technology. The frontrunners being Verizon in the United States, Kt and Reliance in Asia, and recently EE and Vodafone in Europe.\n\nIn January 2014, Korea’s Kt launched the first commercial LTE Broadcast service. The solution includes Kt’s internally developed eMBMS Bearer Service, and Samsung mobile devices fitted with the Expway Middleware as the eMBMS User Service.\n\nIn February 2014, Verizon demonstrated the potential of LTE Broadcast during Super Bowl XLVIII, using Samsung Galaxy Note 3s, fitted with Expway's eMBMS User Service.\n\nIn July 2014, Nokia demonstrated the use of LTE Broadcast to replace Traditional Digital TV. This use case remains controversial as some study are doubting about the capability of LTE Broadcast to address this use case efficiently in its current version.\n\nAlso in July 2014, BBC Research & Development and EE demonstrated LTE Broadcast during the XX Commonwealth Games in Glasgow, Scotland using equipment from Huawei and Qualcomm.\n\nIn August 2014, Ericsson and Polkomtel successfully tested LTE Broadcast technology by streaming the opening game of the 2014 World Volleyball Championship to hundreds of guests at Warsaw’s National Stadium in Poland on August 30.\n\nIn June 2015, BBC Research & Development and EE demonstrated LTE Broadcast during the FA Cup final in the U.K.\nIn September 2015, Verizon demonstrated eMBMS by broadcasting INDYCAR races.\n\nIn October 2015, Verizon commercially launched their Go90 eMBMS service. Go90 offers both On-Demand and LiveTV, in both Unicast and Broadcast, and supports more than 10 different LTE Broadcast mobile devices.\n\nIn February 2016, Akamai demonstrated with Expway, delivery of video streams across LTE networks with live on the fly switching from unicast to broadcast, at Mobile World Congress 2016.\n\nIn April 2016, Verizon, Telstra, KT and EE launched the LTE Broadcast Alliance.\n\nDespite many technology trials and demonstrations, as of September 2018 there are no commercial deployments of 3G MBMS or 4G eMBMS anywhere in the world.\n\nMain competing technologies of MBMS include DVB-H/DVB-T, DVB-SH, DMB, ESM-DAB, and MediaFLO. However, due to spectrum scarcity and the cost of building new broadcast infrastructure some of these technologies may not be viable. MediaFLO has been deployed commercially in the US by Verizon Wireless through their relationship with MediaFLO USA, Inc. (a subsidiary of Qualcomm) however the service was shut down in early 2011. DMB and DVB-H trials have been ongoing for more than a year now, like those during the football 2006 championships in Germany.\n\nHuawei's proprietary CMB is a precursor to the Multimedia Broadcast Multicast Service. It was specified in 3GPP R6 and is using existing UMTS infrastructure. Huawei says that CMB is based on existing UMTS infrastructure and real time streaming application protocol.\n\nThe most significant competition is from services that stream individual video feeds to users over uni-cast data connections. While less efficient in certain situations, particularly the traditional case where everyone watches the same stream simultaneously, the user convenience of individual streaming has taken over the vast majority of the mobile media streaming market.\n\nThe MBMS feature is split into the MBMS Bearer Service and the MBMS User Service and has been defined to be offered over both UTRAN (i.e. WCDMA, TD-CDMA and TD-SCDMA) and LTE (where it is often referred to as eMBMS). The MBMS Bearer Service includes a Unicast and a Broadcast Mode. MBMS Operation On-Demand (MOOD) allows dynamic switching between Unicast and Broadcast over LTE, based on configured triggers. The MBMS Bearer Service uses IP multicast addresses for the IP flows. The advantage of the MBMS Bearer Service compared to unicast bearer services (interactive, streaming, etc.) is that the transmission resources in the core and radio networks are shared. One MBMS packet flow is replicated by GGSN, SGSN and RNCs. MBMS may use an advanced counting scheme to decide, whether or not zero, one or more dedicated (i.e. unicast) radio channels lead to a more efficient system usage than one common (i.e. broadcast) radio channel.\n\n\nThe MBMS User Service is basically the MBMS Service Layer and offers two different data Delivery Methods:\n\n\nMBMS has been standardized in various groups of 3GPP (Third Generation Partnership Project), and the first phase standards are found in UMTS release 6. As Release 6 was functionally frozen by the 3rd quarter of 2004, practical network implementations may be expected by the end of 2007, and the first functional mobile terminals supporting MBMS are estimated to be available by also end of 2007.\n\neMBMS has been standardized in various groups of 3GPP as part of LTE release 9. The LTE version of MBMS, referred to as Multicast-broadcast single-frequency network (MBSFN), supports broadcast only services and is based on a Single Frequency Network (SFN) based OFDM waveform and so is functional similar to other broadcast solutions such as DVB-H, -SH and -NGH.\n\nMBMS Bearer Service (Distribution Layer):\n\n\n\nMBMS User Service (Service Layer):\n\n"}
{"id": "54409589", "url": "https://en.wikipedia.org/wiki?curid=54409589", "title": "Nadya Lev", "text": "Nadya Lev\n\nNadya Lev (born November 28, 1982) is a Russian-American photographer, editor, publisher, designer, and entrepreneur.\n\nNadya Lev spent her childhood in the USSR, growing up in a Soviet dissident family.\n\nLev began her photography career at age 21, shooting the 50th anniversary cover for fetish fashion magazine Skin Two. Subsequently, her work was used by Vogue Italia, MTV, Dazed & Confused, Elle, Marie Claire, New York Magazine, Harper's Bazaar, The Museum at FIT and Nick Knight's SHOWStudio in features on alternative fashion. Lev's work has been described as sitting at the crossroads of portraiture and fashion, with her subjects possessing an \"almost surreal combination of hollow power\" and her work emphasizing \"the model as storyteller rather than empty object.\" Seeking to understand photography from her models' perspective, Lev began posing as a subject for photographer colleagues. She has modeled for Clayton Cubitt, Allan Amato and Christopher Voelker.\n\nIn 2011, Lev became the second-youngest person in medical literature to be diagnosed with genetically-inherited PEX glaucoma. She lost a significant portion of her vision and underwent 15 eye surgeries. After adjusting to hallucination-like distortions in vision, Lev re-trained herself in photography and resumed publishing new work.\n\nIn 2013, Lev was appointed as general manager and later CEO of Zivity, an art patronage platform for models and photographers founded by Cyan Banister.\n\nLev is one of the co-founders of Coilhouse Magazine – a print and web art publication established in 2007, together with Meredith Yayanos and Zoetica Ebb. The magazine's tagline was \"a love letter to alternative culture, in an era where alternative culture no longer exists.\" Coilhouse was widely acclaimed for thoughtful coverage of art, music, fashion, film, technology and literature. Coilhouse has also been praised for its high production value and original graphic design by creative director Courtney Riot. Lev's design for the original Coilhouse website, inspired by Merz Magazine and El Lissitzky, won L.A. Weekly's Best-Designed Site Aesthetic Award.\n\nIn 2017, Lev announced that she was working on a Japanese coffee-table art book titled \"Eros & Thanatos\", together with Tokyo-based art magazine editor Masanao Amano.\n\nIn 2016, Lev co-founded the technology company Aconite together with Star St.Germain. Lev describes Aconite as \"a mixed reality storytelling platform.\" Lev and St.Germain define mixed reality stories as immersive experiences that sit at the intersection of adventure, narrative and gaming. The project draws on several gameplay, real-world exploration and storytelling traditions, including transmedia, psychogeography, escape rooms, and live-action roleplay.\n\nAconite's first prototype was \"Limberlost\", a location-based game with a magical realist narrative written by Steen Comer, designed to be played at any bookstore or library in the world. The first playtest took place at Powell's Books during XOXO Fest 2016 in Portland. Players used their mobile phones to solve puzzles which led them through a sequence of physical books. Each book contained an illustration that acted as an augmented reality marker. When triggered, the markers displayed audiovisual data that provided clues for the next book in the sequence, and unlocked new parts of the game's narrative.\n\n"}
{"id": "5892135", "url": "https://en.wikipedia.org/wiki?curid=5892135", "title": "Nallatech", "text": "Nallatech\n\nNallatech is a computer hardware and software firm based in Camarillo, California, United States\nThe company specializes in field-programmable gate array (FPGA) integrated circuit technology applied in computing. As of 2007 the company's primary markets were defense and high-performance computing.\n\nNallatech was acquired by Interconnect Systems, Inc. in 2008,\nwhich in turn was bought by Molex in 2016.\n\nNallatech was promoted for commercial off-the-shelf (COTS) FPGA technology applied in computing. According to David R. Martinez, Robert A. Bond, and M. Michael Vai, Nallatech systems are \"based on a modular design concept in which the designer chooses the number of FPGAs, amount and type of memory, and other expansion cards to include in a system.\" \n\nNallatech is also noted for its motherboards with PCI cards which provide a \"high throughput connection over which a host PC can provide and receive data and monitor system performance.\"\n\nNallatech's latest innovation in FPGA acceleration is the 385A FPGA Accelerator Card. Released as of June 17, 2015, the 385A features the Altera Arria 10 / 1150 FPGA, PCI-Express form factor and works with most major servers, including IBM, HP and Dell.\n\nIn March 2011 the company announced a miniaturization service for their FPGAs.\n\nNallatech has partnered with Altera, and integrated their PCI Express card with Stratix V FPGAs, which may be valuable in financial analysis.\n\nXilinx's Xtreme DSP kit was developed with Nallatech, and like Xilinx, Nallatech uses \"floating-point cores\" in their FPGAs.\n\nThe company was founded by Allan Cantle ('Nalla' comes from 'Allan' spelled backwards) in 1993 and was backed by over £4m of equity finance provided by Scottish Equity Partners and 3i. Cantle was the CEO for the firm, later moving into president and CTO roles.\n\nIn 2005 Nallatech announced a Scottish group known as the FPGA High Performance Computing Alliance, to work on a supercomputer.\n\nNallatech's direct sales team operates in two main geographic areas, one in the USA and one in UK covering UK, Europe and rest of the world. The team in the USA (Nallatech, Inc.) sales office in Eldersburg, Maryland and headquartered in Camarillo, California.\n\nNallatech is a member of the OpenPOWER Foundation.\n\n"}
{"id": "44677927", "url": "https://en.wikipedia.org/wiki?curid=44677927", "title": "Net neutrality law", "text": "Net neutrality law\n\nNet neutrality law refers to laws and regulations which enforce the principle of net neutrality.\n\nOpponents of net neutrality enforcement claim regulation is unnecessary, because broadband service providers have no plans to block content or degrade network performance. Opponents of net neutrality regulation also argue that the best solution to discrimination by broadband providers is to encourage greater competition among such providers, which is currently limited in many areas.\n\nOn 23 April 2014, the United States Federal Communications Commission (FCC) was reported to be considering a new rule that would permit Internet service providers to offer content providers a faster track to send content, thus reversing their earlier position on net neutrality. Municipal broadband could provide a net neutral environment, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On 15 May 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On 10 November 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On 26 February 2015, the FCC ruled in favor of net neutrality by reclassifying broadband access as a telecommunications service and thus applying Title II (common carrier) of the Communications Act of 1934 to internet service providers. On 14 December 2017, the FCC voted to repeal these net neutrality regulations, particularly by reclassifying broadband providers so that they are not considered common carries under Title II of the Communications Act of 1936.\n\nThe concept of network neutrality predates the current Internet-focused debate, existing since the age of the telegraph. In 1860, a U.S. federal law (Pacific Telegraph Act of 1860) was passed to subsidize a telegraph line, stating that:\n\nIn 1888 Almon Brown Strowger, suspecting his loss of business was caused by a nepotistic telephone operator redirecting his business calls to a competitor, invented an electromechanical-based automatic telephone exchange that effectively removed human interference of telephone calls.\n\nChile became the first country in the world to pass net neutrality legislation in 2010. The laws adopted there prohibit organizations such as Facebook and Wikipedia from subsidizing mobile data usage of consumers. The adoption of net neutrality law usually includes allowance for discrimination in limited conditions, such as preventing spam, malware, or illegal content. The law in Chile allows exceptions for ensuring privacy and security. The law in the Netherlands, allows exceptions for congestion, security, spam, or legal reasons.\n\nCardozo Law School professor Susan P. Crawford believes that in a neutral Internet, packets on the network must be forwarded on a first-come, first-served basis, with no consideration given to quality-of-service concerns.\n\nA number of net neutrality interest groups have emerged, including SaveTheInternet.com which frames net neutrality as an absence of discrimination, saying it ensures Internet providers cannot block, speed up, or slow down content on the basis of who owns it, where it came from, or where it's going. It helps create the situation where any site on the Internet could potentially reach an audience as large as that of a TV or radio station, and its loss would mean the end for this level of freedom of expression.\n\nColumbia University Law School professor Tim Wu observed the Internet is not neutral in terms of its impact on applications having different requirements. It is more beneficial for data applications than for applications that require low latency and low jitter, such as voice and real-time video. He explains that looking at the full spectrum of applications, including both those that are sensitive to network latency and those that are not, the IP suite isn't actually neutral. He has proposed regulations on Internet access networks that define net neutrality as equal treatment among similar applications, rather than neutral transmissions regardless of applications. He proposes allowing broadband operators to make reasonable trade-offs between the requirements of different applications, while regulators carefully scrutinize network operator behavior where local networks interconnect. However, it is important to ensure that these trade-offs among different applications be done transparently so that the public will have input on important policy decisions. This is especially important as the broadband operators often provide competing services—e.g., cable TV, telephony—that might differentially benefit when the need to manage applications could be invoked to disadvantage other competitors.\n\nThe proposal of Google and Verizon would allow discrimination based on the type of data, but would prohibit ISPs from targeting individual organizations or websites: Google CEO Eric Schmidt explains Google's definition of Net neutrality as follows: if the data in question is video, for example, then there is no discrimination between one purveyor's data versus that of another. However, discrimination between different types of data is allowed, so that voice data could be given higher priority than video data. Google and Verizon are both agreed on this type of discrimination.\n\nSome opponents of net neutrality argue that under the ISP market competition, paid-prioritization of bandwidth can induce optimal user welfare. Although net neutrality might protect user welfare when the market lacks competition, they argue that a better alternative could be to introduce a neutral \"public option\" to incentivize competition, rather than enforcing existing ISPs to be neutral.\n\nSome ISPs, such as Comcast, oppose blocking or throttling, but have argued that they are allowed to charge websites for faster data delivery. AT&T has made a broad commitment to net neutrality, but has also argued for their right to offer websites paid prioritization and in favor of its current sponsored data agreements.\n\nWhile many countries lack legislation directly addressing net neutrality, net neutrality can sometimes be enforced based on other laws, such as those preventing anti-competitive practices. This is currently the approach of the US FCC, which justifies their enforcement based on compliance with \"commercially reasonable\" practices.\n\nIn the United States, author Andy Kessler argued in \"The Weekly Standard\" that, though network neutrality is desirable, the threat of eminent domain against the telecommunication companies, instead of new legislation, is the best approach.\n\nIn 2011, Aparna Watal of Attomic Labs said that there had been few violations of net neutrality. She argues that transparency, threat of public backlash, and the FCC's current authority was enough to solve the issues of net neutrality, claiming that the threat of consumers switching providers and the high cost of maintaining a non-neutral network will deter bad practices.\n\nThe \"Wall Street Journal\" has written about the government's responsibility being more along the lines of making sure consumers have the ability to find another Internet provider if they are not satisfied with their service, as opposed to determining how Internet providers should go about managing their networks.\n\nGovernments of countries which comment on net neutrality usually support the concept.\n\nGeorge Mason University fellow Adam Thierer has argued that \"any government agency or process big enough to control a major sector of our economy will be prone to influence by those most affected by it\", and that consequently \"for all the talk we hear about how the FCC's move to impose Net Neutrality regulation is about 'putting consumers first' or 'preserving Net freedom and openness,' it's difficult to ignore the small armies of special interests who stand ready to exploit this new regulatory regime the same way they did telecom and broadcast industry regulation during decades past.\"\n\nGrant Babcock, in the libertarian magazine \"Reason\", wrote in 2014 that U.S. government oversight of ISPs could allow government agencies like the NSA to pressure ISPs into handing over private communication data on their users. He noted that there was a history of U.S. governmental abuse of regulation, including the Federal Reserve forcing some banks in 2008 to accept Troubled Asset Relief Program funding by threatening to use their regulatory powers against non-compliant banks.\n\nOne concern of many Internet service providers is government enforcement of information anti-discrimination. Arguing that such enforcement is an infringement on the freedoms of their businesses, American ISPs such as Verizon have argued that the FCC forcing anti-discrimination policies on information flowing over company networks is a violation of the ISPs constitutional rights, specifically concerning the First Amendment and Fifth Amendment in a court case challenging the Open Internet Order.\n\nVerizon challenged the Open Internet Order on several grounds, including that the Commission lacked affirmative statutory authority to promulgate the rules, that its decision to impose the rules was arbitrary and capricious, and that the rules contravened statutory provisions prohibiting the Commission from treating broadband providers as common carriers.\n\nPoorly conceived legislation could make it difficult for Internet Service Providers to legally perform necessary and generally useful packet filtering such as combating denial of service attacks, filtering E-Mail spam, and preventing the spread of computer viruses. Quoting Bram Cohen, the creator of BitTorrent, \"I most definitely do not want the Internet to become like television where there's actual censorship...however it is very difficult to actually create network neutrality laws which don't result in an absurdity like making it so that ISPs can't drop spam or stop...attacks\".\n\nSome pieces of legislation, like The Internet Freedom Preservation Act of 2009, attempt to mitigate these concerns by excluding reasonable network management from regulation.\n\n"}
{"id": "5309066", "url": "https://en.wikipedia.org/wiki?curid=5309066", "title": "Pascal Brandys", "text": "Pascal Brandys\n\nPascal Brandys (born 30 November 1958 in Roanne) is a French engineer and entrepreneur. He is a graduate of the École Polytechnique and received his M.S. in Economic Systems from Stanford University in 1982. He began his career in venture capital first in Tokyo and then in London, where he contributed to the first wave of biotechnology companies in Europe. He was the former president and founder of Genset Corporation, which became the European flagship in the field of genomics and at some point the second largest biotechnology company in Europe. He was also a co-founder and former president of France Biotech, the trade association of biotechnology companies in France. In 2001 he co-founded the biotechnology holding company Biobank in San Diego.\n\nIn 1999, Brandys was awarded Outstanding Service to Biotechnology Award at the Seventh Annual European Life Sciences conference held in Amsterdam.\n"}
{"id": "34516380", "url": "https://en.wikipedia.org/wiki?curid=34516380", "title": "Piezotronics", "text": "Piezotronics\n\nPiezotronics effect is using the piezoelectric potential (piezopotential) created in materials with piezoelectricity as a “gate” voltage to tune/control the charge carrier transport properties for fabricating new devices. Neil A Downie showed how simple it was to build simple demonstrations on a macro-scale using a sandwich of piezoelectric material and carbon piezoresistive material to make an FET-like amplifying device and put it in a book of science projects for students in 2006. The fundamental principle of piezotronics was introduced by Prof. Zhong Lin Wang at Georgia Institute of Technology in 2007.\nA series of electronic devices have been demonstrated based on this effect, including piezopotential gated field-effect transistor, piezopotential gated diode, strain sensors, force/flow sensors, hybrid field-effect transistor, piezotronic logic gates, electromechanical memories, etc. Piezotronic devices are regarded as a new semiconductor-device category. Piezotronics is likely to have important applications in sensor, human-silicon technology interfacing, MEMS, nanorobotics and active flexible electronics.\n\nDue to the non-central symmetry in materials such as the wurtzite structured ZnO, GaN and InN, a piezopotential is created in the crystal by applying a stress. Owing to the simultaneous possession of piezoelectricity and semiconductor properties, the piezopotential created in the crystal has a strong effect on the carrier transport process. Generally, the construction of the basic piezotronic devices can be divided into two categories. Here we use the nanowires as the example. The first kind is that the piezoelectric nanowire was put on a flexible substrate with two ends fixed by the electrodes. In this case, when the substrate is bended, the nanowire will be purely stretched or compressed. Piezopotential will be introduced along its axis. It will modify the electric field or the Schottky barrier (SB) height at the contact area. The induced positive piezopotential at one end will reduce the SB height, while the negative piezopotential at the other end will increase it. Thus the electric transport properties will be changed. The second kind of the piezotronic device is that one end of the nanowire is fixed with electrode, while the other end is free. In this case, when a force is applied at the free end of the nanowire to bend it, the piezopotential distribution will be perpendicular to the axis of the nanowire. The introduced piezoelectric field is perpendicular to electron transport direction, just like applying a gate voltage in the traditional field-effect transistor. Thus the electron transport properties will also be changed. \nThe materials for piezotronics should be piezoelectric semiconductors, such as ZnO, GaN and InN. Three-way coupling among piezoelectricity, photoexcitation and semiconductor is the basis of piezotronics (piezoelectricity-semiconductor coupling), piezophotonics (piezoelectric-photon excitation coupling), optoelectronics, and piezophototronics (piezoelectricity-semiconductor-photoexcitation). The core of these coupling relies on the piezopotential created by the piezoelectric materials.\n\n"}
{"id": "28451859", "url": "https://en.wikipedia.org/wiki?curid=28451859", "title": "Pneumatic web guides", "text": "Pneumatic web guides\n\nA Web is a term used in the Converting Industry that refers to continuous rolls of thin, flat materials like paper, film and plastic. Web guiding systems use a sensor to monitor the position of a web as it enters a production process for lateral tracking. Each type of web guide sensor has an actuator to shift the running web mechanically back on course whenever the sensor detects movement away from the set path. Actuators may be pneumatic or hydraulic cylinders, or some kind of electromechanical device. Because the web may be fragile — particularly at its edge — non-contact sensors are used. \n\nSensors developed for Web Guiding applications in the Converting Industry may be pneumatic, photoelectric, ultrasonic, or infrared. The system’s controls must process the output signals from the sensors in to a form that can drive an actuator. Many controls today are electronic, typically using an amplifier to convert signals from the sensor, then commanding a special servo motor incorporating a lead or ball screw for guiding actuation. Some electromechanical guiding systems also utilize computers. \n\nPneumatic web guide systems are typically easier to install, operate and are less expensive than more complex hydraulic and electronic systems. Pneumatic servo controllers are considered explosion-proof, especially in dusty or contaminated environments.\n"}
{"id": "53581459", "url": "https://en.wikipedia.org/wiki?curid=53581459", "title": "Porzellanikon", "text": "Porzellanikon\n\nThe Porzellanikon is a museum complex dealing with the production of porcelain and ceramics in Selb and Hohenberg an der Eger in the district of Wunsiedel i. Fichtelgebirge (Oberfranken) in Germany. The complex of museums was the result of the merger of the European Industrial Museum for Porcelain, the European Museum of Technical Ceramics, the Rosenthal Museum and the German Porcelain Museum in Hohenberg an der Eger. Since 2012, the Porzellanikon has been included in the European Route of Ceramics as a member of the \"UNIC\" (\"Urban Network for Innovation in Ceramics\").\n\nThe city of Selb is considered the center of the German porcelain industry. The porcelain manufacturers located in Selb and the surrounding area are market leaders in the field of tableware porcelain. The museum complex of Porzellanikon Selb is composed of three museums. The European Industrial Museum of Porcelain shows the manufacture of porcelain. The European Museum of Technical Ceramics deals with the use of ceramics in the fields of medicine, electrical engineering and aerospace. The Rosenthal Museum documents the product and design history of Rosenthal AG, a manufacturer of porcelain and other household products.\n\nThe European Industrial Museum for Porcelain is located on the former factory premises of Rosenthal AG in Selb-Plößberg (Bavaria). Founded in 1866, the factory is now a recognized industrial monument. The museum shows the development of porcelain production in a factory and the conditions from the beginning of the production of ceramics to the present including original workplaces. It is an anchor point of the European Route of Industrial Heritage (ERIH).\n\nThe museum shows how crockery was made from the porcelain raw materials over the last centuries. Screens show the inner life of complex machines and make their functioning understandable. The use of special software enables the experimental design of three-dimensional porcelain objects. Touchscreen applications can be used to answer further questions. The museum also addresses social issues.\n\nThe European Museum of Technical Ceramics was opened on 10 October 2005 as part of Porzellanikon Selb. It is the first museum in Europe dedicated to high-performance ceramics. Numerous manufacturers of technical ceramics and the Association of the Ceramic Industry (Selb) participated in its development.\n\nFor the first time, technical ceramics were used in the form of dentures at the time of the French Revolution. Under Frederick the Great the acid resistance of porcelain was used for laboratory and pharmacy equipment. In 1849 the first ceramic bell insulator was used on the telegraph route between Frankfurt and Berlin. In 1891, porcelain insulators were presented for the first time at the International Electrotechnical Exhibition in Frankfurt. The advancement of the technology brought other applications for technical ceramics.\n\nToday, technical ceramics can also be found in everyday objects such as lighters, mobile phones, espresso machines or cars, since the material is heat-resistant, durable and hard. Ceramic blades cut diamonds, for example.\n\nWith the Rosenthal Museum, Rosenthal AG returned to its old factory site on the occasion of its 125th anniversary. The museum is housed in the old kiln of the former factory. Originally built in 1889, the building burnt down in 1982 to the outer walls. The reconstruction took the idea of an \"open space concept\" into account. Of the original three consecutive round ovens, the front one was completely renovated.\n\nThe museum shows the history of the company and its products in various themed islands, which began on 30 August 1879 at Schloss Erkersreuth with Philipp Rosenthal's porcelain painting. In 1917, Rosenthal acquired the porcelain factory founded by Jacob Zeidler in Selb-Plößberg in 1866. The company used the factories until they moved their production to Rotbühl in 1969.\n\nAs a manufacturer of living and lifestyle products, Rosenthal used an early connection between art, architecture, design, porcelain and other materials. Based on the designs of Rosenthal's product designers such as Walter Gropius, Elsa Fischer-Treyden and Bjørn Wiinblad.  The company was able to win internationally renowned artists to design ceramic wares, including Salvador Dalí, Friedensreich Hundertwasser and Niki de Saint Phalle.\n\nIn 2009, the Rosenthal collection of the insolvent porcelain manufacturer Rosenthal was bought by the State Supervision Foundation, including the originals of Salvador Dalí and Andy Warhol. The foundation left the remains of the manufacturing plant to the Porcelain Museum.\n\nThe German Porcelain Museum was opened in Hohenberg on the Eger in 1982. Previously, the complex had served as the home of the family's company Hutschenreuther founded by Carolus Magnus Hutschenreuther. With the foundation of the first porcelain factory in northeast Bavaria in 1814 Hutschenreuther had laid the foundations in Hohenberg for the region to quickly became the center of the German porcelain industry. In 1995, the historical house was extended by a modern extension of glass and steel to 2000 m². Today, the museum shows some 12,000 exhibits, more than 150,000 parts are in the depot.\n\nThe permanent exhibition shows porcelain products from China as well as all kinds of porcelain produced in the German-speaking world, both precious individual products for noblemen as well as industrial products. The tour is chronological. In each room the visitor is shown features of a Kunstepoche. In front of the buildings is the former factory garden with apple trees and rose leaves.\n\n\n"}
{"id": "13038601", "url": "https://en.wikipedia.org/wiki?curid=13038601", "title": "Predeclared", "text": "Predeclared\n\nIn computer languages, the term pre-declared refers to built-in information, encodings or tools that are available to a programmer, often in the form of entities, variables, objects, functions or instructions. It is mostly not possible to overwrite or otherwise manipulate them.\n\nA pre-declared entity is a built-in notation convention for a character or a string. For example, in the HTML markup language, a large number of character and numeric entities are available to represent characters. In HTML, '&lt;' is a possible pre-declared entity to represent '<'. The programmer must not declare this entity by himself before he can use it, since it is already pre-declared by the specifications of the HTML language. Pre-declared entities are often used as escape sequences to represent information that would otherwise cause possible conflicts in its non-encoded form.\n\nWhen a variable is pre-declared, it provides the programmer with information that he might be interested in. For example, in the Perl language, a variable %ENV is pre-declared, holding all kinds of environmental information such as the operating system, host information, user information, and many more. Other pre-declared variables in Perl are %INC and %SIG. Almost all common programming languages provide the programmer with such pre-declared variables in one or another form.\n\nWhen variables are pre-declared, it is commonly assumed that the value for the pre-declared name is also pre-assigned at the same time.\n\nPre-declared objects have the same goal as pre-declared variables. For example, in the javascript language, the navigator-object is available to get all kinds of information about the browser that is running the script in question.\n\nPre-declared functions or instructions are built-in tools to perform common tasks. For example, in the earliest programming languages the square root needed to be calculated by hand. Nowadays programming languages have a pre-declared instruction or function for this task. Pre-declared functions or instructions often hold common tasks and their goal is to simplify the work of the programmer. The available pre-declared instructions or functions can in some languages be extended by using external libraries or modules.\n\nIn a narrow strictly semantic sense, the term pre-declared may also refer to the declaration of a variable before an assignment takes place. In the following example, the first line is the (pre-)declaration and the second the assignment:\n\nBy declaring the name A, the program creates a namespace for the variable called A. In most modern languages, the variable does not need to be pre-declared on a separate line, as the following instruction achieves exactly the same:\n\nIn early computer languages, the variable always needed to be pre-declared as a separate instruction, because the operating system had to reserve a series of bytes in the available RAM, before the actual value of the variable could be stored in it. Declaration and assignment are still two fundamental different things, though they nowadays mostly appear in a same instruction line.\n"}
{"id": "13921762", "url": "https://en.wikipedia.org/wiki?curid=13921762", "title": "Propulsion and Structural Test Facility", "text": "Propulsion and Structural Test Facility\n\nThe Propulsion and Structural Test Facility is a facility of the George C. Marshall Space Flight Center in Huntsville, Alabama. It was the site where the first single-stage rockets with multiple engines were tested.\n\nThis site was built in 1957 by the Army Ballistic Missile Agency and was the primary center responsible for the development of large vehicles and rocket propulsion systems. The Saturn Family of launch vehicles was developed here under the direction of Wernher von Braun. The Saturn V remains the most powerful launch vehicle ever brought to operational status, from a height, weight and payload standpoint.\n\nIt was declared a National Historic Landmark in 1985.\n\n\n"}
{"id": "44838369", "url": "https://en.wikipedia.org/wiki?curid=44838369", "title": "Radiation Control for Health and Safety Act of 1968", "text": "Radiation Control for Health and Safety Act of 1968\n\nRadiation Control for Health and Safety Act of 1968 was an amendment to the Public Health Service Act mandating performance standards for electronic products suspectible of electromagnetic radiation or radiation emissions. The United States statute established provisions\ninvolving research and development programs for the studies of electromagnetic shielding, ionizing radiation, non-ionizing radiation, and exposure assessment to humans.\n\nThe Act of Congress was recodified to Title 21 from Title 42 with the passage of the Safe Medical Device Amendments of 1990. The electronic product radiation control provisions are authorized for administrative law purposes by the Federal Food, Drug, and Cosmetic Act.\n\nThe H.R. 10790 legislation was passed by the 90th United States Congressional session and enacted into law by the 36th President of the United States Lyndon B. Johnson on October 18, 1968.\n\n"}
{"id": "23450191", "url": "https://en.wikipedia.org/wiki?curid=23450191", "title": "Retrofit Films", "text": "Retrofit Films\n\nRetrofit Films is a production company located in Los Angeles, California, that develops and produces digital media and entertainment.\n\nRetrofit Films was founded by Chris Hanada and Tanner Kling in 2004. Starting in 2006, the company began creating companion Web series for television shows. After graduating from Loyola Marymount University’s film school, Hanada and Kling began their careers working at Tom Cruise and Paula Wagner’s C/W Productions on films such as \"\" and \"Vanilla Sky\", where they contributed to script and story development, research, production and promotion. Hanada and Kling are members of the Producers Guild of America.\n\nRetrofit Films produces new media projects for television networks, movie studios and advertising brands, including animated and live-action web series, DVD featurettes, mobisodes and gameisodes.\n\nThe company created three Web series for NBC's \"Heroes\", entitled \"Going Postal\", \"The Recruit\" and \"Nowhere Man\", featuring characters from the series. The Recruit was nominated for two 2009 Webby Awards. They also created \"Kara & The Chronicles of Krypton\", an animated Web series for The CW Network's's \"Smallville\"; the 4-episode companion Web series \"A Darker Truth\" for The CW's \"Vampire Diaries\"; a Web series for \"Gossip Girl\" entitled \"Gossip Girl: Real New York Stories Revealed\"; and a companion Web series for NBC's \"My Own Worst Enemy\" called \"Conspiracy Theory\". Other projects include a 13-part series of 3-minute \"appisodes\" for the iPhone, iPad and iPod Touch for the INHouse app for Fox's \"House\" in 2010, and 10 on-air/Web Sprint New Media Episodes in 2009 for \"Heroes\" entitled \"Slow Burn\". In 2012, Retrofit worked with Marvel on \"Marvel Mash-Up\", a series of interstitials taking footage from classic Marvel animated series and re-editing them in a humorous way. They originally aired on Disney XD and online. In 2015, Retrofit produced the 6-chapter Web series \"\", a prequel to the television miniseries \"Heroes Reborn\", with all six chapters directed by Kling. In 2016, the company executive produced \"This Isn't Working\", a five-part short-form digital series starring Lisa Schwartz, and in 2017 executive produced \"The Off Season\", a five-part short-form digital series starring Robert Belushi and Erica Rhodes, both for ABC.\n\nThe company's client list includes NBCUniversal, Sprint, Mindshare, Ogilvy & Mather, Edelman, The CW, ABC, Warner Bros., Marvel, DreamWorks and AOL.\n\nIn 2015, Retrofit Films launched a science fiction publishing division, Retrofit Publishing, which was renamed Axiomatic Publishing in 2016. In 2015, Retrofit Publishing put out \"First Fleet\", a collection of serialized novellas by Stephen Case; and \"The Rewind Files\", playwright Claire Willett's debut novel.\n\n\n"}
{"id": "15197395", "url": "https://en.wikipedia.org/wiki?curid=15197395", "title": "Return ratio", "text": "Return ratio\n\nThe return ratio of a dependent source in a linear electrical circuit is the \"negative\" of the ratio of \"the current (voltage) returned to the site of the dependent source\" to \"the current (voltage) of a replacement independent source\". The terms \"loop gain\" and \"return ratio\" are often used interchangeably; however, they are necessarily equivalent only in the case of a single feedback loop system with unilateral blocks.\n\nThe steps for calculating the return ratio of a source are as follows:\n\nThese steps may not be feasible when the dependent sources inside the devices are not directly accessible, for example when using built-in \"black box\" SPICE models or when measuring the return ratio experimentally.\nFor SPICE simulations, one potential workaround is to manually replace non-linear devices by their small-signal equivalent model, with exposed dependent sources. However this will have to be redone if the bias point changes.\n\nA result by Rosenstark shows that return ratio can be calculated by breaking the loop at any unilateral point in the circuit. The problem is now finding how to break the loop without affecting the bias point and altering the results. Middlebrook and Rosenstark have proposed several methods for experimental evaluation of return ratio (loosely referred to by these authors as simply \"loop gain\"), and similar methods have been adapted for use in SPICE by Hurst. See Spectrum user note or Roberts, or Sedra, and especially Tuinenga.\n\nFigure 1 (top right) shows a bipolar amplifier with feedback bias resistor \"R\" driven by a Norton signal source. Figure 2 (left panel) shows the corresponding small-signal circuit obtained by replacing the transistor with its hybrid-pi model. The objective is to find the return ratio of the dependent current source in this amplifier. To reach the objective, the steps outlined above are followed. Figure 2 (center panel) shows the application of these steps up to Step 4, with the dependent source moved to the left of the inserted source of value \"i\", and the leads targeted for cutting marked with an \"x\". Figure 2 (right panel) shows the circuit set up for calculation of the return ratio \"T\", which is\n\nThe return current is\n\nThe feedback current in \"R\" is found by current division to be:\n\nThe base-emitter voltage \"v\" is then, from Ohm's law:\n\nConsequently,\n\nThe overall transresistance gain of this amplifier can be shown to be:\n\nwith \"R = R || r\" and \"R = R || r\".\n\nThis expression can be rewritten in the form used by the asymptotic gain model, which expresses the overall gain of a feedback amplifier in terms of several independent factors that are often more easily derived separately than the overall gain itself, and that often provide insight into the circuit. This form is:\n\nwhere the so-called asymptotic gain \"G\" is the gain at infinite \"g\", namely:\n\nand the so-called feed forward or direct feedthrough \"G\" is the gain for zero \"g\", namely:\n\nFor additional applications of this method, see asymptotic gain model and Blackman's theorem.\n\n"}
{"id": "854560", "url": "https://en.wikipedia.org/wiki?curid=854560", "title": "Route 66 (company)", "text": "Route 66 (company)\n\nROUTE 66 Geographic Information Systems B.V. is a privately held company headquartered in Pfäffikon, Switzerland. ROUTE 66 was founded in 1992 and specializes in navigation software and hardware products.\n\nIn 1999 the company opened a software development center in Brașov, Romania. In 2005 the company moved all its research and development operations to the aforementioned location, which at that date employed 45 of the company's total of 65 employees. In 2008, the company announced 2.4 million euros in revenue. In 2010, the company had 92 employees, of which 65 were software developers. In November 2013, ROUTE 66 launched their latest app Navigate 6. \n\n\n"}
{"id": "2496273", "url": "https://en.wikipedia.org/wiki?curid=2496273", "title": "Soft handover", "text": "Soft handover\n\nSoft handover or soft handoff refers to a feature used by the CDMA and W-CDMA standards, where a cell phone is simultaneously connected to two or more cells (or cell sectors) during a call. If the sectors are from the same physical cell site (a sectorised site), it is referred to as softer handoff. This technique is a form of mobile-assisted handover, for IS-95/CDMA2000 CDMA cell phones continuously make power measurements of a list of neighboring cell sites, and determine whether or not to request or end soft handover with the cell sectors on the list.\n\nDue to the properties of the CDMA signaling scheme, it is possible for a CDMA phone to simultaneously receive signals from two or more radio base stations that are transmitting the same bit stream (using different transmission codes) on the different physical channels in the same frequency bandwidth. If the signal power from two or more radio base stations is nearly the same, the phone receiver can combine the received signals in such a way that the bit stream is decoded much more reliably than if only one base station were transmitting to the subscriber station. If any one of these signals fades significantly, there will be a relatively high probability of having adequate signal strength from one of the other radio base stations.\n\nOn the uplink (phone-to-cell-site), all the cell site sectors that are actively supporting a call in soft handover send the bit stream that they receive back to the Radio Network Controller (RNC), along with information about the quality of the received bits. The RNC examines the quality of all these bit streams and dynamically chooses the bit stream with the highest quality. Again, if the signal degrades rapidly, the chance is still good that a strong signal will be available at one of the other cell sectors that is supporting the call in soft handover.\n\nSoft handover results in a diversity gain called soft handover gain.\n\n\n"}
{"id": "54007814", "url": "https://en.wikipedia.org/wiki?curid=54007814", "title": "Strolling of the Heifers", "text": "Strolling of the Heifers\n\nStrolling of the Heifers is an annual local food parade and festival hosted in Brattleboro, Vermont each year. The organization behind the parade has expanded to support other local food initiatives, most notably a Locavoir Index, which evaluates the availability and policy support for local food in American states. \n\nThe celebration was founded in 2002, with a focus on sustainable agriculture in the region. The parade was inspired by the Running of the Bulls, but instead of enraged bulls, groomed heifers are walked down the main street. \n\nSenator Bernie Sanders attends the parade nearly every year, and has been credited with inspiring the parade by founder Orly Munzing. \n\nStrolling of the Heifers has expanded from just running a parade, to also include small business development programs and local food advocacy programs. For example, they ran a culinary skills program for training the local workforce. The organization also publishes its annual Locavoir Index, which rates the capacity of different states for providing local food. \n\nThe Strolling of the Heifers owns The River Garden, a building in downtown Brattleboro.\n"}
{"id": "5843862", "url": "https://en.wikipedia.org/wiki?curid=5843862", "title": "Subiaco Press", "text": "Subiaco Press\n\nThe Subiaco Press was a printing press located in Subiaco, Italy. The Press was established in 1464 by the German monks Arnold Pannartz and Konrad Sweinheim in the Abbey of Santa Scolastica at Subiaco. It was the first printing press in Italy.\n\nThe first book printed at Subiaco was a Donatus; it has not been preserved. This was followed by Cicero's \"De Oratore\" in September 1465 (which is extant - a copy is in the Buchgewerbehaus at Leipzig). The next book was Lactantius's \"De divinis institutionibus\" printed in October 1465. In 1467, Augustine's \"The City of God\" was printed. These early books are notable for their typography. Unlike earlier German books, they were not printed in blackletter type. Instead, they were printed in a \"half Roman\" type, as in Italy there was a desire to use Roman characters. Furthermore, Lactantius's \"De divinis institutionibus\" contains the world's first Greek printed characters. These were used for the extensive quotations in Greek which employed mobile letters now called \"Subiaco type.\"\n\nIn 1467, Pannartz and Sweinheim left Subiaco and settled in Rome.\n\n"}
{"id": "8247903", "url": "https://en.wikipedia.org/wiki?curid=8247903", "title": "TARGET2", "text": "TARGET2\n\nTARGET2 (Trans-European Automated Real-time Gross Settlement Express Transfer System) is the real-time gross settlement (RTGS) system for the Eurozone, and is available to non-Eurozone countries. It was developed by and is owned by the Eurosystem. TARGET2 is based on an integrated central technical infrastructure, called the Single Shared Platform (SSP). SSP is operated by three providing central banks: France (Banque de France), Germany (Deutsche Bundesbank) and Italy (Banca d'Italia). TARGET2 started to replace TARGET in November 2007.\n\nTARGET2 is also an interbank RTGS payment system for the clearing of cross-border transfers in the eurozone. Participants in the system are either direct or indirect. Direct participants hold an RTGS account and have access to real-time information and control tools. They are responsible for all payments sent from or received on their accounts by themselves or any indirect participants operating through them. Indirect participation means that payment orders are always sent to and received from the system via a direct participant, with only the relevant direct participant having a legal relationship with the Eurosystem. Finally, bank branches and subsidiaries can choose to participate in TARGET2 as multi-addressee access or addressable BICs.\n\nThe objectives of TARGET2 are to:\n\nThe use of TARGET2 is mandatory for the settlement of any euro operations involving the Eurosystem. The Eurosystem consists of the European Central Bank (ECB) and the national central banks of the 19 European Union member states that are part of the Eurozone. Participation in TARGET2 is mandatory for new member states joining the Eurozone.\n\nTARGET2 services in euro are available to non-Eurozone states. National central banks of states which have not yet adopted the euro can also participate in TARGET2 to facilitate the settlement of transactions in euro. Central banks from four non-Eurozone states Bulgaria, Denmark, Poland and Romania also participate in TARGET2.\n\nIn 2012, TARGET2 had 999 direct participants, 3,386 indirect participants and 13,313 correspondents.\n\nTARGET2 is the real-time gross settlement (RTGS) system with payment transactions being settled one by one on a continuous basis in central bank money with immediate finality. There is no upper or lower limit on the value of payments. TARGET2 mainly settles operations of monetary policy and money market operations. TARGET2 has to be used for all payments involving the Eurosystem, as well as for the settlement of operations of all large-value net settlement systems and securities settlement systems handling the euro. TARGET2 is operated on a single technical platform. The business relationships are established between the TARGET2 users and their national central bank. In terms of the value processed, TARGET2 is one of the largest payment systems in the world.\n\nTARGET2 is a harmonised RTGS system covering the Eurozone. It operates on the Single Shared Platform (SSP), which replaced the decentralised first-generation TARGET system. It was designed to provide an enhanced service with benefits for economies of scale which allows it to charge lower fees and offer cost-efficiency. All participants of the Eurosystem, and outside it, can access the same functionalities and interfaces, as well as a single price structure. SWIFT standards and services (i.e. FIN, InterAct, FileAct and Browse) are used in the harmonised communication between the system and its participants.\n\nBefore the introduction of TARGET2, some central banks held \"home accounts\" (also called \"proprietary home accounting systems\") outside their RTGS systems. These were used primarily to manage minimum reserves, standing facilities and cash withdrawals, but also to settle ancillary systems’ transactions.\n\nIt was agreed that, in the context of the new system, these types of transaction should ultimately be settled on the RTGS accounts held on the SSP. However, some countries’ domestic arrangements did not allow these operations to be moved rapidly to the SSP. As a result, the Eurosystem agreed on a maximum transition period of four years for moving the settlement of these payments to the SSP.\n\nThe Information and Control Module (ICM) allows direct users to access information and manage parameters linked to balances and payments online. Via the ICM, users have access to the Payments Module and the Static Data Management function. Users of the ICM are able to choose what information they receive and when. Urgent messages (e.g. system broadcasts from central banks and warnings concerning payments with a debit time indicator) are automatically displayed on the screen.\n\nTARGET2 provides settlement services for a wide range of ancillary systems. While each of these used to have its own settlement procedure, TARGET2 now offers six generic procedures for the settlement of ancillary systems and allows these systems to access any account on the SSP via a standardised interface.\n\nIn 2012, TARGET2:\n\nThe availability and cost of liquidity are two crucial issues for the smooth processing of payments in RTGS systems. In TARGET2, liquidity can be managed very flexibly and is available at low-cost since fully remunerated minimum reserves – which credit institutions are required to hold with their central bank – can be used in full for settlement purposes during the day. The averaging provisions applied to minimum reserves allow banks to be flexible in their end-of-day liquidity management. The overnight lending and deposit facilities also allow for continuous liquidity management decisions. The Eurosystem provides intraday credit. This credit must be fully collateralised and no interest is charged. However, all Eurosystem credit must be fully collateralised, i.e. secured by other assets. The range of eligible collateral is very wide. Assets eligible for monetary policy purposes are also eligible for intraday credit. Under Eurosystem rules, credit can only be granted by the national central bank of the Member State where the participant is established. Banks’ treasury managers have a keen interest in the use of automated processes for the optimisation of payment and liquidity management. They need tools that will allow them to track activity across accounts and, where possible, make accurate intraday and overnight funding decisions from a single location – e.g. their head office. TARGET2 users have, via the Information and Control Module, access to comprehensive online information and easy-to-use liquidity management features that meet their business needs.\n\nTARGET2 has a range of features allowing efficient liquidity management, including payment priorities, timed transactions, liquidity reservation facilities, limits, liquidity pooling and optimisation procedures.\n\nThe access criteria for TARGET2 aim to allow broad levels of participation by institutions involved in clearing and settlement activities. Supervision by a competent authority ensures the soundness of such institutions. Supervised credit institutions established within the European Economic Area are the primary participants. Supervised investment firms, clearing and settlement organisations which are subject to oversight and government treasuries can also be admitted as participants.\n\nThere are two pricing schemes:\n\n\nThe TARGET2 system is closed on Saturdays and Sundays and on the following public holidays in all participating countries: 1 January, Good Friday and Easter Monday (according to the calendar used by Western Christianity), 1 May, 25 December and 26 December.\n\nSince the establishment of the European Economic Community in 1958, there has been a progressive movement towards a more integrated European financial market. This movement has been marked by several events: In the field of payments, the most visible were the launch of the euro in 1999 and the cash changeover in the euro area countries in 2002. The establishment of the large-value central bank payment system TARGET was less visible, but also of great importance. It formed an integral part of the introduction of the euro and facilitated the rapid integration of the euro area money market.\n\nThe implementation of TARGET2 was based on a decision of the ECB Council of autumn 2002. TARGET2 started operations on 19 November 2007, when the first group of countries (Austria, Cyprus, Germany, Latvia, Lithuania, Luxembourg, Malta and Slovenia) migrated to the SSP. This first migration was successful and confirmed the reliability of SSP. After this initial migration, TARGET2 already settled around 50% of overall traffic in terms of volume and 30% in terms of value.\n\nOn 18 February 2008, the second migration successfully migrated to TARGET2, comprising Belgium, Finland, France, Ireland, the Netherlands, Portugal and Spain.\n\nOn 19 May 2008, the final group migrated to TARGET2, comprising Denmark, Estonia, Greece, Italy, Poland and the ECB. The six-month migration process went smoothly and did not cause any operational disruptions.\n\nSlovakia joined TARGET2 on 1 January 2009, Bulgaria joined in February 2010, and Romania joined on 4 July 2011.\n\nThe main subjects of criticism are the unlimited credit facilities made available since the establishment of the TARGET system by the national central banks of the Eurosystem on the one hand and by the ECB on the other.\n\nThe issue of the increasing Target balances was brought to public attention for the first time in early 2011 by Hans-Werner Sinn, president of the Munich Ifo Institute. In an article in \"Wirtschaftswoche\", he drew attention to the enormous increase in Target claims held by Germany's Bundesbank, from €5 billion at the end of 2006 to €326 billion at the end of 2010, and to the attendant liability risk. In the German daily \"Süddeutsche Zeitung\" he put the entire volume of the Target liabilities of Greece, Ireland, Portugal, and Spain at 340 billion euros at the end of February 2011. Moreover, he pointed out that if these countries should exit the Eurozone and declare insolvency, Germany's liability risk would amount to 33% of that sum, or 114 billion euros, relating these sums to the other rescue facilities of euro countries and the International Monetary Fund. Before he made them public, Target deficits or surpluses were not explicitly itemised, being usually buried in obscure positions of central bank balance sheets.\n\nShortly thereafter, Sinn interpreted the Target balances for the first time within the context of current account deficits, international private capital movements and the international shifting of the refinancing credit that the national central banks of the Eurosystem grant to the commercial banks in their jurisdiction. He proved that the ECB system compensated the interruption and reversal in capital flows triggered by the financial crisis by shifting refinancing credit among national central banks. The increase in Target liabilities is a direct measure of net payment orders across borders, i.e. of the portion of the current account deficit that is not counterbalanced by capital imports, or, equivalently, the sum of the current account deficit and net capital exports. Indirectly, they also measure a country's amount of central bank money created and lent out beyond what is needed for domestic circulation. Since every country needs a relatively steady amount of central bank money for its domestic transactions, payment orders to other countries, which reduce the domestic stock of money, must be offset by a continuous issuing of new refinancing credit, i.e., the creation of new central bank money. Similarly, the increase in money balances in the country whose central bank honours the payment orders reduces the demand for fresh refinancing credit. Hence, a country's Target liabilities also indicate the extent to which its central bank has replaced the capital markets to finance its current account deficit, as well as any possible capital flight, by creating new central bank money through corresponding refinancing credit. Sinn illustrated that from an economic perspective, Target credit and formal rescue facilities serve the same purpose and involve similar liability risks. Sinn's presentation on 19 May 2011 at the Munich Economic Summit motivated an op-ed column in the \"Financial Times\". They reconstructed the data on the basis of the balance sheets of the Eurosystem's national central banks and the balance-sheet statistics of the International Monetary Fund.\n\nLater, in June 2011, Hans-Werner Sinn and Timo Wollmershaeuser compiled the first panel database of the Eurozone's Target balances. The authors point out that the additional creation of money by the central banks of the crisis-stricken countries was provided by a lowering of the standards for the collateral that commercial banks have to provide to their national central banks to obtain refinancing credit. Furthermore, they showed that the commercial banks of the Eurozone's core countries used the incoming liquidity to reduce the refinancing credit they drew from their national central bank, even lending the surplus liquidity to this central bank, which implies that the Target balances indirectly also measure the reallocation of refinancing credit among the countries of the Eurozone. The authors showed that the national central banks of the northern countries became net debtors to their own banking systems. Sinn and Wollmershaeuser argue that the euro crisis is a balance-of-payments crisis, which in its substance is similar to the Bretton Woods crisis. Moreover, they show the extent to which Target credit financed current account deficits or capital flight in Greece, Ireland, Portugal, Spain and Italy. They also show that the current account deficits of Greece and Portugal were financed for years by refinancing credits of their national central banks and the concomitant Target credit. They document as well the Irish capital flight and the capital flight from Spain and Italy, which began in earnest in summer 2011. Following Sinn, the authors compare the Target balances of the Eurosystem with the corresponding balances in the US settlement system (Interdistrict Settlement Account) and point out that US balances relative to US GDP have decreased thanks to a regularly performed settlement procedure in which ownership shares in a common Fed clearing portfolio are reallocated among the various District Feds comprising the US Federal Reserve System. They advocate the establishment of a similar system in Europe to end the ECB's role as a provider of international public credit that undercuts private market conditions. Hans-Werner Sinn addressed the Target balances issue again in a special edition of 'ifo Schnelldienst' and made it the main topic of his book ‘Die Target-Falle‘ (\"The Target Trap\"), published in early October 2012.\n\nA number of economists took a stand on the issue of the Target balances in a publication of the Ifo Institute, confirming Sinn's analysis. Financial commentator David Marsh, writing in early 2012, noted that TARGET2 provides \"automatic central bank funding for EMU countries suffering capital outflows provided through it\" and that the balances would \"have to be shared out by central banks throughout the Eurosystem ... if EMU fragments into its constituent parts. So the pressure on Germany is to keep the balances growing, in order to avoid crystallization of losses that would be hugely damaging not just to Berlin but also to central banks and governments in Paris and Rome\".\n\nThe official reactions to Sinn's research findings were mixed. At first, in February and March 2011, the Bundesbank downplayed the Target balances as an irrelevant statistical position. However, in early 2012, Bundesbank chief Jens Weidmann wrote a letter to ECB head Mario Draghi on the subject which \"found its way into the columns of the conservative Frankfurter Allgemeine Zeitung newspaper. It appeared to suggest more secure collateralisation for the overall ECB credits to weaker EMU central banks, which now amount to more than €800 billion under the ECB's TARGET2 electronic payment system,\" Marsh noted in a subsequent column.\n\nJens Ulbrich and Alexander Lipponer (economists at the Bundesbank) justified the policy of the ECB during the European balance-of-payments crisis as follows: In the crisis, the Eurosystem consciously assumed a larger intermediation function in view of the massive disruptions in the interbank market by extending its liquidity control instruments. With this greater role in the provision of central bank money – essentially by changing to a full allotment procedure in refinancing operations and the extension of longer-term refinancing operations – the total volume of refinancing credits provided has increased (temporarily even markedly). At the same time, the quality requirements for the underlying collateral were reduced in the crisis. The higher risk was accepted to maintain the functioning of the financial system under more difficult conditions.\n\nThe Ifo Institute’s regularly updated \"Exposure level indicator\" (‘Haftungspegel‘) shows Germany’s potential financial burden should the crisis-stricken euro countries exit the currency union and declare insolvency. In another development, the Institute of Empirical Economic Research at the University of Osnabrueck collects and publishes Target2 data from all euro countries on the basis of the balance sheets of each central bank.\n\nNevertheless, there are also some economists who contradict some points of Sinn's analysis. Paul De Grauwe and Yuemei Ji argue that Germany's and other countries’ Target claims could be made void, without suffering any losses, since that the value of the central bank money, being \"fiat money\", is independent of a central bank's assets. Sinn, in his rejoinder, showed that the Target balances represent the shift of refinancing credit to the crisis-stricken countries, representing thus the claim on the interest returns from these countries. Eliminating the Target balances would thus entail a real loss of resources amounting to the present value of this interest income, which is reflected exactly by the amount of Target claims. This loss would result in a smaller transfer of Bundesbank's revenues to the German budget and, should the situation arise, in the necessity to recapitalise the Bundesbank through increased taxation. Sinn uses the same reasoning in his book ‘Die Target-Falle‘. Sinn points out that the option of self-rescue for the crisis-affected countries by drawing Target credit forces Germany to approve the formal rescue facilities and eventually to accept Eurobonds as well. He considers the resulting path dependence in policy-making a \"trap\". Analysis of TARGET2 balances countering the Ifo conclusions have been advanced by economist Karl Whelan at University College Dublin. In summer 2012, Thomas A. Lubik, a senior economist and research advisor, and Karl Rhodes, a writer, both at the Federal Reserve Bank of Richmond (Virginia, USA), cited Whelan's work and also drew parallels and distinctions between the US Fed and the ECB in analysing the balances. Lubik and Rhodes argued that \"TARGET2 merely reflects persistent imbalances in current accounts and capital accounts. It does not cause them ... [and does not represent] a 'stealth bailout' of the periphery nations\". Sinn countered that he was misinterpreted in this point insofar as he was just \"saying that the current-account deficits were sustained with the extra refinancing credit behind the TARGET balances\" and this would \"not equate to claiming that current-account deficits and TARGET deficits were positively correlated\".\n\nAlexander L. Wolman believes that rising Interdistrict Settlement Account (ISA) balances - the US-equivalent of rising target balances, if there were no yearly rebalancing - would not be a reason for concern in the US, because the borderlines of the Federal Reserve Districts do not follow national, not even state borders. Further, a rising ISA balance of the Federal Reserve District of New York would be regarded as not surprising, New York being the financial center of the United States. So till 1975 there was no rebalancing between Federal Reserve Districts, a fact which did not lead to major discussions.\n\nFinally, in late 2016, after some years of relative improvement but with rising worries over Italy, the level of TARGET2 intra-eurozone balances at the ECB had surpassed 2012's record levels. The claims represented half of the Germany's net foreign assets and were on track shortly to reach €1 trillion if trends continued unchecked.\n\n\n\"This article incorporates text from the corresponding German Wikipedia articles on and as of 4 April 2008. More text also from the website of the European Central Bank which provides and maintains information on TARGET2 both for the general public as for professional users of TARGET2\"\n"}
{"id": "980240", "url": "https://en.wikipedia.org/wiki?curid=980240", "title": "Tachistoscope", "text": "Tachistoscope\n\nA tachistoscope is a device that displays an image for a specific amount of time. It can be used to increase recognition speed, to show something too fast to be consciously recognized, or to test which elements of an image are memorable. Projection tachistoscopes use a slide or transparency projector equipped with the mechanical shutter system typical of a camera. The slide is loaded, the shutter locked open, and focusing and alignment are adjusted, then the shutter is closed. When ready for the test, a shutter speed is selected, and the shutter is tripped normally. \n\nThe first tachistoscope was originally described by the German physiologist A.W. Volkmann in 1859. Samuel Renshaw used it during World War II in the training of fighter pilots to help them identify aircraft silhouettes as friend or foe.\n\nBefore computers became ubiquitous, tachistoscopes were used extensively in psychological research to present visual stimuli for controlled durations. Some experiments employed pairs of tachistoscopes so that an experimental participant could be given different stimulation in each visual field.\n\nTachistoscopes were used during the late 1960s in public schools as an aid to increased reading comprehension for speed reading. There were two types: the student would look through a lens similar to an aircraft bombsight viewfinder and read letters, words and phrases using manually advanced slide film. The second type projected words and phrases on a screen in sequence. Both types were followed up with comprehension and vocabulary testing.\n\nTachistoscopes continue to be used in market research, where they are typically used to compare the visual impact, or memorability of marketing materials or packaging designs. Tachistoscopes used for this purpose still typically employ slide projectors rather than computer monitors, due to\n\n\n"}
{"id": "25433809", "url": "https://en.wikipedia.org/wiki?curid=25433809", "title": "Tracy Caldwell Dyson", "text": "Tracy Caldwell Dyson\n\nTracy Caldwell Dyson (born Tracy Ellen Caldwell; August 14, 1969) is an American chemist and NASA astronaut. Caldwell Dyson was a Mission Specialist on Space Shuttle \"Endeavour\" flight STS-118 in August 2007. She was part of the Expedition 24 crew on the International Space Station between April 4, 2010 and September 25, 2010. She has completed three spacewalks, logging more than 22 hrs of EVA including work to replace a malfunctioning coolant pump.\n\nAs an undergraduate researcher at the California State University, Fullerton (CSUF), Caldwell Dyson designed, constructed and implemented electronics and hardware associated with a laser-ionization, time-of-flight mass spectrometer for studying atmospherically relevant gas-phase chemistry.\n\nAlso at CSUF, she worked for the Research and Instructional Safety Office as a lab assistant performing environmental monitoring of laboratories using hazardous chemicals and radioactive materials, as well as calibrating survey instruments and helping to process chemical and radioactive waste. During that time (and for many years prior) she also worked as an electrician/inside wireman for her father’s electrical contracting company doing commercial and light industrial type construction.\n\nAt the University of California, Davis, Caldwell Dyson taught general chemistry laboratory and began her graduate research. Her dissertation work focused on investigating molecular-level surface reactivity and kinetics of metal surfaces using electron spectroscopy, laser desorption, and Fourier transform mass spectrometry techniques. She also designed and built peripheral components for a variable temperature, ultra-high vacuum scanning tunneling microscopy system.\n\nIn 1997, Caldwell Dyson received the Camille and Henry Drefus Postdoctoral Fellowship in Environmental Science to study atmospheric chemistry at the University of California, Irvine. There she investigated reactivity and kinetics of atmospherically relevant systems using atmospheric pressure ionization mass spectrometry, Fourier transform infrared and ultraviolet absorption spectroscopies. In addition, she developed methods of chemical ionization for spectral interpretation of trace compounds. Caldwell Dyson has published and presented her work in numerous papers at technical conferences and in scientific journals.\nSelected by NASA in June 1998, Caldwell Dyson reported for training in August 1998. Her Astronaut Candidate Training included orientation briefings and tours, numerous scientific and technical briefings, intensive instruction in Shuttle and International Space Station (ISS) systems, physiological training, ground school to prepare for T-38 flight training, as well as learning water and wilderness survival techniques. Completion of this training and evaluation qualified her for flight assignment as a mission specialist.\n\nIn 1999, Caldwell Dyson was assigned to the Astronaut Office ISS Operations Branch as a Russian Crusader, participating in the testing and integration of Russian hardware and software products developed for ISS. In 2000, she was assigned prime Crew Support Astronaut for the ISS Expedition 5 crew, serving as their representative on technical and operational issues throughout the training and on-orbit phase of their mission.\n\nDuring ISS Expeditions 4 through 6, Caldwell Dyson also served as an ISS spacecraft communicator (CAPCOM) inside Mission Control. In 2003, she made a transition to the Astronaut Shuttle Operations Branch and was assigned to flight software verification in the Shuttle Avionics Integration Laboratory (SAIL) and also worked supporting launch and landing operations at Kennedy Space Center, Florida. Caldwell Dyson also served as Lead CAPCOM for Expedition 11.\n\nCaldwell Dyson was assigned to, and later flew on STS-118, Space Shuttle \"Endeavour\", on August 8–21, 2007, which was the 119th space shuttle flight, the 22nd flight to the station, and the 20th flight for \"Endeavour\". Caldwell Dyson was assigned as Mission Specialist #1 on this flight. During the mission Endeavour's crew successfully added another truss segment, a new gyroscope and external spare parts platform to the International Space Station. A new system that enables docked shuttles to draw electrical power from the station to extend visits to the outpost was activated successfully. A total of four spacewalks (EVAs) were performed by three crew members. \"Endeavour\" carried some 5,000 pounds of equipment and supplies to the station and returned to Earth with some 4,000 pounds of hardware and no longer needed equipment. Traveling 5.3 million miles in space, the STS-118 mission was completed in 12 days, 17 hours, 55 minutes and 34 seconds. Finally, during the flight of STS-118, Caldwell Dyson celebrated her 38th birthday in space.\n\nOn April 4, 2010, Caldwell Dyson joined the Expedition 23 crew aboard ISS. She lifted off on April 2, 2010 from the Baikonur spaceport aboard a Russian Soyuz capsule (Soyuz TMA-18). After 176 days duty as part of the Expedition 24 crew, she returned to Earth with the Soyuz TMA-18 landing unit. Together with commander Aleksandr Skvortsov and flight engineer Mikhail Korniyenko, Dyson landed in Kazakhstan on September 25, 2010.\n\nIn a television interview on the 40th anniversary of the first moon landing, she said she is the first astronaut who was born after Apollo 11.\n\nAs Tracy Dyson, she is the host of a series on NASA TV called \"StationLife\", which focuses on facets of life aboard the International Space Station.\n\nShe appeared on Episode 3 of \"MasterChef Junior\" Season 4.\n\nOn March 21, 2017, Dyson stood behind President Trump as he signed a bill for NASA to send humans to Mars in 2030s and receive $19.5 billion in 2018 funding. Dyson and fellow NASA astronaut Chris Cassidy presented Trump with an official flight jacket during the ceremony.\n\nCaldwell Dyson, the younger of two girls, was born in Arcadia, California, and later moved to Beaumont, California, in the early 1980s to attend junior high school where her father worked as an electrician. Her recreational interests include running, weight training, hiking, softball, basketball, and auto repair/maintenance. As an undergraduate, she competed in intercollegiate athletics on the CSUF Titans track team as both a sprinter and long jumper.\n\nCaldwell Dyson is a private pilot and conversational in American Sign Language (ASL) and Russian.\n\nShe is married to Naval Aviator George Dyson. She is also the lead vocalist for the all-astronaut band Max Q.\n\nIn 2011, Caldwell Dyson served as the guest judge on a space-themed episode of the Food Network show \"Cupcake Wars\".\n\nCaldwell Dyson advised Jessica Chastain when the actress was preparing to appear as an astronaut and mission commander in the 2015 movie \"The Martian.\" Chastain said she was very inspired by Caldwell Dyson.\n\nShe believes in God, and was raised Methodist, but no specific claimed religious affiliation of hers was ever publicly available as an adult.\n\n\nCaldwell Dyson belongs to the Sigma Xi Research Society and the American Chemical Society.\n\n\n\n"}
{"id": "20433075", "url": "https://en.wikipedia.org/wiki?curid=20433075", "title": "Unique Device Identification", "text": "Unique Device Identification\n\nThe Unique Device Identification (UDI) System is intended to assign a unique identifier to medical devices within the United States. It was signed into law on September 27, 2007, as part of the Food and Drug Administration Amendments Act of 2007. This act includes language related to the establishment of a Unique Device Identification System. When implemented, the new system will require:\n\n\nA national UDI system will create a common vocabulary for reporting and enhance electronic tracking abilities. Currently, analysis of adverse event reports is limited by the fact that the specific devices involved in an incident are often not known with the required degree of specificity. Without a common vocabulary for medical devices, meaningful analysis based on data from existing voluntary systems is problematic. Reliable and consistent identification of medical devices would enable safety surveillance so that the FDA and manufacturers could better identify potential problems or device defects, and improve patient care.\n\nThe UDI is expected to improve patient safety (in part by helping to identify counterfeit products and by improving the ability of staff to distinguish between devices that are similar in appearance but serve different functions), facilitate and improve the recall process, and create efficiences within the medical system.\n\nIn the most basic format, the UDI would be a coded number registered with standards organizations, and would incorporate a variety of information, including (but not limited to) the manufacturer of the device, expiry dates, the make and model of the device, and any special attributes that the device may possess. In a medical sense, \"device\" refers to any product that is not pharmaceutical in nature, and while the FDA have been given approval to exempt some devices, Jay Crowely (who was responsible for implementing the UDI requirements in the Act), has expressed an intent to apply the UDI to \"everything until somebody gives us good reason not to\", (excluding devices which won't need identification).\n\nFollowing the passing of the Act, there were calls for the FDA to publish a timeline for the implementation of the UDI; this was subsequently done.\n\nGUDID Submission\nThe Final Rule on Unique Device Identifiers also mandates medical device manufacturers to make a submission to the FDA's Global Unique Device Identification Database. The submission to the GUDID will include the Primary Device Identifier portion of the UDI as well as associated data attributes about each model or version number of the device.\n\nCompliance with the submission component of UDI compliance is phased according to the Class of device. Class III device labelers must submit to the GUDID for all existing products by September 24, 2014. Labelers of Implantable, Life Supporting or Life Sustaining devices must submit to the GUDID by September 24, 2015. Class II labelers must comply with submission guidelines by September 24, 2016, and Class I labelers by September 24, 2018.\n\nSubmission to the GUDID may be made in one of two methods. The first method utilizes the FDA's GUDID Web Interface, which is meant for low volumes of GUDID submissions. The second method utilizes an HL7 SPL submission and is transmitted to the FDA through an Electronic Submission Gateway account.\n\nOnce the initial submission is made, labelers of medical devices must update the GUDID submission each time there is a change to the device attributes. While many device attributes may be updated once the device record is published within the GUDID, several attributes cannot be changed and will necessitate the labeler deactivate the existing submission and resubmit using a new Device Identifier.\n\n"}
{"id": "19430754", "url": "https://en.wikipedia.org/wiki?curid=19430754", "title": "Williams v. Pryor", "text": "Williams v. Pryor\n\nWilliams v. Pryor, 229 F.3d 1331 (11th Cir. 2000), rehearing denied, 240 F.3d 944 (11th Cir. 2001) was a federal lawsuit that unsuccessfully challenged an Alabama law criminalizing the sale of sex toys in the state. In 1998, a statute enacted by the legislature of the State of Alabama amended the obscenity provisions of the Alabama Code to make the distribution of certain defined sexual devices a criminal offense. Vendors and users of such devices filed a constitutional challenge to the statute in the United States District Court for the Northern District of Alabama against William H. Pryor, Jr., in his official capacity as the Attorney General of the State of Alabama. The district court declined to hold the statute violated any constitutional right but determined the statute was unconstitutional because it lacked a rational basis. The State appealed to the Eleventh Circuit Court of Appeals, which reversed the lower court ruling on October 12, 2000.\n\nAfter the 1998 amendment, the Alabama Code obscenity provisions provide the following:\n\"It shall be unlawful for any person to knowingly distribute, possess with intent to distribute, or offer or agree to distribute any obscene material or any device designed or marketed as useful primarily for the stimulation of human genital organs for any thing of pecuniary value.\"\n\nA first violation is a misdemeanor punishable by a maximum fine of $10,000 and up to one year of jail or hard labor; a subsequent violation is a class C felony. The State conceded the statute's proscription of the distribution of sexual devices in Alabama does not apply to devices acquired as gifts or by purchases in another state. The statute also does not restrict possession or use of a sexual device by an individual, but only the commercial distribution of the devices. \nAfter considering Supreme Court precedent, the District Court determined the statute does not implicate previously recognized fundamental constitutional rights. The court also declined to extend those rights to provide a fundamental right to the use of sexual devices The district court next reviewed the statute under rational basis scrutiny and concluded the statute lacked a rational basis. The court accordingly held the statute unconstitutional and issued a permanent injunction against its enforcement.\n\nThe Circuit Court explained that whether a statute is constitutional is determined in large part by the level of scrutiny applied by the courts. Statutes that infringe fundamental rights, or that make distinctions based upon suspect classifications such as race or national origin, are subject to strict scrutiny, which requires that the statute be narrowly tailored to achieve a compelling government interest. Most statutes reviewed under the very stringent strict scrutiny standard are found to be unconstitutional. Quoting \"Romer v. Evans\", the Circuit Court stated that \"if a law neither burdens a fundamental right nor targets a suspect class, we will uphold the [law] so long as it bears a rational relation to some legitimate end.\" According to the Court, almost every statute subject to the very deferential rational basis scrutiny standard is found to be constitutional.\n\nThe Circuit Court concluded that the district court erred in determining the Alabama statute lacks a rational basis. It found that the State's interest in public morality is a legitimate interest rationally served by the statute, and that the crafting and safeguarding of public morality has long been an established part of the States' plenary police power to legislate and indisputably is a legitimate government interest under rational basis scrutiny.\n\nThe Court rejected plaintiffs argument that the statute is constitutionally irrational because it is contrary to a wide spectrum of public and professional opinions which recognize numerous legitimate and beneficial uses of sexual devices, especially the necessity of sexual devices for some persons to achieve medical or emotional health. The Court said that however misguided the legislature of Alabama may have been in enacting the statute challenged in this case, the statute is not constitutionally irrational under rational basis scrutiny because it is rationally related to the State's legitimate power to protect its view of public morality. \"The Constitution presumes that ... improvident decisions will eventually be rectified by the democratic process and that judicial intervention is generally unwarranted no matter how unwisely we may think a political branch has acted.\"\n\nThe Court remanded the as-applied challenges for consideration by the district court because the record and stipulations were too narrow to permit the Court to decide whether or to what extent the Alabama statute infringes a fundamental right to sexual privacy of the specific plaintiffs in this case.\n\nIn December 2003 the Supreme court ruled in \"Lawrence v. Texas\" which overturned the previous decision of \"Bowers v. Hardwick\" In light of this the ACLU again challenged the decision before the 11th Circuit in February 2007. The 11th circuit ruled that even though \"Bowers v. Hardwick\" had been overruled that in doing so the Supreme Court had\n\n\"declined the invitation\" to recognize a fundamental right to sexual privacy, which would have compelled us to employ strict scrutiny in assessing the constitutionality of the challenged statute ... Thus, because there is no fundamental right at issue, we apply rational basis scrutiny to the challenged statute.\n<br><br> ... Accordingly, we find that public morality survives as a rational basis for legislation even after \"Lawrence\", and we find that in this case the State's interest in the preservation of public morality remains a rational basis for the challenged statute. By upholding the statute, we do not endorse the judgment of the Alabama legislature.\n\nIn \"Reliable Consultants Inc. v. Earle,\" on February 12, 2008, the 5th Circuit overturned the Texas ban on the sale of sex toys using the similar facts that failed in the above appeal. It held:\n\nJust as in \"Lawrence\", the State here wants to use its laws to enforce a public moral code by restricting private intimate conduct. The case is not about public sex. It is not about controlling commerce in sex. It is about controlling what people do in the privacy of their own homes because the State is morally opposed to a certain type of consensual private intimate conduct. This is an insufficient justification for the statute after \"Lawrence\".\n\nIn \"This That and the Other Gift & Tobacco, Inc. v. Cobb County, Ga.,\" the 11th Circuit struck down on First Amendment grounds a Georgia law that banned advertising obscene material, including sex toys.\n\n"}
