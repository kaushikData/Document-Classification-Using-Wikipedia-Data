{"id": "5300021", "url": "https://en.wikipedia.org/wiki?curid=5300021", "title": "Adaptive feedback cancellation", "text": "Adaptive feedback cancellation\n\nAdaptive feedback cancellation is a common method of cancelling audio feedback in a variety\nof electro-acoustic systems such as digital hearing aids. The time varying acoustic feedback leakage paths can only be eliminated with adaptive feedback cancellation. When an electro-acoustic system with an adaptive feedback canceller is presented with a correlated input signal, a recurrent distortion artifact, entrainment is generated. There is a difference between the system identification and feedback cancellation. \n\nAdaptive feedback cancellation has its application in echo cancellation. The error between the desired and the actual output is taken and given as feedback to the adaptive processor for adjusting its coefficients to minimize the error.\n\nIn hearing aids, feedback arises when a part of the receiver (loudspeaker) signal is captured by the hearing aid microphone(s), gets amplified in the device and starts to loop around through the system. When feedback occurs, it results in a disturbingly loud tonal signal. Feedback is more likely to occur when the hearing aid volume is increased, when the hearing aid fitting is not in its proper position or when the hearing aid is brought close to a reflecting surface (e.g. when using a mobile phone). Adaptive feedback cancellation algorithms are techniques that estimate the transmission path between loudspeaker and microphone(s). This estimate is then used to implement a neutralizing electronic feedback path that suppresses the tonal feedback signal.\n\nDiagram\n"}
{"id": "23204993", "url": "https://en.wikipedia.org/wiki?curid=23204993", "title": "Air data module", "text": "Air data module\n\nAn air data module is a component of the navigation system. Each unit converts pneumatic (air pressure) information from a Pitot tube or a static port into numerical information which is sent on a data bus. This pressure information is received and processed by the Air Data Reference (ADR) component of the Air Data Inertial Reference Unit (ADIRU). This processed information is then sent to a display management computer(s) that present information on the cockpit's primary flight display. Airspeed information is also sent to the flight computers and other electronics. Airspeed information is also sent to the autoflight subsystem (.e.g. flight management and guidance system).\n\nThe air data module is a gas pressure sensor which converts mechanical forces created by gas pressure into digital signals that can be carried to the air data reference unit. ADM generally have a maintenance bus and communication bus, and a connector on the housing for a pressurized gas line that is connected to the Pitot tube or static ports. The maintenance bus can be EIA-485 and the communication bus can be ARINC 429\n"}
{"id": "1114065", "url": "https://en.wikipedia.org/wiki?curid=1114065", "title": "Aortic valve replacement", "text": "Aortic valve replacement\n\nAortic valve replacement is a procedure in which a patient's failing aortic valve is replaced with an artificial heart valve. The aortic valve can be affected by a range of diseases; the valve can either become leaky (aortic insufficiency) or partially blocked (aortic stenosis). Current aortic valve replacement approaches include open heart surgery via a sternotomy, minimally invasive cardiac surgery (MICS) and transcatheter aortic valve replacement (TAVR).\n\nAs risk of aortic valve surgery has decreased and long term data on the survival and quality of life of people after valve replacement has become available, evidence-based guidelines for aortic valve replacement have been developed. The American Heart Association and American College of Cardiology Guidelines for the Management of Patients with Valvular Heart Disease are a widely accepted source of information for cardiologists and surgeons.\n\nPatients with severe aortic stenosis, where the aortic valve is narrowed and blood flow from the heart is obstructed are candidates for surgery when they develop symptoms or when the heart function is impacted. Certain asymptomatic patients may also be candidates for surgery, especially if exercise stress testing is positive.\n\nPatients with leaky aortic valves (aortic insufficiency) often tolerate even severe degrees of insufficiency for a relatively long time before symptoms develop. Surgery is indicated for symptoms such as shortness of breath, and in cases where the heart has begun to enlarge (dilate) from pumping the increased volume of blood that leaks back through the valve.\n\nThere are two basic types of artificial heart valve: mechanical valves and tissue valves.\n\nTissue heart valves are usually made from animal tissue, either animal heart valve tissue or animal pericardial tissue. The tissue is treated to prevent rejection and calcification.\n\nThere are alternatives to animal tissue valves. In some cases a homograft - a human aortic valve—can be implanted. Homograft valves are donated by patients and recovered after the patient dies. The durability of homograft valves is comparable to porcine and bovine tissue valves. Another procedure for aortic valve replacement is the Ross procedure (or pulmonary autograft). In a Ross procedure, the aortic valve is removed and replaced with the patient's own pulmonary valve. A pulmonary homograft (pulmonary valve taken from a cadaver) is then used to replace the patient's own pulmonary valve. This procedure was first used in 1967 and is used primarily in children, as the procedure allows the patient's own pulmonary valve (now in the aortic position) to grow with the child.\n\nMechanical valves are designed to outlast the patient, and have typically been stress-tested to last several hundred years. Although mechanical valves are long-lasting and generally present a one-surgery solution, there is an increased risk of blood clots forming with mechanical valves. As a result, mechanical valve recipients must take anticoagulant (blood thinning) drugs such as warfarin for the rest of their lives, making the patient more prone to bleeding. The sound of mechanical valves may be heard and decrease the quality of life.\n\nTissue valves tend to wear out faster with increased flow demands - such as with a more active (typically younger) person. Tissue valves are increasing lasting longer - now typically approximately 20 years, but they might wear out faster in younger people.\nWhen a tissue valve wears out and needs replacement, the person must undergo another valve replacement surgery. For this reason, younger patients often receive mechanical valves to prevent the increased risk (and inconvenience) of another valve replacement. \n\nAortic valve replacement is most frequently done through a median sternotomy, meaning the incision is made by cutting through the sternum. Once the pericardium has been opened, the patient is put on a cardiopulmonary bypass machine, also known as the heart-lung machine. This machine takes over the task of breathing for the patient and pumping their blood around while the surgeon replaces the heart valve.\n\nOnce the patient is on bypass, a cut is made in the aorta and a crossclamp applied. The surgeon then removes the patient's diseased aortic valve and a mechanical or tissue valve is put in its place. Once the valve is in place and the aorta has been closed, the patient is taken off the heart-lung machine. Transesophageal echocardiogram (TEE, an ultra-sound of the heart done through the esophagus) can be used to verify that the new valve is functioning properly. Pacing wires are usually put in place, so that the heart can be manually paced should any complications arise after surgery. Drainage tubes are also inserted to drain fluids from the chest and pericardium following surgery. These are usually removed within 36 hours while the pacing wires are generally left in place until right before the patient is discharged from the hospital.\n\nAfter aortic valve replacement, the patient will frequently stay in an intensive care unit for 12–36 hours. The patient is often able to go home after this, in about four days, unless complications arise. Common complications include heart block, which typically requires the permanent insertion of a cardiac pacemaker.\n\nRecovery from aortic valve replacement will take about three months, if the patient is in good health. Patients are advised not to do any heavy lifting for 4–6 months after surgery, to avoid damage to the sternum (the breast bone).\n\nThe risk of death or serious complications from aortic valve replacement is typically quoted as being between 1-3%, depending on the health and age of the patient. Older patients, as well as those who are frail and/or have multiple comorbidities (i.e. other health problems), may face significantly higher surgical risk.\n\nMore recently, some cardiac surgeons have been performing aortic valve replacement procedures using an approach referred to as minimally invasive cardiac surgery (MICS), in which the surgeon replaces the valve through small incisions between two and four inches in length using specialized surgical instruments rather than by cutting a six to ten-inch incision down the center of the sternum. MICS typically involves shorter recovery time and more attractive cosmetic results.\n\nAnother promising alternative for many high risk and older patients is transcatheter aortic valve replacement (TAVR), which delivers a new valve to the site of the diseased valve through a catheter. The replacement valve is collapsed and packaged in a way similar to a stent. Once in place it is expanded, pushing the old valve’s leaflets out of the way, and functions in place of the old valve. The catheter may be inserted through the femoral artery or through a small incision in the chest and then through a large artery or the tip of the left ventricle.\n\nGuidelines suggest TAVR for most patients over 75 and surgical replacement for most patients less than 75. Ultimately, the best treatment choice is a decision based on many individual factors.\n\nEarly surgical approaches to aortic valve disease were limited by the necessity of operating with the heart beating. In the 1950s the Hufnagel valve was implanted in the descending thoracic aorta in patients with aortic insufficiency. The first successful replacement of the aortic valve was reported in 1960 by Harken, and early adoption of this technique proceeded slowly based on the limitations of available replacement valves and relatively primitive techniques for protecting the heart during surgery which were available at the time. With the evolution of mechanical heart valves and gradual developments in cardiopulmonary bypass (the heart lung machine) and cardioplegia which allow the heart to be stopped safely during surgery, aortic valve replacement became accepted therapy for patients with severe aortic insufficiency or regurgitation.\n\n\n"}
{"id": "10874917", "url": "https://en.wikipedia.org/wiki?curid=10874917", "title": "Avid DS", "text": "Avid DS\n\nAvid DS (which was called Avid DS Nitris until early 2008) is a high-end offline and finishing system comprising a non-linear editing system and visual effects software. It was developed by Softimage in Montreal. Softimage was owned by Microsoft at the time of the launch of DS v1.0 and the company was acquired by Avid Technology Inc shortly thereafter.\n\nDS was discontinued on September 30, 2013 with support ending on September 30, 2014.\n\nDS was called ‘Digital Studio’ in development. It was envisioned to be a complete platform for video/audio work. The first previews of the system were on the SGI platform, but this version was never released. The system was rewritten on Windows NT with different video hardware platforms (Matrox DigiSuite or Play Trinity running on a NetPower system) before the final system was released on Intergraph/StudioZ hardware in January 1998.\n\nAfter its acquisition by Avid, DS was always positioned as a high end video finishing tool. However, many users found it to be uniquely soup-to-nuts in its capabilities. From version 1.0 of the product, it competed with products like Autodesk Smoke, Quantel and Avid Symphony. The toolset in DS offered video timeline editing, an object-oriented vector-based paint tool, 2D layer compositing, sample based audio and starting with version 3.01 of the product, a 3D environment. Originally, a subset of the SoftimageXSI 3D software was planned to become part of the DS toolset, both were built on the same software foundation, but over time the code bases divided between the applications and the integration never happened.\n\nWhile the first version of the DS still lacked a few key features (no 3D, poor keying, no real-time effects), it had some significant features compared to the competing products at the time. It offered a large number of built in effects. Avid OMF import was available, positioning Softimage DS as a strong finishing tool for then typical off-line Avid systems. Lastly the integration of the toolset of Softimage DS was beyond what other product offered. A Softimage DS user could quickly go from editing, to paint, to compositing with a few mouse clicks all inside the same interface. Some of the lacking features where quickly resolved, within months of version 1.0 a new chroma keyer was released.\n\nEarly versions of the software (up thru 4.0) added additional key features. Development continued with one of the first uncompressed HD editing systems (version 4.01) and an attempt to make the system more friendly to Media Composer editors in version 6. In later versions (v7.5 on beyond) DS was criticized for slow development of compositing tools, mainly lack of a new 3D environment and better tracking tools. Many DS users felt that Avid had not been giving DS the attention that it deserved.\n\nOn July 7, 2013 Avid sent out an email marking the end of life of the DS product.\n\n\"To Our Avid DS customers, We are writing to inform you that Avid will be realigning our business strategy to focus on a core suite of products to best leverage our developmental and creative resources. As part of this transition, we will be ceasing future development of Avid DS with a final sale date of September 30th, 2013\"\n\nUp until version 10.5, DS was sold as a turn-key system; the software was not available without purchasing CPU, I/O and storage hardware from Avid. Beginning with 10.5, customers were able to configure their own systems using widely available components, based on recommended system requirements. In turn-key systems, there were many hardware refreshes over time.\n\nStudioZ single stream: Intergraph TDZ-425 with 30 minutes of uncompressed SCSI storage. CPUs at the time were Pentium II/300 MHz.\n\nStudioZ dual stream: Intergraph TDZ-2000 GT1 with one hour of fibre channel storage. CPUs on first systems were Pentium II/400 MHz, but last shipping systems had Pentium III/1 GHz. DS was one of the first applications to show that real-time effects could be processed with just the CPUs of the system, not requiring special video cards with real-time effect hardware.\n\nEquinox: Developed by Avid, it was one of the first uncompressed HD video cards available. Systems were available on CPUs from Pentium III/1 GHz to Pentium 4/2.8 GHz. Storage was typically SCSI, but fibre channel was also supported.\n\nNitris DNA: Developed by Avid, the Nitris hardware was probably the largest hardware update to the system since it was released. 10-bit HD and SD support was standard. Real-time down and cross convert. This was the only hardware for DS that had on-board effect processing. This allowed a system at the time to play back dual-stream uncompressed HD effects in real-time at 16-bit precision. This was also the first hardware from Avid to support the DNxHD codec. Starting with Pentium 4, Intel Core Xeons were supported. SCSI storage was primarily used.\n\nAJA Video Systems: First available as a 4:4:4 option to be used in conjunction with Nitris hardware. Final-generation DS systems used the AJA Video Systems Kona 3 (Xena 2K) card as the only I/O for the system. The last systems shipped with two Intel Core Xeon 6-core processors. SAS is the recommended storage for these systems.\n\n"}
{"id": "1552303", "url": "https://en.wikipedia.org/wiki?curid=1552303", "title": "Blaw-Knox tower", "text": "Blaw-Knox tower\n\nThe Blaw-Knox company was a manufacturer of steel structures and construction equipment based in Pittsburgh, Pennsylvania. The company is today best known for its radio towers, most of which were constructed during the 1930s in the United States. Although Blaw-Knox built many kinds of towers, the term Blaw-Knox tower (or radiator) usually refers to the company's unusual \"diamond cantilever\" design, which is stabilized by guy wires attached only at the vertical center of the mast, where its cross-section is widest. A 1942 advertisement claims that 70% of all radio towers in the US at the time were built by Blaw-Knox.\n\nThe distinctive diamond-shaped towers became an icon of early radio. Several are listed on the US National Register of Historic Places, the distinctive diamond antenna design has been incorporated into logos of various organizations related to radio and a very large (scale) replica of the WSM (AM) Blaw-Knox tower has been built into the Country Music Hall of Fame and Museum.\n\nThe diamond-shaped tower was patented by Nicholas Gerten and Ralph Jenner for Blaw-Knox July 29, 1930. and was one of the first mast radiators. Previous antennas for medium and long wave broadcasting usually consisted of wires strung between masts, but in the Blaw-Knox antenna, as in modern AM broadcasting mast radiators, the metal mast structure itself functioned as the antenna. To prevent the high frequency potential on the mast from short-circuiting to ground, the narrow lower end of the tower rested on a ceramic insulator about 3 feet wide, shaped like a ball and socket joint. Thus the tower required guy lines to hold it upright.\n\nThe distinguishing feature of the Blaw-Knox tower was its wide diamond (or rhomboidal) shape, which served to make it rigid, to resist shear stresses. One advantage of this was to reduce the number of guy wires needed. Blaw-Knox masts required only one set of 3 or 4 guy cables, attached at the tower's wide \"waist\". In contrast, narrow masts require 2 to 4 sets of guy cables, attached at different heights, to prevent the tower from buckling. The advantage of fewer guy cables was to simplify the electrical design of the antenna, because conductive guy cables interfered with its radiation pattern. If not insulated at their bottom end, guy wires acted as additional \"antennas\" and conducted the power radiated by the antenna to ground, wasting power. Even when insulated from ground, the sections of guy cable could act as \"parasitic\" resonant elements, reradiating the radio waves in other directions and thus altering the antenna's radiation pattern. In some Blaw-Knox mast designs \"(see WBT towers, right)\" the upper pyramidal section was made longer than the lower, to keep the attachment point of the guys as low as possible, to minimize their interference.\n\nAnother advantage mentioned in the patent was that the tower could be erected in two parts. Half of the mast could be built, then its wide central section could be used as a stable base on which to erect the other half.\n\nMany Blaw-Knox towers, of both conventional (uniform cross-section) and diamond design, remain in use in the United States. Few of the diamond towers were built, and several remain; all transmit AM radio signals:\n\nSeveral additional diamond-cantilever towers were built at stations in the Central Valley of California but are less well known. These towers were much smaller in both height and cross-section than the towers listed elsewhere; only one — KSTN, Stockton — remains in use for broadcasting.\n\nThe following Blaw-Knox diamond-cantilever towers remain standing in Europe:\n\nThe 480-foot self-supporting tower (four legs up to 400 ft., topped with an 80-foot pole), of WKQI (FM) Detroit is a Blaw-Knox tower. It began transmitting in 1949, when the station was WLDM. It is located on Ten Mile Rd. in the suburb of Oak Park.\n\n\n"}
{"id": "25160837", "url": "https://en.wikipedia.org/wiki?curid=25160837", "title": "Climatic Research Unit email controversy", "text": "Climatic Research Unit email controversy\n\nThe Climatic Research Unit email controversy (also known as \"Climategate\") began in November 2009 with the hacking of a server at the Climatic Research Unit (CRU) at the University of East Anglia (UEA) by an external attacker, copying thousands of emails and computer files, the Climatic Research Unit documents, to various internet locations several weeks before the Copenhagen Summit on climate change.\n\nThe story was first broken by climate change denialists, with columnist James Delingpole popularising the term \"Climategate\" to describe the controversy. Several climate-change \"skeptics\" argued that the emails showed that global warming was a scientific conspiracy and that scientists manipulated climate data and attempted to suppress critics. The CRU rejected this, saying that the emails had been taken out of context and merely reflected an honest exchange of ideas.\n\nThe mainstream media picked up the story, as negotiations over climate change mitigation began in Copenhagen on 7 December 2009. Because of the timing, scientists, policy makers and public-relations experts said that the release of emails was a smear campaign intended to undermine the climate conference. In response to the controversy, the American Association for the Advancement of Science (AAAS), the American Meteorological Society (AMS) and the Union of Concerned Scientists (UCS) released statements supporting the scientific consensus that the Earth's mean surface temperature had been rising for decades, with the AAAS concluding: \"based on multiple lines of scientific evidence that global climate change caused by human activities is now underway... it is a growing threat to society\".\n\nEight committees investigated the allegations and published reports, finding no evidence of fraud or scientific misconduct. The scientific consensus that global warming is occurring as a result of human activity remained unchanged throughout the investigations.\n\nThe incident began when a server used by the Climatic Research Unit was breached in \"a sophisticated and carefully orchestrated attack\", and 160  of data were obtained including more than 1,000 emails and 3,000 other documents. The University of East Anglia stated that the server from which the data were taken was not one that could be accessed easily, and that the data could not have been released inadvertently. Norfolk Police later added that the offenders used methods that are common in unlawful internet activity, designed to obstruct later enquiries. The breach was first discovered on 17 November 2009 after the server of the RealClimate website was also hacked and a copy of the stolen data was uploaded there. RealClimate's Gavin Schmidt said that he had information that the files had been obtained through \"a hack into [CRU's] backup mail server\". At about the same time, a short comment appeared on Stephen McIntyre's Climate Audit website saying that \"A miracle has happened.\"\n\nOn 19 November, an archive file containing the data was uploaded to a server in Tomsk, Russia, and then copied to numerous locations across the Internet. An anonymous post from a Saudi Arabian IP address to the climate denialist blog \"The Air Vent\" described the material as \"a random selection of correspondence, code, and documents\", adding that climate science is \"too important to be kept under wraps\". That same day, Stephen McIntyre of Climate Audit was forwarded an internal email sent to UEA staff warning that \"climate change sceptics\" had obtained a \"large volume of files and emails\". Charles Rotter, moderator of the climate-sceptic blog \"Watts Up With That\", which had been the first to get a link and download the files, gave a copy to his flatmate Steve Mosher. Mosher received a posting from the hacker complaining that nothing was happening and replied: \"A lot is happening behind the scenes. It is not being ignored. Much is being coordinated among major players and the media. Thank you very much. You will notice the beginnings of activity on other sites now. Here soon to follow.\" Shortly afterwards, the emails began to be widely publicised on climate-sceptic blogs. On 20 November, the story emerged in mainstream media.\n\nNorfolk police subsequently confirmed that they were \"investigating criminal offences in relation to a data breach at the University of East Anglia\" with the assistance of the Metropolitan Police Central e-Crime unit, the Information Commissioner's Office (ICO), and the National Domestic Extremism Team (NDET). Commenting on the involvement of the NDET, a spokesman said: \"At present we have two police officers assisting Norfolk with their investigation, and we have also provided computer forensic expertise. While this is not strictly a domestic extremism matter, as a national police unit we had the expertise and resource to assist with this investigation, as well as good background knowledge of climate change issues in relation to criminal investigations.\" However, the police cautioned that \"major investigations of this nature are of necessity very detailed and as a consequence can take time to reach a conclusion\". On 18 July 2012, the Norfolk police finally decided to close its investigation because they did not have a \"realistic prospect of identifying the offender or offenders and launching criminal proceedings within the time constraints imposed by law\". They also said that the attack had been carried out \"remotely via the internet\", and that there was \"no evidence to suggest that anyone working at or associated with the University of East Anglia was involved in the crime\".\n\nThe material comprised more than 1,000 emails, 2,000 documents, as well as commented source code, pertaining to climate-change research, covering a period from 1996 until 2009. According to an analysis in \"The Guardian\", the vast majority of the emails related to four climatologists: Phil Jones, the head of the CRU; Keith Briffa, a CRU climatologist specialising in tree ring analysis; Tim Osborn, a climate modeller at CRU; and Mike Hulme, director of the Tyndall Centre for Climate Change Research. The four were either recipients or senders of all but 66 of the 1,073 emails, with most of the remainder of the emails being sent from mailing lists. A few other emails were sent by, or to, other staff at the CRU. Jones, Briffa, Osborn and Hulme had written high-profile scientific papers on climate change that had been cited in reports by the Intergovernmental Panel on Climate Change.\n\nMost of the emails concerned technical and mundane aspects of climate research, such as data analysis and details of scientific conferences. \"The Guardian's\" analysis of the emails suggests that the hacker had filtered them. Four scientists were targeted and a concordance plot shows that the words \"data\", \"climate\", \"paper\", \"research\", \"temperature\" and \"model\" were predominant. The controversy has focused on a small number of emails with \"climate sceptic\" websites picking out particular phrases, such as one in which Kevin Trenberth said, \"The fact is that we can’t account for the lack of warming at the moment and it is a travesty that we can’t\". This was actually part of a discussion on the need for better monitoring of the energy flows involved in short-term climate variability, but was grossly mischaracterised by critics.\n\nMany commentators quoted one email in which Phil Jones said that he had used \"Mike's \"Nature\" trick\" in a 1999 graph for the World Meteorological Organization \"to hide the decline\" in proxy temperatures derived from tree-ring analyses when measured temperatures were actually rising. This \"decline\" referred to the well-discussed tree-ring divergence problem, but these two phrases were taken out of context by global warming sceptics, including US Senator Jim Inhofe and former Governor of Alaska Sarah Palin, as though they referred to some decline in measured global temperatures, even though they were written when temperatures were at a record high. John Tierney, writing in \"The New York Times\" in November 2009, said that the claims by sceptics of \"hoax\" or \"fraud\" were incorrect, but that the graph on the cover of a report for policy makers and journalists did not show these non-experts where proxy measurements changed to measured temperatures. The final analyses from various subsequent inquiries concluded that in this context \"trick\" was normal scientific or mathematical jargon for a neat way of handling data, in this case a statistical method used to bring two or more different kinds of data sets together in a legitimate fashion. The EPA notes that in fact, the evidence shows that the research community was fully aware of these issues and that no one was hiding or concealing them.\n\nFormer Republican House Science Committee chairman Sherwood Boehlert called the attacks a \"manufactured distraction\", and the dispute was described as a \"highly orchestrated\" and manufactured controversy by \"Newsweek\" and \"The New York Times\". Concerns about the media's role in promoting early allegations while also minimising later coverage exonerating the scientists were raised by journalists and policy experts. Historian Spencer R. Weart of the American Institute of Physics said the incident was unprecedented in the history of science, having \"never before seen a set of people accuse an entire community of scientists of deliberate deception and other professional malfeasance\". The United States National Academy of Sciences expressed concern and condemned what they called \"political assaults on scientists and climate scientists in particular\".\n\nIn the United Kingdom and United States, there were calls for official inquiries into issues raised by the documents. The British Conservative politician Lord Lawson said: \"The integrity of the scientific evidence ... has been called into question. And the reputation of British science has been seriously tarnished. A high-level independent inquiry must be set up without delay.\" Bob Ward of the Grantham Research Institute on Climate Change and the Environment at the London School of Economics said that there had to be a rigorous investigation into the substance of the email messages, once appropriate action has been taken over the hacking, to clear the impression of impropriety given by the selective disclosure and dissemination of the messages. United States Senator Jim Inhofe, who had previously stated that global warming was \"the greatest hoax ever perpetrated on the American people\", also planned to demand an inquiry. In a debate in the United States House of Representatives on 2 December 2009, Republicans read out extracts from eight of the emails, and Representative Jim Sensenbrenner said: \"These e-mails show a pattern of suppression, manipulation and secrecy that was inspired by ideology, condescension and profit\". In response, the president's science adviser John Holdren said that the science was proper, and the emails only concerned a fraction of the research. Government scientist Jane Lubchenco said that the emails \"do nothing to undermine the very strong scientific consensus\" that the Earth is warming, largely due to human actions.\n\nClimate change sceptics gained wide publicity in blogs and news media, making allegations that the hacked emails showed evidence that climate scientists manipulated data. A few other commentators such as Roger A. Pielke said that the evidence supported claims that dissenting scientific papers had been suppressed. \"The Wall Street Journal\" reported that the emails revealed apparent efforts to ensure that the IPCC included their own views and excluded others, and that the scientists withheld scientific data.\n\nAn editorial in \"Nature\" stated that \"A fair reading of the e-mails reveals nothing to support the denialists' conspiracy theories.\" It said that emails showed harassment of researchers, with multiple Freedom of Information requests to the Climatic Research Unit, but release of information had been hampered by national government restrictions on releasing the meteorological data researchers had been using. \"Nature\" considered that emails had not shown anything that undermined the scientific case on human-caused global warming or raised any substantive reasons for concern about the researchers' own papers. The Telegraph reported that academics and climate change researchers dismissed the allegations, saying that nothing in the emails proved wrongdoing. Independent reviews by FactCheck and the Associated Press said that the emails did not affect evidence that man-made global warming is a real threat, and said that emails were being misrepresented to support unfounded claims of scientific misconduct. The AP said that the \"[e]-mails stolen from climate scientists show they stonewalled sceptics and discussed hiding data\". In this context, John Tierney of \"The New York Times\" wrote: \"these researchers, some of the most prominent climate experts in Britain and America, seem so focused on winning the public-relations war that they exaggerate their certitude — and ultimately undermine their own cause\".\n\nClimate scientists at the CRU and elsewhere received numerous threatening and abusive emails in the wake of the initial incidents. Norfolk Police interviewed Phil Jones about death threats made against him following the release of the emails; Jones later said that the police told him that these \"didn’t fulfil the criteria for death threats\". Death threats against two scientists also are under investigation by the US Federal Bureau of Investigation. Climate scientists in Australia have reported receiving threatening emails including references to where they live and warnings to \"be careful\" about how some people might react to their scientific findings. In July 2012, Michael Mann said that the episode had caused him to \"endure countless verbal attacks upon my professional reputation, my honesty, my integrity, even my life and liberty\".\n\nThe University of East Anglia was notified of the security breach on 17 November 2009, but when the story was published in the press on 20 November, they had no statement ready. On 24 November, Trevor Davies, the University of East Anglia pro-vice-chancellor with responsibility for research, rejected calls for Jones' resignation or firing: \"We see no reason for Professor Jones to resign and, indeed, we would not accept his resignation. He is a valued and important scientist.\" The university announced that it would conduct an independent review into issues including Freedom of Information requests to the Climatic Research Unit: it would \"address the issue of data security, an assessment of how we responded to a deluge of Freedom of Information requests, and any other relevant issues which the independent reviewer advises should be addressed\".\n\nThe university announced on 1 December that Phil Jones was to stand aside as director of the Unit until the completion of the review. Two days later, the university announced that Sir Muir Russell would chair the inquiry, which would be known as the Independent Climate Change Email Review, and would \"examine email exchanges to determine whether there is evidence of suppression or manipulation of data\". The review would also scrutinise the CRU's policies and practices for \"acquiring, assembling, subjecting to peer review, and disseminating data and research findings\" and \"their compliance or otherwise with best scientific practice\". In addition, the investigation would review CRU's compliance with Freedom of Information Act requests and also \"make recommendations about the management, governance and security structures for CRU and the security, integrity and release of the data it holds\". The Independent Climate Change Email Review report was published on 7 July 2010.\n\nOn 22 March 2010 the university announced the composition of an independent Science Assessment Panel to reassess key CRU papers that have already been peer-reviewed and published in journals. The panel did not seek to evaluate the science itself, but rather whether \"the conclusions [reached by the CRU] represented an honest and scientifically justified interpretation of the data\". The university consulted with the Royal Society in establishing the panel. It was chaired by Lord Oxburgh, and its membership consisted of Huw Davies of ETH Zurich, Kerry Emanual of Massachusetts Institute of Technology, Lisa Graumlich of the University of Arizona, David Hand of Imperial College London, and Herbert Huppert and Michael Kelly of the University of Cambridge. It started its work in March 2010 and released its report on 14 April 2010. During its inquiry, the panel examined eleven representative CRU publications, selected with advice from the Royal Society, that spanned a period of over 20 years, as well as other CRU research materials. It also spent fifteen person-days at the UEA carrying out interviews with scientists.\n\nAmong the scientists whose emails were disclosed, the CRU's researchers said in a statement that the emails had been taken out of context and merely reflected an honest exchange of ideas. Michael Mann, director of Pennsylvania State University's Earth System Science Center, said that sceptics were \"taking these words totally out of context to make something trivial appear nefarious\" and called the entire incident a careful, \"high-level, orchestrated smear campaign to distract the public about the nature of the climate change problem\". Kevin E. Trenberth of the National Center for Atmospheric Research said that he was appalled at the release of the emails but thought that it might backfire against climate sceptics, as the messages would show \"the integrity of scientists\". He also said that climate change sceptics had selectively quoted words and phrases out of context and that the timing suggested an attempt to undermine talks at the December 2009 Copenhagen global climate summit. Tom Wigley, a former director of the CRU and now head of the US National Center for Atmospheric Research, condemned the threats that he and other colleagues had received as \"truly stomach-turning\", and commented: \"None of it affects the science one iota. Accusations of data distortion or faking are baseless. I can rebut and explain all of the apparently incriminating e-mails that I have looked at, but it is going to be very time consuming to do so.\" In relation to the harassment that he and his colleagues were experiencing, he said: \"This sort of thing has been going on at a much lower level for almost 20 years and there have been other outbursts of this sort of behaviour – criticism and abusive emails and things like that in the past. So this is a worse manifestation but it's happened before so it's not that surprising.\"\n\nOther prominent climate scientists, such as Richard Somerville, called the incident a smear campaign. David Reay of the University of Edinburgh said that the CRU \"is just one of many climate-research institutes that provide the underlying scientific basis for climate policy at national and international levels. The conspiracy theorists may be having a field day, but if they really knew academia they would also know that every published paper and data set is continually put through the wringer by other independent research groups. The information that makes it into the IPCC reports is some of the most rigorously tested and debated in any area of science.\" Stephen Schneider compared the political attacks on climate scientists to the witch-hunts of McCarthyism.\n\nJames Hansen said that the controversy has \"no effect on the science\" and that while some of the emails reflect poor judgment, the evidence for human-made climate change is overwhelming.\n\nOne of the IPCC's lead authors, Raymond Pierrehumbert of the University of Chicago, expressed concern at the precedent established by this incident: \"[T]his is a criminal act of vandalism and of harassment of a group of scientists that are only going about their business doing science. It represents a whole new escalation in the war on climate scientists who are only trying to get at the truth... What next? Deliberate monkeying with data on servers? Insertion of bugs into climate models?\" Another IPCC lead author, David Karoly of the University of Melbourne, reported receiving hate emails in the wake of the incident and said that he believed that there was \"an organised campaign to discredit individual climate scientists\". Andrew Pitman of the University of New South Wales commented: \"The major problem is that scientists have to be able to communicate their science without fear or favour and there seems to be a well-orchestrated campaign designed to intimidate some scientists.\"\n\nIn response to the incident, 1,700 British scientists signed a joint statement circulated by the UK Met Office declaring their \"utmost confidence in the observational evidence for global warming and the scientific basis for concluding that it is due primarily to human activities\".\n\nPatrick J. Michaels, who was criticised in the emails and who has long faulted evidence pointing to human-driven warming, said: \"This is not a smoking gun; this is a mushroom cloud\". He said that some emails showed an effort to block the release of data for independent review and that some messages discussed discrediting him by stating that he knew his research was wrong in his doctoral dissertation, \"This shows these are people willing to bend rules and go after other people's reputations in very serious ways.\"\n\nJudith Curry wrote that, in her opinion, \"there are two broader issues raised by these emails that are impeding the public credibility of climate research: lack of transparency in climate data, and 'tribalism' in some segments of the climate research community that is impeding peer review and the assessment process\". She hoped that the affair would change the approach of scientists to providing their data to the public and their response to criticisms of their work. She had herself learned to be careful about what to put in emails when a \"disgruntled employee\" made a freedom of information request. Mann described these comments as \"somewhat naive\" considering that in recent years scientists had become much more open with their data. He said that sceptics \"will always complain about something else, want something more. Eventually, as we see, they've found a way to get access to private communications between scientists.\"\n\nHans von Storch, who also concurs with the mainstream view on global warming, said that the University of East Anglia (UEA) had \"violated a fundamental principle of science\" by refusing to share data with other researchers. \"They play science as a power game,\" he said. On 24 November 2009 the university had stated that 95% of the raw station data was accessible via the Global Historical Climatology Network, and had been for several years. They were already working with the Met Office to obtain permissions to release the remaining raw data.\n\nThe Intergovernmental Panel on Climate Change Working Group I issued statements that the assessment process, involving hundreds of scientists worldwide, is designed to be transparent and to prevent any individual or small group from manipulating the process. The statement said that the \"internal consistency from multiple lines of evidence strongly supports the work of the scientific community, including those individuals singled out in these email exchanges\".\n\nThe American Meteorological Society stated that the incident did not affect the society's position on climate change. They pointed to the breadth of evidence for human influence on climate, stating:\nThe American Geophysical Union issued a statement that they found \"it offensive that these emails were obtained by illegal cyber attacks and they are being exploited to distort the scientific debate about the urgent issue of climate change\". They reaffirmed their 2007 position statement on climate change \"based on the large body of scientific evidence that Earth's climate is warming and that human activity is a contributing factor. Nothing in the University of East Anglia hacked e-mails represents a significant challenge to that body of scientific evidence.\"\n\nThe American Association for the Advancement of Science (AAAS) reaffirmed its position on global warming and \"expressed grave concerns that the illegal release of private emails stolen from the University of East Anglia should not cause policy-makers and the public to become confused about the scientific basis of global climate change. Scientific integrity demands robust, independent peer review, however, and AAAS therefore emphasised that investigations are appropriate whenever significant questions are raised regarding the transparency and rigour of the scientific method, the peer-review process, or the responsibility of individual scientists. The responsible institutions are mounting such investigations.\" Alan I. Leshner, CEO of the AAAS and executive publisher of the journal \"Science\", said: \"AAAS takes issues of scientific integrity very seriously. It is fair and appropriate to pursue answers to any allegations of impropriety. It’s important to remember, though, that the reality of climate change is based on a century of robust and well-validated science.\"\n\nOn 23 November 2009, a spokesman for the Met Office, the UK's national weather service, which works with the CRU in providing global temperature information, said that there was no need for an inquiry. \"The bottom line is that temperatures continue to rise and humans are responsible for it. We have every confidence in the science and the various datasets we use. The peer-review process is as robust as it could possibly be.\"\n\nOn 5 December 2009, however, the Met Office indicated its intention to re-examine 160 years of temperature data in the light of concerns that public confidence in the science had been damaged by the controversy over the emails. The Met Office would also publish online the temperature records for over 1,000 worldwide weather stations. It remained confident that its analysis would be shown to be correct and that the data would show a temperature rise over the past 150 years.\n\nRajendra Pachauri, as chairman of the Intergovernmental Panel on Climate Change, told the BBC in December 2009 that he considered the affair to be \"a serious issue\" and that they \"will look into it in detail\". He later clarified that the IPCC would review the incident to identify lessons to be learned and rejected suggestions that the IPCC itself should carry out an investigation.\n\nIn a series of emails sent through a National Academy of Sciences (NAS) listserv, apparently forwarded outside the group by an unknown person, scientists discussing the \"Climategate\" fallout considered launching advertising campaigns, widening their public presence, pushing the NAS to take a more active role in explaining climate science and creating a nonprofit to serve as a voice for the scientific community.\n\nA paper by Reiner Grundmann used a limited account of the events to discuss norms of scientific practice in relation to two science ethics approaches, the Mertonian norms as of Robert K. Merton, and Roger Pielke Jr.'s concept of honest brokering in science policy interactions. Sources for the paper were chosen for accessibility, emphasising \"critical accounts\".\n\nEight committees investigated the allegations and published reports, finding no evidence of fraud or scientific misconduct. The scientific consensus that global warming is occurring as a result of human activity remained unchanged by the end of the investigations. However, the reports urged the scientists to avoid any such allegations in the future, and to regain public confidence following this media storm, with \"more efforts than ever to make available all their supporting data – right down to the computer codes they use – to allow their findings to be properly verified\". Climate scientists and organisations pledged to improve scientific research and collaboration with other researchers by improving data management and opening up access to data, and to honour any freedom of information requests that relate to climate science.\n\nOn 22 January 2010, the House of Commons Science and Technology Select Committee announced it would conduct an inquiry into the affair, examining the implications of the disclosure for the integrity of scientific research, reviewing the scope of the independent Muir Russell review announced by the UEA, and reviewing the independence of international climate data sets. The committee invited written submissions from interested parties, and published 55 submissions that it had received by 10 February. They included submissions from the University of East Anglia, the Global Warming Policy Foundation, the Institute of Physics, the Royal Society of Chemistry, the Met Office, several other professional bodies, prominent scientists, some climate change sceptics, several MEPs and other interested parties. An oral evidence session was held on 1 March 2010.\n\nThe Science and Technology Select Committee inquiry reported on 31 March 2010 that it had found that \"the scientific reputation of Professor Jones and CRU remains intact\". The emails and claims raised in the controversy did not challenge the scientific consensus that \"global warming is happening and that it is induced by human activity\". The MPs had seen no evidence to support claims that Jones had tampered with data or interfered with the peer-review process.\n\nThe committee criticised a \"culture of non-disclosure at CRU\" and a general lack of transparency in climate science where scientific papers had usually not included all the data and code used in reconstructions. It said that \"even if the data that CRU used were not publicly available—which they mostly are—or the methods not published—which they have been—its published results would still be credible: the results from CRU agree with those drawn from other international data sets; in other words, the analyses have been repeated and the conclusions have been verified.\" The report added that \"scientists could have saved themselves a lot of trouble by aggressively publishing all their data instead of worrying about how to stonewall their critics.\" The committee criticised the university for the way that freedom of information requests were handled, and for failing to give adequate support to the scientists to deal with such requests.\n\nThe committee chairman Phil Willis said that the \"standard practice\" in climate science generally of not routinely releasing all raw data and computer codes \"needs to change and it needs to change quickly\". Jones had admitted sending \"awful emails\"; Willis commented that \"[Jones] probably wishes that emails were never invented,\" but \"apart from that we do believe that Prof. Jones has in many ways been scapegoated as a result of what really was a frustration on his part that people were asking for information purely to undermine his research.\" In Willis' view this did not excuse any failure to deal properly with FOI Act requests, but the committee accepted that Jones had released all the data that he could. It stated: \"There is no reason why Professor Jones should not resume his post. He was certainly not co-operative with those seeking to get data, but that was true of all the climate scientists\".\n\nThe committee was careful to point out that its report had been written after a single day of oral testimony and would not be as in-depth as other inquiries.\n\nThe report of the independent Science Assessment Panel was published on 14 April 2010 and concluded that the panel had seen \"no evidence of any deliberate scientific malpractice in any of the work of the Climatic Research Unit.\" It found that the CRU's work had been \"carried out with integrity\" and had used \"fair and satisfactory\" methods. The CRU was found to be \"objective and dispassionate in their view of the data and their results, and there was no hint of tailoring results to a particular agenda.\" Instead, \"their sole aim was to establish as robust a record of temperatures in recent centuries as possible.\"\n\nThe panel commented that it was \"very surprising that research in an area that depends so heavily on statistical methods has not been carried out in close collaboration with professional statisticians.\" It found that although the CRU had not made inappropriate use of statistical methods, some of the methods used may not have been the best for the purpose, though it said that \"it is not clear, however, that better methods would have produced significantly different results.\" It suggested that the CRU could have done more to document and archive its work, data and algorithms and stated that the scientists were \"ill prepared\" for the amount of public attention generated by their work, commenting that \"as with many small research groups their internal procedures were rather informal.\" The media and other scientific organisations were criticised for having \"sometimes neglected\" to reflect the uncertainties, doubts and assumptions of the work done by the CRU. The UK Government's policy of charging for access to scientific data was described as \"inconsistent with policies of open access to data promoted elsewhere.\" The panel was also stated that \"Although we deplore the tone of much of the criticism that has been directed at CRU, we believe that this questioning of the methods and data used in dendroclimatology will ultimately have a beneficial effect and improve working practices.\" It found that some of the criticism had been \"selective and uncharitable\" and critics had displayed \"a lack of awareness\" of the difficulties of research in this area.\n\nSpeaking at a press conference to announce the report, the panel's chair, Lord Oxburgh, stated that his team had found \"absolutely no evidence of any impropriety whatsoever\" and that \"whatever was said in the emails, the basic science seems to have been done fairly and properly.\" He said that many of the criticisms and allegations of scientific misconduct had been made by people \"who do not like the implications of some of the conclusions\" reached by the CRU's scientists. He said that the repeated FOI requests made by climate change sceptic Steve McIntyre and others could have amounted to a campaign of harassment, and the issue of how FOI laws should be applied in an academic context remained unresolved. Another panel member, Professor David Hand, commended the CRU for being explicit about the inherent uncertainties in its research data, commenting that \"there is no evidence of anything underhand – the opposite, if anything, they have brought out into the open the uncertainties with what they are dealing with.\"\n\nAt the press conference, Hand also commented on the well publicised 1998 paper produced in the United States by scientists led by Michael E. Mann, saying that the hockey stick graph it showed was a genuine effect, but he had an \"uneasy feeling\" about the use of \"inappropriate statistical tools\" and said that the 1998 study had exaggerated the effect. He commended McIntyre for pointing out this issue. Mann subsequently told \"The Guardian\" that the study had been examined and approved in the US National Academies of Science North Report, and described Hand's comment as a \"rogue opinion\" not meriting \"much attention or credence\".\n\nThe UEA's vice-chancellor, Edward Acton, welcomed the panel's findings. Describing its report as \"hugely positive\", he stated that \"it is especially important that, despite a deluge of allegations and smears against the CRU, this independent group of utterly reputable scientists have concluded that there was no evidence of any scientific malpractice.\" He criticised the way that the emails had been misrepresented, saying that \"UEA has already put on record its deep regret and anger that the theft of emails from the University, and the blatant misrepresentation of their contents as revealed both in this report and the previous one by the Science and Technology Select Committee, damaged the reputation of UK climate science.\" The UEA issued a statement in which it accepted that \"things might have been done better.\" It said that improvements had already been undertaken by the CRU and others in the climate science community and that the University would \"continue to ensure that these imperatives are maintained.\"\n\nIt later emerged that the Science Assessment Panel was not assessing the quality but instead the integrity of the CRU's science. Phil Willis described this a \"sleight of hand\" and was not what the Parliamentary Committee he had chaired had been led to believe. There were also questions about the selection of publications examined by the panel. Lord Oxburgh said that Acton had been wrong to tell the Science and Technology Select Committee in March that his inquiry would look into the science itself. \"I think that was inaccurate,\" Oxburgh said. \"This had to be done rapidly. This was their concern. They really wanted something within a month. There was no way our panel could evaluate the science.\"\n\nPennsylvania State University announced in December 2009 it would review the work of Michael E. Mann, in particular looking at anything that had not already been addressed in the 2006 North Report review by the National Research Council of the National Academy of Sciences which had investigated Mann's \"hockey stick graph\" studies and found some faults with his 1998 methodology but agreed with the results which had been reaffirmed by later studies using different methods. In response, Mann said he would welcome the review. The inquiry committee determined on 3 February 2010 that there was no credible evidence Mann suppressed or falsified data, destroyed emails, information and/or data related to the IPCC Fourth Assessment Report, or misused privileged or confidential information. The committee did not make a definitive finding on the final point of inquiry — \"whether Dr Mann seriously deviated from accepted practices within the academic community for proposing, conducting, or reporting research or other scholarly activities\". The committee said that the earlier NAS inquiry had found \"that Dr Mann’s science did fall well within the bounds of accepted practice\", but in light of the newly available information this question of conduct was to be investigated by a second panel of five prominent Penn State scientists from other scientific disciplines.\n\nThe second Investigatory Committee reported on 4 June 2010 that it had \"determined that Dr Michael E. Mann did not engage in, nor did he participate in, directly or indirectly, any actions that seriously deviated from accepted practices within the academic community.\" Regarding his sharing unpublished manuscripts with colleagues on the assumption of implied consent, it considered such sharing to be \"careless and inappropriate\" without following the best practice of getting express consent from the authors in advance, though expert opinion on this varied. It said that his success in proposing research and obtaining funding for it, commenting that this \"clearly places Dr Mann among the most respected scientists in his field. Such success would not have been possible had he not met or exceeded the highest standards of his profession for proposing research.\" Mann's extensive recognitions within the research community demonstrated that \"his scientific work, especially the conduct of his research, has from the beginning of his career been judged to be outstanding by a broad spectrum of scientists.\" It agreed unanimously that \"there is no substance\" to the allegations against Mann.\n\nMann said he regretted not objecting to a suggestion from Jones in a 29 May 2008 message that he destroy emails. \"I wish in retrospect I had told him, 'Hey, you shouldn't even be thinking about this,'\" Mann said in March 2010. \"I didn't think it was an appropriate request.\" Mann's response to Jones at the time was that he would pass on the request to another scientist. \"The important thing is, I didn't delete any emails. And I don't think [Jones] did either.\"\n\nFirst announced in December 2009, a British investigation commissioned by the UEA and chaired by Sir Muir Russell, published its final report in July 2010. The commission cleared the scientists and dismissed allegations that they manipulated their data. The \"rigour and honesty\" of the scientists at the Climatic Research Unit were found not to be in doubt. The panel found that they did not subvert the peer review process to censor criticism as alleged, and that the key data needed to reproduce their findings was freely available to any \"competent\" researcher.\n\nThe panel did rebuke the CRU for their reluctance to release computer files, and found that a graph produced in 1999 was \"misleading,\" though not deliberately so as necessary caveats had been included in the accompanying text. It found evidence that emails might have been deleted in order to make them unavailable should a subsequent request be made for them, though the panel did not ask anyone at CRU whether they had actually done this.\n\nAt the conclusion of the inquiry, Jones was reinstated with the newly created post of Director of Research.\n\nThe United States Environmental Protection Agency (EPA) had issued an \"endangerment finding\" in 2009 in preparation for climate regulations on excessive greenhouse gases. Petitions to reconsider this were raised by the states of Virginia and Texas, conservative activists and business groups including the United States Chamber of Commerce, the Competitive Enterprise Institute and the coal company Peabody Energy, making claims that the CRU emails undermined the science.\n\nThe EPA examined every email and concluded that there was no merit to the claims in the petitions, which \"routinely misunderstood the scientific issues\", reached \"faulty scientific conclusions\", \"resorted to hyperbole\", and \"often cherry-pick language that creates the suggestion or appearance of impropriety, without looking deeper into the issues.\" In a statement issued on 29 July 2010, EPA Administrator Lisa P. Jackson said the petitions were based \"on selectively edited, out-of-context data and a manufactured controversy\" and provided \"no evidence to undermine our determination. Excess greenhouse gases are a threat to our health and welfare.\"\n\nThe EPA issued a detailed report on issues raised by petitioners and responses, together with a fact sheet, and a \"myths versus facts\" page stating that \"Petitioners say that emails disclosed from CRU provide evidence of a conspiracy to manipulate data. The media coverage after the emails were released was based on email statements quoted out of context and on unsubstantiated theories of conspiracy. The CRU emails do not show either that the science is flawed or that the scientific process has been compromised. EPA carefully reviewed the CRU emails and found no indication of improper data manipulation or misrepresentation of results.\"\n\nIn May 2010 Senator Jim Inhofe requested the Inspector General of the United States Department of Commerce to conduct an independent review of how the National Oceanic and Atmospheric Administration (NOAA) had dealt with the emails, and whether the emails showed any wrongdoing. The report, issued on 18 February 2011, cleared the researchers and \"did not find any evidence that NOAA inappropriately manipulated data or failed to adhere to appropriate peer review procedures\". It noted that NOAA reviewed its climate change data as standard procedure, not in response to the controversy. One email included a cartoon image showing Infofe and others marooned on a melting ice floe, NOAA had taken this up as a conduct issue. In response to questions raised, NOAA stated that its scientists had followed legal advice on FOIA requests for information which belonged to the IPCC and was made available by that panel. In two instances funding had been awarded to CRU, NOAA stated that it was reviewing these cases and so far understood that the funds supported climate forecasting workshops in 2002 and 2003 assisting the governments of three countries.\n\nThe Office of the Inspector General (OIG) of the National Science Foundation closed an investigation on 15 August 2011 that exonerated Michael Mann of Pennsylvania State University of charges of scientific misconduct. It found no evidence of research misconduct, and confirmed the results of earlier inquiries. The OIG reviewed the findings of the July 2010 Penn State panel, took further evidence from the university and Mann, and interviewed Mann. The OIP findings confirmed the university panel's conclusions which cleared Mann of any wrongdoing, and it stated \"Lacking any evidence of research misconduct, as defined under the NSF Research Misconduct Regulation, we are closing the investigation with no further action.\"\n\nIn two cases, the Information Commissioner's Office (ICO) issued decisions on appeals of Freedom of Information (FOI) requests which had been turned down by the university.\n\nDavid Holland, an electrical engineer from Northampton, made a 2008 FOI request for all emails to and from Keith Briffa about the IPCC Fourth Assessment Report; the university's information policy and compliance manager refused the request. On 23 November 2009, after the start of the controversy, he wrote to the Commissioner explaining in detail the relevance of the alleged CRU emails to his case, with specific reference to a May 2008 email in which Phil Jones asked others to delete emails discussing AR4 with Briffa. In January 2010 news reports highlighted that FOI legislation made it an offence to intentionally act to prevent the disclosure of requested information, but the statute of limitations meant that any prosecution had to be raised within 6 months of the alleged offence. This was discussed by the House of Commons Science and Technology Select Committee. The ICO decision on Holland's requests published on 7 July 2010 concluded that the emails indicated \"prima facie\" evidence of an offence, but as prosecution was time-barred the Commissioner had been unable to investigate the alleged offence. On the issue of the university failing to provide responses within the correct time, no further action was needed as Holland was content not to proceed with his complaint.\n\nThe Climatic Research Unit developed its gridded CRUTEM data set of land air temperature anomalies from instrumental temperature records held by around the world, often under formal or informal confidentiality agreements that restricted use of this raw data to academic purposes, and prevented it from being passed onto third parties. Over 95% of the CRU climate data set had been available to the public for several years before July 2009, when the university received numerous FOI requests for raw data or details of the confidentiality agreements from Stephen McIntyre and readers of his Climate Audit blog. Phil Jones of CRU announced that requests were being made to all the National Meteorological Organisations for their agreement to waive confidentiality, with the aim of publishing all the data jointly with the Met Office. McIntyre complained that data denied to him had been sent to Jones's colleague Peter Webster at the Georgia Institute of Technology for work on a joint publication, and FOI requests for this data were made by Jonathan A. Jones of the University of Oxford and Don Keiller of Anglia Ruskin University. Both requests were refused by the UEA by 11 September 2009.\nThough some National Meteorological Organisations gave full or conditional agreement to waive confidentiality, others failed to respond, and the request was explicitly refused by Trinidad and Tobago and Poland. In discussions with the ICO, the university argued that the data was publicly available from the Met organisations, and the lack of agreement exempted the remaining data. In its decision released on 23 June 2011, the ICO stated that the data was not easily available, and required the university to release the data covered by the FOIA request. On 27 July 2011 CRU announced that the raw instrumental data not already in the public domain had been released and was available for download, with the exception of Poland which was outside the area covered by the FOIA request. The university remained concerned \"that the forced release of material from a source which has explicitly refused to give permission for release could have some damaging consequences for the UK in international research collaborations.\"\n\nIn September 2011 the ICO issued new guidance to universities, taking into account issues raised in relation to the CRU information requests. This describes exceptions and exemptions to protect research, including allowance for internal exchange of views between academics and researchers, leaving formulation of opinions on research free from external scrutiny. It notes the benefits of actively disclosing information when it is in the public interest, and disclosure of personal email information related to public authority business.\n\nThe initial story about the hacking originated in the blogosphere, with columnist James Delingpole picking up the term \"Climategate\" from an anonymous blogger on \"Watts Up With That?\", a blog created by climate sceptic Anthony Watts. The site was one of three blogs that received links to the leaked documents on 17 November 2009. Delingpole first used the word \"Climategate\" in the title of his 20 November article for \"The Telegraph\": \"Climategate: the final nail in the coffin of 'Anthropogenic Global Warming'?\" A week later, his co-worker Christopher Booker gave Delingpole credit for coining the term. Following the release of documents in the blogosphere, unproven allegations and personal attacks against scientists increased and made their way into the traditional media. Physicist Mark Boslough of the University of New Mexico noted that many of the attacks on scientists came from \"bloggers, editorial writers, Fox News pundits, and radio talk show hosts who have called them liars and vilified them as frauds\". According to Chris Mooney and Sheril Kirshenbaum in their book \"Unscientific America\" (2010), the accusations originated in right-wing media and blogs, \"especially on outlets like Fox News\". Journalist Suzanne Goldenberg of \"The Guardian\" reported that according to an analysis by Media Matters, \"Fox had tried to delegitimise the work of climate scientists in its coverage of the hacked emails from the University of East Anglia\" and had \"displayed a pattern of trying to skew coverage in favour of the fringe minority which doubts the existence of climate change\".\n\nThe intense media coverage of the documents stolen from climate researchers at the University of East Anglia created public confusion about the scientific consensus on climate change, leading several publications to comment on the propagation of the controversy in the media in the wake of a series of investigations that cleared the scientists of any wrongdoing. In an editorial, \"The New York Times\" described the coverage as a \"manufactured controversy\" and expressed hope that the investigations clearing the scientists \"will receive as much circulation as the original, diversionary controversies\". Writing for \"Newsweek\", journalist Sharon Begley called the controversy a \"highly orchestrated, manufactured scandal\", noting that the public was unlikely to change their mind. Regardless of the reports exonerating the scientists, Begley noted that \"one of the strongest, most-repeated findings in the psychology of belief is that once people have been told \"X\", especially if \"X\" is shocking, if they are later told, 'No, we were wrong about \"X\",' most people still believe \"X\".\"\n\nJean-Pascal van Ypersele, vice-chair of the Intergovernmental Panel on Climate Change (IPCC) and science historian Naomi Oreskes said that the \"attacks on climate science that were made ahead of the Copenhagen climate change summit were 'organised' to undermine efforts to tackle global warming and mirror the earlier tactics of the tobacco industry\". Noting the media circus that occurred when the story first broke, Oreskes and Erik Conway writing about climate change denial, said that following the investigations \"the vindication of the climate scientists has received very little coverage at all. Vindication is not as sexy as accusation, and many people are still suspicious. After all, some of those emails, taken out of context, sounded damning. But what they show is that climate scientists are frustrated, because for two decades they have been under attack.\"\n\nBill Royce, head of the European practice on energy, environment and climate change at the United States communications firm Burson-Marsteller, also described the incident as an organised effort to discredit climate science. He said that it was not a single scandal, but \"a sustained and coordinated campaign\" aimed at undermining the credibility of the science. Disproportionate reporting of the original story, \"widely amplified by climate deniers\", meant that the reports that cleared the scientists received far less coverage than the original allegations, he said. Journalist Curtis Brainard of the \"Columbia Journalism Review\" criticised newspapers and magazines for failing to give prominent coverage to the findings of the review panels and said that \"readers need to understand that while there is plenty of room to improve the research and communications process, its fundamental tenets remain as solid as ever\". CNN media critic Howard Kurtz expressed similar sentiments.\n\nJon Krosnick, professor of communication, political science and psychology at Stanford University, said that scientists were overreacting. Referring to his own poll results of the American public, he said: \"It's another funny instance of scientists ignoring science.\" Krosnick found that \"Very few professions enjoy the level of confidence from the public that scientists do, and those numbers haven't changed much in a decade. We don't see a lot of evidence that the general public in the United States is picking up on the (University of East Anglia) emails. It's too inside baseball.\"\n\n\"The Christian Science Monitor\", in an article titled \"Climate scientists exonerated in 'climategate' but public trust damaged\", stated: \"While public opinion had steadily moved away from belief in man-made global warming before the leaked CRU emails, that trend has only accelerated.\" Paul Krugman, columnist for \"The New York Times\", argued that this, along with all other incidents that called into question the scientific consensus on climate change, was \"a fraud concocted by opponents of climate action, then bought into by many in the news media\". But UK journalist Fred Pearce called the slow response of climate scientists \"a case study in how not to respond to a crisis\" and \"a public relations disaster\".\n\nA. A. Leiserowitz, Director of the Yale University Project on Climate Change, and colleagues found in 2010 that:\nIn late 2011, Steven F. Hayward wrote that \"Climategate did for the global warming controversy what the Pentagon Papers did for the Vietnam war 40 years ago: It changed the narrative decisively.\" An editorial in \"Nature\" said that many in the media \"were led by the nose, by those with a clear agenda, to a sizzling scandal that steadily defused as the true facts and context were made clear\".\n\nOn 22 November 2011, a second set of approximately 5,000 emails, apparently hacked from University of East Anglia servers at the same time as those in the 2009 release, was posted on a Russian server, with links distributed to the message boards on several climate-sceptic websites. A message accompanying the emails quoted selective passages from them, highlighting many of the same issues raised following the original incident. Juliette Jowit and Leo Hickman of \"The Guardian\" said that the new release was \"an apparent attempt to undermine public support for international action to tackle climate change\" with the start of the 2011 United Nations Climate Change Conference scheduled in Durban, South Africa, a week later. \"Nature\" described the further release as a \"poor sequel\" and claimed that \"it is hard for anyone except the most committed conspiracy theorist to see much of interest in the content of the released e-mails, even taken out of context\".\n\n\n"}
{"id": "5500034", "url": "https://en.wikipedia.org/wiki?curid=5500034", "title": "Coggeshall slide rule", "text": "Coggeshall slide rule\n\nIn measurement, the Coggeshall slide rule, also called a carpenter's slide rule, was a slide rule designed by Henry Coggeshall in 1677 to help in measuring the dimensions, surface area, and volume of timber. With his original design and later improvements, Coggeshall's slide rule brought the tool its first practical use outside of mathematical study. It would remain popular for the next few centuries.\n\nThe Coggeshall rule consisted of two rulers, each a foot (30 cm) long, which were put together in various ways. The most common and convenient arrangement was to have one of the rulers slide within a groove made along the middle of the other, like an ordinary linear slide rule, as shown in the figure below. Another form had one ruler sliding alongside the other, and a third form had a common two-foot folding ruler with a groove along one side in which a thin sliding piece was inserted that carried Coggeshall's lines.\n\nCoggeshall first described this apparatus in a paper he released in London titled, \"Timber-measure by a line of more ease, dispatch and exactness, then any other way now in use, by a double scale : after the countrey-measure, by the length and quarter of the circumference in round timber, and by the length and side of the square in squared timber, and square equal in flat timber : as also stone-measure and gauging of vessels by the same near and exact way, likewise a diagonal scale of 100 parts in a quarter of an inch, very easie both to make and use.\"\n\nAfter improving the design, he republished his work under the title \"A Treatise of Measuring by a Two-foot Rule, which slides to a Foot\" (1682). He released a highly modified version in 1722 titled \"The Art of Practical Measuring easily performed by a Two-foot Rule which slides to a Foot.\" By 1767, seven revised editions had been released.\n"}
{"id": "2583479", "url": "https://en.wikipedia.org/wiki?curid=2583479", "title": "Confit", "text": "Confit\n\nConfit (, ) comes from the French word \"confire\" which means literally \"to preserve,\" a confit being any type of food that is cooked slowly over a long period of time as a method of preservation. \n\nConfit as a cooking term describes when food is cooked in grease, oil or sugar water (syrup), at a lower temperature, as opposed to deep frying. While deep frying typically takes place at temperatures of , confit preparations are done at a much lower temperature, such as an oil temperature of around , sometimes even cooler. The term is usually used in modern cuisine to mean long slow cooking in oil or fat at low temperatures, many having no element of preservation such as dishes like confit potatoes.\n\nIn meat cooking, this requires the meat to be salted as part of the preservation process. After salting and cooking in the fat, sealed and stored in a cool, dark place, confit can last for several months or years. Confit is a specialty of southwestern France.\n\nThe word comes from the French verb \"confire\" (to preserve), which in turn comes from the Latin word (\"conficere\"), meaning \"to do, to produce, to make, to prepare\". The French verb was first applied in medieval times to fruits cooked and preserved in sugar.\n\nFruit confit are candied fruit (whole fruit, or pieces thereof) preserved in sugar. The fruit must be fully infused with sugar, to its core; larger fruit take considerably longer than smaller ones to candy. Thus, while small fruit such as cherries are \"confits\" whole, it is quite rare to see whole large fruit, such as melon \"confits\", since the time and energy involved in producing large \"fruit confits\" makes them quite expensive.\n\nConfit of goose (\"confit d'oie\") and duck (\"confit de canard\") are usually prepared from the legs of the bird. The meat is salted and seasoned with herbs, and slowly cooked submerged in its own rendered fat (never to exceed ), in which it is then preserved by allowing it to cool and storing it in the fat. Turkey and pork may be treated in the same manner. Meat confit is a specialty of the southwest of France (Toulouse, Dordogne, etc.) and is used in dishes such as cassoulet. Confit preparations originated as a means of preserving meat without refrigeration.\n\nIn a restaurant context, confit is usually served after further preparation. Whole confit leg is baked to crisp the skin or added to a casserole type dish. Confit duck leg is used to make rillette.\n\nTraditional meat for confit include both waterfowl such as goose and duck, and pork. Duck gizzards are also commonly cooked in the confit method. Varying forms of this delicacy thrive throughout southern France.\n\n\"Confit country\" is the area of Occitan France where goose fat is used to cook, as opposed to olive oil which is used in Provence where olives were plentiful and thus cheap.\n\nConfit country is divided roughly into regions where one type of meat predominates the confit preparations. Goose confit is associated with the Béarn and Basque regions with their classic specialties of cassoulet and garbure, hearty and earthy dishes of confit and beans. Saintonge and Brantôme feature duck confit, often with potatoes and truffles. \n\nNon-waterfowl meats are frequently treated to the confit process, but they are not classically considered true confits. The French refer only to duck and goose confits as true confits; other meats poached in duck or goose fat are considered \"en confit\" (\"in confit\"). For example, chicken cooked in goose fat is called \"poulet en confit\". Pork is often confit and shredded to create rillettes.\n\nItalian cooking uses a number of \"condiment confits,\" such as onion confit, chili confit, and garlic confit.\n\n\n"}
{"id": "1238697", "url": "https://en.wikipedia.org/wiki?curid=1238697", "title": "Cordless", "text": "Cordless\n\nThe term cordless is generally used to refer to electrical or electronic devices that are powered by a battery or battery pack and can operate without a power cord or cable attached to an electrical outlet to provide mains power, allowing greater mobility. The development of more powerful rechargeable batteries in recent years has allowed the production of battery-powered versions of tools and appliances that once required a power cord, and these are distinguished by the term \"cordless\", as in cordless drills, cordless saws, and cordless irons.\n\nThe term \"cordless\" should not be confused with the term \"wireless\", although it often is in common usage, possibly because some cordless devices (e.g., cordless telephones) are also wireless. The term \"wireless\" generally refers to devices that use some form of energy (e.g., radio waves, infrared, ultrasonic, etc.) to transfer \"information\" or \"commands\" over a distance without the use of communication wires, regardless of whether the device gets its power from a power cord or a battery. The term \"portable\" is an even more general term and, when referring to electrical and electronic devices, usually means devices which are totally self-contained (e.g., have built-in power supplies, have no base unit, etc.) and which may also use wireless technology.\n\n"}
{"id": "8759469", "url": "https://en.wikipedia.org/wiki?curid=8759469", "title": "ESSA-8", "text": "ESSA-8\n\nESSA-8 was a weather satellite launched by the National Aeronautics and Space Administration (NASA) on December 15, 1968, from Vandenberg Air Force Base, California. Its name was derived from that of its oversight agency, the Environmental Science Services Administration (ESSA).\n\nESSA-8 was an 18-sided polygon. It measured in diameter by in height, with a mass of . It was made of aluminum alloy and stainless steel covered with 10,020 solar cells. The cells charged 63 nickel–cadmium batteries, which served as a power source. The satellite could take 8 to 10 pictures every 24 hours. Each photo covered a area at a resolution of per pixel.\n\nESSA-8's mission was to replace ESSA-6, and provide detailed cloud pattern photography to ground stations worldwide. Partners in the project included NASA, ESSA, RCA, the National Weather Service, and the National Centers for Environmental Prediction (NMC).\n\nESSA-8 operated for 2,644 days until it was deactivated on March 12, 1976.\n\n"}
{"id": "42731008", "url": "https://en.wikipedia.org/wiki?curid=42731008", "title": "Empire (2015 TV series)", "text": "Empire (2015 TV series)\n\nEmpire is an American musical drama television series created by Lee Daniels and Danny Strong which debuted on January 7, 2015 on Fox. Although it is filmed in Chicago, the show is set in New York. It centers on a fictional hip hop music and entertainment company, Empire Entertainment, and the drama among the members of the founders' family as they fight for control of it. A hip-hop mogul must choose a successor among his three sons who are battling for control over his multimillion-dollar company, while his wife schemes to reclaim what is hers. The pilot was shown to be a success, garnering praise for Taraji P. Henson's portrayal of Cookie Lyon and the premiere receiving nearly 10 million viewers and the season finale with 17 million viewers.\n\nOn January 11, 2017, Fox renewed the series for a fourth season, consisting of eighteen episodes. The season premiered on September 27, 2017, and crossed over with another Fox series co-created by Daniels, \"Star\". \n\nOn May 2, 2018, Fox renewed the series for a fifth season with Brett Mahoney taking over as showrunner from Ilene Chaiken. The season premiered on September 26, 2018.\n\nLucious Lyon (Terrence Howard) is a hip hop mogul, former drug dealer, and CEO of Empire Entertainment. Informed of his own imminent early death from a certain medical condition, Lucious is forced to choose from amongst his progeny, a successor who will control his dynasty after his demise. He begins working to groom one of his three sons to take over the family business – Empire CFO Andre (Trai Byers), R&B singer-songwriter Jamal (Jussie Smollett), and rapper Hakeem (Bryshere Y. Gray) – In the process, Lucious pits them against each other. Lucious' ex-wife Cookie Lyon (Taraji P. Henson) is released from prison after serving a 17-year sentence, and also pulls for control of both the company and of her sons. The second season primarily focuses on the competition between Lyon Dynasty, formed and headed by Cookie and Empire.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHoward, Henson and Sidibe filmed the pilot episode in Chicago in March 2014. On May 6, 2014, Fox picked up \"Empire\" as a series order for its 2014–15 television schedule. On May 12, 2014, Fox announced that \"Empire\" would air as a midseason replacement, rather than as a part of the fall schedule. On November 19, 2014, it was confirmed that the pilot episode would premiere on January 7, 2015, following the season 14 premiere of \"American Idol\".\n\nThe executive producers are Brian Grazer, Danny Strong and Francie Calfo. The show is written by Strong while Lee Daniels, who has been nominated for the Academy Award for Best Director, made his television directorial debut with the show. According to Strong, the show is based in part on William Shakespeare's \"King Lear\", and James Goldman's \"The Lion in Winter\"; Daniels also acknowledged a strong influence from the ABC prime-time soap opera \"Dynasty\". Ilene Chaiken later joined the series as showrunner.\nOn January 17, 2015, the series was renewed for an 18-episode second season, which premiered on September 23, 2015. On January 15, 2016, \"Empire\" was renewed for a third season, which premiered on September 21, 2016, consisting of eighteen episodes.\n\nHoward was cast in the lead on February 19, 2014. Henson was named as the female lead on February 26, and Jussie Smollett was announced in a starring role. Howard and Henson previously starred together as love interests in the film \"Hustle & Flow\", and starred in the 2005 film \"Four Brothers\". On March 10, 2014, Sidibe, who had previously worked with Daniels in \"Precious\", was cast in a recurring role as Becky, Lucious' assistant. Trai Byers and Grace Gealey were announced in regular roles, while Bryshere Y. Gray and Malik Yoba were announced in starring roles.\n\nCourtney Love was added to the cast on October 23, 2014.\n\nNaomi Campbell was announced in a recurring role on September 29, 2014.\n\nOn June 2, 2015, it was announced Adam Rodriguez would join the cast in a recurring role in the second season of \"Empire\", portraying Laz Delgado, a potential love interest for Cookie.\n\nIn 2016, Phylicia Rashad was cast as a recurring guest star in the role of Diana DuBois in the third season of \"Empire\".\n\nIn February 2017, Rumer Willis joined the third season of \"Empire\" in a recurring role.\n\nAfter finding his lead actors, Lee Daniels set out to find a producer to craft the music for the series. Because he felt his own musical tastes and desires would be a little too dated for the show, Daniels consulted with people he thought could give him a little more insight, his children. At the urging of his son and daughter, Daniels contacted super producer Timothy \"Timbaland\" Mosley, known for scoring hits with some of pop's biggest artists including Missy Elliott, Madonna, Brandy, Destiny's Child, Justin Timberlake, Jay-Z, Nelly Furtado, Ginuwine and Aaliyah.\n\nAs executive music Producer, Timbaland, co-songwriter/producer Jim Beanz, and a team of other musical collaborators took their cues from the writing team, which gave them an idea of where the plot is headed, and craft songs to fit those themes. However, the musical aspect of the series is very reality based, unlike Fox's last musical series, \"Glee\", performances come with the organic development of the characters as artists, much like the performances featured in the ABC prime-time country musical drama \"Nashville\".\n\nAny licensed songs and composer Fil Eisler's score are added after the episodes are filmed. The pilot itself consists of 12 songs. Columbia Records releases weekly soundtracks of \"Empire\" on the iTunes Store, the same strategy used with \"Glee\". In May 2015, Fox announced that Ne-Yo and J.R. Rotem would write music for the show's second season, joining Timbaland. Timbaland left \"Empire\" after season 2, and were replaced for season 3 by Rodney \"Darkchild\" Jerkins and Esther Dean.\n\nOn September 8, 2015, Scott Hoying and Mitch Grassi from Pentatonix arranged a medley of songs from the first season of Empire and posted it on their shared YouTube comedy/music channel, Superfruit. The video was sponsored by the Fox Broadcasting Company.\n\nColumbia Records released the official soundtrack of \"Empire\"’s first season on March 10, 2015. The soundtrack consists of 11 songs and the deluxe version consists of 18 songs, all performed on the show. As of October 2015, the album has sold 439,000 total copies in the United States. Additional releases have followed for subsequent seasons.\n\"Empire\" was set to premiere in Australia on February 19, 2015 on Network Ten, however a week out from this date it was dumped from the schedule and pushed back. It ultimately premiered on March 1, 2015, to a disappointing 377,000 total viewers, although it fared better with viewers in younger demographics. The second season was broadcast on Eleven from September 29, 2015.\n\nIn Canada, the series was simulcast on Omni Television (Omni 2). The fourth episode and the season finale would also air on City, a sibling broadcast network. Sibling cable network FX Canada would also air a marathon of the show on March 14 and 15, 2015. The show moved to City starting in the 2015–16 season, but was dropped from their lineup after the first half of the second season due to low ratings; Fox affiliates are widely available on pay television as well as over-the-air in some areas. Shomi, a subscription streaming service co-owned by City's parent company Rogers Communications, began to add new episodes following their U.S. premiere on Fox. A representative of the service stated that \"Empire\" was among its ten most popular programs.\n\n\"Empire\" started broadcasting in the UK on E4 with the first episode airing on April 28, 2015. 5Star have since taken over the rights as of Season 4. The series premiered in India on STAR World Premiere HD on April 24, 2015 (only in high-definition). Later, on July 14, 2015, Empire also premiered on FX India (both in standard and high definition) with the Season 1 finale airing on July 29, 2015. Empire premiered on June 24, 2015 in Germany on Pro Sieben, with the Pilot reaching 930,000 viewers. In Finland, the show premiered on Sub on December 2, 2015.\nIn South Africa, the show premiered on e.tv on February 3, 2016. In Serbia series start January 23, 2016 on Fox it aired Thursdays at 10pm.\n\nIn 2016, a \"New York Times\" study of the 50 TV shows with the most Facebook Likes found that \"Empire\" \"is most popular in the Black Belt and in parts of the country with a high percentage of Native Americans\".\n\n\"Empire\" has received positive reviews from critics. The praise has gone towards the cast, particularly Howard and Henson.\nOn Rotten Tomatoes, the first season has a rating of 80%, based on 48 reviews, with an average rating of 7.1/10. The site's critical consensus reads, \"Though heavy on melodrama, \"Empire\" elevates the nighttime soap with its top-notch cast, musical entertainment, and engrossing plots.\" On Metacritic the show has a score of 72 out of 100, based on 39 critics, indicating \"generally favorable reviews\". For the second season, it built up more acclaim from critics, praising the performances of Henson and Howard, the character development, plot development, and the show's self-awareness for being a soap opera. On Rotten Tomatoes, the second season has a 95% \"Certified Fresh\" with an average rating of 7.8/10 from critics and a score of 77 from Metacritic, indicating \"generally positive reviews\".\n\nDavid Wiegand wrote in the \"San Francisco Chronicle\": \"Almost nothing about \"Empire\", created by Lee Daniels (\"The Butler\"), feels original, but after a few minutes, you will stop caring\". 50 Cent stated via Twitter that he feels that \"Empire\" is being marketed by FOX in a way that resembles the marketing for the 2014 Starz series that he produces called \"Power\". Michael Logan of \"TV Guide\" described \"Empire\" as a \"sudsy retooling of \"King Lear\" with hip hop as the backdrop and praised Henson for her portrayal of the character Cookie.\n\nThe show's premiere ranked as Fox's highest-rated debut in three years. Viewership has increased continuously; \"Empire\" is the first primetime broadcast series in at least 23 years to have its viewership increase week to week for its first five episodes. The show continues to increase its viewership with further episodes. Episodes of the show have also been heavily watched on Video on Demand and other streaming services. As of its first season finale, \"Empire\" has now surpassed \"The Big Bang Theory\" as the highest rated scripted program in the 2014–2015 television season. The first season finale is also the highest rated debut season finale since May 2005, when \"Grey's Anatomy\" ended its first season. \"Empire's\" season one finale grew 82 percent from its series premiere, making it the show that has grown the most over the course of its first season since \"Men in Trees\" during the 2006–2007 season.\n\n\n\n"}
{"id": "37077875", "url": "https://en.wikipedia.org/wiki?curid=37077875", "title": "Energy Procedia", "text": "Energy Procedia\n\nEnergy Procedia is a peer-reviewed scientific journal published by Elsevier. It is abstracted and indexed in EI-Compendex, Engineering Index, and Scopus. The journal publishes conference proceedings dealing with all aspects of research on energy.\n"}
{"id": "51171662", "url": "https://en.wikipedia.org/wiki?curid=51171662", "title": "Flok (company)", "text": "Flok (company)\n\nflok (formerly Loyalblocks) is an American tech startup based in New York City that provides marketing solutions such as chatbots/AI, customer loyalty programs, mobile apps and CRM services to local businesses.\n\nIn January 2017, the company was acquired by Wix.com.\n\nflok was founded in 2011 by Ido Gaver and Eran Kirshenboim and has offices in Tel Aviv, Israel. In May 2013, flok secured a $9 million Series A Round from General Catalyst Partners with participation from Founder Collective and existing investor Gemini Israel Ventures. In total, flok has raised over $18 million in venture capital in three rounds.\n\nIn May 2014, flok announced a self-service loyalty platform for SMBs to build their own programs with beacon integration. At that time, approximately 40,000 businesses were using the service. In 2016, flok released a turnkey chatbot service for local businesses, and was featured in AdWeek for developing the first \"weed bot\" chatbot for a California cannabis business. \n\nflok offers an eponymous customer-facing app that consumers use to receive rewards and deals form partner businesses, and a flok business app for merchants to manage the platform. Both are available for iOS and Android operating systems. flok's main products center around customer loyalty and rewards. The platform offers a digital punch card, proximity marketing, push messaging and CRM services in addition to its chatbots and AI features.\n\nflok provides partner businesses with beacons that can locate customers and verify store visits. As of July 2016, flok had a 4-star rating on Merchant Maverick review site.\n"}
{"id": "46671034", "url": "https://en.wikipedia.org/wiki?curid=46671034", "title": "Free-piston linear generator", "text": "Free-piston linear generator\n\nThe free-piston linear generator (FPLG), is a free-piston engine coupled with a linear alternator. It converts chemical energy from fuel into electric energy. Because of its versatility, low weight and good efficiency, it can be used in a wide range of applications, although it is of special interest to the mobility industry as range extenders for electric vehicles.\n\nThe free-piston engine linear generators can be divided in 3 subsystems:\nThe FPLG has many potential advantages compared to traditional electric generator powered by an internal combustion engine. One of the main advantages of the FPLG comes from the absence of crankshaft. It leads to a smaller and lighter engine with fewer parts. This also allows a variable compression ratio, which makes it possible to design an engine that works ideally with different kinds of fuel.\n\nThe linear generator also allows the control of the resistance force, and therefore a better control of the piston's movement and of the combustion. The total efficiency (including engine and generator) of free-piston linear generators is around 40%, representing a significant improvement compared to conventional combustion engines, which reach about 20% under typical US driving conditions.\n\nThe first patents of free-piston linear generators date from around 1940, however in the last decades, especially after the development of rare-earth magnets, many different research groups have been working in this field.\nThese include:\n\nAlthough there is a variety of names and abbreviations for the technology, the terms \"Free-piston linear generator\" and \"FPLG\" particularly refer to the project at German Aerospace Center.\n\nThe free-piston linear generator generally consists of three subsystems: combustion chamber, linear generator and return unit (normally a gas spring), which are coupled through a connecting rod.\n\nIn the combustion chamber, a mixture of fuel and air is ignited, increasing the pressure and forcing the moving parts (connection rod, linear generator and pistons) in the direction of the gas spring. The gas spring is compressed, and, while the piston is near the bottom dead center (BDC), fresh air and fuel are injected into the combustion chamber, expelling the exhaust gases.\n\nThe gas spring pushes the moving parts assembly back to the top dead center (TDC), compressing the mixture of air and fuel that was injected and the cycle repeats. This works in a similar manner to the two-stroke engine, however it is not the only possible configuration.\n\nThe linear generator can generate a force opposed to the motion, not only during expansion but also during compression. The magnitude and the force profile affect the piston movement, as well as the overall efficiency.\n\nThe FPLG has been conceived in many different configurations, but for most applications, particularly for the automotive industry, focus has been on two opposed pistons in the same cylinder with one combustion chamber with a gas spring at the end of each cylinder. This balances out the forces in order to reduce vibration and noise. In the simplest case, a second unit is just a mirror of the first, with no functional connection to the first. Alternatively, a single combustion chamber or gas spring can be used, allowing for a more compact design and easier synchronization between the pistons.\n\nThe gas spring and combustion chamber can be placed on the ends of the connection rods, or they to share the same piston, using opposite sides in order to reduce space.\n\nThe linear generator itself has also many different configurations and forms. It can be designed as round tube, a cylinder or even flat plate in order to reduce the center of gravity, and/or improve the heat dissipation.\n\nThe free-piston linear generator's great versatility comes from the absence of a crankshaft, removing a great pumping loss, giving the engine a further degree of freedom. The combustion can be two-stroke engine or four-stroke engine. However, a four-stroke requires a much higher intermediate storage of energy, the rotational inertia of the crankshaft, to propel the piston through the four strokes. With the absence of a crankshaft, a gas spring would need to power the piston through the intake, compression and exhaustion stokes. Hence the reason why most of the current research focuses on the two-strokes cycle.\n\nSeveral variations are possible for combustion:\n\nThe Institute of Vehicle Concepts of the German Aerospace Center is currently developing a FPLG (or Freikolbenlineargenerator - FKLG) since 2002, and has published several papers about this subject.\n\nDuring the first few years of research, the theoretical background along with the 3 subsystems were developed separately. In 2013, the first entire system was built and operated successfully.\nThe German center is currently into its 2nd version of the entire system, on which two opposed cylinders will be used in order to reduce vibration and noise, making it viable for the automotive industry.\n\n\n"}
{"id": "39830911", "url": "https://en.wikipedia.org/wiki?curid=39830911", "title": "Function-Behaviour-Structure ontology", "text": "Function-Behaviour-Structure ontology\n\nThe Function-Behaviour-Structure ontology – or short, the FBS ontology – is an ontology of design objects, i.e. things that have been or can be designed. The Function-Behaviour-Structure ontology conceptualizes design objects in three ontological categories: function (F), behaviour (B), and structure (S). The FBS ontology has been used in design science as a basis for modelling the process of designing as a set of distinct activities. This article relates to the concepts and models proposed by John S. Gero and his collaborators. Similar ideas have been developed independently by other researchers.\n\nThe ontological categories composing the Function-Behaviour-Structure ontology are defined as follows:\nThe three ontological categories are interconnected: Function is connected with behaviour, and behaviour is connected with structure. There is no connection between function and structure.\n\nThe Function-Behaviour-Structure ontology is the basis for two frameworks of designing: the FBS framework, and its extension, the situated FBS framework. They represent the process of designing as transformations between function, behaviour and structure, and subclasses thereof.\n\nThe original version of the FBS framework was published by John S. Gero in 1990. It applies the FBS ontology to the process of designing, by further articulating the three ontological categories. In this articulation, behaviour (B) is specialised into \"expected behaviour\" (Be) (the \"desired\" behaviour) and \"behaviour derived from structure\" (Bs) (the \"actual\" behaviour). In addition, two further notions are introduced on top of the existing ontological categories: \"requirements\" (R) that represent intentions from the client that come from outside the designer, and \"description\" (D) that represents a depiction of the design created by the designer. Based on these articulations, the FBS framework proposes eight processes claimed as fundamental in designing, specifically:\n\n\nThe eight fundamental processes in the FBS framework are illustrated using a turbocharger design process.\n\n\nThe situated FBS framework was developed by John S. Gero and Udo Kannengiesser in 2000 as an extension of the FBS framework to explicitly capture the role of situated cognition or situatedness in designing.\n\nThe basic assumption underpinning the situated FBS framework is that designing involves interactions between three worlds: the external world, the interpreted world and the expected world. They are defined as follows:\n\nThe three worlds are interconnected by four classes of interaction:\n\nThe situated FBS framework is a result of merging the three-world model of situatedness with the original FBS framework, by specialising the ontological categories as follows:\n\n20 processes connect these specialised ontological categories. They elaborate and extend the eight fundamental processes in the FBS framework, providing more descriptive power with regards to the situatedness of designing.\n\n\nThe FBS ontology has been used as a basis for modelling designs (the results of designing) and design processes (the activities of designing) in a number of design disciplines, including engineering design, architecture, construction and software design. While the FBS ontology has been discussed in terms of its completeness, several research groups have extended it to fit the needs of their specific domains.\nIt has also been used as a schema for coding and analysing behavioural studies of designers.\n\n"}
{"id": "15387812", "url": "https://en.wikipedia.org/wiki?curid=15387812", "title": "Governance Interoperability Framework", "text": "Governance Interoperability Framework\n\nThe Governance Interoperability Framework (GIF) is an open, standards-based specification and set of technologies that describes and promotes interoperability among components of a service-oriented architecture (SOA). GIF integrates SOA ecosystem technologies to achieve heterogeneous service lifecycle governance and is supported by Hewlett-Packard Company and by GIF partners.\n\nGovernance is recognized as a foundational requirement for successful enterprise adoption of SOA: Gartner has stated that “governance isn’t an option but an imperative”, and predicts that the dominant mode of SOA project failure will be a lack of adequate governance.\n\nThe primary products used by most organizations to achieve SOA governance are based on an integrated registry-repository, and provide support for managing and communication information in an SOA as well as automating key governance activities. These SOA governance systems provide a central system-of-record for all services and related information in an SOA, and are the place where services can be advertised by providers and discovered by consumers. As such, they act as a key control point for governing service availability, versioning, service lifecycle management, and for ensuring compliance with business and technical policies.\n\nTo be effective, SOA governance systems need a mechanism for exchanging information between all the disparate technologies that support an SOA. Interoperability is a fundamental requirement for the visibility, trust and control required for effective SOA governance. The objective of GIF is to drive interoperability through the adoption of standards and common approaches to modeling and exchanging information.\n\nGIF represents a collection of APIs defined by standards organizations, data mappings and classifications and leverages UDDI and WS-Policy standards, among others, as building blocks. In order to promote commonality of approaches and understanding of the information represented, GIF also defines vocabularies for the purpose of applying metadata to service information.\n\nIntegration with the Governance Interoperability Framework is based on two primary pillars of integration: Control Integration and Service Data Integration. These themes are based on the famed Model-View-Controller (MVC) pattern:\n\nControl integration - Consists of alerting and notification integration; launching events and actions; and integration of business service governance and lifecycle.\n\nData integration - Consists of leveraging the Business Service Registry as the primary service description, characteristic, and policy catalog.\n\nGIF provides control and data integration needed to support activities such as the governance of business service provisioning and lifecycle management. Aspects of this are:\n\nProvisioning integration - Leverage the SOA governance system as part of the provisioning and deployment process of business services. Once integrated, bi-directional exchange of service information between participants is enabled.\n\nDeployment integration - Upon deployment of services, any party should have the ability to alert others to the existence of the service and the need to put the service and its definitions under management.\n\nLifecycle management - Lifecycle management of all facets of a business service is required. This means collaborating and integrating components for the purpose of managing:\n\nGIF has been driven by several use cases, including:\n\n\nGIF is not a standard itself, but rather leverages existing standards to support SOA governance interoperability. GIF is supported by Hewlett-Packard Company and by GIF partners. For more information about the GIF specification, existing GIF partners and how to join GIF, visit HP’s website.\n\n\nInformation about the Governance Interoperability Framework\n\n"}
{"id": "8264048", "url": "https://en.wikipedia.org/wiki?curid=8264048", "title": "Group technology", "text": "Group technology\n\nGroup technology or GT\n\nis a manufacturing technique in which parts having similarities in geometry, manufacturing process and/or functions are manufactured in one location using a small number of machines or processes. Group technology is based on a general principle that many problems are similar and by grouping similar problems, a single solution can be found to a set of problems, thus saving time and effort.\n\nThe group of similar parts is known as part family and the group of machineries used to process an individual part family is known as machine cell. It is not necessary for each part of a part family to be processed by every machine of corresponding machine cell. This type of manufacturing in which a part family is produced by a machine cell is known as cellular manufacturing. \nThe manufacturing efficiencies are generally increased by employing GT because the required operations may be confined to only a small cell and thus avoiding the need for transportation of in-process parts.\n\nGroup technology is an approach in which similar parts are identified and grouped together in order to take advantage of the similarities in design and production. Similarities among parts permit them to be classified into part families.\n\nThe advantage of GT can be divided into three groups:\n\nDisadvantages of GT Manufacturing :\n"}
{"id": "18580803", "url": "https://en.wikipedia.org/wiki?curid=18580803", "title": "Hell's Kitchen (U.S. TV series)", "text": "Hell's Kitchen (U.S. TV series)\n\nHell's Kitchen is an American reality competition television series based on the British series of the same name. Broadcast on Fox, it is hosted by celebrity chef Gordon Ramsay. Two teams of chefs compete for a job as head chef at a restaurant.\n\nA typical episode begins with a challenge between the teams; the winning team gets some sort of a reward that usually involves some sort of recreational activity outside the Hell's Kitchen, while the losers are given a \"punishment\" that usually involves some sort of menial task.\n\nThe second part is a dinner service, where each team works in their own kitchen, attempting, under close and hypercritical supervision from Chef Ramsay, to complete the service without getting kicked out before finishing for too many errors.\n\nThe usual wrap-up involves whichever team is declared the loser of the dinner service choosing two of their own to be up for elimination.\n\nThere are many possible variations; both teams being kicked out before the end of dinner service is an example that has occurred on several occasions.\n\nOn September 9, 2016, Fox renewed \"Hell's Kitchen\" for seasons 17 and 18. The seventeenth season, titled \"All-Stars\", premiered on September 29, 2017. The eighteenth season, titled \"Rookies vs. Veterans\", premiered on September 28, 2018.\n\n\"Hell's Kitchen\" is a reality television show that uses a progressive elimination format to narrow down a field of 12 to 20 aspiring chefs to a single winner over the course of one season. The U.S. version of \"Hell's Kitchen\" follows the format of the UK version though the show is recorded and not performed live, nor is there audience participation in the elimination of chefs. The show is produced at Hell's Kitchen, a modified warehouse in Los Angeles that includes the restaurant, dual kitchen facilities and a dormitory where the chefs reside while on the show. They are also given knife sets that they get to keep, regardless of their progress.\n\nAt the start of each season, Gordon Ramsay breaks the chefs into two teams. With the exception of the first and most recent seasons, this puts women on the red team and men on the blue team; each is given a chef's jacket with panels of that color on the shoulders. The chefs remain on these teams throughout most of the competition; Ramsay may reassign a chef to the other team if the team numbers are uneven or if he feels the chef will perform better on the other team. Each episode typically includes a challenge and a dinner service, followed by the elimination of a chef. When only five or six chefs remain, they are brought into a single common team wearing black-panelled jackets. From this point onward, they compete individually to be one of the final two.\n\nIn challenges, the teams or individual is tasked with a cooking challenge by Ramsay. The type of challenges are varied, including ingredient preparation, meal preparation and taste tests. The first challenge of each season is a signature dish cook-off, giving the chefs an opportunity to show Ramsay their cooking.\n\nEach season typically includes one or more challenges that allows teams to construct several dishes either for a banquet to be held the next dinner service or as part of designing their own menus. Other challenges typically include a \"taste it, now make it\" task, where chefs must attempt to recreate a dish Ramsay has prepared after tasting it only, and a \"blind taste-test\" where chefs identify ingredients while blindfolded and wearing sound-blocking earcovers. Some challenges have been full breakfast or lunch services, where the team completing the service first is declared the winner.\n\nThe winner of the challenge is determined by either a scoring system set for that challenge or by Ramsay's and/or guest judges' opinions. The winning team or chef receives a reward (a recreational activity away from Hell's Kitchen and other potential prizes), while the losing team or chefs are forced to do a mundane task, such as cleaning the kitchens, preparing a specific ingredient for the following dinner, having to prepare the food for both kitchens, and sometimes eating something unsavory (such as food waste blended into a smoothie) for lunch.\n\nFor dinner services, the chefs are expected to work their station (such as appetizers, meat, fish, or garnish) on the kitchen line to prepare food in coordination with their teammates and to Ramsay's high standards for quality and presentation. Dinner service is for about 100 guests (volunteers for the show), with each diner expecting to receive an appetizer, an entree, and a dessert. The chefs are given menus and recipe books by Ramsay to study and memorize, which include some of Ramsay's more difficult dishes including risotto and Beef Wellington. The chefs spend several hours before each service preparing their ingredients.\n\nMenus may be customized for a specific dinner service, such as ethnic-themed dishes or plates that resulted from the earlier challenge. Some seasons feature a service allowing for the teams to develop their own menus, which are reviewed by Ramsay for quality and presentation beforehand. Later seasons may feature a private dinner service, where each team must serve a five course meal to 12 guests, with each member leading their teammates to prepare one course. Dinner services may include additional challenges. A chef from each team may be asked to serve a table-side meal for their team, serve celebrities sitting at the kitchen's chef's table, or act as a server for the evening taking and fulfilling orders. After the chefs are on a single black team, Ramsay will use one dinner service to ask each chef to run the pass to test their quality control, including deliberate mistakes made by the sous chefs or Ramsay himself.\n\nDuring a service, Ramsay demands that all orders for each course for a table go out together, and will send back entire orders if one item is improperly prepared, such as being over- or undercooked or not seasoned correctly. While the chefs are in two teams, Ramsay is assisted by two trusted sous-chefs, each monitoring one of the kitchens, demanding the same standards and alerting Ramsay to any issues. Ramsay's goal is to complete every dinner service, but exceptionally poor kitchen performance by one or both teams will cause him to close one (or both sides) of the kitchen early and send the team(s) back to the dorms. Ramsay may also evict individual chefs from the kitchen based on repeated poor performances during a service, and on rare occasions (once every two seasons on average), may eliminate a chef on the spot.\n\nOnce the dinner service is complete, Ramsay gathers everyone in the kitchen, announces which team is the losing team, and directs them to select two chefs from their team as nominees for elimination. Often, both teams are declared losers, or a different number of chefs may be requested for nomination. In some cases, Ramsay has named both teams winners, but still requires both teams to nominate someone for elimination. This is a group consensus, but Ramsay may occasionally name a chef \"best of the worst\" on their team and instruct them to choose the nominees. This concept, however, has faded away over time due to the \"best of the worst\" sometimes making nominations based on personal bias rather than kitchen performance. Ramsay has also on some occasions declared that nobody would be sent home, but those cases are generally followed by a double-elimination, a team reassignment, or occur after someone has been sent home immediately due to malfeasance.\n\nRamsay reassembles the teams in the dining hall and hears out the nominations from the losing team(s). Ramsay may also nominate other chefs for elimination if he believes it appropriate. After giving these nominees the chance to defend themselves, Ramsay selects one to hand over their jacket and \"leave Hell's Kitchen.\" On rarer occasions, Ramsay can overrule nominations or even eliminate a chef who has not been nominated, even a chef on a winning team.\n\nThe eliminated chef is shown leaving the restaurant, providing some last thoughts on the experience. After dismissing the chefs, Ramsay goes back upstairs to his office. He symbolically hangs the chef's jacket on a sharp hook below their picture in a row with the others, igniting the chef's picture and signaling their departure. During this scene, there is a voiceover of Ramsay explaining his reasons for eliminating the chef. If an eliminated chef has performed exceptionally well, Ramsay may allow them to keep their jacket as a token of their success up to that point, if he sees fit.\n\nChefs may be eliminated from the competition due to medical reasons, both voluntarily and involuntarily. Chefs that violate the competition's rules may be immediately eliminated. Chefs may also exit the competition voluntarily for any other reason; though this is not encouraged, their wishes are ultimately granted (with reasons by Ramsay explained, if applicable).\n\nOnce the number of chefs drops below a certain level (usually once 5 or 6 are left) they are awarded black jackets and assembled into a single team. Eliminations continue until two are left.\n\nIn the finale, the final two chefs are each given the opportunity to develop their own menus and lead a brigade of former competitors through a full dinner service on their own. In the first five seasons, this included the opportunity to decorate half of the Hell's Kitchen restaurant to their liking. Prior to the dinner service, the two chefs compete in a challenge to prepare their menus, and the winner will earn the advantage of picking their brigade of chefs first. Ramsay will ensure that all menu items meet his standards for high cuisine prior to service, and he and his sous chefs will oversee the service to make sure that his high quality standards are retained, but does not otherwise get involved, allowing the two remaining chefs to demonstrate their ability to run the line. The finalists are allowed to reassign stations, or even kick their teammates out of the kitchen should they see fit; the latter has happened four times in the show's history.\n\nRamsay uses his own observations and those from the diners and other sources to decide who is the winning chef. He has two doors in his office leading out to the balcony above the Hell's Kitchen seating area. Each chef stands at a door and Ramsay tells them to both turn their handles at the same time. After a commercial break, only the door of the winning chef is unlocked allowing the winner to walk through and be greeted by the crowd below. The winning chef receives two prizes including the opportunity to work as the head chef or executive chef at a restaurant of Ramsay's choosing, as well as a cash prize of $250,000. In a similar manner to the voiceover at each elimination, Ramsay has a voiceover to explain his reasons for choosing that chef as the winner.\n\nGordon Ramsay is the head chef. Jason Thompson is the narrator. Jean-Philippe Susilovic, a Belgian maître d'hôtel, comes from Petrus, one of Ramsay's London restaurants and appeared in the first seven seasons and later returned for Season 11. He left after Season 12 and was replaced by Marino Monferrato for Season 13. Susilovic was also the maître d'hôtel for the first series of the original British version. James Lukanik replaced Susilovic for Seasons 8–10. Each team also has the services of one of two sous-chefs. The current sous-chefs are Season 10 winner Christina Wilson and James \"Jocky\" Petrie. Previous sous-chefs were Mary Ann Salcedo, Gloria Felix, Season 2 winner Heather West, Scott Leibfried, James Avery, Aaron Mitrano and Andi Van Willigan. In Season 15, Wilson filled in for Van Willigan-Cutspec who was getting married at the time of filming but returned for one episode when her reception was one of the themed dinner services for that season. Van-Willigan returned in season 16 but was replaced by Wilson again for Seasons 17 and 18 to spend time with her infant son.\n\nThe theme song is \"Fire\" by the Ohio Players. When the U.S. version is broadcast in the U.K., Italy, Portugal and some countries (shown on the table below), it features only the instrumental version. The instrumental version also appeared in the uncensored DVD release for the U.S. version.\n\nFor the show's first two seasons, the \"Hell's Kitchen\" restaurant set itself was housed in the former studios of Los Angeles television station KCOP at 915 North La Brea Avenue, in Hollywood, which at one time hosted production of game shows \"Tic Tac Dough\" and \"The Joker's Wild\". KCOP was acquired by News Corporation in 2001 and its studios were integrated with those of Fox affiliate KTTV in 2003, leaving the La Brea facility vacant. Originally the studio was put up for sale, but in the end they were retooled for the production of \"Hell's Kitchen\". The dining room area was the location of the former KCOP news studios, and living quarters for the contestants were built behind the restaurant. Before season three, the Hell's Kitchen facility was moved to Century Studios at 3322 La Cienega Place in Los Angeles. Since the fourth season, Hell's Kitchen's venue has been located at 8660 Hayden Place in Culver City. According to Arthur Perkins, the soundstage is only open for audience members when taping is taking place. The studio sits on the former location of the famous RKO Forty Acres backlot, which was used in movies such as \"Gone With The Wind\" and television series such as \"The Andy Griffith Show\" and \"Adventures of Superman\". The studio building sits on the location of the military camp seen in the television series \"Gomer Pyle U.S.M.C.\".\n\nThe series has drawn numerous online and editorial accusations of staging and dramatic license, mostly due to editing techniques of the producers, who splice together several hours of footage from a dinner service, in order to make certain contestants appear as poor performers, later justifying their elimination. This was most obvious when one episode featured clips showing an already eliminated contestant in the background, still cooking.\n\nOne of the most controversial accusations of staging on \"Hell's Kitchen\" relates to an incident with contestant Joseph Tinnelly from Season 6, who, during one elimination round, angrily confronted Ramsay, challenging him to fight, and was then escorted off the set. The incident drew immediate fire from critics as an overplayed and possibly faked scene, conducted to cause action and tension on the show in order to spark viewer interest.\n\nIn a 2013 interview, Ramsay admitted that the vast majority of his anger towards contestants was acting, his on-camera rage being done to force contestants to give their all and that when off-camera he treated all contestants with friendliness and respect. Ramsay further stated he would often meet with eliminated contestants after their departure from the show, offering positive criticism and advice. Staff and production members further affirmed that in real life, Ramsay was a sociable and friendly person and most of the on-camera drama is played out for the benefit of the audience.\n\n\"Hell's Kitchen\" has been nominated for three Primetime Emmy Awards in the Outstanding Art Direction for Variety, Music or Nonfiction Programming category in 2007, 2008, and 2009. It has also been nominated for two Art Directors Guild Awards in the Television — Awards Show, Variety, Music or Non-Fiction Program category in 2007 and 2008, winning one in 2008. It has also been nominated for a Teen Choice Award for Choice Summer Series.\n\nIn 2009, Gordon Ramsay won an Astra Award for Favourite International Personality or Actor.\n\nAt the 2011 People's Choice Awards, \"Hell's Kitchen\" was nominated for Favorite Reality Show and Gordon Ramsay was nominated for Favorite TV Chef.\n\nAt the 2014 Reality TV Awards ceremony, \"Hell's Kitchen\" won an award for best new cast. In 2015, \"Hell's Kitchen\" won awards for best overall show and guilty pleasure at the 2015 Reality TV Awards.\n\nVisual Entertainment has not released the first fourteen seasons of \"Hell's Kitchen\" in Region 1. Season 14 was released on March 15, 2016, and to the Blu-ray format for the first time.\n\nIn Region 4, Shock Entertainment has released seasons 1–8 on DVD in Australia.\n\nOn September 11, 2008, Ubisoft released \"\" for the Wii, Nintendo DS, Microsoft Windows, and iOS, which features the likeness of Ramsay, and the many important tasks shown in the U.S. version of the show.\nOn April 2, 2009, Ludia and Social2u released the official Facebook version of the \"Hell's Kitchen\" video game.\n\n"}
{"id": "58697476", "url": "https://en.wikipedia.org/wiki?curid=58697476", "title": "Hindou Oumarou Ibrahim", "text": "Hindou Oumarou Ibrahim\n\nHindou Oumarou Ibrahim is an environmental activist and geographer. She is the Coordinator of the Association of Peul Women and Autochthonous Peoples of Chad (AFPAT) and served as the co-director of the pavilion of the World Indigenous Peoples’ Initiative and Pavilion at COP21, COP22 and COP23.\n\nIbrahim is an environmental activist working on behalf of her people, the Mbororo in Chad. She was educated in Chad's capital city of N'Djamena and spent her holidays with the indigenous Mbororo people, who are traditionally nomadic farmers, herding and tending cattle. During the course of her education, she became aware of the ways in which she was discriminated against as an indigenous woman and also of the ways in which her Mbororo counterparts were excluded from the educational opportunities she received. So in 1999, she founded the Association of Indigenous Peul Women and Peoples of Chad (AFPAT), a community-based organization focused on promoting the rights of girls and women in the Mbororo community and inspiring leadership and advocacy in environmental protection. The organization received its operating license in 2005 and has since participated in international negotiations on climate, sustainable development, biodiversity, and environmental protection.\n\nHer focus on environmental advocacy stemmed from her firsthand experience of the effects of global climate change on the Mbororo community, who rely on natural resources for their own survival and for the survival of the animals they care for. For years, they have been experiencing the effects of Lake Chad drying up; the lake is a vital source of water for people from Chad, Cameroon, Niger and Nigeria, and is now 10% of its size from the 1960s. In a written testimony to the International Organization for Migration, Ibrahim emphasized that her people, and indigenous communities like her own, are \"direct victims of climate change,\" which has worked to displace them, forcing them to abandon their own lands in search of ones that can sustain their way of life. In that testimony, she also spoke of the consequences of climate change migration, which disproportionately leaves migrant communities vulnerable.\n\nIbrahim has written on the importance of recognizing indigenous people's rights when crafting global climate change for a variety of outlets, including Quartz and the World Economic Forum's Agenda. Of particular concern to Ibrahim is the legal right of indigenous peoples to own and manage the lands where they live. Such legal rights guarantee that indigenous community have legal agency in economic developments that might displace them, such as oil drilling projects, mining, and hydropower plants.\n\nIbrahim has worked collaboratively with UNESCO and the Indigenous Peoples of Africa Coordinating Committee (IPACC) on a project to 3D map Chad's Sahel desert region, where 250,000 Mbororos currently live, relying on subsistence farming. The project combined 3D mapping technologies with indigenous scientific knowledge to develop a tool to sustainably manage the environment and empower indigenous voices—particularly those of women—to make decisions on planning for a future of climate adaptation and mitigation. In an interview with BBC for \"BBC's 100 Women\" project, Ibrahim noted: \"Every culture has a science. So it's really important for the indigenous voice to be there.\"\n\nIn 2016, Ibrahim was selected to represent civil society at the signing of the historic Paris Climate Agreement on April 22, 2016. In her statement at the signing, she noted: \"Climate change is adding poverty to poverty every day, forcing many to leave home for a better future.\"\n\nIbrahim serves in a number of leadership capacities advocating for the importance of indigenous knowledge in mitigating the effects of climate change. She is co-Chair of the International Indigenous Peoples Forum on Climate Change, representing the group at the United Nations Convention to Combat Desertification (UNCCD) and of the Pan-African Alliance Climate Justice (PACJA), where she also acts as the chair of recruitment. She is also a member of the Policy Board United Nations: Indigenous Peoples Partnership (UNIPP) and of the Executive Committee for the Indigenous Peoples of Africa Coordinating Committee (IPACC).\n\nIn 2017, Ibrahim was recognized as a National Geographic Society Emerging Explorer, a program that recognizes and supports outstanding scientists, conservationists, storytellers, and innovators. In 2017, she was also featured as part of BBC's \"100 Women\" project, recognizing 100 influential and inspiring women every year.\n"}
{"id": "4616717", "url": "https://en.wikipedia.org/wiki?curid=4616717", "title": "Hush-A-Phone", "text": "Hush-A-Phone\n\nThe Hush-A-Phone was a device designed to attach to the transmitter of a telephone to reduce noise pollution and increase privacy. Sold by the Hush-A-Phone company, the device was frequently described in its commercial advertisements as \"a voice silencer designed for confidential conversation, clear transmission and office quiet. Not a permanent attachment. Slips right on and off the mouthpiece of any phone\".\n\nThe device was the topic of a landmark court case, \"Hush-A-Phone v. United States\". The Hush-A-Phone was regularly referred to in telecommunications policy analysis in the 1980s, attracting renewed interest in the 2000s as a symbol of a small company fighting against a monopoly, especially in the context of net neutrality. Indeed, because Hush-A-Phone eventually won its case against the phone company, the final legal proceedings involving the Hush-A-Phone turned out to be relevant to the eventual breakup of the Bell system.\n\nAdvertisements for the Hush-A-Phone not only argued for its importance as an aid to privacy, but also noted the device improved clarity of sound, which AT&T would directly argue against.\n\nThe manufacture of Hush-A-Phones began in 1921, although the Hush-A-Phone company was first mentioned in \"The New York Times\" in a 1922 classified advertisement for a \"typist-dictaphone operator\". At this time, Hush-A-Phone was located in New York's Flatiron District, at 41 Union Square. Only a month later, the company advertised for a salesman, noting that 500 Hush-A-Phones were sold in one week at a business show.\n\nThe company was still seeking a salesman in April 1922, but stopped posting dedicated sales openings until January 1923, this time noting several thousand Hush-A-Phones had already sold in New York. The company's first classified advertisement for the product appeared June 7, 1922, pricing the product at $10 and offering a free 5-day trial offer. Between the end of June 1922 and January 16, 1923, the company moved eleven blocks closer to the Empire State Building, to 1182 Broadway, and the \"free trial\" changed to \"free demonstration offer\". A capital increase to the \"Hush-A-Phone Sales Corp.\" company was announced on December 22, 1922, from $250,000 to $500,000, and in March 1923, the company's name changed from Hush-A-Phone Sales Corp., Manhattan, to Hush-A-Phone Corp.\n\nSome time between May 30, 1923, and October 18, 1923, Hush-A-Phone moved halfway back toward its original Union Square location, to 10 Madison Avenue, and by May 1924, the company had started suggesting that potential customers outside of New York wanting a demonstration would instead be sent a booklet.\n\nThe Wall Street Crash of 1929 brought trouble to many companies. On October 20, 1929, Hush-A-Phone was advertised along with several other companies on the first page of \"The New York Times\" as part of the \"National Business Show\" being held in Grand Central Palace from October 21 to 24. The company was showing its handset model for the first time. The ad noted that Mr. H. C. Tuttle, President of the Hush-A-Phone Corporation, had just returned from a European tour of ten countries where the product would be distributed. The product was described as being \"beautiful,\" made of bakelite, and \"embellished with a work of art in bas-relief. It appears as a handsome desk clock, nine inches high, concealing its function as a Hush-A-Phone\".\n\nSome time between October 1927 and December 1929, Hush-A-Phone moved from its Madison Avenue location to about seven blocks southwest to 43 W. 16th Street. Although one more advertisement appeared in 1929 (December 8, just in time for the holidays), Hush-A-Phone was absent from the \"Times\" until July 1934, when a four-line, text-only advertisement appeared. Advertisements in 1936 noted a new model \"for French phone\" was out, and in October 1937 the Hush-A-Phone company was exhibiting again, this time showing a 200-foot elastic telephone wire at the National Business Show. However, the four-line classified advertisements continued to be the company's public appearances after the show, appearing between ads for cigars and baldness cures, until 1942, when their product appeared in photographs in a few ads run by houseware store Lewis & Conger. In 1944 the company noted \"Models for E-1 and F-1 Handset Phone; Pedestal Phone; Switchboard and Dictating Machines\".\n\nIn 1945, Hush-A-Phone ads began appearing in \"The Washington Post\", and Hush-A-Phone consulted with acoustics expert Leo Beranek at MIT, who began work to design an improved silencer. Beranek would later bring in J. C. R. Licklider to help demonstrate the Hush-A-Phone retained clarity of sound.\n\n125,796 Hush-A-Phone sets were sold between 1922 and 1949.\n\nDuring the 1940s, telephone service was seen as a \"natural monopoly\", and AT&T was the sole provider of all aspects of telephone service in much of the U.S., including telephone equipment. In the late 1940s, phone company repairmen began warning customers that using devices like the Hush-A-Phone could result in termination of phone service.\n\nOn December 22, 1948, Hush-A-Phone and Harry C. Tuttle, its president, protested to the Federal Communications Commission (FCC), asking them to order the phone company to authorize use of the device. The hearing occurred in 1950, but the original hearing examiner involved died, delaying the initial recommendation.\n\nSome time between May 3 and 12, 1949, the company moved a few doors down, to 65 Madison Ave., and occasionally advertisements exceeded the four-line standard, in Oct 1949 offering free tickets to the \"Business Show.\"\n\nIn February 1951, the FCC decided Hush-A-Phone's complaint should be dismissed, but held the case open for the next seven years, permitting further pleadings and reconsideration. A letter to the editor of \"The Washington Post\" by John P. Roberts, a communications engineer, described the FCC decision \"an invasion of the rights of the individual\", adding \"even if this quality deterioration had been satisfactorily demonstrated, it is hard to understand why the FCC should have the power to forbid my use of the Hush-A-Phone if I choose to accept the deterioration in quality for the sake of increased privacy\". On March 23, 1951, Hush-A-Phone and Harry C. Tuttle submitted filings to the FCC reporting scientific tests proving that the Hush-A-Phone \"actually causes a net increase in 'transmission efficiency of the telephone circuit\" and that AT&T and affiliates were \"public utility monopolies unlawfully interfering with the natural and inherent rights of a subscriber\". FCC official Jack Werner's suggestion was that the telephone company should suspend service to any consumer failing to comply with the regulation prohibiting foreign attachments.\n\nThe FCC's final decision was issued on December 23, 1955, and stated \"The unrestricted use of the 'Hush-A-Phone' could result in a general deterioration of the quality of interstate and foreign telephone service. Accordingly, it is not an unjust and unreasonable practice upon the part of the defendants to prohibit its use in connection with their telephone services.\" While the commission agreed that the Hush-A-Phone did provide protection against eavesdroppers and noise from telephone circuits, \"the device sometimes results in loss of voice intelligibility and also has an adverse affect [sic] on voice recognition and naturalness.\"\n\nThe FCC's 1955 decision was rejected by the U.S. Court of Appeals on November 8, 1956, in the landmark case \"Hush-A-Phone v. United States\", with the decision stating it was an \"unwarranted interference with the telephone subscriber's right reasonably to use his telephone in ways which are privately beneficial without becoming publicly detrimental\". The FCC followed up on February 6, 1957 to officially direct AT&T and Bell System subsidiaries to permit subscribers to use the Hush-A-Phone and similar devices. Advertisements proudly noted \"Use of the Hush-A-Phone on telephone is permitted by Federal Appellate Court ruling\" beginning in March 1957, and by July were stating \"Bell System Approves Use of Hush-A-Phone by tariffs Effective May 16, 1957\".\n\nHush-A-Phone was still featured in advertisements by the company during the early 1960s in \"The New York Times\", but their last direct ad seems to have been on March 13, 1962, after which the product was featured in catalog-type ads posted by stationer's store Goldsmith Brothers through 1970. In 1972, the last classified ad for Hush-A-Phone was listed by Harrison-Hoge Industries, Inc. for $13.95 in black and $15.95 in green, ivory, or beige. Hush-A-Phone appears to no longer be for sale except as a collector's item.\n\n\n"}
{"id": "39685337", "url": "https://en.wikipedia.org/wiki?curid=39685337", "title": "IET Achievement Medal", "text": "IET Achievement Medal\n\nThe Institution of Engineering and Technology (IET) awards achievement medals to recognize engineers who have been significant contribution to various fields in engineering\nEvery year, the award committee seeks and evaluates nominations and\nmakes decision on winners. There is no age limit or nationality requirement. It is an international award.\n\nThe awards are made to recognize specific fields in engineering:\n\n[1]. Information & Telecommunication\n\n[2]. Electronics\n\n[3]. Control Engineering\n\n[4]. Manufacturing\n\n[5]. Transport Engineering\n\n[6]. Environmental Engineering\n\n[7]. Energy\n\nThe IET Achievement Medals are named after prominent scientists and engineers.\nThey include:\n\n1. J. J. Thomson - for electronics\n\n2. John Ambrose Fleming - for communications\n\n3. R. E. B. Crompton - for energy\n\n4. Oliver Heaviside - for control\n\n5. Monty Finniston - for general engineering\n\n6. Sarah Guppy - for environment\n\n7. Eric Mensforth - for manufacturing\n\nThe awards are sponsored by several companies, such as BAE Systems, Lockheed Martin, \nBP, RS, Pace, e-ON, EDF energy networks, GCHQ, Transport for London, etc.\n\nEach year, the award is presented in London, UK.\n\nThe ceremony is attended \nby the IET President, distinguished guests, winners and their families and \nindustry sponsors of the event. Also at this ceremony, winners of \nthe IET Faraday Medal and recipient of IET Honorary \nFellow will also be announced on this occasion.\nSome photos taken at the award ceremony can be found here.\n\nAn overview of some of the medalists for the year 2000 to 2017 is shown below.\n\nWinners of J. J. Thomson Medal include John Cioffi (Stanford), Kam-Yin Lau (U C Berkeley).\nChancellor Sir John O'Reilly of Cranfield UK also won the medal in 2003.\nVincent Poor, Dean of Engineering at Princeton won the IET Ambrose Fleming Medal in 2010. Southampton wireless communications professor Lajos Hanzo received the Sir Monty Medal in 2008. \nBath-based Internet Pioneer Paul Kane won the medal in 2012. Prof. Ramesh Agarwal from Washington St. Luis got the IET Medal for Control in 2012.\nSimon Kingsley won the Ambrose Fleming Medal in 2007.\nRon Hui from University of Hong Kong won the IET medal in power in 2010.\nB L Weiss from University of Leeds won the IET medal in electronics.\nChristos Christopoulos from the University of Nottingham, UK won the IET Medal in Communications and Information in 2011\nLajos Hanzo from University of Southampton won IET Medal in engineering and technology in 2008.\nAsad M. Madni from UCLA, USA won the IET Medal in 2005.\nJohn Armitt from Olympic Delivery Authority (ODA) won the IET Sarah Guppy Medal in 2011. Ian Postlethwaite from University of Leicester won the medal in 2007 for control engineering.\nA full list of winners since 1987 can be found here.\n\n\n"}
{"id": "11923116", "url": "https://en.wikipedia.org/wiki?curid=11923116", "title": "Inframat Corporation", "text": "Inframat Corporation\n\nInframat Corporation is a nanotechnology company founded in 1996 with headquarters and a pilot plant / R&D facility in Manchester, CT. Product focus is in the areas of (i) medical and industrial nanocoatings, (ii) high surface area nanofibrous media for water treatment, and (iii) magnetic nanocomposites. Inframat has pioneered the Solution plasma spray (“SPS”) process, based on the substitution of solution for powder feedstocks in thermal spray systems. Inframat’s Advanced Materials subsidiary sells a variety of commercial materials to large companies and research institutions.\n\n"}
{"id": "23155814", "url": "https://en.wikipedia.org/wiki?curid=23155814", "title": "Jacketed vessel", "text": "Jacketed vessel\n\nIn chemical engineering, a jacketed vessel is a container that is designed for controlling temperature of its contents, by using a cooling or heating \"jacket\" around the vessel through which a cooling or heating fluid is circulated.\n\nIn specifying jacketed pressure vessels, the choice of a jacket usually comes down to economy, weight and versatility of the heating or cooling requirement. Heat transfer efficiency is another important consideration when deciding on which jacketed pressure vessel option best suits your needs. Each jacket option provides different benefits to the overall function and performance of the pressure vessel.\n\nA jacket is a cavity external to the vessel that permits the uniform exchange of heat between the fluid circulating in it and the walls of the vessel. There are several types of jackets, depending on the design:\n\nJackets can be applied to the entire surface of a vessel or just a portion of it. For a vertical vessel, the top head is typically left unjacketed. Jackets can be divided into zones, to divide the flow of the heating or cooling medium. Advantages include: ability to direct flow to certain portions of the jacket, such as only the bottom head when minimal heating or cooling is needed and the entire jacket when maximum heating or cooling is required; ability to provide a higher volume of flow overall (zones are piped in parallel) because the pressure drop through a zone is lower than if the entire jacket is a single zone.\n\nJacketed vessels can be employed as chemical reactors (to remove the elevated heat of reaction) or to reduce the viscosity of high viscous fluids (such as tar).\n\nAgitation can be also used in jacketed vessels to improve the homogeneity of the fluid properties (such as temperature or concentration).\n\nThe closed-loop heating system of a jacketed vessel allows for great convenience in batching processes. Many industries require that a vessel be both heated and cooled as part of a single process; a well-designed jacket with a thermal fluid system is a seamless way to achieve the necessary temperatures.\n\nGlass-clad vessels and setups that require frequent cleaning also benefit from jacketed systems, as it’s difficult to equip such vessels with internal coils for heating or cooling. Below are just a few of the many industries benefitting from the convenience and reliability of thermal fluid heating in jacketed vessels.\n\nJacketed Vessels are commonly used in the following industries:\n\n\n\n\n"}
{"id": "656589", "url": "https://en.wikipedia.org/wiki?curid=656589", "title": "LG Electronics", "text": "LG Electronics\n\nLG Electronics Inc. () is a South Korean multinational electronics company headquartered in Yeouido-dong, Seoul, South Korea, and is part of LG Corporation, employing 82,000 people working in 119 local subsidiaries worldwide. With 2014 global sales of USD 55.91 billion (KRW 59.04 trillion), LG comprises four business units: Home Entertainment, Mobile Communications, Home Appliance & Air Solution, and Vehicle Components, with Starion India as its main production vendor for refrigeration and washing machines in the Indian sub-continent. The CEO of LG Electronics is Koo Bon-joon, who assumed the role of vice chairman of LG Electronics on 1 October 2010.\nSince 2008, LG Electronics remains the world's second-largest television manufacturer.\n\nIn 1958, LG Electronics was founded as \"GoldStar\" (Hangul:금성). It was established in the aftermath of the Korean War to provide the rebuilding nation with domestically-produced consumer electronics and home appliances. LG Electronics produced South Korea's first radios, TVs, refrigerators, washing machines, and air conditioners. GoldStar was one of the LG groups with a brethren company, Lak-Hui (pronounced \"Lucky\") Chemical Industrial Corp. which is now LG Chem and LG Households. GoldStar merged with Lucky Chemical and LS Cable on 28 February 1995, changing the corporate name to Lucky-Goldstar, and then finally to LG Electronics.\n\nIn 1978, LG Electronics earned US$100 million in revenue from exports for the first time in its history. Rapid growth by globalization saw the company establish its first overseas production, based in the United States, in 1982. In 1994, GoldStar officially adopted the LG Electronics brand and a new corporate logo. LG Electronics acquired the US-based TV manufacturer Zenith. In 1995, LG Electronics made the world's first CDMA digital mobile handsets and supplied Ameritech and GTE in the US. The company was also awarded UL certification in the US. In 1998, LG developed the world's first 60-inch plasma TV, and in 1999 established a joint venture with Philips LG.Philips LCD which now goes by the name LG Display.\n\nIn order to create a holding company, the former LG Electronics was split off in 2002, with the \"new\" LG Electronics being spun off and the \"old\" LG Electronics changing its name to LG EI. It was then merged with and into LG CI in 2003 (the legal successor of the former LG Chem), so the company that started as Goldstar does not currently exist.\n\nLG Electronics plays a large role in the global consumer electronics industry; it was the second-largest LCD TV manufacturer worldwide as of 2013. By 2005, LG was a Top 100 global brand, and in 2006 LG recorded a brand growth of 14%. Its display manufacturing affiliate, LG Display, as of 2009 was the world's largest LCD panel manufacturer. In 2010, LG Electronics entered the smartphone industry. Since, LG Electronics continued to develop various electronic products, such as releasing the world's first 84-inch ultra-HD TV for retail sale.\n\nOn 5 December 2012, the antitrust regulators of the European Union fined LG Electronics and five other major companies (Samsung, Thomson since 2010 known as Technicolor, Matsushita which today is Panasonic Corp, Philips, and Toshiba) for fixing prices of TV cathode-ray tubes in two cartels lasting nearly a decade.\n\nOn 11 June 2015, LG Electronics found itself in the midst of a human rights controversy when The Guardian published an article by Rosa Moreno, a former employee of an LG television assembly factory.\n\nAt the end of 2016, LG Electronics merged its German branch (situated in Ratingen) and European headquarter (situated in London) together in Eschborn a suburb of Frankfurt am Main.\n\nIn March 2017, LG Electronics was sued for its handling of hardware failures with recent smartphones such as the LG G4.\n\nIn November 2018, LG announced Hwang Jeong-hwan, who took the job as president of LG Mobile Communications in October 2017, will be replaced by Brian Kwon, who is head of LG’s hugely profitable home entertainment business, from December 1, 2018.\n\nLG Electronics has four business units: Home Entertainment, Mobile Communications, Home Appliances & Air Solutions, and Vehicle Components. The company has 128 operations worldwide, employing 83,000 people. LG Electronics owns Zenith and controls 37.9 percent of LG Display.\n\nLG Electronics' products include televisions, home theater systems, refrigerators, washing machines, computer monitors, wearable devices, smart appliances, and smartphones.\n\nThe LG SL9000 was one of several new Borderless HDTV's advertised for release at IFA Berlin in 2009. LG Electronics launched an OLED TV in 2013 and 65-inch and 77-inch sizes in 2014. LG Electronics introduced its first Internet TV in 2007, originally branded as \"NetCast Entertainment Access\" devices. They later renamed the 2011 Internet televisions to \"LG Smart TV\" when more interactive television features were added, that enable the audience to receive information from the Internet while at the same time watching conventional TV programming.\n\nIn November 2013, a blogger discovered that some of LG's smart TVs silently collect filenames from attached USB storage devices and program viewing data, and transmit the information to LG's servers and LG-affiliated servers. Shortly after this blog entry went live, LG disabled playback on its site of the video, explaining how its viewer analytics work, and closed the Brightcove account the video was hosted on.\n\nLG's remote uses Hillcrest Labs' Freespace technology to allow users to change channels using gestures and Dragon NaturallySpeaking technology for voice recognition.\n\nAs of 2014, LG is using webOS with a ribbon interface with some of its smart TVs. LG reported that in the first eight months after release, it had sold over 5 million webOS TVs.\n\nIn 2016, exclusively to India, Indian arm of South Korea's LG Electronics Inc started selling a TV that would reject mosquitoes. It uses ultrasonic waves that are silent to humans but cause mosquitoes to fly away. It was released on 16 June 2016. The technology was also used in air conditioners and washing machines. The TV is aimed for lower-income consumers living in conditions that would make them susceptible to mosquitoes.\n\nLG Electronics manufactures a wide range of smartphones and tablet devices. Other than the G3, LG officially unveiled the curved smartphone, G Flex, on 27 October 2013. LG has released it in South Korea in November 2013, and later announced releases in Europe, the rest of Asia, and North America. At Consumer Electronics Show in January 2014, LG announced a U.S. release for the G2 across several major carriers. In 2015, LG has released LG G4 globally in late May through early June. On 7 September 2016, LG unveiled the V20, and the V30 was announced on 31 August 2017. LG G6 was officially announced during MWC 2017 on 26 February 2017. The introduction of the G7 ThinQ model was scheduled for a 2 May 2018 media briefing.\n\nIn 2014, LG revealed three new additions to the G series of tablets, which each include LG's Knock Code feature, allowing users to unlock devices with a series of taps. The tablets also feature Q Pair which allows tablets to sync up with a smartphone, and for phone calls and text messages passed on to the tablet in real time.\n\nLG and Google announced the Android Wear-based smartwatch, the LG G Watch, that was in June 2014. In August 2014, the LG G Watch R that has a circular face (similar to the Moto 360) was released. The LG Watch Urbane that LG's third Android Wear-based smart watch has released in April 2015. This was the first device to support newer smartwatch features such as Wi-Fi, and new parts of Android Wear's software interface, like the ability to draw emoji to friends.\n\nIn 2015, LG announced the first Bluetooth keyboard that folds up along the four rows of keys that can be tossed in a purse or pocket. The Rolly keyboard is made of solid plastic. Two tiny plastic arms fold out from the end of the keyboard to support a tablet or smartphone, and it can toggle between two different Bluetooth-connected devices at a time. Battery life is an expected three months on a single AAA battery.\n\nLG manufactures and sells home appliances such as refrigerators, washing machines and tumble dryers, vacuum cleaners, kitchen appliances, and air conditioners and even microwave ovens.\nIn June 2014, LG Electronics also announced the launch of its smart appliances with HomeChat messaging service in South Korea. HomeChat employs LINE, the mobile messenger app from Korean company 'Naver', to let homeowners communicate, control, monitor and share content with LG's smart appliances. Users can send simple messages, such as \"start washing cycle,\" in order to control their washing machines.\n\nLG owns the Korean Baseball Organisation (KBO) LG Twins (LG 트윈스). The team is one of the founding members of the KBO, established in 1982, although LG took over ownership of the team in 1989. The team has won two Korean Series (1990 and 1994), although their loyal and large fan base has been starved of success since relative to the budget and size of the club in the KBO. That said, LG continues to attract the largest average attendance out of all KBO teams and has emerged from its slump to become regular playoff contenders in recent years. \n\nIn August 2013, LG Electronics announced that it would sponsor German Bundesliga club Bayer 04 Leverkusen for the next three years with an option to extend for one more year. In the U.S., LG Electronics' brand and product advertisements can be seen in Dodger Stadium of the Los Angeles Dodgers and Great American Ball Park of the Cincinnati Reds. LG sponsors the International Cricket Council, the world governing body for cricket, and also sponsors ICC Awards. \n\nFrom 2009 to 2013, LG Electronics sponsored Formula One for 5 years as a Global Partner and Technology Partner of Formula One. until 2013. LG was also an official supplier to Virgin Racing and Lotus Racing team, plus engine manufacturer Cosworth from 2010 to 2012. LG also sponsors London Fashion Week and the LG Arena in Birmingham.\n\nDuring the period 2001–2003, LG sponsored the snooker Grand Prix. During these years the tournament was known as the \"LG Cup\". In 2008 LG became sponsors of the Extreme Sport 'FSO4 Freeze' festival.\n\nThe LG Electronics company in Australia have dissolved the sponsorship with the former Australian vice-captain, David Warner on 27 March 2018 and dropped him as the brand ambassador of the company over the controversial ball tampering scandal which shook the Australian cricket on the 3rd Test during their 2017–18 tempestuous Test series against South Africa. David Warner had an agreement with LG Electronics company in 2014 and was earlier planned to renew his contract just before being caught for ball tampering row.\n\nChoice magazine, in independent tests of popular LG fridge models in 2010, found the energy consumption in two models was higher than claimed by LG. LG was aware of the problem and had offered compensation to affected customers. In 2004, LG made 4A-rated water efficiency claims for numerous washing machines before they were certified. LG gave undertakings to the Australian Competition and Consumer Commission (ACCC) to provide appropriate corrective notices and upgrade and maintain its trade practices compliance program. In 2006, LG overstated energy efficiency on five of its air conditioner models and was again required to offer consumers rebates to cover the extra energy costs.\n\nIn March 2018, it was announced that one of LG steam clothing care system earned the Asthma and Allergy Friendly Certification.\n\nLG Electronics USA had proposed to build a new headquarters in the borough of Englewood Cliffs in Bergen County, New Jersey, including a tall building that would stand taller than the tree line of the Hudson Palisades, a US National Natural Landmark. The company proposed to build an environmentally friendly facility in Englewood Cliffs, incidental to Bergen County's per-capita leading Korean American population, having received an initially favorable legal decision concerning building height issues. The plan, while approved by the local government, met with resistance from the segments of the general public as well as government officials in New Jersey and adjacent New York. The initial court decision upholding the local government approval was overturned by a New Jersey appellate court in 2015, and LG subsequently submitted a revised, scaled-down, 64-foot building for approval by the borough of Englewood Cliffs in 2016. LG broke ground on the new US$300 million Englewood Cliffs headquarters on 7 February 2017, to be completed in late 2019.\n\n\n\n"}
{"id": "33239227", "url": "https://en.wikipedia.org/wiki?curid=33239227", "title": "Laser surface velocimeter", "text": "Laser surface velocimeter\n\nA laser surface velocimeter (LSV) is a non-contact optical speed sensor measuring velocity and length on moving surfaces. Laser surface velocimeters use the laser Doppler principle to evaluate the laser light scattered back from a moving object. They are widely used for process and quality control in industrial production processes.\n\nThe Doppler effect (or Doppler shift) is the change in frequency of a wave for an observer moving relative to the source of the wave. The wave has a frequency f and propagates at a speed c When the observer moves at a velocity of v relative to the source, he receives a different frequency f' according to\n\nThe above analysis is an approximation for small velocities in comparison to the speed of light which is fulfilled very well for practically all technically relevant velocities.\n\nTo make a measurement on moving objects, which can in principle be of any length, requires a measurement design with an observation axis for the sensor which is at a right angle to the direction of movement of the object under investigation.\n\nLaser surface velocimeters work according to the socalled Difference Doppler Technique. Here, 2 laser beams which are each incident to the optical axis at an angle φ, are superimposed on the surface of the object. For a point P, which moves at velocity v through the intersection point of the two laser beams, the frequencies of the two laser beams are Doppler shifted in accordance with the above\nformula. At the point P of the object which is moving at the velocity \"v\", the following frequencies therefore occur:\n\nThe point P now emits scatter waves in the direction of the detector. As P is moving with the object, the scattered radiation in the direction formula_4 of the detector is also Doppler shifted. Thus for the frequency of the scatter waves in the direction of the detector, it can be said:\n\nThe scatter waves are superimposed on the detector. Due to the interference of the scatter waves from the two laser beams, there are different frequency components in the superimposition. The low-frequency beat frequency of the superimposed scatter radiation which corresponds to the Doppler frequency \"f\" is analyzed metrologically. When both incidental laser beams are at the same frequency (same wavelength), this is seen as a difference of \"f\" and \"f\" to:\n\nIf point P moves vertically with reference to the optical axis and at the same angle of incidence φ, it can be said that:\n\nand\n\nThis means the final result is:\n\nThe Doppler shift is thus directly proportional to the velocity. A graphic explanation which leads to the same result follows:\n\nBoth the laser beams are superimposed in the measurement volume and in this spatial area, generate an interference pattern of bright and dark fringes.\n\nThe fringe spacing Δs is a system constant which depends on the laser wavelength λ and the angle between the laser beams 2φ:\n\nIf a particle moves through the fringe pattern, then the intensity of the light it scatters back is modulated.\n\nAs a result of this, a photo receiver in the sensor head generates an AC signal, the frequency f of which is directly proportional to the velocity component of the surface in measurement direction v and it can be said that:\n\nLaser surface velocimeters work in the so-called heterodyne mode, i.e. the frequency of one of the laser beams is shifted by an offset\nof 40 MHz, e.g.. This makes the fringes in the measurement volume travel with a velocity corresponding to the offset frequency f. This then makes it possible to identify the direction of movement of the object and to measure at the velocity zero. The resulting modulation frequency f at the photo receiver in heterodyne mode is:\n\nThe modulation frequency is determined in the controller using Fourier transformation and converted into the measurement value for the\nvelocity v. The length measurement is made by integrating the velocity signal.\n\nLaser surface velocimeters measure speed and length of moving surfaces on coils, strips, tubes, fiber, film, paper, foil, composite lumber, or almost any other moving material, including hot steel. LSVs can accomplish various tasks like cut-to-length control, part length and spool length measurement, speed measurement and speed control, differential speed measurement for mass flow control, encoder calibration, ink-jet marker control, and many others.\n\n\n\nOperating Principle of Laser Surface Velocimetry (Video)\n"}
{"id": "35134363", "url": "https://en.wikipedia.org/wiki?curid=35134363", "title": "List of LM-series integrated circuits", "text": "List of LM-series integrated circuits\n\nThe following is a list of LM-series integrated circuits. Many were among the first analog integrated circuits commercially produced; some were groundbreaking innovations, and many are still being used. The LM series originated with integrated circuits made by National Semiconductor. The prefix LM stands for \"linear monolithic\", referring to the analog components integrated onto a single piece of silicon. Because of the popularity of these parts, many of them were second-sourced by other manufacturers who kept the sequence number as an aid to identification of compatible parts. Several generations of pin-compatible descendants of the original parts have since become \"de facto\" standard electronic components.\n\n\n\n\n"}
{"id": "1269464", "url": "https://en.wikipedia.org/wiki?curid=1269464", "title": "List of STEP (ISO 10303) parts", "text": "List of STEP (ISO 10303) parts\n\nAn incomplete list of parts making up STEP (ISO 10303):\n\n\n\n\n\n\nThe 'APs' utilize the lower-level information of integrated resources in well defined combinations and configurations to represent a particular data model of an engineering or technical application.\n\nAn ATS is a formal description on how to test STEP implementations for conformance. They contain a test plan for postprocessors (exporting STEP data) and preprocessors (importing STEP data). The structure of an ATS is defined in part 34. \n\nThe original plan of STEP was to have for every AP 2xx a corresponding ATS 3xx, but only a few were finally realized till today.\n\nAICs are specializations of the integrated application and generic resources. This is done by subtyping interfaced entities and adding further constraints and rules. No new stand-alone entities are created and no new explicit attributes are added. Most AICs are specializations in the geometric area. AICs did not exist back in 1994 when the first release of STEP got published. But when the 2nd generation of APs grows up it becomes clear that APs do not only share not only the IRs but also a lot of specializations. AICs are a big step towards AP interoperability.\n\nsee ISO 10303 Application Modules\n\n"}
{"id": "15884402", "url": "https://en.wikipedia.org/wiki?curid=15884402", "title": "London Green500", "text": "London Green500\n\nThe London Green500 is London mayor Boris Johnson’s pioneering carbon mentoring programme. With the aim to lead London’s top organisations to reduce carbon emissions in the capital by 60% by 2025.\n\nThe London Green500 is led by a consortium on behalf of the London Development Agency. AEA Technology is the lead contractor, with Mouchel, National Energy Foundation, Futerra and Future Considerations.\n\nThe aim of Green500 is to enlist some of London’s largest and most prestigious organisations and mentor them through their carbon reduction commitments.\n\nGreen500 is one of a number of London Development Agency (LDA) London Development Agency initiatives, including Better Buildings Partnership, which aim to reduce London’s CO emissions. \n\nMembers of Green500 receive an initial assessment of their London specific carbon footprint from an expert Green500 Carbon Mentor. Members are then presented with a tailored Action Plan outlining how they can reduce their carbon emissions. \nThe individual Action Plans not only aim to reduce the member’s carbon footprint, but also estimates financial savings for energy, waste, water and transport. The Green500 Action Plan also gives information on expected payback periods for each change, if implemented successfully. \n\nAs well as providing a company specific Action Plan, Green500 also offers significant benefits to members including awards, networking opportunities and access to specialist suppliers.\n\n\nMembers of Green500 cover a wide range of sectors including finance, law, services, art and education, transport, food and drink, manufacturing and many more.\n\nMembers include Addison Lee, MiniCab London, Boots Group, Chelsea FC, EDF Energy, HSBC, Marks & Spencer, Natural History Museum, Pret a Manger, Saatchi & Saatchi, Selfridges, Tate & Lyle, T-Mobile and TNT Express. The latest members list can be found at the bottom of this page\n\n"}
{"id": "29206081", "url": "https://en.wikipedia.org/wiki?curid=29206081", "title": "Ministry of Industry, Trade and Small Industries", "text": "Ministry of Industry, Trade and Small Industries\n\nThe Ministry of Trade and Industry has its headquarters located in Cairo. The position of minister has been held by Tareq Qabil since 19 September 2015.\n\n\n\n"}
{"id": "13061712", "url": "https://en.wikipedia.org/wiki?curid=13061712", "title": "Ministry of Industry and Information Technology", "text": "Ministry of Industry and Information Technology\n\nMinistry of Industry and Information Technology (MIIT) of the Chinese government, established in March 2008, is the state agency of the People's Republic of China responsible for regulation and development of the postal service, Internet, wireless, broadcasting, communications, production of electronic and information goods, software industry and the promotion of the national knowledge economy. The MIIT was also responsible for the nation's tobacco control, but this task will be assigned to a new health commission to be established as part of a broad governmental reshuffle.\n\nThe Ministry of Industry and Information Technology is not responsible for the regulation of content for the media industry. This is administered by the State Administration of Radio, Film and Television. The responsibility for regulating the non electronic communications industry in China falls on the General Administration of Press and Publication.\n\nThe state council announced during the 2008 National People's Congress that the Ministry of Industry and Information Technology will supersede the Ministry of Information Industry. The new ministry will also include the former Commission of Science, Technology and Industry for National Defense, the State Council Informatization Office and the State Tobacco Monopoly Bureau.\n\nIn 2013, the ministry's 'Made in China 2025' plan was approved by the State Council. It took over two years to complete by one hundred and fifty people. The plan's aim is to improve production efficiency and quality.\n\n\n"}
{"id": "11282548", "url": "https://en.wikipedia.org/wiki?curid=11282548", "title": "Mobile virtual private network", "text": "Mobile virtual private network\n\nA mobile virtual private network (mobile VPN or mVPN) is a VPN which is capable of persisting during sessions across changes in physical connectivity, point of network attachment, and IP address. The \"mobile\" in the name refers to the fact that the VPN can change points of network attachment, not necessarily that the mVPN client is a mobile phone or that it is running on a wireless network.\n\nMobile VPNs are used in environments where workers need to keep application sessions open at all times, throughout the working day, as they connect via various wireless networks, encounter gaps in coverage, or suspend-and-resume their devices to preserve battery life. A conventional VPN cannot survive such events because the network tunnel is disrupted, causing applications to disconnect, time out, fail, or even the computing device itself to crash. Mobile VPNs are commonly used in public safety, home care, hospital settings, field service management, utilities and other industries. Increasingly, they are being adopted by mobile professionals and white-collar workers.\n\nA VPN maintains an authenticated, encrypted tunnel for securely passing data traffic over public networks (typically, the Internet.) Other VPN types are IPsec VPNs, which are useful for point-to-point connections when the network endpoints are known and remain fixed; or SSL VPNs, which provide for access through a Web browser and are commonly used by remote workers (telecommuting workers or business travelers).\n\nMakers of mobile VPNs draw a distinction between remote access and mobile environments. A remote-access user typically establishes a connection from a fixed endpoint, launches applications that connect to corporate resources as needed, and then logs off. In a mobile environment, the endpoint changes constantly (for instance, as users roam between different cellular networks or Wi-Fi access points). A mobile VPN maintains a virtual connection to the application at all times as the endpoint changes, handling the necessary network logins in a manner transparent to the user.\n\nThe following are functions common to mobile VPNs.\nSome mobile VPNs offer additional \"mobile-aware\" management and security functions, giving information technology departments visibility and control over devices that may not be on the corporate premises or that connect through networks outside IT's direct control.\nMobile VPNs have found uses in a variety of industries, where they give mobile workers access to software applications.\nIn telecommunication, a mobile VPN is a solution that provides data user mobility and ensures secure network access with predictable performance. Data user mobility is defined as uninterrupted connectivity or the\nability to stay connected and communicate to a possibly remote data network while changing the network access medium or points of attachment.\n\nIn 2001, Huawei launched a product named \"MVPN\". In this case \"MVPN\" had a different meaning from the way that later industry sources would use the term. The Huawei product was focused on delivering a seamless corporate phone system to users whether they were on desktop phones or mobile devices. Although the web page is no longer available, the company advertised that their MVPN had the following advantages over a standard phone system:\n\nTmharay\n\n"}
{"id": "18695732", "url": "https://en.wikipedia.org/wiki?curid=18695732", "title": "Molecular scale electronics", "text": "Molecular scale electronics\n\nMolecular scale electronics, also called single-molecule electronics, is a branch of nanotechnology that uses single molecules, or nanoscale collections of single molecules, as electronic components. Because single molecules constitute the smallest stable structures imaginable, this miniaturization is the ultimate goal for shrinking electrical circuits.\n\nThe field is often termed simply as \"molecular electronics\", but this term is also used to refer to the distantly related field of conductive polymers and organic electronics, which uses the properties of molecules to affect the bulk properties of a material. A nomenclature distinction has been suggested so that \"molecular materials for electronics\" refers to this latter field of bulk applications, while \"molecular scale electronics\" refers to the nanoscale single-molecule applications treated here.\n\nConventional electronics have traditionally been made from bulk materials. Ever since their invention in 1958, the performance and complexity of integrated circuits has undergone exponential growth, a trend named Moore’s law, as feature sizes of the embedded components have shrunk accordingly. As the structures shrink, the sensitivity to deviations increases. In a few technology generations, when the minimum feature sizes reaches 13 nm, the composition of the devices must be controlled to a precision of a few atoms \nfor the devices to work. With bulk methods growing increasingly demanding and costly as they near inherent limits, the idea was born that the components could instead be built up atom by atom in a chemistry lab (bottom up) versus carving them out of bulk material (top down). This is the idea behind molecular electronics, with the ultimate miniaturization being components contained in single molecules.\n\nIn single-molecule electronics, the bulk material is replaced by single molecules. Instead of forming structures by removing or applying material after a pattern scaffold, the atoms are put together in a chemistry lab. In this way, billions of billions of copies are made simultaneously (typically more than 10 molecules are made at once) while the composition of molecules are controlled down to the last atom. The molecules used have properties that resemble traditional electronic components such as a wire, transistor or rectifier.\n\nSingle-molecule electronics is an emerging field, and entire electronic circuits consisting exclusively of molecular sized compounds are still very far from being realized. However, the unceasing demand for more computing power, along with the inherent limits of lithographic methods , make the transition seem unavoidable. Currently, the focus is on discovering molecules with interesting properties and on finding ways to obtain reliable and reproducible contacts between the molecular components and the bulk material of the electrodes.\n\nMolecular electronics operates in the quantum realm of distances less than 100 nanometers. The miniaturization down to single molecules brings the scale down to a regime where quantum mechanics effects are important. In conventional electronic components, electrons can be filled in or drawn out more or less like a continuous flow of electric charge. In contrast, in molecular electronics the transfer of one electron alters the system significantly. For example, when an electron has been transferred from a source electrode to a molecule, the molecule gets charged up, which makes it far harder for the next electron to transfer (see also Coulomb blockade). The significant amount of energy due to charging must be accounted for when making calculations about the electronic properties of the setup, and is highly sensitive to distances to conducting surfaces nearby.\n\nThe theory of single-molecule devices is especially interesting since the system under consideration is an open quantum system in nonequilibrium (driven by voltage). In the low bias voltage regime, the nonequilibrium nature of the molecular junction can be ignored, and the current-voltage traits of the device can be calculated using the equilibrium electronic structure of the system. However, in stronger bias regimes a more sophisticated treatment is required, as there is no longer a variational principle. In the elastic tunneling case (where the passing electron does not exchange energy with the system), the formalism of Rolf Landauer can be used to calculate the transmission through the system as a function of bias voltage, and hence the current. In inelastic tunneling, an elegant formalism based on the non-equilibrium Green's functions of Leo Kadanoff and Gordon Baym, and independently by Leonid Keldysh was advanced by Ned Wingreen and Yigal Meir. This Meir-Wingreen formulation has been used to great success in the molecular electronics community to examine the more difficult and interesting cases where the transient electron exchanges energy with the molecular system (for example through electron-phonon coupling or electronic excitations).\n\nFurther, connecting single molecules reliably to a larger scale circuit has proven a great challenge, and constitutes a significant hindrance to commercialization.\n\nCommon for molecules used in molecular electronics is that the structures contain many alternating double and single bonds (see also Conjugated system). This is done because such patterns delocalize the molecular orbitals, making it possible for electrons to move freely over the conjugated area.\n\nThe sole purpose of molecular wires is to electrically connect different parts of a molecular electrical circuit. As the assembly of these and their connection to a macroscopic circuit is still not mastered, the focus of research in single-molecule electronics is primarily on the functionalized molecules: molecular wires are characterized by containing no functional groups and are hence composed of plain repetitions of a conjugated building block. Among these are the carbon nanotubes that are quite large compared to the other suggestions but have shown very promising electrical properties.\n\nThe main problem with the molecular wires is to obtain good electrical contact with the electrodes so that electrons can move freely in and out of the wire.\n\nSingle-molecule transistors are fundamentally different from the ones known from bulk electronics. The gate in a conventional (field-effect) transistor determines the conductance between the source and drain electrode by controlling the density of charge carriers between them, whereas the gate in a single-molecule transistor controls the possibility of a single electron to jump on and off the molecule by modifying the energy of the molecular orbitals. One of the effects of this difference is that the single-molecule transistor is almost binary: it is either \"on\" or \"off\". This opposes its bulk counterparts, which have quadratic responses to gate voltage.\n\nIt is the quantization of charge into electrons that is responsible for the markedly different behavior compared to bulk electronics. Because of the size of a single molecule, the charging due to a single electron is significant and provides means to turn a transistor \"on\" or \"off\" (see Coulomb blockade). For this to work, the electronic orbitals on the transistor molecule cannot be too well integrated with the orbitals on the electrodes. If they are, an electron cannot be said to be located on the molecule or the electrodes and the molecule will function as a wire.\n\nA popular group of molecules, that can work as the semiconducting channel material in a molecular transistor, is the oligopolyphenylenevinylenes (OPVs) that works by the Coulomb blockade mechanism when placed between the source and drain electrode in an appropriate way. Fullerenes work by the same mechanism and have also been commonly used.\nSemiconducting carbon nanotubes have also been demonstrated to work as channel material but although molecular, these molecules are sufficiently large to behave almost as bulk semiconductors.\n\nThe size of the molecules, and the low temperature of the measurements being conducted, makes the quantum mechanical states well defined. Thus, it is being researched if the quantum mechanical properties can be used for more advanced purposes than simple transistors (e.g. spintronics).\n\nPhysicists at the University of Arizona, in collaboration with chemists from the University of Madrid, have designed a single-molecule transistor using a ring-shaped molecule similar to benzene. Physicists at Canada's National Institute for Nanotechnology have designed a single-molecule transistor using styrene. Both groups expect (the designs were experimentally unverified ) their respective devices to function at room temperature, and to be controlled by a single electron.\n\nMolecular rectifiers are mimics of their bulk counterparts and have an asymmetric construction so that the molecule can accept electrons in one end but not the other. The molecules have an electron donor (D) in one end and an electron acceptor (A) in the other. This way, the unstable state D – A will be more readily made than D – A. The result is that an electric current can be drawn through the molecule if the electrons are added through the acceptor end, but less easily if the reverse is attempted.\n\nOne of the biggest problems with measuring on single molecules is to establish reproducible electrical contact with only one molecule and doing so without shortcutting the electrodes. Because the current photolithographic technology is unable to produce electrode gaps small enough to contact both ends of the molecules tested (on the order of nanometers), alternative strategies are applied.\n\nOne way to produce electrodes with a molecular sized gap between them is break junctions, in which a thin electrode is stretched until it breaks. Another is electromigration. Here a current is led through a thin wire until it melts and the atoms migrate to produce the gap. Further, the reach of conventional photolithography can be enhanced by chemically etching or depositing metal on the electrodes.\n\nProbably the easiest way to conduct measurements on several molecules is to use the tip of a scanning tunneling microscope (STM) to contact molecules adhered at the other end to a metal substrate.\n\nA popular way to anchor molecules to the electrodes is to make use of sulfur's high chemical affinity to gold. In these setups, the molecules are synthesized so that sulfur atoms are placed strategically to function as crocodile clips connecting the molecules to the gold electrodes. Though useful, the anchoring is non-specific and thus anchors the molecules randomly to all gold surfaces. Further, the contact resistance is highly dependent on the precise atomic geometry around the site of anchoring and thereby inherently compromises the reproducibility of the connection.\n\nTo circumvent the latter issue, experiments has shown that fullerenes could be a good candidate for use instead of sulfur because of the large conjugated π-system that can electrically contact many more atoms at once than one atom of sulfur.\n\nIn polymers, classical organic molecules are composed of both carbon and hydrogen (and sometimes additional compounds such as nitrogen, chlorine or sulphur). They are obtained from petrol and can often be synthesized in large amounts. Most of these molecules are insulating when their length exceeds a few nanometers. However, naturally occurring carbon is conducting, especially graphite recovered from coal or encountered otherwise. From a theoretical viewpoint, graphite is a semi-metal, a category in between metals and semi-conductors. It has a layered structure, each sheet being one atom thick. Between each sheet, the interactions are weak enough to allow an easy manual cleavage.\n\nTailoring the graphite sheet to obtain well defined nanometer-sized objects remains a challenge. However, by the close of the twentieth century, chemists were exploring methods to fabricate extremely small graphitic objects that could be considered single molecules. After studying the interstellar conditions under which carbon is known to form clusters, Richard Smalley's group (Rice University, Texas) set up an experiment in which graphite was vaporized via laser irradiation. Mass spectrometry revealed that clusters containing specific \"magic numbers\" of atoms were stable, especially those clusters of 60 atoms. Harry Kroto, an English chemist who assisted in the experiment, suggested a possible geometry for these clusters – atoms covalently bound with the exact symmetry of a soccer ball. Coined buckminsterfullerenes, buckyballs, or C, the clusters retained some properties of graphite, such as conductivity. These objects were rapidly envisioned as possible building blocks for molecular electronics.\n\nWhen trying to measure electronic traits of molecules, artificial phenomena can occur that can be hard to distinguish from truly molecular behavior. Before they were discovered, these artifacts have mistakenly been published as being features pertaining to the molecules in question.\n\nApplying a voltage drop on the order of volts across a nanometer sized junction results in a very strong electrical field. The field can cause metal atoms to migrate and eventually close the gap by a thin filament, which can be broken again when carrying a current. The two levels of conductance imitate molecular switching between a conductive and an isolating state of a molecule.\n\nAnother encountered artifact is when the electrodes undergo chemical reactions due to the high field strength in the gap. When the voltage bias is reversed, the reaction will cause hysteresis in the measurements that can be interpreted as being of molecular origin.\n\nA metallic grain between the electrodes can act as a single electron transistor by the mechanism described above, thus resembling the traits of a molecular transistor. This artifact is especially common with nanogaps produced by the electromigration method.\n\nOne of the biggest hindrances for single-molecule electronics to be commercially exploited is the lack of methods to connect a molecular sized circuit to bulk electrodes in a way that gives reproducible results. At the current state, the difficulty of connecting single molecules vastly outweighs any possible performance increase that could be gained from such shrinkage. The difficulties grow worse if the molecules are to have a certain spatial orientation and/or have multiple poles to connect.\n\nAlso problematic is that some measurements on single molecules are carried out in cryogenic temperatures (near absolute zero), which is very energy consuming. This is done to reduce signal noise enough to measure the faint currents of single molecules.\n\nIn their treatment of so-called \"donor-acceptor\" complexes in the 1940s, Robert Mulliken and Albert Szent-Györgyi advanced the concept of charge transfer in molecules. They subsequently further refined the study of both charge transfer and energy transfer in molecules. Likewise, a 1974 paper from Mark Ratner and Ari Aviram illustrated a theoretical molecular rectifier. In 1988, Aviram described in detail a theoretical single-molecule field-effect transistor. Further concepts were proposed by Forrest Carter of the Naval Research Laboratory, including single-molecule logic gates. A wide range of ideas were presented, under his aegis, at a conference entitled \"Molecular Electronic Devices\" in 1988. These were all theoretical constructs and not concrete devices. The \"direct\" measurement of the electronic traits of individual molecules awaited the development of methods for making molecular-scale electrical contacts. This was no easy task. Thus, the first experiment directly-measuring the conductance of a single molecule was only reported in 1995 on a single C molecule by C. Joachim and J. K. Gimzewsky in their seminal Physical Revie Letter paper and later in 1997 by Mark Reed and co-workers on a few hundred molecules. Since then, this branch of the field has advanced rapidly. Likewise, as it has grown possible to measure such properties directly, the theoretical predictions of the early workers have been confirmed substantially.\n\nRecent progress in nanotechnology and nanoscience has facilitated both experimental and theoretical study of molecular electronics. Development of the scanning tunneling microscope (STM) and later the atomic force microscope (AFM) have greatly facilitated manipulating single-molecule electronics. Also, theoretical advances in molecular electronics have facilitated further understanding of non-adiabatic charge transfer events at electrode-electrolyte interfaces.\n\nThe concept of molecular electronics was first published in 1974 when Aviram and Ratner suggested an organic molecule that could work as a rectifier. Having both huge commercial and fundamental interest, much effort was put into proving its feasibility, and 16 years later in 1990, the first demonstration of an intrinsic molecular rectifier was realized by Ashwell and coworkers for a thin film of molecules.\n\nThe first measurement of the conductance of a single molecule was realised in 1994 by C. Joachim and J. K. Gimzewski and published in 1995 (see the corresponding Phys. Rev. Lett. paper). This was the conclusion of 10 years of research started at IBM TJ Watson, using the scanning tunnelling microscope tip apex to switch a single molecule as already explored by A. Aviram, C. Joachim and M. Pomerantz at the end of the 80's (see their seminal Chem. Phys. Lett. paper during this period). The trick was to use an UHV Scanning Tunneling microscope to allow the tip apex to gently touch the top of a single molecule adsorbed on an Au(110) surface. A resistance of 55 MOhms was recorded along with a low voltage linear I-V. The contact was certified by recording the I-z current distance property, which allows measurement of the deformation of the cage under contact. This first experiment was followed by the reported result using a mechanical break junction method to connect two gold electrodes to a sulfur-terminated molecular wire by Mark Reed and James Tour in 1997.\n\nA single-molecule amplifier was implemented by C. Joachim and J.K. Gimzewski in IBM Zurich. This experiment, involving one molecule, demonstrated that one such molecule can provide gain in a circuit via intramolecular quantum interference effects alone.\n\nA collaboration of researchers at Hewlett-Packard (HP) and University of California, Los Angeles (UCLA), led by James Heath, Fraser Stoddart, R. Stanley Williams, and Philip Kuekes, has developed molecular electronics based on rotaxanes and catenanes.\n\nWork is also occurring on the use of single-wall carbon nanotubes as field-effect transistors. Most of this work is being done by International Business Machines (IBM).\n\nSome specific reports of a field-effect transistor based on molecular self-assembled monolayers were shown to be fraudulent in 2002 as part of the Schön scandal.\n\nUntil recently entirely theoretical, the Aviram-Ratner model for a unimolecular rectifier has been confirmed unambiguously in experiments by a group led by Geoffrey J. Ashwell at Bangor University, UK. Many rectifying molecules have so far been identified, and the number and efficiency of these systems is growing rapidly.\n\nSupramolecular electronics is a new field involving electronics at a supramolecular level.\n\nAn important issue in molecular electronics is the determination of the resistance of a single molecule (both theoretical and experimental). For example, Bumm, et al. used STM to analyze a single molecular switch in a self-assembled monolayer to determine how conductive such a molecule can be. Another problem faced by this field is the difficulty of performing direct characterization since imaging at the molecular scale is often difficult in many experimental devices.\n\n"}
{"id": "2079223", "url": "https://en.wikipedia.org/wiki?curid=2079223", "title": "Neutron-induced swelling", "text": "Neutron-induced swelling\n\nNeutron-induced swelling is the increase of volume and decrease of density of materials subjected to intense neutron radiation. Neutrons impacting the material's lattice rearrange its atoms, causing buildup of dislocations, voids, and Wigner energy. Together with the resulting strength reduction and embrittlement, it is a major concern for materials for nuclear reactors.\n\nMaterials show significant differences in their swelling resistance. \n\nSee also:\n"}
{"id": "2642764", "url": "https://en.wikipedia.org/wiki?curid=2642764", "title": "Operation Ruthless", "text": "Operation Ruthless\n\nOperation Ruthless was the name of a deception operation devised by Ian Fleming in the British Admiralty during World War II, in an attempt to gain access to German Naval Enigma codebooks.\n\nWith the help of their Polish allies, British codebreakers at Bletchley Park had considerable success in decoding the Enigma-enciphered traffic of the German air force, army and intelligence and counter-espionage service (\"Abwehr\"), but had made little progress with German naval messages. The methods of communicating the choice and starting positions, of Enigma's rotors, the \"indicator\", were much more complex for naval messages. In 1940 Dilly Knox, the veteran World War I codebreaker, Frank Birch, head of Bletchley Park's German Naval Department, and the two leading codebreakers, Alan Turing and Peter Twinn knew that getting hold of the German Navy Enigma documentation was their best chance of making progress in breaking the code.\n\nThe Royal Navy's Operational Intelligence Centre (OIC) was a leading user of Ultra intelligence from Bletchley Park's decrypts. Lieutenant Commander Ian Fleming of the Admiralty's Naval Intelligence Division, who later wrote the James Bond novels, was the personal assistant to the Director of Naval Intelligence, Rear Admiral John Godfrey. Fleming liaised with the naval department at Bletchley Park, visiting about twice a month, and was well aware of this problem.\n\nIn September, Fleming wrote a note to Godfrey which read:\nI suggest we obtain the loot by the following means:\n1. Obtain from Air Ministry an air-worthy German bomber.\n2. Pick a tough crew of five, including a pilot, W/T operator and word-perfect German speaker. Dress them in German Air Force uniform, add blood and bandages to suit.\n3. Crash plane in the Channel after making S.O.S. to rescue service in P/L.\n4. Once aboard rescue boat, shoot German crew, dump overboard, bring rescue boat back to English port.\nIn order to increase the chances of capturing an R. or M. with, its richer booty, the crash might be staged in mid-Channel. The Germans would presumably employ one of this type for the longer and more hazardous journey.\n\nF. 12.9-40.\n\nThe plan was that the German bomber would follow on behind aircraft returning from a night bombing raid. When crossing the middle of the English Channel, it would cut one engine and lose height with smoke pouring from a 'candle' in the tail, send out a SOS distress signal and then ditch in the sea. The crew would then take to a rubber dinghy, having ensured that the bomber sank before the Germans could identify it, and wait to be rescued by a German naval vessel. When on board the 'survivors' would then kill the German crew, and hijack the ship, thus obtaining the Enigma documentation.\n\nA Heinkel 111 was available for this operation. The aircraft, \"Werk Nr.\" 6853, had been captured in airworthy condition after being operated by the German bomber unit, Kampfgeschwader 26. On 9 February 1940, it had made a forced landing near North Berwick Law after being damaged by a Spitfire over the Firth of Forth. It was subsequently assigned the Royal Air Force serial number \"AW177\" and flown by the RAF's Air Fighting Development Unit and 1426 Flight.\n\nFleming had proposed himself as one of the crew but, as someone who knew about Bletchley Park, he could not be placed at risk of being captured. The aircraft was prepared with an aircrew of German-speaking Englishmen. The operation was planned for the early part of the month because it was known that the code sheets were changed at the start of each month.\n\nFleming took his team to Dover to await the next suitable bombing raid, but aerial reconnaissance and wireless telegraphy monitoring failed to find any suitable German vessels, and the operation was called off. That this was a major disappointment to the codebreakers can be judged by what Frank Birch wrote in a letter dated 20 October 1940.\n\nTuring and Twinn came to me like undertakers cheated of a nice corpse two days ago, all in a stew about the cancellation of operation Ruthless. The burden of their song was the importance of a pinch.\n\nAn alternative account of why the operation did not take place was given when his niece, Lucy Fleming, visited his old office in the Admiralty. \"The Bond Correspondence\", a BBC Radio 4 radio programme broadcast on 24 May 2008, includes an interview there. Her subject in the Admiralty says that the idea was that the Germans would see the floating bomber and the \"survivors\" and send out a rescue vessel; but someone at the RAF pointed out that a downed Heinkel bomber would sink rather than float.\n\n"}
{"id": "1104369", "url": "https://en.wikipedia.org/wiki?curid=1104369", "title": "Organic semiconductor", "text": "Organic semiconductor\n\nOrganic semiconductors are solids whose building blocks are pi-bonded molecules or polymers made up by carbon and hydrogen atoms and – at times – heteroatoms such as nitrogen, sulfur and oxygen. They exist in form of molecular crystals or amorphous thin films. In general, they are electrical insulators, but become semiconducting when charges are either injected from appropriate electrodes, upon doping or by photoexcitation.\n\nIn molecular crystals the energetic separation between the top of the valence band and the bottom conduction band, i.e. the band gap, is typically 2.5–4 eV, while in inorganic semiconductors the band gaps are typically 1–2 eV. This implies that they are, in fact, insulators rather than semiconductors in the conventional sense. They become semiconducting only when charge carriers are either injected from the electrodes or generated by intentional or unintentional doping. Charge carriers can also be generated in the course of optical excitation. It is important to realize, however, that the primary optical excitations are neutral excitons with a Coulomb-binding energy of typically 0.5–1.0 eV. The reason is that in organic semiconductors their dielectric constants are as low as 3–4. This impedes efficient photogeneration of charge carriers in neat systems in the bulk. Efficient photogeneration can only occur in binary systems due to charge transfer between donor and acceptor moieties. Otherwise neutral excitons decay radiatively to the ground state – thereby emitting photoluminescence – or non-radiatively. The optical absorption edge of organic semiconductors is typically 1.7–3 eV, equivalent to a spectral range from 700 to 400 nm (which corresponds to the visible spectrum).\n\nIn 1862, Henry Letheby obtained a partly conductive material by anodic oxidation of aniline in sulfuric acid. The material was probably polyaniline. In the 1950s, researchers discovered that polycyclic aromatic compounds formed semi-conducting charge-transfer complex salts with halogens. In particular, high conductivity of 0.12 S/cm was reported in perylene–iodine complex in 1954. This finding indicated that organic compounds could carry current.\n\nThe fact that organic semiconductors are, in principle, insulators but become semiconducting when charge carriers are injected from the electrode(s) was discovered by Kallmann and Pope. They found that a hole current can flow through an anthracene crystal contacted with a positively biased electrolyte containing iodine that can act as a hole injector. This work was stimulated by the earlier discovery by Akamatu et al. that aromatic hydrocarbons become conductive when blended with molecular iodine because a charge-transfer complex is formed. Since it was readily realized that the crucial parameter that controls injection is the work function of the electrode, it was straightforward to replace the electrolyte by a solid metallic or semiconducting contact with an appropriate work function. When both electrons and holes are injected from opposite contacts, they can recombine radiatively and emit light (electroluminescence). It was observed in organic crystals in 1965 by Sano et al.\n\nIn 1972, researchers found metallic conductivity in the charge-transfer complex TTF-TCNQ. Superconductivity in charge-transfer complexes was first reported in the Bechgaard salt (TMTSF)PF in 1980.\n\nIn 1973 Dr. John McGinness produced the first device incorporating an organic semiconductor. This occurred roughly eight years before the next such device was created. The \"melanin (polyacetylenes) bistable switch\" currently is part of the chips collection of the Smithsonian Institution.\n\nIn 1977, Shirakawa \"et al.\" reported high conductivity in oxidized and iodine-doped polyacetylene. They received the 2000 Nobel prize in Chemistry for \"The discovery and development of conductive polymers\". Similarly, highly conductive polypyrrole was rediscovered in 1979.\n\nRigid-backbone organic semiconductors are now used as active elements in optoelectronic devices such as organic light-emitting diodes (OLED), organic solar cells, organic field-effect transistors (OFET), electrochemical transistors and recently in biosensing applications. Organic semiconductors have many advantages, such as easy fabrication, mechanical flexibility, and low cost.\n\nThe discovery by Kallman and Pope paved the way for applying organic solids as active elements in semiconducting electronic devices, such as organic light-emitting diodes (OLEDs) that rely on the recombination of electrons and hole injected from \"ohmic\" electrodes, i.e. electrodes with unlimited supply of charge carriers. The next major step towards the technological exploitation of the phenomenon of electron and hole injection into a non-crystalline organic semiconductor was the work by Tang and Van Slyke. They showed that efficient electroluminescence can be generated in a vapor-deposited thin amorphous bilayer of an aromatic diamine (TAPC) and Alq3 sandwiched between an indium-tin-oxide (ITO) anode and an Mg:Ag cathode. Another milestone towards the development or organic light-emitting diodes (OLEDs) was the recognition that also conjugated polymers can be used as active materials. The efficiency of OLEDs was greatly improved when realizing that phosphorescent states (triplet excitons) may be used for emission when doping an organic semiconductor matrix with a phosphorescent dye, such as complexes of iridium with strong spin–orbit coupling.\n\nWork on conductivity of anthracene crystals contacted with an electrolyte showed that optically excited dye molecules adsorbed at the surface of the crystal inject charge carriers. The underlying phenomenon is called sensitized photoconductivity. It occurs when photo-exciting a dye molecule with appropriate oxidation/reduction potential adsorbed at the surface or incorporated in the bulk. This effect revolutionized electrophotography, which is the technological basis of today’s office copying machines. It is also the basis of organic solar cells (OSCs), in which the active element is an electron donor, and an electron acceptor material is combined in a bilayer or a bulk heterojunction.\n\nDoping with strong electron donor or acceptors can render organic solids conductive even in the absence of light. Examples are doped polyacetylene and doped light-emitting diodes.\nToday organic semiconductors are used as active elements in organic light-emitting diodes (OLEDs), organic solar cells (OSCs) and organic field-effect transistors (OFETs).\n\nAmorphous molecular films are produced by evaporation or spin-coating. They have been investigated for device applications such as OLEDs, OFETs, and OSCs. Illustrative materials are tris(8-hydroxyquinolinato)aluminium, C, phenyl-C61-butyric acid methyl ester (PCBM), pentacene, carbazoles, and phthalocyanine.\n\nMolecularly doped polymers are prepared by spreading a film of an electrically inert polymer, e.g. polycarbonate, doped with typically 30% of charge transporting molecules, on a base electrode. Typical materials are the triphenylenes. They have been investigated for use as photoreceptors in electrophotography. This requires films have a thickness of several micrometers that can be prepared using the doctor-blade technique.\n\nIn the early days of fundamental research into organic semiconductors the prototypical materials were free-standing single crystals of the acene family, e.g. anthracene and tetracene.\nThe advantage of employing molecular crystals instead of amorphous film is that their charge carrier mobilities are much larger. This is of particular advantage for OFET applications. Examples are thin films of crystalline rubrene prepared by hot wall epitaxy.\n\nThey are usually processed from solution employing variable deposition techniques including simple spin-coating, ink-jet deposition or industrial reel-to-reel coating which allows preparing thin films on a flexible substrate. The materials of choice are conjugated polymers such as poly-thiophene, poly-phenylenevinylene, and copolymers of alternating donor and acceptor units such as members of the poly(carbazole-dithiophene-benzothiadiazole (PCDTBT) family. For solar cell applications they can be blended with C60 or PCBM are electron acceptors.\n\nTo design and characterize organic semiconductors used for optoelectronic applications one should first measure the absorption and photoluminescence spectra using commercial instrumentation. However, in order to find out if a material acts as an electron donor or acceptor one has to determine the energy levels for hole and electron transport. The easiest way of doing this, is to employ cyclic voltammetry. However, one has to take into account that using this technique the experimentally determined oxidation and reduction potential are lower bounds because in voltammetry the radical cations and anions are in a polar fluid solution and are, thus, solvated. Such a solvation effect is absent in a solid specimen. The relevant technique to energetically locate the hole transporting states in a solid sample is UV-photoemission spectroscopy. The equivalent technique for electron states is inverse photoemission.\n\nThere are several techniques to measure the mobility of charge carriers. The traditional technique is the so-called time of flight (TOF) method. Since this technique requires relatively thick samples it is not applicable to thin films. Alternatively, one can extract the charge carrier mobility from the current flowing in a field effect transistor as a function of both the source-drain and the gate voltage. One should be aware, though, that the FET-mobility is significantly larger than the TOF mobility because of the charge carrier concentration in the transport channel of a FET (see below). Other ways to determine the charge carrier mobility involves measuring space charge limited current (SCLC) flow and \"carrier extraction by linearly increasing voltage (CELIV).\n\nIn order to characterize the morphology of semiconductor films, one can apply atomic force microscopy (AFM) scanning electron microscopy (SEM) and Grazing-incidence small-angle scattering (GISAS).\n\nIn contrast to organic crystals investigated in the 1960-70’s, organic semiconductors that are nowadays used as active media in optoelectronic devices are usually more or less disordered. Combined with the fact that the structural building blocks are held together by comparatively weak van der Waals forces this precludes charge transport in delocalized valence and conduction bands. Instead, charge carriers are localized at molecular entities, e.g. oligomers or segments of a conjugated polymer chain and move by incoherent hopping among adjacent sites with statistically variable energies. Quite often the site energies feature a Gaussian distribution. Also the hopping distances can vary statistically (positional disorder). A consequence of the energetic broadening of the density of states (DOS) distribution is that charge motion is both temperature and field dependent and the charge carrier mobility can be several orders of magnitude lower than in an equivalent crystalline system. This disorder effect on charge carrier motion is diminished in organic field-effect transistors because current flow is confined in a thin layer. Therefore, the tail states of the DOS distribution are already filled so that the activation energy for charge carrier hopping is diminished. For this reason the charge carrier mobility inferred from FET experiments is always higher than that determined from TOF experiments.\n\nIn organic semiconductors charge carriers couple to vibrational modes and are referred to as polarons. Therefore, the activation energy for hopping motion contains an additional term due to structural site relaxation upon charging a molecular entity. It turns out, however, that usually the disorder contribution to the temperature dependence of the mobility dominates over the polaronic contribution.\n\n"}
{"id": "1916632", "url": "https://en.wikipedia.org/wiki?curid=1916632", "title": "Peaking power plant", "text": "Peaking power plant\n\nPeaking power plants, also known as peaker plants, and occasionally just \"peakers\", are power plants that generally run only when there is a high demand, known as peak demand, for electricity. Because they supply power only occasionally, the power supplied commands a much higher price per kilowatt hour than base load power. Peak load power plants are dispatched in combination with base load power plants, which supply a dependable and consistent amount of electricity, to meet the minimum demand.\n\nPeak hours usually occur in the morning or late afternoon/evening depending on location. In temperate climates, peak hours often occur when household appliances are heavily used in the evening after work hours. In hot climates, the peak is usually late afternoon when air conditioning load is high, during this time many workplaces are still open and consuming power. In cold climates, the peak is in the morning when space heating and industry are both starting up.\n\nA peaker plant may operate many hours a day, or it may operate only a few hours per year, depending on the condition of the region's electrical grid. Because of the cost of building an efficient power plant, if a peaker plant is only going to be run for a short or highly variable time, it does not make economic sense to make it as efficient as a base load power plant. In addition, the equipment and fuels used in base load plants are often unsuitable for use in peaker plants because the fluctuating conditions would severely strain the equipment. For these reasons, nuclear, geothermal, waste-to-energy, coal and biomass are rarely, if ever, operated as peaker plants.\nFor countries that are trending away from coal fired baseload plants and towards intermittent energy sources such as wind and solar, there is a corresponding increase in the need for peaking or load following power plants and the use of a grid intertie.\n\nPeaker plants are generally gas turbines that burn natural gas. A few burn biogas or petroleum-derived liquids, such as diesel oil and jet fuel, but they are generally more expensive than natural gas, so their use is limited to areas not supplied with natural gas. However, many peaker plants are able to use petroleum as a backup fuel, as storing oil in tanks is easy. The thermodynamic efficiency of simple-cycle gas turbine power plants ranges from 20 to 42%, with between 30 and 42% being average for a new plant.\n\nFor greater efficiency, a heat recovery steam generator (HRSG) is added at the exhaust. This is known as a combined cycle plant. Cogeneration uses waste exhaust heat for process, district heating or other heating uses. Both of these options are used only in plants that are intended to be operated for longer periods than usual. Natural gas and diesel generators with reciprocating engines are sometimes used for grid support using smaller plants.\n\nAnother option for increased efficiency and power output in gas turbines is installing a turbine inlet air cooling system, that cools down the inlet air temperature increasing mass flow ratio. This option, in combination with a thermal energy storage tank, can increase the turbine power output in on-peak periods up to 30%.\nHydroelectric Dams are intentionally variable; they can generate less during off-peak and quickly respond to peak demands, consequently hydroelectricity may function as load following or a peaking plant and with sufficient water, a baseload plant. Natural gas turbines or pumped storage are often used where there is not enough hydroelectricity to respond to daily and weekly variations in generation and consumption. It is not unusual for a dam to be built with more capacity than can be sustained by the water supply, allowing for a higher peak output. Upgrading equipment at existing dams can be one of the least expensive ways of increasing peak generation. The ability to vary the amount of electricity generated is often limited by the requirement that minimum or maximum flows downstream are satisfied.\n\nPumped-storage hydroelectricity is the largest-capacity form of grid energy storage available, used for averaging off-peak and peak electrical demands. The site stores energy using the gravitational potential of water stored in a reservoir. Low-cost off-peak electric power from baseload or intermittent sources is used to pump water at a low elevation to storage in a high elevation reservoir. During periods of high electrical demand, the stored water is released through turbines to produce electric power. Start up times are only a few minutes, and some can start in a few tens of seconds.\n\nBatteries are used in some cases where conditions favor it to smooth flow (avoiding a costly powerline upgrade) as well as supplying peak power and other grid services such as operating reserve, sometimes in hybrid configuration with turbines or diesel engines. Battery power is by far the fastest responding of all powerplants, and can respond to grid conditions at millisecond timescales, giving slower responding equipment a chance to react to outages.\n\nPumped-storage and batteries are net consumers, as they have no inherent energy source, and the conversion between electricity and storage and back incurs some losses.\n\nSolar thermal peaker plants were proposed in 2017, under a US Department of Energy Technology 2 Market award to Hank Price of SolarDynamics, whose paper Dispatchable Solar Power Plant proposed utilizing the thermal energy storage inherent in a solar thermal energy power plant, that enables this heat-based form of solar to generate like a gas peaker, to supply dispatchable power on demand day or night, and in return be controlled by the utility and paid in capacity payments to be available when needed, like a traditional peaker. A solar thermal power plant makes electricity in a steam cycle power plant like a traditional power plant but the heat for steam is supplied by solar energy heating a material such as molten salts and storing the heat until needed to make steam for power generation.\n\nThe opposites of peaking plants are base load power plants. Nuclear and coal burning plants operate continuously, stopping only for maintenance or unexpected outages. Intermediate load following power plants such as hydroelectric operate between these extremes, curtailing their output on nights and weekends when demand is low. Base load and intermediate plants are used preferentially to meet electrical demand because the lower efficiencies of peaker plants make them more expensive to operate.\n\n"}
{"id": "20994326", "url": "https://en.wikipedia.org/wiki?curid=20994326", "title": "Peripheral DMA controller", "text": "Peripheral DMA controller\n\nA peripheral DMA controller (PDC) is a feature found in modern microcontrollers. This is typically a FIFO with automated control features for driving implicitly included modules in a microcontroller such as UARTs.\n\nThis takes a large burden from the operating system and reduces the number of interrupts required to service and control these type of functions.\n\n"}
{"id": "7185437", "url": "https://en.wikipedia.org/wiki?curid=7185437", "title": "Plastic worm", "text": "Plastic worm\n\nA plastic worm (or trout worm) is a plastic fishing lure, generally made to simulate an earthworm. Plastic worms can carry a variety of shapes, colors and sizes, and are made from a variety of synthetic polymers. Some are even scented to simulate live bait.\n\nPlastic worms can be rigged on the line many different ways. Commonly they are used with a small fish hook and a split shot weight to keep the lure deeper in the water. The fishing equipment recommended is a 7-foot fishing rod with 8 to 10 lb fishing line.\n\nA common fishing strategy is to configure them as a Texas Rig, and bounce them off the bottom. The key is to jig near or in cover such as weeds and trees, this technique is commonly referred to as flipping and pitching.\n\"Twister Worm\" is commonly called a grub, not a \"twister worm\", even though the action of the tail is defined as a twisting motion of the body resembling that of a grub.\n\nGenerally there is but one type of worm, the plastic worm. This worm comes in a variety of lengths, styles, and colors to attract different fish species. The plastic worm, sometimes called a \"trout worm \", is often unreliable as a lure for trout fishing, and therefore many anglers do not use them for trout fishing. Bass and panfish species (bluegill, sunfish, etc.) tend to bite these lures more than other species.\n\n"}
{"id": "45707264", "url": "https://en.wikipedia.org/wiki?curid=45707264", "title": "Reem Al Marzouqi", "text": "Reem Al Marzouqi\n\nReem Al Marzouqi ( ) is an Emirati engineer and the first citizen of the UAE to be granted a patent in the United States of America for designing a car that can be driven without hands.\n\nShe received a bachelor's degree in architectural engineering from United Arab Emirates University (UAEU). While studying at UAEU she worked with engineering consultants in Abu Dhabi and Dubai, including Musanada at buildings construction management and design in facilities management firm. For hands-on engineering, she won second place in Emirates Skills national carpentry competition and she was the only and first female participating.\n\nReem received the granted U.S. patent for her original invention, a \"Lower Extremity Vehicle Navigation Control System\". The system uses a steering lever, an acceleration lever and a brake lever which are entirely foot-controlled, allowing people to drive a car without the use of their upper body. The system is the first of its kind, and the UAEU intends to seek patents for it in Japan, China and the European Union., it has become one of the remarkable success of Takamul program \n\nThe idea of Reem's invention crossed her mind when she watched a TV interview on Inside Edition with Jessica Cox, the world's first licensed armless pilot as she mentioned in her interview difficulties of driving cars for long distances using only her feet. Also she mentioned how it got suspended due to violation in safety rules.\n\nReem has since participated in the Expo Science International exhibition. Her invention was included in the British Museum's A History of the World in 100 Objects exhibition, as displayed at Manarat Al Saadiyat, Abu Dhabi, from April–August 2014. It represented the future, and was heralded by Neil MacGregor, Director of the British Museum, as \"a brilliant example of human beings striving to find new things.\"\n\nBased on Reem's academic achievement and extracurricular activities, she was selected by the Academy of Achievement to participate as 2014 innovation and technology delegate at the 51st Annual International Achievement Summit in San Francisco, California.\n\n"}
{"id": "6959783", "url": "https://en.wikipedia.org/wiki?curid=6959783", "title": "Residential gateway", "text": "Residential gateway\n\nIn telecommunications networking, a residential gateway (more commonly known as a home router or home gateway) is a device that allows a local area network (LAN) to connect to a wide area network (WAN) via a modem. The WAN can be a larger computer network (such as a municipal WAN that provides a connection to residences within the municipality) or the Internet. The connection to the WAN may use DSL, cable modem, a broadband mobile phone network, or other connection. \n\nThe term \"residential gateway\" was originally used to distinguish the inexpensive networking devices designated for use in the home from similar devices used in corporate LAN environments (which generally offered a greater array of capabilities). In recent years, however, the less expensive \"residential gateways\" have gained many of the capabilities of corporate gateways and the distinctions are fewer. Many home LANs now are able to provide most of the functions of small corporate LANs.\n\nSince the early 2000s the residential or home gateway has been used by Telecommunications Multiple Service Operators [MSOs] as a termination device for connecting consumer premises to a broadband delivery network. As a part of the carrier network, the home gateway supports remote control, detection and configuration.\n\nMultiple devices have been described as \"residential gateways\":\n\n\nor certain combinations of the above.\n\nA modem (e.g. DSL modem, Cable modem) by itself provides none of the functions of a router. It merely allows ATM or Ethernet or PPP traffic to be transmitted across telephone lines, cable wires, optical fibers, or wireless radio frequencies. On the receiving end is another modem that re-converts the transmission format back into digital data packets.\n\nThis allows network bridging using telephone, cable, optical, and radio connection methods. The modem also provides handshake protocols, so that the devices on each end of the connection are able to recognize each other. However, a modem generally provides few other network functions.\n\n\nA wireless access point can function in a similar fashion to a modem. It can allow a direct connection from a home LAN to a WAN, if a wireless router or access point is present on the WAN as well.\n\nHowever, many modems now incorporate the features mentioned below and thus are appropriately described as residential gateways.\n\nA residential gateway usually provides\n\nIt may also provide other functions such as Dynamic DNS.\n\nMost routers are self-contained components, using internally stored firmware. They are generally OS-independent, i.e., they can be accessed with any operating system.\n\nWireless routers perform the same functions as a router, but also allow connectivity for wireless devices with the LAN, or between the wireless router and another wireless router. (The wireless router-wireless router connection can be within the LAN or can be between the LAN and a WAN.)\n\nLow-cost production and requirement for user friendliness makes the home routers vulnerable to network attacks, which in the past resulted in large clusters of such devices being taken over and used to launch DDoS attacks. A majority of the vulnerabilities were present in the web administration consoles of the routers, allowing unauthorised control either via default passwords, vendor backdoors, or web vulnerabilities.\n\n"}
{"id": "11783365", "url": "https://en.wikipedia.org/wiki?curid=11783365", "title": "Sequence of events recorder", "text": "Sequence of events recorder\n\nA sequence of events recorder (SER) is an intelligent standalone microprocessor based system, which monitors external inputs and records the time and sequence of the changes. Sequence of events recorders usually have an external time source such as a GPS or radio clock. When wired inputs change state, the time and state of each change is recorded.\n\nSERs enable rapid root cause analysis after multiple events have occurred due to the secure recording of the sequence of events in the order of occurrence. SERs are therefore utilized as a diagnostic tool to minimize plant downtime. SERs are often interfaced with a SCADA system, distributed control system (DCS),or programmable logic controller (PLC). \n\nSER reports are used by electrical engineers to analyze large and small electrical system blackouts. After the Northeast blackout of 2003, the North American Electric Reliability Corporation (NERC) specified that electrical system data should be time-tagged to the nearest millisecond. \n\nIn 1984 The Tetragenics Company, a subsidiary of the Montana Power Company, introduced the first remote terminal unit (RTU) that time-tagged events to the nearest millisecond, and now there are also other RTUs with this capability. Digital protective relays and some PLCs now also include time-tagging to the nearest millisecond; SCADA systems that incorporate these devices provide SER functions without a dedicated SER device.\n\n"}
{"id": "39383574", "url": "https://en.wikipedia.org/wiki?curid=39383574", "title": "Smart Mobility Architecture", "text": "Smart Mobility Architecture\n\nComputer-on-Modules integrate the core function of a bootable computer, like SoC, as well as additional circuitry, including DRAM, boot-flash, voltage distribution, Ethernet and display transmitter. The modules are deployed together with an application-specific carrierboard, whose size and form can be defined to meet customer-specific requirements. The carrierboard executes the required interfaces and can integrate, if required, any further functionalities, such as audio codecs, touch controller, wireless communication interfaces, etc.\n\nThe SMARC specification outlines both the dimensions of the module and the positioning of the anchor points as well as the connector to the carrierboard and, most importantly, the executed interfaces with the pin-out. The pin-out is optimized for ARM and low-power SoC interfaces and is distinguished from classical PC interfaces by its target-oriented focus on low-power and mobile applications.\n\nSMARC is based on the ULP-COM form factor which was introduced by the companies Kontron and Adlink in 2011. During the specification process by the SGET the standard was renamed to SMARC.\n\nSMARC defines two module sizes: \n\nSMARC Computer-on-Modules have 314 card edge contacts on the printed circuit board (PCB) of the module which is plugged via a low-profile connector on the carrierboard. In most cases, the connector has a construction height of 4.3 mm. It is also used for Mobile PCI Express Module 3.0 graphic cards, which naturally have completely different pin assignments.\n\nSignal transmission is carried out via a total of 314 pins. 33 of these are reserved signal lines for power supply and grounding, so that with SMARC a total of 281 signal lines are effectively available. ARM- and SoC-typical energy-saving interfaces, like, for instance, parallel LCD for display connection, mobile industry processor interfaces for cameras, Serial Peripheral Interface (SPI) for general peripheral connection, I²S for audio and I2C are included. Besides these, classical computer interfaces such as USB, SATA and PCI Express are also defined.\n\nIn the current version of the SMARC specification not all of the 314 signal lines are assigned to fixed I/Os. The Alternate Function Block (AFB) has free pins available for different requirements. This is to ensure that the SMARC specification can flexibly accommodate up and coming technical developments which today are not foreseeable while remaining fully compatibility to previous designs. On the one hand, extended versions of the SMARC specification can assign new standard functions to these 20 AFB signal lines. On the other hand, the SMARC specification 1.0 currently lists the MOST (Media Oriented System Transport) bus, Dual Gigabit Ethernet, Super Speed USB, or industrial network protocols which from a current point of view could be imagined as or might be assigned as interfaces of the AFB.\n\nThe SMARC hardware specification V1.0 is supervised by the SGET. The specification is freely available as a download on the SGET website.\n\n\n"}
{"id": "39499197", "url": "https://en.wikipedia.org/wiki?curid=39499197", "title": "Stacey Ferreira", "text": "Stacey Ferreira\n\nStacey Ferreira (born September 11, 1992) is an American entrepreneur, speaker and author. She is the co-founder and CEO of Forge.\n\nFerreira, also co-founded the online bookmark vault and password manager MySocialCloud.com with her brother Scott Ferreira. MySocialCloud investors include Sir Richard Branson, Jerry Murdock, and Alex Welch. She has been featured on and contributed to several news outlets, including The Huffington Post, Women 2.0, \"Business Insider\", TechCrunch. In 2016, she entered the \"Forbes\" list of 30 under 30, as part of the 600 brightest young entrepreneurs and leaders in the USA.\n\nFerreira was born on September 11, 1992 in Scottsdale, Arizona. Her mother, Patricia Ferreira, worked in accounting for IBM and later became an accountant at the State of Arizona. Her father, Victor Ferreira, is a Vice President of Sales at IBM.\n\nShe attended Xavier College Preparatory in Phoenix, AZ, to complete her high school degree. She served as a Board Member for the nonprofit Open Table, which her brother cofounded, when she was 11. She also worked at a local Phoenix educational TV station and attended GRAMMY Camp.\n\nAfter founding MySocialCloud, Stacey attended New York University’s Steinhardt School of Culture, Education, and Human Development to study Music Business. After one year, she took a leave of absence to pursue MySocialCloud full-time.\n\nFerreira co-founded MySocialCloud with her brother, Scott Ferreira, and programmer, Shiv Prakash, while she was in high school. The idea came after a computer crash that left Scott without the spreadsheet he used to store usernames and passwords for his online accounts. In 2013, the siblings sold MySocialCloud to Reputation.com and moved from Los Angeles to the San Francisco Bay Area.\n\nAfter selling MySocialCloud, Ferreira worked with Jared Kleinert to co-author \"2 Billion Under 20: How Millennials Are Breaking Down Age Barriers And Changing The World.\" The book rose to the #1 New Release position in the Business Leadership book category on Amazon (company)\".\"\n\nIn 2015, Ferreira became a part of the Thiel Fellowship, a two-year program for individuals who've dropped out of college to build their careers. She co-founded Forge with Scott Ferreira, Lloyd Jones and Talulah Riley through the fellowship. Ferreira now serves as CEO of the company.\n\nFerreira has spoken with the United States State Department at TechCrunch Disrupt, GetInTheRing, We Are the Future’s Startup Summit, NextGen Summit, TEDxNYU, TEDxYouthSanDiego and GRAMMY Media Week. She has been a guest contributor to Virgin Entrepreneurship, Inc., Forbes, and Women 2.0. She has also received recognition for her achievements, including:\n\n\n"}
{"id": "3925463", "url": "https://en.wikipedia.org/wiki?curid=3925463", "title": "Sustainable Agriculture Research and Education", "text": "Sustainable Agriculture Research and Education\n\nSustainable Agriculture Research and Education or (SARE) is a competitive grant program established by the USDA agency, the Cooperative State Research, Education, and Extension Service. The program is subdivided into regional areas (North Central, Northeast, South, and West), each with their own leadership. The purpose of SARE is to promote research and education on sustainable agriculture practices and ensure the economic viability of the agricultural industry in the United States for future generations.\n\n\"SARE's mission is to advance, to the whole of American agriculture, innovations that improve profitability, stewardship and quality of life by investing in groundbreaking research and education.\"\n\nCongress created the Agricultural Productivity Act in response to the need for government establishment of a sustainable agriculture program in 1985. In 1988, the first congressional appropriation was made to the program in the form of $3.9 million.<ref name=\"SARE 20/20\"></ref> In 1991, the United States Environmental Protection Agency (EPA) dedicated $1 million to a joint EPA and USDA subproject of SARE entitled Agriculture in Concert with the Environment.\n\nIn the early 1990s, SARE created a national outreach office to spread public awareness to their cause. They created publications to disperse that contained information on how to begin the conversion from traditional to sustainable agriculture methods. Additionally, SARE began funding farmer-led research while Congress contributed funding to a SARE Professional Development Program, which provides members of the agriculture industry with training, grants, and resources to build awareness, knowledge, and skills related to sustainable agriculture practices.\n\nIn the late 1990s, national attention began to be brought to sustainable and organic agriculture methods. The program began to shift its focus towards marketing, local production, and the use of efficient, renewable energy sources on farms. The Secretary of Agriculture was influenced by the program to issue a memorandum pledging to make sustainability a key focus of all USDA policies and programs. The USDA National Agroforestry Center co-funded a program that lasted for 6 years, which assisted farmers in the development of agroforestry plans. Meanwhile, SARE furthered their funding of research on revolutionary agricultural methods and technology by awarding grants to graduate students' research projects nationwide.\n\nOn November 18, 2000, the program supported the Smithsonian National Museum of Natural History in their opening of an exhibit entitled \"Listening to the Prairie: Farming in Nature's Image\". The exhibit featured the techniques used by farmers to maximize their output while preserving the surrounding prairies. It ran until March 31, 2001.\n\nIn 2008, SARE celebrated its 20th anniversary. To that date, the program had funded 3,700 projects and was operating with an annual budget of approximately $19 million.\n\nThe North Central region of SARE includes: Illinois, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.\n\nThe Northeast region of SARE includes: Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, Vermont, West Virginia, and Washington, D.C..\n\nThe Southern region of SARE includes: Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, Oklahoma, Puerto Rico, South Carolina, Tennessee, Texas, U.S. Virgin Islands, and Virginia.\n\nThe Western region of SARE includes: Alaska, American Samoa, Arizona, California, Colorado, Federated States of Micronesia, Guam, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.\n\nAs of 2008, 64% of farmers who had received SARE grants stated that they had been able to earn increased profits as a result of the funding they received and utilization of sustainable agriculture methods. Additionally, 79% of grantees said that they had experienced a significant improvement in soil quality though the environmentally friendly, sustainable methods that they were utilizing on their farms.\n"}
{"id": "36229214", "url": "https://en.wikipedia.org/wiki?curid=36229214", "title": "The Typewriter", "text": "The Typewriter\n\n\"The Typewriter\" is a short composition of light music by American composer Leroy Anderson, which features an actual typewriter as a percussion instrument.\n\nAnderson completed \"The Typewriter\" on October 9, 1950 in Woodbury, Connecticut. \"The Typewriter\" received its first performance on September 8, 1953 during a recording Anderson and Boston Pops Orchestra made in New York City for Decca Records. Anderson composed melody for symphony and pops orchestras, William Zinn and Floyd Werle arranged it for string orchestras and wind bands respectively.\nIts name refers to the fact that its performance requires a typewriter. Performer uses 3 basic typewriter sounds: the sound of typing, the \"ring\" of the carriage return (a standard desk bell is used for it), and the sound of the typewriter carriage returning. In some case the sound of the typewriter carriage returning is made by musical gourd, flute, string or other instrument.\n\nThe typewriter was modified so that only two keys work to prevent the keys from jamming. According to composer himself and other musicians typewriter part is hard because of typing speed: even professional stenographers can not do it, and only professional drummers have the necessary wrist flexibility.\n\nIt has been called one of \"the wittiest and most clever pieces in the orchestral repertoire\". Author Steve Metcalf has written that \"Despite the almost total disappearance of typewriters in everyday life, the statistics show that \"The Typewriter\" is still a favorite Anderson item.\"\n\nTypewriter is used in composition as percussion instrument from the standpoint of music theory, and typewriter part is performed by percussionist/drummer usually or by conductor rarely.\n\nThe piece was featured in the Jerry Lewis film \"Who's Minding the Store\" (1963), although his first recorded performance was on a January 1954 episode of The Colgate Comedy Hour. The Radio 4 satirical programme \"The News Quiz\" has adopted the tune as its theme song. The original MS-DOS version of \"Mavis Beacon Teaches Typing!\" played a portion of the piece on startup. For several decades the tune was also the opening theme for Israel Radio's weekly news journal, which is broadcast on its \"Network B\" (\"Reshet Bet\") every Saturday morning. It was also featured as a title theme for the German comedy TV series \"Büro, Büro\", and in Adam Elliot's 2009 Australian film \"Mary and Max\". It was integrated into the \"Animaniacs\" cartoon segment \"Temporary Insanity\" (Season 1, episode 6, 1993), mimed by Yakko Warner as being typed on a nonexistent typewriter. The piece also appears in the Amazon series \"The Marvelous Mrs. Maisel\" (season 1, episode 4).\n\nThe variant of Russian name for composition \"Соло на пишущей машинке\" (\"Solo na pishushchey mashinke\", Solo on typewriter) is used for same-named self-help touch typewriting manual by Vladimir Shakhidzhanyan (1991) and later for his touch keyboarding computer trainer \"Соло на клавиатуре\" (\"Solo na klaviature\", Solo on keyboard) and for notebooks \"Соло на ундервуде\" (\"Solo na undervude\", Solo on underwood, 1980) and \"Соло на IBM\" (\"Solo na IBM\", Solo on IBM, 1990) by Sergei Dovlatov.\n\n"}
{"id": "11870065", "url": "https://en.wikipedia.org/wiki?curid=11870065", "title": "Tovex", "text": "Tovex\n\nTovex (also known as Trenchrite, Seismogel, and Seismopac) is a water-gel explosive composed of ammonium nitrate and methylammonium nitrate that has several advantages over traditional dynamite, including lower toxicity and safer manufacture, transport, and storage. It has thus almost entirely replaced dynamite.\nThere are numerous versions ranging from shearing charges to aluminized common blasting agents. Tovex is used by 80% of international oil companies for seismic exploration.\n\nThe Tovex family of products, sometimes generically called \"water gels,\" were developed by the old Explosives Department at DuPont (E.I. du Pont de Nemours & Company, Inc.) in the mid-to-late 1960s when pelletized TNT was included in aqueous gels to create a slurry form of ANFO that displayed water-resistant properties in wet bore holes. TNT-sensitized water gels were commercially successful, but the TNT led to problems with oxygen balance: namely elevated amounts of combustion by-products such as carbon monoxide and nitrogen-dioxide complexes. Not only was TNT \"dirty,\" it was also expensive. TNT was eliminated through the work of DuPont chemists Colin Dunglinson, Joseph Dean Chrisp Sr. and William Lyerly along with a team of others at DuPont's Potomac River Development Laboratory (PRDL) at Falling Waters, West Virginia, and at DuPont's Eastern Laboratories (EL) at Gibbstown, New Jersey. These chemists and engineers formulated a series of water gel-base products that replaced the TNT with methyl ammonium nitrate, also known as monomethylamine nitrate, or PR-M, (which stands for \"Potomac River – monomethylamine nitrate\"), creating the \"Tovex Extra\" product line.\n\nIn late 1973, DuPont declared \"the last days of dynamite\" and switched to the new Tovex formula. The \"Tovex\" (that replaced nitroglycerin-based dynamite) had evolved into a cap-sensitive product. Even though it bore the same name as the earlier \"Tovex,\" it was quite different from the precursors, which could only be initiated in large diameters (5 inches) with a one pound TNT booster. The new Tovex of the mid-to-late 1970s could be detonated in (critical) diameters much smaller than 5-inches by utilizing DuPont's Detaflex, thus making the new Tovex a realistic replacement for dynamite.\n\nUntil then, only nitroglycerin-based explosives were commercially feasible for blasters who wanted cap-sensitive explosives that could be initiated with a #6 blasting cap in bore holes as small as 3/4 of an inch in diameter, sometimes less. The new Tovex satisfied that requirement.\n\nAtlas, Hercules, IRECO, Trojan-US Powder and several other explosives manufacturing firms of the era created emulsions, gels and slurries which accomplished the same end, but it was the DuPont patent of PR-M based explosives () that gave the DuPont Company a competitive edge.\n\nDuPont stopped producing nitroglycerine-based dynamite in 1976, replacing it with Tovex. In 1980, it sold its Tovex technology to Explosive Technology International (ETI), a Canadian company. One ETI licensee is Biafo Industries Limited, headquartered in Islamabad, Pakistan.\n\n, explosives sold under DuPont's original \"Tovex\" trade name are distributed in Europe by Societe Suisse des Explosifs, Brigue, in Switzerland.\n\nTovex is a 50/50 aqueous solution of ammonium nitrate and methylammonium nitrate (sometimes also called monomethylamine nitrate, or PR-M), sensitized fuels, and other ingredients including sodium nitrate prills, finely divided (paint-grade) aluminum, finely divided coal, and proprietary materials to make some grades cap sensitive, and thickening agents to enhance water resistance and to act as crystal modifiers. The detonation velocity of Tovex is around 4,000 to 5,500 m·s. The specific gravity is 0.8–1.4. Tovex looks like a fluid gel which can be white to black in color.\n\n\n\"Tovex Avalanche\" is used in mountain regions for avalanche release.\n\nThe blasting product is malleable to the extent that it can be cut to length, laid out, or bundled for a wide variety of applications. Because the material requires heat and fast compression to detonate, it is safe to transport and manipulate once in the field, even if dropped from high altitudes, set on fire, or peppered with high velocity rifle bullets.\n\nFor dead trees which are considered too hazardous to remove utilizing crosscut saws or chainsaws, one or two wraps of Tovex around the base of the trunk is often enough to fell the tree safely so that the remains may be safely bucked and removed from hiking trails, parks, or other places where people recreate.\n\nFor firebreaks, the product is simply uncoiled and laid along the ground to follow the ridge line or contours of the hillside or mountain along which the fire crews wish to establish a line of defense. Fire crews then follow-up by clearing debris along the blasted line to establish a fuel-free line.\n\nFor more technical blasting which requires greater planning and finesse, Tovex is often bundled according to weight into or up against a solid material, then a detonation cord is applied to the Tovex to create a fast moving, heated dynamo effect within the Tovex when one or more blasting caps ignite the detonation cord, thus accelerating the Tovex, which causes it to detonate.\n\nBlasting caps are ignited utilizing hand-held intrinsically safe control boxes which employ a series of safety interlocks and switches which require a strict radio sessioning handshake protocol between the unit which ignites the cap and the unit used by the Master Blaster controlling the shot, ensuring that the emplaced Tovex, detonation cord, and caps cannot ignite prematurely.\n\nAfter detonation, the material is completely utilized; there is no discernible residue, unless one employs microscopic analysis. Typically, Tovex and other commercial explosives employ embedded taggants which identify the product and often the agency which purchased the material.\n\n\n"}
{"id": "53267635", "url": "https://en.wikipedia.org/wiki?curid=53267635", "title": "Traffic obstruction", "text": "Traffic obstruction\n\nTraffic obstruction is a common tactic used during public protests and demonstrations. The transport users affected by such disruptions are often unsympathetic to the cause. They have been effective in the past.\n\nMost jurisdictions consider the obstruction of traffic an illegal activity and have developed rules to prosecute those who \"block, obstruct, impede, or otherwise interfere with the normal flow of vehicular or pedestrian traffic upon a public street or highway\". Some jurisdictions also penalize slow moving vehicle traffic. The unimpeded flow of traffic in the public road-space is often considered a common right.\n\nExamples of intentional traffic obstructions aimed to articulate a protest agenda include the air traffic controller strike, highway revolts, Critical Mass bicycle rides corking intersections, obstruction of rail transport of nuclear fuel in Germany, road blockades by farmers or truckers in France and other countries, impact on Eurotunnel operations by the Migrant Crisis around Calais, pipeline protests (e.g. Dakota Access Pipeline), etc.\n\n"}
{"id": "4014747", "url": "https://en.wikipedia.org/wiki?curid=4014747", "title": "Vitrified fort", "text": "Vitrified fort\n\nVitrified forts are stone enclosures whose walls have been subjected to vitrification through heat. It was long thought that these structures were unique to Scotland, but they have since been identified in several other parts of western and northern Europe.\n\nVitrified forts are generally situated on hills offering strong defensive positions. Their form seems to have been determined by the contour of the flat summits which they enclose. The walls vary in size, a few being upwards of high, and are so broad that they present the appearance of embankments. Weak parts of the defence are strengthened by double or triple walls, and occasionally vast lines of ramparts, composed of large blocks of unhewn and unvitrified stones, envelop the vitrified centre at some distance from it. The walls themselves are termed vitrified ramparts.\n\nNo lime or cement has been found in any of these structures, all of them presenting the peculiarity of being more or less consolidated by the fusion of the rocks of which they are built. This fusion, which has been caused by the application of intense heat, is not equally complete in the various forts, or even in the walls of the same fort. In some cases the stones are only partially melted and calcined; in others their adjoining edges are fused so that they are firmly cemented together; in many instances pieces of rock are enveloped in a glassy enamel-like coating which binds them into a uniform whole; and at times, though rarely, the entire length of the wall presents one solid mass of vitreous substance.\n\nIt is not clear why or how the walls were subjected to vitrification. Some antiquarians have argued that it was done to strengthen the wall, but the heating actually weakens the structure. Battle damage is also unlikely to be the cause, as the walls are thought to have been subjected to carefully maintained fires to ensure they were hot enough for vitrification to take place.\n\nMost archaeologists now consider that vitrified forts are the product of deliberate destruction either following the capture of the site by an enemy force or by the occupants at the end of its active life as an act of ritual closure. The process has no chronological significance and is found during both Iron Age and Early Medieval Forts in Scotland.\n\nSince John Williams, one of the earliest of British geologists, and author of \"The Natural History of the Mineral Kingdom\", first described these singular ruins in 1777, over 70 examples have been discovered in Scotland. The most remarkable are:\n\nFor a long time it was supposed that these forts were peculiar to Scotland; but they are found also in the Isle of Man (Cronk Sumark) County Londonderry and County Cavan, Ireland; in Upper Lusatia, Bohemia, Silesia, Saxony, and Thuringia; in the provinces on the Rhine, especially in the neighbourhood of the Nahe; in the Ucker Lake; in Brandenburg, where the walls are formed of burnt and smelted bricks; in Hungary; in several places in France, such as Châteauvieux (near Pionnat), Péran, La Courbe, Sainte-Suzanne, Puy de Gaudy, and Thauron; also rarely in the north of England. Barksdale is a vitrified hill-fort in Uppland, Sweden.\n\nThe 16 September 1980 episode of this series features a segment in which the archaeologist Ian Ralston examines the mystery of the vitrified fort Tap o' Noth and tries to recreate how it might be accomplished by piling stones and setting a massive bonfire, repeating the work of V. Gordon Childe and Wallace Thorneycroft in the 1930s. The experiment produced a few partially vitrified stones, but no answers were gleaned as to how large-scale forts could have been crafted with the approach tried in the programme.\n\n"}
{"id": "43188437", "url": "https://en.wikipedia.org/wiki?curid=43188437", "title": "Wadia Institute of Himalayan Geology", "text": "Wadia Institute of Himalayan Geology\n\nWadia Institute of Himalayan Geology, Dehradun is an autonomous research institute for the study of Geology of the Himalaya under the Department of Science and Technology, Ministry of Science and Technology, Govt. of India. It was established in June, 1968 in the Botany Department, Delhi University, the Institute was shifted to Dehradun, Uttrakhand during April, 1976.\n\nThe institute also has three field search stations, at Naddi-Dharamshala, Dokriani Bamak Glacier Station and at Itanagar in Arunachal Pradesh.\n\nThe institute has its origins in department of Geology at University of Delhi, after being shifted to Dehradun it was initially named as the Institute of Himalayan Geology, renamed in 1976 as the Wadia Institute of Himalayan Geology in memory of its founder, late Prof. Darashaw Nosherwan Wadia (F.R.S. and National Professor), in honor to his contributions to the geology of the Himalayas. During the last quarter century the Institute has grown as a centre of excellence in the field Himalayan Geology and is recognised as a National Laboratory of international repute with advanced laboratories and other infrastructural facilities for undertaking higher level of research in the country.\n\nThe museum offers a glimpse of the mighty Himalaya, its origin, evolution in time and space, natural resources, life in the geological past, earthquakes and environmental aspects, basic objective in organising the museum is to educate students and general public as well as to highlight the Institute activities.\nMuseum, the educative wing of the Institute had a large number of student visitors from different universities, local schools and general public and as usual remained the main centre of attraction for the national and international visitors. Students in large groups from different schools, universities, colleges and from other institutions visited the Museum and guided tours were provided to them. A relief model of the Himalaya and paintings depicting the impact of human activities on the environment displayed in the Museum remained a point of attraction for the visitors. Also, over the years visitors from USA, Austria, U.K, Ukraine, Thailand, Australia, England, Japan, Nepal, France, Russia, Moscow, Israel and Canada visited the Museum.\n\nThe Centre for Himalayan Glaciology was inaugurated by the Hon'ble Minister for Science and Technology and Earth Sciences, Shri Prithviraj Chavan on 4 July 2009 in the benign presence of the Secretary, DST, Dr. T. Ramasami and Joint Secretary, Shri Sanjiv Nair.\nThe primary mission of the Centre is to \"Mount a coordinated research initiative on Himalayan glaciology to understand the factors controlling the effects of climate on glaciers in order to develop strategies for climate change adaptability for sustained growth of society\". In addition, the Centre shall take up programmes of capacity building in this very specialized field, which will eventually nurture the independent Indian Institute of Glaciology.\n\nThe Institute provides two categories of Institute Fellowships every year:\na) Junior Research Fellowship\nb) Institute Research Associate\nProject assistantships are advertised as and when vacancies in Institute projects or externally funded projects arise.\n\nThe Wadia Institute of Himalayan Geology provides consultancy and advisory services on small scale to various organisations for purpose of road alignment, site selection for bridges and their foundation, slope stability and control of landslides, site selection for deep tubewells, geotechnical feasibility of major and minor hydel projects and related structures, passenger and haulage ropeways, seismotectonics of hydel projects and environmental feasibility of developmental projects etc.\n\n"}
{"id": "2239156", "url": "https://en.wikipedia.org/wiki?curid=2239156", "title": "Web help", "text": "Web help\n\nWeb help is a type of online help that can either be delivered through the internet or as a stand-alone set of HTML files on a computer. This approach, mixing internet and local resources, is also used in Windows XP's Help and Support feature.\n\nThe original URL was WebHelp.Com (ca. 1994, also known as webhlp.Com or webhelp.us.com) owned by Tom Weaver, Palo Alto,Ca. until 1999 and 2009.\n\nSimple web help may consist of a series of web pages, while more sophisticated web help solutions feature a frameset sidebar that provides a table of contents and occasionally search capability, emulating local help resources such as HTML Help.\n\nThere are a number of tools that are used to make web help, like DocBook XSL, HelpSmith, MadCap Software's \"Flare,\" Adobe's RoboHelp, Macrobject Word-2-Web, XDocs Knowledgebase, Help & Manual, chm2web, FAR HTML, HelpMapper, SuiteHelp, Author-it or Help Explorer Server.\n\nThe advantages of web help solutions are that they permit content to be continually updated and that they sometimes give prospective buyers a deeper preview of products. Web help can be considered as a cross-platform solution since it can be viewed using a regular internet browser, while local online help runs only on a help viewer, and often only on a specific platform.\n\nAdditionally, if using a DocBook XSL system it allows the adoption of single source publishing whereby technical or other documentation (such as product manuals) can be produced in PDF format for printing as well as HTML for on-line publication and WebHelp to be distributed on CD-ROMs.\n\nThe main disadvantage of web help is that if it is exclusively on-line it may become difficult or impossible to access depending on the user's internet connection. Also, it is difficult to effectively implement context-sensitive help with web help. However, most modern tools offer solutions to this problem, making it possible to provide context-sensitive web help. For example, web help systems created with HelpSmith, MadCap Flare, RoboHelp, etc., support a number of URL parameters that can be used for integration with a web or desktop application.\n\n\n"}
