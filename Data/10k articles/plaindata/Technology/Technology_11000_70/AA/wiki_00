{"id": "2999213", "url": "https://en.wikipedia.org/wiki?curid=2999213", "title": "Antonia Fortress", "text": "Antonia Fortress\n\nThe Antonia Fortress (Aramaic:קצטרא דאנטוניה) was a military barracks built over the Hasmonean Baris by Herod the Great. Named for his patron Mark Antony, a pre 31 BC date is certain for the Fort as Mark Anthony was defeated by Octavius (later Augustus Caesar) at the sea battle of Actium in 31 BC. Built in Jerusalem on the site of earlier Ptolemaic and Hasmonean strongholds, the fortress was built at the eastern end of the great wall of the city (the second wall), on the northeastern side of the city, near the Temple Mount and the Pool of Bethesda.\n\nAlthough modern reconstructions often depict the fortress as having a tower at each of four corners, the historian Josephus repeatedly refers to it as \"the tower Antonia\", and stated that it had been built by John Hyrcanus for storing the vestments used in the Temple. However, Josephus states:\n\n\"The general appearance of the whole was that of a tower with other towers at each of the four corners; three of these turrets were fifty cubits high, while that at the south-east angle rose to seventy cubits and so commanded a view of the whole area of the temple.\"\n\nSome archaeologists are of the opinion that the fortress was only a single tower, located at the south-east corner of the site; for example, Pierre Benoit, former professor of New Testament studies at the École Biblique, having carried out extensive archaeological studies of the site, concurs and adds that there is absolutely no [archaeological] support for there having been four towers.\n\nJosephus attests to the importance of the Antonia: \"For if the Temple lay as a fortress over the city, Antonia dominated the Temple & the occupants of that post were the guards of all three.\" Josephus placed the Antonia at the northwest corner of the colonnades surrounding the Temple. Modern depictions often show the Antonia as being located along the north side of the temple enclosure. However, Josephus' description of the siege of Jerusalem suggests that it was separated from the temple enclosure itself and probably connected by two colonnades with a narrow space between them. Josephus' measurements suggest about a 600-foot separation between the two complexes.\n\nWhy did the two 600-foot aerial bridges disappear from the pages of history? They were mentioned in two 19th-century books written by scholars Lewin, Sanday & Waterhouse, who probably read Josephus in the original Greek, whilst others, later relied on William Whiston, an 18th-century translator. We cannot know if Whiston was influenced by traditional thinking but he probably decided that Josephus had erred when he gave the length of the aerial roadways as a furlong (Stadion), so Whiston used the words “no long space of ground”. War VI, 2, 144\n\nBased upon Jerusalem’s topography and the impossibility of placing Fort Antonia six hundred feet further north of the alleged Temple Mount, Whiston’s translation obscured their existence, although there are ten references in Josephus to these bridges.\n\nPrior to the First Jewish–Roman War, the Antonia housed some part of the Roman garrison of Jerusalem. The Romans also stored the high priest's vestments within the Fortress.\n\nDuring the defence of Herod’s Temple, supposedly the Jewish fighters demolished the Tower of Antonia. Josephus is adamant the Jews had no chance of destroying a huge Roman fort with 60-foot walls, defended by thousands of Roman troops. It’s the destruction of the two 600-foot aerial bridges that is meant. It fulfilled the prophecy: “When square the walls, the Temple falls.” Roman soldiers then hastened to construct siege banks against the Temple’s north wall. Battle lasted until they seized the sanctuary.\n\nTraditionally, it has been thought that the vicinity of the Antonia Fortress later became the site of the \"Praetorium\", and that this latter building was the place where Jesus was taken to stand before Pilate (see Pilate's court). However, this tradition was based on the mistaken assumption that an area of Roman flagstones, discovered beneath the Church of the Condemnation and Imposition of the Cross and the Convent of the Sisters of Zion, was the pavement (Greek: \"lithostratos\") which the Bible describes as the location of Pontius Pilate's judgment of Jesus; archaeological investigation now indicates that these slabs are the paving of the eastern of two 2nd century Forums, built by Hadrian as part of the construction of Aelia Capitolina. The site of the Forum had previously been a large open-air pool, the Struthion Pool, which was constructed by the Hasmoneans, is mentioned by Josephus as being adjacent to the Fortress in the 1st century, and is still present beneath Hadrian's flagstones; the traditional scene would require that everyone was walking on water.\n\nLike Philo, Josephus testifies that the Roman governors stayed in Herod's Palace while they were in Jerusalem, and carrying out their judgements on the pavement immediately outside it; Josephus indicates that Herod's palace is on the Western Hill (Upper City) and it has recently (2001) been rediscovered under a corner of the Jaffa Gate citadel according to Jacqueline Schaalje, \"Israeli Archaeologists Discover Herod's Palace\", The Jewish Magazine (October 2001}. Archaeologists now therefore conclude that in the 1st century, the \"Praetorium\" – the residence of the governor (\"Praefectus – later Procurator\") – was on the Western Hill, rather than the Antonia Fortress, on the diametrically opposite side of the city.\n\n"}
{"id": "3581595", "url": "https://en.wikipedia.org/wiki?curid=3581595", "title": "Blackstone Canal", "text": "Blackstone Canal\n\nThe Blackstone Canal was a waterway linking Worcester, Massachusetts, to Providence, Rhode Island (and Narragansett Bay) through the Blackstone Valley via a series of locks and canals during the early 19th century.\n\nThe initiative for the canal came from Providence, where a merchant community wished to profit from trade with the farming country of the Blackstone Valley and Worcester County. The people of Worcester and the Blackstone Valley, eager for transport that would enable them to get better prices for their produce, welcomed the plan. However, since the trade of central Massachusetts was at that time going overland through the port of Boston, Massachusetts commercial interests succeeded in stalling the project for several years. Finally, in 1823, the Blackstone Canal Company was organized through an act of the Massachusetts legislature, with a Rhode Island company soon following. The canal's construction may have been motivated by competition among rival industrialists to curtail \"water rights\".\n\nConstruction began in 1825 and cost $750,000 (twice its initial estimate). The canal opened on October 7, 1828 when the packet boat \"Lady Carrington\" arrived in Worcester, the first vessel to make the trip. The canal brought immediate prosperity to Worcester and the Valley; farmers' profits increased and mills were built, especially in Worcester. Using water to transport goods was a great improvement over the rough roads of the era. At the time of its construction, it represented the best available transportation technology.\n\nIt was a two-day trip for the canal boats from Worcester to Providence and another two-day trip to return to Worcester. The overnight stopping point was in Uxbridge. Boston merchants moved to recapture the trade moving down the canal to Providence, opening a rail line to Worcester in 1835. (Boston merchants opened three railroads in 1835, one to Lowell, one to Worcester, and one to Providence, RI. These were very new technology.) In 1847 the parallel Providence and Worcester Railroad began operation, and the canal closed in 1848.\n\nThe canal was 35 feet or more in width. It ascended 451 feet, passing through an original 49 locks. The canal used \"slack-water\" to bypass narrow valley areas and intersected the Blackstone River 16 times over its 45-mile course. It ran in the river itself for 10% of its length. These portions proved troublesome since in summer, water was sometimes too low for navigation. In other periods, flooding was the problem. Winter added ice to the issues.\n\nEach lock was 70 feet long by 10 feet wide and apparently 4–4.5 feet deep. All, except the wooden tide lock at Providence, were built of granite on a wooden foundation. The average lift of each lock was 9.2 feet, but there were variations.\n\nThe canal was engineered by Benjamin Wright (chief engineer of the Erie Canal) and Holmes Hutchinson (later Chief Engineer of New York's canals) as they were concluding work on the original Erie Canal. Its locks are therefore similar, although smaller, than the original Erie's.\n\nThe canal followed the course of Mill Creek from the center of Worcester, MA to the southern part of that community. It then followed the Blackstone River downstream to Ashton, RI. At Ashton, it paralleled the river's west bank, maintaining elevation until it entered Cranberry and Scotts Ponds on the divide of the watershed with the Moshassuck River. It then locked down into the Moshassuck Valley and followed that stream into Providence, RI.\n\nSince the canal's closure, parts of its watercourse were filled and other segments are now heavily overgrown with brush and trees. Other sections were converted to hydraulic canals for textile mills. Much of the lock masonry was sold for other purposes. Its remains, however, are still visible in many locations. About 85% of the canal remains. Two long sections of the canal exist in the Blackstone River and Canal Heritage State Park in Massachusetts and the Blackstone River State Park in Rhode Island. Other sections are also publicly owned.\n\nThe canal is the subject of several listings on the National Register of Historic Places. The southernmost portion of the canal, between Providence and the Ashton Dam in Lincoln, Rhode Island, was listed in 1971; this list was expanded in 1991 to include the entire Rhode Island section. A section of the canal in Uxbridge and Northbridge, Massachusetts was listed in 1973; this listing was expanded in 1995 to encompass the entire historic footprint of the canal in Massachusetts.\n\nThe Blackstone Canal is historically significant as a transportation network arising from a canal-building frenzy of the early 19th-century. The canal played a key role in stimulating the industry between Providence and Worcester and the towns and villages along the Moshassuck\nand Blackstone Rivers. Though its useful service life was just twenty years long, the canal and the parks incorporating the canal are \"visible and tangible parts of the city's and the state's history of growth, planning, commercial ambition and prosperity\".\n\n\n"}
{"id": "36706385", "url": "https://en.wikipedia.org/wiki?curid=36706385", "title": "Borqs", "text": "Borqs\n\nBorqs Technologies Inc.(), founded in 2007, Borqs marked its debut as a public company on August 18, 2017, changing its name to Borqs Technologies, Inc. Listed on The NASDAQ Stock Market, under the trading symbol, BRQS. With research and development centers in China and India—and presence in the USA, Japan and Korea—Borqs Technologies Inc. designs, develops and deploys products and turnkey solutions. In addition to developing the first commercial-grade software to support video telephony for Android, Borqs was selected by Qualcomm in 2016 to partner in developing the world’s first 4G Android wearable, which includes smartwatches for children and adults, as well as watches designed for fitness. \n\n\n\nBorqs has been a member of the Open Handset Alliance since 2008 and is also a member of the Symbian Foundation. Borqs is notable as the developer of the OPhone, or Open Mobile System (OMS), for China Mobile. The OPhone is a Linux-kernel-based open source software platform that has been used in conjunction with China Mobile's proprietary TD-SCDMA 3G network.\n\nVersion 2.0 of the Ophone software has 50,000 registered developers and was launched in late 2009. China Mobile has signed up more than 20 handset vendors to develop phones for the Ophone OS. OPhone accounted for 38% of the TD-SCDMA smartphone market at the end of 2011. It has been speculated in the Chinese media that, in time, the OPhone may break away from Android entirely and establish itself as a fully independent operating system.\n\nBorqs has brought its OPhone software to the US under the name Android+. Borqs provided software under the Android+ branding for Dell's line-up of smartphones in the US.\n"}
{"id": "34306532", "url": "https://en.wikipedia.org/wiki?curid=34306532", "title": "CHFS", "text": "CHFS\n\nCHFS is a file system developed at the Department of Software Engineering, University of Szeged, Hungary. It was the first open source Flash memory-specific file system written for the NetBSD operating system.\n\nSimilar to UBIFS, the CHFS file system utilizes a separate layer for handling Flash aging and bad blocks, called EBH (erase block handler). The file system itself is modelled after JFFS2, thus the internal structure is very similar.\n\nCHFS was originally called ChewieFS during development. The name was changed to avoid legal issues and to have a more neutral name.\n\n\n"}
{"id": "1921414", "url": "https://en.wikipedia.org/wiki?curid=1921414", "title": "Castle Technology", "text": "Castle Technology\n\nCastle Technology Limited, named after Framlingham Castle, is a British computer company based in Cambridge, England. It began as a producer of ARM computers and manufactured the Acorn-branded range of desktop computers that run RISC OS.\n\nFollowing the break-up of Acorn in 1998, Castle Technology bought the rights to continue production of the RISC PC and A7000+ computers under the Acorn brand. Castle Technology later released the Iyonix PC in November 2002, the first desktop computer to use the Intel XScale microarchitecture and then bought the rights to the RISC OS Technology from Pace in July 2003.\n\nAfter Acorn withdrew from the desktop computer industry in 1998, Castle Technology acquired the rights to produce the A7000, A7000+ and RISC PC using the \"Acorn\" brand.\n\nThe Iyonix PC (codenamed \"Tungsten\") was developed as a set-top unit (STU) in secret by engineers at Pace's Shipley campus along with a 32-bit version of RISC OS 4 (known as RISC OS 5). When management discovered the project the campus was closed.\n\nCastle Technology acquired the proposed designs and the original engineers from Pace to further develop the Tungsten into a desktop computer. Robert Sprowson, the original hardware designer, declined to join Castle Technology and so Peter Wild was recruited.\n\nThe Iyonix PC was released six months later. Although it was well received, it was not designed for long-term production and therefore used some components that were near obsolete when it was released.\n\nA problem for the Iyonix PC was that it used leaded components which were outlawed with the adoption of Directive 2002/95/EC in February 2003 by the European Union. However, by this time Castle Technology was financially troubled and could not afforded to re-engineer the Iyonix PC without the leaded components. The remaining Iyonix stock were passed to Iyonix Limited which stopped distribution on 30 September 2008.\n\nIn 2004, Castle Technology acquired Tematic to further development of RISC OS in embedded systems.\n\nIn December 2005, Castle Technology moved its main office to the former premises of its software development division, following the migration of Tematic to a neighbouring premises in Signet Court, Cambridge.\n"}
{"id": "318378", "url": "https://en.wikipedia.org/wiki?curid=318378", "title": "Crane (machine)", "text": "Crane (machine)\n\nA crane is a type of machine, generally equipped with a hoist rope, wire ropes or chains, and sheaves, that can be used both to lift and lower materials and to move them horizontally. It is mainly used for lifting heavy things and transporting them to other places. The device uses one or more simple machines to create mechanical advantage and thus move loads beyond the normal capability of a human. Cranes are commonly employed in the transport industry for the loading and unloading of freight, in the construction industry for the movement of materials, and in the manufacturing industry for the assembling of heavy equipment.\n\nThe first known construction cranes were invented by the Ancient Greeks and were powered by men or beasts of burden, such as donkeys. These cranes were used for the construction of tall buildings. Larger cranes were later developed, employing the use of human treadwheels, permitting the lifting of heavier weights. In the High Middle Ages, harbour cranes were introduced to load and unload ships and assist with their construction – some were built into stone towers for extra strength and stability. The earliest cranes were constructed from wood, but cast iron, iron and steel took over with the coming of the Industrial Revolution.\n\nFor many centuries, power was supplied by the physical exertion of men or animals, although hoists in watermills and windmills could be driven by the harnessed natural power. The first 'mechanical' power was provided by steam engines, the earliest steam crane being introduced in the 18th or 19th century, with many remaining in use well into the late 20th century. Modern cranes usually use internal combustion engines or electric motors and hydraulic systems to provide a much greater lifting capability than was previously possible, although manual cranes are still utilized where the provision of power would be uneconomic.\n\nCranes exist in an enormous variety of forms – each tailored to a specific use. Sizes range from the smallest jib cranes, used inside workshops, to the tallest tower cranes, used for constructing high buildings. Mini-cranes are also used for constructing high buildings, in order to facilitate constructions by reaching tight spaces. Finally, we can find larger floating cranes, generally used to build oil rigs and salvage sunken ships.\n\nSome lifting machines do not strictly fit the above definition of a crane, but are generally known as cranes, such as stacker cranes and loader cranes.\n\nCranes were so called from the resemblance to the long neck of the bird, cf. , French \"grue\".\n\nThe crane for lifting heavy loads was invented by the Ancient Greeks in the late 6th century BC. The archaeological record shows that no later than c. 515 BC distinctive cuttings for both lifting tongs and lewis irons begin to appear on stone blocks of Greek temples. Since these holes point at the use of a lifting device, and since they are to be found either above the center of gravity of the block, or in pairs equidistant from a point over the center of gravity, they are regarded by archaeologists as the positive evidence required for the existence of the crane.\n\nThe introduction of the winch and pulley hoist soon lead to a widespread replacement of ramps as the main means of vertical motion. For the next 200 years, Greek building sites witnessed a sharp reduction in the weights handled, as the new lifting technique made the use of several smaller stones more practical than fewer larger ones. In contrast to the archaic period with its pattern of ever-increasing block sizes, Greek temples of the classical age like the Parthenon invariably featured stone blocks weighing less than 15–20 metric tons. Also, the practice of erecting large monolithic columns was practically abandoned in favour of using several column drums.\n\nAlthough the exact circumstances of the shift from the ramp to the crane technology remain unclear, it has been argued that the volatile social and political conditions of Greece were more suitable to the employment of small, professional construction teams than of large bodies of unskilled labour, making the crane more preferable to the Greek polis than the more labour-intensive ramp which had been the norm in the autocratic societies of Egypt or Assyria.\n\nThe first unequivocal literary evidence for the existence of the compound pulley system appears in the \"Mechanical Problems\" (\"Mech\". 18, 853a32–853b13) attributed to Aristotle (384–322 BC), but perhaps composed at a slightly later date. Around the same time, block sizes at Greek temples began to match their archaic predecessors again, indicating that the more sophisticated compound pulley must have found its way to Greek construction sites by then.\n\nThe heyday of the crane in ancient times came during the Roman Empire, when construction activity soared and buildings reached enormous dimensions. The Romans adopted the Greek crane and developed it further. We are relatively well informed about their lifting techniques, thanks to rather lengthy accounts by the engineers Vitruvius (\"De Architectura\" 10.2, 1–10) and Heron of Alexandria (\"Mechanica\" 3.2–5). There are also two surviving reliefs of Roman treadwheel cranes, with the Haterii tombstone from the late first century AD being particularly detailed.\n\nThe simplest Roman crane, the \"trispastos\", consisted of a single-beam jib, a winch, a rope, and a block containing three pulleys. Having thus a mechanical advantage of 3:1, it has been calculated that a single man working the winch could raise 150 kg (3 pulleys x 50 kg = 150), assuming that 50 kg represent the maximum effort a man can exert over a longer time period. Heavier crane types featured five pulleys (\"pentaspastos\") or, in case of the largest one, a set of three by five pulleys (\"Polyspastos\") and came with two, three or four masts, depending on the maximum load. The \"polyspastos\", when worked by four men at both sides of the winch, could readily lift 3,000 kg (3 ropes x 5 pulleys x 4 men x 50 kg = 3,000 kg). If the winch was replaced by a treadwheel, the maximum load could be doubled to 6,000 kg at only half the crew, since the treadwheel possesses a much bigger mechanical advantage due to its larger diameter. This meant that, in comparison to the construction of the ancient Egyptian pyramids, where about 50 men were needed to move a 2.5 ton stone block up the ramp (50 kg per person), the lifting capability of the Roman \"polyspastos\" proved to be \"60 times\" higher (3,000 kg per person).\n\nHowever, numerous extant Roman buildings which feature much heavier stone blocks than those handled by the \"polyspastos\" indicate that the overall lifting capability of the Romans went far beyond that of any single crane. At the temple of Jupiter at Baalbek, for instance, the architrave blocks weigh up to 60 tons each, and one corner cornice block even over 100 tons, all of them raised to a height of about 19 m. In Rome, the capital block of Trajan's Column weighs 53.3 tons, which had to be lifted to a height of about 34 m (see construction of Trajan's Column).\n\nIt is assumed that Roman engineers lifted these extraordinary weights by two measures (see picture below for comparable Renaissance technique): First, as suggested by Heron, a lifting tower was set up, whose four masts were arranged in the shape of a quadrangle with parallel sides, not unlike a siege tower, but with the column in the middle of the structure (\"Mechanica\" 3.5). Second, a multitude of capstans were placed on the ground around the tower, for, although having a lower leverage ratio than treadwheels, capstans could be set up in higher numbers and run by more men (and, moreover, by draught animals). This use of multiple capstans is also described by Ammianus Marcellinus (17.4.15) in connection with the lifting of the Lateranense obelisk in the Circus Maximus (c. 357 AD). The maximum lifting capability of a single capstan can be established by the number of lewis iron holes bored into the monolith. In case of the Baalbek architrave blocks, which weigh between 55 and 60 tons, eight extant holes suggest an allowance of 7.5 ton per lewis iron, that is per capstan. Lifting such heavy weights in a concerted action required a great amount of coordination between the work groups applying the force to the capstans.\n\nDuring the High Middle Ages, the treadwheel crane was reintroduced on a large scale after the technology had fallen into disuse in western Europe with the demise of the Western Roman Empire. The earliest reference to a treadwheel (\"magna rota\") reappears in archival literature in France about 1225, followed by an illuminated depiction in a manuscript of probably also French origin dating to 1240. In navigation, the earliest uses of harbor cranes are documented for Utrecht in 1244, Antwerp in 1263, Brugge in 1288 and Hamburg in 1291, while in England the treadwheel is not recorded before 1331.\n\nGenerally, vertical transport could be done more safely and inexpensively by cranes than by customary methods. Typical areas of application were harbors, mines, and, in particular, building sites where the treadwheel crane played a pivotal role in the construction of the lofty Gothic cathedrals. Nevertheless, both archival and pictorial sources of the time suggest that newly introduced machines like treadwheels or wheelbarrows did not completely replace more labor-intensive methods like ladders, hods and handbarrows. Rather, old and new machinery continued to coexist on medieval construction sites and harbors.\n\nApart from treadwheels, medieval depictions also show cranes to be powered manually by windlasses with radiating spokes, cranks and by the 15th century also by windlasses shaped like a ship's wheel. To smooth out irregularities of impulse and get over 'dead-spots' in the lifting process flywheels are known to be in use as early as 1123.\n\nThe exact process by which the treadwheel crane was reintroduced is not recorded, although its return to construction sites has undoubtedly to be viewed in close connection with the simultaneous rise of Gothic architecture. The reappearance of the treadwheel crane may have resulted from a technological development of the windlass from which the treadwheel structurally and mechanically evolved. Alternatively, the medieval treadwheel may represent a deliberate reinvention of its Roman counterpart drawn from Vitruvius' \"De architectura\" which was available in many monastic libraries. Its reintroduction may have been inspired, as well, by the observation of the labor-saving qualities of the waterwheel with which early treadwheels shared many structural similarities.\n\nThe medieval treadwheel was a large wooden wheel turning around a central shaft with a treadway wide enough for two workers walking side by side. While the earlier 'compass-arm' wheel had spokes directly driven into the central shaft, the more advanced 'clasp-arm' type featured arms arranged as chords to the wheel rim, giving the possibility of using a thinner shaft and providing thus a greater mechanical advantage.\n\nContrary to a popularly held belief, cranes on medieval building sites were neither placed on the extremely lightweight scaffolding used at the time nor on the thin walls of the Gothic churches which were incapable of supporting the weight of both hoisting machine and load. Rather, cranes were placed in the initial stages of construction on the ground, often within the building. When a new floor was completed, and massive tie beams of the roof connected the walls, the crane was dismantled and reassembled on the roof beams from where it was moved from bay to bay during construction of the vaults. Thus, the crane 'grew' and 'wandered' with the building with the result that today all extant construction cranes in England are found in church towers above the vaulting and below the roof, where they remained after building construction for bringing material for repairs aloft.\n\nLess frequently, medieval illuminations also show cranes mounted on the outside of walls with the stand of the machine secured to putlogs.\n\nIn contrast to modern cranes, medieval cranes and hoists – much like their counterparts in Greece and Rome – were primarily capable of a vertical lift, and not used to move loads for a considerable distance horizontally as well. Accordingly, lifting work was organized at the workplace in a different way than today. In building construction, for example, it is assumed that the crane lifted the stone blocks either from the bottom directly into place, or from a place opposite the centre of the wall from where it could deliver the blocks for two teams working at each end of the wall. Additionally, the crane master who usually gave orders at the treadwheel workers from outside the crane was able to manipulate the movement laterally by a small rope attached to the load. Slewing cranes which allowed a rotation of the load and were thus particularly suited for dockside work appeared as early as 1340. While ashlar blocks were directly lifted by sling, lewis or devil's clamp (German \"Teufelskralle\"), other objects were placed before in containers like pallets, baskets, wooden boxes or barrels.\n\nIt is noteworthy that medieval cranes rarely featured ratchets or brakes to forestall the load from running backward. This curious absence is explained by the high friction force exercised by medieval tread-wheels which normally prevented the wheel from accelerating beyond control.\n\nAccording to the \"present state of knowledge\" unknown in antiquity, stationary harbor cranes are considered a new development of the Middle Ages. The typical harbor crane was a pivoting structure equipped with double treadwheels. These cranes were placed docksides for the loading and unloading of cargo where they replaced or complemented older lifting methods like see-saws, winches and yards.\n\nTwo different types of harbor cranes can be identified with a varying geographical distribution: While gantry cranes which pivoted on a central vertical axle were commonly found at the Flemish and Dutch coastside, German sea and inland harbors typically featured tower cranes where the windlass and treadwheels were situated in a solid tower with only jib arm and roof rotating. Dockside cranes were not adopted in the Mediterranean region and the highly developed Italian ports where authorities continued to rely on the more labor-intensive method of unloading goods by ramps beyond the Middle Ages.\n\nUnlike construction cranes where the work speed was determined by the relatively slow progress of the masons, harbor cranes usually featured double treadwheels to speed up loading. The two treadwheels whose diameter is estimated to be 4 m or larger were attached to each side of the axle and rotated together. Their capacity was 2–3 tons which apparently corresponded to the customary size of marine cargo. Today, according to one survey, fifteen treadwheel harbor cranes from pre-industrial times are still extant throughout Europe. Some harbour cranes were specialised at mounting masts to newly built sailing ships, such as in Gdańsk, Cologne and Bremen. Beside these stationary cranes, floating cranes which could be flexibly deployed in the whole port basin came into use by the 14th century.\n\nA lifting tower similar to that of the ancient Romans was used to great effect by the Renaissance architect Domenico Fontana in 1586 to relocate the 361 t heavy Vatican obelisk in Rome. From his report, it becomes obvious that the coordination of the lift between the various pulling teams required a considerable amount of concentration and discipline, since, if the force was not applied evenly, the excessive stress on the ropes would make them rupture.\n\nCranes were also used domestically during this period. The chimney or fireplace crane was used to swing pots and kettles over the fire and the height was adjusted by a trammel.\n\nWith the onset of the Industrial Revolution the first modern cranes were installed at harbours for loading cargo. In 1838, the industrialist and businessman William Armstrong designed a hydraulic water powered crane. His design used a ram in a closed cylinder that was forced down by a pressurized fluid entering the cylinder – a valve regulated the amount of fluid intake relative to the load on the crane.\n\nIn 1845 a scheme was set in motion to provide piped water from distant reservoirs to the households of Newcastle. Armstrong was involved in this scheme and he proposed to Newcastle Corporation that the excess water pressure in the lower part of town could be used to power one of his hydraulic cranes for the loading of coal onto barges at the Quayside. He claimed that his invention would do the job faster and more cheaply than conventional cranes. The corporation agreed to his suggestion, and the experiment proved so successful that three more hydraulic cranes were installed on the Quayside.\n\nThe success of his hydraulic crane led Armstrong to establish the Elswick works at Newcastle, to produce his hydraulic machinery for cranes and bridges in 1847. His company soon received orders for hydraulic cranes from Edinburgh and Northern Railways and from Liverpool Docks, as well as for hydraulic machinery for dock gates in Grimsby. The company expanded from a workforce of 300 and an annual production of 45 cranes in 1850, to almost 4,000 workers producing over 100 cranes per year by the early 1860s.\n\nArmstrong spent the next few decades constantly improving his crane design – his most significant innovation was the hydraulic accumulator. Where water pressure was not available on site for the use of hydraulic cranes, Armstrong often built high water towers to provide a supply of water at pressure. However, when supplying cranes for use at New Holland on the Humber Estuary, he was unable to do this because the foundations consisted of sand. He eventually produced the hydraulic accumulator, a cast-iron cylinder fitted with a plunger supporting a very heavy weight. The plunger would slowly be raised, drawing in water, until the downward force of the weight was sufficient to force the water below it into pipes at great pressure. This invention allowed much larger quantities of water to be forced through pipes at a constant pressure, thus increasing the crane's load capacity considerably.\n\nOne of his cranes, commissioned by the Italian Navy in 1883 and in use until the mid-1950s, is still standing in Venice, where it is now in a state of disrepair.\n\nThere are three major considerations in the design of cranes. First, the crane must be able to lift the weight of the load; second, the crane must not topple; third, the crane must not rupture.\n\nFor stability, the sum of all moments about the base of the crane must be close to zero so that the crane does not overturn. In practice, the magnitude of load that is permitted to be lifted (called the \"rated load\" in the US) is some value less than the load that will cause the crane to tip, thus providing a safety margin.\n\nUnder US standards for mobile cranes, the stability-limited rated load for a crawler crane is 75% of the tipping load. The stability-limited rated load for a mobile crane supported on outriggers is 85% of the tipping load. These requirements, along with additional safety-related aspects of crane design, are established by the American Society of Mechanical Engineers in the volume ASME B30.5-2018 \"Mobile and Locomotive Cranes\".\n\nStandards for cranes mounted on ships or offshore platforms are somewhat stricter because of the dynamic load on the crane due to vessel motion. Additionally, the stability of the vessel or platform must be considered.\n\nFor stationary pedestal or kingpost mounted cranes, the moment created by the boom, jib, and load is resisted by the pedestal base or kingpost. Stress within the base must be less than the yield stress of the material or the crane will fail.\n\nThere are four principal types of mobile cranes: truck mounted, rough-terrain, crawler, and floating.\n\nA truck-mounted crane has two parts: the carrier, often referred to as the \"lower\", and the lifting component which includes the boom, referred to as the \"upper\". These are mated together through a turntable, allowing the upper to swing from side to side. These modern hydraulic truck cranes are usually single-engine machines, with the same engine powering the undercarriage and the crane. The upper is usually powered via hydraulics run through the turntable from the pump mounted on the lower. In older model designs of hydraulic truck cranes, there were two engines. One in the lower pulled the crane down the road and ran a hydraulic pump for the outriggers and jacks. The one in the upper ran the upper through a hydraulic pump of its own. Many older operators favor the two-engine system due to leaking seals in the turntable of aging newer design cranes. Hiab invented the world's first hydraulic truck mounted crane in 1947. The name, Hiab, comes from the commonly used abbreviation of Hydrauliska Industri AB, a company founded in Hudiksvall, Sweden 1944 by Eric Sundin, a ski manufacturer who saw a way to utilize a truck's engine to power loader cranes through the use of hydraulics.\n\nGenerally, these cranes are able to travel on highways, eliminating the need for special equipment to transport the crane unless weight or other size constrictions are in place such as local laws. If this is the case, most larger cranes are equipped with either special trailers to help spread the load over more axles or are able to disassemble to meet requirements. An example is counterweights. Often a crane will be followed by another truck hauling the counterweights that are removed for travel. In addition some cranes are able to remove the entire upper. However, this is usually only an issue in a large crane and mostly done with a conventional crane such as a Link-Belt HC-238. When working on the job site, outriggers are extended horizontally from the chassis then vertically to level and stabilize the crane while stationary and hoisting. Many truck cranes have slow-travelling capability (a few miles per hour) while suspending a load. Great care must be taken not to swing the load sideways from the direction of travel, as most anti-tipping stability then lies in the stiffness of the chassis suspension. Most cranes of this type also have moving counterweights for stabilization beyond that provided by the outriggers. Loads suspended directly aft are the most stable, since most of the weight of the crane acts as a counterweight. Factory-calculated charts (or electronic safeguards) are used by crane operators to determine the maximum safe loads for stationary (outriggered) work as well as (on-rubber) loads and travelling speeds.\n\nTruck cranes range in lifting capacity from about to about . Although most only rotate about 180 degrees, the more expensive truck mounted cranes can turn a full 360 degrees.\n\nA rough terrain crane has a boom mounted on an undercarriage atop four rubber tires that is designed for off-road pick-and-carry operations. Outriggers are used to level and stabilize the crane for hoisting.\n\nThese telescopic cranes are single-engine machines, with the same engine powering the undercarriage and the crane, similar to a crawler crane. The engine is usually mounted in the undercarriage rather than in the upper, as with crawler crane. Most have 4 wheel drive and 4 wheel steering for traversing tighter and slicker terrain than a standard truck crane, with less site prep.\n\nA crawler crane has its boom mounted on an undercarriage fitted with a set of crawler tracks that provide both stability and mobility. Crawler cranes range in lifting capacity from about .\n\nThe main advantage of a crawler crane is its ready mobility and use, since the crane is able to operate on sites with minimal improvement and stable on its tracks without outriggers. Wide tracks spread the weight out over a great area and are far better than wheels at traversing soft ground without sinking in. A crawler crane is also capable of traveling with a load. Its main disadvantage is its weight, making it difficult and expensive to transport. Typically a large crawler must be disassembled at least into boom and cab and moved by trucks, rail cars or ships to its next location.\n\nFloating cranes are used mainly in bridge building and port construction, but they are also used for occasional loading and unloading of especially heavy or awkward loads on and off ships. \nSome floating cranes are mounted on pontoons, others are specialized crane barges with a lifting capacity exceeding and have been used to transport entire bridge sections. Floating cranes have also been used to salvage sunken ships.\n\nCrane vessels are often used in offshore construction. \nThe largest revolving cranes can be found on SSCV Thialf, which has two cranes with a capacity of each. For 50 years, the largest such crane was \"Herman the German\" at the Long Beach Naval Shipyard, one of three constructed by Hitler's Germany and captured in the war. The crane was sold to the Panama Canal in 1996 where it is now known as the \"Titan.\"\n\nAn all-terrain crane is a hybrid combining the roadability of a truck-mounted and on-site maneuverability of a rough-terrain crane. It can both travel at speed on public roads and maneuver on rough terrain at the job site using all-wheel and crab steering.\n\nAT’s have 2-9 axles and are designed for lifting loads up to .\n\nA pick and carry crane is similar to a mobile crane in that is designed to travel on public roads; however, Pick and Carry cranes have no stabiliser legs or outriggers and are designed to lift the load and carry it to its destination, within a small radius, then be able to drive to the next job. Pick and Carry cranes are popular in Australia where large distances are encountered between job sites. One popular manufacturer in Australia was Franna, who have since been bought by Terex, and now all Pick and Carry cranes are commonly referred to as \"Frannas\" even though they may be made by other manufacturers. Nearly every medium and large sized crane company in Australia has at least one and many companies have fleets of these cranes. The capacity range is usually ten to twenty tonnes maximum lift, although this is much less at the tip of the boom. Pick and Carry cranes have displaced the work usually completed by smaller truck cranes as the set-up time is much quicker. Many steel fabrication yards also use Pick and Carry cranes as they can \"walk\" with fabricated steel sections and place these where required with relative ease.\n\nA sidelifter crane is a road-going truck or semi-trailer, able to hoist and transport ISO standard containers. Container lift is done with parallel crane-like hoists, which can lift a container from the ground or from a railway vehicle.\n\nA carry deck crane is a small 4 wheel crane with a 360 degree rotating boom placed right in the centre and an operators cab located at one end under this boom. The rear section houses the engine and the area above the wheels is a flat deck. Very much an American invention the Carry deck can hoist a load in a confined space and then load it on the deck space around the cab or engine and subsequently move to another site. The Carry Deck principle is the American version of the pick and carry crane and both allow the load to be moved by the crane over short distances.\n\nTelescopic handlers are like forklift trucks that have a telescoping extendable boom like a crane. Early telescopic handlers only lifted in one direction and did not rotate; however, several of the manufacturers have designed telescopic handlers that rotate 360 degrees through a turntable and these machines look almost identical to the Rough Terrain Crane. These new 360-degree telescopic handler/crane models have outriggers or stabiliser legs that must be lowered before lifting; however, their design has been simplified so that they can be more quickly deployed. These machines are often used to handle pallets of bricks and install frame trusses on many new building sites and they have eroded much of the work for small telescopic truck cranes. Many of the world's armed forces have purchased telescopic handlers and some of these are the much more expensive fully rotating types. Their off-road capability and their on site versatility to unload pallets using forks, or lift like a crane make them a valuable piece of machinery.\nDry bulk or container cranes usually in the bay areas or inland water ways.\n\nA railroad crane has flanged wheels for use on railroads. \nThe simplest form is a crane mounted on a flatcar. More capable devices are purpose-built. Different types of crane are used for maintenance work, recovery operations and freight loading in goods yards and scrap handling facilities.\n\nAerial crane or 'Sky cranes' usually are helicopters designed to lift large loads. Helicopters are able to travel to and lift in areas that are difficult to reach by conventional cranes. Helicopter cranes are most commonly used to lift units/loads onto shopping centers and highrises. They can lift anything within their lifting capacity, (cars, boats, swimming pools, etc.). They also perform disaster relief after natural disasters for clean-up, and during wild-fires they are able to carry huge buckets of water to extinguish fires.\n\nSome aerial cranes, mostly concepts, have also used lighter-than air aircraft, such as airships.\nExchanging mobility for the ability to carry greater loads and reach greater heights due to increased stability, these types of cranes are characterised by the fact that their main structure does not move during the period of use. However, many can still be assembled and disassembled. The structures basically are fixed in one place.\n\nTower cranes are a modern form of balance crane that consist of the same basic parts. Fixed to the ground on a concrete slab (and sometimes attached to the sides of structures), tower cranes often give the best combination of height and lifting capacity and are used in the construction of tall buildings. The base is then attached to the mast which gives the crane its height. Further, the mast is attached to the slewing unit (gear and motor) that allows the crane to rotate. On top of the slewing unit there are three main parts which are: the long horizontal jib (working arm), shorter counter-jib, and the operator's cab.\n\nThe long horizontal jib is the part of the crane that carries the load. The counter-jib carries a counterweight, usually of concrete blocks, while the jib suspends the load to and from the center of the crane. The crane operator either sits in a cab at the top of the tower or controls the crane by radio remote control from the ground. In the first case the operator's cab is most usually located at the top of the tower attached to the turntable, but can be mounted on the jib, or partway down the tower. The lifting hook is operated by the crane operator using electric motors to manipulate wire rope cables through a system of sheaves. The hook is located on the long horizontal arm to lift the load which also contains its motor.\nIn order to hook and unhook the loads, the operator usually works in conjunction with a signaller (known as a 'dogger', 'rigger' or 'swamper'). They are most often in radio contact, and always use hand signals. The rigger or dogger directs the schedule of lifts for the crane, and is responsible for the safety of the rigging and loads.\n\nTower cranes are used extensively in construction and other industry to hoist and move materials. There are many types of tower cranes. Although they are different in type, the main parts are the same, as follows:\nA tower crane is usually assembled by a telescopic jib (mobile) crane of greater reach (also see \"self-erecting crane\" below) and in the case of tower cranes that have risen while constructing very tall skyscrapers, a smaller crane (or derrick) will often be lifted to the roof of the completed tower to dismantle the tower crane afterwards, which may be more difficult than the installation.\nTower Cranes can often be Pedestrian Operated by a remote control, removing the need for a cab for the Crane Operator to work from.\n\nEach model and distinctive style of tower crane has a predetermined lifting chart that can be applied to any radii available depending on its configuration. Similar to a mobile crane, a tower crane may lift an object of far greater mass closer to its center of rotation than at its maximum radius. An operator manipulates several levers and pedals to control each function of the crane.\n\nGenerally a type of pedestrian operated tower crane. Self erecting tower cranes are transported as a single unit and can be assembled by a qualified technician without the assistance of a larger mobile crane. They are bottom slewing cranes that stand on outriggers, have no counter jib, have their counter weights and ballast at the base of the mast, can not climb themselves, have a reduced capacity to standard tower cranes, and seldom have an operator's cabin.\n\nIn some cases, smaller self-erecting tower cranes may have axles permanently fitted to the tower section to make maneuvering the crane onsite easier.\n\nFor a video of a crane getting taller, see here:\n\nFor another animation of such a crane in use see this video: (Here, the crane is used to erect a scaffold which in turn contains a gantry to lift sections of a bridge spire.)\n\nA telescopic crane has a boom that consists of a number of tubes fitted one inside the other. A hydraulic cylinder or other powered mechanism extends or retracts the tubes to increase or decrease the total length of the boom. These types of booms are often used for short term construction projects, rescue jobs, lifting boats in and out of the water, etc. The relative compactness of telescopic booms makes them adaptable for many mobile applications.\n\nThough not all telescopic cranes are mobile cranes, many of them are truck-mounted.\n\nA telescopic tower crane has a telescopic mast and often a superstructure (jib) on top so that it functions as a tower crane. Some telescopic tower cranes also have a telescopic jib.\n\nThe \"hammerhead\", or giant cantilever, crane is a fixed-jib crane consisting of a steel-braced tower on which revolves a large, horizontal, double cantilever; the forward part of this cantilever or jib carries the lifting trolley, the jib is extended backwards in order to form a support for the machinery and counterbalancing weight. In addition to the motions of lifting and revolving, there is provided a so-called \"racking\" motion, by which the lifting trolley, with the load suspended, can be moved in and out along the jib without altering the level of the load. Such horizontal movement of the load is a marked feature of later crane design. These cranes are generally constructed in large sizes and can weigh up to 350 tons.\n\nThe design of \"Hammerkran\" evolved first in Germany around the turn of the 19th century and was adopted and developed for use in British shipyards to support the battleship construction program from 1904 to 1914. The ability of the hammerhead crane to lift heavy weights was useful for installing large pieces of battleships such as armour plate and gun barrels. Giant cantilever cranes were also installed in naval shipyards in Japan and in the United States. The British government also installed a giant cantilever crane at the Singapore Naval Base (1938) and later a copy of the crane was installed at Garden Island Naval Dockyard in Sydney (1951). These cranes provided repair support for the battle fleet operating far from Great Britain.\n\nIn the British Empire, the engineering firm Sir William Arrol & Co Ltd was the principal manufacturer of giant cantilever cranes; the company built a total of fourteen. Among the sixty built in the world, few remain; seven in England and Scotland of about fifteen worldwide.\n\nThe Titan Clydebank is one of the 4 Scottish cranes on the Clydebank and preserved as a tourist attraction.\n\nNormally a crane with a hinged jib will tend to have its hook also move up and down as the jib moves (or \"luffs\"). A level luffing crane is a crane of this common design, but with an extra mechanism to keep the hook level when luffing.\n\nAn overhead crane, also known as a bridge crane, is a type of crane where the hook-and-line mechanism runs along a horizontal beam that itself runs along two widely separated rails. Often it is in a long factory building and runs along rails along the building's two long walls. It is similar to a gantry crane. Overhead cranes typically consist of either a single beam or a double beam construction. These can be built using typical steel beams or a more complex box girder type. Pictured on the right is a single bridge box girder crane with the hoist and system operated with a control pendant. Double girder bridge are more typical when needing heavier capacity systems from 10 tons and above. The advantage of the box girder type configuration results in a system that has a lower deadweight yet a stronger overall system integrity. Also included would be a hoist to lift the items, the bridge, which spans the area covered by the crane, and a trolley to move along the bridge.\n\nThe most common overhead crane use is in the steel industry. At every step of the manufacturing process, until it leaves a factory as a finished product, steel is handled by an overhead crane. Raw materials are poured into a furnace by crane, hot steel is stored for cooling by an overhead crane, the finished coils are lifted and loaded onto trucks and trains by overhead crane, and the fabricator or stamper uses an overhead crane to handle the steel in his factory. The automobile industry uses overhead cranes for handling of raw materials. Smaller workstation cranes handle lighter loads in a work-area, such as CNC mill or saw.\n\nAlmost all paper mills use bridge cranes for regular maintenance requiring removal of heavy press rolls and other equipment. The bridge cranes are used in the initial construction of paper machines because they facilitate installation of the heavy cast iron paper drying drums and other massive equipment, some weighing as much as 70 tons.\n\nIn many instances the cost of a bridge crane can be largely offset with savings from not renting mobile cranes in the construction of a facility that uses a lot of heavy process equipment.\n\nA gantry crane has a hoist in a fixed machinery house or on a trolley that runs horizontally along rails, usually fitted on a single beam (mono-girder) or two beams (twin-girder). The crane frame is supported on a gantry system with equalized beams and wheels that run on the gantry rail, usually perpendicular to the trolley travel direction. These cranes come in all sizes, and some can move very heavy loads, particularly the extremely large examples used in shipyards or industrial installations. A special version is the container crane (or \"Portainer\" crane, named by the first manufacturer), designed for loading and unloading ship-borne containers at a port.\n\nMost container cranes are of this type.\n\nLocated on the ships and boats, these are used for cargo operations or boat unloading and retrieval where no shore unloading facilities are available. Most are diesel-hydraulic or electric-hydraulic.\n\nA jib crane is a type of crane where a horizontal member (\"jib\" or \"boom\"), supporting a moveable hoist, is fixed to a wall or to a floor-mounted pillar. Jib cranes are used in industrial premises and on military vehicles. The jib may swing through an arc, to give additional lateral movement, or be fixed. Similar cranes, often known simply as hoists, were fitted on the top floor of warehouse buildings to enable goods to be lifted to all floors.\n\nBulk-handling cranes are designed from the outset to carry a shell grab or bucket, rather than using a hook and a sling. They are used for bulk cargoes, such as coal, minerals, scrap metal etc.\n\nA loader crane (also called a \"knuckle-boom crane\" or \"articulating crane\") is an electrically powered articulated arm fitted to a truck or trailer, and is used for loading/unloading the vehicle. The numerous jointed sections can be folded into a small space when the crane is not in use. One or more of the sections may be telescopic. Often the crane will have a degree of automation and be able to unload or stow itself without an operator's instruction.\n\nUnlike most cranes, the operator must move around the vehicle to be able to view his load; hence modern cranes may be fitted with a portable cabled or radio-linked control system to supplement the crane-mounted hydraulic control levers.\n\nIn the UK and Canada, this type of crane is often known colloquially as a \"Hiab\", partly because this manufacturer invented the loader crane and was first into the UK market, and partly because the distinctive name was displayed prominently on the boom arm.\n\nA rolloader crane is a loader crane mounted on a chassis with wheels. This chassis can ride on the trailer. Because the crane can move on the trailer, it can be a light crane, so the trailer is allowed to transport more goods.\n\nA crane with a forklift type mechanism used in automated (computer controlled) warehouses (known as an automated storage and retrieval system (AS/RS)). The crane moves on a track in an aisle of the warehouse. The fork can be raised or lowered to any of the levels of a storage rack and can be extended into the rack to store and retrieve product. The product can in some cases be as large as an automobile. Stacker cranes are often used in the large freezer warehouses of frozen food manufacturers. This automation avoids requiring forklift drivers to work in below freezing temperatures every day.\n\nLifetime of existing cranes made of welded metal structures can often be extended for many years by aftertreatment of weldings. During development of cranes, load level (lifting load) can be significantly increased by taking into account the IIW recommendations (the International Institute of Welding Technology IIW published the Guideline \"Recommendations for the HFMI Treatment\" in 2016) leads in most cases to an increase of the permissible lifting load and thus to an efficiency increase.\n\nThe generally accepted definition of a crane is a machine for lifting and moving heavy objects by means of ropes or cables suspended from a movable arm. As such, a lifting machine that does not use cables, or else provides only vertical and not horizontal movement, cannot strictly be called a 'crane'.\n\nTypes of crane-like lifting machine include:\n\nMore technically advanced types of such lifting machines are often known as 'cranes', regardless of the official definition of the term.\n\n\nCrane operators are skilled workers and heavy equipment operators.\n\n\nHistory of cranes\n"}
{"id": "19064449", "url": "https://en.wikipedia.org/wiki?curid=19064449", "title": "Demandware", "text": "Demandware\n\nDemandware is a software technology company headquartered in Burlington, Massachusetts, providing a cloud-based e-commerce platform and related services for retailers and brand manufacturers around the world.\n\nIn 2016, Demandware was acquired by Salesforce for $2.8B.\n\nDemandware was founded in 2004 by Stephan Schambach to provide a hosted service that would enable companies to develop and manage easy-to-use, customizable e-commerce websites, rather than building a site from scratch. The service was launched in the first quarter of 2005. Schambach previously founded the early e-commerce company Intershop in 1992. Seed money for Demandware was provided by venture capital firms General Catalyst Partners and North Bridge Venture Partners.\n\nOn March 15, 2012, Demandware began trading on the New York Stock Exchange, raising $88 million in its initial public offering of $16 per share. Following its IPO, shares were up more than 50% from the IPO price by the next morning. In November 2013, Demandware announced an underwritten registered public offering.\n\nDemandware is headquartered in Burlington, Massachusetts. The company also has offices in Salt Lake City, Utah, Deerfield Beach, Florida, the United Kingdom, France, Denmark, Germany, The Netherlands, Australia, Hong Kong, China and Japan.\n\n\n"}
{"id": "47243646", "url": "https://en.wikipedia.org/wiki?curid=47243646", "title": "Detsky Mir", "text": "Detsky Mir\n\nChildren's World () or Detsky Mir is a Russian children's retailer. Opened on June 6, 1957, as of February 2017, the company had 525 stores. Vladimir Chirakhov serves as CEO. It is the largest children's goods retailer in Russia and the CIS, with the retail chain in both Russia and Kazakhstan. Detsky Mir Group also owns the ELC (Early Learning Center) retail chain in Russia. On February 8, 2017, PAO Detsky Mir listed its shares \"in the first major initial public offering in Russia since the annexation of Crimea.\"\n\nDetsky Mir first opened on June 6, 1957 in the center of Moscow at Lubyanka Square. The original store was built between 1953 and 1957, with design by architect Alexey Dushkin. After the original store opened, \"Bloomberg\" writes that \"Detsky Mir became a household name, prompting the Soviet government to open a network of large stores by the same name.\" Detsky Mir became a chain of children's retailers in Russia in the 2000s.\nIn 2005, the original Detsky Mir building received the status of cultural heritage at the regional level. The original Detsky Mir building was Russia's largest toy shop from 1957 until 2008, when it was sold by Detsky Mir to VTB and was closed for restoration, with the cost of the restoration estimated at US $138 million. The building reopened on March 31, 2015 under the name Central Children's Store on Lubyanka. The historical name still belonged to the owner of the building as the trade network \"Children's World.\"\n\nThe \"Wall Street Journal\" writes that \"the main owner of Detsky Mir, conglomerate AFK Sistema, put off plans for an IPO in 2014.\" \"Bloomberg\" writes \"Sistema’s founder, billionaire Vladimir Evtushenkov, shelved a planned IPO of Detsky Mir in 2014 after Russia’s annexation of Crimea.\"\n\nBy 2015, the Detsky Mir chain had around 320 locations and was owned by Vladimir Evtushenkov. The company expected 2016 revenue of $1.3 billion (79.2 billion rubles) for 2016, a 30% increase from 2015. In 2016, Detsky Mir reported 31-percent growth in revenue to 79.5 billion roubles (1 billion pounds). As of December 31, 2016, it operated 525 stores, with 468 in Russia and 12 in Kazakhstan. “Children’s World” opened around 200 stores between 2015 and early 2017, with plans to open 250 over the next three years, bringing the total to 700. As of February 2017, the company had 525 stores, with plans to open 250 more by 2020.\n\nAfter Detsky Mir closed the book on an IPO on February 7, on February 8, 2017, PAO Detsky Mir listed its shares \"in the first major initial public offering in Russia since the annexation of Crimea.\" At the time of its IPO, Detsky Mir was majority owned by System and partly owned by The Russia China Investment Fund (RCIF). System retained just over 50% of the shares after the IPO, while RCIF retained a share of around 13%. Credit Suisse, Goldman Sachs, and Morgan Stanley were appointed as joint global coordinators and bookrunners, while UBS Investment Bank and Sberbank CIB were also joint book runners. The IPO raised $355 million, valuing the company at around $1 billion, with plans for stock to begin trading on February 10, 2017. It debuted on the Moscow Exchange on Friday on February 10, 2017. When Detsky Mir launched on the RTS Micex stock exchange in Moscow, over the half of the shares went to non-Russian entities and individuals. Around 90 percent of the shares were bought by foreign investors. As of August 2017, Detsky Mir’s IPO was still the largest that year in Russia.\n\nAs of March 2017, the company had a growing e-commerce business. It also had plans to open 250 new stores by 2010, with 70 to open in 2017 and several openings scheduled for Kazakhstan to add to the 12 stores there. That month, Detsky Mir Group (Detsky Mir, Early Learning Center, and online stores) announced it was considering entering into the Indian market.\n\nThe company sells toys, games, large items, products for newborns, apparel and footwear, and stationary. \"Bloomberg\" writes in February 2017 that the franchise \"offers everything from strained carrots to bicycles to frilly party dresses in stores that often feature entertainment areas where kids play with Lego and shoot Nerf balls.\" In August 2017, Detsky Mir partnered with Lego to support \"Ninjago\" in Russia.\n\n\n"}
{"id": "5719877", "url": "https://en.wikipedia.org/wiki?curid=5719877", "title": "Digital wallet", "text": "Digital wallet\n\nA digital wallet refers to an electronic device or online service that allows an individual to make electronic transactions. This can include purchasing items on-line with a computer or using a smartphone to purchase something at a store. An individual's bank account can also be linked to the digital wallet. They might also have their driver's license, health card, loyalty card(s) and other ID documents stored on the phone. The credentials can be passed to a merchant's terminal wirelessly via near field communication (NFC). Increasingly, digital wallets are being made not just for basic financial transactions but to also authenticate the holder's credentials. For example, a digital wallet could verify the age of the buyer to the store while purchasing alcohol. The system has already gained popularity in Japan, where digital wallets are known as \"wallet mobiles\". A cryptocurrency wallet is a digital wallet where private keys are stored for cryptocurrencies like bitcoin.\n\nA digital wallet has both a software and information component. The software provides security and encryption for the personal information and for the actual transaction. Typically, digital wallets are stored on the client side and are easily self-maintained and fully compatible with most e-commerce Web sites. A server-side digital wallet, also known as a thin wallet, is one that an organization creates for and about you and maintains on its servers. Server-side digital wallets are gaining popularity among major retailers due to the security, efficiency, and added utility it provides to the end-user, which increases their satisfaction of their overall purchase. The information component is basically a database of user-input information. This information consists of your shipping address, billing address, payment methods (including credit card numbers, expiry dates, and security numbers), and other information.\n\nDigital wallets are composed of both digital wallet devices and digital wallet systems. There are dedicated digital wallet devices such as the biometric wallet by Dunhill, a physical device that holds cash and cards along with a Bluetooth mobile connection. Presently there are further explorations for smartphones with NFC digital wallet capabilities, such as the Samsung Galaxy series and the Google Nexus smartphones utilizing Google's Android operating system and Apple's iPhone 6 and iPhone 6 Plus utilizing Apple Pay. Others include Samsung Pay, Android Pay, as well as payment services like PayPal and Venmo.\n\nDigital wallet systems enable the widespread use of digital wallet transactions among various retail vendors in the form of mobile payments systems and digital wallet applications. The M-PESA mobile payments system and microfinancing service has widespread use in Kenya and Tanzania, while the MasterCard PayPass application has been adopted by a number of vendors in the U.S. and worldwide.\n\nDigital wallets are being used more and more in Asian countries as well. One in five consumers in Asia are now using a digital wallet, representing twofold increase from two years ago. A recent survey by MasterCard's mobile shopping survey shows on 8500 adults aged 18–64 across 14 markets showed, 45% users in China, 36.7% users in India and 23.3% users in Singapore are the biggest adopters of digital wallet. The survey was conducted on between October and December 2015. Also analysis showed (48.5%) consumers in these regions made purchase using smartphones. Indian consumers are leading the way with 76.4% using a smartphone to make purchase which is a drastic increase of 29.3% from previous year. This has made companies like Reliance and Amazon India to come out with its own digital wallet. Flipkart has already introduced its own digital wallet.\n\nA client-side digital wallet requires minimal setup and is relatively easy to use. Once the software is installed, the user begins by entering all the pertinent information. The digital wallet is now set up. At the purchase or check-out page of an e-commerce site, the digital wallet software has the ability to automatically enter the user information in the online form. By default, most digital wallets prompt when the software recognizes a form in which it can fill out; if one chooses to fill out the form automatically, the user will be prompted for a password. This keeps unauthorized users away from viewing personal information stored on a particular computer.\n\nDigital wallets are designed to be accurate when transferring data to retail checkout forms; however, if a particular e-commerce site has a peculiar checkout system, the digital wallet may fail to properly recognize the form's fields. This problem has been eliminated by sites and wallet software that use Electronic Commerce Modeling Language (ECML) technology. Electronic Commerce Modeling Language is a protocol that dictates how online retailers structure and set up their checkout forms. Participating e-commerce vendors who incorporate both digital wallet technology and ECML include: Microsoft, Discover, IBM, Omaha Steaks and Dell Computers.\n\nConsumers are not required to fill out order forms on each site when they purchase an item because the information has already been stored and is automatically updated and entered into the order fields across merchant sites when using a digital wallet. Consumers also benefit when using digital wallets because their information is encrypted or protected by a private software code; merchants benefit by receiving protection against fraud.\n\nDigital wallets are available to consumers free of charge, and they're fairly easy to obtain. For example, when a consumer makes a purchase at a merchant site that's set up to handle server-side digital wallets, he types his name and payment and shipping information into the merchant's own form. At the end of the purchase, the consumer is asked to sign up for a wallet of his choice by entering a user name and password for future purchases. Users can also acquire wallets at a wallet vendor's site.\n\nAlthough a wallet is free for consumers, vendors charge merchants for wallets. Some wallet vendors make arrangements for merchants to pay them a percentage of every successful purchase directed through their wallets. In other cases, digital wallet vendors process the transactions between cardholders and participating merchants and charge merchants a flat fee.\n\nUpwards of 25% of online shoppers abandon their order due to frustration in filling in forms. The digital wallet combats this problem by giving users the option to transfer their information securely and accurately. This simplified approach to completing transactions results in better usability and ultimately more utility for the customer.\n\nDigital Wallets can also increase the security of the transaction since the wallet typically does not pass payment card details to the website (a unique transaction identifier or token is shared instead). Increasingly this approach is a feature of online payment gateways, especially if the payment gateway offers a \"hosted payment page\" integration approach.\n\n\n"}
{"id": "1792890", "url": "https://en.wikipedia.org/wiki?curid=1792890", "title": "Failure to thrive", "text": "Failure to thrive\n\nFailure to thrive (FTT), more recently known as faltering weight or weight faltering, is a term used in pediatric medicine, as well as veterinary medicine (where it is also referred to as ill-thrift), to indicate insufficient weight gain or inappropriate weight loss. When not more precisely defined, the term refers to pediatric patients. In children, it is usually defined in terms of weight, and can be evaluated either by a low weight for the child's age, or by a low rate of increase in the weight.\n\nThe term \"failure to thrive\" has been used vaguely and in different contexts to refer to different issues in pediatric growth. It is most commonly used to describe a failure to gain weight, but some providers have also used it to describe a failure to grow, or a failure to grow and to gain weight. As used by pediatricians, it covers poor physical growth of any cause. The term has been used in different ways, and different objective standards have been defined. FTT is suggested by a fall in one or more weight centile spaces on a World Health Organization (WHO) growth chart depending on birth weight or when weight is below the 2nd percentile of weight for age irrespective of birth weight. In children whose birth weight was between the 9th and 91st percentile FTT is indicated by a drop across 2 or more centile spaces. Weight loss after birth is normal and most babies return to their birth weight by 3 weeks of age. Clinical assessment for FTT is recommended for babies who lose more than 10% of their birth weight or do not return to their birth weight after 3 weeks.\n\nFTT was first introduced in the early 20th century to describe poor growth in orphan children but became associated with negative implications (such as maternal deprivation) that often incorrectly explained the underlying issues. Throughout the 20th century, FTT was expanded to include many different issues related to poor growth, which made it broadly applicable but non-specific. The current conceptualization of FTT acknowledges the complexity of faltering growth in children and has shed many of the negative stereotypes that plagued previous definitions.\n\nFailure to thrive occurs in children whose nutritional intake is insufficient for supporting normal growth and weight gain. Failure to thrive typically presents before 2 years of age, when growth rates are highest. Parents may express concern about picky eating habits, poor weight gain, or smaller size compared relative to peers of similar age. Physicians often identify failure to thrive during routine office visits, when a child's growth parameters are not tracking appropriately on growth curves. Physicians look for many signs on physical exam that can indicate a potential cause of FTT. For example, findings such as scaling skin, spoon-shaped nails, cheilosis and neuropathy may indicate potential vitamin and mineral deficiencies. Fetal alcohol syndrome (FAS) has also been associated with FTT, and can present with characteristic findings including microcephaly, short palpebral fissures, a smooth philtrum and a thin vermillion border. Malabsorption, due to disorders like Crohn's disease and cystic fibrosis, can present with abdominal distention and hyperactive bowel sounds.\n\nTraditionally, causes of FTT have been divided into endogenous and exogenous causes. These causes can be largely grouped into three categories: inadequate caloric intake, malabsorption/caloric retention defect, and increased metabolic demands. Initial investigation should consider prenatal history, postnatal history, past medical history, feeding history to assess overall caloric intake, developmental history, family history, and psychosocial history.\n\n\n\n\n\nFTT may be evaluated through a multifaceted process, beginning with a patient history that notably includes diet history, which is a key element for identifying potential causes of FTT. Next, a complete physical examination may be done, with special attention being paid to identifying possible organic sources of FTT. This could include looking for dysmorphic features, abnormal breathing sounds, and signs of specific vitamin and mineral deficiencies. The physical exam may also reveal signs of possible child neglect or abuse. Based on the information gained from the history and physical examination, a workup can then be conducted, in which possible sources of FTT can be further probed, through blood work, X-rays, or other tests. Laboratory workup should be directed by concerning history and physical examination findings, as it is estimated that the usefulness of laboratory investigations for children with failure to thrive is 1.4%. Initial bloodwork should be based on the clinical picture of the child. Common bloodwork should include a CBC with differential, a complete metabolic panel to look for electrolyte derangements, a thyroid function test, and a urinalysis. If indicated, anti-TTG IgA antibodies can be used to assess for Celiac's disease, and a sweat chloride test is used to screen for cystic fibrosis. If no cause is discovered, a stool examination could be indicated to look for fat or reducing substances. C-reactive protein and erythrocyte sedimentation rate (ESR) can also be used look for signs of inflammation. \n\nInfants and children who have had unpleasant eating experiences (e.g. acid reflux or food intolerance) may be reluctant to eat their meals. Additionally, force feeding an infant or child can discourage proper self-feeding practices and in-turn cause undue stress on both the child and their parents. Psychosocial interventions can be targeted at encouraging the child to feed themselves during meals. Also, making mealtimes a positive, enjoyable experience through the use of positive reinforcement may improve eating habits in children who present with FTT. If behavioral issues persist and are affecting nutritional habits in children with FTT it is recommended that the child see a psychologist. If an underlying condition, such as inflammatory bowel disease, is identified as the cause of the child's failure to thrive then treatment is directed towards the underlying condition. Special care should be taken to avoid refeeding syndrome when initiating feeds in a malnourished patient. Refeeding syndrome is caused by a shift in fluid and electrolytes in a malnourished person as they receive artificial refeeding. It is potentially fatal, and can occur whether receiving enteral or parenteral nutrition. The most serious and common electrolyte abnormality is hypophosphatemia, although sodium abnormalities are common as well. It can also cause changes in glucose, protein, and fat metabolism. Incidence of refeeding syndrome is high, with one prospective cohort study showing 34% of ICU experienced hypophosphatemia soon after feeding was restarted.\n\nChildren with failure to thrive are at an increased risk for long-term growth, cognitive, and behavioral complications. Studies have shown that children with failure to thrive during infancy were shorter and lower weight at school-age than their peers. Failure to thrive may also result in children not achieving their growth potential, as estimated by mid-parental height. Longitudinal studies have also demonstrated lower IQs (3-5 points) and poorer arithmetic performance in children with a history failure to thrive, compared to peers receiving adequate nutrition as infants and toddlers. Early intervention and restoration of adequate nutrition has been shown to reduce the likelihood of long-term sequelae, however, studies have shown that failure to thrive may cause persistent behavioral problems, despite appropriate treatment.\n\nFailure to thrive is a common presenting problem in the pediatric population. Failure to thrive is very prevalent in the United States, representing 5–10% of children seen as outpatients by primary care physicians. Failure to thrive is more prevalent in children of lower socioeconomic status. Failure to thrive accounts for 3–5% of all hospital admissions for children under two years of age. Retrospective studies suggest that males are slightly more likely than females to be admitted to the hospital for failure to thrive (53.2% vs. 46.7%).\n\n"}
{"id": "16915389", "url": "https://en.wikipedia.org/wiki?curid=16915389", "title": "Fishing popper", "text": "Fishing popper\n\nThe popper is an effective and proven lure designed to move water using a concave or hollowed nose. Poppers aim to simulate any sort of distressed creature that might be moving or struggling on the surface of the water (baitfish, frogs, and insects are the most typical imitations). Originally this timeless lure was crafted from wood and painted or shaped to match the pattern of baitfish. This quickly evolved into more intricate patterns that mimicked a broader scope of the common prey of predatory fish. Along with different imitations, different materials and technologies have been integrated with this classic platform like rattles, soft bodies, and other synthetic body materials. This iconic pattern has been used to create topwater commotion for many decades, but has been most notable for its presence in bass fishing throughout America. Though this pattern is used for many other species of fish the bass is often tightly connected with topwater poppers. Another species often targeted with poppers is the GT. Catching GT on poppers is becoming more and more popular due to the GT's aggressive surface take. Used by fly and conventional anglers alike this pattern has not failed fishermen since its creation around a century ago.\n\n\nhttps://www.heddonlures.com/ retrieved April 30, 2018\nhttps://www.rebellures.com/heritage retrieved April 30, 2018\n"}
{"id": "11328487", "url": "https://en.wikipedia.org/wiki?curid=11328487", "title": "Flint axe", "text": "Flint axe\n\nA flint axe was a Flint tool used during prehistoric times to perform a variety of tasks. These were at first just a cut piece of flint stone used as a hand axe but later wooden handles were attached to these axe heads. The stone exhibits a glass-like fracture similar to obsidian, and can be knapped to form large blades. The offcuts were sharp enough to be used a small flint knives, while the larger parts of a knapped nodule could be polished to form an axe-head. They competed with other hard rocks such as greenstone, which were produced at Langdale in the British Lake District and got larger as working continued. They tend to be larger and heavier than the simple axes, and are sometimes known as axe-hammers. \n\nThere a many different types of flint axes. A specific one that appeared during the Early Stone Age was the core axe. This is an unpolished flint axe that is roughly hewn. The cutting edge is usually the widest part and has a pointed butt. Flake axes are created from the chips from the core axe. \nDuring the prehistoric times, the flint axe was a widely used tools for multiple different tasks. They were widely used during the Neolithic period to clear forests for early farming. The polished axes were used directly to cut timber across the grain, but some types (known as a Splitting maul) were designed to split wood along the grain. The axe was also used to prepare different parts of the animals they killed. They would butcher the meat and prepare the skins. They could also use them to dig up different things when needed. The flint axes were an everyday tool to use for some settlement sites. Some sites used them more for farming and some sites used them more for chopping down trees.\n\nWhen needed, flint axes were used as a weapon. At a burial site associated with the Globular Amphora Culture, there are 15 individual remains with similar injuries. Using forensic medical analyses to determine these injuries, it was determined that majority of the fractures came from a flint axe. Flint axes are normally found within the graves sites if this particular culture. It was undetermined if this was an invasion or if they used the flint axes as a ritual for the Neolithic community.\n\nFlint nodules are commonly found in Cretaceous chalk deposits, such as those of southern Britain and every where but France. They were mined during the Neolithic period in many locations, one of the most famous being at Grimes Graves in Norfolk, England. In Nagada, an Upper Egypt Predynastic settlement site, the Flint axe was one of the most commonly bi-facial tools located there. Flint axes have been discovered along the Nile Valley from Matmar to El Qara. These axes have also been located in Kharga Oasis.\n"}
{"id": "30310333", "url": "https://en.wikipedia.org/wiki?curid=30310333", "title": "GSP Prometeu", "text": "GSP Prometeu\n\nGSP Prometeu is a jackup independent leg cantilever drilling rig operated by GSP Drilling, a Grup Servicii Petroliere subsidiary, and currently contracted by Melrose Resources for drilling in the Bulgarian section of the Black Sea. The drilling unit is registered in Malta.\n\n\"GSP Prometeu\" drilling rig was designed by Sonnat Offshore and was built by Petrom at the Galaţi Shipyard in 1982. The rig was completely reconstructed and refurbished in 2003 at a cost of US$35 million. The rig was owned and operated by Petrom from 1982 to 2005 when the company sold its six offshore platforms (including Atlas, Jupiter, Orizont, Prometeu and Saturn) to Grup Servicii Petroliere for US$100 million.\n\n\"GSP Prometeu\" has a length of , breadth of , draft of , height of and depth of . She has a maximum drilling depth of and she could operate at a water depth of . As a drilling rig, \"GSP Prometeu\" is equipped with advanced drilling equipment and has to meet strict levels of certification under international law. \"GSP Prometeu\" is able to maneuver with its own engines (to counter drift and ocean currents), but for long-distance relocation it must be moved by specialist tugboats. The rig is capable of withstanding severe sea conditions including waves and winds.\n\nCurrently the GSP Prometeu is operated by the British company Melrose Resources which uses the drilling rig at its Black Sea oil and natural gas prospects. The rig is set to drill for natural gas in the Kaliakra 2 prospect.\n\n"}
{"id": "1299689", "url": "https://en.wikipedia.org/wiki?curid=1299689", "title": "Garage door opener", "text": "Garage door opener\n\nA garage door opener is a motorized device that opens and closes garage doors controlled by switches on the garage wall. Most also include a handheld radio remote control carried by the owner, which she can use to open the door from the outside when she drives up, and close it after she leaves.\n\nThe electric overhead garage door opener was invented by C.G. Johnson in 1926 in Hartford City, Indiana. Electric Garage Door openers did not become popular until Era Meter Company of Chicago offered one after World War II where the overhead garage door could be opened via a key pad located on a post at the end of the driveway or a switch inside the garage.\n\nAs in an elevator, the electric motor does not provide most of the power to move a heavy garage door. Instead, most of door's weight is offset by the counterbalance springs attached to the door. (Even manually operated garage doors have counterbalances; otherwise they would be too heavy for a person to open or close them.) In a typical design, torsion springs apply torque to a shaft, and that shaft applies a force to the garage door via steel counterbalance cables. The electric opener provides only a small amount of force to control how far the door opens and closes. In most cases, the garage door opener also holds the door closed in place of a lock.\n\nThe typical electric garage door opener consists of a power unit that contains the electric motor. The power unit attaches to a track. A trolley connected to an arm that attaches to the top of the garage door slides back and forth on the track, thus opening and closing the garage door. The trolley is pulled along the track by a chain, belt, or screw that turns when the motor is operated. A quick-release mechanism is attached to the trolley to allow the garage door to be disconnected from the opener for manual operation during a power failure or in case of emergency. Limit switches on the power unit control the distance the garage door opens and closes once the motor receives a signal from the remote control or wall push button to operate the door.\n\nThe entire assembly hangs above the garage door. The power unit hangs from the ceiling and is located towards the rear of the garage. The end of the track on the opposite end of the power unit attaches to a header bracket that is attached to the header wall above the garage door. The power head is usually supported by punched angle iron.\n\nRecently another type of opener, known as the jackshaft opener, has become more popular. This style of opener was used frequently on commercial doors but in recent years has been adapted for residential use. This style of opener consists of a motor that attaches to the side of the torsion rod and moves the door up and down by simply spinning the rod. These openers need a few extra components to function safely for residential use. These include a cable tension monitor, to detect when a cable is broken, and a separate locking mechanism to lock the door when it is fully closed. These have the advantage that they free up ceiling space that an ordinary opener and rail would occupy. These also have the disadvantage that the door must have a torsion rod to attach the motor to.\n\nThere are five types of garage door openers:\n\nThe first wireless garage door openers were invented and developed by two US inventors at the same time, one in Illinois and the other in Washington state. They were unknown to each other.\n\nThe first garage door opener remote controls were simple and consisted of a simple transmitter (the remote) and receiver which controlled the opener mechanism. The transmitter would transmit on a designated frequency; the receiver would listen for the radio signal, then open or close the garage, depending on the door position. The basic concept of this can be traced back to World War II. This type of system was used to detonate remote bombs. While novel at the time, the technology ran its course when garage door openers became popular. While the garage door remote control transmitter is low power and has limited range, its signal can be received by other, nearby, garage door openers. When two neighbors had garage door openers, then opening one garage door might open the neighbor’s garage door as well.\n\nThe second stage of the wireless garage door opener system solved the opening-the-neighbor's-garage-door problem. The remote controls on these systems transmitted a digital code, and the receiver in the garage responded only to that code. The codes were typically set by eight to twelve DIP switches on the receiver and transmitter, so they allowed for to different codes. As long as neighbors used different codes, they would not open each other's garage doors. The intent of these systems was to avoid interference with nearby garage doors; the systems were not designed with security in mind. Intruders were able to defeat the security of these systems and gain entry to the garage and the house. The number of codes was small enough that even an unsophisticated intruder with a compatible remote control transmitter could just start transmitting all possible codes until he found one that opened the door. More sophisticated intruders could acquire a black box master key that automatically transmitted every possible code in a short time. An even more sophisticated method is known as a replay attack. The attacker would use a code grabber, which has a receiver that captures the remote's digital code and can retransmit that digital code at a later time. The attacker with a code grabber would wait nearby for the homeowner to use his remote, capture the code, and then replay the code to open the door when the homeowner was gone. Multicode openers became unpopular in areas where security was important, but due to their ease of programming, such openers are often used to operate such things as the gates in gated apartment complexes.\n\nAn intermediate stage of the garage door opener market eliminated the DIP switches and used remotes preprogrammed to one out of roughly 3.5 billion unique codes. The receiver would maintain a security list of remotes to which it would respond; the user could easily add the unique remote's code to the list by pressing a button on the garage door opener while activating the remote control. The large number of codes made the brute force try-all-possible-digital-codes attacks infeasible, but the systems were still vulnerable to code grabbers. For user convenience, these systems were also backward compatible with the older DIP switch remote codes, but adding an old technology remote to the security list made the garage door opener vulnerable to a brute force attack to find the DIP switch code. The larger code space approach was an improvement over the fixed DIP switch codes, but was still vulnerable to the replay attack.\n\nThe third stage of garage door opener technology uses a frequency spectrum range between 300-400 MHz and rolling code (code hopping) technology to defeat code grabbers. In addition to transmitting a unique identifier for the remote control, a sequence number and an encrypted message are also sent. Although an intruder could still capture the code used to open a garage door, the sequence number immediately expires, so retransmitting the code later would not open the garage door. The encryption makes it extremely difficult for an intruder to forge a message with the next sequence number that would open the door. Some rolling code systems are more involved than others. Because there is a high probability that someone will push the remote's button while not in range and thus advance the sequence number, the receiver does not insist the sequence number increase by exactly one; it will accept a sequence number that falls within a narrow window or two successive sequence numbers in a much wider window. Rolling code technology is also used on car remote controls and with some internet protocols for secure sites.\n\nThe fourth stage of garage door opener systems is similar to third stage, but it is limited to the 315 MHz frequency. The 315 MHz frequency range avoids interference from the Land Mobile Radio System (LMRS) used by the U.S. military.\n\nThe following standards are used by units manufactured by Chamberlain (including LiftMaster and Craftsman):\n\nRecent Chamberlain garage door openers that have Security+ 2.0 features also use a special serial protocol on wired connections rather than a simple switch closure.\n\nThe following standards are used by units manufactured by Overhead Door Corporation and its subsidiary The Genie Company:\n\nMany garage door opener remote controls use fixed-code encoding which use DIP switches or soldering to do the address pins coding process, and they usually use pt2262/pt2272 or compatible ICs. For these fixed-code garage door opener remotes, one can easily clone the existing remote using a self-learning remote control duplicator (copy remote) which can make a copy of the remote using face-to-face copying.\n\nAdditional features that have been added over the years have included:\n\nMore sophisticated features are also available, such as an integrated carbon monoxide sensor to open the door in case of the garage being flooded with exhaust fumes. Other systems allow door activation over the Internet to allow home owners to open their garage door from their office for deliveries.\n\nAnother recent innovation in the garage door opener is a fingerprint-based wireless keypad. This unit attaches to the outside of the garage door on the jamb and allows users to open and close their doors with the press of a finger, rather than creating a personal identification number (PIN). This is especially helpful for families with children who may forget a code and are latchkey kids.\n\nThe garage door is generally the largest moving object in a home. An improperly adjusted garage door opener can exert strong and deadly forces and might not reverse the garage door in an emergency. The manufacturer's instructions provide guidance to the user on the proper adjustment and maintenance of the opener. \n\nGarage door openers manufactured and installed in the United States since 1982 are required to provide a quick-release mechanism on the trolley that allows for the garage door to be disconnected from the garage door opener in the event of entrapment. Garage door openers manufactured since 1982 are also required to reverse the garage door if it strikes a solid object.\n\nIn the United States, the Consumer Product Safety Improvement Act of 1990 required that automatic residential garage door operators manufactured on or after 1 January 1991 conform to the entrapment protection requirements of the 1988 version of ANSI/UL standard 325. A requirement for redundant entrapment-prevention devices was added in 1993; such a system can use an electric eye, a door edge sensor, or any other device that provides equivalent protection by reversing the travel of the closing door if an object is detected in its path. \n\n"}
{"id": "5979440", "url": "https://en.wikipedia.org/wiki?curid=5979440", "title": "Gas burner", "text": "Gas burner\n\nA gas burner is a device that produces a controlled flame by mixing a fuel gas such as acetylene, natural gas, or propane with an oxidizer such as the ambient air or supplied oxygen, and allowing for ignition and combustion.\n\nThe flame is generally used for the heat, infrared radiation, or visible light it produces. Some burners, such as gas flares, dispose of unwanted or uncontainable flammable gases. Some burners are operated to produce carbon black.\n\nThe gas burner has many applications such as soldering, brazing, and welding, the latter using oxygen instead of air for producing a hotter flame, which is required for melting steel. Chemistry laboratories use natural-gas fueled Bunsen burners. In domestic and commercial settings gas burners are commonly used in gas stoves and cooktops. For melting metals with melting points of up to 1100 °C (such as copper, silver, and gold), a propane burner with a natural drag of air can be used. For higher temperatures, acetylene is commonly used in combination with oxygen.\n\nThe above data is given with the following assumptions:\n\n"}
{"id": "9301886", "url": "https://en.wikipedia.org/wiki?curid=9301886", "title": "High-resolution dynamics limb sounder", "text": "High-resolution dynamics limb sounder\n\nThe high-resolution dynamics limb sounder (HIRDLS) is an instrument on board the NASA Aura. It follows in the heritage of LRIR (Nimbus-6), LIMS and SAMS (Nimbus-7), ISAMS and CLAES (UARS). It was designed to observe global distribution of temperature and concentrations of O, HO, CH, NO, NO, HNO, NO, CFC-11, CFC-12, ClONO, and aerosols in the upper troposphere, stratosphere, and mesosphere.\n\nAfter launch, activation of the HIRDLS instrument revealed that the optical path was blocked so that 20% of the aperture could view the Earth's atmosphere. Engineering studies suggest that a piece of thermal blanketing material ruptured from the back of the instrument during the explosive decompression of launch. Attempts to remove this material mirror failed. However, even with the 80% blockage, measurements at high vertical resolution can be made at one scan angle.\n\n\n"}
{"id": "1430896", "url": "https://en.wikipedia.org/wiki?curid=1430896", "title": "Hypsometer", "text": "Hypsometer\n\nA hypsometer is an instrument for measuring height or elevation. Two different principles may be used: trigonometry and atmospheric pressure.\n\nA simple scale hypsometer allows the height of a building or tree to be measured by sighting across a ruler to the base and top of the object being measured, when the distance from the object to the observer is known. Modern hypsometers use a combination of laser rangefinder and clinometer to measure distances to the top and bottom of objects, and the angle between the lines from the observer to each to calculate height.\n\nAn example of such a scale hypsometer is illustrated here, and can be seen to consist of a sighting tube, a fixed horizontal scale, and an adjustable vertical scale with attached plumb line. The principle of operation of such a scale hypsometer is based on the idea of similar triangles in geometry. First the adjustable vertical scale is set at a suitable height. Then as in step 1 in the illustration, a sighting is taken on the top of the object whose height is to be determined, and the reading on the horizontal scale, h', recorded. Calculation from this value will eventually give the height h, from the eye-line of the observer to the top of the object whose height is to be determined. Similarly as in step 2 of the illustration, a sighting is taken on the base of the object whose height is to be determined, and the reading on the horizontal scale, d', recorded. Calculation from this value will eventually give the distance from the base of the object to the eye-line of the observer. Finally the distance x from the observer to the object needs to be measured.\n\nLooking at the geometry involved in step 1 results in sketch a: two right angled triangles, shown here with the identical small angles in yellow. Next in sketch b we see that the two triangles have identical angles - each has a right angle, the same small angle shown in yellow, and the same larger angle shown in orange. Therefore in sketch c we see that using the principle of similar triangles, given that each triangle has identical angles, the sides will be in proportion: x the distance to the object in proportion to x', the height set on the vertical scale of the hypsometer, and h the height of the object above the observers eye-line in proportion to h', the reading from the horizontal scale of the hypsometer.\n\nGiven that Tan (small yellow angle) = Opposite Side / Adjacent Side, therefore Tan (small yellow angle) = h / x = h' / x'. Therefore h = h'x / x'.\n\nLikewise the geometry involved in step 2 results in sketch d: two right angled triangles. Next in sketch e we see that the two triangles again have identical angles - each has a right angle, the same small angle shown in yellow, and the same larger angle shown in orange. Therefore in sketch f we see that using the principle of similar triangles, given that each triangle has identical angles, the sides will be in proportion: x the distance to the object in proportion to x', the height set on the vertical scale of the hypsometer, and d the depth of the object below the observers eye-line in proportion to d', the reading from the horizontal scale of the hypsometer.\n\nGiven that Tan (small angle) = Opposite Side / Adjacent Side, therefore Tan (small angle) = d / x = d' / x'. Therefore d = d'x / x'.\n\nThus the overall height of the object is x (d' + h') / x'\n\nA pressure hypsometer as shown in the drawing (right) employs the principle that the boiling point of a liquid is lowered by diminishing the barometric pressure, and that the barometric pressure varies with the height of the point of observation. \n\nThe instrument consists of a cylindrical vessel in which the liquid, usually water, is boiled, surmounted by a jacketed column, in the outer partitions of which the vapour circulates, while in the central one a thermometer is placed. To deduce the height of the station from the observed boiling point, it is necessary to know the relation existing between the boiling point and pressure, and also between the pressure and height of the atmosphere.\n\n"}
{"id": "8287330", "url": "https://en.wikipedia.org/wiki?curid=8287330", "title": "Ice screw", "text": "Ice screw\n\nAn ice screw is a threaded tubular screw used as a running belay or anchor by climbers on steep ice surface such as steep waterfall ice or alpine ice during ice climbing or crevasse rescue, to hold the climber in the event of a fall, and at belays as anchor points.\n\nIce screws may come with one or more of the following: an in-built or separate ratchet mechanism to speed up placement, conical centre-hole to aid removal of ice cores, different lengths, different numbers of cutting teeth, different cutting angles, different surface finishes, and different size clip holes. Price and durability are usually design considerations too, as a usable rack of ice screws will be a significant financial investment for many climbers. Many Titanium ice screws were initially made in the former Soviet Union using Cold War-era missile technology, but were generally too brittle and so the majority of ice screws are now made of chromoly steel.\n\nThe strongest and most reliable type of ice screws currently available are the modern tubular ice screws which range in lengths from 10 to 23 cm. The approximate strength rating on a modern tubular ice screw is around 7 kN, and it has been found that short ice screws in good ice hold about 7-8 kN, no matter what the fall factor or configuration is. The International Climbing and Mountaineering Federation drop test specifies that when multi-pitch lead climbing and the leader has only placed one screw (typically merely clipping one of the anchor screws) before climbing up a couple of meters and falling (a fall factor of 2), the screw must hold. The dynamic ropes used in climbing can mitigate failure of an ice screw by keeping the impact forces low, especially if using a new rope that has not had any previous falls. It is rare that an ice screw fails in fall factors of 1 or less, if placed in good ice.\n\nAn older type of screw that is rarely used today is a pound-in ice screw, such as the 'snarg' and the 'warthog'. Instead of screwing these into the ice one would pound them in with a hammer from the ice tool, and then screw them out with the pick of an ice axe. The pound-ins have been largely replaced by modern tubular ice screws that are stronger and easier to use, although warthogs are being manufactured in Britain again for use in creating an anchor in frozen turf when no alternative anchor or placement is available, although it is now recommended only as 'marginal' equipment for when no other CE-tested equipment can be placed.\n\nThe latest ice screws incorporate replaceable tips, thereby increasing the useful life of the screw and enhancing placements.\n\nIt was once thought necessary or beneficial for ice-screws to be placed horizontally or with the hanger up for optimum hold. However, it has since been found experimentally that a screw placed with the tip angled up often holds as well or better. This surprising result is thought to be due to the previously underestimated role of the threads in holding the screw in place. However, horizontal placements are usually recommended.\n\nSome climbers use tape slings to \"tie-off\" long ice-screws that have not been fully screwed into the ice, to reduce the leverage of a fall, which might bend or break the screw. There is some risk that such slings may move or be slice by the screw's hanger. Some novel ice screws include a hanger that can be moved down the shaft, instead of using a sling. Placing a shorter screw is usually the preferred option, hence various lengths are available.\n\nLong ice-screws are now often used to create V-threads (a type of thread-hole /ice-bollard). By making two ice-holes that intersect, an inexpensive but strong cord (typically static climbing accessory cord or tape, made of perlon, Dyneema or Specta) than be threaded through, making a relatively safe and cheap anchor. This can be an attractive option at belays and when ice screws are in short supply. V-threads can be particular useful as rappel or abseil points where it is often necessary to leave the final anchor behind. A piece of wire - typically a piece of old coat hanger shaped for the purpose - or a stiff flexible purpose built tool with a small hook at one end and a hanging loop at the other end, is often used to aid threading of the cord.\n\nIn the \"Rosetta\" project, the European Space Agency equipped its lander with multiple ice screws to obtain stability on comet surface, but they failed to hold, and the lander bounced a significant distance from the initial landing site.\n\n\n"}
{"id": "2292069", "url": "https://en.wikipedia.org/wiki?curid=2292069", "title": "Inotera", "text": "Inotera\n\nThe company was acquired by Micron Technology in 2016. It was renamed to Micron Technology Taiwan, Inc. and became one of Micron's manufacturing locations.\n\nIn 2008, Micron Technology acquired a 35.5% stake in Inotera from Qimonda.\nThe Company reached full conversion to 70 nm shrink technology at the end of June 2008 and started pilot production of Micron’s 50 nm stack process technology from the third quarter of 2009 in order to further improve the productivity and to reduce manufacturing unit cost.\n\nIn January 2013, Micron announced an amendment to the joint development agreement with Nanya that gave Micron access to all of Inotera's manufacturing output based on market pricing.\n\nIn December 2016, Micron completed the acquisition at a transaction value of approximately US$4.0 billion.\n\n"}
{"id": "23080305", "url": "https://en.wikipedia.org/wiki?curid=23080305", "title": "Intelligence-based design", "text": "Intelligence-based design\n\nIntelligence-based design is the purposeful manipulation of the built-environment to effectively engage humans in an essential manner through complex organized information. Intelligence-Based Theory evidences the conterminous relationship between mind and matter, i.e. the direct neurological evaluations of surface, structure, pattern, texture and form. Intelligence-Based Theory maintains that our sense of well-being is established through neuro-engagement with the physical world at the deepest level common to all people i.e. \"Innate Intelligence.\"\n\nThese precursory readings of the physical environment represent an evolved set of information processing skills that the human mind has developed over millennia through direct lived experience. This physiological engagement with the world operates in a more immediate sense than the summary events of applied meaning or intellectual speculation. It is through this direct neurological engagement that humans connect more fully with the world. Many of mankind's early religious associations with physical structures were informed by an intuitive understanding that structure and materials speak to our deeper self, i.e. the human spirit, the soul. Intelligence Based Theory reveals this effectual dimension of the built-environment and its relationship to human cognitive development, mental acuity, perceptual awareness, spirituality, and sense of well-being. It is within this realm that the mind's eye connects, or fails to connect, with the world outside. The degree of neuro-connectivity which occurs at these intervals serves to render the built-environment either intelligible or un-intelligible. The study and theory of this occurrence is known as \"Intelligence-Based Design.\"\n\nSeveral distinct strands of design thinking, in parallel development, lead towards Intelligence-Based Design. Christopher Alexander contributed early on to the scientific approach to design, by proposing a theory of design in his book Notes on the synthesis of form. Those were the years when Artificial Intelligence was being developed by Herbert A. Simon, and Alexander was part of that movement. His later work A Pattern Language, although written for architects and urbanists, was picked up by the software community and used as a combinatorial and organizational rubric for software complexity, especially Design patterns (computer science). Alexander's most recent work The Nature of Order continues by building up a framework for design that relies upon natural and biological structures. Entirely separate from this, E. O. Wilson introduced the Biophilia hypothesis to describe the affinity of humans to other living structures, and to conjecture our innate need for such a connection. This topic was later investigated by Stephen R. Kellert and others, and applied to the design of the artificial environment. The third and independent component of the theory is the recent developments in mobile robotics by Rodney Brooks, where a breakthrough occurred by largely dispensing with internal memory. The practical concept of \"Intelligence without representation\" otherwise known as the Subsumption architecture and Behavior-based robotics introduced by Brooks suggests a parallel with the way human beings interact with, and design their own environment. These notions are brought together in Intelligence-Based Design, which is a topic currently under investigation for design applications in both architecture and urbanism.\n\n"}
{"id": "12750817", "url": "https://en.wikipedia.org/wiki?curid=12750817", "title": "Keil (company)", "text": "Keil (company)\n\nKeil was founded in 1982 by Günter and Reinhard Keil, initially as a German GbR. In April 1985 the company was converted to \"Keil Elektronik GmbH\" to market add-on products for the development tools provided by many of the silicon vendors. Keil implemented the first C compiler designed from the ground-up specifically for the 8051 microcontroller.\n\nKeil provides a broad range of development tools like ANSI C compiler, macro assemblers, debuggers and simulators, linkers, IDE, library managers, real-time operating systems and evaluation boards for Intel 8051, Intel MCS-251, ARM, and XC16x/C16x/ST10 families.\n\nIn October 2005, Keil (Keil Elektronik GmbH in Munich, Germany, and Keil Software, Inc. in Plano, Texas) were acquired by ARM.\n\n\n"}
{"id": "2234982", "url": "https://en.wikipedia.org/wiki?curid=2234982", "title": "Message-waiting indicator", "text": "Message-waiting indicator\n\nA message-waiting indicator (MWI) in telephony, is a Telcordia Technologies (formerly Bellcore) term for an FSK-based telephone calling feature that illuminates an LED on selected telephones to notify a telephone user of waiting voicemail messages on most North American public telephone networks and PBXs.\n\nAs described in Telcordia Generic Requirements document GR-283-CORE, a Message_Waiting_Indicator (MWI) is a mechanism that informs the subscriber about the status of recorded messages. The subscriber may subscribe to a notification feature that makes use of the status of this MWI.\n\nThis feature is also frequently called (and abbreviated) as visual message waiting indicator (VMWI). A VMWI, as defined in Telcordia GR-1401-CORE, is a stored program controlled switching (SPCS) system feature that activates and deactivates a visual indicator on customer-premises equipment (CPE) to notify the customer that new messages are waiting. VMWI differs from existing features that use other message indicators, such as audible stuttered dial tone, in that it activates a visual indicator on the CPE. The visual indicator may be as simple as lighting or flashing a light-emitting diode (LED), or as advanced as displaying a special message on a liquid-crystal display (LCD).\n\nThis technology was invented by Jerome (Jerry) Schull and Wayne Howe at BellSouth's Advanced Technology R&D group in 1992 and was issued as US Patents #5,363,431 and #5,521,964. It was introduced in 1995, with the introduction of CLASS-based calling features and ADSI. It was at one time only compatible with ADSI-compliant telephones but is now compatible with any customer premises equipment (CPE) that simply responds visually to visual FSK.\n\nThis service is often erroneously associated with the abilities of most Caller ID standalone set-top boxes. Caller ID boxes manufactured after 1998 feature an LED that blinks green to notify that new calls have been recorded and red to indicate that a subscriber has new voicemail messages waiting. Some units also display the text \"MESSAGE WAITING\" (similar to ADSI-compliant telephones). These units do not use visual FSK to activate their red LEDs, but instead, they briefly \"pick-up\" the line at certain intervals (normally, within two minutes of a new call) to check for a \"stuttered\" dial tone. The presence of a stutter dial tone activates a red LED; while absence deactivates it.\n\nFor mobile phones, the message-waiting indicator is sent via a Short Message Service (SMS) message — the same system used for texting. (SMS was actually invented for utilitarian uses like this, not for user conversation.) It not only indicates that a message is waiting, but also how many unheard messages there are on the voicemail server for that telephone number. In the event that a phone is deactivated, out of range, or otherwise removed from the network, there is often no way to clear this indicator until another message is recorded to the system, causing the MWI message to be sent again once the phone reconnects. The customer service call center for the mobile network operator may also be able to provide simple technical support to reset this by resending the MWI message manually.\n"}
{"id": "33376919", "url": "https://en.wikipedia.org/wiki?curid=33376919", "title": "Ministry of Energy (Lithuania)", "text": "Ministry of Energy (Lithuania)\n\nThe Ministry of Energy of the Republic of Lithuania () is a government department of the Republic of Lithuania. Its operations are authorized by the Constitution of the Republic of Lithuania, decrees issued by the President and Prime Minister, and laws passed by the Seimas (Parliament). Its mission is to prosecute policy of government of Lithuania in fuel, electricity, thermo-energy production and supply for Lithuania economy. The current head of the Ministry is Žygimantas Vaičiūnas.\n"}
{"id": "30643318", "url": "https://en.wikipedia.org/wiki?curid=30643318", "title": "Modified active gas sampling", "text": "Modified active gas sampling\n\nModified Active Gas Sampling (MAGS) is an environmental engineering assessment technique which rapidly detects unsaturated soil source areas impacted by volatile organic compounds. The technique was developed by HSA Engineers & Scientists in Fort Myers, Florida in 2002, led by Richard Lewis, Steven Folsom, and Brian Moore. It is being used all over the United States, and has been adopted by the state of Florida in its Dry-cleaning Solvent Cleanup Program.\n\nMAGS involves the extraction and analysis of soil vapor from a piezometer screened through the unsaturated soil column for the purpose of locating unsaturated zone source material. According to the MAGS Manual, written by HSA and adopted by the Florida Department of Environmental Protection, MAGS is performed \"by utilizing a typical regenerative blower fitted to a temporary soil vapor extraction well, [such that]a large volume of soil can be assessed with a limited number of samples. While lacking the resolution of traditional soil sampling methods (e.g., discrete soil sampling, low flow active gas sampling, etc.), the statistical representativeness (in the sense of sample coverage) of MAGS results versus traditional methods is much greater. Moreover, the results of the assessment provide useful transport and exposure assessment information over traditional techniques. Lastly, MAGS is effective as both an initial site assessment and remedial assessment tool, in that, MAGS directly yields data required for remedial design.\"\n\nMAGS is an alternative to discrete and composite soil sampling. MAGS, while it does not describe the sample with as much precision as the previously mentioned sampling methods, is more powerful statistically: it represents a larger area of a site which is more useful in determining the presence of a compound. Besides increasing the accuracy in identifying the presence compounds in the soil, MAGS also can quickly and accurately narrow down the location and spread of the compounds after a few trials. Once the location has been determined, more thorough and traditional soil borings can be done in the identified location, instead of sampling a whole site.\n\nHSA found particular success using the technique at solvent-impacted sites that were showing signs of rebound after initial remediation efforts. These rebounds are commonly the result of multiple (relatively small) release areas that had not been previously discovered with discrete soil sampling. MAGS can be useful in detecting how effectively the site had been cleaned up post-remediation.\n\nHSA Engineers & Scientists considered patenting MAGS technology, but decided to trademark MAGS instead, asking that those who use the technique credit the firm.\n\nIn 2009, HSA was recognized by the Environmental Business Journal with a Technology Merit Award in the category of remediation for the invention of MAGS technology.\n"}
{"id": "1949447", "url": "https://en.wikipedia.org/wiki?curid=1949447", "title": "Motion control", "text": "Motion control\n\nMotion control is a sub-field of automation, encompassing the systems or sub-systems involved in moving parts of machines in a controlled manner. The main components involved typically include a motion controller, an energy amplifier, and one or more prime movers or actuators. Motion control may be open loop or closed loop. In open loop systems, the controller sends a command through the amplifier to the prime mover or actuator, and does not know if the desired motion was actually achieved. Typical systems include stepper motor or fan control. For tighter control with more precision, a measuring device may be added to the system (usually near the end motion). When the measurement is converted to a signal that is sent back to the controller, and the controller compensates for any error, it becomes a Closed loop System.\n\nTypically the position or velocity of machines are controlled using some type of device such as a hydraulic pump, linear actuator, or electric motor, generally a servo. Motion control is an important part of robotics and CNC machine tools, however in these instances it is more complex than when used with specialized machines, where the kinematics are usually simpler. The latter is often called General Motion Control (GMC). Motion control is widely used in the packaging, printing, textile, semiconductor production, and assembly industries.\nMotion Control encompasses every technology related to the movement of objects. It covers every motion system from micro-sized systems such as silicon-type micro induction actuators to micro-siml systems such as a space platform. But, these days, the focus of motion control is the special control technology of motion systems with electric actuators such as dc/ac servo motors. Control of robotic manipulators is also included in the field of motion control because most of robotic manipulators are driven by electrical servo motors and the key objective is the control of motion.\n\nThe basic architecture of a motion control system contains:\n\nThe interface between the motion controller and drives it controls is very critical when coordinated motion is required, as it must provide tight synchronization. Historically the only open interface was an analog signal, until open interfaces were developed that satisfied the requirements of coordinated motion control, the first being SERCOS in 1991 which is now enhanced to SERCOS III. Later interfaces capable of motion control include Ethernet/IP, Profinet IRT, Ethernet Powerlink, and EtherCAT.\n\nCommon control functions include:\n\n\n"}
{"id": "793081", "url": "https://en.wikipedia.org/wiki?curid=793081", "title": "Multi-function display", "text": "Multi-function display\n\nA multifunction display (MFD) is a small-screen (CRT or LCD) surrounded by multiple soft keys (configurable buttons) that can be used to display information to the user in numerous configurable ways. MFDs originated in aviation, first in military aircraft, and later were adopted by commercial aircraft, general aviation, automotive use, and shipboard use.\n\nOften, an MFD will be used in concert with a primary flight display, and forms a component of a glass cockpit. MFDs are part of the digital era of modern planes or helicopter. The first MFDs were introduced by air forces in the late 1960s and early 1970s; an early example is the F-111D (first ordered in 1967, delivered from 1970–73). The advantage of an MFD over analog display is that an MFD does not consume much space in the cockpit, as data can be presented in multiple pages, rather than always being present at once. For example, the cockpit of RAH-66 \"Comanche\" does not have analog dials or gauges at all. All information is displayed on the MFD pages. The possible MFD pages could differ for every plane, complementing their abilities (in combat).\n\nMany MFDs allow pilots to display their navigation route, moving map, weather radar, NEXRAD, ground proximity warning system, traffic collision avoidance system, and airport information all on the same screen.\n\nMFDs were added to the Space Shuttle (as the glass cockpit) starting in 1998, replacing the analog instruments and CRTs. The information being displayed is similar, and the glass cockpit was first flown on the STS-101 mission. Although many corporate business jets had them in years prior, the piston-powered Cirrus SR20 became the first part-23 certified aircraft to be delivered with an MFD in 1999 (and one of the first general aviation aircraft with a 10-in, flat-panel screen), followed closely by the Columbia 300 in 2000 and many others in the ensuing years.\n\nIn modern automotive technology, MFDs are used in cars to display navigation, entertainment, and vehicle status information.\n\n"}
{"id": "53630613", "url": "https://en.wikipedia.org/wiki?curid=53630613", "title": "Nickel hydrazine nitrate", "text": "Nickel hydrazine nitrate\n\nNickel hydrazine nitrate (NHN), (chemical formula: [Ni(NH)](NO)) is an energetic material having explosive\nproperties in between that of primary explosive and a secondary explosive. It is a salt of a coordination compound of nickel.\n\nNHN can be synthesized by reacting nickel nitrate with a dilute aqueous solution of hydrazine at 65 C. To help speed the drying of the product after filtration from the hot water, it can be rinsed with alcohol. The product is a fluffy powder (density=0.9 g/cm). To increase its bulk density ( 1.2 g/cm), dextrin can be added.\n\nNHN needs minimal confinement to properly make the deflagration to detonation transition (DDT). Thin steel / aluminum tubing with dimensions 5/16\" OD, 0.273\" ID (a 0.020\" wall thickness) is sufficient for NHN to make the DDT. Properly made NHN will even detonate in a plastic tube with a sensitive secondary like powdered ETN to set off melt-cast ETN or materials such as PETN, or RDX. Using powdered ETN as a transition explosive completely eliminates the need to use a primary explosive in the detonator thus making an NHN based detonator a true NPED.\n\nNHN straddles the line between primary and secondary. Because of this it is a relatively safe explosive to work with having much less sensitivity to shock and friction (16.0 N) than Lead Azide (0.1N) as shown in table 2.\n\nFriction sensitivities of some traditional explosives (lead azide – 0.1N; lead trinitroresorcinate – 1.5 N; mercury fulminate (white) – 5,0 N;\ntetrazene – 8.0 N; PETN – 60 N; hexogen – 120 N; octogen – 120 N, show that NHN is not very sensitive, and is thereby not exceedingly hazardous in handling.\n\n Values in brackets are theoretical\n\n Experimental value, literature value, and theoretical value\n"}
{"id": "777118", "url": "https://en.wikipedia.org/wiki?curid=777118", "title": "Nut (climbing)", "text": "Nut (climbing)\n\nIn rock climbing, a nut (or \"chock\" or \"chockstone\") is a metal wedge threaded on a wire and is used for protection by wedging it into a crack in the rock. Quickdraws are clipped to the nut wire by the ascending climber and the rope threads through the quickdraw. Nuts come in a variety of sizes and styles, and several different brands are made by competing manufacturers. Most nuts are made of aluminum. Larger nuts may be threaded on Dyneema cord instead of wire, but this has become unusual. \n\nThe very smallest nuts are known as \"micronuts\" and may be made of brass or other metal, and typically have their wires soldered into them, instead of looped through drilled holes. They are mostly used in aid climbing, and their value as protection, arresting a climber's fall, is marginal due to their low breaking strength, and the tiny surface area (the HB 0 measures about 4 x 7 x 2.5 mm) in contact with the rock, though this can be offset by placing several micronuts at a time. Other names used include \"RPs\" (the brand name of the first commercially available micronuts) and \"brassies\". They are available from several manufacturers in a variety of styles. \n\nBritish climbers in the 1950s and 1960s were the first to use nuts as climbing protection. In addition to using pitons, they picked up machine nuts from the side of railway tracks, climbed with them in their pockets, and used them as artificial chocks. This developed to the point where they drilled the thread from the middle, threaded them with slings, and used them in cracks.\n\nIn 1972, when clean climbing became an issue in the US, Yvon Chouinard began manufacturing chocks made specifically for rock climbing, with the familiar wedge shape still in use today. With Tom Frost, Chouinard invented a larger, six-sided nut called a Hexentric or hex. Prominent climbers like Henry Barber and John Stannard helped popularize the use of nuts, especially after it was discovered that a nut was lighter and easier to place and remove while climbing, as well as being at least as secure as a well-placed piton, and less damaging to the rock.\n\nNuts are available in different shapes to help the climber find the best fit for a given crack. Curved nuts have a concave face on one side and a convex face on the other. Larger nuts can be placed in either of two aspects (hexes in three aspects) to suit different-width cracks, with either the main faces or the sides in contact with the rock.\n\nNuts may be generically referred to as \"wires\" or \"stoppers\", though \"Stopper\" is a brand name of a nut made by Black Diamond Equipment.\n"}
{"id": "14089329", "url": "https://en.wikipedia.org/wiki?curid=14089329", "title": "Open Handset Alliance", "text": "Open Handset Alliance\n\nThe Open Handset Alliance (OHA) is a consortium of 84 firms to develop open standards for mobile devices. Member firms include HTC, Sony, Dell, Intel, Motorola, Qualcomm, Texas Instruments, Google, Samsung Electronics, LG Electronics, T-Mobile, Sprint Corporation, Nvidia, and Wind River Systems.\n\nThe OHA was established on 5 November 2007, led by Google with 34 members, including mobile handset makers, application developers, some mobile carriers and chip makers.\nAndroid, the flagship software of the alliance (first developed by Google in 2007), is based on an open-source license and has competed against mobile platforms from Apple, Microsoft, Nokia (Symbian), HP (formerly Palm), Samsung Electronics / Intel (Tizen, bada), and BlackBerry.\n\nAs part of its efforts to promote a unified Android platform, OHA members are contractually forbidden from producing devices that are based on incompatible forks of Android.\n\nAt the same time as the announcement of the formation of the Open Handset Alliance on November 5, 2007, the OHA also unveiled Android, an open-source mobile phone platform based on the Linux kernel. An early look at the SDK was released to developers on 12 November 2007.\n\nThe first commercially available phone running Android was the HTC Dream (also known as the T-Mobile G1). It was approved by the Federal Communications Commission (FCC) on 18 August 2008, and became available on 22 October of that year.\n\nThe members of the Open Handset Alliance are:\n\n\n"}
{"id": "37730844", "url": "https://en.wikipedia.org/wiki?curid=37730844", "title": "Paris-Saclay", "text": "Paris-Saclay\n\nParis-Saclay is a research-intensive and business cluster currently under construction south of Paris, France. It encompasses research facilities, universities, French higher education institutions (\"grandes écoles\") and also research centers of private companies. In 2013, the Technology Review put Paris-Saclay in the top 8 world research clusters. In 2014, it comprised almost 15% of French scientific research capacity.\n\nThe earliest settlements are from the 1950s, and this area was subsequently extended several times during the 1970s and 2000s. Several projects are underway to continue the development of the campus, including the relocation of some facilities. The goal is to strengthen the cluster in order to build an international scientific and technological hub that can compete with other high-technology business districts, such as Silicon Valley or Cambridge, MA. This project started in 2006 and is likely to end in 2020. The main part is the construction of the \"campus du plateau de Saclay\" (in English plateau de Saclay campus), in order to launch the Université Paris-Saclay.\n\nSeveral French national institutions settled on the plateau after the end of World War II. The CNRS is the first to settle there, headed by Frédéric Joliot-Curie, who bought the estate Button at Gif-sur-Yvette in 1946. The following year, the newly created CEA (the High Commissioner is also Joliot-Curie) to purchase land. The same year, ONERA settles on the plateau in Palaiseau. The Saclay center was inaugurated in 1952.\n\nAt the same time, higher education institutions settled nearby. The University of Paris is also up in the region in 1955 with the purchase of 50 hectares in the communes of Orsay and Bures. This Orsay campus brings laboratories of the Paris Faculty of Sciences (later the University of Paris-Sud) and moved to 1956. Other institutions followed with the installation of HEC in 1964 with its move to the town of Jouy-en-Josas, then with the arrival of the \"École supérieure d'optique\" in 1965 on the Orsay campus.\n\nResearch centers related to private companies also settled at that time in 1968 with the arrival of the Central Research Laboratory of Thomson-CSF.\n\nIn the 1970s, the École polytechnique and Supélec settled on the plateau, the first in 1976 in the Palaiseau area, the other in 1975 in the Moulon area. The project had a scheduled time to install other schools soon after. The Moulon farm which currently houses the genetics and plant breeding was restored in 1978.\n\nInstitutions on the plateau at this time begin to join together in an \"association d’établissements scientifiques\" (association of scientific institutions, AES) to reflect future developments of the area.\n\nAt the beginning of the twenty-first century, research centers of private companies settled on the campus. In 2000, Danone chooses to establish a center for Research and Development in the area of Palaiseau, joined in 2006 by Thales laboratories, and in 2009 by Kraft Foods which invests €15 million to install one of its expertise global centers. Other projects removal were also studied, including a research center of EDF, studied in 2010.\n\nTwo thematic advanced research are also on the campus, with the creation of \"Digiteo\" and \"Triangle de la physique\" in 2006. SOLEIL, which creation was decided in 2000 after three years of opposition of Claude Allègre, was inaugurated the same year, built with a budget of 313 million euros. The project of neuroimaging center NeuroSpin is launched in 2006 also on the plateau.\n\nThe first building constructed specifically for the campus is the \"Pôle commun de recherche en informatique\" (Joint Research Cluster Computing), which was inaugurated in November 2011.\n\nThe proposed new construction and renovation of campus was launched by President Nicolas Sarkozy who wants to create a \"French Silicon Valley\". The entire project is estimated to three billion euros funding.\n\nThe different steps to set up the campus are part of several government operations.\n\nSeveral institutions of higher education will be moved on the campus, such as:\nAlong with other institutions already located in the cluster, these education institutions are to be merged in University of Paris-Saclay. This university is to enter the Shanghai ranking between the 10th and 20th position.\n\nIn February 2001, the Versailles Saint-Quentin-en-Yvelines University became a founding member of the scientific cooperation foundation foreshadowing the future campus on the Saclay plateau. In November 2011, the Mines ParisTech finally withdrew the project.\n\nThree administratives structures have been created for this project:\n\nThe campus has currently seven areas:\n\nThe French National Centre for Scientific Research is located at Gif-sur-Yvette since 1946. The area has a dozen research units and service, and also 1,500 people.\n\nHEC Paris has been located at Jouy-en-Josas since 1964. INRA has 1,400 people in the area, and facilities for experimentation on livestock and microbiology. An extension of these activities provided for the arrival of more than 300 people in 2012, with the construction of Biosafety P3 facilities for virology.\n\nThe « Martinière area » is at the center of the whole, between the areas of Palaiseau and Moulon. It should accommodate several components of the University of Paris-Sud (earth sciences, economics and management, law and sport) as part of the development in the 2010s, but also several facilities pooled projected by the campus operation (conference center, students and international doctoral students accommodation centers, home business, documentation, logistics).\n\nThe area includes \"Supélec\" and a part of the University of Paris-Sud, which has 1,160 people in the area including 860 students.\nThe \"École Centrale Paris\", the \"École normale supérieure Paris-Saclay\" and part of the University of Paris-Sud should relocate there. There should then be around 8,100 staff, 5,000 students for engineering schools and 5000 students in the University of Paris-Sud.\n\nIt includes the Saclay Nuclear Research Centre, the Orphée reactor and SOLEIL in Saint-Aubin.\n\nThe area includes the \"École Polytechnique\", the ENSTA ParisTech, the \"Office National d'Études et de Recherches Aérospatiales\" and the \"École supérieure d'optique\".\nIt would bring together ParisTech \"grandes écoles\" in the project developed in the early 2000s : Agro ParisTech, the ENSAE ParisTech and Telecom ParisTech.\n\nIt includes Nokia, in Nozay.\n\nVarious extensions of the campus were criticized by environmental movements in the early 1990s who accuse it of reducing the size of the agricultural areas. These criticisms are reformulated in the expansion projects of the 2000s.\n\nSome also criticize a project that promotes the \"Grandes Ecoles\" too much, especially with regard to the governance of the Campus. The Snesup (\"Syndicat national de l'enseignement supérieur\") denounces \"a project based on an elitist vision of higher education\" and the exclusion of many institutions from the Board of Directors. The management project initiated by the \"campus plan\" has also been criticized by local politicians who criticize the state for being the sole leader of the project, or other project stakeholders who criticize the state of exercising too much intervention.\n\nThe organization referred to as a business cluster is also criticized by the actors who doubt its effectiveness or fear that its development would be detrimental to other geographical areas, as in the case of the University of Paris-Sud and the \"École normale supérieure Paris-Saclay\" leaving towns in the Paris region, or in the case of \"grandes écoles\" leaving Paris.\n\n\n\n"}
{"id": "10708943", "url": "https://en.wikipedia.org/wiki?curid=10708943", "title": "Pen (enclosure)", "text": "Pen (enclosure)\n\nA pen is an enclosure for holding animals such as livestock or pets that are unwanted inside the house. The term describes types of enclosures that may confine one or many animals. Construction and terminology vary depending on the region of the world, purpose, animal species to be confined, local materials used and tradition. \"Pen\" or \"penning\" as a verb refers to the act of confining animals in an enclosure.\n\nIn Australia and New Zealand a \"pen\" is a small enclosure for livestock (especially sheep or cattle), which is part of a larger construction, e.g. \"calf pen\", \"forcing pen\" (or yard) in sheep or cattle yards, or a \"sweating pen\" or \"catching pen\" in a shearing shed. In Australia, a paddock may encompass a large, fenced grazing area of many acres, not to be confused with the American English use of \"paddock\" as interchangeable with \"corral\" or \"pen\", describing smaller, confined areas.\n\nIn British English, a sheep pen is also called a \"folding\", \"sheepfold\" or \"sheepcote\". Modern shepherds more commonly use terms such as \"closing or confinement pen\" for small sheep pens. Most structures today referred to as \"sheepfolds\" are ancient dry stone semicircles.\n\nIn the United States, the term \"pen\" usually describes outdoor small enclosures for holding animals. These may be for encasing livestock or pets that cannot be kept indoors. Pens may be named by their purpose, such as a \"holding pen\", used for short-term confinement. A pen for cattle may also be called a \"corral\", a term borrowed from the Spanish language. Groups of pens that are part of a larger complex may be called a \"stockyard\", where a series of pens holds a large number of animals, or a \"feedlot\", which is type of stockyard used to confine animals that are being fattened. A large pen for horses is called a \"paddock\" (Eastern USA) or a \"corral\" (Western USA). In some places an exhibition arena may be called a \"show pen\". A small pen for horses (no more than 15–20 feet on any side) is only known as a pen if it lacks any roof or shelter, otherwise it is called a \"stall\" and is part of a stable. A large fenced grazing area of many acres is called a \"pasture\", or, in some cases, \"rangeland\".\n\nPrimitive pens in South Africa are called \"kraal\".\n\nFor pets, specialized folding fencing referred to as an exercise pen, x-pen, or ex pen, is used to surround an area, usually outdoors but not always, in which the animals can freely move around. They are commonly used for dogs, such as to give puppies or adult dogs more space than dog crates, but can also be used for rabbits and other animals. Exercise pens are usually made of sturdy wire, but can also be plastic or wood. \n\nHorses, during training, are often exercised in a round pen, sometimes referred to as an exercise pen.\n\n\n"}
{"id": "9986270", "url": "https://en.wikipedia.org/wiki?curid=9986270", "title": "Philip Woodward", "text": "Philip Woodward\n\nPhilip Mayne Woodward (6 September 1919 – 30 January 2018) was a British mathematician, radar engineer and horologist. He has achieved notable success in all three fields. Before retirement, he was a Deputy Chief Scientific Officer at the Royal Signals and Radar Establishment (RSRE) of the British Ministry of Defence in Malvern, Worcestershire.\n\nPhilip Woodward's career in the Scientific Civil Service spanned four decades. He was responsible for the software of one of the UK's first electronic computers (TREAC) followed by the UK's first solid state computer (RREAC). He is the author of the book \"Probability and Information Theory, with Applications to Radar\".\n\nDuring World War II, Philip Woodward developed a mathematical beam-shaping technique for radar antennae, which was later to become standard in the analysis of communication signals. His principal achievement in radar was to evaluate the ambiguities inherent in all radar signals and to show how Bayesian probability can be used as part of the design process to eliminate all but the wanted information the echoes might contain.\n\nIn 1956, Woodward’s work on radar information theory led Nobel Prize winning physicist John H. Van Vleck to invite him to give a postgraduate course on random processes at Harvard University. Professor E. T. Jaynes in his posthumously published book recognized Woodward as having been \"many years ahead of his time\" and as having shown \"prophetic insight into what was to come\" in the application of probability and statistics to the recovery of data from noisy samples. In the 1960s Philip Woodward's computer software team in Malvern provided the Royal Radar Establishment with the ALGOL 68R compiler, the world's first implementation of the programming language ALGOL 68, and provided the armed services with their first standard high-level programming language, Coral 66, for the small military computers of the day.\n\nHis academic posts have included Honorary Professor in Electrical Engineering at the University of Birmingham and Visiting Professor in Cybernetics at the University of Reading. When in 2000 the Woodward Building was opened by Sir John Chisholm at DERA (now privatized as QinetiQ), guests were given complimentary clocks as souvenirs of the occasion and of Philip Woodward's horological interests.\n\nIn June 2005, the Royal Academy of Engineering gave Woodward its first Lifetime Achievement Award, recognizing him as an outstanding pioneer of Radar and for his work in precision mechanical horology. In 2009 he received the Institute of Electrical and Electronics Engineers (IEEE), Dennis J. Picard Medal for Radar Technologies and Applications: “for pioneering work of fundamental importance in radar waveform design, including the Woodward ambiguity function, the standard tool for waveform and matched filter analysis.”\n\nIn retirement Philip Woodward wrote another classic book, \"My Own Right Time\" fondly known as MORT, a record of his passion for horology. Along with many other topics, MORT describes in detail the design of his clocks, including his masterpiece \"W5\".\n\nWoodward contributed dozens of articles to horological periodicals over more than 30 years. From his experience as a mathematician and analyst of complex systems, he has made major contributions to scientific horology, including the definitive analysis of balance springs and much work on the properties of pendulums. In 2006 the British Horological Institute published a hard-cover collection of 63 articles with new notes by Dr. Woodward. The collection, \"Woodward on Time\", originally compiled by Bill Taylor, ASC. became instantly known as \"WOT\". It was very well received.\n\n\"W5\" was built in a small workshop with the simplest of tools, but displays an elegance of concept and design rarely seen in the history of the science. It was acclaimed by Jonathan Betts, the Senior Curator of Horology at the Royal Observatory, Greenwich as \"the nearest approach to perfection by any mechanical timekeeper not employing a vacuum chamber\". Woodward built even the case, assembling it with intricate but invisible secret mitre joints.\n\nThe eminent horologist Anthony Randall carried on a long series of timekeeping trials of W5, showing unprecedented accuracy over periods of more than 100 days. Although the clock was widely celebrated, and Dr. Woodward published a series of ever-more-detailed articles on its construction to encourage others to carry its ideas forward, no one completed another clock like it for more than twenty years. Finally, in 2006 the Australian clockmaker David Walter (now of Buellton, California) succeeded in making a highly skeletonized version that while quite different in details, closely followed the basic Woodward design.\n\nWoodward was born on 6 September 1919 and educated at Blundell's School in Tiverton, Devon. He lived in Malvern, Worcestershire, England, where he died on 30 January 2018 at the age of 98.\n\n"}
{"id": "1866904", "url": "https://en.wikipedia.org/wiki?curid=1866904", "title": "Puffed grain", "text": "Puffed grain\n\nPuffed grains have been made for centuries with the simplest methods like popping popcorn. Modern puffed grains are often created using high temperature, pressure, or extrusion.\n\nPeople eat puffed grains in many ways, but it can be as simple as puffed grain alone and with sugar or salt for taste. Commercial products such as corn flakes and Corn Pops mix many ingredients into a homogeneous batter. The batter is then formed into shapes then toasted and/or extruded. This causes them to rise, but not puff or pop. Puffed grains are very healthful if plain, but when other ingredients are mixed with them they lose some of their previous health benefits.\n\nPuffed grains are popular as breakfast cereals and in the form of rice cakes. While it is easy to recognize that cereals came from whole grains, the expansion factor for rice cakes is even greater, and the final product is somewhat more homogeneous.\n\nThe oldest puffed grain was found in west central New Mexico in 1948 and 1950. Ears of popcorn were found that were up to 4,000 years old. These pieces of puffed grain were smaller than a penny to two inches in size and can be made in a similar way to popping popcorn.\nThe modern process of making puffed grains was invented by Dr. Alexander P. Anderson in 1901. He was doing an experiment dealing with the effect of heat and pressure on corn starch granules where he put them in six glass tubes, sealed them, and put them in an oven until they changed color. When Dr. Anderson took them out and cracked them open an explosion happened, he had made the corn starch turn into a puffed, white mass.\nDr. Alexander’s invention of puffed grain was first introduced at the World’s Fair in St. Louis in 1904. The puffed grain was shot from a battery of eight guns and on a poster it was called “The Eighth Wonder of the World.”\n\nHigh pressure puffed grain is created by placing whole grains under high pressure with steam in a containment vessel. When the vessel's seal is suddenly broken, the entrained steam then flashes and bloats the endosperm of the kernel, increasing its volume to many times its original size.\n\nPuffed rice or other grains are occasionally found as street food in China, Korea (called \"ppeong twigi\" 뻥튀기), and Japan (called \"pon gashi\" ), where hawkers implement the puffing process using an integrated pushcart/puffer featuring a rotating steel pressure chamber heated over an open flame. The great booming sound produced by the release of pressure serves as advertising.\n\nManufacturing puffed grain by venting a pressure chamber is essentially a batch process. To achieve large-scale efficiencies, continuous-process equipment has been developed whereby the pre-cooked cereal is injected into a high pressure steam chamber. It then exits the steam chamber via a Venturi tube to an expansion chamber, where the puffed cereal is collected and conveyed to the next process step. These devices, generally called stream puffing machines, were perfected in the latter half of the 20th century in Switzerland and Italy, but are now available from manufacturers in China as well.\n\n\n\nSnacks and food products made from puffed grain include:\n\n\n"}
{"id": "44077799", "url": "https://en.wikipedia.org/wiki?curid=44077799", "title": "Purex (bathroom tissue)", "text": "Purex (bathroom tissue)\n\nPurex bathroom tissue is Western Canada's #1 selling bathroom tissue and is manufactured and distributed by Kruger Inc. The product is manufactured at Kruger's plant in New Westminster, British Columbia.\n\nLaunched nearly a century ago, Purex Bathroom Tissue is available at grocery, drug, mass merchandise, club and dollar retailers in Western Canada in regular 2-ply and Ultra 3-ply formats, as well as Purex EnviroCare which is made from 100 per cent recycled paper.\n\nPurex Bathroom Tissue is hypoallergenic and dermatologist approved and are septic and sewer safe.\n"}
{"id": "5232825", "url": "https://en.wikipedia.org/wiki?curid=5232825", "title": "Radar tracker", "text": "Radar tracker\n\nA radar tracker is a component of a radar system, or an associated command and control (C2) system, that associates consecutive radar observations of the same target into tracks. It is particularly useful when the radar system is reporting data from several different targets or when it is necessary to combine the data from several different radars or other sensors.\n\nA classical rotating air surveillance radar system detects target echoes against a background of noise. It reports these detections (known as \"plots\") in polar coordinates representing the range and bearing of the target. In addition, noise in the radar receiver will occasionally exceed the detection threshold of the radar's Constant false alarm rate detector and be incorrectly reported as targets (known as false alarms). The role of the radar tracker is to monitor consecutive updates from the radar system (which typically occur once every few seconds, as the antenna rotates) and to determine those sequences of plots belonging to the same target, whilst rejecting any plots believed to be false alarms. In addition, the radar tracker is able to use the sequence of plots to estimate the current speed and heading of the target. When several targets are present, the radar tracker aims to provide one track for each target, with the track history often being used to indicate where the target has come from.\n\nWhen multiple radar systems are connected to a single reporting post, a multiradar tracker is often used to monitor the updates from all of the radars and form tracks from the combination of detections. In this configuration, the tracks are often more accurate than those formed from single radars, as a greater number of detections can be used to estimate the tracks.\nIn addition to associating plots, rejecting false alarms and estimating heading and speed, the radar tracker also acts as a filter, in which errors in the individual radar measurements are smoothed out. In essence, the radar tracker fits a smooth curve to the reported plots and, if done correctly, can increase the overall accuracy of the radar system.\nA multisensor tracker extends the concept of the multiradar tracker to allow the combination of reports from different types of sensor - typically radars, secondary surveillance radars (SSR), identification friend or foe (IFF) systems and electronic support measures (ESM) data.\n\nA radar track will typically contain the following information:\n\n\nIn addition, and depending on the application or tracker sophistication, the track will also include:\n\n\nThere are many different mathematical algorithms used for implementing a radar tracker, of varying levels of sophistication. However, they all perform steps similar to the following every time the radar updates:\n\n\nPerhaps the most important step is the updating of tracks with new plots. All trackers will implicitly or explicitly take account of a number of factors during this stage, including:\n\n\nUsing these information, the radar tracker attempts to update the track by forming a weighted average of the current reported position from the radar (which has unknown errors) and the last predicted position of the target from the tracker (which also has unknown errors). The tracking problem is made particularly difficult for targets with unpredictable movements (i.e. unknown target movement models), non-Gaussian measurement or model errors, non-linear relationships between the measured quantities and the desired target coordinates, detection in the presence of non-uniformly distributed clutter, missed detections or false alarms. In the real world, a radar tracker typically faces a combination of all of these effects; this has led to the development of an increasingly sophisticated set of algorithms to resolve the problem. Due to the need to form radar tracks in real time, usually for several hundred targets at once, the deployment of radar tracking algorithms has typically been limited by the available computational power.\n\nIn this step of the processing, the radar tracker seeks to determine which plots should be used to update which tracks. In many approaches, a given plot can only be used to update one track. However, in other approaches a plot can be used to update several tracks, recognising the uncertainty in knowing to which track the plot belongs. Either way, the first step in the process is to update all of the existing tracks to the current time by predicting their new position based on the most recent state estimate (e.g. position, heading, speed, acceleration, etc.) and the assumed target motion model (e.g. constant velocity, constant acceleration, etc.). Having updated the estimates, it is possible to try to associate the plots to tracks.\n\nThis can be done in a number of ways:\n\n\nOnce a track has been associated with a plot, it moves to the track smoothing stage, where the track prediction and associated plot are combined to provide a new, smoothed estimate of the target location.\n\nHaving completed this process, a number of plots will remain unassociated to existing tracks and a number of tracks will remain without updates. This leads to the steps of track initiation and track maintenance.\n\nTrack initiation is the process of creating a new radar track from an unassociated radar plot. When the tracker is first switched on, all the initial radar plots are used to create new tracks, but once the tracker is running, only those plots that couldn't be used to update an existing track are used to spawn new tracks. Typically a new track is given the status of tentative until plots from subsequent radar updates have been successfully associated with the new track. Tentative tracks are not shown to the operator and so they provide a means of preventing false tracks from appearing on the screen - at the expense of some delay in the first reporting of a track. Once several updates have been received, the track is confirmed and displayed to the operator. The most common criterion for promoting a tentative track to a confirmed track is the \"M-of-N rule\", which states that during the last N radar updates, at least M plots must have been associated with the tentative track - with M=3 and N=5 being typical values. More sophisticated approaches may use a statistical approach in which a track becomes confirmed when, for instance, its covariance matrix falls to a given size.\n\nTrack maintenance is the process in which a decision is made about whether to end the life of a track. If a track was not associated with a plot during the plot to track association phase, then there is a chance that the target may no longer exist (for instance, an aircraft may have landed or flown out of radar cover). Alternatively, however, there is a chance that the radar may have just failed to see the target at that update, but will find it again on the next update. Common approaches to deciding on whether to terminate a track include:\n\n\nIn this important step, the latest track prediction is combined with the associated plot to provide a new, improved estimate of the target state as well as a revised estimate of the errors in this prediction. There is a wide variety of algorithms, of differing complexity and computational load, that can be used for this process.\n\nAn early tracking approach, using an alpha beta filter, that assumed fixed covariance errors and a constant-speed, non-maneuvering target model to update tracks.\n\nThe role of the Kalman Filter is to take the current known state (i.e. position, heading, speed and possibly acceleration) of the target and predict the new state of the target at the time of the most recent radar measurement. In making this prediction, it also updates its estimate of its own uncertainty (i.e. errors) in this prediction. It then forms a weighted average of this prediction of state and the latest measurement of state, taking account of the known measurement errors of the radar and its own uncertainty in the target motion models. Finally, it updates its estimate of its uncertainty of the state estimate. A key assumption in the mathematics of the Kalman filter is that measurement equations (i.e. the relationship between the radar measurements and the target state) and the state equations (i.e. the equations for predicting a future state based on the current state) are linear.\n\nThe Kalman filter assumes that the measurement errors of the radar, and the errors in its target motion model, and the errors in its state estimate are all zero-mean with known covariance. This means that all of these sources of errors can be represented by a covariance matrix. The mathematics of the Kalman filter is therefore concerned with propagating these covariance matrices and using them to form the weighted sum of prediction and measurement.\n\nIn situations where the target motion conforms well to the underlying model, there is a tendency of the Kalman filter to become \"overconfident\" of its own predictions and to start to ignore the radar measurements. If the target then manoeuvres, the filter will fail to follow the manoeuvre. It is therefore common practice when implementing the filter to arbitrarily increase the magnitude of the state estimate covariance matrix slightly at each update to prevent this.\n\nThe MHT allows a track to be updated by more than one plot at each update, spawning multiple possible tracks. As each radar update is received every possible track can be potentially updated with every new update. Over time, the track branches into many possible directions. The MHT calculates the probability of each potential track and typically only reports the most probable of all the tracks. For reasons of finite computer memory and computational power, the MHT typically includes some approach for deleting the most unlikely potential track updates. The MHT is designed for situations in which the target motion model is very unpredictable, as all potential track updates are considered. For this reason, it is popular for problems of ground target tracking in Airborne Ground Surveillance (AGS) systems.\n\nThe IMM is an estimator which can either be used by MHT or JPDAF. IMM uses two or more Kalman filters which run in parallel, each using a different model for target motion or errors. The IMM forms an optimal weighted sum of the output of all the filters and is able to rapidly adjust to target maneuvers.\nWhile MHT or JPDAF handles the association and track maintenance, an IMM helps MHT or JPDAF in obtaining a filtered estimate of the target position.\n\nNon-linear tracking algorithms use a Non-linear filter to cope with the situation where the measurements have a non-linear relationship to the final track coordinates, where the errors are non-Gaussian, or where the motion update model is non-linear. The most common non-linear filters are:\n\n\nThe EKF is an extension of the Kalman filter to cope with cases where the relationship between the radar measurements and the track coordinates, or the track coordinates and the motion model, is non-linear. In this case, the relationship between the measurements and the state is of the form h = f(x) (where h is the vector of measurements, x is the target state and f(.) is the function relating the two). Similarly, the relationship between the future state and the current state is of the form x(t+1) = g(x(t)) (where x(t) is the state at time t and g(.) is the function that predicts the future state). To handle these non-linearities, the EKF linearises the two non-linear equations using the first term of the Taylor series and then treats the problem as the standard linear Kalman filter problem. Although conceptually simple, the filter can easily diverge (i.e. gradually perform more and more badly) if the state estimate about which the equations are linearised is poor.\n\nThe unscented Kalman filter and particle filters are attempts to overcome the problem of linearising the equations.\n\nThe UKF attempts to improve on the EKF by removing the need to linearise the measurement and state equations. It avoids linearization by representing the mean and covariance information in the form of a set of points, called sigma points. These points, which represent a distribution with specified mean and covariance, are then propagated directly through the non-linear equations, and the resulting five updated samples are then used to calculate a new mean and variance. This approach then suffers none of the problems of divergence due to poor linearisation and yet retains the overall computational simplicity of the EKF.\n\nThe particle filter could be considered as a generalisation of the UKF. It makes no assumptions about the distributions of the errors in the filter and neither does it require the equations to be linear. Instead it generates a large number of random potential states (\"particles\") and then propagates this \"cloud of particles\" through the equations, resulting in a different distribution of particles at the output. The resulting distribution of particles can then be used to calculate a mean or variance, or whatever other statistical measure is required. The resulting statistics are used to generate the random sample of particles for the next iteration. The particle filter is notable in its ability to handle multi-modal distributions (i.e. distributions where the PDF has more than one peak). However, it is computationally very intensive and is currently unsuitable for most real-world, real-time applications.\n\n\n"}
{"id": "48479695", "url": "https://en.wikipedia.org/wiki?curid=48479695", "title": "Ray Zinn", "text": "Ray Zinn\n\nRaymond D. \"Ray\" Zinn (born in El Centro, California, US, on September 24, 1937) is an inventor, entrepreneur and published author. In addition to a number of significant inventions improving wireless radio (RF) communication and voltage/power regulation devices, he is also the longest serving Chief Executive Officer of a Silicon Valley company (Micrel Corporation, acquired in 2015 by Microchip Technology).\n\nBorn on September 24, 1937, to Milton and Pauline Zinn, Ray Zinn grew up on a cattle ranch in the farming community of El Centro, CA. The eldest of 11 children, including six sisters and four brothers, Zinn was raised Mormon, and a strong work ethic was instilled at an early age by his parents. When he was four, his mother sent him to the store to lug home heavy bottles of milk. He had a driver's license at 13 to chauffeur his siblings in a Ford pickup and work with water management officials in the dead of night to assure the family ranch received their allocation. \"Work becomes instilled in you from an early age. So if somebody asks me how 'success' is spelled, I tell them it's spelled W-O-R-K.\"\n\nThroughout his high school and college years, Zinn was an accomplished gymnast, hurdler and member of the track and field team.\n\nDuring his junior year in college, he told his father he was quitting school and his father locked him naked, wrapped in a paper towel diaper, in the ranch office restroom for six hours. Zinn's father was giving his son a lesson about what it meant to cut short his education. \"He was saying, 'Well, if you're going to quit school, you're going out in the world the way you came in.' He wanted me to think about living with decisions,\" Zinn said. Ray Zinn changed his mind and finished college.\n\nHe graduated from Brigham Young University in 1960 with a BS in Industrial Management, and an MS in Business Administration from San Jose State University in 1968.\n\nBefore College, Zinn started at a janitorial business, working through sewage and waste on his spare time. He started out with a few years of experience, and quickly learned the way of maintaining hygiene. Although he started out with these humble beginnings, he wanted to be less like his father. His father and family had a long line of doing sanitation services throughout the bay area.\n\nAfter college, Zinn moved to the San Francisco Bay area, where high-tech companies were gaining traction. His interest in the space program and the desire to become an astronaut landed him a position at rocket motor manufacturing, United Technologies. Later, Zinn became interested in semiconductors through his father-in-law, who worked at Fairchild Semiconductor Corporation and joined the company as an engineer in 1963.\n\nHe left Fairchild in 1968 and subsequently held various executive management positions at several semiconductor-related companies, including Teledyne Inc. (1966–69), Nortek Inc. (1971-73) and Electromask TRE (1973–76). During this period, and later through Micrel, Zinn worked nearly every aspect of semiconductor management; sales, marketing, finance, accounting, wafer fab operations, assembly, packaging, test, quality assurance, and design.\n\nWhile working as a sales representative at Electromask, Zinn conceptualized the Wafer Stepper and sold it, before it had been designed or engineered, to Texas Instruments. According to Zinn, this unauthorized selling of equipment that had not yet been designed caused his boss at Electromask to tell him \"You really shouldn’t work for anybody else.\" Due to Electromask's partnership with IBM and that company's commitment to e-beam technology, Electromask never sought or acquired the patents for the Wafer Stepper.\n\nAlong with his friend Warren Muller, Zinn founded semiconductor company Micrel in 1978. The pair used $300,000 in savings and bank loans to found the company, eschewing the financial support of venture capitalists. \"I wanted control of my destiny and to do it my way.\" Zinn's desire to remain independent led him at one time to personally guarantee $4 million in bank loans to keep Micrel afloat.\n\nUnder Zinn's leadership, Micrel was profitable in its first year, and for 36 of its 37 years. Zinn credits this success to the company's disciplined focus on profitability. Micrel was also credited with having the lowest employee turn-over in the semiconductor industry and, though it is an unofficial measure, the highest \"boomerang\" employee rate, the rate at which employees who leave the company return.\n\nDuring an investor \"roadshow\" promoting Micrel's initial public offering in London, England, Zinn suffered a retinal vein occlusion which reduced his vision to 20/400, making Zinn legally blind. Zinn resisted turning control of Micrel over to another CEO. Instead, Zinn took advantage of every technology he could to compensate for his limited eyesight. Zinn continued to lead Micrel for another 20 years.\n\nZinn established a unique corporate culture at Micrel, which included unusual guidelines such as his \"no swearing\" policy. Swearing or using condescending language at the company was banned. Workers were also urged to be honest, show integrity and respect others at the company. \"We believe that work should be an extension of the home.\" Micrel's humanistic corporate culture has been referred \"an almost spiritual approach to doing business.\"\n\nMicrel was sold to Microchip Technology in 2015. Zinn was profiled in Sand Hill's leadership series because of the leadership ability demonstrated throughout his tenure as founder and CEO of Micrel.\n\n\nOne of Zinn's most significant inventions includes a method allowing radio devices to operate without the need to synchronize a receiver with a Transmitter. Using Frequency hopping (an idea originally co-created by Hedy Lamarr and George Antheil) and sweep modes, this method is primarily applied in low data rate wireless applications such as utility metering, machine and equipment monitoring and metering, and remote control. In 2006 Zinn received for his \"\"Wireless device and method using frequency hopping and sweep modes\".\" \n\nZinn also invented a method to quickly encode and/or decode RF signals. This technology forms the core of a number of devices, including (but not limited to) garage door openers and accompanying remote access; security or alarm systems and a remote access to activate and deactivate the security system; and low power, two-way radio communication devices.\n\n\nZinn is also accredited with the development of the \"analog control of a digital decision process\", an achievement he shares with \"Peter Chambers\" and \"Scott Brown\"; a \"pulse frequency modulated voltage regulator with linear regulator control\" (in collaboration with \"Charles L. Vinn\"), as well as:\n\nIn 2015, Zinn wrote \"Tough Things First\" (Published by McGraw Hill Education), in which he distilled his knowledge of leadership and management. \"Tough Things First\" is an autobiographical treatise on the interaction of people, management, and leadership, Self-discipline and organization discipline, and the mechanics of enterprises.\n\nIn 2018, Zinn wrote \"Zen of Zinn\", in which he discusses the interrelationships of people, society, entrepreneurship, business, leadership and life\n\nT.J. Rodgers, President & CEO, Cypress Semiconductor Corp\n\"Tough Things First is not a typical business book about a market success or effective business methods. It is about the success of one Silicon Valley startup company, Micrel, which has been run profitably by the same CEO/founder for 36 years. It is about how a founder's dedication to basic principles is required to make any startup successful. The precise recipe for success may change—for example, my 32-year-old company, Cypress Semiconductor, used venture funding, while Zinn preached and achieved financial independence—but Zinn shows how startups must have and truly practice their core values to succeed.\"\n\nGreg McKeown, Author of the New York Times bestseller \"Essentialism: The Disciplined Pursuit of Less\".\n\"The disciplined pursuit of what is essential is ten times harder than the undisciplined pursuit of the nonessential but it is a hundred times more valuable. This is brilliantly illustrated in Tough Things First.\"\n\nReed Wilcox, President of Southern Virginia University said of \"Tough Things First\" \"Many business self-help books are by experts who study and talk to the people who actually do it, but have not lived it themselves. It's not in their blood. This is very different. I have not seen a better, more practical, more realistic, more helpful, or more engaging invitation to entrepreneurship.\"\n\n"}
{"id": "48191844", "url": "https://en.wikipedia.org/wiki?curid=48191844", "title": "Scandiobabingtonite", "text": "Scandiobabingtonite\n\nScandiobabingtonite was first discovered in the Montecatini granite quarry near Baveno, Italy in a pegmatite cavity. Though found in pegmatites, the crystals of scandiobabingtonite are sub-millimeter sized, and are tabular shaped. Scandiobabingtonite was the sixth naturally occurring mineral discovered with the rare earth element scandium, and grows around babingtonite, with which it is isostructural, hence the namesake. It is also referred to as scandian babingtonite. The ideal chemical formula for scandiobabingtonite is Ca(Fe,Mn)ScSiO(OH).\n\nScandiobabingtonite is found in association with orthoclase, quartz, light blue albite, stilbite, fluorite, and mica. When found with these minerals, the scandiobabingtonite crystals are emplanted on the surface of the other minerals. It also occurs as growth around green-black prismatic crystals of babingtonite. The samples of scandiobabingtonite that have been discovered have shown that they start out growing from a seed of babingtonite crystal. This is how scandiobabingtonite gets its chemical structure. The starting seed of babingtonite is still present in the center of the resulting crystal and can be detected with optical and chemical studies. Scandiobabingtonite is a uniquely rare mineral, as it occurs in very small amounts in few locations around the world. It is one of thirteen naturally occurring minerals where scandium is a dominant member. The other scandium minerals are bazzite, cascandite, hetftetjernite, jervisite, juonniite, kolbeckite, kristiansenite, magbasite, oftedalite, pretulite, thortveitite, and titanowodginite. Scandium can also concentrate in other minerals, such as in ferromagnesian minerals, aluminum phosphate minerals, meteoric minerals, and other minerals containing rare earth elements, but it occurs in trace amounts.\n\nScandiobabingtonite is a colorless or lightly gray-green colored transparent mineral with a glassy or vitreous luster. It exhibits a hardness of 6 on the Mohs hardness scale. Scandiobabingtonite occurs as short, prismatic crystals that are slightly elongated on the [001] axis which gives it a tabular or platy shape. Its crystals are characterized by the {010}, {001}, {110}, {1-10}, and {101} faces. Scandiobabingtonite is brittle and shows perfect cleavage along the {001} and {1-10} planes. The measured density is 3.24 g/cm.\n\nScandiobabingtonite is biaxial positive, which means it will refract light along two axes. It exhibits a 2V=64(2)°, strong dispersion with r>v, and displays strong pleochroism with colors ranging from pink (γ') to green(α'). The extinction angle along the (110) is 6°. Z:Φ=-250°, ρ=47°; Y:Φ=146°, ρ=75°; X:Φ=42°, ρ=47°.\n\nScandiobabingtonite is isostructural with babingtonite, and has the same chemical properties as well. It is an inosilicate with 5-periodic single chains. Scandium replaces the Fe in babingtonite in six-fold coordination. The empirical chemical formula for scandiobabingtonite is (Ca,Na)(Fe,Mn)(Sc,Sn,Fe)SiO(OH). Simplified, the formula is Ca(Fe,Mn)ScSiO(OH)\n\nScandiobabingtonite is in the triclinic crystal system, with space group P1. The unit cell dimensions are a=7.536(2) Å, b=11.734(2) Å, c=6.748(2) Å, α=91.70(2)°, β=93.86(2)°, γ=104.53(2)°. These dimensions are almost identical to those of babingtonite. The difference in dimensions is caused by the replacement of iron with scandium in the Fe-centered octahedra. The Fe-O distance measures as 2.048 Å, while the Sc-O distance is 2.092 Å. This equates to a slightly larger octahedra in scandiobabingtonite than babingtonite.\n\nList of Minerals\n"}
{"id": "1095325", "url": "https://en.wikipedia.org/wiki?curid=1095325", "title": "Smart bullet", "text": "Smart bullet\n\nA smart bullet is a bullet that is able to do something other than simply follow its given trajectory, such as turning, changing speed or sending data.\n\nFirst efforts in this area were the 1980 NATO 'Copperhead' 155mm artillery shell project using mid flight adjustment onto target reflected laser guidance. This was shelved.\n\nIn 2008 the EXACTO program began under DARPA to develop a \"fire and forget\" smart sniper rifle system including a guided smart bullet and improved scope. The exact technologies of this smart bullet have not been released. EXACTO was test fired in 2014 and 2015 and results showing the bullet alter course to correct its path to its target were released.\n\nIn 2012 Sandia National Laboratories announced a self-guided bullet prototype that could track a target illuminated with a laser designator. The bullet is capable of updating its position 30 times a second and hitting targets over a mile away.\n\nIn mid-2016, Russia revealed it was developing a similar \"smart bullet\" weapon designed to hit targets at a distance of up to .\n\nOne kind of smart bullet is a projectile that is capable of changing its course during flight. One use of this would be to enable soldiers to stay behind protective cover and shoot around corners. One implementation uses a spoiler and micro gyro to control the bullet.\n\nHoneywell Aerospace has produced inertial measurement units based on MEMS and microelectronics technologies that it claims can survive the shock of being fired out of a gun. \n\nAnother smart bullet is one that can transmit data about the location into which it has been fired. A prototype has been created by researchers at the University of Florida in Gainesville, Florida, USA with funding from Lockheed Martin. The bullet (projectile) has a sensor inside of it that can send wireless data up to 70 meters.\n\nAnother smart bullet is one that self-destructs within a limited range. This would be used to minimize collateral damage of a bullet in case of a miss. For example, hunting near a populated area. This would involve either change in course into the ground, or near vaporization.\n\n"}
{"id": "40100778", "url": "https://en.wikipedia.org/wiki?curid=40100778", "title": "Software map", "text": "Software map\n\nA software map represents static, dynamic, and evolutionary information of software systems and their software development processes by means of 2D or 3D map-oriented information visualization. It constitutes a fundamental concept and tool in software visualization, software analytics, and software diagnosis. Its primary applications include risk analysis for and monitoring of code quality, team activity, or software development progress and, generally, improving effectiveness of software engineering with respect to all related artifacts, processes, and stakeholders throughout the software engineering process and software maintenance.\n\nSoftware maps are applied in the context of software engineering: Complex, long-term software development projects are commonly faced by manifold difficulties such as the friction between completing system features and, at the same time, obtaining a high degree of code quality and software quality to ensure software maintenance of the system in the future. \nIn particular, \"Maintaining complex software systems tends to be costly because developers spend a significant part of their time with trying to understand the system’s structure and behavior.\" The key idea of software maps is to cope with that challenge and optimization problems by providing effective communication means to close the communication gap among the various stakeholders and information domains within software development projects and obtaining insights in the sense of information visualization.\n\nSoftware maps take advantage of well-defined cartographic map techniques using the virtual 3D city model metaphor to express the underlying complex, abstract information space. The metaphor is required \"since software has no physical shape, there is no natural mapping of software to a two-dimensional space\". Software maps are non-spatial maps that have to convert the hierarchy data and its attributes into a spatial representation.\n\nSoftware maps generally allow for comprehensible and effective communication of course, risks, and costs of software development projects to various stakeholders such as management and development teams. \nThey communicate the status of applications and systems currently being developed or further developed to project leaders and management at a glance. \"A key aspect for this decision making is that software maps provide the structural context required for correct interpretation of these performance indicators\". As an instrument of communication, software maps act as open, transparent information spaces which enable priorities of code quality and the creation of new functions to be balanced against one another and to decide upon and implement necessary measures to improve the software development process.\n\nFor example, they facilitate decisions as to where in the code an increase of quality would be beneficial both for speeding up current development activities and for reducing risks of future maintenance problems.\n\nDue to their high degree of expressiveness (e.g., information density) and their instantaneous, automated generation, the maps additionally serve to reflect the current status of system and development processes, bridging an essential information gap between management and development teams, improve awareness about the status, and serve as early risk detection instrument.\n\nSoftware maps are based on objective information as determined by the KPI driven code analysis as well as by imported information from software repository systems, information from the source codes, or software development tools and programming tools. In particular, software maps are not bound to a specific programming language, modeling language, or software development process model.\n\nSoftware maps use the hierarchy of the software implementation artifacts such as source code files as base to build a tree mapping, i.e., a rectangular area that represents the whole hierarchy, subdividing the area into rectangular sub-areas. A software map, informally speaking, looks similar to a virtual 3D city model, whereby artifacts of the software system appear as virtual, rectangular 3D buildings or towers, which are placed according to their position in the software implementation hierarchy.\n\nSoftware maps can express and combine information about software development, software quality, and system dynamics by mapping that information onto visual variables of the tree map elements such as footprint size, height, color or texture. They can systematically be specified, automatically generated, and organized by templates.\n\nSoftware maps \"combine thematic information about software development processes (evolution), software quality, structure, and dynamics and display that information in a cartographic manner\". For example: \n\nWith this exemplary configuration, the software map shows crucial points in the source code with relations to aspects of the software development process. For example, it becomes obvious at a glance what to change in order to:\n\nSoftware maps represent key tools in the scope of automated software diagnosis software diagnostics.\n\nSoftware maps can be used, in particular, as analysis and presentation tool of business intelligence systems, specialized in the analysis of software related data. Furthermore, software maps \"serve as recommendation systems for software engineering\".\n\nSoftware maps are not limited by software-related information: They can include any hierarchical system information as well, for example, maintenance information about complex technical artifacts.\n\nSoftware maps are investigated in the domain of software visualization. The visualization of software maps is commonly based on tree mapping, \"a space-filling approach to the visualization of hierarchical information structures\" or other hierarchy mapping approaches.\n\nTo construct software maps, different layout approaches are used to generate the basic spatial mapping of components such as: \n\nThe spatial arrangement computed by layouts such as defined by tree maps strictly depends on the hierarchy. If software maps have to be generated frequently for an evolving or changing system, the usability of software maps is affected by non-stable layouts, that is, minor changes to the hierarchy may cause significant changes to the layout.\n\nIn contrast to regular Voronoi treemap algorithms, which do not provide deterministic layouts, layout algorithm for Voronoi treemaps can be extended to provides a high degree of layout similarity for varying hierarchies. Similar approaches exist for the tree-map based case.\n\nSoftware maps methods and techniques belong the scientific displine of Software visualization and information visualization. They form a key concept and technique within the fields of software diagnosis. They have applications also in software mining and software analytics. Software maps have been extensively developed and researched by, e.g., at the Hasso Plattner Institute for IT systems engineering, in particular for large-scale, complex IT systems and applications.\n\n"}
{"id": "1220790", "url": "https://en.wikipedia.org/wiki?curid=1220790", "title": "Spallation", "text": "Spallation\n\nSpallation is a process in which fragments of material (spall) are ejected from a body due to impact or stress. In the context of impact mechanics it describes ejection or vaporization of material from a target during impact by a projectile. In planetary physics, spallation describes meteoritic impacts on a planetary surface and the effects of a stellar wind on a planetary atmosphere. In the context of mining or geology, spallation can refer to pieces of rock breaking off a rock face due to the internal stresses in the rock; it commonly occurs on mine shaft walls. In the context of anthropology, spallation is a process used to make stone tools such as arrowheads by knapping. In nuclear physics, spallation is the process in which a heavy nucleus emits a large number of nucleons as a result of being hit by a high-energy particle, thus greatly reducing its atomic weight.\n\nSpallation can occur when a tensile stress wave propagates through a material and can be observed in flat plate impact tests. It is caused by an internal cavitation due to stresses, which are generated by the interaction of stress waves, exceeding the local tensile strength of materials. A fragment or multiple fragments will be created on the free end of the plate. This fragment known as \"spall\" acts as a secondary projectile with velocities that can be as high as one third of the stress wave speed on the material. This type of failure is typically an effect of high explosive squash head (HESH) charges.\n\nLaser induced spallation is a recent experimental technique developed to understand the adhesion of thin films with substrates. A high energy pulsed laser (typically ) is used to create a compressive stress pulse in the substrate wherein it propagates and reflects as a tensile wave at the free boundary. This tensile pulse spalls/peels the thin film while propagating towards the substrate. Using theory of wave propagation in solids it is possible to extract the interface strength. The stress pulse created in this example is usually around 3-8 nanoseconds in duration while its magnitude varies as a function of laser fluence. Due to the non-contact application of load, this technique is very well suited to spall ultra-thin films (1 micrometre in thickness or less). It is also possible to mode convert a longitudinal stress wave into a shear stress using a pulse shaping prism and achieve shear spallation.\n\nNuclear spallation occurs naturally in Earth's atmosphere owing to the impacts of cosmic rays, and also on the surfaces of bodies in space such as meteorites and the Moon. Evidence of cosmic ray spallation (also known as \"spoliation\") is seen on outer surfaces of bodies, and gives a means of measuring the length of time of exposure. The composition of the cosmic rays themselves also indicates that they have suffered spallation before reaching Earth, because the proportion of light elements such as Li, B, and Be in them exceeds average cosmic abundances; these elements in the cosmic rays were evidently formed from spallation of oxygen, nitrogen, carbon and perhaps silicon in the cosmic ray sources or during their lengthy travel here. \"Cosmogenic\" isotopes of aluminium, beryllium, chlorine, iodine and neon, formed by spallation of terrestrial elements under cosmic ray bombardment, have been detected on Earth.\n\nNuclear spallation is one of the processes by which a particle accelerator may be used to produce a beam of neutrons. A particle beam consisting of protons at around 1 GeV are shot into a target consisting of mercury, tantalum, lead or another heavy metal. The target nucleii are excited and upon deexcitation, 20 to 30 neutrons are expelled per nucleus. Although this is a far more expensive way of producing neutron beams than by a chain reaction of nuclear fission in a nuclear reactor, it has the advantage that the beam can be pulsed with relative ease. Furthermore the energetic cost of one spallation neutron is six times lower than that of a neutron gained via nuclear fission. In contrast to nuclear fission, the spallation neutrons cannot trigger further spallation or fission processes to produce further neutrons. Therefore, there is no chain-reaction, which makes the process non-critical. The concept of nuclear spallation was first coined by Nobelist Glenn T. Seaborg in his doctoral thesis on the inelastic scattering of neutrons in 1937.\n\nGenerally the production of neutrons at a spallation source begins with a high-powered proton accelerator. The accelerator may consist of a linac only (as in the European Spallation Source) or a combination of linac and synchrotron (e.g. ISIS neutron source) or a cyclotron (e.g. PSI) . As an example, the ISIS neutron source is based on some components of the former Nimrod synchrotron. Nimrod was uncompetitive for particle physics so it was replaced with a new synchrotron, initially using the original injectors, but which produces a highly intense pulsed beam of protons. Whereas Nimrod would produce around 2 µA at 7 GeV, ISIS produces 200 µA at 0.8 GeV. This is pulsed at the rate of 50 Hz, and this intense beam of protons is focused onto a target. Experiments have been done with depleted uranium targets but although these produce the most intense neutron beams, they also have the shortest lives. Generally, therefore, tantalum or tungsten targets have been used. Spallation processes in the target produce the neutrons, initially at very high energies—a good fraction of the proton energy. These neutrons are then slowed in moderators filled with liquid hydrogen or liquid methane to the energies that are needed for the scattering instruments. Whilst protons can be focused since they have charge, chargeless neutrons cannot be, so in this arrangement the instruments are arranged around the moderators.\nInertial confinement fusion has the potential to produce orders of magnitude more neutrons than spallation. This could be useful for Neutron radiography which can be used to locate hydrogen atoms in structures, resolve atomic thermal motion and study collective excitations of photons more effectively than X-rays.\n\n\n\n"}
{"id": "346001", "url": "https://en.wikipedia.org/wiki?curid=346001", "title": "Telecommunication circuit", "text": "Telecommunication circuit\n\nA telecommunication circuit is any line, conductor, or other conduit by which information is transmitted. Originally, this was analog, and was often used by radio stations as a studio/transmitter link (STL) or remote pickup unit (RPU) for their audio, sometimes as a backup to other means. Later lines were digital, and used for private corporate data networks.\n\nA leased line is a circuit that is dedicated to only one use. The opposite of a dedicated circuit is a switched circuit, which can be connected to different paths. A POTS or ISDN telephone line is a switched circuit, because it can connect to any other telephone number.\n\nOn digital lines, a virtual circuit can be created to serve either purpose, while sharing a single physical circuit.\n\nA telecommunication circuit may be defined as follows: \n\n\n"}
{"id": "518398", "url": "https://en.wikipedia.org/wiki?curid=518398", "title": "The Codebreakers", "text": "The Codebreakers\n\nThe Codebreakers – The Story of Secret Writing () is a book by David Kahn, published in 1967, comprehensively chronicling the history of cryptography from ancient Egypt to the time of its writing. The United States government attempted to have the book altered before publication, and it succeeded in part.\n\nBradford Hardie III, an American cryptographer during World War II, contributed insider information, German translations from original documents, and intimate real-time operational explanations to \"The Codebreakers\".\n\n\"The Codebreakers\" is widely regarded as the best account of the history of cryptography up to its publication. William Crowell, the former deputy director of the National Security Agency, was quoted in \"Newsday\" magazine: \"Before he (Kahn) came along, the best you could do was buy an explanatory book that usually was too technical and terribly dull.\"\n\nKahn, then a journalist, was contracted to write a book on cryptology in 1961. He began writing it part-time, and then he quit his job to work on it full-time. The book was to include information on the NSA, and according to the author James Bamford, in 1982, the agency attempted to stop its publication. The NSA considered various options, including writing a negative review of Kahn's work to be published in the press to discredit him.\n\nA committee of the United States Intelligence Board concluded that the book was \"a possibly valuable support to foreign COMSEC authorities\" and recommended \"further low-key actions as possible, but short of legal action, to discourage Mr. Kahn or his prospective publishers\". Kahn's publisher, Macmillan and Sons, handed over the manuscript to the government for review without Kahn's permission on 4 March 1966. Kahn and Macmillan eventually agreed to remove some material from the manuscript, particularly concerning the relationship between the NSA and its counterpart in the United Kingdom, GCHQ.\n\nThe book finishes with a chapter on SETI. Because of the year of its publication, the book did not cover most of the history concerning the breaking of the German Enigma machine, which became public knowledge during the 1970s. Hence, not much was said of Alan Turing. It also did not cover the advent of strong cryptography in the public domain, beginning with the invention of public key cryptography and the specification of the Data Encryption Standard in the mid-1970s. The book was republished in 1996, and this new edition included an additional chapter briefly covering the events since the original publication.\n\n\n"}
{"id": "9279338", "url": "https://en.wikipedia.org/wiki?curid=9279338", "title": "Thin filament pyrometry", "text": "Thin filament pyrometry\n\nThin filament pyrometry (TFP) is an optical method used to measure temperatures. It involves the placement of a thin filament in a hot gas stream. Radiative emissions from the filament can be correlated with filament temperature. Filaments are typically silicon carbide (SiC) fibers with a diameter of 15 micrometres. Temperatures of about 800–2500 K can be measured.\n\nTFP was first used by V. Vilimpoc and L.P. Goss (1988). A recent paper using TFP is Maun et al. (2007).\n\nThe typical TFP apparatus consists of a flame or other hot gas stream, a filament, and a camera. \n\nTFP has several advantages, including the ability to simultaneously measure temperatures along a line and minimal intrusiveness. Most other forms of pyrometry are not capable of providing gas-phase temperatures.\n\nCalibration is required. Calibration typically is performed with a thermocouple. Both thermocouples and filaments require corrections in estimating gas temperatures from probe temperatures. Also, filaments are fragile and typically break after about an hour in a flame.\n\nThe primary application is to combustion and fire research.\n\n\n"}
{"id": "9557604", "url": "https://en.wikipedia.org/wiki?curid=9557604", "title": "Tire uniformity", "text": "Tire uniformity\n\nTire Uniformity refers to the dynamic mechanical properties of pneumatic tires as strictly defined by a set of measurement standards and test conditions accepted by global tire and car makers. These standards include the parameters of radial force variation, lateral force variation, conicity, plysteer, radial run-out, lateral run-out, and sidewall bulge. Tire makers worldwide employ tire uniformity measurement as a way to identify poorly performing tires so they are not sold to the marketplace. Both tire and vehicle manufacturers seek to improve tire uniformity in order to improve vehicle ride comfort.\n\nThe circumference of the tire can be modeled as a series of very small spring elements whose spring constants vary according to manufacturing conditions. These spring elements are compressed as they enter the road contact area, and recover as they exit the footprint. Variation in the spring constants in both radial and lateral directions cause variations in the compressive and restorative forces as the tire rotates. Given a perfect tire, running on a perfectly smooth roadway, the force exerted between the car and the tire will be constant. However, a normally manufactured tire running on a perfectly smooth roadway will exert a varying force into the vehicle that will repeat every rotation of the tire. This variation is the source of various ride disturbances. Both tire and car makers seek to reduce such disturbances in order to improve the dynamic performance of the vehicle.\n\nTire forces are divided into three axes: radial, lateral, and tangential (or fore-aft). The radial axis runs from the tire center toward the tread, and is the vertical axis running from the roadway through the tire center toward the vehicle. This axis supports the vehicle’s weight. The lateral axis runs sideways across the tread. This axis is parallel to the tire mounting axle on the vehicle. The tangential axis is the one in the direction of the tire travel.\n\nIn so far as the radial force is the one acting upward to support the vehicle, radial force variation describes the change in this force as the tire rotates under load. As the tire rotates and spring elements with different spring constants enter and exit the contact area, the force will change. Consider a tire supporting a 1,000 pound load running on a perfectly smooth roadway. It would be typical for the force to vary up and down from this value. A variation between 995 pounds and 1003 pounds would be characterized as an 8-pound radial force variation (RFV). RFV can be expressed as a peak-to-peak value, which is the maximum minus minimum value, or any harmonic value as described below.\n\nSome tire manufactures mark the sidewall with a red dot to indicate the location of maximal radial force and runout, the high spot. A yellow dot indicates the point of least weight. Use of the dots is specified in Technology Maintenance Council's performance standard RP243. To compensate for this variation, tires are supposed to be installed with the red dot near the valve stem, assuming the valve stem is at the low point, or with the yellow dot near the valve stem, assuming the valve stem is at the heavy point.\n\nRFV, as well as all other force variation measurements, can be shown as a complex waveform. This waveform can be expressed according to its harmonics by applying Fourier Transform (FT). FT permits one to parameterize various aspects of the tire dynamic behavior. The first harmonic, expressed as RF1H (radial force first harmonic) describes the force variation magnitude that exerts a pulse into the vehicle one time for each rotation. RF2H expresses the magnitude of the radial force that exerts a pulse twice per revolution, and so on. Often, these harmonics have known causes, and can be used to diagnose production problems. For example, a tire mold installed with 8 bolts may thermally deform as to induce an eighth harmonic, so the presence of a high RF8H would point to a mold bolting problem. RF1H is the primary source of ride disturbances, followed by RF2H. High harmonics are less problematic because the rotating speed of the tire at highway speeds times the harmonic value makes disturbances at such high frequencies that they are damped or overcome by other vehicle dynamic conditions.\n\nInsofar as the lateral force is the one acting side-to-side along the tire axle, lateral force variation describes the change in this force as the tire rotates under load. As the tire rotates and spring elements with different spring constants enter and exit the contact area, the lateral force will change. As the tire rotates it may exert a lateral force on the order of 25 pounds, causing steering pull in one direction. It would be typical for the force to vary up and down from this value. A variation between 22 pounds and 26 pounds would be characterized as a 4-pound lateral force variation, or LFV. LFV can be expressed as a peak-to-peak value, which is the maximum minus minimum value, or any harmonic value as described above. Lateral force is signed, such that when mounted on the vehicle, the lateral force may be positive, making the vehicle pull to the left, or negative, pulling to the right.\n\nInsofar as the tangential force is the one acting in the direction of travel, tangential force variation describes the change in this force as the tire rotates under load. As the tire rotates and spring elements with different spring constants enter and exit the contact area, the tangential force will change. As the tire rotates it exerts a high traction force to accelerate the vehicle and maintain its speed under constant velocity. Under steady-state conditions it would be typical for the force to vary up and down from this value. This variation would be characterized as TFV. In a constant velocity test condition, TFV would be manifested as a small speed fluctuation occurring every rotation due to the change in rolling radius of the tire.\n\nConicity is a parameter based on lateral force behavior. It is the characteristic that describes the tire’s tendency to roll like a cone. This tendency affects the steering performance of the vehicle. In order to determine Conicity, lateral force must be measured in both clockwise (LFCW) and counterclockwise direction (LFCCW). Conicity is calculated as one-half the difference of the values, keeping in mind that CW and CCW values have opposite signs. Conicity is an important parameter in production testing. In many high-performance cars, tires with equal conicity are mounted on left and right sides of the car in order that their conicity effects will cancel each other and generate a smoother ride performance, with little steering effect. This necessitates the tire maker measuring conicity and sorting tires into groups of like-values.\n\nPly steer describes the lateral force a tire generates due to asymmetries in its carcass as is rolls forward with zero slip angle and may be called pseudo side slip. It is the characteristic that is usually described as the tire’s tendency to “crab walk”, or move sideways while maintaining a straight-line orientation. This tendency affects the steering performance of the vehicle. In order to determine ply steer, the lateral force generated is measured as the tire rolls both forward and back, and ply steer is then calculated as one-half the sum of the values, keeping in mind that values have opposite signs.\n\nRadial run-out (RRO) describes the deviation of the tire’s roundness from a perfect circle. RRO can be expressed as the peak-to-peak value as well as harmonic values. RRO imparts an excitation into the vehicle in a manner similar to radial force variation. RRO is most often measured near the tire’s centerline, although some tire makers have adopted measurement of RRO at three positions: left shoulder, center, and right shoulder.\n\nSome tire manufactures mark the sidewall with a red dot to indicate the location of maximal radial force and runout.\n\nLateral run-out (LRO) describes the deviation of the tire’s sidewall from a perfect plane. LRO can be expressed as the peak-to-peak value as well as harmonic values. LRO imparts an excitation into the vehicle in a manner similar to lateral force variation. LRO is most often measured in the upper sidewall, near the tread shoulder.\n\nGiven that the tire is an assembly of multiple components that are cured in a mold, there are many process variations that cause cured tires to be classified as rejects. Bulges and depressions in the sidewall are such defects. A bulge is a weak spot in the sidewall that expands when the tire is inflated. A depression is a strong spot that does not expand in equal measure as the surrounding area. Both are deemed visual defects. Tires are measured in production to identify those with excessive visual defects. Bulges may also indicate defective construction conditions such as missing cords, which pose a safety hazard. As a result, tire makers impose stringent inspection standards to identify tires with bulges. Sidewall Bulge and Depression is also referred to as bulge and dent, and bumpy sidewall.\n\nTire Uniformity Machines are special-purpose machines that automatically inspect tires for the tire uniformity parameters described above. They consist of several subsystems, including tire handling, chucking, measurement rims, bead lubrication, inflation, load wheel, spindle drive, force measurement, and geometry measurement.\n\nThe tire is first centered, and the bead areas are lubricated to assure a smooth fitment to the measurement rims. The tire is indexed into the test station and placed on the lower chuck. The upper chuck lowers to make contact with the upper bead. The tire is inflated to the set point pressure. The load wheel advances to contact the tire and apply the set loading force. The spindle drive accelerates the tire to the test speed. Once speed, force, and pressure are stable, load cells measure the force exerted on the load wheel by the tire. The force signal is processed in analog circuitry, and then analyzed to extract the measurement parameters. Tires are marked according to various standards that may include RFV high point angle, side of positive conicity, and conicity magnitude.\n\nThere are numerous variations and innovations among several tire uniformity machine makers. The standard test speed for tire uniformity machines is 60 rpm of a standard load wheel that approximates 5 miles per hour. High speed uniformity machines are used in research and development environments that reach 250 km/h and higher. High speed uniformity machines have also been introduced for production testing. Machines that combine force variation measurement with dynamic balance measurement are also in use.\n\nRadial and Lateral Force Variation can be reduced at the Tire Uniformity Machine via grinding operations. In the Center Grind operation, a grinder is applied to the tread center to remove rubber at the high point of RFV. On the top and bottom tread shoulder grinders are applied to reduce the size of the road contact area, or footprint, and the resulting force variation. Top and bottom grinders can be controlled independently to reduce conicity values. Grinders are also employed to correct excessive radial run-out.\n\nEffects of tire variations can also be reduced by mounting the tire in such a way that unbalanced rims and valve stems helps compensate for imperfect tires.\n\nRadial run-out, Lateral run-out, Conicity, and Bulge measurements are also performed on the tire uniformity machine. There are several generations of measurement technologies in use. These include Contact Stylus, Capacitive Sensors, Fixed-Point Laser Sensors, and Sheet-of-Light Laser Sensors.\n\nContact Stylus technology utilizes a touch-probe to ride along the tire surface as it rotates. Analog instrumentation senses the movement of the probe, and records the run-out waveform. When used to measure radial runout, the stylus is fitted to a large-area paddle that can span the voids in the tread pattern. When used to measure lateral runout on the sidewall the stylus runs in a very narrow smooth track. The contact stylus method is one of the earliest technologies, and requires considerable effort to maintain its mechanical performance. The small area-of-interest in the sidewall area limits the effectiveness in discerning sidewall bulges and depressions elsewhere on the sidewall.\n\nCapacitive Sensors generate a dielectric field between the tire and sensor. As the distance between the tire and the sensor varies, the voltage and/or current properties of the dielectric field change. Analog circuitry is employed to measure the field changes and record the run-out waveform. Capacitive sensors have a larger area-of-interest, on the order of 10mm compared to the very narrow contact stylus method. The capacitive sensor method is one of the earliest technologies, and has proven highly reliable; however, the sensor must be positioned very close to the tire surface during measurement, so collisions between tire and sensor have led to long-term maintenance problems. In addition, some sensors are very sensitive to moisture/humidity and ended with erroneous readings. The 10mm area-of-interest also means that bulge measurement is limited to a small portion of the tire. Capacitive sensors employ void filtering to remove the effect of the voids between the tread lugs in radial runout measurement, and letter filtering to remove the effect of raised letters and ornamentation on the sidewall.\n\nFixed-Point Laser Sensors were developed as an alternative to the above methods. Lasers combine the narrow-track area-of-interest with a large stand off distance from the tire. In order to cover a larger area-of-interest, mechanical positioning systems have been employed to take readings at multiple positions in the sidewall. Fixed-Point Laser sensors employ void filtering to remove the effect of the voids between the tread lugs in radial run-out measurement, and letter filtering to remove the effect of raised letters and ornamentation on the sidewall.\n\nSheet-of-Light Laser (SL) Systems were introduced in 2003, and have emerged as the most capable and reliable run-out, bulge and depression measurement methods. SL sensors project a laser line instead of a laser point, and thereby create a very large area-of-interest. Sidewall sensors can easily span an area from the bead area to the tread shoulder, and inspect the complete sidewall for bulge and depression defects. Large radial sensors can span 300mm or more to cover the entire tread width. This enables characterization of RRO in multiple tracks. SL sensors also feature stand off distances large enough to assure no collisions with the tire. Two-dimensional tread void filtering and sidewall letter filtering are also employed to eliminate these characteristics from the runout measurements.\n"}
{"id": "54086653", "url": "https://en.wikipedia.org/wiki?curid=54086653", "title": "Tomato slicer", "text": "Tomato slicer\n\nA tomato slicer is an apparatus designed to slice tomatoes and other soft fruits and vegetables.\n\nTomato slicers have been sold since the 1950s. A tomato slicer was patented in the United States in 1968, by Clayton Giangiulio.\n\n"}
{"id": "21551858", "url": "https://en.wikipedia.org/wiki?curid=21551858", "title": "Trans Europe Halles", "text": "Trans Europe Halles\n\nTrans Europe Halles (TEH) is a European-based network of cultural centres initiated by citizens and artists.\n\nIn 1983 the independent cultural centre \"Les Halles de Schaerbeek\" (Brussels, Belgium) organised a weekend of discussions in Brussels to enable European independent cultural centres to exchange experiences and participate in events under the theme of \"Adventures of the rediscovered ark\". This three-day forum focused on an alternative culture emerging in rehabilitated industrial buildings, and asserting its identity despite the reservations of political authorities. Seven centres from seven European cities took part in this initial meeting.\n\nThat first international meeting in 1983 was the seed of what nowadays is Trans Europe Halles (TEH), the European-based network of cultural centres initiated by citizens and artists, that currently gathers together around eighty members and associated organisations in almost thirty countries.\n\nTEH members are cultural centres emerged from civil society initiatives that have a multidisciplinary artistic policy encouraging interaction between art forms, with an emphasis on contemporary art. One of the common and most defining characteristics of these centres is that they are based on industrial or commercial buildings that have been adapted to artistic and cultural uses. Besides this, they are very diverse in terms of geographical location, size, inspirational backgrounds, funding models, artistic programmes, etc.\n\nTrans Europe Halles has currently almost 60 members and 17 associate organisations in 29 European countries. Most of Trans Europe Halles centres are established in buildings from industrial heritage, from factories and warehouses to dairies and army barracks.\nAll Trans Europe Halles centres are multidisciplinary, but their main artistic disciplines as well as their regular activities vary a lot.\nTo become a TEH Member, applicant cultural centres must fulfil the following membership criteria:\n\n\nTEH is run in a decentralised way; the member centres participate in the decision-making of the network during the network meetings. Each centre has one vote regardless of its size or economic status. The Executive Committee, selected biannually, is the governing and policy making body of the network.\n\nThe members of Trans Europe Halles meet twice a year during TEH Meetings. Decisions are made at the General Assembly of each meeting. During the Spring meeting a formal General Assembly is held with the approval of final accounts, election of Executive Committee and approval of the yearly budget. Each member has one vote regardless of the size of the centre.\n\nTrans Europe Halles’ Executive Committee is the governing and policy making body of the network. It consists of a minimum of five and a maximum of eight people and is elected for two years at a time.\n\nThe Trans Europe Halles Coordination Office is located at the cultural centre Mejeriet in Lund, Sweden. With a broad and multi-lingual membership and an intense flow of information and ideas, the Trans Europe Halles Coordination Office is a crucial hub. The Coordination Office is responsible for internal and external communication of the network and publishes the monthly TEH Newsletter. The Coordination Office also supports project-planning and fundraising activities for the network and has become a focal point of expertise within the network.\n\nOrganised twice a year and hosted by a different member centre each time, the TEH meetings are a source of inspiration and a birthplace for new collaborations. It is also the opportunity for our members to meet and discover new countries and new cultural centres.\n\nThe activities and programme are all of interest of the participants, with a strong focus on capacity building. During three days, participants are invited to take part to workshops, seminars, lectures, and networking sessions. On the last day of each meeting, an outing is organised for the participants to get to know more about the local culture.\n\nThe network coordinates several bilateral and multilateral projects:\n\n\n\nTrans Europe Halles also publishes reports, handbooks and other types of media on subjects concerning culture in Europe.\n\nTrans Europe Halles' website \n"}
{"id": "5672534", "url": "https://en.wikipedia.org/wiki?curid=5672534", "title": "Turboexpander", "text": "Turboexpander\n\nA turboexpander, also referred to as a turbo-expander or an expansion turbine, is a centrifugal or axial-flow turbine, through which a high-pressure gas is expanded to produce work that is often used to drive a compressor or generator.\n\nBecause work is extracted from the expanding high-pressure gas, the expansion is approximated by an isentropic process (i.e., a constant-entropy process), and the low-pressure exhaust gas from the turbine is at a very low temperature, −150 °C or less, depending upon the operating pressure and gas properties. Partial liquefaction of the expanded gas is not uncommon.\n\nTurboexpanders are very widely used as sources of refrigeration in industrial processes such as the extraction of ethane and natural-gas liquids (NGLs) from natural gas, the liquefaction of gases (such as oxygen, nitrogen, helium, argon and krypton) and other low-temperature processes.\n\nTurboexpanders currently in operation range in size from about 750 W to about 7.5 MW (1 hp to about 10,000 hp).\n\nAlthough turboexpanders are very commonly used in low-temperature processes, they are used in many other applications as well. This section discusses one of the low-temperature processes, as well as some of the other applications.\n\nRaw natural gas consists primarily of methane (CH), the shortest and lightest hydrocarbon molecule, as well as various amounts of heavier hydrocarbon gases such as ethane (CH), propane (CH), normal butane (\"n\"-CH), isobutane (\"i\"-CH), pentanes and even higher-molecular-mass hydrocarbons. The raw gas also contains various amounts of acid gases such as carbon dioxide (CO), hydrogen sulfide (HS) and mercaptans such as methanethiol (CHSH) and ethanethiol (CHSH).\n\nWhen processed into finished by-products (see Natural-gas processing), these heavier hydrocarbons are collectively referred to as NGL (natural-gas liquids). The extraction of the NGL often involves a turboexpander and a low-temperature distillation column (called a \"demethanizer\") as shown in the figure. The inlet gas to the demethanizer is first cooled to about −51 °C in a heat exchanger (referred to as a \"cold box\"), which partially condenses the inlet gas. The resultant gas–liquid mixture is then separated into a gas stream and a liquid stream.\n\nThe liquid stream from the gas–liquid separator flows through a valve and undergoes a \"throttling expansion\" from an absolute pressure of 62 bar to 21 bar (6.2 to 2.1 MPa), which is an isenthalpic process (i.e., a constant-enthalpy process) that results in lowering the temperature of the stream from about −51 °C to about −81 °C as the stream enters the demethanizer.\n\nThe gas stream from the gas–liquid separator enters the turboexpander, where it undergoes an isentropic expansion from an absolute pressure of 62 bar to 21 bar (6.2 to 2.1 MPa) that lowers the gas stream temperature from about −51 °C to about −91 °C as it enters the demethanizer to serve as distillation reflux.\n\nLiquid from the top tray of the demethanizer (at about −90 °C) is routed through the cold box, where it is warmed to about 0 °C as it cools the inlet gas, and is then returned to the lower section of the demethanizer. Another liquid stream from the lower section of the demethanizer (at about 2 °C) is routed through the cold box and returned to the demethanizer at about 12 °C. In effect, the inlet gas provides the heat required to \"reboil\" the bottom of the demethanizer, and the turboexpander removes the heat required to provide reflux in the top of the demethanizer.\n\nThe overhead gas product from the demethanizer at about −90 °C is processed natural gas that is of suitable quality for distribution to end-use consumers by pipeline. It is routed through the cold box, where it is warmed as it cools the inlet gas. It is then compressed in the gas compressor driven by the turboexpander and further compressed in a second-stage gas compressor driven by an electric motor before entering the distribution pipeline.\n\nThe bottom product from the demethanizer is also warmed in the cold box, as it cools the inlet gas, before it leaves the system as NGL.\n\nThe figure depicts an electric power-generation system that uses a heat source, a cooling medium (air, water or other), a circulating working fluid and a turboexpander. The system can accommodate a wide variety of heat sources such as:\n\nThe circulating working fluid (usually an organic compound such as R-134a) is pumped to a high pressure and then vaporized in the evaporator by heat exchange with the available heat source. The resulting high-pressure vapor flows to the turboexpander, where it undergoes an isentropic expansion and exits as a vapor–liquid mixture, which is then condensed into a liquid by heat exchange with the available cooling medium. The condensed liquid is pumped back to the evaporator to complete the cycle.\n\nThe system in the figure implements a Rankine cycle as it is used in fossil-fuel power plants, where water is the working fluid and the heat source is derived from the combustion of natural gas, fuel oil or coal used to generate high-pressure steam. The high-pressure steam then undergoes an isentropic expansion in a conventional steam turbine. The steam turbine exhaust steam is next condensed into liquid water, which is then pumped back to steam generator to complete the cycle.\n\nWhen an organic working fluid such as R-134a is used in the Rankine cycle, the cycle is sometimes referred to as an organic Rankine cycle (ORC).\n\nA refrigeration system utilizes a compressor, a turboexpander and an electric motor.\n\nDepending on the operating conditions, the turboexpander reduces the load on the electric motor by 6–15% compared to a conventional vapor-compression refrigeration system that uses a \"throttling expansion\" valve rather than a turboexpander. Basically, this can be seen as a form of turbo compounding.\n\nThe system employs a high-pressure refrigerant (i.e., one with a low normal boiling point) such as:\n\nAs shown in the figure, refrigerant vapor is compressed to a higher pressure, resulting in a higher temperature as well. The hot, compressed vapor is then condensed into a liquid. The condenser is where heat is expelled from the circulating refrigerant and is carried away by whatever cooling medium is used in the condenser (air, water, etc.).\n\nThe refrigerant liquid flows through the turboexpander, where it is vaporized, and the vapor undergoes an isentropic expansion, which results in a low-temperature mixture of vapor and liquid. The vapor–liquid mixture is then routed through the evaporator, where it is vaporized by heat absorbed from the space being cooled. The vaporized refrigerant flows to the compressor inlet to complete the cycle.\n\nThe combustion flue gas from the catalyst regenerator of a fluid catalytic cracker is at a temperature of about 715 °C and at a pressure of about 2.4 barg (240 kPa gauge). Its gaseous components are mostly carbon monoxide (CO), carbon dioxide (CO) and nitrogen (N). Although the flue gas has been through two stages of cyclones (located within the regenerator) to remove entrained catalyst fines, it still contains some residual catalyst fines.\n\nThe figure depicts how power is recovered and utilized by routing the regenerator flue gas through a turboexpander. After the flue gas exits the regenerator, it is routed through a secondary catalyst separator containing \"swirl tubes\" designed to remove 70–90% of the residual catalyst fines. This is required to prevent erosion damage to the turboexpander.\n\nAs shown in the figure, expansion of the flue gas through a turboexpander provides sufficient power to drive the regenerator's combustion air compressor. The electrical motor-generator in the power-recovery system can consume or produce electrical power. If the expansion of the flue gas does not provide enough power to drive the air compressor, the electric motor-generator provides the needed additional power. If the flue gas expansion provides more power than needed to drive the air compressor, then the electric motor-generator converts the excess power into electric power and exports it to the refinery's electrical system. The steam turbine is used to drive the regenerator's combustion air compressor during start-ups of the fluid catalytic cracker until there is sufficient combustion flue gas to take over that task.\n\nThe expanded flue gas is then routed through a steam-generating boiler (referred to as a \"CO boiler\"), where the carbon monoxide in the flue gas is burned as fuel to provide steam for use in the refinery.\n\nThe flue gas from the CO boiler is processed through an electrostatic precipitator (ESP) to remove residual particulate matter. The ESP removes particulates in the size range of 2 to 20 micrometers from the flue gas.\n\nThe possible use of an expansion machine for isentropically creating low temperatures was suggested by Carl Wilhelm Siemens (Siemens cycle), a German engineer in 1857. About three decades later, in 1885, Ernest Solvay of Belgium attempted to use a reciprocating expander machine, but could not attain any temperatures lower than −98 °C because of problems with lubrication of the machine at such temperatures.\n\nIn 1902, Georges Claude, a French engineer, successfully used a reciprocating expansion machine to liquefy air. He used a degreased, burnt leather packing as a piston seal without any lubrication. With an air pressure of only 40 bar (4 MPa), Claude achieved an almost isentropic expansion resulting in a lower temperature than had before been possible.\n\nThe first turboexpanders seem to have been designed in about 1934 or 1935 by Guido Zerkowitz, an Italian engineer working for the German firm of Linde AG.\n\nIn 1939, the Russian physicist Pyotr Kapitsa perfected the design of centrifugal turboexpanders. His first practical prototype was made of Monel metal, had an outside diameter of only 8 cm (3.1 in), operated at 40,000 revolutions per minute and expanded 1,000 cubic metres of air per hour. It used a water pump as a brake and had an efficiency of 79–83%. Most turboexpanders in industrial use since then have been based on Kapitsa's design, and centrifugal turboexpanders have taken over almost 100% of the industrial gas liquefaction and low-temperature process requirements. The availability of liquid oxygen revolutionized the production of steel using the basic oxygen steelmaking process.\n\nIn 1978, Pyotr Kapitsa was awarded a Nobel physics prize for his body of work in the area of low-temperature physics.\n\nIn 1983, San Diego Gas and Electric was among the first to install a turboexpander in a natural-gas letdown station for energy recovery.\n\nTurboexpanders can be classified by loading device or bearings.\n\nThree main loading devices used in turboexpanders are centrifugal compressors, electrical generators or hydraulic brakes. With centrifugal compressors and electrical generators the shaft power from the turboexpander is recouped either to recompress the process gas or to generate electrical energy, lowering utility bills.\n\nHydraulic brakes are used when the turboexpander is very small and harvesting the shaft power is not economically justifiable.\n\nBearings used are either oil bearings or magnetic bearings.\n\nOne should also notice the new Quasiturbine technology , which is a positive displacement rotary turbine type.\n\n"}
{"id": "1286331", "url": "https://en.wikipedia.org/wiki?curid=1286331", "title": "U-matic", "text": "U-matic\n\nU-matic is an analogue recording videocassette format first shown by Sony in prototype in October 1969, and introduced to the market in September 1971. It was among the first video formats to contain the videotape inside a cassette, as opposed to the various reel-to-reel or open-reel formats of the time. Unlike most other cassette-based tape formats, the supply and take-up reels in the cassette turn in opposite directions during playback, fast-forward, and rewind: one reel would run clockwise while the other would run counter-clockwise. A locking mechanism integral to each cassette case secures the tape hubs during transportation to keep the tape wound tightly on the hubs. Once the cassette is taken off the case, the hubs are free to spin. A spring-loaded tape cover door protects the tape from damage; when the cassette is inserted into the VCR, the door is released and is opened, enabling the VCR mechanism to spool the tape around the spinning video drum. Accidental recording is prevented by the absence of a red plastic button fitted to a hole on the bottom surface of the tape; removal of the button disabled recording.\n\nAs part of its development, in March 1970, Sony, Matsushita Electric Industrial Co. (Panasonic), Victor Co. of Japan (JVC), and five non-Japanese companies reached agreement on unified standards.\n\nThe videotape was wide, so the format is often known as \"three-quarter-inch\" or simply \"three-quarter\", compared to open reel videotape formats in use during the decade, such as type C videotape and quadruplex videotape.\n\nThe first generation of U-matic VCRs were large devices, approximately wide, deep, and high, requiring special shelving, and had mechanical controls limited to Record, Play, Rewind, Fast-Forward, Stop and Pause (with muted video on early models). Later models sported improvements such as chassis sized for EIA 19-inch rack mounting, with sliding rack rails for compressed storage in broadcast environments, solenoid control mechanics, jog-shuttle knob, remote controls, Vertical Interval Time Code (VITC), longitudinal time code, internal cuts-only editing controls, \"Slo-Mo\" slow-motion playback, and Dolby audio noise reduction.\n\nU-matic was named after the shape of the tape path when it was threaded around the helical scan video head drum, which resembled the letter U. Betamax used a similar type of \"B-load\" as well. Recording time was limited to one hour.\n\nAt the 1971 introduction of U-Matic, Sony originally intended it to be a videocassette format oriented at the consumer market. This proved to be something of a failure, because of the high manufacturing cost and resulting retail price of the format's first VCRs. But the cost was affordable enough for industrial and institutional customers, where the format was very successful for such applications as business communication and educational television. As a result, Sony shifted U-Matic's marketing to the industrial, professional, and educational sectors.\n\nU-Matic saw even more success from the television broadcast industry in the mid-1970s, when a number of local TV stations and national TV networks used the format when its first portable model, the Sony VO-3800, was released in 1974. This model ushered in the era of ENG, or Electronic News Gathering, which eventually made obsolete the previous 16mm film cameras normally used for on-location television news gathering. Film required developing which took time, compared to the instantly available playback of videotape, making faster breaking news possible.\n\nU-matic is also available in a smaller cassette size, officially known as U-Matic S. Much like VHS-C, U-Matic S was developed as a more portable version of U-Matic, to be used in smaller-sized S-format recorders such as the aforementioned Sony VO-3800, as well as the later VO-4800, VO-6800, VO-8800, BVU-50, BVU-100 and BVU-150 models from Sony, among others from Sony, Panasonic, JVC and other manufacturers. To minimise weight and bulk in the field, portable recorders had an external AC power supply, or could be operated from rechargeable nickel-cadmium batteries.\n\nThe price point of the VO series was oriented toward educational, corporate and industrial fields, featured unbalanced audio connectors, and did not typically include SMPTE time code (although one or two companies offered after-market modification services to install longitudinal time code). The VO-3800 was largely metal, which made the unit heavy, but still technically portable. The VO-4800 had the same functionality as the VO-3800, but at a greatly reduced weight and size, by replacing many components with plastic. The VO-6800 added the improvement of a long, thin battery standard (\"candy bars\") that permitted storage of the batteries in a trouser pocket. Common model numbers for these batteries were NP-1, NP-1A and NP-1B. The VO-8800 was the last of the portable VO series to be produced by Sony, and featured solenoid-controlled transport.\n\nThe Sony BVU series added longitudinal and vertical interval SMPTE time code, balanced audio XLR connectors, and heavier-duty transport features. The BVU-50 enabled recording in the field but not playback, and the BVU-100 permitted both recording and playback in the field. Portable recorders were connected to the camera with a multi-conductor cable terminated with multi-pin connectors on each end. The cable carried bi-directional audio, video, synchronisation, record on/off control, and power. Early studio and all portable U-Matic VCRs had a drawer-type mechanism which required the tape to be inserted, followed by manual closure of the drawer (a \"top-loading\" mechanism). Later studio VCRs accepted the cassette from a port opening and the cassette was pulled into and seated in the transport (a \"front-loading\" mechanism).\n\nS-format tapes could be played back in older top-loading standard U-Matic decks with the aid of an adapter (the KCA-1 from Sony) which fitted around an S-sized tape; newer front-loading machines can accept S-format tapes directly, as the tapes have a slot on the underside that rides along a tab. U-Matic S tapes had a maximum recording time of 20 minutes, and large ones 1-hour, although some tape manufacturers such as 3M came out with 30-minute S-tapes and 75-minute large cassettes (and DuPont even managed 90-minute tapes) by using a thinner tape. It was the U-Matic S-format decks that ushered in the beginning of ENG, or Electronic News Gathering.\n\nSome U-Matic VCRs could be controlled by external video editing controllers, such as the cuts-only Sony RM-440 for linear video editing systems. Sony and other manufacturers such as Convergence, Calaway, and CMX Systems produced A/B roll systems, which permitted two or more VCRs to be controlled and synchronised for video dissolves and other motion effects, integration of the character generator, audio controllers and digital video effects (DVE).\n\nIn 1976, Sony introduced the semi backwards-compatible \"high-band\" Broadcast Video U-matic (BVU) format. The original U-matic format became known as \"low-band\". The BVU format had an improved colour recording system and lower noise levels. BVU gained immense popularity in ENG and location programme-making, spelling the end of 16 mm film in everyday production. By the early 1990s, Sony's Betacam SP format had all but replaced BVU outside of corporate and budget programme making. With BVU 800 series, Sony made a final improvement to BVU, by further improving the recording system and giving it the same \"SP\" suffix as Betacam. SP had a horizontal resolution of 330 lines. The BVU 800 series Y-FM carrier frequency was upped to 1.2 MHz giving it wider bandwidth. BVU 800 series also added Dolby audio noise reduction. Sony's BVU 900 series was the last U-matic VTR made by Sony. First-generation BVU-SP and Beta-SP recordings were hard to tell apart, but despite this the writing was on the wall for the U-matic family, due to intrinsic problems with the format.\n\nA recurring problem with the format was damage to the videotape caused by prolonged friction of the spinning video drum heads against a paused videotape. The drum would rub oxide off the tape or the tape would wrinkle; when the damaged tape was played back, a horizontal line of distorted visual image would ascend in the frame, and audio would drop out. Manufacturers attempted to minimise this issue with schemes in which the tape would loosen around the spinning head or the head would stop spinning after resting in pause mode for a pre-determined period of time.\n\nThe format video image also suffered from head-switching noise, a distortion of the image in which a section of video at the bottom of the video frame would be horizontally askew from the larger portion.\n\nThe format also had difficulty reproducing the colour red, and red images would be noisier than other colours in the spectrum. For this reason, on-camera talent was discouraged from wearing red clothing that would call attention to the technical shortcoming.\n\nCopying video from one U-matic VCR to another compromised playback reliability, and levels of head-switching noise, chroma smearing, and chroma noise compounded with every generation. These issues motivated videotape editors and engineers to use work-arounds to minimise this degradation. A time-base corrector (TBC) could be used to regenerate the sync tip portion of the video signal sent to the \"recording\" VCR, improving playback reliability. The \"dub\" cable, more formally called \"demodulated\" (or \"demod\" for short), was a multi-conductor cable that circumvented a portion of the video circuitry, minimising amplification noise.\n\nFor synchronisation to broadcast or post-production editing house genlock systems, U-Matic VCRs required a time base corrector (TBC). Some TBCs had a drop-out compensation (DOC) circuit which would hold lines of video in temporary digital memory to compensate for oxide drop-out or wrinkle flaws in the videotape, however the DOC circuits required several cables and expert calibration for use.\n\nU-matic tapes were also used for easy transport of filmed scenes for dailies in the days before VHS, DVD, and portable hard drives. Several movies have surviving copies in this form. The first rough cut of \"Apocalypse Now\", for example (the raw version of what became \"Apocalypse Now Redux\"), survived on three U-Matic cassettes.\n\nAudio quality was compromised due the use of longitudinal audio tape heads in combination with slow tape speed. Sony eventually implemented Dolby noise reduction circuitry (using Dolby C) to improve audio fidelity.\n\nThe 2012 film \"No\", set in 1980s Chile, used U-matic tape for filming.\n\nU-matic was also used for the storage of digital audio data. Most digital audio recordings from the 1980s were recorded on U-matic tape via a Sony PCM-1600, -1610, or -1630 PCM adaptor. These devices accepted stereo analogue audio, digitised it, and generated \"pseudo video\" from the bits, storing 48 bits—three 16-bit samples—as bright and dark regions along each scan line. (On a monitor the \"video\" looked like vibrating checkerboard patterns.) This could be recorded on a U-matic recorder. This was the first system used for mastering audio compact discs in the early 1980s. The famous compact disc 44.1 kHz sampling rate was based on a best-fit calculation for NTSC and PAL's video's horizontal line period and rate and U-matic's luminance bandwidth. On playback the PCM adapter converted the light and dark regions back to bits. Glass masters for audio CDs were made via laser from the PCM-1600's digital output to a photoresist- or dye-polymer-coated disc. This method was common until the mid-1990s.\n\nU-matic is no longer used as a mainstream television production format, but it has found lasting appeal as a cheap, well specified, and hard-wearing format. The format permitted many broadcast and non-broadcast institutions to produce television programming on an accessible budget, spawning programming distribution, classroom playback, etc. At its peak popularity, U-matic recording and playback equipment was manufactured by Sony, Panasonic, JVC and Sharp, with many spin-off product manufacturers, such as video edit controllers, time base correctors, video production furniture, playback monitors and carts, etc.\n\nMany television facilities the world over still have a U-matic recorder for archive playback of material recorded in the 1980s. For example, the Library of Congress facility in Culpeper, Virginia holds thousands of titles on U-matic video as a means of providing access copies and proof for copyright deposit of old television broadcasts and films.\n\nMore than four decades after it was introduced, the format is still used for the menial tasks of the industry, being more highly specialised and suited to the needs of production staff than the domestic VHS, although as time passes it has been replaced at the bottom of the tree of tape-based production formats by Betacam and Betacam SP as these in turn are replaced by Digital Betacam and HDCAM.\n\n\n"}
{"id": "49994569", "url": "https://en.wikipedia.org/wiki?curid=49994569", "title": "Vehicle-to-device", "text": "Vehicle-to-device\n\nVehicle-to-device (V2D) communication is a particular type of vehicular communication system that consists in the exchange of information between a vehicle and any electronic device that may be connected to the vehicle itself. \n\nThe ever-increasing tendency of developing mobile applications for our everyday use has ultimately entered also the automotive sector. Vehicle connectivity with mobile apps have the great potential to offer a better driving experience, by providing information regarding the surrounding vehicles and infrastructure and making the interaction between the car and its driver much simpler. The fact that apps may significantly improve driving safety has attracted the attention of car users and caused a rise in the number of new apps developed specifically for the car industry. This trend has such a great influence that now manufacturers are beginning to design cars taking care of their interaction with mobile phones. For example, starting from 2017 Volvo is going to sell keyless cars, thanks to an app that makes it possible to open and start the vehicle remotely.\nAnother sector that could coherently benefit from this technology is car sharing.\n"}
{"id": "15399635", "url": "https://en.wikipedia.org/wiki?curid=15399635", "title": "Water timer", "text": "Water timer\n\nA water timer is an electromechanical device that, when placed on a water line, increases or decreases the water flow through the use of an embedded (solenoid) valve. It is used in conjunction with irrigation sprinklers to form an automated or non-automated sprinkler system, capable of administering precise amounts of water, at a regular basis.\n\nAn alternative to a water timer to administer precise amounts of water at a regular basis, is to precisely calculate the needed waterflow and arrange a system composed by a Time switch and an electric pump that. The electric pump is to be arranged to be turned on and off at regular intervals by the time switch to administer (fairly) precise and regular water quantities. This approach, used frequently in DIY watering systems, allows financial savings as the water timer can be discarded. For high-end applications however, water timers sometimes allow more flexibility and more options.\n\n"}
