{"id": "2644968", "url": "https://en.wikipedia.org/wiki?curid=2644968", "title": "Acidogenesis", "text": "Acidogenesis\n\nAcidogenesis is the second stage in the four stages of anaerobic digestion:\nAnaerobic digestion is a complex biochemical process of biologically mediated reactions by a consortium of microorganisms to convert organic compounds into methane and carbon dioxide. It is a stabilization process, reducing odor, pathogens, and mass reduction.\n\nHydrolytic bacteria form a variety of reduced end-products from the fermentation of a given substrate. One fundamental question that arises concerns the metabolic features that control carbon and electron flow to a given reduced end-product during pure culture and mixed methanogenic cultures of hydrolytic bacteria. \"Thermoanaerobium brockii\" is a representative thermophilic, hydrolytic bacterium, which ferments glucose, via the Embden–Meyerhof Parnas Pathway. \"T. brockii\" is an atypical hetero-lactic acid bacterium because it forms molecular hydrogen (H), in addition to lactic acid and ethanol. The reduced end-products of glucose fermentation are enzymatically formed from pyruvate, via the following mechanisms: lactate by fructose 1-6 all-phosphate (F6P) activated lactate dehydrogenase; H2 by pyruvate ferredoxin oxidoreductase and hydrogenase; and ethanol via NADH- and NADPH-linked alcohol dehydrogenase.\n\nBy its side, the acidogenic activity was found in the early 20th century, but it was not until the mid-1960s that the engineering of phases separation was assumed in order to improve the stability and waste digesters treatment. In this phase, complex molecules (carbohydrates, lipids, and proteins) are depolymerized into soluble compounds by hydrolytic enzymes (cellulases, hemicellulases, amylases, lipases and proteases). The hydrolyzed compounds are fermented into volatile fatty acids (acetate, propionate, butyrate, and lactate), neutral compounds (ethanol, methanol), ammonia, hydrogen and carbon dioxide.\n\nAcetogenesis is one of the main reactions of this stage, in this, the intermediary metabolites produced are metabolized to acetate, hydrogen and carbonic gas by the three main groups of bacteria: \n\nFor the acetic acid production are considered three kind of bacteria:\nWinter y Wolfe, in 1979, demonstrated that \"A. wodii\" in syntrophic association with \"Methanosarcina\" produce methane and carbon dioxide from fructose, instead of three molecules of acetate. \"Moorella thermoacetica\" and \"Clostridium formiaceticum\" are able to reduce the carbonic gas to acetate, but they do not have hydrogenases which inhabilite the hydrogen use, so they can produce three molecules of acetate from fructose. Acetic acid is equally a co-metabolite of the organic substrates fermentation (sugars, glycerol, lactic acid, etc.) by diverse groups of microorganisms which produce different acids: *propionic bacteria (propionate + acetate);\n"}
{"id": "22262111", "url": "https://en.wikipedia.org/wiki?curid=22262111", "title": "Area 2 (Nevada National Security Site)", "text": "Area 2 (Nevada National Security Site)\n\nArea 2 is a division of the Nevada Test Site in the Mojave Desert. The area is located 18 miles south-east of Area 51.\n\nArea 2 was the site of 144 tests comprising 169 detonations. Shot \"Gabbs\", intended for 1993, was abandoned in place.\n"}
{"id": "1355398", "url": "https://en.wikipedia.org/wiki?curid=1355398", "title": "Blackboard system", "text": "Blackboard system\n\nA blackboard system is an artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the \"blackboard\", is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution. Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state. In this way, the specialists work together to solve the problem. The blackboard model was originally designed as a way to handle complex, ill-defined problems, where the solution is the sum of its parts.\n\nThe following scenario provides a simple metaphor that gives some insight into how a blackboard functions:\nA group of specialists are seated in a room with a large blackboard. They work as a team to brainstorm a solution to a problem, using the blackboard as the workplace for cooperatively developing the solution.\n\nThe session begins when the problem specifications are written onto the blackboard. The specialists all watch the blackboard, looking for an opportunity to apply their expertise to the developing solution. When someone writes something on the blackboard that allows another specialist to apply their expertise, the second specialist records their contribution on the blackboard, hopefully enabling other specialists to then apply their expertise. This process of adding contributions to the blackboard continues until the problem has been solved.\n\nA blackboard-system application consists of three major components\n\n\nFamous examples of early academic blackboard systems are the Hearsay II speech recognition system and Douglas Hofstadter's Copycat and Numbo projects.\n\nMore recent examples include deployed real-world applications, such as the PLAN component of the Mission Control System for RADARSAT-1, an Earth observation satellite developed by Canada to monitor environmental changes and Earth's natural resources.\n\nGTXImage CAD software by GTX Corporation was developed in the early 1990s using a set of rulebases and neural networks as specialists operating on a blackboard system.\n\nAdobe Acrobat Capture (now discontinued) used a Blackboard system to decompose and recognize image pages to understand the objects, text, and fonts on the page. This function is currently built into the retail version of Adobe Acrobat as \"OCR Text Recognition\". Details of a similar OCR blackboard for Farsi text are in the public domain.\n\nBlackboard systems are used routinely in many military C4ISTAR systems for detecting and tracking objects.\n\nBlackboard systems were popular before the AI Winter and, along with most symbolic AI models, fell out of fashion during that period. Along with other models it was realised that initial successes on toy problems did not scale well to real problems on the available computers of the time. Most problems using blackboards are inherently NP-hard, so resist tractable solution by any algorithm in the large size limit. During the same period, statistical pattern recognition became dominant, most notably via simple Hidden Markov Models outperforming symbolic approaches such as Hearsay-II in the domain of speech recognition.\n\nBlackboard-like systems have been constructed within modern Bayesian machine learning settings, using agents to add and remove Bayesian network nodes. In these 'Bayesian Blackboard' systems, the heuristics can acquire more rigorous probabilistic meanings as proposal and acceptances in Metropolis Hastings sampling though the space of possible structures. Conversely, using these mappings, existing Metropolis-Hastings samplers over structural spaces may now thus be viewed as forms of blackboard systems even when not named as such by the authors. Such samplers are commonly found in musical transcription algorithms for example.\n\nBlackboard systems have also been used to build large-scale intelligent systems for the annotation of media content, automating parts of traditional social science research. In this domain, the problem of integrating various AI algorithms into a single intelligent system arises spontaneously, with blackboards providing a way for a collection of distributed, modular natural language processing algorithms to each annotate the data in a central space, without needing to coordinate their behavior.\n\n\n\n"}
{"id": "6921492", "url": "https://en.wikipedia.org/wiki?curid=6921492", "title": "Brush hog", "text": "Brush hog\n\nA brush hog or \"bush hog\" is a type of rotary mower. Typically these mowers attach to the back of a farm tractor using the three-point hitch and are driven via the Power take-off (PTO). It has blades that are not rigidly attached to the drive like a lawnmower blade, but are on hinges so if the blade hits a rock or stump, it bounces backward and inward, and then centrifugal force makes it go outwards again.\n\nThe rotary blades are not sharpened in the same way as a conventional mower blade. They are usually quite dull so they whack through dense plant growth, whereas a sharp blade often gets stuck or slowed down. The blades are very heavy, up to an inch thick, so the centrifugal force pulling out is stronger than the forces of the vegetation bouncing in. They are made of heat treated high carbon steel that can withstand strikes with hard objects such as rocks and stones.\n\nTwo archetypes of this type of mower are the Bush Hog which is made by Bush Hog, Inc. Selma, Alabama and the Flex-Wing by RhinoAg Gibson City, Illinois. The formal name for this type of implement is a rotary cutter or rotary mower, although it differs from mowers in that it does not cut with a sharp blade, but rather severs with an intentionally very dull wedge-like blade.\n\nSeveral other manufacturers make these types of mowers including: \nThere are two types of attachment for a rotary mower, either using the three-point hitch or via a draw bar. The height of the cut on models attached on the three-point hitch is adjusted using the hitch control lever. This adjusts the height of the front of mower. For more radical height adjustment the rear wheel can also be adjusted. Typically the mowers in this category range from mowing widths.\n\nA rotary mower that applies chemical while cutting was patented by Tom Burch of Boone, N.C and was later sold to Diamond Mowers Inc. of Sioux Falls, SD under the name Wet Blade.\n\nOn draw bar models the height of the cut is determined using hydraulic rams on the mower itself; this requires remote hydraulics on the tractor. These mowers in this category range from mowing width. Draw bar mowers which are able to mow larger widths up to are available. These are multi-spindle and hinged so that the side wings can move over varied terrain. Also these wings can be folded up into a vertical position to reduce width for transport on roadways.\n\nThe blades on a rotary cutter can have top speeds of over . Therefore the kinetic energy enables the blades to cut through saplings and small trees up to several inches/centimetres in diameter. Consequently, these mowers can be very dangerous and contact with the blades would cause severe bodily injury or death.\n\nThese cutters also pose a thrown object hazard. Objects such as rocks, stones and roadside debris can be thrown in excess of .\n\nLarger rotary cutters require more powerful tractors to effectively operate. Tractor dealers can recommend the right size brush hog for any of their tractors, or tractor and brush hog sizing charts are available on the Internet. Calculators used to estimate the time required to mow different size areas with different size cutters are also available and can be used to help decide which brush hog is the best investment for different situations.\n"}
{"id": "338985", "url": "https://en.wikipedia.org/wiki?curid=338985", "title": "Business performance management", "text": "Business performance management\n\nBusiness performance management is a set of performance management and analytic processes that enables the management of an organization's performance to achieve one or more pre-selected goals. Synonyms for \"business performance management\" include \"corporate performance management (CPM)\" and \"enterprise performance management\".\n\nBusiness performance management is contained within approaches to business process management.\n\nBusiness performance management has three main activities:\n\nAlthough presented here sequentially, typically all three activities will run concurrently, with interventions by managers affecting the choice of goals, the measurement information monitored, and the activities being undertaken by the organization.\n\nBecause business performance management activities in large organizations often involve the collection and reporting of large volumes of data, many software vendors, particularly those offering business intelligence tools, market products intended to assist in this process. As a result of this marketing effort, business performance management is often incorrectly understood as an activity that necessarily relies on software systems to work, and many definitions of business performance management explicitly suggest software as being a definitive component of the approach.\n\nThis interest in business performance management from the software community is sales-driven - \"The biggest growth area in operational BI analysis is in the area of business performance management.\"\n\nSince 1992, business performance management has been strongly influenced by the rise of the balanced scorecard framework. It is common for managers to use the balanced scorecard framework to clarify the goals of an organization, to identify how to track them, and to structure the mechanisms by which interventions will be triggered. These steps are the same as those that are found in BPM, and as a result, balanced scorecard is often used as the basis for business performance management activity with organizations.\n\n, owners have sought to drive strategy down and across their organizations, transform these strategies into actionable metrics, and use analytics to expose the cause-and-effect relationships that, if understood, could give insight into decision-making.\n\nReference to non-business performance management occurs in Sun Tzu's \"The Art of War\". Sun Tzu claims that to succeed in war, one should have full knowledge of one's own strengths and weaknesses as well as those of one's enemies. Lack of either set of knowledge might result in defeat. Parallels between the challenges in business and those of war include:\n\n\nPrior to the start of the Information Age in the late 20th century, businesses sometimes took the trouble to laboriously collect data from non-automated sources. As they lacked computing resources to properly analyze the data, they often made commercial decisions primarily on the basis of intuition.\n\nAs businesses started automating more and more systems, more and more data became available. However, collection often remained a challenge due to a lack of infrastructure for data exchange or due to incompatibilities between systems. Reports on the data gathered sometimes took months to generate. Such reports allowed informed long-term strategic decision-making. However, short-term tactical decision-making often continued to rely on intuition.\n\nIn 1989 Howard Dresner, a research analyst at Gartner, popularized \"business intelligence\" (BI) as an umbrella term to describe a set of concepts and methods to improve business decision-making by using fact-based support systems. Performance management builds on a foundation of BI, but marries it to the planning-and-control cycle of the enterprise with enterprise planning, consolidation, and modeling capabilities.\n\nIncreasing standards, automation, and technologies have led to vast amounts of data becoming available. Data warehouse technologies have allowed the building of repositories to store this data. Improved ETL and enterprise application integration tools have increased the timely collecting of data. OLAP reporting technologies have allowed faster generation of new reports which analyze the data. , business intelligence has become the art of sieving through large amounts of data, extracting useful information and turning that information into actionable knowledge.\n\nBusiness performance management consists of a set of management and analytic processes, supported by technology, that enable businesses to define strategic goals and then measure and manage performance against those goals. Core business performance management processes include financial planning, operational planning, business modeling, consolidation and reporting, analysis, and monitoring of key performance indicators linked to strategy.\n\nBusiness performance management involves consolidation of data from various sources, querying, and analysis of the data, and putting the results into practice.\n\nVarious frameworks for implementing business performance management exist. The discipline gives companies a top-down framework by which to align planning and execution, strategy and tactics, and business-unit and enterprise objectives. Reactions may include the Six Sigma strategy, balanced scorecard, activity-based costing (ABC), Objectives and Key Results (OKR), Total Quality Management, economic value-add, integrated strategic measurement and Theory of Constraints.\n\nSome of the areas from which bank management may gain knowledge by using business performance management include:\n\n\nThough the above list describes what a bank might monitor, it could refer to a telephone company or to a similar service-sector company.\n\nItems of generic importance include:\n\n\nBusiness performance management integrates the company's processes with CRM or ERP. Companies should become better able to gauge customer satisfaction, control customer trends and influence shareholder value.\n\nPeople working in business intelligence have developed tools that ease the work of business performance management, especially when the business-intelligence task involves gathering and analyzing large amounts of unstructured data.\n\nTool categories commonly used for business performance management include:\n\n\n\nQuestions asked when implementing a business performance management program include:\n\n\n\n\n"}
{"id": "49819095", "url": "https://en.wikipedia.org/wiki?curid=49819095", "title": "Cascade impactor", "text": "Cascade impactor\n\nA cascade impactor measures the reach range of a particulate substance as it moves through an opening with the use of aerosol. Cascade impactors are strictly measurement-related devices. In addition to measuring the range of substances moved through an opening by aerosol, the impactor can also be used to determine the particle size of the distributed substance. A cascade impactor collects its samples in a graduated manner. This allows the user to identify the sizes of the substance particles as the particles are distributed from the propellant aerosol source.When the aerosol substance is distributed into the cascade impactor, the substance enters a series of discs designed to collect solids and different particulate matter. The substance is thus collected as it passes through the disc series. Each disc is set in sequence with both the prior and the previous disc. The size of the discs is graduated as well, to properly determine the size of the particulate matter at each stage of the impactor.\n\nAn impactor is a device that classifies particles present in a sample of air or gas into known size ranges. It does this by drawing the air sample through a cascade of progressively finer nozzles. The air jets from these nozzles impact on plane sampling surfaces and each stage collects finer particles than its predecessor. The samples may be analysed under the microscope or by any method of chemical analysis that may be suitable for obtaining the mass of material of interest on each stage, e.g. atomic absorption or gas chromatography-mass spectrometry (GC/MS).\n\nAll cascade impactors have certain design features in common. By using suitable pumps, jet dimensions can be decreased to the point where the velocity is close to that of sound, enabling particles down to about 0.25 µm to be impacted.\n\n\n"}
{"id": "16473495", "url": "https://en.wikipedia.org/wiki?curid=16473495", "title": "Cement-mortar lined ductile iron pipe", "text": "Cement-mortar lined ductile iron pipe\n\nCement-mortar lined ductile iron pipe is a ductile iron pipe with cement lining on the inside surface, and is commonly used for water distribution.\n\nCement-mortar lined ductile iron pipe is governed by standards set forth by DIPRA (Ductile Iron Pipe Research Association), and was first used in 1922 in Charleston, South Carolina.\n\nDuctile Iron is commonly used in place of cast iron pipe for fluid distribution systems, the idea of lining the formerly cast iron and currently ductile iron was put into practice for the first time in Charleston, South Carolina in 1922. The purpose of installing a cement/mortar lining to the interior wall of the pipe is to reduce the process of tuberculation inside the pipe network. The cement/mortar lining provides an area of high pH near the pipe wall and provides a barrier between the water and the pipe, reducing its susceptibility to corrosion.\n"}
{"id": "56907265", "url": "https://en.wikipedia.org/wiki?curid=56907265", "title": "Clarke's equation", "text": "Clarke's equation\n\nIn combustion, Clarke's equation is a third-order nonlinear partial differential equation, first derived by John Frederick Clarke in 1978. The equation describes the thermal explosion process, including both effects of constant-volume and constant-pressure processes, as well as the effects of adiabatic and isothermal sound speeds. The equation reads as\n\nwhere formula_2 is the non-dimensional temperature perturbation and formula_3 is the specific heat ratio. The term formula_4 describes the explosion at constant pressure and the term formula_5 describes the explosion at constant volume. Similarly, the term formula_6 describes the wave propagation at adiabatic sound speed and the term formula_7 describes the wave propagation at isothermal sound speed. Molecular transports are neglected in the derivation.\n\n"}
{"id": "1698597", "url": "https://en.wikipedia.org/wiki?curid=1698597", "title": "Composite monitor", "text": "Composite monitor\n\nA composite monitor is any analog video display that receives input in the form of an analog composite video signal to a defined specification. A composite video signal encodes all information on a single conductor; a composite cable has a single live conductor plus earth. Other equipment with display functionality includes monitors with more advanced interfaces and connectors giving a better picture, including analog VGA, and digital DVI, HDMI, and DisplayPort; and television (TV) receivers which are self-contained, receiving and displaying video RF broadcasts received with an internal tuner. Video monitors are used for displaying computer output, closed-circuit television (e.g. security cameras) and other applications requiring a two-dimensional monochrome or colour image.\n\nComposite monitors usually use RCA jacks or BNC connectors for video input. Earlier equipment (1970s) often used UHF connectors. Typically simple composite monitors give a picture inferior to other interfaces.\n\nIn principle a monitor can have one or several of multiple types of input, including composite—in addition to composite monitors as such, many monitors accept composite input among other standards. In practice computer monitors ceased to support composite input as other interfaces became predominant.\n\nA composite monitor must have a two-dimensional approximately flat display device with circuitry to accept a composite signal with picture and synchronisation information, process it into monochrome chrominance and luminance, or the red, green, and blue of RGB, plus synchronisation pulses, and display it on a screen, which was predominantly a CRT until the 21st century, and then a thin panel using LCD or other technology.\n\nA critical factor in the quality of this display is the type of encoding used in the TV camera to combine the signal together and the decoding used in the monitor to separate the signals back to RGB for display. Composite monitors can be very high quality, with professional broadcast reference displays costing US$10k-$15k as of 2000. Comb filters are frequently used to improve the quality of a composite monitor, and devices using Faroudja decoders are frequently considered the pinnacle of composite displays.\n\nOriginally, these monitors were used for commercial studios. Composite video first saw home use for dubbing tapes on VCRs. Early computers, both commercial and amateur, mostly used teleprinters for output; simple home models might simply display an array of lights to be interpreted as binary information. Later the concept of the TV Typewriter was born, effectively the video monitor used for digital information; this was implemented as dedicated monitors and as interfaces to the television receivers present in many homes. Many computers incorporated a display. From the late 1970s stand-alone composite monitors came into use, including by the Apple II, Commodore VIC 20/64/128, Atari, the IBM PC with CGA card, some computers compatible with it, and other home and business computers of the 1980s. These computers had composite video outputs, and sometimes composite monitors bundled with the systems. Some computer companies separately sold their own composite monitors for use with their computers.\n\nDuring the same time period, home game consoles chose to stick with RF modulation since many people had color televisions without composite video inputs. However, in 1985, the NES was released and was the first game console to feature direct composite outputs. Although the redesigned NES (NES 2) lacked these outputs, the Super NES and nearly all consoles made since have included the direct composite outputs. From the fifth generation systems (such as the Sony PlayStation and the Nintendo 64) onward, many consoles used these outputs as the primary means of connecting to the television, requiring a separate adapter for use on televisions lacking composite inputs. As of today, some people still use stand-alone composite monitors with modern game consoles even with the advent of televisions with a tuner and composite inputs combined.\n\nWhen a composite monitor is not available, a device requiring one can use an RF modulator to encode a composite output onto a RF signal which can be received by an analog television, available in many homes until the digital switchover.\n\nDevices are available to convert from composite to other standards such as analog VGA or digital HDMI; such devices may be called upscalers or scan converters, although not all devices with these names will handle composite input. Price and quality vary. A converter with electronics is necessary; a simple cable will not do the job.\n\nWith a whole market full of all sorts of solutions to convert composite (or its related standard S-video) to all sorts of other video transmission standards, it has even offered opportunities to repurpose non-composite monitors for composite video input, in which composite monitors themselves have also been repurposed for other applications of their own.\n\n\nSome monitors used in video surveillance give a monochrome picture.\n\n\nOther video standards include\n\nMonitors sometimes support several standards. Absence of certain video inputs may require purchase of signal adapters to reuse electronics that are otherwise incompatible.\n\n"}
{"id": "16881372", "url": "https://en.wikipedia.org/wiki?curid=16881372", "title": "Delta Scientific Corporation", "text": "Delta Scientific Corporation\n\nDelta Scientific Corporation also known just as Delta Scientific is a company based in Palmdale, California, United States. As a worldwide corporation, Delta Scientific Corp. has been developing and making vehicle access control equipment since 1974. In addition to the Palmdale headquarters, they have offices in Lorton, Virginia and Great Britain.\n\nDelta Scientific is the leading manufacturer of vehicle access control equipment. including guard booths, parking control equipment, and high security vehicle barricade systems. Delta has supplied counter-terrorist barricade systems to over 160 U.S. embassies and consulates in 130-plus countries as well as those of the United Kingdom and other nations. In the United States, Delta has products at over 110 Federal buildings, including courthouses and Federal Bureau of Investigation locations. Many U.S. nuclear power plants are also protected by Delta's vehicle barricade systems.\n\nDelta Scientific products are also located at the U.S. Capitol and other Washington, D.C. landmark buildings, various high-rises throughout the world, along the streets of New York City, some corporate facilities, the Parliament of Singapore, power and water infrastructure locales, some European castles and some private homes of celebrities and other people in Beverly Hills, California and other upscale communities.\n\n"}
{"id": "25879925", "url": "https://en.wikipedia.org/wiki?curid=25879925", "title": "ESR meter", "text": "ESR meter\n\nAn ESR meter is a two-terminal electronic measuring instrument designed and used primarily to measure the equivalent series resistance (ESR) of real capacitors; usually without the need to disconnect the capacitor from the circuit it is connected to. Other types of meters used for routine servicing, including normal capacitance meters, cannot be used to measure a capacitor's ESR, although combined meters are available which measure both ESR and out-of-circuit capacitance. A standard (DC) milliohmmeter or multimeter cannot be used to measure ESR, because a steady direct current cannot be passed through the capacitor.\nMost ESR meters can also be used to measure non-inductive low-value resistances, whether or not associated with a capacitor; this leads to a number of additional applications described below.\n\nAluminium electrolytic capacitors have a relatively high ESR that increases with age, heat, and ripple current; this can cause the equipment using them to malfunction. In older equipment, this tended to cause hum and degraded operation; modern equipment, in particular switch-mode power supplies, is very sensitive to ESR, and a capacitor with high ESR can cause equipment to malfunction or cause permanent damage requiring repair, typically by causing power supply voltages to become excessively high. Electrolytic capacitors are, nevertheless, very often used because they are inexpensive and have a very high capacitance per unit volume or weight; typically, these capacitors have capacitance from about one microfarad to tens of thousands of microfarads.\n\nCapacitors with faults leading to high ESR often overheat and thereafter bulge and leak as the electrolyte chemicals decompose into gases, making them somewhat easy to identify visually; however, capacitors that appear visually perfect may still have high ESR, detectable only by measurement.\n\nPrecise measurement of ESR is rarely necessary, and any usable meter is adequate for troubleshooting. When precision is required, measurements must be taken under appropriately specified conditions, because ESR varies with frequency, applied voltage, and temperature. A general-purpose ESR meter operating with a fixed frequency and waveform will usually be unsuitable for precise laboratory measurements.\n\nMeasuring ESR can be done by applying an alternating voltage at a frequency at which the capacitor's reactance is negligible, in a voltage divider configuration.\nIt is easy to check ESR well enough for troubleshooting by using an improvised ESR meter comprising a simple square-wave generator and oscilloscope, or a sinewave generator of a few tens of kilohertz and a AC voltmeter, using a known good capacitor for comparison, or by using a little mathematics.\n\nA professional ESR meter is more convenient for checking multiple capacitors in rapid succession. \nA standard measurement bridge, and many LCR and Q meters, can also measure ESR accurately, in addition to many other circuit parameters. The dedicated ESR meter is a relatively inexpensive special-purpose instrument of modest accuracy, used mainly to identify capacitors with unacceptably large ESR and sometimes to measure other low resistances; measurements of other parameters cannot be made.\n\nMost ESR meters work by discharging a real electrolytic capacitor (more or less equivalent to an ideal capacitor in series with an unwanted resistance, the ESR) and passing an electric current through it for a short time, too short for it to charge appreciably. This will produce a voltage across the device equal to the product of the current and the ESR plus a negligible contribution from a small charge in the capacitor; this voltage is measured and its value divided by the current (i.e., the ESR) shown in ohms or milliohms on a digital display or by the position of a pointer on a scale. The process is repeated tens or hundreds of thousands of times a second.\n\nAlternatively an alternating current at a frequency high enough that the capacitor's reactance is much less than the ESR can be used. Circuit parameters are usually chosen to give meaningful results for capacitance from about one microfarad up, a range that covers typical aluminium capacitors whose ESR tends to become unacceptably high.\n\nAn acceptable ESR value depends upon capacitance (larger capacitors usually have lower ESR), and may be read from a table of \"typical\" values, or compared with a new component. In principle, the capacitor manufacturer's upper limit specification for ESR can be looked up in a datasheet, but this is usually unnecessary. When a capacitor whose ESR is critical degrades, power dissipation as the ESR increases usually causes a rapid and large runaway increase, so go/no-go measurement is usually good enough as the ESR often rapidly moves from a clearly acceptable to a clearly unacceptable level; an ESR of over a few ohms (less for a large capacitor) is unacceptable.\n\nIn a practical circuit, the ESR will be much lower than any other resistance in parallel with the capacitor, so it is not necessary to disconnect the component, and an in-circuit measurement can be made. Practical ESR meters use a voltage too low to switch on any semiconductor junctions that may be present in the circuit; this might present a low \"on\" impedance that would interfere with measurements.\n\n\nAn ESR meter is more accurately described as a pulsed or high-frequency AC milliohmmeter (depending upon type), and it can be used to measure any low resistance. Depending upon the exact circuit used, it may also be used to measure the internal resistance of batteries (many batteries end their useful life largely due to increased internal resistance, rather than low EMF. However, an ESR meter with back-to-back protective diodes across its input cannot be used to measure batteries), contact resistance of switches, the resistance of sections of printed circuit (PCB) track, etc.\n\nWhile there are specialised instruments to detect short-circuits between adjacent PCB tracks, an ESR meter is useful because it can measure low resistances while injecting a voltage too low to confuse readings by switching on semiconductor junctions in the circuit. An ESR meter can be used to find short-circuits, even finding which of a group of capacitors or transistors connected in parallel by printed circuit tracks or wires is short-circuited. Many conventional ohmmeters and multimeters are not usable for very low resistances, and those that are often use too high a voltage, risking damage to the circuit being tested.\n\nTweezer probes are useful when test points are closely spaced, such as in equipment made with surface mount technology. The tweezer probes can be held in one hand, leaving the other hand free to steady or manipulate the equipment being tested.\n\nThe first major device to measure in-circuit ESR was based on Carl W. Vette's under the Creative Electronics brand. The Creative Electronics ESR meter was the primary device many used for the duration of the patent. The patent expired in 1998, when many other companies entered the market.\n\nAdditional patents extended the original work, including John G. Bachman's 2001 \n\n"}
{"id": "4938868", "url": "https://en.wikipedia.org/wiki?curid=4938868", "title": "Ecological goods and services", "text": "Ecological goods and services\n\nEcological goods and services (EG&S) are the benefits arising from the ecological functions of ecosystems. Such benefits accrue to all living organisms, including animals and plants, rather than to humans alone. However, there is a growing recognition of the importance to society that ecological goods and services provide for health, social, cultural, and economic needs. \n\nExamples of ecological goods include clean air, and abundant fresh water. Examples of ecological services include purification of air and water, maintenance of biodiversity, decomposition of wastes, soil and vegetation generation and renewal, pollination of crops and natural vegetation, groundwater recharge through wetlands, seed dispersal, greenhouse gas mitigation, and aesthetically pleasing landscapes. The products and processes of ecological goods and services are complex and occur over long periods of time. They are a sub-category of public goods.\n\nThe concern over ecological goods and services arises because we are losing them at an unsustainable rate, and therefore land use managers must devise a host of tools to encourage the provision of more ecological goods and services. Rural and suburban settings are especially important, as lands that are developed and converted from their natural state lose their ecological functions. Therefore, ecological goods and services provided by privately held lands become increasingly important.\n\nA market may be created wherein ecological goods and services are demanded by society and supplied by public and private landowners. Some believe that public lands alone are not adequate to supply this market, and that privately held lands are needed to close this gap. What has emerged is the notion that rural landowners who provide ecological goods and services to society through good stewardship practices on their land should be duly compensated. The main tool to accomplish this to date has been to pay farmers directly to set-aside portions of their land that would otherwise be in production. This exemplifies a shift in thinking from the \"polluter pays\" to the \"beneficiary pays\". \n\nFinancial incentives to landowners is one approach, but provision of EG&S can also be achieved through regulation, stewardship incentives under existing programs such Environmental Farm Plans, market-based instruments, and tax rebates.\n\nAccording to the Millennium Ecosystem Assessment project, biodiversity is a necessary underlying component of ecological goods and services. Biodiversity supports ecological goods and services such as biological control and genetic resources. However, biodiversity is also sometimes referred to as an actual ecological good or service which can be confusing. \n\nThe following policy tools can be used to ensure production of ecological goods and services: (in French)\n\n\n\n"}
{"id": "11395894", "url": "https://en.wikipedia.org/wiki?curid=11395894", "title": "Electric fireplace", "text": "Electric fireplace\n\nAn electric fireplace is an electric heater that mimics a fireplace burning coal, wood, or natural gas. Electric fireplaces are often placed in conventional fireplaces, which can then no longer be used for conventional fires. They plug into the wall, and can run on a \"flame only\" setting, or can be used as a heater, typically consuming 1.4-1.6 kW, that can heat a room.\n\nThe electric fire was invented in 1912 and became popular in the 1950s.\n\nTechniques for electrical \"flame effects\" have been around since at least 1981.\n\nCommercial electric fireplace techniques include the Optiflame, introduced in 1988 by Dimplex.\n\nDimplex claims to have produced the first electric fireplace with a \"realistic\" wood-burning flame effect in 1995. It is unclear what specific technique is being referred to, although it may be .\n\nIn 2008 Dimplex launched the Opti-myst effect which simulates both flames and smoke.\n\nIn 2013 Dimplex launched the Opti-V effect which combines realistic flickering flames with three dimensional LED logs that sporadically sparks and an audio element of crackling logs.\n\nAdvantages of electric fireplaces are that they:\n\nDisadvantages of electric fireplaces are that they:\n\n"}
{"id": "7118210", "url": "https://en.wikipedia.org/wiki?curid=7118210", "title": "Electrical engineering technology", "text": "Electrical engineering technology\n\nElectrical/Electronics engineering technology (EET) is an engineering technology field that implements and applies the principles of electrical engineering. Like electrical engineering, EET deals with the \"design, application, installation, manufacturing, operation or maintenance of electrical/electronic(s) systems.\" However, EET is a specialized discipline that has more focus on application, theory, and applied design, and implementation, while electrical engineering may focus more of a generalized emphasis on theory and conceptual design. Electrical/Electronic engineering technology is the largest branch of engineering technology and includes a diverse range of sub-disciplines, such as applied design, electronics, embedded systems, control systems, instrumentation, telecommunications, and power systems.\n\nThe Accreditation Board for Engineering and Technology (ABET) is the recognized organization for accrediting both undergraduate engineering and engineering technology programs in the United States.\n\nEET curricula can vary widely by institution type, degree type, program objective, and expected student outcome. Each year, however, ABET publishes a set of minimum criteria that a given EET program (either associate degree or bachelor's degree) must meet in order to maintain its ABET accreditation. These criteria may be classified as either general criteria, which apply to all ABET accredited programs, or as program criteria, which apply to discipline-specific criteria.\n\nAssociate degree programs emphasize the practical field knowledge that is needed to maintain or troubleshoot existing electrical/electronic systems or to build and test new design prototypes.\n\nDiscipline-specific program outcomes include the application of circuit analysis and design, analog and digital electronics, computer programming, associated software, and relevant engineering standards\n\nCoursework must be at a minimum algebra and trigonometry based.\n\nBachelor’s degree programs emphasize the analysis, design, and implementation of electrical/electronic systems. Some programs may focus on a specific sub-discipline, such as control systems or communications systems, while others may take a broader approach, introducing the student to several different sub-disciplines.\n\nMath to differential equations is a minimum requirement for ABET accredited bachelor’s level EET degrees. In addition, graduates must demonstrate an understanding of basic project management skills.\n\nThe United States Department of Commerce classifies the bachelor of science in electrical engineering technology (BSEET) as a STEM undergraduate engineering degree field.\n\nIn many states, recent graduates and students who are close to finishing an undergraduate BSEET degree are qualified to sit-in for the Fundamentals of Engineering exam while those BSEETs who have already gained at least four years’ post-college experience are qualified to sit-in for the Professional Engineer exam for their licensure in the United States. The importance of the licensing board requirements depend upon location, level of education, required years of experience, and the BSEETs sub-discipline are the passageways for becoming a licensed engineer. The knowledge obtained by a TAC/ABET accredited program is one pathway that may help students prepare for and pass the FE/PE exam. For example, in the United States and Canada, \"only a licensed engineer may seal engineering work for public and private clients\".\n\nGraduates of electrical/electronics engineering technology programs work in a wide range of career fields. Some examples include:\n\nElectrical/electronic engineering technicians may have a two-year associate degree and considered craftsman technicians. Eventually, with additional experience and certifications obtained then the craftsman technicians may advance to master craftsman technicians.\n\nElectrical/electronic engineering technologists are broad specialists, rather than central technicians. EETs have a bachelor's degree and are considered applied electrical or electronic engineers because they have electrical engineering concepts to use in their work. Entry-level jobs in electrical or electronics engineering generally require a bachelor's degree in electrical engineering, electronics engineering, or electrical engineering technology.\n\n\n"}
{"id": "23983698", "url": "https://en.wikipedia.org/wiki?curid=23983698", "title": "Enterprise Mashup Markup Language", "text": "Enterprise Mashup Markup Language\n\nEMML, or Enterprise Mashup Markup Language, is an XML markup language for creating enterprise mashups, which are software applications that consume and mash data from variety of sources, often performing logical or mathematical operations as well as presenting data. Mashed data produced by enterprise mashups are presented in graphical user interfaces as mashlets, widgets, or gadgets. EMML can also be considered a declarative mashup domain-specific language (DSL). A mashup DSL eliminates the need for complex, time-consuming, and repeatable procedural programming logic to create enterprise mashups. EMML also provides a declarative language for creating visual tools for enterprise mashups. \n\nThe primary benefits of EMML are mashup design portability and interoperability of mashup solutions. These benefits are expected to accelerate the adoption of enterprise mashups by creating transferable skills for software developers and reducing vendor lock-in. The introduction of EMML is expected to help accelerate the trend toward the integration of Web-based applications and service-oriented architecture (SOA) technologies. Bank of America was a high-profile early supporter of EMML. Other prominent early supporters included Hewlett-Packard, Capgemini, Adobe Systems, and Intel.\n\nRaj Krishnamurthy (chief architect at JackBe Corporation) and Deepak Alur (VP engineering at JackBe Corporation) started working on EMML in 2006. Their objective was to enable user-oriented and user-enabled mashups by creating what was then a new type of middleware called an Enterprise Mashup Platform. Raj Krishnamurthy became the chief language designer and implementer of EMML and also led the team to create an Eclipse-based EMML IDE called Mashup Studio. This work evolved into the EMML reference implementation that was donated to the Open Mashup Alliance. Raj Krishnamurthy continues to be one of the key contributors to EMML through the Open Mashup Alliance.\n\nEMML language provides a rich set of high-level mashup-domain vocabulary to consume and mash a variety of Web data-sources in flexible ways. EMML provides a uniform syntax to invoke heterogeneous service styles: REST, WSDL, RSS/ATOM, RDBMS, and POJO. The EMML language also provides ability to mix diverse data formats: XML, JSON, JDBC, JavaObjects, and primitive types. \n\nHigh-level EMML language features include:\n\nEMML is primarily a XML-based declarative language, but also provides ability to encode complex logic using embedded scripting engines. XPath is the expression language used in EMML.\n\ncodice_6 provides ability to invoke and consume a variety of data services. These data services may be REST, RSS/ATOM, or SOAP services. codice_6 also supports Web clipping by allowing HTML pages to be specified as service endpoints. codice_8, codice_9, codice_10, and codice_11 protocols are supported in codice_6. HTTP Header and cookie support is also available thus providing capability to consume a wide variety of REST/SOAP Web services. It is possible to use codice_6 with a proxy server.\n\nCode sample of passing attributes as parameters to a service:\n\nThe codice_14 statement filters the content of a variable using an XPath expression and places the result in a new variable.\n\nCode sample for filtering west-coast customers using region data-item:\n\nThe codice_15 statement sorts the content of a document-type variable or variable fragment based on key expressions and places the result in another variable.\n\nCode sample that sorts tickets based on created date and customer:\n\ncodice_16 provides the ability to group and aggregate data sets. Standard XPath aggregation operations can be used and there is an extension mechanism for adding user-defined functions. Nested Grouping of hierarchical data sets are also supported. There is a codice_17 clause to filter Group attributes.\n\nCode sample that groups books by genre and computes total copies for each genre:\n\ncodice_18 provides ability to combine various data sources including RSS/ATOM feeds, XML, JSON payload formats. The merge feature is similar to codice_19 functionality but merges hierarchical document structures.\n\nCode sample that merges Yahoo! News, Financial News, and Reuters feeds:\n\ncodice_20 provides ability to enrich the semantic meaning of source service data with microformat-like elements/attributes. These data annotations can be used by mashlets or gadgets to provide richer visual user interfaces.\n\nCode sample for annotating vendor payload with geo-coordinates:\n\nThe codice_21 statement defines how disparate, hierarchical data formats are joined and is comparable to inner joins for relational databases.\n\nCode sample where output variable contains a codice_22 element with a repeating set of codice_23 children, which are the repeating items. Each codice_23 contains a codice_25 child with data from the variable named movies and codice_26 and codice_27 children with data from the variable named reviews:\n\nEMML is a declarative language, but provides programmatic scripting extensions for performing complex mashup logic. JavaScript, JRuby, Groovy, POJO, XQuery scripting environments are supported. Data flows seamlessly between EMML and scripting environments. \n\nCode sample where JavaScript snippet is used to extract authentication token that is required for subsequent calls \"result\" variable that gets propagated to JavaScript environment:\n"}
{"id": "943923", "url": "https://en.wikipedia.org/wiki?curid=943923", "title": "Fault model", "text": "Fault model\n\nA fault model is an engineering model of something that could go wrong in the construction or operation of a piece of equipment. From the model, the designer or user can then predict the consequences of this particular fault. Fault models can be used in almost all branches of engineering.\n\nBasic fault models in digital circuits include:\n\nA fault model, falls under one of the following assumptions:\n\nThere are two main ways for collapsing fault sets into smaller sets.\n\nIt is possible that two or more faults produce same faulty behavior for all input patterns. These faults are called equivalent faults. Any single fault from the set of equivalent faults can represent the whole set. In this case, much less than k×n fault tests are required for a circuit with n signal line. removing equivalent faults from entire set of faults is called fault collapsing. fault collapsing significantly decreases the number of faults to check.\nIn the example diagram, red faults are equivalent to the faults that being pointed to with the arrows, so those red faults can be removed from the circuit. In this case, the fault collapse ratio is 12/20.\n\nFault F is called dominant to F' if all tests of F' detects F. In this case, F can be removed from the fault list. If F dominates F' and F' dominates F, then these two faults are equivalent.\n\nIn the example, a NAND gate has been shown, the set of all input values that can test output's SA0 is {00,01,10}. the set of all input values that can check first input's SA1 is {01}. In this case, output SA0 fault is dominant and can be removed from fault list.\n\nTwo faults are functionally equivalent if they produce identical faulty functions or we can say, two faults are functionally equivalent if we can not distinguish them at primary outputs (PO) with any input test vector.\n\nA fault model in an aerospace context is a set of structured information which helps users or systems to identify and isolate a problem that occurs on an engine, line-replaceable unit (LRU), or auxiliary power unit (APU) during a flight. Associated with this fault model may be a suggested repair procedure along with references to aircraft maintenance manuals (~ Light maintenance manual).\n\n"}
{"id": "6209845", "url": "https://en.wikipedia.org/wiki?curid=6209845", "title": "Fishing tackle", "text": "Fishing tackle\n\nFishing tackle is the equipment used by anglers when fishing. Almost any equipment or gear used for fishing can be called fishing tackle. Some examples are hooks, lines, sinkers, floats, rods, reels, baits, lures, spears, nets, gaffs, traps, waders and tackle boxes.\n\nGear that is attached to the end of a fishing line is called \"terminal tackle\". This includes hooks, leaders, swivels, sinkers, floats, split rings and wire, snaps, beads, spoons, blades, spinners and clevises to attach spinner blades to fishing lures. Sometimes the term fishing rig is used for a completed assembly of tackle ready for fishing.\n\nFishing tackle can be contrasted with fishing techniques. Fishing tackle refers to the physical equipment that is used when fishing, whereas fishing techniques refers to the manner in which the tackle is used when fishing.\n\nThe term \"tackle\", with the meaning \"apparatus for fishing\", has been in use from 1398 AD. Fishing tackle is also called \"fishing gear\". However the term fishing gear is more usually used in the context of commercial fishing, whereas fishing tackle is more often used in the context of recreational fishing. This article covers equipment used by recreational anglers.\n\nHook, line and sinker is a classic combination of tackle empowering an angler to catch fish.\n\nThe use of the hook in angling is descended, historically, from what would today be called a \"gorge\". The word \"gorge\", in this context, comes from an archaic word meaning \"throat\". Gorges were used by ancient peoples to capture fish. A gorge was a long, thin piece of bone or stone attached by its midpoint to a thin line. The gorge would be fixed with a bait so that it would rest parallel to the lay of the line. When a fish swallowed the bait, a tug on the line caused the gorge to orient itself at right angles to the line, thereby sticking in the fish's gullet.\n\nA fish hook is a device for catching fish either by impaling them in the mouth or, more rarely, by snagging the body of the fish. Fish hooks have been employed for millennia by anglers to catch fresh and saltwater fish. Early hooks were made from the upper bills of eagles and from bones, shells, horns and thorns of plants (Parker 2002). In 2005, the fish hook was chosen by \"Forbes\" as one of the top twenty tools in the history of man. Fish hooks are normally attached to some form of line or lure device which connects the caught fish to the angler. There is an enormous variety of fish hooks. Sizes, designs, shapes, and materials are all variable depending on the intended purpose of the hook. They are manufactured for a range of purposes from general fishing to extremely limited and specialized applications. Fish hooks are designed to hold various types of artificial, processed, dead or live baits (bait fishing); to act as the foundation for artificial representations of fish prey (fly fishing); or to be attached to or integrated into other devices that represent fish prey (lure fishing).\n\nA fishing line is a cord used or made for fishing. The earliest fishing lines were made from leaves or plant stalk (Parker 2002). Later lines were constructed from horse hair or silk thread, with catgut leaders. From the 1850s, modern industrial machinery was employed to fashion fishing lines in quantity. Most of these lines were made from linen or silk, and more rarely cotton.\n\nModern lines are made from artificial substances, including nylon, polyethylene, dacron and dyneema. The most common type is monofilament made of a single strand. Anglers often use monofilament because of its buoyant characteristics and its ability to stretch under load. Recently alternatives such as fluorocarbon, which is the least visible type, and braided fishing line, also known as 'superlines' because of their small diameter, minimal amount of stretch, and great strength relative to standard nylon monofilament lines.\n\nImportant parameters of a fishing line are its breaking strength and its diameter (thicker, sturdier lines are more visible to fish). Factors that may determine what line an angler chooses for a given fishing environment include breaking strength, diameter,castability, buoyancy, stretch, color, knot strength, UV resistance, limpness, abrasion resistance, and visibility.\n\nFishing with a hook and line is called angling. In addition to the use of the hook and line used to catch a fish, a heavy fish may be landed by using a landing net or a hooked pole called a gaff. Trolling is a technique in which a fishing lure on a line is drawn through the water. Snagging is a technique where the object is to hook the fish in the body.\n\nA sinker or plummet is a weight used when angling to force the lure or bait to sink more rapidly or to increase the distance that it may be cast. The ordinary plain sinker is traditionally made of lead. It can be practically any shape, and is often shaped round like a pipe-stem, with a swelling in the middle. However, the use of smaller lead based fishing sinkers has now been banned in the UK, Canada and some states in the USA, since lead can cause toxic lead poisoning if ingested. There are loops of brass wire on either end of the sinker to attach the line. Weights can range from a quarter of an ounce for trout fishing up to a couple of pounds or more for sea bass and menhaden.\n\nThe swivel sinker is similar to the plain one, except that instead of loops, there are swivels on each end to attach the line. This is a decided improvement, as it prevents the line from twisting and tangling. In trolling, swivel sinkers are indispensable. The slide sinker, for bottom fishing, is a leaden tube which allows the line to slip through it, when the fish bites. This is an excellent arrangement, as the angler can feel the smallest bite, whereas in the other case the fish must first move the sinker before the angler feels him.\n\nA fishing rod is an additional tool used with the hook, line and sinker. A length of fishing line is attached to a long, flexible rod or pole: one end terminates with the hook for catching the fish. Early fishing rods are depicted on inscriptions in ancient Egypt, China, Greece and Rome. In Medieval England they were called \"angles\" (hence the term \"angling\"). As they evolved they were made from materials such as split Tonkin bamboo, Calcutta reed, or ash wood, which were light, tough, and pliable. The butts were frequently made of maple. Handles and grips were made of cork, wood, or wrapped cane. Guides were simple wire loops.\n\nModern rods are sophisticated casting tools fitted with line guides and a reel for line stowage. They are most commonly made of fibreglass, carbon fibre or, classically, bamboo. Fishing rods vary in action as well as length, and can be found in sizes between 24 inches and 20 feet. The longer the rod, the greater the mechanical advantage in casting. There are many different types of rods, such as fly rods, tenkara rods, spin and bait casting rods, spinning rods, ice rods, surf rods, sea rods and trolling rods.\n\nFishing rods can be contrasted with fishing poles. A fishing pole is a simple pole or stick with a line which is fastened to the tip and suspended with a hooked lure or bait at the other end.\n\nA fishing reel is a device used for the deployment and retrieval of a fishing line using a spool mounted on an axle. Fishing reels are traditionally used in angling. They are most often used in conjunction with a fishing rod, though some specialized reels are mounted on crossbows or to boat gunwales or transoms. The earliest known illustration of a fishing reel is from Chinese paintings and records beginning about 1195 A.D. Fishing reels first appeared in England around 1650 A.D., and by the 1760s, London tackle shops were advertising multiplying or gear-retrieved reels. Paris, Kentucky native George Snyder is generally given credit for inventing the first fishing reel in America around 1820, a bait casting design that quickly became popular with American anglers.\n\nThe natural bait angler usually uses a common prey species of the fish as an attractant. The natural bait used may be alive or dead. Common natural baits include bait fish, worms, leeches, minnows, frogs, salamanders, nightcrawlers and other insects. Natural baits are effective due to the lifelike texture, odour and colour of the bait presented.\n\nThe common earthworm is a universal bait for fresh water angling. In the quest for quality worms, some fishers culture their own worm compost or practice worm charming. Grubs and maggots are also considered excellent bait when trout fishing. Grasshoppers, flies, bees and even ants are also used as bait for trout in their season, although many anglers believe that trout or salmon roe is superior to any other bait. Studies show that natural baits like croaker and shrimp are more recognized by the fish and are more readily accepted. A good bait for red drum is menhaden. Because of the risk of transmitting whirling disease, trout and salmon should not be used as bait.\n\nProcessed baits, such as groundbait and boilies, can work well with coarse fish, such as carp. For example, in lakes in southern climates such as Florida, fish such as bream will take bread bait. Bread bait is a small amount of bread, often moistened by saliva, balled up to a small size that is bite size to small fish.\n\nMany people prefer to fish solely with lures, which are artificial baits designed to entice fish to strike. The artificial bait angler uses a man-made lure that may or may not represent prey. The lure may require a specialised presentation to impart an enticing action as, for example, in fly fishing. Recently, electronic lures have been developed to attract fish. Anglers have also begun using plastic bait. A common way to fish a soft plastic worm is the Texas rig. \nA bite indicator or commonly referred to as \"strike indicator\" is a mechanical or electronic device which indicates to an angler that something is happening at the hook end of the fishing line. There are many types of bite indicators—which ones work best depends on the type of fishing.\n\nStrike indicators are also commonly used in fly fishing for trout. Nymphs or wet flies will be suspended below the indicator to alert the angler of a fish. This has been used successfully for all trout and salmon species.\n\nAnother common form of using an indicator in fly fishing is the \"hopper dropper rig\" With this technique a nymph or wet fly will be hung from the bottom of a floating dry fly. This will effectively double the opportunity of a strike from an actively feeding fish.\n\nOther devices which are widely used as bite indicators are floats which float in the water, and dart about if a fish bites, and quiver tips which are mounted onto the tip of the fishing rod. Bite alarms are electronic devices which bleep when a fish tugs a fishing line. Whereas floats and quiver tips are used as visual bite detectors, bite alarms are audible bite detectors.\nSpearfishing is an ancient method of fishing conducted with an ordinary spear or a specialised variant such as a harpoon, trident, arrow or eel spear.\nHarpoons are spears which have a barb at the end. Their use was widespread in palaeolithic times. Cosquer cave in Southern France contains cave art over 16,000 years old, including drawings of seals which appear to have been harpooned. Tridents are spears which have three prongs at the business end. They are also called leisters or gigs. They feature widely in early mythology and history.\n\nModern spears can be used with a speargun. Some spearguns use slings (or rubber loops) to propel the spear. Polespears have a sling attached to the spear, Hawaiian slings have a sling separate from the spear, in the manner of an underwater bow and arrow.\n\nA bow or crossbow can be used with arrows in bowfishing. \nFishing nets are meshes usually formed by knotting a relatively thin thread. \nBetween 177 and 180 the Greek author Oppian wrote the \"Halieutica\", a didactic poem about fishing. He described various means of fishing including the use of nets cast from boats, scoop nets held open by a hoop, and various traps \"which work while their masters sleep\". Ancient fishing nets used threads made from leaves, plant stalk and cocoon silk. They could be rough in design and material but some designs were amazingly close to designs we use today (Parker 2002). Modern nets are usually made of artificial polyamides like nylon, although nets of organic polyamides such as wool or silk thread were common until recently and are still used.\n\nHand nets are held open by a hoop, and maybe on the end of a long stiff handle. They have been known since antiquity and may be used for sweeping up fish near the water surface like muskellunge and northern pike. When such a net is used by an angler to help land a fish it is known as a \"landing net\". In the UK, hand-netting is the only legal way of catching glass eels and has been practised for thousands of years on the River Parrett and River Severn.\n\nCast nets are small round nets with weights on the edges which is thrown by the fisher. Sizes vary up to about four metres in diameter. The net is thrown by hand in such a manner that it spreads out on the water and sinks. Fish are caught as the net is hauled back in.\nFishing traps are culturally almost universal and seem to have been independently invented many times. There are essentially two types of trap, a permanent or semi-permanent structure placed in a river or tidal area and pot-traps that are baited to attract prey and periodically lifted. They might have the form of a fishing weir or a lobster trap. A typical trap can have a frame of thick steel wire in the shape of a heart, with chicken wire stretched around it. The mesh wraps around the frame and then tapers into the inside of the trap. When a fish swims inside through this opening, it cannot get out, as the chicken wire opening bends back into its original narrowness. In earlier times, traps were constructed of wood and fibre. \nA fish stringer is a line of rope or chain along which an angler can string fish that have been caught so they can be immersed and kept alive in water.\n\nFly fishing tackle is equipment used by, and often specialised for use by fly anglers. Fly fishing tackle includes fly lines designed for easy casting, specialised fly reels designed to hold a fly line and supply drag if required for landing heavy or fast fish, specialised fly rods designed to cast fly lines and artificial flies, terminal tackle including artificial flies, and other accessories including fly boxes used to store and carry artificial flies.\n\nFishing tackle boxes have for many years been an essential part of the anglers equipment. Fishing tackle boxes were originally made of wood or wicker and eventually some metal fishing tackle boxes were manufactured. The first plastic fishing tackle boxes were manufactured by Plano in response to the need for a product that didn't rust. Early plastic fishing tackle boxes were similar to tool boxes but soon evolved into the hip roof cantilever tackle boxes with numerous small trays for small tackle. These types of tackle boxes are still available today but they have the disadvantage that small tackle gets mixed up. Fishing tackle boxes have also been manufactured so the drawers themselves become small storage boxes, each with their own lids. This prevents small tackle from mixing, and can turn each drawer into a stand-alone container which can be used to carry small tackle to a rod some distance from the main tackle box.\nWorldwide, the recreational fishing tackle industry is big business, worth over five billion US dollars annually in the United States alone. Notable brands include:\n\n\n"}
{"id": "16432961", "url": "https://en.wikipedia.org/wiki?curid=16432961", "title": "Fougère", "text": "Fougère\n\nFougère, , is one of the main olfactive families of perfumes. The name comes from the French language word for \"fern\". \"Fougère\" perfumes are made with a blend of fragrances: top-notes are sweet, with the scent of lavender flowers; as the more volatile components evaporate, the scents of oakmoss, derived from a species of lichen and described as woody, sharp and slightly sweet, and coumarin, similar to the scent of new-mown hay, become noticeable. Aromatic \"fougère\", a derivative of this class, contains additional notes of herbs, spice and/or wood.\n\nThe name originated with Houbigant Parfum's \"Fougère Royale\". This perfume, created by Houbigant owner Paul Parquet, was later added to the scent archives known as the Osmothèque, in Versailles, France. Houbigant re-introduced this fragrance in 2010.\n\nPerfumes of this type are especially popular as fragrances for men. Many modern fougère perfumes have various citrus, herbaceous, green, floral and animalic notes included. The most common additions to the basic fragrance blend include vetiver and geranium. Bergamot is often present to add sharpness to the lavender top-note.\n\nExamples of men's fragrances which fall into the \"fougère\" class include Brut by Fabergé, Paco Rabanne Pour Homme, Azzaro Pour Homme, Boss by Hugo Boss, Prada for Men, Eternity for Men by Calvin Klein, Canoe for Men by Dana, Dolce & Gabbana Pour Homme, Drakkar Noir by Guy Laroche, Tabac for Men, Michael for Men by Michael Kors, Clubman Pinaud After Shave and Special Reserve, Polo Blue and Chaps by Ralph Lauren, and Kouros by Yves Saint Laurent.\n\n\n"}
{"id": "6257840", "url": "https://en.wikipedia.org/wiki?curid=6257840", "title": "Gate fee", "text": "Gate fee\n\nA gate fee (or tipping fee) is the charge levied upon a given quantity of waste received at a waste processing facility.\n\nIn the case of a landfill it is generally levied to offset the cost of opening, maintaining and eventually closing the site. It may also include any landfill tax which is applicable in the region. The gate fee differs from the waste removal fee which is the charge levied on people in areas, such as Ireland, where waste collection is not covered as part of local taxes.\n\nWith waste treatment facilities such as incinerators, mechanical biological treatment facilities or composting plants the fee offsets the operation, maintenance, labour costs, capital costs of the facility along with any profits and final disposal costs of any unusable residues.\n\nThe fee can be charged per load, per tonne, or per item depending on the source and type of the waste.\n\n"}
{"id": "25856119", "url": "https://en.wikipedia.org/wiki?curid=25856119", "title": "Glider (sailplane)", "text": "Glider (sailplane)\n\nA glider or sailplane is a type of glider aircraft used in the leisure activity and sport of gliding. This unpowered aircraft uses naturally occurring currents of rising air in the atmosphere to remain airborne. Gliders are aerodynamically streamlined and are capable of gaining altitude and remaining airborne, and maintaining forward motion. \n\nGliders benefit from producing the least drag for any given amount of lift, and this is best achieved with long, thin wings, a fully faired narrow cockpit and a slender fuselage. Aircraft with these features are able to soar - climb efficiently in rising air produced by thermals or hills. In still air, gliders can glide long distances at high speed with a minimum loss of height in between.\n\nGliders have rigid wings and either skids or undercarriage. In contrast hang gliders and paragliders use the pilot's feet for the start of the launch and for the landing. These latter types are described in separate articles, though their differences from gliders are covered below. Gliders are usually launched by winch or aerotow, though other methods: auto tow and bungee, are occasionally used.\n\nSome gliders do not soar and are simply engineless aircraft towed by another aircraft to a desired destination and then cast off for landing. Military gliders (such as those used on D-Day) are single-use only, and are abandoned after landing, having served their purpose.\n\nMotor gliders are gliders with engines which can be used for extending a flight and even, in some cases, for take-off. Some high-performance motor gliders (known as \"self-sustaining\" gliders) may have an engine-driven retractable propeller which can be used to sustain flight. Other motor gliders have enough thrust to launch themselves before the engine is retracted and are known as \"self-launching\" gliders. Another type is the self-launching \"touring motor glider\", where the pilot can switch the engine on and off in flight without retracting their propellers.\n\nSir George Cayley's gliders achieved brief wing-borne hops from around 1849. In the 1890s, Otto Lilienthal built gliders using weight shift for control. In the early 1900s, the Wright Brothers built gliders using movable surfaces for control. In 1903, they successfully added an engine.\n\nAfter World War I gliders were first built for sporting purposes in Germany. Germany's strong links to gliding were to a large degree due to post-WWI regulations forbidding the construction and flight of motorised planes in Germany, so the country's aircraft enthusiasts often turned to gliders and were actively encouraged by the German government, particularly at flying sites suited to gliding flight like the Wasserkuppe.\n\nThe sporting use of gliders rapidly evolved in the 1930s and is now their main application. As their performance improved, gliders began to be used for cross-country flying and now regularly fly hundreds or even thousands of kilometres in a day if the weather is suitable.\n\nEarly gliders had no cockpit and the pilot sat on a small seat located just ahead of the wing. These were known as \"primary gliders\" and they were usually launched from the tops of hills, though they are also capable of short hops across the ground while being towed behind a vehicle. To enable gliders to soar more effectively than primary gliders, the designs minimized drag. Gliders now have very smooth, narrow fuselages and very long, narrow wings with a high aspect ratio and winglets.\n\nThe early gliders were made mainly of wood with metal fastenings, stays and control cables. Later fuselages made of fabric-covered steel tube were married to wood and fabric wings for lightness and strength. New materials such as carbon-fiber, fiber glass and Kevlar have since been used with computer-aided design to increase performance. The first glider to use glass-fiber extensively was the Akaflieg Stuttgart FS-24 Phönix which first flew in 1957. This material is still used because of its high strength to weight ratio and its ability to give a smooth exterior finish to reduce drag. Drag has also been minimized by more aerodynamic shapes and retractable undercarriages. Flaps are fitted to the trailing edges of the wings on some gliders to minimize the drag from the tailplane at all speeds.\n\nWith each generation of materials and with the improvements in aerodynamics, the performance of gliders has increased. One measure of performance is the glide ratio. A ratio of 30:1 means that in smooth air a glider can travel forward 30 meters while losing only 1 meter of altitude. Comparing some typical gliders that might be found in the fleet of a gliding club – the Grunau Baby from the 1930s had a glide ratio of just 17:1, the glass-fiber Libelle of the 1960s increased that to 39:1, and modern flapped 18 meter gliders such as the ASG29 have a glide ratio of over 50:1. The largest open-class glider, the eta, has a span of 30.9 meters and has a glide ratio over 70:1. Compare this to the Gimli Glider, a Boeing 767 which ran out of fuel mid-flight and was found to have a glide ratio of 12:1, or to the Space Shuttle with a glide ratio of 4.5:1.\n\nDue to the critical role that aerodynamic efficiency plays in the performance of a glider, gliders often have aerodynamic features seldom found in other aircraft. The wings of a modern racing glider have a specially designed low-drag laminar flow airfoil. After the wings' surfaces have been shaped by a mold to great accuracy, they are then highly polished. Vertical winglets at the ends of the wings are computer-designed to decrease drag and improve handling performance. Special aerodynamic seals are used at the ailerons, rudder and elevator to prevent the flow of air through control surface gaps. Turbulator devices in the form of a zig-zag tape or multiple blow holes positioned in a span-wise line along the wing are used to trip laminar flow air into turbulent flow at a desired location on the wing. This flow control prevents the formation of laminar flow bubbles and ensures the absolute minimum drag. Bug-wipers may be installed to wipe the wings while in flight and remove insects that are disturbing the smooth flow of air over the wing.\n\nModern competition gliders carry jettisonable water ballast (in the wings and sometimes in the vertical stabilizer). The extra weight provided by the water ballast is advantageous if the lift is likely to be strong, and may also be used to adjust the glider's center of mass. Moving the center of mass toward the rear by carrying water in the vertical stabilizer reduces the required down-force from the horizontal stabilizer and the resultant drag from that down-force. Although heavier gliders have a slight disadvantage when climbing in rising air, they achieve a higher speed at any given glide angle. This is an advantage in strong conditions when the gliders spend only a small amount of time climbing in thermals. The pilot can jettison the water ballast before it becomes a disadvantage in weaker thermal conditions. Another use of water ballast is to dampen air turbulence such as might be encountered during ridge soaring. To avoid undue stress on the airframe, gliders must jettison any water ballast before landing.\n\nMost gliders are built in Europe and are designed to EASA Certification Specification CS-22 (previously Joint Aviation Requirements-22). These define minimum standards for safety in a wide range of characteristics such as controllability and strength. For example, gliders must have design features to minimize the possibility of incorrect assembly (gliders are often stowed in disassembled configuration, with at least the wings being detached). Automatic connection of the controls during rigging is the common method of achieving this.\n\nThe two most common methods of launching gliders are by aerotow and by winch. When aerotowed, the glider is towed behind a powered aircraft using a rope about 60 meters (about 200 ft) long. The glider pilot releases the rope after reaching the desired altitude. However, the rope can be released by the towplane also in case of emergency. Winch launching uses a powerful stationary engine located on the ground at the far end of the launch area. The glider is attached to one end of 800–1200 metres (about 2,500–4,000 ft) of cable and the winch rapidly winds it in. The glider can gain about 900–3000 feet (about 300–900 metres) of height with a winch launch, depending on the head wind. Less often, automobiles are used to pull gliders into the air, by pulling them directly or through the use of a reverse pulley in a similar manner to the winch launch. Elastic ropes (known as bungees) are occasionally used at some sites to launch gliders from slopes, if there is sufficient wind blowing up the hill. Bungee launching was the predominant method of launching early gliders. Some modern gliders can self-launch with the use of retractable engines and/or propellers, which can also be used to sustain flight once airborne (see motor glider).\n\nOnce launched, gliders try to gain height using thermals, ridge lift, lee waves or convergence zones and can remain airborne for hours. This is known as \"soaring\". By finding lift sufficiently often, experienced pilots fly cross-country, often on pre-declared tasks of hundreds of kilometers, usually back to the original launch site. Cross-country flying and aerobatics are the two forms of competitive gliding. For information about the forces in gliding flight, see lift-to-drag ratio.\n\nPilots need some form of control over the glide slope to land the glider. In powered aircraft, this is done by reducing engine thrust. In gliders, other methods are used to either reduce the lift generated by the wing, increase the drag of the entire glider, or both. Glide slope is the distance traveled for each unit of height lost. In a steady wings-level glide with no wind, glide slope is the same as the lift/drag ratio (L/D) of the glider, called \"L-over-D\". Reducing lift from the wings and/or increasing drag will reduce the L/D allowing the glider to descend at a steeper angle with no increase in airspeed. Simply pointing the nose downwards only converts altitude into a higher airspeed with a minimal initial reduction in total energy. Gliders, because of their long low wings, create a high ground effect which can significantly increase the glide angle and make it difficult to bring the glider to Earth in a short distance.\n\nEarly glider designs used skids for landing, but modern types generally land on wheels. Some of the earliest gliders used a dolly with wheels for taking off and the dolly was jettisoned as the glider left the ground, leaving just the skid for landing. A glider may be designed so the center of gravity (CG) is behind the main wheel so the glider sits nose high on the ground. Other designs may have the CG forward of the main wheel so the nose rests on a nose-wheel or skid when stopped. Skids are now mainly used only on training gliders such as the Schweizer SGS 2–33. Skids are around 100mm (3 inches) wide by 900mm (3 feet) long and run from the nose to the main wheel. Skids help with braking after landing by allowing the pilot to put forward pressure on the control stick, thus creating friction between the skid and the ground. The wing tips also have small skids or wheels to protect the wing tips from ground contact.\n\nIn most high performance gliders the undercarriage can be raised to reduce drag in flight and lowered for landing. Wheel brakes are provided to allow stopping once on the ground. These may be engaged by fully extending the spoilers/air-brakes or by using a separate control. Although there is only a single main wheel, the glider's wing can be kept level by using the flight controls until it is almost stationary.\n\nPilots usually land back at the airfield from which they took off, but a landing is possible in any flat field about 250 metres long. Ideally, should circumstances permit, a glider would fly a standard pattern, or circuit, in preparation for landing, typically starting at a height of 300 metres (1,000 feet). Glide slope control devices are then used to adjust the height to assure landing at the desired point. The ideal landing pattern positions the glider on final approach so that a deployment of 30–60% of the spoilers/dive brakes/flaps brings it to the desired touchdown point. In this way the pilot has the option of opening or closing the spoilers/air-brakes to extend or steepen the descent to reach the touchdown point. This gives the pilot wide safety margins should unexpected events occur.\n\nIn addition to an altimeter, compass, and an airspeed indicator, gliders are often equipped with a variometer, turn and bank indicator and an airband radio (transceiver), each of which may be required in some countries. An Emergency Position-Indicating Radio Beacon (ELT) may also be fitted into the glider to reduce search and rescue time in case of an accident.\n\nMuch more than in other types of aviation, glider pilots depend on the variometer, which is a very sensitive vertical speed indicator, to measure the climb or sink rate of the plane. This enables the pilot to detect minute changes caused when the glider enters rising or sinking air masses. Both mechanical and electronic 'varios' are usually fitted to a glider. The electronic variometers produce a modulated sound of varying amplitude and frequency depending on the strength of the lift or sink, so that the pilot can concentrate on centering a thermal, watching for other traffic, on navigation, and weather conditions. Rising air is announced to the pilot as a rising tone, with increasing pitch as the lift increases. Conversely, descending air is announced with a lowering tone, which advises the pilot to escape the sink area as soon as possible. (Refer to the \"variometer\" article for more information).\n\nGliders' variometers are sometimes fitted with mechanical devices such as a \"MacCready Ring\" to indicate the optimal speed to fly for given conditions. These devices are based on the mathematical theory attributed to Paul MacCready though it was first described by Wolfgang Späte in 1938. MacCready theory solves the problem of how fast a pilot should cruise between thermals, given both the average lift the pilot expects in the next thermal climb, as well as the amount of lift or sink encountered in cruise mode. Electronic variometers make the same calculations automatically, after allowing for factors such as the glider's theoretical performance, water ballast, headwinds/tailwinds and insects on the leading edges of the wings.\n\nSoaring flight computers, often used in combination with PDAs running specialized soaring software, have been designed for use in gliders. Using GPS technology in conjunction with a barometric device these tools can:\n\nAfter the flight the GPS data may be replayed on computer software for analysis and to follow the trace of one or more gliders against a backdrop of a map, an aerial photograph or the airspace.\n\nBecause collision with other gliders is a risk, the anti-collision device FLARM is becoming increasingly common in Europe and Australia. In the longer term, gliders may eventually be required in some European countries to fit transponders once devices with low power requirements become available.\n\nSo that ground-based observers may identify gliders in flight or in gliding competition, registration marks (\"insignias\" or \"competition numbers\" or \"contest ID\") are displayed in large characters on the underside of a single wing, and also on the fin and rudder. Registration marks are assigned by gliding associations such as the US Soaring Society of America, and are unrelated to national registrations issued by entities such as the US Federal Aviation Administration. This need for visual ID has somewhat been supplanted by GPS position recording. Insignias are useful in two ways: First, they are used in radio communications between gliders, as pilots use their competition number as their call signs. Secondly, to easily tell a glider's contest ID when flying in close proximity to one another to alert them of potential dangers. For example, during gatherings of multiple gliders within thermals (known as \"gaggles\"), one pilot might report \"Six-Seven-Romeo I am right below you\".\n\nFibreglass gliders are invariably painted white to minimise their skin temperature in sunlight. Fibreglass resin loses strength as its temperature rises into the range achievable in direct sun on a hot day. Color is not used except for a few small bright patches on wing tips; these patches (typically orange or red) improving a glider's visibility to pilots while in flight. Such patches are obligatory for mountain flying in France. Non-fibreglass gliders made of aluminum and wood are not so subject to deterioration at higher temperatures and are often quite brightly painted.\n\nThere is sometimes confusion about gliders, hang gliders and paragliders. In particular, paragliders and hang gliders are both foot-launched. The main differences between the types are:\n\nEight competition classes of glider have been defined by the FAI. They are:\n\nA large proportion of gliders have been and are still made in Germany, the birthplace of the sport. In Germany there are several manufacturers but the three principal companies are:\nGermany also has Stemme and Lange Aviation. Elsewhere in the world, there are other manufacturers such as Jonker Sailplanes in South Africa, Sportinė Aviacija in Lithuania, Allstar PZL in Poland, HpH in the Czech Republic and AMS Flight in Slovenia.\n\n\n\n"}
{"id": "11264248", "url": "https://en.wikipedia.org/wiki?curid=11264248", "title": "Glossary of systems theory", "text": "Glossary of systems theory\n\nA glossary of terms relating to systems theory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31502109", "url": "https://en.wikipedia.org/wiki?curid=31502109", "title": "Golondrina point", "text": "Golondrina point\n\nGolondrina points (formerly Plainview Golondrina) are lanceolate spear or dart projectile points, of medium size, dated to the transitional Paleo-Indian Period, between 9000–7000 BP. Golondrina points were attached on split-stem hafts and may have served to bring down medium-sized animals such as deer, as well as functioning as butchering knives. Distribution is widespread throughout most of Texas, and points have also been discovered in Arkansas and Mexico. The concentration of Golondrina specimens is highest across the South Texas Plains, where the point is the most prevalent of Paleo-Indian types and defines a distinctive cultural pattern for the region. The Golondrina point is so named for its flared basal corners (\"ears\"), which resemble a swallow's (\"golondrina\" in Spanish) split tail. Classification of Golondrina can be difficult because of its similarity to other types, particularly the Plainview point, to which it was originally thought to be related.\n\nClassification of the Golondrina point was made by Texas Historical Commission archeologist LeRoy Johnson Jr. in 1964, after the discovery of a collection of unrecognized projectile points at the Devil's Mouth site in the Amistad Reservoir, Texas. Initially believed to be related to the Plainview point classification, the new type was termed \"Plainview Golondrina\" by Johnson, who used a genus-species approach for the naming. This classification method sought to describe the relationship between the two types, placing Plainview as the genus, and Golondrina as the species, to highlight key similarities and differences. But by 1977, the genus-species classification approach had been discarded, and the name Golondrina alone was being used to represent the Devil's Mouth specimens. Subsequent research and technological analysis determined this type to be separate and distinct from the Plainsview point, and the name was shortened to simply \"Golondrina\" by Thomas C. Kelly in 1982. The type takes its name from a pronounced flaring of the basal corners (stem), which recall the split tail of a swallow (\"golondrina\" in Spanish). The Golondrina is considered to be a Plano point.\n\nThe Golondrina point is medium-sized and lanceolate-shaped with a lenticular cross-section that exhibits convex sides. The type displays a distinctive auriculate (\"eared\") stem with basal corners that flare outward. The blade edges are slightly serrated with a recurved outline—wide at the bottom, then narrowing before becoming wide and then thin again at the distal end, a so-called \"fish shaped profile\". The flaking style is generally random, with no attention given to alignment of flake scars. However, collateral flaking—where parallel flakes have been removed equally, resulting in a median ridge on the blade edges—has also been observed in some specimens. The basal edge of the Golondrina presents concave with a deep basal notch that varies from a flattened, inverted, v-shape to recurved. The Golondrina point can range in length from 32–61 mm, with a width ranging from 23–32 mm and a thickness from 6–8 mm. The width of the base ranges from 22–29 mm with a typical basal concavity of 4 mm or more. The Golondrina is unfluted, without a longitudinal channel flake.\n\nThe point has an expanding hafting area where the width, upward from the stem, increases in size. Analysis suggests that Golondrina points were attached on split-stem hafts either with or without foreshafts. They may have served a dual function as projectile points as well as butchering knives. By nature of a split-stem haft style, Golondrina points would not need to be deeply set, resulting in a largely exposed cutting edge. The hafting area, as well as the side and basal edges are usually ground dull.\n\nEdwards chert, Alibates agate, and Tecovas jasper were the major materials utilized by Paleo-Indians in the Southern Plains for the manufacture of flaked stone implements. Chert was the most important stone for tool making throughout pre-historic central Texas and there were many available sites where it was acquired and knapped.\n\nMost Golondrina points have been dated to the Transitional Paleo-Indian Period, between 9000–7000 BP, with excavation of stratified sites along with radiocarbon dating providing a definitive age. The first dating of Golondrina points was made after excavations of area C in the Devil's Mouth site which revealed Paleo-Indian projectile points that were radiocarbon assayed to 8700 BP. Later excavations in 1976, at the nearby Baker Cave in Texas, revealed a large hearth in the Golondrina stratum containing a wide variety of small game and plant remains left by early hunter gathers. This archaeological assemblage was termed Golondrina Complex, and the materials were attributed to the post-Pleistocene period. At the same site, Golondrina materials stratified near the base of a rock-shelter deposit were radiocarbon dated at 9000 BP\n\nProjectile points featuring more Archaic characteristics, including early barbed and early stemmed, share an overlapping chronology with Golondrina.\n\nGolondrina points are widespread across much of central, southern, and western Texas, with distribution extending northward along the Balcones Escarpment. They are particularly prevalent across the South Texas Plains, so much so as to be said to represent the first unique cultural pattern in the area. Most Golondrina specimens from the South Texas Plains are not excavated but rather found on the site's surface. They often appear as part of mixed collections with artifacts of later periods. Several sites have also been found in Arkansas as well as in the Lower Pecos Canyonlands and the Mexican states of Tamaulipas and Nuevo León.\n\nThe Golondrina can be difficult to type because of its similarity to other point classifications. In particular the distinction between Golondrina and Plainview is not yet completely resolved.\n\nPast hypotheses have suggested mistakenly that the Golondrina was a descendant or variant of the Plainview type. Although the two points exhibit similarities, particularly in terms of shape, they are now recognized as separate types. Identification is made from a lack of an auriculated, fishtail base on the Plainview and a deeper basal concavity on the Golondrina. Plainview points may have served to bring down large bison, while Golondrina points may have been used for killing smaller game such as deer, as well as doubling as a butchering knife. Compared to the split-stem hafting style of the Golondrina point without a deep setting, the shape of the Plainview points would necessitate them being set relatively deeply into a socketed haft that once bound would leave less of the cutting edge exposed. Stratigraphic evidence from the St. Mary's Hall site in southern, central Texas implies that Golondrina may have superseded Plainview on the southern Plains.\n\nThe Simpson and Suwannee points, found in Florida and the Southeastern United States, are similar to the Golondrina in shape and age. The Dalton point, found in the central United States, shares a similar outline and basal corner auriculation with Golondrina, which may imply that they are part of a series. Identification is made by examination of the blade edges—Golondrina exhibits much less serration. The Meserve type has also been known to cause confusion, as it is considered a resharpened variant of the Plainview and Golondrina types.\n\n"}
{"id": "2545687", "url": "https://en.wikipedia.org/wiki?curid=2545687", "title": "Heat-shrink tubing", "text": "Heat-shrink tubing\n\nHeat-shrink tubing (or, commonly, \"heat shrink\" or \"heatshrink\") is a shrinkable plastic tube used to insulate wires, providing abrasion resistance and environmental protection for stranded and solid wire conductors, connections, joints and terminals in electrical work. It can also be used to repair the insulation on wires or to bundle them together, to protect wires or small parts from minor abrasion, and to create cable entry seals, offering environmental sealing protection. Heat-shrink tubing is ordinarily made of nylon or polyolefin, which shrinks radially (but not longitudinally) when heated, to between one-half and one-sixth of its diameter.\n\nHeat-shrink tubing is manufactured in a multitude of varieties and chemical makeups with the exact composition of each type being dependent on the intended application. From near microscopically-thin-wall tubing to rigid, heavy-wall tubing, each type has precise design and chemical additives that make it suitable for meeting any of a wide variety of environmental demands. Heat-shrink tubing is rated by its expansion ratio, a comparison of the differences in expansion and recovery rate.\n\nThe unshrunk tubing is fitted on the wire before making the connection, then slid down to cover the joint after it is made. If the fit is tight, silicone lubricant can be applied without compromising the heat-shrink material. The tubing is then shrunk to wrap tightly around the joint by heating in an oven or with a hot air gun or other source of hot gas flow. Convenient but less consistent methods for shrinking the tube include a soldering iron held close to but not touching the tube, or the heat from a lighter. Uncontrolled heat can cause uneven shrinkage, physical damage and insulation failure, and these methods are not recommended by heatshrink suppliers. If overheated, heat-shrink tubing can melt, scorch or catch fire like any other plastic. Heating causes the tubing to contract to between half and one sixth of its original diameter, depending on the material used, providing a snug fit over irregularly shaped joints. There is also longitudinal shrinking, usually unwanted and to a lesser extent than narrowing, of typically around 6%. The tubing provides good electrical insulation, protection from dust, solvents and other foreign materials, and mechanical strain relief, and is mechanically held in place (unless incorrectly oversized or not properly shrunk) by its tight fit.\nSome types of heat-shrink contain a layer of thermoplastic adhesive on the inside to help provide a good seal and better adhesion, while others rely on friction between the closely conforming materials. Heating non-adhesive shrink tube to very near the melting point may allow it to fuse to the underlying material as well.\n\nHeatshrink tubing is sometimes sold in pre-cut lengths, with a solder blob at the center of the length, as this configuration is specified by Daimler-Benz for automotive electrical repairs.\n\nOne application that has used heatshrink in large quantities since the early 1970s is the covering of fibreglass helical antennas, used extensively for 27 MHz CB Radio. Many millions of these antennas have been coated this way.\n\nHeat-shrink tubing was invented by Raychem Corporation in 1962. It is manufactured from a thermoplastic material such as polyolefin, fluoropolymer (such as FEP, PTFE or Kynar), PVC, neoprene, silicone elastomer or Viton.\n\nThe process for making heat-shrink tubing is as follows: First the material is chosen based on its properties. The material is often compounded with other additives (such as colorants, stabilizers, etc.) depending on the application. A starting tube is extruded from the raw material. Next, the tube is taken to a separate process where it is cross-linked, usually through radiation. The cross-linking creates a memory in the tube. Then the tube is heated to just above the polymer's crystalline melting point and expanded in diameter, often by placing it in a vacuum chamber. While in the expanded state it is rapidly cooled. Later, when heated (above the crystalline melting point of the material) by the end user, the tubing shrinks back to its original extruded size.\n\nThe material is often cross-linked through the use of electron beams, peroxides, or moisture. This cross-linking creates the memory in the tubing so that it is able to shrink back to its original extruded dimensions upon heating, producing a material called heat-shrink tubing.\nFor external use, heat-shrink tubing often has a UV stabiliser added.\n\nDifferent applications require different materials:\n\nOther special materials exist, offering qualities such as resistance to diesel and aviation fuels, and there is also woven fabric, providing increased abrasion resistance in harsh environments.\n\nHeat-shrink tubing is available in a variety of colors for color-coding of wires and connections. In the early twenty-first century heat-shrink tubing started to be used for PC modding to tidy up the interior of computers and provide an appearance considered pleasing. In response to this opening market, manufacturers started producing heat-shrink tubing in luminous and UV reactive varieties.\n\nAlthough usually used for insulation, heat-shrink tubing with a conductive lining is also available, for use on joints which are not soldered.\n\nSpecialty heat-shrink tubing, known as \"solder sleeves\", have a tube of solder inside of the heat-shrink tubing, allowing the heat source to electrically join the two wires by melting the solder and simultaneously insulate the junction with the tubing. Solder sleeves also commonly contain a ring of heat-activated sealant on the inside of each end of the tubing, allowing the connection to also be made waterproof.\n\nHeat-shrink end caps, closed at one end, are used to insulate the exposed cut ends of insulated wires.\n\n\nUL224-2010\nASTM D 2671\n\n"}
{"id": "52484417", "url": "https://en.wikipedia.org/wiki?curid=52484417", "title": "Hot Laboratory and Waste Management Center", "text": "Hot Laboratory and Waste Management Center\n\nThe Hot Laboratory and Waste Management Center (HLWMC), is dedicated for radioactive waste disposal as well as development of expertise in the back end of nuclear fuel cycle and radioisotope production for medical and industrial applications. The HLWMC was established in 1980, owned and operated by the Egyptian Atomic Energy Authority (AEA) in Inshas, northeast of Cairo.\n\nThe Hot Laboratory and Waste Management Center (HLWMC) consists of a low and intermediate level liquid waste station, radioisotope production laboratories, and a radioactive waste disposal site. HLWMC contains a French-supplied hot cell complex for plutonium extraction research, which is the only known facility in Egypt that could be used to separate weapons-usable plutonium from irradiated reactor fuel. However, the facility does not contain nuclear materials requiring IAEA safeguards.\n\n"}
{"id": "22990751", "url": "https://en.wikipedia.org/wiki?curid=22990751", "title": "IEEE Medal for Environmental and Safety Technologies", "text": "IEEE Medal for Environmental and Safety Technologies\n\nThe Medal for Environmental and Safety Technologies was established by the Institute of Electrical and Electronics Engineers (IEEE) Board of Directors in 2008. This award is presented for outstanding accomplishments in the application of technology in the fields of interest to IEEE that improve the environment and/or public safety. The medal is sponsored by Toyota Corporation.\n\nThe award may be presented to an individual or a team of up to three people.\n\nRecipients of this award receive a gold medal, a bronze replica, a certificate and an honorarium.\n\nThe award was presented for the first time in 2010.\n\nBasis for Judging: In the evaluation process, the following criteria are considered: public benefits of the contribution; degree in improvement in important performance metrics; innovative design, development, or application engineering; favorable influence on the contribution on technical professions.\n\nNomination deadline: 1 July\n\nNotification: Recipients are typically approved during the November IEEE Board of Directors meeting. Recipients and their nominators will be notified following the meeting. Then the nominators of unsuccessful candidates will be notified of the status of their nomination.\n\nPresentation: At the annual IEEE Honors Ceremony\n"}
{"id": "12866416", "url": "https://en.wikipedia.org/wiki?curid=12866416", "title": "Information Systems Security Association", "text": "Information Systems Security Association\n\nInformation Systems Security Association (ISSA) is a not-for-profit, international professional organization of information security professionals and practitioners. It was founded in 1984, after work on its establishment started in 1982. It provides educational forums, publications and peer interaction opportunities.\n\nPresident:\nKeyaan Williams, CISSP, CCISO\n\nVice President:\nRoy Wilkinson PhD, CPCS, CHS-V, CISSP\n\nSecretary/Director of Operations:\nAnne Rogers CISSP, CCE, PMP, EnCE, IAM, IEM\n\nTreasurer/Chief Financial Officer:\nPamela Fusco, CISSP\n\nISSA has an international membership base.\n\nThe primary goal of the ISSA is to promote management practices that will ensure the confidentiality, integrity and availability of information resources. The ISSA facilitates interaction and education to create a more successful environment for global information systems security and for the professionals involved.\n\nLocal chapters host periodic meetings, usually monthly, for educational and networking purposes. Events with broader scope are held at the regional, national and international levels.\n\nISSA members receive continuing education credits for attending all ISSA-sponsored activities, as well as activities sponsored by other organizations that uphold similar membership standards.\n\nThe following list includes some of the important ways that ISSA members work toward achieving the Association's goals:\n\n\nAs an applicant for membership, the individual is expected to be bounded to a principle of ethics related to the Information Security career.\n\nApplicants for ISSA membership attest that they have and will:\n\n\nISSA is present in more than one hundred countries, including Europe and Asia, with more than 10,000 members.\n\n\nISSA has established an Information Security Program alliance with Microsoft Corporation.\n\n\n"}
{"id": "478934", "url": "https://en.wikipedia.org/wiki?curid=478934", "title": "Janice E. Voss", "text": "Janice E. Voss\n\nJanice Elaine Voss (October 8, 1956 – February 6, 2012) was an American engineer and a NASA astronaut. She flew in space five times, jointly holding the record for American women. Voss died on February 6, 2012, from breast cancer.\n\nVoss graduated from Minnechaug Regional High School in Wilbraham, Massachusetts, in 1972. She earned a bachelor's degree in engineering from Purdue University while working on a co-op at the Johnson Space Center. She earned an M.S. in Electrical Engineering and Computer Science from MIT in 1977. After studying space physics at Rice University from 1977 to 1978, she went on to earn a doctorate in aeronautics/astronautics from MIT in 1987.\n\nVoss was selected as an astronaut candidate in 1990 and flew as a mission specialist on missions STS-57 (1993), STS-63 (1995), STS-83 (1997), STS-94 (1997) and STS-99 (2000). All of her flights included another female astronaut as well.\n\nDuring her career as an astronaut, she participated in the first Shuttle rendezvous with the Mir space station on STS-63: it flew around the station, testing communications and inflight manoeuvres for later missions, but did not actually dock. As an STS-99 crew member on the Shuttle Radar Topography Mission, she and her fellow crew members worked continuously in shifts to produce what was at the time the most accurate digital topographical map of the Earth.\n\nFrom October 2004 to November 2007, she was Science Director for NASA's Kepler Space Observatory, an Earth-orbiting satellite designed to find Earth-like extrasolar planets in nearby solar systems. It was launched in March 2009 and was still operational at the time of her death at age 55 from breast cancer.\n\nAt the Astronaut Office Station Branch, she served as the Payloads Lead. She also worked for Orbital Sciences Corporation in flight operations support.\n\nThe Cygnus CRS Orb-2 capsule was named \"SS Janice Voss\" in her honor.\n\nThe VOSS Model is a scaled model of the solar system, dedicated to Janice Voss, located at Purdue University’s Discovery Park in West Lafayette, Indiana.\n"}
{"id": "42671484", "url": "https://en.wikipedia.org/wiki?curid=42671484", "title": "Julep (company)", "text": "Julep (company)\n\nJulep Beauty, Inc. is a privately held Seattle, WA based cosmetics company founded in 2007 by Jane Park, a former Starbucks and Boston Consulting Group executive. The company sells its products online, which includes the company’s Julep Maven service, in retail and in its branded beauty parlors.\n\nStarting in 2007, Julep opened four nail parlors in the Seattle area that offer manicures, pedicures, facials, and waxing. CEO Jane Park saw a rise in popularity for nail care products, prompting her to focus on selling new products online and through television. While expanding the parlors as a \"third place\" for women was the initial business model, Park now says she uses the parlors as a testing ground for new products.\n\nJulep moved into e-commerce in 2008. The company’s website is now its main sales channel, making available more than 300 new products on its website in 2013. Julep products are also available through their Seattle parlors, through retailers such as Sephora, Nordstrom and on TV though QVC. Also in 2013, it opened its first pop-up store in New York City. Julep reportedly tripled its revenue in 2013.\n\nJulep is backed by venture capital firms Andreessen Horowitz, Altimeter Capital, Azure Capital Partners, Madrona Venture Group, Maveron, as well as celebrity backers such as Will Smith and Jay Z. Other investors include AFSquare, Alliance of Angels, Atom Factory, Creative Artists Agency, Overbrook Entertainment, Precedent Investments, Roc Nation, Troy Carter, Version One Ventures, and Western Technology Investment. Julep has raised $56M in venture capital financing to date.\n\nJulep currently employs around 200 people and plans on using funding to accelerate the introduction of new products and other operating activities. While Jane Park and Julep generally enjoy a favorable reception in most media articles (see reference section below), employee reviews on Glassdoor suggest there are problems with retaining staff, and speak to issues with management and an inability to use vacation time—including accounts of former employees not being reimbursed for earned vacation time upon leaving the company.\n\nIn 2014, Julep's subscription service came under fire when the company received an \"F\" rating from the Better Business Bureau. The BBB's David Quinlan said about the 173 total complaints made during that period, \"It's an alarming amount. I mean, think about it, 113 people have complained about this company to us and not one of them has received a response back.\" The BBB notified Julep on Aug. 19, 2014 of the complaint pattern, but its website noted that the company has not responded to its request to “address the pattern.” Julep responded with a statement blaming the influx of complaints on a surge in business and shipping errors. As of January 2015, Julep still maintains an \"F\" rating from the Better Business Bureau.\n\nAs of October 2016, Julep has a \"B-\" rating from the Better Business Bureau.\n\nIn March of 2015, Julep announced that they were laying off 8% of their staff. According to CEO Jane Park, seventeen employees were laid off as part of a decision that \"align[s] with Julep’s greatest long-term opportunities and to best position the company for success and category leadership.\"\n\nJulep designs, produces and sells its own products, working with scientists and manufacturers that also work for larger brands. The company sells over 200 shades of nail polish, each of which is given a woman’s name. It has also expanded its line into makeup and skin care products.\n\nIn 2014, Julep launched its Plié Wand, an ergonomic nail polishing brush that bends, pivots, and attaches to the top of nail polish caps. Demand for the wand was first tested through a $75,000 crowdfunding campaign.\n\nThe company promotes its products as free of fumes and toxins.\n\nJulep introduced its monthly subscription service, Julep Maven, in 2011. Subscribers or “Mavens” pay a monthly fee to receive custom boxes of nail polish and makeup. To complement Julep’s crowdfunding campaigns, Mavens may also act as beta-testers in the company’s Idea Lab, answering surveys and posting on social media, allowing Julep to see how their products in development are being received.\n"}
{"id": "49013857", "url": "https://en.wikipedia.org/wiki?curid=49013857", "title": "List of Greek inventions and discoveries", "text": "List of Greek inventions and discoveries\n\nThis article is a list of major inventions and scientific and mathematical discoveries by Greek people from antiquity through the present day.\n\n\n\n\n"}
{"id": "57177015", "url": "https://en.wikipedia.org/wiki?curid=57177015", "title": "List of defunct instant messaging platforms", "text": "List of defunct instant messaging platforms\n\nBelow is a List of defunct instant messaging platforms.\n\nName * When discontinued * Type of client\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "33148559", "url": "https://en.wikipedia.org/wiki?curid=33148559", "title": "Magnadata Group", "text": "Magnadata Group\n\nMagnadata Group, also known as Magnadata International, is a specialised printing company in Boston in Lincolnshire. Since 2010 it has had the national contract for producing the UK's train tickets. It is one of the main companies in Europe that can produce magnetic ID cards for a company's workforce. It claims to be a global leader in RFID (radio-frequency identification) technology. The technology is referred to as smart ticketing.\n\nDapag had been formed in 1943. It made machines for printing tickets, labels and tags. It was based at Plumtree Court in EC4 in London, near where the A40 and A201 meet at Holborn Viaduct. It employed around 170 staff.\n\nNorcros was founded on 29 May 1956. Dapag was part of Norcros. The name Norcros comes from Normanby, North Lincolnshire, home of Normanby Hall. A Director of the company was John Vincent Sheffield, former High Sheriff of Lincolnshire from 1944-5, and great-uncle of the wife of David Cameron. Other members of the Sheffield family - Edmund and George - were directors.\n\nNorprint International was based on Norfolk Street in Boston. It had a division on \"Valley Road\" in Dovercourt on the Essex coast with 550 staff. Norprint also made ticket printing machines. Norprint claimed to be Europe's largest producer of industrial and retail labelling systems, and the largest in the world outside of America.\n\nIn early 1968 it helped to start the \"I'm Backing Britain\" campaign, by running off the promotional stickers for free. Before the days of barcodes in supermarkets, Norprint made hand-held price labellers, used by the main supermarkets; these were made in Harwich.\n\nIn April 1989 it received a Queen's Award for Technological Achievement for its \"magnetic striped and encoded tickets and payment tokens\".\n\nNorprint produced tickets for airlines and the London Underground. It was owned by Norcros plc.\n\nOn 25 June 1998 Norcros put Norprint up for sale. In July 1998 Norprint was bought by a management buyout (MBO) for £13.7 million. After the buyout it was known as Norprint Labelling Systems.\n\nIn 2005 it developed the \"Nortag\", a small inconspicuous RFID label on products to combat shoplifting, which won in April 2005. It is known as electronic article surveillance (EAS). The Nortag took seven years to develop.\n\nCustomers include the New York Metro, Dublin Bus, Valencia Metro (Metrovalencia), Sydney Rail and London Underground.\n\nOn 7 December 2000, Magnadata was visited by Prince Edward, Duke of Kent, in his capacity as Vice-Chairman of British Trade International, being accompanied by Bridget Cracroft-Eley.\n\nIn February 2006 Magnadata, also based in Boston, bought Norprint. Norprint had around 220 staff and Magnadata had around 200.\n\nIn 1987 Magnadata was the first company to produce magnetic strip tickets for British Rail. For this it received the ; it was the first printing company to receive this award.\n\nIn August 2010 it was awarded the £24 million contract to produce the magnetic strip tickets for train tickets in the UK for the Association of Train Operating Companies (ATOC). This is for around 750 million tickets a year at over 2500 railway stations.\n\nIt is based in the north of Boston in Lincolnshire on the B1183, which follows alongside the Maud Foster Drain. Haven High Academy, a secondary school, is next door.\n\nMagnadata USA is based in Toms River, New Jersey.\n\nIn May 2002 it bought Spectron Transit Tickets Pty Ltd, of Australia, which is now Magnadata Pty, based in St Marys, New South Wales.\n\nNorprint is a subsidiary of Magnadata. It has thirty printing presses running 24 hours a day at the site in Boston, and claims to be the largest printing facility in the UK. It produces over 10 billion labels per year. Norprint claims to be the UK's largest distributor and reseller of thermal transfer ribbons for printers.\n\n\n"}
{"id": "381100", "url": "https://en.wikipedia.org/wiki?curid=381100", "title": "Magnetic ink character recognition", "text": "Magnetic ink character recognition\n\nMICR code is a character-recognition technology used mainly by the banking industry to ease the processing and clearance of cheques and other documents. The MICR encoding, called the \"MICR line\", is at the bottom of cheques and other vouchers and typically includes the document-type indicator, bank code, bank account number, cheque number, cheque amount, and a control indicator. The technology allows MICR readers to scan and read the information directly into a data-collection device. Unlike barcodes and similar technologies, MICR characters can be read easily by humans. The MICR E-13B font has been adopted as the international standard in ISO 1004:1995, but the CMC-7 font is widely used in Europe, Brazil, Mexico and some other countries.\n\nThere are two major MICR fonts in use: E-13B and CMC-7. E-13B has a 14 character set, while CMC-7 has 15—the 10 numeric characters along with control characters.\n\nThe MICR E-13B font is the standard in Australia, Canada, the United Kingdom, the United States, and other countries. Besides decimal digits, it also contains the following symbols:\n\n\nMajor European countries, including France and Italy, and others like Brazil and Mexico use the CMC-7 font, developed by Groupe Bull in 1957.\n\nMICR characters are printed on documents in either of the MICR fonts. The ink used in the printing is a magnetic ink or toner, usually containing iron oxide. The MICR text is passed before a MICR reader. The ink in the plane of the paper is first magnetized. Then the characters are passed over a MICR reader head, a device similar to the playback head of a tape recorder. As each character passes over the head it produces a unique waveform that can be easily identified by the system.\n\nThe use of MICR allows the characters to be read reliably even if they have been overprinted or obscured by other marks, such as cancellation stamps and signature. The error rate for the magnetic scanning of a typical cheque is smaller than with optical character recognition systems. For well printed MICR documents, the \"can't read\" rate is usually less than 1% while the substitution rate (misread rate) is in the order of 1 per 100,000 characters.\n\nThus, when the cheque is inserted in the MICR reader (also called a cheque sorting machine), it can read the MICR code even if there are other marks or stamps on it. Thus, the machine easily finds out which bank branch and location the cheque belongs to. Eventually, the cheques are sorted bank, branch and location wise by MICR reader automatically based on the MICR code. Sorting of cheques is done as per the geographical coverage of banks in a nation. \nMICR characters were added to the Unicode Standard in June 1993 with the release of version 1.1.\n\nThe Unicode block that includes MICR characters is called Optical Character Recognition and covers U+2440–U+245F:\nBefore the mid-1940s, cheques were processed manually using the Sort-A-Matic or Top Tab Key method. The processing and clearance of cheques was very time consuming and was a significant cost in cheque clearance and bank operations. As the number of cheques increased, ways were sought for automating the process. Standards were developed to ensure uniformity in financial institutions. By the mid-1950s, the Stanford Research Institute and General Electric Computer Laboratory had developed the first automated system to process cheques using MICR. The same team also developed the E13B MICR font. \"E\" refers to the font being the fifth considered, and \"B\" to the fact that it was the second version. The \"13\" refers to the 0.013 inch character grid.\n\nIn 1958, the American Bankers Association (ABA) adopted E13B font as the MICR standard for negotiable documents in the United States. By the end of 1959, the first cheques had been printed using MICR. The ABA adopted MICR as its standard because machines could read MICR accurately, and MICR could be printed using existing technology. In addition, MICR remained machine readable, even through overstamping, marking, mutilation and more.\n\nMICR technology has been adopted in many countries, with some variations. In 1963, ANSI adopted the ABA’s E13B font as the American standard for MICR printing. Although compliance with MICR standards is voluntary in the United States, their use with cheques is almost universal. E13B MICR has also been standardized as ISO 1004:1995. The E13B font was adopted as the standard in the United States, Canada, United Kingdom, Australia and other countries.\n\nThe CMC-7 font was developed in France by Groupe Bull in 1957. It was adopted as the MICR standard in Argentina, France, Italy, and some other European countries.\n\nIn the 1960s, the MICR fonts became a symbol of modernity or futurism, leading to the creation of lookalike \"computer\" typefaces that imitated the appearance of the MICR fonts, which unlike real MICR fonts, had a full character set.\n\nMICR, or E-13B, is also used to encode information in other applications like: sales promotions, coupons, credit cards, airline tickets, insurance premium receipts, deposit tickets, and more.\n\nE13b is the version specifically developed for Offset Litho printing. There was a subtly different version for letterpress, called E13a. Also, there was a Rival system named 'Fred' (Figure Reading Electronic Device) which used figures that looked more conventional.\n\n\n"}
{"id": "47906367", "url": "https://en.wikipedia.org/wiki?curid=47906367", "title": "Margaret Taber", "text": "Margaret Taber\n\nMargaret R. Taber (April 29, 1935 – June 10, 2015) was a pioneer for women in engineering. She was an electrical and electronics engineering educator. She was the author of several nonfiction books and articles on computer programming. She has had computer labs named in her honor. She has established scholarships in her name.\n\nTaber was born in St. Louis, Missouri, as the only child to Wynn Orr and Margaret Ruth (Feldman) Stevens. Her father was an electrical engineer. Her mother was a department store clerk. They divorced and her mother raised her alone during World War II.\n\nThey moved to Cleveland, Ohio when Taber was in the sixth grade. Taber described her child-self as a tomboy. She had played with toy soldiers/sailors, pretended war, and owned a sailor suit. Before attending Wilbur Wright Junior High School, Tabor discovered the value of hard work. Tabor's favorite class at this point was physical education; she wanted to be a gym teacher.\n\nAt South High School in Akron, Ohio, Taber was inspired by many teachers. There was a math teacher who seated students according to their score on the math tests. Instead of intimidating Taber, it inspired her to compete for the first row. Taber was inspired by another unnamed teacher, who was so dedicated to their future in college that she taught physics at her own expense. The physics class was not provided by the school. Her mechanical drawing teacher allowed her to progress at her own rate. During Taber's time in high school, she decided on electrical engineering, because she believed it \"took math the furthest of all the engineering\".\n\nIn 1958, Margaret graduated from Cleveland State University in Cleveland, Ohio. She had two bachelor's degrees in electrical engineering and engineering science, with an emphasis on mathematics. In 1967, Taber obtaining her Master of Science in Engineering from the University of Akron. In 1976, Margaret received an Ed.D. from Nova Southeastern University in Fort Lauderdale, Florida.\n\nTaber was a licensed professional engineer and a certified engineering technologist.\n\nTaber began her career at the Tocco Division of The Ohio Crankshaft Company as an engineering trainee and Development Engineer. Briefly, she was a digital systems consultant for Design and Development Inc. Then, Taber began working in academia as an instructor in Electrical-Electronic Engineering at Cuyahoga Community College in Cleveland, Ohio. She went on to become an assistant, associate, and full professor as well as Chairperson of Engineering Technologies at the College. In 1979, she became an Associate Professor at Purdue University and an Educational Consultant and Writer for the Cleveland Institute of Electronics.\n\nTaber was the only woman faculty member of Purdue University's Department of Electrical Engineering Technology. She received tenure as an associate professor, was made full professor (1983-2000), and became a Professor Emeritus in 2000.\n\nTaber has designed, developed, and taught many fundamental and advanced microprocessor courses. She has also written several books/manuals and articles on computer programming.\n\nTaber was extremely supportive of women in engineering.\n\nWhen Taber retired, Purdue University dedicated the Margaret R. Taber Microcomputer Lab in her honor.\n\nIn 1993, she established the Dr. Margaret R. Taber Scholarship for Women in Engineering to provide for outstanding women engineering students at the University of Akron. In 2007, she established a second scholarship fund for women in Electronic Engineering Technology.\n\nIn 1987, Dr. Taber received the 1987 Distinguished Engineering Educator Award from the Society of Women Engineers. In 1991, she was honored with the Helen B. Schleman Gold Medallion Award at Purdue University. She also received the Survivorship Award from the Greater Lafayette community. In 1994, she received the Outstanding Alumni Award from the University of Akron. In 2002, Taber received the Lifetime Leadership award from Cleveland State University.\n\nIn 2009, Taber received the President’s Award for Leadership, Volunteer Service & Philanthropy from Rainbow Acres in Arizona. In 2012, Margaret was honored with the YWCA Woman of Distinction Award for her \"willingness to go the extra mile, extreme tenacity, enthusiasm for life and reliability, a true inspiration for many.\"\n\nMargaret married William J. Taber on September 6, 1958. Her husband's death preceded hers by 2 days, on June 8, 2015.\n\nDr. Taber spent her retirement volunteering for organizations such as the American Cancer Society, CanSurmount 1993-1998, and the computer lab of the Lafayette Adult Reading Academy. Taber was a dedicated member of the Federated Church for 35 years. She had a passion for mission work. She volunteered at Rainbow Acres in Arizona for 25 years to help adults with developmental disabilities, including the creation and ongoing support of a computer lab and the teaching of computer skills.\n"}
{"id": "3037149", "url": "https://en.wikipedia.org/wiki?curid=3037149", "title": "Nanocar", "text": "Nanocar\n\nThe nanocar is a molecule designed in 2005 at Rice University by a group headed by Professor James Tour. Despite the name, the original nanocar does not contain a molecular motor, hence, it is not really a car. Rather, it was designed to answer the question of how fullerenes move about on metal surfaces; specifically, whether they roll or slide (they roll).\n\nThe molecule consists of an H-shaped 'chassis' with fullerene groups attached at the four corners to act as wheels.\n\nWhen dispersed on a gold surface, the molecules attach themselves to the surface via their fullerene groups and are detected via scanning tunneling microscopy. One can deduce their orientation as the frame length is a little shorter than its width.\n\nUpon heating the surface to 200 °C the molecules move forward and back as they roll on their fullerene \"wheels\". The nanocar is able to roll about because the fullerene wheel is fitted to the alkyne \"axle\" through a carbon-carbon single bond. The hydrogen on the neighboring carbon is no great obstacle to free rotation. When the temperature is high enough, the four carbon-carbon bonds rotate and the car rolls about. Occasionally the direction of movement changes as the molecule pivots. The rolling action was confirmed by Professor Kevin Kelly, also at Rice, by pulling the molecule with the tip of the STM.\n\nThe concept of a nanocar built out of molecular \"tinkertoys\" was first hypothesized at the Fifth Foresight Conference on Molecular Nanotechnology (November 1997). Subsequently, an expanded version was published in Annals of Improbable Research. These papers supposed to be a not-so-serious contribution to a fundamental debate on the limits of bottom-up Drexlerian nanotechnology and conceptual limits of how far mechanistic analogies advanced by Eric Drexler could be carried out. The important feature of this nanocar concept was the fact that all molecular component tinkertoys were known and synthetized molecules (alas, some very exotic and only recently discovered, e.g. staffanes, and notably – ferric wheel, 1995), in contrast to some Drexlerian diamondoid structures that were only postulated and never synthesized; and the drive system that was embedded in a ferric wheel and driven by inhomogeneous or time-dependent magnetic field of a substrate – an \"engine in a wheel\" concept.\n\nThe Nanodragster, dubbed the world's smallest hot rod, is a molecular nanocar. The design improves on previous nanocar designs and is a step towards creating molecular machines. The name comes from the nanocar's resemblance to a dragster, as it has a shorter axle with smaller wheels in the front and a larger axle with larger wheels in the back.\n\nThe nanocar was developed at Rice University’s Richard E. Smalley Institute Nanoscale Science and Technology by the team of James Tour, Kevin Kelly and other colleagues involved in its research. The previous nanocar developed was 3 to 4 nanometers which was a little over a strand of DNA and was around 20,000 times thinner than a human hair. These nanocars were built with carbon buckyballs for their four wheels, which made it need to get it moving. On the other hand, a nanocar which utilized p-carborane wheels moves as if on ice. Such observations led to the production of nanocars which had both wheel designs.\n\nThe Nanodragster is 50,000 times thinner than a human hair and has a top speed of 0.014 millimeters per hour (0.0006 in/h). The rear wheels are spherical fullerene molecules, or buckyballs, composed of sixty carbon atoms each, which are attracted to a dragstrip that is made up of a very fine layer of gold. This design also enabled Tour’s team to operate the device at lower temperatures.\n\nThe nanodragster and other nano-machines are designed for use in transporting items. The technology can be used in manufacturing computer circuits and electronic components, or in conjunction with pharmaceuticals inside the human body. Tour also speculated that the knowledge gained from the nanocar research would help build efficient catalytic systems in the future.\n\nKudernac \"et al.\" described a specially designed molecule that has four motorized \"wheels\". By depositing the molecule on a copper surface and providing them with sufficient energy from electrons of a scanning tunnelling microscope they were able to drive some of the molecules in a specific direction, much like a car, being the first single molecule capable to continue moving in the same direction across a surface. Inelastic electron tunnelling induces conformational changes in the rotors and propels the molecule across a copper surface. By changing the direction of the rotary motion of individual motor units, the self-propelling molecular 'four-wheeler' structure can follow random or preferentially linear trajectories. This design provides a starting point for the exploration of more sophisticated molecular mechanical systems, perhaps with complete control over their direction of motion.\nThis electrically driven nanocar was built under supervision of University of Groningen chemist Bernard L. Feringa, who was awarded the Nobel Prize for Chemistry in 2016 for his pioneering work on nanomotors, together with Jean-Pierre Sauvage and J. Fraser Stoddart.\n\nA future nanocar with a synthetic molecular motor has been developed by Jean-Francois Morin \"et al.\" It is fitted with carborane wheels and a light-powered helicene synthetic molecular motor. Although the motor moiety displayed unidirectional rotation in solution, light-driven motion on a surface has yet to be observed. Mobility in water and other liquids can be also realized by a molecular propeller in the future.\n\n"}
{"id": "53182111", "url": "https://en.wikipedia.org/wiki?curid=53182111", "title": "Needlecase", "text": "Needlecase\n\nA needlecase or needle case is a small, often decorative, holder for sewing needles. Early needlecases were usually small tubular containers of bone, wood, or bronze with tight-fitting stoppers, often designed to hang from a belt. Needlecases are sometimes called by the French name \"étui\" and are typically one of the tools attached to a chatelaine. A pin poppet is a similar container for pins, common in the 18th century.\n\nEarly sewing needles were precious items and easily lost. Needlecases were a necessity for storing these fragile objects, and are found in cultures around the world. Tubular bronze needlecases are common finds from Viking-age sites in Europe. Cane needlecases were found in a grave from Cerro Azul, Peru, dated to 1000–147. Bone, leather, and metal needlecases have been found from Medieval London, and bone or ivory needlecases were made by the Inuit people. Bone and ivory needlecases and pin poppets were also popular in 18th century America.\n\nElaborate needlework confections like the frog-shaped needlecase in the Los Angeles County Museum of Art appeared by the 16th century. Heavily decorated silver and brass needlecases are typical of the Victorian period. \n\nBetween 1869 and 1887, W. Avery & Son, an English needle manufactory, produced a series of figural brass needlecases, which are now highly collectible. Avery's dominance of this market was such that all similar brass Victorian needlecases are called \"Averys\".\n\n Needlecases in museum collections\n"}
{"id": "4890180", "url": "https://en.wikipedia.org/wiki?curid=4890180", "title": "O'Sullivan v Noarlunga Meat Ltd (No 2)", "text": "O'Sullivan v Noarlunga Meat Ltd (No 2)\n\nO'Sullivan v Noarlunga Meat Ltd (No 2), was a High Court of Australia case, in which a certificate, under s 74 of the Australian Constitution, was sought for leave to appeal to the Privy Council against the previous decision of \"O'Sullivan v Noarlunga Meat Ltd\".\n\nIn the preceding case, it was held that the Commonwealth's extensive regulations regarding premises used for the slaughtering of livestock for export were valid under s 51(i) of the Constitution. In June 1955 the Privy Council gave special leave to appeal except to the extent they required a certificate of appeal under section 74. O'Sullivan applied to the High Court for a certificate of appeal.\n\nDixon CJ, Williams, Webb and Fullagar JJ wrote a joint judgment refusing a certificate of appeal stating that the policy of section 74 was to confine the decision of essentially federal questions to the High Court. McTiernan, Kitto and Taylor JJ each delivered concurring judgments.\n\nThe Privy Council took a narrower view of section 74 than the High Court, holding that the question of whether laws were inconsistent involved the application of section 109 of the Constitution and did not involve a question in relation to the constitutional powers of the Commonwealth and the States. The Privy Council dismissed the appeal, approving the earlier decision of the statutory majority of the High Court, particularly the judgment of Fullager J.\n\n\n"}
{"id": "10853262", "url": "https://en.wikipedia.org/wiki?curid=10853262", "title": "Ostomy pouching system", "text": "Ostomy pouching system\n\nAn ostomy pouching system\n\nPouching systems usually consist of a collection pouch plastic bag, known as a one-piece system or, in some instances involves a mounting plate, commonly called a flange, wafer or a baseplate, and a collection pouch that is attached mechanically or with an adhesive in\nan airtight seal, known as a two-piece system. The selection of systems varies greatly between individuals and is often based on personal preference and lifestyle. Ostomy pouching systems collect waste that is output from a stoma. The pouching system allows the stoma to drain into a sealed collection pouch, while protecting the surrounding skin from contamination.\n\nOstomy pouching systems are air- and water-tight and allow the wearer to lead an active lifestyle that can include all forms of sports and recreation.\n\nOstomy pouching systems are also sometimes referred to as an appliance, where the term appliance refers to a prosthesis, as a mechanical replacement for a biological function.\n\nMost wafers/baseplates, also called ostomy barriers, are manufactured using pectin or similar organic material and are available in a wide variety of sizes to accommodate a person's particular anatomy.\n\nThe internal opening must be the correct size to accommodate the individual's stoma while protecting the skin from contact with waste. The methods for sizing this opening vary depending on the type of wafer/baseplate; some pre-cut sizes are available, some users customize the opening using scissors. Manufacturers have recently introduced moldable wafers than can be shaped by hand without the need for scissors.\n\nSkin adhesion for modern wafers/baseplates/flanges are optimized on all the five parameters required in an adhesive:\n\n\nIn addition, modern wafers/baseplates/flanges with adhesive border can provide additional security that the system stays in place. Using a barrier film spray before applying a new flange will improve adhesion, soothe irritated skin and protect the skin from irritation.\n\nA wafer/baseplate/flange may last between one and many days before it needs to be replaced; this is highly dependent on the individual's lifestyle, ostomy type, and anatomy.\n\nThe method of attachment to the wafer varies between manufactures and includes permanent (one-piece), press-on/click (\"Tupperware\" type), turning locking rings and \"sticky\" adhesive mounts. The two-piece arrangement allows pouches to be exchanged without removing the wafer; for example, some people prefer to temporarily switch to a \"mini-pouch\" for swimming, intimate and other short-term activities. Mini-pouches are suitable for minimum usage only.\n\nPouches can be divided into two basic types: open-end (drainable) and closed-end (disposable).\n\nThe use of open-end vs. closed-end pouches is dependent on the frequency in which an individual needs to empty the contents, as well as economics.\n\nGas is created during digestion, and an airtight pouch will collect this and inflate. To prevent this some pouches are available with special charcoal filtered vents that will allow the gas to escape, and prevent ballooning at night. Some odor can be expelled through the charcoal filter especially if sufficient deodorant is not used in the pouch.\n\nPouch covers are helpful to disguise the plastic pouch when it is exposed when reaching or other physical activity. These are usually made of cloth and can be decorative or plain to blend in with clothing. Various sources stock sizes for most manufacturers pouches. There are flexible elastic pouch belts available for extreme physical activity but some of these require the pouch to be worn sideways so it does not fill properly and the tight fit causes pancaking of the effluent.\n\n\n"}
{"id": "8806722", "url": "https://en.wikipedia.org/wiki?curid=8806722", "title": "Planar Hall sensor", "text": "Planar Hall sensor\n\nThe planar Hall sensor is based on the planar Hall effect of ferromagnetic materials. \n\nIt measures the change in anisotropic magnetoresistance caused by an external magnetic field in the Hall geometry. The sensor responds to magnetic field components in the sensor plane as opposed to the ordinary Hall sensor, which measures field components perpendicular to the sensor plane. Hence the name \"planar Hall\". Generally speaking, for ferromagnetic materials, the resistance is larger when the current flows along the direction of magnetization than when it flows perpendicular to the magnetization vector. This creates an asymmetric electric field perpendicular to the current, which depends on the magnetization state of the sensor. Exactly controlling the magnetization state is the key to the operation of the planar Hall sensor. From fabrication the magnetization is confined to one certain direction in zero applied field, and the application of a field perpendicular to this direction changes the magnetization state in such a way that the electronic readout is linear with respect to the magnitude of the applied field. This is true for applied fields smaller than a fourth of the intrinsic effective anisotropy field (see ref. 1 for details on the working principle).\n\nThe planar Hall sensor has been demonstrated as a magnetic bead detector and to measure the Earth's field with nanotesla precision As a magnetic bead sensor, the planar Hall sensor can be used as sensing principle in a magnetic bioassay. In ref. 5 detection of influenza virusses was demonstrated using an immunoassay imitating a sandwich ELISA based on monoclonal antibodies. \n"}
{"id": "31942390", "url": "https://en.wikipedia.org/wiki?curid=31942390", "title": "PrimerDesign", "text": "PrimerDesign\n\nPrimerdesignis a UK-based biotechnology company that designs and sells products for quantitative real-time polymerase chain reaction (qPCR). The company was established in 2005 in Southampton, United Kingdom.\n\nPrimerdesign was founded by Dr Jim Wicks, Dr Rob Powell and Professor Tom Brown within the University of Southampton to share their expertise in real-time PCR and DNA chemistry. The company has grown since then and its products have been widely used in over 100 countries. The company has a portfolio of over 400 qPCR detection kits and over 9000 research targets.\n\nDuring the Swine Flu pandemic in 2009, Primerdesign developed the world's first swine flu detection kit. The kit which was designed within a fortnight is considered a breakthrough in the sector. The one step detection kit is designed to give results within two hours. The first shipment of rapid result tests was dispatched to Mexico in April 2009 and is now being used in more than 30 countries across the globe.\n\nDue to their extensive range of products, the company played a part in the 2013 meat adulteration scandal by being able to provide qPCR kits to quantitatively measure for horse meat in food.\nTheir kits were used throughout Ireland as well as the UK.\n\nAmong their product ranges are lines for:\n\n\nPrimerdesign has a worldwide presence, serving over 9,000 customers. Its core customer segments include academic institutions, pharmaceutical companies, independent diagnostic laboratories, hospitals, official government health agencies, and others.\n\nIn 2006, Primerdesign initiated a sponsorship program to reward meritorious PhD students and provide further assistance to the PCR component of their study.\n\nReal-time polymerase chain reaction\nHigh Resolution Melt\n\n"}
{"id": "44467079", "url": "https://en.wikipedia.org/wiki?curid=44467079", "title": "Production planning", "text": "Production planning\n\nProduction planning is the planning of production and manufacturing modules in a company or industry. It utilizes the resource allocation of activities of employees, materials and production capacity, in order to serve different customers.\n\nDifferent types of production methods, such as single item manufacturing, batch production, mass production, continuous production etc. have their own type of production planning. Production planning can be combined with production control into production planning and control, or it can be combined .enterprise resource planning.\n\nProduction planning is used in companies in several different industries, including agriculture, industry, amusement industry, etc...\n\nProduction planning is a future of production that can be manufacture or company can be set in which the facilities needed are determined and arranged. A production plan is made periodically for a specific time period, called the planning horizon. It can comprise the following activities:\n\nIn order to develop production plans, the production planner or production planning department needs to work closely together with the marketing department and sales department. They can provide sales forecasts, or a listing of customer orders.\" The \"work is usually selected from a variety of product types which may require different resources and serve different customers. Therefore, the selection must optimize customer-independent performance measures such as cycle time and customer-dependent performance measures such as on-time delivery.\"\n\nA critical factor in production planning is \"the accurate estimation of the productive capacity of available resources, yet this is one of the most difficult tasks to perform well\". Production planning should always take \"into account material availability, resource availability and knowledge of future demand\".\n\nModern production planning methods and tools have been developed since late 19th century. Under Scientific Management, the work for each man or each machine is mapped out in advance (see image). The origin of production planning back goes another century. Kaplan (1986) summarized that \"the demand for information for internal planning and control apparently arose in the first half of the 20th century when firms, such as textile mills and railroads, had to devise internal administrative procedures to coordinate the multiple processes involved in the performance of the basic activity (the conversion of raw materials into finished goods by textile mills, the transportation of passengers and freight by the railroads.\"\n\nHerrmann (1996) further describes the circumstances in which new methods for internal planning and control evolved: \"The first factories were quite simple and relatively small. They produced a small number of products in large batches. Productivity gains came from using interchangeable parts to eliminate time-consuming fitting operations. Through the late 1800s, manufacturing firms were concerned with maximizing the productivity of the expensive equipment in the factory. Keeping utilization high was an important objective. Foremen ruled their shops, coordinating all of the activities needed for the limited number of products for which they were responsible. They hired operators, purchased materials, managed production, and delivered the product. They were experts with superior technical skills, and they (not a separate staff of clerks) planned production. Even as factories grew, they were just bigger, not more complex.\n\nAbout production planning Herrmann (1996) recounts that \"production scheduling started simply also. Schedules, when used at all, listed only when work on an order should begin or when the order is due. They didn't provide any information about how long the total order should take or about the time required for individual operations ...\"\n\nIn 1923 \"Industrial Management\" cited a Mr. Owens who had observed: \"Production planning is rapidly becoming one of the most vital necessities of management. It is true that every establishment, no matter how large or how small has production planning in some form; but a large percentage of these do not have planning that makes for an even flow of material, and a minimum amount of money tied up in inventories.\"\n\nDifferent types of production planning can be applied:\n\nRelated kind of planning in organizations\n\nProduction control is the activity of controlling the workflow in the production. It is partly complementary to production planning.\n\n"}
{"id": "8171428", "url": "https://en.wikipedia.org/wiki?curid=8171428", "title": "Ron Brown Award", "text": "Ron Brown Award\n\nThe Ron Brown Award for Corporate Leadership is a U.S. presidential honor to recognize companies \"for the exemplary quality of their relationships with employees and communities\". It is presented to companies that \"have demonstrated a deep commitment to innovative initiatives that not only empower employees and communities but also advance strategic business interests\".\n\nPresident Bill Clinton, along with prominent business leaders, established the award in 1997 following Ron Brown's death in a plane crash in 1996. The award was originally referred to as Ron Brown Corporate Citizenship Award. The Conference Board, a non-profit organization, was chosen to manage the award's administration. Ron Brown was the U.S. Secretary of Commerce from 1993–1996; he was the first African-American to hold that position. This award and the Malcolm Baldrige National Quality Award are the two presidential awards to corporations.\n\nThe award is presented in an annual White House ceremony, either by the President or by the Secretary of Commerce.\n\n\n\nhttps://www.jnj.com/media-center/press-releases/the-amazing-nurses-contest-celebrating-americas-caregivers\n\n\n\n\n\n\n\n"}
{"id": "32543456", "url": "https://en.wikipedia.org/wiki?curid=32543456", "title": "Schoology", "text": "Schoology\n\nSchoology is a learning management system (LMS) for K-12 schools, higher education institutions, and corporations that allows users to create, manage, and share content and resources. Also known as a course management system (CMS) or virtual learning environment (VLE), the cloud-based platform provides tools to manage any classroom or blended learning environment.\n\nThe Schoology platform was designed by Jeremy Friedman, Ryan Hwang and Tim Trinidad in 2007 while still undergraduates at Washington University in St. Louis, MO. Originally designed for sharing notes, additional features and functionality continue to be added.\n\nSchoology secured its first institutional round of venture capital funding amounting to $1.25M from Meakam Becker Venture Capital in June 2010, following an angel investment in 2009 from an unnamed investor. In 2012, Schoology raised $6M in a round led by Firstmark Capital, and in 2014, a $15M round of funding was led by Intel Capital.\n\nAs of November 2014, Schoology has over 9.5 million users across 100,000 schools in 200 countries around the world.\n\nIn November 2015, Schoology announced its $32 million Series D funding round, led by JMI Equity.\n\nVisually familiar to users of Facebook and other popular social networking websites, the service includes attendance records, online gradebook, tests and quizzes, and homework dropboxes. The social media interface facilitates collaboration among a class, a group, or a school. Schoology can be integrated with existing school reporting and information systems and also provides the added security, filters and support that school districts may require.\n\nSchoology is offered to educators free of charge. Revenue is generated with a fee-based Enterprise product that includes premium add-ons such as customized branding, advanced analytics, single sign on (SSO), and data integration with existing student information systems (SIS). Native mobile applications are available for iOS, Android, and Kindle devices.\n\nProduct enhancements have included text message notifications, integrations with Google Drive, Dropbox, and Evernote, IMS Global learning tools interoperability (LTI), educational and non-educational apps, a shared resources library, mastery analytics and a question importer for tests and quizzes.\nCurrent districts and other users using Schoology include Baltimore County Public Schools San Juan USD, Colorado State University-Global Campus, Department of Defense Education Activity, Uruguay's Plan Ceibal, Greenwich Public Schools, Judson Independent School District, Westminster Christian Academy, Palo Alto Unified School District, Cempaka Schools in Malaysia, the Downingtown STEM Academy, Rocklin Unified School District, San Diego Unified School District, Seaman School District, Jefferson County School District, Cincinnati Public Schools, Cabarrus County Schools, Los Angeles Unified School District, Mercer Island School District, Falls Church City Public Schools, Grosse Pointe Public School System and Australian Christian College (for both On campus & Distance Education students).\n\n"}
{"id": "7016507", "url": "https://en.wikipedia.org/wiki?curid=7016507", "title": "Seadrill", "text": "Seadrill\n\nSeadrill is a deepwater drilling contractor that provides drilling services to the oil and gas industry. It is incorporated in Bermuda and managed from London. Seadrill has operations in Angola, Brunei, the Republic of Congo, Indonesia, Malaysia, Nigeria, Norway, Thailand, Brazil, Saudi Arabia, UAE, United States and the United Kingdom.\n\nThe company operates semi-submersibles, jack-ups and drillships. The company provides quarterly online updates on the status of each rig in its fleet on its website.\n\nSeadrill was incorporated as a Bermuda company in May 2005 by the Norwegian-born shipping tycoon John Fredriksen and listed on the Oslo Stock Exchange in November 2005.\n\nIn July 2005, the company acquired the drilling company Odfjell.\n\nIn September 2006, the company acquired a controlling share of Smedvig by outbidding Noble Corporation.\n\nIn 2007, the company acquired Eastern Drilling.\n\nIn May 2010, the company outbid Ensco plc for Scorpion Offshore, valuing the company at $567 million.\n\nIn 2014, Seadrill conducted an initial public offering of its North Atlantic Drilling subsidiary, which owns harsh environment rigs. Seadrill continued to own 70% of North Atlantic Drilling.\n\nIn April 2013, the company sold the majority of its tender rig and semi-tender operation to SapuraKencana.\n\nIn May 2013, the company sold Tender rigs \"T-15\", \"T16\" and the \"West Vencedor\" to Seadrill Partners LLC.\n\nIn June 2013, the company acquired a majority interest in Sevan Drilling.\n\nIn November 2014, the company suspended dividend payments as a result of a downturn in the industry.\n\nIn September 2017, the company filed for bankruptcy protection.\n\nIn 2015, Petrobras alleged bribery in the negotiations of rig contracts signed by Sevan Drilling, a subsidiary of Seadrill, in 2005-2008.\n\nIn November 2009, Seadrill's West Atlas jackup rig, which was located on Montara field, about 690 kilometers offshore of Darwin, Australia, caught fire after the field had been leaking oil for ten weeks. The rig was operated by the Thai firm PTTEP. For 74 days, gas and oil ran into the Timor Sea, in what was one of the worst environmental disasters in Australian history.\n"}
{"id": "11925029", "url": "https://en.wikipedia.org/wiki?curid=11925029", "title": "Self-service software vendors", "text": "Self-service software vendors\n\nSelf-service software vendors is a subset within the self-service software category which highlights a range of software vendors that specialize in the knowledgement management, decision support, expert systems, business process management (BPM), rule and logic engines and taxonomies.\n\nSelf-service software vendors allows authors (typically subject matter experts or agents) to readily automate the deployment of, the timeliness of and compliance around a variety of processes of which they are involved in communicating without having to physically address the questions, needs and solicitations of end users who are inquiring about the particular process being automated. \n\nMost self-service software addresses \"closed-loop\" inquiries whereby the author emulates a variety of known (finite) questions and related (known) responses on hand or required steps that must be addressed to derive and deliver a final answer or directive. Often the author using such software codifies such known processes and steps and then generates (publishes) end-user facing applications which can encompass a variety of code bases and platforms.\n\n\n"}
{"id": "30958622", "url": "https://en.wikipedia.org/wiki?curid=30958622", "title": "Service Desk Institute", "text": "Service Desk Institute\n\nThe Service Desk Institute, founded in 1988, is a worldwide professional organisation for those working in the IT service and support industry. SDI delivers information to help improve the knowledge of help desk professionals through IT support training, community based forums and event-based services. Their stated goals are to help individuals improve their knowledge and skills to enhance the careers of IT service professionals. \n\nSDI was first named The Help Desk User Group and was founded by Howard Kendall. The company later became known as the Help Desk Institute. In 2008 the company undertook a re-branding exercise and on 9 April 2008 changed its name to The Service Desk Institute. \n\n\nSDI's Standards for Service Desks and for IT Service professionals\n\nSDI's Free Online Service Desk Assessment\n\nService Desk Certification, A Pocket Guide \n\nIT Service Buyer's Guide\n\nIT Service & Support Awards\n\nThe SDI Service Desk Benchmarking Trends Report\n\nFuture IT Service Trends: The Service Desk in 2017 & Beyond\n\nSDI Events Calendar: Inspiring one-day events for IT service professionals who want to improve their service desk\n\nService Desk Analyst Qualification\n\nService Desk Manager Qualification\n\n"}
{"id": "2121650", "url": "https://en.wikipedia.org/wiki?curid=2121650", "title": "Signal lamp", "text": "Signal lamp\n\nA signal lamp (sometimes called an Aldis lamp, after Arthur Cyril Webb Aldis who invented a widely used design, or a Morse lamp) is a visual signaling device for optical communication, typically using Morse code. Modern signal lamps are focused lamps which can produce a pulse of light. In large versions, this pulse is achieved by opening and closing shutters mounted in front of the lamp, either via a manually operated pressure switch or, in later versions, automatically. With hand held lamps, a concave mirror is tilted by a trigger to focus the light into pulses. The lamps were usually equipped with some form of optical sight, and were most commonly used on naval vessels and in airport control towers (using color signals for stop or clearance). In manual signaling, a signalman would aim the light at the recipient ship and turn a lever, opening and closing the shutter over the lamp, to emit flashes of light to spell out text messages in Morse code. On the recipient ship, a signalman would observe the blinking light, often with binoculars, translating the Morse code into text.\n\nSignal lamps were pioneered by the Royal Navy in the late 19th century and continue to be used through the present day, on naval vessels. They provide handy, secure communications, which is especially useful during periods of radio silence and were particularly useful for convoys operating during the Battle of the Atlantic. \n\nThere are several types. Some signal lamps are mounted on the mastheads of ships, some small hand-held versions are also used (e.g., the 'Aldis lamp'); other more powerful versions are mounted on pedestals. These larger ones use a carbon arc lamp as their light source, with a diameter of 20 inches (50 cm). These can be used to signal to the horizon, even in conditions of bright sunlight. Although it was originally thought that it was only possible to communicate by line-of-sight, in practice it is possible to illuminate cloud bases both during the night and day, which allow for communication beyond the horizon. \n\nThe maximum transmission rate possible via flashing lights is no more than 14 wpm. They have a secondary function as simple spotlights.\n\nThe idea of flashing dots and dashes from a lantern was first put into practice by Captain, later Vice Admiral, Philip Colomb, of the British Royal Navy, in 1867. His original code, which the Navy used for seven years, was not identical with Morse's, but Morse code was eventually adopted with the addition of several special signals. \n\nAnother signalling lamp was the Begbie lamp, a kerosene lamp with a lens to focus the light over a long distance.\n\nFlashing lights were the second generation of signalling in the Royal Navy, after the flag signals most famously used to spread Nelson's rallying-cry before the Battle of Trafalgar.\n\nDuring the trench warfare of World War I when wire communications were often cut, German signals used three types of optical Morse transmitters, called \"\", the intermediate type for distances of up to 4 km (2.5 miles) at daylight and of up to 8 km (5 miles) at night, using red filters for undetected communications.\n\nThe Commonwealth Navies and NATO forces use signal lamps when radio communications need to be silent or electronic \"spoofing\" is likely. Also, given the prevalence of night vision equipment in today's armed forces, signaling at night is usually done with lights that operate in the infrared (IR) spectrum, making them less likely to be detected. All modern forces have followed suit due to technological advances in digital communications.\n\nIn air traffic control towers, signal lamps are still used today, as a backup device in case of a complete failure of an aircraft's radio. Light signals can be green, red, or white, and steady or flashing. Messages are limited to a handful of basic instructions (e.g., \"land\", \"stop\", etc.); they are not intended to be used for transmitting messages in Morse code. Aircraft can acknowledge signals by rocking their wings or flashing their landing lights.\n\n\n"}
{"id": "35536407", "url": "https://en.wikipedia.org/wiki?curid=35536407", "title": "Space surveillance", "text": "Space surveillance\n\nSpace surveillance is the study and monitoring of satellites orbiting the earth. It involves the detection, tracking, cataloging and identification of artificial objects, i.e. active/inactive satellites, spent rocket bodies, or fragmentation debris.\n\nSpace surveillance accomplishes the following:\n\n\nSystems include:\n\n"}
{"id": "5710033", "url": "https://en.wikipedia.org/wiki?curid=5710033", "title": "TRANZ 330", "text": "TRANZ 330\n\nThe TRANZ 330 is a popular point-of-sale device manufactured by VeriFone in 1985. The most common application for these units is bank and credit card processing, however, as a general purpose computer, they can perform other novel functions. Other applications include gift/benefit card processing, prepaid phone cards, payroll and employee timekeeping, and even debit and ATM cards. They are programmed in a proprietary VeriFone TCL language (Terminal Control Language), which is unrelated to the Tool Command Language used in UNIX environments.\n"}
{"id": "43942281", "url": "https://en.wikipedia.org/wiki?curid=43942281", "title": "The Summerland Project", "text": "The Summerland Project\n\nThe Summerland Project is a stage play and subsequent movie adaptation written by Rob Merritt about a dying woman whose mind is placed into an artificial body.\n\nAmelia and Carter Summerland are a newly married couple. Amelia has an aneurysm and becomes locked in. Carter is approached by a corporation who would like to use Amelia as a test subject for a procedure where they will copy her personality and memories into an android body.\n\nThe themes of \"The Summerland Project\" have been compared to \"Pygmalion\", \"Blade Runner\" (and its source material \"Do Androids Dream of Electric Sheep?\"), \"Frankenstein\", and \"The Monkey's Paw\". The story raises questions about the nature of humanity.\n\nThe play was initially produced as part of the Theatre Cedar Rapids Underground New Play Festival in 2011. It then again appeared as a TCR mainstage production in 2013. Subsequently, the play was performed at the Olathe Civic Theatre in Kansas City, Kansas.\n\nFilming for the movie version, entitled \"Amelia 2.0\" began on September 16, 2014 in Cedar Rapids, Iowa. The film version features Ben Whitehair, Ed Begley Jr., Chris Ellis, Eddie Jemison, Kate Vernon, Debra Wilson, Kamar de los Reyes, as well as Angela Billman reprising the role of Amelia Summerland that she performed in the 2013 stage version. The film version is being directed by Adam Orton. The film is targeted for release for the 2016 film festival season. The film had a budget of $1.2 million.\n\n"}
{"id": "55818592", "url": "https://en.wikipedia.org/wiki?curid=55818592", "title": "United Income", "text": "United Income\n\nUnited Income, Inc. is an online investment management and financial planning company located in Washington D.C. The company was founded in September 2016 by Matt Fellowes, an American entrepreneur and former Brookings Institution scholar. It launched its services in September 2017. The company uses data insights to provide investment advisory services and retirement planning solutions to retired and near-retirement individuals.\n\nUnited Income is an investment firm that offers a \"hybrid service\" with access to human advisors who rely on financial software in their decision making process. The company deploys sets of statistics on investment performance, retiree spending, longevity, and other factors to simulate potential financial outcomes. Its \"unified consumer finance system\" recommends a retirement strategy based on millions of simulations for each individual. Its purpose is to offer retirees financial advice that is different from traditional financial planning software.\n\nUnited Income's team of policy experts includes former U.S. Social Security Administration Director of the Office of Policy Evaluation and Modeling (OPEM) Howard Iams; former Bureau of Labor Statistics (BLS) commissioner Erica Groshen; former Deputy Assistant Secretary of the Treasury, Office of Tax Policy Adam Looney; former Deputy Assistant Secretary of the Treasury, Office of Retirement and Health Policy Mark Iwry; and Steve Utkus, Head of the Vanguard Center for Investor Research.\n\nThe company attracted $200 million in assets under management during its beta stage. \nUnited Income raised $5.8 million in seed funding in 2016. According to the U.S. Securities and Exchange Commission (SEC) filings, Morningstar, Inc. and Fellowes, each own between 25% and 50% of United Income. A smaller stake is owned by the Omidyar Network, funded by billionaire eBay founder Pierre Omidyar.\n\nAccording to a study conducted by United Income, retirees spend less because they are pessimistic about their future financial health.\n\n"}
