{"id": "44108551", "url": "https://en.wikipedia.org/wiki?curid=44108551", "title": "ALBEDO Telecom", "text": "ALBEDO Telecom\n\nALBEDO Telecom is a company that designs and manufactures products for the telecom industry including testers, synchronization nodes and networking devices. Typical users are R&D laboratories, Mobile and Telecom operators to verify and install the infrastructures that support any kind of applications based on voice, video and data. It is headquartered in Barcelona, Spain in the European Union.\n\nALBEDO was formed by the 2010 by the former owners of ICT electronics and relevant managers that have been working in the Telecom Industry.\n\nThere are several lines including hand-hed Gigabit Ethernet, Synchronous Ethernet, E1, PDH, SDH, Ethernet, IP, Jitter, Wander, Datacom testers all in a handy battery operated set.\n\nHandy devices that emulate accurately by hardware links/networks in terms of bandwidth & traffic impairments. Bandwidth control is done by means of Traffic Shaping & Policing while packet impairments are based on delays, loss, jitter, errors and duplications.\n\nThese tools are unique and able to Emulate WANs, Filter Traffic, Capture, Storage and Tap by hardware at wirespeed in a small, compact and battery operated devices.\n\nThis market includes PTP and Sync-E testers, WAN emulators to find out the tolerance of the synchronous network under heavy traffic condition and PTP/SybcE network clocks to deliver SyncE and PTP timing services\n\nDesign of acceptance & Inter connectivity Labs to execute interoperability test suites that will ensure the performance of telecom equipment based on xDSL, VoIP, IPTV, C-Ethernet, NG-SDH, EFM, and FTTH.\n\nElectronic instruments to measure voltage, current, resistance and isolation. Instruments may include TDR functionalities for massive cable testing.\n\nSalutions based on network probes to verify the quality of Ethernet / IP networks providing multimedia an critical data services.\n\n"}
{"id": "1323819", "url": "https://en.wikipedia.org/wiki?curid=1323819", "title": "A unit", "text": "A unit\n\nAn A unit, in railroad terminology, is a diesel locomotive (or more rarely an electric locomotive) equipped with a driving cab and a control system to control other locomotives in a multiple unit, and therefore able to be the lead unit in a consist of several locomotives controlled from a single position. This terminology is generally used in North America, since only there was it commonplace to build B units—cabless locomotive units which normally could not lead a train.\n\nTypical driving cab features, and therefore A unit features, include windshields, rectangular side windows, crew seats, heating, and sometimes, radios, air conditioning and toilets. B units always lack all of these features, except that some EMD F-units have an extra porthole-style side window(s) for a hostler (an employee permitted to move a locomotive in a yard only — not on the road).\n\nThis terminology has fallen out of use for newer locomotives, since it only really applied to the cab unit style of locomotive. Thus, the term \"cab unit\" is used only when an A unit has a carbody design. Hood unit \"road switcher\" types were generally equipped with driving cabs and the term \"A unit\" was not generally applied to them, although the rare cabless road switchers were still called B units.\n\nIn some cases, A units were converted to B units. If the unit had been involved in a collision which damaged the cab, it was sometimes more cost-effective to rebuild the unit without the cab. In rarer cases, B units were converted to A units. The Chicago and North Western Railway converted several E8B units purchased from the Union Pacific Railroad. The cabs on the rebuilt units were referred to as \"Crandall Cabs.\" The BNSF also experimented with a single GP60B to make it a \"A unit\" by using an Ex-UP SD40-2 cab on a GP60B frame and body, also required to move was the Dynamic blister from the front of the unit to the middle of the unit to make room for the cab.\n\n"}
{"id": "2835857", "url": "https://en.wikipedia.org/wiki?curid=2835857", "title": "Braille translator", "text": "Braille translator\n\nA braille translator is a software program that translates a script into braille and sends it to a braille embosser, which produces a hard copy of the original print text. Only the \"script\" is transformed, not the \"language\".\n\nFor the purposes of this article, the word \"inkprint\" means text prepared for reading by the eye, whether printed, displayed on a screen, or stored in a computer; \"braille\" means text prepared for reading by the finger, whether brailled, displayed on an electronic device, or stored in a computer.\n\nBraille translation software or embedded hardware converts inkprint into braille or braille into inkprint. Usually someone has inkprint in a word processor file or at an URL and wants braille. The braille could be sent to a braille embosser to produce physical braille or to an electronic notetaker. Another circumstance is that someone has braille in an electronic braille notetaker that they want to produced in inkprint to be shared with someone who does not read braille.\n\nBraille translation software is usually classified as assistive technology, since the action of the software provides braille for a blind person. Braille translators can be run by people with or without sight.\n\nA braille translator can run on a smartphone, personal computer, network server, or \n(historically) larger mini-computers or mainframes of larger institutions.\n\nSome languages use uncontracted braille, where each letter uses a specific braille character. Uncontracted braille requires manipulation of capitalization, emphasis, numbers, and punctuation. Some languages use contracted braille, where the rules for various braille abbreviations are quite complex. For example, in contracted English braille, the word \"think\" (5 letters) is rendered as 3 characters: ⠹⠔⠅(th)(in)k. The use or non-use of these contractions is related to pronunciation. For example, the \"th\" sign is used in \"think\", but not \"pothole\". Unless properly programmed, a computer might make a mistake that no person would make, such as using the contraction for \"mother\" in the word \"chemotherapy\". The most difficult part of producing braille is making the decision of when and when not to use contractions. When people make these decisions it is \"braille transcription\"; when computers make these decisions it is \"braille translation\".\n\nThe first practical application of computer translation and production of braille used mainframe computers at the American Printing House for the Blind of Louisville, Kentuck.\n\nDuring the 1960s, there was an MIT project to automate the production of braille. Robert Mann wrote and supervised software for braille translation called DOTSYS, while another group created an embossing device which became known as the \"M.I.T. Braillemboss.\". Eventually, MIT outsourced the software work to Mitre Corporation.\nThe Mitre Corporation team of Robert Gildea, Jonathan Millen, Reid Gerhart and Joseph Sullivan (now president of Duxbury Systems) developed DOTSYS III, the first braille translator written in a portable programming language. DOTSYS III was developed for the Atlanta Public Schools as a public domain program.\n\nAt the first International Workshop on Computerized Braille Production, held in Muenster, Germany, March 1973, many braille-translation projects from around the world were described.\n\nAn archive of documents on the history of braille, braille translation, and some braille devices is maintained by Duxbury Systems.\n\n\n"}
{"id": "843067", "url": "https://en.wikipedia.org/wiki?curid=843067", "title": "Butcher paper", "text": "Butcher paper\n\nButcher paper is a kraft paper. Originally sold to butchers for the purpose of wrapping meat and fish, butcher paper is now used for a wide variety of purposes, notably in primary education where it is used for arts and crafts, such as hanging artwork. Many high schools use butcher paper for posters of clubs, and upcoming events. It is a cheap but sturdy paper that is sold in large rolls. Butcher paper is usually white or reddish in colour, made from kraft pulp and is generally considered to have a density of between 30 lb/3000 sq ft (45 g/m) and 50 lb/3000 sq ft (81 g/m).\n\nMoving companies use butchers paper to pack china, glass, and other fragile items for safe transport. The use of printed paper, such as newspaper, could transfer ink onto the packaged item causing damage.\n"}
{"id": "7853149", "url": "https://en.wikipedia.org/wiki?curid=7853149", "title": "Cinema Europe: The Other Hollywood", "text": "Cinema Europe: The Other Hollywood\n\nCinema Europe: The Other Hollywood (1995) is a documentary film series produced by David Gill and silent film historian Kevin Brownlow.\n\nThe six-part mini-series focuses on the origin of European cinema, from its infancy as a novelty created by French inventors Auguste and Louis Lumière to its flourishing as the pinnacle of film-making in the silent era and as a serious commercial contender against America (that is, until the surge of the Nazis). The important series contains much rare footage and offers an even-handed analysis of the specific strengths and weaknesses of the various national film industries during this first flourishing of film as art.\n\nThe documentary is narrated by filmmaker and actor Kenneth Branagh. Original music in the film was composed by Carl Davis, Philip Appleby & Nic Raine.\n\nThe series originally aired on the BBC in 1995, and on Turner Classic Movies in the US in 1996. In 2000, Image Entertainment released the whole series on a 2-disc DVD (3 episodes on each disc). \n\nThe documentary was shown from time to time on public television stations, usually at late night slots, due to its length and occasional sexual frankness.\n\nThe documentary is divided into the following episodes (with original BBC airdates):\n\n\n\n\n\n\n"}
{"id": "27876461", "url": "https://en.wikipedia.org/wiki?curid=27876461", "title": "Data mining in agriculture", "text": "Data mining in agriculture\n\nData mining in agriculture is a very recent research topic. It consists in the application of data mining techniques to agriculture. Recent technologies are nowadays able to provide a lot of information on agricultural-related activities, which can then be analyzed in order to find important information. A related, but not equivalent term is precision agriculture.\n\nFruit defects are often recorded (for a multitude of reasons, sometimes for insurance reasons when exporting fruit overseas). It may be done manually or through computer vision (detecting surface defects when grading fruit). Spray diaries are a legal requirement in many countries and at the very least record the date of spray and the product name. It is known that spraying can have affect different fruit defects for different fruit. Fungicidal sprays are often used to prevent rots from being expressed on fruit. It is also known that some sprays can cause russeting on apples. Currently much of this knowledge comes anecdotally, however some efforts have been in regards to the use of data mining in horticulture.\n\nWine is widely produced all around the world. The fermentation process of the wine is very important, because it can impact the productivity of wine-related industries and also the quality of wine. If the fermentation could be categorized and predicted at the early stages of the process, it could be altered in order to guarantee a regular and smooth fermentation. Fermentations are nowadays studied by using different techniques, such as, for example, the k-means algorithm, and a technique for classification based on the concept of biclustering. Note that these works are different from the ones where a classification of different kinds of wine is performed. See the wiki page Classification of wine for more details.\n\nA group method of data handling-type neural network (GMDH-type network) with an evolutionary method of genetic algorithm was used to predict the metabolizable energy of feather meal and poultry offal meal based on their protein, fat, and ash content. Published data samples were collected from literature and used to train a GMDH-type network model. The novel modeling of GMDH-type network with an evolutionary method of genetic algorithm can be used to predict the metabolizable energy of poltry feed samples based on their chemical content. It is also reported that the GMDH-type network may be used to accurately estimate the poultry performance from their dietary nutrients such as dietary metabolizable energy, protein and amino acids.\n\nThe detection of animal's diseases in farms can impact positively the productivity of the farm, because sick animals can cause contaminations. Moreover, the early detection of the diseases can allow the farmer to cure the animal as soon as the disease appears. Sounds issued by pigs can be analyzed for the detection of diseases. In particular, their coughs can be studied, because they indicate their sickness. A computational system is under development which is able to monitor pig sounds by microphones installed in the farm, and which is also able to discriminate among the different sounds that can be detected.\n\nPolymerase chain reaction-single strand conformation polymorphism (PCR-SSCP) method was used to determine the growth hormone (GH), leptin, calpain, and calpastatin polymorphism in Iranian Baluchi male sheep. An artificial neural network (ANN) model was developed to describe average daily gain (ADG) in lambs from input parameters of GH, leptin, calpain, and calpastatin polymorphism, birth weight, and birth type. The results revealed that the ANN-model is an appropriate tool to recognize the patterns of data to predict lamb growth in terms of ADG given specific genes polymorphism, birth weight, and birth type. The platform of PCR-SSCP approach and ANN-based model analyses may be used in molecular marker-assisted selection and breeding programs to design a scheme in enhancing the efficacy of sheep production.\n\nBefore going to market, apples are checked and the ones showing some defects are removed. However, there are also invisible defects, that can spoil the apple flavor and look. An example of invisible defect is the watercore. This is an internal apple disorder that can affect the longevity of the fruit. Apples with slight or mild watercores are sweeter, but apples with moderate to severe degree of watercore cannot be stored for any length of time. Moreover, a few fruits with severe watercore could spoil a whole batch of apples. For this reason, a computational system is under study which takes X-ray photographs of the fruit while they run on conveyor belts, and which is also able to analyse (by data mining techniques) the taken pictures and estimate the probability that the fruit contains watercores.\n\nRecent studies by agriculture researchers in Pakistan (one of the top four cotton producers of the world) showed that attempts of cotton crop yield maximization through pro-pesticide state policies have led to a dangerously high pesticide use. These studies have reported a negative correlation between pesticide use and crop yield in Pakistan. Hence excessive use (or abuse) of pesticides is harming the farmers with adverse financial, environmental and social impacts. By data mining the cotton Pest Scouting data along with the meteorological recordings it was shown that how pesticide use can be optimized (reduced). Clustering of data revealed interesting patterns of farmer practices along with pesticide use dynamics and hence help identify the reasons for this pesticide abuse.\n\nTo monitor cotton growth, different government departments and agencies in Pakistan have been recording pest scouting, agriculture and metrological data for decades. Coarse estimates of just the cotton pest scouting data recorded stands at around 1.5 million records, and growing. The primary agro-met data recorded has never been digitized, integrated or standardized to give a complete picture, and hence cannot support decision making, thus requiring an Agriculture Data Warehouse. Creating a novel Pilot Agriculture Extension Data Warehouse followed by analysis through querying and data mining some interesting discoveries were made, such as pesticides sprayed at the wrong time, wrong pesticides used for the right reasons and temporal relationship between pesticide usage and day of the week.\n\nA platform of artificial neural network-based models with sensitivity analysis and optimization algorithms was used successfully to integrate published data on the responses of broiler chickens to threonine. Analyses of the artificial neural network models for weight gain and feed efficiency from a compiled data set suggested that the dietary protein concentration was more important than the threonine concentration. The results revealed that a diet containing 18.69% protein and 0.73% threonine may lead to producing optimal weight gain, whereas the optimal feed efficiency may be achieved with a diet containing 18.71% protein and 0.75% threonine.\n\nSince this research topic is quite recent, there is only one reference book. \"Data Mining in Agriculture\" is published by Springer and it is co-authored by Antonio Mucherino, Petraq Papajorgji and Panos Pardalos. A quick survey of the book can be found at this address. There are a few precision agriculture journals, such as Springer's Precision Agriculture or Elsevier's Computers and Electronics in Agriculture, but those are not exclusively devoted to data mining in agriculture.\n\nThere are many conferences organized every year on data mining techniques and applications, but rather few of them consider problems arising in the agricultural field.\n"}
{"id": "54415866", "url": "https://en.wikipedia.org/wiki?curid=54415866", "title": "Dreamscape Immersive", "text": "Dreamscape Immersive\n\nDreamscape Immersive is an American entertainment and technology company. It creates story-based full-roam virtual reality (VR) experiences which allow up to six people to simultaneously explore a virtual 3D environment, seeing fully rendered avatars of one another. Using real-time motion capture technology, full body mapping, virtual reality headsets, and real-life room-scale stage sets, it enables users to move untethered in a virtual environment and interact with physical objects. The technology was created by Dr. Caecilia Charbonnier and Sylvain Chagué and developed by engineers at Artanim, a Swiss research center specialized in motion-capture technologies.\n\nDreamscape Immersive was co-founded by Walter Parkes, a film producer whose credits include the \"Men in Black\" series and \"Minority Report\", Kevin Wall, a global live events producer, investor, and entrepreneur, Caecilia Charbonnier and Sylvain Chagué, co-founders of Artanim, and Ronald Menzel, serial entrepreneur. Parkes helped to establish DreamWorks as the head of its motion picture division; Dreamscape Immersive was envisioned as both a technology company and original content studio, planning to create large scale cinematic experiences using virtual reality (VR) as a medium.\n\nThe company launched in mid-2016, whereupon Bruce Vaughn, the former head of Disney's Imagineering, was appointed CEO and Aaron Grosky, former president of Control Room, was appointed COO. It operated in stealth mode until February of the following year. Based in Culver City, California, its first investors included IMAX, Westfield Malls, three film studios, and Steven Spielberg.\n\nIn September 2017, AMC Theatres announced that it had invested $10 million to lead the series B equity round, and made additional commitments to finance the rapid rollout of VR centers within AMC cinemas and standalone locations in North America and the UK. AMC also invested $10 million to fund the production and development of content for Dreamscape experiences. In December, additional investors joined the series B round including Nickelodeon and Majid Al Futtaim. The Nickelodeon investment was made to produce a VR experience intended for children and families; Majid Al Futtaim invested with the intent to expand the company globally.\n\n"}
{"id": "21420090", "url": "https://en.wikipedia.org/wiki?curid=21420090", "title": "Experiments in Fluids", "text": "Experiments in Fluids\n\nExperiments in Fluids is a scientific, peer-reviewed scientific journal published monthly by Springer Science+Business Media. The journal presents contributions that employ existing experimental techniques to gain an understanding of the underlying flow physics in specific areas. These areas include turbulence, aerodynamics, hydrodynamics, convective heat transfer, combustion, turbomachinery, multi-phase flows, and chemical, biological and geological flows. In addition, papers report on investigations combining experimental and analytical/numerical approaches. The journal also publishes letters and review articles.\n\n\"Experiments in Fluids\" had a 2017 impact factor of 2.195.\n\nThe editors of the journal are E. K. Longmire (University of Minnesota, USA), C. Tropea (Technical University of Darmstadt, Germany) and J. Westerweel (Laboratory for Aero & Hydrodynamics, Delft University of Technology, Netherlands).\n\nThe founding editors are J. H. Whitelaw (Imperial College, United Kingdom) and W. Merzkirch (University of Essen, Germany). Jim Whitelaw served the journal from 1982 until 1999; Wolfgang Merzkirch was with the journal from 1982 until 2002.\n"}
{"id": "4302166", "url": "https://en.wikipedia.org/wiki?curid=4302166", "title": "Explosively formed penetrator", "text": "Explosively formed penetrator\n\nAn explosively formed penetrator (EFP), also known as an explosively formed projectile, a self-forging warhead, or a self-forging fragment, is a special type of shaped charge designed to penetrate armor effectively. As the name suggests, the effect of the explosive charge is to deform a metal plate into a slug or rod shape and accelerate it toward a target. They were first developed as oil well perforators by American oil companies in the 1930s, and were first deployed as weapons in World War II.\n\nA conventional shaped charge generally has a conical metal liner that projects a hypervelocity jet of metal able to penetrate steel armour to great depths; in travel over some distance the jet breaks up along its length into particles that drift out of alignment, greatly diminishing its effectiveness at a distance.\n\nAn EFP, on the other hand, has a liner face in the shape of a shallow dish. The force of the blast moulds the liner into any of a number of shapes, depending on the shape of the plate and how the explosive is detonated. Some sophisticated EFP warheads have multiple detonators that can be fired in different arrangements causing different types of waveform in the explosive, resulting in either a long-rod penetrator, an aerodynamic slug projectile, or multiple high-velocity fragments. A less sophisticated approach for changing the formation of an EFP is the use of wire-mesh in front of the liner: with the mesh in place the liner fragments into multiple penetrators.\n\nIn addition to single-penetrator EFPs (also called single EFPs or SEFPs), there are EFP warheads whose liners are designed to produce more than one penetrator; these are known as multiple EFPs, or MEFPs. The liner of an MEFP generally comprises a number of dimples that intersect each other at sharp angles. Upon detonation the liner fragments along these intersections to form up to dozens of small, generally spheroidal projectiles, producing an effect similar to that of a shotgun. The pattern of impacts on target can be finely controlled based on the design of the liner and the manner in which the explosive charge is detonated. A nuclear-driven MEFP was apparently proposed by a member of the JASON group in 1966 for terminal ballistic missile defense. A related device was the proposed nuclear pulse propulsion unit for Project Orion.\n\nThe (single) EFP generally remains intact and is therefore able to penetrate armour at a long range, delivering a wide spray of fragments of liner material and vehicle armour backspall into the vehicle's interior, injuring its crew and damaging other systems.\n\nAs a rule of thumb, an EFP can perforate a thickness of armour steel equal to half the diameter of its charge for a copper or iron liner, and armour steel equal to the diameter of its charge for a tantalum liner, whereas a typical shaped charge will go through six or more diameters.\n\nThe penetration is proportional to the density of the liner metal; tantalum 16.654 g/cm, copper 8.960 g/cm, iron 7.874 g/cm. Tantalum is preferable in delivery systems that have limitations in size, like the SADARM, which is delivered by a howitzer. For other weapon systems where the size does not matter, a copper liner of double the calibre is used.\n\nExtensive research is going on in the zone between jetting charges and EFPs, which combines the advantages of both types, resulting in very long stretched-rod EFPs for short-to-medium distances (because of the lack of aerostability) with improved penetration capability.\n\nEFPs have been adopted as warheads in a number of weapon systems, including the CBU-97 and BLU-108 air bombs (with the Skeet submunition), the M303 Special Operations Forces demolition kit, the M2/M4 Selectable Lightweight Attack Munition (SLAM), the SADARM submunition, the Low Cost Autonomous Attack System, and the TOW-2B anti-tank missile.\n\n EFPs have been used in improvised explosive devices against armoured cars, for example in the 1989 assassination of the German banker Alfred Herrhausen (attributed to the Red Army Faction), and by Hezbollah in the 1990s. A recent development is their widespread use in IEDs by insurgents in Iraq against coalition vehicles.\n\nThe charges are generally cylindrical, fabricated from commonly available metal pipe, with the forward end closed by a concave copper or steel disk-shaped liner to create a shaped charge. Explosive is loaded behind the metal liner to fill the pipe. Upon detonation, the explosive projects the liner to form a projectile.\n\nThe effects of traditional explosions like blast-forces and metal fragments seldom disable armored vehicles, but the explosively formed solid copper penetrator is quite lethal —even to the new generation of mine-resistant vehicles (which are made to withstand an anti-tank mine), and many tanks.\n\nOften mounted on crash barriers at window level, they are placed along roadsides at choke points where vehicles must slow down, such as intersections and junctions. This gives the operator time to judge the moment to fire, when the vehicle is moving more slowly.\n\nDetonation is controlled by cable, radio control, TV or IR remote controls, or remote arming with a passive infrared sensor, or via a pair of ordinary cell phones. EFPs can be deployed singly, in pairs, or in arrays, depending on the tactical situation.\n\nAt close range there is no need for the cone-tailed projectiles' aerodynamic stability, which means that the liner can be simplified.\n\nThe SIM-EFP is a construction that fits in between the military linear cutting charge (see shaped charge) and the platter charge (see improvised explosive device). The main difference is how much the rectangular liner plate is bent.\n\nSIM-EFPs are built from simple cut and bent steel bars, side by side, with explosives attached. Upon explosion it creates multiple projectiles spread out along the vehicles' length, which makes it more probable that the enemy troops inside the vehicle are hit. The construction also allows for longer timing errors, and makes it easier to hit fast moving vehicles.\n\nThe SIM-EFP is a modified version of the platter charge, but the projectiles are better optimized for armour penetration instead of demolition, and spread out perfectly for effective killing of armoured vehicles (optimized for long projectiles instead of a broad flat projectile).\n\nThe simplified EFP design also makes it easier for small groups of motivated individuals to build big EFPs that can penetrate a heavy battle tank or stationary high-value targets of virtually any sort.\n\nOther examples of non-circular EFPs are U.S. patents 6606951 and 4649828.\n\nOther terrorist uses\n\nIn Northern Ireland similar devices have been discovered that were developed by dissident Republican groups for intended use against the police.\n\nThe spacecraft Hayabusa 2 will carry a Small Carry-on Impactor (SCI). It will be dropped off Hayabusa 2 on to an asteroid and detonated. The explosion will form a copper explosively formed penetrator hitting the asteroid with a velocity of 2 km/s. The crater created by the impact will be a target for further observations by the onboard instruments. The shaped charge will consist of 4.5 kg of plasticized HMX and a 2.5 kg copper liner.\n\n\n\n"}
{"id": "28974483", "url": "https://en.wikipedia.org/wiki?curid=28974483", "title": "Federal Telegraph Company", "text": "Federal Telegraph Company\n\nThe Federal Telegraph Company was a United States manufacturing and communications company that played a pivotal role in the 20th century in the development of radio communications. Founded in Palo Alto, California in 1909 by Cyril Frank Elwell, the company would eventually merge in August 1927 with the Mackay Companies. In 1911-13, Lee De Forest and two assistants worked at FTC on the first vacuum tube amplifier and oscillator, which De Forest called the \"Oscillaton\" after his earlier Audion.\n\nThe company remained a separate entity within the Mackay Companies, however, and when International Telephone and Telegraph (ITT) purchased the Mackay Companies in 1928 Federal remained a component of the Mackay structure as a manufacturing entity.\n\nIn 1940, Sosthenes Behn moved Federal Telegraph under ITT directly so that its manufacturing capabilities could help ITT replace those in Europe that had been shut down because of the war and the Fall of France.\n\nIn 1954, FTR changed its name from \"Federal Telegraph and Radio Corporation - an IT&T associate\" to \"Federal Telegraph and Radio Company - division of IT&T\", and its research division became the \"Federal Telecommunications Laboratories\", both continuing as subsidiaries of ITT after World War II through at least the 1950s.\n\n"}
{"id": "766895", "url": "https://en.wikipedia.org/wiki?curid=766895", "title": "Frankfurt kitchen", "text": "Frankfurt kitchen\n\nThe Frankfurt kitchen was a milestone in domestic architecture, considered the forerunner of modern fitted kitchens, for it realised for the first time a kitchen built after a unified concept, designed to enable efficient work and to be built at low cost. It was designed in 1926 by Austrian architect Margarete Schütte-Lihotzky for architect Ernst May's social housing project New Frankfurt in Frankfurt, Germany. Some 10,000 units were built in the late 1920s in Frankfurt.\n\nGerman cities after the end of World War I were plagued by a serious housing shortage. Various social housing projects were built in the 1920s to increase the number of rental apartments. These large-scale projects had to provide affordable apartments for a great number of typical working class families and thus were subject to tight budget constraints. As a consequence, the apartments designed were comfortable but not spacious, and so the architects sought to reduce costs by applying one design for large numbers of apartments.\n\nMargarete Schütte-Lihotzky's design of the kitchen for the \"Römerstadt\" thus had to solve the problem of how to build many kitchens, without allowing it to occupy too much of the total space of the apartment. Her design departed from the then common kitchen-cum-living room. The typical worker's household lived in a two-room apartment, in which the kitchen served many functions at once: besides cooking, one dined, lived, bathed, and even slept there, while the second room, intended as the parlour, often was reserved for special occasions such as a rare Sunday dinner. Instead, Schütte-Lihotzky's kitchen was a small separate room, connected to the living room by a sliding door; thus separating the functions of work (cooking etc.) from those of living and relaxing, consistent with her view about life:\n\nSchütte-Lihotzky's design was strongly influenced by the ideas of Taylorism, which was in vogue at the beginning of the 20th century. Started by Catharine Beecher in the middle of the 19th century and reinforced by Christine Frederick's publications in the 1910s, the growing trend that called for viewing household work as a true profession had the logical consequence that the industrial optimisation pioneered by Taylorism spilled over into the domestic area. Frederick's \"The New Housekeeping\", which argued for rationalising the work in the kitchen using a Taylorist approach, had been translated into German under the title \"Die rationelle Haushaltsführung\" in 1922. These ideas were received well in Germany and Austria and formed the base of German architect Erna Meyer's work and were also instrumental in Schütte-Lihotzky's design of the Frankfurt kitchen. She did detailed time-motion studies to determine how long each processing step in the kitchen took, re-designed and optimised workflows, and planned her kitchen design such that it should optimally support these workflows. Improving the ergonomics of the kitchen and rationalising the kitchen work was important to her:\n\nThis quote succinctly sums up the reasons for the appeal of Taylorism at the time. On the one hand, the trend to rationalise the household was reinforced by the intention to reduce the time spent in (economically speaking) \"unproductive\" housework, so that women had more time for factory work. On the other hand, emancipatory efforts to improve women's status, also in the home, called for rationalisation to relieve women and enable them to pursue other interests.\n\nSchütte-Lihotzky was strongly inspired by the extremely space-constrained railway dining car kitchens, which she saw as a Taylorist ideal: even though these were very small, two people could prepare and serve the meals for about 100 guests, and then wash and store the dishes.\n\nThe Frankfurt kitchen was a narrow double-file kitchen measuring . The entrance was located in one of the short walls, opposite which was the window. Along the left side, the stove was placed, followed by a sliding door connecting the kitchen to the dining and living room. On the right wall were cabinets and the sink, in front of the window a workspace. There was no refrigerator, but there was a foldable ironing board visible in the image folded against the left wall.\n\nThe narrow layout of the kitchen was not due solely to the space constraints mentioned above, it was equally a conscious design decision in a Taylorist attempt to minimise the number of steps needed when working in the kitchen. The sliding door also helped minimise the walking distance between the kitchen and the table in the adjacent room.\n\nDedicated, labelled storage bins for common ingredients such as flour, sugar, rice and others were intended to keep the kitchen tidy and well-organised; the workspace had an integrated, removable \"waste drawer\" such that scraps could just be shoved into it while working and the whole thing emptied at once afterwards.\n\nBecause conventional kitchen furniture of the time fit neither the new workflows nor the narrow space, the Frankfurt kitchen was installed complete with furniture and major appliances such as the stove, a novelty at that time in Germany. It was the first fitted kitchen. The wooden door and drawer fronts were painted blue because researchers had found that flies avoided blue surfaces. Lihotzky used oak wood for flour containers, because it repelled mealworms, and beech for table tops because beech is resistant to staining, acids, and knifemarks. The seating was a revolving stool on castors for maximum flexibility.\n\nSchütte-Lihotzky actually designed three different variations of the Frankfurt kitchen. Type 1, the one described here, was the most common and least costly. She also designed \"Type 2\" and \"Type 3\", which were larger, had tables, and were spacious enough for one or even two additional persons to help in the kitchen. These two latter types, however, did not have the impact her \"Type 1\" model had.\n\nErna Meyer responded to the criticisms of the Frankfurt kitchen with her \"Stuttgart kitchen\", presented in 1927. It was slightly larger and had a more square ground plan, and used unit furniture in an attempt to make it adaptable to both the future users' needs and different room shapes.\n\nSchütte-Lihotzky's Frankfurt kitchen was installed in some 10,000 units in Frankfurt and as such was a commercial success. The cost of a single kitchen, fully equipped, was moderate (a few hundred Reichsmark); the costs were passed on to the rent (which reportedly increased the rents by 1 RM per month).\n\nHowever, the users of these kitchens often had their difficulties with them. Unaccustomed to Schütte-Lihotzky's custom-designed workflows for which the kitchen was optimised, they often were at loss as to how to use the kitchen. It was frequently described as not flexible enough—the dedicated storage bins often were used for other things than their labels said. Another problem with these bins was that they were easily reachable by small children. Schütte-Lihotzky had designed the kitchen for one adult person only, children or even a second adult had not entered the picture, and in fact, the kitchen was too small for two people to work in. Most contemporary criticism concentrated on such rather technical aspects. Nevertheless, the Frankfurt kitchen became a model for a modern work kitchen. For the rest of the 20th century, the small, rationalised work kitchen was a standard in tenement buildings throughout Europe.\n\nSociological aspects of the \"work kitchen\" were criticised only much later, in the 1970s and 80s, when feminist criticism found that the emancipatory intentions that had in part motivated the development of the work kitchen had actually backfired: precisely because of the specialised rationalisation and the small size of these kitchens such that only one person could work comfortably, housewives tended to become isolated from the life in the rest of the house. What had started as an emancipatory attempt (although all proponents such as Beecher, Frederick, or Meyer had always implicitly assumed that the kitchen was the woman's domain) to professionalise and revalue work in the home was now seen as a confinement of the woman to the kitchen.\n\nKitchens in the 1930s until the 1960s in Germany were often smaller and less comfortable. Housing societies thought that the Frankfurt kitchen was too luxurious. But the principles of this kitchen were adapted in other countries like Sweden and Switzerland and reimported to Germany, and recognized to be the same like the Frankfurt kitchen before. The major difference of most of the later kitchens was that the Frankfurt kitchen used relatively expensive materials and not particle boards.\n\nMost Frankfurt kitchens were thrown away in the 1960s and 1970s, when modern kitchens with easy to clean surfaces like \"Resopal\" were affordable. Often only the aluminium drawers survived, which aren't typical of a modern kitchen. They were also sold separately for a few years by Haarer, the manufacturing company and chosen by architects and cabinet makers for their furniture.\n\nWhen the public interest on the work of Margarete Schütte-Lihotzky in the late 1990s was growing, most kitchens did not exist any more. Some homeowners have built replicas; a very few originals still exist. The original house \"Im Burgfeld 136, Frankfurt\" was chosen to be a museum because of the surviving Frankfurt kitchen.\n\nIn 2005 the Victoria and Albert Museum acquired a \"Frankfurt\" kitchen for its traveling exhibition \"Modernism: Designing a New World\" with stops in London, the USA and Germany. The kitchen was dismantled from its original place, restored and repainted.\n\nOne kitchen was sold in 2005 for €22,680, another for €34,200. But these prices seem to apply only for the classic type: a white variation without the characteristic wall cupboard was sold for €11,000\n\nAuctions sometimes feature the original drawers. In 2010, a piece of furniture with six drawers was sold for €380, another with ten for €1000, and another with nine for €1200.\n\nThe Frankfurt kitchen is found in the following public collections:\n\n\n"}
{"id": "4026270", "url": "https://en.wikipedia.org/wiki?curid=4026270", "title": "FullContact", "text": "FullContact\n\nFullContact Inc. is a privately held technology company that provides a suite of cloud-based contact management solutions for businesses, developers, and individuals.\n\nFullContact received considerable press in July 2012 when it announced a \"Paid Paid Vacation\" policy. As part of the policy, employees are awarded $7,500 USD annually to use on vacation with the only stipulations being that the employee take a vacation, disconnect from technology, and not work while on vacation.\n\nFullContact is headquartered in Denver, Colorado, U.S., and has offices in Dallas (U.S.), Riga (Latvia), Kochi (India), and Tel Aviv (Israel).\n\nFullContact was founded in 2010 by Bart Lorang, Travis Todd, and Dan Lynn and went through the Techstars Boulder accelerator in 2011. Over the history of the company, it has raised approximately $50 million in venture-capital financing.\n\nIn 2017, FullContact acquired Mattermark, an aggregator of data about startups and other companies.\n\n"}
{"id": "39034242", "url": "https://en.wikipedia.org/wiki?curid=39034242", "title": "GP5 chip", "text": "GP5 chip\n\nThe GP5 is a co-processor accelerator built to accelerate discrete belief propagation on factor graphs and other large-scale tensor product operations for machine learning. It is related to, and anticipated by a number of years, the Google Tensor Processing Unit\n\nIt is designed to run as a co-processor with another controller (such as a CPU or an Tensilica/ARM core). It was developed as the culmination of DARPA's Analog Logic program (although the GP5 chip architecture is digital, it could contemplate an entirely analog implementation of its Bayesian logic instruction set).\n\nThe GP5 has a fairly exotic architecture, resembling neither a GPU nor a DSP, and leverages massive fine-grained and coarse-grained parallelism. It is deeply pipelined. The different algorithmic tasks involved in performing belief propagation updates are performed by independent, heterogeneous compute units. The performance of the chip is governed by the structure of the machine learning workload being evaluated. In typical cases, the GP5 is roughly 100 times faster and 100 times more energy efficient than a single core of a modern core i7 performing a comparable task. It is roughly 10 times faster and 1000 times more energy efficient than a state-of-the art GPU. It is roughly 1000 times faster and 10 times more energy efficient than a state-of-the-art ARM processor. It was benchmarked on typical machine learning and inference workloads that included protein side-chain folding, turbo error correction decoding, stereo vision, signal noise reduction, and others.\n\nAnalog Devices, Inc. acquired the intellectual property for the GP5 when it acquired Lyric Semiconductor, Inc. in 2011.\n\n"}
{"id": "34996340", "url": "https://en.wikipedia.org/wiki?curid=34996340", "title": "Game Gadget", "text": "Game Gadget\n\nThe Game Gadget is an open source gaming handheld that supports music and video playback, open game development, and some e-reader features. It was available in one colour (white). It was released on April 6, 2012.\n\n\n\n\n\n\nFirmware v1.01\n\n\n"}
{"id": "46911229", "url": "https://en.wikipedia.org/wiki?curid=46911229", "title": "Garageio", "text": "Garageio\n\nGarageio is a smart home garage door controller created by Alottazs Labs, LLC headquartered in Columbus, Ohio. Alottazs Labs, LLC is a privately held company founded in October 2012. Garageio is an Internet of Things product providing managed garage and gate access via a smartphone and other web-enabled devices.\nGarageio consists of a WiFi enabled control hub which attaches to the garage door opener and controlled through the Garageio mobile or desktop application. The control hub can connect up to three separate garage doors from the same device.\nThe Garageio iOS and Android application allows the user to remotely manage each connected garage door independently. The user can also grant and accept access to other garage doors from other Garageio users.\n\nThe idea for Garageio began in 2012 when co-founder Zak Dziczkowski discovered his garage door remote was missing. Zak soon discovered that the replacement cost for a new remote was too expensive and envisioned a better way to control the garage door. That same night Zak called co-founder Dave Reif and together they began to work on the Garageio prototype.\n\nIn October 2012, Garageio became a product of Alottazs Labs, LLC bringing aboard additional co-founders, entrepreneur Greg Colarich, creative designer Jess Boonstra, and marketer Zach Cochran.\n\nGarageio spent the early part of the year focusing on application development and product design. Garageio was the recipient of the Cool Idea! Award by Minnesota-based plastic manufacturer Proto Labs which provided gratis prototype casings and a subsequent production run of parts. In November 2013, Garageio launched a successful crowd-funding campaign through Fundable raising $30,000 in pre-order sales.\n\nGarageio began shipping early in 2014 to its early crowd-funding backers and continued to take orders through its website. Garageio released an updated version of the application and announced additional platform integrations such as IFTTT (if this than that), which allows users to create recipes that trigger customized events. Garageio IFTTT recipes include events such as if it rains then close my garage or if I leave my house close my garage. Garageio increased its manufacturing network and continued to ship throughout North America.\n\nGarageio was one of the first recipients of the Alexa Fund, a fund by Amazon.com to support startups on voice control technology. As such, Garageio will be one of the first smart home devices that will be controllable with the Amazon Echo.\n"}
{"id": "47918129", "url": "https://en.wikipedia.org/wiki?curid=47918129", "title": "GoWarrior", "text": "GoWarrior\n\nGoWarrior is an open-source and community-supported computing platform. GoWarrior is designed for the world of makers, hackers, educators, hobbyists, and newbies to build electronics projects. It offers a complete package of hardware, software and cloud service.\n\nTIGER Board is a single-board computer performs as hardware for GoWarrior platform. It contains ARM Cortex A9 based M3733-AFAAA (SoC), ARM Mali-400 MP2, as well as integrated 1GB DDR3 RAM.\n\nGoWarrior specifications are: \n\nGoDroid is an Android KitKat 4.4.4. based optimized operating system for GoWarrior platform. In addition to original Android functionalities, GoDroid pre-integrates some useful middleware components and libraries, as well as some self-developed function blocks, which makes it also a software development kit for Android applications.\n\nGoDroid supports booting from NAND Flash or from MicroSD card that contains the boot code and image files.\n\nBy replacing Android native media engine StageFright with GStreamer and utilizing hardware acceleration facilities, GoDroid supports various audio/video/container hardware decoding and multiple network protocols including Microsoft Smooth Streaming, HTTP Live Streaming and KODI 14.2 has variety of supported video/audio plug-ins.\n\nBesides the screen mirroring function of Android Miracast, GoDroid also implements DLNA system service for sharing digital media among multimedia devices. DMR, DMS and DMP are supported.\n\nIn addition to C/C++/JAVA, GoDroid also integrates QPython2engine for Python2 programming on Android.\n\nTIGER Board provides 2 sets of 40-pin expansion headers, one of which is compatible to Raspberry Pi connector. Raspberry Pi Python applications can be ported and run on GoDroid.\n\nGoDroid supports Android Studio as application IDE. With API level 19 configuration to match GoDroid provides the availability of not only original Android API, but also proprietary extended API, such as GPIO/IC2/SPI/PWM.\n\nGoDroid supports standard ADB debugging via Ethernet, Wi-Fi and USB.\n\nGoBian is a Linux-embedded operating system running on TIGER Board for the GoWarrior platform.\n\nGoBian is developed based on Raspbian which is from Debian 7 wheezy armhf, and differs from Raspbian for the extra features, for example, GoBian encapsulates the RPi.gpio and other I/O libraries to facilitate transplanting projects which use the related libraries from Raspberry Pi to TIGER Board. Furthermore, GoBian provides support for multimedia by integrating GOF, KODI and other middle-ware modules and applications out-of-the-box.\n\nGoBian enables the Internet connection through Ethernet or Wi-Fi through the Ethernet port and Wi-Fi module on TIGER Board, and thus supports various methods to access the projects and transfer data, such as FTP, SSH.\n\nGoBian automatically synchronizes the system time with Internet time servers using the NTP protocol.\n\nGoBian integrates a built-in file system for data management.\n\nC, C++, Python, Perl, and shell script.\n\nGoBian lets you call the GPIO/I2C/UART/SPI interfaces directly in your projects with the built-in RPi.gpio and other libraries.\n\nGoBian makes it easy for the secondary development of multimedia applications with the customized GOF middle-ware for TIGER Board.\n\nThe on-board M3733-AFAAA processor makes GoBian a multitasking system with good performance.\n\nThe software programs that are available for Debian are basically compatible with GoBian.\n\nGoBian supports the ultra-power-saving sleep mode (PMU Standby), with the entire board power consumption as low as 0.35W.\n\nGoBian integrates with DLNA to fully support the multimedia sharing and multi-screen interaction.\n\nThe GoWarrior community is already launched to support your projects with GoBian.\n\nCloudQ is a comprehensive back-end community support to help users easily build up their projects.\n\n\n"}
{"id": "13297359", "url": "https://en.wikipedia.org/wiki?curid=13297359", "title": "H-infinity loop-shaping", "text": "H-infinity loop-shaping\n\nH-infinity loop-shaping is a design methodology in modern control theory. It combines the traditional intuition of classical control methods, such as Bode's sensitivity integral, with H-infinity optimization techniques to achieve controllers whose stability and performance properties hold despite bounded differences between the nominal plant assumed in design and the true plant encountered in practice. Essentially, the control system designer describes the desired responsiveness and noise-suppression properties by weighting the plant transfer function in the frequency domain; the resulting 'loop-shape' is then 'robustified' through optimization. Robustification usually has little effect at high and low frequencies, but the response around unity-gain crossover is adjusted to maximise the system's stability margins. H-infinity loop-shaping can be applied to multiple-input multiple-output (MIMO) systems.\n\nH-infinity loop-shaping can be carried out using commercially available software.\n\nH-infinity loop-shaping has been successfully deployed in industry. In 1995, R. Hyde, K. Glover and G. T. Shanks published a paper describing the successful application of the technique to a VSTOL aircraft. In 2008, D. J. Auger, S. Crawshaw and S. L. Hall published another paper describing a successful application to a steerable marine radar tracker, noting that the technique had the following benefits:\n\n\n\n"}
{"id": "141932", "url": "https://en.wikipedia.org/wiki?curid=141932", "title": "Handkerchief code", "text": "Handkerchief code\n\nThe handkerchief code (also known as the hanky code, the bandana code, and flagging) is a color-coded system, employed usually among the gay male casual-sex seekers or BDSM practitioners in the United States, Canada, Australia and Europe, to indicate preferred sexual fetishes, what kind of sex they are seeking, and whether they are a top/dominant or bottom/submissive.\n\nWearing a handkerchief on the left side of the body typically indicates one is a \"top\" (one considered active in the practice of the fetish indicated by the color of the handkerchief), while wearing it on the right side of the body would indicate one is a \"bottom\" (one considered passive in the practice of the fetish indicated by the color of the handkerchief). The hanky code was widely used in the 1970s by gay and bisexual men.\n\nThe wearing of various colored bandanas around the neck was common in the mid- and late-nineteenth century among cowboys, steam railroad engineers, and miners in the Western United States. It is thought that the wearing of bandanas by gay men originated in San Francisco after the Gold Rush, when, because of a shortage of women, men dancing with each other in square dances developed a code wherein the man wearing the blue bandana took the male part in the square dance, and the man wearing the red bandana took the female part (these bandanas were usually worn around the arm or hanging from the belt or in the back pocket of one's jeans) . It is thought that the modern hanky code started in New York City in late 1970 or early 1971 when a journalist for the \"Village Voice\" joked that instead of simply wearing keys to indicate whether someone was a \"top\" or a \"bottom\", it would be more efficient to subtly announce their particular sexual focus by wearing different colored hankies. Other sources attribute the expansion of the original red–blue system into today’s code to marketing efforts around 1971 by the San Francisco department store for erotic merchandise, The Trading Post, promoting handkerchiefs by printing cards listing the meanings of various colors. Alan Selby, founder of Mr. S Leather in San Francisco, claimed that he created the first hanky code with his business partners at Leather 'n' Things in 1972, when their bandana supplier inadvertently doubled their order and the expanded code would help them sell the extra colors they had received.\n\nThis table is drawn from Larry Townsend's \"The Leatherman's Handbook II\" (the 1983 second edition; the 1972 first edition did not include this list) and is generally considered authoritative. Implicit in this list is the concept of left/right polarity, left as usual indicating the top, dominant, or active partner; right the bottom, submissive, or passive partner. Negotiation with a prospective partner remains important because, as Townsend noted, people may wear hankies of any color \"only because the idea of the hankie turns them on\" or \"may not even know what it means\".\n\nThe longer lists found on the web are more elaborate and the many color codes in them are less often used in practice, although some of these colors are offered for sale at LGBT stores along with free cards listing their meanings.\n\n"}
{"id": "21897699", "url": "https://en.wikipedia.org/wiki?curid=21897699", "title": "Hexanitrodiphenylamine", "text": "Hexanitrodiphenylamine\n\nHexanitrodiphenylamine (abbreviated HND), is an explosive chemical compound with the formula CHNO. HND was used extensively by the Japanese during World War II but was discontinued due to its toxicity.\n\nDinitrodiphenylamine is treated with 98% nitric acid. The starting material, dinitrodiphenylamine, is obtained from the reaction of aniline, dinitrochlorobenzene, and soda ash.\n\nHND is a booster-class explosive that was used in World War II by the Germans as a component of Hexanite (60% TNT - 40% HND) and by the Japanese as a component of Kongo (Type 98 H) (60% Trinitroanisol - 40% HND) for use in bombs, sea mines and depth charges; Seigate (Type 97 H) (60% TNT - 40% HND) for use in torpedo warheads and depth charges; and also in Otsu-B (60% TNT, 24% HND & 16% aluminium powder) for use in torpedo warheads.\n\nIts ammonium salt, also known as Aurantia or Imperial Yellow, was discovered in 1873 by Emil Kopp and used as a yellow colorant for leather, wool and silk in the 19th and early 20th centuries.\n\nA most toxic and poisonous explosive, it attacks the skin, causing blisters which resemble burns. Dust from HND is injurious to the mucous membranes of the mouth, nose, and lungs.\n\n\n"}
{"id": "53347", "url": "https://en.wikipedia.org/wiki?curid=53347", "title": "Hockey puck", "text": "Hockey puck\n\nA hockey puck is a disk made of vulcanized rubber that serves the same functions in various games as a ball does in ball games. The best-known use of pucks is in ice hockey, a major international sport.\n\nIce hockey and its various precursor games utilized balls until the late 19th century. By the 1870s, flat pucks were made of wood as well as rubber. At first, pucks were square. The first recorded organized game of ice hockey used a wooden puck, to prevent it from leaving the rink of play. Rubber pucks were first made by slicing a rubber ball, then trimming the disc square. The Victoria Hockey Club of Montreal is credited with making and using the first round pucks, in the 1880s.\n\nMany indigenous persons throughout North America played a version of field hockey which involved some type of \"puck\" or ball, and curved wooden sticks. It was first observed by Europeans being played by Mi'kmaqs in Nova Scotia in the late 17th century. It was called \"ricket\" by the Mi'kmaqs. Eventually, they began to carve pucks from cherrywood, which was the puck of preference until late in the century when rubber imported by Euro-Americans replaced the wood.\n\nThe origin of the word \"puck\" is obscure. The Oxford English Dictionary suggests the name is related to the verb \"to puck\" (a cognate of \"poke\") used in the game of hurling for striking or pushing the ball, from the Scottish Gaelic \"puc\" or the Irish \"poc\", meaning \"to poke, punch or deliver a blow\":\n\nIt is possible that settlers of Halifax, Nova Scotia, many of whom were Irish and played hurling, may have introduced the word to Canada. The first known printed reference was in Montreal, in 1876 (\"Montreal Gazette\" of February 7, 1876), just a year after the first indoor game was played there.\n\nA hockey puck is also referred to colloquially as a \"biscuit\". To put the \"biscuit in the basket\" (colloquial for the goal) is to score a goal.\n\nIce hockey requires a hard disk of vulcanized rubber. A standard ice hockey puck is black, thick, in diameter, and weighs between ; some pucks are heavier or lighter than standard \"(see below)\". Pucks are often marked with silkscreened team or league logos on one or both faces. Pucks are frozen before the game to reduce bouncing during play.\n\nThere are several variations on the standard black, hockey puck. One of the most common is a blue, puck that is used for training younger players who are not yet able to use a standard puck. Heavier training pucks, typically reddish pink or reddish orange in colour, are also available for players looking to develop the strength of their shots or improve their stick handling skills. Players looking to increase wrist strength often practice with steel pucks that weigh ; these pucks are not used for shooting, as they could seriously harm other players. White pucks are used for goaltender practice. These are regulation size and weight, but made from white rubber. A hollow, light-weight fluorescent orange puck is available for road or floor hockey. Other variants, some with plastic ball-bearings or glides, are available for use for road or roller hockey.\n\nTwo major developments have been devised to create better puck visibility on television broadcasts, but both were short-lived:\n\nThe use of a \"Firepuck\" in the early 1990s was the first attempt to improve the visibility of hockey pucks as seen on television. This invention incorporated coloured retro reflective materials of either embedded lens elements or prismatic reflectors laminated into recesses on the flat surfaces and the vertical edge of a standard hockey puck. Yellow was the preferred reflected colour. A spotlight was required to be positioned on the TV camera and focused at the centre of the viewing area.\n\nA short demonstration tape of the Minnesota North Stars skating with the Firepuck was shown during the period break at the 1993 NHL All-Star Game in Montreal. The International Hockey League (IHL) pursued testing the Firepuck with its inventor, Donald Klassen. The next television viewing was the IHL All-Star Game in Fort Wayne, Indiana, January 1994, where the Firepuck was used for the entire game. The IHL tested the Firepuck in two more games, and finally the East Coast Hockey League used it January 17, 1997, for their all-star game.\n\nThe use of the Firepuck was discontinued because of these reasons:\n\nThe Firepuck name was branded during the 1990s but has since been discontinued.\n\nThe FoxTrax \"smart puck\" was developed by the Fox television network when it held National Hockey League (NHL) broadcasting rights for the United States. The puck had integrated electronics to track its position on screen; a blue streak traced the path of the puck across the ice. The streak would turn red if the puck was shot especially hard. This was an experiment in broadcasting intended to help viewers unfamiliar with hockey to better follow the game by making the puck more visible. It was ill-received by many traditional hockey fans, but appreciated by many of the more casual viewers. The system debuted with much publicity in the NHL All-Star game at the Boston Fleet Center on January 20, 1996, but the system was shelved when Fox Sports lost the NHL broadcast rights three years later.\n\nDuring a game, pucks can reach speeds of or more when struck. Zdeno Chára, whose slapshot clocked in the 2013 NHL All-Star Game SuperSkills competition, broke his own earlier record. The current world record is held by Denis Kulyash of KHL's Avangard Omsk, who slapped a puck at the 2011 KHL All-Star Game skills competition in Riga, Latvia with a speed of .\n\nFast-flying pucks are potentially dangerous to players and spectators. Puck-related injuries at hockey games are not uncommon. This led to the evolution of various types of protective gear for players, most notably the goaltender mask. The most serious incident involving a spectator took place on March 18, 2002, when a 13-year-old girl, Brittanie Cecil, died two days after being struck on the head by a hockey puck deflected into the crowd at a NHL game between the Calgary Flames and Columbus Blue Jackets in Columbus. This is the only known incident of this type to have occurred in the history of the league. Partly as a result of this event, the glass or plexiglass panels that sit atop the boards of hockey rinks to protect spectators have been supplemented with mesh nets that extend above the upper edge of the glass.\n\nNHL regulation pucks were not \"required\" for professional play until the 1990–91 season, but were standardized for consistent play and ease of manufacture half a century earlier, by Art Ross, in 1940. Major manufacturers of pucks exist in Canada, Russia, the Czech Republic, the People's Republic of China, and Slovakia.\n\nThe list of former or present-day major producers includes\n\nThe black rubber of the puck is made up of a mix of natural rubber, antioxidants, bonding materials and other chemicals to achieve a balance of hardness and resilience. This mixture is then turned in a machine with metal rollers, where workers add extra natural rubber, and ensure that the mixing is even. Samples are then put into a machine that analyses if the rubber will harden at the right temperature. An automated apparatus, called a pultrusion machine, extrudes the rubber into long circular logs that are in diameter and then cut into thick pieces while still soft. These pre-forms are then manually put into moulds that are the exact size of a finished puck. There are up to 200 mould cavities per moulding palette, capable of producing up to 5,000 pucks per week. The moulds are then compressed. This compression may be done cold or with the moulds heated to for 18 minutes, depending on the proprietary methods of the manufacturer. They come out hard and then are allowed to sit for 24 hours. Each puck is manually cleaned with a trimmer machine to remove excess rubber. The moulding process adds a diamond cross-hatch texture around the edge of the puck for more friction between the stick and puck for better control and puck handling.\n\nThe practice pucks are made by a similar but faster process that uses larger pre-forms, thick, puts them into moulds automatically, and applies more pressure and heat over a shorter period of time to compress the puck into the standard size. This allows approximately twice as many pucks to be manufactured in the same time period as the more exacting production of NHL regulation pucks. People sometimes freeze pucks to prevent them from sticking to the ice.\n\nRoller hockey pucks are similar to ice-hockey pucks, but made from plastic and thus lighter. They have small ribs protruding from their tops and bottoms which limit contact with the surface, allowing better sliding motion and less friction.\n\nMost commonly red, roller hockey pucks can be found in almost any colour, although light, visible colours such as red, orange, yellow, pink, and green are typical.\n\nRoller hockey pucks were created so inline hockey and street hockey players could play with a puck instead of a ball on surfaces such as hardwood, concrete, and asphalt.\n\nAn underwater hockey puck (originally but now rarely referred to as a \"squid\" in the United Kingdom), while similar in appearance to an ice hockey puck, differs in that it has a lead core weighing approximately within a teflon, plastic or rubber coating. This makes the puck dense enough to sink in a swimming pool, though it can be lofted during passes, while affording some protection to the pool tiles.\n\nA smaller and lighter version of the standard puck exists for junior competition and is approximately 1 lb 12 oz (0.80–0.85 kg) and of similar construction to the standard puck.\n\nWhile there are numerous regional variations in colour, construction and materials all must conform to international regulations stipulating overall dimensions and weight. The regulations state that pucks should be a bright distinctive colour, for example high-visibility pink or orange, and that for World Championships these are the only acceptable colours.\n\nThe term \"puck\" is sometimes also applied to similar (though often smaller) gaming discs in other sports and games, including novuss, shuffleboard, table shuffleboard, box hockey and air hockey.\n\nIce hockey pucks of regulation diameter and thickness may be used as mechanical vibration dampening isolators in places such as feet for light industrial air compressors, and air conditioning units because they are of regulation materials and therefore consistent manufacture, size and shape, and are constructed of a repeatable and consistent vulcanized rubber material. Since the material is rubber, it may be drilled out or milled easily to a fixed depth as rubber feet or used as rubber spacer or gasket material. A very common use of a slotted hockey puck is as an adaptor between the metal foot of a trolley jack and the sill (rocker panel) of an automobile. The sill has a spot-welded lip which fits into the slot of the puck and would otherwise be bent or marked by the metal foot. Hockey pucks have been used to level furniture, beverage refrigeration systems, wedding mementos, and as paper weights or door stops. The hockey puck has many uses other than its original, intended purpose by virtue of its consistent physical properties.\n\nIn November 2018, faculty of Oakland University in Michigan received hockey pucks and training to throw them as a possible last-ditch defense against active shooters. The American Association of University Professors distributed pucks to its 800 members, and is working with student groups to distribute an additional 1,700 pucks to students.\n\n"}
{"id": "27703588", "url": "https://en.wikipedia.org/wiki?curid=27703588", "title": "Jean-Marie Bottequin", "text": "Jean-Marie Bottequin\n\nJean-Marie Albert Bottequin (born 29 April 1941 in Ghent) is a Belgian photographer, photojournalist, photo artist and mime.\n\nJean-Marie Bottequin is the son of Wallonian (French speaking southern part of Belgium) Prof. Dr. Armand Bottequin (professor for philosophy, theatrical science und Romanic languages) and his Flemish wife, nurse and social worker, Odile Maenhout. His father’s first marriage produced the sons Jacques and Pierre; the marriage with Odile Maenhout produced Jean-Marie Albert, Guy Antoine and Monique.\n\nFollowing his high school graduation (baccalauréat), in 1957, Bottequin went on to study classical theater at the \"Ecole des Hautes Etudes\". In 1959, he was accepted to study Romance studies, educational studies und art education at Ghent University; in 1960, he decided to study painting and art education with renowned Belgian artist, Octave Landuyt at the ‘’Normalschool voor plastische Kunsten’’ in Ghent. Since 1962, Bottequin has been working as a free-lance photographer, photo artist, and self-taught artist.\n\nIn 1968, Bottequin created his first social reportages for Quick magazine, following which he became the personal photographer and reporter with IOS, Investors Overseas Services, Geneva - Munich. In 1970, Bottequin decided to also work as a free-lance photo journalist. From 1972-1981, aside from his free-lance commitments, he went on to become a member of the dramaturgy of the Bayerische Staatsschauspiel Munich, and consequently its personal photographer. From 1975-1985, he was a lecturer at the ‘‘Lehrinstitut für Grafikdesign‘‘ (now ‘‘Akademie in der Einsteinstraße‘‘).\n\nBottequin’s first marriage to fellow Belgian Monique Kesteman produced the children Caroline and Michel. His second marriage to American-Belgian actress, voice talent, translator, book author and poet Marietta Meade produced the children Ezra, Avital and Ayalah.\n\nJean-Marie Bottequin speaks French, Dutch, German and English.\n\nBottequin has been living and working in Munich, Germany since 1968.\n\n\nBottequin has been internationally exhibited (individual and group exhibits) since 1965.\n\nHis oeuvre is an integral part of the following museums:\n\nBottequin was published in the following books:\n\nAmong Bottequin’s most famous clients is the automotive manufacturer BMW.\n\n\nBottequin also acted as an advisor to the European Council and for UNESCO.\n\nHaving founded IPS (\"International Photographic Seminars\": Photographic seminars for professional and amateur photographers), Bottequin has been facilitating \"fine art photographic seminars\" since 1980. In addition, he gives public lectures on visual perception and active vision.\n\n\nBottequin is a student of the French mimes, Étienne Decroux and Marcel Marceau.\n\nhttp://www.dozentenplattform.de/9.html\n\n"}
{"id": "22631535", "url": "https://en.wikipedia.org/wiki?curid=22631535", "title": "List of software for nuclear engineering", "text": "List of software for nuclear engineering\n\nWith the decreased cost and increased capabilities of computers, Nuclear Engineering has implemented computer software (Computer code to Mathematical model) into all facets of this field. There are a wide variety of fields associated with nuclear engineering, but computers and associated software are used most often in design and analysis. Neutron Kinetics, Thermal-hydraulics, and structural mechanics are all important in this effort. each software need to test and verify before use. The codes can be separated by use and function. most of software's written in C and Fortran.\n\n\n\n\n\n\n\n\n\nMany codes are supported by the U.S. Nuclear Regulatory Commission (NRC). The include SCALE, PARCS, TRACE (Formerly RELAP5 and TRAC-B), MELCOR, and many others.\n\nhttp://www.nrc.gov/about-nrc/regulatory/research/safetycodes.html\n\n"}
{"id": "18302417", "url": "https://en.wikipedia.org/wiki?curid=18302417", "title": "Microwave Imaging Radiometer with Aperture Synthesis", "text": "Microwave Imaging Radiometer with Aperture Synthesis\n\nMicrowave Imaging Radiometer with Aperture Synthesis (MIRAS) is the major instrument on the Soil Moisture and Ocean Salinity satellite (SMOS). MIRAS employs a planar antenna composed of a central body (the so-called hub) and three telescoping, deployable arms, in total 69 receivers on the Unit. Each receiver is composed of one LICEF module, which detects radiation in the microwave L-band, both in horizontal and vertical polarizations. The aperture on the LICEF detectors, planar in arrangement on MIRAS, point directly toward the Earth's surface as the satellite orbits. The arrangement and orientation of MIRAS makes the instrument a 2-D interferometric radiometer that generates brightness temperature images, from which both geophysical variables are computed. The salinity measurement requires demanding performance of the instrument in terms of calibration and stability. The MIRAS instrument's prime contractor was EADS CASA Espacio, manufacturing the payload of SMOS under ESA's contract.\n\nThe LICEF detector is composed of a round patch antenna element, with 2 pairs of probes for orthogonal linear polarisations, feeding two receiver channels in a compact lightweight package behind the antenna. It picks up thermal radiation emitted by the Earth near 1.4 GHz in the microwave L-band, amplifies it 100 dB, and digitises it with 1-bit quantisation.\n"}
{"id": "40931317", "url": "https://en.wikipedia.org/wiki?curid=40931317", "title": "Nature Republic", "text": "Nature Republic\n\nNature Republic (Hangul: 네이처리퍼블릭) is a South Korean cosmetics brand.\n\nCreated in March 2009, Nature Republic is a cosmetics brand that utilizes natural ingredients for their products. The brand's philosophy is 'A Sense of the Beginning'.\n\nNature Republic opened its first store in March 2009. Stores were opened outside of South Korea in Taiwan, Singapore, Thailand and Malaysia in 2011.\n\nIn 2011, Nature Republic opened its first store in Cambodia and launched its first store in the Philippines on November 13, 2012.\n\nThe brand was involved in a scandal in 2015, when its founder and former CEO Jung Woon-ho was imprisoned for embezzlement and breach of trust for gambling overseas. He also allegedly spent billions of won in lobbying for a shorter sentence. After replacing him with Kim Chang-ho, the brand opened more stores in the U.S, which already has 17 of its stores since 2012. It launched a store in Torrance, California, and has plans of opening another in Elmhurst, New York as part of its plans to continue building up its presence in the country.\n\nIts best-known products are Ginseng Royal Silk Watery Cream, 92% Soothing Aloe Vera Gel, Nature Origin Collagen BB Cream 01 Light Beige, Argan Essential Conditioner and Jeju Sparkling Cleansing Water. Other products include cleaning foam,body lotion, body shower, hair essence series and so on.\n\nRain became an endorser of Nature Republic in 2009.\n\nOn September 13, 2010, JYJ became the new endorsers of the brand after signing a one-year contract with Nature Republic. The company plans to use their strong influence in Asia by enacting “differentiated marketing strategies”, targeting not only domestic customers, but also foreign customers as well.\n\nIn 2011, KARA's Gyuri, Hara and Jiyoung were selected as Nature Republic's new models. Jang Keun-suk also became an endorser of the brand in the same year.\n\nIn August 2013, it was announced that Girls' Generation's Taeyeon and EXO signed a 2-year endorsement deal with Nature Republic.\n\nIn November 2015, Girls' Generation's Taeyeon and EXO renewed their contract as Nature Republic's exclusive models. \n\n\n"}
{"id": "5876658", "url": "https://en.wikipedia.org/wiki?curid=5876658", "title": "Optical cross-connect", "text": "Optical cross-connect\n\nAn optical cross-connect (OXC) is a device used by telecommunications carriers to switch high-speed optical signals in a fiber optic network, such as an optical mesh network.\n\nThere are several ways to realize an OXC:\n\n\nAn optical add-drop multiplexer (OADM) can be viewed as a special case of an OXC, where to node degree is two.\n\n\n"}
{"id": "47229343", "url": "https://en.wikipedia.org/wiki?curid=47229343", "title": "Ordnance Corps (Ireland)", "text": "Ordnance Corps (Ireland)\n\nThe Ordnance Corps () is a combat support corps of the Irish Army, a branch of the Defence Forces, that has logistical and operational responsibility for military ordnance in Ireland. The logistical role of the Army Ordnance Corps is to provide technical support to the Defence Forces for the procurement, storage, distribution, inspection, maintenance, repair and disposal of all items of ordnance equipment. The operational role of the Ordnance Corps is to train personnel for and provide the state's bomb disposal capability.\n\nThe seven logistical taskings of the Ordnance Corps include;\n\nThe responsibility for the procurement and maintenance of all ordnance equipment is vested with the Ordnance Corps and encompasses a spectrum of equipment ranging from anti-aircraft missiles and naval armament to the uniforms worn by military personnel. The corps is also responsible for the procurement of food and provision of commercial catering services. These tasks are of a technical nature and the corps personnel are appropriately qualified and with the expertise to afford technical evaluation of complete weapon systems, it also includes embracing weapons, ammunition, fire control instruments and night vision equipment.\n\nThe Ordnance Corps provide the only dedicated Explosive Ordnance Disposal (EOD) and Improvised Explosive Device Disposal (IEDD) teams within the Republic of Ireland. This service supports the Garda Síochána, the national police force, in an Aid to the Civil Power (ATCP) role, as well as providing the three branches of the Defence Forces (Army, Naval Service and Air Corps) with EOD, IEDD and CBRNE defence (chemical, biological, radiological, nuclear and explosives) specialist capability across its full spectrum of operations.\n\nThe corps has a long history of Counter-IED efforts (C-IED) within the country owing to \"The Troubles\" and the proliferation in use of improvised explosive devices (IEDs) by dissident republican paramilitary and terrorist groups, and more recently Irish and international organised criminal drug gangs. Significant experience was built up during UNIFIL missions in Lebanon, and the ISAF counter-IED programme in Afghanistan was largely developed by senior Irish Army ordnance officers. Information obtained during the combatting of IRA bombs has been particularly relevant to security forces countering the activities of Hezbollah and the Taliban, whose bomb making techniques are similar. The corps must keep abreast of current developments in international terrorist devices and the equipment needed to counteract these devices. The Defence Forces Ordnance Corps has internationally recognised expertise on bomb disposal, and courses are conducted for its own personnel and for students from the military and police forces of many other nations. Ordnance Corps personnel also serve in overseas missions and are an essential component of missions involving troops.\n\nAs a member of NATO Partnership for Peace (PfP) and NATO Euro-Atlantic Partnership Council (EAPC), Ireland participates in the PfP Planning and Review Process (PARP), and as part of this process the Defence Forces have adopted Partnership Goals aimed at assisting Ireland to meet its United Nations and European Union commitments in the areas of Counter Improvised Explosive Devices (C-IED), and improving the Irish military's interoperability with other professional military forces in this area.\n\nThere are a number of Irish Army EOD teams located across the country in military barracks, ready for operations throughout the state 24/7, 365 days a year. EOD teams use Swedish Scania P270 (Wilker Group) and armoured Swiss Mowag Duro II EOD trucks, with a motorcycle escort from the Garda Traffic Corps on internal callouts. Bomb squads are protected by an armed group of support soldiers, who provide a cordon, cover and protect the sensitive equipment carried by EOD trucks. Bomb disposal robots, John Deere Gators and Segways are also in use with Ordnance Corps EOD teams. Teams can be dispatched on helicopters via the Air Corps if there is a need.\n\nIn the year ending 2014, Irish Army EOD squads were called out to 141 domestic incidents, 53 of which involved viable improvised explosive devices.\n\n\n"}
{"id": "27598453", "url": "https://en.wikipedia.org/wiki?curid=27598453", "title": "Oscilloscope types", "text": "Oscilloscope types\n\nThis is a subdivision of the Oscilloscope article, discussing the various types and models of oscilloscopes in greater detail.\n\nWhile analog devices make use of continually varying voltages, digital devices employ binary numbers which correspond to samples of the voltage. In the case of digital oscilloscopes, an analog-to-digital converter (ADC) is used to change the measured voltages into digital information. Waveforms are taken as a series of samples. The samples are stored, accumulating until enough are taken in order to describe the waveform, which are then reassembled for display. Digital technology allows the information to be displayed with brightness, clarity, and stability. There are, however, limitations as with the performance of any oscilloscope. The highest frequency at which the oscilloscope can operate is determined by the analog bandwidth of the front-end components of the instrument and the sampling rate.\n\nDigital oscilloscopes can be classified into three primary categories: digital storage oscilloscopes, digital phosphor oscilloscopes, and digital sampling oscilloscopes. Newer variants include PC-based oscilloscopes (which attach to a PC for data processing and display) and mixed-signal oscilloscopes (which employ other functions in addition to voltage measurement).\n\nThe digital storage oscilloscope, or DSO for short, is now the preferred type for most industrial applications. Instead of storage-type cathode ray tubes, DSOs use digital memory, which can store data as long as required without degradation. A digital storage oscilloscope also allows complex processing of the signal by high-speed digital signal processing circuits.\n\nThe vertical input is digitized by an analog to digital converter to create a data set that is stored in the memory of a microprocessor. The data set is processed and then sent to the display, which in early DSOs was a cathode ray tube, but is now more likely to be an LCD flat panel. DSOs with color LCD displays are common. The data set can be sent over a LAN or a WAN for processing or archiving. The screen image can be directly recorded on paper by means of an attached printer or plotter, without the need for an oscilloscope camera. The oscilloscope's own signal analysis software can extract many useful time-domain features (e.g., rise time, pulse width, amplitude), frequency spectra, histograms and statistics, persistence maps, and a large number of parameters meaningful to engineers in specialized fields such as telecommunications, disk drive analysis and power electronics.\n\nDigital storage also makes possible another type of oscilloscope, the equivalent-time sample oscilloscope. Instead of taking consecutive samples after the trigger event, only one sample is taken. However, the oscilloscope is able to vary its timebase to precisely time its sample, thus building up the picture of the signal over the subsequent repeats of the signal. This requires that either a clock or repeating pattern be provided. This type of oscilloscope is frequently used for very high speed communication because it allows for a very high \"sample rate\" and low amplitude noise compared to traditional real-time oscilloscopes.\n\nDigital oscilloscopes are limited principally by the performance of the analog input circuitry, the duration of the sample window, and resolution of the sample rate. When not using equivalent-time sampling, the sampling frequency should be at least the Nyquist rate, double the frequency of the highest-frequency component of the observed signal, otherwise aliasing occurs.\n\nAdvantages over the analog oscilloscope are:\nA disadvantage of digital oscilloscopes is the limited refresh rate of the screen. On an analog oscilloscope, the user can get an intuitive sense of the trigger rate simply by looking at the steadiness of the CRT trace. For a digital oscilloscope, the screen looks exactly the same for any signal rate which exceeds the screen's refresh rate. Additionally, it is sometimes difficult to spot \"glitches\" or other rare phenomena on the black-and-white screens of standard digital oscilloscopes; the slight persistence of CRT phosphors on analog oscilloscopes makes glitches visible even if many subsequent triggers overwrite them. Both of these difficulties have been overcome recently by \"digital phosphor oscilloscopes\", which store data at a very high refresh rate and display it with variable intensity, to simulate the trace persistence of a CRT oscilloscope.\n\nDigital sampling oscilloscopes operate on the same principle as analog sampling oscilloscopes and, like their analog counterparts, are of great use when analyzing high-frequency signals; that is, repetitive signals whose frequencies are higher than the oscilloscope's sampling rate. For measuring repetitive signals, this type can have bandwidth and high-speed timing up to ten times greater than any real-time oscilloscope.\n\nA real-time oscilloscope, sometimes called a “single-shot” scope, captures an entire waveform on each trigger event. This requires the scope to capture a large number of data points in one continuous record. A sequential equivalent-time sampling oscilloscope, sometimes simply called a “sampling scope,” measures the input signal only once per trigger. The next time the scope is triggered, a small delay is added and another sample is taken. Thus a large number of trigger events must occur in order to collect enough samples to build a picture of the waveform. The measurement bandwidth is determined by the frequency response of the sampler which currently can extend beyond 90 GHz.\n\nAn alternative to sequential equivalent-time sampling is called random equivalent-time sampling. Samples are synchronised not with trigger events but with the scope's internal sampling clock. This causes them to occur at apparently random times relative to the trigger event. The scope measures the time interval between the trigger and each sample, and uses this to locate the sample correctly on the x-axis. This process continues until enough samples have been collected to build up a picture of the waveform. The advantage of this technique over sequential equivalent-time sampling is that the scope can collect data from before the trigger event as well as after it, in a similar way to the pre-trigger function of most real-time digital storage scopes. Random equivalent-time sampling can be integrated into a standard DSO without requiring special sampling hardware, but has the disadvantage of poorer timing precision than the sequential sampling method.\n\nHandheld oscilloscopes (also called scopemeters) are useful for many test and field service applications. Today, a hand held oscilloscope is usually a digital sampling oscilloscope, using a liquid crystal display. Typically, a hand held oscilloscope has two analog input channels, but four input channel versions are also available. Some instruments combine the functions of a digital multimeter with the oscilloscope. These usually are lightweight and have good accuracy.\n\nA PC-based oscilloscope is new type of \"oscilloscope\" that is emerging that consists of a specialized signal acquisition board (which can be an external USB or Parallel port device, or an internal add-on PCI or ISA card). The hardware itself usually consists of an electrical interface providing isolation and automatic gain controls, several high-speed analog-to-digital converters and some buffer memory, or even on-board Digital Signal Processor (DSPs). Depending on the exact hardware configuration, the hardware could be best described as a digitizer, a data logger or as a part of a specialized automatic control system.\n\nThe PC provides the display, control interface, disc storage, networking and often the electrical power for the acquisition hardware. The viability of PC-based oscilloscopes depends on the current widespread use and low cost of standardized PCs. Since prices can range from as little as US$100 to as much as US$10,500 depending on their capabilities, such instruments are particularly suitable for the educational market, where PCs are commonplace but equipment budgets are often low.\n\nPCO acquisition hardware, in certain cases, may only consist of a standard sound card or even a game port, if only audio and low-frequency signals are involved, though in many cases it will be considerably more robust. The PCO can transfer data to the computer in two main ways — streaming, and block mode. In streaming mode the data is transferred to the PC in a continuous flow without any loss of data. The way in which the PCO is connected to the PC (e.g., IEEE1394, Ethernet, USB etc.) will dictate the maximum achievable speed and thereby frequency and resolution using this method. Block mode utilizes the on-board memory of the PCO to collect a block of data which is then transferred to the PC after the block has been recorded. The PCO hardware then resets and records another block of data. This process happens very quickly, but the time taken will vary according to the size of the block of data and the speed at which it can be transferred. This method enables a much higher sampling speed, but in many cases the hardware will not record data whilst it is transferring the existing block, meaning that some data loss will occur.\n\nThe advantages of PC-based oscilloscopes include:\n\nThere are also some disadvantages, which include:\n\nAs more processing power and data storage is included in oscilloscopes, the distinction is becoming blurred. Mainstream oscilloscope vendors manufacture large-screen, PC-based oscilloscopes, with very fast (multi-GHz) input digitizers and highly customized user interfaces.\n\nSoftware for a PC may use the sound card or game port to acquire analog signals, instead of dedicated signal acquisition hardware. However, these devices have very restricted input voltage ranges, limited precision/resolution, and very restricted frequency ranges. The ground reference for these inputs is the same as the ground for the PC logic and power supply; this may inject unacceptable amounts of noise into the circuit under test. However, these devices can be useful for demonstration, hobby use, or specific setups where these factors won't interfere. Ground reference can also be eliminated with capacitor AC coupling or a signal transformer.\n\nIf a sound card is used, frequency response is usually limited to the audio range, and DC signals cannot be measured without hardware modification. The number of inputs is limited by the number of recording channels and the inputs can handle only audio line-level voltages (usually ~1 Vpp) without the risk of damage.\n\nIf the game port is used as the acquisition hardware, the possible sampling frequency is very low, typically below , and the input voltages can only vary over a range of a couple of volts. In addition, the game port cannot easily be programmed for a specific sampling rate, nor can it be easily assigned a precise quantization step. The analog to digital conversion is accomplished by triggering the discharge of a capacitor and then measuring how long it takes to charge it to a fixed threshold that is seen as a \"0\" to \"1\" transition on the PC ISA bus. This means a huge resistance at the input takes longer to measure than a low resistance, which results in asymmetrical sampling intervals. These limitations only make it suitable for low-precision visualization of low frequency signals.\n\nA mixed-signal oscilloscope (or MSO) has two kinds of inputs, a small number (typically two or four) of analog channels, and a larger number (typically sixteen) of digital channels. These measurements are acquired with a single time base, they are viewed on a single display, and any combination of these signals can be used to trigger the oscilloscope.\n\nAn MSO combines all the measurement capabilities and the use model of a Digital Storage Oscilloscope (DSO) with some of the measurement capabilities of a logic analyzer. MSOs typically lack the advanced digital measurement capabilities and the large number of digital acquisition channels of full-fledged logic analyzers, but they are also much less complex to use. Typical mixed-signal measurement uses include the characterization and debugging of hybrid analog/digital circuits like: embedded systems, Analog-to-digital converters (ADCs), Digital-to-analog converters (DACs), and control systems.\n\nThe earliest and simplest type of oscilloscope consisted of a cathode ray tube, a vertical amplifier, a timebase, a horizontal amplifier and a power supply. These are now called \"analog\" oscilloscopes to distinguish them from the \"digital\" oscilloscopes that became common in the 1990s and 2000s.\n\nBefore the introduction of the CRO in its current form, the cathode ray tube had already been in use as a measuring device. The cathode ray tube is an evacuated glass envelope, similar to that in a black-and-white television set, with its flat face covered in a fluorescent material (the phosphor). The screen is typically less than 20 cm in diameter, much smaller than the one in a television set. Older CROs had round screens or faceplates, while newer CRTs in better CROs have rectangular faceplates.\nIn the neck of the tube is an electron gun, which is a small heated metal cylinder with a flat end coated with electron-emitting oxides. Close to it is a much-larger-diameter cylinder carrying a disc at its cathode end with a round hole in it; it's called a \"grid\" (G1), by historic analogy with amplifier vacuum-tube grids. A small negative grid potential (referred to the cathode) is used to block electrons from passing through the hole when the electron beam needs to be turned off, as during sweep retrace or when no trigger events occur.\n\nHowever, when G1 becomes less negative with respect to the cathode, another cylindrical electrode designated G2, which is hundreds of volts positive referred to the cathode, attracts electrons through the hole. Their trajectories converge as they pass through the hole, creating quite-small diameter \"pinch\" called the crossover. Following electrodes (\"grids\"), electrostatic lenses, focus this crossover onto the screen; the spot is an image of the crossover.\n\nTypically, the CRT runs at roughly -2 kV or so, and various methods are used to correspondingly offset the G1 voltage. Proceeding along the electron gun, the beam passes through the imaging lenses and first anode, emerging with an energy in electron-volts equal to that of the cathode. The beam passes through one set of deflection plates , then the other, where it is deflected as required to the phosphor screen.\n\nThe average voltage of the deflection plates is relatively close to ground, because they have to be directly connected to the vertical output stage.\n\nBy itself, once the beam leaves the deflection region, it can produce a usefully bright trace. However, for higher bandwidth CROs where the trace may move more rapidly across the phosphor screen, a positive post-deflection acceleration (\"PDA\") voltage of over 10,000 volts is often used, increasing the energy (speed) of the electrons that strike the phosphor. The kinetic energy of the electrons is converted by the phosphor into visible light at the point of impact.\n\nWhen switched on, a CRT normally displays a single bright dot in the center of the screen, but the dot can be moved about electrostatically or magnetically. The CRT in an oscilloscope always uses electrostatic deflection. Ordinary electrostatic deflection plates can typically move the beam roughly only 15 degrees or so off-axis, which means that oscilloscope CRTs have long, narrow funnels, and for their screen size, are usually quite long. It's the CRT length that makes CROs \"deep\", from front to back. Modern flat-panel oscilloscopes have no need for such rather-extreme dimensions; their shapes tend to be more like one kind of rectangular lunchbox.\n\nBetween the electron gun and the screen are two opposed pairs of metal plates called the deflection plates. The vertical amplifier generates a potential difference across one pair of plates, giving rise to a vertical electric field through which the electron beam passes. When the plate potentials are the same, the beam is not deflected. When the top plate is positive with respect to the bottom plate, the beam is deflected upwards; when the field is reversed, the beam is deflected downwards. The horizontal amplifier does a similar job with the other pair of deflection plates, causing the beam to move left or right. This deflection system is called electrostatic deflection, and is different from the electromagnetic deflection system used in television tubes. In comparison to magnetic deflection, electrostatic deflection can more readily follow random and fast changes in potential, but is limited to small deflection angles.\n\nCommon representations of deflection plates are misleading. For one, the plates for one deflection axis are closer to the screen than the plates for the other. Plates that are closer together provide better sensitivity, but they also need to be extend far enough along the CRT's axis to obtain adequate sensitivity. (The longer the time a given electron spends in the field, the farther it's deflected.) However, closely spaced long plates would cause the beam to contact them before full amplitude deflection occurs, so the compromise shape has them relatively close together toward the cathode, and flared apart in a shallow vee toward the screen. They are not flat in any but quite-old CRTs! \n\nThe timebase is an electronic circuit that generates a ramp voltage. This is a voltage that changes continuously and linearly with time. When it reaches a predefined value the ramp is reset and settles to its starting value. When a trigger event is recognized, provided the reset process (holdoff) is complete, the ramp starts again. The timebase voltage usually drives the horizontal amplifier. Its effect is to sweep the screen end of the electron beam at a constant speed from left to right across the screen, then blank the beam and return its deflection voltages to the left, so to speak, in time to begin the next sweep. Typical sweep circuits can take significant time to reset; in some CROs, fast sweeps required more time to retrace than to sweep.\n\nMeanwhile, the vertical amplifier is driven by an external voltage (the vertical input) that is taken from the circuit or experiment that is being measured. The amplifier has a very high input impedance, typically one megohm, so that it draws only a tiny current from the signal source. Attenuator probes reduce the current drawn even more. The amplifier drives the vertical deflection plates with a voltage that is proportional to the vertical input. Because the electrons have already been accelerated by typically 2kV (roughly), this amplifier also has to deliver almost a hundred volts, and this with a very wide bandwidth. The gain of the vertical amplifier can be adjusted to suit the amplitude of the input voltage. A positive input voltage bends the electron beam upwards, and a negative voltage bends it downwards, so that the vertical deflection at any part of the trace shows the value of the input at that time.\n\nThe response of any oscilloscope is much faster than that of mechanical measuring devices such as the multimeter, where the inertia of the pointer (and perhaps damping) slows down its response to the input.\n\nObserving high speed signals, especially non-repetitive signals, with a conventional CRO is difficult, due to non-stable or changing triggering threshold which makes it hard to \"freeze\" the waveform on the screen. This often requires the room to be darkened or a special viewing hood to be placed over the face of the display tube. To aid in viewing such signals, special oscilloscopes have borrowed from night vision technology, employing a microchannel plate electron multiplier behind the tube face to amplify faint beam currents.\nAlthough a CRO allows one to view a signal, in its basic form it has no means of recording that signal on paper for the purpose of documentation. Therefore, special oscilloscope cameras were developed to photograph the screen directly. Early cameras used roll or plate film, while in the 1970s Polaroid instant cameras became popular. A P11 CRT phosphor (visually blue) was especially effective in exposing film. Cameras (sometimes using single sweeps) were used to capture faint traces.\n\nThe power supply is an important component of the oscilloscope. It provides low voltages to power the cathode heater in the tube (isolated for high voltage!), and the vertical and horizontal amplifiers as well as the trigger and sweep circuits. Higher voltages are needed to drive the electrostatic deflection plates, which means that the output stage of the vertical deflection amplifier has to develop large signal swings. These voltages must be very stable, and amplifier gain must be correspondingly stable. Any significant variations will cause errors in the size of the trace, making the oscilloscope inaccurate.\n\nLater analog oscilloscopes added digital processing to the standard design. The same basic architecture — cathode ray tube, vertical and horizontal amplifiers — was retained, but the electron beam was controlled by digital circuitry that could display graphics and text mixed with the analog waveforms. Display time for those was interleaved — multiplexed — with waveform display in basically much the same way that a dual/multitrace oscilloscope displays its channels. The extra features that this system provides include:\n\nA dual-beam oscilloscope was a type of oscilloscope once used to compare one signal with another. There were two beams produced in a special type of CRT.\n\nUnlike an ordinary \"dual-trace\" oscilloscope (which time-shared a single electron beam, thus losing about 50% of each signal), a dual-beam oscilloscope simultaneously produced two separate electron beams, capturing the entirety of both signals. One type (Cossor, UK) had a beam-splitter plate in its CRT, and single-ended vertical deflection following the splitter. (There is more about this type of oscilloscope near the end of this article.)\n\nOther dual-beam oscilloscopes had two complete electron guns, requiring tight control of axial (rotational) mechanical alignment in manufacturing the CRT. In the latter type, two independent pairs of vertical plates deflect the beams. Vertical plates for channel A had no effect on channel B's beam. Similarly for channel B, separate vertical plates existed which deflected the B beam only.\n\nOn some dual-beam oscilloscopes the time base, horizontal plates and horizontal amplifier were common to both beams (the beam-splitter CRT worked this way). More elaborate oscilloscopes like the Tektronix 556 and 7844 could employ two independent time bases and two sets of horizontal plates and horizontal amplifiers. Thus one could look at a very fast signal on one beam and a slow signal on another beam.\n\nMost multichannel oscilloscopes do not have multiple electron beams. Instead, they display only one trace at a time, but switch the later stages of the vertical amplifier between one channel and the other either on alternate sweeps (ALT mode) or many times per sweep (CHOP mode). Very few true \"dual-beam\" oscilloscopes were built.\n\nWith the advent of digital signal capture, true dual-beam oscilloscopes became obsolete, as it was then possible to display two truly simultaneous signals from memory using either the ALT or CHOP display technique, or even possibly a raster display mode.\n\nTrace storage is an extra feature available on some analog oscilloscopes; they used direct-view storage CRTs. Storage allows the trace pattern that normally decays in a fraction of a second to remain on the screen for several minutes or longer. An electrical circuit can then be deliberately activated to store and erase the trace on the screen.\n\nThe storage is accomplished using the principle of secondary emission. When the ordinary writing electron beam passes a point on the phosphor surface, not only does it momentarily cause the phosphor to illuminate, but the kinetic energy of the electron beam knocks other electrons loose from the phosphor surface. This can leave a net positive charge. Storage oscilloscopes then provide one or more secondary electron guns (called the \"flood guns\") that provide a steady flood of low-energy electrons traveling towards the phosphor screen. Flood guns cover the entire screen, ideally uniformly. The electrons from the flood guns are more strongly drawn to the areas of the phosphor screen where the writing gun has left a net positive charge; in this way, the electrons from the flood guns re-illuminate the phosphor in these positively charged areas of the phosphor screen.\n\nIf the energy of the flood gun electrons is properly balanced, each impinging flood gun electron knocks out one secondary electron from the phosphor screen, thus preserving the net positive charge in the illuminated areas of the phosphor screen. In this way, the image originally written by the writing gun can be maintained for a long time — many seconds to a few minutes. Eventually, small imbalances in the secondary emission ratio cause the entire screen to \"fade positive\" (light up) or cause the originally written trace to \"fade negative\" (extinguish). It is these imbalances that limit the ultimate storage time possible. \n\nStorage oscilloscopes (and large-screen storage CRT displays) of this type, with storage at the phosphor, were made by Tektronix. Other companies, notably Hughes, earlier made storage oscilloscopes with a more-elaborate and costly internal storage structure. \n\nSome oscilloscopes used a strictly binary (on/off) form of storage known as \"bistable storage\". Others permitted a constant series of short, incomplete erasure cycles which created the impression of a phosphor with \"variable persistence\". Certain oscilloscopes also allowed the partial or complete shutdown of the flood guns, allowing the preservation (albeit invisibly) of the latent stored image for later viewing. (Fading positive or fading negative only occurs when the flood guns are \"on\"; with the flood guns off, only leakage of the charges on the phosphor screen degrades the stored image.\n\nThe principle of sampling was developed during the 1930s in Bell Laboratories by Nyquist, after whom the sampling theorem is named. The first sampling oscilloscope was, however, developed in the late 1950s at the Atomic Energy Research Establishment at Harwell in England by G.B.B. Chaplin, A.R. Owens and A.J. Cole. [\"A Sensitive Transistor Oscillograph With DC to 300 Mc/s Response\", Proc I.E.E. (London) Vol.106, Part B. Suppl., No. 16, 1959].\n\nThe first sampling oscilloscope was an analog instrument, originally developed as a front-end unit for a conventional oscilloscope. The need for this instrument grew out of the requirement of nuclear scientists at Harwell to capture the waveform of very fast repetitive pulses. The current state-of-the-art oscilloscopes — with bandwidths of typically 20 MHz — were not able to do this and the 300 MHz effective bandwidth of their analog sampling oscilloscope represented a considerable advance.\n\nA short series of these \"front-ends\" was made at Harwell and found much use, and Chaplin et al. patented the invention. Commercial exploitation of this patent was ultimately done by the Hewlett-Packard Company (later Agilent Technologies).\n\nSampling oscilloscopes achieve their large bandwidths by not taking the entire signal at a time. Instead, only a sample of the signal is taken. The samples are then assembled to create the waveform. This method can only work for repetitive signals, not transient events. The idea of sampling can be thought of as a stroboscopic technique. When using a strobe light, only pieces of the motion are seen, but when enough of these images are taken, the overall motion can be captured\n\nA large number of instruments used in a variety of technical fields are really oscilloscopes with\ninputs, calibration, controls, display calibration, etc., specialized and optimized for a particular application. In some cases additional functions such as a signal generator are built into the instrument to facilitate measurements that would otherwise require one or more additional instruments.\n\nThe waveform monitor in television broadcast engineering is very close to a standard oscilloscope, but it includes triggering circuits and controls that allow a stable display of a composite video frame, field, or even a selected line out of a field. Robert Hartwig explains the waveform monitor as \"providing a graphic display of the black-and-white portion of the picture.\" The black-and-white portion of the video signal is called the \"luminance\" due to its fluorescent complexion. The waveform monitor's display of black vs. white levels allows the engineer to troubleshoot the quality of the picture and be certain that it is within the required standards. For convenience, the vertical scale of the waveform monitor is calibrated in IRE units.\n\n"}
{"id": "5202584", "url": "https://en.wikipedia.org/wiki?curid=5202584", "title": "Pulverizer", "text": "Pulverizer\n\nA pulverizer or grinder is a mechanical device for the grinding of many different types of materials. For example, a pulverizer mill is used to pulverize coal for combustion in the steam-generating furnaces of fossil fuel power plants.\n\nCoal pulverizers may be classified by speed, as follows:\n\nA ball mill is a pulverizer that consists of a horizontal rotating cylinder, up to three diameters in length, containing a charge of tumbling or cascading steel balls, pebbles, or rods.\n\nA tube mill is a revolving cylinder of up to five diameters in length used for fine pulverization of ore, rock, and other such materials; the material, mixed with water, is fed into the chamber from one end, and passes out the other end as a slurry.\n\nBoth types of mill include liners that protect the cylindrical structure of the mill from wear. Thus the main wear parts in these mills are the balls themselves, and the liners. The balls are simply \"consumed\" by the wear process and must be re-stocked, whereas the liners must be periodically replaced.\n\nThe ball and tube mills are low-speed machines that grind the coal with steel balls in a rotating horizontal cylinder. Due to its shape, it is called a tube mill and due to use of grinding balls for crushing, it is called a ball mill, or both terms as a ball tube mill..\n\nThese mills are also designated as an example size, BBD-4772,\n\n\nThe grinding in the ball and tube mill is produced by the rotating quantity of steel balls by their fall and lift due to tube rotation. The ball charge may occupy one third to half of the total internal volume of the shell. The significant feature incorporated in the BBD mills is its double end operation, each end catering to one elevation of a boiler. The system facilitated entry of raw coal and exit of pulverized fuel from same end simultaneously. This helps in reducing the number of installations per unit.\n\nA ball tube mill may be described as a cylinder made of steel plates having separate heads or trunions attached to the ends with each trunion resting on suitable bearings for supporting the machine. The trunions are hollow to allow for the introduction of discharge of the materials undergoing reduction in size. The mill shell is lined with chilled iron, carbon steel, manganese steel, or high chrome liners attached to shell body with counter sunk bolts. These liners are made in different shapes so that the counter inside surface of the mill is suited for requirement of a particular application.\n\nThe shells are of three pieces. The intermediate shell connects to the end shells by flange joints and the total length of shell is 7.2 m. The liners are fastened to the inner side of mill shell (cylindrical part) to protect the shell from the impact of the steel balls. There are 600 liners of ten variants in each shell weighing 60.26 tonnes. The original lift value of the liners is 55 mm. and the minimum lift allowed is 20 mm.\n\nThe primary air input to a ball tube mill performs a dual function. It is used for drying and as the fuel transport medium, and by regulating it the mill output is regulated. Governed by the pulverized fuel outlet temperature requirement, the cold air and hot air dampers are regulated to achieve the correct primary air temperature. In addition to raising the coal temperature inside the mill for drying and better grinding, the same air works as the transport medium to move the pulverized coal out of the mill: it travels through the annular space between the fixed trunnion tubes and the rotating hot air tube onwards to the classifier. Coal-laden air passes through double cone static classifiers, with adjustable classifier vanes, for segregation into pulverized fuel of the desired fineness, and coarse particles. The pulverised fuel continues its journey towards the coal burners for combustion. The coarse particles rejected in the classifier are returned to the mill for another cycle of grinding.\n\nIn order to avoid excess sweeping of coal from the mill, only part of the primary air, directly proportional to the boiler load demand, is passed through the mill. Furthermore, to ensure sufficient velocity of pulverized fuel to avoid settling in the pipes, an additional quantity of primary air is fed into a mixing box on the raw coal circuit. This by-pass air tapped from the primary air duct going into the mill makes an appreciable contribution to the drying of raw coal, by a flash drying effect, in addition to picking up the pulverized fuel from the mill outlet for its transportation towards the classifiers.\n\nThe tube mill output (responding to boiler load demand) is controlled by regulating the primary air-flow. This regulation, by sweeping pulverized fuel from the mill, is very fast; comparable with oil firing response, but needs the coal level to be maintained in the mill. A control circuit monitors the coal level in the mill, and controls the speed of the raw coal feeder to maintain it. Maintaining the coal level in the mill offers a built-in capacity cushion of pulverized fuel to take care of short interruptions in the raw coal circuit.\n\nThe mill is pressurized and the air-tightness is ensured by plenum chambers around the rotating trunnion filled with pressurized seal air. Bleeding seal air from plenum chamber to the mill maintains separation between pulverized fuel in the Mill and the outside atmosphere. Inadequacy or absence of seal air will allow escape of pulverized fuel into atmosphere. On the other hand, an excess of seal air leaking into mill will affect the mill outlet temperature. As such the seal air is controlled by a local control damper maintaining just sufficient differential pressure for sealing.\n\nThis type of mill consists of two types of rings separated by a series of large balls, like a thrust bearing. The lower ring rotates, while the upper ring presses down on the balls via a set of spring and adjuster assemblies, or pressurised rams. The material to be pulverized is introduced into the center or side of the pulverizer (depending on the design). As the lower ring rotates, the balls to orbit between the upper and lower rings, and balls roll over the bed of coal on the lower ring. The pulverized material is carried out of the mill by the flow of air moving through it. The size of the pulverized particles released from the grinding section of the mill is determined by a classifier separator. If the coal is fine enough to be picked up by the air, it is carried through the classifier. Coarser particles return to be further pulverized.\n\nSimilar to the ring and ball mill, the vertical spindle roller mill uses large \"tires\" to crush the coal. These mills are usually found in utility plants.\n\nRaw coal is gravity-fed through a central feed pipe to the grinding table where it flows outwardly by centrifugal action and is ground between the rollers and table. Hot primary air for drying and coal\ntransport enters the windbox plenum underneath the grinding table and flows upward through a swirl ring having multiple sloped nozzles surrounding the grinding table. The air mixes with and\ndries coal in the grinding zone and carries pulverized coal particles upward into a classifier.\n\nFine pulverized coal exits the outlet section through multiple discharge coal pipes leading to the burners, while oversized coal particles are rejected and returned to the grinding zone for further grinding.\nPyrites and extraneous dense impurity material fall through the nozzle ring and are plowed, by scraper blades attached to the grinding table, into the pyrites chamber to be removed.\nMechanically, the vertical roller mill is categorized as an applied force mill. There are three grinding roller wheel assemblies in the mill grinding section, which are mounted on a loading frame via pivot point.\nThe fixed-axis roller in each roller wheel assembly rotates on a segmentally-lined grinding table that is supported and driven by a planetary gear reducer direct-coupled to a motor. The grinding force for coal pulverization is applied by a loading frame. This frame is connected by vertical tension rods to three hydraulic cylinders secured to the mill foundation. All forces used in the pulverizing process are transmitted to the foundation via the gear reducer and loading elements. The pendulum movement of the roller wheels provides a freedom for wheels to move in a radial direction, which results in no radial loading against the mill housing during the pulverizing process.\n\nDepending on the required coal fineness, there are two types of classifier that may be selected for a vertical roller mill. The dynamic classifier, which consists of a stationary angled inlet vane assembly surrounding a rotating vane assembly or cage, is capable of producing micrometer-fine pulverized coal with a narrow particle size distribution. In addition, adjusting the speed of the rotating cage can easily change the intensity of the centrifugal force field in the classification zone to achieve coal fineness control real-time to make immediate accommodation for a change in fuel or boiler load conditions. For the applications where a micrometer-fine pulverized coal is not necessary, the static classifier, which consists of a cone equipped with adjustable vanes, is an option at a lower cost since it contains no moving parts. With adequate mill grinding capacity, a vertical mill equipped with a static classifier is capable of producing a coal fineness up to 99.5% or higher <50 mesh and 80% or higher <200 mesh, while one equipped with a dynamic classifier produces coal fineness levels of 100% <100 mesh and 95% <200 mesh, or better.\n\nIn 1954 a Jet Pulverizer was developed in which operates like a Vertical Pulverizer only the item is pulverized by the high speed air action. For example, forcing coal against coal.\n\nSimilar to the vertical roller mill, it also uses tires to crush coal. There are two types, a deep bowl mill, and a shallow bowl mill.\n\nA hammer mill is used on farms for grinding grain and chaff for animal feed.\n\nAn attachment fitted to an excavator. Commonly used in demolition work to break up large pieces of concrete.\n\n"}
{"id": "3176460", "url": "https://en.wikipedia.org/wiki?curid=3176460", "title": "Real time Java", "text": "Real time Java\n\nReal time Java is a catch-all term for a combination of technologies that enables programmers to write programs that meet the demands of real-time systems in the Java programming language.\n\nJava's sophisticated memory management, native support for threading and concurrency, type safety, and relative simplicity have created a demand for its use in many domains. Its capabilities have been enhanced to support real time computational needs:\n\n\nTo overcome typical real time difficulties, the Java Community introduced a specification for real-time Java, JSR001. A number of implementations of the resulting \"Real-time specification for Java\" (\"RTSJ\") have emerged, including a reference implementation from Timesys, IBM's WebSphere Real Time, Sun Microsystems's Java SE Real-Time Systems, PTC Perc from PTC, Inc., or JamaicaVM from aicas.\n\nThe RTSJ addressed the critical issues by mandating a minimum specification for the threading model (and allowing other models to be plugged into the VM) and by providing for areas of memory that are not subject to garbage collection, along with threads that are not preemptable by the garbage collector. These areas are instead managed using region-based memory management. The latest specification, 2.0, supports direct device access and deterministic garbage collection as well.\n\nThe \"Real-Time Specification for Java\" (RTSJ) is a set of interfaces and behavioral refinements that enable real-time computer programming in the Java programming language. RTSJ 1.0 was developed as JSR 1 under the Java Community Process, which approved the new standard in November, 2001. RTSJ 2.0 is being developed under JSR 282. A draft version is available at JSR 282 JCP Page. More information can be found at RTSJ 2.0\n\n\n"}
{"id": "1135781", "url": "https://en.wikipedia.org/wiki?curid=1135781", "title": "Refrigerator car", "text": "Refrigerator car\n\nA refrigerator car (or \"reefer\") is a refrigerated boxcar (U.S.), a piece of railroad rolling stock designed to carry perishable freight at specific temperatures. Refrigerator cars differ from simple insulated boxcars and ventilated boxcars (commonly used for transporting fruit), neither of which are fitted with cooling apparatus. Reefers can be ice-cooled, come equipped with any one of a variety of mechanical refrigeration systems, or utilize carbon dioxide (either as dry ice, or in liquid form) as a cooling agent. Milk cars (and other types of \"express\" reefers) may or may not include a cooling system, but are equipped with high-speed trucks and other modifications that allow them to travel with passenger trains.\n\nAfter the end of the American Civil War, Chicago, Illinois emerged as a major railway center for the distribution of livestock raised on the Great Plains to Eastern markets. Transporting the animals to market required herds to be driven up to to railheads in Kansas City, Missouri, where they were loaded into specialized stock cars and transported live (\"on-the-hoof\") to regional processing centers. Driving cattle across the plains also caused tremendous weight loss, with some animals dying in transit.\n\nUpon arrival at the local processing facility, livestock were either slaughtered by wholesalers and delivered fresh to nearby butcher shops for retail sale, smoked, or packed for shipment in barrels of salt. Costly inefficiencies were inherent in transporting live animals by rail, particularly the fact that about sixty percent of the animal's mass is inedible. The death of animals weakened by the long drive further increased the per-unit shipping cost. Meat sought a way to ship dressed meats from his Chicago packing plant to eastern markets.\n\nDuring the mid-19th century, attempts were made to ship agricultural products by rail. As early as 1842, the Western Railroad of Massachusetts was reported in the June 15 edition of the \"Boston Traveler\" to be experimenting with innovative freight car designs capable of carrying all types of perishable goods without spoilage. The first refrigerated boxcar entered service in June 1851, on the Northern Railroad (New York) (or NRNY, which later became part of the Rutland Railroad). This \"icebox on wheels\" was a limited success since it was only functional in cold weather. That same year, the Ogdensburg and Lake Champlain Railroad (O&LC) began shipping butter to Boston in purpose-built freight cars, utilizing ice for cooling.\nThe first consignment of dressed beef left the Chicago stock yards in 1857 in ordinary boxcars retrofitted with bins filled with ice. Placing meat directly against ice resulted in discoloration and affected the taste, proving to be impractical. During the same period Gustavas Swift experimented by moving cut meat using a string of ten boxcars with their doors removed, and made a few test shipments to New York during the winter months over the Grand Trunk Railway (GTR). The method proved too limited to be practical.\n\nDetroit's William Davis patented a refrigerator car that employed metal racks to suspend the carcasses above a frozen mixture of ice and salt. In 1868, he sold the design to George H. Hammond, a Detroit meat packer, who built a set of cars to transport his products to Boston using ice from the Great Lakes for cooling. The load had the tendency of swinging to one side when the car entered a curve at high speed, and use of the units was discontinued after several derailments. In 1878 Swift hired engineer Andrew Chase to design a ventilated car that was well insulated, and positioned the ice in a compartment at the top of the car, allowing the chilled air to flow naturally downward. The meat was packed tightly at the bottom of the car to keep the center of gravity low and to prevent the cargo from shifting. Chase's design proved to be a practical solution, providing temperature-controlled carriage of dressed meats, This allowed Swift and Company to ship their products across the United States and internationally.\nSwift's attempts to sell Chase's design to major railroads were rebuffed, as the companies feared that they would jeopardize their considerable investments in stock cars, animal pens, and feedlots if refrigerated meat transport gained wide acceptance. In response, Swift financed the initial production run on his own, then — when the American roads refused his business — he contracted with the GTR (a railroad that derived little income from transporting live cattle) to haul the cars into Michigan and then eastward through Canada. In 1880 the Peninsular Car Company (subsequently purchased by ACF) delivered the first of these units to Swift, and the Swift Refrigerator Line (SRL) was created. Within a year, the Line's roster had risen to nearly 200 units, and Swift was transporting an average of 3,000 carcasses a week to Boston, Massachusetts. Competing firms such as Armour and Company quickly followed suit. By 1920, the SRL owned and operated 7,000 of the ice-cooled rail cars. The General American Transportation Corporation would assume ownership of the line in 1930.\nIn the 1870s, the lack of a practical means to refrigerate peaches limited the markets open to Samuel Rumph, a Georgia peach grower. In 1875, he invented a refrigerated railcar and crates that allowed him to grow peaches on a very large scale and ship them to distant markets. He was the first to achieve this. His innovations created Georgia's fame for peaches, a crop now eclipsed economically by blueberries.\n\nEdwin Tobias Earl was born on a fruit ranch near Red Bluff, California on May 30, 1858. His father was Joseph Earl and his mother, Adelia Chaffee. His brother was Guy Chaffee Earl.\nCareer. He started his career in the shipping of fruits. By 1886, he was President of the Earl Fruit Company. In 1890, he invented the refrigerator car to transport fruits to the East Coast of the United States. He established the Continental Fruit Express and invested US$2,000,000 in refrigerator cars. In 1901, he sold his refrigerator cars to Armour and Company of Chicago and became a millionaire.\n\nLive cattle and dressed beef deliveries to New York (short tons):\n\n19th Century American Refrigerator Cars:\n\nThe use of ice to refrigerate and preserve food dates back to prehistoric times. Through the ages, the seasonal harvesting of snow and ice was a regular practice of many cultures. China, Greece, and Rome stored ice and snow in caves, dugouts or ice houses lined with straw or other insulating materials. Rationing of the ice allowed the preservation of foods during hot periods, a practice that was successfully employed for centuries. For most of the 19th century, natural ice (harvested from ponds and lakes) was used to supply refrigerator cars. At high altitudes or northern latitudes, one foot tanks were often filled with water and allowed to freeze. Ice was typically cut into blocks during the winter and stored in insulated warehouses for later use, with sawdust and hay packed around the ice blocks to provide additional insulation. A late-19th century wood-bodied reefer required re-icing every to .\nBy the turn of the 20th century, manufactured ice became more common. The Pacific Fruit Express (PFE), for example, maintained seven natural harvesting facilities, and operated 18 artificial ice plants. Their largest plant (located in Roseville, California) produced of ice daily, and Roseville's docks could accommodate up to 254 cars. At the industry's peak, of ice was produced for refrigerator car use annually.\nTop icing is the practice of placing a to layer of crushed ice on top of agricultural products that have high respiration rates, need high relative humidity, and benefit from having the cooling agent sit directly atop the load (or within individual boxes). Cars with pre-cooled fresh produce were top iced just before shipment. Top icing added considerable dead weight to the load. Top-icing a reefer required in excess of of ice. It had been postulated that as the ice melts, the resulting chilled water would trickle down through the load to continue the cooling process. It was found, however, that top-icing only benefited the uppermost layers of the cargo, and that the water from the melting ice often passed through spaces between the cartons and pallets with little or no cooling effect. It was ultimately determined that top-icing is useful only in preventing an increase in temperature, and was eventually discontinued.\n\nThe typical service cycle for an ice-cooled produce reefer (generally handled as a part of a bice was sporadic) using specially designed field icing cars.\n\nRefrigerator cars required effective insulation to protect their contents from temperature extremes. \"Hairfelt\" derived from compressed cattle hair, sandwiched into the floor and walls of the car, was inexpensive, yet flawed  over its three- to four-year service life it would decay, rotting out the car's wooden partitions and tainting the cargo with a foul odor. The higher cost of other materials such as \"Linofelt\" (woven from flax fibers) or cork prevented their widespread adoption. Synthetic materials such as fiberglass and polystyrene foam, both introduced after World War II, offered the most cost-effective and practical solution.\n\nThe United States Office of Defense Transportation implemented mandatory pooling of class RS produce refrigerator cars from 1941 through 1948. World War II experience found the cars spending 60 percent of their time traveling loaded, 30 percent traveling empty, and 10 percent idle; and indicated the average 14 loads each car carried per year included 5 requiring bunker icing, 1 requiring heating, and 8 using ventilation or top icing.\nFollowing experience with assorted car specifications, the United Fresh Fruit and Vegetable Association (UFF&VA) listed what they considered the best features of ice refrigerator cars in 1948:\n\nIn the latter half of the 20th century, mechanical refrigeration began to replace ice-based systems. Soon after, mechanical refrigeration units replaced the \"armies\" of personnel required to re-ice the cars. The plug door was introduced experimentally by P.F.E. (Pacific Fruit Express) in April 1947, when one of their R-40-10 series cars, #42626, was equipped with one. P.F.E.'s R-40-26 series reefers, designed in 1949 and built in 1951, were the first production series cars to be so equipped. In addition, the Santa Fe Railroad first used plug doors on their SFRD RR-47 series cars, which were also built in 1951. This type of door provided a larger six foot opening to facilitate car loading and unloading. These tight-fitting doors were better insulated and could maintain an even temperature inside the car. By the mid-1970s, the few remaining ice bunker cars were relegated to \"top-ice\" service, where crushed ice was applied atop the commodity.\n\nThe Topeka, Kansas shops of the Santa Fe Railway built five experimental refrigerator cars employing liquid nitrogen as the cooling agent in 1965. A mist induced by liquified nitrogen was released throughout the car if the temperature rose above a pre-determined level. Each car carried of refrigerant and could maintain a temperature of minus 20 degrees Fahrenheit (−30 °C). During the 1990s, a few railcar manufacturers experimented with the use of liquid carbon dioxide (CO) as a cooling agent. The move was in response to rising fuel costs, and was an attempt to eliminate the standard mechanical refrigeration systems that required periodic maintenance. The CO system can keep the cargo frozen solid as long as 14 to 16 days.\n\nSeveral hundred \"cryogenic\" refrigerator cars were placed in service transporting frozen foodstuffs, though they failed to gain wide acceptance (due, in part, to the rising cost of liquid carbon dioxide). Because cryogenic refrigeration is a proven technology and environmentally friendly, the rising price of fuel and the increased availability of carbon dioxide from Kyoto Protocol-induced capturing techniques may lead to a resurgence in cryogenic railcar usage.\n\nSeveral experimental cars were built when wartime production restrictions were relaxed in 1946:\n\n\nDuring the 1930s, the North American Car Company produced a one-of-a-kind, four-wheeled ice bunker reefer intended to serve the needs of specialized shippers who did not generate sufficient product to fill a full-sized refrigerator car. NADX #10000 was a -long, all-steel car that resembled the forty-and-eights used in Europe during World War I. The prototype weighed and was outfitted with a ice bunker at each end. The car was leased to Hormel and saw service between Chicago, Illinois and the southern United States. The concept failed to gain acceptance with eastern railroads and no additional units were built.\n\nThe Santa Fe Refrigerator Despatch (SFRD) briefly experimented with dry ice as a cooling agent in 1931. The compound was readily available and seemed like an ideal replacement for frozen water. Dry ice melts at (versus for conventional ice) and was twice as effective thermodynamically. Overall weight was reduced as the need for brine and water was eliminated. While the higher cost of dry ice was certainly a drawback, logistical issues in loading long lines of cars efficiently prevented it from gaining acceptance over conventional ice. Worst of all, it was found that dry ice can adversely affect the color and flavor of certain foods if placed too closely to them.\n\nIn 1969, the Northern Pacific Railroad ordered a number of modified covered hopper cars from American Car and Foundry for transporting perishable food in bulk. The -long cars were blanketed with a layer of insulation, equipped with roof hatches for loading, and had centerflow openings along the bottom for fast discharge. A mechanical refrigeration unit was installed at each end of the car, where sheet metal ducting forced cool air into the cargo compartments.\n\nThe units, rated at capacity (more than twice that of the largest conventional refrigerator car of the day) were economical to load and unload, as no secondary packaging was required. Apples, carrots, onions, and potatoes were transported in this manner with moderate success. Oranges, on the other hand, tended to burst under their own weight, even after wooden baffles were installed to better distribute the load. The Santa Fe Railway leased 100 of the hoppers from ACF, and in April 1972 purchased 100 new units, known as \"Conditionaire\" cars.\n\nThe cars' irregular, orange-colored outer surface (though darker than the standard AT&SF yellow-orange used on reefers) tended to collect dirt easily, and proved difficult to clean. Santa Fe eventually relegated the cars to more typical, non-refrigerated applications.\n\nThe first refrigerated cars in Japan entered service in 1908 for fish transport. Many of these cars were equipped with ice bunkers, however the bunkers were not used generally. Fish were packed in wooden or foam polystyrene boxes with crushed ice.\n\nFruit and meat transportation in refrigerated rail cars was not common in Japan. For fruits and vegetables, ventilator cars were sufficient due to the short distances involved in transportation. Meat required low temperature storage, transported by ship, since most major Japanese cities are located along the coast.\n\nRefrigerator cars suffered heavy damage in World War II. After the war, the occupation forces confiscated many cars for their own use, utilizing the ice bunkers as originally intended. Supplies were landed primarily at Yokohama, and reefer trains ran from the port to U.S. bases around Japan.\n\nIn 1966, JNR developed \"resa 10000\" and \"remufu 10000\" type refrigerated cars that could travel at They were used in fish freight express trains. \"Tobiuo\" (Flying fish) train from Shimonoseki to Tokyo, and \"Ginrin\" (Silver scale) train from Hakata to Tokyo, were operated.\n\nBy the 1960s, refrigerator trucks had begun to displace railcars. Strikes in the 1970s resulted in the loss of reliability and punctuality, important to fish transportation. In 1986, the last refrigerated cars were replaced by reefer containers.\n\nMost Japanese reefers were four-wheeled due to small traffic demands. There were very few bogie wagons in late years. The total number of Japanese reefers numbered approximately 8,100. At their peak, about 5,000 refrigerated cars operated in the late 1960s. Mechanical refrigerators were tested, but did not see widespread use.\n\nThere were no privately owned reefers in Japan. This is because fish transportation was protected by national policies and rates were kept low, and there was little profit in refrigerated car ownership.\n\nExamples of many styles of refrigerator and ice cars can be found at railroad museums around the world.\n\nThe Western Pacific Railroad Museum at Portola, California features a very complete roster of 20th century cars, including wood bodied ice cars, steel bodied ice cars, one of the earliest mechanical refrigerator cars, later mechanical refrigerator cars and a cryogenic reefer, as well as several \"insulated\" boxcars also used for food transport.\n\n\nStandard refrigerated transport is often utilized for goods with less than 14 days of refrigerated \"shelf life\" — avocados, cut flowers, green leafy vegetables, lettuce, mangoes, meat products, mushrooms, peaches and nectarines, pineapples and papayas, sweet cherries, and tomatoes. \"Express\" reefers are typically employed in the transport of special perishables: commodities with a refrigerated shelf life of less than seven days, such as human blood, fish, green onions, milk, strawberries, and certain pharmaceuticals.\nThe earliest express-service refrigerator cars entered service around 1890, shortly after the first express train routes were established in North America. The cars did not come into general use until the early 20th century. Most units designed for express service are larger than their standard counterparts, and are typically constructed more along the lines of baggage cars than freight equipment. Cars must be equipped with speed-rated trucks and brakes, and — if they are to be run ahead of the passenger car, must also incorporate an air line for pneumatic braking, a communication signal air line, and a steam line for train heating. Express units were typically painted in passenger car colors, such as Pullman green.\n\nThe first purpose-built express reefer emerged from the Erie Railroad Susquehanna Shops on August 1, 1886. By 1927, some 2,218 express cars traveled America's rails, and three years later that number rose to 3,264. In 1940, private rail lines began to build and operate their own reefers, the Railway Express Agency (REA) being by far the largest. In 1948, the REA roster (which would continue to expand into the 1950s) numbered approximately 1,800 cars, many of which were World War II \"troop sleepers\" modified for express refrigerated transport. By 1965, due to a decline in refrigerated traffic, many express reefers were leased to railroads for use as bulk mail carriers.\n\nFor many years, virtually all of the perishable traffic in the United States was carried by the railroads. While railroads were subject to government regulation regarding shipping rates, trucking companies could set their own rate for hauling agricultural products, giving them a competitive advantage. In March 1979, the ICC exempted rail transportation of fresh fruits and vegetables from all economic regulation. Once the \"Agricultural Exemption Clause\" was removed from the \"Interstate Commerce Act\", railroads began aggressively pursuing trailer-on-flatcar (TOFC) business (a form of intermodal freight transport) for refrigerated trailers. Taking this one step further, a number of carriers (including the PFE and SFRD) purchased their own refrigerated trailers to compete with interstate trucks.\n\nIn 1970, Tropicana orange juice was shipped in bulk via insulated boxcars in one weekly round-trip from Bradenton, Florida, to Kearny, New Jersey. By the following year, the company was operating two 60-car unit trains a week, each carrying around of juice. On June 7, 1971, the \"Great White Juice Train\" (the first unit train in the food industry, consisting of 150 insulated boxcars fabricated in the Alexandria, Virginia, shops of Fruit Growers Express) commenced service over the route. An additional 100 cars were soon added, and small mechanical refrigeration units were installed to keep temperatures constant. Tropicana saved $40 million in fuel costs during the first ten years in operation.\n\nIn 2006 Railex LLC launched service in partnership with the Union Pacific Railroad and CSX between Wallula, Washington, and Rotterdam, New York, followed in 2008 by a Delano, California, to NY lane, and Jacksonville, Florida service from the west coast in 2014. Railex runs unit trains of 55 large, \"plate F\" refrigerated cars.\nTwo additional refrigerated unit-train services were announced in 2013, the Green Express, from Tampa, Florida to Kingsbury, Indiana, operated by CSX and the Tampa Port Authority, and the TransCold Express operated by McKay Transcold, LLC and BNSF, connecting the California Central Valley with the midwest.\n\n\n\n"}
{"id": "18779063", "url": "https://en.wikipedia.org/wiki?curid=18779063", "title": "SK-42 reference system", "text": "SK-42 reference system\n\nThe SK-42 reference system also known as the Krasovsky 1940 ellipsoid, is a coordinate system established in the Soviet Union in 1942 as \"Systema koordinat\" (), and provides parameters which are linked to the geocentric Cartesian coordinate system PZ-90. It was used in geodetic calculations, notably in military mapping and determining state borders.\n\nThe Krasovsky 1940 ellipsoid uses a semi-major axis (equatorial radius) of 6,378,245 m, and an inverse flattening of 298.3.\n\n"}
{"id": "15121409", "url": "https://en.wikipedia.org/wiki?curid=15121409", "title": "Scraper (kitchen)", "text": "Scraper (kitchen)\n\nA kitchen scraper is a kitchen implement made of metal, plastics (such as polyethylene, nylon, or polypropylene), wood, rubber or silicone rubber. In practice, one type of scraper is often interchanged with another or with a spatula (thus scrapers are often called spatulas) for some of the various uses.\n\nBowl scrapers (also known as rubber feet) are, as the name suggests, used to remove material from mixing bowls. Often, a plate scraper is used for this purpose, particularly since the long handle allows it to be used to remove contents of bowls as well as jars, such as mayonnaise jars; however, for bowls, dedicated scrapers are available, lacking the handle, and consisting of a flat, flexible piece of plastic or silicone rubber sized for convenient holding with the palm and fingers, with a curved edge to match the curvature of the average bowl. The degree of curvature can vary from a slight curvature along one edge of a rectangle, to a complex shape composed of changing radii to adapt better to bowls of different sizes. Sometimes a hole is provided in one corner, to allow for hanging the utensil, as well as for placement of the thumb to allow for more secure grip. Prices vary from below one American dollar, to as much as $20 American.\n\nThe technique for use of either form of bowl scraper is essentially intuitive.\n\nDough scrapers, or pastry scrapers, are more rigid implements, often made of a metal rectangle with a wooden, plastic, or metal handle running along one long edge not only for more comfortable grip, but also to add rigidity; some bowl scrapers, however, are designed to be stiff enough to serve a dual purpose and are sold as such. Occasionally, an implement resembling a putty knife is sold for this purpose.,\n\nThis implement is used to manipulate raw dough, by scraping it from a surface on which it has been rolled, as well as to slice it. It can also be called a spatula.\n\nA dough scraper is a tool used by bakers to manipulate dough and to clean surfaces on which dough has been worked. It is generally a small sheet of stainless steel (approximately 3\"×5\" or 8 cm × 13 cm) with a handle of wood, plastic, or simply a roll in the steel blade along one of the long sides. \n\nBakers and pastry chefs use this tool to help pick up, turn, and portion dough. When finished, the dough scraper can be used to scrape up the little bits of dough that have dried onto the kneading surface during the forming process. It can also be used in a more generic kitchen role to transfer sliced or diced foods from cutting board to pan.\n\nThis tool is known by a variety of names, including \"dough scraper\", \"dough cutter\", \"dough knife\", \"pastry cutter\", \"bench scraper\", \"board scraper\", and \"bench knife\".\n\nSome modern varieties of this tool have handles of nylon or silicone and even feature both straight and curved edges. These are intended primarily for the use of scraping cutting boards and bowls rather than working dough. These uses have introduced further variants of the name, including terms such as \"bowl scraper\" or \"chopper, scooper, scraper\".\n\nA grill scraper is a device used to clean cooking grills by scraping stuck particles of food from their surface. For flat surfaced grills, their design can vary from similar to a putty knife, to a more complex device with provision to protect the hands from the hot grill surface, targeted to professional cooks and chefs, to even more complex models costing $100 American. Varieties sold for cleaning wire grills are also available, with notches in the edge of the blade to match the wires of the grill.\n\nA plate scraper consists of a plastic, wooden, or metal handle attached to a flexible rubber head. Although the original use of the implement was to remove food from plates before washing, its use has evolved to more of a utilitarian implement, the bowl scraper.\n\nThe pan scraper is, as the name suggests, an implement designed for the forcible removal of tightly stuck or burned food from the bottom of pots and pans before washing. They usually resemble a putty knife with a metal blade and a metal, wood, or plastic handle, sometimes with the handle mounted at an angle to the blade to allow for more vigorous scraping parallel to the surface; others, however, are a wedge shaped piece of hard plastic molded to fit the hand and with a slightly rounded sharp edge.\n\nA shellfish scraper is a specialized utensil used for removing meat from cooked shellfish at the dining table. It consists of a stainless steel rod about ten inches in length, with a flattened tip at one end and a forked tip at the other.\n\nAlthough not a cooking utensil, a crumb scraper is used during a meal to remove crumbs and other unwanted small debris from the surface of table, for cleanliness. Although historically, when crumb scrapers were mostly used in homes, ornate designs were used, the variety most often seen currently is sized and shaped for a waiter to carry in a breast pocket, and consists of a piece of sheet metal bent into a semi-cylindrical shape closely resembling a laboratory scoopula, which is dragged across the table so that the debris is dragged towards the edge, where it can be disposed of.\n\nAlso known as Rubber Spatula. Because it’s flexible, you can scrape dough from the sides of the bowl without scratching it.\n</ref]>\n\n\n"}
{"id": "743067", "url": "https://en.wikipedia.org/wiki?curid=743067", "title": "Scrip", "text": "Scrip\n\nA scrip (or \"chit\" in India) is any substitute for legal tender. It is often a form of credit. Scrips have been created for payment of employees under truck systems, and for use in local commerce at times when regular currency was unavailable, for example in remote coal towns, military bases, ships on long voyages, or occupied countries in wartime. Besides company scrip, other forms of scrip include land scrip, vouchers, token coins such as subway tokens, IOUs, arcade tokens and tickets, and points on some credit cards.\n\nScrips have gained historical importance and become a subject of study in numismatics and exonumia due to their wide variety and recurring use. Scrip behaves similarly to a currency, and as such can be used to study monetary economics.\n\nA variety of forms of scrip were used at various times in the 19th and 20th centuries.\n\nCompany scrip was a credit against the accrued wages of employees. \n\nIn United States mining or logging camps where everything was owned and operated by a single company, scrip provided the workers with credit when their wages had been depleted. These remote locations were cash poor. Workers had very little choice but to purchase food and other goods at a company store. In this way, the company could charge enormous markups on goods in a company store, making workers completely dependent on the company, thus enforcing their \"loyalty\" to the company. Additionally, while employees could exchange scrip for cash, this could rarely be done at face value. This kind of scrip was valid only within the settlement where it was issued. While store owners in neighboring communities could accept the scrip as money, they rarely did so at face value, as it was worth less than that.\n\nWhen U.S. President Andrew Jackson issued his Specie Circular of 1836 due to credit shortages, Virginia Scrip was accepted as payment for federal lands.\n\nIn 19th-century Western Canada, the federal government devised a system of land grants called scrip. Notes in the form of money scrip (valued at $160 or $240) or land scrip, valued at or , were offered to Métis people in exchange for their Aboriginal rights.\n\nDuring the Great Depression, at the height of the crisis, many local governments paid employees in scrip. Vermilion, Alberta was just one example. \n\nIn the U.S., payment of wages in scrip became illegal under the Fair Labor Standards Act of 1938.\n\nThe expression \"scrip\" is also used in the stock market where companies can sometimes pay dividends in the form of additional shares/stock rather than in money. It is also a written document that acknowledges debt.\n\nAfter World War I and World War II, scrip was used as \"emergency money\" or Notgeld in Germany and Austria.\n\nScrip was used extensively in prisoner-of-war (POW) camps during World War II, at least in countries that complied with the Third Geneva Convention. Under the Geneva Conventions, enlisted POWs could be made to work, but had to be paid for their labor, but not necessarily in cash. Since ordinary money could be used in escape attempts, they were given scrip that could only be used with the approval of camp authorities, usually only within the camps. The utility of the scrip varied from case to case. In Germany, in particular, the general lack of goods available at POW camps meant that many Allied POWs in German captivity found little use for their POW scrip (\"Lagergeld\" in German).\n\nPoker chips, also referred to as casino tokens, are commonly used as money with which to gamble. The use of chips as company money in the early 19th century in Devon, England, in the Wheal Friendship copper mine gave its name to a local village: Chipshop.\n\nStamp scrip was a type of local money designed to be circulated and not to be hoarded. \n\nOne type of this worked this way: Each scrip certificate had printed boxes; every month a stamp costing a certain amount (in a typical case, 1% of the face value) had to be purchased and stuck in a box, otherwise the scrip lost all its value, providing a great incentive to spend it quickly. It was used successfully in Germany and Austria in the early 1930s, after national currencies collapsed. National governments considered themselves threatened by the success of stamp scrip projects, and shut them down; similar misgivings discouraged their later use elsewhere.\n\nThe Alberta Social Credit Party government in 1937 issued \"prosperity certificates,\" a form of provincial currency, in an effort to encourage spending. This scrip had boxes in which a stamp equal to 2% of the value had to be affixed each week. Thus, the value of the certificate was covered by the cost of the stamps at year's end when it matured. It is said there were literal showers of dried out stamps cascading to the ground when these certificates were pulled out of pockets in payment for something. But they did give a boost to the provincial economy.\n\nScrip survives in modern times in various forms.\n\nThe use of locally issued scrip accepted by multiple businesses within a community has increased during the Late-2000s recession. Community-wide scrip usage has begun or is on the rise in Ithaca, New York; Detroit; The Berkshires; Pittsboro, North Carolina; Traverse City, Michigan; Lamar, Colorado; Calgary, Canada and Hagen, Germany.\n\nThailand's township Amphoe Kut Chum once issued its own local scrip called \"Bia Kut Chum\": Bia is Thai for cowry shell, which was once used as small change, and still so used in metaphorical expressions. To side-step implications that the community intended their scrip as an unlawful substitute for currency, it now issues exchange coupons called \"Boon\" Kut Chum.\n\nSome companies still issue scrip notes and token coin, good for use at company points of sale. Among these are the Canadian Tire money for the Canadian Tire stores and gasbars in Canada, and the Disney dollars (no longer printed, but still accepted), used at Disney resorts.\n\nIn the retail and fundraising industries, scrip is now issued in the form of gift cards, eCards, or less commonly paper gift certificates. Physical gift cards often have a magnetic strip or optically readable bar code to facilitate redemption at the point of sale.\n\nIn the late 1980s, the term scrip evolved to include a fundraising method popular with non-profit organizations like schools, bands and athletic groups. With scrip fundraising, retailers offer the gift certificates and gift cards to non-profit organizations at a discount. The non-profit organizations sell the gift cards to member's families at full face value. The families redeem the gift cards at full face value, and the discount or rebate is retained by the non-profit organization as revenue.\n\nVISA, MasterCard and American Express gift cards are initially funded by a credit card or bank account, after which the funding account and gift card are not connected to one another. Once the predetermined funds are consumed, the card number expires. A gift of a gift card, maybe in an attractive wrapper, may be seen as more socially acceptable than a gift of cash. It also prevents the gift being spent on something the giver views as undesirable (or used as savings).\n\nHowever, unless the gift card is obtained at a discount (paying less than the actual value of the card), buying scrip with ordinary money is arguably pointless, as it then ties up the money until it is used, and usually it may only be used at one store. Furthermore, not all gift cards issued are redeemed. In 2006, the value of unredeemed gift cards was estimated at almost US$8 billion.\n\nAnother disadvantage of gift cards is that some issuers charge \"maintenance fees\" on the cards, particularly if they are not used after a certain period of time; or the card will expire after a given period of time. Some provinces and states in North America (e.g. California, Ontario, Massachusetts, Ohio, Washington) have enacted laws to eliminate non-use fees or expirations, but because the laws often only apply to single-merchant cards buyers have to review the gift card conditions prior to purchase to determine exact restrictions and fees. Additionally, if a retailer goes bankrupt, gift cards can suddenly become worthless. Even if stores do not close immediately, the company may stop accepting the cards. This became a significant issue during the global financial crisis of 2008–2009, prompting the Consumers Union to call upon the Federal Trade Commission to regulate the issue.\n\nLand scrip was a right to purchase federal public domain land in the United States, a common form of investment in the 19th century. As a type of federal aid to local governments or private corporations, Congress would grant land in lieu of cash. Most of the time the grantee did not seek to acquire any actual land but rather would sell the right to claim the land to private investors in the form of scrip. Often the land title was finalized only after the scrip was resold several times utilizing land agents also called warrant brokers. These grants came in the form of railroad land grants, university land grants, and grants to veterans for war service.\n\n"}
{"id": "1281745", "url": "https://en.wikipedia.org/wiki?curid=1281745", "title": "SeaWiFS", "text": "SeaWiFS\n\nSeaWIFS (Sea-Viewing Wide Field-of-View Sensor) was a satellite-borne sensor designed to collect global ocean biological data. Active from September 1997 to December 2010, its primary mission was to quantify chlorophyll produced by marine phytoplankton (microscopic plants).\n\nSeaWiFS was the only scientific instrument on GeoEye's OrbView-2 (AKA SeaStar) satellite, and was a follow-on experiment to the Coastal Zone Color Scanner on Nimbus 7. Launched August 1, 1997 on an Orbital Sciences Pegasus small air-launched rocket, SeaWiFS began scientific operations on September 18, 1997 and stopped collecting data on December 11, 2010, far exceeding its designed operating period of 5 years. The sensor resolution is 1.1 km (LAC), 4.5 km (GAC). The sensor recorded information in the following optical bands:\n\nThe instrument was specifically designed to monitor ocean characteristics such as chlorophyll-a concentration and water clarity. It was able to tilt up to 20 degrees to avoid sunlight from the sea surface. This feature is important at equatorial latitudes where glint from sunlight often obscures water colour. SeaWiFS had used the Marine Optical Buoy for vicarious calibration.\n\nThe SeaWiFS Mission is an industry/government partnership, with NASA's Ocean Biology Processing Group at Goddard Space Flight Center having responsibility for the data collection, processing, calibration, validation, archive and distribution. The current SeaWiFS Project manager is Gene Carl Feldman.\n\nChlorophyll concentrations are derived from images of the ocean's color. Generally speaking, the greener the water, the more phytoplankton are present in the water, and the higher the chlorophyll concentrations. absorbs more blue and red light than green, with the resulting reflected light changing from blue to green as the amount of chlorophyll in the water increases. Using this knowledge, scientists were able to use ratios of different reflected colors to estimate chlorophyll concentrations. \n\nMany formulas estimate chlorophyll by comparing the ratio of blue to green light and relating those ratios to known chlorophyll concentrations from the same times and locations as the satellite observations. The color of light is defined by its wavelength, and visible light has wavelengths from 400 to 700 nanometers, progressing from violet (400 nm) to red (700 nm). A typical formula used for SeaWiFS data (termed OC4v4) divides the reflectance of the maximum of several wavelengths (443, 490, or 510 nm) by the reflectance at 550 nm. This roughly equates to a ratio of blue light to green light for two of the numerator wavelengths, and a ratio of two different green wavelengths for the other possible combination.\n\nThe reflectance (R) returned by this formula is then plugged into a cubic polynomial that relates the band ratio to chlorophyll.\n\nformula_1\n\nThis formula, along with others, was derived empirically using observed chlorophyll concentrations. To facilitate these comparisons, NASA maintains a system of oceanographic and atmospheric data called SeaBASS (SeaWiFS Bio-optical Archive and Storage System). This data archive is used to develop new algorithms and validate satellite data products by matching chlorophyll concentrations measured directly with those estimated remotely from a satellite. These data can also be used to assess atmospheric correction (discussed below) that also can greatly influence chlorophyll concentration calculations.\n\nNumerous chlorophyll algorithms were tested to see which ones best matched chlorophyll globally. Various algorithms perform differently in different environments. Many algorithms estimate chlorophyll concentrations more accurately in deep clear water than in shallow water. In shallow waters reflectance from other pigments, detritus, and the ocean bottom may cause inaccuracies. The stated goals of the SeaWiFS chlorophyll estimates are \"… to produce water leaving radiances with an uncertainty of 5% in clear-water regions and concentrations within ±35% over the range of 0.05–50 mg m-3.\". When accuracy is assessed on a global scale, and all observations are grouped together, then this goal is clearly met. Many satellite estimates range from one-third to three times of those directly recorded at sea, though the overall relationship is still quite good. Differences arise when examined by region, though overall the values are still very useful. One pixel may not be particularly accurate, though when averages are taken over larger areas, the values average out and provide a useful and accurate view of the larger patterns. The benefits of chlorophyll data from satellites far outweigh any flaws in their accuracy simply by the spatial and temporal coverage possible. Ship-based measurements of chlorophyll cannot come close to the frequency and spatial coverage provided by satellite data.\n\nLight reflected from the sub-surface ocean is called water-leaving radiance and is used to estimate chlorophyll concentrations. However, only about 5–10% of light at the top of the atmosphere is from water-leaving radiance. The remainder of light is reflected from the atmosphere and from aerosols within the atmosphere. In order to estimate chlorophyll concentrations this non-water-leaving radiance must be accounted for. Some light reflected from the ocean, such as from whitecaps and sun glint, must also be removed from chlorophyll calculations since they are representative ocean waves or the angle of the sun instead of the subsurface ocean. The process of removing these components is called atmospheric correction.\n\nA description of the light, or radiance, observed by the satellite's sensor can be more formally expressed by the following radiative transfer equation:\n\nformula_2\n\nWhere L(λ) is total radiance at the top of the atmosphere, L(λ) is Rayleigh scattering by air molecules, L(λ) is scattering by aerosols in the absence of air, L(λ) is interactions between air molecules and aerosols, TL(λ) is reflections from glint, t(L(λ) is reflections from foam, and L(λ)) is reflections from the subsurface of the water, or the water-leaving radiance. Others may divide radiance into some slightly different components, though in each case the reflectance parameters must be resolved in order to estimate water-leaving radiance and thus chlorophyll concentrations.\n\nThough SeaWiFS was designed primarily to monitor ocean concentrations from space, it also collected many other parameters that are freely available to the public for research and educational purposes. These parameters aside from include reflectance, the diffuse attenuation coefficient, particulate organic carbon concentration (POC), particulate inorganic carbon concentration (PIC), colored dissolved organic matter (CDOM) index, photosynthetically active radiation (PAR), and normalized fluorescence line height (NFLH). In addition, despite being designed to measure ocean chlorophyll, SeaWiFS also estimates Normalized Difference Vegetation Index (NDVI), which is a measure of photosynthesis on land.\n\nSeaWiFS data are freely accessible from a variety of websites, most of which are government run. The primary location for SeaWiFS data is NASA's OceanColor website , which maintains the time series of the entire SeaWiFS mission. The website allows users to browse individual SeaWiFS images based on time and area selections. The website also allows for browsing of different temporal and spatial scales with spatial scales ranging from 4 km to 9 km for mapped data. Data are provided at numerous temporal scales including daily, multiple days (e.g., 3, 8), monthly, and seasonal images, all the way up to composites of the entire mission. Data are also available via ftp and bulk download.\n\nData can be browsed and retrieved in a variety of formats and levels of processing, with four general levels from unprocessed to modeled output. Level 0 is unprocessed data that is not usually provided to users. Level 1 data are reconstructed but either unprocessed or minimally processed. Level 2 data contain derived geophysical variables, though are not on a uniform space/time grid. Level 3 data contain derived geophysical variables binned or mapped to a uniform grid. Lastly, Level 4 data contain modeled or derived variables such as ocean primary productivity .\n\nScientists who aim to create calculations of chlorophyll or other parameters that differ from those provided on the OceanColor website would likely use Level 1 or 2 data. This might be done, for example, to calculate parameters for a specific region of the globe, whereas the standard SeaWiFS data products are designed for global accuracy with necessary tradeoffs for specific regions. Scientists who are more interested in relating the standard SeaWiFS outputs to other processes will commonly use Level 3 data, particularly if they do not have the capacity, training, or interest in working with Level 1 or 2 data. Level 4 data may be used for similar research if interested in a modeled product.\n\nNASA offers free software designed specifically to work with SeaWiFS data through the ocean color website. This software, entitled SeaDAS (SeaWiFS Data Analysis System), is built for visualization and processing of satellite data and can work with Level 1, 2, and 3 data. Though it was originally designed for SeaWiFS data, its capabilities have since been expanded to work with many other satellite data sources. Other software or programming languages can also be used to read in and work with SeaWiFS data, such as Matlab, IDL, or Python.\n\nEstimating the amount of global or regional chlorophyll, and therefore phytoplankton, has large implications for climate change and fisheries production. Phytoplankton play a huge role in the uptake of the world's carbon dioxide, a primary contributor to climate change. A percentage of these phytoplankton sink to ocean floor, effectively taking carbon dioxide out of the atmosphere and sequestering it in the deep ocean for at least a thousand years. Therefore, the degree of primary production from the ocean could play a large role in slowing climate change. Or, if primary production slows, climate change could be accelerated. Some have proposed fertilizing the ocean with iron in order to promote phytoplankton blooms and remove carbon dioxide from the atmosphere. Whether these experiments are undertaken or not, estimating chlorophyll concentrations in the world's oceans and their role in the ocean's biological pump could play a key role in our ability to foresee and adapt to climate change.\n\nPhytoplankton is a key component in the base of the oceanic food chain and oceanographers have hypothesized a link between oceanic chlorophyll and fisheries production for some time. The degree to which phytoplankton relates to marine fish production depends on the number of trophic links in the food chain, and how efficient each link is. Estimates of the number of trophic links and trophic efficiencies from phytoplankton to commercial fisheries have been widely debated, though they have been little substantiated. More recent research suggests that positive relationships between and fisheries production can be modeled and can be very highly correlated when examined on the proper scale. For example, Ware and Thomson (2005) found an r of 0.87 between resident fish yield (metric tons km-2) and mean annual concentrations (mg m-3). Others have found the Pacific's Transition Zone Chlorophyll Front (chlorophyll density of 0.2 mg m-3) to be defining feature in loggerhead turtle distribution.\n\n"}
{"id": "3480522", "url": "https://en.wikipedia.org/wiki?curid=3480522", "title": "Silvopasture", "text": "Silvopasture\n\nSilvopasture (Latin, \"silva\" forest) or wood pasture, now also known as agroforestry, is the practice of combining woodland (trees) and the grazing of domesticated animals in a mutually beneficial way. Advantages of a properly managed silvopasture operation are enhanced soil protection and increased long-term income due to the simultaneous production of trees and grazing animals. The trees are managed for high-value sawlogs, brushwood, foliage, fodder and, at the same time, provide shade and shelter for livestock and some forage, reducing stress and sometimes increasing forage production.\n\nPerhaps the oldest agroforestry system used in the temperate regions of the world, silvopastoral systems are characterized by integrating trees with forage and livestock production. Such systems have the potential to increase agricultural production in the long term. The trees have to be repeatedly pollared rather than coppiced so that the trees' re-growths are out of reach of the livestock. After hundreds of years the trees' boles become notably squat but this restriction on size gives the tree a long life. \n\nWood pasture is a historical European land management system in which open woodland provided shelter and forage for grazing animals, particularly sheep and cattle, as well as woodland products such as timber for construction and fuel, coppiced stems for wattle and charcoal making and pollarded poles.\n\nIn the UK, there is a nationwide experiment for silvopastoral systems in which a number of tree species and planting densities are being studied over a range of sites. It is called The Silvopastoral National Network Experiment. Natural England's Environmental Stewardship scheme defines Wood Pasture, in its Farm Environmental Plan booklet, as a structure of open grown or high forested trees in a matrix of grazed grassland, heathland and/or woodland floras.\n\nTheir experience shows that sheep use the trees for shelter from wind. This could provide significant animal welfare benefits. However the fact that the sheep do spend time close to trees results in greater soil compaction close to trees with the greatest compaction when trees are planted at very low densities. It is recommend that trees are planted at no less than 400 per hectare to ensure good establishment.\n\nEvidence of old wood pasture management systems can be detected in many of the ancient woodlands of Scotland, such as Rassal Ashwood in Ross-shire, and at Glen Finglas in the Trossachs. The Dalkeith Old Wood, belonging to the Duke of Buccleuch, where cattle still graze beneath ancient oak trees to this day is designated as a Site of Special Scientific Interest (SSSI) \n\nAncient veteran pollard oaks, and a couple of sweet chestnuts, in western Berkshire's Aldermaston Court's derelict wood pasture.\nSilvopastoral systems are definitely the most prominent agroforestry practice in the United States, particularly in the southeast.\n\n\n\n"}
{"id": "3892697", "url": "https://en.wikipedia.org/wiki?curid=3892697", "title": "Smart Battery", "text": "Smart Battery\n\nA smart battery or a smart battery pack is a rechargeable battery pack with a built-in battery management system (BMS), usually designed for use in a portable computer such as a laptop. In addition to the usual positive and negative terminals, a smart battery has two or more terminals to connect to the BMS; typically the negative terminal is also used as BMS \"ground\". BMS interface examples are: SMBus, PMBus, EIA-232, EIA-485, and Local Interconnect Network.\n\nInternally, a smart battery can measure voltage and current, and deduce charge level and SoH (State of Health) parameters, indicating the state of the cells. Externally, a smart battery can communicate with a smart battery charger and a \"smart energy user\" via the bus interface. A smart battery can demand that the charging stop, request charging, or demand that the smart energy user stop using power from this battery. There are standard specifications for smart batteries: Smart Battery System, MIPI BIF and many ad-hoc specifications.\n\n\n"}
{"id": "718973", "url": "https://en.wikipedia.org/wiki?curid=718973", "title": "Smethwick Engine", "text": "Smethwick Engine\n\nThe Smethwick Engine is a Watt steam engine made by Boulton and Watt, which was installed near Birmingham, England, and was brought into service in May 1779. Now at Thinktank, Birmingham Science Museum, it is the oldest working steam engine and the oldest working engine in the world.\n\nOriginally, it was one of two steam engines used to pump water back up to the summit level of the BCN Old Main Line (Birmingham Canal) canal at Smethwick, not far from the Soho Foundry where it was made. The other engine, also built by Boulton and Watt, was at the other end of the summit level at Spon Lane. In 1804 a second Boulton and Watt engine was added alongside the 1779 engine.\n\nThe engines were needed because local water sources were insufficient to supply water to operate the six locks either side of the canal's original summit. The locks could have been avoided if a tunnel had been built, but the ground was too unstable for James Brindley to build a tunnel using the techniques available at the time. In the 1780s, a cutting was constructed by John Smeaton, enabling three of the six locks on each side to be removed.\nIn the 1820s, Thomas Telford constructed a new canal parallel to the old in a deeper cutting, at the 453 ft Birmingham Level, creating the largest man-made earthworks in the world at the time. It was spanned by the Galton Bridge. The engine was still needed, despite both these developments, and Thomas Telford constructed the Engine Arm Aqueduct carrying the Engine Arm branch canal over his New Main Line so that coal could still be transported along the arm to feed the Smethwick Engine.\nIn 1892, a replacement engine was built in a new pumping house, now Grade II listed, next to Brasshouse Lane, as the original Smethwick Engine was considered uneconomic to repair; the latter was removed for preservation in 1897-98 to the BCN, later British Waterways, Ocker Hill depot where it remained until acquired by Birmingham City Council. It is now part of the collection of Birmingham Museums and is on display at Thinktank, Birmingham Science Museum at Millennium Point. It is the oldest working engine in the world.\nThe engine house was demolished in 1897. Its original site and foundations can still be seen on Bridge Street North in Smethwick, just north of the junction with Rolfe Street. Tours of the site can be arranged through the Galton Valley Canal Heritage Centre which is based in the New Smethwick Pumping Station and regularly opened by Sandwell Museum Service and The Friends of Galton Valley.\n\nThe pumping station was featured in an episode of \"The Water Boatman\" presented by Alan Herd on the Discovery Shed TV channel in November 2011.\n\n\n"}
{"id": "28262", "url": "https://en.wikipedia.org/wiki?curid=28262", "title": "Snowboard", "text": "Snowboard\n\nSnowboards are boards where both feet are secured to the same board, which are wider than skis, with the ability to glide on snow. Snowboards widths are between 6 and 12 inches or 15 to 30 centimeters. Snowboards are differentiated from monoskis by the stance of the user. In monoskiing, the user stands with feet inline with direction of travel (facing tip of monoski/downhill) (parallel to long axis of board), whereas in snowboarding, users stand with feet transverse (more or less) to the longitude of the board. Users of such equipment may be referred to as \"snowboarder\"s. \"Commercial snowboards\" generally require extra equipment such as bindings and special boots which help secure both feet of a snowboarder, who generally rides in an upright position. These types of boards are commonly used by people at ski hills or resorts for leisure, entertainment, and competitive purposes in the activity called snowboarding.\n\nIn 1939, Vern Wicklund, at the age of 13, fashioned a shred deck in Cloquet, Minnesota. This modified sled was dubbed a “bunker\" by Vern and his friends. He, along with relatives Harvey and Gunnar Burgeson, patented the very first snowboard twenty two years later.\n\nHowever, a man by the name of Sherman Poppen, from Muskegon, MI, came up with what most consider the first \"snowboard\" in 1965 and was called the Snurfer (a blend of \"snow\" and \"surfer\") who sold his first 4 \"snurfers\" to Randall Baldwin Lee of Muskegon, MI who worked at Outdoorsman Sports Center 605 Ottawa Street in Muskegon, MI (owned by Justin and Richard Frey or Muskegon). Randy believes that Sherman took an old water ski and made it into the snurfer for his children who were bored in the winter. He added bindings to keep their boots secure. (Randy Lee, October 14, 2014) Commercially available Snurfers in the late 1960s and early 1970s had no bindings. The snowboarder held onto a looped nylon lanyard attached to the front of the Snurfer, and stood upon several rows of square U-shaped staples that were partially driven into the board but protruded about 1 cm above the board's surface to provide traction even when packed with snow. Later Snurfer models replaced the staples with ridged rubber grips running longitudinally along the length of the board (originally) or, subsequently, as subrectangular pads upon which the snowboarder would stand. It is widely accepted that Jake Burton Carpenter (founder of Burton Snowboards) and/or Tom Sims (founder of Sims Snowboards) invented modern snowboarding by introducing bindings and steel edges to snowboards.\n\nIn 1981, a couple of Winterstick team riders went to France at the invitation of Alain Gaimard, marketing director at Les Arcs. After seeing an early film of this event, French skiers/surfers Augustin Coppey, Olivier Lehaneur, Olivier Roland and Antoine Yarmola made their first successful attempts during the winter of 1983 in France (Val Thorens), using primitive, home-made clones of the Winterstick. Starting with pure powder, skateboard-shaped wooden-boards equipped with aluminium fins, foot-straps and leashes, their technology evolved within a few years to pressed wood/fiber composite boards fitted with polyethylene soles, steel edges and modified ski boot shells. These were more suitable for the mixed conditions encountered while snowboarding mainly off-piste, but having to get back to ski lifts on packed snow.\nIn 1985, James Bond popularized snowboarding in the movie \"A View to a Kill\". In the scene, he escapes Soviet agents who are on skis. The snowboard he used was from the debris of a snowmobile that exploded.\n\nAt the same time the Snurfer was turning into a snowboard on the other side of the iron curtain.\nIn 1980, Aleksey Ostatnigrosh and Alexei Melnikov - two members of the only Snurfer club in the Soviet Union started changing the Snurfer design to allow jumping and to improve control on hard packed snow. Being completely unaware of the developments in the Snurfer/snowboard world, they attached a bungee cord to the Snurfer tail which the rider could grab before jumping. Later, in 1982, they attached a foot binding to the Snurfer. The binding was only for the back foot, and had a release capability.\nIn 1985, after several iterations of the Snurfer binding system, Aleksey Ostatnigrosh made the first Russian snowboard. The board was cut out of a single vinyl plastic sheet and had no metal edges. The bindings were attached by a central bolt and could rotate while on the move or be fixed at any angle.\nIn 1988, OstatniGROsh and MELnikov started the first Russian snowboard manufacturing company, GROMEL.\n\nBy 1986, although still very much a minority sport, commercial snowboards started appearing in leading French ski resorts.\n\nIn 2008, selling snowboarding equipment was a $487 million industry. In 2008, average equipment ran about $540 including board, boots, and bindings.\n\nThe bottom or 'base' of the snowboard is generally made of UHMW and is surrounded by a thin strip of steel, known as the 'edge'. Artwork was primarily printed on PBT using a sublimation process in the 1990s, but poor color retention and fade after moderate use moved high-end producers to longer-lasting materials.\n\nSnowboards come in several different styles, depending on the type of riding intended:\n\nSnowboards are generally constructed of a hardwood core which is sandwiched between multiple layers of fibreglass. Some snowboards incorporate the use of more exotic materials such as carbon fiber, Kevlar, aluminium (as a honeycomb core structure), and have incorporated piezo dampers. The front (or \"nose\") of the board is upturned to help the board glide over uneven snow. The back (or \"tail\") of the board is also upturned to enable backwards (or \"switch\") riding. The base (the side of the board which contacts the ground) is made of Polyethylene plastic. The two major types of base construction are extruded and sintered. An extruded base is a basic, low-maintenance design which basically consists of the plastic base material melted into its form. A sintered base uses the same material as an extruded base, but first grinds the material into a powder, then, using heat and pressure, molds the material into its desired form. A sintered base is generally softer than its extruded counterpart, but has a porous structure which enables it to absorb wax. This wax absorption (along with a properly done 'hot wax'), greatly reduces surface friction between the base and the snow, allowing the snowboard to travel on a thin layer of water. Snowboards with sintered bases are much faster, but require semi-regular maintenance and are easier to damage. The bottom edges of the snowboard are fitted with a thin strip of steel, just a couple of millimeters wide. This steel edge allows the board to grab or 'dig into' hard snow and ice (like the blade of an ice skate), and also protects the boards internal structure. The top of the board is typically a layer of acrylic with some form of graphic designed to attract attention, showcase artwork, or serve the purpose similar to that of any other form of printed media. Flite Snowboards, an early designer, pressed the first closed-molded boards from a garage in Newport, Rhode Island, in the mid-1980s. Snowboard topsheet graphics can be a highly personal statement and many riders spend many hours customizing the look of their boards. The top of some boards may even include thin inlays with other materials, and some are made entirely of epoxy-impregnated wood. The base of the board may also feature graphics, often designed in a manner to make the board's manufacturer recognizable in photos.\n\nSnowboard designs differ primarily in:\n\nThe various components of a snowboard are:\n\n\n\n\nAmongst Climate Change, the winter sports community is a growing environmentalist group, whom depend on snowy winters for the survival of their culture. This movement is, in part, being energized by a nonprofit named \"Protect Our Winters\" and the legendary rider Jeremy Jones. The organization provides education initiatives, support for community based projects, and is active in climate discussions with the government. Alongside this organization, there are many other winter sports companies who see the ensuing calamity and are striving to produce products that are less damaging to the environment. Snowboard manufacturers are adapting to decreasing supplies of petroleum and timber with ingenious designs.\n\n\nWhen it comes down to it \"the least of our worries will be that skiers and snowboarders don't get to go play,\" says Jeremy Jones.\n\nSnowboard boots are mostly considered soft boots, though alpine snowboarding uses a harder boot similar to a ski boot. A boot's primary function is to transfer the rider's energy into the board, protect the rider with support, and keep the rider's feet warm. A snowboarder shopping for boots is usually looking for a good fit, flex, and looks. Boots can have different features such as lacing styles, heat molding liners, and gel padding that the snowboarder also might be looking for. Tradeoffs include rigidity versus comfort, and built in forward lean, versus comfort.\n\nThere are three incompatible types:\n\nBindings are separate components from the snowboard deck and are very important parts of the total snowboard interface. The bindings' main function is to hold the rider's boot in place tightly to transfer their energy to the board. Most bindings are attached to the board with three or four screws that are placed in the center of the binding. Although a rather new technology from Burton called Infinite channel system uses two screws, both on the outsides of the binding.\n\nThere are several types of bindings. Strap-in, step-in, and hybrid bindings are used by most recreational riders and all freestyle riders.\n\nThese are the most popular bindings in snowboarding. Before snowboard specific boots existed, snowboarders used any means necessary to attach their feet to their snowboards and gain the leverage needed for turning. Typical boots used in these early days of snowboarding were Sorels or snowmobile boots. These boots were not designed for snowboarding and did not provide the support desired for doing turns on the heel edge of a snowboard. As a result, early innovators such as Louis Fournier conceived the \"high-back\" binding design which was later commercialized and patented by Jeff Grell. The highback binding is the technology produced by most binding equipment manufacturers in the snowboard industry. The leverage provided by highbacks greatly improved board control. Snowboarders such as Craig Kelly adapted plastic \"tongues\" to their boots to provide the same support for toe-side turns that the highback provided for heel-side turns. In response, companies such as Burton and Gnu began to offer \"tongues\".\n\nWith modern strap bindings, the rider wears a boot which has a thick but flexible sole, and padded uppers. The foot is held onto the board with two buckle straps – one strapped across the top of the toe area, and one across the ankle area. They can be tightly ratcheted closed for a tight fit and good rider control of the board. Straps are typically padded to more evenly distribute pressure across the foot. While nowhere near as popular as two-strap bindings, some people prefer three-strap bindings for more specialized riding such as carving. The third strap tends to provide additional stiffness to the binding.\n\nCap-strap bindings are a recent modification that provide a very tight fit to the toe of the boot, and seats the boot more securely in the binding. Numerous companies have adopted various versions of the cap strap.\n\nInnovators of step-in systems produced prototypes and designed proprietary step-in boot and binding systems with the goal of improving the performance of snowboard boots and bindings, and as a result, the mid-90s saw an explosion of step-in binding and boot development. New companies, Switch and Device, were built on new step-in binding technology. Existing companies Shimano, K2 and Emery were also quick to market with new step-in technology. Meanwhile, early market leaders Burton and Sims were noticeably absent from the step-in market. Sims was the first established industry leader to market with a step-in binding. Sims licensed a step-in system called DNR which was produced by the established ski-binding company Marker. Marker never improved the product which was eventually discontinued. Sims never re-entered the step-in market.\n\nThe risk of commercial failure from a poorly performing Step-in binding presented serious risk to established market leaders. This was evidenced by Airwalk who enjoyed 30% market share in snowboard boot sales when they began development of their step-in binding system. The Airwalk step-in System experienced serious product failure at the first dealer demonstrations, seriously damaging the company's credibility and heralded a decline in the company's former position as the market leader in Snowboard boots. Established snowboarding brands seeking to gain market share while reducing risk, purchased proven step-in innovators. For example, snowboard boot company Vans purchased the Switch step-in company, while Device step-in company was purchased by Ride Snowboards.\n\nAlthough initially refusing to expose themselves to the risk and expense associated with bringing a step-in system to market, Burton chose to focus primarily on improvements to existing strap-in technology. However, Burton eventually released 2 models of step-in systems, the SI and the PSI, Burton's SI system enjoyed moderate success, yet never matched the performance of the company's strap-in products and was never improved upon. Burton never marketed any improvements to either of their step-in binding systems and eventually discontinued the products.\n\nMost Popular (and incompatible) step-in systems used unique and proprietary mechanisms, such as the step-ins produced by Burton, Rossignol and Switch. Shimano and K2 used a technology similar to clipless bicycle pedals. Burton and K2 Clicker step-in binding systems are no longer in production as both companies have opted to focus on the strap-in binding system. Rossignol remains as the sole provider of step-in binding systems and offers them primarily to the rental market as most consumers and retailers alike have been discouraged by lack of adequate development and industry support for step-in technology.\n\nThere are also proprietary systems that seek to combine the convenience of step-in systems with the control levels attainable with strap-ins. An example is the Flow binding system, which is similar to a strap-in binding, except that the foot enters the binding through the back. The back flips down and allows the boot to slide in; it's then flipped up and locked into place with a clamp, eliminating the need to loosen and then re-tighten straps every time the rider frees and then re-secures their rear foot. The rider's boot is held down by an adjustable webbing that covers most of the foot. Newer Flow models have connected straps in place of the webbing found on older models; these straps are also micro adjustable. In 2004, K2 released the Cinch series, a similar rear-entry binding; riders slip their foot in as they would a Flow binding, however rather than webbing, the foot is held down by straps.\n\nA stiff molded support behind the heel and up the calf area. The HyBak was originally designed by inventor Jeff Grell and built by Flite Snowboards. This allows the rider to apply pressure and effect a \"heelside\" turn. Some high backs are stiff vertically but provide some flex for twisting of the riders legs.\n\nPlate bindings are used with hardboots on Alpine or racing snowboards. Extreme carvers and some Boarder Cross racers also use plate bindings. The stiff bindings and boots give much more control over the board and allow the board to be carved much more easily than with softer bindings. Alpine snowboards tend to be longer and thinner with a much stiffer flex for greater edge hold and better carving performance.\n\nSnowboard bindings, unlike ski bindings, do not automatically release upon impact or after falling over. With skis, this mechanism is designed to protect from injuries (particularly to the knee) caused by skis torn in different directions. Automatic release is not required in snowboarding, as the rider's legs are fixed in a static position and twisting of the knee joint cannot occur to the same extent. Furthermore, it reduces the dangerous prospect of a board hurtling downhill riderless, and the rider slipping downhill on his back with no means to maintain grip on a steep slope. Nevertheless, some ski areas require the use of a \"leash\" that connects the snowboard to the rider's leg or boot, in case the snowboard manages to get away from its rider. This is most likely to happen when the rider removes the board at the top or the bottom of a run (or while on a chairlift, which could be dangerous).\n\nA Noboard is a snowboard binding alternative with only peel and stick pads applied directly to any snowboard deck and no attachment.\n\nStomp pads, which are placed between the bindings closer to the rear binding, allow the rider to better control the board with only one boot strapped in, such as when maneuvering onto a chair lift, riding a ski tow or performing a one footed trick. Whereas the upper surface of the board is smooth, the stomp pad has a textured pattern which provides grip to the underside of the boot. Stomp pads can be decorative and vary in their size, shape and the kind and number of small spikes or friction points they provide.\n\nThere are two types of stance-direction used by snowboarders. A \"regular\" stance places the rider's left foot at the front of the snowboard. \"Goofy\", the opposite stance direction, places the rider's right foot at the front, as in skateboarding. Regular is the most common. There are different ways to determine whether a rider is \"regular\" or \"goofy\". One method used for first time riders is to observe the first step forward when walking or climbing up stairs. The first foot forward would be the foot set up at the front of the snowboard. Another method used for first time riders is to use the same foot that you kick a football with as your back foot (though this can be an inaccurate sign for some, as there are people who prefer goofy though are right handed, and therefore naturally kick a football with their right foot). This is a good method for setting up the snowboard stance for a new snowboarder. However having a surfing or skateboarding background will also help a person determine their preferred stance, although not all riders will have the same stance skateboarding and snowboarding. Another way to determine a rider's stance is to get the rider to run and slide on a tiled or wooden floor, wearing only socks, and observe which foot the person puts forward during the slide. This simulates the motion of riding a snowboard and exposes that persons natural tendency to put a particular foot forward. Another method is to stand behind the first-timer and give them a shove, enough for them to put one foot forward to stop themselves from falling. Other good ways of determining which way you ride are rushing a door (leading shoulder equals leading foot) or going into a defensive boxing stance (see which foot goes forward).\n\nMost experienced riders are able to ride in the opposite direction to their usual stance (i.e. a \"regular\" rider would lead with their right foot instead of their left foot). This is called riding \"fakie\" or \"switch\".\n\nStance width helps determine the rider's balance on the board. The size of the rider is an important factor as well as the style of their riding when determining a proper stance width. A common measurement used for new riders is to position the bindings so that the feet are placed a little wider than shoulder width apart. Another, less orthodox form of measurement may be taken by putting your feet together and place your hands, palm down, on the ground in a straight line with your body by squatting down. This generally gives a good natural measurement for how wide of a base your body uses to properly balance itself when knees are bent. However, personal preference and comfort are important and most experienced riders will adjust the stance width to personal preference. Skateboarders should find that their snowboarding and skateboarding stance widths are relatively similar.\n\nA wider stance, common for freestyle riders, gives more stability when landing a jump or jibbing a rail. Control in a wider stance is reduced when turning on the piste. Conversely a narrow stance will give the rider more control when turning on the piste but less stability when freestyling. A narrow stance is more common for riders looking for quicker turn edge-hold (i.e. small radius turns). The narrow stance will give the rider a concentrated stability between the bindings allowing the board to dig into the snow quicker than a wider stance so the rider is less prone to wash out.\n\nBinding angle is defined by the degrees off of perpendicular from the length of the snowboard. A binding angle of 0° is when the foot is perpendicular to the length of the snowboard. Positive angles are pointed towards the front of the board, whereas negative angles are pointed towards the back of the board. The question of \"how much\" the bindings are angled depends on the rider's purpose and preference. Different binding angles can be used for different types of snowboarding. Someone who participates in freestyle competition would have a much different \"stance\" than someone who explores backcountry and powder. The recent advancement and boom of snowboard culture and technology has made binding angle adjustments relatively easy. Binding companies design their bindings with similar baseplates that can easily mount onto any type of snowboard regardless of the brand. With the exception of Burton, and their newly released \"channel system\", adjusting bindings is something that remains constant among all snowboarders. Done with a small screw-driver or a snowboard tool, the base plates on bindings can be easily rotated to whatever preferred stance. One must un-screw the baseplate, pick their degree angles, and then re-screw the baseplates. Bindings should also regularly be checked to ensure that the screws don't come undone from the movements of snowboarding. \n\n\n\n\n\n"}
{"id": "4547081", "url": "https://en.wikipedia.org/wiki?curid=4547081", "title": "Sorbent tube", "text": "Sorbent tube\n\nSorbent tubes are the most widely used collection media for sampling hazardous gases and vapors in air, mostly as it relates to Industrial hygiene. They were developed by the US National Institute for Occupational Safety and Health (NIOSH) for air quality testing of workers. Sorbent Tubes are available from SKC Inc., 7Solutions BV, Uniphos Ltd., SKC Ltd, Zefon International, Sigma-Aldrich/Supelco and Markes International. SKC Inc. manufactured the first commercially available sorbent tubes.\n\nSorbent tubes are typically made of glass and contain various types of solid adsorbent material (sorbents). Commonly used sorbents include activated charcoal, silica gel, and organic porous polymers such as Tenax and Amberlite XAD resins. Solid sorbents are selected for sampling specific compounds in air because they:\n\n\nSorbent tubes are attached to air sampling pumps for sample collection. A pump with a calibrated flow rate in ml/min is normally placed on a worker’s belt and it draws a known volume of air through the sorbent tube. Alternatively, pumps and sorbent tubes are placed in areas for fixed-point sampling. Chemicals are trapped onto the sorbent material throughout the sampling period.\n\nOccasionally, when desorbing the air sample from the sorbent tube, a large portion of the analyte will fail to go into the solution. In these cases, the sorbent tubes will have to be adjusted for desorption efficiency (DE).\n\n"}
{"id": "50079645", "url": "https://en.wikipedia.org/wiki?curid=50079645", "title": "Soviet integrated circuit designation", "text": "Soviet integrated circuit designation\n\nThis article describes the nomenclature for integrated circuits manufactured in the Soviet Union. 25 years after the dissolution of the Soviet Union this designation is still used by a number of manufacturers in Russia, Belarus, Ukraine, Latvia, and Uzbekistan. The designation uses the Cyrillic alphabet which sometimes leads to confusion where a Cyrillic letter has the same appearance as a Latin letter but is romanized as a different letter. Furthermore, for some Cyrillic letters the Romanization is ambiguous.\nThe nomenclature for integrated circuits has changed somewhat over the years as new standards were published:\nThroughout this article the standards are referred to by the year they came into force. Before 1968 each manufacturer used its own integrated circuit designation. Following the dissolution of the Soviet Union in 1991, the standards were not as strictly enforced anymore and a number of manufacturers introduced manufacturer-specific designations again. These were typically used in parallel with the standards. However, integrated circuits for military, aerospace, and nuclear applications in Russia still have to follow the standard designation. Underlining this, the 2010 standard is explicitly labelled a Russian military standard. Beside Russia the 2010 standard is applied in Belarus as well. Companies in Ukraine mostly stayed with the 1980 standard and prefixed the designation with the letter \"У\" (U), e.g. \"УМ5701ВЕ51\". The 1980 standard was published in Ukraine as DSTU 3212—95 (). Bulgarian designations for bipolar integrated circuits, e.g. , look confusingly similar to the 1968 Soviet designations but the standards differ. The functional group is also indicated by two letters in the cyrillic alphabet and many groups were obviously copied from the Soviet standard (\"АГ\", \"ИД\", \"ИЕ\", \"ЛБ\", \"ЛН\", \"ЛП\", \"МП\", \"ПК\", \"СА\", \"УС\"). Some subgroups differ (\"ТД\", \"УМ\", \"УО\") and some groups are completely different (\"НС\", \"ОИ\", \"РН\"). For the number after the functional group there is no concept of a series. Instead, that number usually matches the Western counterpart (e.g. the \"1УО709С\" is equivalent to a μA709).\nAlso as a consequence of dissolution of the Soviet Union, COCOM restrictions were lifted and Russian integrated circuit design firms gained access to foundries abroad. In that sense it could be argued that the importance of the Soviet designation has spread across the globe. When foundries are not able to label the circuit in the Cyrillic alphabet then the Latin alphabet is used (e.g. KF1174PP1).\n\nIn general, devices already in production when a new standard came out kept their old designation. However, in some case devices were renamed:\n\nElements:\n\nThe package of an integrated circuit was generally not indicated in the 1973 designation, except:\n\nFor bare chips without a package an additional digit indicates the constructive variant. For the 1973 and 1980 standards the variant digit is appended with a dash after the designation (e.g. \"К712РВ2-1\" and , respectively). For the 2000 and 2010 standards the variant digit follows immediately after the package designation N (e.g. \"5862ПФ1Н4\" and \"1374МХ01Н1\", respectively).\nA manufacturer designation was introduced only with the 2000 standard. The table below is incomplete, many manufacturers still do not use their assigned designation. Manufacturer logos are more common.\n\nOther manufacturers which as of 2016 used a version of the Soviet integrated circuit designation include NTC Module, MCST, ELVEES Multicore, Fizika, Optron, Sapfir, NPK TTs, and Progress, all of them in Moscow, as well as PKK Milandr, Soyuz, and NIITAP in Zelenograd, NIIET Voronesh, SKTB ES Voronesh, Proton and Proton-Impuls Oryol, Vostok Novosibirsk, Orbita Saransk, SIT Bryansk, NZPP-KBR Nalchik, Planeta Novgorod, Iskra Ulyanovsk, NIIEMP Penza, Almaz Kotovsk, Eltom Tomilino, NPK Daleks Alexandrov, DELS Minsk, Kvazar Kiev, Kristall Kiev, Elektronni Komponenti Ivano-Frankivsk, Dnepr Kherson, and Foton Tashkent.\n\nAlthough not strictly part of the designation, a number of markings are often found on integrated circuit packages:\n\nFor mask-programmed devices (e.g. gate arrays, mask-programmed single chip microcontrollers, Mask ROMs) a three or four digit mask number follows the type designation (e.g. \"К1801ВП1-014\").\n\nFor bare chips a one digit constructive variant identifier follows the type designation.\n\nA date code is usually printed on the package. In the early 1970s the date code consisted of a Roman numeral for the month and a two-digit year (e.g. IX 72). Later the month was given as one or two digits (e.g. 5-73 or 0386). In the late 1980s most plants switched to a 4-digit code with a 2-digit year followed by a 2-digit month (e.g. 8909) or a 2-digit week (e.g. 9051). Overall, the date code format was not strictly enforced. At least the 1821 series of integrated circuits bore an IEC 60062 letter and digit code (e.g. A1 for January 1990).\n\nThe Romanization of Russian is standardized, only there are at least 11 standards to choose from. Fortunately, the Soviet integrated circuit designation uses a subset of the Cyrillic alphabet where rather few letters are ambiguous:\nThe more common romanizations in bold are given as alternatives in the above tables.\n\n\"Е\" and \"Э\" are both romanized as E.\n\nIt should be noted that the and the differ in some letters from the one used in English. For instance, the Russian \"КР580ВМ80A\" becomes KR580VM80A in English and French but KR580WM80A in German literature.\n\n"}
{"id": "1521688", "url": "https://en.wikipedia.org/wiki?curid=1521688", "title": "TDK", "text": "TDK\n\n, formerly , is a Japanese multinational electronics company that manufactures electronic materials, electronic components, and recording and data-storage media. Its motto is \"Contribute to culture and industry through creativity\".\n\n\"TDK\" is an initialism of the original Japanese name of the company: Tokyo Denki Kagaku Kōgyō K.K. (Tokyo Electric Chemical Industry Co., Ltd.). The company is listed on the Tokyo Stock Exchange and is a constituent of the Nikkei 225 and TOPIX indices.\n\nTDK was founded in Tokyo, Japan, on 7 December 1935 to manufacture the iron-based magnetic material ferrite, which had been recently invented by Yogoro Kato and Takeshi Takei. In 1952 and 1957 they began production of magnetic tapes, with compact cassette tapes following in 1966; it is for these that the company is most widely noted. TDK used to manufacture an extensive portfolio of magnetic and optical media, including several formats of videotape and blank CD-R and recordable DVD discs until the recording business was sold to Imation in 2007.\n\nOperations in the USA began in 1965 with a New York City office, and European operations began in 1970 with an office in Frankfurt, West Germany.\n\nIn 1980, TDK developed a multilayering technology to create chip capacitors and inductors inside personal computers, laptops, smartphones, and other electronic devices.\n\nIn 1986, TDK acquired SAE Magnetics and introduced high-density recording heads to their product offerings.\n\nIn the 1990s TDK's Mass Storage Division included brushless DC spindle motors, magnetoresistance (MR) heads, and thin-film heads.\n\nSince 1997 TDK has gradually withdrawn from the production of compact cassettes. First with the MA-X and AR (\"Acoustic Response\"), then the AD (\"Acoustic Dynamic\") and SA-X line in 2001 and 2002 respectively, then the MA (\"Metal Alloy\") line in 2004. The SA (\"Super Avilyn\") and D (\"Dynamic\") lines were withdrawn in 2012 under Imation ownership. Industry trends see the company moving into new forms of media; in 2004 TDK was the first media manufacturer to join the companies developing BD post-DVD technology. \nTDK operated a semiconductor division in California for about a decade, but divested it in 2005.\n\nIn late 2007, Imation acquired TDK's recording business, including flash media, optical media, magnetic tape, and accessories, for $300 million. This also included a license to use the \"TDK Life on Record\" brand on data storage and audio products for 25 years. In September 2015, Imation announced that it had agreed to relinquish this license and would cease selling TDK-branded products by the end of the year.\n\nSince the 2000s, TDK has turned its focus to the development, manufacture and sales of electronic components, HDD heads and suspension, and power supplies.\n\nBeginning in 2005, TDK has acquired a manner of electronic device manufacturers including passive component developers, sensors manufacturers and power supply companies. These areas remain TDK’s focus today.\n\nSince 2016, Shigenao Ishiguro has been President and CEO of TDK.\n\n\nTDK has sponsored the IAAF World Championships in Athletics since the 1983 inaugural event in Helsinki.\n\nTDK sponsored Ajax for several years in the 1980s during which it won the European Cupwinners Cup in 1987. From 1993 to 1999, TDK were also the sponsors of the English football club Crystal Palace, who were promoted to the Premier League twice during this era, though lasting for just one season before being relegated on both occasions. TDK was also a minor sponsor of the Brisbane Broncos Rugby League Team during the early 90's. It is a current sponsor of the IAAF World Championships in Athletics. It also sponsors activities and events such as those at The Cross nightclub in Central London, and since 1990 has had a prominent sign at Piccadilly Circus although it was announced in November 2014 that they would not be renewing the contract.\n\nTDK has owned a sign on One Times Square since 2000. The screen is placed under that of Toshiba and can be seen during the annual Times Square New Year's Ball Drop.\n\nSince 2001, TDK has supported performances of some of the world’s distinguished orchestras in Japan within the company’s \"TDK Orchestra Concerts\" program. TDK's \"Outreach-Mini Concerts\" and \"Special Rehearsals and Main Concert Invitations\" additionally serve as avenues for the company to attract younger audiences.\n\nIn 2002, the company's consumer electronics division was the presenting sponsor of the Third Annual Jammy Awards, with the TDK Live Performance of the Year award honoring the best live performance that was legally available on the Web as a free download. The award was given to the band moe. for their performance at the Bonnaroo Music Festival.\n\nTDK's own football club, based in Nikaho, Akita, recently split from the corporation to become independent football club Blaublitz Akita, with the aim for the professional leagues.\n\nTDK operates a company museum in Nikaho, Akita, Japan. The museum is open to the public, free of charge. Among its exhibits are a comprehensive history of the company, its products and technologies, and emerging developments.\n\n"}
{"id": "18740459", "url": "https://en.wikipedia.org/wiki?curid=18740459", "title": "Technical translation", "text": "Technical translation\n\nTechnical translation is a type of specialized translation involving the translation of documents produced by technical writers (owner's manuals, user guides, etc.), or more specifically, texts which relate to technological subject areas or texts which deal with the practical application of scientific and technological information. While the presence of specialized terminology is a feature of technical texts, specialized terminology alone is not sufficient for classifying a text as \"technical\" since numerous disciplines and subjects which are not \"technical\" possess what can be regarded as specialized terminology. Technical translation covers the translation of many kinds of specialized texts and requires a high level of subject knowledge and mastery of the relevant terminology and writing conventions.\n\nThe importance of consistent terminology in technical translation, for example in patents, as well as the highly formulaic and repetitive nature of technical writing makes computer-assisted translation using translation memories and terminology databases especially appropriate. In his book \"Technical Translation\" Jody Byrne argues that technical translation is closely related to technical communication and that it can benefit from research in this and other areas such as usability and cognitive psychology.\n\nIn addition to making texts with technical jargon accessible for a wider ranging audience, technical translation also involves linguistic features of translating technological texts from one language to another.\n\nTranslation as a whole is a balance of art and science influenced by both theory and practice. Having knowledge of both the linguistic features as well as the aesthetic features of translation applies directly to the field of technical translation.\n\nAs a field, technical translation has been recognized, studied, and developed since the 1960s. Stemming from the field of translation studies, the field of technical translation traditionally emphasized much importance on the source language from which text is translated. However, over the years there has been a movement away from this traditional approach to a focus on the purpose of the translation and on the intended audience. This is perhaps because only 5–10% of items in a technical document are terminology, while the other 90–95% of the text is language, most likely in a natural style of the source language.\nThough technical translation is only one subset of the different types of professional translation, it is the largest subset as far as output is concerned. Currently, more than 90% of all professionally translated work is done by technical translators, highlighting the importance and significance of the field.\n\nThe role of the technical translator is to not only be a transmitter of information, but also to be a constructor of procedural discourse and knowledge through meaning, particularly because often, the technical translator may also take on the role of the technical writer. Research has demonstrated that technical communicators do, in fact, create new meaning as opposed to simply repackaging (198) old information. This emphasizes the important role that technical translators play in making meaning, whether they are doing technical translation in one language or in multiple languages.\n\nMuch like professionals in the field of technical communication, the technical translator must have a cross-curricular and multifaceted background. In addition to grasping theoretical and linguistic orientations for the actual translation process, an understanding of other subjects, such as cognitive psychology, usability engineering, and technical communication, is necessary for a successful technical translator. Additionally, most technical translators work within a specialized field such as medical or legal technical translation, which highlights the importance of an interdisciplinary background.\nFinally, the technical translators should also become familiar with the field of professional translation through training.\n\nTechnical translation requires a solid knowledge base of technological skills, particularly if the translator chooses to utilize computer-assisted translation (CAT) or machine translation (MT). Though some technical translators complete all translation without the use of CAT or MT, this is often with pieces that require more creativity in the document. Documents dealing with mechanics or engineering that contain frequently translated phrases and concepts are often translated using CAT or MT.\n\nTechnical translators can leverage translation memory to build a bilingual repository of technical terms and abstract concepts. This can save them considerable time on future technical translations. This especially applies to the translation of technical documents, which typically have a long shelf life, but require regular updating.\n\nAnalysis\n\nTranslators might read the document to understand what they will be translating, and determine the context of the text. In technical translation, the register and tone would then be determined based on the type of text and the context, although generally the tone of technical texts are neutral. The register can be very formal and scientific, or made to be easily understood by the general public. A translator might also need to use documentation techniques find resource materials as aids in order to translate the text.\n\nComprehension\n\nDepending on the translator's experience and nature or the text, the translator might need to assess the degree of difficulty and type of difficulty in a text, such as whether they are able to translate the text properly in a timely manner, or whether there are more specific translation problems that they do not understand. Often, translators may have an area of expertise, and may be very familiar with certain terminology and texts. However, when a translator cannot learn all of the subject knowledge, it is possible to transfer over knowledge from other subjects that might be similar in nature, or do some research.\n\nResearch enables translators to have a “good and solid understanding of the basic principles and technologies…” The translator must not only translate the terminology, but also the style in which the author originally wrote the document, to create the same effect in the target language. Along with previous subject knowledge, research helps the translator understand the basics of the text. Some of the tools a technical translator might use as well are glossaries, encyclopedias, and technical dictionaries, most of which may be recently published, as technology evolves quickly. The translator must always keep up to date with new technologies in the field they are translating into as well, by attending conferences or courses, or subscribing to magazines, so that they are using the latest terminology.\n\nIn the case of terminological or language issues that the translator cannot solve on their own, the translator may do research or call on the experts of a particular field for more clarification and explanations. This includes working with all types of workers in certain technological and industrial fields, such as engineers, managers, etc. Two types of experts that a translator may consult while translating are the author who wrote the text in the source language and the expert in the target language. The author can explain the context and what they are trying to say, whereas the expert in the target language may be able to explain the terminology or what the author was trying to convey in the target language. Translation is teamwork rather than strict cooperation between the translator and the experts. However, if the information the experts provided does not resolve problems, for example, if there are terms that are difficult to translate and some that cannot be translated, it may be possible to explain concepts in the target language through examples.\n\nTranslation\n\nTranslators may bounce back and forth between steps, depending on their time constraints and their experience in translation. For instance they might revise at the same time as they are translating. A translator may also go through their reference materials and research depending on how familiar they are with the type of text. If they need to find the closest matches for clients, they may use translation memories or machine translation software. The translation process also depends on the laws and ethics codes put into place in certain regions, as well as any censorship, which might affect the outcome of the text.\n\nRevision\n\nRevision may depend on the translator's experience or nature of the text. In translation agencies, revisers may be hired to do the revising, but a freelancer may have to revise their own work. In the case of a pharmaceutical text, depending on the laws, it would require revision since the information in the source text could cause potential harm if mistranslated. There also may be certain style guides that the translation agencies may use that must be followed.\n\nAlthough technical writing and technical translation may be similar in the content they work with, they are different as translators translate what the technical writers produce. The purpose of technical writing, is to explain how to do something. Technical translating is similar, however it attempts to communicate how someone else explains how something is done. “The technical translator, like the technical writer, wants to produce a document that is clear and easy to understand”. Translators may also consider controlled language and whether it applies in their target language culture.\n\nPractitioners within the field of technical translation often employ what is called machine translation (MT), or machine-assisted translation. This method of translation uses various types of computer software to generate translations from a source language to a target language without the assistance of a human. There are different methods of machine translation. A plethora of machine translators in the form of free search engines are available online. However, within the field of technical communication, there are two basic types of machine translators, which are able to translate massive amounts of text at a time. There are transfer-based and data-driven machine translators. Transfer-based machine translation systems, which are quite costly to develop, are built by linguists who determine the grammar rules for the source and target languages. The machine works within the rules and guidelines developed by the linguist. Due to the nature of developing rules for the system, this can be very time-consuming and requires an extensive knowledge base about the structures of the languages on the part of the linguist; nonetheless, the majority of commercial machine translators are transfer-based machines. Yahoo! BabelFish is a common example of a platform that uses this type of translation technology.\n\nData-driven machine translators, also known as statistical-based machine translators, work by aggregating massive amounts of previously translated bits of information, and uses statistical analysis to determine matches between the source language and target language with the previously aggregated corpora. This method is less expensive and requires less development time than transfer-based machine translation, but the generated translation is often not to the same quality as transfer-based translation. The translation services offered through Google use transfer-based translation technology.\n\nFor technical translators without access to expensive machinery, the Internet hosts many online translation sites that are either free or require a small fee. Some research has been done in order to test the effectiveness of various online translation tools. In one article, researchers looked at the success of online machine translators in retrieving appropriate search results. Looking at Google translator, Babelfish (previous to the merge of Babelfish and Yahoo!), Yahoo!, and Prompt, test searches were based on translating key search words and comparing the search results with a monolingual search. Using computer-based statistical analysis, the results showed that translated search results were only 10% less effective than a monolingual search, making the translated search fairly successful in retrieving appropriate information. However, the success in this particular study was only possible when English was one of the target languages.\n\nOther research points to the effectiveness of machine translation when paired with human interaction. In a mixed methods experiment, researchers first examined the effectiveness of machine translations using statistical analysis and then used subjects to test out a new type of machine translation (TransType2) that required human interaction as a part of the translation process. The results of the experiment showed that human interaction is a vital supplement for overall accuracy in machine translations. This research demonstrates the importance of the role that technical translators can play in the process of translating technical documents.\n\nWhile no machine translation device is able to replicate or replace the dynamics of a human translator, machine translation certainly poses important advantages. In fact, there are many practical uses for and implications of machine translation for the field of technical translation.\nMachine translation has major cost advantages as compared to human translation. In fields of technical communication where information is constantly changing, for example, the stock market or jobs related to the weather, the cost of paying a human translator to constantly update information would become quite expensive. Additionally, situations that involve translating massive volumes of information over a short period of time, or situations that require speedy and frequent communication would benefit from machine translation. In such circumstances, a machine translator would be advantageous from a financial perspective.\n\nJust as important as proper translation of linguistic qualities of languages is the subject of culture and how specific cultural features are transferred and communicated in the field of technical translation. In fact, a mutual understanding of cultural components is just as important as linguistic knowledge in technical translation. This highlights the complicated nature of working with technical translation. Various cultures can exhibit drastic differences in how communication occurs, even when both cultures are working with the same target language. One Canadian technical translator and consultant working with Russian colleagues detailed difficulties while working with both North American English and global English. Encountering discrepancies in rhetorical writing strategies, differentiation in tones, document formatting issues, and conflicting conceptual goals for engineering reports, the author emphasizes cultural practices, outside of the direct realm of linguistic forms, that can impede proper communication in technical translation.\n\nIn an example using a commonly translated document, the United Nation's Universal Declaration of Human Rights, a researcher used correlation analyses, including semantic network analysis and spatial modeling, to interpret data describing differences among seven different translated versions of the document. Demonstrating how culture plays an important role in the process of technical translation, the results of the study showed that while the translations were fairly similar, cultural subtleties and differences existed in each language's translated version. For example, across the seven languages, common words such as \"people\", \"individual\", \"man\", \"nation\", \"law\", \"faith\", and \"family' had differing levels of importance in relation to other words in the language. While in Arabic the word \"man\" exhibited high levels of importance in the text, other languages placed higher levels of importance with words such as \"person\" or \"individual\". In another example, the English word for \"entitle\" and the Chinese word for \"enjoy\" carried connotations attached to the concept of \"rights\", demonstrating a linkage of concepts unique to each individual language. These slight differences demonstrate the culturally specific nuances that exist across languages.\nAs with any type of non-MT, it is still a process completed by human beings, making it impossible for total objectivity. International technical communication cannot ignore cultural differences, so seeing how the differences affect translation is fundamental for professionals in the field.\n\nAdditionally, one's cultural knowledge base, or lack thereof, can be detrimental to the effectiveness of communication, particularly when communicating warnings or risk factors. Considering how differing knowledge paradigms as a result of cultural factors can prompt people to respond in a variety of ways to different rhetorical strategies, particularly when communicating messages containing warnings of hazards or risks, understanding culture must be a priority in technical translation. One researcher found that a variance of definition of terms and inconsistent paradigms of cultural knowledge highlight the need for a new delineation of what technical writers consider as the target audience while communicating risk factors. What might be appropriate for one audience must be reconsidered for a culturally different audience. Looking at a specific example concerning the hazardous occupation of mining, one piece of research demonstrates how different cultures different perceptions about safety information. Comparing risk communication in mining in the United States and the United Kingdom, the researcher discovered variations among the perceptions of who is responsible for promoting safety in the workplace. While one culture felt that the user or worker was responsible for promoting his or her own safety in the workplace, another culture perceived the science behind the process or document to be responsible for the promotion of safety. As risks, warnings, or cautions are often important components of a technical document in need of translation, the technical translator will understand how such cultural differences can affect the effectiveness of the translated message. Avoiding assumptions about a culture and allowing one's own knowledge base to consider more diverse populations will create more effective cross-cultural communication not only when working with risky environments, but in general communication as well.\n\nSome research has investigated the possibility of a universal writing style in order to help with the translatability of writing across different cultures and languages. However, demonstrating the difficulty of such a task, one researcher addressed the assumption that unambiguous wording eases effective communication. He gave examples from certain Asian contexts when unclear communication was actually helpful because the unequivocal language forced communicators to rely more heavily on oral discourse than on written documents. The example of the effectiveness about ambiguous language not only shows problems with a universal writing style for technical translation, but also reiterates another example of how culture plays an important role in proper technical translation.\n\nIn an age where technology allows for increased accessibility and faster communication, the technical translator must understand the role that culture plays in how people interact with, react to, and utilize technology and how these culturally related concepts can affect communicated messages.\n\nDemonstrating how technology use differs across cultures, one researcher created a presentation that took a holistic look at preparing documents for ethnically diverse audiences, pointing out other non-linguistic topics that require special attention in communication across cultures. For example, the presenter noted items to be considered including measurement systems, types of graphics and symbols, and types of media presentation tools. The author also pointed out significant differences that would affect communication among English languages including paper layouts, spelling, meaning, and use of humor. This important and practical information can be used by professionals working with technical translation.\n\nAdditionally, technical translation involves understanding how the Internet has influenced different cultures across the globe. Varying languages, cultural influences on Internet usage patterns, and media preferences force professionals in the field of technical communication to utilize a number of different strategies in order to effectively reach diverse populations across the globe. With international online populations the technical translator must be culturally diverse in a technological sense.\n\nFinally, as technology makes intercultural and international communication easier, the technical translator must understand intercultural communication as it relates to ethics. Traditional models for ethical decision-making can be applied to difficult situations in technical translation, but the professional must avoid stereotyping and ethnocentrism in technical communication and translation\n\nTechnical translation is the medium through which language, discourse and communication can exist in a global world. As technology creates easier and faster means of communication and the world moves toward becoming a global community, the need to communicate with people from multiple language backgrounds also grows. Rather than working with multiple languages, some have proposed the idea of using English as the primary language for global communication, making English the lingua franca—or a common world language. However, English as a lingua franca has various implications for the field of technical communication. Particularly for technical translators who are native speakers of English, there is the tendency to assume a unilateral stance on translation. In other words, the technical translator's objective is to translate to and from English, with the English message being the main focus. While English is a language of global communication, it is not the only language being used for communication, highlighting the importance of moving away from \"singular perspective\" of only communicating in English. The concept of maintaining technical communication in languages other than English is of particular significance in countries with high volumes of multilingual speakers. For example, research has shown that the English-speaking bias, due to the language's position as the lingua franca, within technical translation and communication has negatively affected native Spanish speakers in the United States. Lacking both in quality and quantity, user manuals for various electronic devices exemplified sub-par translations into Spanish, demonstrating the limited accessibility of certain technical documents to speakers of languages other than English, perhaps partly as a result of English as the lingua franca. Finally, when discussing English as a lingua franca it is noteworthy to mention what some researchers call \"untranslatable\" words and what that means for technical translation. Such words or phrases are composed of concepts that are not easily translated from one language to another. A word is considered \"untranslatable\" when there is either no direct corresponding word in the target language, requiring the word to be described or when important cultural connotations from the source language are not properly communicated through the target word. For example, a common example in English of an untranslatable word is the German word \"schadenfreude\", which means to exhibit joy as a result of someone else's misfortune. This word exemplifies untranslatability due to the lack of a corresponding word; however words can be untranslatable due to a lack of a corresponding word, loss of cultural meaning, or for both reasons.\nOne study demonstrated that when faced with untranslatable words, technical translators resorted to avoidance tactics that evaded using the words altogether. The implications of untranslatable words and phrases suggest that the technical translation may not benefit from only utilizing English as a lingua franca, and rather, should focus efforts toward having more effective means of translating documents among multiple languages.\n\n"}
{"id": "41782", "url": "https://en.wikipedia.org/wiki?curid=41782", "title": "Telecommunications service", "text": "Telecommunications service\n\nIn telecommunication, a telecommunications service is a service provided by a telecommunications provider, or a specified set of user-information transfer capabilities provided to a group of users by a telecommunications system.\n\nThe telecommunications service user is responsible for the information content of the message. The telecommunications service provider has the responsibility for the acceptance, transmission, and delivery of the message.\n\nFor purposes of regulation by the Federal Communications Commission under the U.S. Communications Act of 1934 and Telecommunications Act of 1996, the definition of telecommunications service is \"the offering of telecommunications for a fee directly to the public, or to such classes of users as to be effectively available directly to the public, regardless of the facilities used.\" \"Telecommunications\", in turn, is defined as \"the transmission, between or among points specified by the user, of information of the user’s choosing, without change in the form or content of the information as sent and received.\" \n\n\n"}
{"id": "49336189", "url": "https://en.wikipedia.org/wiki?curid=49336189", "title": "The Fiber Optic Association", "text": "The Fiber Optic Association\n\nThe Fiber Optic Association (FOA) is an international professional society of fiber optics. The FOA was founded in 1995 by a group of trainers from industry, government and education who wanted to create industry standards for training and certifying fiber optic technicians. The FOA is a not-for-profit 501 (c)6 organization based in California, USA, that has over 200 affiliated training organizations in over 40 countries. FOA approves fiber optic training organizations and certifies their instructors who train designers, installers and operators of all types of fiber optic networks. FOA programs are used by many organizations, companies and trade unions to train and certify their workers.\n\nAt the 1994 Fiber U Conference in Boston, the instructors decided that it was time to create a professional society for fiber optics. The instructors represented a number of fiber optic manufacturers including 3M, Siecor/Corning, Panduit and Fotec, two universities which had started fiber optic courses (Wentworth and Lincoln Trail College), two US military services and several independent trainers and consultants.\n\nThe FOA was incorporated in July 1995 as a non-profit education organization (501(C)6). The founders and some other recruits began meeting to create a certification program. Since the focus of the FOA was technicians not engineers or scientists and there were no good textbooks at the proper level, the first project was to develop a textbook that incorporated the FOA’s desired technical curriculum. That textbook, The Fiber Optic Technicians Manual, was published in 1997. The FOA advisory group developed the KSAs for the basic certification and tests which covered the knowledge part of the KSAs. By late 1997, the first FOA certifications were granted.\n\nSince FOA was founded to be a professional society to set standards for training, not to do training itself, it next set standards to approve schools and certify instructors. Schools already teaching fiber optics around the world were invited to join. Since then over 300 schools have participated in the FOA program and certified 57,000 techs in more than 40 countries.\n\nIn 2009, FOA expanded its technical web site which now encompasses almost 1000 pages and added free online training at its Fiber U website. At the same time, FOA began to self-publish its reference textbooks in order to provide faster updates and lower costs than were possible with a commercial publisher.\n\nIn 2012, FOA recognized the dedication and inspiration of its dedicated core of instructors, many of whom had been founders of the FOA and helped its development by awarding them the designation of \"Master Instructor.\" These 27 Master Instructors had more than 500 years of experience in fiber optics and several had been teaching for more than 30 years.\n\nFOA offers certifications for basic fiber optics and premises cabling, installation skills, and specific applications. All certifications are based on KSAs - the Knowledge, Skills and Abilities - important to the topics. FOA certifications and KSAs are developed by a worldwide panel of experienced trainers and technicians. All FOA certifications follow ANSI/ISO/IEC STANDARD 17024 — General requirements for bodies operating certification of persons. FOA certifications are recognized by the US Department of Labor.\n\n\n\n\nThe Fiber Optic Association offers certification of skills and knowledge in various fiber optic related areas and specialties. FOA does not offer training courses directly. Training courses for FOA certification is only offered by FOA-Approved schools, educational institutions, private training programs, and community development training centers. All schools teaching FOA certifications have been approved through the FOA's School Approval Program. Each instructor teaching FOA certifications has been certified through the FOA's Train-The-Trainer Program.\n\nMembers of the FOA have demonstrated their knowledge, skills and abilities (KSAs) in training courses and have successfully passed the FOA certification exam. Experienced industry professionals have applied to FOA directly through the FOA’s “Work To Cert” Program for certification based upon their knowledge and skills developed working the field. FOA Corporate Members are organizations and companies involved in the fiber optic industry such as manufacturers, contractors, installers, consultants, etc.\n\nThe FOA publishes textbooks on fiber optics, premises cabling, outside plant and outside plant construction, and network design. FOA maintains a large online library of technical information and has created over 100 online videos, and free online self-study programs.\n\n"}
{"id": "57858120", "url": "https://en.wikipedia.org/wiki?curid=57858120", "title": "Transit desert", "text": "Transit desert\n\nA transit desert is an area with limited transportation supply . Developed from the concept of food deserts various methods have been proposed to measure transit deserts. Transit deserts are generally characterized by poor public transportation options and possibly poor bike, sidewalk, or road infrastructure. The lack of transportation options present in transit deserts may have negative effects of people’s health, job prospects, and economic mobility.\n\nThe term ‘desert’ has been variously applied to areas that lack key services like banks, food access, or even books. The idea of transit deserts was coined by Dr. Junfeng Jiao and first appeared in print in 2013. Since that time the concept of transit deserts has been expanded upon and competing definitions and measurement techniques have emerged.\nGap based measurement techniques are the most prominent and well-defined definition of transit deserts. Such methods typically use Geographic Information Systems (GIS) based methods to measure the gap between transportation supply and demand. These methods quantify demand and supply and then subtract demand from supply in order to find the “gap” in transit service. Areas that fall below a certain threshold are termed \"transit deserts\". Using this method studies have found that nearly all cities in the United States have transit deserts. Studies have also consistently shown that central business districts are almost never transit deserts, but the locations of transit desert areas varies considerably in different cities.\n\nA more informal definition of transit deserts has also emerged in which areas that lack some type of transportation, most often public transportation like buses and subway stops, are termed transit deserts.. Sometimes this definition has been expanded or slightly redefined to refer areas that lack a certain type of transportation such as \"subway deserts\".\n\nThe causes of transit deserts are much debated. Some have cited suburban sprawl and deliberately segregationist policies as some of the leading causes of transit deserts.. Still others contend that transit deserts are often the result of poor planning practices and that better transit planning can help alleviate them.\n\n"}
{"id": "7125851", "url": "https://en.wikipedia.org/wiki?curid=7125851", "title": "Turning radius", "text": "Turning radius\n\nThe turning radius or turning circle of a vehicle is the radius (or, depending on usage, \"diameter\") of the smallest circular turn (i.e. U-turn) that the vehicle is capable of making. \nThe term \"turning radius\" is a technical term that has become popular automotive jargon. In the jargon sense, it is commonly used to mean the full diameter of the smallest circle, but in technical usage the turning radius is still used to denote the radius. The less ambiguous term \"turning circle\" avoids the mistaken jargon use of the word 'radius' . As an example, \"Motor Trend\" refers to a \"curb-to-curb turning circle\" of a 2008 Cadillac CTS as , but the terminology is not yet settled. AutoChannel.com refers to the \"turning radius\" of the same car as . It is often used as a generalized term rather than a numerical figure. For example, a vehicle with a very small turning circle may be described as having a \"tight turning radius\".\n\nTwo different measurements can be quoted for a vehicle. A curb or curb-to-curb turning circle will show the straight-line distance from one side of the circle to the other, through the center. The name \"curb-to-curb\" indicates that a street would have to be this wide before this car can make a u-turn and not hit a street curb with a wheel. If you took the street curb and built it higher, as high as the car, and tried to make a u-turn in the street, parts of the car (bumper) would hit the wall. The name wall or wall-to-wall turning circle denotes how far apart the two walls would have to be to allow a u-turn without scraping the walls. One can find these two ways of measuring the turning circle used in auto specifications, for example, a van might be listed as having a turning circle (in meters) of 12.1(C)/12.4(W).\n\nA notable exception in this description is of vehicles that are capable of spinning around their central axis, such as certain lawnmowers and wheelchairs as they do not follow a circular path as they turn. In this case the vehicle is referred to as a \"zero turning radius\" vehicle.\n\nSome camera dollies used in the film industry have a \"round\" mode which allows them to spin around their z axis by allowing synchronized inverse rotation of their front and rear wheel sets, effectively giving them \"zero\" turning radius.\n\n\n\n"}
{"id": "2133492", "url": "https://en.wikipedia.org/wiki?curid=2133492", "title": "Vapor lock", "text": "Vapor lock\n\nVapor lock is a problem that mostly affects gasoline-fueled internal combustion engines. \n\nIt occurs when the liquid fuel changes state from liquid to gas while still in the fuel delivery system. This disrupts the operation of the fuel pump, causing loss of feed pressure to the carburetor or fuel injection system, resulting in transient loss of power or complete stalling. Restarting the engine from this state may be difficult. \n\nThe fuel can vaporize due to being heated by the engine, by the local climate or due to a lower boiling point at high altitude. In regions where fuels with higher volatility are used during the winter to improve engine startup, continued use of the specialized fuels during the summer can cause vapor lock to occur more readily.\n\nVapor lock was far more common in older gasoline fuel systems incorporating a low-pressure mechanical fuel pump driven by the engine, located in the engine compartment and feeding a carburetor. Such pumps were typically located higher than the fuel tank, were directly heated by the engine and fed fuel directly to the float bowl inside the carburetor. Fuel was drawn under negative pressure (gauge pressure) from the feed line, increasing the risk of a vapor lock developing between the tank and pump. A vapor lock being drawn into the fuel pump could disrupt the fuel pressure long enough for the float chamber in the carburetor to partially or completely drain, causing fuel starvation in the engine. Even temporary disruption of fuel supply into the float chamber is not ideal; most carburetors are designed to run at a fixed level of fuel in the float bowl and reducing the level will reduce the fuel to air mixture delivered to the engine.\n\nCarburetor units may not effectively deal with fuel vapor being delivered to the float chamber. Most designs incorporate a pressure balance duct linking the top of the float bowl with either the intake to the carburetor or the outside air. Even if the pump can handle vapor locks effectively, fuel vapor entering the float bowl has to be vented. If this is done via the intake system, the mixture is, in effect, enriched, creating a mixture control and pollution issue. If it is done by venting to the outside, the result is direct hydrocarbon pollution and an effective loss of fuel efficiency and possibly a fuel odor problem. For this reason, some fuel delivery systems allow fuel vapor to be returned to the fuel tank to be condensed back to the liquid phase, or using an active carbon filled canister where fuel vapor is absorbed. This is usually implemented by removing fuel vapor from the fuel line near the engine rather than from the float bowl. Such a system may also divert excess fuel pressure from the pump back to the tank. \n\nMost modern engines are equipped with fuel injection, and have an electric submersible fuel pump in the fuel tank. Moving the fuel pump to the interior of the tank helps prevent vapor lock, since the entire fuel delivery system is under positive pressure and the fuel pump runs cooler than if it is located in the engine compartment. This is the primary reason that vapor lock is rare in modern fuel systems. For the same reason, some carburetted engines are retrofitted with an electric fuel pump near the fuel tank.\n\nA vapor lock is more likely to develop when the vehicle is in traffic because the under-hood temperature tends to rise. A vapor lock can also develop when the engine is stopped while hot and the vehicle is parked for a short period. The fuel in the line near the engine does not move and can thus heat up sufficiently to form a vapor lock. The problem is more likely in hot weather or high altitude in either case.\n\nGravity feed fuel systems are not immune to vapor lock. Much of the foregoing applies equally to a gravity feed system; if vapor forms in the fuel line, its lower density reduces the pressure developed by the weight of the fuel. This pressure is what normally moves fuel from the tank to the carburetor, so fuel supply will be disrupted until the vapor is removed, either by the remaining fuel pressure forcing it into the float bowl and out the vent or by allowing the vapor to cool and re-condense.\n\nVapor lock has been the cause of forced landings in aircraft. That is why aviation fuel is manufactured to far lower vapor pressure than automotive gasoline (petrol). In addition, aircraft are far more susceptible because of their ability to change altitude and associated ambient pressure rapidly. Liquids boil at lower temperatures when in lower pressure environments.\n\nVapor lock was a common occurrence in stock car racing, since the cars have traditionally used gasoline and carburetors. With the introduction of the fuel injection requirement for NASCAR-sanctioned events in 2012, vapor lock has been largely eliminated.\n\nVapor lock is also less common in other motorsports, such as Formula One and IndyCar racing, due to the use of fuel injection and alcohol fuels (ethanol or methanol), which have a lower vapor pressure than gasoline.\n\nThe higher the volatility of the fuel, the more likely it is that vapor lock will occur. Historically, gasoline was a more volatile distillate than it is now and was more prone to vapor lock. Conversely, diesel fuel is far less volatile than gasoline, so that diesel engines almost never suffer from vapor lock. However, diesel engine fuel systems are far more susceptible to air locks in their fuel lines, because standard diesel fuel injection pumps rely on the fuel being non-compressible. Air locks are caused by air leaking into the fuel delivery line or entering from the tank. Air locks are eliminated by turning the engine over for a time using the starter motor, or by bleeding the fuel system. \n\nModern diesel injection systems have self-bleeding electric pumps which eliminate the air lock problem.\n\n"}
{"id": "1967506", "url": "https://en.wikipedia.org/wiki?curid=1967506", "title": "Vehicle Information and Communication System", "text": "Vehicle Information and Communication System\n\nVehicle Information and Communication System (VICS) is a technology used in Japan for delivering traffic and travel information to road vehicle drivers.\n\nIt can be compared with the European TMC technology.\n\nIt can be transmitted using:\n\n\nIt is an application of ITS.\n\nThe VICS information can be displayed on the car navigation unit at 3 levels:\n\n\nInformation transmitted includes traffic congestion data, data on availability of service areas (SA) and parking areas (PA), information on road works and traffic collisions.\n\nSome advanced navigation units might utilize this data for route calculation (e.g., choosing a route to avoid congestion) or the driver might use his own discretion while using this information.\n\n\n"}
{"id": "264501", "url": "https://en.wikipedia.org/wiki?curid=264501", "title": "Vicia faba", "text": "Vicia faba\n\nVicia faba, also known in the culinary sense as the broad bean, fava bean, or faba bean is a species of flowering plant in the pea and bean family Fabaceae. It is of uncertain origin and widely cultivated as a crop for human consumption. It is also used as a cover crop, the bell bean, which has smaller beans. Varieties with smaller, harder seeds that are fed to horses or other animals are called field bean, tic bean or tick bean. Horse bean, \"Vicia faba\" var. \"equina\" is a variety recognized as an accepted name.\n\nSome people suffer from favism, a hemolytic response to the consumption of broad beans, a condition linked to G6PDD. Otherwise the beans, with the outer seed coat removed, can be eaten raw or cooked. In young plants, the outer seed coat can be eaten, and in very young plants, the seed pod can be eaten.\n\n\"Vicia faba\" is a stiffly erect plant tall, with stems that are square in cross-section. The leaves are long, pinnate with 2–7 leaflets, and colored a distinct glaucous () grey-green color. Unlike most other vetches, the leaves do not have tendrils for climbing over other vegetation.\n\nThe flowers are long with five petals; the standard petals are white, the wing petals are white with a black spot (true black, not deep purple or blue as is the case in many \"black\" colorings) and the keel petals are white. Crimson-flowered broad beans also exist, which were recently saved from extinction. The flowers have a strong sweet scent which is attractive to bees and other pollinators.\n\nThe fruit is a broad, leathery pod that is green, but matures to a dark blackish-brown, with a densely downy surface; the wild species has pods that are long and 1 cm diameter, but many modern cultivars developed for food use have pods long and 2–3 cm thick. Each bean pod contains 3–8 seeds that are round to oval and have a 5–10 mm diameter in the wild plant, but are usually flattened and up to 20–25 mm long, 15 mm broad and 5–10 mm thick in food cultivars. \"V. faba\" has a diploid (2n) chromosome number of 12 (six homologous pairs). Five pairs are acrocentric chromosomes and one pair is metacentric.\n\nBroad beans have a long tradition of cultivation in Old World agriculture, being among the most ancient plants in cultivation and also among the easiest to grow. Along with lentils, peas, and chickpeas, they are believed to have become part of the eastern Mediterranean diet around 6000 BCE or earlier. They are still often grown as a cover crop to prevent erosion because they can overwinter and, as a legume, they fix nitrogen in the soil.\n\nThe broad bean has high plant hardiness; it can withstand harsh and cold climates. Unlike most legumes, the broad bean can be grown in soils with high salinity, as well as in clay soil. However, it prefers rich loams.\n\nIn much of the English-speaking world, the name \"broad bean\" is used for the large-seeded cultivars grown for human food, while \"horse bean\" and \"field bean\" refer to cultivars with smaller, harder seeds that are more like the wild species and used for animal feed, though their stronger flavour is preferred in some human food recipes, such as falafel. The term \"fava bean\" (from for the bean) is used in some English-speaking countries such as the US, but \"broad bean\" is the most common name in the UK and Australia and New Zealand.\n\nBroad bean plants are highly susceptible to early summer infestations of the black bean aphid, which can cover large sections of growing plants with infestations, typically starting at the tip of the plant. Severe infestations can significantly reduce yields, and can also cause discolouration of pods and reduction in their saleable values.\n\nFaba bean rust is a fungal pathogen commonly affecting broad bean plants at maturity, causing small orange dots with yellow halos on the leaves, which may merge to form an orange lawn on both leaf surfaces.\n\nBeans are also attacked by chocolate spot fungus, which can have a severe impact on yield.\n\nIn mainland Europe and North Africa, the plant parasite \"Orobanche crenata\" (carnation-scented broomrape) can cause severe impacts on fields of broad beans, devastating their yields.\n\nBroad beans are generally eaten while still young and tender, enabling harvesting to begin as early as the middle of spring for plants started under glass or overwintered in a protected location, but even the main crop sown in early spring will be ready from mid to late summer. Horse beans, left to mature fully, are usually harvested in the late autumn, and are then eaten as a pulse. The immature pods are also cooked and eaten, and the young leaves of the plant can also be eaten, either raw or cooked as a pot herb (like spinach).\n\nBroad beans were a major food of old Mediterranean civilizations, particularly for the Romans and Greeks.\n\nPreparing broad beans involves first removing the beans from their pods, then steaming or boiling the beans, either whole or after parboiling them to loosen their exterior coating, which is then removed.\n\nThe beans can be fried, causing the skin to split open, and then salted and/or spiced to produce a savory, crunchy snack. These are popular in China, Malaysia, Colombia, Peru (\"habas saladas\"), Guatemala (\"habas\"), Mexico (\"habas con chile\"), Gilan (North of Iran) and Thailand (where their name means \"open-mouth nut\").\n\nIn some Arab countries, the fava bean is used for a breakfast dish called \"ful medames\".\n\nFava beans are common in Latin American cuisines, as well. In central Mexico, mashed fava beans are a common filling for many corn flour-based \"antojito\" snacks such as \"tlacoyos\". In Colombia, they are most often used whole in vegetable soups. Dried and salted fava beans are a popular snack in many Latin countries.\n\nBroad beans are widely cultivated in the Kech and Panjgur districts of Balochistan Province in Pakistan, and in the eastern province of Iran. They are called \"bakalaink\" in the Balochi language, and \"baghalee\" in Persian.\n\nIn south Algerian cuisine, broad beans are used to make Besarah and Doubara. Doubara is popular in the city of Biskra.\n\nIn the Sichuan cuisine of China, broad beans are combined with soybeans and chili peppers to produce a spicy fermented bean paste called \"doubanjiang\". Perhaps due to the bean's popularity in Sichuan cuisine, in addition to the regular Chinese terms of \"silkworm bean\" (蚕豆 \"cándòu\") or \"orchid bean\" (兰花豆 \"lánhuādòu\"), they are also known as \"Sichuan beans\" (川豆 \"chuāndòu\") in Chinese.\n\nFava beans (Colombia: Haba(s)) are a common food in most regions of Colombia, mostly in Bogota and Boyacá.\n\nFava beans are used mostly in Dalmatia as a part of the traditional dish stuffed artichokes with fava beans and peas.\n\nSteamed fava beans (known as \"habitas\") with cheese are common in the cold-weather regions of Ecuador, especially around the Andes mountains and surroundings of Ambato.\n\nFava beans ( \"\" ) are a common staple food in the Egyptian diet, eaten by rich and poor alike. Egyptians eat fava beans in various ways: they may be shelled and then dried, bought dried and then cooked by adding water in very low heat for several hours, etc. They are the primary ingredient in falafel. However, the most popular way of preparing them in Egypt is by taking the mashed, cooked beans and adding oil, salt and cumin to them. The dish, known as \"ful medames\", is traditionally eaten with bread (generally at breakfast) and is considered the Egyptian national dish.\n\nBroad beans (Amharic: \"baqella\") are one of the most popular legumes in Ethiopia. They are tightly coupled with every aspect of Ethiopian life. They are mainly used as an alternative to peas to prepare a flour called \"shiro\", which is used to make \"shiro wot\" (a stew almost ubiquitous in Ethiopian dishes). During the fasting period in the Ethiopian Orthodox Church tradition called \"Tsome Filliseta, Tsome arbeå, Tsome Tahsas, and Tsome Hawaria\" (which are in August, end of February–April, mid-November–beginning of January and June–July), two uncooked spicy vegetable dishes are made using broad beans. The first is \"Hilibet\", a thin, white paste of broad bean flour mixed with pieces of onion, green pepper, garlic, and other spices based on personal taste. The second is \"siljo\", a fermented, sour, spicy thin yellow paste of broad bean flour. Both are served with other stews and \"injera\" (a pancake-like bread) during lunch and dinner.\n\n\"Baqella nifro\" (boiled broad beans) are eaten as a snack during some holidays and during a time of mourning. This tradition goes well into religious holidays, too. On the Thursday before Good Friday, in the Ethiopian Orthodox Church tradition \"tselote hamus\" (the Prayer of Thursday), people eat a different kind of \"nifro\" called \"gulban\". \"Gulban\" is made of peeled, half beans collected and well cooked with other grains such as wheat, peas and chickpeas. This is done to mourn the crucifixion of Jesus Christ.\n\n\"Boq'ullit\" (boiled salted broad beans embryo) is one of the most favorite snacks in the evening, the common story-telling time in north and central Ethiopia. It is particularly a favorite for the story-teller (usually a society elder), as it is delicious, and easy to chew and swallow.\n\nRipe broad beans are eaten by passers-by. Besides that, they are one of the gift items from a countryside relative in a period close to the Ethiopian Epiphany.\n\nThe Greek word \"fáva\" (φάβα) does \"not\" refer to broad beans, but to the yellow split pea and also to another legume, known as \"Lathyrus sativus\". Broad beans are known instead as \"koukiá\" (), and are eaten in a stew combined with artichokes, while they are still fresh in their pods. Dried broad beans are eaten boiled, sometimes combined with garlic sauce (\"skordalia\").\nIn Crete, fresh broad beans are shelled and eaten as a companion to \"tsikoudia\", the local alcoholic drink.\nFavism is quite common in Greece because of malaria endemicity in previous centuries, and people afflicted by it do not eat broad beans.\n\nIn India, fava beans are eaten in the northeastern state of Manipur. They are locally known as \"hawai-amubi\" and are ingredients in the dish eromba.\n\nBroad beans, or \"Baghalee\" (Persian: باقالی) are primarily cultivated in the central and north parts of Iran. The city of Kashan has the highest production of broad beans with high quality in terms of the taste, cooking periods and color. However, broad beans have a very short season (roughly two weeks). The season is usually in the middle of spring. When people have access to fresh beans in season, they cook them in brine and then add vinegar and \"Heracleum persicum\" depending on taste. They also make an extra amount to dry to be used year-round. The dried beans can be cooked with rice, which forms one of the most famous dishes in north of Iran (Gilan) called \"baghalee polo\" (Persian: باقالی پلو) which means \"rice with broad beans\". In Iran broad beans are cooked, served with Golpar-origan and salt and sold on streets in the winter. This food is also available preserved in metal cans.\n\nBroad beans which are called Bagilla (باگله/باقله) in the Iraqi dialect of Arabic are a common ingredient in many Iraqi foods. One of the most popular Iraqi dishes that uses the broad bean is Bagilla Bil-Dihin (باگله بالدهن) also called Tishreeb Bagilla (تشريب باگله). This dish is a common breakfast dish in Iraq and consists of bread soaked in boiled broad beans’ water then topped with broad beans, melted Ghee, and often also a boiled or fried egg. Fool (فول) is another common breakfast dish in Iraq as well as many other Arab countries and consists of mashed fava beans. Another famous Iraqi dish is Timmen Bagilla (تمن باگله), which is Arabic for 'broad bean rice'. This classic Iraqi dish consists of rice cooked with broad bean and dill.\n\nIn Rome, Italy, Fava beans are popular either cooked with guanciale or with globe artichokes, as side dish together with lamb or kid, or raw with Pecorino romano. \"Fave e Pecorino\" is the traditional dish for 1 May picnic in Tuscany, Umbria and Latium.\n\nIn Sicily, \"Maccu\" is a Sicilian soup prepared with fava beans as a primary ingredient.\n\nIn Apulia, broad bean purée with wild chicory is typical dish.\n\nBroad beans, called Soramame (Japanese:そら豆) lit: “Sky Bean”, are consumed in a variety of ways in Japan. Most commonly, the beans are boiled and are eaten straight or added to rice. It is also consumed as a popular snack called “ikarimame” (Japanese:いかり豆) lit: “Anchor Bean”, where the beans are roasted or fried.\n\n\"Judd mat Gaardebounen\", or smoked collar of pork with broad beans, is the national dish of Luxembourg.\n\nThey are a primary ingredient of the Maltese Kusksu, a vegetable soup primarily containing fava beans and pasta beads. They are also used in a popular appetizer called bigilla where they are served as a pureé mixed with olive oil, lemon juice, garlic, parsley and mint. It is served with bread or crackers and is the Maltese answer to hummus.\n\nIn Mexico, fava beans are often eaten in a soup called \"sopa de habas\", meaning \"fava soup\". They are also eaten as a snack, in which they are fried, salted, and dried. They are either by themselves as a snack or in combination with other salted, dried beans and nuts.\n\nIn Morocco, fava beans are made into bessara, a dip sold as a street food.\n\nIn Nepal, fava beans are called \"bakulla\". They are eaten as a green vegetable when the pods are young, generally stir-fried with garlic. When dried, fava beans are eaten roasted, or mixed with other legumes, such as moong beans, chick peas, and peas, and called \"qwati\". The mixture, soaked and germinated, is cooked as soup and consumed with rice or beaten rice on the occasion of \"Janai Purnima\" also known as \"Rakshya Bandhan\", a festival celebrated by the Hindus. The dry and stir-fried version of \"qwati\" is called \"biraula\". The \"qwati\" soup is believed to reinvigorate the body affected by monsoon paddy season.\n\nIn the Netherlands, they are traditionally eaten with fresh savory and some melted butter. The combination of the beans tossed with crispy fried bacon is also common. When rubbed, the velvet insides of the pods are a folk remedy against warts.\n\nFava beans (Peru: Haba(s)) are eaten fresh or dried as stew, toasted, boiled, roasted, stewed, soup etc. Habas are one of the essential ingredients of the famous \"Pachamanca\" in the Andes of Peru, and are also an additive for \"Panetela\", which is a homemade remedy to keep your child fed and hydrated in cases of diarrhea or stomach infection and even for cholera treatment. To make Panetela combine and roast a cup of: fava bean (habas), barley, corn, wheat, rice and / or beans without allowing it to burn; add a cup of water, a carrot cut into pieces and a pinch of salt until fully cooked; drain, add water until it reaches a liter and boil one last time. For babies only the fluid is used.\n\nPeruvian dishes with fava beans include:\n\nFava beans (Portuguese: \"favas\") is widely cultivated in Portugal and are very popular throughout the country. The most popular dish cooked with favas is \"favada\", a stew with onion and pork - depending on the region of the country the pork may be chorizo, bacon, pork shoulder, ribs or the mixture of many of these. In Alentejo a lot of coriander will be added in the end. Besides favada, fava beans may be served dry and fried as an apetiser. \n\nBroad beans (Spanish: \"habas\") are widely cultivated in Spain. Culinary uses vary among regions, but they can be used as the main pulse in a stew (\"fabada, habas estofadas, michirones\") or as an addition to other dishes (\"menestra, paella\"). In certain regions they can be eaten while unripe or fried and packaged as a snack.\n\nFava beans are one of the most widely consumed foods in Sudan. For most Sudanese they form the main dish during lunch time (ghada), especially more so for city and urban dwellers. The beans are cooked by steadily boiling over a sustained period of time. Similar to Egypt, the cooked beans are mashed, and prepared by adding salt and pepper. For additional flavour, sesame oil is added along with a sprinkling of jibna (\"feta\" cheese) on top. The dish is then eaten with bread and is sometimes refereed to as \"ful medames\".\n\nBroad beans (, literally: \"farmer beans\"), which in Sweden were traditionally eaten as soaked brown and boiled dried broad beans fried in lard, were for a very long time popular to add to other foods as a filling side, specially with fried pork. The green raw and lightly boiled broad beans were only used seasonally as a side green.\nIt almost disappeared from Swedish plates in the 70's during a time of rapid increase in expectations of food quality and variety, and changes in food habits.\nHowever, since the 2000's, broad beans have made a comeback, partly as healthy, locally grown produce, but even more because of an increased interest in legumes, vegetarianism, healthy foods, but also a general greater curiosity about food in general, including a renewed interest in forgotten traditional foods of Sweden, as well as the increased popularity of foreign food, specially Middle-Eastern food, among Swedes, but even more among immigrants from North Africa and the Middle-East specially in the Egyptian/Sudanese dish Ful Medames (; ), or as an ingredient in Egyptian and Gaza style falafel, which often contains broad beans instead of chickpeas, or both, which has lately become one of the most popular fast foods in Sweden.\n\nIn Syria, broad beans are prepared in multiple ways for breakfast, lunch or dinner. Ful Medames is the same as the Egyptian dish (it is not mashed though) but with the addition of tomato, parsley and onion and with olive oil. Another version of it includes the addition of tahini (sesame paste), olive oil, garlic and lemon. For lunch, broad beans are cooked with a mix of minced and big chunks of meat and is topped on white rice and eaten with cold yogurt and cucumber salad. Bulgur is sometimes used in preparing this recipe instead of rice. Broad beans are cooked with pieces of garlic, meat and meat stock with the addition of lemon juice and cilantro. This dish is called foulieh and is eaten on the side with rice. Same recipe is prepared without meat as a vegan dish eaten on lent by Christians in Syria.\n\nIn Turkey, broad beans are called '. This is also the name of a \"zeytinyağlı\" dish made by simmering young and tender broad bean pods with chopped onions in olive oil. It is traditionally garnished with dill and served cool, together with yoghurt. Another popular dish is ', a meze prepared by soaking and boiling shelled dried broad beans until soft and then pureeing with olive oil and optionally fresh dill. The puree is left to set overnight, and served cold, garnished with dill and slices of lemon.\n\nRaw mature fava beans are 11% water, 58% carbohydrates, 26% protein, and 2% fat. A 100 gram reference amount supplies 341 calories and numerous essential nutrients in high content (20% or more of the Daily Value, DV). Folate (106% DV) and dietary minerals, such as manganese, phosphorus, magnesium, and iron (range of DV 52 to 77%), have considerable content (table). B vitamins have moderate to rich content (19 to 48% DV).\n\nSufferers of favism must avoid broad beans, since these trigger a hemolytic crisis. Broad beans are rich in tyramine, and thus should be avoided by those taking monoamine oxidase inhibitors.\n\n\n\n"}
