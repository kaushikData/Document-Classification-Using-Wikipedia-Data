{"id": "44701137", "url": "https://en.wikipedia.org/wiki?curid=44701137", "title": "Anti-submarine drone", "text": "Anti-submarine drone\n\nAnti-submarine drones are Unmanned surface vehicles designed to stalk and hunt submarines. They are an emerging technology with a prototype ACTUV being designed by DARPA as a potentially smaller, more efficient Anti-submarine warfare capability for the United States Navy. \n\nUnmanned aerial vehicle\n"}
{"id": "26031877", "url": "https://en.wikipedia.org/wiki?curid=26031877", "title": "Arithmaurel", "text": "Arithmaurel\n\nFirst patented in France by Timoleon Maurel, in 1842, the Arithmaurel was a mechanical calculator that had a very intuitive user interface, especially for multiplying and dividing numbers because the result was displayed as soon as the operands were entered. It received a gold medal at the French national show in Paris in 1849. Unfortunately its complexity and the fragility of its design prevented it from being manufactured.\n\nIts name came from the concatenation of Arithmometer, the machine that inspired its design and of Maurel, the name of its inventor. The heart of the machine uses one Leibniz stepped cylinder driven by a set of differential gears.\n\nTimoleon Maurel patented an early version of his machine in 1842, he then improved its design with the help of Jean Jayet and patented it in 1846. This is the design that won a gold medal at the \"Exposition nationale de Paris\" in 1849.\n\nWinnerl, a French clockmaker, was asked to manufacture the device in 1850, but only thirty machines were built because the machine was too complex for the manufacturing capabilities of the time. During the first four years, Winnerl was not able to build any of the 8 digit machines (a minimum for any professional usage) that had been ordered while Thomas de Colmar delivered, during the same period, two hundred 10 digit Arithmometers and fifty 16 digit ones.\n\nIt is to be noted that none of the machines that were built and none of the machines described in the patents could be used at full capacity because the capacity of the result display register was equal to the capacity of the operand register (for a multiplication, the capacity of the result register should be equal to the capacity of the operand register augmented by the capacity of the operator register).\n\nFollowing is a description of one of the two machines introduced in the 1846 patent. It has a capacity of five digits for the operator and ten digits for the operand and the result registers.\n\nAll the registers are located on the front panel, the reset mechanism is on the side.\n\n\n\n"}
{"id": "2580605", "url": "https://en.wikipedia.org/wiki?curid=2580605", "title": "Astrolite", "text": "Astrolite\n\nAstrolite is the trade name of a family of explosives, invented by chemist Gerald Hurst in the 1960s during his employment with the Atlas Powder Company. The Astrolite family consists of two compounds, Astrolite G and Astrolite A. Both are two-part liquid-state high explosive mixtures, composed of ammonium nitrate oxidizer and hydrazine rocket fuel. The explosives were extensively studied, manufactured, and used in many countries because of their advantages of high energy, excellent\nperformance, and wide application. They still find some use in commercial and civil blasting applications, but have mostly been superseded by cheaper and safer compounds, largely due to the expense and exceptionally poisonous nature of the hydrazine component.\n\nAstrolite G, the most common type of Astrolite, is a mixture of ammonium nitrate and hydrazine at a ratio of 2:1, measured in weight, forming a clear, viscous liquid approximately the consistency of motor oil. It is a relatively stable (secondary) high explosive compound, requiring a blasting cap to detonate. It has a detonation velocity of approximately 8,600 m/s, twice the explosive strength of TNT. It has been widely referred to as the \"world's most powerful non-nuclear explosive\", caused largely by a comparison of Astrolite G's detonation velocity to that of first and second-generation high explosives such as nitroglycerine and TNT. Current-generation high explosive compounds such as PETN and RDX can feature comparable detonation velocities and brisance to Astrolite G.\n\nAstrolite A, a secondary (and less common) type of Astrolite, is synthesized by the addition of finely powdered aluminium to the Astrolite G mixture. Though it has a lower detonation velocity (approximately 7,600 m/s) than Astrolite G, the addition of the aluminium increases both its density and brisance, moderately increasing its overall effectiveness.\n\nA notable characteristic of the Astrolite family is its remarkable degree of persistency for a liquid explosive compound. Due to its low volatility, it can be dispersed in an area, be absorbed by the soil, and still retain its full explosive characteristics for a period of approximately 4 days. This has shown to be true even when rainwater had also been absorbed by the soil.\n\n"}
{"id": "1224642", "url": "https://en.wikipedia.org/wiki?curid=1224642", "title": "Audience analysis", "text": "Audience analysis\n\nAudience analysis is a task that is often performed by technical writers in a project's early stages. It consists of assessing the audience to make sure the information provided to them is at the appropriate level. The audience is often referred to as the end-user, and all communications need to be targeted towards the defined audience. Defining an audience requires the consideration of many factors, such as age, culture and knowledge of the subject. After considering all the known factors, a profile of the intended audience can be created, allowing writers to write in a manner that is understood by the intended audience.\n\nAudience analysis involves gathering and interpreting information about the recipients of oral, written, or visual communication.\n\nThere are numerous methods that a technical communicator can use to conduct the analysis. Because the task of completing an audience analysis can be overwhelming, using a multi-pronged approach to conduct the analysis is recommended by most professors, often yielding improved accuracy and efficiency. Michael Albers suggests that an analysis use several independent guestofaguest that work together, such as reader knowledge of the topic and reader cognitive comprehension.\n\nWriters can also use conversation, in-depth interviews or focus groups to help them to complete an audience analysis. Conversation as well as other qualitative research techniques will allow the communicator to consider the multiple cultural, disciplinary, and institutional contexts of their target audience, producing a valuable audience analysis.\n\nDavid L. Carson of the Rensselaer Polytechnic Institute asserted that technical communicators most often perform their jobs with little or no knowledge about their audience. Carson states that the analysis should include a reader's level of comprehension of the technical vocabulary and motivation, as well as reading level. Indicators of a reader's high level of motivation include high interest in the subject matter, relatively high knowledge of the content, and high personal stakes in mastering the information.\n\nAnother technique used to conduct an audience analysis is the \"bottom-up\" approach. Leon de Stadler and Sarah van der Land explore this type of approach in reference to a document produced by an organization that develops different kinds of interventions in the field of HIV/AIDS education. This particular document focused on the use of contraception and targeted the black youth of South Africa. The initial document was created by document designers in the United States who did not base their design on an extensive audience analysis. As a result, the document, which used the informal slang of black South African youth, did not effectively communicate with its target audience. After the dissemination of the document, Van der Land used focus groups and interviews of a sample of the target audience to discover what improvements should be made. Upon considering the audience's perspective, she found that the initial document's use of the hip-style language backfired. The interviewees indicated that the use of the popular language was not effective because it was not used correctly or consistently throughout the document. Additionally, to the target audience, the informal language did not fit the seriousness of the topic being discussed. The suggested \"bottom-up\" approach should have incorporated the target audience during the design process instead of as an afterthought.\n\nMarjorie Rush Hovde provides even more tactics that can be implemented in the process of an audience analysis in relation to one's organization. She suggests talking with users during phone support calls, interacting with users face-to-face, drawing on the writer's own experiences with the software and documentation, interacting with use-contact people within the organization, studying responses sent from users after the documentation is released, and conducting internal user-testing. Like Michael Albers, Hovde asserts that the use of a combination of tactics proves to produce a more accurate audience analysis than using one tactic alone.\n\nKaren D. Holl discusses what writers should consider when writing papers that address an international audience. She focuses on those writers who attempt to publish studies in publications that are circulated abroad. She suggests that these writers consider the following questions when framing their papers: What conclusions from my study would be relevant and novel to land managers and scientists working in other ecosystems and socio-economic contexts?, What is the geographic scope of the literature I am citing?, To which ecological and socio-economic systems do my world view and results apply?, Is my study sufficiently well replicated to generalize my results?, and Are my conclusions supported by my data and, conversely, are all my data necessary to support my conclusions?. Although she focuses her suggestions on scientific studies, she acknowledges that \"what is critical to effectively communicate the results of any study is to consider what conclusions will be of most interest to the target audience.\" Holl concludes that knowing how to address an international audience is a vital skill that successful scientists, as well as technical communicators, must possess.\n\nThere are often a large number of factors to consider, thus making it hard for the writer to completely assess the target audience within a reasonable amount of time. Therefore, an attempt to reach the most accurate and effective audience analysis, in a timely manner, is vital to the technical communication process. The depth of the audience analysis also depends of the size of the intended audience.\n\nBecause people constantly change in terms of technological exposure, the audience to be analyzed constantly changes. As a result, the technical communicator must consider the possibility that their audience changes over time. An article in the European Journal of Communication examined the change audience research has experienced due to the growing range of information and communication technologies. The article pointed out that there are three main challenges that drive the search for methodological rigor: the difference between what people say they do and what they do in practice, the interpretation of the text by the reader, and why the received meanings of television matter in everyday life. An absolutely perfect audience analysis is generally impossible to create, and it is similarly difficult to create an analysis that is relevant for a long period of time. Revising and rewriting an audience analysis is often required in order to maintain the relevance of the analysis.\n\nR. C. Goldworthy, C. B. Mayhorn and A. W. Meade, dealt with the hazard mitigation, including warning development, validation, and dissemination as an important aspect of product safety and workplace and consumer protection in their article \"Warnings in Manufacturing: Improving Hazard-Mitigation Messaging through Audience Analysis\". In this study, they focused on the potential role of latent class analysis in regards to the audience analysis performed in hazard communication and warning messages. Their qualitative study involved 700 adult and adolescent participants who answered a structured questionnaire about prescription medication history, prescription medication loaning/borrowing history, and likelihood of sharing/borrowing medication. With this information, four latent classes were identified: Abstainers, Pragmatic frequent sharers, At-risk sharers, and Emergency sharers. The identification of latent classes based on behaviors of interest facilitated tailoring hazard-mitigation efforts to specific groups. Although their study is limited, in that all participants were between the ages of twelve and forty-four and were from heavily populated urban area (so the generalizability of the data to rural settings has not been generated), this study establishes that latent class analysis can play a vital role. They conclude that latent analysis is a worthwhile addition to the analytical toolbox because it allows, in this case, risk reduction and hazard-mitigation efforts to tailor interventions to a diverse target audience. For the technical writer, analyzing latent classes would enable them to better identify homogenous groups within the broader population of readers and across many variables to tailor messages to these better-specified groups.\n\nThe population of older adults is growing, and Gail Lippincott asserts that technical communicators have not accounted for the needs of these audiences, nor drawn from the wide range of research on aging. In her article \"Gray Matters: Where are the Technical Communicator in Research and Design for Aging Audiences?\", Lippincott suggests four challenges that practitioners, educators, and researchers must undertake to accommodate older adults' physical, cognitive, and emotional needs: They must refine the demographic variable of age, operationalize age to enrich current methods of audience analysis, investigate multidisciplinary sources of aging research, and participate in research on aging by offering our expertise in document design and communication strategies. Lippincott acknowledges that there is so much more research that must be done in this area, for \"the body of literature on older adults and computer use is relatively small.\" Lippincott provides insight into an often overlooked audience that technical communicators must learn to address.\n\nTeresa Lipus argues that devoting company resources to produce adequate instructions for international users is both practical and ethical. She also provides a brief overview of the consumer protection measures that leading U.S. trade partners have implemented. She also presents the following guidelines for developing adequate instructions for international audiences: 1) define the scope of the instructions, 2) identify the audience, 3) describe the product's functions and limitations, 4) identify the constraints, and 5) use durable materials. She offers tips for getting and keeping the attention of the audience. These tips are 1) organize the information, 2) structure the information, and 3) design the page layout. For aiding the comprehension of the readers, Lipus suggests that technical communicators write readable texts and design effective graphics. In an effort to motivate compliance, she says to make the instructions relevant and credible and to improve user recall of the information by organizing the information into small meaningful groups and providing concise summaries and on-product reminders. When presenting safety information, Lipus says to not only include the necessary safety messages but to also design effective safety messages. Before distributing instructions, they must be evaluated. She recommends testing the product and the accuracy of the instructions, communicating using means that reach users, and continuing to test and to inform users even after marketing. She explains that because the potential for making subtle but offensive errors is so high in international dealings, a language-sensitive native speaker from the target culture should always review the instructions before they are distributed to consumers. Although Lipus provides information in analyzing and writing for an international audience regarding consumer protection, the strategies offered can be applied to document preparation in general.\n\nJenni Swenson, Helen Constantinides, and Laura Gurak, in their case study, address the problem of defining medical web site credibility and identifying the gap in web design research that fails to recognize or address specific audience needs in web site design. The information they gathered assisted the researchers in identifying and fulfilling specific audience needs, describing a framework, and presenting a case study in audience-driven Web design. The researchers used the qualitative method of conducting a survey to find the audience of the Algenix, Inc. Web site. Algenix is a biomedical liver disease management company. The study showed that an audience-driven design would do more to reassure the audience that personal information would not be collected without consent as well as provide clear policies of security, privacy, and data collection. The survey informed the researchers that the audience would also like to experience a site with minimal graphics and short download times and one that is intuitive and easy to navigate. This study illustrates how an audience analysis should not only address what the users are able to do but also what they, as the users, would prefer.\n\nIn the article \"Real Readers, Implied Readers, and Professional Writers: Suggested Research\", Charlotte Thralls, Nancy Ror, and Helen Rothschild Ewald of Iowa State University define \"real readers\" versus \"implied readers\". The real reader is a concrete reality and determines the writer's purpose and persona. A writer who perceives an audience as real tends to conceive of readers as living persons with specific attitudes and demographic characteristics. Therefore, the writer's task is to accommodate the real reader by analyzing this reader's needs and deferring to them. The implied reader, on the other hand, is a mental construct or role which the actual reader is invited to enter, even though the characteristics embodied in that role may not perfectly fit his or her attitudes or reactions. When the reader is implied, the writer invents and determines the audience within the text. The researchers assert that writers must appreciate the complex interplay that may take place between the real and implied representations of the reader in every document. The researchers discuss how their study was conducted for the sole purpose of developing a hypothesis for further study: Are professional writers aware of real and implied readers; does a writer's way of perceiving a reader affect contextual development; do shifts occur in writers' conceptions of readers; are writers' perceptions of readers linked to a sense of genre and explained by principles of cognitive processing?\n\n\n"}
{"id": "2984169", "url": "https://en.wikipedia.org/wiki?curid=2984169", "title": "Aviation archaeology", "text": "Aviation archaeology\n\nAviation archaeology is a recognized sub-discipline within archaeology and underwater archaeology as a whole. It is an activity practiced by both enthusiasts and academics in pursuit of finding, documenting, recovering, and preserving sites important in aviation history. For the most part, these sites are aircraft wrecks and crash sites, but also include structures and facilities related to aviation. It is also known in some circles and depending on the perspective of those involved as aircraft archaeology or aerospace archaeology and has also been described variously as crash hunting, underwater aircraft recovery, wreck chasing, or wreckology .\n\nThe activity dates to post-World War II Europe when, after the conflict, numerous aircraft wrecks studded the countryside. Many times, memorials to those involved in the crashes were put together by individuals, families, landholders, or communities.\nCrash sites vary in size and content; some may have fuselages, engines, and thousands of parts and debris. Other sites, like in civilian/commercial crashes, the Federal Aviation Administration and the National Transportation Safety Board (NTSB) will have almost all of the aircraft and debris removed; which makes aviation archaeology more challenging. Remains of military aircraft crash sites may also be removed by various aircraft restoration groups, particularly if the aircraft was found largely intact. In general, most recent-day (since the 1980s) aircraft crashes are removed entirely, due to environmental regulations, leaving very little to indicate the existence of a wreck.\n\nFor example, military crashes in Arizona originate from numerous air bases, past and present. Because of the warm and sunny weather, much of the U.S. Army Air Forces flight training was located in the state, both during and after WWII. Numerous air bases dotted the states - creating conditions for numerous training accidents. Old abandoned US Army Air Corp auxiliary fields and those converted to city municipal airports provide archaeological sites to be researched and investigated.\n\nKeeping a record of a crash site, such as photographs, maps, journals, logs, and all terrain and weather recordings are essential, i.e. the Glenwood Springs, Colorado, B-17 crash site or the Tells Peak, CA, B-17 crash site.\nThe internet is an ideal media for sharing, recording, educating, and promoting aviation archaeology as a hobby, as well as research projects for local and state aviation historical groups. For identifying aircraft type and manufacturer by part numbers and manufacturing inspection stamps can be analysed. From detailed GPS data & maps, to researching accident reports information, numerous resources help create a complete picture of the historic event. Accident reports, such as the official US Air Force Accident Report Form 14 becomes the foundation of archaeology research. From there, newspaper articles, county clerk records, sheriff & coroner reports, and library records all aid an aviation archaeologist in their research.\n\nLegal protection of aircraft wreck sites is highly variable. In terms of protection by aircraft ownership, the U.S. Navy retains indefinite ownership of all Naval aircraft, including terrestrial or submerged wreck sites. The U.S. Air Force has no policies regarding disturbance of vintage aircraft wreck sites, unless human remains or weaponry remain unrecovered at the site.\nFor vintage aircraft, including vintage military aircraft, that are usually considered abandoned when wrecked, the wreck site and all associated contents are subject to the protection laws of the land upon which it rests. The language of cultural heritage protection laws are not aviation specific, so all protection laws pertaining to aviation sites are based on interpretation. Most federal and state laws are, however, explicit in describing cultural resources as either ‘objects, sites, or otherwise, of historic value’ or ‘military or social history’ and deem the time limit as over fifty years old.\nIf an aircraft wreck is over fifty years old, which includes all aviation wreck sites from WWII, and crashed on what is currently federal lands, the sites are automatically protected under National Park Service Law 36CFR2.1 against disturbance of any kind without a permit. Aviation sites, for example, a vintage hangar on an airport or a wreck site on the path of a proposed highway, are also immediately subject to Section 106 review if they are to be disturbed by a project that either requires a federal permit or uses federal funds. In most cases, the State Historic Preservation Officer will determine whether or not an aviation site is eligible for the register.\n\nThe National Register deems aviation wreck sites as “any aircraft that has been crashed, ditched, damaged, stranded, or abandoned”. It designates the protection terms for aviation history sites as well, including abandoned airfields or facilities sites, testing or experimental sites, land or water air terminals, or airway beacons and navigational aids.\n\nState lands protection laws vary widely across the nation but the language describing a historical resource is the same as federal laws. Therefore, aviation properties and aircraft wrecks on State lands can be protected under various environmental, public resource, and historical property laws as outlined per state for the protection of archaeological and historic resources.\nAny archaeological survey, excavation, or activity that disturbs wither wreck or aviation property remains can, in some cases, be permitted on federal and state lands under a permitting process through the regulating entity.\nIf an aircraft wreck, or the remains of any aviation property, is located on private land it is not automatically protected by any federal, state, or local law and any survey or excavation work must be permitted by the land owner.\n\nUnder the 'Sunken Military Craft Act’ (SMCA) of 2004, it is illegal to disturb, remove, or injure the wreck sites or associated contents of U.S. Naval or any submerged military aircraft. The act identifies military craft as including any sunken military aircraft or military spacecraft that was owned or operated by a government when it sank, and includes the associated contents. Because of the U.S. Navy’s retaining of ownership of all military craft, the act applies to any U.S. Navy aircraft, even if in international or other country’s territorial waters. The act also applies to any foreign military craft in U.S. territorial waters. Persons wishing to conduct archaeological or research exploration of any submerged military aircraft can apply to the Naval History and Heritage Command’s Underwater Archaeology Branch for a permit. The U.S. Marine Corps and U.S. Coast Guard has similar policies and permitting requirements to the Navy.\nThe SMCA includes penalties associated with any unauthorized disturbances of sunken military craft as a fine and a liability for the reasonable costs incurred in recovery of archaeological or cultural information, storage, restoration, care, maintenance, and conservation.\n\nAs a part of federal air regulations, NTSB Part 830, protects any aircraft whose accident cause is under investigation. NTSB officials will routinely seize portions of wrecked aircraft for further analysis. Most of the time, after their study is complete, the sequestered debris is returned to the owners' representation - most often the aircraft's insurance company. However, examples like the reconstructed wreckage of TWA Flight 800 are held in perpetuity by the NTSB to education the public and future investigators on the NTSB's role in transportation safety.\nThe laws in the UK cover the remains of all aircraft which have crashed during military service (land or sea) are protected by the Protection of Military Remains Act 1986. This Act defines an offence of tampering with, damage to, moving, or unearthing the aircraft remains. Exceptions apply to those holding licences, which can be issued by the Secretary of State, authorising specific procedures to be performed.\n\nFor the wreck-chasing hobbyist there is a self-regulating body, the British Aviation Archaeological Council (BAAC), which defines ethical standards of behaviour, coordinates activities and provides a forum for discussion for its member groups. Not all active groups in the UK are members of this organisation.\n\nAviation history sites on land that can be subject to archaeological survey or excavation can include airports (which can contain hangars, terminal, other facilities, etc.), crash sites, monuments, or even properties associated with important persons or events in aviation history.\nSome examples of potential and current archaeological sites:\n\nThe Loon Lake B-23 Dragon crash site in Payette National Forest, Idaho is a remarkably intact example of an aircraft wreck. The crew survived and was rescued, and some avionics removed from the site, and it currently is the subject of a teaching aviation archaeology field school in various years.\n\nAbandoned airfields can yield much information of historic information about aviation and related industries. From civilian airfields to military airfields, aviation archaeologists can find, uncover, and recover a variety of artifacts, just to name a few: aircraft parts with serial numbers, equipment parts, asphalt or runway material, variety of contamination, structures and foundations, businesses and economics, to community and cultural changes. With the closure of a military airbase, the street system and runways become local expansion of city streets and business; one example is the community conversion of Lowry Air Force Base to a local residential, commercial, and educational environment. Other bases, like the Arlington Auxiliary Army Airfield reverted to farming and ranching.\n\nIn 1990, 1994, and 1998, archaeologists investigated, using airborne remote sensing studies and limited excavation, a vintage hangar of the Huffman Prairie Flying Field Site at Wright-Patterson Air Force Base, Ohio. The investigations were “designed to provide information needed for site management by Wright-Patterson Air Force Base and the Dayton Aviation Heritage National Historical Park of the National Park Service. The geophysical and remote sensing investigations revealed magnetic, electromagnetic, and ground penetrating radar anomalies and infrared thermal images associated with the hangar structure. The archaeological excavations located an in situ wood post, posthole features, and artifacts which represent archaeological remains of the actual hangar”. Huffman Prairie Flying Field is listed on the National Register of Historic Places.\n\nAnother example is Hamilton Army Airfield in Novato, California. It was in use from 1929 until 1976. It was eventually turned over to the city of Novato for development of housing. The runway is also part of tidal wetland restoration effort currently underway by the U.S. Army Corps of Engineers, California Coastal Conservancy, and the San Francisco Bay Conservation and Development Commission.\n\nCalifornia has missile launch sites abandoned by the US Army. Archaeological research includes these sites throughout the United States. Exploring and hiking around abandoned silos and sites may constitute trespassing as well as being dangerous. Permission from current land owners or caretakers is imperative. Research and formal site investigations adds to the historical record of the Cold War. One such site is the Minuteman Missile National Historic Site.\n\nIn the Golden Gate National Recreation Area is a decommissioned Cold War era Nike Missile base, Nike Missile Site SF-88. In 1954, it was armed with Nike Ajax missiles. In 1958, it was converted to Nike Hercules nuclear missiles. After it was shut down in 1974, it was turned over to the National Park Service, and it open for to the public regularly. \n\nA B-29 \"Superfortress\" Serial No. 45-21847 ditched in Lake Mead in 1949. This particular aircraft is listed in the National Register under Criterion C as an example of a significant type of aircraft construction and under Criterion D for it potential to yield important information.\n\nThe remains of the USS Macon Airship and its associated F9C Sparrowhawks are located at around 1500 feet in the Monterey Bay National Marine Sanctuary. The National Oceanic and Atmospheric Administration (NOAA) has run survey expeditions to the site, creating photomosaics to track deterioration. The wreck site is listed on the National Register.\n\nUnderwater search and recovery is a complex aspect of aviation archaeology. Dive and recovery team have to do extensive research and planning before any recovery is performed. The aircraft site may be left as a memorial and not recovered. Once an aircraft has been located, an underwater survey is conducted before recovery operations begin. Many tasks are established and the research is a long process that requires the detailed review numerous and various sources of information. The complexities include a great deal of preparation, extensive training, precise planning, and very technical equipment and coordination. Conservation has often proved very difficult\n\nThe Australian focus has been on underwater aviation archaeology, partly as a result of the interest of the relatively large number of maritime archaeologists and shipwreck conservators in the field. This has resulted in numerous studies and reports, including some cross-fertilization or ideas, theory and techniques with practitioners in other parts of the world, with a strong emphasis on the involvement of conservators. Underwater aviation archaeology commenced in Australia at the wrecks of the Dornier, Catalina, and Sunderland Flying Boats destroyed by Japanese fighters at Broome in WWII. These lie, both in the intertidal zone, and in deeper water. The study continued in Darwin in the Northern Territory with research and fieldwork at its series of submerged PBY Catalina wrecks, Subsequently, the study has spread to other regions in Australia, partly as a result of the Interest of Flinders University and its postgraduate student body. While military aircraft remain the property of their respective governments unless delegated to a third party, submerged aircraft wrecks (such as the wrecks at Broome in Western Australia), have proven to be quite difficult to protect from unauthorized recoveries and looting. Those in Broome are now protected under the provisions of the 1990 Heritage of Western Australia Act.\n\nIn America, aviation archaeologists, crosstrained in other areas of study, are found in the employ of Joint POW/MIA Accounting Command (JPAC), traveling to former war zones throughout the world, to search for the remains of American servicemen and women that have been lost. Many of these losses involve aircraft mishaps in remote and difficult to reach areas. A group of volunteers, under the banner of \"The BentProp Project\", have pursued American military wreck sites and remains without disturbing them; their findings are forwarded to JPAC. In Australia and in some other parts of the world, where there are human remains involved, a tendency has been for the armed forces to secure the services of forensic anthropologists and crash investigators.\n\nProfessional aviation archaeologists may also be involved in the recovery of near-complete examples of wrecked or abandoned aircraft for profit. The clients of these professionals range from private individuals and aviation museums, to government agencies. Often these aircraft are in remote areas, which aids wreckage preservation. Examples include \"Glacier Girl\", a Lockheed P-38 Lightning that was successfully recovered from below the Greenland ice cap, and restored to airworthy condition, and \"Kee Bird\", a Boeing B-29 Superfortress also abandoned on the Greenland ice cap, but severely damaged by recovery efforts.\n\n\n\n"}
{"id": "11796103", "url": "https://en.wikipedia.org/wiki?curid=11796103", "title": "Blackle", "text": "Blackle\n\nBlackle is a website powered by Google Custom Search and created by Heap Media, which aims to save energy by displaying a black background and using grayish-white font color for search results. Blackle claims having saved nearly 6 MWh of electrical energy up to December 2016, a claim currently under dispute. For comparison, the average American household consumes 11 MWh of electrical energy per year.\n\nThe concept behind Blackle is that computer monitors can be made to use less energy by displaying much darker colors. Blackle is based on a study which tested a variety of CRT and LCD monitors. There is dispute over whether there really are any energy saving effects, especially for users of LCD screens, where there is a constant backlight.\n\nThis concept was first brought to the attention of Heap Media by a blog post, which estimated that Google could save 750 megawatt hours a year by utilizing it for CRT screens. The homepage of Blackle provides a count of the number of watt hours claimed to have been saved by enabling this concept.\n\n\nBlack Google Energy Saver Blog Search : BLAck goOGLE\n"}
{"id": "22386383", "url": "https://en.wikipedia.org/wiki?curid=22386383", "title": "Bousillage", "text": "Bousillage\n\nBousillage (bouzillage, bousille, bouzille) is a mixture of clay and grass or other fibrous substances used as the infill (chinking) between the timbers of a half-timbered building. This material was commonly used by 18th century French colonial settlers in the historical New France region of the United States and is similar to the material cob and adobe. In French \"torchis\" has the same meaning or the meaning of a loaf of this material.\n\n\"Bousillage\" in south Louisiana is a mixture of clay earth and retted Spanish moss, but in the Upper Mississippi River Valley and Canada contains straw, grass or hair, used to fill in the panels in poteaux-sur-sol, poteaux-en-terre, and half-timbered framing (called \"colombage\" in French). This was a technique used in French Louisiana by colonists from the 18th to 19th centuries. In France the framing was typically in-filled between the post with brick (briquette-entre-poteaux), stone and mud (Pierrotage) or bousillage. There was no stone in south Louisiana, and bricks were not being made during early colonial times. The colonist picked up on a technique that the Native Americans were using to build their wattle and daub structures, and that was heavy clay soil and retted Spanish moss as the binder. Split sticks or staves, known as \"barreaux\", \"rabbits\" or \"batons\" were used as rungs between the upright post. They were shaped to fit at an angle and hammered into place without the use of nails.\n\nBousillage is made by layering a \"taché\" (hole in the ground) with mud and moss and adding water. Then \"tacherons\" (barefoot men) worked the mixture into a mortar. \"Torchis\" (bousillage shaped like a bread dough loaf) are hung over the barreaux being compacted as placed one next to the other. The finished wall would have been either lime washed or covered with lime plaster. The plaster contains animal hair as a binder.\n\n\n"}
{"id": "38743590", "url": "https://en.wikipedia.org/wiki?curid=38743590", "title": "Commotion Wireless", "text": "Commotion Wireless\n\nCommotion Wireless is an open-source wireless mesh network for electronic communication. The project was developed by the Open Technology Institute, and development included a $2 million grant from the United States Department of State in 2011 for use as a mobile ad hoc network (MANET), concomitant with the Arab Spring. It was preliminarily deployed in Detroit in late 2012, and launched generally in March 2013. The project has been called an \"Internet in a Suitcase\".\n\nCommotion 1.0, the first non-beta release, was launched on December 30, 2013.\n\nCommotion relies on several open source projects: OLSR, OpenWrt, OpenBTS, and Serval project.\n\nUbiquiti:\n\nTP-Link:\n\nMikrotik:\n"}
{"id": "1997759", "url": "https://en.wikipedia.org/wiki?curid=1997759", "title": "Concorde aircraft histories", "text": "Concorde aircraft histories\n\nTwenty Concorde aircraft were built, six for development and 14 for commercial service.\n\nThese were\n\nAll but two of these aircraft are preserved.\n\nThe two prototype aircraft were used to expand the flight envelope of the aircraft as quickly as possible and prove that the design calculations for supersonic flight were correct.\n\n\nBoth pre-production aircraft were used to further develop the design of the aircraft. Changes to design include different wing plan form, more fuel, different engine standard, different air intake systems etc.\n\n\nThe production aircraft were different in many ways to the original aircraft necessitating re-examining certain areas to obtain certification. In all there were six \"development\" aircraft. The two prototypes (001/002), the two pre-production (101/102) and two production aircraft (201/202)\n\n\nBritish Airways had seven production aircraft in commercial service:\n\n\n\n\n\nAs part of tenth-anniversary celebrations on 24 December 1985, British Airways photographed G-BOAA, G-BOAC, G-BOAF and G-BOAG formation flying for their publicity material.\n\nAir France also had seven production aircraft in commercial service:\n\n\n"}
{"id": "189768", "url": "https://en.wikipedia.org/wiki?curid=189768", "title": "Consumer electronics", "text": "Consumer electronics\n\nConsumer electronics or home electronics are electronic (analog or digital) equipments intended for everyday use, typically in private homes. Consumer electronics include devices used for entertainment (flatscreen TVs, DVD players, video games, remote control cars, etc.), communications (telephones, cell phones, e-mail-capable laptops, etc.), and home-office activities (e.g., desktop computers, printers, paper shredders, etc.). In British English, they are often called brown goods by producers and sellers, to distinguish them from \"white goods\" which are meant for housekeeping tasks, such as washing machines and refrigerators. In the 2010s, this distinction is not always present in large big box consumer electronics stores, such as Best Buy, which sell both entertainment, communication, and home office devices and kitchen appliances such as refrigerators. \n\nRadio broadcasting in the early 20th century brought the first major consumer product, the broadcast receiver. Later products included telephones, televisions and calculators, then audio and video recorders and players, game consoles, personal computers and MP3 players. In the 2010s, consumer electronics stores often sell GPS, automotive electronics (car stereos), video game consoles, electronic musical instruments (e.g., synthesizer keyboards), karaoke machines, digital cameras, and video players (VCRs in the 1980s and 1990s, followed by DVD players and Blu-ray disc players). Stores also sell digital cameras, camcorders, cell phones, and smartphones. As of 2016, some of the newer products sold include virtual reality head-mounted display goggles, smart home devices that connect home devices to the Internet (such as smartphone-controllable thermostats and lights) and wearable technology such as Fitbit digital exercise watches.\n\nIn the 2010s, most consumer electronics have become based on digital technologies, and have largely merged with the computer industry in what is increasingly referred to as the consumerization of information technology. Some consumer electronics stores, such as Best Buy, have also begun selling office and baby furniture. Consumer electronics stores may be \"bricks and mortar\" physical retail stores, online stores, where the consumer chooses items on a website and pays online (e.g., Amazon). or a combination of both models (e.g., Best Buy has both bricks and mortar stores and an e-commerce website for ordering its products). The CEA (Consumer Electronics Association) estimated the value of 2015 consumer electronics sales at US$220 billion.\n\nFor its first fifty years the phonograph turntable did not use electronics; the needle and soundhorn were purely mechanical technologies. However, in the 1920s radio broadcasting became the basis of mass production of radio receivers. The vacuum tubes that had made radios practical were used with record players as well, to amplify the sound so that it could be played through a loudspeaker. Television was soon invented, but remained insignificant in the consumer market until the 1950s. The transistor, invented in 1947 by Bell Laboratories, led to significant research in the field of solid-state semiconductors in the early 1950s. The transistor's advantages revolutionized that industry along with other electronics. By 1959 Fairchild Semiconductor had introduced the first planar transistor from which come the origins of Moore's law. Integrated circuits followed when manufacturers built circuits (usually for military purposes) on a single substrate using electrical connections between circuits within the chip itself. Bell's invention of the transistor and the development of semiconductors led to far better and cheaper consumer electronics, such as transistor radios, televisions, and by the 1980s, affordable video game consoles and personal computers that regular middle-class families could buy.\n\nMain consumer electronics products include radio receivers, television sets, MP3 players, video recorders, DVD players, digital cameras, camcorders, personal computers, video game consoles, telephones and mobile phones. Increasingly these products have become based on digital technologies, and have largely merged with the computer industry in what is increasingly referred to as the consumerization of information technology such as those invented by Apple Inc. and MIT Media Lab.\n\nOne overriding characteristic of consumer electronic products is the trend of ever-falling prices. This is driven by gains in manufacturing efficiency and automation, lower labor costs as manufacturing has moved to lower-wage countries, and improvements in semiconductor design. Semiconductor components benefit from Moore's law, an observed principle which states that, for a given price, semiconductor functionality doubles every two years.\n\nWhile consumer electronics continues in its trend of convergence, combining elements of many products, consumers face different decisions when purchasing. There is an ever-increasing need to keep product information updated and comparable, for the consumer to make an informed choice. Style, price, specification, and performance are all relevant. There is a gradual shift towards e-commerce web-storefronts.\n\nMany products include Internet connectivity using technologies such as Wi-Fi, Bluetooth, EDGE or Ethernet. Products not traditionally associated with computer use (such as TVs or Hi-Fi equipment) now provide options to connect to the Internet or to a computer using a home network to provide access to digital content. The desire for high-definition (HD) content has led the industry to develop a number of technologies, such as WirelessHD or ITU-T G.hn, which are optimized for distribution of HD content between consumer electronic devices in a home.\n\nMost consumer electronics are built in China, due to maintenance cost, availability of materials, quality, and speed as opposed to other countries such as the United States. Cities such as Shenzhen have become important production centres for the industry, attracting many consumer electronics companies such as Apple Inc.\n\nAn electronic component is any basic discrete device or physical entity in an electronic system used to affect electrons or their associated fields. Electronic components are mostly industrial products, available in a singular form and are not to be confused with electrical elements, which are conceptual abstractions representing idealized electronic components.\n\nConsumer electronics such as personal computers use various types of software. Embedded software is used within some consumer electronics, such as mobile phones. This type of software may be embedded within the hardware of electronic devices. Some consumer electronics include software that is used on a personal computer in conjunction with electronic devices, such as camcorders and digital cameras, and third-party software for such devices also exists.\n\nSome consumer electronics adhere to protocols, such as connection protocols \"to high speed bi-directional signals\". In telecommunications, a communications protocol is a system of digital rules for data exchange within or between computers.\n\nThe Internationale Funkausstellung Berlin (IFA) trade show has taken place Berlin, Germany since its foundation in 1924. The event features new consumer electronics and speeches by industry pioneers.\n\nThe Consumer Electronics Show (CES) trade show has taken place yearly in Las Vegas, Nevada since its foundation in 1973. The event, which grew from having 100 exhibitors in its inaugural year to more than 3,600 exhibitors in its 2014 edition, features new consumer electronics and speeches by industry pioneers.\n\nInstitute of Electrical and Electronics Engineers (IEEE), the world's largest professional society, has many initiatives to advance the state-of-the art of consumer electronics. IEEE has a dedicated society of thousands of professionals to promote CE, called the Consumer Electronics Society (CESoc). IEEE has multiple periodicals and international conferences to promote CE and encourage collaborative research and development in CE. The flagship conference of CESoc, called IEEE International Conference on Consumer Electronics (ICCE), is on its 35th year.\n\nElectronics retailing is a significant part of the retail industry in many countries. In the United States, big-box store retailers include Best Buy and Sears, with Best Buy being the largest consumer electronics retailer in the country. Broad-based retailers, such as Wal-Mart and Target, also sell consumer electronics in many of their stores. In April 2014, retail e-commerce sales were the highest in the consumer electronic and computer categories as well. Some consumer electronics retailers offer extended warranties on products with programs such as \"SquareTrade\".\nAn electronics district is an area of commerce with a high density of retail stores that sell consumer electronics.\nThe electronics industry, especially meaning consumer electronics, emerged in the 20th century and has now become a global industry worth billions of dollars. Contemporary society uses all manner of electronic devices built in automated or semi-automated factories operated by the industry.\n\nConsumer electronic service can refer to the maintenance of said products. When consumer electronics have malfunctions, they may sometimes be repaired.\n\nIn 2013 in Pittsburgh, Pennsylvania, the increased popularity in listening to sound from analog audio devices, such as record players, as opposed to digital sound, has sparked a noticeable increase of business for the electronic repair industry there.\n\nThe energy consumption of consumer electronics and their environmental impact, either from their production processes or the disposal of the devices, is increasing steadily. EIA estimates that electronic devices and gadgets account for about 10%–15% of the energy use in American homes – largely because of their number; the average house has dozens of electronic devices. The energy consumption of consumer electronics increases – in America and Europe – to about 50% of household consumption, if the term is redefined to include home appliances such as refrigerators, dryers, clothes washers and dishwashers.\n\nStandby power – used by consumer electronics and appliances while they are turned off – accounts for 5–10% of total household energy consumption, costing $100 annually to the average household in the United States. A study by United States Department of Energy's Berkeley Lab found that a videocassette recorders (VCRs) consume more electricity during the course of a year in standby mode than when they are used to record or playback videos. Similar findings were obtained concerning satellite boxes, which consume almost the same amount of energy in \"on\" and \"off\" modes.\n\nA 2012 study in the United Kingdom, carried out by the Energy Saving Trust, found that the devices using the most power on standby mode included televisions, satellite boxes and other video and audio equipment. The study concluded that UK households could save up to £86 per year by switching devices off instead of using standby mode. A report from the International Energy Agency in 2014 found that $80 billion of power is wasted globally per year due to inefficiency of electronic devices. Consumers can reduce unwanted use of standby power by unplugging their devices, using power strips with switches, or by buying devices that are standardized for better energy management, particularly Energy Star marked products.\n\nElectronic waste describes discarded electrical or electronic devices. Many consumer electronics may contain toxic minerals and elements, and many electronic scrap components, such as CRTs, may contain contaminants such as lead, cadmium, beryllium, mercury, dioxins, or brominated flame retardants. Electronic waste recycling may involve significant risk to workers and communities and great care must be taken to avoid unsafe exposure in recycling operations and leaking of materials such as heavy metals from landfills and incinerator ashes. However, large amounts of the produced electronic waste from developed countries is exported, and handled by the informal sector in countries like India, despite the fact that exporting electronic waste to them is illegal. Strong informal sector can be a problem for the safe and clean recycling.\n\nReuse and repair\n\nE-waste policy has gone through various incarnations since the 1970s, with emphases changing as the decades passed. More weight was gradually placed on the need to dispose of e-waste more carefully due to the toxic materials it may contain. There has also been recognition that various valuable metals and plastics from waste electrical equipment can be recycled for other uses. More recently the desirability of reusing whole appliances has been foregrounded in the ‘preparation for reuse’ guidelines. The policy focus is slowly moving towards a potential shift in attitudes to reuse and repair.\n\nWith turnover of small household appliances high and costs relatively low, many consumers will throw unwanted electric goods in the normal dustbin, meaning that items of potentially high reuse or recycling value go to landfills. While larger items such as washing machines are usually collected, it has been estimated that the 160,000 tonnes of EEE in regular waste collections was worth £220 million. And 23% of EEE taken to Household Waste Recycling Centres was immediately resaleable – or would be with minor repairs or refurbishment. This indicates a lack of awareness among consumers as to where and how to dispose of EEE, and of the potential value of things that are literally going in the bin.\n\nFor reuse and repair of electrical goods to increase substantially in the UK there are barriers that must be overcome. These include people’s mistrust of used equipment in terms of whether it will be functional, safe, and the stigma for some of owning second-hand goods. But the benefits of reuse could allow lower income households access to previously unaffordable technology whilst helping the environment at the same time.<Cole, C., Cooper, T. and Gnanapragasam, A., 2016. Extending product lifetimes through WEEE reuse and repair: opportunities and challenges in the UK. In: Electronics Goes Green 2016+ Conference, Berlin, Germany, 7–9 September 2016>\n\nDesktop monitors and laptops produce major physical health concerns for humans when bodies are forced into positions that are unhealthy and uncomfortable in order to see the screen better. From this, neck and back pains and problems increase, commonly referred to as repetitive strain injuries. Using electronics before going to bed makes it difficult for people to fall asleep, which has a negative effect on human health. Sleeping less prevents people from performing to their full potential physically and mentally and can also “increase rates of obesity and diabetes,” which are “long-term health consequences”. Obesity and diabetes are more commonly seen in students and in youth because they tend to be the ones using electronics the most. “People who frequently use their thumbs to type text messages on cell phones can develop a painful affliction called De Quervain syndrome that affects their tendons on their hands. The best known disease in this category is called carpal tunnel syndrome, which results from pressure on the median nerve in the wrist”.\n\n"}
{"id": "1551353", "url": "https://en.wikipedia.org/wiki?curid=1551353", "title": "Copy stand", "text": "Copy stand\n\nIn photography, a copy stand is a device used to copy images and/or text with a camera. The stand consists of a board onto which the media is placed and a tripod-mount parallel to it, usually with an adjustable height. Light is provided by bright lamps mounted on either side of the media at 45° angles. This provides uniform lighting and reduces specular reflection, keeping glare low.\n\nIn film cameras, copy stands are traditionally used with slide film. The fine resolution of slide film allows the images to be reproduced with high fidelity when they are projected.\n\nCopy stands can be used for reprography (that is, to copy documents). To do so, the camera is mounted, usually with a standard 1/4\" tripod-mount screw, onto the stand, pointing the lens down at the base, where the document to be copied would be placed.\n\nCopy stands can be used in forensics to photograph evidence, in much the same way documents are reproduced. Evidence is brought to the lab, placed on the copy stand and recorded. Some copy stands designed for forensic use have ultraviolet light sources on them, to illuminate latent fingerprints.\n\nCopy stands are sometimes recommended as a low-cost alternative to a full-featured animation stand for traditional cel animation, especially when using small film formats such as Super 8.\n"}
{"id": "17787148", "url": "https://en.wikipedia.org/wiki?curid=17787148", "title": "Cyber-physical system", "text": "Cyber-physical system\n\nA cyber-physical (also styled cyberphysical) system (CPS) is a mechanism that is controlled or monitored by computer-based algorithms, tightly integrated with the Internet and its users. In cyber-physical systems, \"physical and software components are deeply intertwined, each operating on different spatial and temporal scales, exhibiting multiple and distinct behavioral modalities, and interacting with each other in a lot of ways that change with context\". Examples of CPS include smart grid, autonomous automobile systems, medical monitoring, process control systems, robotics systems, and automatic pilot avionics.\n\nCPS involves transdisciplinary approaches, merging theory of cybernetics, mechatronics, design and process science. The process control is often referred to as embedded systems. In embedded systems, the emphasis tends to be more on the computational elements, and less on an intense link between the computational and physical elements. CPS is also similar to the Internet of Things (IoT), sharing the same basic architecture; nevertheless, CPS presents a higher combination and coordination between physical and computational elements.\n\nPrecursors of cyber-physical systems can be found in areas as diverse as aerospace, automotive, chemical processes, civil infrastructure, energy, healthcare, manufacturing, transportation, entertainment, and consumer appliances.\n\nUnlike more traditional embedded systems, a full-fledged CPS is typically designed as a network of interacting elements with physical input and output instead of as standalone devices. The notion is closely tied to concepts of robotics and sensor networks with intelligence mechanisms proper of computational intelligence leading the pathway. Ongoing advances in science and engineering improve the link between computational and physical elements by means of intelligent mechanisms, increasing the adaptability, autonomy, efficiency, functionality, reliability, safety, and usability of cyber-physical systems.\nThis will broaden the potential of cyber-physical systems in several directions, including: intervention (e.g., collision avoidance); precision (e.g., robotic surgery and nano-level manufacturing); operation in dangerous or inaccessible environments (e.g., search and rescue, firefighting, and deep-sea exploration); coordination (e.g., air traffic control, war fighting); efficiency (e.g., zero-net energy buildings); and augmentation of human capabilities (e.g. in healthcare monitoring and delivery).\n\nMobile cyber-physical systems, in which the physical system under study has inherent mobility, are a prominent subcategory of cyber-physical systems. Examples of mobile physical systems include mobile robotics and electronics transported by humans or animals. The rise in popularity of smartphones has increased interest in the area of mobile cyber-physical systems. Smartphone platforms make ideal mobile cyber-physical systems for a number of reasons, including:\n\n\nFor tasks that require more resources than are locally available, one common mechanism for rapid implementation of smartphone-based mobile cyber-physical system nodes utilizes the network connectivity to link the mobile system with either a server or a cloud environment, enabling complex processing tasks that are impossible under local resource constraints. Examples of mobile cyber-physical systems include applications to track and analyze CO emissions, detect traffic accidents, insurance telematics and provide situational awareness services to first responders, measure traffic, and monitor cardiac patients.\n\nCommon applications of CPS typically fall under sensor-based communication-enabled autonomous systems. For example, many wireless sensor networks monitor some aspect of the environment and relay the processed information to a central node. Other types of CPS include smart grid, autonomous automotive systems, medical monitoring, process control systems, distributed robotics, and automatic pilot avionics.\n\nA real-world example of such a system is the Distributed Robot Garden at MIT in which a team of robots tend a garden of tomato plants. This system combines distributed sensing (each plant is equipped with a sensor node monitoring its status), navigation, manipulation and wireless networking.\n\nA focus on the control system aspects of CPS that pervade critical infrastructure can be found in the efforts of the Idaho National Laboratory and collaborators researching resilient control systems. This effort takes a holistic approach to next generation design, and considers the resilience aspects that are not well quantified, such as cyber security, human interaction and complex interdependencies.\n\nAnother example is MIT's ongoing CarTel project where a fleet of taxis work by collecting real-time traffic information in the Boston area. Together with historical data, this information is then used for calculating fastest routes for a given time of the day.\n\nIn industry domain, the cyber-physical systems empowered by Cloud technologies have led to novel approaches that paved the path to Industry 4.0 as the European Commission IMC-AESOP project with partners such as Schneider Electric, SAP, Honeywell, Microsoft etc. demonstrated.\n\nCyber-physical models for future manufacturing—With the motivation a cyber-physical system, a \"coupled-model\" approach was developed. The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. The coupled model first constructs a digital image from the early design stage. System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. The simulation model can be considered as a mirrored image of the real machine, which is able to continuously record and track machine condition during the later utilization stage. Finally, with ubiquitous connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited. These features pave the way toward implementing cyber manufacturing.\n\nA challenge in the development of embedded and cyber-physical systems is the large differences in the design practice between the various engineering disciplines involved, such as software and mechanical engineering. Additionally, as of today there is no \"language\" in terms of design practice that is common to all the involved disciplines in CPS. Today, in a marketplace where rapid innovation is assumed to be essential, engineers from all disciplines need to be able to explore system designs collaboratively, allocating responsibilities to software and physical elements, and analyzing trade-offs between them. Recent advances show that coupling disciplines by using co-simulation will allow disciplines to cooperate without enforcing new tools or design methods. Results from the MODELISAR project show that this approach is viable by proposing a new standard for co-simulation in the form of the Functional Mock-up Interface.\n\nDesigning and deploying a cyber-physical production system can be done based on the 5C architecture (connection, conversion, cyber, cognition, and configuration). In the \"Connection\" level, devices can be designed to self-connect and self-sensing for its behavior. In the \"Conversion\" level, data from self-connected devices and sensors are measuring the features of critical issues with self-aware capabilities, machines can use the self-aware information to self-predict its potential issues. In the \"Cyber\" level, each machine is creating its own \"twin\" by using these instrumented features and further characterize the machine health pattern based on a \"Time-Machine\" methodology. The established \"twin\" in the cyber space can perform self-compare for peer-to-peer performance for further synthesis. In the \"Cognition\" level, the outcomes of self-assessment and self-evaluation will be presented to users based on an \"infographic\" meaning to show the content and context of the potential issues. In the \"Configuration\" level, the machine or production system can be reconfigured based on the priority and risk criteria to achieve resilient performance. \n\nThe original twin model idea came from , in which a physical operation was coupled with a virtual operation by means of an intelligent reasoning agent. The detailed version of this concept is presented in .\n\nThe US National Science Foundation (NSF) has identified cyber-physical systems as a key area of research. Starting in late 2006, the NSF and other United States federal agencies sponsored several workshops on cyber-physical systems.\n\n\n\n"}
{"id": "23890326", "url": "https://en.wikipedia.org/wiki?curid=23890326", "title": "Encompass", "text": "Encompass\n\nEncompass, the Enterprise Computing Association, was the original computer user group for business customers of Hewlett-Packard. Encompass's history begins with DECUS, founded in 1961, for customers of the Digital Equipment Corporation, which was acquired in 1998 by Compaq.\nThe U.S. Chapter incorporated as the user group Encompass U.S.\n\nHewlett-Packard acquired Compaq in 2002. Encompass continued as an HP user group, aimed at business customers of all of HP's hardware, software, and services.\n\nThe Encompass mission was to promote technical information exchange among its members and between the members and Hewlett-Packard.\n\nThe Connect User Group Community, formed from the consolidation in May 2008 of Encompass, Interex EMEA, and ITUG is Hewlett-Packard's largest user community representing more than 50,000 participants. See Connect (users group) for more information.\n\nThe Encompass Board at the time of the consolidation consisted of Nina Buik, Kristi Browder, Glen Kuykendall, Anthony Ioele, Steve Davidek, Dena Wright, John Maynen, Clyde Poole, and Bill Johnson. Two former directors were credited with being instrumental in facilitating the consolidation, Chris Koppe and Jim Becker.\n\nEncompass partnered with first Compaq and then Hewlett-Packard on national technical conferences (in the U.S.):\n\nEncompass also had a number of Local User Groups (LUGs) throughout the U.S., and Special Interest Groups (SIGs) on various topics.\n\nEncompass also ran webcasts, local seminars, and other programs.\n\nEncompass manages the pre-conference seminar program and trade show at the HP Technology Forum & Expo.\n\nNon-U.S. Encompass organizations include:\n\nConnect (users group)\n\n"}
{"id": "23307981", "url": "https://en.wikipedia.org/wiki?curid=23307981", "title": "Exoskeletal engine", "text": "Exoskeletal engine\n\nThe exoskeletal engine (ESE) is a concept in turbomachinery design. Current gas turbine engines have central rotating shafts and fan-discs and are constructed mostly from heavy metals. They require lubricated bearings and need extensive cooling for hot components. They are also subject to severe imbalance (or vibrations) that could wipe out the whole rotor stage, are prone to high- and low-cycle fatigue, and subject to catastrophic failure due to disc bursts from high tensile loads, consequently requiring heavy containment devices. To address these limitations, the ESE concept turns the conventional configuration inside-out and utilizes a drum-type rotor design for the turbomachinery in which the rotor blades are attached to the inside of a rotating drum instead of radially outwards from a shaft and discs. Multiple drum rotors could be used in a multi-spool design.\n\nFundamentally, the ESE drum-rotor configuration typically consists of four concentric open-ended drums or shells: \n\nIn the ESE design, the rotating blades are primarily in radial compression as opposed to radial tension, which means that materials that do not possess high-tensile strength, such as ceramic materials, can be used for their construction. Ceramics behave well in compressive loading situations where brittle fracture is minimized, and would provide greater operating efficiency through higher operating temperatures and lighter engine weight when compared to the metal alloys that typically are used in turbomachinery components. The ESE design and the use of composite materials could also reduce the part count, reduce or eliminate cooling, and result in increased component life. The use of ceramics would also be a beneficial feature for hypersonic propulsion systems, where high stagnation temperatures can exceed the limits of traditional turbomachinery materials.\n\nThe cavity within the inner shell could be exploited in several different ways. In subsonic applications, venting the centre cavity with a free-stream flow could potentially contribute to a large noise reduction; while in supersonic-hypersonic applications it might be used to house a ramjet or scramjet (or other devices such as a pulse-detonation engine) as part of a turbine-based combined-cycle engine. Such an arrangement could reduce the overall length of the propulsion system and thereby reduce weight and drag significantly.\n\n\"From\" Chamis and Blankson:\n\nOne of the major challenges is in bearing design as there are no known lubricated systems that can\nhandle the magnitude of velocity encountered in the ESE; foil- and magnetic bearings have been suggested as possible solutions to this problem. \n\nAlthough both bearing systems theoretically meet the requirements of the exoskeletal application, neither technology is currently ready for operation at practical sizes. Developments in foil bearing technology indicate it may take 20 years to achieve foil bearings for this diameter, and magnetic bearings appear to be too heavy for this application and would also face a lengthy technology\ndevelopment programme.\n"}
{"id": "48797724", "url": "https://en.wikipedia.org/wiki?curid=48797724", "title": "Fair &amp; Lovely (cosmetics)", "text": "Fair &amp; Lovely (cosmetics)\n\nFair & Lovely is a skin-lightening cosmetic product of Hindustan Unilever introduced to the market in India in 1975. Fair & Lovely is available in India, Bangladesh, Malaysia, Indonesia, Singapore, Brunei, Thailand, Sri Lanka, Pakistan and other parts of Asia and is also exported to other parts of the world such as the West, where they are sold in Asian supermarkets.\n\nUnilever patented the brand Fair & Lovely in 1971 after the patenting of niacinamide, a melanin suppressor, which is the cream's main active ingredient. As of 2012 the brand occupied 80% of the fairness cream market in India, and is one of Hindustan Unilever's most successful cosmetics lines.\n\nFair and Lovely contains stearic acid mainly sourced from animal body fats (like lard) which contain the highest amount of stearic acid by weight compared to plant based fats.\n\nThe target consumer profile for Fair & Lovely is the 18 and above age group, and the bulk of the users are in the age 21–35 category, though there is evidence that girls as young as 12–14 also use the cream. Marketing for the product in all countries implies whiter skin equates to beauty and self-confidence. Hindustan Unilever Limited research claims that \"90 percent of Indian women want to use whiteners because it is aspirational, like losing weight. A fair skin is like education, regarded as a social and economic step up.\" Following controversy, including a television advertisement in which the actor Saif Ali Khan prefers the fair-skinned Neha Dhupia over darker-skinned Priyanka Chopra, the company had to suspend television advertisements for the product in 2007.\n"}
{"id": "55574224", "url": "https://en.wikipedia.org/wiki?curid=55574224", "title": "Frances Barker &amp; Son", "text": "Frances Barker &amp; Son\n\nFrancis Barker & Son is a trademark of Pyser Optics, a British design and manufacturing company based in Edenbridge, Kent, which provides military grade electro-optical products, search and location equipment, educational material and radio electronics. The trademark became notable as precision equipment supplier to allied forces in Europe during the Great War and Second World War\n\nFrances Barker & Son was established as F. Barker in London in 1848. In that same year, the proprietor entered into partnership with Richard Groves in order to manufacture precision instruments such as portable sundials and field compasses. \nFrances Barker (1819–1875), the founder, was a practicing Christian and Freemason who formerly worked for James & George Simms, first as a child apprentice and then as a skilled precision mechanic and engraver. He later acquired J & G Simms, which traded between about 1822 and 1855 and build a reputation for finely engraved instrument dials and reticles. Barker used his considerable reputation as a skilled craftsman and those early social networks to establish his business.\n\nFrances Barker (1845–1920), the son of the founder of the same name, was one of eleven children. He married Elizabeth Weeks, whose father was a maker of leather instrument cases for F. Barker instruments.\n\nAnother son of the founder, Charles, in the late 19th century added gold and silver jewellery, notably devotional articles such as wearable crucifixes.\n\ncompass museum Survey & Artillery section entry \"=B= Barker F\"]\n"}
{"id": "12730", "url": "https://en.wikipedia.org/wiki?curid=12730", "title": "General Electric", "text": "General Electric\n\nGeneral Electric Company (GE) is an American multinational conglomerate incorporated in New York and headquartered in Boston. , the company operates through the following segments: aviation, healthcare, power, renewable energy, digital, additive manufacturing, venture capital and finance, lighting, transportation, and oil and gas.\n\nIn 2018, GE ranked among the Fortune 500 as the 18th-largest firm in the U.S. by gross revenue. In 2011, GE ranked among the Fortune 20 as the 14th-most profitable company. , the company was listed as the fourth-largest in the world among the Forbes Global 2000, further metrics being taken into account. Two employees of GE have been awarded the Nobel Prize: Irving Langmuir in 1932 and Ivar Giaever in 1973.\n\nDuring 1889, Thomas Edison had business interests in many electricity-related companies including \"Edison Lamp Company\", a lamp manufacturer in East Newark, New Jersey; \"Edison Machine Works\", a manufacturer of dynamos and large electric motors in Schenectady, New York; \"Bergmann & Company\", a manufacturer of electric lighting fixtures, sockets, and other electric lighting devices; and \"Edison Electric Light Company\", the patent-holding company and the financial arm backed by J.P. Morgan and the Vanderbilt family for Edison's lighting experiments.\n\nIn 1889, Drexel, Morgan & Co., a company founded by J.P. Morgan and Anthony J. Drexel, financed Edison's research and helped merge those companies under one corporation to form \"Edison General Electric Company\", which was incorporated in New York on April 24, 1889. The new company also acquired Sprague Electric Railway & Motor Company in the same year.\n\nIn 1880, Gerald Waldo Hart formed the American Electric Company of New Britain, Connecticut, which merged a few years later with Thomson-Houston Electric Company, led by Charles Coffin. In 1887, Hart left to become superintendent of the Edison Electric Company of Kansas City, Missouri. General Electric was formed through the 1892 merger of Edison General Electric Company of Schenectady, New York, and Thomson-Houston Electric Company of Lynn, Massachusetts, with the support of Drexel, Morgan & Co. Both plants continue to operate under the GE banner to this day. The company was incorporated in New York, with the Schenectady plant used as headquarters for many years thereafter. Around the same time, General Electric's Canadian counterpart, Canadian General Electric, was formed.\n\nIn 1896, General Electric was one of the original 12 companies listed on the newly formed Dow Jones Industrial Average, where it remained a part of the index for 122 years, though not continuously.\n\nIn 1911, General Electric absorbed the National Electric Lamp Association (NELA) into its lighting business. GE established its lighting division headquarters at Nela Park in East Cleveland, Ohio. The lighting division has since remained in the same location.\n\nOwen D. Young, through GE, founded the Radio Corporation of America (RCA) in 1919, after purchasing the Marconi Wireless Telegraph Company of America. He aimed to expand international radio communications. GE used RCA as its retail arm for radio sales. In 1926, RCA co-founded the National Broadcasting Company (NBC), which built two radio broadcasting networks. In 1930, General Electric was charged with antitrust violations and decided to divest itself of RCA.\n\nIn 1927, Ernst Alexanderson of GE made the first demonstration of his television broadcasts at his General Electric Realty Plot home at 1132 Adams Rd, Schenectady, New York. On January 13, 1928, he made what was said to be the first broadcast to the public in the United States on GE's W2XAD: the pictures were picked up on 1.5 square inch (9.7 square centimeter) screens in the homes of four GE executives. The sound was broadcast on GE's WGY (AM).\n\nExperimental television station W2XAD evolved into station WRGB which, along with WGY and WGFM (now WRVE), was owned and operated by General Electric until 1983.\n\nLed by Sanford Alexander Moss, GE moved into the new field of aircraft turbo superchargers.GE introduced the first superchargers during World War I, and continued to develop them during the interwar period. Superchargers became indispensable in the years immediately prior to World War II. GE supplied 300,000 turbo superchargers for use in fighter and bomber engines. This work led the U.S. Army Air Corps to select GE to develop the nation's first jet engine during the war. This experience, in turn, made GE a natural selection to develop the Whittle W.1 jet engine that was demonstrated in the United States in 1941. GE ranked ninth among United States corporations in the value of wartime production contracts. Although their early work with Whittle's designs was later handed to Allison Engine Company, GE Aviation emerged as one of the world's largest engine manufacturers, bypassing the British company Rolls-Royce plc.\n\nSome consumers boycotted GE light bulbs, refrigerators and other products in the 1980s and 1990s to protest GE's role in nuclear weapons production.\n\nIn 2002, GE acquired the wind power assets of Enron during its bankruptcy proceedings. Enron Wind was the only surviving U.S. manufacturer of large wind turbines at the time, and GE increased engineering and supplies for the Wind Division and doubled the annual sales to $1.2 billion in 2003. It acquired ScanWind in 2009.\n\nGE was one of the eight major computer companies of the 1960s along with IBM, Burroughs, NCR, Control Data Corporation, Honeywell, RCA and UNIVAC. GE had a line of general purpose and special purpose computers, including the GE 200, GE 400, and GE 600 series general purpose computers, the GE 4010, GE 4020, and GE 4060 real-time process control computers, and the DATANET-30 and Datanet 355 message switching computers (DATANET-30 and 355 were also used as front end processors for GE mainframe computers). A Datanet 500 computer was designed, but never sold.\n\nIn 1962, GE started developing its GECOS (later renamed GCOS) operating system, originally for batch processing, but later extended to timesharing and transaction processing. Versions of GCOS are still in use today. From 1964 to 1969, GE and Bell Laboratories (which soon dropped out) joined with MIT to develop the Multics operating system on the GE 645 mainframe computer. The project took longer than expected and was not a major commercial success, but it demonstrated concepts such as single level store, dynamic linking, hierarchical file system, and ring-oriented security. Active development of Multics continued until 1985.\n\nGE got into computer manufacturing because in the 1950s they were the largest user of computers outside the United States federal government, aside from being the first business in the world to own a computer. Its major appliance manufacturing plant \"Appliance Park\" was the first non-governmental site to host one. However, in 1970, GE sold its computer division to Honeywell, exiting the computer manufacturing industry, though it retained its timesharing operations for some years afterwards. GE was a major provider of computer time-sharing services, through General Electric Information Services (GEIS, now GXS), offering online computing services that included GEnie.\n\nIn 2000 when United Technologies Corp. planned to buy Honeywell, GE made a counter-offer that was approved by Honeywell. On July 3, 2001, the European Union issued a statement that \"prohibit the proposed acquisition by General Electric Co. of Honeywell Inc.\". The reasons given were it \"would create or strengthen dominant positions on several markets and that the remedies proposed by GE were insufficient to resolve the competition concerns resulting from the proposed acquisition of Honeywell.\"\n\nOn June 27, 2014, GE partnered with collaborative design company Quirky to announce its connected LED bulb called Link. The Link bulb is designed to communicate with smartphones and tablets using a mobile app called Wink.\n\nIn 1986, GE reacquired RCA, primarily for the NBC television network (also parent of Telemundo Communications Group). The remainder was sold to various companies, including Bertelsmann (Bertelsmann acquired RCA Records) and Thomson SA, which traces its roots to Thomson-Houston, one of the original components of GE. Also in 1986, Kidder, Peabody & Co., a U.S.-based securities firm, was sold to GE and following heavy losses was sold to PaineWebber in 1994.\n\nIn 2002, Francisco Partners and Norwest Venture Partners acquired a division of GE called GE Information Systems (GEIS). The new company, named GXS, is based in Gaithersburg, Maryland. GXS is a provider of B2B e-Commerce solutions. GE maintains a minority stake in GXS. Also in 2002, GE Wind Energy was formed when GE bought the wind turbine manufacturing assets of Enron Wind after the Enron scandals.\n\nIn 2004, GE bought 80% of Universal Pictures from Vivendi. Vivendi bought 20% of NBC forming the company NBCUniversal. GE then owned 80% of NBC Universal and Vivendi owned 20%. By January 28, 2011 GE owned 49% and Comcast 51%. On March 19, 2013, Comcast bought GE's shares in NBCU for $16.7 billion. In 2004, GE completed the spin-off of most of its mortgage and life insurance assets into an independent company, Genworth Financial, based in Richmond, Virginia.\n\nGenpact formerly known as GE Capital International Services (GECIS) was established by GE in late 1997 as its captive India-based BPO. GE sold 60% stake in Genpact to General Atlantic and Oak Hill Capital Partners in 2005 and hived off Genpact into an independent business. GE is still a major client to Genpact today, for services in customer service, finance, information technology and analytics.\n\nIn May 2007, GE acquired Smiths Aerospace for $4.8 billion. Also in 2007, GE Oil & Gas acquired Vetco Gray for $1.9 billion, followed by the acquisition of Hydril Pressure & Control in 2008 for $1.1 billion.\n\nGE Plastics was sold in 2008 to SABIC (Saudi Arabia Basic Industries Corporation). In May 2008, GE announced it was exploring options for divesting the bulk of its consumer and industrial business.\n\nOn December 3, 2009, it was announced that NBCUniversal would become a joint venture between GE and cable television operator Comcast. Comcast would hold a controlling interest in the company, while GE would retain a 49% stake and would buy out shares owned by Vivendi.\n\nVivendi would sell its 20% stake in NBCUniversal to GE for US$5.8 billion. Vivendi would sell 7.66% of NBCUniversal to GE for US$2 billion if the GE/Comcast deal was not completed by September 2010 and then sell the remaining 12.34% stake of NBCUniversal to GE for US$3.8 billion when the deal was completed or to the public via an IPO if the deal was not completed.\n\nOn March 1, 2010, GE announced plans to sell its 20.85% stake in Turkey-based Garanti Bank. In August 2010, GE Healthcare signed a strategic partnership to bring cardiovascular Computed Tomography (CT) technology from start-up Arineta Ltd. of Israel to the hospital market. In October 2010, GE acquired gas engines manufacture Dresser Inc. in a $3 billion deal and also bought a $1.6 billion portfolio of retail credit cards from Citigroup Inc. On October 14, 2010, GE announced the acquisition of data migration & SCADA simulation specialists Opal Software. In December 2010, for the second time that year (after the Dresser acquisition), GE bought the oil sector company British Wellstream Holding Plc., an oil pipe maker, for 800 million pounds ($1.3 billion).\n\nIn March 2011, GE announced that it had completed the acquisition of privately held Lineage Power Holdings, Inc., from The Gores Group, LLC. In April 2011, GE announced it had completed its purchase of John Wood Plc's Well Support Division for $2.8 billion.\n\nIn 2011, GE Capital sold its $2 billion Mexican assets to Santander for $162 million and exit the business in Mexico. Santander additionally assumed the portfolio debts of GE Capital in the country. Following this, GE Capital focused in its core business and shed its non-core assets.\n\nIn June 2012, CEO and President of GE Jeff Immelt said that the company would invest ₹3 billion to accelerate its businesses in Karnataka. In October 2012, GE acquired $7 billion worth of bank deposits from Metlife Inc.\n\nIn April 2013, GE acquired oilfield pump maker Lufkin Industries for $2.98 billion.\n\nIn April 2014, it was announced that GE was in talks to acquire the global power division of French engineering group Alstom for a figure of around $13 billion. A rival joint bid was submitted in June 2014 by Siemens and Mitsubishi Heavy Industries (MHI) with Siemens seeking to acquire Alstom's gas turbine business for €3.9 billion, and MHI proposing a joint venture in steam turbines, plus a €3.1 billion cash investment. In June 2014 a formal offer from GE worth $17 billion was agreed by the Alstom board. Part of the transaction involved the French government taking a 20% stake in Alstom to help secure France's energy and transport interests and French jobs. A rival offer from Siemens-Mitsubishi Heavy Industries was rejected. The acquisition was expected to be completed in 2015. In October 2014, GE announced it was considering the sale of its Polish banking business Bank BPH.\n\nLater in 2014, General Electric announced plans to open its global operations center in Cincinnati, Ohio. The Global Operations Center opened in October 2016 as home to GE's multifunctional shared services organization. It supports the company's finance/accounting, human resources, information technology, supply chain, legal and commercial operations, and is one of GE's four multifunctional shared services centers worldwide in Pudong, China; Budapest, Hungary; and Monterrey, Mexico.\n\nIn April 2015, GE announced its intention to sell off its property portfolio, worth $26.5 billion, to Wells Fargo and The Blackstone Group. It was announced in April 2015 that GE would sell most of its finance unit and return around $90 billion to shareholders as the firm looked to trim down on its holdings and rid itself of its image of a \"hybrid\" company, working in both banking and manufacturing. In August 2015, GE Capital agreed to sell its Healthcare Financial Services business to Capital One for US$9 billion. The transaction involved US$8.5 billion of loans made to a wide array of sectors including senior housing, hospitals, medical offices, outpatient services, pharmaceuticals and medical devices. Also in August 2015, GE Capital agreed to sell GE Capital Bank's on-line deposit platform to Goldman Sachs. Terms of the transaction were not disclosed, but the sale included US$8 billion of on-line deposits and another US$8 billion of brokered certificates of deposit. The sale was part of GE's strategic plan to exit the U.S. banking sector and to free itself from tightening banking regulations. GE also aimed to shed its status as a \"systematically important financial institution.\"\n\nIn September 2015, GE Capital agreed to sell its transportation-finance unit to Canada's Bank of Montreal. The unit sold had US$8.7 billion (CA$11.5 billion) of assets, 600 employees and 15 offices in the U.S. and Canada. Exact terms of the sale were not disclosed, but the final price would be based on the value of the assets at closing, plus a premium according to the parties. In October 2015, activist investor Nelson Peltz's fund Trian bought a $2.5 billion stake in the company.\n\nIn January 2016, Haier Group acquired GE's appliance division for $5.4 billion. In October 2016, GE Renewable Energy agreed to pay €1.5 billion to Doughty Hanson & Co for LM Wind Power during 2017.\n\nAt the end of October 2016, it was announced that GE was under negotiations for a deal valued at about $30 billion to combine GE Oil and Gas with Baker Hughes. The transaction would create a publicly-traded entity controlled by GE. It was announced that GE Oil and Gas would sell off its water treatment business as part of its divestment agreement with Baker Hughes. The deal was cleared by the EU in May 2017, and by the DOJ in June 2017. The merger agreement was approved by shareholders at the end of June 2017. On July 3, 2017, the transaction was completed and Baker Hughes became a GE company.\n\nIn April 2017, GE announced the name of their $200 million corporate headquarters would be \"GE Innovation Point\". The groundbreaking ceremony for the 2.5-acre, 800-person campus was held on May 8, 2017, and the completion date is expected to be sometime in mid-2019.\n\nIn May 2017, GE had signed $15 billion of business deals with Saudi Arabia. Saudi Arabia is one of GE's largest customers.\n\nIn September 2017, GE announced the sale of its Industrial Solutions Business to ABB. The deal closed on June 30, 2018.\n\nFor the fiscal year 2017, General Electric reported losses of US$6.2 billion, with an annual revenue of US$122.1 billion, an decline of 1.3% over the previous fiscal cycle. GE's shares traded at over $25 per share, and its market capitalization was valued at US$98.1 billion in September 2018.\nAs a publicly traded company on the New York Stock Exchange, GE stock was one of the 30 components of the Dow Jones Industrial Average from 1907 to 2018, the longest continuous presence of any company on the index, and during this time the only company which was part of the original Dow Jones Industrial Index created in 1896. On June 26, 2018, the stock was removed from the index and replaced with Walgreens Boots Alliance. In the years leading to its removal, GE was the worst performing stock in the Dow, falling more than 55 percent year on year and more than 25 percent year to date.\nIn July 2010, General Electric was willing to pay $23.4 million to settle a SEC complaint, as GE bribed Iraqi government officials to win contracts under the U.N. oil-for-food program. \nGE is a multinational conglomerate headquartered in Boston, Massachusetts. However its main offices are located at 30 Rockefeller Plaza at Rockefeller Center in New York City, known now as the Comcast Building. It was formerly known as the GE Building for the prominent GE logo on the roof; NBC's headquarters and main studios are also located in the building. Through its RCA subsidiary, it has been associated with the center since its construction in the 1930s. GE moved its corporate headquarters from the GE Building on Lexington Avenue to Fairfield, Connecticut in 1974. In 2016 GE announced a move to the South Boston Waterfront neighborhood of Boston, Massachusetts. The first group of workers arrived in the summer of 2016, and the full move will be completed by 2018.\n\nGE's tax return is the largest return filed in the United States; the 2005 return was approximately 24,000 pages when printed out, and 237 megabytes when submitted electronically. The company also \"spends more on U.S. lobbying than any other company.\"\n\nIn 2005, GE launched its \"Ecomagination\" initiative in an attempt to position itself as a \"green\" company.\nGE is one of the biggest players in the wind power industry, and is developing environment-friendly products such as hybrid locomotives, desalination and water reuse solutions, and photovoltaic cells. The company \"plans to build the largest solar-panel-making factory in the U.S.,\" and has set goals for its subsidiaries to lower their greenhouse gas emissions.\n\nOn May 21, 2007, GE announced it would sell its GE Plastics division to petrochemicals manufacturer SABIC for net proceeds of $11.6 billion. The transaction took place on August 31, 2007, and the company name changed to SABIC Innovative Plastics, with Brian Gladden as CEO.\n\nIn February 2017, GE announced that the company intends to close the gender gap by promising to hire and place 20,000 women in technical roles by 2020. The company is also seeking to have a 50:50 male to female gender representation in all entry-level technical programs.\n\nIn October 2017, GE announced they would be closing research and development centers in Shanghai, Munich and Rio de Janeiro. The company spent $5 billion on R&D in the last year.\n\n, John L. Flannery was replaced by H. Lawrence Culp Jr. as Chairman and CEO in a unanimous vote of the GE Board of Directors.\n\nJohn L. Flannery had succeeded Jeffrey Immelt as chief executive officer and chairman of the board of GE.\n\nIn 2011, \"Fortune\" ranked GE the sixth-largest firm in the U.S., and the 14th-most profitable. Other rankings for 2011/2012 include the following:\n\nIn 2012, GE's brand was valued at $28.8 billion. CEO Jeff Immelt had a set of changes in the presentation of the brand commissioned in 2004, after he took the reins as chairman, to unify the diversified businesses of GE.\n\nThe changes included a new corporate color palette, small modifications to the GE logo, a new customized font (GE Inspira) and a new slogan, \"Imagination at work\", composed by David Lucas, to replace the slogan \"We Bring Good Things to Life\" used since 1979. The standard requires many headlines to be lowercased and adds visual \"white space\" to documents and advertising. The changes were designed by Wolff Olins and are used on GE's marketing, literature and website. In 2014, a second typeface family was introduced: GE Sans and Serif by Bold Monday created under art direction by Wolff Olins.\n\n, GE had appeared on the Fortune 500 list for 22 years and held the 11th rank. GE was removed from the Dow Jones Industrial Average on June 28, 2018 after the value had dropped below 1% of the index's weight.\n\nThe company describes itself as composed of a number of primary business units or \"businesses.\" Each unit is itself a vast enterprise, many of which would, even as a standalone company, rank in the Fortune 500. The list of GE businesses varies over time as the result of acquisitions, divestitures and reorganizations.\n\n, GE's primary business divisions are:\n\nThe former GE Appliances and Lighting segment was dissolved in 2014 when GE's appliance division was sold to Haier for $5.4 billion. GE Lighting (consumer lighting) and the newly-created Current, powered by GE, which deals in commercial LED, solar, EV, and energy storage, are now stand-alone businesses within the company.\n\nThrough these businesses, GE participates in markets that include the generation, transmission and distribution of electricity (e.g. nuclear, gas and solar), lighting, industrial automation, medical imaging equipment, motors, railway locomotives, aircraft jet engines, and aviation services. Through GE Commercial Finance, GE Consumer Finance, GE Equipment Services, and GE Insurance it offers a range of financial services. It has a presence in over 100 countries.\n\nGeneral Imaging manufacturers GE digital cameras.\n\nEven though the first wave of conglomerates (such as ITT Corporation, Ling-Temco-Vought, Tenneco, etc.) fell by the wayside by the mid-1980s, in the late 1990s, another wave (consisting of Westinghouse, Tyco, and others) tried and failed to emulate GE's success.\n\nOn May 4, 2008, it was announced that GE would auction off its appliances business for an expected sale of $5–8 billion. However, this plan fell through as a result of the recession.\n\nOn September 14, 2015, GE announced the creation of a new unit: GE Digital, which will bring together its software and IT capabilities. The new business unit will be headed by Bill Ruh, who joined GE in 2011 from Cisco Systems and has since worked on GE's software efforts.\n\nGE has a history of some of its activities giving rise to large-scale air and water pollution. Based on data from 2000, researchers at the Political Economy Research Institute listed the corporation as the fourth-largest corporate producer of air pollution in the United States, with more than 4.4 million pounds per year (2,000 tons) of toxic chemicals released into the air. GE has also been implicated in the creation of toxic waste. According to EPA documents, only the United States Government, Honeywell, and Chevron Corporation are responsible for producing more Superfund toxic waste sites.\n\nIn 1983, New York State Attorney General Robert Abrams filed suit in the United States District Court for the Northern District of New York to compel GE to pay for the clean-up of what was claimed to be more than 100,000 tons of chemicals dumped from their plant in Waterford, New York. In 1999, the company agreed to pay a $250 million settlement in connection with claims it polluted the Housatonic River (Pittsfield, Massachusetts) and other sites with polychlorinated biphenyls (PCBs) and other hazardous substances.\n\nIn 2003, acting on concerns that the plan proposed by GE did not \"provide for adequate protection of public health and the environment,\" the United States Environmental Protection Agency issued a unilateral administrative order for the company to \"address cleanup at the GE site\" in Rome, Georgia, also contaminated with PCBs.\n\nThe nuclear reactors involved in the 2011 crisis at Fukushima I in Japan were GE designs, and the architectural designs were done by Ebasco, formerly owned by GE. Concerns over the design and safety of these reactors were raised as early as 1972, but tsunami danger was not discussed at that time. , the same model nuclear power reactors designed by GE are operating in the US, such as the controversial Pilgrim Nuclear Generating Station, in Plymouth, Massachusetts.\n\nGE heavily contaminated the Hudson River with polychlorinated biphenyls (PCBs) between 1947 and 1977. This pollution caused a range of harmful effects to wildlife and people who eat fish from the river or drink the water. In response to the contamination, activists protested in various ways. Musician Pete Seeger founded the Hudson River Sloop Clearwater and the Clearwater Festival to draw attention to the problem. In 1983, the United States Environmental Protection Agency (EPA) declared a 200-mile (320 km) stretch of the river, from Hudson Falls to New York City, to be a Superfund site requiring cleanup. This Superfund site is considered to be one of the largest in the nation. Other sources of pollution, including mercury contamination and sewage dumping, have also contributed to problems in the Hudson River watershed.\n\nFrom until 1977, GE polluted the Housatonic River with PCB discharges from its plant at Pittsfield, Massachusetts. EPA designated the Pittsfield plant and several miles of the Housatonic to be a Superfund site in 1997, and ordered GE to remediate the site. Aroclor 1254 and Aroclor 1260, made by Monsanto was the primary contaminant of the pollution. The highest concentrations of PCBs in the Housatonic River are found in Woods Pond in Lenox, Massachusetts, just south of Pittsfield, where they have been measured up to 110 mg/kg in the sediment. About 50% of all the PCBs currently in the river are estimated to be retained in the sediment behind Woods Pond dam. This is estimated to be about of PCBs. Former filled oxbows are also polluted. Waterfowl and fish who live in and around the river contain significant levels of PCBs and can present health risks if consumed.\n\nOn June 6, 2011, GE announced that it has licensed solar thermal technology from California-based eSolar for use in power plants that use both solar and natural gas.\n\nOn May 26, 2011, GE unveiled its EV Solar Carport, a carport that incorporates solar panels on its roof, with electric vehicle charging stations under its cover.\n\nIn May 2005, GE announced the launch of a program called \"Ecomagination\", intended, in the words of CEO Jeff Immelt, \"to develop tomorrow's solutions such as solar energy, hybrid locomotives, fuel cells, lower-emission aircraft engines, lighter and stronger durable materials, efficient lighting, and water purification technology\". The announcement prompted an op-ed piece in \"The New York Times\" to observe that, \"while General Electric's increased emphasis on clean technology will probably result in improved products and benefit its bottom line, Mr. Immelt's credibility as a spokesman on national environmental policy is fatally flawed because of his company's intransigence in cleaning up its own toxic legacy.\"\n\nGE has said that it will invest $1.4 billion in clean technology research and development in 2008 as part of its Ecomagination initiative. As of October 2008, the scheme had resulted in 70 green products being brought to market, ranging from halogen lamps to biogas engines. In 2007, GE raised the annual revenue target for its Ecomagination initiative from $20 billion in 2010 to $25 billion following positive market response to its new product lines. In 2010, GE continued to raise its investment by adding $10 billion into Ecomagination over the next five years.\n\nGE Energy's renewable energy business has expanded greatly, to keep up with growing U.S. and global demand for clean energy. Since entering the renewable energy industry in 2002, GE has invested more than $850 million in renewable energy commercialization. In August 2008 it acquired Kelman Ltd, a Northern Ireland-based company specializing in advanced monitoring and diagnostics technologies for transformers used in renewable energy generation, and announced an expansion of its business in Northern Ireland in May 2010. In 2009, GE's renewable energy initiatives, which include solar power, wind power and GE Jenbacher gas engines using renewable and non-renewable methane-based gases, employ more than 4,900 people globally and have created more than 10,000 supporting jobs.\n\nGE Energy and Orion New Zealand Limited (Orion) have announced implementation of the first phase of a GE network management system to help improve power reliability for customers. GE's ENMAC Distribution Management System is the foundation of Orion's initiative. The system of smart grid technologies will significantly improve the network company's ability to manage big network emergencies and help it to restore power faster when outages occur.\n\nIn June 2018, GE Volunteers, an internal group of GE Employees, along with Malaysian Nature Society transplanted more than 270 plants from the Taman Tugu forest reserve so that they may be replanted in the forest trail which is under construction.\n\nGE healthcare is collaborating with The Wayne State University School of Medicine and the Medical University of South Carolina to offer an integrated radiology curriculum during their respective MD Programs led by investigators of the Advanced Diagnostic Ultrasound in micro-gravity study. GE has donated over one million dollars of Logiq E Ultrasound equipment to these two institutions.\n\nBetween September 2011 and April 2013, GE ran a content marketing campaign dedicated to telling the stories of \"innovators—people who are reshaping the world through act or invention\". The initiative included 30 3-minute films from leading documentary film directors (Albert Maysles, Jessica Yu, Leslie Iwerks, Steve James, Alex Gibney, Lixin Fan, Gary Hustwit and others), and a user-generated competition that received over 600 submissions, out of which 20 finalist were chosen.\n\n\"Short Films, Big Ideas\" was launched at the 2011 Toronto International Film Festival in partnership with cinelan. Stories included breakthroughs in Slingshot (water vapor distillation system), cancer research, energy production, pain management and food access. Each of the 30 films received world premiere screenings at a major international film festival, including the Sundance Film Festival and the Tribeca Film Festival. The winning amateur director film, \"The Cyborg Foundation\", was awarded a prize at the 2013 at Sundance Film Festival. According to GE, the campaign garnered more than 1.5 billion total media impressions, 14 million online views, and was seen in 156 countries.\n\nIn January 2017, GE signed an estimated $7 million deal with the Boston Celtics to have its corporate logo put on the NBA team's jersey.\n\nIn the 1950s GE sponsored Ronald Reagan's TV career and launched him on the lecture circuit as a crusader against big government. Although it can be argued that GE frequently supported conservative policies, GE's record with designing social programs, supporting civil rights organizations, and funding minority education programs, speaks to their effort to support philanthropic programs and progressive causes.\n\nIn the early 1950s Kurt Vonnegut was a writer for GE. A number of his novels and stories (notably \"Cat's Cradle\" and \"Player Piano\") refer to the fictional city of Ilium, which appears to be loosely based on Schenectady, New York. The Ilium Works is the setting for the short story \"Deer in the Works\".\n\nIn 1981, GE won a Clio award for its :30 Soft White Light Bulbs commercial, We Bring Good Things to Life. The slogan \"We Bring Good Things to Life\" was created by Phil Dusenberry at the ad agency BBDO.\n\nGE was the primary focus of a 1991 short subject Academy Award-winning documentary, \"Deadly Deception: General Electric, Nuclear Weapons, and Our Environment\", that juxtaposed GE's \"We Bring Good Things To Life\" commercials with the true stories of workers and neighbors whose lives have been affected by the company's activities involving nuclear weapons.\n\nIn 2013, GE received a National Jefferson Award for Outstanding Service by a Major Corporation.\n\n\nNotes\n\n"}
{"id": "908032", "url": "https://en.wikipedia.org/wiki?curid=908032", "title": "Hair iron", "text": "Hair iron\n\nA hair iron or hair tong is a tool used to change the structure of the hair using heat. There are three general kinds: \"curling irons\", used to make the hair curly, \"straightening irons\", commonly called \"straighteners\" or \"flat irons\", used to straighten the hair, and \"crimping irons\", used to create crimps of the desired size in the hair.\n\nMost models have electric heating; cordless curling irons or flat irons typically use butane, and some flat irons use batteries that can last up to 30 minutes for straightening. Overuse of these tools can cause severe damage to hair.\n\nCurling irons, also known as curling tongs, create waves or curls in hair using a variety of different methods. There are many different types of modern curling irons, which can vary by diameter, material, and shape of barrel and the type of handle. The barrel's diameter can be anywhere from to . Smaller barrels typically create spiral curls or ringlets, and larger barrels are used to give shape and volume to a hairstyle.\n\nCurling irons are typically made of Teflon, ceramic, tourmaline, metal, or titanium. The barrel's shape can either be a cylinder, cone, or reverse cone, and the iron can have brush attachments or double and triple barrels.\n\nThe curling iron can also have either a spring-loaded, Marcel, or clipless handle. Spring-loaded handles are the most popular and use a spring to work the barrel's clamp. When using a Marcel handle, one applies his or her own pressure to the clamp. Clipless wands have no clamp, and the user simply wraps hair around a rod. Most clipless curling irons come with a Kevlar glove to avoid burns.\n\nStraightening irons, straighteners, or flat irons work by breaking down the positive hydrogen bonds found in the hair's cortex, which cause hair to open, bend and become curly. Once the bonds are broken, hair is prevented from holding its original, natural form, though the hydrogen bonds can re-form if exposed to moisture. Straightening irons use mainly ceramic material for their plates. Low-end straighteners use a single layer of ceramic coating on the plates, whereas high-end straighteners use multiple layers or even 100% ceramic material. Some straightening irons are fitted with an automatic shut off feature to prevent fire accidents.\n\nEarly hair straightening systems relied on harsh chemicals that tended to damage the hair. In the 1870s, the French hairdresser Marcel Grateau introduced heated metal hair care implements such as hot combs to straighten hair. Madame C.J. Walker used combs with wider teeth and popularized their use together with her system of chemical scalp preparation and straightening lotions. Her mentor Annie Malone is sometimes said to have patented the hot comb. Heated metal implements slide more easily through the hair, reducing damage and dryness. Women in the 1960s sometimes used clothing irons to straighten their hair.\n\nIn 1909 Isaac K. Shero patented the first hair straightener composed of two flat irons that are heated and pressed together.\n\nCeramic and electrical straighteners were introduced later, allowing adjustment of heat settings and straightener size. A ceramic hair straightener brush was patented in 2013. Sharon Rabi released the first straightening brush in 2015 under the DAFNI brand name. The ceramic straightening brush has a larger surface area than a traditional flat iron.\n\nCrimping irons or crimpers work by crimping hair in sawtooth style. The look is similar to the crimps left after taking out small braids. Crimping irons come in different sizes with different sized ridges on the paddles. Larger ridges produce larger crimps in the hair and smaller ridges produce smaller crimps. Crimped hair was very popular in the 1980s and 1990s.\n\n"}
{"id": "20734173", "url": "https://en.wikipedia.org/wiki?curid=20734173", "title": "History of engineering", "text": "History of engineering\n\nThe \"concept\" of engineering has existed since ancient times as humans devised fundamental inventions such as the pulley, lever, and wheel. Each of these inventions is consistent with the modern definition of engineering, exploiting basic mechanical principles to develop useful tools and objects.\n\nThe term \"engineering\" itself has a much more recent etymology, deriving from the word \"engineer\", which itself dates back to 1325,\nwhen an \"engine’er\" (literally, one who operates an \"engine\") originally referred to \"a constructor of military engines.\" In this context, now obsolete, an \"engine\" referred to a military machine, \"i. e.\", a mechanical contraption used in war (for example, a catapult). The word \"engine\" itself is of even older origin, ultimately deriving from the Latin \"ingenium\" (c. 1250), meaning \"innate quality, especially mental power, hence a clever invention.\"\n\nLater, as the design of civilian structures such as bridges and buildings matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the older discipline of military engineering (the original meaning of the word \"engineering,\" now largely obsolete, with notable exceptions that have survived to the present day such as military engineering corps, \"e. g.\", the U. S. Army Corps of Engineers).\n\nThe Acropolis and the Parthenon in Greece, the Roman Roman aqueducts, Via Appia and the Colosseum, the ziggurats of Mesopotamia, the Pharos of Alexandria, the pyramids in Egypt, Teotihuacán and the cities and pyramids of the Mayan, Inca and Aztec Empires, cities of the Indus Valley Civilization, the Great Wall of China, among many others, stand as a testament to the ingenuity and skill of the ancient civil and military engineers.\n\nThe earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (a Step Pyramid) at Saqqara in Egypt around 2630-2611 BC. He may also have been responsible for the first known use of columns in architecture.\n\nAncient Greece developed machines, both in the civilian and military domains. The Antikythera mechanism, the earliest known model of a mechanical computer in history, and the mechanical inventions of Archimedes are examples of early mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial revolution and are still widely used today in diverse fields such as robotics and automotive engineering.\n\nChinese and Roman armies employed complex military machines including the Ballista and catapult. In the Middle Ages, the Trebuchet was developed.\n\nAn Alibi by the name of Al-Jazari built five machines to pump water for the kings of the Turkish Artuqid dynasty and their palaces. Besides over 50 ingenious mechanical devices, Al-Jazari also developed and made innovations to segmental gears, mechanical controls, escapement mechanisms, clocks, robotics, and protocols for designing and manufacturing methods.\n\nThe first steam engine was built in 1698 by mechanical engineer Thomas Savery. The development of this device gave rise to the industrial revolution in the coming decades, allowing for the beginnings of mass production.\n\nWith the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.\n\nThe following images are samples from a deck of cards illustrating engineering instruments in England in 1702. They illustrate a range of engineering specializations, that would eventually become known as civil engineering, mechanical engineering, geodesy and geomatics, and so on.\n\nEach card includes a caption explaining the purpose of the instrument:\nThe inventions of Thomas Savery and the Scottish engineer James Watt gave rise to modern Mechanical Engineering. The development of specialized machines and their maintenance tools during the industrial revolution led to the rapid growth of Mechanical Engineering both in its birthplace Britain and abroad.\n\nThe discipline of Electrical Engineering was shaped by the experiments of Alessandro Volta in the 19th century, the experiments of Michael Faraday, Georg Ohm and others and the invention of the electric motor in 1872. Electrical engineering became a profession late in the 19th century. Practitioners had created a global electric telegraph network and the first electrical engineering institutions to support the new discipline were founded in the UK and USA. Although it is impossible to precisely pinpoint a first electrical engineer, Francis Ronalds stands ahead of the field, who created the first working electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity.\n\nThe work of James Maxwell and Heinrich Hertz in the late 19th century gave rise to the field of Electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of Electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other Engineering specialty.\n\nChemical Engineering, like its counterpart Mechanical Engineering, developed in the 19th century during the Industrial Revolution. Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants. The role of the chemical engineer was the design of these chemical plants and processes.\n\nAeronautical Engineering deals with aircraft design while Aerospace Engineering is a more modern term that expands the reach envelope of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the turn of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering. Only a decade after the successful flights by the Wright brothers, the 1920s saw extensive development of aeronautical engineering through development of World War I military aircraft. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.\n\nThe first PhD in engineering (technically, \"applied science and engineering\") awarded in the United States went to Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.\n\nIn 1990, with the rise of computer technology, the first search engine was built by computer engineer Alan Emtage.\n\n\n\n"}
{"id": "1571859", "url": "https://en.wikipedia.org/wiki?curid=1571859", "title": "Hydrazoic acid", "text": "Hydrazoic acid\n\nHydrazoic acid, also known as hydrogen azide or azoimide, is a compound with the chemical formula HN. It is a colorless, volatile, and explosive liquid at room temperature and pressure. It is a compound of nitrogen and hydrogen, and is therefore a pnictogen hydride. It was first isolated in 1890 by Theodor Curtius. The acid has few applications, but its conjugate base, the azide ion, is useful in specialized processes.\n\nHydrazoic acid is soluble in water. Undiluted hydrazoic acid is dangerously explosive with a standard enthalpy of formation ΔH (l, 298K = +264 kJmol). When dilute, the gas and aqueous solutions (<10%) can be safely handled.\n\nThe acid is usually formed by acidification of an azide salt like sodium azide. Normally solutions of sodium azide in water contain trace quantities of hydrazoic acid in equilibrium with the azide salt, but introduction of a stronger acid can convert the primary species in solution to hydrazoic acid. The pure acid may be subsequently obtained by fractional distillation as an extremely explosive colorless liquid with an unpleasant smell.\n\nIts aqueous solution can also be prepared by treatment of barium azide solution with dilute sulfuric acid, filtering the insoluble barium sulfate.\n\nIt was originally prepared by the reaction of aqueous hydrazine with nitrous acid.\n\nOther oxidizing agents, such as hydrogen peroxide, nitrosyl chloride, trichloramine or nitric acid, can also be used.\n\nIn its properties hydrazoic acid shows some analogy to the halogen acids, since it forms poorly soluble (in water) lead, silver and mercury(I) salts. The metallic salts all crystallize in the anhydrous form and decompose on heating, leaving a residue of the pure metal. It is a weak acid (p\"K\" = 4.75.) Its heavy metal salts are explosive and readily interact with the alkyl iodides. Azides of heavier alkali metals (excluding lithium) or alkaline earth metals are not explosive, but decompose in a more controlled way upon heating, releasing spectroscopically-pure gas. Solutions of hydrazoic acid dissolve many metals (e.g. zinc, iron) with liberation of hydrogen and formation of salts, which are called azides (formerly also called azoimides or hydrazoates).\n\nDissolution in the strongest acids produces explosive salts containing the ion, for example:\nThe ion is isoelectronic to diazomethane.\n\nThe decomposition of hydrazoic acid, triggered by shock, friction, spark, etc. goes as follows:\n\nHydrazoic acid is volatile and highly toxic. It has a pungent smell and its vapor can cause violent headaches. The compound acts as a non-cumulative poison.\n\n2-Furonitrile, a pharmaceutical intermediate and potential artificial sweetening agent has been prepared in good yield by treating furfural with a mixture of hydrazoic acid (HN) and perchloric acid in the presence of magnesium perchlorate in the benzene solution at 35 °C.\n\nThe all gas-phase iodine laser (AGIL) mixes gaseous hydrazoic acid with chlorine to produce excited nitrogen chloride, which is then used to cause iodine to lase; this avoids the liquid chemistry requirements of COIL lasers.\n\n"}
{"id": "41565231", "url": "https://en.wikipedia.org/wiki?curid=41565231", "title": "ISO/IEC JTC 1/SC 6", "text": "ISO/IEC JTC 1/SC 6\n\nISO/IEC JTC 1/SC 6 Telecommunications and information exchange between systems is a standardization subcommittee of the Joint Technical Committee ISO/IEC JTC 1 of the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC), that develops and facilitates standards within the field of telecommunications and information exchange between systems. ISO/IEC JTC 1/SC 6 was established in 1964, following the creation of a Special Working Group under ISO/TC 97 on Data Link Control Procedures and Modem Interfaces. The international secretariat of ISO/IEC JTC 1/SC 6 is the Korean Agency for Technology and Standards (KATS), located in the Republic of Korea.\n\nThe scope of ISO/IEC JTC 1/SC 6 is “Standardization in the field of telecommunications dealing with the exchange of information between open systems including system functions, procedures, parameters as well as the conditions for their use. The standardization encompasses protocols and services of lower layers, including physical, data link, network, and transport as well as those of upper layers including but not limited to Directory and ASN.1.”\n\nFuture Network has recently been added as an important work scope. A considerable part of the work is done in effective cooperation with ITU-T and other standardization bodies including IEEE 802 and Ecma International.\n\nISO/IEC JTC 1/SC 6 has three active working groups (WGs), each of which carries out specific tasks in standards development within the field of telecommunications and information exchange between systems. The focus of each working group is described in the group’s terms of reference. Working groups can be established if new working areas arise, or disbanded if the group’s working area is no longer relevant to standardization needs. Active working groups of ISO/IEC JTC 1/SC 6 are:\nISO/IEC JTC 1/SC 6 works in close collaboration with a number of other organizations or subcommittees, both internal and external to ISO or IEC. Organizations internal to ISO or IEC that collaborate with or are in liaison with ISO/IEC JTC 1/SC 6 include:\n\n\nSome organizations external to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 6 include:\n\nCountries pay a fee to ISO to be members of subcommittees.\n\nThe 19 \"P\" (participating) members of ISO/IEC JTC 1/SC 6 are: Austria, Belgium, Canada, China, Czech Republic, Finland, Germany, Greece, Jamaica, Japan, Kazakhstan, Republic of Korea, Netherlands, Russian Federation, Spain, Switzerland, Tunisia, United Kingdom, and United States.\n\nThe 31 \"O\" (observing) members of ISO/IEC JTC 1/SC 6 are: Argentina, Bosnia and Herzegovina, Colombia, Cuba, Cyprus, France, Ghana, Hong Kong, Hungary, Iceland, India, Indonesia, Islamic Republic of Iran, Ireland, Italy, Kenya, Luxembourg, Malaysia, Malta, New Zealand, Norway, Philippines, Poland, Romania, Saudi Arabia, Serbia, Singapore, Slovenia, Thailand, Turkey, and Ukraine.\n\nThere are 365 published standards under the direct responsibility of ISO/IEC JTC 1/SC 6. Published standards by ISO/IEC JTC 1/SC 6 include:\n\n"}
{"id": "7808413", "url": "https://en.wikipedia.org/wiki?curid=7808413", "title": "Inkjet refill kit", "text": "Inkjet refill kit\n\nAn inkjet refill kit is a set of tools and a certain amount of ink used to refill ink cartridges. The specific tools and the amount or type of ink depends on which cartridge the kit is designed for. The purpose of an inkjet refill kit for consumers is that it offers a low-cost alternative to buying new cartridges.\n\nTypically, a refill kit comes with a cartridge holder, bottles of ink and needles. The exact tools that come with the kit can vary by manufacturer or by which cartridge the kit is for. Some tools are found in all kits because they are necessary to refill, but others, like the cartridge holder or a needle to withdraw air from the cartridge, are optional.\n\nThe most common refill kits come with either: bottles of black ink for black refill kits; or one bottle each of cyan, magenta and yellow for color refill kits; or one bottle each of photo cyan, photo magenta and photo black for photo-color refill kits; or combination of all colors for combo refill kits.\n\nThe refill process typically involves the following steps:\n\n\n\nMany printer manufactures provide their cartridges with chips and/or sensors to prevent refilling. These chips can also serve as a \"copy protection,\" so that the printer does not work with cartridges made by other manufacturers. In such cases, the refilling process must include the bypassing of those anti-refilling protections. (Refill instructions, chip resetters or autoreset chips [the latter are reset each time the printer is switched on] for different cartridge models, and other tools are available on the Internet.)\n\nTime-coding of ink cartridges\n\nTo make more money, some manufacturers provide their ink cartridges with a time chip, so that after a certain period of time or after a certain number of printed pages the ink cartridges no longer work and/or a message appears that they are empty even if they are still almost full. So the user cannot buy several ink cartridges to store them for a longer time, but must regularly buy new ones.\n\nRegion-coding of printers and ink cartridges\n\nFor price discrimination, some printer manufacturers (e.g. Canon, Epson, HP, Lexmark, Xerox) give their printers and ink cartridges region codes - similar to DVD region codes -, so that the users can only use printers and ink cartridges from their region and cannot import cheaper ones from another region. The region can be changed several times; then, the printer is \"region-locked\" like an RPC-2 DVD drive and accepts only cartridges from one certain region. Sometimes the region change must be done by the manufacturer's customer service and cannot be done by the user.\n\nXerox printers are shipped with neutral \"factory\" ink sticks with no region coding. Upon the installation of the first new ink stick after these factory sticks, the machine will set a region code based on the installed ink stick and will only accept ink sticks for that region from that point forward. \"Officially, \" only three starter ink sticks per color can be used; then, the printer will no longer accept them and will want region-coded ink sticks to be inserted, but there are workarounds for that problem.\n\nWhen moving to a new region, it seems a good idea to store empty cartridges from the old region (or to re-use the region-free Xerox factory ink sticks) and to refill them with ink from the new region. However, they usually also have chips and sensors to prevent refilling (such as the \"time chips\" mentioned above).\nSome manufacturers of region-coded printers also offer region-free printers specially designed for travelers, but, generally, \"the best solution seems to be to avoid region-coded printers.\"\n\nIntegration of the print-head in the cartridge\n\nSome manufacturers install the print-head not in the printer, but integrate it in the cartridge. This makes it more difficult or even illegal (because of patent laws) for other manufacturers to rebuild cartridges, and just refilling the cartridge (including cleaning the print-head and resetting or disabling chips/sensors, if necessary) is sometimes not sufficient, because one cannot print anymore once the print-head no longer works.\n\nThe main benefit of using a refill kit is the claimed cost savings. Environmental benefit is also claimed, as the process reuses a cartridge that would have otherwise been thrown away after one use.\n\nThe downside to refill is the time associated with it and the unpredictability. Refilling a cartridge can take 10–15 minutes for those unfamiliar with the process, and some may prefer buying a new cartridge to the effort it takes to refill. Also, ink cartridges usually last for 4-5 refills, but there are those that can only be refilled one time before they are worn out.\n\nThe biggest perceived downside to refilling is the mess associated with it. Many consumers shy away from refilling either based on past experiences or stories they have heard. Many of the unsuccessful refill kits of the past were so-called \"universal\" kits, meaning they were designed for use with multiple cartridges. Because all manufacturers use different types of ink, and because different cartridge designs require different refilling processes, these universal kits had a high failure rate. Today these kits are harder to find, as refill kits made for specific cartridges have become more the norm, but perception that all refill kits are messy still remains.\n\nThe main reason for the decline in refill kits is the emergence of large chains of ink stores that offer a refill process. This is similar to the evolution of the automobile oil change. Just as cars became too complex for the average driver to change oil, the new cartridges have also become too complex for the average consumer to do it by themselves.\n\nInkjet refill kits are available in different sizes and with different grades of ink.\n\n"}
{"id": "46181426", "url": "https://en.wikipedia.org/wiki?curid=46181426", "title": "Institute for Medical Engineering and Science", "text": "Institute for Medical Engineering and Science\n\nThe Institute for Medical Engineering and Science or IMES is a research institute at the Massachusetts Institute of Technology that aims to combine engineering, medicine, and science to solve challenges in human health. The institute was established in 2012 and is currently directed by Elazer Edelman.\n\nAt MIT, IMES serves to bring together scientific advances with clinical medicine by serving as the point of intersection with major hospitals and industry partners. IMES also serves as the MIT home for the Harvard-MIT Division of Health Sciences and Technology, an educational program administered jointly through Harvard and MIT.\n"}
{"id": "18134326", "url": "https://en.wikipedia.org/wiki?curid=18134326", "title": "Integrated business planning", "text": "Integrated business planning\n\nIntegrated Business Planning is a planning process that integrates across two or more functions in a business or government entity referred to as an enterprise to maximize\nfinancial value.\n\nThe specific functional areas in a company as well as the industry domain associated with the company defines the specific type of IBP process. \nExamples of IBP processes are:\n\n\nThe key requirement for IBP is that two or more functional process areas must be involved and maximizing (optimizing) of financial value should be done.\n\nCorporate executives, business unit heads and planning managers use IBP to evaluate plans and activities based on the economic impact of each consideration.\n\nBridging Corporate Performance Management and S&OP\nThere has been a lot of focus on Integrated Business Planning in the context of Sales and Operations Planning. Gartner (www.gartner.com) refers to a 5-stage\nS&OP Maturity model wherein IBP is referred to as the Phased 4 & 5. Integrated Business Planning however is more broader than S&OP. It is an approach that combines Enterprise Performance Management (EPM) and sales and operations planning (S&OP) to provide incremental capabilities that neither provides individually. In so doing, IBP platforms address long-standing challenges that financial and operational professionals have struggled to overcome. The result: opportunities for step change improvements to how manufacturers plan, manage and govern their business. Here, the focus is on strengthening the financial integration and reconciliation of plans, as well as increasing the responsiveness of the supply chain using ad-hoc reports and what-if scenario analyses.\n\nIntegrated Business Planning requires the following capabilities to be enabled:\n\na) Enterprise Model\nb) Integrated Planning\n\nc) Enterprise Optimization\n\nIBP has been used to model and integrate the planning efforts in a number of applications, including:\nAll of the above can be summarized as Enterprise Optimization use cases.\n\nSome argue that IBP is not any different from S&OP. See article by Patrick Bower where he calls IBP a marketing hoax, a name developed to create confusion and sell consulting and system services. The main proponents of IBP are in fact consulting companies. In return to this criticism,\nit has been pointed out that IBP is not a marketing hoax, but an important\npart of Enterprise Performance Management (EPM) system.\n\nAnother criticism is that IBP is not academically defined and is supply chain biased in its definition. The lack of academic standard leaves room for interpretation to what IBP is, which is confusing practitioners. In a 2015 S&OP survey, 32% of participants answered that there is no difference between S&OP and IBP, 20% \"did not know\", and 71% of participants answered that there is a need for more industry standards around S&OP.\n\nIt has been called out that IBP has a lack of governance and in need of an industry group to create a unified definition. Due this lack of academic and industry standards, there has been an attempt to create an open source definition for IBP. This definition is as follows:\n\nIntegrated Business Planning (IBP): A holistic planning philosophy, where all organizational functions participate in providing executives periodically with valid, reliable information, in order to\ndecide how to align the enterprise around executing the plans to achieve budget, strategic intent and the envisioned future.\n\n"}
{"id": "57094192", "url": "https://en.wikipedia.org/wiki?curid=57094192", "title": "Kastus", "text": "Kastus\n\nKastus’ technologies prevent the growth of bacteria on the surface it has been applied to, with no toxic additives or negative side effects. The coating is applied to a range of materials, such as ceramics, glass, and metals in the final stage of production.\n\nJohn Browne, Kastus CEO, founded the company in 2013 in Dublin following 10 years of collaborative research with Dublin Institute of Technology. \n\nKastus was developed in response to government, industry & healthcare requests to help reduce the spread of antibiotic-resistant infections, specifically those caused by bacteria and fungus surviving on surfaces and indoor environment. In October 2017, The Department of Health published “Ireland’s National Action Plan on Antimicrobial Resistance 2017-2020”, which highlighted the threat antimicrobial resistance poses and the urgent need for new technology to combat this.\n\n·2016 April: makes a significant breakthrough in antimicrobial technology. \nThis research is supported by a €1.5 million funding investment from Atlantic Bridge. \n\n·2017 January: attends Arab Health in Dubai — the largest healthcare event in the MEMA region. Here Kastus publicly exhibits its coating technology for the first time.\n\n·2017 March: Kastus is awarded the Spin-out Company Impact Award. The Knowledge Transfer Impact Award is awarded for substantial commercial achievements by knowledge transfer professionals who work in higher education and publicly-funded research organizations. Kastus is recognized for having achieved a “successful and significant event” in the previous calendar year. \n\n·2017 October: recipient of The Irish Times Innovation Award Overall Prize 2017. Kastus also picks up the Life Science and Healthcare category award at the Irish Times Awards ceremony.\n\n·2017 October: launches Kastus Metals at Med In Ireland – Ireland’s largest medical technologies event.\n\n·2005: begins research and development into a powder plasma solution\n\n·2007: begins compound testing and development\n\n·2009: begins research and development into plasma deposition\n\n·2010: develops an antimicrobial powder\n\n·2011: begins developing antimicrobial ceramic spray\n\n·2012: begins developing antimicrobial glass spray\n\n·2014: testing for antimicrobial, fungus and life cycle\n\n·2016: final validation on formula\n\n·2017: receives independent certification to ISO, BS and ASTM standards\n·2017: begins developing antimicrobial metals spray\n\n·2018: scales-up production of ceramic and glass solutions\n\n·2018: receives on-site client validation\n\n·2018: living lab with Tallaght hospital using Kastus solutions\n\n·2018: develops new IP and tech scouting\n"}
{"id": "148226", "url": "https://en.wikipedia.org/wiki?curid=148226", "title": "Leisure industry", "text": "Leisure industry\n\nThe leisure industry is the segment of business focused on recreation, entertainment, sports, and tourism (REST)-related products and services (including all playable simulacra).\n\nThe field has developed to the point of having university degrees and disciplines focused on it, such as the Cornell University School of Hotel Administration, Webber, and San Jose State University's departments of hospitality, recreation and tourism management. Some universities offer leisure degrees, two of those universities can be found in the Netherlands: the Breda University of Applied Sciences and the Stenden University of Applied Sciences. Both offer bachelor's in international leisure management.\n\n\n"}
{"id": "54562473", "url": "https://en.wikipedia.org/wiki?curid=54562473", "title": "List of Industrial Training Institute (ITI) in West Bengal", "text": "List of Industrial Training Institute (ITI) in West Bengal\n\nIndustrial Training Institutes (ITI) are post-secondary schools in India constituted under Directorate General of Employment & Training (DGET), Ministry of Skill Development and Entrepreneurship, Union Government to provide training in various trades. At present, there are many Industrial Training Institute in West Bengal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "48618549", "url": "https://en.wikipedia.org/wiki?curid=48618549", "title": "List of Portuguese inventions and discoveries", "text": "List of Portuguese inventions and discoveries\n\n\n\n\n\n"}
{"id": "31151358", "url": "https://en.wikipedia.org/wiki?curid=31151358", "title": "National Tsunami Warning Center", "text": "National Tsunami Warning Center\n\nThe National Tsunami Warning Center (NTWC) is one of two tsunami warning centers that are operated by the National Oceanic and Atmospheric Administration (NOAA) in the United States. It was called the West Coast and Alaska Tsunami Warning Center (WC/ATWC) until October 1, 2013. The name was changed to reflect its geographical zone of responsibility. Headquartered in Palmer, Alaska, the NTWC is part of an international tsunami warning system (TWS) program and serves as the operational center for TWS of all coastal regions of Canada and the United States, except Hawaii, the Caribbean, and the Gulf of Mexico. The other tsunami warning center is the Pacific Tsunami Warning Center (PTWC) in Ford Island, Hawaii, serving participating members and other nations in the Pacific Ocean area of responsibility.\n\nFollowing the March 27, 1964 Alaska earthquake and tsunami, the NTWC (formerly known as The Palmer Observatory) was established in 1967 in Palmer, Alaska, under the auspices of the Coast and Geodetic Survey. This earthquake alerted State and Federal officials that a facility was necessary to provide timely and effective tsunami warnings and earthquake information to the coastal areas of Alaska. Congress provided funds in 1965 to construct two new observatories and establish a tsunami warning system in Alaska. The first observatory constructed was at the U.S. Naval Station on Adak Island in the Andreanof Islands in the Central Aleutians. The City of Palmer, in the Matanuska Valley 42 miles northeast of Anchorage, was selected as the site for the primary observatory due to its proximity to bedrock for instrumentation and to communications facilities. Construction of the observatory installations, the task of engineering and assembling the data systems, and the hookup of the extensive telecommunications and data telemetry network was completed in the summer of 1967. With the dedication of the Palmer Observatory on September 2, 1967, the Alaska Regional Tsunami Warning System (ARTWS) became operational.\n\nOriginally, the tsunami warning responsibility for Alaska was shared by the three observatories located at Palmer, Adak and Sitka. Sitka, a seismological observatory since 1904, and Fairbanks were the only two seismic stations operating in Alaska in 1964. The responsibilities of Adak and Sitka were limited to issuing a tsunami warning for events occurring within 300 miles of their location. In later years, the responsibility to provide tsunami warning services for Alaska was transferred from the Adak and Sitka observatories to the Palmer Observatory. Sitka and Adak Observatories were eventually closed in the early 1990s, although the seismic instrumentation is still maintained.\n\nIn 1973, the Palmer Observatory was transferred to the National Weather Service's Alaska Region and changed its name to Alaska Tsunami Warning Center (ATWC). In 1982, its area of responsibility (AOR) was enlarged to include the issuing of tsunami warnings to California, Oregon, Washington, and British Columbia for potential tsunamigenic earthquakes occurring in their coastal areas. In 1996, the responsibility was again expanded to include all Pacific-wide tsunamigenic sources that could affect the California, Oregon, Washington, British Columbia and Alaska coasts, and the name was changed to the West Coast/Alaska Tsunami Warning Center (WC/ATWC) to reflect those new responsibilities. \n\nIn 2003, a new Tsunami Warning Center building was constructed in the yard of the original building. This new facility was the first LEED certified building in the state of Alaska, and within the U.S. Department of Commerce. LEED (Leadership in Energy and Environmental Design) certification is granted by the U.S. Green Building Council, and awards environmentally sensitive construction practices. This new facility provides upgraded power and communications capability, as well as office space for the expanded staff, assuring that the center will continue to provide quality products to the public well into the future.\n\nFollowing the devastating Indian Ocean tsunami in late 2004, the WC/ATWC expanded its scope to the U.S. Atlantic coast, and the Atlantic coast of Canada. On 1 October 2013, the name was changed to the National Tsunami Warning Center (NTWC) to reflect this expanded geographical zone of responsibility.\n\nTo accomplish its mission of providing accurate and timely tsunami bulletins to its area-of-responsibility (AOR) – which includes Canadian coastal regions, Puerto Rico and the Virgin Islands, and the ocean coasts of all U.S. States except Hawaii – the NTWC detects, locates, sizes, and analyzes earthquakes worldwide. Earthquakes that activate the Center's alarm system initiate an earthquake and tsunami investigation which includes the following four basic steps: automatic locating and sizing the earthquake; earthquake analysis and review; sea level data analysis to verify the existence of a tsunami and to calibrate models; and disseminating information to the appropriate emergency management officials.\n\nTsunami bulletins are issued to state/province departments of emergency services; federal disaster preparedness agencies; National Weather Service offices; Canada's Atlantic Storm Prediction Center; Federal Aviation Administration offices; the U.S. Coast Guard; military bases; local emergency managers; United States Geological Survey offices; and many other recipients located in the U.S. and Canada. Earthquakes large enough to be felt near the coast, but below the tsunami warning/watch/advisory threshold size, prompt informational statements to the same recipients as warnings to help prevent needless evacuations.\n\nIn addition to its basic functions, the Center conducts a community preparedness program intended to increase public awareness of the tsunami hazard and improve tsunami planning at the community level. The Center also actively pursues developmental projects which enhance tsunami warning operations.\n\nThe Center operates 24 hours every day with two watchstanders on duty. Center personnel are notified of activity by an alarm system which is activated by several methods:\n\nDepending on the seismic data (Message Definitions), NTWC will issue the following types of bulletins:\n\nA tsunami warning is issued when a tsunami with the potential to generate widespread inundation is imminent, expected, or occurring. Warnings alert the public that dangerous coastal flooding accompanied by powerful currents is possible and may continue for several hours after initial arrival. Warnings alert emergency management officials to take action for the entire tsunami hazard zone. Appropriate actions to be taken by local officials may include the evacuation of low-lying coastal areas, and the repositioning of ships to deep waters when there is time to safely do so. Warnings may be updated, adjusted geographically, downgraded, or canceled. To provide the earliest possible alert, initial warnings are normally based only on seismic information.\n\nA tsunami advisory is issued when a tsunami with the potential to generate strong currents or waves dangerous to those in or very near the water is imminent, expected, or occurring. The threat may continue for several hours after initial arrival, but significant inundation is not expected for areas under an advisory. Appropriate actions to be taken by local officials may include closing beaches, evacuating harbors and marinas, and the repositioning of ships to deep waters when there is time to safely do so. Advisories are normally updated to continue the advisory, expand/contract affected areas, upgrade to a warning, or cancel the advisory.\n\nA tsunami watch is issued to alert emergency management officials and the public of an event which may later impact the watch area. The watch area may be upgraded to a warning or advisory – or canceled – based on updated information and analysis. Therefore, emergency management officials and the public should prepare to take action. Watches are normally issued based on seismic information without confirmation that a destructive tsunami is underway.\n\nA tsunami information statement is issued to inform emergency management officials and the public that an earthquake has occurred, or that a tsunami warning, watch or advisory has been issued for another section of the ocean. In most cases, information statements are issued to indicate there is no threat of a destructive tsunami and to prevent unnecessary evacuations as the earthquake may have been felt in coastal areas. An information statement may, in appropriate situations, caution about the possibility of destructive local tsunamis. Information statements may be re-issued with additional information, though normally these messages are not updated. However, a watch, advisory or warning may be issued for the area, if necessary, after analysis and/or updated information becomes available.\n\nThis definition applies to Mass News Dissemination Product Types, Tsunami Information Statements, and Tsunami Seismic Information Statements.\n\nA final product indicating the end of the damaging tsunami threat. A cancellation is usually issued after an evaluation of sea level data confirms that a destructive tsunami will not impact the warning, advisory, or watch area.\n\nIn 1995, NOAA began developing the Deep-ocean Assessment and Reporting of Tsunamis (DART) system. By 2001, an array of six stations had been deployed in the Pacific Ocean.\n\nBeginning in 2005, as a result of the tsunami caused by the 2004 Indian Ocean earthquake, plans were announced to add 32 more DART buoys to be operational by mid-2007.\n\nThese stations give detailed information about tsunamis while they are still far off shore. Each station consists of a sea-bed bottom pressure recorder (at a depth of 1000–6000 m) which detects the passage of a tsunami and transmits the data to a surface buoy via acoustic modem. The surface buoy then radios the information to the NTWC via the GOES satellite system. The bottom pressure recorder lasts for two years while the surface buoy is replaced every year. The system has considerably improved the forecasting and warning of tsunamis in the Pacific Ocean.\n\n"}
{"id": "2860430", "url": "https://en.wikipedia.org/wiki?curid=2860430", "title": "Neural oscillation", "text": "Neural oscillation\n\nNeural oscillations, or brainwaves, are rhythmic or repetitive patterns of neural activity in the central nervous system. Neural tissue can generate oscillatory activity in many ways, driven either by mechanisms within individual neurons or by interactions between neurons. In individual neurons, oscillations can appear either as oscillations in membrane potential or as rhythmic patterns of action potentials, which then produce oscillatory activation of post-synaptic neurons. At the level of neural ensembles, synchronized activity of large numbers of neurons can give rise to macroscopic oscillations, which can be observed in an electroencephalogram. Oscillatory activity in groups of neurons generally arises from feedback connections between the neurons that result in the synchronization of their firing patterns. The interaction between neurons can give rise to oscillations at a different frequency than the firing frequency of individual neurons. A well-known example of macroscopic neural oscillations is alpha activity.\n\nNeural oscillations were observed by researchers as early as 1924 (by Hans Berger). More than 50 years later, intrinsic oscillatory behavior was encountered in vertebrate neurons, but its functional role is still not fully understood. The possible roles of neural oscillations include feature binding, information transfer mechanisms and the generation of rhythmic motor output. Over the last decades more insight has been gained, especially with advances in brain imaging. A major area of research in neuroscience involves determining how oscillations are generated and what their roles are. Oscillatory activity in the brain is widely observed at different levels of organization and is thought to play a key role in processing neural information. Numerous experimental studies support a functional role of neural oscillations; a unified interpretation, however, is still lacking.\n\nAdolf Beck published in 1890 his observations of spontaneous electrical activity of the brain of rabbits and dogs that included rhythmic oscillations altered by light detected with electrodes directly placed on the surface of brain. Before Hans Berger, Vladimir Vladimirovich Pravdich-Neminsky published the first animal EEG and the evoked potential of a dog. \n\nNeural oscillations are observed throughout the central nervous system at all levels, and include spike trains, local field potentials and large-scale oscillations which can be measured by electroencephalography (EEG). In general, oscillations can be characterized by their frequency, amplitude and phase. These signal properties can be extracted from neural recordings using time-frequency analysis. In large-scale oscillations, amplitude changes are considered to result from changes in synchronization within a neural ensemble, also referred to as local synchronization. In addition to local synchronization, oscillatory activity of distant neural structures (single neurons or neural ensembles) can synchronize. Neural oscillations and synchronization have been linked to many cognitive functions such as information transfer, perception, motor control and memory.\n\nNeural oscillations have been most widely studied in neural activity generated by large groups of neurons. Large-scale activity can be measured by techniques such as EEG. In general, EEG signals have a broad spectral content similar to pink noise, but also reveal oscillatory activity in specific frequency bands. The first discovered and best-known frequency band is alpha activity (7.5–12.5 Hz) that can be detected from the occipital lobe during relaxed wakefulness and which increases when the eyes are closed. Other frequency bands are: delta (1–4 Hz), theta (4–8 Hz), beta (13–30 Hz), low gamma (30–70 Hz), and high gamma (70–150 Hz) frequency bands, where faster rhythms such as gamma activity have been linked to cognitive processing. Indeed, EEG signals change dramatically during sleep and show a transition from faster frequencies to increasingly slower frequencies such as alpha waves. In fact, different sleep stages are commonly characterized by their spectral content. Consequently, neural oscillations have been linked to cognitive states, such as awareness and consciousness.\n\nAlthough neural oscillations in human brain activity are mostly investigated using EEG recordings, they are also observed using more invasive recording techniques such as single-unit recordings. Neurons can generate rhythmic patterns of action potentials or spikes. Some types of neurons have the tendency to fire at particular frequencies, so-called \"resonators\". Bursting is another form of rhythmic spiking. Spiking patterns are considered fundamental for information coding in the brain. Oscillatory activity can also be observed in the form of subthreshold membrane potential oscillations (i.e. in the absence of action potentials). If numerous neurons spike in synchrony, they can give rise to oscillations in local field potentials. Quantitative models can estimate the strength of neural oscillations in recorded data.\n\nNeural oscillations are commonly studied from a mathematical framework and belong to the field of \"neurodynamics\", an area of research in the cognitive sciences that places a strong focus upon the dynamic character of neural activity in describing brain function. It considers the brain a dynamical system and uses differential equations to describe how neural activity evolves over time. In particular, it aims to relate dynamic patterns of brain activity to cognitive functions such as perception and memory. In very abstract form, neural oscillations can be analyzed analytically. When studied in a more physiologically realistic setting, oscillatory activity is generally studied using computer simulations of a computational model.\n\nThe functions of neural oscillations are wide-ranging and vary for different types of oscillatory activity. Examples are the generation of rhythmic activity such as a heartbeat and the neural binding of sensory features in perception, such as the shape and color of an object. Neural oscillations also play an important role in many neurological disorders, such as excessive synchronization during seizure activity in epilepsy or tremor in patients with Parkinson's disease. Oscillatory activity can also be used to control external devices in brain–computer interfaces, in which subjects can control an external device by changing the amplitude of particular brain rhythmics .\n\nOscillatory activity is observed throughout the central nervous system at all levels of organization. Three different levels have been widely recognized: the micro-scale (activity of a single neuron), the meso-scale (activity of a local group of neurons) and the macro-scale (activity of different brain regions).\n\nNeurons generate action potentials resulting from changes in the electric membrane potential. Neurons can generate multiple action potentials in sequence forming so-called spike trains. These spike trains are the basis for neural coding and information transfer in the brain. Spike trains can form all kinds of patterns, such as rhythmic spiking and bursting, and often display oscillatory activity. Oscillatory activity in single neurons can also be observed in sub-threshold fluctuations in membrane potential. These rhythmic changes in membrane potential do not reach the critical threshold and therefore do not result in an action potential. They can result from postsynaptic potentials from synchronous inputs or from intrinsic properties of neurons.\n\nNeuronal spiking can be classified by their activity patterns. The excitability of neurons can be subdivided in Class I and II. Class I neurons can generate action potentials with arbitrarily low frequency depending on the input strength, whereas Class II neurons generate action potentials in a certain frequency band, which is relatively insensitive to changes in input strength. Class II neurons are also more prone to display sub-threshold oscillations in membrane potential.\n\nA group of neurons can also generate oscillatory activity. Through synaptic interactions the firing patterns of different neurons may become synchronized and the rhythmic changes in electric potential caused by their action potentials will add up (constructive interference). That is, synchronized firing patterns result in synchronized input into other cortical areas, which gives rise to large-amplitude oscillations of the local field potential. These large-scale oscillations can also be measured outside the scalp using electroencephalography (EEG) and magnetoencephalography (MEG). The electric potentials generated by single neurons are far too small to be picked up outside the scalp, and EEG or MEG activity always reflects the summation of the synchronous activity of thousands or millions of neurons that have similar spatial orientation. Neurons in a neural ensemble rarely all fire at exactly the same moment, i.e. fully synchronized. Instead, the probability of firing is rhythmically modulated such that neurons are more likely to fire at the same time, which gives rise to oscillations in their mean activity (see figure at top of page). As such, the frequency of large-scale oscillations does not need to match the firing pattern of individual neurons. Isolated cortical neurons fire regularly under certain conditions, but in the intact brain cortical cells are bombarded by highly fluctuating synaptic inputs and typically fire seemingly at random. However, if the probability of a large group of neurons is rhythmically modulated at a common frequency, they will generate oscillations in the mean field (see also figure at top of page). Neural ensembles can generate oscillatory activity endogenously through local interactions between excitatory and inhibitory neurons. In particular, inhibitory interneurons play an important role in producing neural ensemble synchrony by generating a narrow window for effective excitation and rhythmically modulating the firing rate of excitatory neurons.\n\nNeural oscillation can also arise from interactions between different brain areas coupled through the structural connectome. Time delays play an important role here. Because all brain areas are bidirectionally coupled, these connections between brain areas form feedback loops. Positive feedback loops tends to cause oscillatory activity where frequency is inversely related to the delay time. An example of such a feedback loop is the connections between the thalamus and cortex – the thalamocortical radiations. This thalamocortical network is able to generate oscillatory activity known as recurrent thalamo-cortical resonance. The thalamocortical network plays an important role in the generation of alpha activity. In a whole-brain network model with realistic anatomical connectivity and propagation delays between brain areas, oscillations in the beta frequency range emerge from the partial synchronisation of subsets of brain areas oscillating in the gamma-band (generated at the mesoscopic level).\n\nScientists have identified some intrinsic neuronal properties that play an important role in generating membrane potential oscillations. In particular, voltage-gated ion channels are critical in the generation of action potentials. The dynamics of these ion channels have been captured in the well-established Hodgkin–Huxley model that describes how action potentials are initiated and propagated by means of a set of differential equations. Using bifurcation analysis, different oscillatory varieties of these neuronal models can be determined, allowing for the classification of types of neuronal responses. The oscillatory dynamics of neuronal spiking as identified in the Hodgkin–Huxley model closely agree with empirical findings. In addition to periodic spiking, subthreshold membrane potential oscillations, i.e. resonance behavior that does not result in action potentials, may also contribute to oscillatory activity by facilitating synchronous activity of neighboring neurons. Like pacemaker neurons in central pattern generators, subtypes of cortical cells fire bursts of spikes (brief clusters of spikes) rhythmically at preferred frequencies. Bursting neurons have the potential to serve as pacemakers for synchronous network oscillations, and bursts of spikes may underlie or enhance neuronal resonance.\n\nApart from intrinsic properties of neurons, biological neural network properties are also an important source of oscillatory activity. Neurons communicate with one another via synapses and affect the timing of spike trains in the post-synaptic neurons. Depending on the properties of the connection, such as the coupling strength, time delay and whether coupling is excitatory or inhibitory, the spike trains of the interacting neurons may become synchronized. Neurons are locally connected, forming small clusters that are called neural ensembles. Certain network structures promote oscillatory activity at specific frequencies. For example, neuronal activity generated by two populations of interconnected \"inhibitory\" and \"excitatory\" cells can show spontaneous oscillations that are described by the Wilson-Cowan model.\n\nIf a group of neurons engages in synchronized oscillatory activity, the neural ensemble can be mathematically represented as a single oscillator. Different neural ensembles are coupled through long-range connections and form a network of weakly coupled oscillators at the next spatial scale. Weakly coupled oscillators can generate a range of dynamics including oscillatory activity. Long-range connections between different brain structures, such as the thalamus and the cortex (see thalamocortical oscillation), involve time-delays due to the finite conduction velocity of axons. Because most connections are reciprocal, they form feed-back loops that support oscillatory activity. Oscillations recorded from multiple cortical areas can become synchronized to form large scale brain networks, whose dynamics and functional connectivity can be studied by means of spectral analysis and Granger causality measures. Coherent activity of large-scale brain activity may form dynamic links between brain areas required for the integration of distributed information.\n\nIn addition to fast direct synaptic interactions between neurons forming a network, oscillatory activity is regulated by neuromodulators on a much slower time scale. That is, the concentration levels of certain neurotransmitters are known to regulate the amount of oscillatory activity. For instance, GABA concentration has been shown to be positively correlated with frequency of oscillations in induced stimuli. A number of nuclei in the brainstem have diffuse projections throughout the brain influencing concentration levels of neurotransmitters such as norepinephrine, acetylcholine and serotonin. These neurotransmitter systems affect the physiological state, e.g., wakefulness or arousal, and have a pronounced effect on amplitude of different brain waves, such as alpha activity.\n\nOscillations can often be described and analyzed using mathematics. Mathematicians have identified several dynamical mechanisms that generate rhythmicity. Among the most important are harmonic (linear) oscillators, limit cycle oscillators, and delayed-feedback oscillators. Harmonic oscillations appear very frequently in nature—examples are sound waves, the motion of a pendulum, and vibrations of every sort. They generally arise when a physical system is perturbed by a small degree from a minimum-energy state, and are well understood mathematically. Noise-driven harmonic oscillators realistically simulate alpha rhythm in the waking EEG as well as slow waves and spindles in the sleep EEG. Successful EEG analysis algorithms were based on such models. Several other EEG components are better described by limit-cycle or delayed-feedback oscillations. Limit-cycle oscillations arise from physical systems that show large deviations from equilibrium, whereas delayed-feedback oscillations arise when components of a system affect each other after significant time delays. Limit-cycle oscillations can be complex but there are powerful mathematical tools for analyzing them; the mathematics of delayed-feedback oscillations is primitive in comparison. Linear oscillators and limit-cycle oscillators qualitatively differ in terms of how they respond to fluctuations in input. In a linear oscillator, the frequency is more or less constant but the amplitude can vary greatly. In a limit-cycle oscillator, the amplitude tends to be more or less constant but the frequency can vary greatly. A heartbeat is an example of a limit-cycle oscillation in that the frequency of beats varies widely, while each individual beat continues to pump about the same amount of blood.\n\nComputational models adopt a variety of abstractions in order to describe complex oscillatory dynamics observed in brain activity. Many models are used in the field, each defined at a different level of abstraction and trying to model different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of how the dynamics of neural circuitry arise from interactions between individual neurons, to models of how behaviour can arise from abstract neural modules that represent complete subsystems.\n\nA model of a biological neuron is a mathematical description of the properties of nerve cells, or neurons, that is designed to accurately describe and predict its biological processes. The most successful and widely used model of neurons, the Hodgkin–Huxley model, is based on data from the squid giant axon. It is a set of nonlinear ordinary differential equations that approximates the electrical characteristics of a neuron, in particular the generation and propagation of action potentials. The model is very accurate and detailed and Hodgkin and Huxley received the 1963 Nobel Prize in physiology or medicine for this work.\n\nThe mathematics of the Hodgkin–Huxley model are quite complicated and several simplifications have been proposed, such as the FitzHugh–Nagumo model and the Hindmarsh–Rose model. Such models only capture the basic neuronal dynamics, such as rhythmic spiking and bursting, but are more computationally efficient. This allows the simulation of a large number of interconnected neurons that form a neural network.\n\nA neural network model describes a population of physically interconnected neurons or a group of disparate neurons whose inputs or signalling targets define a recognizable circuit. These models aim to describe how the dynamics of neural circuitry arise from interactions between individual neurons. Local interactions between neurons can result in the synchronization of spiking activity and form the basis of oscillatory activity. In particular, models of interacting pyramidal cells and inhibitory interneurons have been shown to generate brain rhythms such as gamma activity.\n\nNeural field models are another important tool in studying neural oscillations and are a mathematical framework describing evolution of variables such as mean firing rate in space and time. In modeling the activity of large numbers of neurons, the central idea is to take the density of neurons to the continuum limit, resulting in spatially continuous neural networks. Instead of modelling individual neurons, this approach approximates a group of neurons by its average properties and interactions. It is based on the mean field approach, an area of statistical physics that deals with large-scale systems. Models based on these principles have been used to provide mathematical descriptions of neural oscillations and EEG rhythms. They have for instance been used to investigate visual hallucinations.\n\nThe Kuramoto model of coupled phase oscillators is one of the most abstract and fundamental models used to investigate neural oscillations and synchronization. It captures the activity of a local system (e.g., a single neuron or neural ensemble) by its circular phase alone and hence ignores the amplitude of oscillations (amplitude is constant). Interactions amongst these oscillators are introduced by a simple algebraic form (such as a sine function) and collectively generate a dynamical pattern at the global scale. The Kuramoto model is widely used to study oscillatory brain activity and several extensions have been proposed that increase its neurobiological plausibility, for instance by incorporating topological properties of local cortical connectivity. In particular, it describes how the activity of a group of interacting neurons can become synchronized and generate large-scale oscillations. Simulations using the Kuramoto model with realistic long-range cortical connectivity and time-delayed interactions reveal the emergence of slow patterned fluctuations that reproduce resting-state BOLD functional maps, which can be measured using fMRI.\n\nBoth single neurons and groups of neurons can generate oscillatory activity spontaneously. In addition, they may show oscillatory responses to perceptual input or motor output. Some types of neurons will fire rhythmically in the absence of any synaptic input. Likewise, brain-wide activity reveals oscillatory activity while subjects do not engage in any activity, so-called resting-state activity. These ongoing rhythms can change in different ways in response to perceptual input or motor output. Oscillatory activity may respond by increases or decreases in frequency and amplitude or show a temporary interruption, which is referred to as phase resetting. In addition, external activity may not interact with ongoing activity at all, resulting in an additive response.\n\nSpontaneous activity is brain activity in the absence of an explicit task, such as sensory input or motor output, and hence also referred to as resting-state activity. It is opposed to induced activity, i.e. brain activity that is induced by sensory stimuli or motor responses. The term \"ongoing brain activity\" is used in electroencephalography and magnetoencephalography for those signal components that are not associated with the processing of a stimulus or the occurrence of specific other events, such as moving a body part, i.e. events that do not form evoked potentials/evoked fields, or induced activity. Spontaneous activity is usually considered to be noise if one is interested in stimulus processing; however, spontaneous activity is considered to play a crucial role during brain development, such as in network formation and synaptogenesis. Spontaneous activity may be informative regarding the current mental state of the person (e.g. wakefulness, alertness) and is often used in sleep research. Certain types of oscillatory activity, such as alpha waves, are part of spontaneous activity. Statistical analysis of power fluctuations of alpha activity reveals a bimodal distribution, i.e. a high- and low-amplitude mode, and hence shows that resting-state activity does not just reflect a noise process. In case of fMRI, spontaneous fluctuations in the blood-oxygen-level dependent (BOLD) signal reveal correlation patterns that are linked to resting states networks, such as the default network. The temporal evolution of resting state networks is correlated with fluctuations of oscillatory EEG activity in different frequency bands.\n\nOngoing brain activity may also have an important role in perception, as it may interact with activity related to incoming stimuli. Indeed, EEG studies suggest that visual perception is dependent on both the phase and amplitude of cortical oscillations. For instance, the amplitude and phase of alpha activity at the moment of visual stimulation predicts whether a weak stimulus will be perceived by the subject.\n\nIn response to input, a neuron or neuronal ensemble may change the frequency at which it oscillates, thus changing the rate at which it spikes. Often, a neuron's firing rate depends on the summed activity it receives. Frequency changes are also commonly observed in central pattern generators and directly relate to the speed of motor activities, such as step frequency in walking. However, changes in \"relative\" oscillation frequency between different brain areas is not so common because the frequency of oscillatory activity is often related to the time delays between brain areas.\n\nPhase resetting occurs when input to a neuron or neuronal ensemble resets the phase of ongoing oscillations. It is very common in single neurons where spike timing is adjusted to neuronal input (a neuron may spike at a fixed delay in response to periodic input, which is referred to as phase locking) and may also occur in neuronal ensembles when the phases of their neurons are adjusted simultaneously. Phase resetting is fundamental for the synchronization of different neurons or different brain regions because the timing of spikes can become phase locked to the activity of other neurons.\n\nPhase resetting also permits the study of evoked activity, a term used in electroencephalography and magnetoencephalography for responses in brain activity that are directly related to stimulus-related activity. Evoked potentials and event-related potentials are obtained from an electroencephalogram by stimulus-locked averaging, i.e. averaging different trials at fixed latencies around the presentation of a stimulus. As a consequence, those signal components that are the same in each single measurement are conserved and all others, i.e. ongoing or spontaneous activity, are averaged out. That is, event-related potentials only reflect oscillations in brain activity that are phase-locked to the stimulus or event. Evoked activity is often considered to be independent from ongoing brain activity, although this is an ongoing debate.\n\nNext to evoked activity, neural activity related to stimulus processing may result in induced activity. Induced activity refers to modulation in ongoing brain activity induced by processing of stimuli or movement preparation. Hence, they reflect an indirect response in contrast to evoked responses. A well-studied type of induced activity is amplitude change in oscillatory activity. For instance, gamma activity often increases during increased mental activity such as during object representation. Because induced responses may have different phases across measurements and therefore would cancel out during averaging, they can only be obtained using time-frequency analysis. Induced activity generally reflects the activity of numerous neurons: amplitude changes in oscillatory activity are thought to arise from the synchronization of neural activity, for instance by synchronization of spike timing or membrane potential fluctuations of individual neurons. Increases in oscillatory activity are therefore often referred to as event-related synchronization, while decreases are referred to as event-related desynchronization.\n\nIt has recently been proposed that even if phases are not aligned across trials, induced activity may still cause event-related potentials because ongoing brain oscillations may not be symmetric and thus amplitude modulations may result in a baseline shift that does not average out. This model implies that slow event-related responses, such as asymmetric alpha activity, could result from asymmetric brain oscillation amplitude modulations, such as an asymmetry of the intracellular currents that propagate forward and backward down the dendrites. Under this assumption, asymmetries in the dendritic current would cause asymmetries in oscillatory activity measured by EEG and MEG, since dendritic currents in pyramidal cells are generally thought to generate EEG and MEG signals that can be measured at the scalp.\n\nNeural synchronization can be modulated by task constraints, such as attention, and is thought to play a role in feature binding, neuronal communication, and motor coordination. Neuronal oscillations became a hot topic in neuroscience in the 1990s when the studies of the visual system of the brain by Gray, Singer and others appeared to support the neural binding hypothesis. According to this idea, synchronous oscillations in neuronal ensembles bind neurons representing different features of an object. For example, when a person looks at a tree, visual cortex neurons representing the tree trunk and those representing the branches of the same tree would oscillate in synchrony to form a single representation of the tree. This phenomenon is best seen in local field potentials which reflect the synchronous activity of local groups of neurons, but has also been shown in EEG and MEG recordings providing increasing evidence for a close relation between synchronous oscillatory activity and a variety of cognitive functions such as perceptual grouping.\n\nCells in the sinoatrial node, located in the right atrium of the heart, spontaneously depolarize approximately 100 times per minute. Although all of the heart's cells have the ability to generate action potentials that trigger cardiac contraction, the sinoatrial node normally initiates it, simply because it generates impulses slightly faster than the other areas. Hence, these cells generate the normal sinus rhythm and are called pacemaker cells as they directly control the heart rate. In the absence of extrinsic neural and hormonal control, cells in the SA node will rhythmically discharge. The sinoatrial node is richly innervated by the autonomic nervous system, which up or down regulates the spontaneous firing frequency of the pacemaker cells.\n\nSynchronized firing of neurons also forms the basis of periodic motor commands for rhythmic movements. These rhythmic outputs are produced by a group of interacting neurons that form a network, called a central pattern generator. Central pattern generators are neuronal circuits that—when activated—can produce rhythmic motor patterns in the absence of sensory or descending inputs that carry specific timing information. Examples are walking, breathing, and swimming, Most evidence for central pattern generators comes from lower animals, such as the lamprey, but there is also evidence for spinal central pattern generators in humans.\n\nNeuronal spiking is generally considered the basis for information transfer in the brain. For such a transfer, information needs to be coded in a spiking pattern. Different types of coding schemes have been proposed, such as rate coding and temporal coding. Neural oscillations could create periodic time windows in which input spikes have larger effect on neurons, thereby providing a mechanism for decoding temporal codes.\n\nSynchronization of neuronal firing may serve as a means to group spatially segregated neurons that respond to the same stimulus in order to bind these responses for further joint processing, i.e. to exploit temporal synchrony to encode relations. Purely theoretical formulations of the binding-by-synchrony hypothesis were proposed first, but subsequently extensive experimental evidence has been reported supporting the potential role of synchrony as a relational code.\n\nThe functional role of synchronized oscillatory activity in the brain was mainly established in experiments performed on awake kittens with multiple electrodes implanted in the visual cortex. These experiments showed that groups of spatially segregated neurons engage in synchronous oscillatory activity when activated by visual stimuli. The frequency of these oscillations was in the range of 40 Hz and differed from the periodic activation induced by the grating, suggesting that the oscillations and their synchronization were due to internal neuronal interactions. Similar findings were shown in parallel by the group of Eckhorn, providing further evidence for the functional role of neural synchronization in feature binding. Since then, numerous studies have replicated these findings and extended them to different modalities such as EEG, providing extensive evidence of the functional role of gamma oscillations in visual perception.\n\nGilles Laurent and colleagues showed that oscillatory synchronization has an important functional role in odor perception. Perceiving different odors leads to different subsets of neurons firing on different sets of oscillatory cycles. These oscillations can be disrupted by GABA blocker picrotoxin, and the disruption of the oscillatory synchronization leads to impairment of behavioral discrimination of chemically similar odorants in bees and to more similar responses across odors in downstream β-lobe neurons. Recent follow-up of this work has shown that oscillations create periodic integration windows for Kenyon cells in the insect mushroom body, such that incoming spikes from the antennal lobe are more effective in activating Kenyon cells only at specific phases of the oscillatory cycle.\n\nNeural oscillations are also thought be involved in the sense of time and in somatosensory perception. However, recent findings argue against a clock-like function of cortical gamma oscillations.\n\nOscillations have been commonly reported in the motor system. Pfurtscheller and colleagues found a reduction in alpha (8–12 Hz) and beta (13–30 Hz) oscillations in EEG activity when subjects made a movement. Using intra-cortical recordings, similar changes in oscillatory activity were found in the motor cortex when the monkeys performed motor acts that required significant attention. In addition, oscillations at spinal level become synchronised to beta oscillations in the motor cortex during constant muscle activation, as determined by cortico-muscular coherence. Likewise, muscle activity of different muscles reveals inter-muscular coherence at multiple distinct frequencies reflecting the underlying neural circuitry involved in motor coordination.\n\nRecently it was found that cortical oscillations propagate as travelling waves across the surface of the motor cortex along dominant spatial axes characteristic of the local circuitry of the motor cortex. It has been proposed that motor commands in the form of travelling waves can be spatially filtered by the descending fibres to selectively control muscle force. Simulations have shown that ongoing wave activity in cortex can elicit steady muscle force with physiological levels of EEG-EMG coherence.\n\nOscillatory rhythms at 10 Hz have been recorded in a brain area called the inferior olive, which is associated with the cerebellum. These oscillations are also observed in motor output of physiological tremor and when performing slow finger movements. These findings may indicate that the human brain controls continuous movements intermittently. In support, it was shown that these movement discontinuities are directly correlated to oscillatory activity in a cerebello-thalamo-cortical loop, which may represent a neural mechanism for the intermittent motor control.\n\nNeural oscillations, in particular theta activity, are extensively linked to memory function. Theta rhythms are very strong in rodent hippocampi and entorhinal cortex during learning and memory retrieval, and they are believed to be vital to the induction of long-term potentiation, a potential cellular mechanism for learning and memory. Coupling between theta and gamma activity is thought to be vital for memory functions, including episodic memory. Tight coordination of single-neuron spikes with local theta oscillations is linked to successful memory formation in humans, as more stereotyped spiking predicts better memory.\n\nSleep is a naturally recurring state characterized by reduced or absent consciousness and proceeds in cycles of rapid eye movement (REM) and non-rapid eye movement (NREM) sleep. Sleep stages are characterized by spectral content of EEG: for instance, stage N1 refers to the transition of the brain from alpha waves (common in the awake state) to theta waves, whereas stage N3 (deep or slow-wave sleep) is characterized by the presence of delta waves. The normal order of sleep stages is N1 → N2 → N3 → N2 → REM.\n\nNeural oscillations may play a role in neural development. For example, retinal waves are thought to have properties that define early connectivity of circuits and synapses between cells in the retina.\n\nSpecific types of neural oscillations may also appear in pathological situations, such as Parkinson's disease or epilepsy. These pathological oscillations often consist of an aberrant version of a normal oscillation. For example, one of the best known types is the spike and wave oscillation, which is typical of generalized or absence epileptic seizures, and which resembles normal sleep spindle oscillations.\n\nA tremor is an involuntary, somewhat rhythmic, muscle contraction and relaxation involving to-and-fro movements of one or more body parts. It is the most common of all involuntary movements and can affect the hands, arms, eyes, face, head, vocal cords, trunk, and legs. Most tremors occur in the hands. In some people, tremor is a symptom of another neurological disorder. Many different forms of tremor have been identified, such as essential tremor or Parkinsonian tremor. It is argued that tremors are likely to be multifactorial in origin, with contributions from neural oscillations in the central nervous systems, but also from peripheral mechanisms such as reflex loop resonances.\n\nEpilepsy is a common chronic neurological disorder characterized by seizures. These seizures are transient signs and/or symptoms of abnormal, excessive or hypersynchronous neuronal activity in the brain.\n\nIn thalamocortical dysrhythmia (TCD), normal thalamocortical resonance is disrupted. The thalamic loss of input allows the frequency of the thalamo-cortical column to slow into the theta or delta band as identified by MEG and EEG by machine learning. TCD can be treated with neurosurgical methods like thalamotomy.\n\nNeural oscillations are sensitive to several drugs influencing brain activity; accordingly, biomarkers based on neural oscillations are emerging as secondary endpoints in clinical trials and in quantifying effects in pre-clinical studies. These biomarkers are often named \"EEG biomarkers\" or \"Neurophysiological Biomarkers\" and are quantified using Quantitative electroencephalography (qEEG). EEG biomarkers can be extracted from the EEG using the open-source Neurophysiological Biomarker Toolbox.\n\nNeural oscillation has been applied as a control signal in various brain–computer interfaces (BCIs). For example, a non-invasive BCI interface can be created by placing electrodes on the scalp and then measuring the weak electric signals. Although individual neuron activities cannot be recovered through non-invasive BCI because the skull damps and blurs the electromagnetic signals, oscillatory activity can still be reliably detected. In particular, some forms of BCI allow users to control a device by measuring the amplitude of oscillatory activity in specific frequency bands, including mu and beta rhythms.\n\nA non-inclusive list of types of oscillatory activity found in the central nervous system:\n\n\n\n"}
{"id": "27239040", "url": "https://en.wikipedia.org/wiki?curid=27239040", "title": "OSAMI-F", "text": "OSAMI-F\n\nThe research project OSAMI-F is the French subproject of the European ITEA \"2\" project OSAMI (Open Source AMbient Intelligence).\n\nThe aim of the international project OSAMI is the design of a basic, widely applicable SOA-oriented component platform, its development, test and its provision as open source software. The project consists of a number of national sub-projects, each focusing on a certain field of application.\n\nOSGi and Web Services forms the technical basis of the OSAMI platform in order to implement distributed, dynamically configurable, vendor-neutral and device-independent solutions.\n\nThe French sub-project OSAMI-F, funded by the Ministry of the Economy, Industry and Employment, contributes to different transversal areas such as engineering, architecture, tools and security and with demonstrators in the fields of sensor networking and efficient energy.\n\n\nThe main objective of OSAMI is to connect technologically vertical markets on the basis of an open platform and, hence, to facilitate the market entry for small and medium-sized enterprises (SME). \n\n\n\n\n"}
{"id": "24166960", "url": "https://en.wikipedia.org/wiki?curid=24166960", "title": "O Boticário", "text": "O Boticário\n\nO Boticário () is the second biggest Brazilian cosmetic company. It has 4,070 stores in Brazil, Portugal, Mexico, Bolivia, Peru, United States, Paraguay, Japan, France and Venezuela. O Boticário is the largest cosmetic franchise in the world. The main competitors of the company are Natura, Avon Products and Jequiti.\n\nO Boticário was created in 1977 as a small prescription drugstore in the city of Curitiba, capital of the state of Paraná, in southern Brazil. Today the company is the world’s largest perfumery and cosmetics franchising network.\n\nO Boticário’s industrial and administrative complex has 34.4 thousand square meters of floor space in the city of São José dos Pinhais in the Curitiba Metropolitan Area. It employs 1,300 people and creates approximately 10 thousand jobs through its franchising network. O Boticário’s first manufacturing plant was inaugurated in 1982, with just 1 thousand square meters of floor space. Then it employed 27 people who worked to manufacture about 400 thousand items a year. O Boticário’s current production exceeds 59 million units.\n\nIn 1990, the company created the Fundação O Boticário de Proteção à Natureza (O Boticário Nature Protection Foundation), a nonprofit organization that has already sponsored 800 conservationist projects including studies, scientific research, environmental education programs and direct fauna and flora protection actions all over Brazil. The Foundation also supports the \"Natural Areas Protection Program\", which aims at implementing its own network of private natural heritage reserves.\n\nThe first one is the Salto Morato Private Natural Heritage Reserve, which occupies a 2,340-hectare area, in Guaraqueçaba, on the north coast of the state of Paraná, in southern Brazil. This reservation protects a significant area of the Atlantic Rainforest, besides being provided with infrastructure for scientific research, environmental education, and outdoor recreation. In November 1999, the reservation supported by Fundação O Boticário de Proteção à Natureza was declared a Natural Heritage Site by UNESCO.\n\nO Boticário’s product lines consist of approximately 480 items, divided into the following categories: body care, facial care, sun care, makeup, deodorizing colognes, deodorants, soaps and shampoos.\n\nAmazonian plants such as açaí, cupuaçu, carnaúba, guaraná, cashew, and passion flower; gums extracted from algae and vegetal extracts, such as arnica and urucum, are among the active ingredients present in the brand’s products.\n\nIn May 2015, the company launched in Brazil a campaign titled \"Toda Forma de Amor\" (Every Kind of Love), made for Brazilian Valentine's Day (celebrated every June 12, the day before the Saint Anthony of Padua festivities), the campaign presented heterosexual and homosexual couples embracing and exchanging gifts of the brand. The video generated huge repercussion, mainly in social networks. The commercial, uploaded on YouTube, earned more than three million views and had more than 360,000 likes, against over 180,000 dislikes (until June 6, 2015).\n\nAmong conservative sectors of society, however, the campaign had negative repercussions. There have been calls for boycotting the brand by people like Silas Malafaia and homophobic statements in social networks. The National Council for Self-Regulatory Advertising (Conar) has also received inunerous complaints and initiated a process to verify possible abuses against the consumer in the advertising campaign of the company. On July 16, however, Conar decided to file a lawsuit against the video. The rapporteur of the process emphasized in his vote: \"Don't count on publicity to omit the reality.\" \n\nBoticário replied that \"the proposal of the 'Couples' campaign, which was first broadcast on May 24, is to address, with respect and sensitivity, the current resonance about the most different forms of love regardless of age, race, gender or sexual orientation—represented by the pleasure of giving the person you love a gift on Valentine's Day.\" In an article published on the UOL Economia website, journalist James Cimino also pointed to inconsistencies in the brand's boycott movement as large multinationals—such as Apple, Microsoft, Google, HP, Intel, Facebook, The Coca-Cola Company, Colgate-Palmolive, Disney, Twitter, Visa, MasterCard, Starbucks, Nike, Xerox, Levi's, Gillette, Absolut, Amazon.com, Ray-Ban, Gap, American Airlines, Tiffany & Co., Budweiser and others—also support the LGBT movement, but were not threatened with boycott claims. A tumblr called \"Aproveita e Boicota Também\" (Boycott These As Well) was created to bring together all the brands that support the gay movement and should be avoided by homophobes.\n\n"}
{"id": "26174788", "url": "https://en.wikipedia.org/wiki?curid=26174788", "title": "Proactive information delivery", "text": "Proactive information delivery\n\nIn information systems, proactive information delivery (PID) is a paradigm of supporting users in doing their work by delivering them information related to the current working situation.\nUnlike information search process where the user has to initiate the search, PID tries to identify the user's current information need.\n\nPID can be used for Just-In-Time learning embedded in working processes.\n\nIntellext Watson was a prominent tool illustrating the concept of lightweight PID.\n\nClassification of PID tools is given in \n"}
{"id": "21425480", "url": "https://en.wikipedia.org/wiki?curid=21425480", "title": "Psion Teklogix", "text": "Psion Teklogix\n\nPsion Teklogix Inc. was the operational business of Psion.\n\nPsion Teklogix is a global provider of solutions for mobile computing and wireless data collection. Psion Teklogix' products and services include rugged mobile hardware, secure wireless networks, software, professional services and support programs.\n\nPsion Teklogix was formed in September 2000 as a result of the merger between U.K.-based Psion Enterprise division of Psion PLC, and Canadian-based Teklogix Inc.\n\nPsion Teklogix is headquartered in Mississauga, Ontario, Canada with additional corporate offices located in Europe, the United States, Asia, Latin America and the Middle East.\n\nPsion Teklogix is an registered company, and holds a certificate of registration from the \"British Standards Institution\".\n\nIn 2012 Motorola Solutions purchased Psion Teklogix for $200million \n\nIn April 2014, it was announced that Motorola Solutions' enterprise business (including assets acquired in the Psion Teklogix purchase) is to be sold to Zebra Technologies.\n\nTeklogix was created in 1967 by Rod Coutts, a 1964 bachelor of applied science in electrical engineering, together with a small group of young Canadian engineers. The company grew to specialize in empowering mobile workers with wireless data transmission and real-time data management within the logistics industry.\n\nThe Psion Group, founded in 1980 by David Potter, is widely credited with having created the world's first volume produced PDA with the launch of the Psion Organiser in 1984. Generally recognized as the world's first practical pocket computer, the Organiser helped evolve Psion into a major technology player.\n\nIn 2000 Psion acquired Teklogix in Canada for £240 million, and merged its business-to-business division, Psion Enterprise, with the newly acquired company. Teklogix was re-branded Psion Teklogix. This division now forms the core of Psion Plc's business. \n\nIn 2002 Psion Teklogix created a new division called Psion Software. This business developed push email solutions for Symbian smartphones, Microsoft Exchange and Lotus Notes. This business was sold to Visto (USA) in 2003. \n\nIn 2004, Psion Teklogix announced its intention to dispose of the company's remaining Symbian shareholding to Nokia, as they no longer regarded it as a core part of their strategy.\n\nPsion PLC had a lengthy, but distant, interest in Linux as an operating system on its electronic devices. In 1998, it supported the Linux7K project that had been initiated by Ed Bailey at Red Hat, which was to port Linux to its Series 5 personal computer. The project was named after the\nCirrus Logic PS-7110 chip of the Series 5. Although this project was one of the earliest attempts to port Linux to a handheld computer, it did not come to fruition for Psion. The project soon transitioned to an informal open source project at Calcaria.net, that kept the name Linux7K. After the project transitioned again to sourceforge.net, the project's name was changed to a more general name \"PsiLinux\", and more recently to \"OpenPsion\". The project has developed Linux kernels and filesystems for the Revo, Series 5 and 5MX, and Series 7 and netBook.\n\nIn 2003–4, Psion Teklogix and its founder David Potter expressed interest in Linux as the operating system for its devices as it divested from Symbian. However, the only result of that interest was Linux as the operating system on a limited number of custom NetBook Pros designed for a hospital setting.\n\nPsion registered the trademark \"NETBOOK\" in various territories, including European Union and , which was applied for on 18 December 1996 and registered by USPTO on 21 November 2000. They used this trademark for the Psion netBook product (discontinued in November 2003) and more recently the NETBOOK PRO, from October 2003 onwards.\n\nIntel began the use of the term \"netbook\" in March 2008 as a generic term to describe \"small laptops that are designed for wireless communication and access to the Internet\", believing they were \"not offering a branded line of computers here\" and \"see no naming conflict\".\n\nIn response to the growing use of this term, on 23 December 2008 Psion Teklogix sent cease and desist letters to various parties including enthusiast website(s) demanding they no longer use the term \"netbook\".\n\nIn early 2009 Intel sued Psion Teklogix (US & Canada) and Psion (UK) in the Federal Court, seeking a cancellation of the trademark and an order enjoining Psion from asserting any trademark rights in the term \"netbook\", a declarative judgement regarding their use of the term, attorneys' fees, costs and disbursements and \"such other and further relief as the Court deems just and proper\". The suit was settled out of court, and on June 2, 2009 Psion announced that the company was withdrawing all of its trademark registrations for the term \"Netbook\" and that Psion agreed to \"waive all its rights against third parties in respect of past, current or future use\" of the term.\n\nSimilar marks have been recently rejected by the USPTO citing a \"likelihood of confusion\" under section 2(d), including 'G NETBOOK' ( rejected 31 October 2008), MSI's 'WIND NETBOOK' () and Coby Electronics' 'COBY NETBOOK' ( rejected 13 January 2009)\n\n"}
{"id": "52103614", "url": "https://en.wikipedia.org/wiki?curid=52103614", "title": "RSRP", "text": "RSRP\n\nReference Signals Received Power is a measurement of the received power level in an LTE cell network. The average power is measurement of the power received from a single reference signal.\n\n"}
{"id": "23667782", "url": "https://en.wikipedia.org/wiki?curid=23667782", "title": "Railway surgery", "text": "Railway surgery\n\nRailway surgery was a branch of medical practice that flourished in the 19th and early 20th centuries. It concerned itself with the medical requirements of railway companies. Depending on country, it included some or all of; general practice for railway staff, trauma surgery as a result of accidents on the railways, occupational health and safety, medico-legal activities regarding compensation claims against the company, and occupational testing.\n\nRailway surgery was especially well developed in the US with formal professional organisations and scholarly journals. One of the reasons for this was that US railways were particularly dangerous with a high number of casualties from crashes and an even higher number of workers hurt in industrial accidents. Another reason was that many US routes passed through areas with little or no existing medical infrastructure. In Europe the railways were safer and infrastructure was generally already in place. The duties of railway surgeons here was mostly concerned with investigations into accidents and the resulting claims arising from passengers. In India the railways also faced a lack of existing medical infrastructure and, like the US, had to build it from scratch. Indian Railways to this day still maintain a network of hospitals.\n\nBy the middle of the 20th century railway surgery had lost its separate identity. The growth of other industries and improving rail safety meant that the railways no longer stood out as a leading cause of injuries. Likewise, industrial medicine and health and safety became a concern for all industries. There was no longer a need for an industry-specific branch of medicine.\n\nThe primary purpose of railway surgeons was to attend the many injuries to staff and passengers on the railways. Railway surgeons were also responsible for assessing medical claims against their companies and providing routine medical examinations of employees and applicants. Railway surgeons were keen to establish their profession as a separate discipline with unique problems. For instance, the textbook \"Railway Surgery\", published in 1899, spends a whole chapter on the claim that crushing injuries from the heavy moving equipment on railways is of a kind not found in other industries.\n\nRailway surgeons were generally not well paid. While a chief surgeon or consultant surgeon might attract a good salary, the ordinary doctors working out on the lines were not well rewarded.\n\nRail crashes were common in the 19th century with many deaths and broken bones. In Europe, the majority of injuries were due to collisions, hence passengers rather than employees formed the bulk of the injured. For instance, in Britain, accidents on the line such as crushing between wagons and being struck by trains (accidents suffered mostly by railway staff) were noted as only the third most common class of injury in 1887. In the US, the railways were significantly more dangerous. Counterintuitively, this led to staff injuries being by far the greatest proportion. For instance, in 1892 US railroads reported 28,000 injuries to workers compared to 3,200 for passengers. The most dangerous role was brakeman, whose job included coupling and uncoupling wagons. The most frequent injury was the brakeman being crushed between wagons while carrying out this task. One third of all railway injuries to staff were crushed hands or fingers.\n\nThe number of deaths and injuries grew rapidly as the century progressed. In the US, the first death occurred in 1831. There were 234 deaths in 1853, the year in which there were eleven major crashes. In 1890, there were 6,335 deaths and 35,362 injuries, and by 1907 it had reached its peak at nearly 12,000 deaths. In England, an early railway death was William Huskisson, Member of Parliament for Liverpool, killed during the opening of the Liverpool and Manchester Railway in 1830. In the UK as a whole, deaths and injuries grew much less dramatically than the US. There were 338 passenger deaths and injuries in 1855 and 435 in 1863, but at the same time passenger numbers were increasing. From about the middle of this period the \"rate\" of injury started to go down with improving safety—2.85 per million passengers in 1855 to 2.12 in 1863. A similar picture of low rates pertained in other European countries such as France, Prussia, Belgium and Germany.\n\nMany injuries required amputation. For instance, in 1880 the Baltimore and Ohio Railroad reported 184 crushed limbs, including 85 fractures and fifteen amputations. The need to perform amputations and other surgery may be why railway surgeons were named \"surgeon\" rather than \"doctor\" or \"physician\". While traumatic injuries were not the most frequent ailment they had to deal with (that was infectious diseases) it did distinguish them from typical general practitioners. Other influences may have been the comparison to ship's surgeons in the age of sail, and the greater status and pay of surgeons.\n\nFirst aid kits and training were introduced at the instigation of Charles Dickson of the Canadian St. John Ambulance Association and backed by many railway surgeons. One railway surgeon reported that his fatality rate following amputation more than halved after first aid was introduced. There was some opposition to first aid through fear that it eroded the professional status of doctors and that local contract railway surgeons would lose the fees they would otherwise have accrued for the work. However, by World War I first aid kits on trains had become widespread.\n\nAs the twentieth century progressed, rail safety improved dramatically. So much so that by 1956 the president of the Association of Surgeons of the Chesapeake and Ohio Railway, when asked if he saw a lot of accident cases at the company hospital, could reply \"yes, I see a lot accidents from the highway\" and \"[t]he primary purpose of our group has all but passed out of the picture.\" The discipline of railway surgery had been folded into the more general discipline of trauma surgery.\n\nAn important function of railway surgeons was to control the costs of claims against their companies, leading to conflict of interest regarding treatment of their patients. For instance, the chief surgeon of the Missouri Pacific Railroad proudly boasted that he kept claims in the region of 14–35 dollars whereas claims against another railroad were nearly 600 dollars on average. Company medical staff were also expected to help achieve political aims with expert evidence.\n\nFraudulent claims were a big concern for railway companies. Sometimes, these were entirely contrived \"accidents\" by confidence tricksters; more commonly, however, they were exaggerations of the results of a genuine accident. In 1906, railway surgeon Willis King claimed that 96% of claimants were actually malingering. One common claim that was hotly debated by railway surgeons was the condition known as \"railway spine\". Over a hundred papers were published on the subject in the medical literature for the period 1866–1890. In this condition, the patient reports cerebral symptoms such as headache or loss of memory, but there is no visible damage to the spine. It was first described by Danish-British surgeon John Eric Erichsen. Erichsen believed the condition was physiological, due to inflammation of the spinal cord. He compared it to meningitis which had similar symptoms and a similar cause. Others, such as Herbert W. Page (surgeon to the London and North Western Railway), argued that it was psychological, or else, like Arthur Dean Bevan (1918 president of the American Medical Association), outright faked. Railway surgeons were generally skeptical of the condition and disliked the implication that there was an injury peculiar to just railways. British surgeon Edwin Morris described the term as \"most reprehensible\". Bevan claimed to have cured one patient in ten minutes by standing him up and speaking to him forcefully.\n\nIn Britain, a number of major train crashes led to parliament passing an act which compelled rail companies to pay compensation to victims of accidents. A large proportion of the thousands of resulting claims were for railway spine. In a five-year period in the 1870s English railway companies paid £2.2 million ($11 million) in claims for railway spine. Following a similar law in Germany, 78% of claims in the period 1871–1879 were for back injury. Claims for railway spine had subsided by 1900. From 1890 there were more claims for psychosomatic conditions such as hysteria and the neurasthenia of George Miller Beard rather than physiological conditions such as railway spine. Even Erichsen himself agreed that his original cases were actually traumatic neurasthenia. However, medical concern remained over these emotional damage claims regarding malingering and false claims.\n\nRailway companies pioneered the disciplines of industrial medicine and occupational safety and health. They were the first organisations to give physical examinations to prospective employees. The motivation was partly to prevent fraudulent claims for pre-existing conditions, but also to improve rail safety. The Baltimore and Ohio Railroad in 1884 was the first to do this and had a particularly thorough examination. They rejected 7% of applicants. Colour blindness tests were introduced for all employees in most US railway companies in the 1880s because of the need for correct signal recognition. Hearing tests were also implemented.\n\nColour blindness tests were a result of the 1875 Lagerlunda rail accident in Sweden. The accident was investigated by Frithiof Holmgren who concluded that the crash was a result of colour blindness of the driver. Holmgren created a test for use in the transportation industry based on matching coloured skeins of yarn. However, Holmgren's test was disliked by employees and there were many complaints of unfairness. Many companies preferred to test with lamps and flags, but this led to conflict with the railway surgeons who were concerned that non-professionals were administering medical tests. Holmgren's identification of colour blindness as the cause of the Swedish crash has also been disputed.\n\nRailway companies increasingly improved the health of their employees with preventative measures against disease. Vaccination schemes were introduced. Sanitation aboard trains became an issue, as did the quality of water supplies. In the 1920s the Cotton Belt Railroad succeeded in cutting hospitalization due to malaria of its workers from 21.5% to 0.2%. They did this with a program that included suppressing mosquito populations, preventative quinine treatment, education, and blood tests.\n\nRailway surgery was particularly well developed in the US in the nineteenth and early-twentieth centuries. Early American railroads passed through vast, thinly populated regions with little possibility of finding local medical care. At the same time American railways were uniquely dangerous. Accident rates were far higher than European railways. That, together with difficulty in retaining skilled staff and the risk of legal action by injured passengers resulted in American railway companies developing medical infrastructure and organisations to a degree that was not seen elsewhere.\n\nThe difference in risk between the US and Europe was largely driven by economics. With their higher population densities and shorter routes, European countries carried more freight per mile of track than in the US. To be profitable, American railways needed to be built and run more cheaply. This led to a host of safety issues. American railways were poorly signalled compared to British railways; in the US there were no signal boxes, instead signalling was largely the responsibility of staff on board trains. Rails were laid on thin beds and badly secured leading to many derailments. Most tracks in Europe were double tracks whereas in the US they were usually single track. Poor control of single-track rails led to many collisions. There was no fencing on American tracks, leading to many collisions with people and animals.\n\nCertainly, railway companies in other countries had railway surgeons. In Australia and Canada railway surgeons were appointed during the construction of major lines in remote regions. British companies employed railway surgeons as consultants. However, only in the US were railway surgeons employed in such large numbers and as such an organised discipline. India is perhaps the closest to the US in widespread use of railway surgeons which continues to this day, but there they never seem to have organised themselves as a discipline with professional associations until the 2000s.\n\nConstruction of railways in the US began in 1828, becoming significant by 1840. By 1900 the railways were the largest US industry employing over a million, far more than any other dangerous occupation. Many thousands of injuries occurred on these railroads throughout the 19th century. From 1850 until World War I American railroad companies started to develop their own medical arrangements in order to retain workers, care for their passengers and injured third parties, and avoid legal action. The earliest example of a company railroad surgeon was James P. Quinn on the Baltimore and Ohio Railroad in 1834, but the practice did not become widespread until the 1850s. By the end of the 1850s most companies had some kind of medical service.\n\nLegal claims for compensation in the nineteenth century under tort laws were weaker than under twentieth century compensation laws. Railroad workers (the majority of injuries) were not usually able to claim. Nor were trespassers on railway property. Passengers, however, were able to claim, but payments were low and few victims got anything compared to modern standards. Nevertheless, the large number of crashes still led to a substantial number of legal actions.\n\nThese organisations pioneered industrial medical care and predated the provision of medical care in manufacturing industries (although some mining companies did so). Railway surgery was an important sector of the medical profession; by World War I railway organisations employed 14,000 doctors, some 10% of the US total. Railway surgery operated outside of the American Medical Association (AMA) which railway surgeons were not permitted to join. Many in the medical profession opposed company doctors altogether on the grounds of ethical conflict between the needs of the patient and the needs of the company. The AMA also disliked the practice of doctors bidding for local contracts with the railway companies as this also introduced commercial pressures.\n\nEarly medical arrangements on the railroads were informal. The Philadelphia & Reading Railroad in particular had no formal contracts as late as the 1870s. This led to many disputes over payment. By the 1880s, however, all railroads had more formal arrangements. They appointed a salaried chief surgeon and the larger ones had salaried regional surgeons as well. Local doctors were paid according to fixed fees under contract. In the West, the lack of existing medical facilities led to employee-funded hospitals. The first of these was set up by the Central Pacific Railroad around 1867 and modelled on the US Marine Hospital Service. It was funded by a fixed deduction from all employees, except Chinese employees, who were excluded from the service. A different kind of arrangement was the mutual benefit society. The first of these was set up by the Baltimore & Ohio Railroad in 1880 following the Great Railroad Strike of 1877. Other railroads started to follow this model, partly because of its effect of dampening industrial strife. By World War I about 30% (around two million) of railroad workers were members.\n\nBy 1896 there were twenty-five railway hospitals run by thirteen different railway companies. Between them they treated 165,000 patients annually. Some of these hospitals became important teaching hospitals and generally they were all well financed and well equipped.\n\nAfter World War I railway surgery began to decline as a separate discipline. It lost its political influence in the 1930s during the Great Depression as industry, including the railroads, reduced their involvement in health care for their workers. At the same time accident risks were steadily falling. Between 1890 and 1940 risks for passengers and workers fell by 90% and 80% respectively.\n\nThe first inter-city railway passenger service in the world was the Liverpool and Manchester Railway, opened in 1830. By 1860, the UK had 10,000 miles of track and employed 180,000 people. Despite being the pioneer, British railways (and European railways in general) were much safer than American lines and consequently had fewer injuries, although early injury rates were still high by modern standards. Furthermore, British lines did not pass through vast, thinly populated regions. Medical infrastructure was already well developed in the country. Railway companies still employed surgeons. Page at the London and North Western has already been mentioned. Another example is Thomas Bond who was retained as surgeon by both the Great Eastern and Great Western Railways. However, Bond's function for the railways was primarily as medico-legal consultant regarding injury claims rather than practical surgery. He did, however, treat the injured of an overturned train on which he was himself a passenger. Bond's last major work for the railways was investigations in connection with the Slough rail accident of 1900. Bond also wrote a lengthy article on railway injuries for Heath's \"Dictionary of Practical Surgery\". There is a similar picture with James O. Fletcher, surgeon to the Manchester, Sheffield and Lincolnshire and Great Northern Railways, who wrote the book \"Railways in Their Medical Aspects\". Fletcher's book discusses numerous cases where he examined or treated patients post-surgery and discusses medico-legal issues at length. However, he does not cite a single example of his own, or railway colleagues, emergency surgery.\n\nRailways in India began to be built during the British India period. The first passenger service opened between Mumbai and Thane in 1853. As in many parts of the US, there was no pre-existing medical infrastructure. Medical facilities had to be entirely provided by the railways themselves. This, along with other social needs, led to the establishment of railway \"colonies\" to house the railway workers and their families. For instance, the railway colony of Jamalpur was established in 1862. By 1869, one railway surgeon and four assistants were employed at Jamalpur by the East Indian Railway Company to cover the 800 inhabitants of the colony and over 200 miles of railway line. This section had around 200 accidents per year to deal with but that was not their only duty. They also acted as general practitioners for the colony, including attending at childbirth.\n\nBy 1956, the railways had established 80 hospitals and 426 dispensaries. Additionally, mobile railway surgeons served railway staff posted along the line. This infrastructure of railway colonies, medical staff, and hospitals is still maintained by the railways. A major example of a railway colony is the colony at Kharagpur, West Bengal, established 1898–1900. It serves one of the largest railway workshops in India. A major example of a railway hospital is the Southern Railway Headquarters Hospital at Chennai. Southern Railway also maintain divisional hospitals such as the Divisional Railway Hospital, Golden Rock as do the other zones of Indian Railways.\n\nHospital trains were in use early in the railway age, but usually in wartime, starting in the Crimean War in 1855. For normal civilian use, they were a fairly late development in the railway surgery period. In the US, a hospital car came into use on several railways that could be coupled to a train to transport the surgeon and staff to the scene of an incident along with all the required equipment. It contained an operating theatre and a number of beds for patients in a recovery room. Hospital cars significantly improved the outcomes for victims. These mobile hospitals were the forerunner of the US Army's mobile army surgical hospitals.\n\nIndian Railways introduced the Lifeline Express hospital train in 1991. Its purpose was to serve the needs of rural poor in India. In 2007 a modernised hospital train, the New Lifeline Express replaced it.\n\nFrom the 1880s onwards specific societies were formed for railway surgery. The first of these was the Surgical Society of the Wabash, formed in 1881 and organised by the chief surgeon of the Wabash Railroad. The inaugural meeting had only twenty-two attendees with two papers presented. This was soon followed by many other company centred organisations. The Pennsylvania Railroad organisation was the prime mover in forming the National Association of Railway Surgeons (NARS) in 1888. The inaugural meeting had two hundred attendees and by 1891 the membership had grown to nearly a thousand. However, NARS was short-lived, dying out in the 1920s as industrial expansion caused the railways to no longer be unique in their medical needs. Nevertheless, there were still practising railway surgeons for some time after this.\n\nAlso in 1891, the journal \"Railway Age\" began a column on railway surgery, including the proceedings of NARS. The editor, R. Harvey Reed, was a charter member of NARS but left to form the more exclusive American Academy of Railway Surgeons. The two organisations remained rivals until 1904 when they merged to form the American Association of Railway Surgeons (AARS). The \"Railway Surgeon\" was launched in 1894 and took over from \"Railway Age\" as the official journal of NARS, but it also carried articles from the American Academy. It was the official journal of NARS, 1894–1897; of the International Association of Railway Surgeons, 1897–1904; and of the AARS, 1904–1929.\n\nThe changing names and affiliations of the \"Railway Surgeon\" across the years reflects the absorption of railway surgery into the more general occupational health and the spread of industrial medicine. \"Railway Surgeon\" (1894–1904), became \"Railway Surgical Journal\" (1904–1921), became \"Surgical Journal Devoted to Traumatic and Industrial Surgery\" (1921–1929), absorbed by \"International Journal of Medicine and Surgery\" (1923–1935), with \"Industrial Medicine\" (1932–1935), became \"International Journal of Industrial Medicine and Surgery\" (1935–1949), became \"\"Industrial Medicine and Surgery\" (1949–1967), became \"IMS, Industrial Medicine and Surgery\" (1968–1973), became \"The International Journal of Occupational Health and Safety\" (1974–1975), became \"Occupational Health and Safety\" (1976–present).\n\nCriticism of Erichsen and \"railway spine\" was particularly strident in the US with NARS and its journal, the \"Railway Surgeon\" leading the attack. So much effort was devoted to discrediting the condition as medically valid that one writer claimed the issue was the reason for existence of the association. Prominent in NARS for speaking out against Erichsen was Warren Bell Outten, chief surgeon for the Missouri Pacific Railway.\n\nIn India, the Association of Railway Surgeons of India held its first annual meeting in 2001.\n\n\n\n"}
{"id": "41641", "url": "https://en.wikipedia.org/wiki?curid=41641", "title": "Reflection coefficient", "text": "Reflection coefficient\n\nIn physics and electrical engineering the reflection coefficient is a parameter that describes how much of an electromagnetic wave is reflected by an impedance discontinuity in the transmission medium. It is equal to the ratio of the amplitude of the reflected wave to the incident wave, with each expressed as phasors. For example, it is used in optics to calculate the amount of light that is reflected from a surface with a different index of refraction, such as a glass surface, or in an electrical transmission line to calculate how much of the electromagnetic wave is reflected by an impedance. The reflection coefficient is closely related to the \"transmission coefficient\". The reflectance of a system is also sometimes called a \"reflection coefficient\".\n\nDifferent specialties have different applications for the term.\n\nIn telecommunications, the reflection coefficient is the ratio of the complex amplitude of the reflected wave to that of the incident wave. In particular, at a discontinuity in a transmission line, it is the complex ratio of the electric field strength of the reflected wave (formula_1) to that of the incident wave (formula_2). This is typically represented with a formula_3 (capital gamma) and can be written as:\n\nThe reflection coefficient may also be established using other field or circuit quantities. \n\nThe reflection coefficient of a load is determined by its impedance formula_5(load impedance) and the impedance toward the source formula_6(source impedance).\n\nformula_7\n\nNotice that a negative reflection coefficient means that the reflected wave receives a 180°, or formula_8, phase shift.\n\nThe magnitude (designated by vertical bars) of the reflection coefficient can be calculated from the standing wave ratio, formula_9:\n\nThe reflection coefficient is displayed graphically using a Smith chart.\n\nReflection coefficient is used in feeder testing for reliability of medium.\n\nIn optics and electromagnetics in general, \"reflection coefficient\" can refer to either the amplitude reflection coefficient described here, or the reflectance, depending on context. Typically, the reflectance is represented by a capital \"R\", while the amplitude reflection coefficient is represented by a lower-case \"r\".These related concepts are covered by Fresnel equations in classical optics.\n\nAcousticians use reflection coefficients to understand the effect of different materials on their acoustic environments.\n\n\n\n"}
{"id": "418544", "url": "https://en.wikipedia.org/wiki?curid=418544", "title": "Screen reader", "text": "Screen reader\n\nA screen reader is a form of assistive technology (AT) which is essential to people who are blind, as well as useful to people who are visually impaired, illiterate, or have a learning disability. Screen readers are software applications that attempt to convey what people with normal eyesight see on a display to their users via non-visual means, like text-to-speech, sound icons, or a Braille device. They do this by applying a wide variety of techniques that include for example interacting with dedicated accessibility APIs, using various operating system features (like inter-process communication and querying user interface properties) and employing hooking techniques.\n\nMicrosoft Windows operating systems have included the Microsoft Narrator screen reader since Windows 2000. Apple Inc.'s macOS, iOS, and tvOS include VoiceOver as a built-in screen reader, while Google's Android provides Talkback screen reader since 2009. Similarly, Android-based devices from Amazon provide the VoiceView screen reader. BlackBerry 10 devices such as the BlackBerry Z30 also include a built-in screen reader. There is also a free screen reader application for older BlackBerry (BBOS7 and earlier) devices.\n\nThere are also popular free and open source screen readers, such as Speakup and Orca for Linux and Unix-like systems and NonVisual Desktop Access for Windows.\n\nThe most widely used screen readers are often separate commercial products: JAWS from Freedom Scientific, Window-Eyes from GW Micro, Dolphin Supernova by Dolphin, System Access from Serotek, and ZoomText Magnifier/Reader from AiSquared are prominent examples.\n\nIn early operating systems, such as MS-DOS, which employed command-line interfaces (CLIs), the screen display consisted of characters mapping directly to a screen buffer in memory and a cursor position. Input was by keyboard. All this information could therefore be obtained from the system either by hooking the flow of information around the system and reading the screen buffer or by using a standard hardware output socket and communicating the results to the user.\n\nIn the 1980s, the Research Centre for the Education of the Visually Handicapped (RCEVH) at the University of Birmingham developed Screen Reader for the BBC Micro and NEC Portable.\n\nWith the arrival of graphical user interfaces (GUIs), the situation became more complicated. A GUI has characters and graphics drawn on the screen at particular positions, and therefore there is no purely textual representation of the graphical contents of the display. Screen readers were therefore forced to see employ new low-level techniques, gathering messages from the operating system and using these to build up an \"off-screen model\", a representation of the display in which the required text content is stored.\n\nFor example, the operating system might send messages to draw a command button and its caption. These messages are intercepted and used to construct the off-screen model. The user can switch between controls (such as buttons) available on the screen and the captions and control contents will be read aloud and/or shown on refreshable Braille display.\n\nScreen readers can also communicate information on menus, controls, and other visual constructs to permit blind users to interact with these constructs. However, maintaining an off-screen model is a significant technical challenge; hooking the low-level messages and maintaining an accurate model are both difficult tasks.\n\nOperating system and application designers have attempted to address these problems by providing ways for screen readers to access the display contents without having to maintain an off-screen model. These involve the provision of alternative and accessible representations of what is being displayed on the screen accessed through an API. Existing APIs include:\n\nScreen readers can query the operating system or application for what is currently being displayed and receive updates when the display changes. For example, a screen reader can be told that the current focus is on a button and the button caption to be communicated to the user. This approach is considerably easier for the developers of screen readers, but fails when applications do not comply with the accessibility API: for example, Microsoft Word does not comply with the MSAA API, so screen readers must still maintain an off-screen model for Word or find another way to access its contents. One approach is to use available operating system messages and application object models to supplement accessibility APIs. The Thunder screen reader operates without an off-screen model in this way; the latest version of Thunder also includes an off-screen model.\n\nScreen readers can be assumed to be able to access all display content that is not intrinsically inaccessible. Web browsers, word processors, icons and windows and email programs are just some of the applications used successfully by screen reader users. However, according to some users, using a screen reader is considerably more difficult than using a GUI, and many applications have specific problems resulting from the nature of the application (e.g. animations in Macromedia Flash) or failure to comply with accessibility standards for the platform (e.g. Microsoft Word and Active Accessibility).\n\nSome programs and applications have voicing technology built in alongside their primary functionality. These programs are termed self-voicing and can be a form of assistive technology if they are designed to remove the need to use a screen reader.\n\nSome telephone services allow users to interact with the internet remotely. For example, TeleTender can read web pages over the phone and does not require special programs or devices on the user side.\n\nA relatively new development in the field is web-based applications like Spoken-Web that act as web portals, managing content like news updates, weather, science and business articles for visually-impaired or blind computer users. Other examples are ReadSpeaker or BrowseAloud that add text-to-speech functionality to web content. The primary audience for such applications is those who have difficulty reading because of learning disabilities or language barriers. Although functionality remains limited compared to equivalent desktop applications, the major benefit is to increase the accessibility of said websites when viewed on public machines where users do not have permission to install custom software, giving people greater \"freedom to roam\".\n\nWith the development of smartphones, the ability to listen to written documents (textual web content, PDF documents, e-mails etc.) while driving or during a similar activity in the same way that listening to music, will benefit a much broader audience than visually-impaired people. The best-known examples are Siri for iOS, and Google Now and Iris for Android. With the release of the Galaxy S III, Samsung also introduced a similar intelligent personal assistant called S Voice. On the BlackBerry 10 operating system, their Z30 smartphone also features spoken interaction features, which are similar to the other mobile operating systems.\n\nThis functionality depends on the quality of the software but also on a logical structure of the text. Use of headings, punctuation, presence of alternate attributes for images, etc. is crucial for a good vocalization. Also a web site may have a nice look because of the use of appropriate two dimensional positioning with CSS but its standard linearization, for example, by suppressing any CSS and Javascript in the browser may not be comprehensible.\n\nMost screen readers allow the user to select whether most punctuation is announced or silently ignored. Some screen readers can be tailored to a particular application through scripting. One advantage of scripting is that it allows customizations to be shared among users, increasing accessibility for all. JAWS enjoys an active script-sharing community, for example.\n\nVerbosity is a feature of screen reading software that supports vision-impaired computer users. Speech verbosity controls enable users to choose how much speech feedback they wish to hear. Specifically, verbosity settings allow users to construct a mental model of web pages displayed on their computer screen. Based on verbosity settings, a screen-reading program informs users of certain formatting changes, such as when a frame or table begins and ends, where graphics have been inserted into the text, or when a list appears in the document.\n\nSome screen readers can read text in more than one language, provided that the language of the material is encoded in its metadata.\n\nSome screen reading programs also include language verbosity, which automatically detects verbosity settings related to speech output language. For example, if a user navigated to a website based in the United Kingdom, the text would be read with an English accent.\n\n\n"}
{"id": "1134304", "url": "https://en.wikipedia.org/wiki?curid=1134304", "title": "SimEx-Iwerks", "text": "SimEx-Iwerks\n\nSimEx-Iwerks Entertainment specializes in high-tech entertainment systems, films, film technologies, film-based software, Simulation Hardware Systems and services around the world. The company is a leading innovator of immersive 3-D and 4-D attractions, special effects, and cinematic experiences; including Large Format (870 Projection Systems) and 360 Degree Theatres. It is the first to employ motion simulators and 4D seat technologies and today it is pioneer in the use of flying seats, unique special effects, projection mapping, VR, AI, animation and Interactive technologies. The company has partnerships with various institutions, parks, and destinations.\n\nThe company has been serving the amusement industry for 30 years, through three operating divisions: Attractions Development; Content Licensing, Production & Distribution; and Technology/Engineering.\n\nSimEx Inc. is the parent company of SimEx-Iwerks Entertainment. The company is based in Toronto, Canada with additional locations in Baltimore, Maryland; and Santa Clarita, California.\n\nSince the merger of SimEx Inc. and Iwerks Entertainment Inc. in 2002, the company traded under the name “SimEx-Iwerks Entertainment” with its Head Office in Toronto and Iwerks offices in Baltimore and Los Angeles.\n\nSimEx origins lie with the creation by Michael Needham, Shiori Sudo and Moses Znaimer of Tour of the Universe™(1984) at the CN Tower, Toronto. Tour of the Universe was the world’s first major Simulator Attraction, which inspired many motion ride Attractions. In the 1980s SimEx and Iwerks developed their own separate visions for Motion Ride Attractions.\n\nToronto based SimEx developed a series of Attraction products using innovative and patented motion systems provided by Moog Inc. of East Aurora, NY. Moog’s technology integrated novel electromechanical actuators with solid-state control systems to create a variety of attraction platforms.\n\nIn 1996, SimEx launched SimEx Virtual Voyages™, a 15-minute Attraction that follows a 3-stage storyline: Introduction (pre-show); Plot Development (story theater); and Climax (The Ride). Over 40 SimEx Virtual Voyages™ Attractions were built worldwide.\n\nLos Angeles based Iwerks developed innovative 8/70 projection technologies for Extreme Screen™ Theaters and two-seat motion systems (TurboRide™). In 1998, Iwerks Co-founder Don Iwerks was awarded the prestigious Gordon E. Sawyer Award from the Academy of Motion Picture Arts & Sciences in recognition of his lifetime of contribution to the science and technology of motion pictures.\n\nIn 2000, Iwerks launched its Iwerks Extreme Screen™ Brand that became the leading Brand for 8/70 Screen Theaters. In the same year, the Iwerks 8/70 Linear Loop® projection system was recognized for its industry leading technology with an Academy Award® for Scientific and Technical Achievement.\n\nIn 2001, SimEx acquired assets from the RideFilm™ Division of Imax® Corporation and in 2002, SimEx merged with Iwerks Entertainment.\n\nIn 2003, SimEx-Iwerks designed the first full-motion 4-D seat for Universal Studio’s Shrek 4-D Attractions in Hollywood, Orlando and Osaka.\n\nSimEx-Iwerks continues to be an innovator in digital systems, film creation, 3-D projection, 4-D special effects and as a partner provides ongoing operating and marketing support to numerous attractions.\n\nHistory\n\nIwerks Entertainment Inc. is an American film studio founded in 1985 in Burbank, California, by Oscar winner and Disney Legend Don Iwerks and Stan Kinsey, both former Disney Executives. The company was named to honor Don's father, Ub Iwerks, who was Walt Disney's first business partner and co-creator of Mickey Mouse.\n\nIn the 1980s, Iwerks Entertainment became well known as a leading developer of special venues and films, and virtual reality theatres throughout the world.\n\nIn late 1999, Iwerks Entertainment acquired the assets of McFadden Systems. In 1992, Warner Bros. Movie World approached McFadden to develop a motion simulator for \"Batman Adventure – The Ride\". Later motion systems were built for , London Trocadero.\n\nAs of Dec 2017, Iwerks Entertainment, Inc. is a dominant force in 4D and special effects film production and distribution, operates from offices in Santa Clarita CA as a subsidiary of SimEx Inc., and trades under its own name and as SimEx-Iwerks.\n\nAcademy Awards\n\nIwerks Entertainment has received two Academy Awards by the Academy of Motion Picture Arts and Sciences for Scientific and Technical Achievement.\n\nThe first occurred in 1998 at the 70th Academy Awards show, where founder Don Iwerks was awarded the Gordon E. Sawyer Award, given each year to \"an individual in the motion picture industry whose technological contributions have brought credit to the industry.\"\n\nThe second occurred in 1999 at the 71st Academy Awards show, where the company was awarded an Academy Award for a technical innovation called the Iwerks 8/70 Linear Loop projection system.\n\n\nSimEx-Iwerks designs and builds family attractions that feature charismatic characters and exciting stories often licensed from the world’s great media companies. SimEx-Iwerks Experiences include 3-D, 4-D and VR Attractions; Motion Simulation Rides; and Flying Theaters. Over 350 Attractions have been built by the SimEx-Iwerks group in over 40 countries.\n\nSimEx-Iwerks active film library consists of over 125 films and include the most popular Attraction quality content available. (http://www.simexdesign.com/content/)\n"}
{"id": "41721", "url": "https://en.wikipedia.org/wiki?curid=41721", "title": "Slave clock", "text": "Slave clock\n\nIn telecommunication and horology, a slave clock is a clock that depends for its accuracy on another clock, a master clock. Many modern clocks are synchronized, either through the Internet or by radio time signals, to a worldwide time standard called Coordinated Universal Time (UTC) based on a network of master atomic clocks in many countries. For scientific purposes, precision clocks can be synchronized to within a few nanoseconds by dedicated satellite channels. Slave clock synchronization is usually achieved by phase-locking the slave clock signal to a signal received from the master clock. To adjust for the transit time of the signal from the master clock to the slave clock, the phase of the slave clock may be adjusted with respect to the signal from the master clock so that both clocks are in phase. Thus, the time markers of both clocks, at the output of the clocks, occur simultaneously.\n\nBefore the computer era, the term referred to electrical clocks that are synchronized periodically by an electrical pulse through dedicated wiring issued by a master clock in the same building. From the late 19th to the mid 20th centuries, electrical master/slave clock systems were widely used in public buildings and business offices, with all the clocks in the building synchronized through electric wires to a central master clock. These older styles of slave clocks either keep time by themselves, and are periodically corrected by the master clock, or require impulses from the master clock to advance. Many slave clocks of these types remain in operation, most commonly in schools.\n\nMechanical slave clocks from the 1950s and 1960s era.\n\n"}
{"id": "5730094", "url": "https://en.wikipedia.org/wiki?curid=5730094", "title": "Strength (explosive)", "text": "Strength (explosive)\n\nIn explosive materials, strength is the parameter determining the ability of the explosive to move the surrounding material. It is related to the total gas yield of the reaction, and the amount of heat produced. \"Cf.\" brisance.\n\nThe strength, or \"potential\", of an explosive is the total work that can be performed by the gas resulting from its explosion, when expanded adiabatically from its original volume, until its pressure is reduced to atmospheric pressure and its temperature to 15°C. The potential is therefore the total quantity of heat given off at constant volume when expressed in equivalent work units and is a measure of the strength of the explosive.\n\nExplosive strength is measured by, for example, the Trauzl lead block test.\n\nAn explosion may occur under two general conditions: the first, unconfined, as in the open air where the pressure (atmospheric) is constant; the second, confined, as in a closed chamber where the volume is constant. The same amount of heat energy is liberated in each case, but in the unconfined explosion, a certain amount is used as work energy in pushing back the surrounding air, and therefore is lost as heat. In a confined explosion, where the explosive volume is small (such as occurs in the powder chamber of a firearm), practically all the heat of explosion is conserved as useful energy. If the quantity of heat liberated at constant volume under adiabatic conditions is calculated and converted from heat units to equivalent work units, the potential or capacity for work results.\n\nTherefore, if\n\nThen, because of the conversion of energy to work in the constant pressure case,\n\nfrom which the value of \"Q\" may be determined. Subsequently, the potential of a mole of an explosive may be calculated. Using this value, the potential for any other weight of explosive may be determined by simple proportion.\n\nUsing the principle of the initial and final state, and heat of formation table (resulting from experimental data), the heat released at constant pressure may be readily calculated.\n\nwhere:\n\nThe work energy expended by the gaseous products of detonation is expressed by:\n\nWith pressure constant and negligible initial volume, this expression reduces to:\n\nSince heats of formation are calculated for standard atmospheric pressure (101 325 Pa, where 1 Pa = 1 N/m²) and 15°C, V is the volume occupied by the product gases under these conditions. At this point\n\nand by applying the appropriate conversion factors, work can be converted to units of kilocalories.\n\nOnce the chemical reaction has been balanced, one can calculate the volume of gas produced and the work of expansion. With this completed, the calculations necessary to determine potential may be accomplished.\n\nFor TNT:\n\nfor 10 mol\n\nThen:\n\nNote: Elements in their natural state (H, O, N, C, etc.) are used as the basis for heat of formation tables and are assigned a value of zero. See table 12-2.\n\nAs previously stated, \"Q\" converted to equivalent work units is the potential of the explosive. (MW = Molecular Weight of Explosive)\n\nFor TNT,\n\nRather than tabulate such large numbers, in the field of explosives, TNT is taken as the standard explosive, and others are assigned strengths relative to that of TNT. The potential of TNT has been calculated above to be 2.72 × 10 J/kg. Relative strength (RS) may be expressed as\n"}
{"id": "19871327", "url": "https://en.wikipedia.org/wiki?curid=19871327", "title": "Telephonoscope", "text": "Telephonoscope\n\nA telephonoscope was an early concept of videophone and television, conceptualized in the late 1870s through the 1890s. It was mentioned in various early science fiction works such as \"Le Vingtième siècle. La vie électrique\" (\"The Twentieth Century: The Electrical Life\") and other works written by Albert Robida. It was also sketched in various cartoons by George du Maurier as a fictional invention by Thomas Edison including one made on December 9, 1878 in \"Punch\" magazine.\n\nRobida also forecast the era of television broadcasts, with concept art drawn of a televised opera performance, and of a live battlefield report.\n\n"}
{"id": "35268722", "url": "https://en.wikipedia.org/wiki?curid=35268722", "title": "The Partnership for Excellence", "text": "The Partnership for Excellence\n\nThe Partnership for Excellence (formerly the \"Ohio Partnership for Excellence\") is a nonprofit organization that administers a state-level award program for performance excellence in Ohio, Indiana, and West Virginia. The award is based on the Baldrige Criteria for Performance Excellence, which are developed and maintained by the Baldrige Performance Excellence Program, a program of the National Institute of Standards and Technology (NIST).\nThe Ohio Partnership for Excellence was incorporated as a 501c(3) nonprofit organization in 1998 to provide state level Baldrige assessment services and resources for interested organizations. In 2011, it was renamed The Partnership for Excellence (TPE) to reflect regional expansion that includes the states of Indiana and West Virginia.\n\nThe first award cycle was completed in 1999-2000.\n\nIn January 2008, Dr. Elaine D. Edgar was appointed to Executive Director. After retirement from the role in March 2011, she was replaced by Colonel Alfred C. Faber, Jr. who was appointed to the position of President/CEO.\n\n\nTPE is a membership organization composed of interested individuals and organizations throughout Ohio, Indiana, and West Virginia. TPE collects annual membership dues for the January to December membership year.\n\nTPE's Board of Examiners is composed of volunteer individuals who successfully complete annual training on the Baldrige Criteria for Performance Excellence. Advisors and examiners used during the awards process are drawn from the Board of Examiners.\n\nTPE’s main product is a feedback report to the partnering organizations that successfully complete the award process. The report contains strengths as well as opportunities for improvement (OFI) derived from both the submitted application and a site visit. This is a distinct difference from the national Baldrige Award process where only organizations with highly graded applications are granted a site visit.\n\nTPE also provides Baldrige-related educational services such as conferences, seminars, and direct partnering with individuals and organizations.\n\nTPE provides three options for organizations wanting to improve their organizational performance:\n\nOne or two advisors from TPE Board of Examiners meet with the leadership team of the partnering organization to review the Criteria for Performance Excellence and assist in developing a 5-page Organizational Profile. After review of the organizational profile, TPE provides the organization with a list of strengths and opportunities for improvement. Upon successful completion of this process, the organization receives a \"Spirit\" recognition award at TPE's annual \"Quest for Success\" conference.\n\nOne or two advisors from TPE Board of Examiners meet with the leadership team of the partnering organization to develop a 25-page application addressing the Baldrige Criteria for Performance Excellence at the \"overall\" level. After review of this application, TPE provides the organization with a list of strengths and opportunities for improvement. Upon successful completion of this process, the organization receives a \"Pioneer\" recognition award at TPE's annual \"Quest for Success\" conference.\n\nThis is the traditional 50-page application assessment mirroring the Malcolm Baldrige National Quality Award's assessment cycle. Organizations participating in this option submit an \"Intent to Apply\" in October before the application due date in December each year. TPE assigns a team of 7 members from the Board of Examiners to review (independently and then by consensus) the application before a 3-day site visit is conducted. TPE then provides the organization with a list of strength and opportunities for improvement. Based on the recommendations of the examiners, TPE's Board of Trustees recognizes each organization with a Bronze, Silver, Gold, or Platinum Award at the annual \"Quest for Success\" conference.\n\nNote:\n\n"}
{"id": "1254931", "url": "https://en.wikipedia.org/wiki?curid=1254931", "title": "The Way Things Work", "text": "The Way Things Work\n\nThe Way Things Work is a 1988 children's book by David Macaulay with technical text by Neil Ardley. It is an entertaining introduction to everyday machines, describing machines as simple as levers and gears and as complicated as radio telescopes and automatic transmissions. Every page consists primarily of one or more large diagrams describing the operation of the relevant machine. These diagrams are informative but playful, in that most show the machines operated, used upon, or represented by woolly mammoths, and are accompanied by anecdotes of the mammoths' (fictive) role in the operation. The book's concept was later developed into a short-lived animated TV show (produced by Millimages and distributed by Schlessinger Media), a Dorling Kindersley interactive CD-ROM, and a board game. A family \"ride\" involving animatronics and a 3-D film based on the book was one of the original attractions at the San Francisco Metreon, but closed in 2001.\n\nA newer version, \"The New Way Things Work\", (released on October 26, 1998,) contains additional text on the workings of computers and digital technology. It also lacks two pages of the first edition; both of which demonstrated the working of a mechanical coin-operated parking meter.\n\nA substantially revised edition, \"The Way Things Work Now\", has been published in October 2016 by Houghton Mifflin Harcourt and Dorling Kindersley.\n\nPart One—The Mechanics of Movement\nPart Two—Harnessing the Elements\nPart Three—Working with Waves\n\nPart Four—Electricity and Automation\nPart Five—The Digital Domain\nEureka!—The Invention of Machines\n\nTechnical Terms\n\nIndex\n\n"}
{"id": "2304286", "url": "https://en.wikipedia.org/wiki?curid=2304286", "title": "Uniden", "text": "Uniden\n\nUniden was established on February 7, 1966 by its founder Hidero Fujimoto as \"Uni Electronics Corp\". Uniden became a well-known brand in the 1970s by manufacturing and marketing millions of CB radios, under the Uniden brand as well as many for popular private brand labels such as Realistic, Clegg (amateur transceivers), Cobra, Craig, Fanon-Courier, Midland (only certain clone models, originals were made by Cybernet), President, Teaberry, Stalker, Super Star, Teledyne-Olson, Pearce Simpson, Regency, Robyn and many European brands such as Zodiac, Stabo and Inno-Hit. Uniden also marketed CB Radios in the UK under the Uniden and Uniace brands during the late 1970s.\n\nDuring the 1980s, Uniden grew to become the world's largest manufacturer of cordless telephones in addition to television satellite equipment, mobile radios, advanced marine electronics and radio scanners (the latter under brandname Bearcat).\n\nIn Europe, it became successful in the telecommunications market with its introduction of 900 MHz cordless telephones.\n\nAs Uniden continued to grow and extend its international operations, Uniden Australia and Uniden New Zealand were established in 1989. The company now offers an extensive range of consumer electronic products including Digital Cordless Phones and the top selling XDECT, SCR, WDECT and DSS technology Cordless phones, Transceiver Radios and Scanning Receivers, Wireless Networking Products and GPS Products. The company has just announced it will be selling bluetooth car kits, HD Set Top Boxes, Laser Pocket Projectors, HD Digital Photo Frame and Wireless Power Accessories (most of which have been withdrawn from market due to poor sales).\n\nMost cordless phones with the Radio Shack brand name are Uniden models that are sold under the Radio Shack brand. Most of the cordless phone brands sold in the world (e.g. Philips) are made by Uniden and sold under different electronic brands.\n\nUniden's actual line of business is in research and development, manufacturing and marketing.\n\nIt produces over two million wireless products every month and manufactures one cordless phone every 3.2 seconds.\n\nUniden operates globally, but the main commercial activities are still situated in the United States. Production sites have been located in China, Hong Kong, Taiwan and the Philippines. Uniden has also recently expanded its manufacturing to include Vietnam.\n\n\n"}
{"id": "17211010", "url": "https://en.wikipedia.org/wiki?curid=17211010", "title": "Unified theory of acceptance and use of technology", "text": "Unified theory of acceptance and use of technology\n\nThe unified theory of acceptance and use of technology (UTAUT) is a technology acceptance model formulated by Venkatesh and others in \"User acceptance of information technology: Toward a unified view\". The UTAUT aims to explain user intentions to use an information system and subsequent usage behavior. The theory holds that there are four key constructs: \n1) performance expectancy, \n2) effort expectancy, \n3) social influence, and \n4) facilitating conditions.\n\nThe first three are direct determinants of usage jon intention and behavior, and the fourth is a direct determinant of user behavior. Gender, age, experience, and voluntariness of use are posited to moderate the impact of the four key constructs on usage intention and behavior. The theory was developed through a review and consolidation of the constructs of eight models that earlier research had employed to explain information systems usage behaviour (theory of reasoned action, technology acceptance model, motivational model, theory of planned behavior, a combined theory of planned behavior/technology acceptance model, model of personal computer use, diffusion of innovations theory, and social cognitive theory). Subsequent validation by Venkatesh et al. (2003) of UTAUT in a longitudinal study found it to account for 70% of the variance in Behavioural Intention to Use (BI) and about 50% in actual use.\n\n\n\n\n\n"}
{"id": "5708497", "url": "https://en.wikipedia.org/wiki?curid=5708497", "title": "Wall stud", "text": "Wall stud\n\nA wall stud is a vertical framing member in a building's wall of smaller cross section than a post. They are a fundamental element in frame building.\n\n\"Stud\" is an ancient word related to similar words in Old English, Old Norse, Middle High German, and Old Teutonic generally meaning \"prop\" or \"support\". Other historical words with similar meaning are \"quarter\" and \"scantling\" (one sense meaning a smaller timber, not necessarily the same use). \"Stick\" is a colloquial term for both framing lumber (timber) and a \"timber tree\" (a tree trunk good for using as lumber (timber)). Thus the names \"stick and platform\", \"stick and frame\", \"stick and box\", or simply \"stick framing\". The stud height usually determines the ceiling height thus sayings like: \"...These rooms were usually high in stud...\"\n\nStuds form walls and may carry vertical structural loads or be non load-bearing such as in partition walls which only separate spaces. They hold in place the windows, doors, interior finish, exterior sheathing or siding,stud wall insulation and utilities and help give shape to a building. Studs run from sill plate to wall plate. In modern construction studs are fastened to the plates in a way, such as using ties, to prevent the building from being lifted off the foundation by severe wind or earthquake.\n\nStuds are usually slender so more studs are needed than in post and beam framing. Sometimes studs are long, as in balloon framing where the studs extend two stories and carry a ledger which carries joists. Balloon framing has been made illegal in new construction in many jurisdictions for fire safety reasons because the open wall cavities allow fire to quickly spread such as from a basement to an attic; the plates and platforms in platform framing providing an automatic fire stop inside the walls, and so are deemed much safer by fire safety officials. Being thinner and lighter, stick construction techniques are easier to cut and carry and is speedier than the timber framing.\n\nIn the United States and Canada, studs are traditionally made of wood, usually 2\"×4\" or 2\"×6\" dimensional lumber and typically placed from each other's center, but sometimes also at or . The wood needs to be dry when used or problems may occur as the studs shrink and twist as they dry out. Steel studs are gaining popularity, especially for non load-bearing walls, and are required in some firewalls.\n\nStuds used to frame around window and door openings are given different names, including\n\nA building technique mostly associated with Lincolnshire, England, and parts of Scotland gets part of its name from the studs: \"mud and stud\" (\"stud and mud\"). This building method uses studs in a framework which is then totally covered with mud which resembles the building material cob. Another traditional building method is called \"stud and plaster\" where the plaster walls are held by lath on the studs. Studs are also the namesake of a type of timber framing called close studding.\n\nBased on the American West Coast Lumber Inspection Bureau (WCLIB) grading rules, there is only one grade of stud: STUD. A stud is graded for vertical application and its stress requirements and allowable visual defects reflect that application. A stud is most similar to a #2 grade, which is held to a higher standard during grading. The biggest difference between the two is the frequency, placement and size of knots and overall allowable wane.\n\n"}
{"id": "50639220", "url": "https://en.wikipedia.org/wiki?curid=50639220", "title": "Wealthsimple", "text": "Wealthsimple\n\nWealthsimple Financial Inc. is a Canadian online investment management service focused on making \"investing easier for millennials.\" The firm was founded in September 2014 by Michael Katchen and is based in Toronto. As of February 2018, the firm had over in assets under management. It is primarily owned by Power Corporation indirectly at 77.4% through (the investments were through their holdings in Power Financial, IGM Financial and Portag3).\n\nPrior to founding Wealthsimple, Michael Katchen worked for 1000Memories, a Silicon Valley-based startup. After Ancestry.com bought 1000Memories in 2012, Katchen developed a spreadsheet with tips to help his colleagues set up investment portfolios. Interest in the spreadsheet helped inspire the idea for Wealthsimple. In 2014, he returned to Toronto to launch the company.\n\nIn December 2015, Wealthsimple made its first acquisition with acquiring Canada's first robo-advisor service, Canadian ShareOwner Investments Inc. Through the acquisition Wealthsimple became an owner of one of Canada's 14 discount brokerages (2015) alongside other owners of discount brokerages including Bank of Montreal and Royal Bank of Canada. The acquisition of Canadian ShareOwner Investment Inc. resulted in the assets under management comprising CAD $400,000,000 across 10,000 customer accounts.\n\nIn March 2016, the firm began offering clients access to socially responsible investment funds.\n\nIn May 2016, the firm announced a partnership with Mint, thus allowing clients to sync their Wealthsimple investment account to Mint's budgeting software.\n\nAlso in May 2016, the firm launched Wealthsimple for Advisors, an automated platform for financial advisors. The service is intended for advisors who wish to maintain clients with accounts below their minimum requirements.\n\nWealthsimple combines a robo-advisor platform with access to live advisors. Each client is provided an investment advisor who helps match investments to the client's long-term goals and risk tolerance. The firm does not occupy retail space; instead its advisors are available via phone, text message, email or video chat. There is no account minimum required and no charge per transaction. An annual fee is charged ranging from 0.4% to 0.5% based on account size. Portfolios are monitored daily and automatically rebalanced if they move beyond certain thresholds.\n\nIn April 2018, the company starting offering a savings account with a 1.7% interest rate.\n\nThe first round of raising money was in May 2014 raising CAD $1.9 million investors included Eric Kirzner, Joe Canavan, and Roger Martin.\n\nIn April 2015, the firm received CAD $10 million from Power Financial Corporation in an agreement structured to allow for a future investment of CAD $20 million within 12 months. In total Power Financial Corporation has invested $30 million in Series A funding. It is now primarily owned by Power Corporation indirectly at 77.4% through (the investments were through their holdings in Power Financial, IGM Financial and Portag3)\n\nAn annual fee is charged ranging from 0.35% to 0.5% based on account size.\n\nIn January 2017 CNBC reported that Wealthsimple launched a service for clients whose accounts have in excess of C$100,000 whereby their management fee is 0.4%.\n\nIn 2015, Product Hunt Toronto honored Wealthsimple with its first-ever Product of the Year Award. In 2016, the 20th Annual Webby Awards named Wealthsimple its Best Financial Services/Banking website.\n"}
{"id": "885509", "url": "https://en.wikipedia.org/wiki?curid=885509", "title": "Wide Field and Planetary Camera", "text": "Wide Field and Planetary Camera\n\nThe Wide Field/Planetary Camera (WFPC) (pronounced as wiffpick (Operators of the WFPC1 were known as \"whiff-pickers\")) was a camera installed on the Hubble Space Telescope until December 1993. It was one of the instruments on Hubble at launch, but its functionality was severely impaired by the defects of the main mirror optics which afflicted the telescope. However, it produced uniquely valuable high resolution images of relatively bright astronomical objects, allowing for a number of discoveries to be made by HST even in its aberrated condition.\n\nWFPC was proposed by James A. Westphal, a professor of planetary science at Caltech, and was designed, constructed, and managed by JPL. At the time it was proposed, 1976, CCDs had barely been used for astronomical imaging, though the first KH-11 KENNEN reconnaissance satellite equipped with CCDs for imaging was launched in December 1976. The high sensitivity offered such promise that many astronomers strongly argued that CCDs should be considered for Hubble Space Telescope instrumentation.\n\nThis first WFPC consisted of two separate cameras, each comprising 4 800x800 pixel Texas Instruments CCDs arranged to cover a contiguous field of view. The Wide Field camera had a 0.1 arcsecond pixel scale and was intended for the panoramic observations of faint sources at the cost of angular resolution. The Planetary Camera had a 0.043 arcsecond pixel scale and was intended for high-resolution observations. Selection between the two cameras was done with a four-facetted pyramid that rotated by 45 degrees.\n\nAs part of the corrective service mission (STS-61 in December 1993) the WFPC was swapped out for a replacement version. The Wide Field and Planetary Camera 2 improved on its predecessor and incorporated corrective optics needed to overcome the main mirror defect. To avoid potential confusion, the WFPC is now most commonly referred to as WFPC1.\n\nOn its return to Earth, the WFPC was disassembled and parts of it were used in Wide Field Camera 3, which was installed in Hubble on May 14, 2009 as part of Servicing Mission 4, replacing WFPC2.\n\nThe instrument had two different cameras within, the Wide Field and the Planetary Camera. The Wide Field camera had a wider field of view compared to the Planetary Camera.\n\n\nAlthough there was nothing known to be wrong with this instrument, the spherical aberration in HST's mirror severely limited the performance. \nWFPC was replaced by the Wide Field and Planetary Camera 2 which included its own internal corrective optics. WFPC2 was replaced by the Wide Field Camera 3 in 2009. After return to Earth, WFPC was disassembled and its parts cannibalized to make WFC3.\n\n\n"}
{"id": "57438775", "url": "https://en.wikipedia.org/wiki?curid=57438775", "title": "Xiaomi Smart Home", "text": "Xiaomi Smart Home\n\nThe following is a list of Xiaomi Smart Home Products. They are products released by third-party manufacturers who have partnered with Xiaomi. Mi Home is the only app that manages Xiaomi smart home products: it is used to manage and communicate with devices and connect these to the network and connect devices to each other, using Wifi and Bluetooth.\n\nXiaomi launched a smart blood pressure monitor on 24 September 2014, in partnership with iHealth Labs of Silicon Valley, USA. The accompanying app tracks blood pressure, heart rate, average pulse and other parameters on a real-time chart, then makes recommendations for improvement.\n\nXiaomi released the Mi Air Purifier on 9 December 2014 in Beijing. This can to clean up to 406 m of air in an hour. It uses HEPA technology to reduce polluting particles from as high as 600 ppm to 2.5 ppm. It has a real-time air quality monitor. Users can synch it with a smartphone to control it remotely, receive air quality data, and be alerted when the air filter is dirty.\n\nXiaomi released the Yi (also called Ants or Xiaoyi) Smart Webcam for CNY 149 on 29 October 2014. It has 720p resolution, a 111 wide-angle lens with 4x zoom, and the ability to make two-way voice calls. Activated and viewable via the smartphone, it doubles as both a webcam for chatting and a security camera with recording capabilities. The camera automatically records whenever it detects movement in view. In June 2015, Xiaomi launched a night vision edition of the Yi Camera with a 940 nm infrared sensor.\n\nXiaomi began selling the YI action camera for CNY 399, which was created by YI Technology. It comes with a 16 MP Sony IMX206CQC sensor which can record videos at 1080p60 and 720p120. It is waterproof up to 40 m with the waterproof case.\nSince it became very popular even outside China through its low price and same physical dimensions like products from Hero, there exists some user-made tweaks for features include shooting in RAW, customizable ISO and white balance. Compared to the popular GoPro brand of action cameras, Xiaomi's Yi motion camera has electronic motion stabilization, an integrated LCD display, dual microphones, bigger battery, slow motion mode and timer modes.\n\nThe Mi Smart Scale makes measurements ranging from 5 kg to 150 kg, with a 50 g precision, and shows weight in kg and lbs. When paired with the Mi Fit app, which itself pairs with the Mi Band, users can track their weight and BMI. The Mi Smart Scale is equipped with Bluetooth 4.0 and is compatible with both Android and iOS. The maker of the scale is Huami, a company Xiaomi invested in.\n\nThe Mi Water Purifier, announced on 16 July 2015. It and filters water by reverse osmosis. It can dispense 1,800 litres a day with a 1:1 water production rate. The device also pairs with a smartphone and sends data on filter effectiveness and reminders to change filters. The R&D of the device is done by Yunmi Technology, better known as Viomi, a subsidiary of the Mi Ecological System.\n\nXiaomi unveiled the Smart Home Kit for CNY 199 on 10 June 2015. The kit contains a set of smart sensors, including the multi-function gateway, the door/window sensor, the motion detector and the wireless switch, which can be combined to achieve over 30 different kinds of functions. For example, the motion detector can be paired with the gateway to perform functions such as switching on a light at night when it detects motion; the window sensors can start or stop a connected fan as windows are closed or opened; in another example, users can set the gateway into alert mode with their smartphones as they leave the house, and the gateway would then send push notifications and turn on the automatic recording of a Yi Smart Webcam when the door/window sensor or the motion detector detects abnormal activity.\n\nBeing a major investor in Ninebot, a Chinese company that acquired the self-balancing scooter manufacturer Segway, Xiaomi released a self-balancing scooter called Ninebot Mini for CNY 1,999 on 19 October 2015. It was released for Chinese market only. The scooter has a maximum speed of 16 kph (10 mph), a 15-degree incline climbing capability, a range of 22 km on a single charge, and a recharging time of three hours. It weighs 13 kilograms (28 lbs). Users can monitor speed, check vehicle condition, update firmware, receive theft alarms and remotely control the vehicle via a smartphone app. The scooter can learn and adapt to users' driving habits.\n\nFor European market Xiaomi released Ninebot Mini Pro, known as Segway miniPro in the US. This version has a slightly modified design and a different steering bar. Maximum speed of this scooter is 18.5 kph and a range is 30 km.\n\nLaunched on 29 March 2016. According to Xiaomi, the pressure rice cooker uses 1.2 bar pressure to raise the boiling point of water to around 105℃. Users can use their smartphones to scan the barcode on the rice packaging, with the cooker automatically adjusting its methodology to suit the particular rice type, brand, origin and user preferred softness. The companion app also includes options to cook steamed vegetables, crispy rice and cakes, and users can look up the recipe and set the corresponding heating method through the app.\nThe company had to deal with a PR crisis when people found a resemblance between the design of Xiaomi's rice cooker and an earlier product by MUJI. Lei had to respond in public to settle the disputes.\n\nOn 31 August 2016 the company announced the Xiaomi Mi Robot Vacuum. It is a robovac produced by Roborock and released under Xiaomi's platform. It has 12 sensors, including an ultrasonic radar, cliff radar, gyroscope and accelerometer. The vacuum is equipped with a 5200 mAh lithium ion battery which allows it to vacuum for 2.5 hours on one charge. This covers 250 m (aprox. 2700 square feet) of floor space.\n\nRoborock S5 is the second robovac released by Roborock and it supports mopping and laser mapping for zone cleaning (through the Mi Home app and Wifi).\n\nThe Mi WiFi+ is a wireless repeater announced by Xiaomi in 2015.\n\nOn 15 March 2018, Xiaomi launched the Body Composition Scale in India. The scale measures weight as well as muscle mass, fat, and BMI.\n\n"}
