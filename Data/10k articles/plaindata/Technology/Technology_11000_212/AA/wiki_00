{"id": "1794901", "url": "https://en.wikipedia.org/wiki?curid=1794901", "title": "Accelerograph", "text": "Accelerograph\n\nAn accelerograph can be referred to as a strong-motion instrument or seismograph, or simply an earthquake accelerometer. They are usually constructed as a self-contained box, which previously included a paper or film recorder (an analogue instrument) but now they often record directly on digital media and then the data is transmitted via the Internet.\n\nAccelerographs are useful for when the earthquake ground motion is so strong that it causes the more sensitive seismometers to go off-scale. There is an entire science of strong ground motion, that is dedicated to studying the shaking in the vicinity of earthquakes (roughly within about 100 km of the fault rupture).\n\nAccelerographs record the acceleration of the ground with respect to time. This recording is often called an accelerograms, strong-motion record or acceleration time-history. From this record strong-motion intensity measures (IMs, also called parameters) can be computed. The simplest of which is peak ground acceleration (PGA). Other IMs include Arias intensity, peak ground velocity (PGV), for which the accelerogram needs to be integrated once, peak ground displacement (PGD), for which double integration is required. Often a response spectrum is computed to show how the earthquake would affect structures of different natural frequencies or periods. These observations are useful to assess the seismic hazard of an area.\n\nAs well as their engineering applications, accelerograms are also useful for the study earthquakes from a scientific viewpoint. For example, accelerograms can be used to reconstruct the detailed history of rupture along a fault during an earthquake, which would not be possible with seismograms from standard instruments because they would be too far away to resolve the details. An example of an accelerograph array that was established to improve knowledge of near-source earthquake shaking as well as earthquake rupture propagation is the Parkfield Experiment, which involved a massive set of strong motion instrumentation.\n\nWithin the accelerograph, there is an arrangement of three accelerometer sensing heads. In recent low-cost instruments these are usually micro-machined (MEMS) chips that are sensitive to one direction. Thus constructed, the accelerometer can measure full motion of the device in three dimensions.\n\nUnlike the continually recording seismometer, accelerometers nearly always work in a triggered mode. That means a level of acceleration must be set which starts the recording process. For analogue and older digital instruments this makes maintenance much more difficult without a direct Internet connection (or some other means of communication). Many trips have been made to accelerometers after a large earthquake, only to find that the memory was filled with extraneous noise, or the instrument was malfunctioning.\n\nAccelerometers are used to monitor the response of structures to earthquakes. Analysis of these records along with the shaking recorded at base of the structure can improve building design, through earthquake engineering.\n"}
{"id": "33327095", "url": "https://en.wikipedia.org/wiki?curid=33327095", "title": "Applied Food Technologies", "text": "Applied Food Technologies\n\nApplied Food Technologies, Inc. (AFT) is a privately held corporation in Alachua, Florida that develops diagnostics needed in the seafood industry and runs fee-for-service species identification and verification programs. The DNA-based species identification diagnostics developed by AFT are used in-house and are also packaged in a kit format and sold to federal, state and private laboratories in the US, Europe and Asia. AFT has established an extensive library of taxonomically verified fish species and has developed DNA standards for these fish.\n\nApplied Food Technologies is primarily a research and development company focusing on fish species identification using DNA and environmental contaminant detection using gene expression in sentinel organisms. AFT also offers a fee-for-service business for seafood species identification.\n\nApplied Food Technologies was showcased on a national broadcast of \"The Early Show\" on CBS on June 8, 2011 to discuss seafood mislabeling after release of a national publication on the issue by consumer advocate group Oceana and a subsequent \"New York Times\" publication on the subject. Although in business for several years, Applied Food Technologies came to the national attention after becoming the first company with DNA-based fish species identification methods recognized by the FDA to test all catfish imported from China in the late 2000s. Applied Food Technologies was subsequently interviewed in May, 2010, by ABC affiliate WCJB-TV. Applied Food Technologies was showcased in University of Florida’s \"Explore\" magazine Fall 2011, issue. \n\nAFT has been criticized as being \"in industry's pocket\" because AFT has been a loud voice in support of industry against media reports of mislabeling. AFT claims their internal testing has shown a much lower mislabeling rate than media reports. Although Applied Food Technologies has been included in numerous media reports concerning seafood species mislabeling and has been contacted to conduct testing for the media many times, AFT does not perform species identification testing for the media, which may be the cause of some criticism. AFT's mission statement includes offering a testing service to the seafood industry utilizing the \"best available science\" for the purpose of improving the industry to better serve the consumer. Typically the media outlets are unable to meet the chain of custody requirements within the \"best available science\" component, which is one of the principal reasons AFT does not offer testing to the media.\n\nApplied Food Technologies offers fee-for-service business in several areas including fish species identification (also known as Fish ID), seafood net weight, and antibiotic residue testing, which are described in detail below.\n\nEven with the plethora of laws and regulations protecting consumers from mislabeling of seafood, enforcing compliance has been challenging because of the unique nature of seafood itself. Unlike cattle, poultry, pork, crops, or other land-based food sources, the \"farms\" on which wild seafood is grown cannot easily be inspected. LeeAnn Applewhite realized in 2000 that modern technology could be used to solve this problem, was awarded several USDA SBIR research awards to develop the tools, and founded AFT to create a solution.\n\nApplied Food Technologies maintains an internal DNA database generated from a collection of economically important seafood specimens, which were taxonomically identified by third-party institutions, such as the Smithsonian and the Florida Museum. These taxonomically validated specimens were sequenced in multiple regions to create a database of unique sequences capable of correctly identifying and distinguishing different seafood species.\" Because only species identification testing that compares the sequence to a validated reference meets the current FDA guidelines for species identification, any lab not using a validated reference does not meet the FDA requirement. AFT's Applewhite says, \"Using a public database to determine a fish species is not very useful because the data is only as accurate as the least careful person submitting sequences, thus, the DNA sequences for common substitutes can also appear in the database under the wrong name.\" FDA’s Stephanie Yao agrees, \"Most other labs are pulling publicly available sequences off of the Internet to make their identifications, a practice FDA does not recommend for regulatory decisions.\" FDA's Yao continued, AFT \"often runs samples for importers whose shipments are being held by the FDA and the FDA has released some of those shipments based on AFT’s results.\"\n\nApplied Food Technologies CEO, LeeAnn Applewhite, incorporated APL Sciences in 1997 after inventing a seafood test kit and licensing the technology to Neogen Corporation. APL Sciences received five USDA Small Business Innovation Research (SBIR) Awards totaling approximately $1,000,000.00. AFT was formed in 2003 in Blacksburg, Virginia, by LeeAnn Applewhite and Maureen Dolan to be the fee-for-service testing laboratory and manufacturing and marketing arms for APL Science, Inc. AFT has also been awarded several SBIR grants and industry research contracts including most recently a grant to develop a method for species identification for the U.S. shrimp industry.\n\n\nToday, AFT focuses on research and development work, which constitutes a majority of AFT revenues. AFT offers contract research services in its product portfolio. After the acquisition of EcoArray assets, AFT entered into the environmental water testing market. EcoArray’s core proprietary technology offers a comparison of fish gene expression with and without a certain environmental toxin to determine if the toxin is present in the water. AFT is also branching out into species identification testing in a number of new areas.\n\n"}
{"id": "4375705", "url": "https://en.wikipedia.org/wiki?curid=4375705", "title": "Association for Information Systems", "text": "Association for Information Systems\n\nThe Association for Information Systems (AIS) is an international, not-for-profit, professional association with the stated mission to serve society through the advancement of knowledge and the promotion of excellence in the practice and study of information systems. Membership is made up primarily of academic educators, researchers and institutions that specialize in information systems (IS) development, implementation and evaluation. The association has members in more than 90 countries, and is led by a president who is annually elected from one of three world regions—the Americas, Europe and Africa and Asia-Pacific—on a rotating basis. The governing Council is made up of elected functional vice-presidents and other officers and council members who are elected in the three world regions. The association organizes two annual conferences for IS researchers, educators and students: The International Conference on Information Systems (I.C.I.S.), which alternates between the three world regions and the Americas Conference For Information Systems (AMCIS), which is located at different sites in North, Central and South America. The Association publishes academic journals including:\nAffiliated journals include:\nBoth AIS published titles and affiliated journals are included in the AIS eLibrary, which is accessible as a benefit of membership. \n\nSince 1999 the AIS annually grants the Leo Award to one or more persons, who have made exceptional contributions to the research and practice of Information Systems. Award recipient have been:\n\n"}
{"id": "188578", "url": "https://en.wikipedia.org/wiki?curid=188578", "title": "Betamax", "text": "Betamax\n\nBetamax (also called Beta, as in its logo) is a consumer-level analog-recording and cassette format of magnetic tape for video. It was developed by Sony and was released in Japan on May 10, 1975. The first Betamax device introduced in the United States was the LV-1901 console, which included a color monitor, and appeared in stores in early November 1975. The cassettes contain videotape in a design similar to that of the earlier, professional , U-matic format. Betamax is obsolete, having lost the videotape format war to VHS. Production of Betamax recorders ceased in 2002; new Betamax cassettes were available until March 2016, when Sony stopped making and selling them.\n\nLike the rival videotape format VHS (introduced in Japan by JVC in October 1976 and in the United States by RCA in August 1977), Betamax has no guard band and uses azimuth recording to reduce crosstalk. According to Sony's history webpages, the name had a double meaning: \"beta\" is the Japanese word used to describe the way in which signals are recorded on the tape; and the shape of the lowercase Greek letter beta (β) resembles the course of the tape through the transport. The suffix \"-max\", from the word \"maximum\", was added to suggest greatness.\nIn 1977, Sony issued the first long-play Betamax VCR, the SL-8200. This VCR had two recording speeds: normal, and the newer half speed. This provided two hours' recording on\nthe L-500 Beta videocassette. The SL-8200 was to compete against the VHS VCRs, which allowed up to 4, and later 6 and 8, hours of recording on one cassette.\n\nSanyo marketed a version as \"Betacord\", which also was casually called \"Beta\". In addition to Sony and Sanyo, Beta-format video recorders were manufactured and sold by Toshiba, Pioneer, Murphy, Aiwa, and NEC. The Zenith Electronics Corporation and WEGA Corporations contracted with Sony to produce VCRs for their product lines. The department stores Sears (in the United States and Canada) and Quelle (in Germany) sold Beta-format VCRs under their house brands, as did the RadioShack chain of electronic stores. Betamax and VHS competed in a fierce format war, which saw VHS win in most markets.\n\nOne other major consequence of the Betamax technology's introduction to the U.S. was the lawsuit \"Sony Corp. v. Universal City Studios\" (1984, the \"Betamax case\"), with the U.S. Supreme Court determining home videotaping to be legal in the United States, wherein home videotape cassette recorders were a legal technology since they had substantial noninfringing uses. This precedent was later invoked in \"MGM v. Grokster\" (2005), where the high court agreed that the same \"substantial noninfringing uses\" standard applies to authors and vendors of peer-to-peer file sharing software (notably excepting those who \"actively induce\" copyright infringement through \"purposeful, culpable expression and conduct\").\n\nFor the professional and broadcast video industry, Sony derived Betacam from Betamax. Released in 1982, Betacam became the most widely used videotape format in ENG (electronic news gathering), replacing the wide U-matic tape format. Betacam and Betamax are similar in some ways: they use the same videocassette shape, use the same oxide tape formulation with the same coercivity, and record linear audio tracks in the same location of the tape. But in the key area of video recording, Betacam and Betamax are completely different. (For details, see the Betacam article.)\n\nSony also offered a range of industrial Betamax products, a Beta I-only format for industrial and institutional users. These were aimed at the same market as U-Matic equipment, but were cheaper and smaller. The arrival of Betacam reduced the demand for both industrial Beta and U-Matic equipment.\nBetamax also had a significant part to play in the music recording industry, when Sony introduced its PCM (Pulse Code Modulation) digital recording system as an encoding box/PCM adaptor that connected to a Betamax recorder. The Sony PCM-F1 adaptor was sold with a companion Betamax VCR SL-2000 as a portable digital audio recording system. Many recording engineers used this system in the 1980s and 1990s to make their first digital master recordings.\n\nInitially, Sony was able to tout several Betamax-only features, such as BetaScan—a high speed picture search in either direction—and BetaSkipScan, a technique that allowed the operator to see where he was on the tape by pressing the FF key (or REW, if in that mode): the transport would switch into the BetaScan mode until the key was released. This feature is discussed in more detail on Peep Search. Sony believed that the M-Load transports used by VHS machines made copying these trick modes impossible. BetaSkipScan (Peep Search) is now available on miniature M-load formats, but even Sony was unable to fully replicate this on VHS. BetaScan was originally called \"Videola\" until the company that made the Moviola threatened legal action.\n\nSony would also sell a BetaPak, a small deck designed to be used with a camera. Concerned with the need for several pieces and cables to connect them, an integrated camera/recorder was designed, which Sony dubbed a \"Camcorder\"; the result was Betamovie. Betamovie used the standard-size cassette, but with a modified transport. The tape was wrapped 300° around a smaller, -diameter head drum, with a single dual-azimuth head to write the video tracks. For playback, the tape would be inserted into a Beta-format deck. Due to the different geometry and writing techniques employed, playback within the camcorder was not feasible. SuperBeta and industrial Betamovie camcorders would also be sold by Sony.\n\nIn June 1983, Sony introduced high fidelity audio to videotape as Beta Hi-Fi. For NTSC, Beta HiFi worked by placing a pair of FM carriers between the chroma (C) and luminance (Y) carriers, a process known as frequency multiplexing. Each head had a specific pair of carriers; in total, four individual channels were employed. Head A recorded its hi-fi carriers at 1.38(L) and 1.68(R) MHz, and the B head employed 1.53 and 1.83 MHz. The result was audio with an 80 dB dynamic range, with less than 0.005% wow and flutter.\n\nPrior to the introduction of Beta Hi-Fi, Sony shifted the Y carrier up by 400 kHz to make room for the four FM carriers that would be needed for Beta Hi-Fi. All Beta machines incorporated this change, plus the ability to hunt for a lower frequency pre-AFM Y carrier. Sony incorporated an \"antihunt\" circuit, to stop the machine hunting for a Y carrier that wasn't there.\n\nSome Sony NTSC models were marketed as \"Hi-Fi Ready\" (with an SL-HFR prefix to the model's number instead of the usual SL or SL-HF). These Betamax decks looked like a regular Betamax model, except for a special 28-pin connector on the rear. If the user desired a Beta Hi-Fi model but lacked the funds at the time, he could purchase an \"SL-HFRxx\" and at a later date purchase the separate Hi-Fi Processor. Sony offered two outboard Beta Hi-Fi processors, the HFP-100 and HFP-200. They were identical except that the HFP-200 was capable of multi-channel TV sound, with the word \"stereocast\" printed after the Beta Hi-Fi logo. This was possible because unlike a VHS Hi-Fi deck, an NTSC Betamax didn't need an extra pair of heads. The HFP-x00 would generate the needed carriers which would be recorded by the attached deck, and during playback the AFM carriers would be passed to the HFP-x00. They also had a small \"fine tracking\" control on the rear panel for difficult tapes.\n\nFor PAL, however, the bandwidth between the chroma and luminance carriers was not sufficient to allow additional FM carriers, so depth multiplexing was employed, wherein the audio track would be recorded in the same way that the video track was. The lower-frequency audio track was written first by a dedicated head, and the video track recorded on top by the video head. The head disk had an extra pair of audio-only heads with a different azimuth, positioned slightly ahead of the regular video heads, for this purpose.\n\nSony was confident that VHS could not achieve the same audio performance feat as Beta Hi-Fi. However, to the chagrin of Sony, JVC did develop a VHS hi-fi system on the principle of depth multiplexing approximately a year after the first Beta Hi-Fi VCR, the SL-5200, was introduced by Sony. Despite initial praise as providing \"CD sound quality\", both Beta Hi-Fi and VHS HiFi suffered from \"carrier buzz\", where high frequency information bled into the audio carriers, creating momentary \"buzzing\" and other audio flaws. Both systems also used companding noise-reduction systems, which could create \"pumping\" artifacts under some conditions. Both formats also suffered from interchange problems, where tapes made on one machine did not always play back well on other machines. When this happened and if the artifacts became too distracting, users were forced to revert to the old linear soundtrack.\n\nIn early 1985, Sony would introduce a new feature, High Band or SuperBeta, by again shifting the Y carrier—this time by 800 kHz. This improved the bandwidth available to the Y sideband and increased the horizontal resolution from 240 to 290 lines on a regular-grade Betamax cassette. Since over-the-antenna and cable signals were only 300–330 lines resolution, SuperBeta could make a nearly identical copy of live television. However, the chroma resolution still remained relatively poor, limited to just under 0.4 MHz or approximately 30 lines resolution, whereas live broadcast chrominance resolution was over 100 lines. The heads were also narrowed to 29 μm to reduce crosstalk, with a narrower head gap to play back the higher carrier frequency at 5.6 MHz. Later, some models would feature further improvement, in the form of Beta-Is, a high band version of the Beta-I recording mode. There were some incompatibilities between the older Beta decks and SuperBeta, but most could play back a high band tape without major problems. SuperBeta decks had a switch to disable the SuperBeta mode for compatibility purposes. (SuperBeta was only marginally supported outside of Sony, as many licensees had already discontinued their Betamax line.)\n\nIn 1988, Sony would again push the envelope with ED Beta, or \"Extended Definition\" Betamax, capable of up to 500 lines of resolution, that equaled DVD quality (480 typical). In order to store the ~6.5 MHz-wide luma signal, with the peak frequency at 9.3 MHz, Sony used a metal formulation tape borrowed from the Betacam SP format (branded \"ED-Metal\") and incorporated some improvements to the transport to reduce mechanically induced aberrations in the picture. Beta ED also featured a luminance carrier deviation of 2.5 MHz, as opposed to the 1.2 MHz used in SuperBeta, improving contrast with reduced luminance noise.\n\nSony introduced two ED decks and a camcorder in the late 1980s. The top end EDV-9500 (EDV-9300 in Canada) deck was a very capable editing deck, rivaling much more expensive U-Matic set-ups for its accuracy and features, but did not have commercial success due to lack of timecode and other pro features. Sony did market Beta ED to \"semiprofessional\" users, or \"prosumers\". One complaint about the EDC-55 ED CAM was that it needed a lot of light (at least 25 lux), due to the use of two CCDs instead of the typical single-CCD imaging device. The Beta ED lineup only recorded in BII/BIII modes, with the ability to play back BI/BIs.\n\nDespite the sharp decline in sales of Betamax recorders in the late 1980s and subsequent halt in production of new recorders by Sony in 2002, Betamax, SuperBetamax and EDBeta are still being used by a small number of people. Even though Sony stopped making new cassettes in 2016, new cassettes are still available for purchase at online shops and used recorders are often found at flea markets, thrift stores or on Internet auction sites. Early format BetaCam cassettes—which are physically based on the Betamax cassette—continue to be available for use in the professional media.\n\nBelow is a list of modern, digital-style resolutions (and traditional analog \"TV lines per picture height\" measurements) for various media. The list only includes popular formats. Note that listed resolution applies to luminance only, with chroma resolution usually halved in each dimension for digital formats, and significantly lower for analog formats.\n\nEquivalent pixel resolutions are calculated from analog line resolution numbers:\n\nThe somewhat unintuitive analog resolution loss for 16:9 DVD compared to 4:3 DVD arises from the fact that analog resolution unit is \"lines per picture height\". When picture height is kept the same, the same 720 pixels are spread to a wider area in 16:9, hence lower horizontal resolution per picture height.\n\nBoth NTSC and PAL/SECAM Betamax cassettes are physically identical (although the signals recorded on the tape are incompatible). However, as tape speeds differ between NTSC and PAL/SECAM, the playing time for any given cassette will vary accordingly between the systems. Other unusual lengths were produced from time to time, such as L-410.\n\n\nTwo-piece camera/VCR systems rapidly displaced Super 8 mm film as the medium of choice for shooting home movies and amateur films. These units included a portable VCR, which the videographer would carry by a shoulder strap, and a separate camera, which was connected to the VCR by a special cable. At this point, Beta had several advantages over VHS systems. The smaller Beta cassette made for smaller and lighter VCRs.\n\nHowever, consumers wanted a one-piece solution. The first one-piece consumer camcorder, the Betamovie, came from Sony. A major requirement for a one-piece camcorder was miniaturizing the recording head drum, and Sony's solution to this involved a nonstandard video signal which would become standard only when played back on full-sized VCRs. A side effect of this was that Beta camcorders were record-only: consumers saw this as a major limitation.\n\nVHS manufacturers found a better solution to drum miniaturization (it involved four heads doing the work of two). Because it used standard video signals, VHS camcorders could review footage in the camcorder and copy to another VCR for editing. This shifted the home movie advantage dramatically away from Beta, and was a primary reason for the loss of Beta market share: owners of Beta VCRs found that a VHS camcorder would allow them to copy and edit footage to their Beta deck – something that Betamovie could not do. If rental movies were not available in Beta, they could rent them in VHS and use their camcorder to play them. Owners of VHS VCRs could also choose a variant camcorder format called VHS-C. This used a miniaturized cassette to make a camcorder smaller and lighter than any Betamovie.\n\nSony could not duplicate the functionality of VHS-C camcorders, and seeing the rapid loss of market share, eventually introduced the Video8 format. Their hope was that Video8 could replace both Beta and VHS for all uses. For more information, see the article on camcorders.\n\nOn November 10, 2015, Sony announced that it would no longer be producing Betamax video cassettes. Production and sales ended March 2016 after nearly 41 years of continuous production. Third party manufacturers continue to make new cassettes. While these cassettes are designed for use with the Betacam format, the cassettes are interchangeable with traditional Betamax systems.\n\nThe VHS format's defeat of the Betamax format became a classic marketing case study. Sony's attempt to dictate an industry standard backfired when JVC made the tactical decision to forgo Sony's offer of Betamax in favor of developing its own technology. JVC felt that accepting Sony's offer would yield results similar to the U-Matic deal, with Sony dominating.\n\nBy 1980, JVC's VHS format controlled 60% of the North American market. The large economy of scale allowed VHS units to be introduced to the European market at a far lower cost than the rarer Betamax units. In the United Kingdom, Betamax held a 25% market share in 1981, but by 1986, it was down to 7.5% and continued to decline further. By 1984, 40 companies made VHS format equipment in comparison with Beta's 12. Sony finally conceded defeat in 1988 when it, too, began producing VHS recorders (early models were made by Hitachi), though it still continued to produce Betamax recorders until 2002.\n\nIn Japan, Betamax had more success and eventually evolved into Extended Definition Betamax, with 500+ lines of resolution.\n\n\n"}
{"id": "50590211", "url": "https://en.wikipedia.org/wiki?curid=50590211", "title": "Bioremediation of radioactive waste", "text": "Bioremediation of radioactive waste\n\nBioremediation of radioactive waste or bioremediation of radionuclides is an application of bioremediation based on the use of biological agents bacteria, plants and fungi (natural or genetically modified) to catalyze chemical reactions that allow the decontamination of sites affected by radionuclides. These radioactive particles are by-products generated as a result of activities related to nuclear energy and constitute a pollution and a radiotoxicity problem (with serious health and ecological consequences) due to its unstable nature of ionizing radiation emissions.\n\nThe techniques of bioremediation of environmental areas as soil, water and sediments contaminated by radionuclides are diverse and currently being set up as an ecological and economic alternative to traditional procedures. Physico-chemical conventional strategies are based on the extraction of waste by excavating and drilling, with a subsequent long-range transport for their final confinement. These works and transport have often unacceptable estimated costs of operation that could exceed a trillion dollars in the US and 50 million pounds in the UK.\n\nThe species involved in these processes have the ability to influence the properties of radionuclides such as solubility, bioavailability and mobility to accelerate its stabilization. Its action is largely influenced by electron donors and acceptors, nutrient medium, complexation of radioactive particles with the material and environmental factors. These are measures that can be performed on the source of contamination (\"in situ\") or in controlled and limited facilities in order to follow the biological process more accurately and combine it with other systems (\"ex situ\").\n\nThe presence of radioactive waste in the environment may cause long-term effects due to the activity and half-life of the radionuclides, leading their impact to grow with time. These particles exist in various oxidation states and are found as oxides, coprecipitates, or as organic or inorganic complexes, according to their origin and ways of liberation. Most commonly they are found in oxidized form, which makes them more soluble in water and thus more mobile. Unlike organic contaminants, however, they cannot be destroyed and must be converted into a stable form or extracted from the environment.\n\nThe sources of radioactivity are not exclusive of human activity. The natural radioactivity does not come from human sources: it covers up to ¾ of the total radioactivity in the world and has its origins in the interaction of terrestrial elements with high energy cosmic rays (cosmogenic radionuclides) or in the existing materials on Earth since its formation (primordial radionuclides). In this regard, there are differences in the levels of radioactivity throughout the Earth's crust. India and mountains like the Alps are among the areas with the highest level of natural radioactivity due to their composition of rocks and sand.\n\nThe most frequent radionuclides in soils are naturally radium-226 (Ra), radon-222 (Rn), thorium-232 (Th), uranium-238 (U) and potassium-40 (K). Potassium-40 (up to 88% of total activity), carbon-14 (C), radium-226, uranium-238 and rubidium-87 (Rb) are found in ocean waters. Moreover, in groundwater abound radius radioisotopes such as radium-226 and radium-228 (Ra). They are also habitual in building materials radionuclides of uranium, thorium and potassium (the latter common to wood).\n\nAt the same time, anthropogenic radionuclides (caused by humans) are due to thermonuclear reactions resulting from explosions and nuclear weapons tests, discharges from nuclear facilities, accidents deriving from the reprocessing of commercial fuel, waste storage from these processes and to a lesser extent, nuclear medicine. Some polluted sites by these radionuclides are the US DOE facilities (like Hanford Site), the Chernobyl and Fukushima exclusion zones and the affected area of Chelyabinsk Oblast due to the Kyshtym disaster.\n\nIn ocean waters, the presence of tritium (H), cesium-137 (Cs), strontium-90 (Sr), plutonium-239 (Pu) and plutonium-240 (Pu) has significantly increased due to anthropogenic causes. In soils, technetium-99 (Tc), carbon-14, strontium-90, cobalt-60 (Co), iodine-129 (I), iodine-131 (I), americium-241 (Am), neptunium-237 (Np) and various forms of radioactive plutonium and uranium are the most common radionuclides.\n\nThe classification of radioactive waste established by the International Atomic Energy Agency (IAEA) distinguishes six levels according to equivalent dose, specific activity, heat released and half-life of the radionuclides:\n\n\nRadioactive contamination is a potential danger for living organisms and results in external hazards, concerning radiation sources outside the body, and internal dangers, as a result of the incorporation of radionuclides inside the body (often by inhalation of particles or ingestion of contaminated food).\n\nIn humans, single doses from 0.25 Sv produce first anomalies in the amount of leukocytes. This effect is accentuated if the absorbed dose is between 0.5 and 2 Sv, in whose first damage, nausea and hair loss are suffered. The strip ranging between 2 and 5 Sv is considered the most serious and include bleeding, ulcers and risk of death; values exceeding 5 Sv involve immediate death. If radiation, likewise, is received in small doses over long periods of time, the consequences can be equally severe. It is difficult to quantify the health effects for doses below 10 mSv, but it has been shown that there is a direct relationship between prolonged exposure and cancer risk (although there is not a very clear dose-response relationship to establish clear limits of exposure).\n\nThe information available on the effect of natural background radiation with respect anthropogenic pollution on wildlife is scarce and refers to very few species. It is very difficult to estimate from the available data the total doses that can accumulate during specific stages of the life cycle (embryonic development or reproductive age), in changes in behavior or depending on environmental factors such as seasonality. The phenomena of radioactive bioaccumulation, bioconcentration and biomagnification, however, are especially known to sea level. They are caused by the recruitment and retention of radioisotopes by bivalves, crustaceans, corals and phytoplankton, which then amounted to the rest of the food chain at low concentration factors.\n\nRadiobiological literature and IAEA establish a safe limit of absorbed dose of 0.001 Gy/d for terrestrial animals and 0.01 Gy/d for plants and marine biota, although this limit should be reconsidered for long-lived species with low reproductive capacity.\n\nRadiation tests in model organisms that determine the effects of high radiation on animals and plants are:\n\n\nThe effects of radioactivity on bacteria are given, as in eukaryotes, by ionization of water and production of reactive oxygen species. These compounds mutate DNA strands and produce genetic damage, inducing newly lysis and subsequent cell death.\n\nIts action on viruses, on the other hand, results in damaged nucleic acids and viral inactivation. They have a sensory threshold ranging between 1000 and 10,000 Gy (range occupying most biological organisms) which decreases with increasing genome size.\n\nThe biochemical transformation of radionuclides into stable isotopes by bacterial species significantly differs from the metabolism of organic compounds coming from carbon sources. They are highly energetic radioactive forms which can be converted indirectly by the process of microbial energy transfer.\n\nRadioisotopes can be transformed directly through changes in valence state by acting as acceptors or by acting as cofactors to enzymes. They can also be transformed indirectly by reducing and oxidizing agents produced by microorganisms that cause changes in pH or redox potential. Other processes include precipitation and complexation of surfactants, or chelating agents that bind to radioactive elements. Human intervention, on the other hand, can improve these processes through genetic engineering and omics, or by injection of microorganisms or nutrients into the treatment area.\n\nAccording to the radioactive element and the specific site conditions, bacteria can enzymatically immobilize radionuclides directly or indirectly. Their redox potential is exploited by some microbial species to carry out reductions that alter the solubility and hence, mobility, bioavailability and radiotoxicity. This waste treatment technique called bioreduction or enzymatic biotransformation is very attractive because it can be done in mild conditions for the environment, does not produce hazardous secondary waste and has potential as a solution for waste of various kinds.\n\nDirect enzymatic reduction is the change of radionuclides of a higher oxidation state to a lower one made by facultative and obligate anaerobes. The radioisotope interact with binding sites of metabolically active cells and is used as terminal electron acceptor in the electron transport chain where compounds such as ethyl lactate act as electron donors under anaerobic respiration.\n\nThe periplasm plays a very important role in these bioreductions. In the reduction of uranium (VI) to insoluble uranium (IV), made by \"Shewanella putrefaciens\", \"Desulfovibrio vulgaris\", \"Desulfovibrio desulfuricans\" and \"Geobacter sulfurreducens\", the activity of periplasmic cytochromes is required. The reduction of technetium (VII) to technetium (IV) made by \"S. putrefaciens\", \"G. sulfurreducens\", \"D. desulfuricans\", \"Geobacter metallireducens\" and \"Escherichia coli\", on the other hand, requires the presence of the complex formate hydrogenlyase, also placed in this cell compartment.\n\nOther radioactive actinides such as thorium, plutonium, neptunium and americium are enzymatically reduced by \"Rhodoferax ferrireducens\", \"S. putrefaciens\" and several species of \"Geobacter\", and directly form an insoluble mineral phase.\n\nThe phenomenon of indirect enzymatic reduction is carried out by sulfate-reducing and dissimilatory metal-reducing bacteria on excretion reactions of metabolites and breakdown products. There is a coupling of the oxidation of organic acids —produced by the excretion of these heterotrophic bacteria— with the reduction of iron or other metals and radionuclides, which forms insoluble compounds that can precipitate as oxide and hydroxide minerals. In the case of sulfate-reducing bacteria hydrogen sulfide is produced, promoting increased solubility of polluting radionuclides and their bioleaching (as liquid waste that can then be recovered).\n\nThere are several species of reducing microorganisms that produce indirect sequestering agents and specific chelators, such as siderophores. These sequestering agents are crucial in the complexation of radionuclides and increasing their solubility and bioavailability. \"Microbacterium flavescens\", for example, grows in the presence of radioisotopes such as plutonium, thorium, uranium or americium and produces organic acids and siderophores that allow the dissolution and mobilization of radionuclides through the soil. It seems that siderophores on bacterial surface could also facilitate the entry of these elements within the cell as well. \"Pseudomonas aeruginosa\" also secretes chelating agents out that meet uranium and thorium when grown in a medium with these elements. In general, it has also been found that enterobactin siderophores are extremely effective in solubilizing actinide oxides of plutonium.\n\nCitrate is a chelator which binds to certain transition metals and radioactive actinides. Stable complexes such as bidentate, tridentate (ligands with more than one atom bound) and polynuclear complexes (with several radioactive atoms) can be formed with citrate and radionuclides, which receive a microbial action. Anaerobically, \"Desulfovibrio desulfuricans\" and species of the genera \"Shewanella\" and \"Clostridium\" are able to reduce bidentate complexes of uranyl-citrate (VI) to uranyl-citrate (IV) and make them precipitate, despite not being able to degrade metabolically complexed citrate at the end of the process. In denitrifying and aerobic conditions, however, it has been determined that it is not possible to reduce or degrade these uranium complexes. Bioreduction do not get a head when they are citrate complex mixed metal complexes or when they are tridentate, monomeric or polynuclear complexes, since they become recalcitrant and persistent in the environment. From this knowledge exists a system that combines the degradation of radionuclide-citrate complex with subsequent photodegradation of remaining reduced uranyl-citrate (previously not biodegradated but sensitive to light), which allows for stable precipitates of uranium and also of thorium, strontium or cobalt from contaminated lands.\n\nThe set of strategies that comprise biosorption, bioaccumulation and biomineralization are closely related to each other, because one way or another have a direct contact between the cell and radionuclide. These mechanisms are evaluated accurately using advanced analysis technologies such as electron microscopy, X-ray diffraction and XANES, EXAFS and X-ray spectroscopies.\n\nBiosorption and bioaccumulation are two metabolic actions that are based on the ability to concentrate radionuclides over a thousand times the concentration of the environment. They consist of complexation of radioactive waste with phosphates, organic compounds and sulfites so that they become insoluble and less exposed to radiotoxicity. They are particularly useful in biosolids for agricultural purposes and soil amendments, although most properties of these biosolids are unknown.\n\nBiosorption method is based on passive sequestration of positively charged radioisotopes by lipopolysaccharides (LPS) on the cell membrane (negatively charged), either live or dead bacteria. Its efficiency is directly related to the increase in temperature and can last for hours, being a much faster method than direct bioreduction. It occurs through the formation of slimes and capsules, and with a preference for binding to the phosphate and phosphoryl groups (although it also occurs with carboxyl, amine or sulfhydryl groups). \"Firmicutes\" and other bacteria like \"Citrobacter freudii\" have significant biosorption capabilities; \"Citrobacter\" does it through electrostatic interaction of uranium with phosphates of their LPS.\n\nQuantitative analyzes determine that, in the case of uranium, biosorption may vary within a range between 45 and 615 milligrams per gram of cell dry weight. However, it is a technique that requires a high amount of biomass to affect bioremediation; it presents problems of saturation and other cations that compete for binding to the bacterial surface.\n\nBioaccumulation refers to uptake of radionuclides into the cell, where they are retained by complexations with negatively charged intracellular components, precipitation or granules formations. Unlike biosorption, this is an active process: it depends on an energy-dependent transport system. Some metals or radionuclides can be absorbed by bacteria accidentally because of its resemblance to dietary elements for metabolic pathways. Several radioisotopes of strontium, for example, are recognized as analogs of calcium and incorporated within \"Micrococcus luteus\". Uranium, however, has no known function and is believed that its entry into the cell interior may be due to its toxicity (it is able to increase membrane permeability).\n\nFurthermore, biomineralization —also known as bioprecipitation— is the precipitation of radionuclides through the generation of microbial ligands, resulting in the formation of stable biogenic minerals. These minerals have a very important role in the retention of radioactive contaminants. A very localized and produced enzymatically ligand concentration is involved and provides a nucleation site for the onset of biomineral precipitation. This is particularly relevant in precipitations of phosphatase activity-derivate biominerals, which cleavage molecules such as glycerol phosphate on periplasm. In \"Citrobacter\" and \"Serratia\" genera, this cleavage liberates inorganic phosphates (HPO) that precipitates with uranyl ion (UO) and cause deposition of polycrystalline minerals around the cell wall. \"Serratia\" also form biofilms that promote precipitation of chernikovite (rich in uranium) and additionally, remove up to 85% of cobalt-60 and 97% of cesium-137 by proton substitution of this mineral. In general, biomineralization is a process in which the cells do not have limitations of saturation and can accumulate up to several times its own weight as precipitated radionuclides.\n\nInvestigations of terrestrial and marine bacterial isolates belonging to the genera \"Aeromonas\", \"Bacillus\", \"Myxococcus\", \"Pantoea\", \"Pseudomonas\", \"Rahnella\" and \"Vibrio\" have also demonstrated the removal of uranium radioisotopes as phosphate biominerals in both oxic and anoxic growth conditions.\n\nAside from bioreduction, biosorption, bioaccumulation and biomineralization, which are bacterial strategies for natural attenuation of radioactive contamination, there are also human methods that increase the efficiency or speed of microbial processes. This accelerated natural attenuation involves an intervention in the contaminated area to improve conversion rates of radioactive waste, which tend to be slow. There are two variants: biostimulation and bioaugmentation.\n\nBiostimulation is the addition of nutrients with trace elements, electron donors or electron acceptors to stimulate activity and growth of natural indigenous microbial communities. It can range from simple fertilization or infiltration (called passive biostimulation) to more aggressive injections to the ground, and is widely used at US DOE sites. Nitrate is used as nutrient to biostimulate the reduction of uranium, because it serves as very energetically favorable electron acceptor for metal-reducing bacteria. However, many of these microorganisms (\"Geobacter\", \"Shewanella\" or \"Desulfovibrio\") exhibit resistance genes to heavy metals that limit their ability to bioremediate radionuclides. In these particular cases, a carbon source such as ethanol is added to the medium to promote the reduction of nitrate at first, and then of uranium. Ethanol is also used in soil injection systems with hydraulic recirculations: it raises the pH and promotes the growth of denitrifying and radionuclide-reducing bacteria, that produce biofilms and achieve almost 90% decrease in the concentration of radioactive uranium.\n\nA number of geophysical techniques have been used to monitor the effects of in situ biostimulation trials including measurement of: spectral ionization potential, self potentials, current density, complex resistivity and also reactive transport modelling (RTM), which measures hydrogeological and geochemical parameters to estimate chemical reactions of the microbial community.\n\nBioaugmentaton, on the other hand, is the deliberated addition to the environment of microorganisms with desired traits to accelerate bacterial metabolic conversion of radioactive waste. They are often added when necessary species for bioremediation do not exist in the treatment place. This technique has shown in field trials over the years that it does not offer better results than biostimulation; neither it is clear that introduced species can be distributed effectively through the complex geological structures of most subsurface environments or that can compete long term with the indigenous microbiota.\n\nOmics, especially genomics and proteomics, allow identifying and evaluating genes, proteins and enzymes involved in radionuclide bioremediation, apart from the structural and functional interactions that exist between them and other metabolites. Genome sequencing of various microorganisms has uncovered, for example, that \"Geobacter sulfurreducens\" possess more than 100 coding regions for c-type cytochromes involved in bioremediation radionuclide, or that \"NiCoT\" gene is significantly overexpressed in \"Rhodopseudomonas palustris\" and \"Novosphingobium aromaticivorans\" when grown in medium with radioactive cobalt.\n\nFrom this information, different genetic engineering and recombinant DNA techniques are being developed to generate specific bacteria for bioremediation. Some constructs expressed in microbial species are phytochelatins, polyhistidines and other polypeptides by fusion-binding domains to outer-membrane-anchored proteins. Some of these genetically modified strains are derived from \"Deinococcus radiodurans\", one of the most radiation-resistant organisms. \"D. radiodurans\" is capable to resist oxidative stress and DNA damage from radiation, and reduces technetium, uranium and chromium naturally as well. Besides, through insertion of genes from other species it has been achieved that it can also precipitates uranyl phosphates and degrades mercury by using toluene as an energy source to grow and stabilize other priority radionuclides.\n\nDirected evolution of bacterial proteins related to bioremediation of radionuclides is also a field research. YieF enzyme, for example, naturally catalyzes the reduction of chromium with a very wide range of substrates. Following protein engineering, however, it has also been able to participate in uranyl ion reduction.\n\nThe use of plants to remove contaminants from the environment or to render them less harmful is called phytoremediation. In the case of radionuclides, it is a viable technology when decontamination times are long and waste are scattered at low concentrations.\n\nSome plant species are able to transform the state of radioisotopes (without suffering toxicity) concentrating them in different parts of their structure, making them rush through the roots, making them volatile or stabilizing them on the ground. As in bacteria, plant genetic engineering procedures and biostimulation —called phytostimulation— have improved and accelerate these processes, particularly with regard to fast-growing plants. The use of \"Agrobacterium rhizogenes\", for example, is quite widespread and significantly increases radionuclide uptake by the roots.\n\nIn phytoextraction (also phytoaccumulation, phytosequesteration or phytoabsorption) plants carry radioactive waste from the root system to the vascular tissue and become concentrated in the biomass of shoots. It is a technique that removes radionuclides without destroying the soil structure, with minimal impact on soil fertility and valid for large areas with a low level of radioactivity. Its efficiency is evaluated through bioaccumulation coefficient (BC) or total removal of radionuclides per m, and is proven to attract cesium-137, strontium-90, technetium-99, cerium-144, plutonium-240, americium-241, neptunium-237 and various radioisotopes of thorium and radium. By contrast, it requires large biomass production in short periods of time.\n\nSpecies like common heather or amaranths are able to concentrate cesium-137, the most abundant radionuclide in the Chernobyl Exclusion Zone. In this region of Ukraine, mustard greens could remove up to 22% of average levels of cesium activity in a single growing season. In the same way, bok choy and mustard greens can concentrate 100 times more uranium than other species.\n\nRhizofiltration is the adsorption and precipitation of radionuclides in plant roots or absorption thereof if soluble in effluents. It has great efficiency in the treatment of cesium-137 and strontium-90, particularly by algae and aquatic plants, such as \"Cladophora\" and \"Elodea\" genera, respectively. It is the most efficient strategy for bioremediation technologies in wetlands, but must have a continuous and rigorous control of pH to make it an optimal process.\n\nFrom this process, some strategies have been designed based on sequences of ponds with a slow flow of water to clean polluted water with radionuclides. The results of these facilities, for flows of 1000 liters of effluent are about 95% retention of radiation in the first pond (by plants and sludge), and over 99% in three-base systems.\n\nThe most promising plants for rhizofiltration are sunflowers. They are able to remove up to 95% of uranium of contaminated water in 24 hours, and experiments in Chernobyl have demonstrated that they can concentrate on 55 kg of plant dry weight all the cesium and strontium radioactivity from an area of 75 m (stabilized material suitable for transfer to a nuclear waste repository).\n\nPhytovolatilization involves the capture and subsequent transpiration of radionuclides into the atmosphere. It does not remove contaminants but releases them in volatile form (less harmful). Despite not having too many applications for radioactive waste, it is very useful for the treatment of tritium, because it exploits plants' ability to transpire enormous amounts of water.\n\nThe treatment applied to tritium (shielded by air produces almost no external radiation exposure, but its incorporation in water presents a health hazard when absorbed into the body) uses polluted effluents to irrigate phreatophytes. It becomes a system with a low operation cost and low maintenance, with savings of about 30% in comparison to conventional methods of pumping and covering with asphalt.\n\nPhytostabilization is an specially valid strategy for radioactive contamination based on the immobilization of radionuclides in the soil by the action of the roots. This can occur by adsorption, absorption and precipitation within root zone, and ensures that radioactive waste can not be dispersed because soil erosion or leaching. It is useful in controlling tailings from strip and open pit uranium mines, and guarantees to retrieve the ecosystem. However, it has significant drawbacks such as large doses of fertilizer needed to reforest the area, apart from radioactive source (which implies long-term maintenance) remaining at the same place.\n\nSeveral fungi species have radioactive resistance values equal to or greater than more radioresistant bacteria; they perform mycoremediation processes. It was reported that some fungi had the ability of growing into, feeding, generating spores and decomposing pieces of graphite from destroyed reactor No. 4 at the Chernobyl Nuclear Power Station, which is contaminated with high concentrations of cesium, plutonium and cobalt radionuclides. They were called radiotrophic fungi.\n\nSince then, it has been shown that some species of \"Penicillium\", \"Cladosporium\", \"Paecilomyces\" and \"Xerocomus\" are able to use ionizing radiation as energy through the electronic properties of melanins. In their feeding they bioaccumulate radioisotopes, creating problems on concrete walls of deep geological repositories. Other fungi like oyster mushrooms can bioremediate plutonium-239 and americium-241.\n\nCurrent research on bioremediation techniques is fairly advanced and molecular mechanisms that govern them are well known. There are, however, many doubts about the effectiveness and possible adversities of these processes in combination with the addition of agrochemicals. In soils, the role of mycorrhizae on radioactive waste is poorly described and sequestration patterns of radionuclides are not known with certainty.\n\nLongevity effects of some bacterial processes, such as maintenance of uranium in insoluble form because of bioreductions or biomineralizations, are unknown. There are not clear details about the electronic transfer from some radionuclides with these bacterial species either.\n\nAnother important aspect is the change of \"ex situ\" or laboratory scale processes to their real application \"in situ\", in which soil heterogeneity and environmental conditions generate reproduction deficiencies of optimal biochemical status of the used species, a fact that decreases the efficiency. This implies finding what are the best conditions in which to carry out an efficient bioremediation with anions, metals, organic compounds or other chelating radionuclides that can compete with the uptake of interest radioactive waste. Nevertheless, in many cases research is focused on the extraction of soil and water and its \"ex situ\" biological treatment to avoid these problems.\n\nFinally, the potential of GMOs is limited by regulatory agencies in terms of responsibility and bioethical issues. Their release require support on the action zone and comparability with indigenous species. Multidisciplinary research is focused on defining more precisely necessary genes and proteins to establish new free-cell systems, which may avoid possible side effects on the environment by the intrusion of transgenic or invasive species.\n\n"}
{"id": "45598197", "url": "https://en.wikipedia.org/wiki?curid=45598197", "title": "Catherine Havasi", "text": "Catherine Havasi\n\nCatherine Havasi (born 1981) is an American scientist who specialises in artificial intelligence (AI) at MIT Media Lab. She is co-founder and CEO of AI company Luminoso. Havasi was a member of the MIT group engaged in the Open Mind Common Sense (also known as OMCS) AI project and that created the natural language AI program ConceptNet.\n\nHavasi grew up in Pittsburgh and became interested in artificial intelligence from reading Marvin Minsky's 1986 book \"The Society of Mind\". She attended the Massachusetts Institute of Technology, where she became involved in the MIT Media Lab and studied under Minsky. . She received a Ph.D in computer science from Brandeis University.\n\nIn 1999, she became involved in the MIT project Open Mind Common Sense with Minsky and Push Singh. , and was part of a team that created ConceptNet, an open-source semantic network based on the information in the OMCS database.\n\nIn 2010, Havasi was among the team that founded Luminoso, a text analytics software company building on the work of ConceptNet. \n\nHavasi was named among \"Boston Business Journal\"'s \"40 Under 40\", of business and civic leaders making a major impact in their respective fields in 2014. \"Fast Company\" included her in its \"100 Most Creative People in Business 2015\" listing.\n\nShe is co-author of 7 peer-reviewed journal articles on AI and language, and many per-reviewed major conference presentations, \n\n\n"}
{"id": "661510", "url": "https://en.wikipedia.org/wiki?curid=661510", "title": "Clovis point", "text": "Clovis point\n\nClovis points are the characteristically-fluted projectile points associated with the New World Clovis culture. They are present in dense concentrations across much of North America; in South America, they are largely restricted to the north of that continent. Clovis points date to the Early Paleoindian period roughly 13,500 to 12,800 calendar years ago. Clovis fluted points are named after the city of Clovis, New Mexico, where examples were first found in 1929 by Ridgely Whiteman.\n\nA typical Clovis point is a medium to large lanceolate point. Sides are parallel to convex, and exhibit careful pressure flaking along the blade edge. The broadest area is near the midsection or toward the base. The base is distinctly concave with a characteristic flute or channel flake removed from one or, more commonly, both surfaces of the blade. The lower edges of the blade and base are ground to dull edges for hafting. Clovis points also tend to be thicker than the typically thin later-stage Folsom points. with length ranging from and width from . Whether the points were knife blades or spear points is an open question.\n\nClovis points are thin, fluted projectile points created using bifacial percussion flaking (that is, each face is flaked on both edges alternatively with a percussor). To finish shaping and sharpening the points they are sometimes pressure flaked along the outer edges.\n\nClovis points are characterized by concave longitudinal shallow grooves called \"flutes\" on both faces one third or more up from the base to the pointed tip; The grooves may have permitted the points to be fastened (hafted) to wooden spears, dart shafts or foreshafts (of wood, bone, etc.) that would have been socketed onto the tip end of a spear or dart. Clovis points could also have been hafted as knives whose handles also served as removable foreshafts of a spear or dart. (This hypothesis is partly based on analogy with aboriginal harpoons that had tethered foreshafts Cotter 1937). There are numerous examples of post-Clovis era points that were hafted to foreshafts, but there is no direct evidence that Clovis people used this type of technological system.\n\nSpecimens are known to have been made of flint, chert, jasper, chalcedony and other stone of conchoidal fracture. Ivory and bone atlatl hooks of Clovis age have been archaeologically recovered. Known bone and ivory tools associated with Clovis archaeological deposits are not considered effective foreshafts for projectile weapons. The idea of Clovis foreshafts is commonly repeated in the technical literature despite the paucity of archaeological evidence. The assembled multiple piece spear or dart could have been thrown by hand or with the aid of an atlatl (spear thrower).\n\nWhether Clovis toolmaking technology was native to the Americas or originated through influences from elsewhere is a contentious issue among archaeologists. Lithic antecedents of Clovis points have not been found in northeast Asia, from where the first human inhabitants of the Americas are believed by the majority of archaeologists to have originated. Strong similarities with points produced by the Solutrean culture in the Iberian peninsula of Europe have been noted, leading to the controversial Solutrean hypothesis, that the technology was introduced by hunters traversing the Atlantic ice-shelf, meaning some of the first American humans were European.\n\nAround 10,000 radio carbon years before present, a new type of fluted projectile point called Folsom appeared in archaeological deposits, and Clovis-style points disappeared from the continental United States. Most Folsom points are shorter in length than Clovis points and exhibit different fluting and pressure flaking patterns. This is particularly easy to see when comparing the unfinished preforms of Clovis and Folsom points.\n\nBesides its function as a tool, Clovis technology may well have been the lithic symbol of a highly mobile culture that exploited a wide range of faunal resources during the Late Pleistocene and early Holocene. As Clovis technology expanded, its very use may have affected resource availability, being a possible contributor to the extinction of the megafauna.\n\nThere are different opinions about the emergence of Clovis points. One is that pre-Clovis people in the New World developed the Clovis tradition independently. Another opinion is that Upper Paleolithic peoples who, after migrating into North America from northeast Asia, reverted to inherited Clovis-style flaked-stone technology that had been in use prior to their entry into the Americas.\n\nClovis points were first discovered near the city of Clovis, New Mexico, and have since been found over most of North America and as far south as Venezuela. Significant Clovis finds include the Anzick site in Montana; the Blackwater Draw type site in New Mexico; the Colby site in Wyoming; the Gault site in Texas; the Simon site in Idaho; the East Wenatchee Clovis Site in Washington; and the Fenn cache, which came to light in private hands in 1989 and whose place of discovery is unknown. Clovis points have been found northwest of Dallas, Texas.\n\nIn May 2008, a major Clovis cache, now called the Mahaffey Cache, was found in Boulder, Colorado, with 83 Clovis stone tools. The tools were found to have traces of horse and cameloid protein. They were dated to 13,000 to 13,500 YBP, a date confirmed by sediment layers in which the tools were found and the types of protein residues found on the artifacts.\n\nA fluted obsidian point from a site near Rancho San Joaquin, Baja California Sur was found in a private collection in 1993. The point was surface collected several years earlier from an alluvial terrace approximately 14 km. to the south of San Ignacio.\n\n\n"}
{"id": "762111", "url": "https://en.wikipedia.org/wiki?curid=762111", "title": "Crocodile clip", "text": "Crocodile clip\n\nA crocodile clip (also alligator clip) is a sprung metal clip with long, serrated jaws which is used for creating a temporary electrical connection. This simple mechanical device gets its name from the resemblance of its jaws to that of an alligator's or crocodile's. It is used to connect an electrical cable to a battery or some other component. Functioning much like a spring-loaded clothespin, the clip's tapered, serrated jaws are forced together by a spring to grip an object. When manufactured for electronics testing and evaluation, one jaw of the clip is typically permanently crimped or soldered to a wire, or is bent to form the inner tubular contact of a ~4 mm female banana jack, enabling quick non-permanent connection between a circuit under test and laboratory equipment or to another electrical circuit. The clip is typically covered by a plastic shroud or \"boot\" to prevent accidental short-circuits.\n\nSmall versions, ranging in size from 15–40 mm in length, are used in electrical laboratory work. \n\nLarge versions of these clips, called automotive clips or battery clamps, are made of solid copper for low electrical resistance, and are used with thick insulated copper cables to make connections between automobile batteries. These jumper cables are capable of delivering hundreds of Amperes of current needed to directly power an automobile starter motor, or to transfer energy from a charged lead-acid battery to a discharged one.\n\nThe United States Defense Logistics Agency specifies several types of electrical clips in Commercial Item Description (CID) A-A-59466. In this CID document, crocodile clips are designated type CC, alligator clips are designated types TCx, and other types of electrical clips have various other, unique designations.\n\n\nA Kelvin clip is a special form of crocodile clip. The jaws of a Kelvin clip are insulated from each other, allowing 2 isolated wires to connect to a single test point. This enables 4-wire measurement of circuits with very low resistances.\n"}
{"id": "46796829", "url": "https://en.wikipedia.org/wiki?curid=46796829", "title": "David Shing", "text": "David Shing\n\nDavid Shing (born 1970) also known as \"Shingy\" is an Australian thought leader and self-proclaimed \"Digital Prophet\". He has held various top marketing jobs at AOL since 2007.\n\nDavid Shing grew up in a two-bedroom house in suburban Australia, in a family of 10 kids. He studied at Billy Blue design school in Sydney.\n\nDavid Shing joined Click Things as VP of Product Strategy in 1999. In 2001 he became VP Creative & Strategy at Decentrix Inc. In 2007 he began a long relationship with AOL as the Marketing Director for AOL Europe. In 2010 he was promoted to VP Media and Marketing for AOL international and became the self anointed Digital Prophet for AOL in 2011, a title he invented for the role. He is charged with identifying new opportunities for the business, actively changing brand perception, and assisting in building the external profile of the company across the globe. He does this by speaking at conferences and events around the world, including SXSW and TEDx \n\nHe speaks at many live events as part of his position as Digital Prophet for AOL and his controversial appearance on MSNBC and the follow up article in The New Yorker made him the focal point for a conversation around the meaning behind buzzwords in the media industry.\n\nHe is also the inspiration of the Shangy character in W/ Bob & David.\n"}
{"id": "8965447", "url": "https://en.wikipedia.org/wiki?curid=8965447", "title": "Deal of the day", "text": "Deal of the day\n\nDeal-of-the-day (also called daily deal or flash sales or one deal a day) is an ecommerce business model in which a website offers a single product for sale for a period of 24 to 36 hours. Potential customers register as members of the deal-a-day websites and receive online offers and invitations by email or social networks.\n\n, deal-of-the-day sites have continued to grow in popularity, although new concerns have arisen over the longevity of the concept and the financial viability of one-day deals for small businesses.\n\nThe deal-of-the-day concept gained popularity with the launching of Woot.com in July 2004, although Woot itself was a modified version of earlier dot-com bubble sites such as uBid. By late 2006, the deal-of-the-day industry had greatly expanded to over 100 deal-a-day sites. In November 2008, Groupon entered the market and became the second fastest online company to reach a billion-dollar valuation.\n\nOther online businesses, including Facebook, and Google tested their own daily deal sites, withdrawing them after they proved unsuccessful. However, the rise of social networks, such as Facebook and Myspace, has accelerated the growth of daily deals sites, allowing popular deals to spread virally.\n\nThe deal-of-the-day business model works by allowing retailers to market discounted services or products directly to the customers of the deal company, who receives a portion of the retailer's profit. This allows retailers to build brand loyalty and quickly sell surplus inventory.\n\nThe majority of deal-of-the-day sites work directly with local businesses and online retailers to develop deals significantly discounted compared to recommended retail prices. Using a group buying formula, a minimum and maximum number of deals are made available. Typically, deal of the day sites segment merchandise by specific designer sales. Deals are typically only offered for 24 hours, although daily deal websites are increasingly offering alternative, longer deal buying periods to increase sales and allow multiple deals to run in a single location concurrently.\n\nDescriptions of the deals are often emailed to customers when the deal goes live, sometimes with creative or humorous descriptions. The practice of sending these emails has been criticized by e-mail marketing professionals and users. However, evidence suggests this aggressive strategy is effective at generating sales. Some sites allow members to receive an e-mail either daily or weekly or to be notified of all current offers. Customers purchase the deal on the deal-of-the-day website, rather than directly from the retailer. The websites then retain the customer data, rather than the retailer.\n\nOnce the minimum number of deals have been sold, customers' credit cards are charged, and the deal is delivered as an electronic voucher redeemable at the retailer or service provider's location. The promotional value of the vouchers purchased from deal-of-the-day websites typically expire after a certain period but maintain the original value paid.\n\nCommon products and services sold through deal-of-the-day websites include apparel, restaurants and bars, salons and spas, special events, health and fitness products, and travel packages.\n\nMost businesses which run contracts with daily deal websites consider doing so as a marketing activity rather than a direct means of generating profit. Between the deep discount offered as part of the deal and the payout to the deal-of-the-day site, the businesses may net little or no profit (effectively making the deals loss leaders). There is evidence that these businesses gain significant increases in overall sales due to the amount of exposure gained from running a one-day deal. Many customers who purchase daily deals are \"price-sensitive deal-seekers\" who are unlikely to return to the business in the future without similar discounts. However, studies have shown that for small businesses and start-ups, daily deals can result in a substantial 30% increase in profits. A survey of businesses who ran daily deals in the past year revealed that more than half (55.5%) profited on their daily deal promotion, whereas just over a quarter (26.6%) lost money. The remainder (17.9%) broke even. Beyond mere exposure, these businesses hope to capitalize on the long-term value of new repeat customers. Thus deal-of-the-day sites also function as marketing platforms.\n\nA study of small businesses revealed that on average, daily deal spending is the single largest expenditure in a company's marketing budget, at 23.5%, which translates to average annual spending on daily deal programs of $46,530. Lesser expenditures include e-mail promotions (16.1% or $31,878) and online search advertising through programs such as Google AdWords (14.7% or $29,106).\n\nMost daily deal websites have an affiliate marketing program, allowing third-party websites to be paid for referring visitors, increasing the presence of the deal-of-the-day sites. These websites display syndicated offers from a number of deals sites, based on location and which categories of deal a user is interested in receiving. These aggregators earn a percentage of any sales made by the deal sites through their affiliate program.\n\nWhereas 2010 was a year of rapid growth for the industry, daily deal sites began to slide in 2011 and 2012. Regardless, revenue forecasts for the industry continue to foresee strong growth. There has been a surge in the private shopping club sector with niche products and offerings such as luxury home ware and high-end food gaining in popularity. Analysts predicted that industry revenues would reach several billion dollars, at an increasing at annual rates in excess of 100% by the end of 2011. According to a study released by BIA/Kelsey, gross revenues are projected to grow from a current $873 million to $3.9 billion by 2015.\nThe increase in venture capital injections and startup launches demonstrates the continued growth of the industry. Examples of such activity include the recent launches of Facebook, Amazon, Google, and AT&T's daily deal sites. Groupon filed for its IPO in June 2011 and went public in November of that year.\nDespite positive growth figures, some studies suggest there is a structural weakness to the industry that will have to be addressed. Such shortcomings exist on both the consumer and merchant sides of the industry. For example, deal users very rarely return for a full price purchase, and a large percentage of businesses indicate their disinterest in further deals in the future. Thus the industry may need to settle for lower shares of revenues from businesses compared to their current levels (20-50%), which are not sustainable. It is unclear whether industry diversification, increasing competition, and larger revenue shares for merchants will disrupt the industry leaders or cannibalize the industry as a whole.\n\n"}
{"id": "43342432", "url": "https://en.wikipedia.org/wiki?curid=43342432", "title": "Deductive classifier", "text": "Deductive classifier\n\nA deductive classifier is a type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values. The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to theorem provers in that they take as input and produce output via First Order Logic. Classifiers originated with KL-ONE Frame languages. They are increasingly significant now that they form a part in the enabling technology of the Semantic Web. Modern classifiers leverage the Web Ontology Language. The models they analyze and generate are called ontologies.\n\nA classic problem in knowledge representation for artificial intelligence is the trade off between the expressive power and the computational efficiency of the knowledge representation system. The most powerful form of knowledge representation is First Order Logic (FOL). However, it is not possible to implement knowledge representation that provides the complete expressive power of first order logic. Such a representation will include the capability to represent concepts such as the set of all integers which are impossible to iterate through. Implementing an assertion quantified for an infinite set by definition results in an undecidable non-terminating program. However, the problem is deeper than not being able to implement infinite sets. As Levesque demonstrated, the closer a knowledge representation mechanism comes to FOL, the more likely it is to result in expressions that require infinite or unacceptably large resources to compute.\n\nAs a result of this trade-off, a great deal of early work on knowledge representation for artificial intelligence involved experimenting with various compromises that provide a subset of FOL with acceptable computation speeds. One of the first and most successful compromises was to develop languages based predominately on modus ponens, i.e. IF-THEN rules. Rule-based systems were the predominate knowledge representation mechanism for virtually all early expert systems. Rule-based systems provided acceptable computational efficiency while still providing powerful knowledge representation. Also, rules were highly intuitive to knowledge workers. Indeed, one of the data points that encouraged researchers to develop rule-based knowledge representation was psychological research that humans often represented complex logic via rules.\n\nHowever, after the early success of rule-based systems there arose more pervasive use of frame languages instead of or more often combined with rules. Frames provided a more natural way to represent certain types of concepts, especially concepts in subpart or subclass hierarchies. This led to development of a new kind of inference engine known as a classifier. A classifier could analyze a class hierarchy (also known as an ontology) and determine if it was valid. If the hierarchy was invalid the classifier would highlight the inconsistent declarations. For a language to utilize a classifier it required a formal foundation. The first language to successfully demonstrate a classifier was the KL-ONE family of languages. The LOOM language from ISI was heavily influenced by KL-ONE. LOOM also was influenced by the rising popularity of object-oriented tools and environments. Loom provided a true object-oriented capability (e.g. message passing) in addition to Frame language capabilities. Classifiers play a significant role in the vision for the next generation Internet known as the Semantic Web. The Web Ontology Language provides a formalism that can be validated and reasoned on via classifiers such as Hermit and Fact++.\n\nThe earliest versions of classifiers were logic theorem provers. The first classifier to work with a Frame language was the KL-ONE classifier. A later system built on common lisp was LOOM from the Information Sciences Institute. LOOM provided true object-oriented capabilities leveraging the Common Lisp Object System, along with a frame language. In the Semantic Web the Protege tool from Stanford provides classifiers (also known as reasonsers) as part of the default environment.\n\n"}
{"id": "24608799", "url": "https://en.wikipedia.org/wiki?curid=24608799", "title": "Desaxe", "text": "Desaxe\n\nA desaxe engine, is one in which each cylinder is positioned with its exact center (the bore axis) slightly offset from the center line of the crankshaft. \"Désaxé\", in French, means \"unbalanced\". Desaxe engines are usually automotive, but the term can also apply to steam engines. \n\nIf the offset is in the direction of rotation, it has the effect of increasing the leverage applied to the crankshaft during the \"power\" stroke, and reducing thrust wasted against the cylinder wall.\n\nIn a conventional four-stroke engine, each of the strokes (intake, compression, power, exhaust) involves a nominal rotation of 180°, totaling 720° for the complete 4-stroke combustion cycle. A desaxe engine adds to the duration of the two downward strokes (intake and power), and subtracts the same amount from the two upward strokes (compression and exhaust), with the total remaining 720°. A typical desaxe engine will have strokes of 185° - 175° - 185° - 175°, etc., with the differential roughly (but not directly) proportional to the percentage of offset distance to stroke length.\n\nThe relative proportion of offset distance versus stroke length ranges from very small to almost 20%; viz. an engine with an 80 mm stroke may have a cylinder offset of 20 mm.\n\nEarly adopters of the Desaxe Principle included Henry Ford who would fully implement the Desaxe offset into the Ford Flathead V8 engine throughout the 1930s while adopting greater offsets into the 1940s.\n\nUse of desaxe engines is now becoming more common.. The Volkswagen VR6 & VR5 engines have desaxe cylinders, with an offset of 12.5 mm. The front bank cylinders are offset forward of the crank while the rear are offset rearward. The Toyota Prius has desaxe cylinder bores offset by 13 mm. The new MT-09 motorcycle is the first Yamaha with desaxe pistons. Ford has a new series of engines called dragon series that have an offset crank shaft.\n\nAll single crank split-single engines such as those used by DKW and Puch had, by necessity, highly desaxe cylinders.\n\nThe Scuderi engine, as shown in the animated illustration, has highly desaxe cylinders. The compressor (blue) is offset a greater amount than the power cylinder (yellow). Its benefit is to give a better line of thrust from piston to crank though con-rod during downward power stroke but it has disadvantages in vibration and bore wall thrust.\n"}
{"id": "1607072", "url": "https://en.wikipedia.org/wiki?curid=1607072", "title": "Detonography", "text": "Detonography\n\nDetonography is a method for sculpting metal with plastic or other explosives. Essentially a form of giant printmaking, with the explosive acting as the stamping press, it was created by Evelyn Rosenberg in 1986 with the help of the engineers of New Mexico Institute of Mining and Technology in Socorro, New Mexico. The process can create complex surfaces with delicate etched designs, even welding dissimilar metals. The results are lightweight, durable indoor and out, and relatively vandalproof. This process essentially embosses the metal, but with a raised image instead of a depressed one.\n\nRosenberg has produced more than 40 detonographic works for public buildings around the United States and abroad.\n\n\n"}
{"id": "16021556", "url": "https://en.wikipedia.org/wiki?curid=16021556", "title": "DialogOS", "text": "DialogOS\n\nDialogOS is a graphical programming environment to design computer system which can converse through voice with the user. Dialogs are clicked together in a Flowchart. DialogOS includes bindings to control Lego Mindstorms robots by voice and has bindings to SQL databases, as well as a generic plugin architecture to integrate with other types of backends.\n\nDialogOS is used in computer science courses in schools and universities to teach programming and to introduce beginners in the basic principles of human/computer interaction and dialog design. It has also been used in research systems.\n\nDialogOS was initially developed commercially by CLT Sprachtechnologie GmbH until its liquidation in 2017. The rights were then acquired by Saarland University and the software was released as open-source.\n\nDialogOS can control the LEGO Mindstorms NXT Series. It uses sensor-nodes to obtain values for the following sensors:\n"}
{"id": "8103238", "url": "https://en.wikipedia.org/wiki?curid=8103238", "title": "Drive testing", "text": "Drive testing\n\nDrive testing is a method of measuring and assessing the coverage, capacity and Quality of Service (QoS) of a mobile radio network.\n\nThe technique consists of using a motor vehicle containing mobile radio network air interface measurement equipment that can detect and record a wide variety of the physical and virtual parameters of mobile cellular service in a given geographical area.\n\nBy measuring what a wireless network subscriber would experience in any specific area, wireless carriers can make directed changes to their networks that provide better coverage and service to their customers.\n\nDrive testing requires a mobile vehicle outfitted with drive testing measurement equipment. The equipment are usually highly specialized electronic devices that interface to OEM mobile handsets. This ensures measurements are realistic and comparable to actual user experiences.\n\nDrive test equipment typically collects data relating to the network itself, services running on the network such as voice or data services, radio frequency scanner information and GPS information to provide location logging.\n\nThe data set collected during drive testing field measurements can include information such as:\n\n\nDrive testing can broadly be categorized into three distinct topics:\n\n\nThe result produced by drive testing for each of these purposes is different.\n\nFor benchmarking, sophisticated multi-channel tools such as Focus Infocom's DMTS and XGMA, DingLi Communications' Pilot Fleet, Ascom's Symphony, Rohde & Schwarz-SwissQual's Diversity Benchmarker or Keysight Nemo Invex II are used to measure several network technologies and service types simultaneously to very high accuracy, to provide directly comparable information regarding competitive strengths and weaknesses. Results from benchmarking activities,such a comparative coverage analysis or comparative data network speed analysis, are frequently used in marketing campaigns. Drive testing to gather network bench-marking data is the only way mobile network operators can collect accurate competitive data on the true level of their own and their competitors technical performance and quality levels.\n\nOptimization and troubleshooting information is more typically used to aid in finding specific problems during the rollout phases of new networks or to observe specific problems reported by consumers during the operational phase of the network lifecycle. In this mode drive testing data is used to diagnose the root cause of specific, typically localized, network issues such as dropped calls or missing neighbour cell assignments.\n\nService quality monitoring typically involves making test calls across the network to a fixed test unit to assess the relative quality of various services using Mean opinion score (MOS). Quality monitoring focuses on the end user experience of the service, and allows mobile network operators to react to what effectively subjective quality degradations by investigating the technical cause of the problem in time-correlated data collected during the drive test. Service quality monitoring is typically carried out in an automated fashion, using devices that run largely without human intervention carried in vehicles that regularly ply typical drive testing routes such as garbage collection vehicles, taxis or buses.\n\nDrive testing can be conducted at any time on a live network and very rarely will there be any network intrusion.\n\n1 - Drive testing LTE, available from: <http://www.viavisolutions.com/sites/default/files/technical-library-files/drivetesting_lte_wp_nsd_tm_ae_0.pdf>\n"}
{"id": "24374221", "url": "https://en.wikipedia.org/wiki?curid=24374221", "title": "E-Learning Developers' Community of Practice", "text": "E-Learning Developers' Community of Practice\n\ne-Learning Developers' Community of Practice or ElCoP is practice developed following a decision of European air navigation service (ANS) training providers together with Eurocontrol, to create an e-learning developers community of practice in January 2009.\n\nThe aims of this community of practice are: to share knowledge and information on e-learning development matters between its members; to develop common deliverables; and to improve Eurocontrol deliverables, such as the best practice documentation for e-learning development and the digital learning asset sharing platform.\n\nHistorically, ElCoP is the successor to the e-Learning Developers’ Task Force (EDTF), which was created in 2006 to develop a network of European e-learning developers involved in ANS training.\n\nThis group had the tasks of developing best practices in e-learning development and a platform which would enable users to share digital learning assets.\n\nEven though EDTF finished these tasks in 2008, its members believed that there was added value in keeping a community of practice alive which would enable participants to learn from each other, to share best practices and to develop common deliverables. They suggested ElCoP.\n\nElCoP started work in the first quarter of 2009.\n\nTraining providers that confirmed their participation are:\n\n\n"}
{"id": "23412164", "url": "https://en.wikipedia.org/wiki?curid=23412164", "title": "Environmental design in rail transportation", "text": "Environmental design in rail transportation\n\nEnvironmental design is an emerging topic in railroad technology. Over the past twenty-five years, fuel efficiency in diesel locomotives has increased 85%, allowing these trains to go farther and move more freight while using less fuel. New low-impact electric and hybrid trains reduce overall carbon emissions. Also, train manufacturers have started utilizing hydrogen technology for propulsion, with carbon emissions only coming from the manufacturing of the hydrogen itself.\n\nDiesel trains replaced the steam engine in the late 1920s as a cleaner more efficient way of moving people and goods. Since 1980, the amount of freight being hauled by Diesel Trains has nearly doubled, yet the fuel consumption of trains has virtually stayed the same. Estimates have shown that about of fuel have been saved by increasing the efficiency in diesel trains. US Department of Energy reported that commercial airline energy intensity per passenger mile was 3,587 BTU's, automobiles were 3,549 BTUs, while commuter rail energy intensity was just 2,751 BTUs, which indicates that rail transportation is the most energy efficient of the three. The Union Pacific Rail Road has implemented a particle filter on the their diesel engines. Silicon carbide blocks trap particles from the exhaust as they leave the engine, greatly reducing emissions.\n\nElectric trains have always had no direct carbon emissions because they are run entirely by internal electric motors. However, the means of generating the electricity used to power these motors was predominately by burning fossil fuels or coal, both of which produce a large amount of carbon emissions. With the emergence of 'clean energy' generation, electrical trains actually run with very low environmental impact. For example, the proposal for the high-speed rail line between San Francisco and Los Angeles in California has the potential for zero greenhouse gas emissions, with the 3,350 GWh each year being generated by California's extensive infrastructure of renewable energy sources.\n\nSince 1986, engineers have been developing electric-diesel \"hybrid\" trains. One type of hybrid train implements battery power when the train is idling and at low speed movement, and a diesel engine at higher speeds. To recharge the batteries, power from the diesel motor, charge utilizing regenerative braking, or a combination of both is used. According to the Institute of Electrical and Electronics Engineers, hybrid trains reduce the carbon emissions of Diesel Trains by 19%. Another type of hybrid train, such as the RailPower Technologies Green Goat, uses a large battery, and a small set of generators (\"genset\") for power. The genset is run at a constant speed and is attached to a generator to replenish the battery.\n\nHydrogen propulsion is an emerging technology, and is currently being implemented in locomotives. Hydrogen-powered trains dubbed \"Hydrail\" emit only water as a by product of combustion, and have a zero direct greenhouse gas emission. However, the process used to generate hydrogen in a form useful to power trains does produce a small amount of greenhouse gases. By using wind energy and electrolysis, 6.85 grams of greenhouse gases per MJ of LHV are produced, which is an insignificant amount compared to the 22 pounds of greenhouse gas emissions from one gallon of gasoline. Trains are prime targets for hydrogen propulsion due to their ability to store massive tanks of hydrogen.\n\nRail transportation emits about 0.2 pounds of greenhouse gases per passenger mile (55 g/km) when each car is filled with 50 passengers. This figure increases to about 0.5 pounds per passenger mile (140 g/km) when only filled with half that amount. These numbers are still much lower than those of Jet transportation, about 1 pound per passenger mile (280 g/km), and that of a solo car driver, about 1.15 pounds per passenger mile (325 g/km). Even the fuel efficient Prius emits more greenhouse gases per passenger mile.\n\nEstimates have shown that if just 10% of long-distance freight that is currently moving by truck were to be moved instead by diesel trains, the resulting carbon emission reduction would be the equivalent of taking 2 million cars off the road. The results are more dramatic when the diesel train figures are replaced by hybrid and electric train figures.\n\n\n"}
{"id": "535096", "url": "https://en.wikipedia.org/wiki?curid=535096", "title": "F connector", "text": "F connector\n\nThe F connector (also F-type) is a coaxial RF connector commonly used for \"over the air\" terrestrial television, cable television and universally for satellite television and cable modems, usually with RG-6/U cable or, in older installations, with RG-59/U cable.\n\nThe F connector was invented by Eric E. Winston in the early 1950s while working for Jerrold Electronics on their development of cable television. In the 1970s, it became commonplace on VHF, and later UHF, television antenna connections in the United States, as coaxial cables replaced twin-lead.\n\nIt is now specified in IEC 60169 Radio-frequency connectors, part 24.\n\nThe F connector is an inexpensive, gendered, threaded, compression connector for radio frequency signals. It has good 75 Ω impedance match for frequencies well over 1 GHz and has usable bandwidth up to several GHz.\n\nConnectors mate using a in-32 unified extra fine (UNEF) thread. The female connector has a socket for the center conductor and external threads. The male connector has a center pin, and a captive nut with internal threads.\n\nThe design allows for low-cost construction, where cables are terminated almost exclusively with male connectors. The coaxial cable center conductor forms the pin, and cable dielectric extends up to the mating face of the connector. Thus, the male connector consists of only a body, which is generally crimped onto or screwed over the cable shielding braid, and a captive nut, neither of which require tight tolerances. Push-on versions are also available.\n\nFemale connectors are typically used on bulkheads or as couplers, often being secured with the same threads as for the connectors. Thus can be manufactured as a single piece, with center sockets and dielectric, entirely at the factory where tolerances can easily be controlled.\n\nThis design is subject to the surface properties of the inner conductor (which must be solid wire, not stranded) and is not corrosion resistant. Hence waterproof versions are needed for outside use (for example, on antennas). Corrosion resistance can be improved by coating all bare copper wires with silicone grease.\n\nThe F connector is not weatherproof. Neither the threads nor the joint between male connector body and captive nut seal. However, male connectors are commonly enhanced with an o-ring (of about 7mm) inside the captive nut. This seals between the mating faces of both connectors, providing good waterproofing for the center conductor.\n\nThe cable and satellite television entities (as a near standard practice) use compression fittings with F connectors on customer premises. In Europe, block down-converted satellite signals (950–2150 MHz) from LNBs and DC power and block signalling from satellite receivers are near exclusively passed through F connectors.\n\nF connectors are probably the most suitable for domestic terrestrial, cable, and satellite TV installations where the delivery of very high frequency information is required. Belling-Lee connectors (IEC 169-2; used on European terrestrial receivers) are not well suited for long-haul building delivery of frequencies above 500MHz, because the standard was designed around tube receivers and mediumwave (or shortwave) antennas (but workarounds exist). F connectors require slightly more care to properly install the male connectors to the cable than the Belling-Lee type, with the exception of compression or flex type connections.\n\nPush-on (aka Flex) F connectors provide poorer shielding against microwave signals of high field strength. This leakage problem is more an artifact of bent or partly broken push on connectors, but is mostly not observed with compression connectors. Nearby television, FM radio, mobile & cordless phones, government radiolocation (54–1,002 MHz) transmitters can potentially interfere with a CATV or DTH Satellite reception or operation if the Flex connector poorly installed.\n\n\n"}
{"id": "31429619", "url": "https://en.wikipedia.org/wiki?curid=31429619", "title": "Floor scrubber", "text": "Floor scrubber\n\nFloor scrubber is a floor cleaning device. It can be simple tools such as floor mops and floor brushes, or in a form of walk-behind or ride-on machines to clean larger floor areas by injecting water with cleaning solution, scrubbing, and lifting the residuals off the floor. With the advancement in robotics, autonomous floor-scrubbing robots are available as well.\n\nAutomatic floor scrubbers, also known as auto scrubbers, are a type of floor cleaning machine that are used to scrub a floor clean of light debris, dust, oil, grease or floor marks. These machines have either rotary(disk) or cylindrical scrubbing head and an automated system for dispensing cleaning solution and then vacuuming it up. So, in one pass over the floor, a user can dispense cleaning, scrub it into the floor, then vacuum it all up with an autoscrubber squeegee attachment at the back of the machine. Auto scrubbers have a separate dispensing (solution) tank and collection (recovery) tank to keep the clean water separate from the dirty water and can be categorized into one of three main types: walk behind, stand-on, and rider.\n\nFloor scrubbers are a more hygienic alternative to traditional cleaning methods such as a mop and bucket. Environmentally safe soaps can be used in conjunction with a reduced water system to save on both the amount of chemicals released into the environment as well as the amount of gray water produced. Some floor scrubbers are even capable of cleaning without a water and chemical system at all.\n\nMost autoscrubbers can't reach edges, corners, clean under obstructions such as drinking fountains, and can't fit into alcoves. Therefore, mopping is needed to clean areas the autoscrubber can't reach.\nSome manufacturers now produce Floor Scrubbers with Orbital / Oscillating brush decks allowing edges, corners and overhangs to be fully cleaned.\n\nModern floor scrubbers have a pre-sweep option that removes the need to sweep the floor before scrubbing. The pre-sweep brush head is placed in front of the vacuum system to collect dust and debris before it can block the vacuum system. In the past it was important to sweep the floor before scrubbing to remove any debris and dust that could clog the vacuum hose or build up in the vacuum motor, which can decrease performance. If this happens, the vac hose may need to be removed to clear the obstruction and/or the vac motor may need to be blown out with compressed air.\n\nStripping Solution should never be used as it can cause damage to the solution dispensing system, but can still be vacuumed up by the machine without harm. Occasionally, the solution system should be flushed with water mixed with vinegar to remove any soap and calcium deposits that could build in the solution system.\n\nAfter each use, the dispensing (solution) and especially the collection (recovery) tanks should be emptied and rinsed out to prevent dirt build up. Also, the pads/brushes, vac hose, and squeegee should also be rinsed to prevent dirt build up. The vac motor should be run for several minutes afterwards to remove any moisture that could be present in the vac motor to reduce chances of corrosion that could damage the vac motor. Failure do to this maintenance could cause in a loss of vacuum airflow and increase in costly repairs.\n\nThere are 3 common types of automatic floor scrubber heads: Disk, Cylindrical, and Square Oscillating. \n\nThe most common, disk style floor scrubbers use a circular motion with a round pad or brush to agitate a cleaning solution against the floor to release soils. Disk floor scrubber heads work best on smooth floors.\n\nUsing counter rotating tube style brushes that rotates perpendicular to the floor, cylindrical floor scrubbers clean rough or uneven surfaces. Cylindrical brushes usually have a collection tray behind the brushes that can pick up larger debris such as rocks, screws and small bolts. This reduces the need to sweep or dust mop prior to scrubbing, although it is still a good idea if possible. \n\nSimilar to the disk style, square oscillating floor scrubbers use a flat pad to scrub the floor. The difference is that instead of spinning, it moves in a vibrating motion at a much faster speed. The square design allows for cleaning closer to walls and in corners. \nThe high speed motion and down pressure also allow this style of floor scrubber head to be useful for removing floor finish from vinyl composite tile and well as prepare wood floors for refinishing. A specialized, abrasive pad is used for these procedures.\n\nWhen floor scrubbing machines became more available to many types of facilities, there was a need to cover a different type of flooring. Floor buffers or rotary floor machines were invented to scrub and polish the floor with linoleum surface. The machines use rotary brushes with soft material to clean and make the floor shine. For marble and wood floors, floor polishers may be used to apply protective coating to floor.\n\nIt is also known as a floor burnisher if it is a high speed floor buffer with a pad that rotates at over 1000 RPM. \n\nClosely resembling a large upright, wide-based vacuum cleaner with handlebar controls and requiring, until familiar with the machine, two-handed steering, a floor buffer uses one or more variable-speed circular rotary brushes to dislodge dirt and dust from and apply a polished finish to flat surfaces. They have a large, round scrubbing pad which is spun in one direction by a small motor, usually mounted directly over the center of the pad.\n\nLarger powered floor buffers are used in schools, hospitals, offices and public buildings. These have wheels and are powered to allow the user to easily move and clean items stuck on floors. Scaled-down versions are available for home use and often sold as hard floor cleaners.\n\nWith the advancement in technologies used in autonomous robots, floor-scrubbing robots were created by combining the features of automatic floor scrubbers with self-control operations without an operator. Non-residential models such as HydroBot by Intellibot Robotics are suitable for education, retail, healthcare and manufacturing facilities. The Intellibot commercial floor cleaning machines can clean area in one hour.\n\nAs with other applications of mobile robotics, the capability of robotic floor scrubbers will increase over time, coinciding with the availability of improved sensors and computing components. The latest generation of mobile robotics sensors includes LIDAR and 3D cameras, which are used in the newest robotic floor scrubbers, such as the Avidbots Neo. LIDAR sensors allow a floor scrubber robot to detect surrounding walls and objects at a longer range, allowing the machine to determine its precise location in larger environments, such as malls and airports.\n\nUnlike earlier residential cleaning robots that followed a random pattern when cleaning, commercial floor scrubber robots tend to have a precise plan for cleaning, allowing these robots to cover the entire floor in a predictable pattern each time they clean. They miss very few spots on the floor, since they know exactly where they've just cleaned and where they still need to clean. Robotic floor scrubbers are also designed to navigate around people and obstacles that they encounter during autonomous operation.\n\nFloor-scrubbing robots are also available in a small form factor for residential applications. The Scooba by iRobot is one example.\n\n\nhttps://kafsabialmas.com/"}
{"id": "35186455", "url": "https://en.wikipedia.org/wiki?curid=35186455", "title": "Future Airborne Capability Environment", "text": "Future Airborne Capability Environment\n\nThe Open Group Future Airborne Capability Environment (FACE Consortium) was formed in 2010 to define an open avionics environment for all military airborne platform types. Today, it is a real-time software-focused professional group made up of industry suppliers, customers, academia, and users. The FACE approach is a government-industry software standard and business strategy for acquisition of affordable software systems that promotes innovation and rapid integration of portable capabilities across programs. The FACE Consortium provides a vendor-neutral forum for industry and government to work together to develop and consolidate the open standards, best practices, guidance documents, and business strategy necessary to result in:\n\nThe FACE Technical Standard is an open real-time standard for making safety-critical computing operations more robust, interoperable, portable and secure. Although the consortium started with a focus on avionics, the applicability of the technical standard and its associated data model have become much broader. The standard enables software developers to create and deploy a wide catalog of applications for use across the entire spectrum of real-time systems through a common operating environment. The latest edition of the standard further promotes application interoperability and portability with enhanced requirements for exchanging data among FACE components, including a formally specified data model, and emphasis on defining common language requirements for the standard.\n\nThe FACE effort sprang from US Navy open architecture programs, promoted by the US Naval Air Systems Command (NAVAIR), to enhance interoperability and software portability for avionics software applications across DoD aviation platforms. Both the US Army and US Air Force have been participating in the consortium. NAVAIR led the pack with early acquisitions, followed later by Army and Air Force. \n\nThe FACE Consortium was formed by The Open Group as a \"Voluntary Consensus Standards Body\", as defined by the National Technology Transfer Act and OMB Circular A-119. This facilitates government participation in the consortium. One goal of the effort is to reduce the typical development and deployment cycle of new capabilities in military airborne platforms from as long as six years under the current methodology to as little as six months.\n\nThe FACE reference architecture ecosystem includes software product conformance verification and certification processes. In October 2016, a suite of flight management software earned the first FACE certificate of conformance. One may view information on all certified FACE conformant products at the FACE Registry\n\nThe FACE technical approach tackles barriers to software modularity, portability, and interoperability by defining a Reference Architecture and employing design principles to enhance software portability. To meet the objectives of the technical approach, the FACE Technical Standard uses a standardized architecture describing a conceptual breakdown of functionality, called the FACE Reference Architecture, to promote the reuse of software components able to share common functionality across disparate systems. This architecture defines standardized interfaces to allow software components to be moved between systems, including those developed by different vendors. The standardized interfaces follow a data architecture to ensure the data communicated between the software components is fully described to facilitate their integration on new systems.\n\nThe FACE Reference Architecture is composed of logical segments where variance occurs. The structure created by connecting these segments together is the foundation of the FACE Reference Architecture. The five (5) segments of the FACE Reference Architecture are the Operating System Segment (OSS), Input/Output Services Segment (IOSS), Platform-Specific Services Segment (PSSS), Transport Services Segment (TSS), and Portable Components Segment (PCS).\n\nThe FACE Reference Architecture defines a set of standardized interfaces providing connections between the FACE architectural segments. The standardized interfaces within the FACE Reference Architecture are the Operating System Segment Interface (OSS Interface), the Input/Output Services Interface (IOS Interface), the Transport Services Interfaces, and Component-Oriented Support Interfaces.\n\nThe FACE Reference Architecture defines three FACE OSS Profiles tailoring the Operating System (OS) Application Programming Interfaces (APIs), programming languages, programming language features, run-times, frameworks, and graphics capabilities to meet the requirements of software components for differing levels of criticality. The three Profiles are Security, Safety, and General Purpose. The Security Profile constrains the OS APIs to a minimal useful set allowing assessment for high-assurance security functions executing as a single process. The Safety Profile is less restrictive than the Security Profile and constrains the OS APIs to those that have a safety certification pedigree. The General Purpose Profile is the least constrained profile and supports OS APIs meeting real-time deterministic or non-real-time, non-deterministic requirements depending on the system or subsystem implementation.\n\nThe FACE Data Architecture defines the FACE Data Model Language (including the language binding specification), Query and Template language, FACE Shared Data Model (SDM) and the rules of construction of the Unit of Portability (UoP) Supplied Model (USM). Each PCS Unit of Conformance (UoC), PSSS UoC, or TSS UoC providing using TS Interfaces is accompanied by a USM consistent with the FACE SDM and defines its interfaces in terms of the FACE Data Model Language. A Domain-Specific Data Model (DSDM) captures content relevant to a domain of interest and can be used as a basis for USMs.\n"}
{"id": "48772489", "url": "https://en.wikipedia.org/wiki?curid=48772489", "title": "Georges Peignot", "text": "Georges Peignot\n\nGeorges Peignot (Paris, June 24, 1872 – Givenchy, September 28, 1915) was a French type designer, type founder and manager of the G. Peignot & Fils foundry until his death in the World War I. Father of four children (including poet Colette Peignot called Laure), he hoisted the G. Peignot & Fils foundry among most striking French typography companies of the twentieth century (an \"« elite house »\", according to a former French Prime Minister): in 17 years of practice, he created or launched prestigious fonts, including Grasset, Cochin and Garamont.\n\nBorn in 1872, Georges Peignot was the fourth child of eight. His father, Gustave Peignot (1839–1899), engineer graduated of Arts et Métiers school, was the head of a fixed spaces foundry (specialized in the fabrication of hand-set metal type to achieve letter-spacing) in Paris, created in 1842 by Pierre Leclerc and bought and directed by his mother, Clémentine Dupont de Vieux Pont (1815–1897), widow of Laurent Peignot.\nGeorges Peignot frequented unsuccessfully the Chaptal College in Paris, before attending an apprenticeship with his godfather, Émile Faconnet, master intaglio printer. In 1890 he was admitted to \"Arts Déco\" school. In 1891, he moved to Germany, first in Leipzig in the Schwinger foundry where he discovered the world of printing and learned punchcutting. In 1892, he was in Hamburg in the Gentzsch foundry where, with the son of the family, same age, he toured the services and workshops, continued to be passionate about types, and passed all his spare time admiring international typographic catalogs.\n\nBack in France in 1893, Georges Peignot spent two and a half years in military service, where he graduated sergeant, the highest rank for those who do not have the baccalaureate. In 1896, he married Suzanne Chardon, daughter of a master intaglio printer in charge of chalcography for the Louvre. They had four children (Charles, 1897; Madeleine, 1899; Geneviève, 1900; Colette, 1903).\nIn 1896, hired in his father's owned \"G. Peignot\" foundry, Georges Peignot was responsible for the management of recently acquired types (G. Peignot et Fils has had absorbed Cochard & David foundry and Longien foundry) and possibly for creating new fonts. In 1898, his father, suffering, transformed the company into a Kommanditgesellschaft on behalf of \"G. Peignot et Fils\" and distributed the shares to his 8 children. He had time to appoint Georges co-manager before dying the following year. In 1899, Georges Peignot became officially sole manager of the company. Board members were Robert Peignot, eldest son, engineer in charge of manufacturing, Georges Peignot, and Charles Tuleu, husband of Jane Peignot, the eldest daughter, and owner of the rival foundry Deberny. In 1906, Paul Payet, husband of Julia Peignot, the second daughter and close ally from Gustave Peignot's widow, top executive in a railways company, joined the board on the instruction of the widow.\n\nIn 1897, Georges Peignot, young industrialist aged 25, met Eugène Grasset already famous in the Art Nouveau world (for his furniture, posters, stamps, titles and patterns of books, textiles and printed wallpapers, etc.). Grasset has had freely adapted to the alphabet of Nicolas Jenson (1471) with the intention of using it to print a book on his own method for ornamental composition, inspired by the courses he gave to the Guérin school. With his father's agreement, Georges Peignot acquired Grasset's alphabet, get an official patent October 7, 1897 for the typeface under the name \"Grasset\" and gave Henri Parmentier, the workshop's punchcutter, the mission to engrave it. For harmonious compositions, he decided to offer 13 sizes of the same type, and \"« for the first time in a French workshop, the scale of sizes of a character is created by photographic reductions of a genuine drawing »\" (Thibaudeau). In the fall of 1898 comes out, printed in Grasset, \"Les aventures merveilleuses de Huon de Bordeaux, chanson de geste\" (a medieval novel, since the Middle Age is the favorite epoch of Art Nouveau). The world of typography was alerted and revealed very supportive. In 1900, only 7 sizes were punched, but orders showed up. Georges Peignot and Francis Thibaudeau, high-quality master typographer he has hired, created a small catalog, discreet but very tasteful. After the sending to all important printers, the orders flocked to the company, and in the same time compliments praised from specialized press and art connoisseurs. In the courtyard of the Boulevard de Montrouge (later renamed \"boulevard Edgar-Quinet\") where they settled for 34 years, the Peignot workshops were suddenly engorged. The company has had to move at the corner of Cabanis and Ferrus streets (XIV arrondissement) in Paris, and the new plant opens in 1904.\n\nThe success meant for Georges Peignot, 29, the recognition of his peers: he became treasurer of the Chambre syndicale (typographic trade association). His work was also copied: in June 1902 the justice seized forged types of Renault foundry and the two foundries went on court. Surprisingly, the infringement case has been lost in 1905 and the company G. Peignot & Fils has had to pay the costs for… having accused Renault foundry of being inspired by Gryphe work, a Lyon publisher of the sixteenth century, amateur of Jenson, whose work belongs to all and can be copied. The judges have had no regard for the particular draw of reed and other specific qualities of Grasset.\n\nTwo of Georges Peignot's brothers, Robert (engineer of Arts et Métiers) and Lucien (engineer of the École centrale), sailed to the United States where they know they could find the most modern automatic typographic machines.\n\nIn 1898, Georges Peignot ordered an Art Nouveau typeface to George Auriol (aka Georges Huyot), a gifted singer-poet-painter. A year later, Auriol proposed already \"la Française-légère\". Georges Peignot filed it October 11, 1899, and launched the punchcutting despite family's opposition. In 1902 the complete alphabet was available in five sizes. Success was around the corner again, but the career of the new typeface was not as productive as the Grasset: Française-légère is a fantasy typeface, unlike Didot or Garamond devoted to serious works, and it's intended for short texts, advertising, subtitles, etc. Therefore, the use is not so frequent, nor the replacement of lead fonts: for a foundry, it's not a good deal.\n\nDuring the following years the fantasy production was still privileged: Georges Peignot's foundry launched \"l’Auriol Labeur \" (Auriol book, 1904), the \"Française-allongée\" (1905), \"Auriol Champlevé\" (1906), the series of eight \"Robur\" typefaces (black, pale, striped, \"clair-de-lune\", etc., 1907). Promoting these \"fancy\" characters, Georges Peignot played with the classic structure of the letter (unchanged since the fifteenth century), and took easily the risk that its customers would sacrifice readability to the beauty of characters, very \"Art Nouveau\". He advocated a \"Typography\", which meant – for him – that a typeface comes with many sizes, italics, vignettes and ornaments. Sort of typographic philosophy!\n\nIn the continuity, the foundry launched a series of ornaments and vignettes for the Grasset typeface. The creations of George Auriol were also enshrined in two series of \"« creepers, flowers, flames »\" that Francis Thibaudeau laid out in two booklets (\"Vignettes Art Français\" et \"Ornements français\", 31 pages to be published again in the \"Spécimen général\" few months after). G. Peignot et Fils released also a booklet entitled \"Album d’application des nouvelles créations françaises\" (\"Catalog for applications of new French creations,\" 1901), real pamphlet written by Francis Thibaudeau in favor of Art Nouveau.\n\nOnly after having launched \"Grasset\" (thirteen sizes) and \"Française-légère\" (five sizes), Georges Peignot decided to publish a \"Spécimen\" and thus to benefit from the enormous success of its new characters. All the fonts created or acquired by the G. Peignot & Fils foundry were there available.\n\nThe \"Spécimen\" consists of two volumes of 450 and 200 pages (the first appears at the end of July 1903, the second in 1906). The layout is generous: 7 chapter headings in 4 colors, airy presentation of each typeface or ornament, often in two colors, with variation of different sizes, funny or informative sentences. Beneath their aesthetic success, the two volumes were also useful: all the technical details that can be used in a printshop were clearly set out in tables, lists, diagrams: return rates and prizes of old fonts, sizes of various folded formats, instructions on cutting lines, etc. The text is serious and didactic: the last chapter is written by Francis Thibaudeau, who paints a retrospective of typography and its scenery, from the Renaissance to present day.\n\nBecause the Grasset was slowing, Georges Peignot sought a new text dedicated typeface. In 1910, he launched the \"Bellery-Desfontaines\", an upscale fantasy character in rupture with Art nouveau style: any vegetal form was excluded.\n\nGeorges Peignot found inspiration in the engravings of the eighteenth century: supported by Lucien Peignot, his younger brother became co-manager and close friend, and by Francis Thibaudeau, his typography master, he noticed that the writers of this time rejected the solemn style of founders like Louis-René Luce, Fournier, Didot... and preferred to engrave themselves the text accompanying their illustrations. Georges Peignot was impassioned by the work of the writer and illustrator of Menus-Plaisirs du roi : Cochin. He proposed then a new typeface, inspired by a design found in the archives (still anonymous today), gave it the name \"Cochin\" and submitted in October 1912. That was not all. He proposed a complete typeface suite, 2000 punches (in January 1914), composed of a \"Nicolas-Cochin\" stretched to the poles, a \"Moreau-le-jeune\" champlevé, and a 200 years old fantasy typeface, \"Fournier-le-jeune\". Last but not least, adequate decorations and decorated letters were entrusted to Pierre Roy and André-Édouard Marty, illustrators to \"La Gazette du Bon Ton\".\n\nIn 1912, the Cochin suite was launched on the market in two different ways: the first coup recalled the publication of the medieval book for Grasset and consisted, before marketing the lead fonts, to compose in Cochin a new fashion magazine: \"La Gazette du Bon Ton\" (launched by Lucien Vogel of \"Vogue\", the \"Jardin des modes\", etc.). The success was great, not only because of Cochin only but also because the magazine wss beyond anything seen in terms of taste, quality of illustrations (mostly watercolors), discovery of new trends, etc. The second and most important promotional vehicle of Cochin was a high typographic quality booklet, published January 18, 1914, and sent to all Paris, including typographers, printers, artists, journalists… The trio composed by Georges Peignot, Lucien Peignot, and Francis Thibaudeau has had time to polish up their weapons of seduction: it took two years for cutting 62 alphabets, plus ornaments and vignettes. The result, as it was possible to admire it, was in the booklet, and the booklet itself: pink and gold cover, white and mid-tone laid paper, black, gold and color printing, examples or bilboquets on full pages, precious illustrations by the use of Roy and Marty vignettes… The text too, written by Lucien Peignot, is of excellent literary outfit.\n\nThanks to Cochin suite and to some very profitable and recent acquisitions (thus the \"Didot\" from the Beaudoire foundry), the profits of the company G. Peignot & Fils rose to unforeseen heights. Unfortunately, Georges Peignot did not benefit because he was put in minority within the company's board after a maneuver of his own mother (who thereby expressed his hostility to the unloved son and his preference for his two eldest, Jane and Robert, that her husband had had previously excluded from company's decisions). Added to the personal attacks within his family, Georges Peignot has had also serious concern caused by constant improvements of automatic typographic machines he intended danger since 1905. Rejected, depressed, Georges Peignot stepped back from the daily management of the company and confided it to his younger Lucien Peignot.\n\nHe was dedicated to the launch of a new character. He had noticed that the Garamond typeface of the sixteenth century has been created at a time when we printed on thick coton-based paper, in which the characters sank, leaving a greasy track. The same typeface used on a wood-based paper seemed thin. His idea was to re-draw the character with the original bold effect found on rag paper. He started manufacturing a new Garamont (\"sic\") character with the help of engraver Henri Parmentier. The result will be presented and marketed in 1926 only, 11 years after his death. It will be a great success, sustainable, prestigious.\n\nMeanwhile, in 1910, Georges Peignot commissioned the engraver Bernard Naudin for a new typeface in roman, italics and champlevé; the typeface is recorded in 1912 and 1913; but it will be placed on the market in 1924, without much success.\n\nWhen World War is declared, Georges was mobilized as adjutant of artillery of the Territorial army (composed of men aged 34–49 years, considered too old and not enough trained to integrate an active frontline regiment nor reserve). He was assigned to the 23rd Battery of the 1st Artillery Regiment and stationed at Fort Cormeilles. On September 25, 1914, his closest younger brother, André Peignot, was killed. The shock was immense for Georges Peignot. He immediately requested to be placed on the front in the same regiment as his late brother, the 23rd Colonial Infantry Regiment. In March, he succeeded and was posted on the front line. Everything went fast: May 15, 1915, the youngest of his brothers, Rémy, was killed in the same Somme sector of the front. On July 25, Georges Peignot transmitted to his maternal cousin, Henri Menut, his power as manager of the company. September 28, 1915, north of Arras, between Souchez and Givenchy, Georges Peignot was struck by a bullet in the forehead \"« immediately after shouting to his troops: \"En avant ! (Forward !)\" »,\" as Lucien Peignot reported (the fourth and last brother who will also lose his life June 29, 1916), and who had had time to conduct a long investigation to find his lost brother in the no man's land where he laid for a month. Georges Peignot, buried next to Rémy, is quoted in the order of the Division and awarded the Military Cross and Military Medal.\n\nLouis Barthou, former French Prime Minister, wrote in 1916 about Georges Peignot that he was recognized \"« for his active and open mind, impatient of initiatives, for the righteousness of his strong and loyal character, for his simmering and thoughtful passion for the noble art to which he had devoted his life. »\"\n\nGeorges Lecomte, Director of the École Estienne, said in 1918 about Georges and Lucien: \"« The Peignot brothers had conquered affectionate esteem of all book industry, of printers and publishers, of craftsmen and workers of the profession, of enthusiasts of fine editions, of writers who pay attention to how you print them. »\" They came in 1914 present him the Cochins and he still remembers \"« their tone of serious simplicity and modest satisfaction, (…) their refined but unpretentious friendliness. »\"\nIn 1922, the National \"Committee for Education and Fine Arts\" proposes to honor the history of Peignot: all the genuine punches of the Foundry and the bronze Gustave Peignot's statue are carried in the building of the Imprimerie nationale, across the Gutenberg street. The Committee proposes that the extension of this street would be called \"rue des Quatre-Frères-Peignot\" (Four-Brothers-Peignot street) in memory of the four dead brothers.\n\nThe typographer Maximilien Vox acknowledges his debt to Georges Peignot, for whom he was \"« the first French typographer who did not think of his job as confined to supplying the printer with little pieces of metal ».\"\n\nThe foundry's posterity is tainted by family maneuvers: after the war, Georges Peignot and four of his other brothers are dead (whose eldest died of illness in 1913); the potential successors are the two girls or the mother. The latter manages in 1919 to impose his surviving children or their widows an 1 million capital increase, given to a competitor, the Deberny foundry, in financial difficulty… which is the property of Jane's husband. In 1923, under the pen of Mr. Pascaut, notary, emerges a Deberny & Peignot company, result of the merger of Deberny (2.6 millions francs capital, 1 million Peignot's family included) and G. Peignot et Fils briefly renamed \"Peignot & Cie\" (4.1 millions francs). During half a century, Deberny & Peignot will struggle along, riding on its past glory, and goes out in 1974, bloodless, exposed to automatic typographic machines and phototypesetting machines, and a haphazard management.\n\nList of types created by Georges Peignot:\n\n\n"}
{"id": "40127966", "url": "https://en.wikipedia.org/wiki?curid=40127966", "title": "Jeremy Hitchcock", "text": "Jeremy Hitchcock\n\nJeremy Hitchcock (born September 11, 1981) is co-founder and former CEO of Dyn. Hitchcock resigned from the business in May 2016 prior to the Oracle acquisition, which officially closed on January 31, 2017. The rumored purchase price was $600M.\n\nHe was born in Rochester, New York, and attended Worcester Polytechnic Institute (WPI), graduating with a BS, Management Information Systems. Hitchcock began his academic career at WPI by studying chemistry but in his sophomore year, he met up with Tim Wilde, Chris Reinhardt and Tom Daly to work on a remote access project. This project has now become Dyn.\n\nUpon graduating, Hitchcock moved the company to Manchester, New Hampshire, where it is still headquartered. As CEO, he funded the company until 2012 when it finally took its first round of outside capital – $38 million from North Bridge Venture Partners. The company raised $100M total outside funding.\n\nHitchcock is an active angel investor and is currently Founder and CEO of wifi management and IoT security startup, Minim, based in Manchester, NH\n\n"}
{"id": "33404248", "url": "https://en.wikipedia.org/wiki?curid=33404248", "title": "Juliana Rotich", "text": "Juliana Rotich\n\nJuliana Rotich is an information technology professional who has developed web tools for crowdsourcing crisis information and coverage of topics related to the environment. She is the head of East Africa Country Cluster for BASF. She is also a trustee of Bankinter Foundation for Entrepreneurship and Innovation in Spain, as well as a board member of Standard Media Group and the Kenya Vision 2030 Delivery Board.\n\nRotich is from Kenya. She has a degree in information technology from the University of Missouri, and has worked in the IT industry for over ten years.\n\nUntil September 2015, Rotich was the Executive Director for \"Ushahidi,\" an Open-source software project which uses crowdsourced geolocation, mobile phone, and web reporting data to provide crisis reporting and information. \"Ushahidi\" is the Swahili word for \"testimony.\" Ushahidi was first put into practice during the Kenyan presidential election crisis of 2007-8; it has since been used in Chile, Japan, New Zealand, Australia, Pakistan, Tanzania, and Haiti.\n\nAs a blogger, Rotich has authored articles on Afrigadget.com, acted as Environmental Editor of \"Global Voices Online\", and participated in the TED Global conference in Arusha in 2007. As a public speaker, she is known for her commentary on technology in Africa and voicing concerns about the loss of indigenous forest and water catchment areas in Kenya. She is a Senior TED Fellow.\n\nIn 2014, Rotich presented at the annual Design Indaba Conference in Cape Town. From 2014 until 2015, she served on United Nations Secretary-General Ban Ki-moon's Independent Expert Advisory Group on the Data Revolution for Sustainable Development, co-chaired by Enrico Giovannini and Robin Li.\n\nIn 2017 Rotich participated in the W20 summit in Berlin, Germany and took part in a panel discussion together with Chancellor Angela Merkel, IMF director Christine Lagarde, Queen Máxima of the Netherlands, Canadian minister of foreign affairs Chrystia Freeland, Ivanka Trump and others, representing BRCK. \n\nIn 2011, Rotich was named Schwab Foundation Social Entrepreneur of the Year in Africa by the World Economic Forum.\n\n"}
{"id": "31816986", "url": "https://en.wikipedia.org/wiki?curid=31816986", "title": "Librestream", "text": "Librestream\n\nLibrestream Technologies Inc. is a privately owned, venture capital–backed company based in Winnipeg, Canada. Librestream provides technologies that enable mobile enterprise collaboration.\n\nLibrestream is notable in that its technology, consisting of unique hand-held mobile devices and accompanying software, extends traditional video conferencing and collaborative services to places previously unreachable such as an off-shore oil rig or a manufacturing plant floor a continent away.\n\nMobile collaboration is a technology-based process of communicating utilizing electronic assets and accompanying software designed for use in remote locations. Newest generation hand-held electronic devices include video, audio, and telestration (on-screen drawing) capabilities broadcast over secure networks, enabling multi-party conferencing in real time.\n\nDiffering from traditional video conferencing, mobile collaboration utilizes wireless, cellular and broadband technologies enabling effective collaboration independent of location. Where traditional video conferencing has been limited to boardrooms, offices, and lecture theatres, recent technological advancements have extended the capabilities of video conferencing for use with discreet, hand-held mobile devices, permitting true mobile collaborative possibilities.\n\nThe origins of Librestream date back to the late 1980s in Winnipeg, Canada, when Kerry Thacher co-founded Ubitrex Corporation, a small high-tech start-up that designed and developed a clinical information system for use in hospitals to capture patient data. The system included a hand-held device designed for use by clinicians. Subsequent to successful clinical use of this system, Ubitrex was sold in 1994 to U.S-based Continental Healthcare Systems.\n\nTwo years later, Thacher bought back the device side of the business from Continental and formed AirWire Mobile Technologies Inc. which continued to support the substantial base of hospitals using the technology. Only a few weeks into AirWire’s operation, Symbol Technologies Inc., a large U.S-based manufacturer of mobile devices, took notice of AirWire’s unique expertise and contracted with Airwire to create a product to help Symbol pursue the healthcare market.\n\nIn 1999, Symbol bought AirWire. Over the next few years, the team, continuing to operate out of Winnipeg, successfully designed and developed a number of mobile devices, one of which delivered Voice over IP (VoIP), a significant advancement in mobile device technology at that time. This activity led to the development of high-volume hand-held mobile expertise in Winnipeg for the first time, a capability that would later form the foundation for the creation of Librestream.\n\nIn August 2003 Thacher left Symbol, and weeks later Symbol closed its Winnipeg office. Although many of the engineers received offers from Symbol to re-locate to New York, all declined. Later in 2003, a group of eight professionals from the former Winnipeg arm of Symbol gathered to plan their next venture. In addition to Thacher, this group included Bill Gillanders, Rob McConnell, Don Freiling, Tim Braun, Kent Wotherspoon, Conway Wieler, and Chris Kavanagh.\n\nTwo things were clear: the emergence and continued growth of the Internet was certain, and there existed a tangible opportunity to develop enhanced video-handling capabilities for a next generation of hand-held devices. The group was convinced that together they could design and develop the necessary technology to allow individuals, regardless of location, the ability to collaborate in new ways. The technology would extend the boundaries of traditional video-conferencing beyond the boardroom to workplaces previously unreachable such as a manufacturing plant floor a continent away.\n\nLed by the founding team’s vision, Librestream (a combination of ‘free’ and stream’) was formed. Backed by venture capital and individual investors, Librestream developed their first alpha product, the MCD-1000, coupled with desktop collaboration software, MCA, in 2006. Eventually the mobile collaborative device evolved into the Onsight 1000, one of multiple rugged hand-held devices in the Onsight product line. The desktop collaboration software evolved into the Onsight Expert application.\n\nIn 2006, Librestream formed a marketing partnership with Tandberg, one of the leading video conferencing industry players at that time. The arrangement offered Librestream industry exposure and allowed the breadth and depth of the product to grow in response to evolving requirements. After the arrangement expired in July 2010, Librestream continued to develop the technology and support their growing customer base.\n\nBy 2010, Librestream’s customer base grew to include global enterprises, many of them Fortune 500 firms, in industries such as manufacturing, energy, healthcare, insurance, government and public safety.\n\nThe impact of mobile collaboration technology is significant in its potential to change the way people work. Live, visual interaction removes traditional restrictions of distance and time. Business processes are optimized through accelerated problem resolution, reductions in downtimes and travel, improvements in customer service and increased productivity.\n\nLibrestream works with three strategic partners. With Cisco Systems, Librestream is a Registered Cisco Developer Network member, and Onsight is a core component of the Cisco Manufacturing Mobile Video Collaboration (MMVC) solution. With Inmarsat, Librestream has successfully tested the Onsight system over the Inmarsat BGAN satellite network to provide mobile collaboration to land and maritime satellite customers. With Verizon Wireless, Librestream has tested and optimized the Onsight mobile devices for 4G LTE networks.\n\n"}
{"id": "13107862", "url": "https://en.wikipedia.org/wiki?curid=13107862", "title": "List of anti-ship missiles", "text": "List of anti-ship missiles\n\nThis is a list of anti-ship missiles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "44016300", "url": "https://en.wikipedia.org/wiki?curid=44016300", "title": "List of automated transit networks suppliers", "text": "List of automated transit networks suppliers\n\nThis is a list of well-known automated transit networks suppliers.\n\nCurrently, five advanced transit networks (ATN) systems are operational, and several more are in the planning stage.\n\nThe following table summarizes several well-known automated transit networks (ATN) suppliers as of 2016.\n"}
{"id": "18729407", "url": "https://en.wikipedia.org/wiki?curid=18729407", "title": "Micro Four Thirds system", "text": "Micro Four Thirds system\n\nThe Micro Four Thirds system (MFT or M4/3) is a standard released by Olympus and Panasonic in 2008, for the design and development of mirrorless interchangeable lens digital cameras, camcorders and lenses. Camera bodies are available from Blackmagic, DJI, JVC, Kodak, Olympus, Panasonic, and Xiaomi. MFT lenses are produced by Cosina Voigtländer, DJI, Kowa, Kodak, Mitakon, Olympus, Panasonic, Samyang, Sigma, SLR Magic, Tamron, Tokina, Veydra, and Xiaomi, amongst others.\n\nMFT shares the original image sensor size and specification with the Four Thirds system, designed for DSLRs. Unlike Four Thirds, the MFT system design specification does not provide space for a mirror box and a pentaprism, which facilitates smaller body designs and a shorter flange focal distance, and hence smaller lenses. With adapters, most lenses can be used on MFT camera bodies, including those produced by Canon and Nikon, and lenses produced for cinema, \"e.g.\", PL mount or C mount.\n\n\"For comparison of the original Four Thirds with competing DSLR system see Four Thirds system#Advantages, disadvantages and other considerations\"\n\nCompared to most digital compact cameras and many bridge cameras, MFT cameras have better, larger sensors, and interchangeable lenses. They provide far greater control over depth-of-field than compact cameras. There are many lenses available. On top of this, a large number of other lenses (even from the analogue film era) can be fitted using an adapter. Different lenses yield greater creative possibilities. However, Micro Four Thirds cameras also tend to be slightly larger, heavier and more expensive than compact cameras.\n\nCompared to most digital SLRs, the Micro Four Thirds system (body and lenses) is smaller and lighter. However, their sensors are smaller than full-frame or even APS-C systems. As such, they may produce noisier/grainier images in low light conditions when compared with contemporary cameras with larger sensors. Unlike DSLRs, which use an optical viewfinder, Micro Four Thirds cameras use an electronic viewfinder. Resolutions and refresh speeds on these EVF displays were originally compared negatively to optical viewfinders, but today's EVF systems are faster, brighter and much higher resolution than the original displays. Micro Four Thirds cameras always afford a greater depth-of-field than SLRs when shooting at the same focal length and aperture, but it is more difficult to design a wide-aperture lens for Micro Four Thirds. Original Micro Four Thirds cameras used a contrast-detection autofocus system, slower than the phase-detect autofocus that is standard on DSLRs. To this day most Micro Four Thirds cameras continue to use a contrast-based focussing system. Although some current models, such as the Olympus OM-D E-M1 Mark II, feature a hybrid phase-detect/contrast detect system, Panasonic Lumix cameras have continued to use a contrast-based system called DFD (Depth from Defocus). Both systems today provide focussing speeds to rival or even surpass many current DSLRs.\n\nThe image sensor of Four Thirds and MFT measures 18 mm × 13.5 mm (22.5 mm diagonal), with an imaging area of 17.3 mm × 13.0 mm (21.6 mm diagonal), comparable to the frame size of 110 film.<ref name='Olympus-Europe 4/3'></ref> Its area, ca. 220 mm², is approximately 30% less than the quasi-APS-C sensors used in other manufacturers' DSLRs; it is around 9 times larger than the 1/2.3\" sensors typically used in compact digital cameras.\n\nThe Four Thirds system uses a 4:3 image aspect ratio, like compact digital cameras. In comparison, DSLRs usually adhere to the 3:2 aspect ratio of the traditional 35 mm format. Thus, \"Four Thirds\" refers to both the size and the aspect ratio of the sensor. However, the chip diagonal is shorter than 4/3 of an inch; the 4/3 inch designation for this size of sensor dates back to the 1950s and vidicon tubes, when the external diameter of the camera tube was measured, not the active area.\n\nThe MFT design standard also specifies multiple aspect ratios: 4:3, 3:2, 16:9 (the native HD video format specification), and 1:1 (a square format). With the exception of two MFT cameras, all MFT cameras record in a native 4:3 format image aspect ratio, and through cropping of the 4:3 image, can record in 16:9, 3:2 and 1:1 formats.\n\nIn addition, all current Micro Four Thirds cameras have sensor dust removal technologies.\n\nThe MFT system design specifies a bayonet type lens mount with a flange focal distance of 19.25 mm. By avoiding internal mirrors, the MFT standard allows a much thinner camera body.\n\nViewing is achieved on all models by live view electronic displays with LCD screens. In addition, some models feature a built-in electronic viewfinder (EVF), while others may offer optional detachable electronic viewfinders.\nAn independent optical viewfinder typically matched to a particular non-zoom prime lens is sometimes an option.\n\nThe flange diameter is about 38 mm, 6 mm less than that of the Four Thirds system. Electrically, MFT uses an 11-contact connector between lens and camera, adding to the nine contacts in the Four Thirds system design specification. Olympus claims full backward compatibility for many of its existing Four Thirds lenses on MFT bodies, using a purpose built adapter with both mechanical and electrical interfaces.\n\nThe shallow but wide MFT lens mount also allows the use of existing lenses including Leica M, Leica R, and Olympus OM system lenses, via Panasonic and Olympus adapters. Aftermarket adapters include Leica Screw Mount, Contax G, C mount, Arri PL mount, Praktica, Canon, Nikon, and Pentax, amongst others. In fact, almost any still camera, movie or video camera interchangeable lens that has a flange focal distance greater than or marginally less than 20 mm can often be used on MFT bodies via an adapter. While MFT cameras can use many of these \"legacy\" lenses only with manual focus and manual aperture control mode, hundreds of lenses are available, even those designed for cameras no longer in production.\n\nWhile lens manufacturers seldom publish lens mount specifications, the µ4/3rds mount has been reverse-engineered by enthusiasts, with CAD files available.\n\nMFT cameras usually use contrast-detection autofocus (CDAF), a common autofocus system for mirrorless compact or \"point-and-shoot\". By comparison, DSLRs use phase-detection autofocus (PDAF). The use of separate PDAF sensors has been favored in DSLR systems because of mirror box and pentaprism design, along with better performance for fast-moving subjects.\n\nThe (non-Micro) Four Thirds system design standard specifies a 40 mm flange focal length distance, which allowed for using a single lens reflex design, with mirror box and pentaprism. Four Thirds DSLR cameras designed by Olympus and Panasonic initially used exclusively PDAF focusing systems. Olympus then introduced the first live view DSLR camera, which incorporated both traditional DSLR phase focus and also optional contrast detection focus. As a result, newer Four Thirds system lenses were designed both for PDAF and contrast focus. Several of the Four Thirds lenses focus on Micro Four Thirds proficiently when an electrically compatible adapter is used on the Micro Four Thirds cameras, and they focus on Micro Four Thirds cameras much quicker than earlier generation Four Thirds lenses can.\n\nSome MFT cameras, such as the OM-D E-M1 and E-M1 Mark II incorporate phase-detection hardware on the sensor to support legacy lenses. These camera bodies perform better with legacy lenses (eg, focus performance of the 150mm f/2 and 300mm f/2.8 lenses are as quick and accurate as a native Four Thirds body).\n\nThe much shorter flange focal distance enabled by the removal of the mirror allows normal and wide angle lenses to be significantly smaller because they do not have to use strongly retrofocal designs.\n\nThe Four Thirds sensor format used in MFT cameras is equivalent to a 2.0 crop factor when compared to a 35 mm film (full frame) camera. This means that the field of view of an MFT lens is the same as a full frame lens with twice the focal length. For example, a 50 mm lens on a MFT body would have a field of view equivalent to a 100 mm lens on a full frame camera. For this reason, MFT lenses can be smaller and lighter because to achieve the equivalent 35 mm film camera field of view, the MFT focal length is much shorter. See the table of lenses below to understand the differences better. For comparison, typical DSLR sensors, such as Canon's APS-C sensors, have a crop factor of 1.6.\n\nThis section gives a brief introduction to the subject of \"equivalence\" in photography. Equivalent images are made by photographing the same angle of view, with the same depth of field and the same Angular resolution due to diffraction limitation (which requires different f-stops on different focal length lenses), the same motion blur (requires the same shutter speed), therefore the ISO setting must differ to compensate for the f-stop difference. The use of this is only to let us compare the effectiveness of the sensors given the same amount of light hitting them. In normal photography with any one camera, equivalence is not necessarily an issue: there are several lenses faster than f/2.4 for Micro Four Thirds (see the tables under Fixed Focal Length Lenses, below), and there are certainly many lenses faster than f/4.8 for full frame and no one hesitates to use them even though they can have shallower depth of field than a Nikon 1 at f/1.7, in fact that can be seen as advantageous, but it has to be taken into consideration that a further aspect of image resolution is limitation by optical aberration, which can be compensated the better the smaller the focal lengths of a lens is. Lenses designed for mirrorless camera systems such as Nikon 1 or Micro Four Thirds often use image-space telecentric lens designs, which reduce shading and therefore light loss and blurring at the microlenses of the image sensor. Furthermore, in low light conditions by using low f-numbers a too-shallow depth of field can lead to less satisfying image results, especially in videography, when the object being filmed by the camera or the camera itself is moving. For those interested in producing equivalent images, read on.\n\nEquivalent focal lengths are given, if the angle of view is identical.\n\nThe depth of field is identical, if angle of view and absolute aperture width are identical. Also the relative diameters of the Airy disks representing the limitation by diffraction are identical. Therefore, the equivalent f-numbers are varying.\n\nIn this case, i.e., with the same luminous flux within the lens, the illuminance quadracially decreases and the luminous intensity quadratically increases with the image size. Therefore, all systems detect the same luminances and the same exposure values in the image plane, and as a consequence of this the equivalent exposure indexes (respectively equivalent ISO speeds) are different in order to get the identical shutter speeds (i.e., exposure times) with the same levels of motion blur and image stabilisation. Furthermore, for a given guide number of a photoflash device all systems have the same exposure at the same flash-to-subject distance.\n\nThe following table exemplarily shows a few identical image parameters for some popular image sensor classes compared to Micro Four Thirds: The smaller the focal length, the smaller is also the displacement in the image space between the last principal plane of the lens and the image sensor in order to focus a certain object. Therefore, the energy needed for focussing as well as the appropriate delay for shifting the focussing lens system are shorter, the smaller the focal length is.\n\nMicro Four Thirds has several advantages over larger format cameras and lenses:\n\nThough many DSLRs also have \"live view\" functionality, these function relatively poorly compared to a Micro Four Thirds electronic viewfinder (EVF), which has the following advantages:\n\nOlympus and Panasonic approached the implementation of electronic viewfinders in two ways: the built-in EVF, and the optional hotshoe add-on EVF.\n\nUntil the introduction of the OM-D E-M5 in February, 2012, none of the Olympus designs included a built-in EVF. Olympus has four available add-on hotshoe viewfinders. The Olympus VF-1 is an optical viewfinder with an angle of view of 65 degrees, equivalent to the 17mm pancake lens field of view, and was designed primarily for the EP-1. Olympus has since introduced the high resolution VF-2 EVF, and a newer, less expensive, slightly lower resolution VF-3 for use in all its MFT cameras after the Olympus EP-1. These EVF's not only slip into the accessory hotshoe, but also plug into a dedicated proprietary port for power and communication with Olympus cameras only. Both the VF-2 and VF-3 may also be used on high-end Olympus compact point and shoot cameras such as the Olympus XZ-1. Olympus announced the VF-4 in May 2013, along with the fourth generation PEN flagship, the E-P5.\n\nAs of mid-2011, Panasonic G and GH series cameras have built in EVF's, while two of the three GF models are able to use the add-on LVF1 hotshoe EVF. The LVF1 must also plug into a proprietary port built into the camera for power and communication. This proprietary port and the accessory is omitted in the Panasonic Lumix DMC-GF3 design. Similar to Olympus, the LVF1 is usable on high-end Panasonic compact point and shoot cameras, such as the Panasonic Lumix DMC-LX5.\n\n\n\n\nDue to the short native flange distance of the Micro Four Thirds System, the usage of adapted lenses from practically all formats has become widely popular. Because lenses can be used from old and abandoned camera systems, adapted lenses typically represent good value for the money. Adapters ranging from low- to high-quality are readily available for purchase online. Canon FD, Nikon F (G lenses require special adapters), MD/MC, Leica M, M42 Screw Mount, and C-mount Cine lenses to name a few are all easily adaptable to the Micro Four Thirds system with glassless adapters resulting in no induced loss of light or sharpness.\n\nAdapted lenses retain their native focal lengths but field of view is reduced by half —i.e., an adapted 50mm lens is still a 50mm lens in terms of focal length but has a narrower FOV equivalent to a 100mm lens due to the Micro Four Thirds System 2x crop factor. Therefore, most adapted glass from the 35mm film era and current DSLR lineups provide effective fields of view varying from normal to extreme telephoto. Wide angles are generally not practical for adapted use from both an image quality and value point of view.\n\nUsing older adapted lenses on micro four thirds sometimes leads to a slight losses in image quality. This is the result of placing high resolution demands on the center crop of decade old 35mm lenses. Therefore, 100% crops from the lenses do not usually represent the same level of pixel-level sharpness as they would on their native formats. Another slight disadvantage of using adapted lenses can be size. By using a 35mm film lens, one would be using a lens that casts an image circle that is far larger than what is required by Micro Four Thirds Sensors.\n\nThe main disadvantage of using adapted lenses however, is that focus is manual even with natively autofocus lenses. Full metering functionality is maintained however, as are some automated shooting modes (aperture priority). A further disadvantage with some LM and LTM lenses is that lenses with significant rear protrusions simply do not fit inside the camera body and risk damaging lens or body. An example is the Biogon type of lens.\n\nOverall, the ability to use adapted lenses gives Micro Four Thirds a great advantage in overall versatility and the practice has gained a somewhat cult following. Image samples can be found readily online, and in particular on the MU-43 adapted lenses forum.\n\nAs of , Olympus, Panasonic, Cosina Voigtländer, Carl Zeiss AG, Jos. Schneider Optische Werke GmbH, Komamura Corporation, Sigma Corporation, Tamron, Astrodesign, Yasuhara, and Blackmagic Design have a commitment to the Micro Four Thirds system.\n\nThe first Micro Four Thirds system camera was Panasonic Lumix DMC-G1, which was launched in Japan in October 2008. In April 2009, Panasonic Lumix DMC-GH1 with HD video recording added to it. The first Olympus model, the Olympus PEN E-P1, was shipped in July 2009.\n\nIn August 2013 SVS Vistek GmbH in Seefeld, Germany introduced the first high-speed industrial MFT lens mount camera using 4/3\" sensors from Truesense Imaging, Inc (formally Kodak sensors), now part of ON Semiconductor. Their Evo \"Tracer\" cameras range from 1 megapixels at 147 frames per second (fps) to 8 megapixels at 22 fps.\n\nIn 2014, JK Imaging Ltd., which holds the Kodak brand, released its first Micro Four Thirds camera, the Kodak Pixpro S-1; several lenses and niche camera makers have products made for the standard. In 2015, DJI provided its drone with optional MFT cameras. Both cameras can capture 16MP stills and up to 4K/30fps video with an option of 4 interchangeable lenses ranging from 12mm to 17mm. In 2016, Xiaomi introduced the YI M1, a 20MP MFT camera with 4K video capability.\n\nBlackmagic design has a range of cameras made for cinematography.\n\nBecause the flange focal distance of Micro Four Thirds cameras are shorter than DSLRs, most lenses are smaller and cheaper.\n\nOf particular interest in illustrating this fact are the Panasonic 7–14 mm ultra-wide angle (equivalent to 14–28 mm in the 35 mm film format) and the Olympus M.Zuiko Digital ED 9–18 mm ultra wide-angle lens (equivalent to an 18–36 mm zoom lens in the 35 mm film format). This feature also permitted the lens designers to develop the world's fastest fisheye lens with autofocus, the Olympus ED 8 mm f/1.8.\n\nOn the telephoto end, the Panasonic 100–300 mm or the Leica DG 100-400 mm as well as the Olympus 75–300 mm zooms show how small and light extreme telephotos can be made. The 400 mm focal length in Micro Four Thirds has the same angle of view as a 800 mm focal length in full frame cameras.\n\nWhen compared to a full frame camera lens providing a similar angle of view, rather than weighing a few kilograms (several pounds) and generally having a length of over 60 cm (2 ft) end to end, the optically stabilized Panasonic Lumix G Vario 100–300 mm lens weighs just 520 grams (18.3 oz), is only 126 mm (5.0 in) long, and uses a relatively petite 67 mm filter size. As a point of comparison, the Nikon 600 mm f5.6 telephoto weighs 3600 grams (7.9 lb), is 516.5 mm (20.3 in) in length and uses a custom 122 mm filter.\n\nOlympus and Panasonic have both produced cameras with sensor-based stabilization, and lenses with stabilization. However, the lens stabilization will only work together with body stabilization for cameras of the same brand. Before 2013, Olympus and Panasonic approached image stabilization (IS) differently. Olympus used sensor-shift image stabilization only, which it calls IBIS (In-Body Image Stabilization), a feature included all of its cameras. Until 2013, Panasonic used lens-based stabilization only, called Mega OIS or Power OIS. These stabilize the image by shifting a small optical block within the lens.\n\nIn 2013, Panasonic began including sensor-based stabilization in its cameras, beginning with the Lumix DMC-GX7. Panasonic called the combination of lens and body stabilization \"Dual IS,\" and this function won an award of the European Imaging and Sound Association (EISA) in the category Photo Innovation 2016-2017. In 2016, Olympus added lens-based stabilization to the M. Zuiko 300mm f/4.0 Pro telephoto prime lens and the M. Zuiko 12-100mm f/4.0 IS Pro lens.\n\nPanasonic claims that OIS is more accurate because the stabilization system can be designed for the particular optical characteristics of each lens. A disadvantage of this approach is that the OIS motor and shift mechanism must be built into each lens, making lenses more expensive than comparable non-OIS lenses. Of all Panasonic lenses only few with short focal lengths, and therefore wide angles of view and low susceptibility to image shaking, are not image stabilized, including the 8 mm fisheye, 7–14 mm wide angle zoom, 14 mm prime, the 15 mm prime, and the 20 mm prime.\n\nThe advantage of in-body IS is that even unstabilized lenses can make use of the in-body stabilization.\n\nSince most Micro Four Thirds lenses have neither a mechanical focusing ring nor an aperture ring, adapting these lenses for other camera mounts is impossible or compromised. A variety of companies manufacture adapters to use lenses from nearly any legacy lens mount (such lenses, of course, support no automatic functions.) For the Four Third lenses that can be mounted on MFT bodies, see Four Thirds system lenses. For the Four Third lenses that support AF, see the Olympus website. For those that support fast AF (Imager AF), see the Olympus website.\n\nWide zoom lenses\nStandard zoom lenses\n\nTelephoto zoom lenses\nSuperzoom lenses\nOn Jan 9, 2012 Sigma announced its first two lenses for Micro Four Thirds, the \"30mm 2.8 EX DN and the 19mm 2.8 EX DN lenses in Micro Four Thirds mounts\". In a press release posted on January 26, 2012, Olympus and Panasonic jointly announced that \"ASTRODESIGN, Inc., Kenko Tokina Co., Ltd. and Tamron Co., Ltd. join[ed] the Micro Four Thirds System Standard Group\". On January 26, 2012, Tokina and Tamron have indicated they would be designing lenses for the Micro 4/3 system as well. To date, both have released a single lens for the system, each.\n\nTable notes\n\n3D lenses\n\nDigiscoping lenses\n\nPinhole\n\nOn July 27, 2010 Panasonic announced the development of a three-dimensional optic solution for the Micro Four Thirds system. A specially designed lens allows it to capture stereo images compatible with VIERA 3D-TV-sets and Blu-ray 3D Disc Players.\n\n\n"}
{"id": "12074649", "url": "https://en.wikipedia.org/wiki?curid=12074649", "title": "NFPA 70B", "text": "NFPA 70B\n\nNFPA 70B (\"Recommended Practice for Electrical Equipment Maintenance\") is a standard of the National Fire Protection Association that addresses recommended electrical equipment maintenance.\nNFPA 70B is part of NFPA 70.\n\nThis recommended practice applies to preventive maintenance for electrical, electronic, and communication systems and equipment and is not intended to duplicate or supersede instructions that manufacturers normally provide. Systems and equipment covered are typical of those installed in industrial plants, institutional and commercial buildings, and large multifamily residential complexes.\n\n\n"}
{"id": "35997832", "url": "https://en.wikipedia.org/wiki?curid=35997832", "title": "National Centre for Earth Observation", "text": "National Centre for Earth Observation\n\nThe National Centre for Earth Observation (NCEO) is part of the Natural Environment Research Council (NERC) and looks at improving knowledge of the earth by taking observations from space and aircraft, as well as observations from the ground to produce models which will help understand, respond and adapt to climate and environmental changes.\n\nThe organisation was previously centred at the University of Reading, known nationally for its department of meteorology, which requires satellite observation of the Earth. The NCEO is now at the University of Leicester, known for its research into astronomy and its National Space Centre. \n\nThe NCEO is based in the United Kingdom and works closely with the UK satellite industry. It is a part of the larger Natural Environment Research Council (NERC).\n\n\n"}
{"id": "20165277", "url": "https://en.wikipedia.org/wiki?curid=20165277", "title": "New England Biotech Association", "text": "New England Biotech Association\n\nThe New England Biotech Association (NEBA) is a coalition of biotechnology companies, academic institutions, pharmaceutical companies, and trade organizations from all six New England States.\n\nNEBA serves as a regional policy and public affairs voice for the biotechnology and biopharmaceutical industry.\n\nNEBA is a non-profit, member driven organization, with over 600 members. \n\nThe Chairman of NEBA is Paul Pescatello, Director of Connecticut United for Research Excellence -(CURE.) \n\nIn 2010, NEBA advocated against measures that would harm the biotechnology industry in Maine and other New England states. The organization also launched a website www.MassRxHelp.org to help consumers save money on prescription medications. \n\n"}
{"id": "58246", "url": "https://en.wikipedia.org/wiki?curid=58246", "title": "Nitrocellulose", "text": "Nitrocellulose\n\nNitrocellulose (also known as cellulose nitrate, flash paper, flash cotton, guncotton, and flash string) is a highly flammable compound formed by nitrating cellulose through exposure to nitric acid or another powerful nitrating agent. When used as a propellant or low-order explosive, it was originally known as guncotton.\n\nPartially nitrated cellulose has found uses as a plastic film and in inks and wood coatings. In 1862, the first man-made plastic, nitrocellulose (branded Parkesine), was created by Alexander Parkes from cellulose treated with nitric acid and a solvent. In 1868, American inventor John Wesley Hyatt developed a plastic material he named Celluloid, improving on Parkes' invention by plasticizing the nitrocellulose with camphor so it could be processed into finished form and used as a photographic film. Celluloid was used by Kodak, and other suppliers, from the late 1880s as a film base in photography, X-ray films, and motion-picture films, and was known as nitrate film. After numerous fires caused by unstable nitrate films, \"safety film\" (cellulose acetate film) started to be used from the 1930s in the case of X-ray stock and from 1948 for motion-picture film.\n\nHenri Braconnot discovered in 1832 that nitric acid, when combined with starch or wood fibers, would produce a lightweight combustible explosive material, which he named \"xyloïdine\". A few years later in 1838, another French chemist, Théophile-Jules Pelouze (teacher of Ascanio Sobrero and Alfred Nobel), treated paper and cardboard in the same way. Jean-Baptiste Dumas obtained a similar material, which he called \"nitramidine\". These substances were highly unstable and were not practical explosives.\n\nHowever, around 1846 Christian Friedrich Schönbein, a German-Swiss chemist, discovered a more practical solution.\n\nAs he was working in the kitchen of his home in Basel, he spilled a mixture of nitric acid and sulfuric acid (HSO) on the kitchen table. He reached for the nearest cloth, a cotton apron, and wiped it up. He hung the apron on the stove door to dry, and as soon as it was dry, a flash occurred as the apron ignited. His preparation method was the first to be widely imitated—one part of fine cotton wool to be immersed in 15 parts of an equal blend of sulfuric and nitric acids. After two minutes, the cotton was removed and washed in cold water to set the esterification level and remove all acid residue. It was then slowly dried at a temperature below 40°C (about 100°F). Schönbein collaborated with the Frankfurt professor Rudolf Christian Böttger, who had discovered the process independently in the same year. By coincidence, a third chemist, the Brunswick professor F. J. Otto had also produced guncotton in 1846 and was the first to publish the process, much to the disappointment of Schönbein and Böttger.\n\nThe process uses nitric acid to convert cellulose into cellulose nitrate and water:\nThe sulfuric acid is present as a catalyst to produce the nitronium ion, . The reaction is first order and proceeds by electrophilic substitution at the C−OH centers of the cellulose.\n\nGuncotton is made by treating cotton (used as the source of cellulose) with concentrated sulfuric acid and 70% nitric acid cooled to 0°C to produce cellulose trinitrate. While guncotton is dangerous to store, the hazards it presents can be reduced by storing it dampened with various liquids, such as alcohol. For this reason, accounts of guncotton usage dating from the early 20th century refer to \"wet guncotton\".\n\nThe power of guncotton made it suitable for blasting. As a projectile driver, it had around six times the gas generation of an equal volume of black powder and produced less smoke and less heating.\n\nThe patent rights for the manufacture of guncotton were obtained by John Hall & Son in 1846, and industrial manufacture of the explosive began at a purpose-built factory at Marsh Works in Faversham, Kent, a year later. However, the manufacturing process was not properly understood and few safety measures were put in place. A serious explosion in July of that year killed almost two dozen workers, resulting in the immediate closure of the plant. Guncotton manufacture ceased for over 15 years until a safer procedure could be developed.\n\nFurther research indicated the importance of very careful washing of the acidified cotton. Unwashed nitrocellulose (sometimes called pyrocellulose) may spontaneously ignite and explode at room temperature, as the evaporation of water results in the concentration of unreacted acid.\n\nThe British chemist Frederick Augustus Abel developed the first safe process for guncotton manufacture, which he patented in 1865. The washing and drying times of the nitrocellulose were both extended to 48 hours and repeated eight times over. The acid mixture was changed to two parts sulfuric acid to one part nitric. Nitration can be controlled by adjusting acid concentrations and reaction temperature. Nitrocellulose is soluble in a mixture of alcohol and ether until nitrogen concentration exceeds 12%. Soluble nitrocellulose, or a solution thereof, is sometimes called collodion.\nGuncotton containing more than 13% nitrogen (sometimes called insoluble nitrocellulose) was prepared by prolonged exposure to hot, concentrated acids for limited use as a blasting explosive or for warheads of underwater weapons such as naval mines and torpedoes. Safe and sustained production of guncotton began at the Waltham Abbey Royal Gunpowder Mills in the 1860s, and the material rapidly became the dominant explosive, becoming the standard for military warheads, although it remained too potent to be used as a propellant. More-stable and slower-burning collodion mixtures were eventually prepared using less-concentrated acids at lower temperatures for smokeless powder in firearms. The first practical smokeless powder made from nitrocellulose, for firearms and artillery ammunition, was invented by French chemist Paul Vieille in 1884.\n\nJules Verne viewed the development of guncotton with optimism. He referred to the substance several times in his novels. His adventurers carried firearms employing this substance. The most noteworthy reference is in his \"From the Earth to the Moon\", in which guncotton was used to launch a projectile into space.\n\nOn May 2, 1887, Hannibal Goodwin filed a patent for \"a photographic pellicle and process of producing same ... especially in connection with roller cameras\", but the patent was not granted until September 13, 1898. In the meantime, George Eastman had already started production of roll-film using his own process.\n\nNitrocellulose was used as the first flexible film base, beginning with Eastman Kodak products in August, 1889. Camphor is used as a plasticizer for nitrocellulose film, often called nitrate film. Goodwin's patent was sold to Ansco, which successfully sued Eastman Kodak for infringement of the patent and was awarded $5,000,000 in 1914 to Goodwin Film.\n\nNitrate film was used for X-ray photography for some time, where its flammability hazard was most acute, thus in 1933, became disused for such purposes, along with its uses for motion-picture films in 1951, where it was replaced by safety film with an acetate base. Nitrocellulose X-ray film ignition was the cause behind the Cleveland Clinic fire of 1929 in Cleveland, Ohio, which claimed the lives of 123 people during the fire, and a number who were rescued, but died several days later due to inhalation of the toxic smoke.\nThe use of nitrocellulose film for motion pictures led to the requirement for fireproof projection rooms with wall coverings made of asbestos. A training film for projectionists included footage of a controlled ignition of a reel of nitrate film, which continued to burn when fully submerged in water. Unlike many other flammable materials, nitrocellulose does not need air to keep burning, as the material contains sufficient oxygen within its molecular structure. Once burning, it is extremely difficult to extinguish. Immersing burning film in water may not extinguish it, and could actually increase the amount of smoke produced. Owing to public safety precautions, the London Underground forbade transport of movies on its system until well past the introduction of safety film.\n\nCinema fires caused by ignition of nitrocellulose film stock were the cause of the 1926 Dromcolliher cinema tragedy in County Limerick in which 48 people died and the 1929 Glen Cinema disaster in Paisley, Scotland, which killed 69 children. Today, nitrate film projection is normally highly regulated and requires extensive precautionary measures including extra projectionist health and safety training. Projectors certified to run nitrate films have many precautions, among them the chambering of the feed and takeup reels in thick metal covers with small slits to allow the film to run through. The projector is modified to accommodate several fire extinguishers with nozzles aimed at the film gate. The extinguishers automatically trigger if a piece of flammable fabric placed near the gate starts to burn. While this triggering would likely damage or destroy a significant portion of the projection components, it would prevent a fire which could cause far greater damage. Projection rooms may be required to have automatic metal covers for the projection windows, preventing the spread of fire to the auditorium. The Dryden Theatre at the George Eastman Museum is one of a few theaters in the world that is capable of safely projecting nitrate films, and regularly screens films to the public.\n\nNitrocellulose was found to gradually decompose, releasing nitric acid and further catalyzing the decomposition (eventually into a flammable powder). Decades later, storage at low temperatures was discovered as a means of delaying these reactions indefinitely. The great majority of films produced during the early 20th century are thought to have been lost either through this accelerating, self-catalyzed disintegration or through studio warehouse fires. Salvaging old films is a major problem for film archivists (see film preservation).\n\nNitrocellulose film base manufactured by Kodak can be identified by the presence of the word 'nitrate' in dark letters along one edge; the word only in clear letters on a dark background indicates derivation from a nitrate base original negative or projection print, but the film in hand itself may be a later print or copy negative, made on safety film. Acetate film manufactured during the era when nitrate films were still in use was marked 'Safety' or 'Safety Film' along one edge in dark letters. 8, 9.5, and 16 mm film stocks, intended for amateur and other nontheatrical use, were never manufactured with a nitrate base in the west, but rumors exist of 16 mm nitrate film having been produced in the former Soviet Union and/or China.\n\nCellulose is treated with sulfuric acid and potassium nitrate to give cellulose mononitrate. This was used commercially as 'celluloid', a highly flammable plastic used in the first half of the 20th century for lacquers and photographic film. \n\nNitrate dominated the market for professional-use 35 mm motion picture film from the industry's origins to the early 1950s. While cellulose acetate-based so-called \"safety film\", notably cellulose diacetate and cellulose acetate propionate, was produced in the gauge for small-scale use in niche applications (such as printing advertisements and other short films to enable them to be sent through the mails without the need for fire safety precautions), the early generations of safety film base had two major disadvantages relative to nitrate: it was much more expensive to manufacture, and considerably less durable in repeated projection. The cost of the safety precautions associated with the use of nitrate was significantly lower than the cost of using any of the safety bases available before 1948. These drawbacks were eventually overcome with the launch of cellulose triacetate base film by Eastman Kodak in 1948. Cellulose triacetate superseded nitrate as the film industry's mainstay base very quickly. While Kodak had discontinued some nitrate film stocks earlier, they stopped producing various nitrate roll films in 1950 and ceased production of nitrate 35 mm motion picture film in 1951.\n\nThe crucial advantage cellulose triacetate had over nitrate was that it was no more of a fire risk than paper (the stock is often referred to as \"non-flam\": this is true—but it is combustible, just not in as volatile or as dangerous a way as nitrate), while it almost matched the cost and durability of nitrate. It remained in almost exclusive use in all film gauges until the 1980s, when polyester/PET film began to supersede it for intermediate and release printing.\n\nPolyester is much more resistant to polymer degradation than either nitrate or triacetate. Although triacetate does not decompose in as dangerous a way as nitrate does, it is still subject to a process known as deacetylation, often nicknamed \"vinegar syndrome\" (due to the acetic acid smell of decomposing film) by archivists, which causes the film to shrink, deform, become brittle and eventually unusable. PET, like cellulose mononitrate, is less prone to stretching than other available plastics. By the late 1990s, polyester had almost entirely superseded triacetate for the production of intermediate elements and release prints.\n\nTriacetate remains in use for most camera negative stocks because it can be \"invisibly\" spliced using solvents during negative assembly, while polyester film can only be spliced using adhesive tape patches or ultrasonically, both of which leave visible marks in the frame area. Also, polyester film is so strong, it will not break under tension and may cause serious damage to expensive camera or projector mechanisms in the event of a film jam, whereas triacetate film breaks easily, reducing the risk of damage. Many were opposed to the use of polyester for release prints for precisely this reason, and because ultrasonic splicers are very expensive items, beyond the budgets of many smaller theaters. In practice, though, this has not proved to be as much of a problem as was feared. Rather, with the increased use of automated long-play systems in cinemas, the greater strength of polyester has been a significant advantage in lessening the risk of a film performance being interrupted by a film break.\n\nDespite its self-oxidizing hazards, nitrate is still regarded highly as the stock is more transparent than replacement stocks, and older films used denser silver in the emulsion. The combination results in a notably more luminous image with a high contrast ratio.\n\n\nBecause of its explosive nature, not all applications of nitrocellulose were successful. In 1869, with elephants having been poached to near extinction, the billiards industry offered a US$10,000 prize to whomever came up with the best replacement for ivory billiard balls. John Wesley Hyatt created the winning replacement, which he created with a new material he invented called camphored nitrocellulose—the first thermoplastic, better known as celluloid. The invention enjoyed a brief popularity, but the Hyatt balls were extremely flammable, and sometimes portions of the outer shell would explode upon impact. An owner of a billiard saloon in Colorado wrote to Hyatt about the explosive tendencies, saying that he did not mind very much personally but for the fact that every man in his saloon immediately pulled a gun at the sound. The process used by Hyatt to manufacture the billiard balls, patented in 1881, involved placing the mass of nitrocellulose in a rubber bag, which was then placed in a cylinder of liquid and heated. Pressure was applied to the liquid in the cylinder, which resulted in a uniform compression on the nitrocellulose mass, compressing it into a uniform sphere as the heat vaporized the solvents. The ball was then cooled and turned to make a uniform sphere. In light of the explosive results, this process was called the \"Hyatt gun method\".\n\nCollodion, a solution of nitrocellulose in ether and ethanol, is a flammable liquid.\n\nWhen dry, nitrocellulose is explosive and can be ignited with heat, spark, or friction. An overheated container of dry nitrocellulose is believed to be the initial cause of the 2015 Tianjin explosions.\n\n"}
{"id": "316405", "url": "https://en.wikipedia.org/wiki?curid=316405", "title": "Non-directional beacon", "text": "Non-directional beacon\n\nA non-directional (radio) beacon (NDB) is a radio transmitter at a known location, used as an aviation or marine navigational aid. As the name implies, the signal transmitted does not include directional information, in contrast to other navigational aids such as low frequency radio range, VHF omnidirectional range (VOR) and TACAN. NDB signals follow the curvature of the Earth, so they can be received at much greater distances at lower altitudes, a major advantage over VOR. However, NDB signals are also affected more by atmospheric conditions, mountainous terrain, coastal refraction and electrical storms, particularly at long range.\n\nNDBs used for aviation are standardised by ICAO Annex 10 which specifies that NDBs be operated on a frequency between 190 kHz and 1750 kHz, although normally all NDBs in North America operate between 190 kHz and 535 kHz. Each NDB is identified by a one, two, or three-letter Morse code callsign. In Canada, privately owned NDB identifiers consist of one letter and one number. North American NDBs are categorized by power output, with low power rated at less than 50 watts, medium from 50 W to 2,000 W and high being over 2,000 W.\n\nThere are four types of non-directional beacons in the aeronautical navigation service:\n\nThe last two types are used in conjunction with an Instrument Landing System (ILS).\n\nNDB navigation consists of two parts — the \"automatic direction finder\" (ADF) equipment on the aircraft that detects an NDB's signal, and the NDB transmitter. The ADF can also locate transmitters in the standard AM medium wave broadcast band (530 kHz to 1700 kHz at 10 kHz increments in the Americas, 531 kHz to 1602 kHz at 9 kHz increments in the rest of the world).\n\nADF equipment determines the direction or bearing to the NDB station relative to the aircraft by using a combination of directional and non-directional antennae to sense the direction in which the combined signal is strongest. This bearing may be displayed on a relative bearing indicator (RBI). This display looks like a compass card with a needle superimposed, except that the card is fixed with the 0 degree position corresponding to the centreline of the aircraft. In order to track toward an NDB (with no wind), the aircraft is flown so that the needle points to the 0 degree position. The aircraft will then fly directly to the NDB. Similarly, the aircraft will track directly away from the NDB if the needle is maintained on the 180 degree mark. With a crosswind, the needle must be maintained to the left or right of the 0 or 180 position by an amount corresponding to the drift due to the crosswind. (Aircraft Heading +/- ADF needle degrees off nose or tail = Bearing to or from NDB station).\n\nThe formula to determine the compass heading to an NDB station (in a no wind situation) is to take the relative bearing between the aircraft and the station, and add the magnetic heading of the aircraft; if the total is greater than 360 degrees, then 360 must be subtracted. This gives the magnetic bearing that must be flown: (RB + MH) mod 360 = MB.\n\nWhen tracking to or from an NDB, it is also usual that the aircraft track on a specific bearing. To do this it is necessary to correlate the RBI reading with the compass heading. Having determined the drift, the aircraft must be flown so that the compass heading is the required bearing adjusted for drift at the same time as the RBI reading is 0 or 180 adjusted for drift. An NDB may also be used to locate a position along the aircraft's current track (such as a radial path from a second NDB or a VOR). When the needle reaches an RBI reading corresponding to the required bearing, then the aircraft is at the position. However, using a separate RBI and compass, this requires considerable mental calculation to determine the appropriate relative bearing.\n\nTo simplify this task, a compass card driven by the aircraft's magnetic compass is added to the RBI to form a \"Radio Magnetic Indicator\" (RMI). The ADF needle is then referenced immediately to the aircraft's magnetic heading, which reduces the necessity for mental calculation. Many RMIs used for aviation also allow the device to display information from a second radio tuned to a VOR station; the aircraft can then fly directly between VOR stations (so-called \"Victor\" routes) while using the NDBs to triangulate their position along the radial, without the need for the VOR station to have a collocated DME. This display, along with the \"Omni Bearing Indicator\" for VOR/ILS information, was one of the primary radionavigation instruments prior to the introduction of the Horizontal Situation Indicator (HSI) and subsequent digital displays used in glass cockpits.\n\nThe principles of ADFs are not limited to NDB usage; such systems are also used to detect the locations of broadcast signals for many other purposes, such as finding emergency beacons.\n\nA bearing is a line passing through the station that points in a specific direction, such as 270 degrees (due West). NDB bearings provide a charted, consistent method for defining paths aircraft can fly. In this fashion, NDBs can, like VORs, define \"airways\" in the sky. Aircraft follow these pre-defined routes to complete a flight plan. Airways are numbered and standardized on charts. Colored airways are used for low to medium frequency stations like the NDB and are charted in brown on sectional charts. Green and red airways are plotted east and west, while amber and blue airways are plotted north and south. There is only one colored airway left in the continental United States, located off the coast of North Carolina and is called G13 or Green 13. Alaska is the only other state in the United States to make use of the colored airway systems. Pilots follow these routes by tracking radials across various navigation stations, and turning at some. While most airways in the United States are based on VORs, NDB airways are common elsewhere, especially in the developing world and in lightly populated areas of developed countries, like the Canadian Arctic, since they can have a long range and are much less expensive to operate than VORs.\n\nAll standard airways are plotted on aeronautical charts, such as U.S. sectional charts, issued by the National Oceanographic and Atmospheric Administration (NOAA).\n\nNDBs have long been used by aircraft navigators, and previously mariners, to help obtain a fix of their geographic location on the surface of the Earth. Fixes are computed by extending lines through known navigational reference points until they intersect. For visual reference points, the angles of these lines can be determined by compass; the bearings of NDB radio signals are found using radio direction finder (RDF) equipment. \n\nPlotting fixes in this manner allow crews to determine their position. This usage is important in situations where other navigational equipment, such as VORs with distance measuring equipment (DME), have failed. In marine navigation, NDBs may still be useful should GPS reception fail.\n\nTo determine the distance in relation to an NDB station in nautical miles, the pilot uses this simple method:\n\nA runway equipped with NDB or VOR (or both) as the only navigation aid is called a non-precision approach runway; if it is equipped with ILS it is called a precision approach runway.\n\nNDBs are most commonly used as markers or \"locators\" for an instrument landing system (ILS) approach or standard approach. NDBs may designate the starting area for an ILS approach or a path to follow for a standard terminal arrival procedure, or STAR. In the United States, an NDB is often combined with the outer marker beacon in the ILS approach (called a locator outer marker, or LOM); in Canada, low-powered NDBs have replaced marker beacons entirely. Marker beacons on ILS approaches are now being phased out worldwide with DME ranges used instead to delineate the different segments of the approach. \n\nGerman Navy U-boats during World War II were equipped with a Telefunken Spez 2113S homing beacon. This transmitter could operate on 100 kHz to 1500 kHz with a power of 150 W. It was used to send the submarine's location to other submarines or aircraft, which were equipped with DF receivers and loop antennas.\n\nNDBs typically operate in the frequency range from 190 kHz to 535 kHz (although they are allocated frequencies from 190 to 1750 kHz) and transmit a carrier modulated by either 400 or 1020 Hz. NDBs can also be collocated with a DME in a similar installation for the ILS as the outer marker, only in this case, they function as the inner marker. NDB owners are mostly governmental agencies and airport authorities.\n\nNDB radiators are vertically polarised. NDB antennas are usually too short for resonance at the frequency they operate – typically perhaps 20m length compared to a wavelength around 1000m. Therefore, they require a suitable matching network that may consist of an inductor and a capacitor to \"tune\" the antenna. Vertical NDB antennas may also have a 'top hat', which is an umbrella-like structure designed to add loading at the end and improve its radiating efficiency. Usually a ground plane or counterpoise is connected underneath the antenna.\n\nApart from Morse Code Identity of either 400 Hz or 1020 Hz, the NDB may broadcast:\n\nNavigation using an ADF to track NDBs is subject to several common effects:\n\n\nWhile pilots study these effects during initial training, trying to compensate for them in flight is very difficult; instead, pilots generally simply choose a heading that seems to average out any fluctuations.\n\nRadio-navigation aids must keep a certain degree of accuracy, given by international standards, FAA, ICAO, etc.; to assure this is the case, Flight inspection organizations periodically check critical parameters with properly equipped aircraft to calibrate and certify NDB precision. The ICAO minimum accuracy for NDBs is ±5°\n\nBesides their use in aircraft navigation, NDBs are also popular with long-distance radio enthusiasts (\"DXers\"). Because NDBs are generally low-power (usually 25 watts, some can be up to 5 kW), they normally cannot be heard over long distances, but favorable conditions in the ionosphere can allow NDB signals to travel much farther than normal. Because of this, radio DXers interested in picking up distant signals enjoy listening to faraway NDBs. Also, since the band allocated to NDBs is free of broadcast stations and their associated interference, and because most NDBs do little more than transmit their Morse Code callsign, they are very easy to identify, making NDB monitoring an active niche within the DXing hobby.\n\nIn North America, the NDB band is from 190 to 435 kHz and from 510 to 530 kHz. In Europe, there is a longwave broadcasting band from 150 to 280 kHz, so the European NDB band is from 280 kHz to 530 kHz with a gap between 495 and 505 kHz because 500 kHz was the international maritime distress (emergency) frequency.\n\nThe beacons that are between 510 kHz and 530 kHz can sometimes be heard on AM radios that can tune below the beginning of the Medium Wave(MW) broadcast band. Frequencies close to the MW band, like 515 kHz, may be within the receive bandwidth of some AM radios. However, reception of NDBs generally requires a radio receiver that can receive frequencies below 530 kHz (the longwave band). A NDB in Miramichi, New Brunswick once operated at 530 kHz as \"F9\" but had later moved to 520 kHz. Most so-called \"shortwave\" radios also include mediumwave and longwave, and they can usually receive all frequencies from 150 kHz to 30 MHz, which makes them ideal for listening to NDBs. Whilst this type of receiver is adequate for reception of local beacons, specialized techniques (receiver preselectors, noise limiters and filters) are required for the reception of very weak signals from remote beacons.\n\nThe best time to hear NDBs that are very far away (i.e. that are \"DX\") is the last three hours before sunrise. Reception of NDBs is also usually best during the fall and winter because during the spring and summer, there is more atmospheric noise on the LF and MF bands.\n\n\n\n"}
{"id": "622217", "url": "https://en.wikipedia.org/wiki?curid=622217", "title": "Pastured poultry", "text": "Pastured poultry\n\nPastured poultry is a sustainable agriculture technique that calls for the raising of laying chickens, meat chickens (broilers), and/or turkeys on pasture, as opposed to indoor confinement. Humane treatment and the perceived health benefits of pastured poultry are causing an increase in demand for such products.\n\nJoel Salatin of Swoope, Virginia, helped to reintroduce the technique at Polyface Farm, and wrote his book \"Pastured Poultry Profits\" to spread the idea to other farmers. Andy Lee and Herman Beck-Chenoweth expanded on Salatin's techniques, and created some of their own.\n\nThe American Pastured Poultry Producers' Association (APPPA) was formed to promote pastured poultry. Its membership consists\nlargely of pastured poultry farmers.\n\nThough pasture feeding improves the nutritive quality of ruminant meats, the effect of pasture feeding on poultry meat composition is not well established. One trial showed low impact of pasture feeding on vitamin E and fatty acid composition.\n\nThe pens that house the fowl can be made from wood and scrap metal or out of PVC pipe and white tarps.\n\nPastured poultry is also gaining popularity because it helps the farmer, through reducing capital costs, and increasing pasture fertility. It is very well suited for incorporation within a system of managed intensive grazing.\n\nPastured poultry is not limited to chickens and turkeys. It includes a variety of other birds, including ducks, geese, and exotics in the poultry family.\n\nHerman Beck-Chenoweth reintroduced the free-range system that was the most popular way to raise poultry in the U.S. from the 1930s through the 1960s. The system allows birds to range freely during the day and be safely sequestered on secure skid houses over night. The addition of a guard animal, such as an Komondor or Anatolian Shepherd dog, controls predators. In the modern American free-range poultry production system, birds are much less crowded and freer to practice normal bird behaviour than in any other pasture-based system.\n\nAlthough frequently listed as a \"pasture\" method, free-\"range\" refers to the length of the forage. Cattle graze \"pasture\" which is forage over six inches long. \"Range\" refers to short forage of 2-4 inches. Free-range is a very sustainable production system that improves the farmer's soil and produces poultry with strong bones and meat with good \"mouthfeel\". Combined with proper aging after slaughter, the meat is tender and flavorful.\n\n\n"}
{"id": "17026149", "url": "https://en.wikipedia.org/wiki?curid=17026149", "title": "Photo-Me International", "text": "Photo-Me International\n\nPhoto-Me International plc () based in Bookham, Surrey operates photobooths. It became a public limited company in 1963 and has built operations in several countries including Indonesia, Japan, Philippines, Germany and France in addition to the UK. Although most well known for its photobooths, Photo-Me operates, sells and services a range of instant service equipment.\n\nThe Company's shares have been listed on the London Stock Exchange since 1962. In 2007 a shareholder revolt over plans to sell off the vending division forced Vernon Sankey and Serge Crasnianski to resign.\n\nIn February 2009 there was a profit warning and in March 2009 chief executive Thierry Barel resigned.\n\nSerge Crasnianski was reappointed to the board as a non-executive director in May 2009. He was subsequently appointed Deputy Chairman and Joint Chief Executive and in May 2010 assumed the role of Chief Executive.\n\nFollowing his reappointment, a major restructuring was carried out at KIS and the Group after losses of £6.3 million in 2008 and recorded a pre-tax profit of £1.6 million in the year ending April 30, 2009. In the year to April 30, 2010, the Group’s loss-making wholesale photo-processing labs business was sold and pre-tax profits were reported at £14.0 million. Of note was the £31.6 million improvement in the overall cash position such that the net cash on the Balance Sheet was £8.1 million compared to net debt of £23.5 million the previous year.\nCanary Wharf in London has installed the company's photo booths but in this case they are ones designed in conjunction with Philippe Stark. \n\nThe company has diversified into the laundry business, with a division called 'Revolution'. Revolution is a 24/7 outdoor self-service launderette. This division is trading beyond expectations with 2000 units planned for the end of 2015.\n\nIn November 2016 it was announced that the company had bought the photo division of Asda stores. Photo-Me will take over the supermarket's 191 photo centres and 172 self-service kiosks. \n\nOn 18 January 2017 it was announced in the United Kingdom House of Lords, that the government is seeking to allow the public to submit selfies direct to the Passport Office for approval.\n\n"}
{"id": "10810905", "url": "https://en.wikipedia.org/wiki?curid=10810905", "title": "Post Irradiation Examination", "text": "Post Irradiation Examination\n\nPost Irradiation Examination (PIE) is the study of used nuclear materials such as nuclear fuel. It has several purposes. It is known that by examination of used fuel that the failure modes which occur during normal use (and the manner in which the fuel will behave during an accident) can be studied. In addition information is gained which enables the users of fuel to assure themselves of its quality and it also assists in the development of new fuels. After major accidents the core (or what is left of it) is normally subject to PIE in order to find out what happened. One site where PIE is done is the ITU which is the EU centre for the study of highly radioactive materials.\n\nMaterials in a high radiation environment (such as a reactor) can undergo unique behaviors such as swelling and non-thermal creep. If there are nuclear reactions within the material (such as what happens in the fuel), the stoichiometry will also change slowly over time. These behaviors can lead to new material properties, cracking, and fission gas release: \n\nAs the fuel is degraded or heated the more volatile fission products which are trapped within the uranium dioxide may become free.\n\nAs the fuel expands on heating, the core of the pellet expands more than the rim which may lead to cracking. Because of the thermal stress thus formed the fuel cracks, the cracks tend to go from the centre to the edge in a star shaped pattern.\n\nIn order to better understand and control these changes in materials, these behaviors are studied. . Due to the intensely radioactive nature of the used fuel this is done in a hot cell. A combination of nondestructive and destructive methods of PIE are common.\n\nIn addition to the effects of radiation and the fission products on materials, scientists also need to consider the temperature of materials in a reactor, and in particular, the fuel. Too high fuel temperatures can compromise the fuel, and therefore it is important to control the temperature in order to control the fission chain reaction. \n\nThe temperature of the fuel varies as a function of the distance from the centre to the rim. At distance x from the centre the temperature (T) is described by the equation where ρ is the power density (W m) and K is the thermal conductivity.\n\nTo explain this for a series of fuel pellets being used with a rim temperature of 200 C (typical for a BWR) with different diameters and power densities of 250 Wm have been modeled using the above equation. Note that these fuel pellets are rather large; it is normal to use oxide pellets which are about 10 mm in diameter.\n\nRadiochemistry and Nuclear Chemistry, G. Choppin, J-O Liljenzin and J. Rydberg, 3rd Ed, 2002, Butterworth-Heinemann, \n\n"}
{"id": "2266487", "url": "https://en.wikipedia.org/wiki?curid=2266487", "title": "Problem solving environment", "text": "Problem solving environment\n\nA problem solving environment (PSE) is a completed, integrated and specialised computer software for solving one class of problems, combining automated problem-solving methods with human-oriented tools for guiding the problem resolution. A PSE may also assist users in formulating problem resolution. A PSE may also assist users in formulating problems, selecting algorithm, simulating numerical value and viewing and analysing results.\n\nMany PSEs were introduced in the 1990s. They use the language of the respective field and often employ modern graphical user interfaces. The goal is to make the software easy to use for specialists in fields other than computer science. PSEs are available for generic problems like data visualization or large systems of equations and for narrow fields of science or engineering like gas turbine design.\n\nThe Problem Solving Environment (PSE) released a few years after the release of Fortran and Algol 60, people thought that this system with high-level language would cause elimination of professional programmers. However, surprisingly, PSE has been accepted and even though scientists used it to write programs.\n\nThe Problem Solving Environment for Parallel Scientific Computation was introduced in 1960, where this was the first Organised Collection was introduced in 1960, where this was the first Organised Collections with minor standardisation. In 1970, PSE was initially researched for providing high-class programming language rather than Fortran, also Libraries Plotting Packages advent. Development of Libraries were continued, and there were introduction of Emergence of Computational Packages and Graphical systems which is data visualisation. By 1990s, Hypertext, Point and Click had moved towards inter-operability. Moving on, a \"Software Parts\" Industry finally existed.\n\nThroughout a few decades, recently, many PSEs have been developed and to solve problem and also support users from different categories, including education, general programming, CSE software learning, job executing and Grid/Cloud computing.\n\nThe shell software GOSPEL is an example of how a PSE can be designed for EHL modelling using a Grid resource. With the PSe, one can visualise the optimisation progress, as well as interact with other simulations.\n\nThe PSE parallelise and embed many individual numerical calculations in an individual numerical calculations in an industrial serial optimisation code. It is built in NAG's IRIS Explorer package to solve EHL and Parallelism problems and can use the gViz libraries, to run all the communication between the PSE and the simulation. Also use MPI, which is part of the NAG libraries, gives significant quick and better solution by combining the max. levels of continuation.\n\nMoreover, the system is designed to allow users to steer simulations using visualised output. An example is utilising local minima, or layering additional details when around a local in and out of the simulation and it can imagine the information which is produced in any sharp and also still allow to steer the simulation.\n\nPSEs are require a large amount of resources that strain even the most powerful computers of today. Translating PSEs into software that can be used for mobile devices in an important challenge that faces programmers today.\n\nGrid computing is seen as a solution to the rescue issues of PSEs for mobile devices. This is made possible through a \"Brokering Service\". This service is started by an initiating device that sends the necessary information for PSE to resolve task. The brokering service then breaks this down into subtasks that distributes the information to various subordinate devices that perform these subtasks. The brokering necessitates an Active Agent Repository (AAR) and a Task Allocation Table (TAT) that both work to manage the subtasks. A keep-Alive Server is tapped to handle communication between the brokering service and the subordinate devices. The Keep-Alive server relies on a lightweight client application installed in the participating mobile devices.\n\nSecurity, transparency and dependability are issues that may arise when using the grid for mobile device-based PSEs.\n\nThere are a revolution for network-based learning and e-learning for education but it is very difficult to collect education data and data of the student activities. TSUNA-TASTE, is developed by T. Teramoto, a PSE to support education and learning processes. This system may create a new idea of the e-learning by supporting teachers and students in computer-related education. It consists of four parts, including agents of students, an education support server, a database system and a Web server. This system makes e-learning more convenient as information is earlier to store and collect for students and teachers.\n\nA computer-assisted parallel program generation support(P-NCAS), is a PSE, creates a new way to reduce the programming hard task for the computer programming. This program can avoid or reduce the chance that huge computer software breaking down so this restrict uncertainty and major accidents in the society. Moreover, partial differential equations(PDEs) problems can be solved by parallel programs which are generated by P-NCAS supports. P-NCAS employs the Single Program Multi Data (SPMD) and uses a decomposition method for the parallelisation. These enable users of P-NCAS to input problems described by PDES, algorithm and discretisation scheme etc., and to view and edit all details through the visualisation and windows for edition. At last, parallel program will be outputted in C language by P-NCAS and also include documents which show everything has inputted in the beginning.\n\nFirstly, it was difficult doing 2-D EHL problems because of the expense and computer power available. The development of parallel 2-D EHL codes and faster computers have now paved the way for 2-D EHL problem solving to be possible. Friction and lubricant data need a higher level of security given their sensitivity. Accounting for simulations may be difficult because these are done in rapidly and in the thousands. This can be solved by a registration system or a 'directory'. Collaborative PSEs with multiple users will encounter difficulties tracking changes, especially which specific changes were made and when those changes were made. This may also be solved with a directory of changes made.\n\nSecondly, future improvement of the Grid-based PSEs for mobile devices, the group aims to generate new scenarios through manipulation of the control variables available. By changing those control variables, the simulation software is able to create scenarios from each other, allowing for more scrutiny of the conditions in each scenario. It is expected that manipulation of three variables will generate twelve different scenarios.\n\nThe variables that we are interested in studying are network stability and device mobility. We feel that these variables will hater the greatest impact on grid performance. Our study will measure performance using task completion time as the primary outcome.\n\nAs PSEs grow more complex, the need for computing resources has risen dramatically. Conversely, with PSE applications venturing into fields and environments of growing complexity, the creation of PSEs have become tedious and difficult.\n\nHirumichi Kobashi and his colleagues have designed a PSE meant to create other PSEs. This has been dubbed as a 'meta PSE' or a PSEs. This was how PSE PSRk was born.\n\nThe architecture of PSE Park emphasises flexibility and extensibility. These characteristics make it an attractive platform for varied levels of expertise, from entry-level users to developers.\n\nPSE Park provides these through its repository of functions. the repository contains modules required to build PSEs. Some of the most basic modules, called Cores, are used as the foundation of PSEs. More complex modules are available for use by programmers. Users access PSE Park through a console linked to the programmers. Once the user is register, he/she has assess to the repository. A PIPE server is used as the mediator between the user and PSE Park. It grants access to modules and constructs the selected functions into a PSE.\n\nDevelopers can develop functions, or even whole PSEs, for inclusion into the repository. Entry-level and expert users can access these pre-made PSEs for their own purposes. Given this architecture, PSE Park requires a cloud computing environment to support the enormous data sharing that occurs during PSe use and development.\n\nThe PIPE Server differs from other servers in terms of how it handles intermediate results. Since the PIPE Server acts as a mediator in a meta-PSE, any results or variables generated by a core module are retrieved as global variables to be used by the next core. The sequence or hierarchy is defined by the user. The way, same name variables are revised to the new set of variables.\n\nAnother important characteristics of the PIPE Server is that it executes each module or core independently. This means that the language of each module does not have to be the same as the others in the PSE. Modules are implemented depending on the defined hierarchy. This feature brings enormous flexibility for developers and users who have varied backgrounds in programming. The modular format also enables that existing PSEs can be extended and modified easily.\n\nIn order be registered, a core must be fully defined. The input and output definitions allow the PIPE server to determine compatibility with other cores and modules. Any lack of definition is flagged by the PIPE server for incompatibility.\n\nThe registration engine keeps track of all cores that may be used in PSE Park. A history of use is also created. A core map may be developed in order to help users understand a core to help users understand a core or module better. The console is the users' main interface with PSE Park. It is highly visual and diagrammatic, allowing users to better understand the linkages between modules and cores for the PSEs that they are working on.\n\n\n"}
{"id": "15743436", "url": "https://en.wikipedia.org/wiki?curid=15743436", "title": "Recursive economics", "text": "Recursive economics\n\nRecursive economics is a branch of modern economics based on a paradigm of individuals making a series of two-period optimization decisions over time.\n\nThe neoclassical model assumes a one-period utility maximization for a consumer and one-period profit maximization by a producer. The adjustment that occurs within that single time period is a subject of considerable debate within the field, and is often left unspecified. A time-series path in the neoclassical model is a series of these one-period utility maximizations.\n\nIn contrast, a recursive model involves two or more periods, in which the consumer or producer trades off benefits and costs across the two time periods. This trade-off is sometimes represented in what is called an Euler equation. A time-series path in the recursive model is the result of a series of these two-period decisions.\n\nIn the neoclassical model, the consumer or producer maximizes utility (or profits). In the recursive model, the subject maximizes value or welfare, which is the sum of current rewards or benefits and discounted future expected value.\n\nThe field is sometimes called recursive because the decisions can be represented by equations that can be transformed into a single functional equation sometimes called a Bellman equation. This equation relates the benefits or rewards that can be obtained in the current time period to the discounted value that is expected in the next period. The dynamics of recursive models can sometimes also be studied as differential equations.\n\nThe recursive paradigm originated in control theory with the invention of dynamic programming by the American mathematician Richard E. Bellman in the 1950s. Bellman described possible applications of the method in a variety of fields, including Economics, in the introduction to his 1957 book. Stuart Dreyfus, David Blackwell, and Ronald A. Howard all made major contributions to the approach in the 1960s.\n\nIn addition, some scholars also cite the Kalman filter invented by Rudolf E. Kalman and the theory of the maximum formulated by Lev Semenovich Pontryagin as forerunners of the recursive approach in economics.\n\nSome scholars point to Martin Beckmann and Richard Muth as the first application of an explicit recursive equation in economics. However, probably the earliest celebrated economic application of recursive economics was Robert Merton's seminal 1973 article on the intertemporal capital asset pricing model. (See also Merton's portfolio problem). Merton's theoretical model, one in which investors chose between income today and future income or capital gains, has a recursive formulation.\n\nNancy Stokey, Robert Lucas and Edward Prescott describe stochastic and non-stochastic dynamic programming in considerable detail, giving many examples of how to employ dynamic programming to solve problems in economic theory. This book led to dynamic programming being employed to solve a wide range of theoretical problems in economics, including optimal economic growth, resource extraction, principal–agent problems, public finance, business investment, asset pricing, factor supply, and industrial organization.\n\nThe approach gained further notice in macroeconomics from the extensive exposition by Ljungqvist & Sargent. This book describes recursive models applied to theoretical questions in monetary policy, fiscal policy, taxation, economic growth, search theory, and labor economics.\n\nIn investment and finance, Avinash Dixit and Robert Pindyck showed the value of the method for thinking about capital budgeting, in particular showing how it was theoretically superior to the standard neoclassical investment rule. Patrick Anderson adapted the method to the valuation of operating and start-up businesses and to the estimation of the aggregate value of privately held businesses in the US.\n\nThere are serious computational issues that have hampered the adoption of recursive techniques in practice, many of which originate in the curse of dimensionality first identified by Richard Bellman.\n\nApplied recursive methods, and discussion of the underlying theory and the difficulties, are presented in Mario Miranda & Paul Fackler (2002), Meyn (2007) Powell (2011) and Bertsekas (2005).\n\n"}
{"id": "6099082", "url": "https://en.wikipedia.org/wiki?curid=6099082", "title": "Scala (company)", "text": "Scala (company)\n\nScala is a producer of multimedia software. It was founded in 1987 as a Norwegian company called Digital Visjon. It is headquartered near Philadelphia, Pennsylvania, USA, and has subsidiaries in Europe and Asia.\n\nIn 1987 a young Norwegian entrepreneur, Jon Bøhmer founded the company \"Digital Visjon\" in Brumunddal, Norway to create multimedia software on the Commodore Amiga computer platform. In 1988 they released their first product which was named InfoChannel 0.97L, which had hotels and cable-TV companies as their first customers.\n\nIn 1990, they redesigned the program with a new graphical user interface. They renamed the company and the software \"Scala\" and released a number of multimedia applications. The company attracted investors, mainly from Norway and incorporated in the US in 1994 and is now based in the United States with their European headquarters located in the Netherlands.\n\nThe name \"Scala\" was given by Bøhmer and designer Bjørn Rybakken and represents the scales in colors, tones and the opera in Milano. The name inspired a live actor animation made by Bøhmer and Rybakken using an Amiga, a video camera and a frame-by-frame video digitizer. The animation, named \"Lo scalatore\" (Italian for 'The Climber'), featured a magic trick of Indian fakirs of a man climbing a ladder and disappearing in the air. This animation was then included into one of the Demo Disks of Scala Multimedia in order to show the capabilities of that presentation software in loading and playing animations whilst also manipulating it with other features of the software.\n\nIn 1994 Scala released Multimedia MM400 and InfoChannel 500.\n\nIn 1996, due to the bankruptcy of Commodore, Scala left the Amiga platform and started delivering the same applications under MS-DOS. Scala Multimedia MM100, Scala Multimedia Publisher and Scala InfoChannel 100 were released for the x86 platform. Scala MM100 won Byte Magazine's \"Best of Comdex\" in 1996.\n\nAs of December 2013, the CEO of Scala is Tom Nix, who was formerly a regional vice president. Nix succeeds Gerard Bucas, who retired after nine years.\n\nThe first versions for the Amiga computer were a video titler and slide show authoring system. Scala was bundled with typefaces, background images, and a selection of transition effects to be applied to them. The artwork was designed by Bjørn Rybakken. Scala was also capable of working with Genlock equipment to superimpose titles over footage played through the devices video input.\n\nSucceeding versions of the program on the same platform added features such as animation playback, more effects (\"Wipes\") and the ability to interact with multimedia devices through a programming language called \"Lingua\" (Latin for \"language\").\n\nWith its move to Windows, Scala became more complex and gained the ability to support languages such as Python and Visual Basic.\n\nIn late 2008, Scala stopped calling their product line InfoChannel and went through a period of referring only to their \"solutions\". At the start of 2009, the product line was being called 'Scala5' and being referred to as such in all their press releases.\n\nScala5 has three main components: Scala Designer, an authoring program which is used to create dynamic content, Scala Content Manager, which is used to manage and distribute content, and Scala Player, which plays back the distributed content.\n\nScala's latest suite of Digital signage software is referred to as Scala Enterprise. The solution, a software suite consisting of Scala Designer, Scala Player, and Scala Content Manager officially launched in mid- 2013. \nAt launch, release version 10.0 featured HTML5 and Android player support, the usage of interactive features on mobile devices to engage with retail and corporate communications audiences, and social media integrations.\n\nAs of April 2018, the latest version of Scala Enterprise is version 11.05.\n\n"}
{"id": "244658", "url": "https://en.wikipedia.org/wiki?curid=244658", "title": "Skewer", "text": "Skewer\n\nA skewer is a thin metal or wood stick used to hold pieces of food together. The word may sometimes be used as a metonym, to refer to the entire food item served on a skewer, as in \"chicken skewers\". Skewers are used while grilling or roasting meats and fish, and in other culinary applications.\n\nIn English, \"brochette\" is a borrowing of the French word for skewer. In cookery, \"en brochette\" means 'on a skewer', and describes the form of a dish or the method of cooking and serving pieces of food, especially grilled meat or seafood, on skewers; for example \"lamb cubes en brochette\". Skewers are often used in a variety of kebab dishes.\n\nMetal skewers are typically stainless steel rods with a pointed tip on one end and a grip of some kind on the other end for ease of removing the food. Non-metallic skewers are often made from bamboo; however, any suitable wood may be used. Prior to grilling, wooden skewers may be soaked in water to avoid burning. A related device is the rotisserie or spit, a large rod that rotates meat while it cooks.\n\nEvidence of the prehistoric use of skewers, as far back as the Lower Paleolithic, has been found at a 300,000-year-old site in Schöningen, Germany. A stick with a burnt tip was found to have been used to cook meat over a fire. Excavations in Santorini, Greece, unearthed sets of stone cooking supports used before the 17th century BC. In the supports there are pairs of indentations that may have been used for holding skewers. Homer in \"Iliad\" (1.465) mentions pieces of meat roasted on spits (ὀβελός). In Classical Greece, a small spit or skewer was known as ὀβελίσκος (\"obeliskos\"), and Aristophanes mentions such skewers being used to roast thrushes. The story is often told of medieval Middle Eastern soldiers - usually Turkish or Persian, depending on the storyteller - who cooked meat skewered on their swords.\n\nOne of the most well-known skewered foods around the world is the shish kebab. The earliest literary evidence for the Turkish word (shish) as a food utensil comes from the 11th-century \"Diwan Lughat al-Turk\", attributed to Mahmud of Kashgar. He defines shish as both a skewer and 'tool for arranging noodles' (\"minzam tutmaj\"), though he is unique in this regard as all subsequent known historical references to \"shish\" define it as a skewer.\n\nA large variety of dishes cooked on skewers are kebabs (meat dishes prevalent in Middle Eastern cuisine and the Muslim world), or derived from them. Examples include Turkish \"shish kebab\", Iranian \"jujeh kabab\", Chinese \"chuan\", and Southeast Asian satay. However, \"kebab\" is not synonymous with \"skewered food\", and many kebab dishes such as chapli kebab are not cooked on skewers. On the other hand, English speakers may sometimes use the word \"kebab\" to refer to any food on a skewer.\n\nDishes, other than kebabs, prepared with skewers include American city chicken and corn dog, Brazilian \"churrasco\", indigenous Peruvian \"anticucho\", Italian \"arrosticini\", Japanese \"kushiyaki\" and \"kushikatsu\", Korean \"jeok\" and \"kkochi\", Nepali \"sekuwa\", Portuguese \"espetada\", Vietnamese \"nem nướng\" and \"chạo tôm\".\n\nAppetizers and hors d'oeuvres may often be skewered together with small sticks or toothpicks; the Spanish pincho is named after such a skewer. Small, often decorative, skewers of glass, metal, wood or bamboo known as \"olive picks\" or \"cocktail sticks\" are used for garnishes on cocktails and other alcoholic beverages. Many types of snack food, such as candy apples, banana cue, \"ginanggang\", \"elote\", and \"tanghulu\", are sold and served \"on a stick\" or skewer, especially at outdoor markets, fairs, and sidewalk or roadside stands around the world.\n\n"}
{"id": "36807603", "url": "https://en.wikipedia.org/wiki?curid=36807603", "title": "Steam to oil ratio", "text": "Steam to oil ratio\n\nThe steam to oil ratio is a measure of the water and energy consumption related to oil production in cyclic steam stimulation and steam assisted gravity drainage oil production. SOR is the ratio of unit of steam required to produce unit of Oil. The typical values are three to eight and two to five respectively. This means two to eight barrels of water converted into steam is used to produce one barrel of oil.\n\n"}
{"id": "23939931", "url": "https://en.wikipedia.org/wiki?curid=23939931", "title": "TOSLINK", "text": "TOSLINK\n\nTOSLINK (from \"Toshiba Link\") is a standardized optical fiber connector system. Also known generically as an \"optical audio cable\" or just \"optical cable\", its most common use is in consumer audio equipment (via a \"digital optical\" socket), where it carries a digital audio stream from components such as CD and DVD players, DAT recorders, computers, and modern video game consoles, to an AV receiver that can decode two channels of uncompressed lossless PCM audio or surround sound such as Dolby Digital or DTS Surround System. Unlike HDMI, TOSLINK does not have the bandwidth to carry the lossless versions of Dolby TrueHD, DTS-HD Master Audio, or more than two channels of PCM audio.\n\nAlthough TOSLINK supports several different media formats and physical standards, digital audio connections using the rectangular EIAJ/JEITA RC-5720 (also CP-1201 and JIS C5974-1993 F05) connector are by far the most common. The optical signal is a red light with a peak wavelength of Depending on the type of modulated signal being carried, other optical wavelengths may be present.\n\nToshiba originally created TOSLINK to connect their CD players to the receivers they manufactured, for PCM audio streams. The software layer is based on the \"Sony Philips Digital Interconnect Format\" (S/PDIF), while the hardware layer utilizes a fiber optic transmission system, rather than the electrical (copper) hardware layer of S/PDIF. TOSLINK was soon adopted by manufacturers of most CD players. It can often be found on video source (DVD and Blu-ray players, cable boxes and game consoles) to connect the digital audio stream to Dolby Digital/DTS decoders.\n\nThe name is a registered trademark of Toshiba, created from \"TOShiba-LINK\". Variations of the name, such as \"TOSlink\", \"TosLink\", and \"Tos-link\", are also seen, while the official generic name for the standard is \"EIAJ optical\".\n\nADAT Lightpipe or simply ADAT Optical uses an optical transmission system similar to TOSLINK, and is used in the professional music/audio industry. While the ADAT Lightpipe format uses the same JIS F05 connectors as TOSLINK, the ADAT Lightpipe data format is not compatible with S/PDIF. \n\nDue to their high attenuation of light, the effective range of plastic optical cables is limited to . They can temporarily fail or be permanently damaged if tightly bent. Although less commonly available and more expensive than plastic optical fiber (POF) cables, glass or silica optical fibers have lower losses and can extend the range of the TOSLINK system.\n\nOptical cables are not susceptible to electrical problems such as ground loops and RF interference.\n\nSeveral types of fiber can be used for TOSLINK: inexpensive plastic optical fiber, higher-quality multistrand plastic optical fibers, or quartz glass optical fibers, depending on the desired bandwidth and application. TOSLINK cables are usually limited to in length, with a technical maximum of for reliable transmission without the use of a signal booster or a repeater. However, it is very common for interfaces on newer consumer electronics (satellite receivers and PCs with optical outputs) to easily run over on even low-cost TOSLINK cables. TOSLINK transmitters operate at a nominal optical wavelength of \n\nMini-TOSLINK is a standardized optical fiber connector smaller than the standard square TOSLINK connector commonly used in larger consumer audio equipment. The plug is almost the same size and shape as the ubiquitous stereo minijack. Adapters are available to connect a full-size TOSLINK plug to a mini-TOSLINK socket. Combined jack and mini-TOSLINK sockets exist which can accept a jack or a mini-TOSLINK plug; mini-TOSLINK plugs are made longer than electrical jack plugs so that the latter are too short to touch and damage the LED of combined connectors. Many laptop computer and portable digital audio equipment models, such as the Google Chromecast Audio device and Apple AirPort Express and iPod Hi-Fi, use these connectors that allow for the insertion of analog (electrical) headphone output or microphone input or mini-TOSLINK digital (optical) output.\n\n"}
{"id": "15925983", "url": "https://en.wikipedia.org/wiki?curid=15925983", "title": "Tablet press", "text": "Tablet press\n\nA tablet press is a mechanical device that compresses powder into tablets of uniform size and weight. A press can be used to manufacture tablets of a wide variety of materials, including pharmaceuticals, illicit drugs such as MDMA, cleaning products, and cosmetics. To form a tablet, the granulated material must be metered into a cavity formed by two punches and a die, and then the punches must be pressed together with great force to fuse the material together.\n\nA tablet is formed by the combined pressing action of two punches and a die. In the first step of a typical operation, the bottom punch is lowered in the die creating a cavity into which the granulated feedstock is fed. The exact depth of the lower punch can be precisely controlled to meter the amount of powder that fills the cavity. The excess is scraped from the top of the die, and the lower punch is drawn down and temporarily covered to prevent spillage. Then, the upper punch is brought down into contact with the powder as the cover is removed. The force of compression is delivered by high pressure compression rolls which fuse the granulated material together into a hard tablet. After compression, the lower punch is raised to eject the tablet.\nTablet tooling design is critical to ensuring a robust tablet compression process. Considerations when designing pharmaceutical tablet compression tool design include tooling set, head flat, top head angle, top head radius, head back angle, and punch shank. As well as ensuring a single dose of drug, the tablet tooling is also critical in ensuring the size, shape, embossing and other physical characteristics of the tablet that are required for identification.\n\nThere are 2 types of tablet presses: single-punch and rotary tablet presses. Most high speed tablet presses take the form of a rotating turret that holds any number of punches. As they rotate around the turret, the punches come into contact with cams which control the punch's vertical position. Punches and dies are usually custom made for each application, and can be made in a wide variety of sizes, shapes, and can be customized with manufacturer codes and scoring lines to make tablets easier to break. Depending on tablet size, shape, material, and press configuration, a typical modern press can produce from 250,000 to over 1,000,000 tablets an hour.\n\n"}
{"id": "44732855", "url": "https://en.wikipedia.org/wiki?curid=44732855", "title": "Target Motion Analysis", "text": "Target Motion Analysis\n\nTarget Motion Analysis (TMA) is a process to determine the position of a target using passive sensor information. Sensors like passive \"RADAR\" and \"SONAR\" provide directional and occasionally frequency information. TMA is done by marking from which direction the sound comes at different times, and comparing the motion with that of the operator's own ship. Changes in relative motion are analyzed using standard geometrical techniques along with some assumptions about limiting cases. There are two different ways to execute TMA: manual and automated.\n\nManual TMA methods involve computation executed by humans instead of computers. There exist several manual TMA methods such as: Ekelund Ranging, 1934 Rule, Spears Wheel etc.\n\nOne of the best known TMA techniques is Ekelund ranging.\nIt is a method that is specifically designed for a 2leg-1zig scenario. This method works by first estimating the bearing rates during the first formula_1 and second leg formula_2. Secondly, one calculates the speed of advance along the line of sight with the target on the first formula_3and second leg formula_4. The rule then states that the range of the target at the moment of maneuver is given by:\n\nTo check the solution of an Ekelund Ranging solution there is also an iPhone app available.\n\nAutomated TMA methods involve computations executed by computers. This allows for the simultaneous tracking of multiple targets. There exist several automated TMA methods such as: Maximum Likelihood Estimator (MLE), etc.\n\nThe MLE method tries to fit the directional measurements (bearings) to a theoretical linear motion model of the target. The bearing function to be fitted is:\n\nIf formula_7 measerements of formula_8 have been collected, the problem reduces to an overdetermined system of formula_7 non-linear equations. The state vector associated is\nand can be solved by numerical estimation procedures like \"Gauss-Newton\".\n\n"}
{"id": "417573", "url": "https://en.wikipedia.org/wiki?curid=417573", "title": "Telegraph sounder", "text": "Telegraph sounder\n\nA telegraph sounder is an antique electromechanical device used as a receiver on electrical telegraph lines during the 19th century. It was invented by Alfred Vail after 1850 to replace the previous receiving device, the cumbersome Morse register and was the first practical application of the electromagnet. When a telegraph message comes in it produces an audible \"clicking\" sound representing the short and long keypresses – \"dots\" and \"dashes\" – which are used to represent text characters in Morse code. A telegraph operator would translate the sounds into characters representing the telegraph message.\n\nTelegraph networks, used from the 1850s to the 1920s to transmit text messages long distances, transmitted information by pulses of current of two different lengths, called \"dots\" and \"dashes\" which spelled out text messages in Morse code. A telegraph operator at the sending end of the line would create the message by tapping on a switch called a telegraph key, which rapidly connects and breaks the circuit to a battery, sending pulses of current down the line.\n\nThe telegraph sounder was used at the receiving end of the line to make the Morse code message audible. Its simple mechanism was similar to a relay. It consisted of an electromagnet attached to the telegraph line, with an iron armature near the magnet's pole balanced on a pivot, held up by a counterweight. When current flowed through the electromagnet's winding, it created a magnetic field which attracted the armature, pulling it down to the electromagnet, resulting in a \"click\" sound. When the current ended, the counterweight pulled the armature back up to its resting position, resulting in a \"clack\" sound. Thus, as the telegraph key at the sending end makes and breaks the contact, the sounder echoes the up and down state of the key.\n\nIt was important that a sounder make a sound both when the circuit was broken and when it was restored. This was necessary for the operator to clearly distinguish the long and short keypresses – the \"dashes\" and \"dots\" – that make up the characters in morse code.\n\n"}
{"id": "8899175", "url": "https://en.wikipedia.org/wiki?curid=8899175", "title": "The COED Project", "text": "The COED Project\n\nThe COED Project, or the COmmunications and EDiting Project, was an innovative software project created by the Computer Division of NOAA, US Department of Commerce in Boulder, Colorado in the 1970s. This project was designed, purchased and implemented by the in-house computing staff rather than any official organization.\n\nThe computer division previously had a history of frequently replacing its mainframe computers. Starting with a CDC 1604, then a CDC 3600, a couple of CDC 3800s, and finally a CDC 6600. The department also had an XDS 940 timesharing system which would support up to 32 users on dial-up modems. Due to rapidly changing requirements for computer resources, it was expected that new systems would be installed on a regular basis, and the resultant strain on the users to adapt to each new system was perceived to be excessive. The COED project was the result of a study group convened to solve this problem.\n\nThe project was implemented by the computer specialists who were also responsible for the purchase, installation, and maintenance of all the computers in the division. COED was designed and implemented in long hours of overtime. The data communications aspect of the system was fully implemented and resulted in greatly improved access to the XDS 940 and CDC 6600 systems. It was also used as the front end of the - Free University of Amsterdam's SARA system for many years.\n\nA complete networked system was a pair of Modcomps: one II handled up to 256 communication ports, and one IV handled the disks and file editing. The system was designed to be fully redundant. If one pair failed the other automatically took over. All computer systems in the network were kept time-synchronized so that all file dates/times would be accurate - synchronized to the National Bureau of Standards atomic clock, housed in the same building. Another innovation was asynchronous dynamic speed recognition. After a terminal connected to a port, the user would type a Carriage Return character, and the software would detect the speed of the terminal (in the range of 110 to 9600 bit/s) and present a log in message to the user at the appropriate speed. Due to limitations of the operating systems which came with the Modcomps, new Operating systems had to be created, CORTEX for the Modcomp II's and IV BRAIN for the Modcomp IV's.\n\n\nThose involved in the original design meetings were:\nRalph Slutz, George Sugar, Jim Winkelman and most of the COED implementors. Support was also provided by Tom Gray.\n\nThe COED implementors were:\nW. Schyler (Sky) Stevenson, Project Manager and operating system implementer\nHoward Bussey, Mark Emmer, David Lillie, and Vern Schryver. The 6600 interface to COED was implemented by Anthony Brittain, Dan Dechatelets and Kathy Browne.\n\n"}
{"id": "35756148", "url": "https://en.wikipedia.org/wiki?curid=35756148", "title": "Thermal conductance quantum", "text": "Thermal conductance quantum\n\nIn physics, the thermal conductance quantum formula_1 describes the rate at which heat is transported through a single ballistic phonon channel of temperature formula_2. It is given by:\n\nformula_3.\n\nThe thermal conductance of any electrically insulating structure that exhibits ballistic phonon transport is a positive integer multiple of formula_1. The thermal conductance quantum was first measured in 2000. These measurements employed suspended silicon nitride nanostructures that exhibited a constant thermal conductance of 16formula_1 at temperatures below approximately 0.6 kelvin.\n\nFor ballistic electrical conductors, the electron contribution to the thermal conductance is also quantized as a result of the electrical conductance quantum and the Wiedemann–Franz law, which has been quantitatively measured at both cryogenic (~20 mK) and room temperature (~300K) . \n\nThe thermal conductance quantum, also called as quantized thermal conductance, maybe understood from the Wiedemann-Franz law, which show that \n\nformula_6\n\nwhere formula_7 is a universal constant, \n\nformula_8\n\nIn the regime with quantized electric conductance, one may have\n\nformula_9\n\nwhere formula_10 is an integer, called as TKNN number. Then \n\nformula_11\n\nwhere formula_1 was defined in the beginning of this file. \n\n"}
{"id": "2362396", "url": "https://en.wikipedia.org/wiki?curid=2362396", "title": "VBS1", "text": "VBS1\n\nVBS1 (Virtual Battlefield Systems 1) is a military simulator which relies heavily on modern game technology and is therefore generally referred to as a serious game. The platform is derived from the first-person entertainment game and is developed by Bohemia Interactive Australia. The system enables the practice of small unit military tactics in an interactive multiplayer 3D environment. The platform provides real-time scenario management facilities, customized vehicles and equipment, user-definable mission scenarios, and variable environmental conditions. This combination of military simulator functionality and modern gaming technology proved to be a success and resulted in a broad military customer base. VBS2 is the successor of this platform.\n\nThe Virtual Battlespace Systems 1 concept was initially conceived in 2001 as the result of a business decision made between Bohemia Interactive Studio (BIS) and creator David Lagettie. The BIS computer game (\"OFP\") aimed for a highly realistic military gameplay. In order to achieve this the game featured large scale terrain areas combined with a high level of detail, an integrated topographic map, a fully functional command and control system for small teams, and considerable flexibility in the game engine. These functionalities gave the game obvious military potential, and as a result Bohemia Interactive Australia (BIA) was formed and given the task of converting the product explicitly for military purposes.\n\nThe first military customer for \"VBS1\" was the United States Marine Corps (USMC), who were provided \"VBS1\", a USMC addon pack and MOUT training facilities modelled to a high level of detail in late 2001. The majority of development work was carried out by BIA, with distribution and limited development conducted by Coalescent Technologies.\n\nIn 2002, the product was further developed, which resulted in the first versions of the \"VBS1\" After Action Review (AAR) and Observer being developed. The Real Virtuality engine was upgraded to version 1.94 in order to output data as required by the AAR system.\n\nThe Australian Defence Force (ADF) began conducting trials with \"VBS1\" in 2003, and a large amount of work was conducted by the Virtual Environments and Simulation Lab (VESL), part of the University of New South Wales at the Australian Defence Force Academy. VESL conducted (for the ADF) the Virtual Infantry Section Experiment (VISE), which was the first analytical use of the product by a military organisation.\n\nThe AAR 2 and Observer 2 were developed in 2003 in order to record the large quantities of data generated during VISE (previous versions were unable to handle more than a few squads in the one scenario), and the engine was updated to version 1.99. This gave VBS1 an inherent and powerful data recording and mission playback capability.\n\n\"VBS1\" was refined and improved and underwent limited public release on May 21, 2004 (previously the product was only released to military (or similar) organisations). The release occurred primarily to increase awareness of the product and foster a user community. Exclusive distribution rights for \"VBS1\" to North America was given to Coalescent Technologies, with BIA and BIS distributing to the rest of the world via the online shop.\n\nThe ADF conducted trials of \"VBS1\" in November 2004 as part of the VICE. The trials resulted in \"VBS1\" being recommended as a suitable training tool for a range of military purposes: from Infantry Minor Tactics through to combined arms operations. The results of the trial are summarized in the VESL paper \"Proficient Soldier to Skilled Gamer: Training for COTS Success\".\n\nThe ADF provided a large amount of feedback regarding the product and this resulted in engine updates and improvements to the AAR and Observer. The \"VBS1\" 2.07 patch was released in mid-2005 and also AAR 3 and Observer 3. In accordance with ADF requests, the \"VBS1\" Instructor Interface and also numerous convoy training enhancements were implemented. The ADF used these new features as part of pre-deployment training for the Al Muthanna Task Group 2 (AMTG2), who were to deploy to Iraq in late 2005. BIA developed the town of As Samawah (as part of Terrain Pack 3) in \"VBS1\" to a high level of detail from photos and maps, and also a 50 km² Al Muthanna terrain area.\n\nIn 2005 computer game Operation Flashpoint was used by the US based company BBN Technologies to create DARWARS Ambush! Convoy Simulator a commercial military training product developed as part of the DARPA DARWARS program.\n\nIn April 2006, a \"VBS1\" enterprise license has been negotiated with the Australian Defence Force (ADF) and the New Zealand Defence Force (NZDF), which includes provision of \"VBS1\" and an update to \"VBS2\" for both the ADF and the NZDF.\n\nIn April 2006 was BIA chosen by Australian Defence Force (ADF) to develop a number of Loadmaster Virtual Reality Simulators (LVRS) utilizing \"VBS1\" simulator engine.\n\nIn August 2006 the USMC has purchased an enterprise license of Virtual Battlespace (VBS) covering an unlimited number of VBS1 and VBS2 licenses. Two version of VBS1 delivered: 'VBS1 Developer' provides a fully functional product for use in simulation centers and 'VBS1 Lite' for wider distribution.\n\nIn November 2006 the United States Army John F. Kennedy Special Warfare Center and School (USAJFKSWCS) has purchased a site license of Virtual Battlespace (VBS). The license provides a large number of VBS1 and VBS2 licenses to USAJFKSWCS.\n\n\"VBS1\" offers realistic battlefield simulations and the ability to operate land, sea and air vehicles. Instructors may create new scenarios and then run the simulation from multiple viewpoints. The squad management system enables users to issue orders to squad members as well as coordinate both lethal and non-lethal tasks. \"VBS1\" allows free play within scenario-based training missions. It also incorporates simulation of wind, rain, fog, clouds, time of day, sunrise and sunset and tides.\n\n\"VBS1\" is based on a commercial game created by Bohemia Interactive Studio. It was designed for federal, state and local government agencies and can be specifically tailored to meet the individual needs of military, law enforcement, homeland defense, and first responder training environments. \"VBS1\" can be deployed over a LAN or through the Internet on both mobile and desktop computers.\n\n\"VBS1\" may be used to teach doctrine, tactics, techniques, and procedures during squad and platoon offensive, defensive, and patrolling operations. It may also be used to teach and rehearse security emergency response procedures in lethal and non-lethal environments. \"VBS1\" delivers a synthetic environment for the practical exercise of the leadership and organizational behavior skills required to successfully execute small unit missions.\n\nVBS1 external client API has been used to create HLA and DIS gateways.\n\nIn alphabetical order:\n\n\"* - Sources needed to confirm\"\n\nThe VBS1 system is also being evaluated for fielding in the United Kingdom, Israel, Singapore, the Czech Republic and expanded fielding within the U.S. Marine Corps & U.S. Navy. \nAlso State of South Carolina has now joined the list of State, Federal, and Local governments ordering the VBS1 synthetic training system.\n\nThanks to LVC (Live-Virtual-Constructive) Game by Calytrix, VBS1 and VBS2 will also be integrated into the USMC DVTE network, seamlessly interoperating with simulations such as JSAF. The DVTE will soon leverage a wide range of VBS2 enhancements including command and control functionality, modifiable agent-based AI and real-time mission editing.\n\nVBS1 and VBS2 may also be integrated with C2PC to provide a high-fidelity, networked environment linking the live and virtual domains (live marines in the field, tracked by GPS, interoperating with virtual entities controlled from within a USMC simulation centre).\n\n\"VBS1\" was developed by *Bohemia Interactive Australia (division of *Bohemia Interactive Studio) with limited amount of development managed by *Coalescent Technologies.\n\n\"VBS1\" is in distribution worldwide by *Bohemia Interactive Studio and *Bohemia Interactive Australia division.\n\nPrior to March 2006 was \"VBS1\" distribution in North America handled by *Coalescent Technologies.\n\n\n"}
{"id": "11347091", "url": "https://en.wikipedia.org/wiki?curid=11347091", "title": "Valve audio amplifier technical specification", "text": "Valve audio amplifier technical specification\n\nTechnical specifications and detailed information on the valve audio amplifier, including its development history.\n\nValves (also known as vacuum tubes) are very high input impedance (near infinite in most circuits) and high-output impedance devices. They are also high-voltage / low-current devices.\n\nThe characteristics of valves as gain devices have direct implications for their use as audio amplifiers, notably that power amplifiers need output transformers (OPTs) to translate a high-output-impedance high-voltage low-current signal into a lower-voltage high-current signal needed to drive modern low-impedance loudspeakers (cf. transistors and FETs which are relatively low voltage devices but able to carry large currents directly).\n\nAnother consequence is that since the output of one stage is often at ~100 V offset from the input of the next stage, direct coupling is normally not possible and stages need to be coupled using a capacitor or transformer. Capacitors have little effect on the performance of amplifiers. Interstage transformer coupling is a source of distortion and phase shift, and was avoided from the 1940s for high-quality applications; transformers also add cost, bulk, and weight.\n\nThe following circuits are simplified conceptual circuits only, real world circuits also require a smoothed or regulated power supply, heater for the filaments (the details depending on if the selected valve types are directly or indirectly heated), and the cathode resistors are often bypassed, etc.\n\nThe basic gain stage for a valve amplifier is the auto-biased common cathode stage, in which an anode resistor, the valve, and a cathode resistor form a potential divider across the supply rails. The resistance of the valve varies as a function of the voltage on the grid, relative to the voltage on the cathode.\n\nIn the auto-bias configuration, the \"operating point\" is obtained by setting DC potential of the input grid at zero volts relative to ground via a high-value \"grid leak\" resistor. The anode current is set by the value of the grid voltage relative to the cathode and this voltage is now dependent upon the value of the resistance selected for the cathode branch of the circuit.\n\nThe anode resistor acts as the load for the circuit and is typically order of 3-4 times the anode resistance of the valve type in use. The output from the circuit is the voltage at the junction between the anode and anode resistor. This output varies relative to changes in the input voltage and is a function of the voltage amplification of the valve \"mu\" and the values chosen for the various circuit elements.\n\nAlmost all audio preamplifier circuits are built using cascaded common cathode stages.\n\nThe signal is usually coupled from stage to stage via a coupling capacitor or a transformer, although direct coupling is done in unusual cases.\n\nThe cathode resistor may or may not be bypassed with a capacitor. Feedback may also be applied to the cathode resistor.\n\nA simple SET power amplifier can be constructed by cascading two stages, using an output transformer as the load.\n\nTwo triodes with the cathodes coupled together to form a differential pair. This stage has the ability to cancel common mode (equal on both inputs) signals, and if operated in class A also has the merit of having the ability to largely reject any supply variations (since they affect both sides of the differential stage equally), and conversely the total current drawn by the stage is almost constant (if one side draws more instantaneously the other draws less), resulting in minimal variation in the supply rail sag, and this possibly also interstage distortion.\n\nTwo power valves (may be triodes or tetrodes) being differentially driven to form a push–pull output stage, driving a push–pull transformer load. This output stage makes much better use of the transformer core than the single-ended output stage.\n\nA \"long tail\" is a constant current (CC) load as the shared cathode feed to a differential pair. In theory the more constant current linearises the differential stage.\n\nThe CC may be approximated by a resistor dropping a large voltage, or may be generated by an active circuit (either valve, transistor or FET based)\n\nThe long-tail pair can also be used as a phase splitter. It is often used in guitar amplifiers (where it is referred to as the \"phase inverter\") to drive the power section.\n\nAs an alternate to the long-tail pair, the \"concertina\" uses a single triode as a variable resistance within a potential divider formed by Ra and Rk either side of the valve. The result is that the voltage at the anode swings exactly and opposite to the voltage at the cathode, giving a perfectly balanced phase split. the disadvantage of this stage (cf the differential long-tail pair) is that it does not give any gain. Using a double triode (typically octal or noval) to form a SET input buffer (giving gain) to then feed a concertina phase splitter is a classic push–pull front end, typically followed by a driver (triode) and (triode or pentode) output stage (in ultra linear in many cases) to form the classic push–pull amplifier circuit.\n\nThe push–pull output circuit shown is a simplified variation of the Williamson topology, which comprises four stages:\n\n\nThe cascode (a contraction of the phrase \"cascade to cathode\") is a two-stage amplifier composed of a transconductance amplifier followed by a current buffer. In valve circuits, the cascode is often constructed from two triodes connected in series, with one operating as a common grid and thus acting as a voltage regulator, providing a nearly constant anode voltage to the other, which operates as a common cathode. This improves input-output isolation (or reverse transmission) by eliminating the Miller effect and thus contributes to a much higher bandwidth, higher input impedance, high output impedance, and higher gain than a single-triode stage.\n\nThe tetrode has a screen grid (g2) which is between the anode and the first grid and normally serves, like the cascode, to eliminate the Miller effect and therefore also allows a higher bandwidth and/or higher gain than a triode, but at the expense of linearity and noise performance.\n\nA pentode has an additional suppressor grid (g3) to eliminate the tetrode kink. This is used for improved performance rather than extra gain and is usually not accessible externally. Some of these valves use aligned grids to minimise grid current and beam plates instead of a third grid, these are known as \"beam tetrodes\".\n\nIt was realised (and many pentodes were specifically designed to permit) that by strapping the screens to the grid/anode a tetrode/pentode just became a triode again, as such making these late design valves very flexible. \"Triode strapped\" tetrodes are often used in modern amplifier designs that are optimised for quality rather than power output.\n\nIn 1937, Alan Blumlein originated a configuration between a \"triode strapped\" tetrode and normal tetrode, that connects the extra grid (screen) of a tetrode to a tap from the OPT \"part way between\" the anode voltage and the supply voltage. This electrical compromise gives a gain and linearity equal to the best traits of both extremes. In a 1951 engineering paper published by David Hafler and Herbert Keroes, they determined that when the screen tap was set to approximately 43% of anode voltage, an optimized condition within the output stage occurred, which they referred to as \"ultra-linear\". By the late 1950s, this design became the dominant configuration for high-fidelity PP amplifiers.\n\nJulius Futterman pioneered a type of amplifier known as \"output transformerless\" (OTL). These use paralleled valves to match with speaker impedances (typically 8 ohms). This design require numerous valves, run hot, and because they attempt to match impedances in a way fundamentally different from a transformer, they often have a unique sound quality. 6080 triodes, designed for regulated power supplies, were low-impedance types sometimes pressed into transformerless use.\n\nSome valve amplifiers use the single-ended triode (SET) topology that uses the gain device in class A. SETs are extremely simple and have low parts count. Such amplifiers are expensive because of the output transformers required.\n\nThis type of design results in an extremely simple distortion spectrum comprising a monotonically decaying series of harmonics. Some consider this distortion characteristic is a factor in the attractiveness of the sound such designs produce. Compared with modern designs SETs adopt a minimalist approach, and often have just two stages, a single stage triode voltage amplifier followed by a triode power stage. However, variations using some form of active current source or load, not considered a gain stage, are used.\n\nThe typical valve using this topology in (rare) current commercial production is the 300B, which yields about 5 watts in SE mode. Rare amplifiers of this type use valves such as the 211 or 845, capable of about 18 watts. These valves are bright emitter transmitting valves, and have thoriated tungsten filaments which glow like light bulbs when powered.\n\nSee paragraphs further down regarding high-power commercially available SET amplifers offering up to 40 watts with no difficulty, following the development of output transformers to overcome the above restrictions.\n\nThe pictures below are of a commercial SET amplifier, and also a prototype of a hobbyist amplifier.\n\nOne reason for SETs being (usually) limited to low power is the extreme difficulty (and consequent expense) of making an output transformer that can handle the plate current without saturating, while avoiding excessively large capacitive parasitics.\n\nThe use of differential (\"push–pull\") output stages cancels standing bias current drawn through the output transformer by each of the output valves individually, greatly reducing the problem of core saturation and thus facilitating the construction of more powerful amplifiers at the same time as using smaller, wider bandwidth and cheaper transformers.\n\nThe cancellation of the differential output valves also largely cancels the (dominant) even-order harmonic distortion products of the output stage, resulting in less THD, albeit dominated now by odd-order harmonics and no longer monotonic.\n\nIdeally, cancellation of even-order distortion is perfect, but it the real world it is not, even with closely matched valves. PP OPTs usually have a gap to prevent saturation, though less than required by a single-ended circuit.\n\nSince the 1950s the vast majority of high-quality valve amplifiers, and almost all higher-power valve amplifiers have been of the push–pull type.\n\nPush–pull output stages can use triodes for lowest Z and best linearity, but often use tetrodes or pentodes which give greater gain and power. Many output valves such as KT88, EL34, and EL84 were specifically designed to be operated in either triode or tetrode mode, and some amplifiers can be switched between these modes. Post-Williamson, most commercial amplifiers have used tetrodes in the \"ultra-linear\" configuration.\n\nClass A pure triode PP stages are sufficiently linear that they can be operated without feedback, although modest NFB to reduce distortion, reduce Z, and control gain may be desirable. Their power efficiency is, however, much less than class AB (and, of course, class B); significantly less output power is available for the same anode dissipation.\n\nClass A PP designs have no crossover distortion and distortion becomes negligible as signal amplitude is reduced. The effect of this is that class A amplifiers perform extremely well with music that has a low average level (with negligible distortion) with momentary peaks.\n\nA disadvantage of Class A operation for power valves is a shortened life, because the valves are always fully \"on\" and dissipate maximum power all of the time. Signal amplifier valves not operating at high power are not affected in this way.\n\nPower supply regulation (variation of voltage available with current drawn) is not an issue, as average current is essentially constant; AB amplifiers, which draw current dependent upon signal level, require attention to supply regulation.\n\nClass B and AB amplifiers are more efficient than class A, and can deliver higher power output levels from a given power supply and set of valves.\n\nHowever, the price for this is that they suffer from crossover distortion, of more or less constant amplitude regardless of signal amplitude. This means that class AB and B amplifiers produce their lowest distortion percentage at near maximum amplitude, with poorer distortion performance at low levels. As the circuit changes from pure class A, through AB1 and AB2, to B, open-loop crossover distortion worsens.\nClass AB and B amplifiers use NFB to reduce open-loop distortion. Measured distortion spectra from such amplifiers show that distortion percentage is dramatically reduced by NFB, but the residual distortion is shifted towards higher harmonics.\n\nIn a class B push–pull amplifier, output valve current which must be provided by the power supply ranges from nearly zero for zero signal to a maximum at maximum signal. Consequently, for linear response to transient signal changes the power supply must have good regulation.\n\nOnly class A can be used in single-ended mode, as part of the signal would otherwise be cut off. The driver stage for class AB2 and B valve amplifiers must be capable of supplying some signal current to the power valve grids (\"driving power\").\n\nThe biasing of a push–pull output stage can be adjusted (at the design stage, usually not in a finished amplifier) between class A (giving best open-loop linearity) through classes AB1 and AB2, to class B (giving greatest power and efficiency from a given power supply, output valves and output transformer).\n\nMost commercial valve amplifiers operate in Class AB1 (typically pentodes in the ultra-linear configuration), trading open-loop linearity against higher power; some run in pure class A.\n\nThe typical topology for a PP amplifier has an input stage, a phase splitter, a driver and the output stage, although there are many variations of the input stage / phase splitter, and sometimes two of the listed functions are combined in one valve stage. The dominant phase splitter topologies today are the concertina, floating paraphase, and some variation of the long-tail pair.\n\nThe gallery shows a modern home-constructed, fully differential, pure class A amplifier of about 15 watts output power without negative feedback, using 6SN7 low-power dual triodes and KT88 power tetrodes.\n\nBecause of their inability to drive low impedance loads directly, valve audio amplifiers must employ output transformers to step down the impedance to match the loudspeakers.\n\nOutput transformers are not perfect devices and will always introduce some odd harmonic distortion and amplitude variation with frequency to the output signal. In addition, transformers introduce frequency-dependent phase shifts which limit the overall negative feedback which can be used, to keep within the Nyquist stability criteria at high frequencies and avoid oscillation. In recent years, however, the development of improved transformer designs and winding techniques greatly reduce these unwanted effects within the desired pass-band, moving them further out to the margins.\n\nFollowing its invention by Harold Stephen Black, negative feedback (NFB) has been almost universally adopted in amplifiers of all types, to substantially reduce distortion, flatten frequency response, and reduce the effect of component variations. This is especially needed with non-class-A amplifiers.\n\nFeedback very much reduces distortion percentage, but the distortion spectrum becomes more complex, with a far higher contribution from higher harmonics; the high harmonics, if at an audible level, are much more undesirable than lower ones, so that the improvement due to lower overall distortion is partly cancelled by its nature. It is reported that under some circumstances the absolute amplitude of higher harmonics may increase with feedback, although total distortion decreases.\n\nNFB reduces output impedance (Z) (which may vary as a function of frequency in some circuits). This has two important consequences:\n\n\nLike any amplifying device, valves add noise to the signal to be amplified. Noise is due to device imperfections plus unavoidable temperature-dependent thermal fluctuations (systems are usually assumed to be at room temperature, \"T\" = 295 K). Thermal fluctuations cause an electrical noise power of formula_1, where formula_2 is the Boltzmann constant and \"B\" the bandwidth. Correspondingly, the voltage noise of a resistance \"R\" into an open circuit is formula_3 and the current noise into a short circuit is formula_4.\nThe noise figure is defined as the ratio of the noise power at the output of the amplifier to the noise power that would be present at the output if the amplifier were noiseless (due to amplification of thermal noise of the signal source). An equivalent definition is: noise figure is the factor by which insertion of the amplifier degrades the signal to noise ratio. It is often expressed in decibels (dB). An amplifier with a 0 dB noise figure would be perfect.\n\nThe noise properties of valves at audio frequencies can be modelled well by a perfect noiseless valve having a source of voltage noise in series with the grid. For the EF86 low-noise audio pentode valve, for example, this voltage noise is specified (see e.g., the Valvo, Telefunken or Philips data sheets) as 2 microvolts integrated over a frequency range of approximately 25 Hz to 10 kHz. (This refers to the integrated noise, see below for the frequency dependence of the noise spectral density.) This equals the voltage noise of a 25 kΩ resistor. Thus, if the signal source has an impedance of 25 kΩ or more, the noise of the valve is actually smaller than the noise of the source. For a source of 25 kΩ, the noise generated by valve and source are the same, so the total noise power at the output of the amplifier is the square root of two times the noise power at the output of the perfect amplifier. It is not simply double because the noise sources are random and there is some partial cancellation in the combined noise. The noise figure is then 1.414, or 1.5 dB. For higher impedances, such as 250 kΩ, the EF86's voltage noise is 1/10 lower than the sources's own noise, and the noise figure is ~1 dB. For a low-impedance source of 250 Ω, on the other hand, the noise contribution of the valve is 10 times larger than the signal source, and the noise figure is approximately ten, or 10 dB.\n\nTo obtain low noise figure, the impedance of the source can be increased by a transformer. This is eventually limited by the input capacitance of the valve, which sets a limit on how high the signal impedance can be made if a certain bandwidth is desired.\n\nThe noise voltage density of a given valve is a function of frequency. At frequencies above 10 kHz or so, it is basically constant (\"white noise\"). White noise is often expressed by an equivalent noise resistance, which is defined as the resistance which produces the same voltage noise as present at the valve input. For triodes, it is approximately (2-3)/\"g\", where \"g\" is the transconductivity. For pentodes, it is higher, about (5-7)/\"g\". Valves with high \"g\" thus tend to have lower noise at high frequencies.\n\nIn the audio frequency range (below 1–100 kHz), \"1/\"f\"\" noise becomes dominant, which rises like 1/\"f\". Thus, valves with low noise at high frequency do not necessarily have low noise in the audio frequency range. For special low-noise audio valves, the frequency at which 1/\"f\" noise takes over is reduced as far as possible, maybe to something like a kilohertz. It can be reduced by choosing very pure materials for the cathode nickel, and running the valve at an optimized (generally low) anode current.\n\nUnlike solid-state devices, valves are assemblies of mechanical parts whose arrangement determines their functioning, and which cannot be totally rigid. If a valve is jarred, either by the equipment being moved or by acoustic vibrations from the loudspeakers, or any sound source, it will produce an output signal, as if it were some sort of microphone (the effect is consequently called microphony). All valves are subject to this to some extent; low-level voltage amplifier valves for audio are designed to be resistant to this effect, with extra internal supports. The EF86 mentioned in the context of noise is also designed for low microphony, though its high gain makes it particularly susceptible.\n\nFor high-end audio, where cost is not the primary consideration, valve amplifiers have remained popular and indeed during the 1990s made a commercial resurgence.\n\nCircuits designed since then in most cases remain similar to circuits from the valve age, but benefit from advances in ancillary component quality (including capacitors) as well as general progress across the electronics industry which gives designers increasingly powerful insight into circuit operation. Solid-state power supplies are more compact, efficient, and can have very good regulation.\n\nSemiconductor power amplifiers do not have the severe limitations on output power imposed by thermionic devices; accordingly loudspeaker design has evolved in the direction of smaller. more convenient, loudspeakers, trading off power efficiency for small size, giving speakers of similar quality but smaller size which require much greater power for the same loudness than hitherto. In response, many modern valve push–pull amplifiers are more powerful than earlier designs, reflecting the need to drive inefficient speakers.\n\nWhen valve amplifiers were the norm, user-adjustable \"tone controls\" (a simple two-band non-graphic equaliser) and electronic filters were used to allow the listener to change frequency response according to taste and room acoustics; this has become uncommon. Some modern equipment uses graphic equalisers, but valve preamplifiers tend not to supply these facilities (except for RIAA and similar equalisation needed for vinyl and shellac discs).\n\nModern signal sources, unlike vinyl discs, supply line level signals without need for equalisation. It is common to drive valve power amps directly from such source, using passive volume and input source switching integrated into the amplifier, or with a minimalist \"line level\" control amplifier which is little more than passive volume and switching, plus a buffer amplifier stage to drive the interconnects.\n\nHowever, there is some small demand for valve preamps and filter circuits for studio microphone amplifiers, equalising preamplifiers for vinyl discs, and exceptionally for active crossovers.\n\nWhen valve amplifiers were the norm, SETs more-or-less disappeared from western products except for low-power designs (up to 5 watts), with push–pull indirectly heated triodes or triode-connected valves such as EL84 becoming the norm.\n\nHowever, the far east never abandoned valves, and especially the SET circuit; indeed the extreme interest in all things audiophile in Japan and other far eastern countries sustained great interest in this approach.\n\n\nSince the 1990s a niche market has developed again in the west for low-power commercial SET amplifies (up to 7 watts), notably using the 300B valve in recent years, which has become fashionable and expensive. Lower-power amplifiers based on other vintage valve types such as 2A3 and 45 are also made.\n\nEven more rarely, higher powered SETs are produced commercially, usually using the 211 or 845 transmitting valves, which are able to deliver 20 watts, operating at 1000 V. Notable amplifiers in this class are those from Audio Note corporation (designed in Japan), including the \"Ongaku\", voted amplifier of the year during the late 1990s. A very small number of hand-built products of this class sell at very high prices (from US$10,000). The Wavac 833 may be the world's most expensive hi-fi amplifier, delivering around 150 watts using an 833A valve.\n\nAside from this Wavac and a very few other high-power SETs, SET amplifiers usually need to be carefully paired with very efficient speakers, notably horn and transmission-line enclosures and full-range drivers such as those made by Klipsch and Lowther, which invariably have their own quirks, offsetting their advantages of very high efficiency and minimalism.\n\nSome companies such as the Chinese company \"Ming Da\" make low power SETs using valves other than the 300B, such as KT90 (a development of the KT88) and up to the more powerful sister of the 845, the 805ASE, with output power of 40 watts over the full audio range from 20 Hz. This is made possible by an output transformer design which does not saturate at high levels and has high efficiency.\n\nMainstream modern loudspeakers give good sound quality in a compact size, but are much less power-efficient than older designs and require powerful amplifiers to drive them. This makes them unsuitable for use with valve amplifiers, particularly lower-power single-ended designs. Valve hi-fi power amplifier designs since the 1970s have had to move mainly to class AB1 push–pull (PP) circuits. Tetrodes and pentodes, sometimes in ultra-linear configuration, with significant negative feedback, are the usual configuration.\n\nSome class A push–pull amplifiers are made commercially. Some amplifiers can be switched between classes A and AB; some can be switched into triode mode.\n\nMajor manufacturers in the PP valve market include:\n\n\nThe simplicity of valve amplifiers, especially single-ended designs, makes them viable for home construction. This has some advantages:\n\n\nPoint-to-point hand-wiring tends to be used rather than circuit boards in low-volume high-end commercial constructions as well as by hobbyists. This construction style is satisfactory due to ease of construction, adapted to the number of physically large and chassis mounted components (valve sockets, large supply capacitors, transformers), the need to twist heater wiring to minimise hum, and as a side effect benefiting from the fact that \"flying\" wiring minimises capacitive effects.\n\nOne picture below shows circuit constructed using \"standard\" modern industrial parts (630 V MKP capacitors/metal film resistors). One advantage a hobbyist has over a commercial producer is the ability to use higher quality parts that are not reliably available in production volumes (or at a commercially viable cost price). For example, the \"silver top getter\" Sylvania brown base 6SN7s in use in the external picture date from the 1960s.\n\nAnother picture shows exactly the same circuit constructed using Russian military production Teflon capacitors and non-inductive planar film resistors, of the same values.\n\nThe wiring of a commercial amplifier is also shown for comparison\nVery occasionally, very-high-power valves (usually designed for use in radio transmitters) from decades ago are pressed into service to create one-off SET designs (usually at very high cost). Examples include valves 211 and 833.\n\nThe main problem with these designs is constructing output transformers able to sustain the plate current and resultant flux density without core saturation over the full audio-frequency spectrum. This problem increases with power level.\n\nAnother problem is that the voltages for such amplifiers often pass well beyond 1 kV, which forms an effective disincentive to commercial products of this type.\n\nMany modern commercial amplifiers (and some hobbyist constructions) place multiple pairs of output valves of readily obtainable types in parallel to increase power, operating from the same voltage required by a single pair. A beneficial side effect is that the output impedance of the valves, and thus the transformer turns ratio needed, is reduced, making it easier to construct a wide bandwidth transformer.\n\nSome high-power commercial amplifiers use arrays of standard valves (e.g. EL34, KT88) in the parallel push–pull (PPP) configuration (e.g. Jadis, Audio Research, McIntosh, Ampeg SVT).\n\nSome home-constructed amplifiers use pairs of high-power transmitting valves (e.g. 813) to yield 100 watts or more of output power per pair in class AB1 (ultra-linear).\n\nThe output transformer (OPT) is a major component in all mainstream valve power amplifiers, accounting for significant cost, size, and weight. It is a compromise, balancing the needs for low stray capacitance, low losses in iron and copper, operation without saturation at the required direct current, good linearity, etc.\n\nOne approach to avoid the problems of OPTs is to avoid the OPT entirely, and directly couple the amplifier to the loudspeaker, as is done with most solid-state amplifiers. Some designs without output transformers (OTLs) were produced by Julius Futterman in the 1960s and '70s, and more recently in different embodiments by others.\n\nValves normally match much higher impedances than that of a loudspeaker. Low-impedance valve types and purpose-designed circuits are required. Reasonable efficiency and moderate Z (damping factor) can be achieved.\n\nThese effects mean that OTLs have selective speaker load requirements, just like any other amplifier. Generally a speaker of at least 8 ohms is required, although larger OTLs are often quite comfortable with 4 ohm loads. Electrostatic speakers (often considered difficult to drive) often work especially well with OTLs.\n\nThe more recent and more successful OTL circuits employ an output circuit generally known as a Circlotron. The Circlotron has about one-half the output impedance of the Futterman-style (totem-pole) circuits. The Circlotron is fully symmetrical and does not require large amounts of feedback to reduce output impedance and distortion. Successful embodiments use the 6AS7G and the Russian 6C33-CB power triodes.\n\nA common myth is that a short-circuit in an output valve may result in the loudspeaker being connected directly across the power supply and destroyed. In practice, the older Futterman-style amplifiers have been known to damage speakers, due not to shorts but to oscillation. The Circlotron amplifiers often feature direct-coupled outputs, but proper engineering (with a few well-placed fuses) ensures that damage to a speaker is no more likely than with an output transformer.\n\nModern OTLs are often more reliable, sound better, and are less expensive than many transformer-coupled valve approaches.\n\nIn a sense this niche is a subset of OTLs however it merits treating separately because unlike an OTL for a loudspeaker, which has to push the extremes of a valve circuit's ability to deliver relatively high currents at low voltages into a low impedance load, some headphone types have impedances high enough for normal valve types to drive reasonably as OTLs, and in particular electrostatic loudspeakers and headphones which can be driven directly at hundreds of volts but minimal currents.\n\nOnce more there are some safety issues associated with direct drive for electrostatic loudspeakers, which in extremis may use transmitting valves operating at over 1 kV. Such systems are potentially lethal.\n\n\n"}
{"id": "26887148", "url": "https://en.wikipedia.org/wiki?curid=26887148", "title": "Yerevan Computer Research and Development Institute", "text": "Yerevan Computer Research and Development Institute\n\nThe Yerevan Computer Research and Development Institute (YCRDI) ( (\"Yerevani mat'ematikakan mekenaneri gitahetazotakan institut (YerMMGHI)\")), is a scientific research institute and the pioneer of the IT and software industry in the Republic of Armenia. It was founded by the government of USSR in 1956 in Yerevan for the development of computer equipment. It became a major centre for the development of computers and automatic control systems for civil and defense purposes.\n\nAt the beginning of the 1990s, the institute employed over 7,000 staff.\n\nYCRDI is specializing in the design and implementation of complex management information systems. YCRDI is an ISO 9000 certified company.\n\nPortfolio includes the following products:\nYerevan Computer Research and Development Institute has extensive experience in the design, development, and implementation of large computerized systems, software and hardware solutions for radio electronics industry, specialized systems, and customized software products.\n\n\n"}
