{"id": "378634", "url": "https://en.wikipedia.org/wiki?curid=378634", "title": "Advanced Authoring Format", "text": "Advanced Authoring Format\n\nThe Advanced Authoring Format (AAF) is a professional file interchange format designed for the video post-production and authoring environment. It was created by the Advanced Media Workflow Association. The AMWA develops specifications and technologies to facilitate the deployment and operation of efficient media workflows, working closely with standards bodies like the SMPTE.\n\nTechnical work of the AMWA is through projects that strive for compatibility between AAF (Advanced Authoring Format), BXF, MXF (Material Exchange Format) and XML. The current projects fall into three categories: data models, interface specifications, and application specifications.\n\nAAF was created to help address the problem of multi-vendor, cross-platform interoperability for computer-based digital video production. There are two kinds of data that can be interchanged using AAF:\n• Audio, video, still image, graphics, text, animation, music, and other forms of multimedia data. In AAF these kinds of data are called essence data, because they are the essential data within a multimedia program that can be perceived directly by the audience\n• Data that provides information on how to combine or modify individual sections of essence data or that provides supplementary information about essence data. In AAF these kinds of data are called metadata, which is defined as data about other data. The metadata in an AAF file can provide the \ninformation needed to combine and modify the sections of essence data in the AAF file to produce a complete multimedia program.\n\nThere are two major parts to AAF: the AAF Object Specification and the AAF Software Development Kit (SDK) Reference Implementation.\n\nThe AAF Object Specification defines a structured container for storing essence data and metadata using an object-oriented model. It defines the logical contents of the objects and the rules for how the objects relate to each other. The AAF Low-Level Container Specification describes how each object is stored on disk. It uses Structured Storage, a file storage system developed by Microsoft, to store the objects on disk.\n\nAAF does a number of things:\n\n\nBy preserving source referencing, and abstracting the creative decisions that are made, AAF tries to improve workflow and simplify project management.\n\nAAF is designed to be a data representation of works in progress, as compared to MXF (Material Exchange Format), which is for exchanging finished media products. While MXF uses a KLV (Key Length Value) format for storage, AAF uses the Microsoft Structured Storage system. MXF was developed to be essentially a subset of the AAF data model, under the Zero Divergence Directive (ZDD) policy. This allows for workflows that involve the mixing of AAF and MXF.\n\nAAF's rich data model combining metadata and various types of essence has led to its use in non-broadcast applications as well. For example, AAF has been adopted by the DoD/IC Motion Imagery Standards Board (MISB) for their Aerial Surveillance and Photogrammetry Applications standard (ASPA).\n\nThe elements of AAF include:\n\n\nAAF was originally created by the Advanced Media Workflow Association (AMWA), formerly the AAF Association Inc., a broadly based trade association created to promote the development and adoption of AAF, MXF and SOA technology in media workflows. The AAF Object Model is now being standardized through SMPTE, including a better definition of the mapping between MXF and AAF essence.\n\n\n\n"}
{"id": "26039991", "url": "https://en.wikipedia.org/wiki?curid=26039991", "title": "Apsley Pellatt", "text": "Apsley Pellatt\n\nApsley Pellatt (27 November 1791 – 17 August 1863) was an English glassware manufacturer and politician.\n\nHe was the son of glassware maker Apsley Pellatt (1763–1826) and Mary (née Maberly) Pellatt.\n\nHe joined the family glass-making company of Pellatt and Green in 1811. He took over the London-based glass-works on his father's death, renaming it Apsley Pellatt & Co. \n\nHis main interest lay in the chemistry of glass-making. In 1819, he took out his first patent for the manufacture of \"sulfides\" or Cameo Incrustations. Pellatt originally called them \"Crystallo-Ceramie,\" reflecting their French origin. The process involved the embedding of ceramic figurines into the glass sides of paperweights, jugs, decanters, etc., by cutting a hole in the hot glass, sliding in the insert, and resealing the glass afterward.\n\nPellatt became the most famous and successful producers of sulfides in England from 1819 to the mid-century rivalled only by Baccarat in France. He described their manufacture in a book on glass-making entitled \"Curiosities of Glassmaking\" published in 1849. After his retirement around 1850, the glass-works went into decline in the hands of his brother Frederic.\n\nPellatt was a public-spirited man who for some years served on the Common Council of the City of London. He unsuccessfully contested Bristol at the 1847 general election, and was elected at the 1852 general election as a Member of Parliament (MP) for Southwark.\nHe held the seat until his defeat at the 1857 general election, and was unsuccessful when he stood again in 1859.\n\nHe died in Balham in 1863 and was buried at Staines, where he had lived in later life. He had married twice, firstly in 1814 to Sophronia, daughter of Thomas Kemp; she died in 1815 aged only 23. He married secondly, in Streatham in 1816, to Margaret Elizabeth, daughter of George Evans, of Balham, with whom he had one son, Apsley (who died young) and four daughters. His second wife died in 1874 and was buried alongside him. His younger brother, Mill Pellatt (1795-1863) was grandfather of Canadian financier Sir Henry Pellatt.\n\n\n"}
{"id": "14676857", "url": "https://en.wikipedia.org/wiki?curid=14676857", "title": "Architectural light shelf", "text": "Architectural light shelf\n\nA light shelf is a horizontal surface that reflects daylight deep into a building. Light shelves are placed above eye-level and have high-reflectance upper surfaces, which reflect daylight onto the ceiling and deeper into the space. \n\nLight shelves are typically used in high-rise and low-rise office buildings, as well as institutional buildings. This design is generally used on the equator-facing side of the building, which is where maximum sunlight is found, and as a result is most effective. Not only do light shelves allow light to penetrate through the building, they are also designed to shade near the windows, due to the overhang of the shelf, and help reduce window glare. Exterior shelves are generally more effective shading devices than interior shelves. A combination of exterior and interior shelves will work best in providing an even illumination gradient. \n\nArchitectural light shelves have been proven to reduce the need for artificial lighting in buildings. Since they can reflect light deeper into a space, the use of incandescent and fluorescent lighting can be reduced or completely eliminated, depending on the space. Light shelves make it possible for daylight to penetrate the space up to 2.5 times the distance between the floor and the top of the window. Today, advanced light shelf technology makes it possible to increase the distance up to 4 times. In spaces such as classrooms and offices, light shelves have been proven to increase occupant comfort and productivity. Furthermore, incorporating light shelves in a building design is admissible for the LEED point system, falling under the “Indoor Environment Quality: Daylight & Views” category.\n\nLight shelves may not be suitable for all climates. They are generally used in mild climates and not in tropical or desert climates due to the intense solar heat gain. These hot climates, compared to mild climates, require very small window openings to reduce the amount of heat infiltration.\n\nThe fact that light shelves extend a fair distance into a room may result in interference with sprinkler systems. In Canada, they cannot exceed 1200 mm (4 ft.) in width if sprinklers are present or the design will require integration with sprinkler system to cover the floor area under the light shelf. They also require a higher than average floor-to-ceiling heights in order for them to be effective, or daylight may be inadvertently redirected into occupants' eyes.\n\nThe distance into a space that light is cast is variable depending on both the time of day and the time of year.\n\nLight shelves also increase maintenance requirements and window coverings must be coordinated with light shelf design.\n\nAlternatives to light shelves for window daylighting include blinds and louver systems, both of which can be interior or exterior.\n\nBlinds reduce solar gain, but do little to redirect light into the interior space.\n\nExterior louver systems often rely on adjustments from either complex servo motors or building occupants throughout the day to operate well. Both of these systems can be unreliable at times, reducing the overall benefit of having a daylighting system.\n\n"}
{"id": "6365606", "url": "https://en.wikipedia.org/wiki?curid=6365606", "title": "Autodyne", "text": "Autodyne\n\nThe autodyne circuit was an improvement to radio signal amplification using the De Forest Audion vacuum tube amplifier. By allowing the tube to oscillate at a frequency slightly different from the desired signal, the sensitivity over other receivers was greatly improved. The autodyne circuit was invented by Edwin Howard Armstrong of Columbia University, New York, NY. He inserted a tuned circuit in the output circuit of the Audion vacuum tube amplifier. By adjusting the tuning of this tuned circuit, Armstrong was able to dramatically increase the gain of the Audion amplifier. Further increase in tuning resulted in the Audion amplifier reaching self-oscillation.\n\nThis oscillating receiver circuit meant that the then latest technology continuous wave (\"CW\") transmissions could be demodulated. Previously only spark, interrupted continuous wave (\"ICW\", signals which were produced by a motor chopping or turning the signal on and off at an audio rate), or modulated continuous wave (MCW), could produce intelligible output from a receiver.\n\nWhen the autodyne oscillator was advanced to self-oscillation, continuous wave Morse code dots and dashes would be clearly heard from the headphones as short or long periods of sound of a particular tone, instead of an all but impossible to decode series of thumps. Spark and chopped CW (ICW) were amplitude modulated signals which didn't require an oscillating detector.\nSuch a regenerative circuit is capable of receiving weak signals, if carefully coupled to an antenna. Antenna coupling interacts with tuning, making optimum adjustments difficult.\n\nEarly transmitters emitted \"damped waves\", which were radio frequency sine wave bursts of a number of cycles duration, of decreasing amplitude with each cycle. These bursts recurred at an audio frequency rate, producing an amplitude modulated transmission. The damped waves were a result of the available technologies to generate radio frequencies. \"See\" spark gap transmitter. The transmitters could be keyed on and off to send Morse code.\n\nReceivers could be made with a tuned circuit, a crystal detector, and a headphone. The headphone would respond to the detected bursts, and the operator could copy the Morse code. The received signal was not a sinewave. Instead of a crystal detector, a Fleming valve (tube diode) could be used; it was a stable detector, but not very sensitive. Even better was a using a vacuum triode because it provided some amplification. The regenerative receiver supplied even more gain, but required careful adjustment.\n\nDamped wave transmission had drawbacks, and the focus shifted to \"undamped waves\" or \"continuous wave\" (CW) transmission. The arc converter could produce high power CW transmissions.\n\nThe typical damped wave receiver was ineffective for receiving CW because CW had, ideally, no modulation of the radio frequency during the period of the dot or dash. Several methods were employed to generate an audible tone at the receiver: (1) a chopper, (2) a variable condensor with rotating plates (slope demodulation), (3) a tikker, (4) a separate heterodyne, and (5) the autodyne.\n\nFessenden researched the heterodyne detector.\n\nThe autodyne was widely used in both commercially produced and amateur receiver designs from shortly after the time of its invention until the middle 1930s. It became popular at the beginning of the Depression (ca early 1930s) for \"first detector\" applications in superheterodyne receivers.\n\nMore recently, \"autodyne converters\" are employed in radio receivers for the AM and FM broadcast band. A single transistor combines the functions of amplifier, mixer and local oscillator of an otherwise conventional superheterodyne receiver. Such a stage accepts as input the antenna signal, and provides an output to the intermediate frequency amplifier. In this application, the transistor is made to self-oscillate at the local oscillator frequency.\n\nThe autodyne detector has appeared in specialized fields in the 1960s through the 1990s.\n\n\n\n"}
{"id": "26007951", "url": "https://en.wikipedia.org/wiki?curid=26007951", "title": "Chevron STB process", "text": "Chevron STB process\n\nThe Chevron STB process (also known as Staged Turbulent Bed Retorting Process) is an above-ground shale oil extraction technology. It is classified as a hot recycled solids technology.\n\nThe Chevron STB process is a hot recycled solids technology, which processes small particles of oil shale. It was invented by Paul W. Tamm and Gordon E. Langlois in the Chevron Research Company laboratory in Richmond, California. As a heat carrier, it uses oil shale ash, produced by combustion of spent oil shale in the separate combustor.\n\nIn this process, crushed oil shale is fed into the top of the retort where it is mixed with the hot oil shale ash. The oil shale moves downward through the retort as fluidized bed of particles. While descending, the heat is transferred from the oil shale ash to the raw oil shale causing pyrolysis. As a result, oil shale decomposes to shale oil vapors, oil shale gas and spent oil shale. A stripping gas is inserted from the bottom of retort, which carries oil vapors into solids separation section. The fine particles are directed to the combustor while oil vapors are moved to the condenser. In condenser, shale oil is separated from water vapor and product gases. On the bottom of retort the spent shale is transported to cumbustor.\n\n"}
{"id": "31306690", "url": "https://en.wikipedia.org/wiki?curid=31306690", "title": "Chief information officer (higher education)", "text": "Chief information officer (higher education)\n\nA chief information officer in higher education is the senior executive who is responsible for information and communications technology in the university, college or other higher education institution. The position may not necessarily be called a CIO in some institutions. The CIO title is often coupled with Vice President/Vice Chancellor of information technology, is primarily used at doctoral/research institutions, while the titles of Director or Dean are more common at the other five types of Carnegie Classification of Institutions of Higher Education; MA I, MA II, BA Liberal Arts, BA General, and AA. In addition, the CIO title used at different institutions may represent unique positions with differing roles and responsibilities. Ultimately, there is no one definition for a CIO; it has a variety of meanings, functions, areas of purview, reporting structures, and required qualifications.\nSome traditional executive and administrative positions in higher education, such as a chief financial officer position or chief academic officer/provost have been in existence for a long time and typically have a definite career path. In contrast, the position of CIO in higher education, which has only been around for about 35 years, has no single career path nor single model to explain what can be expected of CIOs in higher education, which makes it a challenge and an opportunity for those interested in preparing for such a role (Brown, 2009; Cash & Pearlson, 2004; Nelson, 2003). Brian L. Hawkins (2004), former president of Educause concluded:\n\nThat being said, research in this field including recent surveys of technology leaders, CIOs and academic leaders in higher education institutions provides data on the current and expected levels of academic degrees of CIOs and their perceived required skill set. This, in turn, may mean that prospective CIOs who prepare in a manner to meet these perceived expectations in terms of academic degrees and skills may be more likely to be successful in being hired into the position of CIO and succeeding at it. The next two sections explore these two areas, that is, the academic degree preparation and skill set needed for CIOs in higher education.\n\nA review of CIO vacancy positions and requirements between April 2009 and May 2010 from the Chronicle of Higher Education, Educause and HigherEdJobs.com found that only 44.30% of the positions required a graduate degree as a requirement (Brown, 2010a). Brown speculates that position postings that do state a graduate degree requirement are simply attempting to create a larger pool of candidates. Brown (2010b) reported that 79% of CIOs (who responded to his survey) have graduate degrees, and this percentage has been steadily increasing since 2007. According to Brown’s 2010 survey, 58% of CIOs have master's degrees while 21% have a doctorate. Of those possessing a doctorate, these CIOs were working in all types of institutions ranging from doctoral-granting institutions to those with a special focus. About one third of these CIOs were working at a Master’s institutions while another third were working at a doctoral-granting institution. In contrast, his survey showed that the majority of CIOs with a master's degree as their highest level of academic preparation were working at institutions granting only associate degrees.\n\nIn terms of degree majors for CIO positions, Brown found that 40% of the job postings did not identify the major preferred and another 44% requested a computer related major, or in IT or business. 46% of CIOs and 48% of members of the institution management team believed that the degree major was not important (Brown, 2010a). On a related note, the top four degree majors for technology leaders which comprised 70% of the responses were technology, business, education and administration (Brown, 2010a).\n\nWhat skill set is needed to successfully serve in the role of Chief Information Officer in higher education? A review of the literature provides some useful perspectives. Linda Fleit (1999), the former president and founder of the IT Consulting Firm Edutech International from 1985 to 2008 and through this experience, well acquainted with the role CIOs in higher education institutions, indicated seven areas which she felt were requirements for CIOs. These included (as cited in Hawkins, 2004): 1) a clear vision about the role on information technology in higher education; 2) excellent oral and written communication and listening skills; 3) ability to form alliances and relationships with key campus constituents; 4) the ability to work collaboratively and effectively; 5) the ability to make and back hard decisions; 6) the ability to manage resources judiciously, and 7) deep expertise and knowledge in at least one aspect of technology. Cash and Pearlson (2004), representing the Harvard Graduate School of Business Administration and the Concours Group, respectively, identify leadership, business and technical competencies as essential for CIOs in higher education. Carol A. Cartwright (2002), who served as president of Kent State University from 1991 to 2006 and who is currently serving as president of Bowling Green State University, sought several essential qualifications in a CIO who would be expected to serve as a full-fledged member of her “executive orchestra”: proven leadership skills, strong management skills, and an understanding of the difference between these two. Wayne Brown (2010b), the CIO at Excelsior College who has conducted annual surveys with CIOs and executive management on the role and effectiveness of CIOs since 2003, surveyed 440 CIOs in higher education in 2010 to determine what they considered the top skills needed to be effective in their positions. Brown (2010b) reported that a CIOs’ top five skills in order were: communication skills, leadership, technical knowledge, interpersonal skills and higher education knowledge. The same question posed to members of the management team at higher education institutions revealed that they ranked the same top five skills as important but in a different order: technical knowledge, communication skills, leadership, higher education knowledge, and interpersonal skills. Lastly, Hawkins (2004) identified five skills that he believed were critical to success as a CIO in higher education: strong communication skills, boundary-spanning ability (i.e. the ability to work across the silos that often exist at institutions), leadership ability, management experience, and a strong understanding of the academic environment.\n\nAlthough there is some variance in the literature as to the best skills and competencies to possess to succeed as a CIO in higher education, the common elements across most of these appear to include leadership skills, management skills, communication skills, business knowledge, higher education experience/knowledge, interpersonal skills, and technical skills/deep knowledge in one aspect of technology. What is unclear in the literature is the quantity and quality of these skills, but most likely these will vary depending on the individual institutional context.\n\nAccording to Brown (2010b), in 2010 59% of CIOs responding to his survey were over 51 years of age compared to 55% in this age bracket in 2009. In addition, he reported that 47% of CIOs in 2010 planned to retire within the next 10 years. Furthermore, his report showed that the average CIO tenure in higher education in 2010 was an average of 6 years, 8 months, which is a drop from 2007 where the average tenure was 7 years, 5 months. The combination of these factors---the aging CIO, retirement plans, the faster change-over in CIO positions—presents a promising picture of job prospects for those seeking CIO positions in higher education in coming years.\n\n\n"}
{"id": "11288562", "url": "https://en.wikipedia.org/wiki?curid=11288562", "title": "Cloud9 (service provider)", "text": "Cloud9 (service provider)\n\nCloud9 is a mobile network operator focussed on providing mobile subscriptions over the air to programmable SIM cards, SoftSIMs and eSIMs. Their service is used in both smartphones and IoT devices.\n\nCloud9 was originally owned by Wire9 Telecom Plc, funded and established by investor and telecom specialist, Lee Jones, before being sold for an undisclosed sum by Jones to billionaire Romain Zaleski. It was established in the UK, Gibraltar and Isle of Man as a domestic Mobile Network Operator obtaining spectrum licences in the Isle of Man in 2007 and Gibraltar in 2010. Around 2011 it was decided that the business would be better focussed on supplying global SIM cards that could be used to save roaming charges. The Gibraltar spectrum licence was sold to another company. The business relocated its core network to Telehouse in London and became a subsidiary of BlueMango Technologies Ltd.\n\nThe company is privately held with headquarters in the United Kingdom.\n\nCloud9 have shipped several million 'Travel SIMs'. They do not supply end users instead preferring to offer a white label service to travel and telecoms resellers. All SIM cards have been branded with the logo of these resellers.\n\nIn addition the company now provides the digital signatures ( 'profiles' or 'IMSIs' ) that provide a SIM card with the ability to register with a network and function. These can be provisioned over the air to dynamic SIM cards such as programmable removable UICCs, SoftSIMs and eSIMs. They are members of the GSM Association and are involved in the GSMA remote SIM provisioning standard for eSIMs that will be released soon.\n\nRemotely provisioned SIMs are gaining traction with smartphone manufacturers (SoftSIMs) and IoT devices (eSIMs).\n\nCloud9 continue to sell SIMs for travellers on a white label basis in addition to the above.\n\nIts Mobile Country Code is 234 and its Mobile Network Code is 18. TADIG code is GBRC9.\n\nThe company has been allocated the following UK number ranges by Ofcom:\n\n4478722, 4477000, 4474409, 4479782, 4479783 and 4475588\n\nIn 2013 they acquired the IPR of a UK manufacturer of core networks, Zynetix Ltd. This means that they now possess all of their own IPR with regards to their core network (HLR/SMSC/GGSN/GMSC etc.). and supply core network components to other companies. Through this they have achieved sales as an MVNE. The Cloud9 core network additionally supports 4G (HSS/PDG).\n\nThe core network is hosted on Cloud9 servers at Telehouse near Canary Wharf in London. Additional components are hosted in Amazon Web Services facilities around the world in order to minimise latency and provide scalability.\n\nThe company has been voted as a Red Herring Top 100 Europe finalist.\n"}
{"id": "13751165", "url": "https://en.wikipedia.org/wiki?curid=13751165", "title": "Coherent diffraction imaging", "text": "Coherent diffraction imaging\n\nCoherent diffractive imaging (CDI) is a “lensless” technique for 2D or 3D reconstruction of the image of nanoscale structures such as nanotubes, nanocrystals, porous nanocrystalline layers, defects, potentially proteins, and more. In CDI, a highly coherent beam of x-rays, electrons or other wavelike particle or photon is incident on an object.\n\nThe beam scattered by the object produces a diffraction pattern downstream which is then collected by a detector. This recorded pattern is then used to reconstruct an image via an iterative feedback algorithm. Effectively, the objective lens in a typical microscope is replaced with software to convert from the reciprocal space diffraction pattern into a real space image. The advantage in using no lenses is that the final image is aberration–free and so resolution is only diffraction and dose limited (dependent on wavelength, aperture size and exposure). Applying a simple inverse Fourier transform to information with only intensities is insufficient for creating an image from the diffraction pattern due to the missing phase information. This is called the phase problem.\n\nThere are two relevant parameters for diffracted waves: amplitude and phase. In typical microscopy using lenses there is no phase problem, as phase information is retained when waves are refracted. When a diffraction pattern is collected, the data is described in terms of absolute counts of photons or electrons, a measurement which describes amplitudes but loses phase information. This results in an ill-posed inverse problem as any phase could be assigned to the amplitudes prior to an inverse Fourier transform to real space.\n\nThree ideas developed that enabled the reconstruction of real space images from diffraction patterns. The first idea was the realization by Sayre in 1952 that Bragg diffraction under-samples diffracted intensity relative to Shannon’s theorem. If the diffraction pattern is sampled at twice the Nyquist frequency (inverse of sample size) or faster it can yield a unique real space image. The second was an increase in computing power in the 1980s which enabled iterative Hybrid input output (HIO) algorithm for phase retrieval to optimize and extract phase information using adequately sampled intensity data with feedback. This method was introduced by Fienup in the 1980s. Finally, the development of “phase recovery” algorithms led to the first demonstration of CDI in 1999 by Miao\nusing a secondary image to provide low resolution information\n. Reconstruction methods were later developed that could remove the need for a secondary image.\n\nIn a typical reconstruction the first step is to generate random phases and combine them with the amplitude information from the reciprocal space pattern. Then a Fourier transform is applied back and forth to move between real space and reciprocal space with the modulus squared of the diffracted wave field set equal to the measured diffraction intensities in each cycle. By applying various constraints in real and reciprocal space the pattern evolves into an image after enough iterations of the HIO process. To ensure reproducibility the process is typically repeated with new sets of random phases with each run having typically hundreds to thousands of cycles. The constraints imposed in real and reciprocal space typically depend on the experimental setup and the sample to be imaged. The real space constraint is to restrict the imaged object to a confined region called the “support.” For example, the object to be imaged can be initially assumed to reside in a region no larger than roughly the beam size. In some cases this constraint may be more restrictive, such as in a periodic support region for a uniformly spaced array of quantum dots. Other researchers have investigated imaging extended objects, that is, objects that are larger than the beam size, by applying other constraints.\n\nIn most cases the support constraint imposed is a priori in that it is modified by the researcher based on the evolving image. In theory this is not necessarily required and algorithms have been developed\n\nThe diffraction pattern of a perfect crystal is symmetric so the inverse Fourier transform of that pattern is entirely real valued. The introduction of defects in the crystal leads to an asymmetric diffraction pattern with a complex valued inverse Fourier transform. It has been shown that the crystal density can be represented as a complex function where its magnitude is electron density and its phase is the “projection of the local deformations of the crystal lattice onto the reciprocal lattice vector Q of the Bragg peak about which the diffraction is measured”. Therefore, it is possible to image the strain fields associated with crystal defects in 3D using CDI and it has been reported in one case. Unfortunately, the imaging of complex-valued functions (which for brevity represents the strained field in crystals) is accompanied by complementary problems namely, the uniqueness of the solutions, stagnation of the algorithm etc. However, recent developments that overcame these problems (particularly for patterned structures) were addressed. On the other hand, if the diffraction geometry is insensitive to strain, such as in GISAXS, the electron density will be real valued and positive. This provides another constraint for the HIO process, thus increasing the efficiency of the algorithm and the amount of information that can be extracted from the diffraction pattern.\n\nClearly a highly coherent beam of waves is required for CDI to work since the technique requires interference of diffracted waves. Coherent waves must be generated at the source (synchrotron, field emitter, etc.) and must maintain coherence until diffraction. It has been shown that the coherence width of the incident beam needs to be approximately twice the lateral width of the object to be imaged. \nHowever determining the size of the coherent patch to decide whether the object does or does not meet the criterion is subject to debate. As the coherence width is decreased, the size of the Bragg peaks in reciprocal space grows and they begin to overlap leading to decreased image resolution.\n\nCoherent x-ray diffraction imaging (CXDI or CXD) uses x-rays (typically .5-4keV) to form a diffraction pattern which may be more attractive for 3D applications than electron diffraction since x-rays typically have better penetration. For imaging surfaces, the penetration of X-rays may be undesirable, in which case a glancing angle geometry may be used such as GISAXS. A typical x-ray CCD is used to record the diffraction pattern. If the sample is rotated about an axis perpendicular to the beam a 3-Dimensional image may be reconstructed.\nDue to radiation damage, resolution is limited (for continuous illumination set-ups) to about 10 nm for frozen-hydrated biological samples but resolutions of as high as 1 to 2 nm should be possible for inorganic materials less sensitive to damage (using modern synchrotron sources). It has been proposed that radiation damage may be avoided by using ultra short pulses of x-rays where the time scale of the destruction mechanism is longer than the pulse duration. This may enable higher energy and therefore higher resolution CXDI of organic materials such as proteins. However, without the loss of information “the linear number of detector pixels fixes the energy spread needed in the beam” which becomes increasingly difficult to control at higher energies.\n\nIn a 2006 report, resolution was 40 nm using the Advanced Photon Source (APS) but the authors suggest this could be improved with higher power and more coherent X-ray sources such as the X-ray free electron laser.\n\nCoherent electron diffraction imaging works the same as CXDI in principle only electrons are the diffracted waves and an imaging plate is used to detect electrons rather than a CCD. In one published report a double walled carbon nanotube (DWCNT) was imaged using nano area electron diffraction (NAED) with atomic resolution. In principle, electron diffraction imaging should yield a higher resolution image because the wavelength of electrons can be much smaller than photons without going to very high energies. Electrons also have much weaker penetration so they are more surface sensitive than X-rays. However, typically electron beams are more damaging than x-rays so this technique may be limited to inorganic materials.\n\nIn Zuo’s approach, a low resolution electron image is used to locate a nanotube. A field emission electron gun generates a beam with high coherence and high intensity. The beam size is limited to nano area with the condenser aperture in order to ensure scattering from only a section of the nanotube of interest. The diffraction pattern is recorded in the far field using electron imaging plates to a resolution of 0.0025 1/Å. Using a typical HIO reconstruction method an image is produced with Å resolution in which the DWCNT chirality (lattice structure) can be directly observed. Zuo found that it is possible to start with non-random phases based on a low resolution image from a TEM to improve the final image quality.\n\nIn 2007, Podorov \"et al.\" proposed an exact analytical solution of CDXI problem for particular cases.\n\nIn 2016 using the coherent diffraction imaging (CXDI) beamline at ESRF (Grenoble, France), the researchers quantified the porosity of large faceted nanocrystalline layers at the origin of photoluminescence emission band in the infrared. It has been shown that phonons can be confined in sub-micron structures, which could help enhance the output of photonic and photovoltaic (PV) applications.\n\nPtychography is a technique which is closely related to coherent diffraction imaging. Instead of recording just one coherent diffraction pattern, several - and sometimes hundreds or thousands - of diffraction patterns are recorded from the same object. Each pattern is recorded from a different area of the object, although the areas must partially overlap with one another. Ptychography is only applicable to specimens that can survive irradiation in the illuminating beam for these multiple exposures. However, it has the advantage that a large field of view can be imaged. The extra translational diversity in the data also means the reconstruction procedure can be faster and ambiguities in the solution space are reduced.\n\n\n"}
{"id": "14979748", "url": "https://en.wikipedia.org/wiki?curid=14979748", "title": "Cool Chips (symposium)", "text": "Cool Chips (symposium)\n\nCool Chips is an international symposium which is held every year since 1998. The symposium is sponsored by Technical Committees on Microprocessors and Microcomputers and Computer Architecture of the IEEE Computer Society. The focus of the symposium is on presentation and discussion of advancements in the areas of low power and high performance chips.\n\n\n"}
{"id": "15584416", "url": "https://en.wikipedia.org/wiki?curid=15584416", "title": "Curve resistance (railroad)", "text": "Curve resistance (railroad)\n\nIn railroad engineering, curve resistance is a part of train resistance, namely the additional rolling resistance a train must overcome when travelling on a curved section of track. Curve resistance is typically measured in per mille, with the correct physical unit being Newton per kilo-Newton or N/kN. Older texts still use the wrong unit of kilogram-force per tonne or kgf/t, which mixes an (outdated) unit of force and a unit of mass. Sometimes also kg/t was used, which confused the resisting force with a mass.\n\nCurve resistance depends on various factors, the most important being the radius and the superelevation of a curve. Since curves are usually banked by superelevation, there will exist some speed at which there will be no sideways force on the train and where therefore curve resistance is minimum. At higher or lower speeds, curve resistance may be a few (or several) times greater.\n\nFormulas typically used in railway engineering in general compute the resistance as inversely proportional to the radius of curvature (thus, they neglect the fact that the resistance is dependent on both speed and superelevation). For example, in the USSR, the standard formula is Wr (curve resistance in parts per thousand or kgf/tonne) = 700/\"R\" where \"R\" is the radius of the curve in meters. Other countries often use the same formula, but with a different numerator-constant. For example, the US used 446/\"R\", Italy 800/\"R\", England 600/\"R\", China 573/\"R\", etc. In Germany, Austria, Switzerland, Czechoslovakia, Hungary, and Romania the term \"R - b\" is used in the denominator (instead of just \"R\"), where \"b\" is some constant. Typically, the expressions used are \"Röckl's formula\", which uses 650/(\"R\" - 55) for \"R\" above 300 meters, and 500/(\"R\" - 30) for smaller radii. The fact that, at 300 meters, the two values of Röckl's formula differ by more than 30% shows that these formulas are rough estimates at best.\n\nThe Russian experiments cited below show that all these formulas are inaccurate. At balancing speed, they give a curve resistance a few times too high (or worse). However, these approximation formulas are still contained in practically all standard railway engineering textbooks. For the US, AREMA American Railway Engineering ..., PDF, p.57 claims that curve resistance is 0.04% per degree of curvature (or 8 lbf/ton or 4 kgf/tonne). Hay's textbook also claims it is independent of superelevation. For Russia in 2011, internet articles use 700/R.\n\nIn the 1960s in the Soviet Union curve resistance was found by experiment to be highly dependent on both the velocity and the banking of the curve, also known as superelevation or cant, as can be seen in the graph above. If a train car rounds a curve at balancing speed such that the component of centrifugal force in the lateral direction (towards the outside of the curve and parallel with the plane of the track) is equal to the component of gravitational force in the opposite direction there is very little curve resistance. At such balancing speed there is zero cant deficiency and results in a frictionless banked turn. But deviate from this speed (either higher or lower) and the curve resistance increases due to the unbalance in forces which tends to pull the vehicle sideways (and would be felt by a passenger in a passenger train). Note that for empty rail cars (low wheel loads) the specific curve resistance is higher, similar to the phenomena of higher rolling resistance for empty cars on a straight track.\n\nHowever, these experiments did not provide usable formulas for curve resistance, because the experiments were, unfortunately, all done on a test track with the same curvature (radius = 955 meters). Therefore, it is not clear how to account for curvature. The Russian experiments plot curve resistance against velocity for various types of railroad cars and various axle loads. The plots all show smooth convex curves with the minimums at balancing speed where the slope of the plotted curve is zero. These plots tend to show curve resistance increasing more rapidly with decreases in velocity below balancing speed, than for increases in velocity (by the same amounts) above balancing speeds. No explanation for this \"asymmetrical velocity effect\" is to be found in the references cited nor is any explanation found explaining the smooth convex curve plots mentioned above (except for explaining how they were experimentally determined).\n\nThat curve resistance is expected to be minimized at balancing speed was also proposed by Schmidt in 1927, but unfortunately the tests he conducted were all at below balancing speed. However his results all show curve resistance decreasing with increasing speed in conformance with this expectation.\n\nTo experimentally find the curve resistance of a certain railroad freight car with a given load on its axles (partly due to the weight of the freight) the same car was tested both on a curved track and on a straight track. The difference in measured resistance(at the same speed) was assumed to be the curve resistance. To get an average for several cars of the same type, and to reduce the effect of aerodynamic drag, one may test a group of the same type of cars coupled together (a short train without a locomotive). The curved track used in the experiments was the of the National Scientific Investigation Institute of Railroad Transport (ВНИИЖТ). A single test run can find the train resistance (force) at various velocities by letting the rolling stock being tested coast down from a higher speed to a low speed, while continuously measuring the deceleration and using Newton's second law of motion (force = acceleration*mass) to find the resistance force that is causing the railroad cars to slow. In such calculations, one must take into account the moment of inertia of the car wheels by adding an equivalent mass (of rotating wheels) to the mass of the train consist. Thus the effective mass of a rail car used for Newton's second law, is larger than the car mass as weighed on a car weighing scale. This additional equivalent mass is tantamount to having the mass of each wheel-axle set be located at its radius of gyration See \"Inertia Resistance\" (for automobile wheels, but it's the same formula for railroad wheels).\n\nDeceleration was measured by measuring the distance traveled (using what might be called a recording odometer or by distance markers placed along the track say every 50 meters), versus time. A division of distance by time results in velocity and then the differences in velocities divided by time gives the deceleration. A sample data sheet shows time (in seconds) being recorded with 3 digits after the decimal point (thousandths of a second).\n\nIt turns out that there is no need to know the mass of the rolling stock to find the specific train resistance in kgf/tonne. This unit is force divided by mass which is acceleration per Newton's second law. But one must multiply kilograms of force by g (gravity) to get force in the metric units of Newtons. So the specific force (the result) is the deceleration multiplied by a constant which is 1/g times a factor to account for the equivalent mass due to wheel rotation. Then this specific force in kgf/kg must be multiplied by 1000 to get kgf/tonne since a tonne is 1000 kg.\n\nАстахов proposed the use of a formula which when plotted is in substantial disagreement with the experimental results curves previously mentioned. His formula for curve resistance (in kgf/tonne) is the sum of two terms, the first term being a conventional k/R term (R is the curve radius in meters) with k=200 instead of 700. The second term is directly proportional to (1.5 times) the absolute value of the unbalanced acceleration in the plane of the track and perpendicular to the rail, such lateral acceleration being equal to the centrifugal acceleration formula_1, minus the gravitation component opposing this acceleration: g·tan(θ), where θ is the angle of the banking due to superelevation and v is the train velocity in m/s.\n\n\n\n"}
{"id": "4124072", "url": "https://en.wikipedia.org/wiki?curid=4124072", "title": "Cyrillic Projector", "text": "Cyrillic Projector\n\nThe Cyrillic Projector is a sculpture created by American artist Jim Sanborn in the early 1990s, and was purchased by the University of North Carolina at Charlotte in 1997. It is currently installed between the campus' Friday and Fretwell Buildings.\n\nThe encrypted sculpture \"Cyrillic Projector\" is part of an encrypted family of three intricate puzzle-sculptures by Sanborn, the other two named \"Kryptos\" and \"Antipodes\". The \"Kryptos\" sculpture (located at CIA headquarters in Langley, Virginia) has text which is duplicated on \"Antipodes.\" \"Antipodes\" has two sides — one with the Latin alphabet and one with Cyrillic. The Latin side is similar to \"Kryptos\". The Cyrillic side is similar to the \"Cyrillic Projector\".\n\nThe Russian text of the \"Cyrillic Projector\" was finally decrypted and translated in English in 2003 after Elonka Dunin \"led the charge\", with the ciphertext independently decrypted by Frank Corr and Mike Bales, and plaintext translation from Russian provided by Dunin.\n\nThe sculpture includes two messages. The first is a Russian text that explains the use of psychological control to develop and maintain potential sources of information. The second is a partial quote about the Soviet dissident, Nobel Peace Prize awarded scientist Sakharov. The text is from a classified KGB memo, detailing concerns that his report at the 1982 Pugwash conference was going to be used by the U.S. for anti-Soviet propaganda purposes.\n\n"}
{"id": "18062390", "url": "https://en.wikipedia.org/wiki?curid=18062390", "title": "DM-11 mine", "text": "DM-11 mine\n\nThe DM-11 is a German anti-tank mine popular in countries of Africa, not suited to be disarmed or neutralized\n"}
{"id": "2625968", "url": "https://en.wikipedia.org/wiki?curid=2625968", "title": "DO-178B", "text": "DO-178B\n\nDO-178B, Software Considerations in Airborne Systems and Equipment Certification is a guideline dealing with the safety of safety-critical software used in certain airborne systems. Although technically a guideline, it was a \"de facto\" standard for developing avionics software systems until it was replaced in 2012 by DO-178C.\n\nThe FAA applies DO-178B as the document it uses for guidance to determine if the software will perform reliably in an airborne environment, when specified by the Technical Standard Order (TSO) for which certification is sought. In the United States, the introduction of TSOs into the airworthiness certification process, and by extension DO-178B, is explicitly established in Title 14: Aeronautics and Space of the Code of Federal Regulations (CFR), also known as the Federal Aviation Regulations, Part 21, Subpart O.\n\nIt was jointly developed by the safety-critical working group RTCA SC-167 of RTCA and WG-12 of EUROCAE. RTCA published the document as RTCA/DO-178B, while EUROCAE published the document as ED-12B.\n\nThe Software Level, also known as the Development Assurance Level (DAL) or also '\"Item Development Assurance Level\"' (IDAL)is determined from the safety assessment process and hazard analysis by examining the effects of a failure condition in the system. The failure conditions are categorized by their effects on the aircraft, crew, and passengers.\n\nDO-178B alone is not intended to guarantee software safety aspects. Safety attributes in the design and as implemented as functionality must receive additional mandatory system safety tasks to drive and show objective evidence of meeting explicit safety requirements. Typically IEEE STD-1228-1994 Software Safety Plans are allocated and software safety analyses tasks are accomplished in sequential steps (requirements analysis, top level design analysis, detailed design analysis, code level analysis, test analysis and change analysis). These software safety tasks and artifacts are integral supporting parts of the process for hazard severity and DAL determination to be documented in system safety assessments (SSA). The certification authorities require and DO-178B specifies the correct DAL be established using these comprehensive analyses methods to establish the software level A-E. Any software that commands, controls, and monitors safety-critical functions should receive the highest DAL - Level A. It is the software safety analyses that drive the system safety assessments that determine the DAL that drives the appropriate level of rigor in DO-178B. The system safety assessments combined with methods such as SAE ARP 4754A determine the after mitigation DAL and may allow reduction of the DO-178B software level objectives to be satisfied if redundancy, design safety features and other architectural forms of hazard mitigation are in requirements driven by the safety analyses. Therefore, DO-178B central theme is design assurance and verification after the prerequisite safety requirements have been established.\n\nThe number of objectives to be satisfied (eventually with independence) is determined by the software level A-E. The phrase \"with independence\" refers to a separation of responsibilities where the objectivity of the verification and validation processes is ensured by virtue of their \"independence\" from the software development team. For objectives that must be satisfied with independence, the person verifying the item (such as a requirement or source code) may not be the person who authored the item and this separation must be clearly documented. In some cases, an automated tool may be equivalent to independence. However, the tool itself must then be qualified if it substitutes for human review.\n\nProcesses are intended to support the objectives, according to the software level (A through D—Level E was outside the purview of DO-178B). Processes are described as abstract areas of work in DO-178B, and it is up to the planners of a real project to define and document the specifics of how a process will be carried out. On a real project, the actual activities that will be done in the context of a process must be shown to support the objectives. These activities are defined by the project planners as part of the Planning process.\n\nThis objective-based nature of DO-178B allows a great deal of flexibility in regard to following different styles of software life cycle. Once an activity within a process has been defined, it is generally expected that the project respect that documented activity within its process. Furthermore, processes (and their concrete activities) must have well defined entry and exit criteria, according to DO-178B, and a project must show that it is respecting those criteria as it performs the activities in the process.\n\nThe flexible nature of DO-178B's processes and entry/exit criteria make it difficult to implement the first time, because these aspects are abstract and there is no \"base set\" of activities from which to work. The intention of DO-178B was not to be prescriptive. There are many possible and acceptable ways for a real project to define these aspects. This can be difficult the first time a company attempts to develop a civil avionics system under this standard, and has created a niche market for DO-178B training and consulting.\n\nFor a generic DO-178B based process, a visual summary is provided including the Stages of Involvement (SOIs) defined by FAA on the \"Guidance and Job Aids for Software and Complex Electronic Hardware\".\n\nSystem requirements are typically input to the entire project.\n\nThe last 3 documents (standards) are not required for software level D.\n\nDO-178B is not intended as a software development standard; it is software assurance using a set of tasks to meet objectives and levels of rigor.\n\nThe development process output documents:\n\nTraceability from system requirements to all source code or executable object code is typically required (depending on software level).\n\nTypically used software development process:\n\nDocument outputs made by this process:\n\nAnalysis of all code and traceability from tests and results to all requirements is typically required (depending on software level).\n\nThis process typically also involves:\n\nOther names for tests performed in this process can be:\n\nDocuments maintained by the configuration management process:\n\nThis process handles problem reports, changes and related activities. The configuration management process typically provides archive and revision identification of:\n\nOutput documents from the quality assurance process:\n\nThis process performs reviews and audits to show compliance with DO-178B. The interface to the certification authority is also handled by the quality assurance process.\n\nTypically a Designated Engineering Representative (DER) reviews technical data as part of the submission to the FAA for approval.\n\nSoftware can automate, assist or otherwise handle or help in the DO-178B processes. All tools used for DO-178B development must be part of the certification process. Tools generating embedded code are qualified as development tools, with the same constraints as the embedded code. Tools used to verify the code (simulators, test execution tool, coverage tools, reporting tools, etc.) must be qualified as verification tools, a much lighter process consisting in a comprehensive black box testing of the tool.\n\nA third party tool can be qualified as a verification tool, but development tools must have been developed following the DO-178 process. Companies providing these kind of tools as COTS are subject to audits from the certification authorities, to which they give complete access to source code, specifications and all certification artifacts.\n\nOutside of this scope, output of any used tool must be manually verified by humans.\n\n\nRequirements traceability is concerned with documenting the life of a requirement. It should be possible to trace back to the origin of each requirement and every change made to the requirement should therefore be documented in order to achieve traceability. Even the use of the requirement after the implemented features have been deployed and used should be traceable.\n\nVDC Research notes that DO-178B has become \"somewhat antiquated\" in that it is not adapting well to the needs and preferences of today's engineers. In the same report, they also note that DO-178C seems well-poised to address this issue.\n\n\n\n"}
{"id": "58899779", "url": "https://en.wikipedia.org/wiki?curid=58899779", "title": "Data center management", "text": "Data center management\n\nData center management is the collection of tasks performed by those responsible for managing ongoing operation of a data center This includes planning for the future.\n\nHistorically, \"data center management\" was seen as something performed by employees, with the help of tools collectively called Data Center Infrastructure Management (DCIM) tools. Now an outsourcing option exists: \"Data-center Management As A Service\" - .\n\nBoth for in-house operation and outsourcing, Service-level agreements must be managed to ensure data-availability.\n\nData center management is a growing major topic for a growing list of large companies who both compete and cooperate:\n\nHardware/software vendors who are willing to live with coopetition are working on projects such as \"The Distributed Management Task Force\" (DMTF) with a goal of learning to \"more effectively manage mixed Linux, Windows and cloud environments.\" \n\nWith the \"DMTF\" a decade old, the list of companies is growing, and also includes companies much smaller than IBM, Microsoft, et al.\n\nAmong the topics currently being explored are:\n\n\"Remote Data Center Management\" allows offsite experts to watch for situations needing their timely intervention at a lower cost than having such staff be onsite 24/7/365.\n\nFurthermore, there is more recognition that while some requirements for on-site hardware have been reduced, spending in other hardware areas such as UPS may have to increase.\n\nData center infrastructure management (DCIM) is the integration of information technology (IT) and facility management disciplines to centralize monitoring, management and intelligent capacity planning of a data center's critical systems. Achieved through the implementation of specialized software, hardware and sensors, DCIM enables common, real-time monitoring and management platform for all interdependent systems across IT and facility infrastructures.\n\nDepending on the type of implementation, DCIM products can help data center managers identify and eliminate sources of risk to increase availability of critical IT systems. DCIM products also can be used to identify interdependencies between facility and IT infrastructures to alert the facility manager to gaps in system redundancy, and provide dynamic, holistic benchmarks on power consumption and efficiency to measure the effectiveness of \"green IT\" initiatives.\n\nIt's important to measure and understand data center efficiency metrics. A lot of the discussion in this area has focused on energy issues, but other metrics beyond the PUE can give a more detailed picture of the data center operations. Server, storage, and staff utilization metrics can contribute to a more complete view of an enterprise data center. In many cases, disc capacity goes unused and in many instances the organizations run their servers at 20% utilization or less. More effective automation tools can also improve the number of servers or virtual machines that a single admin can handle.\n\nDCIM providers are increasingly linking with computational fluid dynamics providers to predict complex airflow patterns in the data center. The CFD component is necessary to quantify the impact of planned future changes on cooling resilience, capacity and efficiency.\n\nPreventive maintenance (or \"preventative maintenance\" (PM) ) is ongoing scheduled inspection intended to detect and correct incipient failures either before they occur or before they develop into major problemssuch as downtime.\n\nWith the increasing use of \"the cloud\" and what has been called \"the Era of Infinite Capacity\", there is still a need for professional Data Center Capacity Planners.\n\nSeveral parameters may limit the capacity of a data center. For long term usage, the main limitations will be available area, then available power. In the first stage of its life cycle, a data center will see its occupied space growing more rapidly than consumed energy. With constant densification of new IT technologies, the need in energy is going to become dominant, equaling then overcoming the need in area (second then third phase of cycle). \n\nIt is important to define a data center strategy before being cornered. The decision, conception and building cycle lasts several years, hence it is imperative to initiate this strategic consideration when the data center reaches about 50% of its power capacity. \n\nMaximum occupation of a data center needs to be stabilized around 85%, be it in power or occupied area. Resources thus managed will allow a rotation zone for managing hardware replacement and will allow temporary cohabitation of old and new generations. In the case where this limit would be overcrossed durably, it would not be possible to proceed to material replacements, which would invariably lead to smothering the information system. The data center is a resource in its own right with its own constraints of time and management (life span of 25 years), it therefore needs to be taken into consideration in the framework of the SI midterm planning (between 3 and 5 years).\n\nAccording to Cloudscene’s Leaderboard for Q1 2018, data center operators are ranked “based on both data center density (total operated data centers)\", as well as \"the number of listed service providers in the facility\". Cloud service providers are ranked based on \"connectivity (the total number of PoPs) for the region.” Chosen from a pool of more than 6,000 providers, the rankings are as follows:\n\n\n\n"}
{"id": "1127038", "url": "https://en.wikipedia.org/wiki?curid=1127038", "title": "Distributed Interactive Simulation", "text": "Distributed Interactive Simulation\n\nDistributed Interactive Simulation (DIS) is an IEEE standard for conducting real-time platform-level wargaming across multiple host computers and is used worldwide, especially by military organizations but also by other agencies such as those involved in space exploration and medicine.\n\nThe standard was developed over a series of \"DIS Workshops\" at the Interactive Networked Simulation for Training symposium, held by the University of Central Florida's Institute for Simulation and Training (IST). The standard itself is very closely patterned after the original SIMNET distributed interactive simulation protocol, developed by Bolt, Beranek and Newman (BBN) for Defense Advanced Research Project Agency (DARPA) in the early through late 1980s. BBN introduced the concept of dead reckoning to efficiently transmit the state of battle field entities.\n\nIn the early 1990s, IST was contracted by the United States Defense Advanced Research Project Agency to undertake research in support of the US Army Simulator Network (SimNet) program. Funding and research interest for DIS standards development decreased following the proposal and promulgation of its successor, the High Level Architecture (simulation) (HLA) in 1996. HLA was produced by the merger of the DIS protocol with the Aggregate Level Simulation Protocol (ALSP) designed by MITRE.\n\nThere was a NATO standardisation agreement (STANAG 4482, \"Standardised Information Technology Protocols for Distributed Interactive Simulation (DIS)\", adopted in 1995) on DIS for modelling and simulation interoperability. This was retired in favour of HLA in 1998 and officially cancelled in 2010 by the NATO Standardization Agency (NSA).\n\nDIS is defined under IEEE Standard 1278:\n\n\nIn addition to the IEEE standards, the Simulation Interoperability Standards Organization (SISO) maintains and publishes an \"enumerations and bit encoded fields\" document yearly. This document is referenced by the IEEE standards and used by DIS, TENA and HLA federations. Both PDF and XML versions are available.\n\nSISO, a sponsor committee of the IEEE, promulgates improvements in DIS. Major changes occurred in the DIS 7 update to IEEE 1278.1 to make DIS more extensible, efficient and to support the simulation of more real world capabilities.\n\nSimulation state information is encoded in formatted messages, known as protocol data units (PDUs) and exchanged between hosts using existing transport layer protocols, including multicast, though broadcast User Datagram Protocol is also supported. There are several versions of the DIS application protocol, not only including the formal standards, but also drafts submitted during the standards balloting process.\n\n\nThe current version (DIS 7) defines 72 different PDU types, arranged into 13 families. Frequently used PDU types are listed below for each family. PDU and family names shown in \"italics\" are found in DIS 7.\n\n\nThe RPR FOM is a Federation Object Model (FOM) for the High-Level Architecture designed to organize the PDUs of DIS into an HLA object class and interaction class hierarchy. It has been developed as the SISO standard SISO-STD-001. The purpose is to support transition of legacy DIS systems to the HLA, to enhance a priori interoperability among RPR FOM users and to support newly developed federates with similar requirements. The most recent version is RPR FOM version 2.0 that corresponds to DIS version 6.\n\n\n"}
{"id": "9407796", "url": "https://en.wikipedia.org/wiki?curid=9407796", "title": "Ecoagriculture", "text": "Ecoagriculture\n\nEco friendly agriculture describes landscapes that support both agricultural production and biodiversity conservation, working in harmony together to improve the livelihoods of rural communities.\nWhile many rural communities have independently practiced eco-agriculture for thousands of years, over the past century many of these landscapes have given way to segregated land use patterns, with some areas employing intensive farming practices without regard to biodiversity impacts, and other areas fenced off completely for habitat or watershed protection. A new eco-agriculture movement is now gaining momentum to unite land managers and other stakeholders from diverse environments to find compatible ways to conserve biodiversity while also enhancing agricultural production.\n\nThe term \"eco-agriculture\" was coined by Charles Walters, economist, author, editor, publisher, and founder of \"Acres Magazine\" in 1970 to unify under one umbrella the concepts of \"ecological\" and \"economical\" in the belief that unless agriculture was ecological it could not be economical. This belief became the motto of the magazine: \"To be economical agriculture must be ecological.\"\n\nEco-agriculture is both a conservation strategy and a rural development strategy. Eco-agriculture recognizes agricultural producers and communities as key stewards of ecosystems and biodiversity and enables them to play those roles effectively. Eco-agriculture applies an integrated ecosystem approach to agricultural landscapes to address all three pillars—conserving biodiversity, enhancing agricultural production, and improving livelihoods—drawing on diverse elements of production and conservation management systems. Meeting the goals of eco-agriculture usually requires collaboration or coordination between diverse stakeholders who are collectively responsible for managing key components of a landscape.\n\nEco-agriculture uses the landscape as a unit of management. A landscape is a cluster of local ecosystems with a particular configuration of topography, vegetation, land use, and settlement. The goals of eco-agriculture—to maintain biodiversity and ecosystem services, manage agricultural production sustainably, and contribute to improved livelihoods among rural people—cannot be achieved at just a farm or plot level, but are linked at the landscape level. Therefore, to make an impact, all of the elements of a landscape as a whole must be considered; integrated landscape management is an approach that seeks to achieve this.\n\nDefining a landscape depends on the local context. Landscapes may be defined or delimited by natural, historical, and/or cultural processes, activities or values. Landscapes can incorporate many different features, but all of the various features have some influence or effect on each other. Landscapes can vary greatly in size, from the Congo Basin in west-central Africa where landscapes are often huge because there are vast stretches of apparently undifferentiated land, to western Europe where landscapes tend to be much smaller because of the wide diversity of topographies and land use activities occurring close to each other.\n\nAgriculture is the most dominant human influence on earth. Nearly one-third of the world’s land area is heavily influenced by cropland or planted pastures. An even greater area is being fallowed as part of an agricultural cycle or is in tree crops, livestock grazing systems, or production forestry. In addition, most of the world’s 100,000+ protected areas contain significant amounts of agricultural land. And over half of the most species-rich areas in the world contain large human populations whose livelihoods depend on farming, forestry, herding, or fisheries.\n\nAgriculture as it is often practiced today threatens wild plant and animal species and the natural ecosystem services upon which both humans and wildlife depend. Over 70% of the fresh water withdrawn by humans goes to irrigation for crops, causing a profound impact on the hydrological cycles of ecological systems. Moreover, fertilizers, pesticides, and agricultural waste threaten habitats and protected areas downstream. Landclearing for agriculture also disrupts sources of food and shelter for wild biodiversity, and unsustainable fishing practices deplete freshwater and coastal fisheries.\n\nAdditionally, an increase in the planting and marketing of monoculture crops across the globe has decreased diversity in agricultural products, to the extent that many local varieties of fruits, vegetables, and grains have now become extinct. Given that demands on global agricultural production are increasing, it is imperative that the management of agricultural landscapes be improved to both increase productivity and enhance biodiversity conservation. Wild biodiversity increasingly depends on agricultural producers to find ways to better protect habitats, and agriculture critically needs healthy and diverse ecosystems to sustain productivity.\n\nTraditionally there has existed a divide between conservationists, who want to set land aside for the protection of wild biodiversity, and agriculturalists, who want to use land for production. Because more than half of all plant and animal species exist principally outside protected areas –- mostly in agricultural landscapes –- there is a great need to close the gap between conservation efforts and agricultural production. For example, conservation of wetlands within agricultural landscapes is critical for wild bird populations. Such species require initiatives by and with farmers. Ecoagriculture provides a bridge for these two communities to come together.\n\nFarming communities play a vital role as managers of their ecosystems and biodiversity. As Ben Falk points out, they are often viewed as stewards. In his understanding, \"Stewardship implies dominion, whereas partnership implies co[-]evolution; mutual respect; whole-archy, not hierarchy. A partner is sometimes a guide, always a facilitator, always a co[-]worker.\" Since a farmer's dependence on their land and natural resources necessitates a conservation ethic, their farm productivity critically demands their assistance in delivering a range of ecosystem services. Wild species often also play an important role in providing livestock fodder, fuel, veterinary medicines, soil nutrient supplements and construction materials to farmers, as well constituting an essential element of cultural, religious, and spiritual practices. The dominance of agriculture in global land use requires that eco-agriculture approaches be fostered by rural producers and their communities on a globally significant scale. To do this, farmers need to be able to conserve biodiversity more consistently in ways that benefit their livelihoods. Experiences from around the world suggest that there are a number of incentives to encourage and enable farmers and their communities to preserve or transition towards eco-agriculture landscapes:\n\n\nAgricultural landscapes that aim to achieve the objectives of ecoagriculture –- enhanced biodiversity conservation, increased food production, and improved rural livelihoods –- should be managed in ways that protect and expand natural areas and improve wildlife habitats and ecosystem functions, in collaboration with local communities to insure their benefit. Specific land management practices that may be incorporated include:\n\nMany indigenous peoples and rural communities have developed, maintained, and adapted different types of ecoagriculture systems for centuries. Local farmers, pastoralists, fishers, forest users, and other community members are the foundation of rural land stewardship. Their knowledge, traditions, land use practices, and resource-management institutions are essential to the development of viable ecoagriculture systems for their landscapes.\n\nThe mainstreaming of ecoagriculture approaches will be crucially dependent upon mobilizing local communities to become leaders in ecoagriculture, as teachers and as advocates for political and institutional change. Communities facing similar challenges can share questions, ideas, and solutions with each other. Local communities also need effective processes for sharing their expertise with national policymakers and the international community and thus play a more central role in settinge coagriculture objectives in policy and program development.\n\nThe Millennium Development Goals (MDGs), eight ambitious targets which range from halving extreme poverty to halting the spread of HIV/AIDS and providing universal primary education, were put forth by the United Nations in 2000, to be achieved by 2015. Ecoagriculture strategies will be essential to achieving the MDGs, particularly for hunger and poverty, water and sanitation, and environmental sustainability.\nThe MDGs will not be reached without securing the ability of the rural poor to feed their families and gain income, while at the same time protecting the biodiversity and ecosystem services that sustain their livelihoods. Of the estimated 800 million people who do not have access to sufficient food, half are smallholder farmers, one-fifth are rural landless, and one-tenth are principally dependent on rangelands, forests and fisheries. For most of them, reducing poverty and hunger will depend centrally on their ability to sustain and increase crop, livestock, forest, and fishery production.\n\nA key opportunity for enhancing progress towards the MDGs is investment in locally-driven land management approaches –- such as ecoagriculture strategies –- that build upon synergies between rural livelihoods, environmental sustainability, and food security.\n\nThe ecoagriculture movement was first recognized internationally in a joint study of the World Conservation Union and the Future Harvest Foundation published in 2001 called “Common Ground, Common Future” (McNeely and Scherr 2001). The report was later expanded to become a book called “Ecoagriculture: Strategies to Feed the World and Save Wild Biodiversity” (McNeely and Scherr 2003). The study confirmed the dominant influence of agriculture on wild species and habitats around the world, and also identified promising examples of land use strategies and practices that benefited both. The international non-profit \"EcoAgriculture Partners\" was incorporated in 2004 to promote ecoagriculture globally, with Scherr as President and CEO and McNeely as one of the independent governing board members. Scherr and McNeely edited a second book in 2009, entitled \"Farming with Nature: The Science and Practice of Ecoagriculture\" (Scherr and McNeely 2009).\n\nThe values and/or principles of ecoagriculture have much in common with existing concepts, such as integrated landscape management, sustainable agriculture, permaculture, agroecology, integrated natural resource management, organic agriculture, agroforestry, conservation agriculture, protected area management, and others. In fact, ‘ecoagriculture’ landscapes often feature many of these approaches. Ecoagriculture draws heavily on these and many other innovations in rural land use planning and management. The landscape management framework defined by ecoagriculture has four particularly important characteristics:\n\n"}
{"id": "10787889", "url": "https://en.wikipedia.org/wiki?curid=10787889", "title": "Electronic pest control", "text": "Electronic pest control\n\nElectronic pest control is the name given to any of several types of electrically powered devices designed to repel or eliminate pests, usually rodents or insects. Since these devices are not regulated under the Federal Insecticide, Fungicide, and Rodenticide Act in the United States, the EPA does not require the same kind of efficacy testing that it does for chemical pesticides. Studies on ultrasound pest control devices have been described as ineffective, a waste of money and potentially harmful to users and their efforts to deter insects and prevent disease. \n\nUltrasonic devices operate through emitting short wavelength, high frequency sound waves that are too high in pitch to be heard by the human ear (generally accepted to be frequencies greater than 20,000 Hz). Humans are usually unable to hear sounds higher than 20 kHz due to physiological limitations of the cochlea, though there is considerable variation between individuals, especially at such high frequencies. Some animals, such as bats, dogs, and rodents, can hear well into the ultrasonic range. Some insects, such as grasshoppers and locusts, can detect frequencies from 50,000 Hz to 100,000 Hz, and lacewings and moths can detect ultrasound as high as 240,000 Hz produced by insect-hunting bats. Contrary to popular belief, birds cannot hear ultrasonic sound. Some smartphone applications attempt to use this technology to produce high frequency sounds to repel mosquitoes and other insects, but the claims of effectiveness of these applications and of ultrasonic control of pest creatures in general has been questioned. The ultrasonic repeller has several inconvenient side effects in addition to its questionable effectiveness.\n\nThe concept of radio wave (RW) or radio frequency (RF) to control the behavior of living organisms has shown promise. According to Drs. Juming Tang and Shaojin Wang at Washington State University (WSU) with colleagues at the University of California-Davis and USDA's Agricultural Research Service in Parlier, California, since RF energy generates heat through agitation of bound water molecules, it generates heat through ionic conduction and agitation of free water molecules in insects. As a result, more thermal energy is converted in insects.\n\nRF treatments control insect pests without negatively affecting food stuffs and storage locations. RF treatments may serve as a non-chemical alternative to chemical fumigants for post-harvest pest control in commodities (such as almonds, pecans, pistachios, lentils, peas, and soybeans), reducing the long-term impact on the environment, human health, and competitiveness of agricultural industries. \n\nIn 2003, the Federal Trade Commission required Global Instruments, the maker of the Pest-A-Cator/Riddex series of electromagnetic pest control devices, to discontinue any claims for their efficacy until they are backed by credible scientific evidence. This ban continues to be in effect.\n\nIn 2007 a Cochrane report reviewed by the Infectious Diseases Group determined that there was no evidence based on 10 field studies, in which ultrasonic repellent devices had been put to the test to suggest that EMRs had any repellent effect on mosquitoes, and therefore no evidence to support their promotion. They advised discontinuing further randomized controlled trials due to field studies showing no promise in the effort to combat malaria.\n\n\nBased on a review of tests of six commercial products, a report made at the University of Lincoln, Nebraska in 1995 concluded that all the devices, when evaluated at a range of frequencies and decibel levels, were insufficient in repelling rodents. The EPA pursued legal action against purveyors of the products, and none were subsequently marketed as a result of fines against the manufacturers.\nProfessor Tim Leighton at the Institute of Sound and Vibration Research, University of Southampton, U.K. produced an 83-page paper entitled \"What is Ultrasound?\" (2007), in which he expressed concern about the growth in commercial products which exploit the discomforting effects of in-air ultrasound (to pests for whom it is within their audible frequency range, or to humans for whom it is not, but who can experience unpleasant subjective effects and, potentially, shifts in the hearing threshold). Leighton claims that commercial products are often advertised with cited levels which cannot be critically accepted due to lack of accepted measurement standards for ultrasound in air, and little understanding of the mechanism by which they may represent a hazard.\n\nThe UK's independent Advisory Group on Non-ionising Radiation (AGNIR) produced a 180-page report on the health effects of human exposure to ultrasound and infrasound in 2010. The UK Health Protection Agency (HPA) published their report, which recommended an exposure limit for the general public to airborne ultrasound sound pressure levels (SPL) of 70 dB (at 20 kHz), and 100 dB (at 25 kHz and above).\n\n\n"}
{"id": "3375714", "url": "https://en.wikipedia.org/wiki?curid=3375714", "title": "Engineers for a Sustainable World", "text": "Engineers for a Sustainable World\n\nEngineers for a Sustainable World (ESW) is a not-for-profit network headquartered in Denver, CO. ESW is an umbrella organization with chapters established at 50 colleges, universities, and city chapters all located primarily in the United States. ESW members work on technical design projects that have a focus on sustainability and environmental issues. Projects can be located either on-campus, in the local community, or internationally. Chapters are made up of students, and are semi-autonomous.\n\nESW was known as Engineers Without Frontiers USA (EWF-USA) through 2004. ESW was established in 2001 in Ithaca, New York at Cornell University. ESW was based at Cornell from 2001 through August 30, 2007, when it moved its headquarters to the San Francisco Bay Area. In July 2011, ESW moved its headquarters to Merced, California at the University of California, Merced. In July 2013, the organization became an independent legal entity with its headquarters currently in Denver, CO.\n\nESW is managed by the National Leadership Team that consists entirely of volunteers. They include the Executive Director, Program Directors, Chapter Relations Director, Professional Relations Director, along with affiliated departments. Volunteers include current chapter members as well as graduated professionals. Since incorporation, the National Leadership Team is overseen by a Board of Directors.\n\nESW also has a Board of Directors with additional members from academia and corporations.\nOn its official website, ESW defines its vision as the following:\n\nESW defines its mission as:\n\nESW defines its goals as follows:\n\nIn support of the mission, ESW's primary goals are to:\nWhile earning an engineering master's degree at Cornell University, Regina Clewlow began developing the vision for Engineers Without Frontiers USA (EWF-USA) in early 2001. Working with her friend and mentor, Krishna Athreya, Regina began to develop the framework for EWF-USA's national organization. As a part of an MBA course at Cornell, she developed the business plan for EWF-USA and secured a partnership with a non-profit incubator based at Cornell called the Center for Transformative Action. EWF-USA was then officially established, with Regina Clewlow as its founding executive director.\n\nIn the spring of 2002, the first collegiate chapters were formed at Cornell and Pennsylvania State University. By December 2002, chapters had formed at other universities across the United States, including Stanford, Northwestern, Caltech, and UC-Berkeley. In March 2004, EWF-USA changed its name to Engineers for a Sustainable World following a dispute with Engineers Without Borders - USA over the similarity between the two names and to broaden its vision to include sustainability in development. In October 2006, the current world-in-gear logo was adopted. In 2007, the ESW national office relocated from Ithaca, New York, to the San Francisco Bay Area. In September 2008, Regina Clewlow stepped down as executive director to pursue a doctoral degree in engineering at MIT. She was replaced by Julie Chow.\n\nDuring Julie Chow's tenure, ESW underwent a period of significant organizational and programmatic restructuring. In 2009, ESW's vision and mission was revised and the national team structure was introduced. Also during this time, greater emphasis was placed on funding domestic sustainability projects. From 2009-2011, the number of active ESW collegiate chapters doubled and paid memberships increased by six-folds.\n\nTo further strengthen its ties to the engineering education community and to improve programming in the engineering education space, on July 1, 2011, ESW moved its physical and fiscal home to the University of California, Merced. Concurrent with ESW's headquarters move, Julie Chow stepped down as executive director. Dr. E. Daniel Hirleman, dean of the school of engineering at UC-Merced, served as acting Executive Director until Dr. Alexander Dale was appointed Executive Director on January 1, 2013. In June 2016, Dr. Dale stepped down as Executive Director. He was replaced by Brittany Bennett on July 1, 2016. He served as Treasurer on the Board of Directors until June 2018 when he was replaced by Shifali Chawla. Dr. Dale remains on the Board of Directors as an advisor for organization support and operation.\n\nSolar Canopy Charging Station\n\nStudent laptop and cell phone charging around campus is creating an increase in power consumption and cost. The campus purchased about 100 outside canopy tables for students and many of these are located in sunny areas. The project scope is to design an integral and affordable charging station that is solar powered, can be retrofitted to these campus canopy tables, and that can be mass production manufactured.\n\nLotus Project\n\nThe team targeted polluted river systems in areas where a mix of lack of infrastructure, factory pollution and lots of rainfall results in a substantial amount of human waste entering in the rivers, and eventually the oceans. They designed a series of free-standing filters to be placed within a river.\n\nApparatus X\n\nApparatus X is a disaster relief vehicle intended to help survivors of natural disasters rebuild their communities.\n\nWaste To Energy\n\nThe Waste to Energy Project's mission is to research, design and build a single prototype biodigester that can ultimately be scaled up in order to process dining hall food waste to produce usable methane gas to power utilities and facilities at UCSD.\n\nAutoswitch\n\nThe idea of the AutoSwitch is to save electricity in a household setting without the users having to think about it every time. The device will be a power strip that is using a micro controller, in our case an arduino yun, to control a relay switch built into the strip. This micro controller will be connected to the homes wifi and will be able to tell when a pair device connects or disconnects to the same wifi. If the device is connected then the controller will flip the relay switch, turning on the power strip and everything plugged into it. And when the device is not in range of the wifi (disconnected) then the micro controller will turn off the power strip saving energy.\n\nLow Cost Wind Turbine\n\nLow cost and simple sources of electrical power are needed in rural and remote areas, especially in third world countries. The project scope is to design, build, and test the minimal cost wind power generation system using common materials such as automotive components or common consumer components.\n\nCommUnity is a challenge that empowers interdisciplinary student teams to develop solutions to improve access to resources, quality of life, and climate-resistant infrastructure system. We will educate teams in community based learning, guiding them to work collaboratively with organizations to propose solutions to local resiliency challenges. The individual programs support ESW's \"Big Idea\" of resilient and sustainable communities. Students can put to use their technical skills while also learning throughout the process in an authentic and hands-on way, thereby gaining valuable experiences.\n\nThe initiative is made up of two programs, the \"Resilient CommUnity Design Challenge and GreatChina CommUnity Challenge\".\n\n2015-2016 CommUnity Winners\n\n\"Georgia Institute of Technology\"\n\nThe Georgia Tech team partnered with the Atlanta Community Food Bank and looked how the lack of access to clean water and fresh food affected low-income communities in Atlanta. The team then used their previous research into optimizing natural herbicide solutions to educate community gardeners on organic weed control.\n\n\"California State University – Long Beach\"\n\nThe CSULB team partnered with Long Beach Organic Inc., a nonprofit organization that provides organic community gardens for the Long Beach community. The team discussed several community issues with LBO, including transportation and accessibility to public space, but decided to focus on sustainable agriculture and monarch conservancy in the gardens.\n\n\"University of California – San Diego\"\n\nThe UCSD team partnered with the Global Action Research Center and Hoover High School in San Diego's City Heights area to focus on improving the connection between local youth and their community. The team is working to implement after-school programs focused on sustainability and STEM topics at the high school.\n\nBuild Day is a collaborative project design and build initiative that brings together student engineers, technical experts, and community leaders to create sustainable change in their local communities. Engineers will work alongside community partners and organizations to identify a locally pressing sustainability issue (e.g. food deserts, resilient infrastructure, clean water access, or disaster preparation) and design and implement innovative sustainable solutions. These solutions will improve a community's resilience to climate change and sudden shocks and stresses—especially for underserved and marginalized communities. Build Day provides student engineers the opportunity to work directly in the field and gain real-world design build experience.\n\n\"Solar Sprouts Project (Buffalo, NY)\"\n\nA team from the University of Buffalo partnered with the Centers Health Care - Buffalo Center, which provides a range of therapy and other medical services for both short and long-term residents. To support the Center's mission of growth and rehabilitation, the team designed and built a wheelchair-accessible sustainable solar tabletop garden with a rainwater capture system. This tabletop garden allowed recovering disabled residents to participate in therapeutic gardening activities who would have otherwise been unable to engage in any outdoor recreation.\n\n\"HOPES Project (Oakland, CA)\"\n\nA team from the University of California, Berkeley partnered with Hoover Elementary School in Oakland, CA where 80% of students are on the free/reduced lunch program. The project was named \"HOPES\" (Hoover Outreach Program for Environmental Sustainability). The team aimed to bring fresh and nutritious food and encourage outdoor gardening through building a chicken coop, strawberry patch, and sheet mulching composter at the school's Hoover Hawk Victory Garden. To supplement the existing gardening curriculum, students and their families will visit the garden and take part in weekly gardening classes further encouraging outdoor recreational activities.\n\n"}
{"id": "3160969", "url": "https://en.wikipedia.org/wiki?curid=3160969", "title": "Epson Robots", "text": "Epson Robots\n\nEPSON Robots is the robotics design and manufacturing department of Japanese corporation Seiko Epson, the brand-name watch and computer printer producer.\n\nEPSON manufactures Cartesian, SCARA and 6-axis industrial robots for factory automation. Cleanroom and ESD compliant models are available.\n\nThey offer PC-based controllers and integrated vision systems utilizing Epson's own vision processing technology.\n\nEPSON has a 30-year heritage and there are more than 30,000 EPSON robots installed in manufacturing industries around the world.\nEPSON uses a standardized PC-based controller for 6-axis robots, SCARA, and Linear Module needs. A move that simplifies support and reduces learning time.\n\nEpson offers four different lines of SCARA robots including the T-Series, G-Series, RS-Series, and LS-Series. The performance and features offered for each series of robot is determined by the intended purpose and needs of the robot. The T- Series robot is a high performance alternative to slide robots for pick-and-place operations. The G-Series offers a wide variety of robots in regards to the size, arm design, payload application, and more. The RS-Series offers two SCARA robots that are mounted from above and have the ability to move the second axis under the first axis. The LS-Series features several low cost and high performance robots that come in a variety of sizes.\n"}
{"id": "32457715", "url": "https://en.wikipedia.org/wiki?curid=32457715", "title": "Ettridge Collection", "text": "Ettridge Collection\n\nThe Ettridge Collection is a collection of trade literature relating to domestic appliances which was donated to the British Library by Ian Ettridge in 1997. Ettridge worked at domestic appliance manufacturers such as Electrolux, Hotpoint, Morphy Richards, Moulinex and Swan for over 45 years before his retirement.\n\nThe collection, held in the Library's Business & IP Centre, includes:\n"}
{"id": "37846", "url": "https://en.wikipedia.org/wiki?curid=37846", "title": "Gaseous fission reactor", "text": "Gaseous fission reactor\n\nA gas nuclear reactor (or gas fueled reactor or vapor core reactor) is a proposed kind of nuclear reactor in which the nuclear fuel would be in a gaseous state rather than liquid or solid. In this type of reactor, the only temperature-limiting materials would be the reactor walls. Conventional reactors have stricter limitations because the core would melt if the fuel temperature were to rise too high. It may also be possible to confine gaseous fission fuel magnetically, electrostatically or electrodynamically so that it would not touch (and melt) the reactor walls. A potential benefit of the gaseous reactor core concept is that instead of relying on the traditional Rankine or Brayton conversion cycles, it may be possible to extract electricity magnetohydrodynamically, or with simple direct electrostatic conversion of the charged particles.\n\nThe vapor core reactor (VCR), also called a gas core reactor (GCR), has been studied for some time. It would have a gas or vapor core composed of uranium tetrafluoride (UF) with some helium (He) added to increase the electrical conductivity, the vapor core may also have tiny UF droplets in it. It has both terrestrial and space based applications. Since the space concept doesn't necessarily have to be economical in the traditional sense, it allows the enrichment to exceed what would be acceptable for a terrestrial system. It also allows for a higher ratio of UF to helium, which in the terrestrial version would be kept just high enough to ensure criticality in order to increase the efficiency of direct conversion. The terrestrial version is designed for a vapor core inlet temperature of about 1,500 K and exit temperature of 2,500 K and a UF to helium ratio of around 20% to 60%. It is thought that the outlet temperature could be raised to that of the 8,000 K to 15,000 K range where the exhaust would be a fission-generated non-equilibrium electron gas, which would be of much more importance for a rocket design. A terrestrial version of the VCR's flow schematic can be found in reference 2 and in the summary of non-classical nuclear systems in the second external link. The space based concept would be cut off at the end of the MHD channel.\n\nHe may be used in increase the ability of the design to extract energy and be controlled. A few sentences from Anghaie et al. sheds light on the reasoning:\n\nThe spacecraft variant of the gaseous fission reactor is called the gas core reactor rocket. There are two approaches: the open and closed cycle. In the open cycle, the propellant, most likely hydrogen, is fed to the reactor, heated up by the nuclear reaction in the reactor, and exits out the other end. Unfortunately, the propellant will be contaminated by fuel and fission products, and although the problem can be mitigated by engineering the hydrodynamics within the reactor, it renders the rocket design completely unsuitable for use in atmosphere.\n\nOne might attempt to circumvent the problem by confining the fission fuel magnetically, in a manner similar to the fusion fuel in a tokamak. Unfortunately it is not likely that this arrangement will actually work to contain the fuel, since the ratio of ionization to particle momentum is not favourable. Whereas a tokamak would generally work to contain singly ionized deuterium or tritium with a mass of two or three daltons, the uranium vapour would be at most triply ionized with a mass of 235 dalton (unit). Since the force imparted by a magnetic field is proportional to the charge on the particle, and the acceleration is proportional to the force divided by the mass of the particle, the magnets required to contain uranium gas would be impractically large; most such designs have focused on fuel cycles that do not depend upon retaining the fuel in the reactor.\n\nIn the closed cycle, the reaction is entirely shielded from the propellant. The reaction is contained in a quartz vessel and the propellant merely flows outside of it, being heated in an indirect fashion. The closed cycle avoids contamination because the propellant can't enter the reactor itself, but the solution carries a significant penalty to the rocket's Isp.\n\nFor energy production purposes, one might use a container located inside a solenoid. The container is filled with gaseous uranium hexafluoride, where the uranium is enriched, to a level just short of criticality. Afterward, the uranium hexafluoride is compressed by external means, thus initiating a nuclear chain reaction and a great amount of heat, which in turn causes an expansion of the uranium hexafluoride. Since the UF is contained within the vessel, it can't escape and thus compresses elsewhere. The result is a plasma wave moving in the container, and the solenoid converts some of its energy into electricity at an efficiency level of about 20%. In addition, the container must be cooled, and one can extract energy from the coolant by passing it through a heat exchanger and turbine system as in an ordinary thermal power plant.\n\nHowever, there are enormous problems with corrosion during this arrangement, as the uranium hexafluoride is chemically very reactive.\n\n\n"}
{"id": "9614456", "url": "https://en.wikipedia.org/wiki?curid=9614456", "title": "Gladstone bag", "text": "Gladstone bag\n\nA Gladstone bag is a small portmanteau suitcase built over a rigid frame which could separate into two equal sections. Unlike a suitcase, a Gladstone bag is \"deeper in proportion to its length.\" Gladstones are typically made of stiff leather and often belted with lanyards. The bags are named after William Ewart Gladstone (1809–1898), the four-time Prime Minister of the United Kingdom.\n\nHinged luggage was first developed in the mid 19th century. One of the first recorded official documentations of the Gladstone bag is a British Patent registered by Edward Cole of Hemmings Row in the city of Westminster. Edward Cole was a Leather Case Maker based at No. 9 Hemmings Row. City of Westminster.\nThe Patent for \"An Improvement In The Frames Of Traveling Bags\" was registered by Edward Cole on 4 February 1854 and sealed 14 July 1854. This original patent is still held by Cole Brothers of England in their archive. The business of Edward Cole was taken over and ran by two of his sons James and Edward at the end of the 19th Century and subsequently changed to Cole Brothers in 1907, being located at 24a Floral Street, Covent Garden after the earlier demolition of the Hemmings Row site in 1886 to make way for the extension to the National Gallery.\n"}
{"id": "3616959", "url": "https://en.wikipedia.org/wiki?curid=3616959", "title": "Gonioreflectometer", "text": "Gonioreflectometer\n\nA gonioreflectometer is a device for measuring a bidirectional reflectance distribution function (BRDF).\n\nThe device consists of a light source illuminating the material to be measured and a sensor that captures light reflected from that material. The light source should be able to illuminate and the sensor should be able to capture data from a hemisphere around the target. The hemispherical rotation dimensions of the sensor and light source are the four dimensions of the BRDF. The 'gonio' part of the word refers to the device's ability to measure at different angles.\n\nSeveral similar devices have been built and used to capture data for similar functions. Most of these devices use a camera instead of the light intensity-measuring sensor to capture a two-dimensional sample of the target. Examples include:\n\n"}
{"id": "53787469", "url": "https://en.wikipedia.org/wiki?curid=53787469", "title": "Grace Banu", "text": "Grace Banu\n\nGrace Banu is a Dalit and transgender activist. An Electrical and Electronics Engineering (EEE) student, she is the first transgender person to be admitted to an engineering college in the state of Tamil Nadu. As of 2014, she is studying at Sri Krishna College of Engineering.\n\nBanu was born and raised in Tuticorin district, Tamil Nadu. A Dalit, she says that from early in her schooldays she was not allowed to attend the regular hours of 9.30 am to 4 pm. \n\nShe was told that in order to attend school she had to agree to come in to school at 10 am, after all the other students were in and settled, and leave at 3.30 pm before others finished.\n\nOther students were told that they would be punished if they interacted with her. This kind of untouchability, based on both her caste and gender identity, caused her to attempt suicide and give up on the idea of finishing school.\n\nBanu's family rejected her in 2008 when she told them of her gender identity.\n\nDespite financial difficulties and discrimination from classmates and teachers, Banu undertook a Diploma in Computer Engineering. \n\nShe was the first transgender person to be admitted to an engineering college in the state of Tamil Nadu. Banu struggled financially to remain in college, in part because she was not receiving any support from her family at the time. Responding to a call for help, a local businessman launched an online campaign raising funds for her to complete the course.\n\nAfter completing her Diploma with honours (95%), Banu was selected to work for a software firm when she had excelled at a campus interview. She worked as a programmer until she quit due to alleged discrimination.\n\nShe filed a Right to Information (RTI) to find out if Anna University accepted transgender students. On finding out that they did not, she applied against their rules anyway and was given admission to a private affiliated college, Sri Krishna College of Engineering.\n\nBanu believes that ultimately Reservation, dedicated places for members of different groups, is key to the uplift of transgender people. \"No amount of temporary governmental and non-governmental schemes can have the transgenerational impact that reservations can have. Reservations are the only way,\" she says. She has been advocating for Dalit and transgender rights, demanding along with other transgender people for reservation based on gender identity as well as caste.\n\nBanu insists that the intersectionality of these oppressions matter. She believes that Dalits can be transphobic and that the transgender community replicates structures of caste privilege. She says that upper-caste transgender people bring Brahminism into transgender cultural, community and organising spaces. Despite their oppressions, upper-caste transgender women dominate all the positions of leadership, call the shots and define the needs for the whole community.\" Denying caste in the transgender community is like \"hiding a whole pumpkin in a plate of rice,\" she says.\n\nBanu was active in voicing concerns and questioning the death of a fellow transwoman named Tara (Thara), who burned to death in Chennai.\n\n"}
{"id": "8611776", "url": "https://en.wikipedia.org/wiki?curid=8611776", "title": "Green Man (PGI)", "text": "Green Man (PGI)\n\nThe \"Green Man\" is a figure associated with the Pyrotechnics Guild International (PGI). He appears on the Guild's emblem and was selected from John Bate's 1635 fireworks treatise, \"The Second Booke\" , to symbolize the long tradition of using fireworks as an essential part of festivals and celebrations - at that time led by so-called \"Green Men\" appointed to head processions with a \"Fire Club\" shooting sparks. They were called \"Green Men\" because they clothed themselves with fresh leaves to protect themselves from sparks produced by their hand-held fireworks. This gave rise to the traditional salutation 'Stay Green' amongst those involved with fireworks.\n\n\n"}
{"id": "29474319", "url": "https://en.wikipedia.org/wiki?curid=29474319", "title": "Heated towel rail", "text": "Heated towel rail\n\nA towel dryer or a heated towel rail is a feature designed to heat towels before using them. For many years, European hotels have used them as combined towel- dryers/racks. \n\nTowel dryers can be made from different metals such as steel, stainless steel, or aluminum. In some types, brass or copper is used. The finish can be in chrome plating, polished steel, or lacquer.\n\nDryers can be heated electrically (heating cartridge or heating cable), or by circulating hot water (connected to the central heating). Often, a combination of the methods is used. In these cases, the dryer is heated by hot water in wintertime, and by electricity in the summer. A towel dryer, with high output, can also serve as a radiator in a small bathroom.\n\nDryers come in a variety of appearances, including: ladder types, turnable types, and others.\n"}
{"id": "4688909", "url": "https://en.wikipedia.org/wiki?curid=4688909", "title": "High dynamic range", "text": "High dynamic range\n\nHigh dynamic range (HDR) is a dynamic range higher than what is considered to be standard dynamic range. The term is often used in discussing display devices, photography, 3D rendering, and sound recording including digital imaging and digital audio production. The term may apply to an analog or digitized signal, or to the means of recording, processing, and reproducing such signals.\n\nHigh-dynamic-range imaging (HDRI) is the compositing and tone-mapping of images to extend the dynamic range beyond the native capability of the capturing device.\n\nHigh-dynamic-range video (HDR video) is greater than standard dynamic range (SDR) video which uses a conventional gamma curve.\n\nHigh-dynamic-range rendering (HDRR) is the real-time rendering and display of virtual environments using a dynamic range of 65,535:1 or higher (used in computer, gaming, and entertainment technology).\n\nOn January 4, 2016, the Ultra HD Alliance announced their certification requirements for a HDR display. The HDR display must have either a peak brightness of over 1000 cd/m and a black level less than 0.05 cd/m (a contrast ratio of at least 20,000:1) or a peak brightness of over 540 cd/m and a black level less than 0.0005 cd/m (a contrast ratio of at least 1,080,000:1). The two options allow for different types of HDR displays such as LCD and OLED.\n\nHDR transfer functions that better match the human visual system than a conventional gamma curve include the Hybrid Log-Gamma (HLG) and Perceptual Quantizer (PQ). HLG and PQ require a bit depth of 10-bits per sample.\n\nXDR (audio) is used to provide higher-quality audio when using microphone sound systems or recording onto cassette tapes.\n\nHDR Audio is a dynamic mixing technique used in EA Digital Illusions CE Frostbite Engine to allow relatively louder sounds to drown out softer sounds.\n\nDynamic range compression is a set of techniques used in audio recording and communication to put high-dynamic-range material through channels or media of lower dynamic range. Optionally, dynamic range expansion is used to restore the original high dynamic range on playback.\n\nIn radio, high dynamic range is important especially when there are potentially interfering signals. Measures such as spurious-free dynamic range are used to quantify the dynamic range of various system components such as frequency synthesizers. HDR concepts are important in both conventional and software-defined radio design.\n\nIn many fields, instruments need to have a very high dynamic range. For example, in seismology, HDR accelerometers are needed, as in the ICEARRAY instruments.\n\nIn the 1970s and 1980s, Steve Mann invented the Generation-1 and\nGeneration-2 \"Digital Eye Glass\", as a vision aid to help people\nsee better, with some versions being built into welding helmets for HDR vision\n\nSee also, IEEE Technology & Society 31(3) and the supplemental material entitled \"GlassEyes\".\n"}
{"id": "525028", "url": "https://en.wikipedia.org/wiki?curid=525028", "title": "High tech", "text": "High tech\n\nHigh technology, often abbreviated to high tech (adjective forms high-technology, high-tech or hi-tech) is technology that is at the cutting edge: the most advanced technology available. The opposite of high tech is \"low technology\", referring to simple, often traditional or mechanical technology; for example, a slide rule is a low-tech calculating device.\n\nThe phrase was used in a 1958 \"The New York Times\" story advocating \"atomic energy\" for Europe: \"... Western Europe, with its dense population and its high technology ...\" Robert Metz used the term in a financial column in 1969: \"Arthur H. Collins of Collins Radio] controls a score of high technology patents in variety of fields.\" and in a 1971 article used the abbreviated form, \"high tech.\"\n\nA widely-used classification of high-technological manufacturing industries is provided by the OECD. It is based on the intensity of research and development activities used in these industries within OECD coutries, resulting in four distinct categories.\n\n"}
{"id": "30604849", "url": "https://en.wikipedia.org/wiki?curid=30604849", "title": "Information Systems Journal", "text": "Information Systems Journal\n\nThe Information Systems Journal is a peer-reviewed scientific journal that publishes papers on any aspect of information systems, but has particular emphasis on the relationship between information systems and people, business, and organisations. The journal was established in 1991 by David Avison and Guy Fitzgerald with the name \"Journal of Information Systems\". The name was changed to the \"Information Systems Journal\" in 1994. The current editors-in-chief are Robert M Davison, Philip Powell, and Eileen Trauth.\n\nThe journal is abstracted and indexed in the Science Citation Index, ProQuest, CSA Computer Abstracts, Current Contents/Social & Behavioral Sciences, EBSCO Databases, InfoTrac, Inspec, Psychological Abstracts/PsycINFO, Scopus, and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 1.333, ranking it 24th among 83 journals in the category of \"Information Science & Library Science\".\n"}
{"id": "53117514", "url": "https://en.wikipedia.org/wiki?curid=53117514", "title": "Intensive farming in Almería", "text": "Intensive farming in Almería\n\nThe intensive agriculture of the province of Almeria, Spain, is a model of agricultural exploitation of high technical and economic yield based on the rational use of water, sanding, use of plastic greenhouses, high technical training and high level of employment of Inputs, on the peculiar characteristics of the environment.\n\nAlthough this area of Spain is sometimes known as the “market garden” of Europe, it is responsible for the hidden practice of the slave labour of thousands of migrant African migrant workers.\n\nAnother unknown fact is that for many years the discarded plastic sheeting from the hundreds of square kilometres of greenhouses has been dumped on adjacent land. This plastic slowly breaks down into minute particles and ends up in the Mediterranean where it pollutes the waters and aquatic life and ironically gets into the human food chain.\n\nThe first greenhouse was built in 1963 and the technique was extended by the \"Campo de Dalías\" or \"Poniente Almeriense\" and later by the \"Campo de Níjar\", in the east. The use of polyethylene as a substitute for glass had already been tested in the Canary Islands and Catalonia before. The plastic was spread over wooden posts or metal structures and secured by wire. The transparent plastic intensifies the heat and maintains the humidity. This allows harvests to be harvested one month earlier than in the open field and more ahead than in other regions, starting harvesting in December and allowing the plant growth of the autumn-winter plantings until March, doubling and sometimes tripling the number of harvests.\n\nIn February 2010 a new certification regulation of the N brand of AENOR for fruits and vegetables for fresh consumption came into force. This regulation describes the control system of the ISO 155 standard. This mark guarantees to customers that the products comply with quality protocols that include good agricultural practices, respect for the environment, traceability and social measures. The fulfillment of the norm covers almost all the requirements that the great European distribution demands to the producers of fruits and vegetables. These standards are homologated with the GLOBALGAP protocol.\n\nAccording to data from EXTENDA (Andalusian Agency for Foreign Promotion), the value of exports of fruit and vegetables in 2012 amounted to 1,914.1 million euros, a growth of 9.7% compared to 2011. Fresh vegetables and vegetables contributed 1,665.5 million. There were 359 exporting companies, 222 regular. These sales accounted for 47.3% of the total of the autonomous community. Among the client countries are Germany, 29.7% of the total, France, 15%, the Netherlands, 13.1%, the United Kingdom, 11.3%, and Italy, 7.2%. They are followed by Poland, Belgium, Sweden, Denmark and Portugal. According to the same source, in the first six months of 2013 sales totaled 1,600 million euros, 14.6% more than the previous year.\n\nBetween January and October 2013, the province exported more than 12.8 million kilos of live plants and cut flowers, 18.4% more than in the same period of 2012. The turnover amounted to almost 18.7 million euros, 56% more. Exports of ornamental plants accounted for more than 17.8 million euros, an increase of 59% over the same period in 2012. The main buyers are France, with 59.6% of the plants, Germany, with a 14.2%, and the Netherlands, with 10.6%. They are followed by Belgium, Portugal, Italy, United Kingdom, United States and Morocco.\n\nThe specialization of farmers by a single product is increasingly observed, as is the concentration of gender marketing in a few large firms. The largest companies such as Agroponiente, Unica Group, CASI, Alhóndiga La Unión, Agroiris and Vicasol account for 35% of the market share in 2015.\n\nIn agriculture, integrated pest management (IPM) or integrated pest control (IPC) is understood as a strategy that uses a variety of complementary methods: physical, mechanical, chemical, biological, genetic, legal and cultural for control of pests. These methods are applied in three stages: prevention, observation and application. It is an ecological method that aims to reduce or eliminate the use of pesticides and minimize the impact on the environment. There is also talk about ecological pest management (EPM) and natural pest management.\n\nUp to 2015, 60% of the area devoted to horticultural crops in the province used biological pest control techniques. The percentages are higher in some fundamental crops like pepper, 100%, and tomato, 85%. In all, some 26600 hectares of protected horticulture use these techniques, when in 2006 they were only used in about 129.\n\nThe Regional Ministry of Agriculture and Fisheries of the Andalusian Government is launching a plan (Compromiso Verde or Green Commitment) in 2016 to expand the area to 100%. For the Regional Government, this is the model that should extend the distances with traditional crops and leave a definitive patent that Almeria produces with more quality, more traceability and more food security than any:\n\n...guarantees the quality and improves the positioning of our products in international markets, increases the profitability of farms, enhances respect for the environment and minimizes the presence of insect vectors of viruses and favors the correct management of pests.\" (Carmen Ortiz Rivas, Minister of Agriculture, Fisheries and Rural Development.)\n\nAnalyzes of horticultural products indicate that only 0,6% of the samples show pesticide residues, when the European average is 2,8% (five times more).\nPlastic waste from the greenhouses is reported to run off into the mediterranean.\n\n"}
{"id": "1159382", "url": "https://en.wikipedia.org/wiki?curid=1159382", "title": "Jackass Flats", "text": "Jackass Flats\n\nJackass Flats is a shallow alluvial basin located in the southwest portion of the Nevada National Security Site in Nye County, Nevada. The area lies east of Yucca Mountain, south of the Calico Hills and Shoshone Mountain and northwest of Skull Mountain. The valley drains to the southwest via the Tonopah and Fortymile washes into the Amargosa Valley across US Route 95 at Amargosa Valley. The \"flat\" covers an area of approximately and ranges in elevation from about just north of US 95 to at the mountain bases to the north and east.\n\nIt is located mostly within Area 25 and extends into Area 14 and Area 26.\n\nIt was the test site of nuclear-powered rocket engines during project Project Rover and NERVA.\n"}
{"id": "8097563", "url": "https://en.wikipedia.org/wiki?curid=8097563", "title": "LRK", "text": "LRK\n\nLong Range Kinematic (LRK) technology is a sophisticated kinematic method developed by Magellan (formerly Thales) Navigation that optimises the advantages of dual-frequency GPS operation. Other conventional methods use the dual-frequency only during initialisation. LRK makes solving ambiguities during initialisation easy and continuous dual-frequency kinematic operation possible at distances up to 40 kilometres.\n\nConventional dual-frequency kinematic operation is limited to about 10 kilometres, using a combined observation on GPS L1 and L2 frequencies to produce an initial wide lane solution, ambiguous to around 86 centimetres. During a second phase, the conventional kinematic method uses measurements from the L1 frequency only. This method only allows for kinematic operation as long as the de-correlation of atmospheric errors is compatible with a pure phase single-frequency solution.\n\nSimilar to the KART process, LRK is a simple and reliable method that allows any initialisation mode, from a static or fixed reference point, to On The Fly ambiguity resolution, when performing dual-frequency GPS positioning. LRK technology reduces initialisation times to a few seconds by efficiently using L2 measurements in every mode of operation. LRK maintains optimal real-time positioning accuracy to within a centimetre at a range up to 40-50 kilometres, even with a reduced number of visible satellites.\n\n"}
{"id": "2124436", "url": "https://en.wikipedia.org/wiki?curid=2124436", "title": "Local oscillator", "text": "Local oscillator\n\nIn electronics, a local oscillator (LO) is an electronic oscillator used with a mixer to change the frequency of a signal. This frequency conversion process, also called heterodyning, produces the sum and difference frequencies from the frequency of the local oscillator and frequency of the input signal. Processing a signal at a fixed frequency gives a radio receiver improved performance.\nIn many receivers, the function of local oscillator and mixer is combined in one stage called a \"converter\" - this reduces the space, cost, and power consumption by combining both functions into one active device. \n\nLocal oscillators are used in the superheterodyne receiver, the most common type of radio receiver circuit. They are also used in many other communications circuits such as modems, cable television set top boxes, frequency division multiplexing systems used in telephone trunklines, microwave relay systems, telemetry systems, atomic clocks, radio telescopes, and military electronic countermeasure (antijamming) systems.\nIn satellite television reception, the microwave frequencies used from the satellite down to the receiving antenna are converted to lower frequencies by a local oscillator and mixer mounted at the antenna. This allows the received signals to be sent over a length of cable that would otherwise have unacceptable signal loss at the original reception frequency. In this application, the local oscillator is of a fixed frequency and the down-converted signal frequency is variable. \n\nApplication of local oscillators in a receiver design requires care to ensure no spurious signals are radiated. Such signals can cause interference in the operation of other receivers.\nThe performance of a signal processing system depends on the characteristics of the local oscillator. The local oscillator must produce a stable frequency with low harmonics. Stability must take into account temperature, voltage, and mechanical drift as factors. The oscillator must produce enough output power to effectively drive subsequent stages of circuitry, such as mixers or frequency multipliers. It must have low phase noise where the timing of the signal is critical. In a channelized receiver system, the precision of tuning of the frequency synthesizer must be compatible with the channel spacing of the desired signals. \n\nA crystal oscillator is one common type of local oscillator that provides good stability and performance at relatively low cost, but its frequency is fixed, so changing frequencies requires changing the crystal. Tuning to different frequencies requires a variable-frequency oscillator which leads to a compromise between stability and tunability. With the advent of high-speed digital microelectronics, modern systems can use frequency synthesizers to obtain a stable tunable local oscillator, but care must still be taken to maintain adequate noise characteristics in the result.\n\nDetection of local oscillator radiation may disclose the presence of the receiver, such as in detection of automotive radar detectors, or detection of unlicensed television broadcast receivers in some countries. \n\n"}
{"id": "24453700", "url": "https://en.wikipedia.org/wiki?curid=24453700", "title": "Mackworth Clock", "text": "Mackworth Clock\n\nThe Mackworth Clock is an experimental device used in the field of experimental psychology to study the effects of long term vigilance on the detection of signals. It was originally created by Norman Mackworth as an experimental simulation of long term monitoring by radar operators in the British Air Force during World War II. The device has a large black pointer in a large circular background like a clock. The pointer moves in short jumps like the second hand of an analog clock, approximately every second. At infrequent and irregular intervals, the hand makes a double jump, e.g. 12 times every 30 seconds. The task is to detect when the double jumps occur by pressing a button. Typically, Mackworth's participants would do this task for two hours. The Mackworth Clock was used to establish one of the fundamental findings in the vigilance and sustained attention literature: the vigilance decrement, that is, signal detection accuracy decreases notably after 30 minutes on task. The test continues to be used today in vigilance research in various forms, including computer-displayed versions.\n"}
{"id": "1922978", "url": "https://en.wikipedia.org/wiki?curid=1922978", "title": "Meat industry", "text": "Meat industry\n\nThe term meat industry describes modern industrialized livestock agriculture for production, packing, preservation and marketing of meat (in contrast to dairy products, wool, etc.). In economics, it is a fusion of primary (agriculture) and secondary (industry) activity and hard to characterize strictly in terms of either one alone. The greater part of the entire meat industry is termed meat packing industry- the segment that handles the slaughtering, processing, packaging, and distribution of animals such as cattle, pigs, sheep and other livestock.\n\nA great portion of the ever-growing meat branch in the food industry involves intensive animal farming in which livestock are kept almost entirely indoors or in restricted outdoor settings like pens.\n\nMany aspects of the raising of animals for meat have become industrialized, even many practices more associated with smaller family farms, e.g. gourmet foods such as foie gras.\n\nThe production of livestock is a heavily vertically integrated industry where the majority of supply chain stages are integrated and owned by one company.\n\nThe livestock industry not only uses more land than any other human activity; it's also one of the largest contributors to water pollution and a huge source of greenhouse gas emissions. In this respect, a relevant factor is the produced species' feed conversion efficiency. Additionally taking into account other factors like use of energy, pesticides, land, and nonrenewable resources, beef, lamb, goat, and bison as resources of red meat show the worst efficiency; poultry and eggs come out best. \n\nAmong the largest meat producers worldwide are:\n\nCritical aspects of the effects of industrial meat production include\nMany observers suggest that the expense of dealing with the above are grossly undercounted in present economic metrics and that true/full cost accounting would drastically raise the price of industrial meat.\n\nCultured meat (aka \"clean meat\") potentially offers some advantages in terms of efficiency of resource use and animal welfare. It is, however, still at an early stage of development and its advantages are still contested.\n\nIncreasing health care costs for an aging baby boom population suffering from obesity and other food-related diseases, concerns about obesity in children have spurred new ideas about healthy nutrition with less emphasis on meat. \n\nNative wild species like deer and bison in North America would be cheaper and potentially have less impact on the environment. The combination of more wild game meat options and higher costs for natural capital affected by the meat industry could be a building block towards a more sustainable livestock agriculture.\nA growing trend towards vegetarian or vegan diets and the Slow Food movement are indicators of a changing consumer conscience in western countries. Producers on the other hand have reacted to consumer concerns by slowly shifting towards ecological or organic farming.\n\n"}
{"id": "36335648", "url": "https://en.wikipedia.org/wiki?curid=36335648", "title": "Melville Walker House", "text": "Melville Walker House\n\nThe Melville Walker House is an inn on Maine Street in the Kennebunkport Historic District in Kennebunkport, Maine. The inn was added to the National Register of Historic Places on May 6, 1976. It is now known as the Maine Stay Inn.\n\nThe Melville Walker House was built as a private residence in 1860 by merchant sea captain Melville Walker on land provided to him by his father, William H. Walker. Captain Walker transferred the title to his wife, Abbie, following their marriage.\n\nThe property then passed to brother-in-law, Hiram Fairfield on October 4, 1876, who in turn left it to his wife, Adelaide, (Melville's sister) and their son, Harry. 31 years later in 1891, the property passed from the founding family for the first time when Adelaide sold it to the Heuvelman family. The Heuvelmans held the property until 1899, at which time they sold it to George Little, an executive with the New York, New Haven & Hartford Railroad. The Society's records show that electric lights were installed in 1905 at the Little's on Maine Street, then known as “The Maples.”\n\nTitle to the house and property then passed to Senator Wickes of New York in 1924. The next owners, the Eldridge family, gave the property its current name “The Maine Stay” and opened their guest house. The first cottage was added to the property in 1954. The inn remained under the Eldridge ownership until 1970 when it was sold to the Taylor-Milligan family. They sold The Maine Stay to Max and Jane Andrews in 1976. Jacques and Carol Gagnon purchased the house in 1983 and turned it into a bed and breakfast. In April 1989, title passed to Carol and Lindsay Copeland who sold it to George and Janice Yankowski in 2002. On April 30, 2008 Janice and George sold the inn to Judi and Walter Hauer.\n\nThe design of the main house is considered to be square block Italianate architecture, contoured in a low-hip roof design, with additions and renovations added over the years. In the early 1900s, Queen Anne style architecture was introduced to the inn with construction of the suspended spiral partially flying staircase, starburst crystal glass windows, ornately carved mantels and moldings, and additions of bay windows and the wrap-around porch.\n"}
{"id": "25382878", "url": "https://en.wikipedia.org/wiki?curid=25382878", "title": "Michael Gregg", "text": "Michael Gregg\n\nMichael Gregg is an American computer security specialist, noted speaker at security related events, and an author/coauthor of multiple books such as \"Build Your Own Network Security Lab\" and \"Inside Network Security Assessment\". Gregg has served as an expert witness before congressional committee on cyber security and identity theft.\n\nGregg holds two associate degrees, a bachelor's degree, and a master's degree. Gregg’s certifications include CISSP and CISA. Gregg has been quoted in newsprint and featured on various television and radio shows including, NPR, The New York Times, ABC, CBS, Fox TV and others discussing cybersecurity and ethical hacking. He is the lead faculty member for Villanova University's online Cyber Security program.\nPresident of Superior Solutions, Inc., a Houston-based security assessment and training firm, Mr. Gregg has more than 20 years of experience in the IT field. He has earned more than 20 certifications, including CISSP®, MCSE, CCNA®, CTT+™, CIW™ Security Analyst, CEH™, NSA IAM, SCNP™ and TICSA. Mr. Gregg has consulted and taught for many Fortune 500 companies. Although consulting consumes the bulk of his time, he also writes for several publications and is the author of “CISSP® Exam Cram 2” and creator of the Assessing IT Infrastructure Vulnerabilities training class.\n\nGregg has contributed to the following published works:\n\n\nGregg has written articles for print and Internet publications such as:\n\n"}
{"id": "14257234", "url": "https://en.wikipedia.org/wiki?curid=14257234", "title": "Ministry of Science and Technology (China)", "text": "Ministry of Science and Technology (China)\n\nThe Ministry of Science and Technology (MOST) of the People's Republic of China, formerly the State Science and Technology Commission, is the central government ministry which coordinates science and technology activities in the country.\n\nIt succeeded the State Science and Technology Commission in 1998.\n\n"}
{"id": "12222140", "url": "https://en.wikipedia.org/wiki?curid=12222140", "title": "Molecular spring", "text": "Molecular spring\n\nA Molecular spring is a device or part of a biological system based on molecular mechanics and is associated with molecular vibration.\n\nAny molecule can be deformed in several ways - A-A bond length, A-A-A angle, A-A-A-A torsion angle.\nDeformed molecule store energy, which can be released and cause mechanical work as the molecule return into its optimal geometrical conformation.\n\nThe term molecular string is usually used in nano-science and molecular biology, however theoretically also macroscopic molecular springs can be considered, if it is manufactured. Such a device composed for example of arranged ultra-high molecular mass polymer fibres (Helicene, Polyacetylene) could store extraordinary (0.1-10MJ/kg in comparison to 0.0003MJ/kg of clockwork spring) amount of energy which can be stored and released almost instantly, with high energy conversion efficiency. The amount of energy storable in molecular spring is limited by the value of deformation the molecule can withstand until it undergoes chemical change. Manufacturing of such macroscopic device is however out of reach of contemporary technology, because of difficulties of synthesis and molecular arrangement of such long polymer molecules. In addition, the force needed to draw molecular string to its maximum length could be impractically high - comparable to the tensile strength of particular polymer molecule (~100GPa for some carbon compounds)\n\n\n"}
{"id": "50049257", "url": "https://en.wikipedia.org/wiki?curid=50049257", "title": "Nanotronics Imaging", "text": "Nanotronics Imaging\n\nNanotronics Imaging is a nanotechnology startup in Cuyahoga Falls, Ohio. It has an office in Brooklyn, New York at New Lab and manufactures its devices in California.\n\nNanotronics was founded by Matthew Putman, a materials science professor at Columbia University, and his father John Putman. Matthew Putman was inspired to start the company after coming down with esophageal cancer at age 31. Nanotronics has received $7 million in venture capital funding from Founders Fund and PayPal founder Peter Thiel, who sits on the company's board of directors.\n\nIn 2015, Nanotronics acquired Franklin Mechanical & Control, a manufacturer of optical equipment based in Hollister, California.\n\nNanotronics builds hardware and software that can be used to see features down to the nanometer scale. It integrates off-the-shelf high resolution microscopes with custom software, including machine learning and artificial intelligence. Nanotronics technology has a number of medical applications, such as screening for cervical cancer.\n\nIn April 2015, Nanotronics announced a new virtual reality system, called nVisible, that allows anyone to \"walk through\" a 3D model of objects at the microscopic scale.\n\n"}
{"id": "3216683", "url": "https://en.wikipedia.org/wiki?curid=3216683", "title": "Open music model", "text": "Open music model\n\nThe open music model is an economic and technological framework for the recording industry based on research conducted at the Massachusetts Institute of Technology. It predicts that the playback of prerecorded music will be regarded as a service rather than as individually sold products, and that the only system for the digital distribution of music that will be viable against piracy is a subscription-based system supporting file sharing and free of digital rights management. The research also indicated that US$9 per month for unlimited use would be the market clearing price at that time, but recommended $5 per month as the long-term optimal price.\n\nSince its creation in 2002, a number of its principles have been adopted throughout the recording industry, and it has been cited as the basis for the business model of many music subscription services.\n\nThe model asserts that there are five necessary requirements for a viable commercial music digital distribution network:\nThe model was proposed by Shuman Ghosemajumder in his 2002 research paper \"Advanced Peer-Based Technology Business Models\" at the MIT Sloan School of Management. The following year, it was publicly referred to as the Open Music Model.\n\nThe model suggests changing the way consumers interact with the digital property market: rather than being seen as a good to be purchased from online vendor, music would be treated as a service being provided by the industry, with firms based on the model serving as intermediaries between the music industry and its consumers. The model proposed giving consumers unlimited access to music for the price of US$5 per month (as of 2002), based on research showing that this could be a long-term optimal price, expected to bring in a total revenue of over US$3 billion per year.\n\nThe research demonstrated the demand for third-party file sharing programs. Insofar as the interest for a particular piece of digital property is high, and the risk of acquiring the good via illegitimate means is low, people will naturally flock towards third-party services such as Napster and Morpheus (more recently, Bittorrent and The Pirate Bay).\n\nThe research showed that consumers would use file sharing services not primarily due to cost but because of convenience, indicating that services which provided access to the most music would be the most successful.\n\nThe model predicted the failure of online music distribution systems based on digital rights management.\n\nCriticisms of the model included that it would not eliminate the issue of piracy. Others countered that it was in fact the most viable solution to piracy, since piracy was \"inevitable\". Supporters argued that it offered a superior alternative to the current law-enforcement based methods used by the recording industry. One startup in Germany, Playment, announced plans to adapt the entire model to a commercial setting as the basis for its business model.\n\nSeveral aspects of the model have been adopted by the recording industry and its partners over time:\n\n\n"}
{"id": "26555841", "url": "https://en.wikipedia.org/wiki?curid=26555841", "title": "Rachel Carson Prize (academic book prize)", "text": "Rachel Carson Prize (academic book prize)\n\nThe Rachel Carson Prize is awarded annually by the Society for Social Studies of Science, an international academic association based in the United States. It is given for a book \"of social or political relevance\" in the field of science and technology studies. This prize was created in 1996.\n\n"}
{"id": "36627068", "url": "https://en.wikipedia.org/wiki?curid=36627068", "title": "Rascal (single-board computer)", "text": "Rascal (single-board computer)\n\nRascal is a single-board computer. It is designed by Brandon Stafford and sold by Rascal Micro LLC in Somerville, Massachusetts.\n\nThe Rascal runs Linux. Its board design is compatible with Arduino shields. It includes web server software and is intended to be programmed in Python.\n\nThe Rascal's web server includes an editor that lets users edit the Python programs running on the Rascal from any web browser, without needing to reflash anything.\n\nMost Arduino shields are compatible with the standard headers on the Rascal.\n\nThe Rascal's design files have been released as Open Hardware\nunder the Creative Commons CC BY-SA license.\nThose design files have been posted to github.\n"}
{"id": "3156825", "url": "https://en.wikipedia.org/wiki?curid=3156825", "title": "Recent change memory administration center", "text": "Recent change memory administration center\n\nRCMAC stands for recent change memory administration center, sometimes mistakenly called \"recent change message accounting center\", in late 20th century Bell System parlance, or \"recent change memory administration group\" (RCMAG). It is an organization of people in a phone company which is responsible for programming the service and features purchased by residential and business customers into the central office. Generally the term is used only in large US phone companies called Regional Bell Operating Companies (RBOCs).\n\nInstalling a telephone line is a complex process, involving coordinated work on outside plant and inside. Inside plant work includes running a jumper on the main distribution frame and programming the switch. Middle 20th century crossbar switches had no computer, hence the same workers who installed the jumper generally wired the necessary information into switch cross connect translations as well. Records were kept as pencil notations in ledger books or index cards. \n\nStored program control exchanges in the 1970s had teleprinter channels for entering and verifying translation information, which allowed centralizing these functions. In the 1980s, the resulting conglomeration of Teletype machines were replaced with a more organized system called MARCH which could more easily be coordinated with COSMOS, TIRKS and other operations support systems.\n\nGenerally, the existence of the RCMAC organization started with 1A switches from Bell Labs (later Lucent, now known as Alcatel-Lucent), from which the term \"recent change memory\" originated. \n\nWith the introduction of various automation systems, the function of the RCMAC would recently be described as an organization of people in the phone company responsible for programming the service and features of phone service where service orders have failed to follow the automated process, investigate and resolve customer trouble reports possibly related to incorrect programming of service and features, and to support outside plant technicians repairing or installing a customer's phone service.\n"}
{"id": "40199755", "url": "https://en.wikipedia.org/wiki?curid=40199755", "title": "Stephanie Hallowich, H/W, v. Range Resources Corporation", "text": "Stephanie Hallowich, H/W, v. Range Resources Corporation\n\nStephanie Hallowich and Chris Hallowich, H/W, v. Range Resources Corporation, Williams Gas/Laurel Mountain Midstream, MarkWest Energy Partners, L.P., MarkWest Energy Group, L.L.C., and Pennsylvania Department of Environmental Protection was a case in the Court of Common Pleas of Washington County, Pennsylvania (Civil Division). The trial was held in August 2011.\n\nChris Hallowich and his wife Stephanie Hallowich operated a farm in Mount Pleasant, Pennsylvania. They sued Range Resources Corporation, Williams Gas/Laurel Mountain Midstream, MarkWest Energy, and the Pennsylvania Department of Environmental Protection for compensation for \"health and environmental impacts\" from \"natural gas development operations\".\n\nIn the settlement, the plaintiffs received $750,000 from the sale of their home for the purchase of a home elsewhere, but were required to abstain, along with their two minor children, from ever again discussing hydraulic fracturing or Marcellus Shale. The filed Washington County court documents in the case show that the Hallowich family was represented by Peter M. Villari, Esq. and Robert N. Wilkey, Esq. of Villari, Brandes, Kline,P.C., a litigation firm located in Conshohocken, Pennsylvania. The case received significant international notoriety concerning the issue of First Amendment rights as it relates to confidential settlement agreements, gag-orders, and the constitutional rights of minors within the scope of the natural gas drilling industry, operations, and civil litigation. On March 20, 2011, Judge Debbie O'Dell-Senaca, issued an Order and Opinion, reversing a prior trial court's order to seal the settlement dockets, and ordering that the Court record, including the settlement documents be unsealed. Many of the Defendants in the case subsequently appealed, seeking to keep the settlement documents sealed. Eventually, the settlement documents in the Hallowich case were restored and made publicly available.\n\n"}
{"id": "26847511", "url": "https://en.wikipedia.org/wiki?curid=26847511", "title": "Stephen Ibaraki", "text": "Stephen Ibaraki\n\nStephen K. Ibaraki is a past teacher, an industry analyst, writer and consultant in the IT industry, and the past president of the Canadian Information Processing Society.\n\nCurrently, Ibaraki is a venture capitalist, entrepreneur and futurist. He is also the founder of the UN ITU AI for Good Global Summit. Ibaraki also serves as the founding chair of the Financial Services Roundtable Tech Advisory Council and founding chair of the International Federation for Information Processing Global Industry Council. \n\nIbaraki has been the recipient of the Microsoft MVP award for 13 years in a row since 2006\n\nREDDS Capital official website\n\nStephen Ibaraki CIPS Fellow Profile\n"}
{"id": "31434896", "url": "https://en.wikipedia.org/wiki?curid=31434896", "title": "Timeline of cultivation and domestication in South and West Asia", "text": "Timeline of cultivation and domestication in South and West Asia\n\nSouth and West Asia consists of a wide region extending from the present-day country of Turkey in the west to Bangladesh and India in the east.\n\n\n\n"}
{"id": "49166857", "url": "https://en.wikipedia.org/wiki?curid=49166857", "title": "Timeline of music technology", "text": "Timeline of music technology\n\nThe timeline of music technology provides the major dates in the history of electric music technologies inventions from the 1800s to the early 1900s and electronic and digital music technologies from 1917 (the date of the Theremin's development) and electric music technologies to the 2010s.\n\n\n"}
