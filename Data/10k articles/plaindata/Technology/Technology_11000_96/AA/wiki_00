{"id": "7483405", "url": "https://en.wikipedia.org/wiki?curid=7483405", "title": "ACORD", "text": "ACORD\n\nACORD, the Association for Cooperative Operations Research and Development, is a non-profit organization that provides the global insurance industry with data standards and implementation solutions. ACORD is widely known across the industry for the publication and maintenance of an extensive archive of standardized forms. ACORD has also developed a comprehensive library of electronic data standards with more than 1200 standardized transaction types to support exchange of insurance data between trading partners.\n\nToday, many of the forms and electronic data standards utilized by the insurance and related industries – both in the United States and in several countries across the globe – were developed by ACORD. The Lloyd's of London insurance market uses ACORD standards for messaging between counterparties.\n\nACORD has also worked with the Centre for Study of Insurance Operations on numerous initiatives including the development of North American XML data standards and the creation of a telematics data standard.\n\nACORD is headquartered in Pearl River, NY and maintains an office in London, U.K.\n\nEstablished in 1970 as a non-profit organization, ACORD was formed by insurance carriers and agents focused on building efficiencies in the United States property casualty insurance market. Originally named Agent Company Operations Research and Development (ACORD), the organization's initial goal was to standardize the many proprietary forms being used by carriers for new business and claims submission. In the late 1970s, ACORD began developing electronic standards to complement its form standards. ACORD subsequently expanded both its forms and electronic data standards beyond property and casualty insurance to encompass life and annuity, surety, and reinsurance markets.\n\nACORD hosts several events on technology and related business topics. Among its events are an annual conference dedicated to advancing innovation in insurance technology.\n"}
{"id": "19681450", "url": "https://en.wikipedia.org/wiki?curid=19681450", "title": "Aerial bomb", "text": "Aerial bomb\n\nAn aerial bomb is a type of explosive or incendiary weapon intended to travel through the air on a predictable trajectory, usually designed to be dropped from an aircraft. Aerial bombs include a vast range and complexity of designs, from unguided gravity bombs to guided bombs, hand tossed from a vehicle, to needing a large specially built delivery vehicle; or perhaps \"be\" the vehicle itself such as a glide bomb, instant detonation or delay-action bomb. The act is termed aerial bombing. As with other types of explosive weapons, aerial bombs are designed to kill and injure people and destroy materiel through the projection of blast and fragmentation outwards from the point of detonation.\n\nThe first bombs delivered to their targets by air were launched on unmanned hot air balloons, carrying a single bomb, by the Austrians against Venice in 1849.\n\nThe first bombs dropped from a heavier-than-air aircraft were grenades or grenade-like devices. Historically, the first use was by Giulio Gavotti on 1 November 1911, during the Italo-Turkish War.\n\nIn 1912, during the First Balkan War, Bulgarian Air Force pilot Christo Toprakchiev suggested the use of aircraft to drop \"bombs\" (called grenades in the Bulgarian army at this time) on Turkish positions. Captain Simeon Petrov developed the idea and created several prototypes by adapting different types of grenades and increasing their payload.\n\nOn 16 October 1912, observer Prodan Tarakchiev dropped two of those bombs on the Turkish railway station of Karağaç (near the besieged Edirne) from an Albatros F.2 aircraft piloted by Radul Milkov, for the first time in this campaign.\n\nAerial bombs typically use a contact fuze to detonate the bomb upon impact, or a delayed-action fuze initiated by impact.\n\nNot all bombs dropped detonate; failures are common. It was estimated that during the Second World War about 10% of German bombs failed to detonate, and that Allied bombs had a failure rate of 15% or 20%, especially if they hit soft soil and used a pistol-type detonating mechanism rather than fuzes. A great many bombs were dropped during the war; thousands of unexploded bombs which may be able to detonate are discovered every year, particularly in Germany, and have to be defused or detonated in a controlled explosion, in some cases requiring evacuation of thousands of people beforehand. Old bombs occasionally detonate when disturbed, or when a faulty time fuze eventually functions, showing that precautions are still essential when dealing with them.\n\nTypes of aerial bomb:\n"}
{"id": "42391472", "url": "https://en.wikipedia.org/wiki?curid=42391472", "title": "Agriculture for Impact", "text": "Agriculture for Impact\n\nAgriculture for Impact is an independent advocacy initiative led by Professor Sir Gordon Conway, author of the book One Billion Hungry: Can We Feed the World. It is based at Imperial College London and is supported through the Agriculture for Impact Bill & Melinda Gates Foundation.\n\nIt aims to enable better European government support for productive, sustainable, equitable and resilient agricultural development in sub-Saharan Africa, focusing in particular on the needs of smallholder farmers. Agriculture for Impact also acts as the convenor of the Montpellier Panel, a group of international experts from the fields of agriculture, trade, policy, ecology and global development.\n\n"}
{"id": "43408158", "url": "https://en.wikipedia.org/wiki?curid=43408158", "title": "All-you-can-eat seats", "text": "All-you-can-eat seats\n\nAll-you-can-eat seats, also called all-inclusive sections, are blocks of seats in a Major League Baseball or Minor League Baseball park in which seat holders are entitled to unlimited hot dogs, nachos, popcorn, peanuts, soft drinks, and bottled water before and during a game. Typically located in less desirable areas of the ballpark, such as the bleachers and upper decks, all-you-can-eat (AYCE) seats are priced approximately 50% higher than seats in the same section, but are viewed by patrons as a bargain considering the added cost of ballpark food. The first AYCE section was introduced at Dodger Stadium in 2007. The trend spread to 19 of the 30 major league parks by 2010 and numerous minor league parks by 2012. In 2008 AYCE seats were also inaugurated in numerous NBA and NHL arenas and at several NASCAR tracks.\n\nThe Los Angeles Dodgers introduced the first Major League Baseball AYCE section in April 2007 after conducting three pilots during the 2006 season. Soon after, the Atlanta Braves, Baltimore Orioles, Kansas City Royals, and Texas Rangers converted their under-utilized seats to AYCE seats. The concept spread to 13 Major League Baseball parks in 2008 and 19 parks in 2010.\n\nMajor League Baseball teams offering all-you-can-eat seats include the Arizona Diamondbacks, Atlanta Braves, Baltimore Orioles, Cincinnati Reds, Detroit Tigers, Houston Astros, Kansas City Royals, Los Angeles Dodgers, Miami Marlins, Minnesota Twins, Pittsburgh Pirates, San Diego Padres, Tampa Bay Rays, Texas Rangers, and Toronto Blue Jays.\n\nBeginning with the 2018 NFL Season, the Detroit Lions introduced Club 200. This premium seating option includes all-you-can-eat items such as hot dogs, popcorn, nachos, pretzels, chips, cotton candy, salad and soda. The section in Ford Field also includes two drink tickets and a dedicated giveaway item at each game.\n\nAll-you-can-eat seats are typically located in \"distant bleacher or upper-deck sections\". Seat prices are marked up approximately 50% over the regular price of seats in that section.\n\nThe AYCE buffet generally operates from the time the stadium gates open until the beginning or end of the seventh inning. Some parks put an hourly limit on it – for example, food service is open for two hours after the first pitch at San Diego Padres games and until 9 p.m. at Minnesota Twins games. The basic menu includes traditional ballpark food such as hot dogs, nachos, peanuts, popcorn, and soft drinks. Some ballparks add other options, such as \"veggie dogs\" at Petco Park, green salad, ice cream, and kosher and veggie dogs (by advance request) at Oriole Park at Camden Yards, and \"burgers, salads, peanut butter-filled pretzels and ice cream\" at PNC Park.\n\nSome ballparks limit the amount of food an AYCE patron can take on each trip to the buffet. At Dodger Stadium, AYCE patrons are limited to four hot dogs per visit, but can take as many soft drinks and bottles of water as they wish. At Camden Yards, patrons may take up to two of each food item on each visit. The lines move quickly, as no cash transactions are involved.\n\nAt Dodger Stadium, AYCE ticket-holders enter and exit through a different gate than other ticket-holders and can access only the AYCE buffet and a set of restrooms. At other ballparks, ticket-holders wear colored wristbands to identify themselves as AYCE patrons. At Camden Yards, AYCE ticket-holders have their hand stamped.\n\nAll-you-can-eat seats have successfully boosted attendance in ballparks experiencing low turnouts, as well as increased occupancy of stadium sections that were previously under-used. At Dodger Stadium, for example, before 2007 the right-field bleachers were opened only when the left-field bleachers sold out, or for group sales. Following the conversion of the right-field bleachers into an AYCE section of 3,300 seats, occupancy zoomed to 85%. The Arizona Diamondbacks boosted ticket sales by 70% when it created a left-field AYCE section in 2009, while the Houston Astros averaged 95% capacity in its AYCE section.\n\nFrom the patrons' point of view, AYCE seats are viewed as a bargain considering the added cost of ballpark food. After a few orders of hot dogs, nachos, and soft drinks, the AYCE seat pays for itself. The Atlanta Braves estimated that a typical AYCE patron in 2007 consumed 3.35 hot dogs, 20 ounces of soda, 7.9 ounces of peanuts; 3 ounces of nachos; and 32 ounces of popcorn. AYCE seats have been described as a way to indulge in junk food \"with baseball as a nominal backdrop\", an opportunity to eat a cheap dinner with a baseball game thrown in, and a way to feed a family on a budget. AYCE patrons have been known to engage in eating contests and to sneak food home with them.\n\n\n"}
{"id": "23676060", "url": "https://en.wikipedia.org/wiki?curid=23676060", "title": "Aquamarine Power", "text": "Aquamarine Power\n\nAquamarine Power was a wave energy company, which was founded in 2005 to commercialise a wave energy device concept known as the Oyster wave energy converter. The company's head offices were based in Edinburgh. The company had further operations in Orkney, Ireland, Northern Ireland and the United States. Its chief executive officer was Martin McAdam, who joined the company in 2008. The company was advised by Trevor Whittaker, inventor of the Oyster concept, and Stephen Salter, inventor of the Salter's Duck.\n\nThe Oyster concept originated from studies conducted in 2003 by the wave power research team at Queen's University, Belfast, led by Professor Trevor Whittaker. The Queen's University, Belfast studies were co-funded by the Engineering and Physical Sciences Research Council and Allan Thomson, who had previously founded and led the UK’s first commercial wave energy company, Wavegen. In 2005, Allan Thomson founded Aquamarine Power to progress the commercialisation of the Oyster device. In 2007 Scottish & Southern Energy subsidiary Renewable Technology Ventures Limited invested in Aquamarine with a further investment in 2010. In February 2009, Aquamarine Power and Queen's University, Belfast signed an agreement to extend their R&D partnership to 2014.\nIn February 2009, Aquamarine Power signed an agreement with renewable energy company Airtricity, a subsidiary of Scottish & Southern Energy, to develop marine energy sites using the Oyster system. In November 2009, the first full-scale, 315 kW, demonstrator Oyster began producing power when it was launched at the European Marine Energy Centre (EMEC) on Orkney.\n\nIn March 2012, Aquamarine announced it has plans to install 50 Oyster devices on the seabed off of the Western Isles in Scotland (provisionally dubbed the Orkney Wave Power Station). The project was intended to be able to supply electricity to more than 38,000 homes (2.4 MW in installed capacity).\n\nIn November 2009 Aquamarine Power announced an investment of £11m in the business. The principal investor during this investment round was ABB Group who invested £8m. The other investors during the round included Scottish and Southern Energy who invested £2.7m with other historical investors making up the balance of £300k. The investors in the business include: ABB, SSE, Sigma Capital Group, Scottish Enterprise and others.\n\nAquamarine Power won several awards. In 2008 it was named Emerging Technology Promoter of the Year by Ernst & Young Euromoney Global Renewable Energy Awards. In 2009, it was named Innovator of the Year by the British Renewable Energy Association. It also got Innovation Award for Energy of the Engineer Technology and Innovation Awards 2009 and Scottish Green Awards for the Best Green Industry SME. In 2010 it was listed on GlobalCleantech 100 list.\n\nOn 28 October 2015, BBC News reported that Aquamarine Power had called in administrators. No buyer was found and less than a month later, on 20 November, the company ceased to trade with the loss of fourteen jobs.\n\n\n"}
{"id": "22221458", "url": "https://en.wikipedia.org/wiki?curid=22221458", "title": "Automotive electronics", "text": "Automotive electronics\n\nAutomotive electronics are electronic systems used in vehicles, including engine management, ignition, radio, carputers, telematics, in-car entertainment systems and others. Ignition, engine, and transmission electronics are also found in trucks, motorcycles, off-road vehicles, and other internal combustion-powered machinery such as forklifts, tractors, and excavators. Related elements for control of relevant electrical systems are found on hybrid vehicles and electric cars as well.\n\nElectronic systems have become an increasingly large component of the cost of an automobile, from only around 1% of its value in 1950 to around 30% in 2010.\n\nThe earliest electronics systems available as factory installations were vacuum tube car radios, starting in the early 1930s. The development of semiconductors after WWII greatly expanded the use of electronics in automobiles, with solid-state diodes making the automotive alternator the standard after about 1960, and the first transistorized ignition systems appearing about 1955.\n\nThe availability of microprocessors after about 1974 made another range of automotive applications economically feasible. In 1978 the Cadillac Seville introduced a \"trip computer\" based on a 6802 microprocessor. Electronically-controlled ignition and fuel injection systems allowed automotive designers to achieve vehicles meeting requirements for fuel economy and lower emissions, while still maintaining high levels of performance and convenience for drivers. Today's automobiles contain a dozen or more processors, in functions such as engine management, transmission control, climate control,antilock braking, passive safety systems, navigation, and other functions.\n\nModern electric cars rely on power electronics for the main propulsion motor control, as well as managing the battery system. Future autonomous cars will rely on powerful computer systems, an array of sensors, networking, and satellite navigation, all of which will require electronics.\n\nAutomotive electronics or automotive embedded systems are distributed systems, and according to different domains in the automotive field, they can be classified into: \n\nOne of the most demanding electronic parts of an automobile is the engine control unit (ECU). Engine controls demand one of the highest real time deadlines, as the engine itself is a very fast and complex part of the automobile. Of all the electronics in any car the computing power of the engine control unit is the highest, typically a 32-bit processor.\n\nA modern car may have up to 100 ECU's and a commercial vehicle up to 40.\n\nAn engine ECU controls such functions as:\n\nIn a diesel engine: \n\nIn a gasoline engine: \n\nMany more engine parameters are actively monitored and controlled in real-time. There are about 20 to 50 that measure pressure, temperature, flow, engine speed, oxygen level and NOx level plus other parameters at different points within the engine. All these sensor signals are sent to the ECU, which has the logic circuits to do the actual controlling. The ECU output is connected to different actuators for the throttle valve, EGR valve, rack (in VGTs), fuel injector (using a pulse-width modulated signal), dosing injector and more. There are about 20 to 30 actuators in all.\n\nThese control the transmission system, mainly the shifting of the gears for better shift comfort and to lower torque interrupt while shifting. Automatic transmissions use controls for their operation, and also many semi-automatic transmissions having a fully automatic clutch or a semi-auto clutch (declutching only). The engine control unit and the transmission control exchange messages, sensor signals and control signals for their operation.\n\nThe chassis system has a lot of sub-systems which monitor various parameters and are actively controlled:\n\nThese systems are always ready to act when there is a collision in progress or to prevent it when it senses a dangerous situation:\n\n\n\n\nAll of the above systems forms an infotainment system. Developmental methods for these systems vary according to each manufacturer. Different tools are used for both hardware and software development.\n\nThese are new generation hybrid ECUs that combine the functionalities of multiple ECUs of Infotainment Head Unit, Advanced Driver Assistance Systems (ADAS), Instrument Cluster, Rear Camera/Parking Assist, Surround View Systems etc. This saves on cost of electronics as well as mechanical/physical parts like interconnects across ECUs etc. There is also a more centralized control so data can be seamlessly exchanged between the systems.\n\nThere are of course challenges too. Given the complexity of this hybrid system, a lot more rigor is needed to validate the system for robustness, safety and security. For example, if the infotainment system's application which could be running an open source Android OS is breached, there could be possibility of hackers to take control of the car remotely and potentially misuse it for anti social activities. Typically so, usage of a hardware+software enabled hypervisors are used to virtualize and create separate trust and safety zones that are immune to each other's failures or breaches. Lot of work is happening in this area and potentially will have such systems soon if not already.\n\nIn order to minimize the risk of dangerous failures, safety related electronic systems have to be developed following the applicable product liability requirements. Disregard for, or inadequate application of these standards can lead to not only personal injuries, but also severe legal and economic consequences such as product cancellations or recalls.\n\nThe IEC 61508 standard, generally applicable to electrical/electronic/programmable safety-related products, is only partially adequate for automotive-development requirements. Consequently, for the automotive industry, this standard is replaced by the existing ISO 26262, currently released as a Final Draft International Standard (FDIS). ISO/DIS 26262 describes the entire product life-cycle of safety related electrical/electronic systems for road vehicles. It has been published as an international standard in its final version in November 2011. The implementation of this new standard will result in modifications and various innovations in the automobile electronics development process, as it covers the complete product life-cycle from the concept phase until its decommissioning.\n\nAs more functions of the automobile are connected to short- or long-range networks, cybersecurity of systems against unauthorized modification is required. With critical systems such as engine controls, transmission, air bags, and braking connected to internal diagnostic networks, remote access could result in a malicious intruder altering the function of systems or disabling them, possibly causing injuries or fatalities. Every new interface presents a new \"attack surface\". The same facility that allows the owner to unlock and start a car from a smart phone app also presents risks due to remote access. Auto manufacturers may protect the memory of various control microprocessors both to secure them from unauthorized changes and also to ensure only manufacturer-authorized facilities can diagnose or repair the vehicle. Systems such as keyless entry rely on cryptographic techniques to ensure \"replay\" or \"man-in-the-middle attacks\" attacks cannot record sequences to allow later break-in to the automobile. \n\nIn 2015 the German general automobile club commissioned an investigation of the vulnerabilities of one manufacturer's electronics system, which could have led to such exploits as unauthorized remote unlocking of the vehicle. \n\n\n"}
{"id": "5548053", "url": "https://en.wikipedia.org/wiki?curid=5548053", "title": "Best coding practices", "text": "Best coding practices\n\nCoding best practices are a set of informal rules that the software development community has learned over time which can help improve the quality of software.\n\nMany computer programs remain in use for far longer than the original authors ever envisaged (sometimes 40 years or more), so any rules need to facilitate both initial development and subsequent maintenance and enhancement by people other than the original authors.\n\nIn Ninety-ninety rule, Tom Cargill is credited with this explanation as to why programming projects often run late: \"The first 90% of the code accounts for the first 90% of the development time. The remaining 10% of the code accounts for the other 90% of the development time.\" Any guidance which can redress this lack of foresight is worth considering.\n\nThe size of a project or program has a significant effect on error rates, programmer productivity, and the amount of management needed.\n\nAs listed below, there are many attributes associated with good software. Some of these can be mutually contradictory (e.g. very fast versus full error checking), and different customers and participants may have different priorities. Weinberg provides an example of how different goals can have a dramatic effect on both effort required and efficiency. Furthermore, he notes that programmers will generally aim to achieve any explicit goals which may be set, probably at the expense of any other quality attributes.\n\nSommerville has identified four generalised attributes which are not concerned with what a program does, but how well the program does it:\n\nWeinberg has identified four targets which a good program should meet:\n\nHoare has identified seventeen objectives related to software quality, including:\n\nBefore coding starts, it is important to ensure that all necessary prerequisites have been completed (or have at least progressed far enough to provide a solid foundation for coding). If the various prerequisites are not satisfied then the software is likely to be unsatisfactory, even if it is completed.\n\nFrom Meek & Heath: \"What happens before one gets to the coding stage is often of crucial importance to the success of the project.\"\n\nThe prerequisites outlined below cover such matters as:\n\nFor small simple projects involving only one person, it may be feasible to combine architecture with design and adopt a very simple life cycle.\n\nA software development methodology is a framework that is used to structure, plan, and control the life cycle of a software product. Common methodologies include waterfall, prototyping, iterative and incremental development, spiral development, agile software development, rapid application development, and extreme programming.\n\nThe waterfall model is a sequential development approach; in particular, it assumes that the requirements can be completely defined at the start of a project. However, McConnell quotes three studies which indicate that, on average, requirements change by around 25% during a project. The other methodologies mentioned above all attempt to reduce the impact of such requirement changes, often by some form of step-wise, incremental, or iterative approach. Different methodologies may be appropriate for different development environments.\n\nMcConnell states: \"The first prerequisite you need to fulfill before beginning construction is a clear statement of the problem the system is supposed to solve.\"\n\nMeek and Heath emphasise that a clear, complete, precise, and unambiguous written specification is the target to aim for. Note that it may not be possible to achieve this target, and the target is likely to change anyway (as mentioned in the previous section).\n\nSommerville distinguishes between less detailed user requirements and more detailed system requirements. He also distinguishes between functional requirements (e.g. update a record) and non-functional requirements (e.g. response time must be less than 1 second).\n\nHoare points out: \"there are two ways of constructing a software design: one way is to make it so simple that there are \"obviously\" no deficiencies; the other way is to make it so complicated that there are no \"obvious\" deficiencies. The first method is far more difficult.\"\n\nSoftware architecture is concerned with deciding what has to be done, and which program component is going to do it (how something is done is left to the detailed design phase, below). This is particularly important when a software system contains more than one program since it effectively defines the interface between these various programs. It should include some consideration of any user interfaces as well, without going into excessive detail.\n\nAny non-functional system requirements (response time, reliability, maintainability, etc.) need to be considered at this stage.\n\nThe software architecture is also of interest to various stakeholders (sponsors, end-users, etc.) since it gives them a chance to check that their requirements can be met.\n\nThe main purpose of design is to fill in the details which have been glossed over in the architectural design. The intention is that the design should be detailed enough to provide a good guide for actual coding, including details of any particular algorithms to be used. For example, at the architectural level, it may have been noted that some data has to be sorted, while at the design level it is necessary to decide which sorting algorithm is to be used. As a further example, if an object-oriented approach is being used, then the details of the objects must be determined (attributes and methods).\n\nMayer states: \"No programming language is perfect. There is not even a single best language; there are only languages well suited or perhaps poorly suited for particular purposes. Understanding the problem and associated programming requirements is necessary for choosing the language best suited for the solution.\"\n\nFrom Meek & Heath: \"The essence of the art of choosing a language is to start with the problem, decide what its requirements are, and their relative importance since it will probably be impossible to satisfy them all equally well. The available languages should then be measured against the list of requirements, and the most suitable (or least unsatisfactory) chosen.\"\n\nIt is possible that different programming languages may be appropriate for different aspects of the problem. If the languages or their compilers permit, it may be feasible to mix routines written in different languages within the same program.\n\nEven if there is no choice as to which programming language is to be used, McConnell provides some advice: \"Every programming language has strengths and weaknesses. Be aware of the specific strengths and weaknesses of the language you're using.\"\n\nThis section is also really a prerequisite to coding, as McConnell points out: \"Establish programming conventions before you begin programming. It's nearly impossible to change code to match them later.\"\n\nAs listed near the end of Coding conventions, there are different conventions for different programming languages, so it may be counterproductive to apply the same conventions across different languages.\n\nThe use of coding conventions is particularly important when a project involves more than one programmer (there have been projects with thousands of programmers). It is much easier for a programmer to read code written by someone else if all code follows the same conventions.\n\nFor some examples of bad coding conventions, Roedy Green provides a lengthy (tongue-in-cheek) article on how to produce unmaintainable code.\n\nDue to time restrictions or enthusiastic programmers who want immediate results for their code, commenting of code often takes a back seat. Programmers working as a team have found it better to leave comments behind since coding usually follows cycles, or more than one person may work on a particular module. However, some commenting can decrease the cost of knowledge transfer between developers working on the same module.\n\nIn the early days of computing, one commenting practice was to leave a brief description of the following:\n\nThe \"description of the module\" should be as brief as possible but without sacrificing clarity and comprehensiveness.\n\nHowever, the last two items have largely been obsoleted by the advent of revision control systems. Modifications and their authorship can be reliably tracked by using such tools rather than by using comments.\n\nAlso, if complicated logic is being used, it is a good practice to leave a comment \"block\" near that part so that another programmer can understand what is exactly happening.\n\nUnit testing can be another way to show how code is intended to be used.\n\nUse of proper naming conventions is considered good practice. Sometimes programmers tend to use X1, Y1, etc. as variables and forget to replace them with meaningful ones, causing confusion.\n\nIn order to prevent this waste of time, it is usually considered good practice to use descriptive names in the code since it’s about real data.\n\nExample: A variable for taking in weight as a parameter for a truck can be named TrkWeight or TruckWeightKilograms, with TruckWeightKilograms being the more preferable one, since it is instantly recognisable. See CamelCase naming of variables.\n\nThe code that a programmer writes should be simple. Complicated logic for achieving a simple thing should be kept to a minimum since the code might be modified by another programmer in the future. The logic one programmer implemented may not make perfect sense to another. So, always keep the code as simple as possible.\n\nFor example, consider these equivalent lines of C code:\n\nand \n\nand\n\nThe 1st approach, which is much more commonly used, is considerably larger than the 3rd. In particular, it consumes 5 times more screen vertical space (lines), and 97 characters versus 52 (though editing tools may reduce the difference in actual typing). It is arguable, however, which is \"simpler\". The first has an explicit if/then else, with an explicit return value obviously connected with each; even a novice programmer should have no difficulty understanding it. The 2nd merely discards the braces, cutting the \"vertical\" size in half with little change in conceptual complexity. In most languages the \"return\" statements could also be appended to the prior lines, bringing the \"vertical\" size to only one more line that the 3rd form. \n\nThe third form obviously minimizes the size, but may increase the complexity: It leaves the \"true\" and \"false\" values implicit, and intermixes the notions of \"condition\" and \"return value\". It is likely obvious to most programmers, but a novice might not immediately understand that the result of evaluating a condition is actually a value (of type Boolean, or its equivalent in whatever language), and thus can be manipulated or returned. In more realistic examples, the 3rd form could have problems due to operator precedence, perhaps returning an unexpected type, where the prior forms would in some languages report an error. Thus, \"simplicity\" is not merely a matter of length, but of logical and conceptual structure; making code shorter may make it less or more complex. \n\nFor large, long lived programs using verbose alternatives could contribute to bloat.\n\nCompactness can allow coders to view more code per page, reducing scrolling gestures and keystrokes. Given how many times code might be viewed in the process of writing and maintaining, it might amount to a significant savings in programmer keystrokes in the life of the code. This might not seem significant to a student first learning to program. But when producing and maintaining large programs which often reach thousands or even millions of lines, it becomes apparent how much a minor code simplification might speed work, and lessen finger, wrist and eye strain, which are common medical issues suffered by production coders and information workers.\n\nTerser coding speeds compilation very slightly, as fewer symbols need to be processed. Furthermore, the 3rd approach may allow similar lines of code to be more easily compared, particularly when many such constructs can appear on one screen at the same time.\n\nFinally, very terses layouts might better utilize modern wide-screen computer displays. In the past screens were limited to 40 or 80 characters (such limits originated far earlier: manuscripts, printed books, and even scrolls, have for millennia used quite short lines (see for example Gutenberg Bible). Modern screens can easily display 200 or more characters, allowing extremely long lines. Most modern coding styles and standards do not take up that entire width. Thus, if using one window as wide as the screen, a great deal of available space is wasted. On the other hand, with multiple windows, or using an IDE or other tool with various information in side panes, the available width for code is in the range familiar from earlier systems. \n\nIt is also worth noting that the human visual system is greatly affected by line length; very long lines slightly increase reading speed, but reduce comprehension and add to eye-tracking errors. Some studies suggest that longer lines fare better online than in print , but this still only goes up to about 10 inches, and mainly for raw speed of reading prose.\n\nProgram code should not contain \"hard-coded\" (literal) values referring to environmental parameters, such as absolute file paths, file names, user names, host names, IP addresses, URLs, UDP/TCP ports. Otherwise the application will not run on a host that has a different design than anticipated. A careful programmer can parametrize such variables and configure them for the hosting environment outside of the application proper (for example in property files, on an application server, or even in a database). Compare the mantra of a \"single point of definition\"\n(SPOD).\n\nAs an extension, resources such as XML files should also contain variables rather than literal values, otherwise the application will not be portable to another environment without editing the XML files. For example, with J2EE applications running in an application server, such environmental parameters can be defined in the scope of the JVM and the application should get the values from there.\n\nA general overview of all of the above:\n\n\nA best practice for building code involves daily builds and testing, or better still continuous integration, or even continuous delivery.\n\nTesting is an integral part of software development that needs to be planned. It is also important that testing is done proactively; meaning that test cases are planned before coding starts, and test cases are developed while the application is being designed and coded.\n\nProgrammers tend to write the complete code and then begin debugging and checking for errors. Though this approach can save time in smaller projects, bigger and complex ones tend to\nhave too many variables and functions that need attention. Therefore, it is good to debug every module once you are done and not the entire program. This saves time in the long run so that one does not end up wasting a lot of time on figuring out what is wrong. Unit tests for individual modules, and/or functional tests for web services and web applications, can help with this.\n\nDeployment is the final stage of releasing an application for users. Some best practices are:\n\n\n\n"}
{"id": "39345917", "url": "https://en.wikipedia.org/wiki?curid=39345917", "title": "Big Hero 6 (film)", "text": "Big Hero 6 (film)\n\nBig Hero 6 is a 2014 American 3D computer-animated superhero film produced by Walt Disney Animation Studios and released by Walt Disney Pictures. Loosely based on the superhero team of the same name by Marvel Comics, the film is the 54th Disney animated feature film. Directed by Don Hall and Chris Williams, the film tells the story of Hiro Hamada, a young robotics prodigy who forms a superhero team to combat a masked villain. The film features the voices of Scott Adsit, Ryan Potter, Daniel Henney, T.J. Miller, Jamie Chung, Damon Wayans Jr., Genesis Rodriguez, Alan Tudyk, James Cromwell, and Maya Rudolph.\n\n\"Big Hero 6\" is the first Disney animated film to feature Marvel Comics characters, whose parent company was acquired by The Walt Disney Company in 2009. Walt Disney Animation Studios created new software technology to produce the film's animated visuals.\n\n\"Big Hero 6\" premiered at the 27th Tokyo International Film Festival on October 23, 2014, and at the Abu Dhabi Film Festival on October 31; it was theatrically released in the Disney Digital 3-D and RealD 3D formats in the United States on November 7, 2014. The film was met with both critical and commercial success, grossing over $657.8 million worldwide and becoming the highest-grossing animated film of 2014. It won the Academy Award for Best Animated Feature and the Kids' Choice Award for Favorite Animated Movie. It also received nominations for the Annie Award for Best Animated Feature, the Golden Globe Award for Best Animated Feature Film, and the BAFTA Award for Best Animated Film. \"Big Hero 6\" was released on DVD and Blu-ray Disc on February 24, 2015.\n\nA , which continues the story of the film, debuted on November 20, 2017 on Disney Channel and Disney XD.\n\nHiro Hamada is a 14-year-old robotics genius living in the futuristic city of San Fransokyo who spends much of his free time participating in illegal robot fights. To redirect Hiro, his older brother Tadashi takes him to the research lab at the San Fransokyo Institute of Technology, where Hiro meets Tadashi's friends, GoGo, Wasabi, Honey Lemon, and Fred. Hiro also meets Professor Robert Callaghan, the head of the university's robotics program. Amazed, Hiro decides to apply to the university.\n\nTo enroll, he signs up for the school's science fair and presents his project: microbots, swarms of tiny robots that can link together in any arrangement imaginable using a neurocranial transmitter. At the fair, Hiro declines an offer from Alistair Krei, CEO of Krei Tech, to market the microbots, and Callaghan accepts him into the school. At the end of the day, a fire breaks out among the exhibits and Tadashi rushes in to save Callaghan, the only person left inside. The building explodes moments later, killing Tadashi.\n\nWeeks later, a depressed Hiro, in mourning for Callaghan's and Tadashi's death, inadvertently activates Baymax, the inflatable healthcare robot that Tadashi created; the two find Hiro's only remaining microbot and follow it to an abandoned warehouse. There they discover that someone has been mass-producing the microbots, and are attacked by a man wearing a Kabuki mask who is controlling them. After they escape, Hiro equips Baymax with armor and a battle chip containing various karate moves, and they track the masked man to the docks. GoGo, Wasabi, Honey Lemon, and Fred arrive, responding to a call from Baymax, and the masked man chases the group. The six escape to Fred's mansion, where they decide to form a high-tech superhero team to combat the villain.\n\nThe group tracks the masked man, whom they suspect to be Krei, to an abandoned Krei Tech laboratory which was used for teleportation research until a test pilot was lost in an accident. The masked man attacks, but the group subdues him and knocks off his mask – revealing him to be Callaghan, who had stolen and used Hiro's microbots to shield himself from the explosion, leaving Tadashi to die. Enraged, Hiro removes Baymax's healthcare chip, leaving only the battle chip, and orders him to kill Callaghan. Honey reinstalls the healthcare chip at the last second, preventing Baymax from carrying out the kill order. Callaghan escapes, and Hiro leaves with Baymax, intent on avenging Tadashi.\n\nBack home, Hiro tries to remove the healthcare chip again, but Baymax stops him and states that vengeance is not what Tadashi would have wanted. To calm him down, Baymax shows Hiro videos of Tadashi running numerous tests during Baymax's development as a demonstration of Tadashi's benevolence and legacy. Hiro remorsefully apologizes to his friends, who reassure him they will catch Callaghan the right way.\n\nVideo footage from the laboratory accident reveals that the lost test pilot was Callaghan's daughter Abigail, and that Callaghan is seeking revenge on Krei. Callaghan interrupts Krei at a public event and attempts to destroy his headquarters using Krei's teleportation portal. After a lengthy battle, the team deprives Callaghan of his microbots and the mask, saving Krei, but the portal remains active. Baymax detects Abigail inside, alive but in hyper-sleep, and leaps into the portal with Hiro to rescue her. They find Abigail's pod, but on the way back out, Baymax is struck by debris, damaging his armor and disabling his thrusters. With no other option Baymax activates his armor's rocket fist, and asks Hiro to say he is satisfied with his care, to the shock of Hiro, but Baymax convinces him to do it, saying he will always be with him. Hiro agrees, and Baymax fires his rocket fist, propelling Hiro and Abigail back through the portal before it closes. Callaghan is arrested while Abigail is taken to the hospital.\n\nSome time later, Hiro discovers Baymax's health care chip clenched in the rocket fist. He rebuilds Baymax's body, and the six friends continue their exploits throughout the city. During the end credits, a series of newspaper headlines reveals that the university has awarded Hiro a grant and dedicated a building in Tadashi's honor, and that the team has continued protecting the city. In a post-credits scene, Fred discovers a hidden cache of superhero equipment in his family mansion. His father, a retired superhero, returns from vacation and says, \"We have a lot to talk about.\"\n\n\nAfter Disney's acquisition of Marvel Entertainment in 2009, CEO Bob Iger encouraged the company's divisions to explore Marvel's properties for adaptation concepts. By deliberately picking an obscure title, it would give them the freedom to come up with their own version. While directing \"Winnie the Pooh\", director Don Hall was scrolling through a Marvel database when he stumbled upon \"Big Hero 6\", a comic he had never heard of before. \"I just liked the title,\" he said. He pitched the concept to John Lasseter in 2011, as one of five ideas for possible productions for Walt Disney Animation Studios, and this particular idea \"struck a chord\" with Lasseter, Hall, and Chris Williams.\n\nIn June 2012, Disney confirmed that Walt Disney Animation Studios was adapting Marvel Comics' series and that the film had been commissioned into early stages of development. Because they wanted the concept to feel new and fresh, head of story Paul Briggs (who also voices Yama in the film) only read a few issues of the comic, while screenwriter Robert Baird admitted he had not read the comic at all.\n\n\"Big Hero 6\" was produced solely by Walt Disney Animation Studios, although several members of Marvel's creative team were involved in the film's production including Joe Quesada, Marvel's chief creative officer, and Jeph Loeb, head of Marvel Television. According to an interview with Axel Alonso by CBR, Marvel did not have any plans to publish a tie-in comic. Disney planned to reprint the Marvel version of \"Big Hero 6\" themselves, but reportedly Marvel disagreed. They eventually came to agreement that Yen Press would publish the Japanese manga version of \"Big Hero 6\" for Disney.\n\nConversely, Lasseter dismissed the idea of a rift between the two companies, and producer Roy Conli stated that Marvel allowed Disney \"complete freedom in structuring the story\". Disney Animation Studio President Andrew Millstein stated: \"\"Hero\" is one example of what we've learned over the years and our embracing some of the Pixar DNA.\" Regarding the film's story, Quesada stated, \"The relationship between Hiro and his robot has a very Disney flavor to it ... but it's combined with these Marvel heroic arcs.\" The production team decided early on not to connect the film to the Marvel Cinematic Universe and instead set the film in a stand-alone universe.\nWith respect to the design of Baymax, Hall mentioned in an interview, \"I wanted a robot that we had never seen before and something to be wholly original. That's a tough thing to do, we've got a lot of robots in pop culture, everything from The Terminator to WALL-E to C-3PO on down the line and not to mention Japanese robots, I won't go into that. So I wanted to do something original.\" Even if they did not yet know what the robot should look like, artist Lisa Keene came up with the idea that it should be a huggable robot. Other sources of inspiration cited by the team include Japanese anime, such as Hayao Miyazaki films (including \"Spirited Away\" and \"The Wind Rises\") and \"Pokémon\", as well as Shogun Warriors toys. Mecha designer Shigeto Koyama, who previously did design work for mecha anime such as \"Gunbuster 2\", \"Eureka Seven\", \"Gurren Lagann\", and \"Rebuild of Evangelion\", worked on the concept design for Baymax.\n\nEarly on in the development process, Hall and the design team took a research trip to Carnegie Mellon University's Robotics Institute, where they met a team of DARPA-funded researchers who were pioneering the new field of 'soft robotics' using inflatable vinyl, which ultimately inspired Baymax's inflatable, vinyl, truly huggable design. Hall stated that \"I met a researcher who was working on soft robots. ... It was an inflatable vinyl arm and the practical app would be in the healthcare industry as a nurse or doctor's assistant. He had me at vinyl. This particular researcher went into this long pitch but the minute he showed me that inflatable arm, I knew we had our huggable robot.\" Hall stated that the technology \"will have potential probably in the medical industry in the future, making robots that are very pliable and gentle and not going to hurt people when they pick them up.\"\n\nHall mentioned that achieving a unique look for the mechanical armor took some time and \"just trying to get something that felt like the personality of the character\". Co-director Williams stated, \"A big part of the design challenge is when he puts on the armor you want to feel that he's a very powerful intimidating presence ... at the same time, design-wise he has to relate to the really adorable simple vinyl robot underneath.\" Baymax's face design was inspired by a copper \"suzu\" bell that Hall noticed while at a Shinto shrine.\n\nAccording to Conli, Lasseter initially disliked Baymax's description (while low on battery power) of Hiro's cat as a \"hairy baby\", but Williams kept the line in anyway, and at the film's first test screening, Lasseter admitted that Williams was correct.\n\nAccording to Williams, Baymax was originally going to be introduced rather late in the film, but then story artist John Ripa conceived of a way for Baymax to meet Hiro much earlier. The entire film became much stronger by establishing the relationship between Hiro and Baymax early on, but the filmmakers ended up having to reconstruct \"a fair amount of the first act\" in order to make that idea work.\n\nAbout ninety animators worked on the film at one point or another; some worked on the project for as long as two years. In terms of the film's animation style and settings, the film combines Eastern world culture (predominantly Japanese) with Western world culture (predominantly California). In May 2013, Disney released concept art and rendered footage of San Fransokyo from the film. San Fransokyo, the futuristic mashup of San Francisco and Tokyo, was described by Hall as \"an alternate version of San Francisco. Most of the technology is advanced, but much of it feels retro … Where Hiro lives, it feels like the Haight. I love the Painted ladies. We gave them a Japanese makeover; we put a cafe on the bottom of one. They live above a coffee shop.\" According to production designer Paul Felix, \"The topography is exaggerated because what we do is caricature, I think the hills are 1½ times exaggerated. I don't think you could really walk up them ... When you get to the downtown area, that's when you get the most Tokyo-fied, that pure, layered, dense kind of feeling of the commercial district there. When you get out of there, it becomes more San Francisco with the Japanese aesthetic. … (It's a bit like) \"Blade Runner\", but contained to a few square blocks. You see the skyscrapers contrasted with the hills.\"\n\nThe reason why Disney wanted to merge Tokyo (which is where the comic book version takes place) with San Francisco was partly because San Francisco had not been used by Marvel before, partly because of all the city's iconic aspects, and partly because they felt its aesthetics would blend well with Tokyo. The filmmakers' idea was that San Fransokyo is based on an alternative history in which San Francisco was largely rebuilt by Japanese immigrants in the aftermath of the 1906 earthquake, although this premise is never stated in the film.\n\nTo create San Fransokyo as a detailed digital simulation of an entire city, Disney purchased the actual assessor data for the entire city and county of San Francisco. The final city contains over 83,000 buildings and 100,000 vehicles.\n\nA software program called Denizen was used to create over 700 distinctive characters that populate the city. Another one named Bonzai was responsible for the creation of the city's 250,000 trees, while a new rendering system called Hyperion offered new illumination possibilities, like light shining through a translucent object (e.g. Baymax's vinyl covering). Pixar's RenderMan was considered as a \"Plan B\" for the film's rendering, if Hyperion was not able to meet production deadlines.\n\nDevelopment on Hyperion started in 2011 and was based upon research into multi-bounce complex global illumination originally conducted at Disney Research in Zürich. Disney, in turn, had to assemble a new super-computing cluster just to handle Hyperion's immense processing demands, which consists of over 2,300 Linux workstations distributed across four data centers (three in Los Angeles and one in San Francisco). Each workstation, , included a pair of 2.4 GHz Intel Xeon processors, 256 GB of memory, and a pair of 300 GB solid-state drives configured as a RAID Level 0 array (i.e., to operate as a single 600 GB drive). This was all backed by a central storage system with a capacity of five petabytes, which holds all digital assets as well as archival copies of all 54 Disney Animation films.\n\nThe emotional climax that takes place in the middle of a wormhole portal is represented by the stylized interior of a mandelbulb.\n\nThe post-credits scene was only added to the film in August 2014, late in production, after co-director Don Hall and his crew went to see Marvel Studios' \"Guardians of the Galaxy\". He stated that \"[i]t horrified us, that people were sat waiting for an end credits thing, because of the Marvel DNA. We didn't want people to leave the movie disappointed.\"\n\nHenry Jackman composed the score for the film. The soundtrack features an original song titled \"Immortals\" written and recorded by American rock band Fall Out Boy, which was released by Walt Disney Records on October 14, 2014. The soundtrack album was digitally released by Walt Disney Records on November 4, 2014, and had a CD release on November 24. While not part of the soundtrack, a brief instrumental section of \"Eye of the Tiger\" plays in the film.\n\n\"Big Hero 6\" premiered on October 23, 2014 as the opening film at the Tokyo International Film Festival. The world premiere of \"Big Hero 6\" in 3D took place at the Abu Dhabi Film Festival on October 31, 2014. It was theatrically released in the United States and Canada on November 7, 2014 with limited IMAX international showings. Theatrically, the film was accompanied by the Walt Disney Animation Studios short, \"Feast\".\n\nFor the South Korean release of the film, it was retitled \"Big Hero\", to avoid the impression of being a sequel, and edited to remove indications of the characters' Japanese origin. This is owing to the tense relations between Korea and Japan. For instance, the protagonist's name, Hiro Hamada, was changed to \"Hero Armada\", and Japanese-language signage onscreen was changed to English. Nonetheless, the film caused some online controversy in South Korea, because of small images resembling the Rising Sun Flag in the protagonist's room.\n\nThe film was released in China on February 28, 2015.\n\n\"Big Hero 6\" was released in the United States by Walt Disney Studios Home Entertainment on Blu-ray and DVD on February 24, 2015. Writer Steven T. Seagle, who co-created the comic book \"Big Hero 6\", criticized the Blu-ray featurette documenting the origins of the group, for not mentioning him or co-creator Duncan Rouleau. Seagle also criticized the book \"Art of Big Hero 6\" for the same omission.\n\n\"Big Hero 6\" earned $222.5 million in North America and $435.3 million in other territories for a worldwide estimated total of $657.8 million. Calculating in all expenses, \"Deadline\" estimated that the film made a profit of $187.34 million. Worldwide, it is the highest-grossing animated film of 2014, the third-highest-grossing non-Pixar animated film from Disney, and the 16th-highest-grossing animated film of all time. By grossing over $500 million worldwide, it became the fourth Disney release of 2014 to do so; the other titles being \"Guardians of the Galaxy\", \"Maleficent\", and \"\".\n\nIn the U.S. and Canada, the film is the second-highest-grossing science-fiction animated film (behind 2008's \"WALL-E\"), the second-highest-grossing animated superhero comedy film (behind 2004's \"The Incredibles\"), and the second-highest-grossing Disney animated film (behind 2013's \"Frozen\"). The film earned $1.4 million from late Thursday night showings, which is higher than the previews earned by \"Frozen\" ($1.2 million) and \"The Lego Movie\" ($400,000). In its opening day on November 7, the film earned $15.8 million, debuting at number two behind \"Interstellar\" ($16.9 million). \"Big Hero 6\" topped the box office in its opening weekend, earning $56.2 million from 3,761 theaters ahead of \"Interstellar\" ($47.5 million); it is Walt Disney Animation Studios' second-best opening behind \"Frozen\" ($67.4 million), both adjusted and unadjusted.\n\nOn February 15, 2015, \"Big Hero 6\" became the third-highest-grossing Disney animated film in both the U.S. and Canada, behind \"The Lion King\" and \"Frozen\".\n\nTwo weeks ahead of its North American release, \"Big Hero 6\" was released in Russia (earned $4.8 million) and Ukraine (earned $0.2 million) in two days (October 25–26). The main reason behind the early release was in order to take advantage of the two weeks of school holidays in Russia. Jeff Bock, box office analyst for \"Exhibitor Relations\", said \"For a two-day gross, that's huge. It's a giant number in Russia.\" In its second weekend, the film added $4.8 million (up 1%) bringing its total nine-day cumulative audience to $10.3 million in Russia and $10.9 including its revenue from Ukraine.\n\nIn its opening weekend, the film earned $7.6 million from seventeen markets for a first weekend worldwide total of $79.2 million, behind \"Interstellar\" ($132.2 million). It went to number one in the Philippines, Vietnam, and Indonesia. It opened with $4.8 million in Mexico. In Japan, where the film is locally known as \"Baymax\", it opened at second place behind \"\", with $5.3 million, marking it the second-biggest Disney opening in Japan behind \"Frozen\". and topped the box office for six consecutive weekends. The film opened in second place with $6 million ($6.8 million including previews) in the U.K., which is 15% lower than \"Frozen\". It opened at No. 1 with $14.8 million in China, which is the biggest opening for a Disney and Pixar animated film (breaking \"Frozen\" record) and topped the box office for three consecutive weekends.\n\nThe film became the highest-grossing Disney animated film in Vietnam and in China (surpassed by \"Zootopia\")), the second-highest-grossing Disney animated film of all time in Russia, in the Philippines (behind \"Toy Story 3\"), and in Japan (behind \"Frozen\"). In addition to being the second-highest-grossing Disney animated film, it is also the fifth-highest-grossing animated film of all time in China. In total earnings, its biggest markets outside of the United States and Canada are China ($83.5 million) and Japan ($76 million).\n\nThe review aggregation website Rotten Tomatoes reports that 89% of critics gave the film a positive review based on 211 reviews, with an average score of 7.3/10. The site's consensus states: \"Agreeably entertaining and brilliantly animated, \"Big Hero 6\" is briskly-paced, action-packed, and often touching.\" Metacritic, which assigns a normalized rating out of 100 from top reviews from mainstream critics, calculated a score of 74 based on 38 reviews, indicating \"generally favorable reviews\".\n\nMichael O'Sullivan of \"The Washington Post\" gave the film 3.5/4 stars, writing that \"The real appeal of \"Big Hero 6\" isn't its action. It's the central character's heart.\" Maricar Estrella of \"Fort Worth Star-Telegram\" gave the film 5 stars, saying it \"offers something for everyone: action, camaraderie, superheroes and villains. But mostly, Baymax offers a compassionate and healing voice for those suffering, and a hug that can be felt through the screen.\" Peter Travers of \"Rolling Stone\" gave the film 3 out of 4 stars, stating, \"The breakthrough star of the season is here. His name is Baymax and he's impossible not to love. The 3-D animated \"Big Hero 6\" would be a ton less fun without this irresistible blob of roly-poly, robot charisma.\" Kofi Outlaw of \"Screen Rant\" gave the film 4 out of 5 stars or \"excellent\", explaining that \"\"Big Hero 6\" combines Disney wonder and charm with Marvel awe and action to deliver a film that exhibits the best of both studios.\" Alonso Duralde of \"The Wrap\" gave the film a positive review, calling it \"sweet and sharp and exciting and hilarious\" and says that the film \"comes to the rescue of what's become a dreaded movie trope—the origin story—and launches the superhero tale to pleasurable new heights.\" Calvin Wilson of \"St. Louis Post-Dispatch\" gave the film 3.5 of 4 stars, writing that \"the storytelling is solid, propelled by characters that you come to care about. And that should make \"Big Hero 6\" a big hit.\"\n\nBill Goodykoontz of \"The Arizona Republic\" gave the film a positive review, writing, \"Directors Don Hall and Chris Williams have made a terrific movie about a boy (Ryan Potter) and his robot friend, who seek answers to a deadly tragedy,\" calling it an \"unexpectedly good treat\". Soren Anderson of \"The Seattle Times\" gave the film 3.5 out of 4 stars, saying that \"Clever, colorful, fast on its feet, frequently very funny and sweet (but not excessively so), \"Big Hero 6\" mixes its myriad influences into a final product that, while in no way original, is immensely entertaining.\" Michael Rechtshaffen of \"The Hollywood Reporter\" gave the film a positive review, saying that \"the funny and heartwarming story about the bond between a teen tech geek and a gentle robot represents another can't-miss proposition by Walt Disney Animation Studios.\" Jon Niccum of \"The Kansas City Star\" gave the film 3.5 out of four stars, writing that while it \"may hit a few familiar beats inherent to any superhero 'origin story,'\" it is still \"the best animated film of the year, supplying \"The Incredibles\"-size adventure with a level of emotional bonding not seen since \"The Iron Giant\"\", and that it \"never runs low on battery power\". Elizabeth Weitzman of the \"Daily News\" gave the film 4 out of 5 stars, calling it a \"charming animated adventure\", saying that with \"appealing 3D animation\" and a smart and \"sharp story and script\", it is \"one of the rare family films that can fairly boast of having it all: humor, heart and huggability\". Rafer Guzmán from \"Newsday\" gave the film 3 out of 4 stars, saying that \"Marvel plus Disney plus John Lasseter equals an enjoyable jumble of kid-approved action\", with \"rich, vivid colors and filled with clever details\".\n\n\n\nVinyl toy company Funko released the first images of the toy figures via their \"Big Hero 6\" Funko. The POP Vinyl series collection features Hiro Hamada, GoGo Tomago, Wasabi, Honey Lemon, Fred, and a 6-inch Baymax.\n\nBandai released a number of action figures related to the film; these toys including a number of different Baymax figures. One is a soft plastic 10-inch version that includes a series of projected stills from the film on his stomach, which can be changed when the figure's arm is moved, and which emits accompanying sounds. Deluxe Flying Baymax, which retails for $39.99, depicts the armored version of the character and features lights and sounds that activate at the push of a button. Placing the Hiro figurine on his back changes the sounds into speech and when the figure is tilted, the sounds are those of flying. The Armor-Up Baymax (original retail cost $19.99) comes with 20 pieces of armor that can be assembled onto the robot by the owner. The other characters from the film, including the other members of team and Professor Callaghan (who is called Yokai) are issued in 4-inch action figures, each of which have eight points of articulation.\n\nOn February 18, 2015, the film's directors, Don Hall and Chris Williams, said a sequel was possible. Hall added, \"Having said that, of course, we love these characters, and the thought of working with them again some day definitely has its appeal.\" In March 2015, Genesis Rodriguez told MTV that a sequel was being considered, saying, \"…There's nothing definitive. There's talks of something happening. We just don't know what yet.\" In April 2015, Stan Lee mentioned a projected sequel as one of several that he understood were in Marvel's plans for upcoming films.\n\nIn March 2016, Disney announced that a \"Big Hero 6\" television series was in development and premiered on Disney Channel and Disney XD in 2017. The series takes place immediately after the events of the film, and is created and executive produced by \"Kim Possible\"s Mark McCorkle and Bob Schooley, and co-executive produced by Nick Filippi. The majority of the cast from the film returned to voice the characters, except for Wayans Jr. and Miller.\n\n"}
{"id": "1739097", "url": "https://en.wikipedia.org/wiki?curid=1739097", "title": "Cage aerial", "text": "Cage aerial\n\nA cage antenna (British cage aerial) is a radio antenna that consists of the top portion of a tower or mast and of several parallel wires, which are radially arranged around the lower part of the mast. One advantage of the cage aerial is that the supporting tower can be grounded, allowing it to be used for other radio services, such as a support for VHF or UHF antennas. A grounded tower also simplifies the installation of aircraft warning lamps. Cage aerials have been built in different variants for broadcasting stations in the longwave and mediumwave bands.\n\nThe cage is electrically one-quarter of the operating wavelength. It is connected to the mast at its upper end. That way it isolates the lower part of the mast (λ/4 stub) and makes the upper part of the mast the radiator. Very often the typical height of such an antenna is no problem as the height of the mast is selected for the TV or FM antennas on top.\n\nExample: At 1000 kHz the wavelength is 300 m. Therefore, the minimum length of the cage antenna is a bit more than 150 m; 75 m for the radiator, 75 m for the cage and a few metres to make the lower end of the cage inaccessible from the ground, as the lower end of the cage carries a very high RF voltage. This type of antenna is known in America as a \"folded unipole\", which has been extensively studied by John H. Mullaney.\n"}
{"id": "1256197", "url": "https://en.wikipedia.org/wiki?curid=1256197", "title": "Canada Geographic Information System", "text": "Canada Geographic Information System\n\nThe Canada Geographic Information System (CGIS) was an early geographic information system (GIS) developed for the Government of Canada beginning in the early 1960s. CGIS was used to store geospatial data for the Canada Land Inventory and assisted in the development of regulatory procedures for land-use management and resource monitoring in Canada.\n\nAt that time, Canada was beginning to realize problems associated with its large land mass and attempting to discern the availability of natural resources. The federal government decided to launch a national program to assist in management and inventory of its resources. The simple automated computer processes designed to store and process large amounts of data enabled Canada to begin a national land-use management program and become a foremost promoter of geographic information systems (GIS).\n\nCGIS was designed to withstand great amounts of collected data by managing, modeling, and analyzing this data very quickly and accurately. As Canada presented such large geospatial datasets, it was necessary to be able to focus on certain regions or provinces in order to more effectively manage and maintain land-use. CGIS enabled its users to effectively collect national data and, if necessary, break it down into provincial datasets. Early applications of CGIS benefited land-use management and environmental impact monitoring programs across Canada.\n\nIn 1960, Roger Tomlinson was working at Spartan Air Services, an aerial survey company based in Ottawa, Ontario. The company was focused on producing large-scale photogrammetric and geophysical maps, mostly for the Government of Canada. In the early 1960s, Tomlinson and the company were asked to produce a map for site-location analysis in an east African nation. Tomlinson immediately recognized that the new automated computer technologies might be applicable and even necessary to complete such a detail-oriented task more effectively and efficiently than humans. Eventually, Spartan met with IBM offices in Ottawa to begin developing a relationship to bridge the previous gap between geographic data and computer services. Tomlinson brought his geographic knowledge to the table as IBM brought computer programming and data management.\nThe Government of Canada began working towards the development of a national program after a 1962 meeting between Tomlinson and Lee Pratt, head of the Canada Land Inventory (CLI). Pratt was charged with creation of maps covering the entire region of Canada's commercially productive areas by showing agriculture, forestry, wildlife, and recreation, all with the same classification schemes. Not only was the development of such maps a formidable task, but Pratt understood that computer automation may assist in the analytical processes as well. Tomlinson was the first to produce a technical feasibility study on whether computer mapping programs would be a viable solution for the land-use inventory and management programs, such as CLI. He is also given credit for coining the term \"geographic information system\" and is recognized as the \"Modern Father of GIS.\"\n\nCGIS continued to be developed and operated as a stand alone system by the Government of Canada until the late 1980s, at which point the widespread emergence of commercial GIS software slowly rendered it obsolete. In the early 1990s, a group of volunteers successfully extracted all of the data from the old computer tapes, and the data made available on GeoGratis.\n\n\n"}
{"id": "31557649", "url": "https://en.wikipedia.org/wiki?curid=31557649", "title": "CineGrid", "text": "CineGrid\n\nCineGrid is a non-profit organization based in California that began in 2004 in order to distribute 1 Gbit/s to 1 Pbps (Petabit per second) digital networks, utilize grid computing to manage digital media applications, and increase the demand for digital media exchange among remote participants in science, education, research, art and entertainment. With its headquarters at the California Institute for Telecommunications and Information Technology, renamed in 2013 as Qualcomm Institute (Qi) in San Diego, California CineGrid is composed of 61 separate organizations and corporations. CineGrid has facilitated grants for its members through the National Science Foundation and provides access to Ethernet VLAN and TCP/IP connectivity.\n\nCineGrid is composed of innovators from scientific, artistic, and technological backgrounds. Its founding members include:\n\n\nEvery year CineGrid holds a conference for its member at the University of California, San Diego. At the annual meeting members are able to present their current research and technological developments. Workshops and demonstrations are held to introduce and give members first hand experience with new networking tools and multi-media advancements. The CineGrid International Workshop Programs include several demonstrations from the CRCA Visiting Artist Lab, CRCA Spatialized Audio Lab, AESOP Wall, Hi-Per Wall, SAGE, StarCAVE, LAVID, Laboratory of Cinematic Arts (LabCine) and the Virtulab.\n\n"}
{"id": "17606012", "url": "https://en.wikipedia.org/wiki?curid=17606012", "title": "Coupling nut", "text": "Coupling nut\n\nA coupling nut, also known as extension nut, is a threaded fastener for joining two male threads, most commonly a threaded rod, but also pipes. The outside of the fastener is usually a hex so a wrench can hold it. Variations include \"reducing coupling nuts\", for joining two different size threads; \"sight hole coupling nuts\", which have a sight hole for observing the amount of engagement; and coupling nuts with left-handed threads. These are used to make up long rod assemblies from shorter lengths of rods. The rods are threaded into the coupling nut for a certain distance. Coupling nut dimensions are described by the Industrial Fasteners Institute in standard IFI-128. It is given a plain finish. It is either cold drawn or hot rolled depending on the bar stock used. Coupling nuts threaded with two different thread sizes and profiles are called reducing coupling nuts.\n\nCoupling nuts can be used to tighten a rod assembly inward or to press a rod assembly outward.\n"}
{"id": "7206492", "url": "https://en.wikipedia.org/wiki?curid=7206492", "title": "Directive on the legal protection of designs", "text": "Directive on the legal protection of designs\n\nDirective 98/71/EC of the European Parliament and of the Council of 13 October 1998 on the legal protection of designs \nis a European Union directive in the field of industrial design rights, made under the internal market \nprovisions of the Treaty of Rome. It sets harmonised standards for eligibility and protection of most types of \nregistered design.\n\nA design is defined as \"the appearance of the whole or a part of a product resulting from the features of, in particular, \nthe lines, contours, colours, shape, texture and/or materials of the product itself and/or its ornamentation\" (Art. 2). \nDesigns may be protected if:\nWhere a design forms part of a more complex product, the novelty and individual character of the design are judged on the part \nof the design which is visible during normal use.\n\nDesigns are not protected insofar as their appearance is wholly determined by their technical function, or by the need to \ninterconnect with other products to perform a technical function (the \"must-fit\" exception). However modular systems such as \nLego or Mechano may be protected [Art. 8(3)].\n\nThe holder of a registered design right has the exclusive right to authorise or prohibit others from using the design in any \nway, notably by producing, importing, selling or using products based on the design. However, rightholders may not prevent \nprivate and non-commercial use, use for research or use for teaching. There is also an exception for foreign-registered ships \nand aeroplanes, based on the principles of maritime sovereignty.\n\nProtection under a registered design right last initially for one or more periods of five years, and may be renewed up to a \nmaximum total of twenty-five years. In respect of a given product, they are exhausted when it is sold with the consent of the \nrightholder (the first-sale doctrine).\n\nProtection by a registered design right does not affect any other intellectual property rights in the product, notably \nunregistered design rights, patents and trade marks. The question of copyright protection is left to the \nlaws of the Member States, which apply varying criteria of originality to the copyright protection of \"applied art\"; the point, however, is that the existence of the registered design right does not stop the design also being eligible for copyright protection.\n\nThe Directive leaves the question of component parts mostly without harmonisation, given the widely varying practices between \nMember States.\n\n\n"}
{"id": "20788787", "url": "https://en.wikipedia.org/wiki?curid=20788787", "title": "Down-the-hole drill", "text": "Down-the-hole drill\n\nA down-the-hole drill, usually called DTH by most professionals, is basically a mini jackhammer screwed on the bottom of a drill string. The fast hammer action breaks hard rock into small flakes and dust and is blown clear by the air exhaust from the DTH hammer. The DTH hammer is one of the fastest ways to drill hard rock. Now smaller portable drillcat drilling rigs with DTH hammers can drill as fast as much larger truck rigs with this newer technology. The system is thought to have been invented independently by Stenuick Frères in Belgium and Ingersoll Rand in the USA in the mid-1950s.\n\nA pneumatic tool is first thought to have been used for rock drilling in 1844. Many quarries used hand held tools that required the driller to suspend himself from a rope over the quarry face in order to place the drill hole in the required position. This system used small diameter holes and was not only terribly inefficient, but very dangerous due to flying rock as a result of the inaccuracy of the drilled borehole.\n\nSome quarries used primitive top hammer machines that carried the jack hammer on a mast - the slenderness of the drill rods working with a relatively large diameter drill bit caused bore holes to deviate which sometimes meant that a bore hole might finish dangerously close to its neighbour or indeed be closer to the face of the quarry than had been intended. In any event boreholes that are not aligned correctly which are then loaded with high explosive can be extremely dangerous, resulting in rock being projected beyond the intended site.\n\nLarger quarries used big rotary machines that required huge amounts of down thrust and high rotation speeds to drive the tri-cone bit hard enough to crush the rock. This system could not be successfully used for holes below 6 inches (150mm) and the machines were very expensive to buy and to run. Another system in use was the very primitive cable tool machine (or bash and splash as it was known by the drillers) which caused a heavy bar and chisel to be lifted and dropped on the rock to crush it whilst water was introduced to create a slurry, which in the process, enabled the hole to be drilled. This system could not guarantee a finished hole size and only pure vertical holes could be drilled as the system basically relied on gravity. Debris from the hole was baled out using a baling tube with a clack valve, which was periodically dropped on a winch to capture the slurry, which was then brought to the top of the hole to be discharged.\n\nIt was only when the DTH system came along that many of the problems associated with the other systems were overcome - with the DTH system the energy source is constantly behind the drill bit, the drill tubes (or drill string) are rigid being only slightly less in diameter than the drill bit, copious amounts of air can be passed through the drill string to operate the DTH Hammer which is then used to efficiently flush the bore hole clean. DTH did not require heavy down thrusts or high rotational speeds and as such a light, cheap machine could be employed to carry out the drilling process - the machine could also be worked by one man, whereas some other systems required two operatives. The benefits that DTH brought to the industry were enormous - for the first time a drill hole could be placed where it was required because DTH gave a truly aligned, straight, accurately placed, clean bore hole that could be easily charged with explosive to provide good control over the blasting process that was safer and which provided good fragmentation of the rock. Holes could be drilled to increasing depths without the loss of performance since the energy source was always directly behind the drill bit. The system was able to drill in almost all rock conditions that other systems were unable to do. Quarry faces became safer, well profiled and quarry floors were level and easier for loading equipment to operate and move across.\n\nThe DTH system completely revolutionised the blast hole industry with many quarries embracing it with open arms. Eventually the larger DTH systems then found their way into other applications, such as water well drilling and construction work.\n\nIt still offers the same benefits to the operator that it initially brought to the quarry industry but it is now being used in many different applications such as gold exploration, ground consolidation, geo-thermal drilling, shallow oil and gas well, directional and piling. The advent of tungsten carbide for the drill bits (the first bits were all-steel) and the development of the button drill bit coupled with the introduction of high air pressures (25 bar plus) has meant that the DTH system can compete easily and efficiently with other drilling systems.\n\nDTH tools were used to locate the trapped miners in Chile and enabled food, water, and medicine to be passed to them and communication systems to be set up that eventually led to their safe rescue.\n\nDTH is short for “down-the-hole”. Since the DTH method was originally developed to drill large-diameter holes downwards in surface-drilling applications, its name originated from the fact that the percussion mechanism followed the bit down into the hole. Applications were later found for the DTH method underground, where the direction of drilling is generally upwards instead of downwards.\n\nIn \"DTH\" drilling, the percussion mechanism – commonly called the hammer – is located directly behind the drill bit. The drill pipes transmit the necessary feed force and rotation to hammer and bit plus compressed air or fluids for the hammer and flushing of cuttings. The drill pipes are added to the drill string successively behind the hammer as the hole gets deeper. The piston strikes the impact surface of the bit directly, while the hammer casing gives straight and stable guidance of the drill bit. This means that the impact energy does not have to pass through any joints at all. The impact energy therefore is not lost in joints allowing for much deeper percussion drilling. This is a great breakthrough for smaller portable water well drilling rigs, that before were limited. The DTH on smaller rigs now can get same results as large heavy truck rigs.\n\nWith recent advances in technology DTH hammers and bits can now be operated to run at up to 500Psi, increasing the penetration speed.\n\nDTH drilling is used in the construction industry to produce piles into rock, also water wells, and drilling bores for geothermal ground source heat pumps.\n\nDTH products can be used in the following applications:\n\nMining- Drill & Blast holes in Open Pit mining, Where the drill operator will drill several holes, then fill with explosives and detonate to lift rock allowing access to ore body\n\nRC- Exploration & Pit grade control\n\nGW- Geothermal Bore Holes & Waterwells\n\nOil & Gas- Deepwell Bore Holes : Air hammers can be used as long as cutting uplift and borehole stability are ensured. For deeper wells, new DTH technologies including water hammer and mudhammer can be used to improve drilling rates in hard rocks.\n\nConstruction- Piling, Footings\n\n"}
{"id": "42576174", "url": "https://en.wikipedia.org/wiki?curid=42576174", "title": "FedACH", "text": "FedACH\n\nFedACH is the Federal Reserve Banks' Automated Clearing House (ACH) service. In 2007, FedACH processed about 37 million transactions per day with an average aggregate value of about $58 billion. For comparison, Fedwire processed about 537,000 transactions per day valued at nearly $2.7 trillion in the same year.\n"}
{"id": "298223", "url": "https://en.wikipedia.org/wiki?curid=298223", "title": "Glassblowing", "text": "Glassblowing\n\nGlassblowing is a glassforming technique that involves inflating molten glass into a bubble (or parison) with the aid of a blowpipe (or blow tube). A person who blows glass is called a glassblower, glassmith, or gaffer. A lampworker (often also called a glassblower or glassworker) manipulates glass with the use of a torch on a smaller scale, such as in producing precision laboratory glassware out of borosilicate glass.\n\nAs a novel glass forming technique created in the middle of the 1st century BC, glassblowing exploited a working property of glass that was previously unknown to glassworkers; inflation, which is the expansion of a molten blob of glass by introducing a small amount of air to it. That is based on the liquid structure of glass where the atoms are held together by strong chemical bonds in a disordered and random network, therefore molten glass is viscous enough to be blown and gradually hardens as it loses heat.\n\nTo increase the stiffness of the molten glass, which in turn facilitates the process of blowing, there was a subtle change in the composition of glass. With reference to their studies of the ancient glass assemblages from Sepphoris of Israel, Fischer and McCray postulated that the concentration of natron, which acts as flux in glass, is slightly lower in blown vessels than those manufactured by casting. Lower concentration of natron would have allowed the glass to be stiffer for blowing.\n\nDuring blowing, thinner layers of glass cool faster than thicker ones and become more viscous than the thicker layers. That allows production of blown glass with uniform thickness instead of causing blow-through of the thinned layers.\n\nA full range of glassblowing techniques was developed within decades of its invention. The two major methods of glassblowing are free-blowing and mold-blowing.\n\nThis method held a pre-eminent position in glassforming ever since its introduction in the middle of the 1st century BC until the late 19th century, and is still widely used nowadays as a glassforming technique, especially for artistic purposes. The process of free-blowing involves the blowing of short puffs of air into a molten portion of glass called a '\"gather\" which has been spooled at one end of the blowpipe. This has the effect of forming an elastic skin on the interior of the glass blob that matches the exterior skin caused by the removal of heat from the furnace. The glassworker can then quickly inflate the molten glass to a coherent blob and work it into a desired shape.\n\nResearchers at the Toledo Museum of Art attempted to reconstruct the ancient free-blowing technique by using clay blowpipes. The result proved that short clay blowpipes of about facilitate free-blowing because they are simple to handle and to manipulate and can be re-used several times. Skilled workers are capable of shaping almost any vessel forms by rotating the pipe, swinging it and controlling the temperature of the piece while they blow. They can produce a great variety of glass objects, ranging from drinking cups to window glass.\n\nAn outstanding example of the free-blowing technique is the Portland Vase, which is a cameo manufactured during the Roman period. An experiment was carried out by Gudenrath and Whitehouse with the aim of re-creating the Portland Vase. A full amount of blue glass required for the body of the vase was gathered on the end of the blowpipe and was subsequently dipped into a pot of hot white glass. Inflation occurred when the glassworker blew the molten glass into a sphere which was then stretched or elongated into a vase with a layer of white glass overlying the blue body.\n\nMold-blowing was an alternative glassblowing method that came after the invention of free-blowing, during the first part of the second quarter of the 1st century AD. A glob of molten glass is placed on the end of the blowpipe, and is then inflated into a wooden or metal carved mold. In that way, the shape and the texture of the bubble of glass is determined by the design on the interior of the mold rather than the skill of the glassworker.\n\nTwo types of molds, namely single-piece mold and multi-piece mold, are frequently used to produce mold-blown vessels. The former allows the finished glass object to be removed in one movement by pulling it upwards from the single-piece mold and is largely employed to produce tableware and utilitarian vessels for storage and transportation. Whereas the latter is made in multi-paneled mold segments that join together, thus permitting the development of more sophisticated surface modeling, texture and design.\n\nThe Roman leaf beaker which is now on display in the J. Paul Getty Museum was blown in a three-part mold decorated with the foliage relief frieze of four vertical plants. Meanwhile, Taylor and Hill tried to reproduce mold-blown vessels by using three-part molds made of different materials. The result suggested that metal, in particular bronze, molds are more effective in producing high-relief design on glass than plaster molds and wooden molds.\n\nThe development of the mold-blowing technique has enabled the speedy production of glass objects in large quantity, thus encouraging the mass production and widespread distribution of glass objects.\n\nThe transformation of raw materials into glass takes place around ; the glass emits enough heat to appear almost white hot. The glass is then left to \"fine out\" (allowing the bubbles to rise out of the mass), and then the working temperature is reduced in the furnace to around . At this stage, the glass appears to be a bright orange color. Though most glassblowing is done between , \"soda-lime\" glass remains somewhat plastic and workable as low as . Annealing is usually done between .\nGlassblowing involves three furnaces. The first, which contains a crucible of molten glass, is simply referred to as the furnace. The second is called the glory hole, and is used to reheat a piece in between steps of working with it. The final furnace is called the lehr or annealer, and is used to slowly cool the glass, over a period of a few hours to a few days, depending on the size of the pieces. This keeps the glass from cracking or shattering due to thermal stress. Historically, all three furnaces were contained in one structure, with a set of progressively cooler chambers for each of the three purposes.\nThe major tools used by a glassblower are the blowpipe (or blow tube), punty (or punty rod, pontil, or mandrel), bench, marver, blocks, jacks, paddles, tweezers, newspaper pads, and a variety of shears.\n\nThe tip of the blowpipe is first preheated; then dipped in the molten glass in the furnace. The molten glass is \"gathered\" onto the end of the blowpipe in much the same way that viscous honey is picked up on a honey dipper. This glass is then rolled on the marver, which was traditionally a flat slab of marble, but today is more commonly a fairly thick flat sheet of steel. This process, called marvering, forms a cool skin on the exterior of the molten glass blob, and shapes it. Then air is blown into the pipe, creating a bubble. Next, the glassworker can gather more glass over that bubble to create a larger piece. Once a piece has been blown to its approximate final size, the bottom is finalized. Then, the molten glass is attached to a stainless steel or iron rod called a punty for shaping and transferring the hollow piece from the blowpipe to provide an opening and/or to finalize the top.\n\nThe bench is a glassblower's workstation, and has a place for the glassblower to sit, a place for the handheld tools, and two rails that the pipe or punty rides on while the blower works with the piece. Blocks are ladle-like tools made from water-soaked fruitwood, and are used similarly to the marver to shape and cool a piece in the early steps of creation. In similar fashion, pads of water-soaked newspaper (roughly square, thick), held in the bare hand, can be used to shape the piece. Jacks are tools shaped somewhat like large tweezers with two blades, which are used for forming shape later in the creation of a piece. Paddles are flat pieces of wood or graphite used for creating flat spots such as a bottom. Tweezers are used to pick out details or to pull on the glass. There are two important types of shears, straight shears and diamond shears. Straight shears are essentially bulky scissors, used for making linear cuts. Diamond shears have blades that form a diamond shape when partially open. These are used for cutting off masses of glass.\n\nThere are many ways to apply patterns and color to blown glass, including rolling molten glass in powdered color or larger pieces of colored glass called frit. Complex patterns with great detail can be created through the use of cane (rods of colored glass) and murrine (rods cut in cross-sections to reveal patterns). These pieces of color can be arranged in a pattern on a flat surface, and then \"picked up\" by rolling a bubble of molten glass over them. One of the most exacting and complicated caneworking techniques is \"reticello\", which involves creating two bubbles from cane, each twisted in a different direction and then combining them and blowing out the final form.\n\nA lampworker, usually operating on a much smaller scale, historically used alcohol lamps and breath or bellows-driven air to create a hot flame at a workbench to manipulate preformed glass rods and tubes. These stock materials took form as laboratory glassware, beads, and durable scientific \"specimens\"—miniature glass sculpture. The craft, which was raised to an art form in the late 1960s by Hans Godo Frabel (later followed by lampwork artists such as Milon Townsend and Robert Mickelson), is still practiced today. The modern lampworker uses a flame of oxygen and propane or natural gas. The modern torch permits working both the soft glass from the furnace worker and the borosilicate glass (low-expansion) of the scientific glassblower. This latter worker may also have multiple headed torches and special lathes to help form the glass or fused quartz used for special projects.\n\nThe earliest evidence of glassblowing was found by Roman Ghirshman in Chogha Zanbil, where many glass bottles were found in the excavations of the 2nd millennium BC site. Later evidence comes from a collection of waste from a glass shop, including fragments of glass tubes, glass rods and tiny blown bottles, which was dumped in a mikvah, a ritual bath in the Jewish Quarter of the Old City of Jerusalem, dated from 37 to 4 BC. Some of the glass tubes recovered are fire-closed at one end and are partially inflated by blowing through the open end while still hot to form a small bottle; thus they are considered as a rudimentary form of blowpipe.\n\nHence, tube blowing not only represents the initial attempts of experimentation by glassworkers at blowing glass, it is also a revolutionary step that induced a change in conception and a deep understanding of glass. Such inventions swiftly eclipsed all other traditional methods, such as casting and core-forming, in working glass.\n\nThe invention of glassblowing coincided with the establishment of the Roman Empire in the 1st century BC, which enhanced the spread and dominance of this new technology. Glassblowing was greatly supported by the Roman government (although Roman citizens could not be \"in trade\", in particular under the reign of Augustus), and glass was being blown in many areas of the Roman world. On the eastern borders of the Empire, the first large glass workshops were set up by the Phoenicians in the birthplace of glassblowing in contemporary Lebanon and Israel as well as in the neighbouring province of Cyprus.\n\nEnnion for example, was among one of the most prominent glassworkers from Lebanon of the time. He was renowned for producing the multi-paneled mold-blown glass vessels that were complex in their shapes, arrangement and decorative motifs. The complexity of designs of these mold-blown glass vessels illustrated the sophistication of the glassworkers in the eastern regions of the Roman Empire. Mold-blown glass vessels manufactured by the workshops of Ennion and other contemporary glassworkers such as Jason, Nikon, Aristeas, and Meges, constitutes some of the earliest evidence of glassblowing found in the eastern territories.\n\nEventually, the glassblowing technique reached Egypt and was described in a fragmentary poem printed on papyrus which was dated to 3rd century AD. The Roman hegemony over the Mediterranean areas resulted in the substitution of glassblowing for earlier Hellenistic casting, core-forming and mosaic fusion techniques. The earliest evidence of blowing in Hellenistic work consists of small blown bottles for perfume and oil retrieved from the glass workshops on the Greek island of Samothrace and at Corinth in mainland Greece which were dated to the 1st century AD.\n\nLater, the Phoenician glassworkers exploited their glassblowing techniques and set up their workshops in the western territories of the Roman Empire, first in Italy by the middle of the 1st century AD. Rome, the heartland of the Empire, soon became a major glassblowing center, and more glassblowing workshops were subsequently established in other provinces of Italy, for example Campania, Morgantina and Aquileia. A great variety of blown glass objects, ranging from unguentaria (toiletry containers for perfume) to cameo, from tableware to window glass, were produced.\n\nFrom there, escaping craftsmen (who had been forbidden to travel) otherwise advanced to the rest of Europe by building their glassblowing workshops in the north of the Alps (which is now Switzerland), and then at sites in northern Europe in present-day France and Belgium.\n\nOne of the most prolific glassblowing centers of the Roman period was established in Cologne on the river Rhine in Germany by late 1st century BC. Stone base molds and terracotta base molds were discovered from these Rhineland workshops, suggesting the adoption and the application of mold-blowing technique by the glassworkers. Besides, blown flagons and blown jars decorated with ribbing, as well as blown perfume bottles with letters CCAA or CCA which stand for Colonia Claudia Agrippiniensis, were produced from the Rhineland workshops. Remains of blown blue-green glass vessels, for example bottles with a handle, collared bowls and indented beakers, were found in abundance from the local glass workshops at Poetovio and Celeia in Slovenia.\n\nSurviving physical evidence, such as blowpipes and molds which are indicative of the presence of blowing, is fragmentary and limited. Pieces of clay blowpipes were retrieved from the late 1st century AD glass workshop at Avenches in Switzerland. Clay blowpipes, also known as mouthblowers, were made by the ancient glassworkers due to the accessibility and availability of the resources before the introduction of the metal blowpipes. Hollow iron rods, together with blown vessel fragments and glass waste dating to approximately 4th century AD, were recovered from the glass workshop in Mérida of Spain, as well as in Salona in Croatia.\n\nThe glass blowing tradition was carried on in Europe from the medieval period through the Middle Ages to the Renaissance in the demise of the Roman Empire in the 5th century AD. During the early medieval period, the Franks manipulated the technique of glassblowing by creating the simple corrugated molds and developing the claws decoration techniques. Blown glass objects, such as the drinking vessels that imitated the shape of the animal horn were produced in the Rhine and Meuse valleys, as well as in Belgium. The Byzantine glassworkers made mold-blown glass decorated with Christian and Jewish symbols in Jerusalem between late 6th century and the middle of the 7th century AD. Mold-blown vessels with facets, relief and linear-cut decoration were discovered at Samarra in the Islamic lands.\n\nRenaissance Europe witnessed the revitalization of glass industry in Italy. Glassblowing, in particular the mold-blowing technique, was employed by the Venetian glassworkers from Murano to produce the fine glassware which is also known as cristallo. The technique of glassblowing, coupled with the cylinder and crown methods, was used to manufacture sheet or flat glass for window panes in the late 17th century. The applicability of glassblowing was so widespread that glass was being blown in many parts of the world, for example, in China, Japan and the Islamic Lands.\n\nThe Nøstetangen Museum at Hokksund, Norway shows how glass was made according to ancient tradition. The Nøstetangenglassworks had operated there from 1741 to 1777, producing table-glass and chandeliers in the German and English style.\n\nThe \"studio glass movement\" began in 1962 when Harvey Littleton, a ceramics professor, and Dominick Labino, a chemist and engineer, held two workshops at the Toledo Museum of Art, during which they started experimenting with melting glass in a small furnace and creating blown glass art. Littleton promoted the use of small furnaces in individual artists studios(). This approach to glassblowing blossomed into a worldwide movement, producing such flamboyant and prolific artists as Dale Chihuly, Dante Marioni, Fritz Driesbach and Marvin Lipofsky as well as scores of other modern glass artists. Today there are many different institutions around the world that offer glassmaking resources for training and sharing equipment.\n\nWorking with large or complex pieces requires a team of several glassworkers, in a complex choreography of precisely timed movements. This practical requirement has encouraged collaboration among glass artists, in both semi-permanent and temporary working groups.\n\nThe writer Daphne du Maurier was descended from a family of glass-blowers in 18th century France, and she wrote about her forebears in the 1963 historical novel \"The Glass-Blowers\".\n\nThe subject of mystery novelist Donna Leon's \"Through a Glass, Darkly\" is the investigation of a crime in a Venetian glassworks on the island of Murano.\n\n\n"}
{"id": "25540033", "url": "https://en.wikipedia.org/wiki?curid=25540033", "title": "Glossary of Russian and USSR aviation acronyms: Avionics and instruments", "text": "Glossary of Russian and USSR aviation acronyms: Avionics and instruments\n\nThis is a of acronyms and initials used for avionics and aircraft instruments in the Russian federation and formerly the USSR. The Latin-alphabet names are phonetic representations of the Cyrillic originals, and variations are inevitable.\n\n"}
{"id": "4524833", "url": "https://en.wikipedia.org/wiki?curid=4524833", "title": "Hakapik", "text": "Hakapik\n\nA hakapik () is a club, of Norwegian design, used for killing seals. The hakapik is a multipurpose hunting tool—a heavy wooden club, with a hammer head (used to crush a seal's skull), and a hook (used to drag away the carcass) on the end.\n\nRegulation Canadian hakapiks consist of a metal ferrule that weighs at least 340 g (12 oz) with a slightly bent spike not more than 14 cm (5.5 in) in length on one side of the ferrule and a blunt projection not more than 1.3 cm (0.5 in) in length on the opposite side of the ferrule and that is attached to a wooden handle that measures not less than 105 cm (3.4 ft) and not more than 153 cm (5 ft) in length and not less than 3 cm and not more than 5.1 cm (2 in) in diameter.\n\nThe hakapik is favored by sealers because it allows them to kill the seal without damaging the pelt. Further, studies by American veterinary scientists on the use of the hakapik on the seal hunt carried out on Pribilof Islands of Alaska suggested that it is an efficient tool designed to kill the animal quickly and humanely when used correctly. A report by members of the Canadian Veterinary Medical Association in September 2002 confirmed this claim.\n\n"}
{"id": "1382285", "url": "https://en.wikipedia.org/wiki?curid=1382285", "title": "Heliodon", "text": "Heliodon\n\nA heliodon (HEE-leo-don) is a device for adjusting the angle between a flat surface and a beam of light to match the angle between a horizontal plane at a specific latitude and the solar beam. Heliodons are used primarily by architects and students of architecture. By placing a model building on the heliodon’s flat surface and making adjustments to the light/surface angle, the investigator can see how the building would look in the three-dimensional solar beam at various dates and times of day.\n\nThe Earth is a ball in space perpetually intercepting a cylinder of parallel energy rays from the sun. (Think of a tennis ball being held in the wind.) The angle of any earthly site to the solar beam is determined by\n\nThe change due to date is the most difficult to visualize. The Earth’s axis is steady but \"tilted\": the plane that includes the Earth’s equator, which is perpendicular to the axis, is not parallel to the plane that includes the center of the sun and the center of the Earth, called the ecliptic. Think of the Earth as a car on a Ferris wheel. The car’s axis always points “down”, which changes its relation to the center of the wheel. A light at the center of the wheel would touch the bottom of the car at the top of the orbit and the top of the car at the bottom of the orbit. As the Earth orbits, the location of the centerline of the solar cylinder changes, sliding from the Tropic of Cancer (in June) to the Tropic of Capricorn (in December) and back again. This changes sun angles all over Earth according to the date. \"See more at analemma.\"\n\nHeliodons can mimic latitude, time of day, and date. They must also show a clear north-south direction on their surface in order to orient models. Some heliodons are very elaborate, using tracks in a high ceiling to carry a light across a large studio. Others are very simple, using a sundial as a guide to the adjustments and the sun of the day as a light source. In general, the date adjustment causes the most difficulty for the heliodon designer, while the light source presents the most problems in use. The parallel rays of the sun are not easy to duplicate with an artificial light at a useful scale, while the real sun is no respecter of deadlines or class hours.\n\nAll heliodons can benefit by including a moveable, tiltable device that can be set to match any surface on a model to show angle of incidence. The angle of incidence device indicates the relative intensity of the direct beam on the surface. The device consists of a diagram of concentric rings around a shadow-casting pointer perpendicular to the diagram. Each ring represents a percent of the direct solar beam incident on the surface. The percentage varies from 100%—the ray runs straight down the pointer perpendicular to the diagram—to zero—the ray runs parallel to the diagram and misses surface. The cosine of the angle of incidence gives the percentage. A cosine of 0.9, 90%, for example, corresponds to an angle of incidence of 26.84 degrees. The radius of the ring for the angle is equal to its tangent times the height of the shadow casting pointer. A 45 degree angle of incidence would generate a cosine of about .7, 70%, for example. Since the tangent of 45 degrees is 1, the radius of the 70% ring would be equal to the height of the shadow-casting rod.\n\n"}
{"id": "54381240", "url": "https://en.wikipedia.org/wiki?curid=54381240", "title": "ID2020", "text": "ID2020\n\nID2020 is a nonprofit public-private partnership committed to improving lives through digital identity.\n\nAn estimated 1.1 billion people, including many millions of children, women and refugees, globally lack any form of officially recognized identification. Without an identity, individuals are often invisible – unable to vote, access healthcare, open a bank account, or receive an education – and bear higher risk for trafficking. Without accurate population data, public and private organizations struggle to broadly and accurately deliver the most basic human services.\n\nIn 2017, ID2020 established an alliance to bring together governments, public and private sector organizations, including Accenture, Microsoft, Mercy Corps, Hyperledger, and UNICC. The alliance model enables a synchronized approach to digital identity initiatives by enabling diverse stakeholders to work collaboratively and by coordinating funding to support high-impact projects.\n\nID2020’s mission supports the UN Sustainable Development Goal Target 16.9. which aims to provide legal identity for all, including birth registration, by 2030. As proof of one’s identity is a prerequisite to social and economic inclusion in the modern world, an identity must be broadly recognized by both the public- and private-sector organizations with which an individual may interact.\n\nID2020 principles for identity focus on protecting the individual and giving the individual control over their own identity and associated data. These principles apply to any project ID2020 may undertake.\n\nID2020 was launched in 2014 by John Edge, a social entrepreneur specializing in financial technology and digital identity.\n\nID2020’s initial funding came from The Rockefeller Foundation, with further support provided by Accenture. \n"}
{"id": "50224437", "url": "https://en.wikipedia.org/wiki?curid=50224437", "title": "InGeneron", "text": "InGeneron\n\nInGeneron Inc. is a private multinational medical device and biotechnology company headquartered in Houston, Texas, United States, and with European headquarters in Munich, Germany. InGeneron develops, manufactures, licenses and sells regenerative cell separation and diagnostics tools. InGeneron's technologies enable the preparation of adipose-derived regenerative cells (ADRC) that contain progenitor and stem cells in humans and animals for autologous use.\n\nInGeneron was founded in 2006, through a recapitalization of aDEPtas, Inc. in Houston, Texas with the focus on translating fundamental research in the regenerative medicine field into clinical practice. Over 40 peer-reviewed studies have been published by InGeneron's team in collaboration with various renowned academic facilities. In 2010 the fully owned subsidiary InGeneron GmbH in Munich was founded to serve as its European headquarters. In 2015 InGeneron Equine GmbH was founded in Germany, which focuses on treating orthopedic indications of sports horses. In 2015, the German-based stem cell banking company eticur) was acquired with the idea of supporting eticur) with scientific evidence from InGeneron's laboratory. In 2012, InGeneron's technology was also used to treat a Malayan tiger at the Houston zoo. In March 2017, InGeneron raised $20 million in Series D funding from Sanford Health.\n\nInGeneron develops and manufactures biomedical equipment that is designed to fit into the workflow of an operating physician for the easy recovery of regenerative cells from the adipose tissue.\n\nAn efficient point-of-care processing system that isolates regenerative cells from adipose tissue has been developed. This system relies on a proprietary enzyme blend and centrifuge technology to isolate and prepare regenerative cells with high yield from adipose tissue within one hour after extraction.\n\nInitial results in the veterinary space show that possible treatment areas include orthopedic injuries, osteoarthritis, incontinence, soft tissue reconstruction and non-healing wounds in animals.\n\nConsisting of the disposables for processing and the Tissue Processing Unit, the Transpose™ RT System is InGeneron's system that allows for point-of-care preparation of regenerative cells. The collected tissue is processed with the InGeneron Tissue Processing Unit, which is a semi-automated, heatable centrifuge that is developed to process tissue prior to stem cell extraction and uses proprietary agitation to assist in the separation of the regenerative cells.\n\nThis system is used in conjunction with the Matrase™ enzyme that ensures a high yield of regenerative cells. The proprietary enzyme is designed to quickly and gently release from adipose tissue the adult regenerative cells.\n\nFor the extraction and processing, InGeneron provides: \nFor veterinarian use, InGeneron provides:\n\nThe InGeneron Processing Unit as well as disposable packs such as the Lipoaspiration Collection Kit, Transpose RT Lipoaspirate Processing Kit and the SmartGraft Systems are CE marked and meet the requirements of the applicable EC directive.\n\nThe Matrase™ enzymatic agent is manufactured mammalian origin free and in strict compliance with cGMP guidelines.\n\nThe SmartGraft™ 30 and SmartGraft™ 200 System have 510(k) approval in the US market.\n"}
{"id": "17641424", "url": "https://en.wikipedia.org/wiki?curid=17641424", "title": "Induced gas flotation", "text": "Induced gas flotation\n\nInduced gas flotation (IGF) is a water treatment process that clarifies wastewaters (or other waters) by the removal of suspended matter such as oil or solids. The removal is achieved by injecting gas bubbles into the water or wastewater in a flotation tank or basin. The small bubbles adhere to the suspended matter causing the suspended matter to float to the surface of the water where it may then be removed by a skimming device.\n\nInduced gas flotation is very widely used in treating the industrial wastewater effluents from oil refineries, petrochemical and chemical plants, natural gas processing plants and similar industrial facilities. A very similar process known as \"dissolved air flotation\" is also used for waste water treatment. \"Froth flotation\" is commonly used in the processing of mineral ores.\n\nIGF units in the oil industry do not use air as the flotation medium due to the explosion risk. These IGF units use natural gas or nitrogen to create the bubbles.\n\nThe feed water to the IGF float tank is often (but not always) dosed with a coagulant (such as ferric chloride or aluminum sulfate) to flocculate the suspended matter.\n\nThe bubbles may be generated by an impeller, eductors or a sparger. The bubbles adhere to the suspended matter, causing the suspended matter to float to the surface and form a froth layer which is then removed by a skimmer. The froth-free water exits the float tank as the clarified effluent from the IGF unit.\n\nSome IGF unit designs utilize parallel plate packing material to provide more separation surface and therefore to enhance the separation efficiency of the unit.\n\n\n"}
{"id": "1381391", "url": "https://en.wikipedia.org/wiki?curid=1381391", "title": "Intergalactic travel", "text": "Intergalactic travel\n\nIntergalactic travel is the term used for hypothetical manned or unmanned travel between galaxies. Due to the enormous distances between our own galaxy the Milky Way and even its closest neighbors<wbr>—<wbr>hundreds of thousands to millions of light-years<wbr>—<wbr>any such venture would be far more technologically demanding than even interstellar travel. Intergalactic distances are roughly a hundred-thousandfold (five orders of magnitude) greater than their interstellar counterparts.\n\nThe technology required to travel between galaxies is far beyond humanity's present capabilities, and currently only the subject of speculation, hypothesis, and science fiction.\n\nHowever, theoretically speaking, there is nothing to conclusively indicate that intergalactic travel is impossible. There are several hypothesized methods of carrying out such a journey, and to date several academics have studied intergalactic travel in a serious manner.\n\nDue to the size of the distances involved any serious attempt to travel between galaxies would require methods of propulsion far beyond what is currently thought possible in order to bring a large craft close to the speed of light.\n\nAccording to the current understanding of physics, an object within space-time cannot exceed the speed of light, which means an attempt to travel to any other galaxy would be a journey of millions of earth years via conventional flight.\n\nManned travel at a speed not close to the speed of light, would require either that we overcome our own mortality with technologies like radical life extension or traveling with a generation ship. If traveling at a speed closer to the speed of light, time dilation would allow intergalactic travel in a timespan of decades of on-ship time.\n\nAdditional constraints include the variety of unknowns regarding the durability of a spaceship for such complex travel. Fluctuating temperatures as in the warm-hot intergalactic medium could potentially disintegrate future spacecraft if not properly shielded.\n\nThese challenges also mean a return trip would be very difficult. Therefore, all future studies on the risks and feasibility of intergalactic travel would have to include a wide range of simulations to increase chances of a successful payload.\n\nVoyages to other galaxies at sub-light speeds would require voyage times anywhere from hundreds of thousands to many millions of years. To date only one design such as this has ever been made.\n\nTheorized in 1988, and observed in 2005, there are stars moving faster than the escape velocity of the Milky Way, and are traveling out into intergalactic space. There are several theories for their existence. One of the mechanisms would be that the supermassive black hole at the center of the Milky Way ejects stars from the galaxy at a rate of about one every hundred thousand years. Another theorized mechanism might be a supernova explosion in a binary system.\n\nThese stars travel at speeds up to about 3,000 km/second. However, recently (November 2014) stars going up to a significant fraction of the speed of light have been postulated, based on numerical methods. Called Semi-Relativistic Hypervelocity Stars by the authors, these would be ejected by mergers of supermassive black holes in colliding galaxies. And, the authors think, will be detectable by forthcoming telescopes.\n\nThese could be used by entering into an orbit around them and waiting.\n\nAnother proposal is to artificially propel a star in the direction of another galaxy.\n\nWhile it takes light approximately 2.54 million years to traverse the gulf of space between Earth and, for instance, the Andromeda Galaxy, it would take a much shorter amount of time from the point of view of a traveler at close to the speed of light due to the effects of time dilation; the time experienced by the traveler depending both on velocity (anything less than the speed of light) and distance traveled (length contraction). Intergalactic travel for humans is therefore possible, in theory, from the point of view of the traveller.\n\nAccelerating to speeds closer to the speed of light with a relativistic rocket would allow the on-ship travel time to be drastically lower, but would require very large amounts of energy. A way to do this is space travel using constant acceleration. Traveling to the Andromeda Galaxy, 2 million light years away, would take 28 years on-ship time with a constant acceleration of 1g and a deceleration of 1g after reaching half way, to be able to stop.\n\nGoing to the Andromeda Galaxy at this acceleration would require 4 100 000 kg fuel per kg payload using the unrealistic assumption of a 100% efficient engine that converts matter to energy. Decelerating at the halfway point in order to stop dramatically increases the fuel requirements to 42 trillion kg fuel per kg payload. This is ten times the mass of Mt Everest required in fuel for each kg of payload. As the fuel contributes to the total mass of the ship, carrying more fuel also increases the energy required to travel at a certain acceleration and extra fuel added to make up for the increased mass would further contribute to the problem.\n\nThe fuel requirements of going to the Andromeda Galaxy with constant acceleration means that either the payload has to be very small, the spaceship has to be very large or it has to collect fuel or receive energy on the way through other means (e.g. using a Bussard ramjet).\n\nThe Alcubierre drive is a hypothetical concept that is able to impulse a spacecraft to speeds faster than light (the spaceship itself would not move faster than light, but the space around it would). This could in theory allow practical intergalactic travel. There is no known way to create the space-distorting wave this concept needs to work, but the metrics of the equations comply with relativity and the limit of light speed.\n\n"}
{"id": "906846", "url": "https://en.wikipedia.org/wiki?curid=906846", "title": "Joint Entrance Examination – Advanced", "text": "Joint Entrance Examination – Advanced\n\nJoint Entrance Examination – Advanced (JEE-Advanced), formerly the Indian Institutes of Technology-Joint Entrance Examination (IIT-JEE) is an annual engineering college entrance examination in India. It is\nconducted by one of the seven zonal IITs (IIT Roorkee, IIT Kharagpur, IIT Delhi, IIT Kanpur, IIT Bombay, IIT Madras, and IIT Guwahati ) under guidance of the Joint Admission Board(JAB). It is used as the sole admission test by the 23 Indian Institutes of Technology (IITs). Other universities like the Rajiv Gandhi Institute of Petroleum Technology, Indian Institute of Science Education and Research (IISERs) and the premier Indian Institute of Science (IISc) also use the score obtained in JEE Advanced as the basis for admission. Any student who took admission to IITs could not appear for the JEE-Advanced exam in the next year, but the same was not the case with IISc, IISER, RGIPT and other institutes as these institutes only used JEE Advanced score for admission. The examination is organised each year by one of the various IITs, on a round robin rotation pattern. It has a very low admission rate (about 9,369 in 479,651 in 2012;which was around 1.95%) The latest admission rate in 2017 was around 0.92% in IITs (about 11,000 out of 1,200,000 who applied for JEE Main)\nIt is recognised as one of the toughest examinations in the world and is one of the most difficult examinations of India to qualify.\n\nIn 2013 the exam, originally called the IIT-JEE, was renamed as JEE (Advanced), along with the AIEEE being renamed JEE(Main). From 2017, IITs started conducting JEE internationally to give admission to international students.\n\nThe first IIT, Indian Institute of Technology Kharagpur, started in 1951. In the initial few years (1951-1954) students were admitted on the basis of their academic results followed by an Interview in several Centers across the country. From 1955-1959 admission was via an all India examination held only for IIT Kharagpur (other IITs had not started by then). Branches were allotted through Interviews/counseling held at Kharagpur.\n\nThe common IIT-JEE was conducted for the first time in 1961, when it had four subjects including an English language paper. The examination since evolved considerably from its initial pattern. The IIT-JEE was initially called the Common Entrance Exam (CEE); its creation coincided with that of the 1961 IIT Act.\n\nFrom 1978, The English paper was stopped being taken into account for counting the rank. From 1998, the English exam was completely stopped.\n\nIn 1997, the IIT-JEE was conducted twice after the question paper was leaked in some centers.\n\nBetween 2000 and 2005, an additional screening test was used alongside the main examination, intended to reduce pressure on the main examination by allowing only about 20,000 top candidates to sit for the paper, out of more than 450,000 applicants.\n\nFrom 2002, an additional exam called the AIEEE was introduced and this was used for admissions to institutions other than IITs. In 2012 The AIEEE was changed to JEE(Main) and IIT-JEE to JEE(Advanced); with this, the JEE (Main) became the screening exam for JEE (Advanced).\n\nFrom June 2005, The Hindu newspaper led a campaign for reforming the IIT-JEE to reduce the coaching mania and to improve the gender and socio-economic diversity. Two possible solutions were proposed - either a convergence between the screening test and the All India Engineering Entrance Examination (AIEEE), or a two-tier examination whereby ranks from the first tier can be used for the purposes of gaining admission to the NITs and other engineering colleges in the country.\n\nIn September 2005, an analysis group of directors of all the IITs announced major reforms to the examination. These were implemented from 2006 onwards. . The revised test consisted of a single objective test, replacing the earlier two-test system. In order to be eligible for the main examination, candidates in the general category had to secure a minimum of 60% aggregated marks in the qualifying examination of the XIIth standard organized by various educational boards of India, while candidates belonging to Scheduled Caste (SC), Scheduled Tribe (ST) and Physically Disabled (PD) categories must secure a minimum of 55%.\n\nFrom 2006, the screening exam was abolished with the introduction of 8 new IITs. The exam became fully objective [obviously incorrect statement; new IITs started in 2008, screening exam was removed and single stage multiple-choice exam started in 2006].\n\nIn 2008, the Director and the Dean of IIT Madras called for revisions to the examination, arguing that the coaching institutes were \"enabling many among the less-than-best students to crack the test and keeping girls from qualifying\". They expressed concern that the present system did not allow for applicants' 12 years of schooling to have a bearing on admissions into IITs.\n\nIn 2008, the Indian Institutes of Technology, for the first time, went overseas with their entrance examination as they set up a centre for the competitive test in Dubai. The number of candidates appearing in Dubai hovered around 200 to 220.\n\nThe two-tier reform suggested in 2005 may become a reality as the Indian government has announced plans for a single entrance exam for all engineering colleges from 2018, with students aspiring for the IITs having to pass the nationwide common entrance test (JEE-Main) with high marks and then take the JEE-Advanced to qualify for the IITs.\n\nFrom 2018, JEE(Advanced) started being conducted online.\n\nCandidates satisfying all the following criteria are eligible to appear in JEE(Advanced).\n\nThis list shows the organizing institute of JEE(Advanced) in the recent years.\nThe number of students taking the examination increased substantially each year with over 4,85,000 sitting IIT-JEE 2011. This represented an increase of 30,000 students (6.5%) from 2010. \n\nThe availability of seats in recent years is as shown below:\n\nFrom 2008, six new IITs (IIT Bhubaneshwar, IIT Gandhinagar, IIT Hyderabad, IIT Jodhpur, IIT Patna and IIT Ropar) were opened with 120 seats each, increasing the total number of seats to almost 7000. For 2009, admissions were made to two more IITs, namely IIT Indore and IIT Mandi (Himachal Pradesh) taking the seat count to almost 8300. In 2011, with additional courses in several old and new IITs, the total seat count crossed 9600. IIT Tirupati and IIT Palakkad started functioning in 2015 and four more (IIT Bhilai, IIT Dharwad, IIT Goa, IIT Jammu) started in 2016; along with seat additions in other Institutions, making the 2017 seat count to almost 11000. In 2018, in an effort to ensure minimum female enrolment of 14%, the IITs introduced \"female only\" and \"gender neutral\" seats based on 2017 enrolment and super-numerary seats were created per-Institute per-program to reach the 14% target. With this and slight overall seats increases, the total seats availability crossed 12000, including 801 super-numerary \"female only\" seats.\n\nReservation are provided to Indian Nationals belonging to certain categories (SC, ST, PwD, etc.) and to the girls in accordance with the rules of Government of India and such candidates are declared qualified in JEE(Advanced) 2018 based on relaxed norms.\n\nCandidates qualifying in the JEE(Advanced) are eligible to admission in the 23 IITs if they satisfy \"any one of the following\" criteria.\n\n\nIn 2012, Super 30 founder and mathematician Anand Kumar criticised the New Admission Norms, saying that the decision of the IIT Council to give chance to students having top 20% from various boards in the class 12 examinations, was a decision in haste. \"This is one decision that will go against the poor, who don't have the opportunity to study in elite schools,\" he added.\n\nIIT-JEE was conducted only in English and Hindi, which was criticised as making it harder for students where regional languages, like Tamil, Telugu, Kannada, Urdu, Oriya, Bengali, Marathi, Assamese or Gujarati, are more prominent. In September 2011, the Gujarat High Court acted on a Public Interest Litigation by the Gujarati Sahitya Parishad, demanding the exams be conducted in Gujarati. A second petition was made in October by Navsari's Sayaji Vaibhav Sarvajanik Pustakalaya Trust. Another petition was made at the Madras High Court for conducting the exam in Tamil. In the petition, it was claimed that not conducting the exam in the regional languages is in violation of article 14 of the Constitution of India. PMK, a political party in Tamil Nadu held a demonstration at Chennai for conducting IIT-JEE and other national entrance exams in regional languages also, particularly Tamil in Tamil Nadu. Pattali Makkal Katchi party has filed Public Interest Litigation in Madras High Court for conducting IIT JEE entrance exam in Tamil also. They submitted that every year 7.63 lakh students were completing 12th standard in Tamil Nadu, 75% of them from Tamil Medium. They had to take the entrance exam in English or Hindi, neither of which was their medium of instruction nor their mother tongue, and so were denied their fundamental right to take up the entrance exam in their medium of instruction, based on their mother tongue.Shiv Sena urged MHRD to conduct IITJEE and other national undergraduate entrance exams in regional languages, particularly Marathi language in Maharashtra. Most of the JEE controversies sparked the nation as the exam is very well known. In 2017, the supreme court ordered JAB to put a bar on the ongoing counseling process. There were three questions comprising a total of 11 marks that were unclear.\n\nThere were several changes made in the exam from 2018. The Joint Admission Board (JAB) decided to conduct the entire exam online from 2018 since it reduces chances of paper leak and makes logistics and evaluation easier. It said that the online exam will neutralise the problem of misprinting.\n\nPreparing for the Joint Entrance Exam normally began two years before students take the test. 90% of students who passed this exam attended coaching institutes, which had created a $3.37 billion industry with annual tuitions of up to $1,700. These academies included tests multiple times a week, up to 200 students per class, and long hours, in addition to regular high school work. There were hundreds of academies across the country and the most famous—in Kota, Rajasthan—attracted approximately 125,000 students each year. Coaching programs had become major corporations and were now not only listed on the Indian stock market, but also attracted millions of dollars of investment from private equity firms. The high-pressure environments, with much competition and high expectations, were blamed for the significant number of suicides that occurred in these academies.\n\n"}
{"id": "13255999", "url": "https://en.wikipedia.org/wiki?curid=13255999", "title": "List of academic computer science departments", "text": "List of academic computer science departments\n\nPlease use the discussion tab to see the methodology used to compile this list and what additions should and should not be made to it.\n"}
{"id": "35113370", "url": "https://en.wikipedia.org/wiki?curid=35113370", "title": "List of advertising technology companies", "text": "List of advertising technology companies\n\nThis is a list of online advertising technology companies.\n\n"}
{"id": "52915298", "url": "https://en.wikipedia.org/wiki?curid=52915298", "title": "List of carbon ranches", "text": "List of carbon ranches\n\nThis is a working list of Carbon Ranches a term referring to regenerative agriculture practices that specifically accomplish carbon sequestration in the soil. \"The world’s cultivated soils have lost between 50 and 70 percent of their original carbon stock.\" (Lal) \"A mere two percent increase in the carbon content of the planet’s soils could offset 100 percent of all greenhouse gas emissions going into the atmosphere.\"(Lal) Grasslands cover 350 billion hectares, comprising 70% of global agricultural land making carbon ranching an especially climate beneficial alternative to meat produced as part of a Concentrated animal feeding operation. Carbon ranching may include, but is not limited to Managed intensive rotational grazing, Compost application, Holistic management (agriculture) and Tree planting.\n\nThere are many research groups and NGO's studying and advancing this field including Arizona State University, The Carbon Cycle Institute, Holistic Management International, The Ohio State University, The Quivira Coalition, The Savory Institute, The Soil Carbon Coalition, Texas A&M University. This list does not necessarily reflect scientific or academic endorsements insofar as best practices and results vary across ranches, climates, time periods, and sources of funding and are therefore challenging to quantify and/or compare. Currently, confirmation of the carbon ranching designation is confirmed through either third party citation or Soil Organic Matter test result.\n\nThis is a and may never be able to satisfy particular standards for completeness. You can help by expanding it with entries.\n"}
{"id": "3974681", "url": "https://en.wikipedia.org/wiki?curid=3974681", "title": "Mailgram", "text": "Mailgram\n\nA Mailgram is a type of telegraphic message which is transmitted electronically from the sender to a post office and then printed and delivered to the recipient via postal means. \n\nWestern Union invented the Mailgram in 1970. Service via Westar, Western Union's own communications satellite, was introduced in 1974. \n\nThe advantage of Mailgrams over postal mail was speed and verifiability of transmission; they were widely used in official notifications and legal transactions. Their advantage over full-rate telegrams was lower cost while still maintaining the look and feel of an important Western Union Telegram. The Mailgram quickly became a widely used medium for business-to-consumer communications. \n\nAlthough iTelegram still provides a mailgram service in the United States, Western Union discontinued all telegram messaging, including Mailgram, in 2006.\n\n"}
{"id": "17999781", "url": "https://en.wikipedia.org/wiki?curid=17999781", "title": "Mud pump", "text": "Mud pump\n\nA \"mud pump\" (sometimes referred to as a \"mud drilling pump\" or \"drilling mud pump\"), is a reciprocating piston/plunger pump designed to circulate drilling fluid under high pressure (up to ) down the drill string and back up the annulus. A mud pump is an important part of the equipment used for oil well drilling.\n\nMud pumps can be divided into single-acting pump and double-acting pump according to the completion times of the suction and drainage acting in one cycle of the piston's reciprocating motion.\n\nMud pumps come in a variety of sizes and configurations but for the typical petroleum drilling rig, the triplex (three piston/plunger) mud pump is the pump of choice. Duplex mud pumps (two piston/plungers) have generally been replaced by the triplex pump, but are still common in developing countries. Two later developments are the hex pump with six vertical pistons/plungers, and various quintuplex's with five horizontal piston/plungers. The advantages that these new pumps have over convention triplex pumps is a lower mud noise which assists with better Measurement while drilling (MWD) and Logging while drilling (LWD) decoding.\n\nThe \"normal\" mud pump consists of two main sub-assemblies, the fluid end and the power end.\n\nThe fluid end produces the pumping process with valves, pistons, and liners. Because these components are high-wear items, modern pumps are designed to allow quick replacement of these parts.\n\nTo reduce severe vibration caused by the pumping process, these pumps incorporate both a suction and discharge pulsation dampener. These are connected to the inlet and outlet of the fluid end.\n\nThe power end converts the rotation of the drive shaft to the reciprocating motion of the pistons.\nIn most cases a crosshead crank gear is used for this.\n\nA mud pump is composed of many parts including mud pump liner, mud pump piston, modules, hydraulic seat pullers, and other parts.\nParts of a mud pump:\n1. housing itself,\n2. liner with packing,\n3. cover plus packing,\n4. piston and piston rod,\n5. suction valve and discharge valve with their seats,\n6. stuffing box (only in double-acting pumps),\n7. gland (only in double-acting pumps),\n8. pulsation dampener.\n\nThere are two main parameters to measure the performance of a mud pump: Displacement and Pressure.\n\nDisplacement is calculated as discharged liters per minute, it is related with the drilling hole diameter and the return speed of drilling fluid from the bottom of the hole, i.e. the larger the diameter of drilling hole, the larger the desired displacement. The return speed of drilling fluid should reach the requirement that can wash away the debris and rock powder cut by the drill from the bottom of the hole in a timely manner, and reliably carry them to the earth surface. When drilling geological core, the speed is generally in range of 0.4 to 1.0 m^3/min.\n\nThe pressure size of the pump depends on the depth of the drilling hole, the resistance of flushing fluid (drilling fluid) through the channel, as well as the nature of the conveying drilling fluid. The deeper the drilling hole and the greater the pipeline resistance, the higher the pressure needed.\n\nWith the changes of drilling hole diameter and depth, it requires that the displacement of the pump can be adjusted accordingly. In the mud pump mechanism, the gearbox or hydraulic motor is equipped to adjust its speed and displacement. In order to accurately grasp the changes in pressure and displacement, a flow meter and pressure gauge are installed in the mud pump.\n\n\nThe construction department should have a special maintenance worker that is responsible for the maintenance and repair of the machine. mud pumps and other mechanical equipment should be inspected and maintained on a scheduled and timely basis to find and address problems ahead of time, in order to avoid unscheduled shutdown. The worker should attend to the size of the sediment particles; when finding large particles, the mud pump wearing parts should frequently be checked for repairing needs or replacement. The wearing parts for mud pumps include pump casing, bearings, impeller, piston, liner, etc. Advanced antiwear measures should be adopted to increase the service life of the wearing parts, which can reduce the investment cost of the project, and improve production efficiency. At the same time, wearing parts and other mud pump parts should be repaired rather than replaced when possible.\n\n\n"}
{"id": "240358", "url": "https://en.wikipedia.org/wiki?curid=240358", "title": "NSAKEY", "text": "NSAKEY\n\nIn computer security and cryptography, _NSAKEY was a variable name discovered in Windows NT 4 Service Pack 5 (which had been released unstripped of its symbolic debugging data) in August 1999 by Andrew D. Fernandes of Cryptonym Corporation. That variable contained a 1024-bit public key.\n\nMicrosoft's operating systems require all cryptography suites that work with its operating systems to have a digital signature. Since only Microsoft-approved cryptography suites can be installed or used as a component of Windows, it is possible to keep export copies of this operating system (and products with Windows installed) in compliance with the Export Administration Regulations (EAR), which are enforced by the US Department of Commerce Bureau of Industry and Security (BIS).\n\nIt was already known that Microsoft used two keys, a primary and a spare, either of which can create valid signatures. Microsoft had failed to remove the debugging symbols in ADVAPI32.DLL, a security and encryption driver, when it released Service Pack 5 for Windows NT 4.0, and Andrew Fernandes, chief scientist with Cryptonym, found the primary key stored in the variable _KEY and the second key was labeled _NSAKEY. Fernandes published his discovery, touching off a flurry of speculation and conspiracy theories, including the possibility that the second key was owned by the United States National Security Agency (the NSA) and allowed the intelligence agency to subvert any Windows user's security.\n\nDuring a presentation at the Computers, Freedom and Privacy 2000 (CFP2000) conference, Duncan Campbell, senior research fellow at the Electronic Privacy Information Center (EPIC), mentioned the _NSAKEY controversy as an example of an outstanding issue related to security and surveillance.\n\nIn addition, Dr. Nicko van Someren found a third key in Windows 2000, which he doubted had a legitimate purpose, and declared that \"It looks more fishy\".\n\nMicrosoft denied the speculations on _NSAKEY. \"This report is inaccurate and unfounded. The key in question is a Microsoft key. It is maintained and safeguarded by Microsoft, and we have not shared this key with the NSA or any other party.\" Microsoft said that the key's symbol was \"_NSAKEY\" because the NSA is the technical review authority for U.S. export controls, and the key ensures compliance with U.S. export laws.\n\nRichard Purcell, Microsoft’s Director of Corporate Privacy, approached Campbell after his presentation and expressed a wish to clear up the confusion and doubts about _NSAKEY. Immediately after the conference, Scott Culp, of the Microsoft Security Response Center, contacted Campbell and offered to answer his questions. Their correspondence began cordially but soon became strained; Campbell apparently felt Culp was being evasive and Culp apparently felt that Campbell was hostilely repeating questions that he had already answered. On 28 April 2000, Culp stated that \"we have definitely reached the end of this discussion ... [which] is rapidly spiraling into the realm of conspiracy theory\" and Campbell's further inquiries went unanswered.\n\nMicrosoft claimed the third key was only in beta builds of Windows 2000 and that its purpose was for signing Cryptographic Service Providers.\n\nSome in the software industry question whether the BXA's EAR has specific requirements for backup keys. However, none claim the legal or technical expertise necessary to authoritatively discuss that document. The following theories have been presented.\n\nMicrosoft stated that the second key is present as a backup to guard against the possibility of losing the primary secret key. Fernandes doubts this explanation, pointing out that the generally accepted way to guard against loss of a secret key is secret splitting, which would divide the key into several different parts, which would then be distributed throughout senior management. He stated that this would be far more robust than using two keys; if the second key is also lost, Microsoft would need to patch or upgrade every copy of Windows in the world, as well as every cryptographic module it had ever signed.\n\nOn the other hand, if Microsoft failed to think about the consequences of key loss and created a first key without using secret splitting (and did so in secure hardware which doesn't allow protection to be weakened after key generation), and the NSA pointed out this problem as part of the review process, it might explain why Microsoft weakened their scheme with a second key and why the new one was called _NSAKEY. (The second key might be backed up using secret splitting, so losing both keys needn't be a problem.)\n\nA second possibility is that Microsoft included a second key to be able to sign cryptographic modules outside the United States, while still complying with the BXA's EAR. If cryptographic modules were to be signed in multiple locations, using multiple keys is a reasonable approach. However, no cryptographic module has ever been found to be signed by _NSAKEY, and Microsoft denies that any other certification authority exists.\n\nMicrosoft denied that the NSA has access to the _NSAKEY secret key.\n\nIt was possible to remove the second _NSAKEY using the following (note this was for Windows software in 1999).\n\nThere is good news among the bad, however. It turns out that there is a flaw in the way the \"crypto_verify\" function is implemented. Because of the way the crypto verification occurs, users can easily eliminate or replace the NSA key from the operating system without modifying any of Microsoft's original\ncomponents. Since the NSA key is easily replaced, it means that non-US companies are free to install \"strong\" crypto services into Windows, without Microsoft's or the NSA's approval. Thus the NSA has effectively removed export control of \"strong\" crypto from Windows. A demonstration program that replaces the NSA key can be found on Cryptonym's website.\n\nIn September 1999, an anonymous researcher reverse-engineered both the primary key and the _NSAKEY into PGP-compatible format and published them to the key servers.\n\n"}
{"id": "1160565", "url": "https://en.wikipedia.org/wiki?curid=1160565", "title": "Niger Coast Protectorate", "text": "Niger Coast Protectorate\n\nThe Niger Coast Protectorate was a British protectorate in the Oil Rivers area of present-day Nigeria, originally established as the Oil Rivers Protectorate in 1884 and confirmed at the Berlin Conference the following year, renamed on 12 May 1893, and merged with the chartered territories of the Royal Niger Company on 1 January 1900 to form the Southern Nigeria Protectorate.\n\n"}
{"id": "19232224", "url": "https://en.wikipedia.org/wiki?curid=19232224", "title": "Norwegian Museum of Hydropower and Industry", "text": "Norwegian Museum of Hydropower and Industry\n\nThe Norwegian Museum of Hydropower and Industry () is a cultural history museum at Odda \nin Hordaland, Norway. The museum is located in the village of Tyssedal. The museum is dedicated to the industrial history of Odda and Tyssedal, and more generally to history related to rivers and water, hydropower production, electricity, power intensive industry and its society.\n\nA main attraction is the power station Tysso I, which was designed by architect Thorvald Astrup (1876–1940) and constructed between 1906 and 1918. The station contains machinery and control room equipment from the entire period of operation, 1908–1989. In 2000 Tyssedal power plant – including Tysso I, pipelines, distribution pool and watchman's house at Lilletopp, and intake pool and valve house at Vetlevann – was protected for posterity by the Norwegian Directorate for Cultural Heritage. During the following years the power plant went through extensive restoration.\n\nClose to the power station is the former administration building for AS Tyssefaldene, the power generation company that owns and operates the hydropower plant. Today the building houses the museum's exhibitions, library, archives, photo and audio collections, as well as an auditorium. Further upstream it is possible to visit the installations at Lilletopp, Vetlevann and the Ringedals Dam. Three old workmen's houses in Odda and a small former hydrologic research station at the glacier Folgefonna are also part of the museum. A via ferrata has been built that allows safe climbing along the very steep pipeline.\n\n"}
{"id": "38147", "url": "https://en.wikipedia.org/wiki?curid=38147", "title": "Oak Ridge National Laboratory", "text": "Oak Ridge National Laboratory\n\nOak Ridge National Laboratory (ORNL) is an American multiprogram science and technology national laboratory sponsored by the U.S. Department of Energy (DOE) and administered, managed, and operated by UT–Battelle as a federally funded research and development center (FFRDC) under a contract with the DOE. ORNL is the largest science and\nenergy national laboratory in the Department of Energy\nsystem by size and by annual budget. ORNL is located in Oak Ridge, Tennessee, near Knoxville. ORNL's scientific programs focus on materials,\nneutron science, energy, high-performance computing,\nsystems biology and national security.\n\nORNL partners with the state of Tennessee, universities and industries to solve challenges in energy, advanced materials, manufacturing, security and physics.\n\nThe laboratory is home to several of the world's top supercomputers including the world's most powerful supercomputer ranked by the TOP500, Summit, and is a leading neutron science and nuclear energy research facility that includes the Spallation Neutron Source and High Flux Isotope Reactor. ORNL hosts the Center for Nanophase Materials Sciences, the BioEnergy Science Center, and the Consortium for Advanced Simulation of Light-Water Reactors.\n\nOak Ridge National Laboratory is managed by UT-Battelle, a limited liability partnership between the University of Tennessee and the Battelle Memorial Institute, formed in 2000 for that purpose. The annual budget is US$1.65 billion, 80% of which is from the Department of Energy; the remainder is from various sources paying for use of the facilities. As of 2012 there are 4,400 staff working at ORNL, 1,600 of whom are directly conducting research, and an additional 3,000 guest researchers annually.\n\nThere are five campuses on the Department of Energy's Oak Ridge reservation; the National Laboratory, the Y-12 National Security Complex, the East Tennessee Technology Park (formerly the Oak Ridge Gaseous Diffusion Plant), the Oak Ridge Institute for Science and Education, and the developing Oak Ridge Science and Technology Park, although the four other facilities are unrelated to the National Laboratory. The total area of the reservation 150 square kilometres (58 sq mi) of which the lab takes up 18 square kilometres (7 sq mi).\n\nThe town of Oak Ridge was established by the Army Corps of Engineers as part of the Clinton Engineer Works in 1942 on isolated farm land as part of the Manhattan Project.\nDuring the war, advanced research for the government was managed at the site by the University of Chicago. In 1943, construction of the \"Clinton Laboratories\" was completed, later renamed to \"Oak Ridge National Laboratory\". The site was chosen for the X-10 Graphite Reactor, used to show that plutonium can be created from enriched uranium. Enrico Fermi and his colleagues developed the world's second self-sustaining nuclear reactor after Fermi's previous experiment, the Chicago Pile-1. The X-10 was the first reactor designed for continuous operation. After the end of World War II the demand for weapons-grade plutonium fell and the reactor and the laboratory's 1000 employees were no longer involved in nuclear weapons. Instead, it was used for scientific research. In 1946 the first medical isotopes were produced in the X-10 reactor, and by 1950 almost 20,000 samples had been shipped to various hospitals. As the demand for military science had fallen dramatically, the future of the lab was uncertain. Management of the lab was contracted by the US government to Monsanto; however, they withdrew in 1947. The University of Chicago re-assumed responsibility, until in December 1947 Union Carbide and Carbon Co., which already operated two other facilities at Oak Ridge, took control of the laboratory Alvin Weinberg was named Director of Research, ORNL, and in 1955 Director of the Laboratory.\n\nIn 1950 the Oak Ridge School of Reactor Technology was established with two courses in reactor operation and safety; almost 1000 students graduated. Much of the research performed at ORNL in the 1950s was relating to nuclear reactors as a form of energy production, both for propulsion and electricity. More reactors were built in the 1950s than in the rest of the ORNL's history combined.\n\nAnother project was the world's first light water reactor. With its principles of neutron moderation and fuel cooling by ordinary water, it is the direct ancestor of most modern nuclear power stations. The US Military funded much of its development, for nuclear-powered submarines and ships of the US Navy.\n\nThe US Army contracted portable nuclear reactors in 1953 for heat and electricity generation in remote military bases. The reactors were designed at ORNL, produced by American Locomotive Company and used in Greenland, the Panama Canal Zone and Antarctica. The United States Air Force (USAF) also contributed funding to three reactors, the lab's first computers, and its first particle accelerators. ORNL designed and tested a nuclear-powered aircraft in 1954 as a proof-of-concept for a proposed USAF fleet of long-range bombers, although it never flew.\n\nThe provision of radionuclides by X-10 for medicine grew steadily in the 1950s with more isotopes available. ORNL was the only Western source of californium-252. ORNL scientists lowered the immune systems of mice and performed the world's first successful bone marrow transplant.\n\nIn the early 1960s there was a large push at ORNL to develop nuclear-powered desalination plants, where deserts met the sea, to provide water. The project, called Water for Peace, was backed by John F. Kennedy and Lyndon B. Johnson, and presented at a 1964 United Nations conference, but increases in the cost of construction and public confidence in nuclear power falling caused the plan to fail. The Health Physics Research Reactor built in 1962 was used for radiation exposure experiments leading to more accurate dosage limits and dosimeters, and improved radiation shielding.\n\nIn 1964 the Molten-Salt Reactor Experiment began with the construction of the reactor. It was operated from 1966 until 1969 (with six months down time to move from U-235 to U-233 fuel), and proved the viability of molten salt reactors, while also producing fuel for other reactors as a byproduct of its own reaction.\n\nThe High Flux Isotope Reactor built in 1965 had the highest neutron flux of any reactor at the time. It improved upon the work of the X-10 reactor, producing more medical isotopes, as well as allowing higher fidelity of materials research.\n\nResearchers in the Biology Division studied the effects of chemicals on mice, including petrol fumes, pesticides, and tobacco.\n\nIn the late 1960s, cuts in funding led to the cancellation of plans for another particle accelerator, and the United States Atomic Energy Commission cut the breeder reactor program by two-thirds, leading to a downsizing in staff from 5000 to 3800.\n\nIn the 1970s, the prospect of fusion power was strongly considered, sparking research at ORNL. A tokamak called ORMAK, made operational in 1971, was the first tokamak to achieve a plasma temperature of 20 million Kelvin. After the success of the fusion experiments, it was enlarged and renamed ORMAK II in 1973; however, the experiments ultimately failed to lead to fusion power plants.\n\nThe US Atomic Energy Commission required improved safety standards in the early 1970s for nuclear reactors, so ORNL staff wrote almost 100 requirements covering many factors including fuel transport and earthquake resistance. In 1972 the AEC held a series of public hearings where emergency cooling requirements were highlighted and the safety requirements became more stringent.\n\nORNL was involved in analysing the damage to the core of the Three Mile Island Nuclear Generating Station after the accident in 1979.\n\nAlso in 1972, Peter Mazur, a biologist at ORNL, froze with liquid nitrogen, thawed and implanted mouse embryos in a surrogate mother. The mouse pups were born healthy. The technique is popular in the livestock industry, as it allows the embryos of valuable cattle to be transported easily and a prize cow can have multiple eggs extracted and thus, through \"in vitro\" fertilisation, have many more offspring than would naturally be possible.\n\nIn 1974 Alvin Weinberg, director of the lab for 19 years, was replaced by Herman Postma, a fusion scientist.\n\nIn 1977 construction began for 6 metre (20 foot) superconducting electromagnets, intended to control fusion reactions. The project was an international effort: three electromagnets were produced in the US, one in Japan, one in Switzerland and the final by remaining European states. Experimentation continued into the 1980s.\n\nThe 1980s brought more changes to ORNL: a focus on efficiency became paramount.\n\nAn accelerated climate simulation chamber was built that applied varying weather conditions to insulation to test its efficacy and durability faster than real time. Materials research into heat resistant ceramics for use in truck and high-tech car engines was performed, building upon the materials research that began in the nuclear reactors of the 1950s. In 1987 the High Temperature Materials Laboratory was established, where ORNL and industry researchers cooperated on ceramic and alloy projects. The materials research budget at ORNL doubled after initial uncertainty regarding Reagan's economic policy of less government expenditure.\n\nIn 1981, the Holifield Heavy Ion Research Facility, a 25 MV particle accelerator, was opened at ORNL. At the time, Holifield had the widest range of ion species and was twice as powerful as other accelerators, attracting hundreds of guest researchers each year.\n\nThe Department of Energy was concerned with the pollution surrounding ORNL and it began clean-up efforts. Burial trenches and leaking pipes had contaminated the groundwater beneath the lab, and radiation tanks were sitting idle, full of waste. Estimates of the total cost of clean-up were into the hundreds of millions of US dollars.\n\nThe five older reactors were subjected to safety reviews in 1987, ordered to be deactivated until the reviews were complete. By 1989 when the High Flux Isotope Reactor was restarted the US supply of certain medical isotopes was depleted.\n\nIn 1989 the former executive officer of the American Association for the Advancement of Science, Alvin Trivelpiece, became director of ORNL; he remained in the role until 2000.\n\nIn 1992, a whistleblower, Charles Varnadore, filed complaints against ORNL, alleging safety violations and retaliation by his superiors. While an administrative law judge ruled in Varnadore's favor, the Secretary of Labor, Robert Reich, overturned that ruling. However, Varnadore's case saw prime contractor Martin Marietta cited for safety violations, and ultimately led to additional whistleblower protection within DOE.\n\nHousing built at Oak Ridge was located at \"Townsite,\" along with stores and other necessary buildings. Housing was segregated. There were separate dorms for single men and women, with houses available for marrieds. The dorms tended to be overcrowded, with \"four sinks for forty-two women.\" Most of Townsite has now been replaced by the modern city of Oak Ridge.\n\nORNL conducts research and development activities that span a wide range of scientific disciplines. Many research areas have a significant overlap with each other; researchers often work in two or more of the fields listed here. The laboratory's major research areas are described briefly below.\n\nThe laboratory has a long history of energy research; nuclear reactor experiments have been conducted since the end of World War II in 1945. Because of the availability of reactors and high-performance computing resources an emphasis on improving the efficiency of nuclear reactors is present. The programs develop more efficient materials, more accurate simulations of aging reactor cores, sensors and controls as well as safety procedures for regulatory authorities.\n\nThe Energy Efficiency and Electricity Technologies Program (EEETP) aims to improve air quality in the US and reduce dependence on foreign oil supplies. There are three key areas of research; electricity, manufacturing and mobility. The electricity division focuses on reducing electricity consumption and finding alternative sources for production. Buildings, which account for 39% of US electricity consumption as of 2012, are a key area of research as the program aims to create affordable, carbon-neutral homes by 2020. Research also takes place into higher efficiency solar panels, geothermal electricity and heating, lower cost wind generators and the economic and environmental feasibility of potential hydro power plants.\n\nFusion is another area with a history of research at ORNL, dating back to the 1970s. The Fusion Energy Division pursues short-term goals to develop components such as high temperature superconductors, high-speed hydrogen pellet injectors and suitable materials for future fusion research. Much research into the behaviour and maintenance of a plasma takes place at the Fusion Energy Division to further the understanding of plasma physics, a crucial area for developing a fusion power plant. The US ITER office is at ORNL with partners at Princeton Plasma Physics Laboratory and Savannah River National Laboratory. The US contribution to the ITER project is 9.09% which is expected to be in excess of US$1.6 billion throughout the contract.\n\nOak Ridge National Laboratory's biological research covers genomics, computational biology, structural biology and bioinformatics. The BioEnergy Program aims to improve the efficiency of all stages of the biofuel process to improve the energy security of the United States. The program aims to make genetic improvements to the potential biomass used, formulate methods for refineries that can accept a diverse range of fuels and to improve the efficiency of energy delivery both to power plants and end users.\n\nThe Center for Molecular Biophysics conducts research into the behaviour of biological molecules in various conditions. The center hosts projects that examine cell walls for biofuel production, use neutron scattering to analyse protein folding and simulate the effect of catalysis on a conventional and quantum scale.\n\nThere are three neutron sources at ORNL; the High Flux Isotope Reactor (HFIR), the Oak Ridge Electron Linear Accelerator (ORELA) and the Spallation Neutron Source. HFIR provides neutrons in a stable beam resulting from a constant nuclear reaction whereas ORELA and SNS produce pulses of neutrons as they are particle accelerators. HFIR went critical in 1965 and has been used for materials research and as a major source of medical radioisotopes since. As of 2013, HFIR provides the world's highest constant neutron flux as a result of various upgrades. As part of a US non-proliferation effort the HFIR is scheduled to switch from highly enriched uranium (>90%, weapons grade) to low-enriched (3–4%) in 2020; the last reactor in the US to do so. Berkelium used to produce the world's first sample of tennessine was produced in the High Flux Isotope Reactor as part of an international effort. HFIR is likely to operate until approximately 2060 before the reactor vessel is considered unsafe for continued use.\n\nThe Spallation Neutron Source (SNS) is a particle accelerator that has the highest intensity neutron pulses of any man-made neutron source. SNS was made operational in 2006 and has since been upgraded to 1 megawatts with plans to continue up to 3 megawatts. High power neutron pulses permit clearer images of the targets meaning smaller samples can be analysed and accurate results require fewer pulses.\n\nOak Ridge National Laboratory conducts research into materials science in a range of areas. Between 2002 and 2008 ORNL partnered with Caterpillar Inc. (CAT) to form a new material for their diesel engines that can withstand large temperature fluctuations. The new steel, named CF8C Plus, is based on conventional CF8C stainless steel with added manganese and nitrogen; the result has better high–temperature properties and is easier to cast at a similar cost. In 2003 the partners received an R&D 100 award from \"R&D magazine\" and in 2009 received an award for \"excellence in technology transfer\" from the Federal Laboratory Consortium for the commercialisation of the steel.\n\nThere is a high-temperature materials lab at ORNL that permits researchers from universities, private companies and other government initiatives to use their facilities. The lab is available for free if the results are published; private research is permitted but requires payment. A separate lab, the Shared Equipment User Facility, is one of three DOE sponsored facilities with nano-scale microscopy and tomography facilities.\n\nThe Center for Nanophase Materials Sciences (CNMS) researches the behaviour and fabrication of nanomaterials. The center emphasises discovery of new materials and the understanding of underlying physical and chemical interactions that enable creation of nanomaterials. In 2012, CNMS produced a lithium-sulfide battery with a theoretical energy density three to five times greater than existing lithium ion batteries.\n\nOak Ridge National Laboratory provides resources to the US Department of Homeland Security and other defense programs. The Global Security and Nonproliferation (GS&N) program develops and implements policies, both US based and international, to prevent the proliferation of nuclear material. The program has developed safeguards for nuclear arsenals, guidelines for dismantling arsenals, plans of action should nuclear material fall into unauthorised hands, detection methods for stolen or missing nuclear material and trade of nuclear material between the US and Russia. The GS&N's work overlaps with that of the Homeland Security Programs Office, providing detection of nuclear material and nonproliferation guidelines. Other areas concerning the Department Homeland Security include nuclear and radiological forensics, chemical and biological agent detection using mass spectrometry and simulations of potential national hazards.\n\nThroughout the history of the Oak Ridge National Laboratory it has been the site of various supercomputers, home to the fastest on several occasions. In 1953, ORNL partnered with the Argonne National Laboratory to build ORACLE (Oak Ridge Automatic Computer and Logical Engine), a computer to research nuclear physics, chemistry, biology and engineering. ORACLE had 2048 words (80 Kibit) of memory and took approximately 590 microseconds to perform addition or multiplications of integers. In the 1960s ORNL was also equipped with an IBM 360/91 and an IBM 360/65. In 1995 ORNL bought an Intel Paragon based computer called the \"Intel Paragon XP/S 150\" that performed at 154 gigaFLOPS and ranked third on the TOP500 list of supercomputers. In 2005 Jaguar was built, a Cray XT3 based system that performed at 25 teraFLOPS and received incremental upgrades up to the XT5 platform that performed at 2.3 petaFLOPS in 2009. It was recognised as the world's fastest from November 2009 until November 2010. Summit was built for Oak Ridge National Laboratory during 2018, which benchedmarked at 122.3 petaflops. As of June 2018, Summit stands as the world's fasted [clocked] supercomputer with 202,752 CPU cores, 27,648 Nvidia Tesla GPUs and 250 Petabytes of storage. \n\nSince 1992 the National Center for Computational Sciences (NCCS) has overseen high performance computing at ORNL. It manages the Oak Ridge Leadership Computing Facility that contains the machines. In 2012, Jaguar was upgraded to the XK7 platform, a fundamental change as GPUs are used for the majority of processing, and renamed Titan. Titan performs at 17.59 petaFLOPS and holds the number 1 spot on the TOP500 list for November 2012. Other computers include a 77 node cluster to visualise data that the larger machines output in the \"Exploratory Visualization Environment for Research in Science and Technology\" (EVEREST), a visualisation room with a 10 by 3 metre (30 by 10 ft) wall that displays 35 megapixel projections. Smoky is an 80 node linux cluster used for application development. Research projects are refined and tested on Smoky before running on larger machines such as Titan. \n\nIn 1989 programmers at the Oak Ridge National Lab wrote the first version of Parallel Virtual Machine (PVM), software that enables distributed computing on machines of differing specifications. PVM is free software and has become the de facto standard for distributed computing. Jack Dongarra of ORNL and the University of Tennessee wrote the LINPACK software library and LINPACK benchmarks, used to calculate linear algebra and the standard method of measuring floating point performance of a supercomputer as used by the TOP500 organisation.\n\n\n"}
{"id": "22190", "url": "https://en.wikipedia.org/wiki?curid=22190", "title": "Organic electronics", "text": "Organic electronics\n\nOrganic electronics is a field of materials science concerning the design, synthesis, characterization, and application of organic small molecules or polymers that show desirable electronic properties such as conductivity. Unlike conventional inorganic conductors and semiconductors, organic electronic materials are constructed from organic (carbon-based) small molecules or polymers using synthetic strategies developed in the context of organic and polymer chemistry. One of the promised benefits of organic electronics is their potential low cost compared to traditional inorganic electronics. Attractive properties of polymeric conductors include their electrical conductivity that can be varied by the concentrations of dopants. Relative to metals, they have mechanical flexibility. Some have high thermal stability.\n\nOne class of materials of interest in organic electronics are electrical conductive, i.e. substances that can transmit electrical charges with low resistivity. Traditionally, conductive materials are inorganic. Classical (and still technologically dominant) conductive materials are metals such as copper and aluminum as well as many alloys.\n\nThe earliest reported organic conductive material, polyaniline, was described by Henry Letheby in 1862. Work on other polymeric organic materials began in earnest in the 1960s, A high conductivity of 1 S/cm (S = Siemens) was reported in 1963 for a derivative of tetraiodopyrrole. In 1977, it was discovered that polyacetylene can be oxidized with halogens to produce conducting materials from either insulating or semiconducting materials. The 2000 Nobel Prize in Chemistry was awarded to Alan J. Heeger, Alan G. MacDiarmid, and Hideki Shirakawa jointly for their work on conductive polymers. These and many other workers identified large families of electrically conducting polymers including polythiophene, polyphenylene sulfide, and others.\n\nIn the 1950s, a second class of electric conductors were discovered based on charge-transfer salts. Early examples were derivatives of polycyclic aromatic compounds. For example, pyrene was shown to form semiconducting charge-transfer complex salts with halogens. In 1972, researchers found metallic conductivity(conductivity comparable to a metal) in the charge-transfer complex TTF-TCNQ.\n\nConductive plastics have undergone development for applications in industry. In 1987, the first organic diode was produced at Eastman Kodak by Ching W. Tang and Steven Van Slyke.\n\nThe initial characterization of the basic properties of polymer light emitting diodes, demonstrating that the light emission phenomenon was injection electroluminescence and that the frequency response was sufficiently fast to permit video display applications, was reported by Bradley, Burroughes, Friend, et al. in a 1990 Nature paper. Moving from molecular to macromolecular materials solved the problems previously encountered with the long-term stability of the organic films and enabled high-quality films to be easily made. Subsequent research developed multilayer polymers and the new field of plastic electronics and organic light-emitting diodes (OLED) research and device production grew rapidly.\n\nOrganic conductive materials can be grouped into two main classes: conductive polymers and conductive molecular solids and salts.\n\nSemiconducting small molecules include polycyclic aromatic compounds such as pentacene and rubrene.\n\nConductive polymers are often typically intrinsically conductive or at least semiconductors. They sometimes show mechanical properties comparable to those of conventional organic polymers. Both organic synthesis and advanced dispersion techniques can be used to tune the electrical properties of conductive polymers, unlike typical inorganic conductors. The most well-studied class of conductive polymers include polyacetylene, polypyrrole, polyaniline, and their copolymers. Poly(p-phenylene vinylene) and its derivatives are used for electroluminescent semiconducting polymers. Poly(3-alkythiophenes) are also a typical material for use in solar cells and transistors.\n\nAn OLED (organic light-emitting diode) consists of a thin film of organic material that emits light under stimulation by an electric current. A typical OLED consists of an anode, a cathode, OLED organic material and a conductive layer.\n\nAndré Bernanose was the first person to observe electroluminescence in organic materials, and Ching W. Tang, reported fabrication of an OLED device in 1987. The OLED device incorporated a double-layer structure motif consisting of separate hole transporting and electron-transporting layers, with light emission taking place in between the two layers. Their discovery opened a new era of current OLED research and device design.\n\nOLED organic materials can be divided into two major families: small-molecule-based and polymer-based. Small molecule OLEDs (SM-OLEDs) include organometallic chelates(Alq3), fluorescent and phosphorescent dyes, and conjugated dendrimers. Fluorescent dyes can be selected according to the desired range of emission wavelengths; compounds like perylene and rubrene are often used. Very recently, Dr. Kim J. et al. at University of Michigan reported a pure organic light emitting crystal, Br6A, by modifying its halogen bonding, they succeeded in tuning the phosphorescence to different wavelengths including green, blue and red. By modifying the structure of Br6A, scientists are attempting to achieve a next generation organic light emitting diode. Devices based on small molecules are usually fabricated by thermal evaporation under vacuum. While this method enables the formation of well-controlled homogeneous film; is hampered by high cost and limited scalability.\nPolymer light-emitting diodes (PLEDs), similar to SM-OLED, emit light under an applied electric current. Polymer-based OLEDs are generally more efficient than SM-OLEDs requiring a comparatively lower amount of energy to produce the same luminescence. Common polymers used in PLEDs include derivatives of poly(p-phenylene vinylene) and polyfluorene. The emitted color can be tuned by substitution of different side chains onto the polymer backbone or modifying the stability of the polymer. In contrast to SM-OLEDs, polymer-based OLEDs cannot be fabricated through vacuum evaporation, and must instead be processed using solution-based techniques. Compared to thermal evaporation, solution based methods are more suited to creating films with large dimensions. Zhenan Bao. et al. at Stanford University reported a novel way to construct large-area organic semiconductor thin films using aligned single crystalline domains.\n\nAn Organic field-effect transistor is a field-effect transistor utilizing organic molecules or polymers as the active semiconducting layer. A field-effect transistor (FET) is any semiconductor material that utilizes electric field to control the shape of a channel of one type of charge carrier, thereby changing its conductivity. Two major classes of FET are n-type and p-type semiconductor, classified according to the charge type carried. In the case of organic FETs (OFETs), p-type OFET compounds are generally more stable than n-type due to the susceptibility of the latter to oxidative damage.\n\nJ.E. Lilienfeld first proposed the field-effect transistor in 1930, but the first OFET was not reported until 1987, when Koezuka et al. constructed one using Polythiophene which shows extremely high conductivity. Other conductive polymers have been shown to act as semiconductors, and newly synthesized and characterized compounds are reported weekly in prominent research journals. Many review articles exist documenting the development of these materials.\n\nLike OLEDs, OFETs can be classified into small-molecule and polymer-based system. Charge transport in OFETs can be quantified using a measure called carrier mobility; currently, rubrene-based OFETs show the highest carrier mobility of 20–40 cm/(V·s). Another popular OFET material is Pentacene. Due to its low solubility in most organic solvents, it's difficult to fabricate thin film transistors (TFTs) from pentacene itself using conventional spin-cast or, dip coating methods, but this obstacle can be overcome by using the derivative TIPS-pentacene. Current research focuses more on thin-film transistor (TFT) model, which eliminates the usage of conductive materials. Very recently, two studies conducted by Dr. Bao Z. et al. and Dr. Kim J. et al. demonstrated control over the formation of designed thin-film transistors. By controlling the formation of crystalline TFT, it is possible to create an aligned (as opposed to randomly ordered) charge transport pathway, resulting in enhanced charge mobility.\n\nOrganic solar cells could cut the cost of solar power by making use of inexpensive organic polymers rather than the expensive crystalline silicon used in most solar cells. What's more, the polymers can be processed using low-cost equipment such as ink-jet printers or coating equipment employed to make photographic film, which reduces both capital and operating costs compared with conventional solar-cell manufacturing.\n\nSilicon thin-film solar cells on flexible substrates allow a significant cost reduction of large-area photovoltaics for several reasons:\n\n\nInexpensive polymeric substrates like polyethylene terephthalate (PET) or polycarbonate (PC) have the potential for further cost reduction in photovoltaics. Protomorphous solar cells prove to be a promising concept for efficient and low-cost photovoltaics on cheap and flexible substrates for large-area production as well as small and mobile applications.\n\nOne advantage of printed electronics is that different electrical and electronic components can be printed on top of each other, saving space and increasing reliability and sometimes they are all transparent. One ink must not damage another, and low temperature annealing is vital if low-cost flexible materials such as paper and plastic film are to be used. There is much sophisticated engineering and chemistry involved here, with iTi, Pixdro, Asahi Kasei, Merck & Co.|Merck, BASF, HC Starck, Hitachi Chemical and Frontier Carbon Corporation among the leaders.\nElectronic devices based on organic compounds are now widely used, with many new products under development. Sony reported the first full-color, video-rate, flexible, plastic display made purely of organic materials; television screen based on OLED materials; biodegradable electronics based on organic compound and low-cost organic solar cell are also available.\n\nThere are important differences between the processing of small molecule organic semiconductors and semiconducting polymers. Small molecule semiconductors are quite often insoluble and typically require deposition via vacuum sublimation. While usually thin films of soluble conjugated polymers. Devices based on conductive polymers can be prepared by solution processing methods. Both solution processing and vacuum based methods produce amorphous and polycrystalline films with variable degree of disorder. \"Wet\" coating techniques require polymers to be dissolved in a volatile solvent, filtered and deposited onto a substrate. Common examples of solvent-based coating techniques include drop casting, spin-coating, doctor-blading, inkjet printing and screen printing. Spin-coating is a widely used technique for small area thin film production. It may result in a high degree of material loss. The doctor-blade technique results in a minimal material loss and was primarily developed for large area thin film production. Vacuum based thermal deposition of small molecules requires evaporation of molecules from a hot source. The molecules are then transported through vacuum onto a substrate. The process of condensing these molecules on the substrate surface results in thin film formation. Wet coating techniques can in some cases be applied to small molecules depending on their solubility.\n\nCompared to conventional inorganic solar cell, organic solar cells have the advantage of lower fabrication cost. An organic solar cell is a device that uses organic electronics to convert light into electricity. Organic solar cells utilize organic photovoltaic materials, organic semiconductor diodes that convert light into electricity. Figure to the right shows five commonly used organic photovoltaic materials. Electrons in these organic molecules can be delocalized in a delocalized π orbital with a corresponding π* antibonding orbital. The difference in energy between the π orbital, or highest occupied molecular orbital(HOMO), and π* orbital, or lowest unoccupied molecular orbital(LUMO) is called the band gap of organic photovoltaic materials. Typically, the band gap lies in the range of 1-4eV.\n\nThe difference in the band gap of organic photovoltaic materials leads to different chemical structures and forms of organic solar cells. Different forms of solar cells includes single-layer organic photovoltaic cells, bilayer organic photovoltaic cells and heterojunction photovoltaic cells. However, all three of these types of solar cells share the approach of sandwiching the organic electronic layer between two metallic conductors, typically indium tin oxide.\nAn organic field-effect transistor device consists of three major components: the source, the drain and the gate. Generally, a field-effect transistor has two plates, source in contact with drain and the gate respectively, working as conducting channel. The electrons move from source to the drain, and the gate serves to control the electrons’ movement from source to drain. Different types of FETs are designed based on carrier properties. Thin film transistor (TFT), among them, is an easy fabricating one. In a thin film transistor, the source and drain are made by directly depositing a thin layer of semiconductor followed by a thin film of insulator between semiconductor and the metal gate contact. Such a thin film is made by either thermal evaporation, or simply spins coating. In a TFT device, there is no carrier movement between the source and drain. After applying a positive charge, accumulation of electrons on the interface cause bending of the semiconductor and ultimately lowers the conduction band with regards to the Fermi-level of the semiconductor. Finally, a highly conductive channel is formed at the interface.\n\nConductive polymers are lighter, more flexible, and less expensive than inorganic conductors. This makes them a desirable alternative in many applications. It also creates the possibility of new applications that would be impossible using copper or silicon.\n\nOrganic electronics not only includes organic semiconductors, but also organic dielectrics, conductors and light emitters.\n\nNew applications include smart windows and electronic paper. Conductive polymers are expected to play an important role in the emerging science of molecular computers.\n\n\n\n"}
{"id": "24716786", "url": "https://en.wikipedia.org/wiki?curid=24716786", "title": "PearC", "text": "PearC\n\nPearC is the name given to the line of personal computers produced by German manufacturer HyperMegaNet UG, that sells PCs with Mac OS X, similar to Psystar. HyperMegaNet UG claims it is acting within the law because Apple's EULA – that forbids installation of Mac OS X on non-Apple branded computers – only applies if it can be seen before purchase, according to German law.\n\nThe PearC line of personal computers consists of six models: PearC Starter, PearC Advanced, PearC Professional and PearC Expert, PearC Supreme, and PearC Business. All models are built using off-the-shelf components from Intel, Gigabyte Technology, Corsair, Nvidia, Western Digital and Lian-Li. Every computer ships with a USB drive of Mac OS X Mountain Lion to allow re-installation of the operating system. The company also offers dual-boot computers that run Windows 7 and Mac OS X Mountain Lion.\n\nPearCs ship with a completely unmodified copy of Mac OS X. Instead they use the freely available Chameleon bootloader and a number of kernel extensions installed on the EFI System partition to allow them to boot Mac OS X Mountain Lion. If an Apple software update is likely to cause problems, the company advises its customers and releases a freely available patch in the form of a package installer.\n\nAll PearC personal computers come with a two-year warranty.\n\nThe company sells from websites in Spain: http://www.pearc.es/\nand Germany http://www.pearc.de/\n\nAll current products capable of running Mac OS X use Intel's 'Ivy Bridge' generation of processors.\n\n"}
{"id": "1358159", "url": "https://en.wikipedia.org/wiki?curid=1358159", "title": "Radar warning receiver", "text": "Radar warning receiver\n\nRadar warning receiver (RWR) systems detect the radio emissions of radar systems. Their primary purpose is to issue a warning when a radar signal that might be a threat (such as a police speed detection radar or a fighter jet's fire control radar) is detected. The warning can then be used, manually or automatically, to evade the detected threat. RWR systems can be installed in all kind of airborne, sea-based, and ground-based assets (such as aircraft, ships, automobiles, military bases). This article is focused mainly on airborne military RWR systems; for commercial police RWR systems, see radar detector.\n\nDepending on the market the RWR system is designed for, it can be as simple as detecting the presence of energy in a specific radar band (such as police radar detectors). For more critical situations, such as military combat, RWR systems are often capable of classifying the source of the radar by the signal's strength, phase and waveform type, such as pulsed power wave or continuous wave with amplitude modulation or frequency modulation (chirped). The information about the signal's strength and waveform can then be used to estimate the most probable type of threat the detected radar poses. Simpler systems are typically installed in less expensive assets like \nautomobiles, while more sophisticated systems are installed in mission critical assets such as military aircraft.\n\nThe RWR usually has a visual display somewhere prominent in the cockpit (in some modern aircraft, in multiple locations in the cockpit) and also generates audible tones which feed into the pilot's (and perhaps RIO/co-pilot/GIB's in a multi-seat aircraft) headset. The visual display often takes the form of a circle, with symbols displaying the detected radars according to their direction relative to the current aircraft heading (i.e. a radar straight ahead displayed at the top of the circle, directly behind at the bottom, etc.). The distance from the center of the circle, depending on the type of unit, can represent the estimated distance from the generating radar, or to categorize the severity of threats to the aircraft, with tracking radars placed closer to the center than search radars. The symbol itself is related to the type of radar or the type of vehicle that carries it, often with a distinction made between ground-based radars and airborne radars.\n\nThe typical airborne RWR system consists of multiple wideband antennas placed around the aircraft which receive the radar signals. The receiver periodically scans across the frequency band and determines various parameters of the received signals, like frequency, signal shape, direction of arrival, pulse repetition frequency, etc. By using these measurements, the signals are first deinterleaved to sort the mixture of incoming signals by emitter type. These data are then further sorted by threat priority and displayed.\n\nThe RWR is used for identifying, avoiding, evading or engaging threats. For example, a fighter aircraft on a combat air patrol (CAP) might notice enemy fighters on the RWR and subsequently use its own radar set to find and eventually engage the threat. In addition, the RWR helps identify and classify threats—it's hard to tell which blips on a radar console-screen are dangerous, but since different fighter aircraft typically have different types of radar sets, once they turn them on and point them near the aircraft in question it may be able to tell, by the direction and strength of the signal, which of the blips is which type of fighter.\n\nA non-combat aircraft, or one attempting to avoid engagements, might turn its own radar off and attempt to steer around threats detected on the RWR. Especially at high altitude (more than 30,000 feet AGL), very few threats exist that don't emit radiation. As long as the pilot is careful to check for aircraft that might try to sneak up without radar, say with the assistance of AWACS or GCI, it should be able to steer clear of SAMs, fighter aircraft and high altitude, radar-directed AAA.\n\nSEAD and ELINT aircraft often have sensitive and sophisticated RWR equipment like the U.S. HTS (HARM targeting system) pod which is able to find and classify threats which are much further away than those detected by a typical RWR, and may be able to overlay threat circles on a map in the aircraft's multi-function display (MFD), providing much better information for avoiding or engaging threats, and may even store information to be analyzed later or transmitted to the ground to help the commanders plan future missions.\n\nThe RWR can be an important tool for evading threats if avoidance has failed. For example, if a SAM system or enemy fighter aircraft has fired a missile (for example, a SARH-guided missile) at the aircraft, the RWR may be able to detect the change in mode that the radar must use to guide the missile and notify the pilot with much more insistent warning tones and flashing, bracketed symbols on the RWR display. The pilot then can take evasive action to break the missile lock-on or dodge the missile. The pilot may even be able to visually acquire the missile after being alerted to the possible launch. What's more, if an actively guided missile is tracking the aircraft, the pilot can use the direction and distance display of the RWR to work out which evasive maneuvers to perform to outrun or dodge the missile. For example, the rate of closure and aspect of the incoming missile may allow the pilot to determine that if they dive away from the missile, it is unlikely to catch up, or if it is closing fast, that it is time to jettison external supplies and turn toward the missile in an attempt to out-turn it. The RWR may be able to send a signal to another defensive system on board the aircraft, such as a Countermeasure Dispensing System (CMDS), which can eject countermeasures such as chaff, to aid in avoidance.\n\n\n"}
{"id": "5719641", "url": "https://en.wikipedia.org/wiki?curid=5719641", "title": "Rajiv Gupta (technocrat)", "text": "Rajiv Gupta (technocrat)\n\nRajiv Gupta is a Silicon Valley technology executive from India\n\nGupta earned a bachelor's degree from the Indian Institute of Technology Kharagpur around 1984. He received his Ph.D. in compiler optimization from the California Institute of Technology (Caltech) in 1990. He married a British woman, Debra and they have two children - Veda and Anya. He resides in Los Altos.\n\nGupta joined Hewlett-Packard in 1990, and developed the IA-64 architecture, which HP called WideWord and Intel marketed as Itanium.\nFrom 1995 he developed a client utility project at HP Labs, which was an early example of a service-oriented architecture for Web services.\nHe was co-inventor and general manager of the E-speak project when it was announced in 1999.\nAround the same time, he supported his brother Sanjiv Gupta, to start Bodhtree Consulting, Ltd., in Hyderabad, India.\nThe E-speak technology was abandoned in late 2001.\nIn 2002, Gupta founded Confluent Software, developing what became the CoreSV product. It was acquired by Oblix in February 2004, which in turn was acquired by Oracle Corporation in March, 2005.\nIn 2005 he founded Securent, which was acquired by Cisco in November 2007 for an estimated $100 million.\nHe has more than 45 patents.\n\nIn 2011 Gupta founded Skyhigh Networks. The first round of financing was led by Greylock Partners in April, 2012, for about $6.5 million.\nThe company raised $20 million in May, 2013, led by Sequoia Capital.\nAnother investment of $40 million was announced in June, 2014, from existing investors and Salesforce.com.\n\nOn November 28, 2017, McAfee announced it would acquire Skyhigh Networks and appoint Rajiv Gupta as the head of McAfee's entire cloud business.\n"}
{"id": "2412341", "url": "https://en.wikipedia.org/wiki?curid=2412341", "title": "Real-time kinematic", "text": "Real-time kinematic\n\nReal-time kinematic (RTK) positioning is a satellite navigation technique used to enhance the precision of position data derived from satellite-based positioning systems (global navigation satellite systems, GNSS) such as GPS, GLONASS, Galileo, and BeiDou. It uses measurements of the phase of the signal's carrier wave in addition to the information content of the signal and relies on a single reference station or interpolated virtual station to provide real-time corrections, providing up to centimetre-level accuracy. With reference to GPS in particular, the system is commonly referred to as carrier-phase enhancement, or CPGPS. It has applications in land survey, hydrographic survey, and in unmanned aerial vehicle navigation.\n\nThe distance between a satellite navigation receiver and a satellite can be calculated from the time it takes for a signal to travel from the satellite to the receiver. To calculate the delay, the receiver must align a pseudorandom binary sequence contained in the signal to an internally generated pseudorandom binary sequence. Since the satellite signal takes time to reach the receiver, the satellite's sequence is delayed in relation to the receiver's sequence. By increasingly delaying the receiver's sequence, the two sequences are eventually aligned.\n\nThe accuracy of the resulting range measurement is essentially a function of the ability of the receiver's electronics to accurately process signals from the satellite, and additional error sources such as non-mitigated ionospheric and tropospheric delays, multipath, satellite clock and ephemeris errors, etc.\n\nRTK follows the same general concept, but uses the satellite signal's carrier wave as its signal, ignoring the information contained within. RTK uses a fixed base station and a rover to reduce the rover's position error. The base station transmits correction data to the rover.\n\nAs described in the previous section, the range to a satellite is essentially calculated by multiplying the carrier wavelength times the number of whole cycles between the satellite and the rover and adding the phase difference. Determining the number of cycles is non-trivial, since signals may be shifted in phase by one or more cycles. This results in an error equal to the error in the estimated number of cycles times the wavelength, which is 19 cm for the L1 signal. Solving this so-called integer ambiguity search problem results in centimeter precision. The error can be reduced with sophisticated statistical methods that compare the measurements from the C/A signals and by comparing the resulting ranges between multiple satellites.\n\nThe improvement possible using this technique is potentially very high if one continues to assume a 1% accuracy in locking. For instance, in the case of GPS, the coarse-acquisition (C/A) code, which is broadcast in the L1 signal, changes phase at 1.023 MHz, but the L1 carrier itself is 1575.42 MHz, which changes phase over a thousand times more often. A ±1% error in L1 carrier-phase measurement thus corresponds to a ±1.9 mm error in baseline estimation.\n\nIn practice, RTK systems use a single base-station receiver and a number of mobile units. The base station re-broadcasts the phase of the carrier that it observes, and the mobile units compare their own phase measurements with the one received from the base station. There are several ways to transmit a correction signal from base station to mobile station. The most popular way to achieve real-time, low-cost signal transmission is to use a radio modem, typically in the UHF Band. In most countries, certain frequencies are allocated specifically for RTK purposes. Most land-survey equipment has a built-in UHF-band radio modem as a standard option. RTK provides accuracy enhancements up to about 20 km from the base station.\n\nThis allows the units to calculate their \"relative\" position to within millimeters, although their absolute position is accurate only to the same accuracy as the computed position of the base station. The typical nominal accuracy for these systems is (ppm) horizontally and vertically.\n\nAlthough these parameters limit the usefulness of the RTK technique for general navigation, the technique is perfectly suited to roles like surveying. In this case, the base station is located at a known surveyed location, often a benchmark, and the mobile units can then produce a highly accurate map by taking fixes relative to that point. RTK has also found uses in autodrive/autopilot systems, precision farming, machine control systems and similar roles.\n\nThe RTK networks extend the use of RTK to a larger area containing a network of reference stations. Operational reliability and accuracy depend on the density and capabilities of the reference-station network.\n\nA Continuously Operating Reference Station (CORS) network is a network of RTK base stations that broadcast corrections, usually over an Internet connection. Accuracy is increased in a CORS network, because more than one station helps ensure correct positioning and guards against a false initialization of a single base station.\n\nVendors that provide RTK systems and services include: Trimble, Leica Geosystems, Topcon, North Surveying, Hi-Target, Sokkia, NovAtel, Septentrio, Hemisphere GNSS, ANavS, CHCNAV, SunNav, and Unistrong.\n\n\n"}
{"id": "23291901", "url": "https://en.wikipedia.org/wiki?curid=23291901", "title": "Rotary storage", "text": "Rotary storage\n\nRotary storage systems are specialised office filing units designed to offer increased storage volumes per square foot compared to traditional filing units.\n\nRotary storage systems are filing cabinets; specialised office furniture units usually consisting of a double sided rotating unit, allowing the user to access two full sides of filing from one point. A foot pedal or lever is often used to operate the rotation mechanism, thus allowing user easy control.\n\nMany rotary filing systems can store the contents of seventeen four drawer filing cabinets in just three eight-tier units, allowing multiple times the storage volume created by traditional filing cabinets. A wide range of fitments makes the systems suitable for storing a vast array file types and multimedia. For security, each unit can invariably be locked individually.\n\nRotary storage systems usually have no shutters or doors, and therefore can be placed very close to walls or partitioning, for a very efficient use of floor space. Units are often used to divide office space.\n\nRotary storage units can allow faster file access than banks of conventional cabinets, as 50% of stored items are immediately visible to the operator, the rest typically being a pedal push away.\n\n"}
{"id": "44805159", "url": "https://en.wikipedia.org/wiki?curid=44805159", "title": "Ryszard S. Michalski", "text": "Ryszard S. Michalski\n\nRyszard S. Michalski (May 7, 1937 – September 20, 2007) was a Polish-American computer scientist. Michalski was Professor at George Mason University and a pioneer in the field of machine learning.\n\nMichalski was born in Kalusz near Lvov on 7 May 1937.\n\nHe received an equivalent of Bachelor of Science degree in Electrical Engineering at the Universities of Technology in Kraków and Warsaw in 1959; obtained M.S. Computer Science at the Polytechnic Institute of\nSt. Petersburg in 1961; and Ph.D. in Computer Science at the Silesian University of Technology, Gliwice in 1969. In the period 1962–1970 he worked at the Institute of Automation of the Polish Academy of Sciences (PAS) in Warsaw, during which he and Jacek Karpiński developed an early successful learning system for recognizing handwritten alpha-numeric characters.\n\nHe emigrated to America in 1970, worked at the University of Illinois at Urbana-Champaign until moved with his research group to the George Mason University in 1988. Although leaving Poland, he worked part-time at the Institute of Computer Science of PAS in Warsaw.\n\nRyszard S. Michalski died on 20 September 2007 from cancer at his home in Fairfax.\n\nMichalski was cofounder of Machine Learning and Inference Laboratory at George Mason University. He earned a patent for the university with Learnable Non-Darwinian Evolution Model (LEM), a form of evolutionary computation, in 2003.\n\nHe cofounded the Journal of Machine Learning in 1986 and helped organize the first international multistrategy machine learning conferences in 1991.\n\nMichalski influenced wide areas, notably in machine learning, but also in the broadly understood areas of data analysis and knowledge discovery. Some of his offspring are listed below. (See more on his homepage at GMU)\n\nMichalski was elected to Foreign Member of the Polish Academy of Sciences in 2000 and Fellow of AAAI. Poland President honored him with the Officer's Cross of the Order of Merit of the Republic of Poland in July 2007.\n\nMichalski was prolific author of scientific works on various topics in computer science, including machine learning, artificial intelligence, and human plausible reasoning. He wrote, co-wrote, or co-edited more than 350 research publications and more than one dozen books. Some of which are listed below. (See more on his homepage at GMU)\n\n\n\n"}
{"id": "11993615", "url": "https://en.wikipedia.org/wiki?curid=11993615", "title": "Salted fish", "text": "Salted fish\n\nSalted fish, such as kippered herring or dried and salted cod, is fish cured with dry salt and thus preserved for later eating. Drying or salting, either with dry salt or with brine, was the only widely available method of preserving fish until the 19th century. Dried fish and salted fish (or fish both dried and salted) are a staple of diets in the Caribbean, North Africa, Southeast Asia, Southern China, Scandinavia, parts of Canada including Newfoundland, coastal Russia, and in the Arctic. Like other salt-cured meats, it provides preserved animal protein even in the absence of refrigeration.\n\nSalting is the preservation of food with dry edible salt. It is related to pickling (preparing food with brine, i.e. salty water), and is one of the oldest methods of preserving food. Salt inhibits the growth of microorganisms by drawing water out of microbial cells through osmosis. Concentrations of salt up to 20% are required to kill most species of unwanted bacteria. Smoking, often used in the process of curing meat, adds chemicals to the surface of meat that reduce the concentration of salt required. Salting is used because most bacteria, fungi and other potentially pathogenic organisms cannot survive in a highly salty environment, due to the hypertonic nature of salt. Any living cell in such an environment will become dehydrated through osmosis and die or become temporarily inactivated.\n\nThe water activity, a, in a fish is defined as the ratio of the water vapour pressure in the flesh of the fish to the vapour pressure of pure water at the same temperature and pressure. It ranges between 0 and 1, and is a parameter that measures how available the water is in the flesh of the fish. Available water is necessary for the microbial and enzymatic reactions involved in spoilage. There are a number of techniques that have been or are used to tie up the available water or remove it by reducing the a. Traditionally, techniques such as drying, salting and smoking have been used, and have been used for thousands of years. In more recent times, freeze-drying, water binding humectants, and fully automated equipment with temperature and humidity control have been added. Often a combination of these techniques is used.\n\n\n"}
{"id": "12740323", "url": "https://en.wikipedia.org/wiki?curid=12740323", "title": "Self-propelled modular transporter", "text": "Self-propelled modular transporter\n\nA self-propelled modular transporter or sometimes self-propelled modular trailer (SPMT) is a platform vehicle with a large array of wheels. SPMTs are used for transporting massive objects such as large bridge sections, oil refining equipment, motors and other objects that are too big or heavy for trucks. Trucks can however provide traction and braking for the SPMTs on inclines and descents.\n\nSPMTs are used in many industry sectors worldwide such as the construction and oil industries, in the shipyard and offshore industry, for road transportation, on plant construction sites and even for moving oil platforms. Recently in addition, they have begun to be used to replace bridge spans in the United States, Europe, Asia and more recently Canada.\n\nA typical SPMT has a grid of computer-controlled axles, usually 2 axles across and 4–8 axles along. When two (or more) axles are placed side-by-side, this is called an axle line. All axles are individually controllable, in order to evenly distribute weight and steer accurately. Each axle can swivel through 270°, with some manufacturers offering up to a full 360° of motion. The axles are coordinated by the control system to allow the SPMT to turn, move sideways or even spin in place. Some SPMTs allow the axles to telescope independently of each other so that the load can be kept flat and evenly distributed while moving over uneven terrain. Each axle can also contain a hydrostatic drive unit. \n\nA hydraulic power pack can be attached to the SPMT to provide power for steering, suspension and drive functions. This power pack is driven by an internal combustion engine. A single power pack can drive a string of SPMTs. As SPMTs often carry the world's heaviest loads on wheeled vehicles, they are very slow, often moving at under one mile per hour while fully loaded. Some SPMTs are controlled by a worker with a hand-held control panel, while others have a driver cabin. Multiple SPMTs can be linked (lengthwise and side-by-side) to transport massive building-sized objects. The linked SPMTs can be controlled from a single control panel.\n\nThe first modular self-propelled trailers were built in the 1970s. In the early 1980s, heavy haulage company Mammoet refined the concept into the form we see today. They set the width of the modules to 2.44 m, so the modules would fit on an ISO container flatrack. They also added 360° steering. They commissioned Scheuerle to develop and build the first units. Deliveries started in 1983. The two companies defined standard units: a 4-axle SPMT, a 6-axle SPMT and a power pack. Over the years, more modules were added to this system to accommodate a range of payloads.\n\nIn 2016 ESTA (the European Association of Abnormal Load Transport and Mobile Cranes) published the first SPMT Best Practice Guide to help address the problem of trailers tipping over, which has happened on some occasions even though the operating rules and stability calculations have been precisely followed.\n\nSome shipbuilding companies have started to use SPMT instead of gantry cranes for carrying ship sections. This has reduced the cost for transporting loads by millions of dollars.\n\nExecuting the salvage operation of the sunken ferry MV \"Sewol\" in the East China Sea in 2017, the company ALE used SPMTs totalling 600 axle lines and a load weight of 17,000 t, beating two world records.\n\n"}
{"id": "363786", "url": "https://en.wikipedia.org/wiki?curid=363786", "title": "Stereopticon", "text": "Stereopticon\n\nA stereopticon is a slide projector or \"magic lantern\", which has two lenses, usually one above the other.\nThese devices date back to the mid 19th century, and were a popular form of entertainment and education before the advent of moving pictures. Americans William and Frederick Langenheim introduced stereopticon slide technology—slide shows of projected images on glass—in 1850. For a usual fee of ten cents, people could view realistic images of nature, history, and science themes. The two lenses are used to dissolve between images when projected. At first, the shows used random images, but over time, lanternists began to place the slides in logical order, creating a narrative. This \"visual storytelling\" with technology directly preceded the development of the first moving pictures. \n\nThe term stereopticon has been widely misused to name a stereoscope. A stereopticon will not project or display stereoscopic/three-dimensional images on cards. All stereopticons can be classified as magic lanterns, but not all magic lanterns are stereopticons.\n"}
{"id": "24679207", "url": "https://en.wikipedia.org/wiki?curid=24679207", "title": "Submersible drilling rig", "text": "Submersible drilling rig\n\nA submersible drilling rig is a marine vessel design that can be floated to location and lowered onto the sea floor for offshore drilling activities.\n\nThe submersible drilling platform is supported on large pontoon-like structures. These pontoons provide buoyancy allowing the unit to be towed from location to location.\n\nOnce on the location, the pontoon structure is slowly flooded until it rests securely on its anchors, of which there are usually two per corner.\n\nThe operating deck is elevated 100 feet above the pontoons on large steel columns to provide clearance above the waves.\n\nAfter the well is drilled, the water is pumped out of the buoyancy tanks and the vessel is re-floated and towed to the next location.\n\n\"Submersibles\", as they are known informally, operate in relatively shallow water, since they must rest on the sea floor. Other floating vessel types are used in deeper water depths. The term Mobile Offshore Drilling Unit (MODU) is generally used for all offshore drilling rigs that can be moved from location to location.\n\nThe first offshore mobile drilling platform was the Hayward-Barnsdall \"Breton Rig 20\", first operated in 1949. This rig had evolved from the inland drilling barges which were used to drill in marshes and protected waters in up to 10 feet of water. The \"Breton Rig 20\" was 160 feet by 85 feet, and could work in 20 feet water depth.\n\nBy 1958, the number of submersible drilling rigs had increased to around 30.\n\nIn 1961, Shell Oil successfully converted an existing submersible rig \"Blue Water Rig No.1\" into the first semi submersible drilling unit for operation in the Gulf of Mexico when it was found to have good stability and motions whilst being towed at a partial draught.\n\nAlden J. Laborde designed and constructed the first purpose-built V-shaped semi-submersible drilling rig, \"Ocean Driller\", delivered in 1963.\n"}
{"id": "9921781", "url": "https://en.wikipedia.org/wiki?curid=9921781", "title": "Surface lure", "text": "Surface lure\n\nA surface lure is a fishing lure designed to waddle, pop, lock, drop, pulse, twitch or fizz across the surface of the water as it is retrieved, and in doing so imitate surface prey for fish such as mice, lizards, frogs, cicadas, moths and small injured fish.\nA typical surface lure has a solid body made out of wood or plastic, carries one or two treble hooks, and has an eyelet at the front of the lure body to attach the fishing line. Waddlers get their action from a scooped metal dish attached to the front of the lure body. Poppers get their action from a cupped face carved or molded into the front of the lure body. Fizzers get their action both from the fisherman manipulating the lure with the fishing rod and from one or more blades attached to the lure body, that spin when the lure is pulled and create a fizzing noise said to imitate the buzzing wings of a drowning insect. \n\nSizeable fish can create a sudden, noisy and spectacular explosion when they take a surface lure, usually giving the fisherman a fright in the process. Catching fish on surface lures is therefore considered a fairly exciting form of fishing.\n"}
{"id": "47190687", "url": "https://en.wikipedia.org/wiki?curid=47190687", "title": "The Second Machine Age", "text": "The Second Machine Age\n\nThe Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies is a 2014 book by Erik Brynjolfsson and Andrew McAfee which is a continuation of their book \"Race Against the Machine\". They argue that the Second Machine Age involves the automation of a lot of cognitive tasks that make humans and software-driven machines substitutes, rather than complements. They contrast this with what they call the \"First Machine Age\", or Industrial Revolution, which helped make labor and machines complementary.\n\nSome examples that the book cites include \"software that grades students' essays more objectively, consistently and quickly than humans\" and \"news articles on Forbes.com about corporate earnings previews\" — \"all generated by algorithms without human involvement.\"\n\nThe authors summarize the contents of their book's 15 chapters on pages 11 and 12 of the book itself.\n\nAccording to the authors, the book has three sections. Chapters 1 through 6 describe \"the fundamental characteristics of the second machine age,\" based on many examples of modern use of technology. Then chapters 7 through 11 describe economic impacts of technology in terms of two concepts the authors call \"bounty\" and \"spread.\" What the authors call \"bounty\" is their attempt to measure the benefits of new technology in ways reaching beyond such measures as GDP, which they say is inadequate. They use \"spread\" as a shorthand way to describe the increasing inequality that is also resulting from widespread new technology.\n\nFinally, in chapters 12 through 15, the authors prescribe some policy interventions that could enhance the benefits and reduce the harm of new technologies.\n\nThe \"Washington Post\" says that its strength is how it weaves micro and macroeconomics with insights from other disciplines into an accessible story. It says that the weaknesses of the book are that its policy prescriptions are \"straight from the talking points that tech executives have been peddling for years on their visits to the capital\", even though they are \"perfectly reasonable\".\n"}
{"id": "32676158", "url": "https://en.wikipedia.org/wiki?curid=32676158", "title": "Threshing stone", "text": "Threshing stone\n\nA threshing stone is a roller-like tool used for the threshing of wheat. Similar to the use of threshing boards, the stone was pulled by horses over a circular pile of harvested wheat on a hardened dirt surface (threshing floor), and the rolling stone knocked the grain from the head of wheat. The straw was removed from the pile and the remaining grain and chaff was collected. By a process called winnowing, the grain was tossed into the air to allow the chaff and dirt to be blown away, leaving only the grain.\n\nEvidence of the use of threshing stones in various forms date from the Roman era, but eventually it became the method of threshing grain by the Molotschna Mennonite farmers in the Ukraine from about 1840 to 1900. The threshing stone became the preferred method used to knock the grain from the head because it was less labor-intensive than the use of the traditional threshing flail.\n\nIn the 1870s, approximately one-third of all Russian Mennonites immigrated to the Great Plains of North America. The immigrants in central Kansas brought the threshing stone technology with them, and contracted stone masons in the Florence, Kansas area to construct stones made of limestone.\n\nThe use of threshing stones quickly faded as they were superseded by mechanized forms of threshing, especially by threshing machines, and eventually all farmers quit using them by around 1900 to 1905.\n\nThe threshing stone is a cylindrical stone with teeth carved into it. Stones made in Kansas were approximately 29 to 30 inches long, 23 to 24 inches in diameter, with 7 notches (and rarely 8) resembling gear teeth, made of limestone, and 600 to 800 pounds in weight. Some smaller stones may weigh less than 400 pounds.\n\nIn 2011, over 90 threshing stones have been located. Numerous stones in Kansas are located at private residences as family historic artifacts. The following is a list of locations where threshing stones may be viewed by the public.\nMuseums:\n\nVarious:\n\nThe \"Thresher\" is the official mascot of Bethel College in North Newton, Kansas and named after the threshing stone.\n\n\nHistory:\nPhotos:\n"}
{"id": "54523041", "url": "https://en.wikipedia.org/wiki?curid=54523041", "title": "Tone-Lok", "text": "Tone-Lok\n\nTone-Lok Effects are guitar effects pedals from a (now discontinued) product line, introduced by Ibanez in 1999. In contrast with other guitar pedals, they included a \"Lok\" feature, engaged for each adjustment by pressing down on its corresponding potentiometer's control knob.\n\nAP7 Analog Phaser<br>\nAW7 Autowah<br>\nCF7 Stereo Chorus/Flanger<br>\nDE7 Stereo Delay/Echo<br>\nDS7 Distortion<br>\nFZ7 Fuzz<br>\nLF7 Lo Fi<br>\nPH7 Phaser<br>\nPM7 Phase Modulator<br>\nSH7 Seventh Heaven<br>\nSM7 Smashbox<br>\nTC7 Tri Mode Chorus<br>\nTS7 Tubescreamer<br>\nWD7 Weeping Demon<br>\nWD7JR Weeping Demon Junior\n\nPD7 Phat-Hed Bass Overdrive<br>\nSB7 Synthesizer Bass<br>\n"}
{"id": "34580464", "url": "https://en.wikipedia.org/wiki?curid=34580464", "title": "Wheel washing system", "text": "Wheel washing system\n\nA wheel washing system is a device for cleaning the tires of trucks when they are leaving a site, to control and eliminate the pollution of public roads. The installation can be made in or above the ground for either temporary or permanent applications. There are two types of wheel washing systems: roller and drive-through systems.\n\nAt a roller unit, the truck places each set of wheels on the rollers as it leaves the plant. At this point the washing process is automatically triggered. While the wheels of each axle are rotated automatically, water is sprayed out from strategically placed nozzles to clean the tires, wheel wells and mud flaps.\n\nThe advantage of drive-through systems is that the vehicle does not have to stop for cleaning. As the vehicle passes the magnetic-sensor or photo-sensor at the entrance, the washing process is triggered. During the slow passage (5 mph speed) through the wash platform, the vehicle's tires, wheel wells and mud flaps are cleaned by water sprayed from many strategically placed nozzles (spray bar). The nozzles are mounted angle-iron grids which open the tire treads and flushing the mud for a high cleaning performance. The washing platform is usually of galvanized steel. The first modern systems were introduced as the MobyDick “Quick” series at a German construction industry trade exhibition. The Quick system was a significant design improvement from the modified high pressure truck wash systems that were used as wheel and tire wash systems before. Today all modern drive through systems are based on the engineering and design development of the Quick series.\n\nA key aspect of the philosophy of wheel and tire cleaning is the use of high volume water flow at low pressure. The high volume flow of water flow serves to clean the tires and effectively remove the wash sludge slurry from the wash platform into the recycling and solids collection tanks. Mist and overspray from high pressure systems would not be an effective method of cleaning and removing the wash sludge into the recycling and solids collections tanks. Other disadvantages of high pressure cleaning are wasted water, energy and obstruction of driver visibility.\n\nOnce the wash sludge arrives to a recycling tank the solids typically sink by gravity to the bottom. To accelerate the sedimentation of the solids, flocculants are used. Periodically the settled solids at the bottom of the recycling tank are removed from the tank with an automatic scraper conveyor system or by other means such as a wheel loader, an excavator with bucket or a vacuum truck. After the solids settlement process the wash water is ready for the next washing cycle. The goal of the sedimentation process is to deliver clean and optically clear water for every wash.\n\n"}
{"id": "32959790", "url": "https://en.wikipedia.org/wiki?curid=32959790", "title": "Y-factor", "text": "Y-factor\n\nThe Y-factor method is a widely used technique for measuring the gain and noise temperature of an amplifier. It is based on the Johnson-Nyquist noise of a resistor at two different, known temperatures.\n\nConsider a microwave amplifier with a 50-ohm impedance with a 50-ohm resistor connected to the amplifier input. If the resistor is at a physical temperature \"T\", then the Johnson–Nyquist noise power coupled to the amplifier input is \"P\" = \"k\"\"T\"\"B\", where \"k\" is Boltzmann’s constant, and \"B\" is the bandwidth. The noise power at the output of the amplifier (i.e. the noise power coupled to an impedance-matched load that is connected to the amplifier output) is \"P\" = \"Gk\"(\"T\" + \"T\")\"B\", where \"G\" is the amplifier power gain, and \"T\" is the amplifier noise temperature. In the Y-factor technique, \"P\" is measured for two different, known values of \"T\". \"P\" is then converted to an effective temperature \"T\" (in units of kelvin) by dividing by \"k\" and the measurement bandwidth \"B\". The two values of \"T\" are then plotted as a function of \"T\" (also in units of kelvin), and a line is fit to these points (see figure). The slope of this line is equal to the amplifier power gain. The \"x\" intercept of the line is equal to the negative of the amplifier noise temperature −\"T\" in kelvins. The amplifier noise temperature can also be determined from the \"y\" intercept, which is equal to \"T\" multiplied by the gain.\n"}
