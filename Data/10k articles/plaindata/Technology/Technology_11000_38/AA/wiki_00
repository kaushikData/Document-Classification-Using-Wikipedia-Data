{"id": "4845988", "url": "https://en.wikipedia.org/wiki?curid=4845988", "title": "80/20 (framing system)", "text": "80/20 (framing system)\n\nT-slot structural framing is a framing system consisting of lengths of square or rectangular extruded aluminum, typically 6105-T5 aluminium alloy, with a T-slot down the centerline of one or more sides. It is also known as 80/20 framing, after the company 80/20, Inc., one of the prominent T-slot framing brands, the name of whom is based on the 80/20 principle. While there is no published standard defining this framing system, manufacturers have settled into two categories of product comprising several series each that is generally intercompatible across manufacturers.\n\nT-slot framing is divided into metric and fractional (imperial) categories. The T-slot is always centered along the long-axis of the piece. Pieces are available in each series with a square cross-section. Rectangular cross sections are available as well which measure \"x\" by \"2x\" (where \"x\" is the defined width) - e.g. 40mx80mm for 40 series.\n\n\n"}
{"id": "5930520", "url": "https://en.wikipedia.org/wiki?curid=5930520", "title": "Adam Dunkels", "text": "Adam Dunkels\n\nAdam Dunkels is a Swedish entrepreneur, programmer and founder of Thingsquare. His father was Andrejs Dunkels, professor in Mathematics. His mother was Kerstin Vännman also professor. His work is mainly focused on networking technology and distributed communication for small embedded devices and wireless sensor networks on the Internet. Dunkels is best known to the embedded community as the author of the uIP (micro-IP) and lwIP TCP/IP protocol stacks. He is also the creator of protothreads and author of the Contiki operating system. The \"MIT Technology Review\" placed him on the TR35 list of world's top 35 innovators under 35, in 2009.\n\nHis book \"Interconnecting Smart Objects with IP - the Next Internet\", co-authored with JP Vasseur and with a foreword by Vint Cerf, was published in 2010.\n\nHe is a founder of the IPSO Alliance, who promotes IP networking for smart objects such as embedded systems and wireless sensors, and author of the alliance's white paper.\n\nDunkels received the 2008 EuroSys Roger Needham PhD Award for his PhD thesis \"Programming Memory-Constrained Networked Embedded Systems\".\n\nMany of Dunkels's small implementations are used in commercial products from companies, including ABB, Altera, BMW, Cisco Systems, Ericsson, GE, HP, Volvo Technology and Xilinx. They include:\n\n"}
{"id": "8870404", "url": "https://en.wikipedia.org/wiki?curid=8870404", "title": "An Experiment on a Bird in the Air Pump", "text": "An Experiment on a Bird in the Air Pump\n\nAn Experiment on a Bird in the Air Pump is a 1768 oil-on-canvas painting by Joseph Wright of Derby, one of a number of candlelit scenes that Wright painted during the 1760s. The painting departed from convention of the time by depicting a scientific subject in the reverential manner formerly reserved for scenes of historical or religious significance. Wright was intimately involved in depicting the Industrial Revolution and the scientific advances of the Enlightenment. While his paintings were recognized as exceptional by his contemporaries, his provincial status and choice of subjects meant the style was never widely imitated. The picture has been owned by the National Gallery, London, since 1863 and is still regarded as a masterpiece of British art. In June 2015 it was on loan to Tate Britain.\n\nThe painting depicts a natural philosopher, a forerunner of the modern scientist, recreating one of Robert Boyle's air pump experiments, in which a bird is deprived of air, before a varied group of onlookers. The group exhibits a variety of reactions, but for most of the audience scientific curiosity overcomes concern for the bird. The central figure looks out of the picture as if inviting the viewer's participation in the outcome.\n\nIn 1659, Robert Boyle commissioned the construction of an air pump, then described as a \"pneumatic engine\", which is known today as a vacuum pump. The air pump was invented by Otto von Guericke in 1650, though its cost deterred most contemporary scientists from constructing the apparatus. Boyle, the son of the Earl of Cork, had no such concerns—after its construction, he donated the initial 1659 model to the Royal Society and had a further two redesigned machines built for his personal use. Aside from Boyle's three pumps, there were probably no more than four others in existence during the 1660s: Christian Huygens had one in The Hague, Henry Power may have had one at Halifax, and there may have been pumps at Christ's College, Cambridge, and the Montmor Academy in Paris. Boyle's pump, which was largely designed to Boyle's specifications and constructed by Robert Hooke, was complicated, temperamental, and problematic to operate. Many demonstrations could only be performed with Hooke on hand, and Boyle frequently left critical public displays solely to Hooke—whose dramatic flair matched his technical skill.\n\nDespite the operational and maintenance obstacles, construction of the pump enabled Boyle to conduct a great many experiments on the properties of air, which he later detailed in his \"New Experiments Physico-Mechanicall, Touching the Spring of the Air, and its Effects, (Made, for the Most Part, in a New Pneumatical Engine)\". In the book, he described in great detail 43 experiments he conducted, on occasion assisted by Hooke, on the effect of air on various phenomena. Boyle tested the effects of \"rarified\" air on combustion, magnetism, sound, and barometers, and examined the effects of increased air pressure on various substances. He listed two experiments on living creatures: \"Experiment 40,\" which tested the ability of insects to fly under reduced air pressure, and the dramatic \"Experiment 41,\" which demonstrated the reliance of living creatures on air for their survival. In this attempt to discover something \"about the account upon which Respiration is so necessary to the Animals, that Nature hath furnish'd with Lungs\", Boyle conducted numerous trials during which he placed a large variety of different creatures, including birds, mice, eels, snails and flies, in the vessel of the pump and studied their reactions as the air was removed. Here, he describes an injured lark:\n\nBy the time Wright painted his picture in 1768, air pumps were a relatively commonplace scientific instrument, and itinerant \"lecturers in natural philosophy\"—usually more showmen than scientists—often performed the \"animal in the air pump experiment\" as the centrepiece of their public demonstration. These were performed in town halls and other large buildings for a ticket-buying audience, or were booked by societies or for private showings in the homes of the well-off, the setting suggested in both of Wright's demonstration pieces. One of the most notable and respectable of the travelling lecturers was James Ferguson FRS, a Scottish astronomer and probable acquaintance of Joseph Wright (both were friends of John Whitehurst). Ferguson noted that a \"lungs-glass\" with a small air-filled bladder inside was often used in place of the animal, as using a living creature was \"too shocking to every spectator who has the least degree of humanity\".\n\nThe full moon in the picture is significant as meetings of the Lunar Circle (renamed the Lunar Society by 1775) were timed to make use of its light when travelling. \n\nWright met Darwin in the early 1760s, probably through their common connection of John Whitehurst, first consulting Darwin about ill health in 1767 when he stayed in the Darwin household for a week. The energy and vivacity of both Erasmus and Mary (Polly) Darwin impressed Wright. In the 1980s Eric Evans (National Gallery) suggested that Darwin is the figure in the left foreground who holds a watch. As this composed timekeeper is not consistent with Darwin's flamboyant character, it is more likely that this is Dr William Small. The attention to timekeeping fits with Dr Small's role as the social secretary for the Lunar Circle. Small returned from Virginia in 1764 and established his practice in Birmingham in 1765, consistent with this being a meeting in 1767. The profile and wig of this figure are consistent with a contemporary portrait of Small by Tilly Kettle.\n\nDuring his apprenticeship and early career Wright concentrated on portraiture. By 1762, he was an accomplished portrait artist, and his 1764 group portrait \"James Shuttleworth, his Wife and Daughter\" is acknowledged as his first true masterpiece. Benedict Nicolson suggests that Wright was influenced by the work of Thomas Frye; in particular by the 18 bust-length mezzotints which Frye completed just before his death in 1762. It was perhaps Frye's candlelight images that tempted Wright to experiment with subject pieces. Wright's first attempt, \"A Girl reading a Letter by candlelight with a Young Man looking over her shoulder\" from 1762 or 1763, is a trial in the genre, and is fetching though uncomplicated.\nWright's \"An Experiment on a Bird in the Air Pump\" forms part of a series of candlelit nocturnes that he produced between 1765 and 1768.\n\nThere was a long history of painting candlelit scenes in Western art, although as Wright had not at this date travelled abroad, there remains uncertainty as to what paintings he might have seen in the original, as opposed to prints. Nicolson, who made studies of both Wright and other candlelight painters such as the 17th-century Utrecht Caravaggisti, thought their paintings, among the largest in the style, those most likely to have influenced Wright. However Judy Egerton wonders if he could have seen any, preferring as influences the far smaller works of the Leiden fijnschilder Godfried Schalcken (1643–1706), whose reputation was much greater in the early 18th century than subsequently. He had worked in England from 1692 to 1697, and several of his paintings can be placed in English collections in Wright's day.\n\nAlthough he was the leading expert writing in English, Nicolson does not suggest that Wright is likely to have known of the 17th-century candlelit narrative religious subjects of Georges de La Tour and Trophime Bigot, which, in their seriousness, are the closest works to Wright that are lit only by candle. The Dutch painters' works and other candlelit scenes by 18th-century English painters such as Henry Morland (father of George) tended instead to exploit the possibilities of semi-darkness for erotic suggestiveness. Some of Wright's own later candlelit scenes were by no means as serious as his first ones, as seen from their titles: \"Two Boys Fighting Over a Bladder\" and \"Two Girls Dressing a Kitten by Candlelight\". \n\nThe first of his candlelit masterpieces, \"Three Persons Viewing the Gladiator by Candlelight\", was painted in 1765, and showed three men studying a small copy of the \"Borghese Gladiator\". \"Viewing the Gladiator\" was greatly admired; but his next painting, \"A Philosopher giving that Lecture on the Orrery, in which a Lamp is put in place of the Sun\" (normally known by the shortened form \"A Philosopher Giving a Lecture on the Orrery\" or just \"The Orrery\"), caused a greater stir, as it replaced the Classical subject at the centre of the scene with one of a scientific nature. Wright's depiction of the awe produced by scientific \"miracles\" marked a break with traditions in which the artistic depiction of such wonder was reserved for religious events, since to Wright the marvels of the technological age were as awe-inspiring as the subjects of the great religious paintings.\n\nIn both of these works the candlelit setting had a realist justification. Viewing sculpture by candlelight, when the contours showed well and there might even be an impression of movement from the flickering light, was a fashionable practice described by Goethe. In the orrery demonstration the shadows cast by the lamp representing the sun were an essential part of the display, used to demonstrate eclipses. But there seems no reason other than heightened drama to stage the air pump experiment in a room lit by a single candle, and in two later paintings of the subject by Charles-Amédée-Philippe van Loo the lighting is normal.\n\nThe painting was one of a number of British works challenging the set categories of the rigid, French-dictated hierarchy of genres in the late 18th century, as other types of painting aspired to be treated as seriously as the costumed history painting of a Classical or mythological subject. In some respects the \"Orrery\" and \"Air Pump\" subjects resembled conversation pieces, then largely a form of middle-class portraiture, though soon to be given new status when Johann Zoffany began to paint the royal family in about 1766. Given their solemn atmosphere however, and as it seems none of the figures are intended to be understood as portraits (even if models may be identified), the paintings can not be regarded as conversation pieces. The 20th-century art historian Ellis Waterhouse compares these two works to the \"genre serieux\" of contemporary French drama, as defined by Denis Diderot and Pierre Beaumarchais, a view endorsed by Egerton.\n\nAn anonymous review from the time called Wright \"a very great and uncommon genius in a peculiar way\". \"The Orrery\" was painted without a commission, probably in the expectation that it would be bought by Washington Shirley, 5th Earl Ferrers, an amateur astronomer who had an orrery of his own, and with whom Wright's friend Peter Perez Burdett was staying while in Derbyshire. Figures thought to be portraits of Burdett and Ferrers feature in the painting, Burdett taking notes and Ferrers seated with his son next to the orrery.\nFerrers purchased the painting for £210, but the 6th Earl auctioned it off, and it is now held by Derby Museum and Art Gallery.\n\n\"An Experiment on a Bird in the Air Pump\" followed in 1768, the emotionally charged experiment contrasting with the orderly scene from \"The Orrery\". The painting, which measures 72 by 94½ inches (183 by 244 cm), shows a grey cockatiel fluttering in panic as the air is slowly withdrawn from the vessel by the pump. The witnesses display various emotions: one of the girls worriedly watches the fate of the bird, while the other is too upset to observe and is comforted by her father; two gentlemen (one of them dispassionately timing the experiment) and a boy look on with interest, while the young lovers to the left of the painting are absorbed only in each other. The scientist himself looks directly out of the picture, as if challenging the viewer to judge whether the pumping should continue, killing the bird, or whether the air should be replaced and the cockatiel saved.\n\nAside from that of the children, little sympathy is directed toward the bird; David Solkin suggests the subjects of the painting show the dispassionate detachment of the evolving scientific society. Individuals are concerned for each other: the father for his children, the young man for the girl, but the distress of the cockatiel elicits only careful study. To one side of the boy at the rear, the cockatiel's empty cage can be seen on the wall, and to further heighten the drama it is unclear whether the boy is lowering the cage on the pulley to allow the bird to be replaced after the experiment or hoisting the cage back up, certain of its former occupant's death. It has also been suggested that he may be drawing the curtains to block out the light from the full moon. \nJenny Uglow believes that the boy echoes the figure in the last print of William Hogarth's \"The Four Stages of Cruelty\" by pointing out the arrogance and potential cruelty of experimentation, while David Fraser also sees the compositional similarities with the audience grouped round a central demonstration. The neutral stance of the central character and the uncertain intentions of the boy with the cage were both later ideas: an early study, discovered on the back of a self-portrait, omits the boy and shows the natural philosopher reassuring the girls. In this sketch it is obvious that the bird will survive, and thus the composition lacks the power of the final version. \nWright, who took many of his subjects from English poetry, probably knew the following passage from \"The Wanderer\" (1729) by Richard Savage:\n\nThe cockatiel would have been a rare bird at the time, \"and one whose life would never in reality have been risked in an experiment such as this\". It did not become well-known until after it was shown in illustrations to the accounts of the voyages of Captain Cook in the 1770s. Prior to Cook's voyage, cockatiels had been imported only in small numbers as exotic cage-birds. Wright had painted one in 1762 at the home of William Chase, featuring it both in his portrait of Chase and his wife (\"Mr & Mrs William Chase\") and a separate study, \"The Parrot\". In selecting such a rarity for this scientific sacrifice, Wright not only chose a more dramatic subject than the \"lungs-glass\", but was perhaps making a statement about the values of society in the Age of Enlightenment. The grey plumage of the cockatiel also shows much more effectively in the darkened room than the small dull-coloured bird in Wright's early oil sketch. A resemblance has been pointed out between the group of the bird and the two nearest figures and a type of depiction of the Trinity found in Early Netherlandish painting, where the Holy Spirit is represented by a dove, to which God the Father (the philosopher) points, while Christ (the father) gestures in blessing to the viewer.\n\nOn the table are various other pieces of equipment that the natural philosopher would have used during his demonstration: a thermometer, candle snuffer and cork, and close to the man seated to the right is a pair of Magdeburg hemispheres, which would have been used with the air pump to demonstrate the difference in pressure exerted by the air and a vacuum: when the air was pumped out from between the two hemispheres they were impossible to pull apart. The air pump itself is rendered in exquisite detail, a faithful record of the designs in use at the time. What may be a human skull in the large liquid-filled glass bowl would not have been a normal piece of equipment; William Schupbach suggests that it and the candle, which is presumably lighting the bowl from behind, form a \"vanitas\"—the two symbols of mortality reflecting the cockatiel's struggle for life.\n\nThe powerful central light source creates a chiaroscuro effect. The light illuminating the scene has been described as \"so brilliant it could only be the light of revelation\". The single source of light is obscured behind the bowl on the table; some hint of a lamp glass can be seen around the side of the bowl, but David Hockney has suggested that the bowl itself may contain sulphur, giving a powerful single light source that a candle or oil lamp would not. In the earlier study a candle holder is visible, and the flame is reflected in the bowl. Hockney believes that many of the Old Masters used optical equipment to assist in their painting, and suggests that Wright may have used lenses to transfer the image to paper rather than painting directly from the scene, as he believes the pattern of shadows thrown by the lighting could have been too complicated for Wright to have captured so accurately without assistance. It may be observed, however, that the stand on which the pump is situated casts no shadow on the body of the philosopher, as it could be expected to do.\n\nWright's \"Air Pump\" was unusual in that it depicted archetypes rather than specific people, though various models for the figures have been suggested. The young lovers may have been based on Thomas Coltman and Mary Barlow, friends of Wright's, whom he later painted in \"Mr and Mrs Thomas Coltman\" (also in the National Gallery) after their marriage in 1769; Erasmus Darwin has been suggested as the man timing the experiment on the left of the table, and John Warltire, whom Darwin had invited to help with some air pump experiments in real life, as the natural philosopher; but Wright never identified any of the subjects or suggested they were based on real people.\n\nIn \"The Orrery\", all the subjects have been identified apart from the philosopher, who has physical similarities to Isaac Newton but differs enough to make positive identification impossible. Nicolson detects the strong influence of Frye throughout the picture. Particularly striking is the similarity between Frye's mezzotint \"Portrait of a Young Man\" of 1760–1761 and the figure of the boy with his head cocked staring intently at the bird. In 1977, Michael Wynne published one of Frye's chalk drawings from around 1760, \"An old man leaning on a staff\", which is so similar to the observer in the right foreground in Wright's picture to make it impossible that Wright had not seen it. There are other hints of Frye's style in the painting: even the figure of the natural philosopher has touches of Frye's \"Figure with Candle\". Though Henry Fuseli would later also develop on the style of Frye's work there is no evidence of him having painted anything similar until the early 1780s. So, although he had already been in England at the time the \"Air Pump\" was produced, it is unlikely that he was an influence on Wright.\n\nWright's scientific paintings adopted elements from the tradition of history painting but lacked the heroic central action typical of that genre. While ground-breaking, they are regarded as peculiar to Wright, whose unique style has been explained in many ways. Wright's provincial status and ties to the Lunar Society, a group of prominent industrialists, scientists and intellectuals who met regularly in Birmingham between 1765 and 1813, have been highlighted, as well as his close association with and sympathy for the advances made in the burgeoning Industrial Revolution. Other critics have emphasised a desire to capture a snapshot of the society of the day, in the tradition of William Hogarth but with a more neutral stance that lacks the biting satire of Hogarth's work.\n\nThe scientific subjects of Wright's paintings from this time were meant to appeal to the wealthy scientific circles in which he moved. While never a member himself, he had strong connections with the Lunar Society: he was friends with members John Whitehurst and Erasmus Darwin, as well as Josiah Wedgwood, who later commissioned paintings from him. The inclusion of the moon in the painting was a nod to their monthly meetings, which were held when the moon was full. Like \"The Orrery\", Wright apparently painted \"Air Pump\" without a commission, and the picture was purchased by Dr Benjamin Bates, who already owned Wright's \"Gladiator\". An Aylesbury physician, patron of the arts and hedonist, Bates was a diehard member of the Hellfire Club who, despite his excesses, lived to be over 90. Wright's account book shows a number of prices for the painting: P£200 is shown in one place and £210 in another, but Wright had written to Bates asking for £130, stating that the low price \"might much injure me in the future sale of my pictures, and when I send you a receipt for the money I shall acknowledge a greater sum.\" Whether Bates ever paid the full amount is not recorded; Wright only notes in his account book that he received £30 in part payment. \nWright exhibited the painting at the Society of Artists exhibition in 1768 and it was re-exhibited before Christian VII of Denmark in September the same year. Viewers remarked that it was \"clever and vigorous\", while Gustave Flaubert, who saw it on a visit to England in 1865–66, considered it \"charmant de naïveté et profondeur\". It was popular enough that a mezzotint was engraved from it by Valentine Green which was published by John Boydell on 24 June 1769, and initially sold for 15 shillings. This was reprinted throughout the 18th and 19th centuries, in increasingly weak impressions. Ellis Waterhouse called it \"one of the wholly original masterpieces of British art\".\n\nFrom Bates, the picture passed to Walter Tyrell; another member of the Tyrell family, Edward, presented it to the National Gallery, London, in 1863, after it had failed to sell at an auction at Christie's in 1854. The painting was transferred to the Tate Gallery in 1929, although it was actually on loan to Derby Museum and Art Gallery between 1912 and 1947. It has been lent out for exhibitions to the National Gallery of Art in Washington, D.C. in 1976, the National Museum of Fine Arts in Stockholm in 1979–1980, and Paris (Grand Palais), New York (Metropolitan) and the Tate in London in 1990. It was reclaimed by the National Gallery from the Tate in 1986. They describe its condition as good, with minor alterations visible on some figures. It was last cleaned in 1974.\n\nThe striking scene has been used as the cover illustration for many books on topics both artistic and scientific. It has even spawned pastiches and parodies: the book cover of \"The Science of Discworld\", by Terry Pratchett, Ian Stewart and Jack Cohen, is a tribute to the painting by artist Paul Kidby, who replaces Wright's figures with the book's protagonists. Shelagh Stephenson's play \"An Experiment with an Air Pump\", inspired by the painting, was the joint winner of the 1997 Margaret Ramsay Award and had its premiere at the Royal Exchange Theatre, Manchester, in 1998.\n\n\n"}
{"id": "56243621", "url": "https://en.wikipedia.org/wiki?curid=56243621", "title": "Arab Standardization and Metrology Organization", "text": "Arab Standardization and Metrology Organization\n\nThe Arab Organization for Standardization and Metrology (, ), also known as Arab Organization for Standardization and Measures, was founded in 1965 as a specialized agency under the Arab League by the Council of Arab Economic Unity. \n\nThe organization's functions included offering technical advice to Arab states on systems of weights and measures; providing professional training and research on industrial production quality, metrology, test and inspection methods; and seeking standardization of technical terms and product specifications between member nations. Their first general committee was held on March 25, 1968.\n\nThe organization was merged in the 1990s with other organizations to form the Arab Industrial Development and Mining Organization.\n\n\n"}
{"id": "2049725", "url": "https://en.wikipedia.org/wiki?curid=2049725", "title": "Astrogation", "text": "Astrogation\n\nAstrogation (a portmanteau of \"astronomical navigation\"), is the fictional navigation of spacecraft in interplanetary or interstellar travel. The term was first used by science fiction writers in the first half of the 20th century. An early usage is in the 1953 Robert A. Heinlein novel, \"Starman Jones.\"\n\n"}
{"id": "12099711", "url": "https://en.wikipedia.org/wiki?curid=12099711", "title": "BIOMAN (lifting devices)", "text": "BIOMAN (lifting devices)\n\nBIOMAN is a Greek machine manufacturer, based in Koropi, near Athens. It was founded in 1967 by two mechanical engineers and has focused on design and construction of a wide variety of lifting devices/material handling equipment (hydraulic and electric, platforms, lifts, forklifts, dumpers etc.). Its early products included the \"Tiger\" family of vehicles, consisting of light and heavy dumpers and forklifts (no longer produced).\n\n\n"}
{"id": "29138975", "url": "https://en.wikipedia.org/wiki?curid=29138975", "title": "Backshop", "text": "Backshop\n\nA backshop or back-shop is a specialized store or workshop found in service industries, such as locomotive and aircraft repair. Most repairs are carried out in small workshops, except where an industrial service is needed.\n\nIn the military, backshops repair parts known as shop-replaceable units (SRUs). These are commonly stocked subassemblies of a larger system, such as circuit cards components of a line-replaceable unit (LRU), designed to be repaired at the field level. Repair at this level is known as field-level maintenance or intermediate-level (I-level) maintenance. \nCalibration and repair of United States Air Force test equipment is conducted at shops known as precision measurement equipment laboratories.\n\n"}
{"id": "10398774", "url": "https://en.wikipedia.org/wiki?curid=10398774", "title": "BioFuels Security Act", "text": "BioFuels Security Act\n\nThe BioFuels Security Act is a proposed legislative Act of Congress intended to phase out current single-fueled vehicles in favor of flexible-fuel vehicles. Under this proposal, contemporary single-fuel vehicles would cease production in 2016. \nSenator Tom Harkin (on behalf of himself, and Senators Richard Lugar, Tim Johnson, Byron Dorgan, Joe Biden and Barack Obama), introduced this bill (S. 2817/109th) on March 16, 2006. This bill is still under consideration.\n\nThe bill would also require major US gasoline companies to carry E-85 renewable fuel (85 percent ethanol), and for 50 percent of their gasoline stations to extend and increase the Renewable Fuel Standards (RFS). To assist in this increase in FFVs, all major gas companies would be required to have 50 percent ethanol distribution tanks equipped to all their facilities. Gasoline companies would receive tax credits for meeting the requirements of distributing and changing pumps to ethanol (an increase from a 30 percent reduction to that of a 50 percent reduction).\n\n\n"}
{"id": "793384", "url": "https://en.wikipedia.org/wiki?curid=793384", "title": "Carbonic maceration", "text": "Carbonic maceration\n\nCarbonic maceration is a winemaking technique, often associated with the French wine region of Beaujolais, in which whole grapes are fermented in a carbon dioxide rich environment prior to crushing. Conventional alcoholic fermentation involves crushing the grapes to free the juice and pulp from the skin with yeast serving to convert sugar into ethanol. Carbonic maceration ferments most of the juice while it is still inside the grape, although grapes at the bottom of the vessel are crushed by gravity and undergo conventional fermentation. The resulting wine is fruity with very low tannins. It is ready to drink quickly but lacks the structure for long-term aging. In the most extreme case, such as with Beaujolais nouveau, the period between picking and bottling can be less than six weeks.\n\nDuring carbonic maceration, an anaerobic environment is created by pumping carbon dioxide into a sealed container filled with whole grape clusters. The carbon dioxide gas permeates through the grape skins and begins to stimulate fermentation at an intracellular level. The entire process takes place inside each single, intact berry. Ethanol is produced as a by-product of this process but studies have shown that other unique chemical reactions take place that have a distinctive effect on the wine. \n\nThe process of carbonic maceration occurs naturally in a partial state without deliberate intervention and has occurred in some form throughout history. If grapes are stored in a closed container, the force of gravity will crush the grapes on the bottom, releasing grape juice. Ambient yeasts present on the grape skins will interact with the sugars in the grape juice to start conventional ethanol fermentation. Carbon dioxide is released as a by product and, being denser than oxygen, will push out the oxygen through any permeable surface (such as slight gaps between wood planks) creating a mostly anaerobic environment for the uncrushed grape clusters to go through carbonic maceration. Some of the earliest documented studies on the process were conducted by the French scientist Louis Pasteur who noted in 1872 that grapes contained in an oxygen rich environment prior to crushing and fermentation produced wines of different flavors than grapes produced in a carbon dioxide rich environment. This was because the fermentation process had already started within the individual grape clusters prior to the introduction of yeasts during conventional fermentation.\n\nThe use of carbonic maceration is closely associated with the production of Beaujolais nouveau wine from the Gamay grape in the Beaujolais region and some wines in the Rioja Alavesa and Jumilla areas of Spain. This grape lends itself well to the production of simple, fruity wines and Beaujolais winemakers have been able to create a unique identity based on this distinctive style.\n\nProducers in other parts of France and in the New World have frequently utilized carbonic maceration for their own Gamay production, or with other grape varieties. Winemakers in the Languedoc and Rhône wine regions will sometimes employ the technique on coarse and tannic grapes like Carignan, especially if they are to be blended with other varieties. In California, the process is sometimes used with Valdiguié grapes, producing what is labeled a \"Nouveau\" wine.\n\nThe process is almost always used in conjunction with red wine production since some of the flavor compounds produced by volatile phenols tend to form undesirable flavors with white wine grape varieties.\n\nSemi-carbonic maceration is the winemaking technique where grapes are put through a short period of carbonic maceration, followed by conventional yeast fermentations. This is the process used in the production of Beaujolais nouveau wines. To an extent, most wines were historically treated to some form of semi or partial carbonic fermentation (as noted in history section above) with the amount dependent on the shape and size of the vessel that the grapes were stored in prior to crushing. The deeper the vessel, the greater the proportion of grapes that could be exposed to an anaerobic environment caused by the release of carbon dioxide from the crushed grapes on the bottom.\n\nAn alternative name for carbonic maceration is \"whole grape fermentation\", which is distinct from the process known as \"whole bunch fermentation\" which is common in the Burgundy wine production of Pinot noir. With \"whole bunch fermentation\", entire clusters of grapes (including stems) are fermented before being crushed. This creates a large \"cap\" of grape skins with pathways created by the stems that allows juice to flow more evenly through caps, increasing the levels of skin contact or maceration.\n\nCarbonic maceration techniques (e.g semi-carbonic maceration) were recently adapted to coffee processing. The ripe coffee cherries are placed inside a hermetic stainless steel tank and left to undergo an anaerobic fermentation. Such fermentation process brings out intense aroma, with a taste profile akin to red wine and whisky. These coffee beans usually produce full body cup. Known varieties, processed with semi-carbonic maceration are: Red and Yellow Catuai. Detailed information about fermentation techniques recently appeared in various specialty coffee media \n"}
{"id": "41743057", "url": "https://en.wikipedia.org/wiki?curid=41743057", "title": "Cheap meat", "text": "Cheap meat\n\nCheap meat is a term used to describe relatively inexpensive meat (e.g. fatty cuts of lamb or mutton) or to indicate that the consumer price of meat does not include the overall costs of industrial meat production. The term cheap meat is then either related to subsidies, to hidden costs or to non-material costs (\"moral cost\") of meat production. Non-material costs can be related to issues such as animal welfare (e.g. treatment of animals, over-breeding). The term is used by critics of the meat industry.\n\nThe meat industry is subsidized with billions of dollars by governments who support their meat industries. The OECD estimates the total \"producer support\" in OECD countries for 2012 as follows: 18bn USD for beef and veal, 7.3bn USD for pigmeat, 6.5bn USD for poultry and 1bn USD for sheepmeat (provisional numbers). Hidden costs of meat production can be related to the environmental impact of meat production and to the effect on human health (such as resistant antibiotics).\n\n"}
{"id": "9391536", "url": "https://en.wikipedia.org/wiki?curid=9391536", "title": "Cold start (computing)", "text": "Cold start (computing)\n\nCold start is a potential problem in computer-based information systems which involve a degree of automated data modelling. Specifically, it concerns the issue that the system cannot draw any inferences for users or items about which it has not yet gathered sufficient information.\n\nThe cold start problem is very well known and research in recommender systems. Recommender systems form a specific type of information filtering (IF) technique that attempts to present information items (e-commerce, movies, music, books, news, images, web pages) that are likely of interest to the user. Typically, a recommender system compares the user's profile to some reference characteristics. These characteristics may be related to item characteristics (content-based filtering) or the user's social environment and past behavior (collaborative filtering).\nDepending on the system, the user can be associated to various kinds of interactions: ratings, bookmarks, purchases, likes, number of page visits etc.\n\nThere are three cases of cold start :\nThe new community problem, or systemic bootstrapping, refers to the startup of the system, when virtually no information the recommender can rely upon is present.\nThis case presents the disadvantages of both the New user and the New item case, as all items and users are new.\nDue to this some of the techniques developed to deal with those two cases are not applicable to the system bootstrapping.\n\nThe item cold-start problem refers to when items added to the catalogue have either none or very little interactions. This constitutes a problem mainly for collaborative filtering algorithms due to the fact that they rely on the item's interactions to make recommendations. If no interactions are available then a pure collaborative algorithm cannot recommend the item. In case only a few interactions are available, although a collaborative algorithm will be able to recommend it, the quality of those recommendations will be poor.\nThis arises another issue, which is not anymore related to new items, but rather to \"unpopular items\".\nIn some cases (e.g. movie recommendations) it might happen that a handful of items receive an extremely high number of interactions, while most of the items only receive a fraction of them. This is referred to as popularity bias. \n\nIn the context of cold-start items the popularity bias is important because it might happen that many items, even if they have been in the catalogue for months, received only a few interactions. This creates a negative loop in which unpopular items will be poorly recommended, therefore will receive much less visibility than popular ones, and will struggle to receive interactions. While it is expected that some items will be less popular than others, this issue specifically refers to the fact that the recommender has not enough collaborative information to recommend them in a meaningful an reliable way.\n\nContent-based filtering algorithms, on the other hand, are in theory much less prone to the new item problem. Since content based recommenders choose which items to recommend based on the feature the items possess, even if no interaction for a new item exist, still its features will allow for a recommendation to be made.\nThis of course assumes that a new item will be already described by its attributes, which is not always the case. Consider the case of so called \"editorial\" features (e.g. director, cast, title, year), those are always known when the item, in this case movie, is added to the catalogue. However, other kinds of attributes might not be e.g. features extracted from user reviews and tags. Content-based algorithms relying on user provided features suffer from the cold-start item problem as well, since for new items if no (or very few) interactions exist, also no (or very few) user reviews and tags will be available.\n\nThe new user case refers to when a new user enrolls in the system and for a certain period of time the recommender has to provide recommendation without relying on the user's past interactions, since none has occurred yet.\nThis problem is of particular importance when the recommender is part of the service offered to users, since a user who is faced with recommendations of poor quality might soon decide to stop using the system before providing enough interaction to allow the recommender to understand his/her interests.\nThe main strategy in dealing with new users is to ask them to provide some preferences to build an initial user profile. A threshold has to be found between the length of the user registration process, which if too long might indice too many users to abandon it, and the amount of initial data required for the recommender to work properly. \n\nSimilarly to the new items case, not all recommender algorithms are affected in the same way.\nItem-item recommenders will be affected as they rely on user profile to weight how relevant other user's preferences are. Collaborative filtering algorithms are the most affected as without interactions no inference can be made about the user's preferences.\nUser-user recommender algorithms behave slightly differently. A user-user content based algorithm will rely on user's features (e.g. age, gender, country) to find similar users and recommend the items they interacted with in a positive way, therefore being robust to the new user case. Note that all these information is acquired during the registration process, either by asking the user to input the data himself, or by leveraging data already available e.g. in his social media accounts.\n\nDue to the high number of recommender algorithms available as well as system type and characteristics, many strategies to mitigate the cold-start problem have been developed. The main approach is to rely on hybrid recommenders, in order to mitigate the disadvantages of one category or model by combining it with another. \n\nAll three categories of cold-start (new community, new item, and new user) have in common the lack of user interactions and presents some commonalities in the strategies available to address them. \nA common strategy when dealing with new items is to couple a collaborative filtering recommender, for warm items, with a content-based filtering recommender, for cold-items. While the two algorithms can be combined in different ways, the main drawback of this method is related to the poor recommendation quality often exhibited by content-based recommenders in scenarios where it is difficult to provide a comprehensive description of the item characteristics. \nIn case of new users, if no demographic feature is present or their quality is too poor, a common strategy is to offer them non-personalized recommendations. This means that they could be recommended simply the most popular items either globally or for his specific geographical region or language.\n\nOne of the available options when dealing with cold users or items is to rapidly acquire some preference data. There are various ways to do that depending on the amount of information required. These techniques are called preference elicitation strategies.\nThis may be done either explicitly (by querying the user) or implicitly (by observing the user's behaviour). In both cases, the cold start problem would imply that the user has to dedicate an amount of effort using the system in its 'dumb' state – contributing to the construction of their user profile – before the system can start providing any intelligent recommendations. \n\nFor example MovieLens, a web-based recommender system for movies, asks the user to rate some movies as a part of the registration. \nWhile preference elicitation strategy are a simple and effective way to deal with new users, the additional requirements during the registration will make the process more time cosuming for the user. Moreover, the quality of the obtained prefences might not be ideal as the user could rate items he/she has seen months or years ago or the provided ratings could be almost random if the user provided them without paying attention just to complete the registration quickly.\n\nThe construction of the user's profile may also be automated by integrating information from other user activities, such as browsing histories or social media platforms. If, for example, a user has been reading information about a particular music artist from a media portal, then the associated recommender system would automatically propose that artist's releases when the user visits the music store.\n\nA variation of the previous approach is to automatically assign ratings to new items, based on the ratings assigned by the community to other similar items. Item similarity would be determined according to the items' content-based characteristics. \n\nIt is also possible to create initial profile of a user based on the personality characteristics of the user and use such profile to generate personalized recommendation.\nPersonality characteristics of the user can be identified using a personality model such as five factor model (FFM).\n\nAnother of the possible techniques is to apply active learning (machine learning). The main goal of active learning is to guide the user in the preference elicitation process in order to ask him to rate only the items that for the recommender point of view will be the most informative ones. This is done by analysing the available data and estimating the usefulness of the data points (e.g., ratings, interactions). \nAs an example, say that we want to build two clusters from a certain cloud of points. As soon as we have identified two points each belonging to a different cluster, which is the next most informative point? If we take a point close to one we already know we can expect that it will likely belong to the same cluster. If we choose a point which is in between the two clusters, knowing which cluster it belongs to will help us in finding where the boundary is, allowing to classify lots of other points with just a few observations.\n\nThe cold start problem is also exhibited by interface agents. Since such an agent typically learns the user's preferences implicitly by observing patterns in the user's behaviour – \"watching over the shoulder\" – it would take time before the agent may perform any adaptations personalised to the user. Even then, its assistance would be limited to activities which it has formerly observed the user engaging in.\nThe cold start problem may be overcome by introducing an element of collaboration amongst agents assisting various users. This way, novel situations may be handled by requesting other agents to share what they have already learnt from their respective users.\n\nIn recent years more advanced strategies have been proposed, they all rely on machine learning and attempt to merge the content and collaborative information in a single model.\nOne example of this approaches is called \"attribute to feature mapping\" which is tailored to matrix factorization algorithms. The basic idea is the following. A matrix factorization model represents the user-item interactions as the product of two rectangular matrices whose content is learned using the known interactions via machine learning. Each user will be associated to a row of the first matrix and each item with a column of the second matrix. The row or column associated to a specific user or item is called \"latent factors\". When a new item is added it has no associated latent factors and the lack of interactions does not allow to learn them, as it was done with other items. If each item is associated to some features (e.g. author, year, publisher, actors) it is possible to define an embedding function, which given the item features estimates the corresponding item latent factors. The embedding function can be designed in many ways and it is trained with the data already available from warm items. The same applies for a new user, as if some information is available for them (e.g. age, nationality, gender) then his/her latent factors can be estimated via an embedding function.\n\nAnother recent approach which bears similarities with feature mapping is building a hybrid content-based filtering recommender in which features, either of the items or of the users, are weighted according to the user's perception of importance. In order to identify a movie that the user could like, different attributes (e.g. which are the actors, director, country, title) will have different importance. As an example consider the James bond movie series, the main actor changed many times during the years, while some did not, like Lois Maxwell. Therefore, her presence will probably be a better identifier of that kind of movie than the presence of one of the various main actors. \nAlthough various techniques exist to apply feature weighting to user or item features in recommender systems, most of them are from the information retrieval domain like tf–idf, Okapi BM25, only a few have been developed specifically for recommenders.\n\nHybrid feature weighting techniques in particular are tailored for the recommender system domain. Some of them learn feature weight by exploiting directly the user's interactions with items, like FBSM. Others rely on an intermediate collaborative model trained on warm items and attempt to learn the content feature weights which will better approximate the collaborative model.\n\nMany of the hybrid methods can be considered special cases of factorization machines. \n\n\n"}
{"id": "4992395", "url": "https://en.wikipedia.org/wiki?curid=4992395", "title": "DAVIC", "text": "DAVIC\n\nDAVIC, Digital Audio Video Council, was founded in 1994 with the aim of promoting the success of interactive digital audio-visual applications and services by promulgating specifications of open interfaces and protocols that maximise interoperability, not only across geographical boundaries but also across diverse applications, services and industries. It was a non-profit international organization based in Switzerland.\n\nDAVIC was closed, according to its statutes, after 5 years of activity.\n\nAt the most DAVIC had 222 companies from more than 25 countries as members, although over its life 295 organisations were members at some stage. It represented all sectors of the audio-visual industry: manufacturing (computer, consumer electronics and telecommunications equipment) and service (broadcasting, telecommunications and CATV), as well as a number of government agencies and research organisations. \n\nThe four major sets of specifications culminated in 1999 with the 1.4 version. The 1.5 version of DAVIC was a set of additional tools and service definitions opening the road towards the setting of IP based audiovisual services such as the TV Anytime and TV Anywhere services. Version 1.3.1 was re-structured and was approved as an ISO/IEC standard and technical report (ISO/IEC 16500 and ISO/IEC TR 16501).\n\nThe management of DAVIC had proposed the continuation beyond the 5-year date with a program of work centred on two projects: \"TV Anytime\" and \"TV Anywhere\". The membership did not support the continuation of DAVIC but the idea of \"TV-Anytime\" continued as a new organisation. \n\n\n\n"}
{"id": "6890125", "url": "https://en.wikipedia.org/wiki?curid=6890125", "title": "Data auditing", "text": "Data auditing\n\nData auditing is the process of conducting a data audit to assess how company's data is fit for given purpose. This involves profiling the data and assessing the impact of poor quality data on the organization's performance and profits.\n"}
{"id": "52920035", "url": "https://en.wikipedia.org/wiki?curid=52920035", "title": "Digital banking", "text": "Digital banking\n\nDigital banking is part of the broader context for the move to online banking, where banking services are delivered over the internet. The shift from traditional to digital banking has been gradual and remains ongoing, and is constituted by differing degrees of banking service digitization. Digital banking involves high levels of process automation and web-based services and may include APIs enabling cross-institutional service composition to deliver banking products and provide transactions. It provides the ability for users to access financial data through desktop, mobile and ATM services.\n\nA digital bank represents a virtual process that includes online banking and beyond. As an end-to-end platform, digital banking must encompass the front end that consumers see, the back end that bankers see through their servers and admin control panels and the middleware that connects these nodes. Ultimately, a digital bank should facilitate all functional levels of banking on all service delivery platforms. In other words, it should have all the same functions as a head office, branch office, online service, bank cards, ATM and point of sale machines.\n\nThe reason digital banking is more than just a mobile or online platform is that it includes middleware solutions. Middleware is software that bridges operating systems or databases with other applications. Financial industry departments such as risk management, product development and marketing must also be included in the middle and back end to truly be considered a complete digital bank. Financial institutions must be at the forefront of the latest technology to ensure security and compliance with government regulations.\n\nThe earliest forms of digital banking trace back to the advent of ATMs and cards launched in the 1960s. As the internet emerged in the 1980s with early broadband, digital networks began to connect retailers with suppliers and consumers to develop needs for early online catalogues and inventory software systems.\n\nBy the 1990s the Internet emerged and online banking started becoming the norm. The improvement of broadband and ecommerce systems in the early 2000s led to what resembled the modern digital banking world today. The proliferation of smartphones through the next decade opened the door for transactions on the go beyond ATM machines. Over 60% of consumers now use their smartphones as the preferred method for digital banking.\n\nThe challenge for banks is now to facilitate demands that connect vendors with money through channels determined by the consumer. This dynamic shapes the basis of customer satisfaction, which can be nurtured with Customer Relationship Management (CRM) software. Therefore, CRM must be integrated into a digital banking system, since it provides means for banks to directly communicate with their customers.\n\nThere is a demand for end-to-end consistency and for services, optimized on convenience and user experience. The market provides cross platform front ends, enabling purchase decisions based on available technology such as mobile devices, with a desktop or Smart TV at home. In order for banks to meet consumer demands, they need to keep focusing on improving digital technology that provides agility, scalability and efficiency.\n\nTraditional banks are facing growing competition from FinTech startups, which are financial technology firms that are based on computer systems that facilitate banking and financial services. These companies have the potential for endless disruptive innovation. Examples of digital banking services and companies are:\n\n\nA study conducted in 2015 revealed that 47% of bankers see potential to improve customer relationship through digital banking, 44% see it as a means to generate competitive advantage, 32% as a channel for new customer acquisition. Only 16% emphasized the potential for cost saving.\n\nMajor benefits of digital benefits are:\n\n\nIn a contemporary Banking era, Digital is a buzzword and Banks have to stay in race for new-gen needs of digital banking Digital Banking is not only front end concepts such as Internet Banking, Mobile Banking, Direct Banking, Various Banking apps, use of Social Media in Banking, Artificial Intelligence, Robotics, Chat-bots, Cognitive computing, Block-chain, Big Data, voice biometrics etc ; however it also includes various back-end modernization programs are done to enable overall goals of digital Banking which includes legacy modernization, Integration, CRM, Document Imaging / OCR etc. Training course on Digital Banking at Udemy called \"Story of Digital Banking\" has video lectures on this. \n\nBanks are going through tremendous challenges of competition from non-banking companies and smaller Fintech companies. Therefore, in order to fight competition and stay ahead of competition in Digital Banking era, it is important for banks to work on not only good web site, social media connect and mobile banking etc; but they also need to innovate with new technology disruptions where AI, ML, Block-chain, Analyics, cloud become buzzwords. Book on A quick view of Global Digital Banking in Just 30 minutes published at amazon has more such details. Also book on \"Pillars of Digital Banking\" helps to focus on these areas. \n\nA key in which digital banks can gain a significant competitive edge is developing a more robust IT architecture. By replacing manual back-office procedures with automated software solutions, banks can reduce employee errors and speed up processes. This paradigm shift can lead to smaller operational units and allow managers to concentrate on improving tasks that require human intervention.\n\nAutomation reduces the need for paper, which inevitably ends up taking up space that can be occupied with technology. By using software that accelerates productivity up to 50%, banks can improve customer service since they will be able to resolve issues at a faster pace. One way a bank can improve its back end business efficiency is to divide hundreds of processes into three categories:\n\nIt still isn't practical to automate all operations for many financial firms, especially those that conduct financial reviews or provide investment advice. But the more a bank can replace cumbersome redundant manual tasks with automation, the more it can focus on issues that involve direct communication with customers. The obstacles currently preventing banks from investing in a more digital back end environment are:\n\n\nDigital cash eliminates many problems associated with physical cash, such as misplacement or the potential for money to be stolen or damaged. Additionally, digital cash can be traced and accounted for more accurately in cases of disputes. As consumers find an increasing number of purchasing opportunities at their fingertips, there is less need to carry physical cash in their wallets.\n\nOther indications that demand for digital cash is growing are highlighted by the use of peer-to-peer payment systems such as PayPal and the rise of untraceable cryptocurrencies such as bitcoin. Almost anything imaginable that can be paid with physical cash can theoretically be paid with the swipe of a bank card, including parking meters. The problem is this technology is still not omnipresent. Cash circulation grew in the United States by 42% between 2007 and 2012, with an average annual growth rate of 7%, according to the BBC.\n\nThe concept of an all digital cash economy is no longer just a futuristic dream but it's still unlikely to outdate physical cash in the near future. All digital banks are possible as a consumer option, but people may still have a need for physical cash in certain situations. ATMs help banks cut overhead, especially if they are available at various strategic locations beyond branch offices.\n\nEmerging forms of digital banking are\nThese solutions build on enhanced technical architectures as well as different business models.\n\nThe decision for banks to add more digital solutions at all operational levels will have a major impact on their financial stability. While not all banks are in a position to make quick changes to IT infrastructure or the architecture on top of it, banks aiming to be disrupters can move toward broad end-to-end automation can do so over about a six month time frame.\n\n"}
{"id": "26896318", "url": "https://en.wikipedia.org/wiki?curid=26896318", "title": "Dillo Dirt", "text": "Dillo Dirt\n\nDillo Dirt is a compost made by the City of Austin, Texas since 1989. It was the first program of its kind in the state and one of the oldest in the nation. Dillo Dirt is named after the nine-banded armadillo \"(Dasypus novemcinctus)\", which is a mammal native to Texas. It is also a trademarked product of the City of Austin Water Department.\n\nThe unique difference between Dillo Dirt and normal compost is that it contains treated municipal sewage sludge along with yard trimmings collected curbside by the City of Austin Resource Recovery Department. These are combined and composted to create Dillo Dirt. Despite this fact, Dillo Dirt meets all Texas and U.S. Environmental Protection Agency requirements for \"unrestricted\" use, which even includes vegetable gardens.\n\nThe heat generated in composting () is sufficient to virtually eliminate human and plant pathogens. After active composting for over a month, the compost is \"cured\" for several months, then screened to produce the finished product.\n\nAccording to the City of Austin, Dillo Dirt contains levels of heavy metals including Arsenic, Cadmium, Copper, Lead, Mercury, Molybdenum, Nickel, Selenium, and Zinc. In a separate toxicological analysis of Dillo Dirt, levels of the following pollutants were found: Beta-BHC, DDE, Dieldrin, Endrin aldehyde, Benzo(b)fluoranthene, Dibenz(a,h)anthracene, Benzo(a)anthracene, Indeno(1,2,3-cd)pyrene, and Bis(2-ethylhexyl)phthalate. Very few tests have been carried out on Dillo Dirt, so average pollutant, radioactivity, or carcinogen levels are generally unknown.\n\nSome opponents of the use and sale of Dillo Dirt claim that it contains above-normal amounts of heavy metals and fluoride which will inevitably find their way back into the human food supply. The city, however, states that the metal levels are well below the federal allowable levels. The city does not test for radioactivity or pharmaceutical residue because this is cost prohibitive. Since people routinely flush medications down the toilet against the city's recommendation, Dillo dirt may contain pharmaceutical residue in some batches. Some medical residue is radioactive from treating cancer patients. \n\nPrior to the Austin City Limits Music Festival, held at Zilker Park in Austin on October 4, 2009, the park's soil was resurfaced and amended with Dillo Dirt. During the festival, heavy rains created a large amount of sludgy surface mud, which some concertgoers claim caused them health issues, such as skin rashes, but in the end no conclusive evidence that the Dillo Dirt caused any problems was determined. The local Austin American Statesman newspaper ran an article advising anyone affected to call the health department.\n\n\n"}
{"id": "81036", "url": "https://en.wikipedia.org/wiki?curid=81036", "title": "Dishwasher", "text": "Dishwasher\n\nA dishwasher is a mechanical device for cleaning dishware and cutlery automatically. Unlike manual dishwashing, which relies largely on physical scrubbing to remove soiling, the mechanical dishwasher cleans by spraying hot water, typically between , at the dishes, with lower temperatures used for delicate items.\n\nA mix of water and dishwasher detergent is pumped to one or more rotating spray arms, which blast the dishes with the cleaning mixture. Once the wash is finished, the water is drained, more hot water enters the tub by means of an electro-mechanical solenoid valve, and the rinse cycle begins. After the rinse cycle finishes and the water is drained, the dishes are dried using one of several drying methods. Typically a rinse-aid, a chemical to reduce surface tension of the water, is used to reduce water spots from hard water or other reasons.\n\nIn addition to domestic units, industrial dishwashers are available for use in commercial establishments such as hotels and restaurants, where a large number of dishes must be cleaned. Washing is conducted with temperatures of and sanitation is achieved by either the use of a booster heater that will provide an \"final rinse\" temperature or through the use of a chemical sanitizer.\n\nA dishwasher has 6 main cycles usually with different options. Rinse and hold is a quick rinse cycle with no drying or detergent for a quick rinse off of dishes. Express is a 60-minute wash usually with drying for washing a small to medium-large load of pre rinsed dishes. Delicate is for a delicate load of prerinsed dishes. It is an energy effiecent cycle. It needs less detergent. It uses mild hot water and a mild dry. Normal is an energy efficient cycle too. It is meant for a regular load of normally stained dishes. It uses mild hot water to save energy and has only 1 or 2 rinses. It has a good drying cycle. Auto or Sensor wash is the recommended wash for most dishes. It uses a turbidity sensor to sense the soil on dishes. it adjusts the water and heat to adapt to the dishes soilness. This is mildly energy efficient. Heavy is meant for heavily soiled dishes and pots and pans. It also sanitizes the dishes on most dishwashers. It sprays a concentrated hot water jet and a super hot rinse cycle. Dishwashers also have additional options. Sanitize raises the rinse water to 160 °F and extends the drying time. This is meant for sick people and dishes exposed to raw meat. High temperature wash raises the wash temperature for a better wash. Extra dry makes the dishes dry better.\n\nUsers operate dishwashers by placing dishes in the dishwasher racks, adding dishwasher detergent, turning on the device, then removing the clean dishes once the cycle is completed.\n\nDishwasher use starts with installation of the appliance. Most home users fix their dishwashers in one place, such as under a countertop. However, portable machines are available that may be rolled up to the sink, with a hose attaching it to the kitchen sink faucet.\n\nFor dishwashers with a built-in food waste disposer, pre-rinsing or scraping is not necessary. Large wastes are tilted off the dishes before loading. For machines lacking a built-in food waste disposal, the user scrapes dishes before loading. Since the early 1960s, manufacturers have designed consumer dishwashers for use without pre-rinsing or pre-washing. Soft food wastes are ground up by the machine and then discharged with the drain water. In the early eighties machines were introduced with \"hard food waste disposers\" that are capable of grinding harder food wastes such as seeds, popcorn hulls, etc. Dishwashers are designed to hold different dishes in different places. For the most commonly installed two-rack consumer style of dishwasher, the user loads cups, bowls, and small dishes onto the top rack. Eating utensils go in the bottom rack into a container, with pointed ends down for safety. Some manufacturers use reverse rack loading, with plates, large dishes, and utensils being loaded in the top rack, while glassware and smaller items go into the lower rack. \nSome dishwashers have a third rack for utensils. Heavier dishes go on the bottom rack, with large pots facing downward toward the spray nozzle. After the dishwasher is loaded, the user puts dishwasher detergent into the machine. Many contemporary dishwashers use sensors to determine how much water, time and temperature is required.\n\nDishwashers and the detergents used in dishwashers are not designed for use with some materials. The washing cycle's heat and chemicals can harm kitchen knives and non-stick surface pans. Detergents have their own usage restrictions, including not being safe for cleaning various materials like wood or certain metals.\n\nThe first mechanical dishwashing device was registered in 1850 in the United States by Joel Houghton. This device was made of wood and was cranked by hand while water sprayed onto the dishes. This device was both slow and unreliable. Another patent was granted to L.A. Alexander\nin 1865 that was similar to the first but featured a hand-cranked rack system. Neither device was practical or widely accepted.\n\nHowever, the most successful of the hand-powered dishwashers was invented in 1887 by Josephine Cochrane together with mechanic George Butters in Josephine's tool shed in Shelbyville, Illinois when Cochrane (a wealthy socialite) wanted to protect her china while it was being washed. Her invention was unveiled at the 1893 World's Fair in Chicago, Illinois under the name of Lavadora but was changed to Lavaplatos as another machine invented in 1858 already held that name. This machine is what everyone now knows as the washing machine. Cochrane's inspiration was her frustration at the damage to her good china that occurred when her servants handled it during cleaning.\n\nEurope's first domestic dishwasher with an electric motor was invented and manufactured by Miele in 1929.\n\nIn the United Kingdom, William Howard Livens invented a small, non-electric dishwasher suitable for domestic use in 1924. It was the first dishwasher that incorporated most of the design elements that are featured in the models of today; it included a front door for loading, a wire rack to hold the dirty crockery and a rotating sprayer. Drying elements were even added to his design in 1940. It was the first machine suitable for domestic use, and it came at a time when permanent plumbing and running water in the house was becoming increasingly common.\n\nDespite this, Liven's design did not become a commercial success, and dishwashers were only successfully sold as domestic utilities in the postwar boom of the 1950s, albeit only to the wealthy. Initially dishwashers were sold as standalone or portable devices, but with the development of the wall-to-wall countertop and standardized height cabinets, dishwashers began to be marketed with standardized sizes and shapes, integrated underneath the kitchen countertop as a modular unit with other kitchen appliances.\n\nBy the 1970s, dishwashers had become commonplace in domestic residences in North America and Western Europe. By 2012, over 75 percent of homes in the United States and Germany had dishwashers.\n\nIn the late 1990s, manufactures began offering various new energy conservation features in dishwashers. One feature was use of \"soil sensors\", which was a computerized tool in the dishwasher which measured food particles coming from dishes. When the dishwasher had cleaned the dishes to the point of not releasing more food particles, then the soil sensor would report the dishes being cleaned. The sensor operated with another innovation of using variable washing time. If dishes were especially dirty, then the dishwasher would run for a longer time than if the sensor detected them to be clean. In this way, the dishwasher saves energy and water by only being in operation for as long as needed.\n\nDishwashers that are installed into standard kitchen cabinets have a standard width and depth of 60 cm (Europe) or 24 inches (US), and most dishwashers must be installed into a hole a minimum of 86 cm (Europe) or 34 inches (US) tall. Portable dishwashers exist in 45 and 60 cm (Europe) or 18 and 24 inch (US) widths, with casters and attached countertops. Dishwashers may come in standard or tall tub designs; standard tub dishwashers have a service kickplate beneath the dishwasher door that allows for simpler maintenance and installation, but tall tub dishwashers have approximately 20% more capacity and better sound dampening from having a continuous front door.\n\nThe international standard for the capacity of a dishwasher is expressed as standard place settings. Commercial dishwashers are rated as plates per hour. The rating is based on standard sized plates of the same size. The same can be said for commercial glass washers, as they are based on standard glasses, normally pint glasses.\n\nPresent-day machines feature a drop-down front panel door, allowing access to the interior, which usually contains two or sometimes three pull-out racks; racks can also be referred to as \"baskets\". In older U.S. models from the 1950s, the entire tub rolled out when the machine latch was opened, and loading/removing washable items was from the top, with the user reaching deep into the compartment for some items. Youngstown Kitchens, which manufactured entire kitchen cabinets and sinks, offered a tub-style dishwasher, which was coupled to a conventional kitchen sink as one unit. Most present day machines allow for placement of dishes, silverware, tall items and cooking utensils in the lower rack, while glassware, cups and saucers are placed in the upper rack. One notable exception were dishwashers produced by the Maytag Corporation from the late sixties until the early nineties. These machines were designed for loading glassware, cups and saucers in the lower rack, while plates, silverware and tall items were placed into the upper rack. This unique design allowed for a larger capacity and more flexibility in loading of dishes and pots and pans. Today, \"dish drawer\" models eliminates the inconvenience of the long reach that was necessary with older full-depth models. \"Cutlery baskets\" are also common. A drawer dishwasher, first introduced by Fisher & Paykel in 1997, is a variant of the dishwasher in which the baskets slide out with the door in the same manner as a drawer filing cabinet, with each drawer in a double-drawer model being able to operate independently of the other.\n\nThe inside of a dishwasher in the North American market is either stainless steel or plastic. Stainless steel tubs resist hard water, and preserve heat to dry dishes more quickly. They also come at a premium price. Older models used baked enamel tubs, while some used a vinyl coating bonded to a steel tub, which provided protection of the tub from acidic foods and provided some sound attenuation. European-made dishwashers feature a stainless steel interior as standard, even on low-end models. The same is true for a built-in water softener.\n\nEuropean dishwashers almost universally use two or three spray arms which are fed from the bottom and back wall of the dishwasher leaving both racks unimpeded and also such models tend to use inline water heaters, removing the need for exposed elements in the base of the machine that can melt plastic items near to them. Many North American dishwashers tend to use exposed elements in the base of the dishwasher. Some North American machines, primarily those designed by General Electric, use a wash tube, often called a wash-tower, to direct water from the bottom of the dishwasher to the top dish rack. Some dishwashers, including many models from Whirlpool and KitchenAid, use a tube attached to the top rack that connects to a water source at the back of the dishwasher, and directs water to a second wash arm beneath the upper rack, this allows full use of the bottom rack. Late-model Frigidaire dishwashers shoot a jet of water from the top of the washer down into the upper wash arm, again allowing full use of the bottom rack (but requiring that a small funnel on the top rack be kept clear).\n\nMid-to-higher end North American dishwashers often come with hard food disposal units, which behave like miniature garbage (waste) disposal units that eliminate large pieces of food waste from the wash water. One manufacturer that is known for omitting hard food disposals is Bosch, a German brand; however, Bosch does so in order to reduce noise. If the larger items of food waste are removed before placing in the dishwasher, pre-rinsing is not necessary even without integrated waste disposal units.\n\nMany new dishwashers feature microprocessor-controlled, sensor-assisted wash cycles that adjust the wash duration to the quantity of dirty dishes (sensed by changes in water temperature) or the amount of dirt in the rinse water (sensed chemically or optically). This can save water and energy if the user runs a partial load. In such dishwashers the electromechanical rotary switch often used to control the washing cycle is replaced by a microprocessor, but most sensors and valves are still required. However, pressure switches (some dishwashers use a pressure switch and flow meter) are not required in most microprocessor controlled dishwashers as they use the motor and sometimes a rotational position sensor to sense the resistance of water; when it senses there is no cavitation it knows it has the optimal amount of water. A bimetal switch or wax motor opens the detergent door during the wash cycle.\n\nSome dishwashers include a child-lockout feature to prevent accidental starting or stopping of the wash cycle by children. A child lock can sometimes be included to prevent young children opening the door during a wash cycle. This prevents accidents with hot water and strong detergents used during the wash cycle.\n\nIn the European Union, the energy consumption of a dishwasher for a standard usage is shown on a European Union energy label. In the United States, the energy consumption of a dishwasher is defined using the energy factor.\n\nMost consumer dishwashers use a 75 °C (167 °F) thermostat in the sanitizing process. During the final rinse cycle, the heating element and wash pump are turned on, and the cycle timer (electronic or electromechanical) is stopped until the thermostat is tripped. At this point, the cycle timer resumes and will generally trigger a drain cycle within a few timer increments.\n\nMost consumer dishwashers use 75 °C (167 °F) rather than 83 °C (181 °F) for reasons of burn risk, energy and water consumption, total cycle time, and possible damage to plastic items placed inside the dishwasher. With new advances in detergents, lower water temperatures (50–55 °C / 122–131 °F) are needed to prevent premature decay of the enzymes used to eat the grease and other build-ups on the dishes.\n\nIn the US, residential dishwashers can be certified to a NSF International testing protocol which confirms the cleaning and sanitation performance of the unit.\n\nThe heat inside the dishwasher dries the contents after the final hot rinse; the final rinse adds a small amount of rinse-aid to the hot water, as this improves drying significantly by reducing the inherent surface tension of the water. Plastic and non-stick items form drops with smaller surface area and may not dry properly compared to china and glass, which also store more heat that better evaporate the little water that remains on them. Some dishwashers incorporate a fan to improve drying. Older dishwashers with a visible heating element (at the bottom of the wash cabinet, below the bottom basket) may use the heating element to improve drying; however, this uses more energy.\n\nNorth American dishwashers tend to use heat-assisted drying via an exposed element. European machines and some high end North American machines use passive methods for drying – a stainless steel interior helps this process and some models use heat exchange technology between the inner and outer skin of the machine to cool the walls of the interior and speed up drying. Most dishwashers feature a drying sensor and as such, a dish-washing cycle is always considered complete when a drying indicator, usually in the form of an illuminated \"end\" light, or in more modern models on a digital display or audible sound, exhibits to the operator that the washing and drying cycle is now over.\n\nGovernmental agencies often recommend air-drying dishes by either disabling or stopping the drying cycle to save energy.\n\nDishwashers are designed to work using specially formulated dishwasher detergent. Over time, many regions have banned the use of phosphates in detergent and phosphorus-based compounds. They were previously used because they have properties that aid in effective cleaning. The concern was the increase in algal blooms in waterways caused by increasing phosphate levels (see eutrophication). Seventeen US states have partial or full bans on the use of phosphates in dish detergent, and two US states (Maryland and New York) ban phosphates in commercial dishwashing. Detergent companies claimed it is not cost effective to make separate batches of detergent for the states with phosphate bans (although detergents are typically formulated for local markets), and so most have voluntarily removed phosphates from all dishwasher detergents.\n\nIn addition, rinse aids have contained nonylphenol and nonylphenol ethoxylates. These have been banned in the European Union by EU Directive 76/769/EEC.\n\nIn some regions depending on water hardness a dishwasher might function better with the use of a dishwasher salt.\n\nGlassware washed by dishwashing machines can develop a white haze on the surface over time. This may be caused by any or all of the below processes, of which only the first is reversible:\n\nOther materials besides glass are also harmed by the strong detergents, strong agitation, and high temperatures of dishwashers, especially on a hot wash cycle when temperatures can reach 75 °C (167 °F). Aluminium, brass, and copper items will discolor, and light aluminum containers will mark other items they knock into. Nonstick pan coatings will deteriorate. Glossy, gold-colored, and hand-painted items will be dulled or fade. Fragile items and sharp edges will be dulled or damaged from colliding with other items and/or thermal stress. Sterling silver and pewter will oxidize and discolour from the heat and from contact with metals lower on the galvanic series such as stainless steel. Pewter has a low melting point and may warp in some dishwashers. Glued items, such as hollow-handle knives or wooden cutting boards, will melt or soften in a dishwasher; high temperatures and moisture damage wood. High temperatures damage many plastics, especially in the bottom rack close to an exposed heating element (many newer dishwashers have a concealed heating element away from the bottom rack entirely). Squeezing plastic items into small spaces may cause the plastic to distort in shape. Cast iron cookware is normally seasoned with oil or grease and heat, which causes the oil or grease to be absorbed into the pores of the cookware, thereby giving a smooth relatively non-stick cooking surface which is stripped off by the combination of alkali based detergent and hot water in a dishwasher.\nKnives and other cooking tools that are made of carbon steel, semi-stainless steels like D2, or specialized, highly hardened cutlery steels like ZDP189 corrode in the extended moisture bath of dishwashers, compared to briefer baths of hand washing. Cookware is made of austenitic stainless steels, which are more stable.\nItems contaminated by chemicals such as wax, cigarette ash, poisons, mineral oils, wet paints, oiled tools, furnace filters, etc. can contaminate a dishwasher, since the surfaces inside small water passages cannot be wiped clean as surfaces are in hand-washing, so contaminants remain to affect future loads. Objects contaminated by solvents may explode in a dishwasher.\n\nDishwashers use less water, and therefore less fuel to heat the water, than hand washing, except for small quantities washed in wash bowls without running water.\n\nHand-washing techniques vary by individual. According to a peer-reviewed study in 2003, hand washing and drying of an amount of dishes equivalent to a fully loaded automatic dishwasher (no cookware or bakeware) could use between of water and between 0.1 and 8 kWh of energy, while the numbers for energy-efficient automatic dishwashers were and 1 to 2 kWh, respectively. The study concluded that fully loaded dishwashers use less energy, water, and detergent than the average European hand-washer. For the automatic dishwasher results, the dishes were not rinsed before being loaded. The study does not address costs associated with the manufacture and disposal of dishwashers, the cost of possible accelerated wear of dishes from the chemical harshness of dishwasher detergent, the comparison for cleaning cookware, or the value of labour saved; hand washers needed between 65 and 106 minutes. Several points of criticism on this study have been raised. For example, kilowatt hours of electricity were compared against energy used for heating hot water without taking into account possible inefficiencies. Also, inefficient human washers were compared against optimal usage of a fully loaded dishwasher without manual pre-rinsing that can take up to of water.\n\nA 2009 study showed that the microwave and the dishwasher were both more effective ways to clean domestic sponges than handwashing.\n\nLarge heavy duty dishwashers are available for use in commercial establishments (e.g. hotels, restaurants) where a large number of dishes must be cleaned.\n\nUnlike a residential dishwasher, a commercial dishwasher does not utilize a drying cycle (commercial drying is achieved by heated ware meeting open air once the wash/rinse/sanitation cycles have been completed) and thus are significantly faster than their residential counterparts. Washing is conducted with 65–71 °C / 150–160 °F temperatures and sanitation is achieved by either the use of a booster heater that will provide the machine 82 °C / 180 °F \"final rinse\" temperature or through the use of a chemical sanitizer. This distinction labels the machines as either \"high-temp\" or \"low-temp\".\n\nSome commercial dishwashers work similarly to a commercial car wash, with a pulley system that pulls the rack through a small chamber (known widely as a \"rack conveyor\" systems). Single-rack washers require an operator to push the rack into the washer, close the doors, start the cycle, and then open the doors to pull out the cleaned rack, possibly through a second opening into an unloading area.\n\nIn the UK, the British Standards Institution set standards for dishwashers. In the US, NSF International (an independent not-for-profit organization) sets the standards for wash and rinse time along with minimum water temperature for chemical or hot-water sanitizing methods. There are many types of commercial dishwashers including under counter, single tank, conveyor, flight type, and carousel machines.\n\nCommercial dishwashers often have significantly different plumbing and operations than a home unit, in that there are often separate spray arms for washing and rinsing/sanitizing. The wash water is heated with an in-tank electric heat element and mixed with a cleaning solution, and is used repeatedly from one load to the next. The wash tank usually has a large strainer basket to collect food debris, and the strainer may not be emptied until the end of the day's kitchen operations.\n\nWater used for rinsing and sanitizing is generally delivered directly through building water supply, and is not reusable. The used rinse water falls into the wash tank reservoir, which dilutes some of the used wash water and causes a small amount to drain out through an overflow tube. The system may first rinse with pure water only, and then sanitize with an additive solution that is left on the dishes as they leave the washer to dry.\n\nAdditional soap is periodically added to the main wash water tank, from either large soap concentrate tanks or dissolved from a large solid soap block, to maintain wash water cleaning effectiveness.\n\nDishwashers can be used to cook certain foods, in particular salmon. They can clean toothbrushes, toys, some sporting goods, switchplate covers, vent covers, and grilles.\n\n\n"}
{"id": "1984211", "url": "https://en.wikipedia.org/wiki?curid=1984211", "title": "E and M signaling", "text": "E and M signaling\n\nE and M signaling is a type of supervisory line signaling that uses DC signals on separate leads, called the \"E\" lead and \"M\" lead, traditionally used in the telecommunications industry between telephone switches. Various mnemonic names have been used to memorize these letters, such as \"Ear\" and \"Mouth\", the most common variation.\n\nE&M was originally developed to allow PABXs in different geographic locations to communicate over an analog private circuit. Some digital interfaces such as Channel Associated Signaling also use versions of E&M signaling. E&M is considered an obsolete technology for new installations, which generally use Basic Rate (BRI) or Primary Rate (PRI) digital interfaces.\n\nThe E&M standards were initially developed by Bell Labs and extended by national PTT administrations. The standard defines two sides to the interface:\n\nThe signaling unit and trunk circuit communicate their status over the E and M leads, using a combination of battery and earth (also known as ground). The Battery signal used in the standard is nominally −48VDC. All E&M installations require that the positive terminal of the battery is connected to a shared reliable earth. The maximum distance between the signaling unit and the trunk interface is determined by the resistance of the wire, but will normally be less than 100m for adequate noise immunity.\n\nThe group of E&M signaling includes the following variations:\n\nE&M defines eight wires:\n\n\"4-wire E&M\" uses a 4-wire (2-pair) transmission path for the voice signal.\n\"2-wire E&M\" uses a single pair for both transmit and receive voice signal. This is much inferior to 4-wire E&M as the 2-wire interface uses hybrid transformers which reduce signal quality and can introduce echo.\n\nThe mechanisms described so far only allow circuit seizure – on-hook and off-hook – to be signaled. In order to allow dialing over the interface, \"start\" signaling mechanisms are defined. This allows the other end to know when to send the dialed digits, which are transmitted by pulse (loop disconnect) or multi-frequency tones. E&M defines three methods of \"start\" signaling:\n\n\nThe choice of letters for the E and M leads was fortuitous, unrelated to any names or meanings. However, various names have been associated with the letters \"E\" and \"M\":\n\n\n"}
{"id": "1095715", "url": "https://en.wikipedia.org/wiki?curid=1095715", "title": "Earthworks (company)", "text": "Earthworks (company)\n\nEarthworks is a professional audio equipment company founded by David E. Blackmer in the mid 1990s after he left dbx to design studio microphones, preamplifiers and studio reference monitors.\n\nTheir products are based on Blackmer's research into ultrasonic frequencies in sound reproduction and his claims that the time resolution of human hearing is 5 microseconds or better - corresponding to 200 kHz, well above the 20 kHz conventionally accepted as the limit of human hearing.\n\n\n"}
{"id": "17477012", "url": "https://en.wikipedia.org/wiki?curid=17477012", "title": "Eduardo Bradley", "text": "Eduardo Bradley\n\nEduardo Bradley (1887–1951) was an Argentine pilot and balloonist who in 1916 made the first balloon crossing of the Andes. He was a leading figure in the founding of civil aviation in South America.\n\nBorn in the city of La Plata, Argentina on April 9, 1887, Bradley was the son of Tomás Bradley Sutton, veteran of the Paraguayan War, and Mary Hayes O’Callaghan. He began his pilot's career alongside Jorge Newbery. His brevet was the first issued following the newly created regulations of the International Civil Aviation Organization. Shortly after Newbery's death in 1914, he set out to honor his late friend by attempting to cross the Andes in an aerostat. Already an experienced balloonist, Bradley had made over one hundred ascensions and set records for altitude (6,900 meters), flight duration (28 hours, 10 minutes), and distance covered in-flight (900 km, to Rio Grande do Sul from Buenos Aires).\n\nPreliminary studies had determined the crossing should be eastbound, which was the direction the winds carried at the altitude required. This later required moving to Santiago de Chile to make the necessary arrangements. Eduardo Bradley presented his plans to Aero Club Argentino, which eventually provided two balloons and the necessary gear to produce high volumes of hydrogen. Although the balloons turned out to be usable, the hydrogen-producing gear was absolutely worthless. The larger of the two balloons, named \"Eduardo Newbery\" (after an Argentine aviation pioneer), was chosen by Bradley for the flight. The second balloon, named \"Teniente Origone\", was used for testing prior to the actual crossing of the Andes.\n\nBradley's first choice for co-pilot was Julio Crespo Vivot, a seasoned aviator with whom he had flown while setting the record for altitude, but Vivot refused to come along on the adventure. In his stead, Bradley selected Angel María Zuloaga, a young army lieutenant.\n\nOnce in Chile the crew had serious difficulties with the generation of hydrogen, due to the accidental loss of most of the sulfuric acid imported from Argentina. Bradley decided to resort to coal gas, produced in Chile and readily available, with a high percentage of hydrogen. The odyssey would have been impossible without the cooperation of Chilean authorities, who responded more favorably to Bradley's request for support than did officials in his native Argentina. The crossing of the Andes on an aerostat filled with coal gas was finally accomplished on June 24, 1916. At an altitude of 8,100m, the temperature dropped to -30 °C. The adventure lasted three and a half hours from the moment of liftoff in Santiago to the landing in Cerro de la Cepa, Uspallata, Mendoza, Argentina. Bradley and Zuloaga were welcomed in Argentina as national heroes.\n\nThe years following the crossing of the Andes were dedicated by Bradley to developing the local civil aviation industry to which he devoted the rest of his life. He pioneered NYRBA, a company founded by Ralph O’Neil, in Argentina, and was manager for Pan American Grace Airways and, later, president of Pan-American Argentina.\nOn September 4, 1929, Bradley, then serving as secretary of the Argentine department of aviation, became the first passenger on a direct flight from Buenos Aires to Miami, flying Pan American. The journey lasted 56 hours. The purpose of his visit to the US was to compete for a second time in the Gordon Bennett Cup in ballooning. Eduardo Bradley died in Buenos Aires on June 3, 1951, and is buried in the Cementerio de la Recoleta.\n\n\n"}
{"id": "27955117", "url": "https://en.wikipedia.org/wiki?curid=27955117", "title": "Engineering optimization", "text": "Engineering optimization\n\nEngineering optimization is the subject which uses optimization techniques to achieve design goals in engineering. It is sometimes referred to as design optimization.\n\n\n"}
{"id": "23722922", "url": "https://en.wikipedia.org/wiki?curid=23722922", "title": "FOAK", "text": "FOAK\n\nFOAK is an acronym for First of a Kind. It is used in engineering economics where the first item or generation of items using a new technology or design can cost significantly more than later items or generations, which are called \"NOAK\" an acronym for \"nth of a kind\".\n\n"}
{"id": "21573927", "url": "https://en.wikipedia.org/wiki?curid=21573927", "title": "Fairfield Materials Management Ltd", "text": "Fairfield Materials Management Ltd\n\nFairfield Materials Management Ltd is a Manchester based social enterprise that operates a community waste management project at Manchester’s New Smithfield Market focused on minimising waste, and bringing social and environmental benefits to Greater Manchester.\n\nBy utilising 'in-vessel composting', Fairfield Materials Management established the UK’s first sustainable biodegradable waste management system to operate on a wholesale market, diverted 16,500 tonnes of organic market waste material away from landfill between 2003 and 2008.\n\nFairfield has developed a composting production model that processes fruit, vegetable, plant and woody waste into peat-free, British Standards Institution PAS 100 accredited compost. It operates on a site fully licensed by the Environment Agency.\n\nIn 2003 Fairfield was recognised as the most innovative social enterprise within its sector by The Composting Association.\n\nFairfield Materials Management was founded in 2003 by a small group of ecological activists, horticulturists and social entrepreneurs.\n\nFairfield Materials Management was initiated by Val Rawlinson (Director) after she became involved in an East Manchester anti-incineration campaign in 1996. The incinerator was not built, but Lucy Danger and Val Rawlinson realised that they had to show Manchester that environmental and social alternatives were available and working successfully throughout the United Kingdom and so Fairfield Composting was established by Val who began promoting home composting and supplying compost bins. and Lucy set up EMERGE Recycling recruiting Val as a director.\n\nVal also recognised the need for a commercial sized composting system for New Smithfield Market and choose a Vertical Composting Unit (VCU) system as the preferred technology; Emma Smith was recruited as a waste auditor in November 2001 to undertake a waste composition analysis of Smithfield Market's waste. Chris Walsh joined the team and between 2001 and 2003 the group secured several hundred thousand pounds through grants and loans, designed the site, obtaining planning permission and a waste management licence.\n\nIn 2003 Fairfield began operating and initially diverted waste from four market traders, taking in green waste from Manchester City Council and producing compost. In 2004 a further two composting units were added and in 2005 a final three units were added to the system. This enabled Fairfield to process all of the market's fruit and vegetable waste through the in-situ technology. Whilst remaining a director of Fairfield Val set up Debdale Eco Centre(DEC) and with Emma's help secured funding to promote home composting. WRAP funding later enabled DEC to run a project supplying low cost compost bins which achieved high home composting levels. When Manchester City Council working in partnership with BIFFA introduced the Green Bin home composting levels were reduced negating the work undertaken by Val. Val retired from DEC aged 70 in 2010 but continues as a Director of Fairfield \n\nThe business became sustainable through the efforts of Ian Trippier who was recruited as office manager bringing his vast experience of social enterprises and business acumen to the organisation. Ian and Eric Lowry, Operations Manager, have proved an invaluable partnership raising standards in Waste Management and turning Fairfield into a very successful organisation. \n\nIn 2009 the Fairfield Group in partnership national renewable energy business Bio Group Ltd secured planning permission in Bredbury, Stockport for their second waste management site.\n\n"}
{"id": "284732", "url": "https://en.wikipedia.org/wiki?curid=284732", "title": "Fairlight CMI", "text": "Fairlight CMI\n\nThe Fairlight CMI (short for Computer Musical Instrument) is a digital synthesizer, sampler and digital audio workstation introduced in 1979 by Fairlight. It was based on the commercial licence of Qasar M8 dual-MC6800 microprocessor instrument developed by Tony Furse of Creative Strategies in Sydney, Australia. It was one of the earliest music workstations with an embedded digital sampling synthesizer. It rose to prominence in the early 1980s and competed with the Synclavier from New England Digital.\n\nIn the 1970s, synthesizer devotee Kim Ryrie initiated the idea to develop a build-it-yourself analogue synthesizer called the ETI 4600 for his family's magazine \"Electronics Today International\". The detailed design was developed by ETI's Barty Wilkinson and Trevor Marshall but Ryrie was frustrated with the limited number of sounds that could be made with an analogue synthesizer. After his classmate, Peter Vogel, graduated from high school, and a brief stint at university in 1975, Ryrie asked Vogel if he would be interested in making \"the world's greatest synthesiser\" based on the recently announced microprocessor. He recalled: \"We had long been interested in computers - I built my first computer when I was about 12 - and it was obvious to me that combining digital technology with music synthesis was the way to go.\" \n\nIn December that year, he and Vogel formed a house-based company intended to manufacture digital synthesizers. They named it Fairlight after the hydrofoil ferry passing before Ryrie's grandmother's home in Sydney harbour. The two planned to design a digital synthesizer that could create sounds reminiscent of acoustic instruments (physical modelling synthesis). They had initially thought of making an analogue synth that was digitally controlled, given that the Moog was much more difficult to control.\n\nAfter the six months that followed involving the two in the company's basement where initial designs included a sample touch-sensitive keyboard and Vogel's video products to help pay the bills they met Motorola consultant Tony Furse. In association with the Canberra School of Electronic Music, Furse built a digital synthesizer that used two 8-bit Motorola 6800 microprocessors, as well as the light pen and some of the graphics that would later be a part of the Fairlight CMI. Despite this, the machine was unable to create harmonic partials, therefore the sounds that came from the synth were sterile and inexpressive. \n\nVogel and Ryrie licensed the design to help them make a digital synthesizer, mainly for its processing power, and decided to use microprocessor technology instead of analogue synthesis. Over the course of a year, the duo made what Ryrie called a \"research design\", the bulky, expensive, and unmarketable eight-voice synthesizer QASAR M8, which included a two-by-two-by-four foot processing box and a keyboard. \n\nIn 1978, Vogel and Ryrie were making \"interesting\" but unrealistic sounds. Vogel decided they might be able to learn how to synthesize an instrument by studying the harmonics of real instrument, and sampled around a second of a piano piece from a radio broadcast. He discovered that, by playing the sample back at different pitches, it sounded much more realistic. He recalled in 2005:\n\nWith the Fairlight CMI, Vogel and Ryrie were able to produce an endless amount of sounds, but control was limited to attack, sustain, decay (ADSR) and vibrato. According to Ryrie, \"We regarded using recorded real-life sounds as a compromise - as cheating - and we didn't feel particularly proud of it.\" They continued to work while making money by creating and distributing computers for offices in the Sydney suburb of Ermington, which Ryrie described as \"a horrendous exercise, but we sold 120 of them\". \n\nIn addition to the keyboard, processing, computer graphics and interactive pen borrowed from Furse's synthesizer, the pair added a QWERTY keyboard, and a large one-by-1.5-by-three foot box stored the sampling, processing and ADC/DAC Hardware and the 8 inch floppy diskette. According to a magazine feature about the Fairlight company, the biggest problem was the short sample length, which typically lasted from a half of to an entire second; it could only handle a sample rate of 24 kilohertz and a frequency response of ten kilohertz at most, so a sample rate had to be as low as eight kilohertz and a bandwidth of 3,500 hertz for sounds of longer length to be used. However, Vogel felt the low quality of the sounds was what gave them their own character. \n\nThe Music Composition Language feature was also criticized as too difficult for empirical users. Other primitive aspects included its limited amount of RAM (208 kilobytes) and its green and black graphics. Nonetheless, the CMI garnered significant attention from Australian distributors and consumers for being able to emulate sounds of acoustic instruments, as well as for its light pen and three-dimensional sound visualization. Still, Vogel was unsure if there would be enough interest in the product. The CMI's ability to emulate real instruments made some refer to it as an \"orchestra-in-a-box\", and each unit came with eight-inch, 500-kilobyte floppy disks that each stored twenty-two samples of orchestral instruments. The Fairlight CMI also garnered publicity in the science industry, being featured on the BBC science and technology series \"Tomorrow's World\"; given that futuristic theories of poor-sounding digital orchestras were also being made, Musicians' Union railed against the CMI who called it a \"lethal threat\" towards its members.\n\nIn the summer of 1979, Vogel went to the home of English singer-songwriter Peter Gabriel, where his third solo studio album was being recorded, to show him the Fairlight CMI. Gabriel, as well as many other people in the studio, was instantly engrossed by it, and he used strange sounds such as breaking glass bottles and bricks on the album. One of those present for the demonstration, Stephen Paine, recalled in 1996: \"The idea of recording a sound into solid-state memory and having real-time pitch control over it appeared incredibly exciting. Until that time everything that captured sound had been tape-based. The Fairlight CMI was like a much more reliable and versatile digital Mellotron. Gabriel was completely thrilled, and instantly put the machine to use during the week that Peter Vogel stayed at his house.\" \n\nGabriel was also interested in selling the CMI in the United Kingdom, and he and Paine formed Syco Systems to distribute the product in the country at a price of £12,000. The first person in Britain to purchase the CMI was Led Zeppelin bassist John Paul Jones. Other well-known figures from the British music industry followed, including Boz Burrell, Kate Bush, Geoff Downes, Trevor Horn, Alan Parsons, Rick Wright and Thomas Dolby. The Fairlight CMI was a commercial success in the United States as well, used by American acts such as Stevie Wonder, Herbie Hancock, Jan Hammer, Todd Rundgren and Joni Mitchell. However, musicians came to realize that the CMI could not match the expressiveness and level of control offered by acoustic instruments, and that sampling was better applied as imaginative sound than pure reproduction.\n\nThe second version of the Fairlight CMI, Series II, was released at a price of £30,000 in 1982. Although it still used 8-bit recordings like the Series I, the sounds produced were of better quality given that the system could handle a sample rate as high as 32 kilohertz and a maximum frequency response of fifteen kilohertz. The CMI's popularity peaked in 1982 following its appearance on a special of the arts magazine series \"The South Bank Show\" that documented the making of Gabriel's fourth self-titled studio album, where he used 64 kilobytes worth of samples of world music instruments and sequenced skippy-rhythm'd percussion.\nFairlight CMI Series II was used on nearly every album released in the early to mid-1980s, and its most commonly used presets included an orchestra stab (\"ORCH 5\") and a breathy vox (\"ARR 1\"). The CMI Series II is also credited as helping launch popular musical styles such as hip hop, big beat, techno and drum and bass.\nThe popularity of Series II was in large part due to a new feature, Page R, their first true music sequencer. As a replacement for the complicated Music Composition Language (MCL) used by Series I, Page R helped the Fairlight CMI Series II become a commercial juggernaut. Page R expanded the CMI's audience beyond that of accomplished keyboard players. \"Audio Media\" magazine described it as an echo of the punk rock era: \"Page R also gave rise to a flow of quasi-socialist sounding ideology, that hailed the impending democratisation of music creation, making it available to the musically chops-challenged.\" Graphically depicting editable notes horizontally from left to right, the music programming profession and the concepts of quantization and cycling patterns of bars where instrument channels could be added or removed were also born out of the Page R sequencer. CMI user Roger Bolton recalled: \"By definition, its sampling limitations and the Page R sequencer forced the composer to make high-quality decisions out of necessity. The CMI II was a high-level composition tool that not only shaped the sound of the 80s, but the way that music was actually written.\" Fairlight kept making updates to the system, such as a 1983 upgrade called the CMI Series IIx which now allowed for MIDI, until the release of Series III in 1985.\n\nWith 14 megabytes of RAM, which equates to about a three-minute long stereo sample, the Series III was the first sampler capable of creating sounds with 16-bit, 44.1 kilohertz sample files, as well as 16-voice polyphonic patches. Its design, graphics, and editing tools were also improved, such as the addition of a tablet next to the QWERTY keys for the lightpen to point on instead of on the screen; this change was done due to arguments from users regarding arm aches from having to hold the pen on the screen. \n\nAn enhanced version of the Page R sequencer called Composer, Arranger, Performer, Sequencer, or CAPS, as well as Eventsync, a post-production utility based on SMPTE timecode linking, were also added to the Series III computer. However, while many people were still using CMIs, sales were starting to diminish significantly due to much lower-cost, MIDI-based sequencers and samplers including the Atari ST and Akai's S612, S900 and 1000 samplers in the market. Paine stopped releasing copies of the CMI in the United Kingdom because of this. The Fairlight company was becoming more focused on post-production products, a market Paine had a hard time getting used to, and when HHB Communications Ltd took over distribution for the United Kingdom, they failed to sell any copies.\n\nPeter Gabriel was the first owner of a Fairlight Series I in the UK. Boz Burrell of Bad Company purchased the second, which Hans Zimmer hired for many recordings during the early part of his career. Other early users of the system included Thomas Dolby, Kate Bush, Icehouse's Iva Davies, and Landscape's Richard James Burgess, who demonstrated it to many British musicians and on BBC TV's \"Tomorrow's World\".\n\nIn the US, Bruce Jackson demonstrated the Series I sampler for a year before selling units to Herbie Hancock and Stevie Wonder in 1980 for US $27,500 each. Meat-packing heir Geordie Hormel bought two for use at The Village Recorder in Los Angeles. Other early adopters included Todd Rundgren, Nick Rhodes of Duran Duran, producer Rhett Lawrence and Ned Liben of Ebn Ozn. The first commercially released album to incorporate it was Kate Bush's \"Never for Ever\" (1980), programmed by Richard James Burgess and John L. Walters. Wonder took his Fairlight out on tour in 1980 in support of the album \"Stevie Wonder's Journey Through \"The Secret Life of Plants\"\" to replace the Computer Music Melodian sampler he had used on the recording. Geoff Downes of Yes conspicuously used a CMI with monitor on the band's 1980 tour to support the album \"Drama\". The first classical album using the CMI was produced by Folkways Records in 1980 with composers Barton McLean and Priscilla McLean. Titled \"Electronic Music from the Outside In,\" it was adopted extensively in electronic music courses worldwide. Jean Michel Jarre used a Fairlight on \"Magnetic Fields\" (1981) and also made extensive use of it on his \"The Concerts in China\" (1982) and \"Zoolook\" (1984) albums. French keyboardist Roland Romanelli used the Fairlight on his 1982 solo album \"Connecting Flight\". A Fairlight was used on \"Eye in the Sky\" and other albums by the Alan Parsons Project. The 1982 science fiction film \"Liquid Sky\" featured a soundtrack entirely performed on the Fairlight CMI.\n\nPeter Gabriel's album \"Peter Gabriel\" (1982) also featured the CMI. In 1981, Austrian musicians Hubert Bognermayr and Harald Zuschrader composed a symphony, \"Erdenklang – Computerakustische Klangsinfonie\". This work premiered live on stage, using five music computers, during the Ars Electronica festival in Linz\n, and was released on LP in 1982.\n\nEbn Ozn's \"AEIOU Sometimes Y\" was the first commercially released American single recorded entirely on a computer, a Fairlight CMI, in 1981/1982, released in 1983 by Elektra Records and Arista Records in London. The first American album recorded entirely via Fairlight was \"Feeling Cavalier\" by EBN-OZN recorded in 1983/1984 released in 1984.\n\nJan Hammer used the CMI to compose the original soundtrack of the 1980s TV drama \"Miami Vice\", and he was shown using the instrument several times in the \"Miami Vice Theme\" music video.\n\nThe English band Art of Noise and producer Trevor Horn used the instrument extensively. In the mid-'90s, former Art of Noise member J. J. Jeczalik would release a sample album titled \"The Art of Sampling\", which featured all of the unique CMI samples they had used throughout their career. Asia's keyboardist and occasional collaborator with Horn, Geoff Downes, included two CMIs as part of his live setup, and recorded his first solo album, \"The Light Program,\" using the CMI, as well as using them on the final Buggles album, \"Adventures in Modern Recording.\"\n\nThe last Fairlight IIx was given away through a contest in the magazine Keyboard in 1987. That particular machine has been in the hands of producer/musician Tim Curtis since 1990 and is still in use as of 2015.\n\nAfter the success of the Fairlight CMI, other firms introduced sampling. New England Digital modified their Synclavier digital synth to perform sampling, while E-mu Systems introduced a less costly sampling keyboard, the Emulator, in 1981. In the United States, a new sampler company, Ensoniq, introduced the Ensoniq Mirage in 1985, at a price that made sampling affordable to the average musician for the first time.\n\nIn America, Joan Gand of Gand Music and Sound in Northfield, Illinois was the top salesperson for Fairlight. The Gand organisation sold CMIs to Prince, James \"J.Y.\" Young of Styx, John Lowry of Petra, Derek St. Holmes of the Ted Nugent band, Al Jourgensen of Ministry, and many private studio owners and rock personalities. Spokesperson Jan Hammer appeared at several Gand-sponsored Musictech pro audio events, to perform the \"Miami Vice Theme\", as well as Keith Emerson, Stanley Jordan, Allan Holdsworth, Todd Rundgren, Jeff Baxter, Terry Fryer, Pat Leonard (Michael Jackson), engineers Roger Nichols (Steely Dan), Bob Clearmountain (David Bowie), Al Schmidt (Frank Sinatra, Diana Krall) and Cubby Colby (Phil Collins).\n\nThe ubiquity of the Fairlight was such that Phil Collins stated on the sleeve notes of his 1985 album \"No Jacket Required\" that \"there is no Fairlight on this record\" to clarify that he had not used one to synthesize horn and string sounds.\n\nCoil considered the device unique and unsurpassed, describing using the Fairlight as \"An aural equivalent of William [S.] Burroughs cut-ups\".\n\nIn 2015, the Fairlight CMI was inducted into the National Film and Sound Archive's Sounds of Australia collection.\n\nSeveral virtual instruments based on the Fairlight sample library have been released:\n\n\nCo-inventor Peter Vogel attempted to resurrect the Fairlight CMI as the CMI-30A, referring to the instrument's 30th anniversary. However, his right to use the Fairlight name was terminated by the original holders of the trademark following legal action, and the instrument is currently unavailable pending the resolution of all legal matters. Vogel had also been planning several other CMI-related hardware products, including a PC-based \"Series IV\" variant.\n\n\"Note: These sound clips require a Vorbis player. Click here for a list of downloadable players.\"\n\n\"Note: These sound clips require an MP3 player.\"\n\nTwo tracks showing Mode 4 (sampling) and Mode 2 (synthesis) and Page-R capabilities on a Fairlight CMI II and a small analog mixer\n\nAmerican band Devo used the CMI extensively on their 1984 album \"Shout\", but only occasionally after that (mostly being used by lead singer Mark Mothersbaugh's music composing company, Mutato Muzika). It also appears as a prop in their home video release, \"We're All Devo\", where it is used by Timothy Leary's character.\n\nJan Hammer was one of the most prolific composers to use the Fairlight in the 1980s, particularly for his work on the television series \"Miami Vice\", for which he provided the theme song as well as an entire catalog of score music throughout the first three seasons.\n\nThe Fairlight CMI also makes an appearance being operated by Nick Rhodes in Duran Duran's video \"The Reflex\". Al Di Meola's \"Sequencer\" video has many shots of the Fairlight CMI and its software. The instrument (series II presumably) can be seen in the music video \"Etude\" by Mike Oldfield (for the track from the album \"The Killing Fields\" and on the DVD \"Elements\"). A monitor of a Fairlight CMI appears at the 1985 music video \"Machine Age Voodoo (Junk Funk)\" by the band SPK. It can also be seen in the Queen documentary \"Magic Years\" and on the back cover of Mecano's live album.\n\nHerbie Hancock made an appearance on \"Sesame Street\" in the early 1980s demonstrating the Fairlight.\n\nJean-Michel Jarre's 1984 album \"Zoolook\" and the single Diva featured the Fairlight's famous female spoken word \"Bizarre\" lead throughout the song.\n\nDavid Hirschfelder made extensive use of the Fairlight CMI while recording with John Farnham for the 1986 album \"Whispering Jack\".\n\nHans Zimmer used the CMI III to make the soundtrack for the Oscar-winning 1988 film \"Rain Man.\"\n\nHaving incorporated the Fairlight extensively into their music in the 1980s, the Pet Shop Boys also used it for many of their TV performances, especially during \"Top of the Pops\" appearances. Chris Lowe can clearly be seen operating a Series III Fairlight (along with an Emulator II) on the show for the 1987 song \"Always On My Mind\".\n\nJun'ichi Sato of the Japanese pop band Fhana used a Fairlight Series III in performing two songs at the 2014 Animelo Summer Live concert.\n\nSources\nCitations\n\n"}
{"id": "15314160", "url": "https://en.wikipedia.org/wiki?curid=15314160", "title": "HCNG", "text": "HCNG\n\nHCNG (or H2CNG) is a mixture of compressed natural gas and 4–9 percent hydrogen by energy. It may be used as a fuel gas for internal combustion engines and home appliances.\n\n(regarding the acronyms in the above emissions chart:\n\nAVL= Average Levels?<br>\nCNG=Compressed Natural Gas<br>\nHCNG= Hydrogen and CNG blend<br>\nNOX= Nitrogen Oxides<br>\nNMHC= Non-Methane Hydrocarbons?<br>\nCH4= Methane<br>\nTHC= Total Hydrocarbons?<br>\nCO= Carbon Monoxide)\n\nHCNG dispensers can be found at Hynor (Norway) Thousand palms and Barstow, California, Fort Collins, Colorado (all US), Chongqing and Shanxi (China), Pico Truncado (Argentina), Islamabad (Pakistan), Dunkerque (France), Gothenburg Sweden, Rio de Janeiro (Brazil), Emilia-Romagna, Lombardia (Italy), Dwarka and Faridabad (Delhi), India and the BC hydrogen highway in Canada.\n\nHCNG for mobile use is premixed at the hydrogen station.\n\nIn the town of Nes on the island of Ameland in the Netherlands, a four-year (2008-2011) field test was carried out where 20% hydrogen was added to the local distribution net supplying a complex of 14 apartments. The appliances involved were kitchen stoves, condensing boilers, and micro-CHP boilers.\n\nThe use of existing natural gas pipelines for HCNG was studied by NaturalHy.\n\nTo get the most out of an internal combustion engine in transportation if higher levels of hydrogen are added, modifications have to be made to the engine and the control strategy. The hydrogen in the blend leads to lower CO emissions.\n\nThe National Fire Protection Association 52 presently covers CNG and hydrogen fueling stations. Blends with < 20% hydrogen by volume\nare treated identically to CNG. For the use of blends with more than 30-40 % of hydrogen in volume decision support tools for the design are used to ensure safe use.\n\n\n"}
{"id": "24046059", "url": "https://en.wikipedia.org/wiki?curid=24046059", "title": "Heather Murren", "text": "Heather Murren\n\nHeather Hay Murren (born May 30, 1966) is an American businesswoman. She is a private investor, former Wall Street securities analyst and special correspondent at The Nevada Independent, which was founded by Jon Ralston. She served as a Congressional appointed commissioner on the Financial Crisis Inquiry Commission in 2009 and the President's Commission on Enhancing National Cybersecurity in 2016. She is a member of the Democratic Party.\n\nMurren began her career on Wall Street at Salomon Brothers in 1988 as a securities analyst and ultimately retired in 2002 as a managing director, Global Securities Research and Economics of Merrill Lynch.\n\nMurren was chosen six consecutive years as a member Institutional Investor's All-American Research Team. Her multi-year inclusion in the Greenwich survey and repeated designations by the Wall Street Journal as an all-star analyst underscore her other notable achievements in the economic and financial services community. She was profiled in FORTUNE magazine as one of the Wall Street's all-star analysts while at Merrill Lynch.\n\nAfter retiring from Wall Street, in 2002 and seeing the extreme need for quality medical care in the State of Nevada, she and her husband co-founded the first non-profit cancer research and treatment center in Las Vegas, the Nevada Cancer Institute (NVCI) where she served as Chairman and CEO until 2009. The 140,00 square foot flagship facility was constructed between 2002 and 2005 when it opened to patients and researchers with fully integrated wet and dry laboratories, imaging, radiation and clinical oncology practices and patient support services. The NVCI carried out the first ever first-in-man clinical trials in the state of Nevada as well as seminal clinical trials making previously unavailable early-stage clinical trial therapies available to the over 15,000 patients served. The research center-clinic flagship facility was acquired by the University of California San Diego Health Systems in January 2012 and subsequently the NVCI Foundation was then merged into Roseman University in December 2013 after reorganization.\n\nMurren was appointed in 2009 by Congress to serve on the Financial Crisis Inquiry Commission (FCIC), a 10-member Federal commission established to examine the domestic and global cause of the financial crisis. The commission, to which subpoena powers were granted, examined and held hearings on more than 20 specific areas of inquiry, including the role of fraud and abuse in the financial sector; state and federal regulatory enforcement; tax treatment of financial products; lending practices and securitization; and corporate governance and executive compensation. The commission reported its finding in January 2011. The published book \"The Financial Crisis Inquiry Report\" was listed on the New York Times bestseller list and was critically acclaimed.\n\nOn April 13, 2016, President Barack Obama announced Murren's appointment to the President's Commission on Enhancing National Cybersecurity.\n\nOn January 21, 2017, she became a special correspondent for The Nevada Independent. She currently writes about cybersecurity, and has since written on the state of cybersecurity in Nevada.\n\nOn May 3, 2017, Fidelity National Financial, Inc. announced that its board of directors adopted a resolution increasing the size of the company's board of birectors to thirteen and elected Murren to serve on its board of directors. \"We are excited to welcome Heather Murren as a member of our board,\" said Chairman William P. Foley, II. \"Heather has extensive financial markets experience and has been appointed to investigate some of the most pressing issues in our country by Congress and the President. She brings a wealth of talent, leadership and knowledge of Wall Street, the financial crisis and cybersecurity that will serve our board and company well.\"\n\nOn May 11, 2017, Murren along with her husband James Murren were presented with the Distinguished Woodrow Wilson Award For Corporate Citizenship. \nThe award is given to those who, by their example and business practices, have demonstrated a profound concern for the common good beyond the bottom line, acting as a force for positive change.\n\nShe is a member of the board of trustees of the Johns Hopkins University (JHU) and the JHU Applied Physics Laboratory. The Applied Physics Laboratory is a not-for-profit university-affiliated research center - providing research and development in the areas of cybersecurity, undersea and air defense, space, national security analysis, special operations and strategic defense to our nation. She has previously held a gubernatorial appointment to the Nevada Academy of Health and the Board of Economic Development for the state of Nevada.\n\nMurren is a graduate of the Johns Hopkins University and a Chartered Financial Analyst. Fluent in Spanish and French, she has served as a translator and medical assistant for Volunteers in Medicine of Southern Nevada, a nonprofit organization that provides healthcare to the community regardless of the patient's ability to pay, she is board member of the Murren Family Foundation, which focuses grants on education, security, healthcare and military veterans.\n"}
{"id": "1222734", "url": "https://en.wikipedia.org/wiki?curid=1222734", "title": "Help authoring tool", "text": "Help authoring tool\n\nA Help Authoring Tool or HAT is a software program used by technical writers to create online help systems.\n\nThe basic functions of a Help Authoring Tool (HAT) can be divided into the following categories:\n\nHATs obtain their source text either by importing it from a file produced by another program, or by allowing the author to create the text within the tool by using an editor. File formats that can be imported vary from HAT to HAT. Acceptable file formats can include ASCII, HTML, OpenOffice Writer and Microsoft Word, and compiled Help formats such as Microsoft WinHelp and Microsoft Compressed HTML Help.\n\nThe output from a HAT can be either a compiled Help file in a format such as WinHelp (*.HLP) or Microsoft Compiled HTML Help (*.CHM), or noncompiled file formats such as Adobe PDF, XML, HTML or JavaHelp.\n\nSome HATs provide extra functions such as:\n\n\nSome common HATs include:\n\n\nTechnical writers often use content management systems and version control systems to manage their work.\n\n"}
{"id": "242883", "url": "https://en.wikipedia.org/wiki?curid=242883", "title": "History of nuclear weapons", "text": "History of nuclear weapons\n\nNuclear weapons possess enormous destructive power from nuclear fission or combined fission and fusion reactions. Building on scientific breakthroughs made during the 1930s, the United States, the United Kingdom and Canada collaborated during World War II, in what was called the Manhattan Project, to counter the suspected Nazi German atomic bomb project. In August 1945, two fission bombs were dropped on Japan, standing to date as the only use of nuclear weapons in combat. The Soviet Union started development shortly thereafter with their own atomic bomb project, and not long after that both countries developed even more powerful fusion weapons known as \"hydrogen bombs\".\n\nIn the first decades of the 20th century, physics was revolutionised with developments in the understanding of the nature of atoms. In 1898, Pierre and Marie Curie discovered that pitchblende, an ore of uranium, contained a substance—which they named radium—that emitted large amounts of radioactivity. Ernest Rutherford and Frederick Soddy identified that atoms were breaking down and turning into different elements. Hopes were raised among scientists and laymen that the elements around us could contain tremendous amounts of unseen energy, waiting to be harnessed.\n\nH. G. Wells was inspired to write about atomic weapons in a 1914 novel, \"The World Set Free\", which appeared shortly before the First World War.\nIn a 1924 article, Winston Churchill speculated about the possible military implications: \"Might not a bomb no bigger than an orange be found to possess a secret power to destroy a whole block of buildings—nay to concentrate the force of a thousand tons of cordite and blast a township at a stroke?\"\n\nIn January 1933, Adolf Hitler was appointed Chancellor of Germany and it quickly became unsafe for Jewish scientists to remain in the country. Leó Szilárd fled to London where he proposed, and in 1934 patented, the idea of a nuclear chain reaction via neutrons. The patent also introduced the term critical mass to describe the minimum amount of material required to sustain the chain reaction and its potential to cause an explosion. (British patent 630,726.) He subsequently assigned the patent to the British Admiralty so that it could be covered by the Official Secrets Act. In a very real sense, Szilárd was the father of the atomic bomb academically.\nAlso in 1934, Irène and Frédéric Joliot-Curie discovered that artificial radioactivity could be induced in stable elements by bombarding them with alpha particles; Enrico Fermi reported similar results when bombarding uranium with neutrons.\n\nIn December 1938, Otto Hahn and Fritz Strassmann sent a manuscript to \"Naturwissenschaften\" reporting that they had detected the element barium after bombarding uranium with neutrons. Lise Meitner and her nephew Otto Robert Frisch correctly interpreted these results as being due to the splitting of the uranium atom. (Frisch confirmed this experimentally on January 13, 1939.) They gave the process the name \"fission\" because of its similarity to the splitting of a cell into two new cells.\nEven before it was published, news of Meitner’s and Frisch’s interpretation crossed the Atlantic.\nScientists at Columbia University decided to replicate the experiment and on January 25, 1939, conducted the first nuclear fission experiment in the United States in the basement of Pupin Hall. The following year, they identified the active component of uranium as being the rare isotope uranium-235.\n\nUranium appears in nature primarily in two isotopes: uranium-238 and uranium-235. When the nucleus of uranium-235 absorbs a neutron, it undergoes nuclear fission, releasing energy and, on average, 2.5 neutrons. Because uranium-235 releases more neutrons than it absorbs, it can support a chain reaction and so is described as fissile. Uranium-238, on the other hand, is not fissile as it does not normally undergo fission when it absorbs a neutron.\n\nBy the time Nazi Germany invaded Poland in 1939, beginning World War II, many of Europe's top scientists had already fled the imminent conflict. Physicists on both sides were well aware of the possibility of utilizing nuclear fission as a weapon, but no one was quite sure how it could be done. \nIn August 1939, concerned that Germany might have its own project to develop fission-based weapons, Albert Einstein signed a letter to U.S. President Franklin D. Roosevelt warning him of the threat. Roosevelt responded by setting up the Uranium Committee under Lyman James Briggs but, with little initial funding ($6,000), progress was slow. It was not until the Japanese attack on Pearl Harbor in December, 1941, that the U.S. decided to commit the necessary resources.\n\nOrganized research first began in Britain as part of the Tube Alloys project. The Maud Committee was set up following the work of Frisch and Rudolf Peierls who calculated uranium-235's critical mass and found it to be much smaller than previously thought which meant that a deliverable bomb should be possible. In the February 1940 Frisch–Peierls memorandum they stated that: \"The energy liberated in the explosion of such a super-bomb...will, for an instant, produce a temperature comparable to that of the interior of the sun. The blast from such an explosion would destroy life in a wide area. The size of this area is difficult to estimate, but it will probably cover the centre of a big city.\"\n\nEdgar Sengier, a director of Shinkolobwe Mine in the Congo which produced by far the highest quality uranium ore in the world, had become aware of uranium's possible use in a bomb. In late 1940, fearing that it might be seized by the Germans, he shipped the mine's entire stockpile of ore to a warehouse on Staten Island.\n\nFor 18 months British research outpaced the American but by mid-1942, it became apparent that the industrial effort required was beyond Britain's already stretched wartime economy. In September 1942, General Leslie Groves was appointed to lead the U.S. project which became known as the Manhattan Project. Two of his first acts were to obtain authorization to assign the highest priority AAA rating on necessary procurements, and to order the purchase of all 1,250 tons of the Shinkolobwe ore. The Tube Alloys project was quickly overtaken by the U.S. effort and after Roosevelt and Churchill signed the Quebec Agreement in 1943, it was relocated and amalgamated into the Manhattan Project.\n\nWith a scientific team led by J. Robert Oppenheimer, the Manhattan project brought together some of the top scientific minds of the day, including many exiles from Europe, with the production power of American industry for the goal of producing fission-based explosive devices before Germany. Britain and the U.S. agreed to pool their resources and information for the project, but the other Allied power, the Soviet Union (USSR), was not informed. The U.S. made an unprecedented investment in the project which at the time was the largest industrial enterprise ever seen, spread across more than 30 sites in the U.S. and Canada. Scientific development was centralized in a secret laboratory at Los Alamos.\n\nFor a fission weapon to operate, there must be sufficient fissile material to support a chain reaction, a critical mass. To separate the fissile uranium-235 isotope from the non-fissile uranium-238, two methods were developed which took advantage of the fact that uranium-238 has a slightly greater atomic mass: electromagnetic separation and gaseous diffusion. Another secret site was erected at rural Oak Ridge, Tennessee, for the large-scale production and purification of the rare isotope, which required considerable investment. At the time, K-25, one of the Oak Ridge facilities, was the world's largest factory under one roof. The Oak Ridge site employed tens of thousands of people at its peak, most of whom had no idea what they were working on.\n\nAlthough uranium-238 cannot be used for the initial stage of an atomic bomb, when it absorbs a neutron, it becomes uranium-239 which decays into neptunium-239, and finally the relatively stable plutonium-239, an element that does not exist naturally on Earth, but is fissile like uranium-235. After Fermi achieved the world's first sustained and controlled nuclear chain reaction with the creation of the first atomic pile, massive reactors were secretly constructed at what is now known as Hanford Site to transform uranium-238 into plutonium for a bomb.\n\nThe simplest form of nuclear weapon is a gun-type fission weapon, where a sub-critical mass would be shot at another sub-critical mass. The result would be a super-critical mass and an uncontrolled chain reaction that would create the desired explosion. The weapons envisaged in 1942 were the two gun-type weapons, Little Boy (uranium) and Thin Man (plutonium), and the Fat Man plutonium implosion bomb.\n\nIn early 1943 Oppenheimer determined that two projects should proceed forwards: the Thin Man project (plutonium gun) and the Fat Man project (plutonium implosion). The plutonium gun was to receive the bulk of the research effort, as it was the project with the most uncertainty involved. It was assumed that the uranium gun-type bomb could then be adapted from it.\n\nIn December 1943 the British mission of 19 scientists arrived in Los Alamos. Hans Bethe became head of the Theoretical Division.\nIn April 1944 it was found by Emilio Segrè that the plutonium-239 produced by the Hanford reactors had too high a level of background neutron radiation, and underwent spontaneous fission to a very small extent, due to the unexpected presence of plutonium-240 impurities. If such plutonium were used in a gun-type design, the chain reaction would start in the split second before the critical mass was fully assembled, blowing the weapon apart with a much lower yield than expected, in what is known as a fizzle.\n\nAs a result, development of Fat Man was given high priority. Chemical explosives were used to implode a sub-critical sphere of plutonium, thus increasing its density and making it into a critical mass. The difficulties with implosion centered on the problem of making the chemical explosives deliver a perfectly uniform shock wave upon the plutonium sphere— if it were even slightly asymmetric, the weapon would fizzle. This problem was solved by the use of explosive lenses which would focus the blast waves inside the imploding sphere, akin to the way in which an optical lens focuses light rays.\n\nAfter D-Day, General Groves ordered a team of scientists to follow eastward-moving victorious Allied troops into Europe to assess the status of the German nuclear program (and to prevent the westward-moving Soviets from gaining any materials or scientific manpower). They concluded that, while Germany had an atomic bomb program headed by Werner Heisenberg, the government had not made a significant investment in the project, and it had been nowhere near success. Similarly, Japan's efforts at developing a nuclear weapon were starved of resources. The Japanese navy lost interest when a committee led by Yoshio Nishina concluded in 1943 that \"it would probably be difficult even for the United States to realize the application of atomic power during the war\".\n\nHistorians claim to have found a rough schematic showing a Nazi nuclear bomb. In March 1945, a German scientific team was directed by the physicist Kurt Diebner to develop a primitive nuclear device in Ohrdruf, Thuringia. Last ditch research was conducted in an experimental nuclear reactor at Haigerloch.\n\nOn April 12, after Roosevelt's death, Vice-President Harry S. Truman assumed the presidency. At the time of the unconditional surrender of Germany on May 8, 1945, the Manhattan Project was still months away from producing a working weapon.\n\nBecause of the difficulties in making a working plutonium bomb, it was decided that there should be a test of the weapon. On July 16, 1945, in the desert north of Alamogordo, New Mexico, the first nuclear test took place, code-named \"Trinity\", using a device nicknamed \"the gadget.\" The test, a plutonium implosion type device, released energy equivalent to 19 kilotons of TNT, far more powerful than any weapon ever used before. The news of the test's success was rushed to Truman at the Potsdam Conference, where Churchill was briefed and Soviet Premier Joseph Stalin was informed of the new weapon. On July 26, the Potsdam Declaration was issued containing an ultimatum for Japan: either surrender or suffer \"complete and utter destruction\", although nuclear weapons were not mentioned.\nAfter hearing arguments from scientists and military officers over the possible use of nuclear weapons against Japan (though some recommended using them as demonstrations in unpopulated areas, most recommended using them against built up targets, a euphemistic term for populated cities), Truman ordered the use of the weapons on Japanese cities, hoping it would send a strong message that would end in the capitulation of the Japanese leadership and avoid a lengthy invasion of the islands. On May 10–11, 1945, the Target Committee at Los Alamos, led by Oppenheimer, recommended Kyoto, Hiroshima, Yokohama, and Kokura as possible targets. Concerns about Kyoto's cultural heritage led to it being replaced by Nagasaki.\nOn August 6, 1945, a uranium-based weapon, Little Boy, was detonated above the Japanese city of Hiroshima, and three days later, a plutonium-based weapon, Fat Man, was detonated above the Japanese city of Nagasaki. To date, Hiroshima and Nagasaki remain the only two instances of nuclear weapons being used in combat. The atomic raids killed at least one hundred thousand Japanese civilians and military personnel outright, with the heat, radiation, and blast effects.\nMany tens of thousands would later die of radiation sickness and related cancers. Truman promised a \"rain of ruin\" if Japan did not surrender immediately, threatening to systematically eliminate their ability to wage war. On August 15, Emperor Hirohito announced Japan's surrender.\n\nThe Soviet Union was not invited to share in the new weapons developed by the United States and the other Allies. During the war, information had been pouring in from a number of volunteer spies involved with the Manhattan Project (known in Soviet cables under the code-name of \"Enormoz\"), and the Soviet nuclear physicist Igor Kurchatov was carefully watching the Allied weapons development. It came as no surprise to Stalin when Truman had informed him at the Potsdam conference that he had a \"powerful new weapon.\" Truman was shocked at Stalin's lack of interest.\n\nThe Soviet spies in the U.S. project were all volunteers and none were Soviet citizens. One of the most valuable, Klaus Fuchs, was a German émigré theoretical physicist who had been part of the early British nuclear efforts and the UK mission to Los Alamos. Fuchs had been intimately involved in the development of the implosion weapon, and passed on detailed cross-sections of the Trinity device to his Soviet contacts. Other Los Alamos spies—none of whom knew each other—included Theodore Hall and David Greenglass. The information was kept but not acted upon, as the Soviet Union was still too busy fighting the war in Europe to devote resources to this new project.\n\nIn the years immediately after World War II, the issue of who should control atomic weapons became a major international point of contention. Many of the Los Alamos scientists who had built the bomb began to call for \"international control of atomic energy,\" often calling for either control by transnational organizations or the purposeful distribution of weapons information to all superpowers, but due to a deep distrust of the intentions of the Soviet Union, both in postwar Europe and in general, the policy-makers of the United States worked to attempt to secure an American nuclear monopoly.\n\nA half-hearted plan for international control was proposed at the newly formed United Nations by Bernard Baruch (The Baruch Plan), but it was clear both to American commentators—and to the Soviets—that it was an attempt primarily to stymie Soviet nuclear efforts. The Soviets vetoed the plan, effectively ending any immediate postwar negotiations on atomic energy, and made overtures towards banning the use of atomic weapons in general.\n\nThe Soviets had put their full industrial might and manpower into the development of their own atomic weapons. The initial problem for the Soviets was primarily one of resources—they had not scouted out uranium resources in the Soviet Union and the U.S. had made deals to monopolise the largest known (and high purity) reserves in the Belgian Congo. The USSR used penal labour to mine the old deposits in Czechoslovakia—now an area under their control—and searched for other domestic deposits (which were eventually found).\n\nTwo days after the bombing of Nagasaki, the U.S. government released an official technical history of the Manhattan Project, authored by Princeton physicist Henry DeWolf Smyth, known colloquially as the Smyth Report. The sanitized summary of the wartime effort focused primarily on the production facilities and scale of investment, written in part to justify the wartime expenditure to the American public.\n\nThe Soviet program, under the suspicious watch of former NKVD chief Lavrenty Beria (a participant and victor in Stalin's Great Purge of the 1930s), would use the Report as a blueprint, seeking to duplicate as much as possible the American effort. The \"secret cities\" used for the Soviet equivalents of Hanford and Oak Ridge literally vanished from the maps for decades to come.\n\nAt the Soviet equivalent of Los Alamos, Arzamas-16, physicist Yuli Khariton led the scientific effort to develop the weapon. Beria distrusted his scientists, however, and he distrusted the carefully collected espionage information. As such, Beria assigned multiple teams of scientists to the same task without informing each team of the other's existence. If they arrived at different conclusions, Beria would bring them together for the first time and have them debate with their newfound counterparts. Beria used the espionage information as a way to double-check the progress of his scientists, and in his effort for duplication of the American project even rejected more efficient bomb designs in favor of ones that more closely mimicked the tried-and-true Fat Man bomb used by the U.S. against Nagasaki.\n\nOn August 29, 1949, the effort brought its results, when the USSR tested its first fission bomb, dubbed \"Joe-1\" by the U.S., years ahead of American predictions. The news of the first Soviet bomb was announced to the world first by the United States, which had detected the nuclear fallout it generated from its test site in Kazakhstan.\n\nThe loss of the American monopoly on nuclear weapons marked the first tit-for-tat of the nuclear arms race. The response in the U.S. was one of apprehension, fear, and scapegoating, which would lead eventually into the Red-baiting tactics of McCarthyism. Yet recent information from declassified Venona intercepts and the opening of the KGB archives after the fall of the Soviet Union show that the USSR had useful spies that helped their program, although none were identified by McCarthy. Before this, though, President Truman announced a decision to begin a crash program that would develop a far more powerful weapon than those the U.S. used against Japan: the hydrogen bomb.\n\nIn 1946 Congress established the civilian Atomic Energy Commission (AEC) to take over the development of nuclear weapons from the military, and to develop nuclear power. The AEC made use of many private companies in processing uranium and thorium and in other urgent tasks related to the development of bombs. Many of these companies had very lax safety measures and employees were sometimes exposed to radiation levels far above what was allowed then or now. In 1974, the Formerly Utilized Sites Remedial Action Program (FUSRAP) of the Army Corps of Engineers was set up to deal with contaminated sites left over from these operations.\n\nThe notion of using a fission weapon to ignite a process of nuclear fusion can be dated back to 1942. At the first major theoretical conference on the development of an atomic bomb hosted by J. Robert Oppenheimer at the University of California, Berkeley, participant Edward Teller directed the majority of the discussion towards Enrico Fermi's idea of a \"Super\" bomb that would use the same reactions that powered the Sun itself.\n\nIt was thought at the time that a fission weapon would be quite simple to develop and that perhaps work on a hydrogen bomb (thermonuclear weapon) would be possible to complete before the end of the Second World War. However, in reality the problem of a regular atomic bomb was large enough to preoccupy the scientists for the next few years, much less the more speculative \"Super\" bomb. Only Teller continued working on the project—against the will of project leaders Oppenheimer and Hans Bethe.\n\nAfter the atomic bombings of Japan, many scientists at Los Alamos rebelled against the notion of creating a weapon thousands of times more powerful than the first atomic bombs. For the scientists the question was in part technical—the weapon design was still quite uncertain and unworkable—and in part moral: such a weapon, they argued, could only be used against large civilian populations, and could thus only be used as a weapon of genocide.\n\nMany scientists, such as Bethe, urged that the United States should not develop such weapons and set an example towards the Soviet Union. Promoters of the weapon, including Teller, Ernest Lawrence, and Luis Alvarez, argued that such a development was inevitable, and to deny such protection to the people of the United States—especially when the Soviet Union was likely to create such a weapon themselves—was itself an immoral and unwise act.\n\nOppenheimer, who was now head of the General Advisory Committee of the successor to the Manhattan Project, the Atomic Energy Commission, presided over a recommendation against the development of the weapon. The reasons were in part because the success of the technology seemed limited at the time (and not worth the investment of resources to confirm whether this was so), and because Oppenheimer believed that the atomic forces of the United States would be more effective if they consisted of many large fission weapons (of which multiple bombs could be dropped on the same targets) rather than the large and unwieldy super bombs, for which there was a relatively limited number of targets of sufficient size to warrant such a development.\n\nWhat is more, if such weapons were developed by both superpowers, they would be more effective against the U.S. than against the USSR, as the U.S. had far more regions of dense industrial and civilian activity as targets for large weapons than the Soviet Union.\n\nIn the end, President Truman made the final decision, looking for a proper response to the first Soviet atomic bomb test in 1949. On January 31, 1950, Truman announced a crash program to develop the hydrogen (fusion) bomb. At this point, however, the exact mechanism was still not known: the classical hydrogen bomb, whereby the \"heat\" of the fission bomb would be used to ignite the fusion material, seemed highly unworkable. However, an insight by Los Alamos mathematician Stanislaw Ulam showed that the fission bomb and the fusion fuel could be in separate parts of the bomb, and that \"radiation\" of the fission bomb could first work in a way to \"compress\" the fusion material before igniting it.\n\nTeller pushed the notion further, and used the results of the boosted-fission \"George\" test (a boosted-fission device using a small amount of fusion fuel to boost the yield of a fission bomb) to confirm the fusion of heavy hydrogen elements before preparing for their first true multi-stage, Teller-Ulam hydrogen bomb test. Many scientists, initially against the weapon, such as Oppenheimer and Bethe, changed their previous opinions, seeing the development as being unstoppable.\n\nThe first fusion bomb was tested by the United States in \"Operation Ivy\" on November 1, 1952, on Elugelab Island in the Enewetak (or Eniwetok) Atoll of the Marshall Islands, code-named \"Mike.\" Mike used liquid deuterium as its fusion fuel and a large fission weapon as its trigger. The device was a prototype design and not a deliverable weapon: standing over 20 ft (6 m) high and weighing at least 140,000 lb (64 t) (its refrigeration equipment added an additional as well), it could not have been dropped from even the largest planes.\n\nIts explosion yielded energy equivalent to 10.4 megatons of TNT—over 450 times the power of the bomb dropped onto Nagasaki— and obliterated Elugelab, leaving an underwater crater 6240 ft (1.9 km) wide and 164 ft (50 m) deep where the island had once been. Truman had initially tried to create a media blackout about the test—hoping it would not become an issue in the upcoming presidential election—but on January 7, 1953, Truman announced the development of the hydrogen bomb to the world as hints and speculations of it were already beginning to emerge in the press.\n\nNot to be outdone, the Soviet Union exploded its first thermonuclear device, designed by the physicist Andrei Sakharov, on August 12, 1953, labeled \"Joe-4\" by the West. This created concern within the U.S. government and military, because, unlike Mike, the Soviet device was a deliverable weapon, which the U.S. did not yet have. This first device though was arguably not a true hydrogen bomb, and could only reach explosive yields in the hundreds of kilotons (never reaching the megaton range of a staged weapon). Still, it was a powerful propaganda tool for the Soviet Union, and the technical differences were fairly oblique to the American public and politicians.\n\nFollowing the Mike blast by less than a year, Joe-4 seemed to validate claims that the bombs were inevitable and vindicate those who had supported the development of the fusion program. Coming during the height of McCarthyism, the effect was pronounced on the security hearings in early 1954, which revoked former Los Alamos director Robert Oppenheimer's security clearance on the grounds that he was unreliable, had not supported the American hydrogen bomb program, and had made long-standing left-wing ties in the 1930s. Edward Teller participated in the hearing as the only major scientist to testify against Oppenheimer, resulting in his virtual expulsion from the physics community.\n\nOn March 1, 1954, the U.S. detonated its first practical thermonuclear weapon (which used isotopes of lithium as its fusion fuel), known as the \"Shrimp\" device of the Castle Bravo test, at Bikini Atoll, Marshall Islands. The device yielded 15 megatons, more than twice its expected yield, and became the worst radiological disaster in U.S. history. The combination of the unexpectedly large blast and poor weather conditions caused a cloud of radioactive nuclear fallout to contaminate over . 239 Marshall Island natives and 28 Americans were exposed to significant amounts of radiation, resulting in elevated levels of cancer and birth defects in the years to come.\n\nThe crew of the Japanese tuna-fishing boat \"Lucky Dragon 5\", who had been fishing just outside the exclusion zone, returned to port suffering from radiation sickness and skin burns; one crew member was terminally ill. Efforts were made to recover the cargo of contaminated fish but at least two large tuna were probably sold and eaten. A further 75 tons of tuna caught between March and December were found to be unfit for human consumption. When the crew member died and the full results of the contamination were made public by the U.S., Japanese concerns were reignited about the hazards of radiation.\n\nThe hydrogen bomb age had a profound effect on the thoughts of nuclear war in the popular and military mind. With only fission bombs, nuclear war was something that possibly could be limited. Dropped by planes and only able to destroy the most built up areas of major cities, it was possible for many to look at fission bombs as a technological extension of large-scale conventional bombing—such as the extensive firebombing of German and Japanese cities during World War II. Proponents brushed aside as grave exaggeration claims that such weapons could lead to worldwide death or harm.\n\nEven in the decades before fission weapons, there had been speculation about the possibility for human beings to end all life on the planet, either by accident or purposeful maliciousness—but technology had not provided the capacity for such action. The great power of hydrogen bombs made worldwide annihilation possible.\n\nThe Castle Bravo incident itself raised a number of questions about the survivability of a nuclear war. Government scientists in both the U.S. and the USSR had insisted that fusion weapons, unlike fission weapons, were cleaner, as fusion reactions did not produce the dangerously radioactive by-products of fission reactions. While technically true, this hid a more gruesome point: the last stage of a multi-staged hydrogen bomb often used the neutrons produced by the fusion reactions to induce fissioning in a jacket of natural uranium, and provided around half of the yield of the device itself.\n\nThis fission stage made fusion weapons considerably more dirty than they were made out to be. This was evident in the towering cloud of deadly fallout that followed the \"Bravo\" test. When the Soviet Union tested its first megaton device in 1955, the possibility of a \"limited\" nuclear war seemed even more remote in the public and political mind. Even cities and countries that were not direct targets would suffer fallout contamination. Extremely harmful fission products would disperse via normal weather patterns and embed in soil and water around the planet.\n\nSpeculation began to run towards what fallout and dust from a full-scale nuclear exchange would do to the world as a whole, rather than just cities and countries directly involved. In this way, the fate of the world was now tied to the fate of the bomb-wielding superpowers.\n\nThroughout the 1950s and the early 1960s the U.S. and the USSR both endeavored, in a tit-for-tat approach, to prevent the other power from acquiring nuclear supremacy. This had massive political and cultural effects during the Cold War.\n\nThe first atomic bombs dropped on Hiroshima and Nagasaki were large, custom-made devices, requiring highly trained personnel for their arming and deployment. They could be dropped only from the largest bomber planes—at the time the B-29 Superfortress—and each plane could only carry a single bomb in its hold.\n\nThe first hydrogen bombs were similarly massive and complicated. This ratio of one plane to one bomb was still fairly impressive in comparison with conventional, non-nuclear weapons, but against other nuclear-armed countries it was considered a grave danger. In the immediate postwar years, the U.S. expended much effort on making the bombs \"G.I.-proof\"—capable of being used and deployed by members of the U.S. Army, rather than Nobel Prize–winning scientists. In the 1950s, the U.S. undertook a nuclear testing program to improve the nuclear arsenal.\n\nStarting in 1951, the Nevada Test Site (in the Nevada desert) became the primary location for all U.S. nuclear testing (in the USSR, Semipalatinsk Test Site in Kazakhstan served a similar role). Tests were divided into two primary categories: \"weapons related\" (verifying that a new weapon worked or looking at exactly how it worked) and \"weapons effects\" (looking at how weapons behaved under various conditions or how structures behaved when subjected to weapons).\n\nIn the beginning, almost all nuclear tests were either atmospheric (conducted above ground, in the atmosphere) or underwater (such as some of the tests done in the Marshall Islands). Testing was used as a sign of both national and technological strength, but also raised questions about the safety of the tests, which released nuclear fallout into the atmosphere (most dramatically with the Castle Bravo test in 1954, but in more limited amounts with almost all atmospheric nuclear testing).\n\nBecause testing was seen as a sign of technological development (the ability to design usable weapons without some form of testing was considered dubious), halts on testing were often called for as stand-ins for halts in the nuclear arms race itself, and many prominent scientists and statesmen lobbied for a ban on nuclear testing. In 1958, the U.S., USSR, and the United Kingdom (a new nuclear power) declared a temporary testing moratorium for both political and health reasons, but by 1961 the Soviet Union had broken the moratorium and both the USSR and the U.S. began testing with great frequency.\n\nAs a show of political strength, the Soviet Union tested the largest-ever nuclear weapon in October 1961, the massive Tsar Bomba, which was tested in a reduced state with a yield of around 50 megatons—in its full state it was estimated to have been around 100 Mt. The weapon was largely impractical for actual military use, but was hot enough to induce third-degree burns at a distance of 62 mi (100 km) away. In its full, dirty, design it would have increased the amount of worldwide fallout since 1945 by 25%.\n\nIn 1963, all nuclear and many non-nuclear states signed the Limited Test Ban Treaty, pledging to refrain from testing nuclear weapons in the atmosphere, underwater, or in outer space. The treaty permitted underground tests.\n\nMost tests were considerably more modest, and worked for direct technical purposes as well as their potential political overtones. Weapons improvements took on two primary forms. One was an increase in efficiency and power, and within only a few years fission bombs were developed that were many times more powerful than the ones created during World War II. The other was a program of miniaturization, reducing the size of the nuclear weapons.\n\nSmaller bombs meant that bombers could carry more of them, and also that they could be carried on the new generation of rockets in development in the 1950s and 1960s. U.S. rocket science received a large boost in the postwar years, largely with the help of engineers acquired from the Nazi rocketry program. These included scientists such as Wernher von Braun, who had helped design the V-2 rockets the Nazis launched across the English Channel. An American program, Project Paperclip, had endeavored to move German scientists into American hands (and away from Soviet hands) and put them to work for the U.S.\n\nEarly nuclear-tipped rockets—such as the MGR-1 Honest John, first deployed by the U.S. in 1953—were surface-to-surface missiles with relatively short ranges (around 15 mi/25 km maximum) and yields around twice the size of the first fission weapons. The limited range meant they could only be used in certain types of military situations. U.S. rockets could not, for example, threaten Moscow with an immediate strike, and could only be used as tactical weapons (that is, for small-scale military situations).\n\nStrategic weapons—weapons that could threaten an entire country—relied, for the time being, on long-range bombers that could penetrate deep into enemy territory. In the U.S., this requirement led, in 1946, to creation of the Strategic Air Command—a system of bombers headed by General Curtis LeMay (who previously presided over the firebombing of Japan during WWII). In operations like Chrome Dome, SAC kept nuclear-armed planes in the air 24 hours a day, ready for an order to attack Moscow.\n\nThese technological possibilities enabled nuclear strategy to develop a logic considerably different from previous military thinking. Because the threat of nuclear warfare was so awful, it was first thought that it might make any war of the future impossible. President Dwight D. Eisenhower's doctrine of \"massive retaliation\" in the early years of the Cold War was a message to the USSR, saying that if the Red Army attempted to invade the parts of Europe not given to the Eastern bloc during the Potsdam Conference (such as West Germany), nuclear weapons would be used against the Soviet troops and potentially the Soviet leaders.\n\nWith the development of more rapid-response technologies (such as rockets and long-range bombers), this policy began to shift. If the Soviet Union also had nuclear weapons and a policy of \"massive retaliation\" was carried out, it was reasoned, then any Soviet forces not killed in the initial attack, or launched while the attack was ongoing, would be able to serve their own form of nuclear retaliation against the U.S. Recognizing that this was an undesirable outcome, military officers and game theorists at the RAND think tank developed a nuclear warfare strategy that was eventually called Mutually Assured Destruction (MAD).\n\nMAD divided potential nuclear war into two stages: \"first strike\" and \"second strike\". First strike meant the first use of nuclear weapons by one nuclear-equipped nation against another nuclear-equipped nation. If the attacking nation did not prevent the attacked nation from a nuclear response, the attacked nation would respond with a second strike against the attacking nation. In this situation, whether the U.S. first attacked the USSR or the USSR first attacked the U.S., the end result would be that both nations would be damaged to the point of utter social collapse.\n\nAccording to game theory, because starting a nuclear war was suicidal, no logical country would shoot first. However, if a country could launch a first strike that utterly destroyed the target country's ability to respond, that might give that country the confidence to initiate a nuclear war. The object of a country operating by the MAD doctrine is to deny the opposing country this first strike capability.\n\nMAD played on two seemingly opposed modes of thought: cold logic and emotional fear. The English phrase MAD was often known by, \"nuclear deterrence,\" was translated by the French as \"dissuasion,\" and \"terrorization\" by the Soviets. This apparent paradox of nuclear war was summed up by British Prime Minister Winston Churchill as \"the worse things get, the better they are\"—the greater the threat of mutual destruction, the safer the world would be.\n\nThis philosophy made a number of technological and political demands on participating nations. For one thing, it said that it should always be assumed that an enemy nation may be trying to acquire first strike capability, which must always be avoided. In American politics this translated into demands to avoid \"bomber gaps\" and \"missile gaps\" where the Soviet Union could potentially outshoot the Americans. It also encouraged the production of thousands of nuclear weapons by both the U.S. and the USSR, far more than needed to simply destroy the major civilian and military infrastructures of the opposing country. These policies and strategies were satirized in the 1964 Stanley Kubrick film Dr. Strangelove, in which the Soviets, unable to keep up with the US's first strike capability, instead plan for MAD by building a Doomsday Machine, and thus, after a (literally) mad US General orders a nuclear attack on the USSR, the end of the world is brought about.\nThe policy also encouraged the development of the first early warning systems. Conventional war, even at its fastest, was fought over days and weeks. With long-range bombers, from the start of a nuclear attack to its conclusion was mere hours. Rockets could reduce a conflict to minutes. Planners reasoned that conventional command and control systems could not adequately react to a nuclear attack, so great lengths were taken to develop computer systems that could look for enemy attacks and direct rapid responses.\n\nThe U.S. poured massive funding into development of SAGE, a system that could track and intercept enemy bomber aircraft using information from remote radar stations. It was the first computer system to feature real-time processing, multiplexing, and display devices. It was the first general computing machine, and a direct predecessor of modern computers.\n\nThe atomic bombings of Hiroshima and Nagasaki and the end of World War II quickly followed the 1945 Trinity nuclear test, and the Little Boy device was detonated over the Japanese city of Hiroshima on 6 August 1945. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings and killed approximately 75,000 people. Subsequently, the world’s nuclear weapons stockpiles grew.\n\nOperation Crossroads was a series of nuclear weapon tests conducted by the United States at Bikini Atoll in the Pacific Ocean in the summer of 1946. Its purpose was to test the effect of nuclear weapons on naval ships. To prepare the Bikini atoll for the nuclear tests, Bikini's native residents were evicted from their homes and resettled on smaller, uninhabited islands where they were unable to sustain themselves.\n\nNational leaders debated the impact of nuclear weapons on domestic and foreign policy. Also involved in the debate about nuclear weapons policy was the scientific community, through professional associations such as the Federation of Atomic Scientists and the Pugwash Conference on Science and World Affairs. Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when a Hydrogen bomb test in the Pacific contaminated the crew of the Japanese fishing boat \"Lucky Dragon\". One of the fishermen died in Japan seven months later. The incident caused widespread concern around the world and \"provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries\". The anti-nuclear weapons movement grew rapidly because for many people the atomic bomb \"encapsulated the very worst direction in which society was moving\".\n\nPeace movements emerged in Japan and in 1954 they converged to form a unified \"Japanese Council Against Atomic and Hydrogen Bombs\". Japanese opposition to the Pacific nuclear weapons tests was widespread, and \"an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons\". The Russell–Einstein Manifesto was issued in London on July 9, 1955 by Bertrand Russell in the midst of the Cold War. It highlighted the dangers posed by nuclear weapons and called for world leaders to seek peaceful resolutions to international conflict. The signatories included eleven pre-eminent intellectuals and scientists, including Albert Einstein, who signed it just days before his death on April 18, 1955. A few days after the release, philanthropist Cyrus S. Eaton offered to sponsor a conference—called for in the manifesto—in Pugwash, Nova Scotia, Eaton's birthplace. This conference was to be the first of the Pugwash Conferences on Science and World Affairs, held in July 1957.\n\nIn the United Kingdom, the first Aldermaston March organised by the Campaign for Nuclear Disarmament took place at Easter 1958, when several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.\n\nIn 1959, a letter in the \"Bulletin of the Atomic Scientists\" was the start of a successful campaign to stop the Atomic Energy Commission dumping radioactive waste in the sea 19 kilometres from Boston. On November 1, 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. It was the largest national women's peace protest of the 20th century.\n\nIn 1958, Linus Pauling and his wife presented the United Nations with the petition signed by more than 11,000 scientists calling for an end to nuclear-weapon testing. The \"Baby Tooth Survey,\" headed by Dr Louise Reiss, demonstrated conclusively in 1961 that above-ground nuclear testing posed significant public health risks in the form of radioactive fallout spread primarily via milk from cows that had ingested contaminated grass. Public pressure and the research results subsequently led to a moratorium on above-ground nuclear weapons testing, followed by the Partial Test Ban Treaty, signed in 1963 by John F. Kennedy and Nikita Khrushchev.\n\nBombers and short-range rockets were not reliable: planes could be shot down, and earlier nuclear missiles could cover only a limited range— for example, the first Soviet rockets' range limited them to targets in Europe. However, by the 1960s, both the United States and the Soviet Union had developed intercontinental ballistic missiles, which could be launched from extremely remote areas far away from their target. They had also developed submarine-launched ballistic missiles, which had less range but could be launched from submarines very close to the target without any radar warning. This made any national protection from nuclear missiles increasingly impractical.\n\nThe military realities made for a precarious diplomatic situation. The international politics of brinkmanship led leaders to exclaim their willingness to participate in a nuclear war rather than concede any advantage to their opponents, feeding public fears that their generation may be the last. Civil defense programs undertaken by both superpowers, exemplified by the construction of fallout shelters and urging civilians about the survivability of nuclear war, did little to ease public concerns.\n\nThe climax of brinksmanship came in early 1962, when an American U-2 spy plane photographed a series of launch sites for medium-range ballistic missiles being constructed on the island of Cuba, just off the coast of the southern United States, beginning what became known as the Cuban Missile Crisis. The U.S. administration of John F. Kennedy concluded that the Soviet Union, then led by Nikita Khrushchev, was planning to station Soviet nuclear missiles on the island (as a response to placing US Jupiter MRBMs in Italy and Turkey), which was under the control of communist Fidel Castro. On October 22, Kennedy announced the discoveries in a televised address. He announced a naval blockade around Cuba that would turn back Soviet nuclear shipments, and warned that the military was prepared \"for any eventualities.\" The missiles had 2,400 mile (4,000 km) range, and would allow the Soviet Union to quickly destroy many major American cities on the Eastern Seaboard if a nuclear war began.\n\nThe leaders of the two superpowers stood nose to nose, seemingly poised over the beginnings of a third world war. Khrushchev's ambitions for putting the weapons on the island were motivated in part by the fact that the U.S. had stationed similar weapons in Britain, Italy, and nearby Turkey, and had previously attempted to sponsor an invasion of Cuba the year before in the failed Bay of Pigs Invasion. On October 26, Khrushchev sent a message to Kennedy offering to withdraw all missiles if Kennedy committed to a policy of no future invasions of Cuba. Khrushchev worded the threat of assured destruction eloquently:\n\"You and I should not now pull on the ends of the rope in which you have tied a knot of war, because the harder you and I pull, the tighter the knot will become. And a time may come when this knot is tied so tight that the person who tied it is no longer capable of untying it, and then the knot will have to be cut. What that would mean I need not explain to you, because you yourself understand perfectly what dreaded forces our two countries possess.\"\nA day later, however, the Soviets sent another message, this time demanding that the U.S. remove its missiles from Turkey before any missiles were withdrawn from Cuba. On the same day, a U-2 plane was shot down over Cuba and another almost intercepted over the Soviet Union, as Soviet merchant ships neared the quarantine zone. Kennedy responded by accepting the first deal publicly, and sending his brother Robert to the Soviet embassy to accept the second deal privately. On October 28, the Soviet ships stopped at the quarantine line and, after some hesitation, turned back towards the Soviet Union. Khrushchev announced that he had ordered the removal of all missiles in Cuba, and U.S. Secretary of State Dean Rusk was moved to comment, \"We went eyeball to eyeball, and the other fellow just blinked.\"\n\nThe Crisis was later seen as the closest the U.S. and the USSR ever came to nuclear war and had been narrowly averted by last-minute compromise by both superpowers. Fears of communication difficulties led to the installment of the first hotline, a direct link between the superpowers that allowed them to more easily discuss future military activities and political maneuverings. It had been made clear that missiles, bombers, submarines, and computerized firing systems made escalating any situation to Armageddon far more easy than anybody desired.\n\nAfter stepping so close to the brink, both the U.S. and the USSR worked to reduce their nuclear tensions in the years immediately following. The most immediate culmination of this work was the signing of the Partial Test Ban Treaty in 1963, in which the U.S. and USSR agreed to no longer test nuclear weapons in the atmosphere, underwater, or in outer space. Testing underground continued, allowing for further weapons development, but the worldwide fallout risks were purposefully reduced, and the era of using massive nuclear tests as a form of saber rattling ended.\n\nIn December 1979, NATO decided to deploy cruise and Pershing II missiles in Western Europe in response to Soviet deployment of intermediate range mobile missiles, and in the early 1980s, a \"dangerous Soviet-US nuclear confrontation\" arose. In New York on June 12, 1982, one million people gathered to protest about nuclear weapons, and to support the second UN Special Session on Disarmament. As the nuclear abolitionist movement grew, there were many protests at the Nevada Test Site. For example, on February 6, 1987, nearly 2,000 demonstrators, including six members of Congress, protested against nuclear weapons testing and more than 400 people were arrested. Four of the significant groups organizing this renewal of anti-nuclear activism were Greenpeace, The American Peace Test, The Western Shoshone, and Nevada Desert Experience.\n\nThere have been at least four major false alarms, the most recent in 1995, that resulted in the activation of nuclear attack early warning protocols. They include the accidental loading of a training tape into the American early-warning computers; a computer chip failure that appeared to show a random number of attacking missiles; a rare alignment of the Sun, the U.S. missile fields and a Soviet early-warning satellite that caused it to confuse high-altitude clouds with missile launches; the launch of a Norwegian research rocket resulted in President Yeltsin activating his nuclear briefcase for the first time.\n\nIn the fifties and sixties, three more countries joined the \"nuclear club.\" The United Kingdom had been an integral part of the Manhattan Project following the Quebec Agreement in 1943. The passing of the McMahon Act by the United States in 1946 unilaterally broke this partnership and prevented the passage of any further information to the United Kingdom. The British Government, under Clement Attlee, determined that a British Bomb was essential. Because of British involvement in the Manhattan Project, Britain had extensive knowledge in some areas, but not in others.\n\nAn improved version of 'Fat Man' was developed, and on 26 February 1952, Prime Minister Winston Churchill announced that the United Kingdom also had an atomic bomb and a successful test took place on 3 October 1952. At first these were free-fall bombs, intended for use by the V Force of jet bombers. A Vickers Valiant dropped the first UK nuclear weapon on 11 October 1956 at Maralinga, South Australia. Later came a missile, Blue Steel, intended for carriage by the V Force bombers, and then the Blue Streak medium-range ballistic missile (later canceled). Anglo-American cooperation on nuclear weapons was restored by the 1958 US-UK Mutual Defence Agreement. As a result of this and the Polaris Sales Agreement, the United Kingdom has bought United States designs for submarine missiles and fitted its own warheads. It retains full independent control over the use of the missiles. It no longer possesses any free-fall bombs.\n\nFrance had been heavily involved in nuclear research before World War II through the work of the Joliot-Curies. This was discontinued after the war because of the instability of the Fourth Republic and lack of finances. However, in the 1950s, France launched a civil nuclear research program, which produced plutonium as a byproduct.\n\nIn 1956, France formed a secret Committee for the Military Applications of Atomic Energy and a development program for delivery vehicles. With the return of Charles de Gaulle to the French presidency in 1958, final decisions to build a bomb were made, which led to a successful test in 1960. Since then, France has developed and maintained its own nuclear deterrent independent of NATO.\n\nIn 1951, China and the Soviet Union signed an agreement whereby China supplied uranium ore in exchange for technical assistance in producing nuclear weapons. In 1953, China established a research program under the guise of civilian nuclear energy. Throughout the 1950s the Soviet Union provided large amounts of equipment. But as the relations between the two countries worsened the Soviets reduced the amount of assistance and, in 1959, refused to donate a bomb for copying purposes. Despite this, the Chinese made rapid progress and tested an atomic bomb on October 16, 1964, at Lop Nur. They tested a nuclear missile on October 25, 1966, and a hydrogen bomb on June 14, 1967.\n\nChinese nuclear warheads were produced from 1968 and thermonuclear warheads from 1974. It is also thought that Chinese warheads have been successfully miniaturised from 2200 kg to 700 kg through the use of designs obtained by espionage from the United States. The current number of warheads is unknown, but as of 2017 it is thought to be in the low hundreds. China is the only nuclear weapons state to have guaranteed the non-first use of nuclear weapons.\n\nAfter World War II, the balance of power between the Eastern and Western blocs and the fear of global destruction prevented the further military use of atomic bombs. This fear was even a central part of Cold War strategy, referred to as the doctrine of Mutually Assured Destruction. So important was this balance to international political stability that a treaty, the Anti-Ballistic Missile Treaty (or ABM treaty), was signed by the U.S. and the USSR in 1972 to curtail the development of defenses against nuclear weapons and the ballistic missiles that carry them. This doctrine resulted in a large increase in the number of nuclear weapons, as each side sought to ensure it possessed the firepower to destroy the opposition in all possible scenarios.\n\nEarly delivery systems for nuclear devices were primarily bombers like the United States B-29 Superfortress and Convair B-36, and later the B-52 Stratofortress. Ballistic missile systems, based on Wernher von Braun's World War II designs (specifically the V-2 rocket), were developed by both United States and Soviet Union teams (in the case of the U.S., effort was directed by the German scientists and engineers although the Soviet Union also made extensive use of captured German scientists, engineers, and technical data).\n\nThese systems were used to launch satellites, such as Sputnik, and to propel the Space Race, but they were primarily developed to create Intercontinental Ballistic Missiles (ICBMs) that could deliver nuclear weapons anywhere on the globe. Development of these systems continued throughout the Cold War—though plans and treaties, beginning with the Strategic Arms Limitation Treaty (SALT I), restricted deployment of these systems until, after the fall of the Soviet Union, system development essentially halted, and many weapons were disabled and destroyed. On January 27, 1967, more than 60 nations signed the Outer Space Treaty, banning nuclear weapons in space.\n\nThere have been a number of potential nuclear disasters. Following air accidents U.S. nuclear weapons have been lost near Atlantic City, New Jersey (1957); Savannah, Georgia (1958) (see Tybee Bomb); Goldsboro, North Carolina (1961); off the coast of Okinawa (1965); in the sea near Palomares, Spain (1966) (see 1966 Palomares B-52 crash); and near Thule, Greenland (1968) (see 1968 Thule Air Base B-52 crash). Most of the lost weapons were recovered, the Spanish device after three months' effort by the DSV Alvin and DSV Aluminaut.\n\nThe Soviet Union was less forthcoming about such incidents, but the environmental group Greenpeace believes that there are around forty non-U.S. nuclear devices that have been lost and not recovered, compared to eleven lost by America, mostly in submarine disasters. The U.S. has tried to recover Soviet devices, notably in the 1974 Project Azorian using the specialist salvage vessel \"Hughes Glomar Explorer\" to raise a Soviet submarine. After news leaked out about this boondoggle, the CIA would coin a favorite phrase for refusing to disclose sensitive information, called glomarization: \"We can neither confirm nor deny the existence of the information requested but, hypothetically, if such data were to exist, the subject matter would be classified, and could not be disclosed.\"\n\nThe collapse of the Soviet Union in 1991 essentially ended the Cold War. However, the end of the Cold War failed to end the threat of nuclear weapon use, although global fears of nuclear war reduced substantially. In a major move of symbolic de-escalation, Boris Yeltsin, on January 26, 1992, announced that Russia planned to stop targeting United States cities with nuclear weapons.\n\nThe designing, testing, producing, deploying, and defending against nuclear weapons is one of the largest expenditures for the nations which possess nuclear weapons. In the United States during the Cold War years, between \"one quarter to one third of all military spending since World War II [was] devoted to nuclear weapons and their infrastructure.\" \nAccording to a retrospective Brookings Institution study published in 1998 by the Nuclear Weapons Cost Study Committee (formed in 1993 by the W. Alton Jones Foundation), the total expenditures for U.S. nuclear weapons from 1940 to 1998 was $5.5 trillion in 1996 Dollars.\n\nFor comparison, the total public debt at the end of fiscal year 1998 was $5,478,189,000,000 in 1998 Dollars or $5.3 trillion in 1996 Dollars. The \"entire public debt\" in 1998 was therefore equal to the cost of research, development, and deployment of U.S. nuclear weapons and nuclear weapons-related programs during the Cold War.\n\nThe \"second nuclear age\" can be regarded as proliferation of nuclear weapons among lesser powers and for reasons other than the American-Soviet-Chinese rivalry.\n\nIndia embarked relatively early on a program aimed at nuclear weapons capability, but apparently accelerated this after the Sino-Indian War of 1962. India's first atomic-test explosion was in 1974 with \"Smiling Buddha\", which it described as a \"peaceful nuclear explosion.\"\n\nAfter the collapse of Eastern Military High Command and the disintegration of Pakistan as a result of the 1971 Winter war, Bhutto of Pakistan launched scientific research on nuclear weapons. The Indian test caused Pakistan to spur its programme, and the ISI conducted successful espionage operations in the Netherlands, while also developing the programme indigenously. India tested fission and perhaps fusion devices in 1998, and Pakistan successfully tested fission devices that same year, raising concerns that they would use nuclear weapons on each other.\n\nAll of the former Soviet bloc countries with nuclear weapons (Belarus, Ukraine, and Kazakhstan) returned their warheads to Russia by 1996.\n\nSouth Africa also had an active program to develop uranium-based nuclear weapons, but dismantled its nuclear weapon program in the 1990s. Experts do not believe it actually tested such a weapon, though it later claimed it constructed several crude devices that it eventually dismantled. In the late 1970s American spy satellites detected a \"brief, intense, double flash of light near the southern tip of Africa.\" Known as the Vela Incident, it was speculated to have been a South African or possibly Israeli nuclear weapons test, though some feel that it may have been caused by natural events or a detector malfunction.\n\nIsrael is widely believed to possess an arsenal of up to several hundred nuclear warheads, but this has never been officially confirmed or denied (though the existence of their Dimona nuclear facility was confirmed by Mordechai Vanunu in 1986).\n\nIn January 2004, Dr A. Q. Khan of Pakistan's programme confessed to having been a key mover in \"proliferation activities\", seen as part of an international proliferation network of materials, knowledge, and machines from Pakistan to Libya, Iran, and North Korea.\n\nNorth Korea announced in 2003 that it had several nuclear explosives. The first claimed detonation was the 2006 North Korean nuclear test, conducted on October 9, 2006. On May 25, 2009, North Korea continued nuclear testing, violating United Nations Security Council Resolution 1718. A third test was conducted on , two tests were conducted in 2016 in January and September, followed by test a year later in September 2017.\n\n\n\n\n\n\n"}
{"id": "38402258", "url": "https://en.wikipedia.org/wiki?curid=38402258", "title": "Industrial porcelain enamel", "text": "Industrial porcelain enamel\n\nIndustrial porcelain enamel (also known as glass lining, glass-lined steel, or glass fused to steel) is the use of porcelain enamel (also known as vitreous enamel) for industrial, rather than artistic, applications. Porcelain enamel, a thin layer of ceramic or glass applied to a substrate of metal, is used to protect surfaces from chemical attack and physical damage, modify the structural characteristics of the substrate, and improve the appearance of the product.\n\nEnamel has been used for art and decoration since the period of Ancient Egypt, and for industry since the Industrial Revolution. It is most commonly used in the production of cookware, home appliances, bathroom fixtures, water heaters, and scientific laboratory equipment.\n\nThe most important characteristic of porcelain enamel, from an industrial perspective, is its resistance to corrosion. Mild steel is used in almost every industry and a huge array of products; porcelain enamel is a very economic way of protecting this, and other chemically vulnerable materials, from corrosion. It can also produce very smooth, glossy finishes in a wide array of colours; these colours will not fade on exposure to UV light, as paint will. Being a fired ceramic, porcelain enamel is also highly heat-resistant; this allows it to be used in high-temperature applications where an organic anti-corrosion coating or galvanization may be impractical or even dangerous (\"see Metal fume fever\").\n\nPorcelain enamel also sees less frequent employment of some of its other properties; examples are its abrasion resistance, where it may perform better than many metals; its resistance to organic solvents, where it is entirely impervious; its resistance to thermal shock, where it can resist rapid cooling from temperatures and higher; and its longevity.\n\nPorcelain enamel is used most often in the manufacture of products that will be expected to come under regular chemical attack or high heat such as cookware, burners, and laboratory equipment. It is used in the production of many household goods and appliances, especially those used in the kitchen or bathroom area: pots, pans, cooktops, appliances, sinks, toilets, bathtubs, even walls, counters, and other surfaces.\n\nPorcelain enamel is also used architecturally as a coating for wall panels. It may be used externally to provide weather resistance and desirable appearance, or internally to provide wear resistance; for example, on escalator side panels and tunnel walls. In recent years, agricultural silos have also been constructed with porcelain enamelled steel plates to protect the interior from corrosion and the exterior from weathering; this may indicate a future trend of coating all outdoor mild steel products in a weather-resistant porcelain enamel.\n\nThe application of industrial porcelain enamel can be a complicated process involving many different and very technical steps. All enamelling processes involve the mixture and preparation of frit, the unfired enamel mixture; the preparation of the substrate; the application and firing; and then finishing processes. Most modern applications also involve two layers of enamel: a ground-coat to bond to the substrate and a cover-coat to provide the desired external properties.\n\nBecause frits frequently must be mixed at higher temperatures than the firing requires, most modern industrial enamellers do not mix their own frits completely; frit is most often purchased from dedicated frit producers in standard compositions and then any special ingredients added before application and firing.\n\nFor ground coats, the composition of a frit for any given application is determined primarily by the metal used as the substrate: different varieties of steel, and different metals such as aluminium and copper, require different frit compositions to bond to them. For cover coats the frit is composed to both bind to the ground-coat and produce the desired external properties. Frit is normally prepared by mixing the ingredients and then milling the mixture into a powder. The ingredients, most often metal oxides and minerals such as quartz (or silica sand), soda ash, borax, and cobalt oxide, are acquired in particulate form; the precise chemical composition and amount of each ingredient must be carefully measured and regulated. Once prepared, this powdered frit is then slumped and stirred to promote even distribution of materials; most frits are smelted at temperatures between 1150 and . After smelting, the frit is again milled into a powder, most often by ball mill grinding.\n\nFor wet application of enamel, a slurry of frit suspended in water must be created. In order to remain in suspension, frits must either be milled to an extremely fine particle size or mixed with a suspension agent such as clay or electrolytes.\n\nThe metal to be used as a substrate is primarily determined by the application to which the product will be put, independent of any enamel considerations. Most commonly used are steels of various compositions, but also used are aluminium and copper.\n\nBefore the application of enamel, the surface of the substrate must be prepared with a number of processes. The most important processes are the cleaning of the surface of the substrate; all remnants of chemicals, rusts, oils, and other contaminants must be completely removed. To facilitate this, frequent processes performed on substrates are degreasing, pickling (which can also etch the surface and provide anchoring points for the enamel), alkaline neutralization, and rinsing.\n\nEnamel may be applied to the substrate via many different methods. These methods are most often delineated into either wet or dry applications, determined by whether the enamel is applied as a dry powder or a liquid slurry suspension.\n\nThe simplest method of dry application, especially for cast-iron substrates, is to heat the substrate and roll it in powdered frit. The frit particles melt on contact with the hot substrate and adhere to its surface. This method requires a high level of operator skill and concentration to achieve an even coating, and due to its inconstant nature is not often used in industrial applications.\n\nThe most common method of dry application used in industry today is electrostatic deposition. Before application, the dry frit must be encapsulated in an organic silane; this allows the frit to hold an electrical charge during application. An electrostatic gun fires the dry frit powder onto the electrically earthed metal substrate; electrical forces bind the charged powder to the substrate and it adheres.\n\nThe simplest method of wet application is to dip the substrate in a bath of liquid slurry; complete immersion coats all available surfaces of the substrate. Dipping is not often used in industry, however, because many preliminary trial dippings are required before the thickness of the coat can be predicted reliably enough for the desired application.\n\nA form of dipping suitable for modern industrial application is flow coating. Rather than dip the product in a bath of slurry, slurry is flowed over the surface of the product to be coated. This method allows for much more economical use of slurry and time; it is capable of allowing very rapid production runs.\n\nWet enamel may also be sprayed onto the product using specialized spray guns. Liquid slurry is fed into the nozzle of a spray gun, and compressed air atomizes the slurry and ejects it from the nozzle of the gun in a controlled jet.\n\nFiring, where coated substrates are passed through a furnace to experience long periods of stable high temperatures, converts the adhering particles of frit into a continuous glass layer. The effectiveness of the process is highly dependent on the time, temperature, and the quality or thickness of the coating on the substrate. Most frits for industrial applications are fired for as low as 20 minutes, but frits for very heavy-duty industrial applications may take double this time. Porcelain enamel coatings on aluminium substrates may be fired at temperatures as low as 530°C, but most steel substrates require temperatures in excess of 800°C.\n\nPorcelain enamel has been applied to jewelry metals such as gold, silver, and copper since antiquity for the purposes of decoration. It was not until the Industrial Revolution that ferrous metals first became the subject of porcelain enamelling processes; these first attempts were met with limited success. A reliably successful technique was not developed until the middle of the 19th century, with the development of a method for enamelling cast-iron cooking pots in Germany. It was not long before this method of enamelling became outdated with the development of new ferrous substrates, and most modern research into porcelain enamelling is concerned with creating an acceptable bond between enamels and new metal substrates.\n\nThe production of porcelain enamelled products on an industrial scale first began in Germany in 1840. The method used was very primitive compared to modern methods: the product was heated to a very high temperature and dusted with enamel, then immediately fired. This frequently resulted in poor adhesion or a spotty coat; two coats were always required to achieve a continuous, corrosion-resistant surface. It could only be applied to cast- and wrought-iron, and only used for relatively simple products like pots and pans.\n\nThe ability to apply porcelain enamel to sheet steels was not developed until 1900, with the discovery that making minor changes to the composition of the enamel, such as including cobalt oxides as minor components, could drastically improve its adhesion ability to carbon steels. Concurrent with this development was the first use of wet-slurry enamel application; this allowed porcelain enamel to be applied to much more complex shapes by dipping the shape into the liquid enamel slurry.\n\nUp until the 1930s, all enamel applications required two coats of enamel: an undercoat to adhere to the substrate which was always blue (due in part to the presence of cobalt oxides), and a top coat of the desired colour (most often white). It was not until 1930 that the use of zero carbon steel (steel with less than 0.005% carbon content) as a substrate was linked to allowing lighter-colored enamels to adhere directly to the substrate.\n\nBibliography\n"}
{"id": "39854746", "url": "https://en.wikipedia.org/wiki?curid=39854746", "title": "Jules Guéron", "text": "Jules Guéron\n\nJules Guéron (2 June 1907 – 11 October 1990) was a French physical chemist and atomic scientist who played a key role in the development of atomic energy in France.\n\nGuéron was educated at Lycée Charlemagne in Paris (1913-1924).\nHe graduated with the \"baccalauréat\" (high school degree) in Latin, Sciences and Mathematics. \nFrom 1926 to 1935 he studied at the University of Paris-Sorbonne in Prof. Marcel Guichard's laboratory, earning a doctorate in physical sciences for which he was awarded the Adrian prize of the French Society of Chemistry.\n\nIn 1938 Guéron was appointed lecturer at the University of Strasbourg. He married Geneviève Bernheim in 1934 and had three sons (Maurice, Henri and Frédéric).\n\nResponding to the historic call for resistance of General Charles de Gaulle, Guéron made his way to Great Britain in June 1940. He enlisted in the Free French Forces and was at first assigned to the Service technique de l'Armement. In December 1941 he was transferred to the Anglo-Canadian Atomic Energy Project, known as \"Tube Alloys\", at the Cavendish Laboratory in Cambridge.\n\nIn 1943 Guéron moved to Montreal as a member of the Tube Alloys team, which at this point also included the French scientists Halban, Auger, Goldschmidt, and Kowarski. \nWork at Tube Alloys did not always proceed smoothly. Most notable was a lengthy interruption of the collaboration with the (American) Manhattan project which lasted until the August 1943 Quebec agreement between Winston Churchill and Franklin D. Roosevelt. The French scientists had their own concerns. Some were highly critical of de Gaulle's constant opposition to the United States, and they imagined that he might reconsider if made aware of this specific and significant instance of America's awesome strength. In this spirit, when General de Gaulle visited Ottawa on 11 July 1944, Guéron personally imparted his near certainty that within one year the US would master a highly powerful weapon: \"une bombe, une ville.\"\n\nIn 1945, the French government established the Atomic Energy Commission (CEA) with the charter of exploring atomic energy. \nGuéron was nominated Head of the Chemistry unit. In 1951 he became the first director of the CEA's nuclear research center in Saclay.\n\nIn 1958 Guéron was recruited by the European Atomic Energy Community (Euratom) \nas General Director of Research and Education (1958–1968).\n\nFrom 1969 to 1976 Guéron was a Professor at the University of Paris-Sud. \nConcurrently, he consulted for Framatome, \nthe firm responsible for building the vast park of French electricity-producing nuclear reactors.\n\nHe also served as Secretary of the International Commission on Atomic Weights \n(1960–1969). He is the author of several books and of many articles on atomic energy. \nHe was made \"Officier de la Légion d'honneur\".\n\n\n"}
{"id": "49228026", "url": "https://en.wikipedia.org/wiki?curid=49228026", "title": "LIFE Factory Microgrid", "text": "LIFE Factory Microgrid\n\nFactory Microgrid is a demonstrative project cofinanced by the LIFE+ 2013 programme of the European Commission and whose origin can be explained within the framework of the 20-20-20 challenge of the European Union to reduce CO2 emissions and energy consumption. More specifically, it can be framed in theme 1, \"Climate Change\", and within the line of action \"Development of innovative practices for the management of smart grids in the context of highly decentralized production of renewable energy\". Its main objective is to demonstrate, through the implementation of a full-scale industrial smartgrid that microgrids can become one of the most suitable solutions for energy generation and management in factories that want to minimize their environmental impact. At a national level it is one of the first experiences regarding the implementation of a smartgrid in an industrial plant with and integrated fleet of electric vehicles. Factory Microgrid will take place between July 2014 and June 2017 and it represents an investment of around 2 million euros. Approximately 50% of the total amount will be financed by the LIFE + programme. Project partners are the Jofemar Corporation and the National Renewable Energy Centre, CENER. Project Manager: Isabel Carrilero (Jofemar)\n\nWhat is a microgrid or a smartgrid? Smart grids are energy networks that can automatically monitor energy flows and adjust to changes in energy supply and demand accordingly. When coupled with smart metering systems, smart grids reach consumers and suppliers by providing information on real-time consumption. Smart grids can also help to better integrate renewable energy.\n\n\n\n\nThe LIFE program programme is the EU's funding instrument\nfor the environment. The general objective\nof LIFE is to contribute to the implementation,\nupdating and development of EU environmental\npolicy and legislation by co-financing pilot or demonstration projects with European added value\n\n"}
{"id": "16036981", "url": "https://en.wikipedia.org/wiki?curid=16036981", "title": "Lava filter", "text": "Lava filter\n\nA lava filter is a biological filter that uses lavastone pebbles as support material on which microorganisms can grow in a thin biofilm. This community of microorganisms, known as the periphyton break down the odor components in the air, such as hydrogen sulfide. The biodegradation processes that occurs is provided by the bacteria themselves. In order for this to work, sufficient oxygen as well as water and nutrients (for cell growth) is to be supplied.\n\nContaminated air enters the system at the bottom of the filter and passes in an upward direction through the filter. Water is supplied through the surface of the biofilter and trickles down over the lava rock to the bottom, where it is collected. Constant water provisioning at the surface prevents dry-out of the active bacteria in the biofilm and ensures a constant pH value in the filter. It also functions to make nutrients available to the bacteria.\nPercolating water collected at the filter bottom contains odour components as well as sulfuric acid from the biological oxidation of hydrogen sulfide. Depending on the process design the collected water is recirculated or subjected to further treatment.\n\nAt present: 2 types of systems are used;\n\nThese are constructed out of 2 layers of lava pebbles and a top layer of nutrient-free soil (only at the plants roots). On top, water-purifying plants (as \"Iris pseudacorus\" and \"Sparganium erectum)\" are placed. Usually, around 1/4 of the dimension of lavastone is required to purify the water and just like slow sand filters, a series of herringbone drains are placed (with lava filters these are placed at the bottom layer).\n\nThe water-purifying plants used with constantly submerged, planted, lavafilters (e.g. treatment ponds, self-purifying irrigation reservoirs, ...) include a wide variety of plants, depending on the local climate and geoographical location. Plants are usually chosen which are indigenous in that location for environmental reasons and optimum workings of the system. In addition to water-purifying (de-nutrifying) plants, plants that supply oxygen, and shade are also added in ecologic water catchments, ponds, ... This to allow a complete ecosystem to form. Finally, in addition to plants, locally grown bacteria and non-predatory fish are also added to eliminate pests. The bacteria are usually grown locally by submerging straw in water and allowing it to form bacteria (arriving from the surrounding atmosphere). The plants used (placed on an area 1/4 of the water mass) are divided in 4 separate water depth-zones; knowingly:\n\n\nFinally, three types of (non-predatory) fish (surface; bottom and ground-swimmers) are chosen. This of course to ensure that the fish may 'get along'. Examples of the three types of fish (for temperate climates) are:\n"}
{"id": "3514565", "url": "https://en.wikipedia.org/wiki?curid=3514565", "title": "Lead shielding", "text": "Lead shielding\n\nLead shielding refers to the use of lead as a form of radiation protection to shield people or objects from radiation so as to reduce the effective dose. Lead can effectively attenuate certain kinds of radiation because of its high density and high atomic number; principally, it is effective at stopping gamma rays and x-rays.\n\nLead's high density is caused by the combination of its high atomic mass and the relatively small size of its bond lengths and atomic radius. The high atomic mass means that more electrons are needed to maintain a neutral charge and the small bond length and a small atomic radius means that many atoms can be packed into a particular lead structure. Because of lead’s density and large number of electrons, it is well suited to scattering x-rays and gamma-rays. These rays form photons, a type of boson, which impart energy onto electrons when they come into contact. Without a lead shield, the electrons within a person’s body would be affected, which could damage their DNA. When the radiation attempts to pass through lead, its electrons absorb and scatter the energy. Eventually though, the lead will degrade from the energy to which it is exposed. However, lead is not effective against all types of radiation. High energy electrons (including beta radiation) incident on lead may create bremsstrahlung radiation, which is potentially more dangerous to tissue than the original radiation. Furthermore, lead is not a particularly effective absorber of neutron radiation.\n\nLead is used for shielding in x-ray machines, nuclear power plants, labs, military equipment, and other places where radiation may be encountered. There is great variety in the types of shielding available both to protect people and to shield equipment and experiments. Personal shielding includes lead aprons (such as the familiar garment used during dental x-rays), thyroid shields, and lead gloves. There are also a variety of shielding devices available for laboratory equipment, including lead castles, structures composed of lead bricks, and lead \"pigs\", thick containers for storing and transporting radioactive samples.\n\n\n"}
{"id": "24434959", "url": "https://en.wikipedia.org/wiki?curid=24434959", "title": "Lift table bellows", "text": "Lift table bellows\n\nA lift table bellows (also known as lift table skirting) is a safety device that forms a protective barrier between the lift table operator and the equipment's moving parts.\n\nThe protective barrier is constructed from a wide range of material with 23oz vinyl being the most common in industrial applications.\n\nThe purpose of the lift table bellows is to keep the operator's hands and feet from getting inside of the lifter where they could be pinched.\n\nIn addition to protecting the operators, lift table skirting also protects the equipment. This protective cover can prevent the ingress of abrasive dust particles, dirt, and other common contaminants from prematurely wearing out precision machine parts. Thereby, this reduces expensive downtime and loss of vital machine accuracy and up time.\n\nCommon purposes of lift table bellows include:\n\nIndustries that commonly use lift tables include:\n\nThey are also used in hospitals and physician's offices.\n\nIn the United States, in some instances, the U.S. Federal Occupational Safety and Health Administration requires that bellows be installed on all moving equipment to protect nearby operators and associates. This varies from state to state and industry.\n"}
{"id": "4981347", "url": "https://en.wikipedia.org/wiki?curid=4981347", "title": "MON-50", "text": "MON-50\n\nThe MON-50 is a claymore shaped (rectangular, slightly concave), plastic bodied, directional type of anti-personnel mine designed and manufactured in the Soviet Union. It is designed to wound or kill by explosive fragmentation. The mine is similar to the American M18 Claymore with a few differences.\n\nIt has folding scissor type legs for supporting and aiming, but it also has an attachment point on the bottom for connecting a special clamp/spike which can be attached to wood, metal etc. It has a peep sight centered on the top which is flanked by two detonator cavities. The mine contains 700g of RDX (PVV-5A) to propel approximately 540 or 485 fragments to a lethal range of 50 meters in a 54° arc (spread of 45 meters at 50 meter range). The fragments can be steel balls (540) or short steel rods (485) depending on the variant.\n\nThe MON-50 is usually command actuated using a PN manual inductor and an EDP-R electric detonator. It can also be actuated by a variety of booby trap (BT) switches including the MUV series pull; the MVE-72 electric breakwire; or the VP13 seismic controller.\n\nThe MON 50 will usually be mounted above ground level on the surface or up in trees to give the greatest dispersion of fragments. It is waterproof and will function effectively from +50 to -50 °C (it can be buried in snow as long as the pack in front of the mine doesn't exceed 10 cm, any more will greatly reduce the mine effectiveness).\n\nThe mine can be located visually or with metal detectors under most field conditions. Depending on its actuation method the MON-50 may be resistant to blast overpressure from explosive breaching systems like the Giant Viper and M58 MICLIC.\n\nThe MON-50 is currently manufactured in Russia and also manufactured for export in Bulgaria. The MON 50 is widely used in many parts of the world. It comes in a two pouch cloth bandolier which holds all the components for securing and command actuating the mine. It may also come packed in a VKPM-2 set which contains 4 mines complete with miscellaneous fuzes, control panel and wire.\n\n\nThe mine has conventional or advanced seismic influence fuzing. It is a hand laid directional fragmentation mine which is normally command actuated (always secure command wires). The MON-50 is known to be used with the VP13 seismic controller which prevents close approach for any clearance operations, or to a variety of BT fuzes.\n\nOn detonation the mine will normally propel lethal fragmentation to a range between 40 and 60 meters, although the actual hazard range for these types of mines can be as high as 300 metres based on US Army tests of the M18A1 \"Claymore\" (this is directly in front of the mine, fragmentation range and density drop off to 125 meters to the sides and rear of these mines).\n\n"}
{"id": "233525", "url": "https://en.wikipedia.org/wiki?curid=233525", "title": "MOOSE", "text": "MOOSE\n\nMOOSE, originally an acronym for Man Out Of Space Easiest but later changed to the more professional-sounding Manned Orbital Operations Safety Equipment, was a proposed emergency \"bail-out\" system capable of bringing a single astronaut safely down from Earth orbit to the planet's surface.\n\nThe design was proposed by General Electric in the early 1960s. The system was quite compact, weighing and fitting inside a suitcase-sized container. It consisted of a small twin-nozzle rocket motor sufficient to deorbit the astronaut, a PET film bag long with a flexible ablative heat shield on the back, two pressurized canisters to fill it with polyurethane foam, a parachute, radio equipment and a survival kit.\n\nThe astronaut would leave the vehicle in a space suit, climb inside the plastic bag, and then fill it with foam. The bag had the shape of a blunt cone, with the astronaut embedded in its base facing outward. The rocket pack would protrude from the bag and be used to slow the astronaut's orbital speed enough so that they would reenter Earth's atmosphere, and the foam-filled bag would act as insulation during the subsequent aerobraking. Finally, once the astronaut had descended to where the air was sufficiently dense, the parachute would automatically deploy and slow the astronaut's fall to . The foam heat shield would serve a final role as cushioning when the astronaut touched down and as a flotation device should they land on water. The radio beacon would guide rescuers.\n\nGeneral Electric performed preliminary testing on some of the components of the MOOSE system, including flying samples of heat shield material on a Mercury mission, inflating a foam-filled bag with a human subject embedded inside, and test-dropping dummies in MOOSE foam shields short distances. U.S. Air Force Capt. Joe Kittinger's historic freefall from a balloon at in August 1960 also helped demonstrate the feasibility of such extreme parachuting. However, the MOOSE system was nonetheless always intended as an extreme emergency measure when no other option for returning an astronaut to Earth existed; falling from orbit protected by nothing more than a spacesuit and a bag of foam was unlikely to ever become a particularly safe—or enticing—maneuver.\n\nNeither NASA nor the U.S. Air Force expressed an interest in the MOOSE system, and so by the end of the 1960s, the program was quietly shelved.\n\n\n"}
{"id": "996678", "url": "https://en.wikipedia.org/wiki?curid=996678", "title": "MPU-401", "text": "MPU-401\n\nThe MPU-401, where \"MPU\" stands for MIDI Processing Unit, is an important but now obsolete interface for connecting MIDI-equipped electronic music hardware to personal computers. It was designed by Roland Corporation, which also co-authored the MIDI standard.\n\nReleased around 1984, the original MPU-401 was an external breakout box providing MIDI IN/MIDI OUT/MIDI THRU/TAPE IN/TAPE OUT/MIDI SYNC connectors, for use with a separately-sold interface card/cartridge (\"MPU-401 interface kit\") inserted into a computer system. For this setup, the following \"interface kits\" were made:\n\nIn 2014 hobbyists built clones of the MIF-IPC-A card for PCs.\n\nLater, Roland would put most of the electronics originally found in the breakout box onto the interface card itself, thus reducing the size of the breakout box. Products released in this manner:\n\n\nStill later, Roland would get rid of the breakout box completely and put all connectors on the back of the interface card itself. Products released in this manner:\n\n\nBy the late 1980s other manufacturers of PCBs developed intelligent MPU-401 clones. Some of these, like Voyetra, were equipped with Roland chips whereas others had retro-engineered ROMs (Midiman / Music Quest).\n\nExamples:\n\nIn 2015 hobbyists developed a Music Quest PC MIDI Card 8BIT clone. In 2017/2018 hobbyists developed a revision of the Music Quest PC MIDI Card 8BIT clone that includes a wavetable header in analogy of the Roland MPU-401AT.\n\nThe MPU-401 can work in two modes, \"normal mode\" and \"UART mode\". \"Normal mode\" would provide the host system with an 8-track sequencer, MIDI clock output, SYNC 24 signal output, Tape Sync and a metronome; as a result of these features, it is often called \"intelligent mode\". Compare this to UART mode, which reduces the MPU-401 to simply relaying in-/outcoming MIDI data bytes.\n\nAs computers became more powerful, the features offered in \"intelligent mode\" became obsolete, as implementing them in the host system's software became more efficient (than paying for dedicated hardware that will do them). As a result, the UART mode became the dominant mode of operation, with many clones not supporting the \"intelligent mode\" at all, being advertised as MPU-401 compatible.\n\nIn the mid 2010s a hobbyist platform software interface, SoftMPU, was written that upgrades UART (non intelligent) MPU-401 interfaces to intelligent MPU-401 interface.\n\nIn 2015 a PCB (HardMPU) was developed that incorporates SoftMPU as logic on hardware (so that the PC's CPU does not have to process intelligent MIDI). \n\nPhysical MIDI connections are increasingly replaced with the USB interface, and a USB to MIDI converter in order to drive musical peripherals which do not yet have their own USB ports. Often, peripherals are able to accept MIDI input through USB and route it to the traditional DIN connectors. While MPU-401 support is no longer included in Windows Vista, a driver is available on Windows Update. As of 2011 the interface was still supported by Linux and Mac OS X.\n\n"}
{"id": "35544723", "url": "https://en.wikipedia.org/wiki?curid=35544723", "title": "Ministry of Energy (Myanmar)", "text": "Ministry of Energy (Myanmar)\n\nThe Ministry of Energy (, 'MOE') is a ministry in the Burmese government responsible for the country's energy sector, in particular exploration of crude oil and natural gas and manufacture and distribution of petrochemicals and petroleum products.\n\nThe ministry was organized as Ministry of Electricity and Energy by President Htin Kyaw in 2016,March.\nMyanmar Engineering Society has identified at least 39 locations capable of geothermal power production and some of these hydrothermal reservoirs lie quite close to Yangon which is a significant underutilized resource for electrical generation to accelerate rural & economic development.\n"}
{"id": "3205596", "url": "https://en.wikipedia.org/wiki?curid=3205596", "title": "Neutron poison", "text": "Neutron poison\n\nIn applications such as nuclear reactors, a neutron poison (also called a neutron absorber or a nuclear poison) is a substance with a large neutron absorption cross-section. In such applications, absorbing neutrons is normally an undesirable effect. However neutron-absorbing materials, also called poisons, are intentionally inserted into some types of reactors in order to lower the high reactivity of their initial fresh fuel load. Some of these poisons deplete as they absorb neutrons during reactor operation, while others remain relatively constant.\n\nThe capture of neutrons by short half-life fission products is known as reactor poisoning; neutron capture by long-lived or stable fission products is called reactor slagging.\n\nSome of the fission products generated during nuclear reactions have a high neutron absorption capacity, such as xenon-135 (microscopic cross-section σ = 2,000,000 b (barns); up to 3 million barns in reactor conditions) and samarium-149 (σ = 74,500 b). Because these two fission product poisons remove neutrons from the reactor, they will affect the thermal utilization factor and thus the reactivity. The poisoning of a reactor core by these fission products may become so serious that the chain reaction comes to a standstill.\n\nXenon-135 in particular tremendously affects the operation of a nuclear reactor because it is the most powerful known neutron poison. The inability of a reactor to be restarted due to the buildup of xenon-135 (reaches a maximum after about 10 hours) is sometimes referred to as \"xenon precluded start-up\". The period of time in which the reactor is unable to override the effects of xenon-135 is called the \"xenon dead time\" or \"poison outage\". During periods of steady state operation, at a constant neutron flux level, the xenon-135 concentration builds up to its equilibrium value for that reactor power in about 40 to 50 hours. When the reactor power is increased, xenon-135 concentration initially decreases because the burn up is increased at the new, higher power level. Thus, the dynamics of xenon poisoning are important for the stability of the flux pattern and geometrical power distribution, especially in physically large reactors.\n\nBecause 95% of the xenon-135 production is from iodine-135 decay, which has a 6- to 7-hour half-life, the production of xenon-135 remains constant; at this point, the xenon-135 concentration reaches a minimum. The concentration then increases to the equilibrium for the new power level in the same time, roughly 40 to 50 hours. The magnitude and the rate of change of concentration during the initial 4 to 6 hour period following the power change is dependent upon the initial power level and on the amount of change in power level; the xenon-135 concentration change is greater for a larger change in power level. When reactor power is decreased, the process is reversed.\n\nBecause samarium-149 is not radioactive and is not removed by decay, it presents problems somewhat different from those encountered with xenon-135. The equilibrium concentration (and thus the poisoning effect) builds to an equilibrium value during reactor operation in about 500 hours (about three weeks), and since samarium-149 is stable, the concentration remains essentially constant during reactor operation. Another problematic isotope that builds up is gadolinium-157, with microscopic cross-section of σ = 200,000 b.\n\nThere are numerous other fission products that, as a result of their concentration and thermal neutron absorption cross section, have a poisoning effect on reactor operation. Individually, they are of little consequence, but taken together they have a significant effect. These are often characterized as \"lumped fission product poisons\" and accumulate at an average rate of 50 barns per fission event in the reactor. The buildup of fission product poisons in the fuel eventually leads to loss of efficiency, and in some cases to instability. In practice, buildup of reactor poisons in nuclear fuel is what determines the lifetime of nuclear fuel in a reactor: long before all possible fissions have taken place, buildup of long-lived neutron-absorbing fission products damps out the chain reaction. This is the reason that nuclear reprocessing is a useful activity: solid spent nuclear fuel contains about 97% of the original fissionable material present in newly manufactured nuclear fuel. Chemical separation of the fission products restores the fuel so that it can be used again.\n\nOther potential approaches to fission product removal include solid but porous fuel which allows escape of fission products and liquid or gaseous fuel (molten salt reactor, aqueous homogeneous reactor). These ease the problem of fission product accumulation in the fuel, but pose the additional problem of safely removing and storing the fission products.\n\nOther fission products with relatively high absorption cross sections include Kr, Mo, Nd, Pm. Above this mass, even many even-mass number isotopes have large absorption cross sections, allowing one nucleus to serially absorb multiple neutrons.\nFission of heavier actinides produces more of the heavier fission products in the lanthanide range, so the total neutron absorption cross section of fission products is higher.\n\nIn a fast reactor the fission product poison situation may differ significantly because neutron absorption cross sections can differ for thermal neutrons and fast neutrons. In the RBEC-M Lead-Bismuth Cooled Fast Reactor, the fission products with neutron capture more than 5% of total fission products capture are, in order, Cs, Ru, Rh, Tc, Pd and Pd in the core, with Sm replacing Pd for 6th place in the breeding blanket.\n\nIn addition to fission product poisons, other materials in the reactor decay to materials that act as neutron poisons. An example of this is the decay of tritium to helium-3. Since tritium has a half-life of 12.3 years, normally this decay does not significantly affect reactor operations because the rate of decay of tritium is so slow. However, if tritium is produced in a reactor and then allowed to remain in the reactor during a prolonged shutdown of several months, a sufficient amount of tritium may decay to helium-3 to add a significant amount of negative reactivity. Any helium-3 produced in the reactor during a shutdown period will be removed during subsequent operation by a neutron-proton reaction.\n\nDuring operation of a reactor the amount of fuel contained in the core decreases monotonically. If the reactor is to operate for a long period of time, fuel in excess of that needed for exact criticality must be added when the reactor is fueled. The positive reactivity due to the excess fuel must be balanced with negative reactivity from neutron-absorbing material. Movable control rods containing neutron-absorbing material is one method, but control rods alone to balance the excess reactivity may be impractical for a particular core design as there may be insufficient room for the rods or their mechanisms, namely in submarines, where space is particularly at a premium.\n\nTo control large amounts of excess fuel reactivity without control rods, burnable poisons are loaded into the core. Burnable poisons are materials that have a high neutron absorption cross section that are converted into materials of relatively low absorption cross section as the result of neutron absorption. Due to the burn-up of the poison material, the negative reactivity of the burnable poison decreases over core life. Ideally, these poisons should decrease their negative reactivity at the same rate that the fuel's excess positive reactivity is depleted. Fixed burnable poisons are generally used in the form of compounds of boron or gadolinium that are shaped into separate lattice pins or plates, or introduced as additives to the fuel. Since they can usually be distributed more uniformly than control rods, these poisons are less disruptive to the core's power distribution. Fixed burnable poisons may also be discretely loaded in specific locations in the core in order to shape or control flux profiles to prevent excessive flux and power peaking near certain regions of the reactor. Current practice however is to use fixed non-burnable poisons in this service.\n\nA non-burnable poison is one that maintains a constant negative reactivity worth over the life of the core. While no neutron poison is strictly non-burnable, certain materials can be treated as non-burnable poisons under certain conditions. One example is hafnium. The removal (by absorption of neutrons) of one isotope of hafnium leads to the production of another neutron absorber, and continues through a chain of five absorbers. This absorption chain results in a long-lived burnable poison which approximates non-burnable characteristics.\n\nSoluble poisons, also called chemical shim, produce a spatially uniform neutron absorption when dissolved in the water coolant. The most common soluble poison in commercial pressurized water reactors (PWR) is boric acid, which is often referred to as soluble boron. The boric acid in the coolant decreases the thermal utilization factor, causing a decrease in reactivity. By varying the concentration of boric acid in the coolant, a process referred to as boration and dilution, the reactivity of the core can be easily varied. If the boron concentration is increased, the coolant/moderator absorbs more neutrons, adding negative reactivity. If the boron concentration is reduced (dilution), positive reactivity is added. The changing of boron concentration in a PWR is a slow process and is used primarily to compensate for fuel burnout or poison buildup. The variation in boron concentration allows control rod use to be minimized, which results in a flatter flux profile over the core than can be produced by rod insertion. The flatter flux profile occurs because there are no regions of depressed flux like those that would be produced in the vicinity of inserted control rods. This system is not in widespread use because the chemicals make the moderator temperature reactivity coefficient less negative. All commercial PWR types operating in the US (Westinghouse, Combustion Engineering, and Babcock & Wilcox) employ soluble boron to control excess reactivity. US Navy reactors and Boiling Water Reactors do not.\n\nSoluble poisons are also used in emergency shutdown systems. During SCRAM the operators can inject solutions containing neutron poisons directly into the reactor coolant. Various solutions, including sodium polyborate and gadolinium nitrate (Gd(NO)·HO), are used.\n"}
{"id": "27264213", "url": "https://en.wikipedia.org/wiki?curid=27264213", "title": "Organic light-emitting transistor", "text": "Organic light-emitting transistor\n\nAn organic light-emitting transistor (OLET) is a form of transistor that emits light. These transistors have potential for digital displays and on-chip optical interconnects. OLET is a new light-emission concept, providing planar light sources that can be easily integrated in substrates like silicon, glass, and paper using standard microelectronic techniques.\n\nOLETs differ from OLEDs in that an active matrix can be made entirely of OLETs, whereas OLEDs must be combined with switching elements such as TFTs.\n\n"}
{"id": "15779952", "url": "https://en.wikipedia.org/wiki?curid=15779952", "title": "PIN pad", "text": "PIN pad\n\nA PIN pad or PIN entry device is an electronic device used in a debit, credit or smart card-based transaction to accept and encrypt the cardholder's personal identification number (PIN). PIN pads are normally used with automated teller machines, payment terminals or integrated point of sale devices in which an electronic cash register is responsible for taking the sale amount and initiating/handling the transaction. The PIN pad is required to read the card and allow the PIN to be securely entered and encrypted before it is sent to the bank. In some cases, with chip cards, the PIN is only transferred from the PIN pad to card and it is verified by the chip card. In this case the PIN does not need to be sent to the bank or card scheme for verification. (This is known as \"offline PIN verification\".)\n\nLike some stand-alone point of sale devices, PIN pads are equipped with hardware and software security features to ensure that the encryption keys and the PIN are erased if someone tries to tamper with the device. The PIN is encrypted immediately on entry and an encrypted PIN block is created. This encrypted PIN block is erased as soon as it has been sent from the PIN pad to the attached point of sale device and/or the chip card. PINs are encrypted using a variety of encryption schemes, the most common in 2010 being triple DES.\n\nPIN pads must be approved to the standards required by the payment card industry to ensure that they provide adequate security at the point of PIN entry and for the PIN encryption process. ISO 9564 is the international standard for PIN management and security, and specifies some required and recommended characteristics of PIN entry devices.\n\nAlthough PIN pads nominally allow entry of numeric values, some PIN pads also have letters assigned to most of the digits, to allow use of alphabetic characters or a words as a mnemonic for the numeric PIN. Not all PIN pads necessarily have the same letters for the same numbers. ISO 9564 does not mandate any particular assignment of letters, and includes two examples that differ in the digits to which Q and Z are assigned.\n\n"}
{"id": "6772408", "url": "https://en.wikipedia.org/wiki?curid=6772408", "title": "Parsé Semiconductor Co.", "text": "Parsé Semiconductor Co.\n\nParsé Semiconductor Co. was established in 2003 in Tehran, Iran, is a digital design house for ASIC, SoC and FPGA designs. The company in 2006 announced it has both designed and produced a 32 bit computer microprocessor inside the country for the first time.\n\nThe computer microprocessor called Aristo has been manufactured by Iranian researchers and engineers at Parsé Semiconductor with the support the company has received from the Modern Industries Center of the ministry of Industries of Iran. In addition Parsé has released its own chip called Tachra, which includes the Aristo processor core, together with a suite of Tachra development tools. These architectures seem to have much in common with Leon3.\n\nDesigned and manufactured in conformity with SPARC processors architecture, Aristo stands to the international standards and can well compete with similar processors existing in the market. The newly Iran-made computer microprocessor can be used in communications projects, auto-manufacturing industry, industrial automation, robotic systems and artificial intelligence, computer and data transfer networks, etc.\n\n\n"}
{"id": "25716", "url": "https://en.wikipedia.org/wiki?curid=25716", "title": "Refreshable braille display", "text": "Refreshable braille display\n\nA refreshable braille display or braille terminal is an electro-mechanical device for displaying braille characters, usually by means of round-tipped pins raised through holes in a flat surface. Visually impaired computer users who cannot use a computer monitor can use it to read text output. Speech synthesizers are also commonly used for the same task, and a blind user may switch between the two systems or use both at the same time depending on circumstances. Deafblind computer users may also use refreshable braille displays.\n\nThe base of a refreshable braille display often integrates a pure braille keyboard. Similar to the Perkins Brailler, the input is performed by two sets of four keys on each side, while output is via a refreshable braille display consisting of a row of electro-mechanical character cells, each of which can raise or lower a combination of eight round-tipped pins. Other variants exist that use a conventional QWERTY keyboard for input and braille pins for output, as well as input-only and output-only devices.\n\nOn some models the position of the cursor is represented by vibrating the dots, and some models have a switch associated with each cell to move the cursor to that cell directly.\n\nThe mechanism which raises the dots uses the piezo effect of some crystals, whereby they expand when a voltage is applied to them. Such a crystal is connected to a lever, which in turn raises the dot. There has to be a crystal for each dot of the display, i.e. eight per character.\n\nBecause of the complexity of producing a reliable display that will cope with daily wear and tear, these displays are expensive. Usually, only 40 or 80 braille cells are displayed. Models with between 18 and 40 cells exist in some notetaker devices.\n\nThe software that controls the display is called a screen reader. It gathers the content of the screen from the operating system, converts it into braille characters and sends it to the display. Screen readers for graphical operating systems are especially complex, because graphical elements like windows or slidebars have to be interpreted and described in text form. Modern operating systems usually have an Application Programming Interface to help screen readers obtain this information, such as UI Automation (UIA) for Microsoft Windows, VoiceOver for OS X and iOS, and AT-SPI for GNOME.\n\n\n\n"}
{"id": "58342882", "url": "https://en.wikipedia.org/wiki?curid=58342882", "title": "Rodent farming", "text": "Rodent farming\n\nRodent farming is an agricultural process in which rodents are bred and raised with the intent of selling them for their meat. They are often categorised in a sub-category of livestock known as micro-livestock, due to their small size. Rodents have been used as food in a wide range of cultures, including Hawaiian, Vietnamese, French, Indian and Thai.\n\nRodent farming has been suggested as a solution to the world's increased requirements for food associated with an increasing population as a result of a number of perceived benefits with their production and consumption.\n\nRodents have been hunted and farmed in a number of cultures. The polynesian rat was hunted and consumed by the common people in pre-contact Hawaii. Capybaras, agoutis, and guinea pigs have historically been eaten in South America -- guinea pigs were farmed as far back in 2500 BCE in what is now Peru. Cane rats can grow up to 60 cm in length and weigh up to 10 kg and are hunted as bush meat in western and central Africa. Rats were commonly eaten during the Tang Dynasty in China; they may have been domesticated as they were called “household deer”. The Mishmi people in the Lohit district in India traditionally hunted rats. Dishes with rats captured in wine cellars are described in \"Larousse Gastronomique\" and rats are eaten in rural Thailand.\n\nIn the contemporary era, rodent farming has been suggested as a sustainable agriculture method to address current global malnutrition and to meet the needs of the world's growing population. Rodent farming can be economically efficient, since they can produce a large number of offspring per year, have a limited gestation period, and have a high feed conversion ratio. They require little space, so could be farmed in urban areas.\n\nEfforts have been made to develop rat farming among the dalit in the Indian state of Bihar; one obstacle to this is that the animal vehicle of the god Ganesh is a rat.\n\nIn Cameroon, cane rats is encouraged in economic development efforts.\n\nIn Australia, a rat farm that provides food for zoos and pet stores was the subject of a 2018 profile.\n\nRodents can be kept in sheds or cages, and fed grain, pellets, or scraps. In nations with strict animal cruelty regulations, such as Australia, the animals must be killed humanely, for example by gassing with carbon dioxide. In nations without these regulations, it is more common for the animals to be killed by drowning or bludgeoning.\n"}
{"id": "3650827", "url": "https://en.wikipedia.org/wiki?curid=3650827", "title": "Roll moment", "text": "Roll moment\n\nIn a vehicle suspension, roll moment is the moment of inertia of the vehicle's sprung mass (the portion of its weight supported by the suspension). The roll moment is the product of the sprung mass and the square of the distance between the vehicle's roll center and its center of mass. If the vehicle is subjected to centrifugal forces, such as in a turn, the roll moment will cause the body to rotate (lean) towards the outside of the turn.\n\nIn aeronautics, the roll moment is the aerodynamic force applied at a distance from an aircraft's center of mass that causes the aircraft to undergo angular acceleration about its roll axis. The roll axis is usually defined as the longitudinal axis, which runs from the nose to the tail of the aircraft. A roll moment can be the result of wind gusts, control surfaces such as ailerons, or simply by flying at an angle of sideslip. See flight dynamics.\n"}
{"id": "94289", "url": "https://en.wikipedia.org/wiki?curid=94289", "title": "Sally Ride", "text": "Sally Ride\n\nSally Kristen Ride (May 26, 1951 – July 23, 2012) was an American astronaut, physicist, and engineer. Born in Los Angeles, she joined NASA in 1978 and became the first American woman in space in 1983. Ride was the third woman in space overall, after USSR cosmonauts Valentina Tereshkova (1963) and Svetlana Savitskaya (1982). Ride remains the youngest American astronaut to have traveled to space, having done so at the age of 32. After flying twice on the Orbiter \"Challenger\", she left NASA in 1987. She worked for two years at Stanford University's Center for International Security and Arms Control, then at the University of California, San Diego as a professor of physics, primarily researching nonlinear optics and Thomson scattering. She served on the committees that investigated the \"Challenger\" and \"Columbia\" space shuttle disasters, the only person to participate in both. Ride died of pancreatic cancer on July 23, 2012.\n\nThe elder child of Dale Burdell Ride and Carol Joyce Ride (née Anderson), Ride was born in Los Angeles. She had one sibling, Karen \"Bear\" Ride, who is a Presbyterian minister. Both parents were elders in the Presbyterian Church. Ride's mother had worked as a volunteer counselor at a women's correctional facility. Her father had been a political science professor at Santa Monica College.\n\nRide attended Portola Junior High (now Portola Middle School) and then Birmingham High School before graduating from the private Westlake School for Girls in Los Angeles on a scholarship. In addition to being interested in science, she was a nationally ranked tennis player. Ride attended Swarthmore College for three semesters, took physics courses at University of California, Los Angeles, and then entered Stanford University as a junior, graduating with a bachelor's degree in English and physics. At Stanford, she earned a master's degree in 1975 and a PhD in physics in 1978 while doing research on the interaction of X-rays with the interstellar medium. Astrophysics and free electron lasers were her specific areas of study.\n\nRide was one of 8,000 people who answered an advertisement in the Stanford student newspaper seeking applicants for the space program. She was chosen to join NASA in 1978. During her career, Ride served as the ground-based capsule communicator (CapCom) for the second and third space shuttle flights (STS-2 and STS-3) and helped develop the space shuttle's \"Canadarm\" robot arm.\n\nPrior to her first space flight, she was subject to media attention due to her gender. During a press conference, she was asked questions such as, \"Will the flight affect your reproductive organs?\" and \"Do you weep when things go wrong on the job?\" Despite this and the historical significance of the mission, Ride insisted that she saw herself in only one way—as an astronaut. On June 18, 1983, she became the first American woman in space as a crew member on space shuttle \"Challenger\" for STS-7. She was preceded by two Soviet women, Valentina Tereshkova in 1963 and Svetlana Savitskaya in 1982. The five-person crew of the STS-7 mission deployed two communications satellites and conducted pharmaceutical experiments. Ride was the first woman to use the robot arm in space and the first to use the arm to retrieve a satellite.\n\nHer second space flight was STS-41-G in 1984, also on board \"Challenger\". She spent a total of more than 343 hours in space. Ride had completed eight months of training for her third flight (STS-61-M, a TDRS deployment mission) when the space shuttle \"Challenger\" disaster occurred. She was named to the Rogers Commission (the presidential commission investigating the accident) and headed its subcommittee on operations. She was the only person to serve on both of the panels investigating shuttle accidents (those for the \"Challenger\" accident and later the \"Columbia\" disaster). Following the Challenger investigation, Ride was assigned to NASA headquarters in Washington, D.C., where she led NASA's first strategic planning effort, authored a report titled \"NASA Leadership and America's Future in Space\" and founded NASA's Office of Exploration. After Sally Ride's death in 2012, General Donald Kutyna revealed that she had discreetly provided him with key information about O-rings (namely, that they become stiff at low temperatures) that eventually led to identification of the cause of the explosion.\n\nIn 1987, Ride left her position in Washington, D.C., to work at the Stanford University Center for International Security and Arms Control. In 1989, she became a professor of physics at the University of California, San Diego, and director of the California Space Institute. From the mid-1990s until her death, Ride led two public-outreach programs for NASA—the ISS EarthKAM and GRAIL MoonKAM projects, in cooperation with NASA's Jet Propulsion Laboratory and UCSD. The programs allowed middle school students to request images of the Earth and moon. In 1999, she acted in the season 5 finale of \"Touched by an Angel\", titled \"Godspeed\". In 2003, she was asked to serve on the Columbia Accident Investigation Board. She was the president and CEO of Sally Ride Science, a company she co-founded in 2001 that creates entertaining science programs and publications for upper elementary and middle school students, with a particular focus on girls.\n\nAccording to Roger Boisjoly, who was the engineer that warned of the technical problems that led to the \"Challenger\" disaster, after the entire workforce of Morton-Thiokol shunned him Ride was the only public figure to show support for him when he went public with his pre-disaster warnings. Sally Ride hugged him publicly to show her support for his efforts.\n\nRide wrote or co-wrote seven books on space aimed at children, with the goal of encouraging children to study science.\n\nRide endorsed Barack Obama for U.S. President in 2008. She was a member of the Review of United States Human Space Flight Plans Committee, an independent review requested by the Office of Science and Technology Policy (OSTP) on May 7, 2009.\n\nRide was extremely private about her personal life. In 1982, she married fellow NASA astronaut Steve Hawley. They divorced in 1987.\n\nAfter Ride's death, her obituary revealed that her partner of 27 years was Tam O'Shaughnessy, a professor emerita of school psychology at San Diego State University and childhood friend, who met her when both were aspiring tennis players. O'Shaughnessy was also a science writer and, later, the co-founder of Sally Ride Science. O'Shaughnessy now serves as the Chief Executive Officer and Chair of the Board of Sally Ride Science. They wrote six acclaimed children's science books together. Their relationship was revealed by the company and confirmed by her sister, who said she chose to keep her personal life private, including her sickness and treatments. She is the first known LGBT astronaut.\n\nRide died on July 23, 2012, at the age of 61, in her home in La Jolla, California, seventeen months after being diagnosed with pancreatic cancer. Following cremation, her ashes were interred next to her father at Woodlawn Memorial Cemetery, Santa Monica.\n\nRide received numerous awards throughout her lifetime and after. She received the National Space Society's von Braun Award, the Lindbergh Eagle, and the NCAA's Theodore Roosevelt Award. She was inducted into the National Women's Hall of Fame and the Astronaut Hall of Fame and was awarded the NASA Space Flight Medal twice. Two elementary schools in the United States are named after her: Sally Ride Elementary School in The Woodlands, Texas, and Sally Ride Elementary School in Germantown, Maryland.\n\nIn 1994, Ride received the Samuel S. Beard Award for Greatest Public Service by an Individual 35 Years or Under, an award given out annually by Jefferson Awards.\n\nOn December 6, 2006, California Governor Arnold Schwarzenegger and First Lady Maria Shriver inducted Ride into the California Hall of Fame at the California Museum for History, Women, and the Arts.\n\nIn 2007, she was inducted into the National Aviation Hall of Fame in Dayton, Ohio.\n\nRide directed public outreach and educational programs for NASA's GRAIL mission, which sent twin satellites to map the moon’s gravity. On December 17, 2012, the two GRAIL probes, Ebb and Flow, were directed to complete their mission by crashing on an unnamed lunar mountain near the crater Goldschmidt. NASA announced that it was naming the landing site in honor of Sally Ride. Also in December 2012, the Space Foundation bestowed upon Ride its highest honor, the General James E. Hill Lifetime Space Achievement Award.\n\nIn April 2013, the U.S. Navy announced that a research ship would be named in honor of Ride. This was done in 2014 with the christening of the oceanographic research vessel RV \"Sally Ride\" (AGOR-28).\n\nOn May 20, 2013, a \"National Tribute to Sally Ride\" was held at the John F. Kennedy Center for the Performing Arts in Washington, D.C. and on that same day, President Barack Obama announced that Ride would receive the Presidential Medal of Freedom, the highest civilian award in the United States. The medal was presented to her life partner Tam O'Shaughnessy in a ceremony at the White House on November 20, 2013. In July 2013, \"Flying magazine\" ranked Ride at number 50 on their list of the \"51 Heroes of Aviation\".\n\nIn 2014, Ride was inducted into the Legacy Walk, an outdoor public display that celebrates LGBT history and people.\n\nIn 2017, a Google Doodle honored her on International Women's Day.\n\nThe U.S. Postal Service issued a first-class postage stamp honoring Ride in 2018.\n\nIn 2013, Janelle Monáe released a song called \"Sally Ride\".\n\nAlso in 2013, astronauts Chris Hadfield and Catherine Coleman performed a song called \"Ride On\".\n\nIn 2017, a \"Women of NASA\" LEGO set went on sale featuring (among other things) mini-figurines of Ride, Margaret Hamilton, Mae Jemison, and Nancy Grace Roman.\n\n\n"}
{"id": "3085284", "url": "https://en.wikipedia.org/wiki?curid=3085284", "title": "Salver", "text": "Salver\n\nA salver is a flat tray of silver, other metal or glass used for carrying or serving glasses, cups, and dishes at a table, or for the presenting of a letter or card by a servant. In a royal or noble household the fear of poisoning led to the custom of tasting the food or drink before it was served to the master and his guests; this was known as the assay of meat and drink, and in Spanish was called \"salva\". The verb \"salvar\" means to preserve from risk, from the Latin \"salvare\", to save. The term \"salva\" was also applied to the dish or tray on which the food or drink was presented after the tasting process. There seems no doubt that this Spanish word is the source of the English salver; a parallel is found in the origin of the term \"credenza\", which comes from Italian.\n\nCeremonial salvers have also been used as major sporting trophies, most notably a sterling silver salver as the Ladies' Singles trophy in the Wimbledon tennis championships since 1886, and, from 1978 onwards, for the runner-up at the Masters Tournament (golf).\n"}
{"id": "33892798", "url": "https://en.wikipedia.org/wiki?curid=33892798", "title": "Simmtronics", "text": "Simmtronics\n\nSimmtronics Semiconductor Ltd. is an Indian privately held, multinational computer technology company that develops, manufactures, \nsells and supports - Memory Module, Motherboard, \nTablet PC, \nSimmbook, \nLow cost PC \nand other computer related parts.\n\nSimmtronics also processes, tests and resells under its label hard disk drives \noriginally manufactured by suppliers such as Western Digital, Seagate, etc.acompany was founded in Delhi, India \nin 1992 by Indrajit Sabharwal, Managing Director. Simmtronics is widely known for its Memory Module.\n\nThe headquarters is in Delhi, India, and has 4 manufacturing facilities in Roorkee, Bhiwadi (India), Singapore and Dubai,(U.A.E.),\n\nSimmtronics have a sales and subsidiary offices in Algeria, France, Mauritius, Macedonia, Nepal, Singapore, Sri Lanka, Thailand, U.K., U.S.A, Vietnam and U.A.E.\n\nIn 1992, the firm was founded by Indrajit Sabharwal, Managing Director, Simmtronics.\nIn December 2010, VIA Technologies tie-up with Simmtronics as Exclusive Manufacturing and Distribution Partner for 'VIA pc-1' Mainboards in India and covers 15 other countries in SAARC, Middle East and Africa\n\nIn July 2012, Simmtronics officially launched its tablets for the Middle East market in Dubai and claimed it is the world’s most affordable tablet.\n\nThe company's products include:\n\n\n"}
{"id": "27897341", "url": "https://en.wikipedia.org/wiki?curid=27897341", "title": "Spiral antenna", "text": "Spiral antenna\n\nIn microwave systems, a spiral antenna is a type of RF antenna. It is shaped as a two-arm spiral, or more arms may be used. Spiral antennas were first described in 1956. Spiral antennas belong to the class of frequency independent antennas which operate over a wide range of frequencies. Polarization, radiation pattern and impedance of such antennas remain unchanged over large bandwidth. Such antennas are inherently circularly polarized with low gain. Array of spiral antennas can be used to increase the gain. Spiral antennas are reduced size antennas with its windings making it an extremely small structure. Lossy cavities are usually placed at the back to eliminate back lobes because a unidirectional pattern is usually preferred in such antennas. Spiral antennas are classified into different types; archimedean spiral, square spiral and star spiral etc. Archimedean spiral is the most popular configuration.\n\nThese antennas operate in 3 ways: traveling wave, fast wave, and leaky wave.\n\nThe traveling wave, formed on spiral arms, allows for broadband performance. Fast wave is due to mutual coupling phenomenon occurring between arms of spiral. Leaky wave “leaks” the energy during propagation through the spiral arms to produce radiation.\n\nRing theory (band theory) explains the working principle of spiral antenna. The theory states that spiral antenna radiates from an \"active region\" where the circumference of spiral equals the wavelength.\n\nDifferent design parameters are to be considered while designing a square spiral antenna. The parameters include spacing between the turns formula_1, width of arm formula_2, inner radius formula_3 and outer radius formula_4. The inner radius is measured from center of the spiral to center of the first turn while the outer radius is measured from center of the spiral to center of the outermost turn. Other than these design parameters, spiral antennas have lowest (formula_5 and highest formula_6 operating frequencies. Here formula_7 corresponds to speed of light. In an formula_8 coordinate system, the spiral grows along the formula_9-axis and formula_10-axis simultaneously. All spirals satisfy formula_11 equation where formula_12 corresponds to growth factor and formula_13 corresponds to multiplication factor. \n\nDifferent designs of spiral antenna can be obtained by varying number of turns it contains, the spacing between its turns and the width of its arm. A dielectric medium is used with a specific permittivity and dimensions over which the spiral is printed. Dielectric mediums like Rogers RT Duroid help in reducing the physical size of antenna. Thin substrates with higher permittivity can achieve the same result as thick substrates with lower permittivity. The only problem with such materials is their less availability and high costs.\n\nA spiral antenna transmits EM waves having a circular polarization. It will receive linearly polarized EM waves in any orientation, but will attenuate signals received with the opposite circular polarization. A spiral antenna will reject circularly polarized waves of one type, while receiving perfectly well waves having the other polarization.\n\nOne application of spiral antennas is wideband communications. Another application of spiral antennas is monitoring of the frequency spectrum. One antenna can receive over a wide bandwidth, for example a ratio 5:1 between the maximum and minimum frequency. Usually a pair of spiral antennas are used in this application, having identical parameters except the polarization, which is opposite (one is right-hand, the other left-hand oriented). Spiral antennas are useful for microwave direction-finding.\n\nThe antenna includes two conductive spirals or arms, extending from the center outwards. The antenna may be a flat disc, with conductors resembling a pair of loosely nested clock springs, or the spirals may extend in a three-dimensional shape like a screw thread. The direction of rotation of the spiral defines the direction of antenna polarization. Additional spirals may be included as well, to form a multi-spiral structure. Usually the spiral is cavity-backed, that is there is a cavity of air or non-conductive material or vacuum, surrounded by conductive walls; the cavity changes the antenna pattern to a unidirectional shape. The output of the antenna is a balanced line. If one input or output line is desired, for example a coaxial line, then a balun or other device is added to so transform the signals.\n\n"}
{"id": "13721526", "url": "https://en.wikipedia.org/wiki?curid=13721526", "title": "Standalone program", "text": "Standalone program\n\nA stand-alone program, also known as a freestanding program, is a computer program that does not load any external module, library function or program and that is designed to boot with the bootstrap procedure of the target processor – it runs on bare metal. In early computers like the ENIAC without the concept of an operating system, standalone programs were the only way to run a computer. Standalone programs are usually written in or compiled to the assembly language for the specific hardware.\n\nLater standalone programs typically were provided for utility functions such as disk formatting. Also, computers with very limited memory used standalone programs, i.e. most computers until the mid-1950s, and later still embedded processors.\n\nStandalone programs are now mainly limited to SoC's or Microcontrollers (where battery life, price, and data space are at premiums) and critical systems. In extreme cases every possible set of inputs and errors must be tested and thus every potential output known; fully independent [separate physical suppliers and programing teams] yet fully parallel system-state monitoring; or where the attack surface must be minimized; an operating system would add unacceptable complexity and uncertainty.\n(examples, industrial operator safety interrupts, commercial airlines, medical devices, and ballistic missile launch controls, lithium-battery charge controllers in consumer devices[fire hazard and chip cost of approx ten cents].) \nResource limited microcontrollers can also be made more tolerant of varied environmental conditions than the more powerful hardware needed for an operating system; this is possible because the much lower clock frequency, pin spacing, lack of large data buses (e.g. ddr4 ram modules), and limited transistor count allow for wider design margins and thus the potential for more robust electrical and physical properties both in circuit layout and material choices.\n\n"}
{"id": "1250809", "url": "https://en.wikipedia.org/wiki?curid=1250809", "title": "Sweet sorghum", "text": "Sweet sorghum\n\nSweet sorghum is any of the many varieties of the sorghum grass whose stalks have a high sugar content. Sweet sorghum thrives better under drier and warmer conditions than many other crops and is grown primarily for forage, silage, and syrup production. Although, in most of the United States the term \"molasses\" refers to a sweet syrup, made as a byproduct of sugarcane or sugar beet sugar extraction, sweet sorghum syrup is known as \"sorghum molasses\" in some regions of the U.S.\n\nSweet sorghum has been widely cultivated in the U.S. since the 1850s for use in sweeteners, primarily in the form of sorghum syrup. By the early 1900s, the U.S. produced of sweet sorghum syrup annually. Making syrup from sorghum (as from sugar cane) is heavily labor-intensive. Following World War II, with the declining availability of farm labor, sorghum syrup production fell drastically. Currently, less than are produced annually in the U.S.\n\nIn Central India it was introduced in the early 1970s by Nimbkar Agricultural Research Institute. Presently it is grown on large area as a fodder crop.\n\nMost sorghum grown for syrup production is grown in Alabama, Arkansas, Georgia, Iowa, Kentucky, Mississippi, North Carolina, and Tennessee.\n\nSorghum syrup and hot biscuits are a traditional breakfast in the Southern United States. Sorghum syrup is also used on pancakes, cornmeal mush, grits and other hot cereals. It can be used as a cooking ingredient with a similar sweetening effect as molasses, though blackstrap molasses still has a higher nutritional value than sorghum syrup in most regards. In India sweet sorghum syrup is presently being promoted as a health food.\n\nIn the U.S. since the 1950s, sorghum has been raised primarily for forage and silage, with sorghum cultivation for cattle feed concentrated in the Great Plains (Texas, Kansas, and Nebraska are the leading producers) where insufficient rainfall and high temperature make corn production unprofitable.\n\nGrain sorghum has also been used by the ethanol industry for quite some time because it yields about the same amount of ethanol per bushel as corn. As new-generation ethanol processes are studied and improved, sorghum's role may continue to expand. Texas A&M University ran trials to ascertain the best varieties for ethanol production from sorghum leaves and stalks in the USA.\n\nIn India and other places, sweet sorghum stalks are used for producing biofuel by squeezing the juice and then fermenting into ethanol. The crop is particularly suitable for growing in dryland conditions, as it only extracts one-seventh of the water used by sugarcane.\n\nA study by researchers at the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) found that growing sweet sorghum instead of grain sorghum could increase farmers' incomes by US$40 per hectare per crop because it can provide food, feed, and fuel. With grain sorghum currently grown on over 11 million ha in Asia and on 23.4 million ha in Africa, a switch to sweet sorghum could have a considerable economic impact.\n\n\n"}
