{"id": "42941598", "url": "https://en.wikipedia.org/wiki?curid=42941598", "title": "Advanced Transit Association", "text": "Advanced Transit Association\n\nThe Advanced Transit Association (ATRA) is a non-profit organisation whose purpose is to encourage the development and deployment of Automated Transit Networks, including personal rapid transit systems. ATRA was formed in 1976 and in 1988 published a report that became an essential factor in increasing the credibility of the personal rapid transit concept.\n\n\"Advanced Transit and Urban Revitalization, an International Dialogue\" was published by the ATRA at an international conference in Indianapolis in 1978.\n\nUltra Global 2getthere, Arup, LogistikCentrum, the consulting firm of Dr. Ingmar Andreasson, PRT Consulting, Podaris, DICAM, Lea+Elliott and BergerABAM.\n\nUniversity of Maryland, \nPrinceton University, \nUniversity of Bologna, \nMineta Transportation Institute, \nSouthern Illinois University\n\n"}
{"id": "9016605", "url": "https://en.wikipedia.org/wiki?curid=9016605", "title": "Aeroscope", "text": "Aeroscope\n\nAeroscope was a type of compressed air camera for making films, constructed by Polish inventor Kazimierz Prószyński in 1909 (French patent from April 10, 1909) and built in England since 1911, at first by Newman & Sinclair, and from 1912 by Cherry Kearton Limited.\n\nPatented in England in 1910 by the Polish inventor Kazimierz Prószyński, Aeroscope was the first successful hand-held operated film camera. It has been powered by compressed air pumped before filming into the camera with a simple hand pump, similar to the one we still use to pump bicycle tires. Filming with Aeroscope, a cameraman did not have to turn the crank to advance the material filming, as in all cameras of that time, so he could operate the camera with both hands, holding the camera and controlling the focus. This made it possible to film with the Aeroscope hand-held in most difficult circumstances, as well as made possible to film from the airplanes, also for the military purposes. Camera carried of 35mm film and, once pressurised, could work with no further pumping for up to 10 minutes. The Aeroscope was known for its simplicity and reliability.\n\nHundreds of light and relatively compact Aeroscope cameras were used by the British War Office for the combat cameramen on the battlefields of World War I, and by all newsreel cameramen all over world, until the late 1920s. Aeroscope has been used among others by Arthur Herbert Malins recognized by Kelly (1997, Page 60) as “the most famous of the war cinematographers” who used it at the battle of the Somme. As several of the cameramen died filming from the firing lines Aeroscope got a name of \"camera of death\".\n\nIn 1928 Prószyński built an improved version of his camera, with an air pressure meter, but the more practical spring cameras like Eyemo and later Bolex took over. However, even by the beginning of World War II, some of the improved Aeroscope cameras were in use by the British combat cameramen.\n\n\n"}
{"id": "10236490", "url": "https://en.wikipedia.org/wiki?curid=10236490", "title": "Ancient Egyptian technology", "text": "Ancient Egyptian technology\n\nAncient Egyptian technology describes devices and technologies invented or used in Ancient Egypt. The Egyptians invented and used many simple machines, such as the ramp and the lever, to aid construction processes. They used rope trusses to stiffen the beam of ships. Egyptian paper, made from papyrus, and pottery were mass-produced and exported throughout the Mediterranean basin. The wheel was used for a number of purposes, but chariots only came into use after the Second Intermediate period. The Egyptians also played an important role in developing Mediterranean maritime technology including ships and lighthouses. \nSignificant advances in ancient Egypt during the dynastic period include astronomy, mathematics, and medicine. Their geometry was a necessary outgrowth of surveying to preserve the layout and ownership of farmland, which was flooded annually by the Nile river. The 3,4,5 right triangle and other rules of thumb served to represent rectilinear structures, and the post and lintel architecture of Egypt. Egypt also was a center of alchemy research for much of the western world.\n\nThe word \"paper\" comes from the Greek term for the ancient Egyptian writing material called papyrus, which was formed from beaten strips of papyrus plants. Papyrus was produced in Egypt as early as 3000 BC, and was sold to ancient Greece and Rome. The establishment of the Library of Alexandria limited the supply of papyrus for others. According to the Roman historian Pliny (Natural History records, xiii.21), as a result of this, parchment was invented under the patronage of Eumenes II of Pergamon to build his rival library at Pergamon. However, this is a myth; parchment had been in use in Anatolia and elsewhere long before the rise of Pergamon.\n\nEgyptian hieroglyphs, a phonetic writing system, served as the basis for the Phoenician alphabet from which later alphabets, such as Hebrew, Greek and Latin were derived. With this ability, writing and record keeping, the Egyptians developed one of the —if not \"the\"— first decimal system.\n\nThe city of Alexandria retained preeminence for its records and scrolls with its library. This ancient library was damaged by fire when it fell under Roman rule, and was destroyed completely by 642 CE. With it, a vast supply of antique literature, history, and knowledge was lost.\n\nSome of the older tools used in the construction of Egyptian housing included reeds and clay. According to Lucas and Harris, “reeds were plastered with clay in order to keep out of heat and cold more effectually”. Other tools that were used were \"limestone, chiseled stones, wooden mallets, and stone hammers\". With these tools, ancient Egyptians were able to create more than just housing, but also sculptures of their gods, goddesses, pyramids, etc.\n\nMany temples from Ancient Egypt are not standing today. Some are in ruin from wear and tear, while others have been lost entirely. The Egyptian structures are among the largest constructions ever conceived and built by humans. They constitute one of the most potent and enduring symbols of Ancient Egyptian civilization. Temples and tombs built by a pharaoh famous for her projects, Hatshepsut, were massive and included many colossal statues of her. Pharaoh Tutankamun's rock-cut tomb in the Valley of the Kings was full of jewellery and antiques. In some late myths, Ptah was identified as the primordial mound and had called creation into being, he was considered the deity of craftsmen, and in particular, of stone-based crafts. Imhotep, who was included in the Egyptian pantheon, was the first documented engineer.\nIn Hellenistic Egypt, lighthouse technology was developed, the most famous example being the Lighthouse of Alexandria. Alexandria was a port for the ships that traded the goods manufactured in Egypt or imported into Egypt. A giant cantilevered hoist lifted cargo to and from ships. The lighthouse itself was designed by Sostratus of Cnidus and built in the 3rd century BC (between 285 and 247 BC) on the island of Pharos in Alexandria, Egypt, which has since become a peninsula. This lighthouse was renowned in its time and knowledge of it was never lost. A 2006 drawing of it created from the study of many references, is shown at the right.\n\nThe Nile valley has been the site of one of the most influential civilizations in the world with its architectural monuments, which include the pyramids of Giza and the Great Sphinx—among the largest and most famous buildings in the world.\nThe most famous pyramids are the Egyptian pyramids—huge structures built of brick or stone, some of which are among the largest constructions by humans. Pyramids functioned as tombs for pharaohs. In Ancient Egypt, a pyramid was referred to as \"mer\", literally \"place of ascendance.\" The Great Pyramid of Giza is the largest in Egypt and one of the largest in the world. The base is over in area. It is one of the Seven Wonders of the World, and the only one of the seven to survive into modern times. The Ancient Egyptians capped the peaks of their pyramids with gold and covered their faces with polished white limestone, although many of the stones used for the finishing purpose have fallen or been removed for use on other structures over the millennia.\n\nThe Red Pyramid of Egypt (c.26th century BC), named for the light crimson hue of its exposed granite surfaces, is the third largest of Egyptian pyramids. Menkaure's Pyramid, likely dating to the same era, was constructed of limestone and granite blocks. The Great Pyramid of Giza (c. 2580 BC) contains a huge granite sarcophagus fashioned of \"Red Aswan Granite.\" The mostly ruined Black Pyramid dating from the reign of Amenemhat III once had a polished granite pyramidion or capstone, now on display in the main hall of the Egyptian Museum in Cairo (see Dahshur). Other uses in Ancient Egypt, include columns, door lintels, sills, jambs, and wall and floor veneer.\n\nThe ancient Egyptians had some of the first monumental stone buildings (such as in Sakkara). How the Egyptians worked the solid granite is still a matter of debate. Archaeologist Patrick Hunt has postulated that the Egyptians used emery shown to have higher hardness on the Mohs scale. Regarding construction, of the various methods possibly used by builders, the lever moved and uplifted obelisks weighing more than 100 tons.\n\nObelisks were a prominent part of the architecture of the ancient Egyptians, who placed them in pairs at the entrances of various monuments and important buildings, such as temples. In 1902, Encyclopædia Britannica wrote, \"The earliest temple obelisk still in position is that of Senusret I of the XIIth Dynasty at Heliopolis (68 feet high)\". The word \"obelisk\" is of Greek rather than Egyptian origin because Herodotus, the great traveler, was the first writer to describe the objects. Twenty-nine ancient Egyptian obelisks are known to have survived, plus the \"unfinished obelisk\" being built by Hatshepsut to celebrate her sixteenth year as pharaoh. It broke while being carved out of the quarry and was abandoned when another one was begun to replace it. The broken one was found at Aswan and provides the only insight into the methods of how they were hewn. The obelisk symbolized the sky deity Ra and during the brief religious reformation of Akhenaten, was said to be a petrified ray of the Aten, the sun disk. It is hypothesized by New York University Egyptologist Patricia Blackwell Gary and \"Astronomy\" senior editor Richard Talcott that the shapes of the ancient Egyptian pyramid and Obelisk were derived from natural phenomena associated with the sun (the sun-god Ra being the Egyptians' greatest deity). It was also thought that the deity existed within the structure. The Egyptians also used pillars extensively.\n\nIt is unknown whether the Ancient Egyptians had kites, but a team led by Maureen Clemmons and Mory Gharib raised a 5,900-pound, obelisk into vertical position with a kite, a system of pulleys, and a support frame. Maureen Clemmons developed the idea that the ancient Egyptians used kites for work. Ramps have been reported as being widely used in Ancient Egypt. A ramp is an inclined plane, or a plane surface set at an angle (other than a right angle) against a horizontal surface. The inclined plane permits one to overcome a large resistance by applying a relatively small force through a longer distance than the load is to be raised. In civil engineering the slope (ratio of rise/run) is often referred to as a grade or gradient. An inclined plane is one of the commonly-recognized simple machines. Maureen Clemmons subsequently led a team of researchers demonstrating a kite made of natural material and reinforced with shellac (which according to their research pulled with 97% the efficiency of nylon), in a 9 mph wind, would easily pull an average 2-ton pyramid stone up the 1st two courses of a pyramid (in collaboration with Cal Poly, Pomona, on a 53-stone pyramid built in Rosamond, CA). \nThe Ancient Egyptians had knowledge to some extent of sail construction. This is governed by the science of aerodynamics. The earliest Egyptian sails were simply placed to catch the wind and push a vessel. Later Egyptian sails dating to 2400 BCE were built with the recognition that ships could sail against the wind using the lift of the sails. Queen Hatshepsut oversaw the preparations and funding of an expedition of five ships, each measuring seventy feet long, and \"with several sails\".Various others exist, also.\n\nAncient Egyptians had experience with building a variety of ships. Some of them survive to this day as Khufu Solar ship. The ships were found in many areas of Egypt as the Abydos boats and remnants of other ships were found near the pyramids.\n\nSneferu's ancient cedar wood ship Praise of the Two Lands is the first reference recorded to a ship being referred to by name.\n\nAlthough quarter rudders were the norm in Nile navigation, the Egyptians were the first to use also stern-mounted rudders (not of the modern type but center mounted steering oars).\n\nIrrigation as the artificial application of water to the soil was used to some extent in Ancient Egypt, a hydraulic civilization (which entails hydraulic engineering). In crop production it is mainly used to replace missing rainfall in periods of drought, as opposed to reliance on direct rainfall (referred to as dryland farming or as rainfed farming). Before technology advanced, the people of Egypt relied on the natural flow of the Nile River to tend to the crops. Although the Nile provided sufficient watering survival domesticated animals, crops, and the people of Egypt, there were times where the Nile would flood the area wreaking havoc amongst the land. There is evidence of the ancient Egyptian pharaoh Amenemhet III in the twelfth dynasty (about 1800 BCE) using the natural lake of the Fayûm as a reservoir to store surpluses of water for use during the dry seasons, as the lake swelled annually with the flooding of the Nile. Construction of drainage canals reduced the problems of major flooding from entering homes and areas of crops; but because it was a hydraulic civilization, much of the water management was controlled in a systematic way. \n\nThe earliest known glass beads from Egypt were made during the New Kingdom around 1500 BC and were produced in a variety of colors. They were made by winding molten glass around a metal bar and were highly prized as a trading commodity, especially blue beads, which were believed to have magical powers. The Egyptians made small jars and bottles using the core-formed method. Glass threads were wound around a bag of sand tied to a rod. The glass was continually reheated to fuse the threads together. The glass-covered sand bag was kept in motion until the required shape and thickness was achieved. The rod was allowed to cool, then finally the bag was punctured and the sand poured out and reused . The Egyptians also created the first colored glass rods which they used to create colorful beads and decorations. They also worked with cast glass, which was produced by pouring molten glass into a mold, much like iron and the more modern crucible steel.\n\nThe Egyptians were a practical people and this is reflected in their astronomy in contrast to Babylonia where the first astronomical texts were written in astrological terms. Even before Upper and Lower Egypt were unified in 3000 BCE, observations of the night sky had influenced the development of a religion in which many of its principal deities were heavenly bodies. In Lower Egypt, priests built circular mud-brick walls with which to make a false horizon where they could mark the position of the sun as it rose at dawn, and then with a plumb-bob note the northern or southern turning points (solstices). This allowed them to discover that the sun disc, personified as Ra, took 365 days to travel from his birthplace at the winter solstice and back to it. Meanwhile, in Upper Egypt a lunar calendar was being developed based on the behavior of the moon and the reappearance of Sirius in its heliacal rising after its annual absence of about 70 days.\n\nAfter unification, problems with trying to work with two calendars (both depending upon constant observation) led to a merged, simplified civil calendar with twelve 30-day months, three seasons of four months each, plus an extra five days, giving a 365-year day but with no way of accounting for the extra quarter day each year. Day and night were split into 24 units, each personified by a deity. A sundial found on Seti I's cenotaph with instructions for its use shows us that the daylight hours were at one time split into 10 units, with 12 hours for the night and an hour for the morning and evening twilights. However, by Seti I's time day and night were normally divided into 12 hours each, the length of which would vary according to the time of year.\n\nKey to much of this was the motion of the sun god Ra and his annual movement along the horizon at sunrise. Out of Egyptian myths such as those around Ra and the sky goddess Nut came the development of the Egyptian calendar, time keeping, and even concepts of royalty. An astronomical ceiling in the burial chamber of Ramesses VI shows the sun being born from Nut in the morning, traveling along her body during the day and being swallowed at night.\n\nDuring the Fifth Dynasty six kings built sun temples in honour of Ra. The temple complexes built by Niuserre at Abu Gurab and Userkaf at Abusir have been excavated and have astronomical alignments, and the roofs of some of the buildings could have been used by observers to view the stars, calculate the hours at night and predict the sunrise for religious festivals.\n\nClaims have been made that precession of the equinoxes was known in Ancient Egypt prior to the time of Hipparchus. This has been disputed however on the grounds that pre-Hipparchus texts do not mention precession and that \"it is only by cunning interpretation of ancient myths and images, which are ostensibly about something else, that precession can be discerned in them, aided by some pretty esoteric numerological speculation involving the 72 years that mark one degree of shift in the zodiacal system and any number of permutations by multiplication, division, and addition.\" \n\nNote however that the Egyptian observation of a slowly changing stellar alignment over a multi-year period does not necessarily mean that they understood or even cared what was going on. For instance, from the Middle Kingdom onwards they used a table with entries for each month to tell the time of night from the passing of constellations. These went in error after a few centuries because of their calendar and precession, but were copied (with scribal errors) long after they lost their practical usefulness or the possibility of understanding and use of them in the current years, rather than the years in which they were originally used.\n\nThe Edwin Smith Papyrus is one of the first medical documents still extant, and perhaps the earliest document which attempts to describe and analyze the brain: given this, it might be seen as the very beginnings of neuroscience. However, medical historians believe that ancient Egyptian pharmacology was largely ineffective. According to a paper published by Michael D. Parkins, 72% of 260 medical prescriptions in the Hearst Papyrus had no curative elements. According to Michael D. Parkins, sewage pharmacology first began in ancient Egypt and was continued through the Middle Ages, and while the use of animal dung can have curative properties, it is not without its risk. Practices such as applying cow dung to wounds, ear piercing, tattooing, and chronic ear infections were important factors in developing tetanus. Frank J. Snoek wrote that Egyptian medicine used fly specks, lizard blood, swine teeth, and other such remedies which he believes could have been harmful.\n\nMummification of the dead was not always practiced in Egypt. Once the practice began, an individual was placed at a final resting place through a set of rituals and protocol. The Egyptian funeral was a complex ceremony including various monuments, prayers, and rituals undertaken in honor of the deceased. The poor, who could not afford expensive tombs, were buried in shallow graves in the sand, and because of the arid environment they were often naturally mummified.\n\nEvidence indicates that Egyptians made use of potter's wheels in the manufacturing of pottery from as early as the 4th Dynasty. Chariots, however, are only believed to have been introduced by the invasion of the Hyksos in the Second Intermediate period; during the New Kingdom era, chariotry became central to Egypt's military.\n\nThe Egyptians developed a variety of furniture. There in the lands of ancient Egypt is the first evidence for stools, beds, and tables (such as from the tombs similar to Tutenkhamen's). Recovered Ancient Egyptian furniture includes a third millennium BC bed discovered in the Tarkhan Tomb, a c.2550 BC. gilded set from the tomb of Queen Hetepheres I, and a c. 1550 BC. stool from Thebes.\n\nSome have suggested that the Egyptians had some form of understanding electric phenomena from observing lightning and interacting with electric fish (such as \"Malapterurus electricus\") or other animals (such as electric eels). The comment about lightning appears to come from a misunderstanding of a text referring to \"high poles covered with copper plates\" to argue this but Dr. Bolko Stern has written in detail explaining why the copper covered tops of poles (which were lower than the associated pylons) do not relate to electricity or lightning, pointing out that no evidence of anything used to manipulate electricity had been found in Egypt and that this was a magical and not a technical installation.\n\nThose exploring fringe theories of ancient technology have suggested that there were electric lights used in Ancient Egypt. Engineers have constructed a working model based on their interpretation of a relief found in the Hathor temple at the Dendera Temple complex. Authors (such as Peter Krassa and Reinhard Habeck) have produced a basic theory of the device's operation. The standard explanation, however, for the \"Dendera light\", which comprises three stone reliefs (one single and a double representation) is that the depicted image represents a lotus leaf and flower from which a sacred snake is spawned in accordance with Egyptian mythological beliefs. This sacred snake sometimes is identified as the Milky Way (the snake) in the night sky (the leaf, lotus, or \"bulb\") that became identified with Hathor because of her similar association in creation.\n\nUnder Hellenistic rule, Egypt was one of the most prosperous regions of the Hellenistic civilization. The ancient Egyptian city of Rhakotis was renovated as Alexandria, which became the largest city around the Mediterranean Basin. Under Roman rule, Egypt was one of the most prosperous regions of the Roman Empire, with Alexandria being second only to ancient Rome in size.\n\nRecent scholarship suggests that the water wheel originates from Ptolemaic Egypt, where it appeared by the 3rd century BC. This is seen as an evolution of the paddle-driven water-lifting wheels that had been known in Egypt a century earlier. According to John Peter Oleson, both the compartmented wheel and the hydraulic Noria may have been invented in Egypt by the 4th century BC, with the Sakia being invented there a century later. This is supported by archeological finds at Faiyum, Egypt, where the oldest archeological evidence of a water-wheel has been found, in the form of a Sakia dating back to the 3rd century BC. A papyrus dating to the 2nd century BC also found in Faiyum mentions a water wheel used for irrigation, a 2nd-century BC fresco found at Alexandria depicts a compartmented Sakia, and the writings of Callixenus of Rhodes mention the use of a Sakia in Ptolemaic Egypt during the reign of Ptolemy IV in the late 3rd century BC.\n\nAncient Greek technology was often inspired by the need to improve weapons and tactics in war. Ancient Roman technology is a set of artifacts and customs which supported Roman civilization and made the expansion of Roman commerce and Roman military possible over nearly a thousand years.\n\nUnder Arab rule, Egypt once again became one of the most prosperous regions around the Mediterranean. The Egyptian city of Cairo was founded by the Fatimid Caliphate and served as its capital city. At the time, Cairo was second only to Baghdad, capital of the rival Abbasid Caliphate. After the fall of Baghdad, however, Cairo overtook it as the largest city in the Mediterranean region until the early modern period.\n\nInventions in medieval Islam covers the inventions developed in the medieval Islamic world, a region that extended from Al-Andalus and Africa in the west to the Indian subcontinent and Central Asia in the east. The timeline of Islamic science and engineering covers the general development of science and technology in the Islamic world.\n\n\n\n"}
{"id": "34269974", "url": "https://en.wikipedia.org/wiki?curid=34269974", "title": "Antoine Poidebard", "text": "Antoine Poidebard\n\nAntoine Poidebard (Lyon, 12 October 1878 – Beirut, 17 August 1955) was a French archaeologist and Jesuit missionary. He pioneered aerial archaeology in the Middle East.\n"}
{"id": "57944342", "url": "https://en.wikipedia.org/wiki?curid=57944342", "title": "Australasian Photo-Review", "text": "Australasian Photo-Review\n\nThe Australasian Photo-Review was an English language magazine, published for photographers by Baker & Rouse and later Kodak (Australasia), and published in Sydney, New South Wales, Australia.\n\nThe magazine was first published in 1894 as the Australian edition of the \"British Photographic Review of Reviews\", after the photographic supply company Baker & Rouse purchased the Australasian publishing rights. At this early stage of its publication, the magazine was issued as a short ten to fifteen page supplement to the British edition. In 1895 the magazine's name was changed to \"Australalasian Photographic Review\", and in 1903, the title was shortened to \"Australalasian Photo-Review\".\n\nThe first editor-in-chief of the magazine was Edwin J. Welch, who reported the declaration, within the first issue, that Australian photographic works would be reviewed with 'bluntness, perhaps, but no namby pamby'. In 1922 Eric Keast Burke became associate editor, and eventually editor, a position he held until the Review ceased publication in 1956. The Review is notable for promoting the work of Australian photographers or 'camerists', as they were referred to as in early editions, and for it's inclusion of high-grade photographic prints. In 1952, Eric Keast Burke, while still acting as editor for the publication, rediscovered the wet plate negative collection of Bernhard Otto Holtermann, showing panoramas of Sydney, and the gold-fields of Hill End and Gulgong.\n\nThe magazine has been digitised by the National Library of Australia.\n\n"}
{"id": "543002", "url": "https://en.wikipedia.org/wiki?curid=543002", "title": "Bovine somatotropin", "text": "Bovine somatotropin\n\nBovine somatotropin or bovine somatotrophin (abbreviated bST and BST), or bovine growth hormone (BGH), is a peptide hormone produced by cows' pituitary glands. Like other hormones, it is produced in small quantities and is used in regulating metabolic processes. After the biotech company Genentech discovered and patented the gene for BST in the 1970s, it became possible to synthesize the hormone using recombinant DNA technology to create recombinant bovine somatotropin (rBST), recombinant bovine growth hormone (rBGH), or artificial growth hormone. Four large pharmaceutical companies, Monsanto, American Cyanamid, Eli Lilly, and Upjohn, developed commercial rBST products and submitted them to the US Food and Drug Administration (FDA) for approval. Monsanto was the first firm to receive approval. Other countries (Mexico, Brazil, India, Russia, and at least ten others) also approved rBST for commercial use. Monsanto licensed Genentech's patent, and marketed their product as \"Posilac\". In October 2008, Monsanto sold this business, in full, to Eli Lilly and Company for $300 million plus additional consideration.\n\nrBST has not been allowed since at least 2000 on the market in Canada, or since 1990 the European Union. Australia, New Zealand, Japan, Israel and Argentina also banned the use of rBST.\n\nThe FDA, World Health Organization, and National Institutes of Health have independently stated that dairy products and meat from BST-treated cows are safe for human consumption. In the United States, public opinion led some manufacturers and retailers to market only milk that is rBST-free.\n\nA European Union report on the animal welfare effects of rBST states that its use often results in \"severe and unnecessary pain, suffering and distress\" for cows, \"associated with serious mastitis, foot disorders and some reproductive problems\".\n\nIn 1937, the administration of BST was shown to increase the milk yield in lactating cows by preventing mammary cell death in dairy cattle. Until the 1980s, use of the compound was very limited in agriculture as the sole source of the hormone was from bovine carcasses. During this time, the knowledge of the structure and function of the hormone increased. With the advent of biotechnology, one of the pioneering biotech companies, Genentech, in 1981 succeeded in cloning and patenting the gene for BST. Monsanto had been working along the same lines and struck a deal with Genentech in 1979 to license Genentech's patents and collaborate on development of a recombinant version of BST – a process on which Monsanto would invest $300 million. The two companies used genetic engineering to clone the BST gene into \"E. coli\". The bacteria are grown in bioreactors, then broken up and separated from the rBST, which is purified to produce the injectable hormone. They published their first field trial results in 1981.\n\nLilly, American Cyanamid, Upjohn, and Monsanto all submitted applications to market rBST to the U.S. FDA, and the FDA completed its review of the human safety component of these applications in 1986 and found food from rBST-treated cows to be safe; however, strong public concern led to calls for more studies, investigations, and public discussions, which included an unprecedented conference on the safety of rBST in 1990 organized by the National Institutes of Health at the request of Sen. Patrick Leahy. FDA approved Monsanto's application in 1993. Monsanto launched rBST, brand-named Posilac, in 1994.\n\nAn average dairy cow begins her lactation with a moderate daily level of milk production. This daily output increases until, at about 70 days into the lactation, production peaks. From that time until the cow is dry, production slowly decreases. This increase and decrease in production is partially caused by the count of milk-producing cells in the udder. Cell counts begin at a moderate number, increase during the first part of the lactation, then decrease as the lactation proceeds. Once lost, these cells generally do not regrow until the next lactation.\n\nAdministration of rBST or BST prior to peak production, in cows that are well fed, slows the rate at which the number of mammary cells decreases, and increases the amount of nutrients directed away from fat and toward the mammary cells, leading to an extension of peak milk production. The effects are mediated by the insulin-like growth factor (IGF) system, which is upregulated in response to BST or rBST administration in well-fed cows.\n\nFrom 2000 to 2005, the USDA National Agricultural Statistics Service survey of dairy producers found that about 17% of producers used rBST. The 2010 USDA National Agricultural Statistics Service survey of Wisconsin farms found that about 18% of dairy farms used rBST.\n\nBy using cows that produce offspring within a one to two week period, synchronized breeding allows dairy farmers to artificially inseminate cows for maximum pregnancy rates with minimal effort. Bovine somatotropin is a placental lactogen (PL) hormone and falls under the class of growth hormone, or somatotropin. It is relevant to the practice of artificially expediting placental development in farm mammals, specifically dairy cattle. The mechanism through which the hormones of this somatotropin exert their effects can be observed at the cellular and molecular level in placental tissue. For mammals exposed to the hormone, bovine somatotropin associates with N-glycosylated proteins that are heavily involved in placental gestation. Knowledge of this and how BT works in conjunction with growth hormones allows for agricultural researchers to experiment on the effects of synchronization in farm animals. Synchronization involves treating cattle with a dose of BsT before artificially inseminating them, according to a fixed interval schedule. Synchronization and bST treatment may be used to increase first-service pregnancy rates while following this timed artificial insemination protocol. This would therefore allow agricultural researchers to observe the effects of hormones in relation to natural estrous cycles. Dairy cattle experienced increased rates of pregnancy during synchronization when exposed to BsT. The effects of BsT on this treatment are dependent on the step in the estrous cycle, seeing how BsT has a stimulatory effect of progesterone on the corpus luteum.\n\nThough approved by the FDA in 1993, rBST has been immersed in controversy since the early 1980s. Part of the controversy concerns potential effects on animal health and human health.\n\nOne meta-analysis published in 2003 suggested a negative impact of rBST's effects on bovine health. Findings suggested an average increase in milk output ranging from 11%–16%, an approximate 24% increase in the risk of clinical mastitis, a 40% reduction in fertility, and 55% increased risk of developing clinical signs of lameness. The same study reported a decrease in body condition score for cows treated with rBST, though an increase in their dry matter intake occurred.\n\nAnother meta-analysis (2003) reported on body condition scores (BCS) but could not reach a conclusion due to lack of homogeneity in study design and reporting. They found a trend towards decreased BCS in treated cows but state \"Depending on the level of body condition in these cows, this effect may have been beneficial or detrimental.\" This analysis did not report on clinical mastitis.\n\nA more recent meta-analysis (2014) published by the Journal of the American Veterinary Medical Association showed no significant increase in risk of clinical mastitis nor other adverse effects on cow health and well-being. This review included 26 peer-reviewed studies that involved the use of the rBST-Zn formulation available to US producers in accordance with the label instructions for treatment initiation (57 to 70 days postpartum), dose (500 mg, q 14 d), and route (SC).\n\nMastitis has cost American dairy industries an estimated $1.5 to 2 billion per year in treating dairy cows.\nIn 1994, a European Union scientific commission was asked to report on the incidence of mastitis and other disorders in dairy cows and on other aspects of their welfare. The commission's statement, subsequently adopted by the European Union, stated that the use of rBST substantially increased health problems with cows, including foot problems, mastitis, and injection site reactions, impinged on the welfare of the animals, and caused reproductive disorders. The report concluded, on the basis of the health and welfare of the animals, rBST should not be used. Health Canada prohibited the sale of rBST in 1999; the external committees found, although there was no significant health risk to humans, the drug presents a threat to animal health, and, for this reason, cannot be sold in Canada.\n\nMonsanto-sponsored trials reviewed by the FDA asked whether the use of rBST makes cows more susceptible to mastitis. According to the FDA, which used data from eight Monsanto-sponsored trials in its decision in 1993 to approve Monsanto's rBST product, the answer is yes. The data from these eight trials, which involved 487 cows, showed that during the period of rBST treatment, mastitis incidence increased by 76% in primiparous cows and by 50% for multiparous cows. Overall, the increase was 53%.\n\nConversely, however a study from the University of Georgia on the monitoring post-approval of rBST, showed that there was no statistically significant (P>0.05) derivation in cases of clinical mastitis in dairy cows treated with rBST.\n\nThe overall composition of the milk including the fat, protein, and lactose content is not altered substantially by the use of rBST in dairy cows. The milk may have a slight change in fat content within the first few weeks of rBST treatment as the cow is allowed to adjust her metabolism and feed intake. The changes in the fat content have been shown to be temporary. The composition of the milk has been examined in more than 200 different experiments. Natural variation within milk is normal with or without rBST treatment in cows due to genetics, location, feed, age, and other environmental factors. Protein in milk content has also been studied and was shown to have no apparent change in rBST treated cows. The vitamins and minerals that are normally in milk were also unaltered in milk from rBST treated cows. Freezing point, pH, thermal properties, and other manufacturing characteristics of milk were shown to be the same regardless of whether it came from rBST treated cows or not.\n\nBST is destroyed in the digestive system and even if directly injected, has not been found to have any direct effect on humans. Researchers have found that \"IGF-1 in milk is not denatured by pasteurization and the extent to which intact, active IGF-1 is absorbed through the human digestive tract remains still however uncertain\" implicating that an extensive study on the nature of IGF-1 in relation to rBST milk is required.\n\nFDA rBST labeling guidelines state, \"FDA is concerned that the term 'rbST free' may imply a compositional difference between milk from treated and untreated cows rather than a difference in the way the milk is produced. Without proper context, such statements could be misleading. Such unqualified statements may imply that milk from untreated cows is safer or of higher quality than milk from treated cows. Such an implication would be false and misleading\".\n\nThe FDA World Health Organization, and National Institutes of Health have independently stated that dairy products and meat from rBST-treated cows are safe for human consumption. The American Cancer Society issued a report declaring, \"The evidence for potential harm to humans [from rBGH milk] is inconclusive. It is not clear that drinking milk produced using rBGH significantly increases IGF-1 levels in humans or adds to the risk of developing cancer. More research is needed to help better address these concerns.\"\n\nThe effects of rBGH on human health is an ongoing debate, in part due to the lack of conclusive evidence. A few of the most debated issues include:\n\nIGF-1 is a hormone found in humans that is responsible for growth promotion, protein synthesis, and insulin actions over the lifecycle. The hormone has been shown to influence the growth of tumors in some studies and may be linked to the development of prostate, colorectal, breast, and other cancers.\n\nIGF-1 is also found in milk. Previous research has proposed an increase of IGF-1 in rBST-treated cows, but this claim is currently not substantiated. In addition, no current evidence shows that orally consumed IGF-1 is absorbed in humans and the dietary amount is negligible when compared to what the body produces on its own. \"IGF-1 in milk is not denatured (inactivated) by pasteurization. The extent to which intact, active IGF-1 is absorbed through the human digestive tract remains uncertain.\n\nThe American Cancer Society has reviewed the evidence concerning IGF-1 in milk from rBST-treated cows, and found that: \"While there may be a link between IGF-1 blood levels and cancer, the exact nature of this link remains unclear. Some studies have shown that adults who drink milk have about 10% higher levels of IGF-1 in their blood than those who drink little or no milk. But this same finding has also been reported in people who drink soymilk. This suggests that the increase in IGF-1 may not be specific to cow's milk, and may be caused by protein, minerals, or some other factors in milk unrelated to rBGH. There have been no direct comparisons of IGF-1 levels in people who drink ordinary cow's milk vs. milk stimulated by rBGH. At this time, it is not clear that drinking milk, produced with or without rBGH treatment, increases blood IGF-1 levels into a range that might be of concern regarding cancer risk or other health effects. IGF-1 concentrations are slightly higher (to variable degrees, depending on the study) in milk from cows treated with rBGH than in untreated milk. This variability is presumed to be much less than the normal range of variation of IGF-1 in cow's milk due to natural factors, but more research is needed.\"\n\nResearch is supportive of milk supplying vital nutrients used in childhood development. As of 2014, evidence does not link rBST-treated milk with adverse health outcomes for children. Several studies have looked at the relationship between type 1 diabetes and infant feeding. Environmental triggers that may elicit an autoimmune reaction is the mechanism in which is being studied. Some studies have shown early exposure to bovine milk may predispose an infant to type 1 diabetes, whereas other studies show no causality.\n\nThe American Society of Animal Science published an article in 2014 after reviewing health issues arising from the rBST debate. The article indicated \"there are no new human health issues related to the use of rbST by the dairy industry. Use of rbST has no effect on the micro- and macrocomposition of milk. Also, no evidence exists that rbST use has increased human exposure to antibiotic residues in milk. Concerns that IGF-I present in milk could have biological effects on humans have been allayed by studies showing that oral consumption of IGF-I by humans has little or no biological activity. Additionally, concentrations of IGF-I in digestive tract fluids of humans far exceed any IGF-I consumed when drinking milk. Furthermore, chronic supplementation of cows with rbST does not increase concentrations of milk IGF-I outside the range typically observed for effects of farm, parity, or stage of lactation. Use of rbST has not affected expression of retroviruses in cattle or posed an increased risk to human health from retroviruses in cattle. Furthermore, risk for development of type 1 or type 2 diabetes has not increased in children or adults consuming milk and dairy products from rbST-supplemented cows. Overall, milk and dairy products provide essential nutrients and related benefits in health maintenance and the prevention of chronic diseases.\" \n\nKeeping in mind that bovine somatropin is a protein growth hormone, it can increase average milk yield anywhere from 10 to 15%, which in turn would lead to cows consuming substantially more nutrients in order to keep up with the increased milk production. Most of a cow's energy consumption goes directly towards milk production. In certain areas of the world, like Ethiopia where this was studied extensively, as the cows needed to intake more nutrition to balance out their milk production, there was also an increased level of chemical fertilizers and heavy metal traces found in the milk due to increased exposure to agricultural chemicals. These chemicals can then easily be passed on to humans and lead to a contaminated milk supply. BST increases the longevity or the activity of the mammary cell leading to higher milk production and some other non-desirable side effects.\n\nDespite public controversy of the use of growth hormones in food-producing animals, the number of violations has decreased substantially in the past two decades. For comparison, the level of rBST detected in bulk milk tank trucks in 2012 was one fifth the level that it was in 1996. As reported by the USDA, the US pattern for milk BTSCC declined steadily from 316.000 cells per mL of milk in 2001 to 224,000 cells/mL in 2010 and 206,000 cells/mL in 2011 (USDA, 2013). Mammary health in the animals themselves has improved over this time period as well as the overall health of the dairy herd in general. This is mostly attributed to better application methods of rBST and a better understanding of how the hormone affects the animals.\n\nOn an industry level, supplementing one million cows with rBST would result in the same amount of milk produced using 157,000 fewer cows. Farmers are, therefore, able to improve milk production with a smaller dairy population.\n\nSome studies show that rBST-treated cows reduce the impact of greenhouse gases in comparison with conventional and organic dairy operations, however, it must be noted that the lead of the project, Roger Cady, was linked to Monsanto's rBST department. Cady's study showed that excretion of nitrogen and phosphorus, two major environmental pollutants arising from animal agriculture, was reduced by 9.1 and 11.8%, respectively. Carbon dioxide is recognized to be the most important anthropogenic greenhouse gas, and livestock metabolism and fossil fuel consumption are the main sources of emissions from animal agriculture.\nWhen conventional, conventional with rBST, and organic dairy operations are compared, 8% fewer cows are needed in an rBST-supplemented population, whereas organic production systems require a 25% increase to meet production targets. This is due to a lower milk yield per cow due to the pasture-based system which is attributed with a greater maintenance energy expenditure associated with grazing behavior.\n\nIn 1997, the news division of WTVT (Channel 13), a Fox-owned station in Tampa, Florida, planned to air an investigative report by Steve Wilson and Jane Akre on the health risks associated with Monsanto's bovine growth hormone product, Posilac. Just before the story was to air, Fox received a letter from Monsanto saying the reporters were biased and that the story would damage the company. Akre stated that Wilson and she went through 83 rewrites over eight months. Negotiations broke down and both reporters were eventually fired. Wilson and Akre alleged the firing was for retaliation, while WTVT contended they were fired for insubordination. The reporters then sued Fox/WTVT in Florida state court under the state's whistleblower statute. In 2000, a Florida jury found that while no evidence showed Fox/WTVT had bowed to any pressure from Monsanto to alter the story, Akre, but not Wilson, was a whistleblower and was unjustly fired. She was awarded a $425,000 settlement. At the time of the decision, \"the station claimed it did not bend to Monsanto's letter and wanted to air a hard-hitting story with a number of statements critical of Monsanto.\" Fox appealed the decision stating that under Florida law, a whistleblower can only act if \"a law, rule, or regulation\" has been broken and argued that the FCC's policy against distortions or misrepresentations presented as news did not fit that definition. On 14 February 2003, the appeals court overturned the verdict, finding that Akre was not a whistleblower because of the Florida \"legislature's requirement that agency statements that fit the definition of a \"rule\" (must) be formally adopted (rules). Recognizing an uncodified agency policy developed through the adjudicative process as the equivalent of a formally adopted rule is not consistent with this policy, and it would expand the scope of conduct that could subject an employer to liability beyond what Florida's Legislature could have contemplated when it enacted the whistle-blower's statute.\"\n\nUse of the recombinant supplement has been controversial. The assessment of the United States FDA is that there is no significant difference between milk from treated and untreated cows. Twenty-one other countries have also approved marketing of rBST: Brazil, Chile, Colombia, Costa Rica, Ecuador, Egypt, Guatemala, Honduras, Jamaica, Lebanon, Mexico, Panama, Paraguay, Peru, Salvador, South Africa, South Korea, Uruguay and Venezuela. However, regulatory bodies in several countries, such as the Canada, Japan, Pakistan, Australia, New Zealand, and Argentina, along with the Euopean Union, rejected Monsanto's application to sell rBST because rBST increases the risk of health problems in cows, including clinical mastitis, reduced fertility, and reduced body condition. In Canada, bulk milk products from the United States that have been produced with rBST are still allowed to be sold and used in food manufacture (cheese, yogurt, etc.) due to holes in the ingredient labelling system.\n\nIn 1990, the European Union placed a moratorium on its sale by all member nations. It was turned into a permanent ban starting from 1 January 2000; the decision was based solely on veterinary concerns, laws, and treaties. An in-depth report published in 1999 analysed in detail the various human health risks associated with rBST.\n\nCanada's health board, Health Canada, refused to approve rBST for use on Canadian dairies, citing concerns over animal health. The study found the occurrence of an antibody reaction, possible hypersensitivity, in a subchronic (90-day) study of rbST oral toxicity in rats that resulted in one test animal's developing an antibody response at low dose (0.1 mg/kg/day) after 14 weeks. However, the board stated, with the exception of concerns raised regarding hypersensitivity, \"the panel finds no biologically plausible reason for concern about human safety if rBST were to be approved for sale in Canada.\"\n\nThe Codex Alimentarius Commission, a United Nations body that sets international food standards, has, as of 2017, refused to approve rBST as safe. The Codex Alimentarius does not have authority to ban or approve the hormone, but its decisions are regarded as a standard and approval by the Codex would have allowed exporting countries to challenge countries with a ban on rBST before the World Trade Organization.\n\nRecombinant bovine somatotropin is an artificially synthesized form of the bovine growth hormone. It is legal for use as an artificial cattle lactation stimulant in several countries, including the US. However, its use is prohibited in most of Europe for reasons of consumer preference, animal well-being, and skepticism of new technology. Farmers who opt to use the synthetic hormone do so because they believe it increases milk production without other inputs. However, concerns of both product safety and livestock abuse have prompted discussions for new methods in assaying the levels of dairy cattle rBST. One proposed method involves quantifying the presence or absence of the rBST-binding antibody in serum. However, this method would require blood samples to be taken regularly and is thus considered by some critics to be too invasive. The development of other alternative methods continue to undergo development. One such example, tested by W.W. Thatcher at the University of Florida, involves collecting milk samples from the cows and analyzing them directly for the presence of the rBST antibody. Dairy cattle do respond to the rBST angtigen and have shown positive results in immunosorbent trials on milk samples. Tests involving pasteurized milk samples have even shown consistent results, with immunoglobulin rBST still detectable and distinguishable despite other antibodies remaining present. Research on the viability of this new technique, as well as many others, as a means of monitoring the levels of rBST in dairy cattle continues to be an issue of practicality, profitability, and humaneness towards livestock in the dairy industry.\n\nIn 1993, the product was approved for use in the U.S. by the FDA, and its use began in 1994. The product is now sold in all 50 states.\n\nThe FDA stated that food products made from rBST-treated cows are safe for human consumption, and no statistically significant difference exists between milk derived from rBST-treated and untreated cows. The FDA found BST to be biologically inactive when consumed by humans and found no biological distinction between rBST and BST. In 1990, an independent panel convened by the National Institute of Health supported the FDA opinion that milk and meat from cows supplemented with rBST is safe for human consumption.\n\nThe FDA does not require special labels for products produced from cows given rBST, but has charged several dairies with \"misbranding\" its milk as having no hormones, because all milk contains hormones and cannot be produced in such a way that it would not contain any hormones. Monsanto sued Oakhurst Dairy of Maine over its use of a label which pledged not to use artificial growth hormones. The dairy stated that its disagreement was not over the scientific evidence for the safety of rBST (Monsanto's complaint about the label), but, \"We're in the business of marketing milk, not Monsanto's drugs.\" The suit was settled when the dairy agreed to add a qualifying statement to its label: \"FDA states: No significant difference in milk from cows treated with artificial growth hormones.\" The FDA recommends this additional labeling, but does not require it. The settlement itself caused much controversy, with anti-rBST advocates claiming that Oakhurst had capitulated in response to intimidation by a larger corporation and others claiming that Oakhurst's milk labels were in and of themselves using misleading scare tactics that deserved legal and legislative response.\n\nIn 2008, Ohio's Department of Agriculture (ODA) banned the use of labeling in dairy products as rBST-free because it was deemed misleading to consumers. However, the International Dairy Foods Association and the Organic Trade Association claimed ODA's ban was a violation of the first amendment by not allowing consumers to decide whether they deem the milk was safe and filed suit against the bill. \"The district court granted summary judgment in favor of Ohio, concluding that using \"rBST\" as a label was inherently misleading because it implies \"a compositional difference between those products that are produced with rBST and those that are not.\"\n\nIn 2009, the Kansas Legislature passed a bill that would have required dairies that did not use rBST to print disclaimers on their labels that stated, \"The Food and Drug Administration has determined there are no significant differences between milk from cows that receive injections of the artificial hormone and milk from those that do not.\" The bill was vetoed in the last days of the 2009 legislative session by then-Governor Kathleen Sebelius. The legislature removed the labeling language and passed the bill without the provision.\n\nIn 2007, Pennsylvania adopted a regulation that would have banned the practice of labeling milk as derived from cows not treated with rBST. Pennsylvania's Agriculture Secretary Dennis Wolff made the following statement in support of the measure:\n\nConsumers are getting confused with the extra labels. They deserve a choice, and so do producers. But from the standpoint of safety, all milk is healthy milk. Our milk is a safe product. The Pennsylvania Department of Agriculture is not in a position to say use rBST or not. The key word is: choice. I used rBST from day one of its approval to the last day that I milked cows. It was an important management tool on my dairy farm. What we oppose is the negative advertising or the selling of fear. If producers are asked to give up a production efficiency, and if that efficiency nets them $3000 or $10,000 a year for their dairy farm  ... That's a lot of money.\n\nThis prohibition was to go into effect 1 January 2008, but after the comment period, the guidelines were adjusted to only ban \"rBST-free\" claims and instead allow claims that farmers had pledged not to use rBST and accompany such claims with a disclaimer such as, \"No significant difference has been shown between milk derived from rbST-treated and non-rbST-treated cows.\"\n\nIn response to concerns from consumers and advocacy groups about milk from cows treated with rBST, some dairies, retailers, and restaurants have published policies on use of rBST in production of milk products they sell, while others offer some products or product lines that are labelled \"rBST-free\" or the like. Other dairies and industry groups have worked to assure the public that milk from rBST-treated cows is safe.\n\nIn reaction to these trends, in early 2008, a pro-rBST advocacy group called American Farmers for the Advancement and Conservation of Technology (AFACT), made up of dairies and originally affiliated with Monsanto, formed and began lobbying to ban such labels. AFACT stated that \"absence\" labels can be misleading and imply that milk from cows treated with rBST is inferior. The organization was dissolved in 2011.\n\nThe International Dairy Foods Association has compiled a list, last updated in 2009, of state regulations in the U.S. for referencing use of growth hormones on milk labels.\n\n\n\nOne study found a profit of $15.88 US per cow on average by using bST.\n\n"}
{"id": "1368703", "url": "https://en.wikipedia.org/wiki?curid=1368703", "title": "Bundt cake", "text": "Bundt cake\n\nA Bundt cake is a cake that is baked in a Bundt pan, shaping it into a distinctive ring shape. The shape is inspired by a traditional European cake known as , but Bundt cakes are not generally associated with any single recipe. The style of mold in North America was popularized in the 1950s and 1960s, after cookware manufacturer Nordic Ware trademarked the name \"Bundt\" and began producing Bundt pans from cast aluminum. Publicity from Pillsbury saw the cakes gain widespread popularity.\n\nThe Bundt cake derives in part from a European brioche-like cake called which was particularly popular among Jewish communities in parts of Germany, Austria, and Poland. In the southwest of Germany is traditionally known as (), a name formed by joining the two words and (cake).\n\nOpinions differ as to the significance of the word . One possibility is that it means \"bunch\" or \"bundle\", and refers to the way the dough is bundled around the tubed center of the pan. In Dutch, the cake is called \"tulband,\" which is Dutch for 'turban.' The pronunciation of the second part of this word is very similar to that of 'bundt.' Another source suggests that it describes the banded appearance given to the cake by the fluted sides of the pan, similar to a tied sheaf or bundle of wheat. Some authors have suggested that instead refers to a group of people, and that is so called because of its suitability for parties and gatherings.\n\nUses of the word \"bund\" outside of Europe to describe cakes can be found in Jewish-American cookbooks from around the start of the 20th century. The alternative spelling \"bundte\" also appears in a recipe as early as 1901.\n\nBundt cakes do not conform to any single recipe; instead, their characterizing feature is their shape. A Bundt pan generally has fluted or grooved sides, but its most defining design element is the central tube or \"chimney\" which leaves a cylindrical hole through the center of the cake. The design means that more of the mixture touches the surface of the pan than in a simple round pan, helping to provide faster and more even heat distribution during cooking. The shape is similar to that of the earlier European or . A differs from contemporary Bundt-style cakes in that it follows a particular yeast-based recipe, with fruit and nuts, and is often deeper in shape and more decorative. Also similar in shape is the Eastern European \"Babka\", dating from early 18th century Poland.\n\nToday, there is no recipe for \"Bundt cake\". Anything can be baked in a Bundt-style pan, and is. Recipes range from \"Pine Nut and Chili\" cakes to ice cream and fruit concoctions. Bundt-style pan design has expanded beyond the original fluted ring to today's designs of skylines and cathedrals, all with the requisite hole in the center of the pan made by Nordic Ware and others. Since a toroidal cake is rather difficult to frost, Bundt cakes are typically either dusted with powdered sugar, drizzle-glazed, or served undecorated. Recipes specifically designed for Bundt pans often have a baked-in filling; Bundt pound cakes are also common.\n\nSince the name \"Bundt\" is a trademark, similar pans are often sold as \"fluted tube pans\" or given other similar descriptive titles. The trademark holder Nordic Ware only produces Bundt pans in aluminum, but similar fluted pans are available in other materials.\n\nThe people credited with popularizing the Bundt cake are American businessman H. David Dalquist and his brother Mark S. Dalquist , who co-founded cookware company Nordic Ware based in St. Louis Park, Minnesota. In the late 1940s, Rose Joshua and Fannie Schanfield, friends and members of the Minneapolis Jewish-American Hadassah Society approached Dalquist asking if he could produce a modern version of a traditional cast iron dish. Dalquist and company engineer Don Nygren designed a cast aluminum version which Nordic Ware then made a small production run of in 1950. In order to successfully trademark the pans, a \"t\" was added to the word \"Bund\". A number of the original Bundt pans now reside in the Smithsonian collection.\n\nInitially, the Bundt pan sold so poorly that Nordic Ware considered discontinuing it. The product received a boost when it was mentioned in the \"New Good Housekeeping Cookbook\" in 1963, but did not gain real popularity until 1966, when a Bundt cake called the \"Tunnel of Fudge\", baked by Ella Helfrich, took second place at the annual Pillsbury Bake-Off and won its baker $5,000. The resulting publicity resulted in more than 200,000 requests to Pillsbury for Bundt pans and soon led to the Bundt pan surpassing the tin Jell-O mold as the most-sold pan in the United States. In the 1970s Pillsbury licensed the name Bundt from Nordic Ware and for a while sold a range of Bundt cake mixes.\n\nTo date more than 60 million Bundt pans have been sold by Nordic Ware across North America. November 15 has been named \"National Bundt Day\".\n\n\n"}
{"id": "27421560", "url": "https://en.wikipedia.org/wiki?curid=27421560", "title": "Bureau of Ocean Energy Management", "text": "Bureau of Ocean Energy Management\n\nThe Bureau of Ocean Energy Management (BOEM) is an agency within the United States Department of the Interior, established in 2010 by Secretarial Order. \n\nThe Outer Continental Shelf Lands Act (OCSLA) states: \"...the outer Continental Shelf is a vital national resource reserve held by the Federal Government for the public, which should be made available for expeditious and orderly development, subject to environmental safeguards, in a manner which is consistent with the of competition and other national needs.\"\n\nBOEM and its sister agency, the Bureau of Safety and Environmental Enforcement are the agencies to which this responsibility is delegated. They exercise the oil, gas, and renewable energy-related management functions formerly under the purview of the Minerals Management Service (MMS). Specifically, BOEM activities involve resource evaluation, planning, and leasing.\n\nThe agency's first director, serving from June 2010 to May 2014, was Tommy Beaudreau. The second director was Abigail Ross Hopper. Currently, the deputy director Walter Cruickshank serves as the acting director.\n\nA function inherited from the MMS is the review of nearly 1,700 planned wells and pipelines every year. The BOEM keeps records of shipwrecks, to ensure the Nation's important historical sites are protected. These shipwrecks, when over fifty years old, are considered National landmarks, and any new wells or pipelines have to be studied for their potential effect on archaeological sites on the outer continental shelf. \n\nThe BOEM maintains a list of shipwrecks and the location.\n\n\nThere were over 100 attacks on ships in the Gulf by German u-boats. Several were listed by the MMS and maintained by the BOEM.\n\n\nThe only known German U-boat to be sunk in the Gulf is U-166. After sinking the SS Robert E. Lee the United States Navy patrol craft PC-566 reported hitting and sinking the submarine. This was questioned and the sinking was attributed to a United States Coast Guard Grumman G-44 Widgeon, that reported an attack over 100 miles away, thought to be the U-166. In 2001 the wreckage of U-166 was identified near the wreckage of the Robert E. Lee and in 2014 the record was set straight that PC-566 actually sunk U-166. In 2014 the position, was designated a war grave.\n\n\n"}
{"id": "229714", "url": "https://en.wikipedia.org/wiki?curid=229714", "title": "Carriage", "text": "Carriage\n\nA carriage is a wheeled vehicle for people, usually horse-drawn; litters (palanquins) and sedan chairs are excluded, since they are wheelless vehicles. The carriage is especially designed for private passenger use, though some are also used to transport goods. A public passenger vehicle would not usually be called a carriage – terms for such include stagecoach, charabanc and omnibus. It may be light, smart and fast or heavy, large and comfortable or luxurious. Carriages normally have suspension using leaf springs, elliptical springs (in the 19th century) or leather strapping. Working vehicles such as the (four-wheeled) wagon and (two-wheeled) cart share important parts of the history of the carriage, as does too the fast (two-wheeled) chariot.\n\nThe word \"carriage\" (abbreviated \"carr\" or \"cge\") is from Old Northern French \"cariage\", to carry in a vehicle. The word \"car\", then meaning a kind of two-wheeled cart for goods, also came from Old Northern French about the beginning of the 14th century (probably derived from the Late Latin \"carro\", a car); it was also used for railway carriages, and was extended to cover \"automobile\" around the end of the nineteenth century, when early models were called \"horseless carriages\".\n\nA carriage is sometimes called a \"team\", as in \"horse and team\". A carriage with its horse is a \"rig\". An elegant horse-drawn carriage with its retinue of servants is an \"equipage\". A carriage together with the horses, harness and attendants is a \"turnout\" or \"setout\". A procession of carriages is a \"cavalcade\".\n\nSome horsecarts found in Celtic graves show hints that their platforms were suspended elastically. Four-wheeled wagons were used in prehistoric Europe, and their form known from excavations suggests that the basic construction techniques of wheel and undercarriage (that survived until the age of the motor car) were established then.\n\nThe earliest recorded sort of carriage was the chariot, reaching Mesopotamia as early as 1900 BC. Used typically for warfare by Egyptians, the near Easterners and Europeans, it was essentially a two-wheeled light basin carrying one or two passengers, drawn by one to two horses. The chariot was revolutionary and effective because it delivered fresh warriors to crucial areas of battle with swiftness.\n\nFirst century BC Romans used sprung wagons for overland journeys. It is likely that Roman carriages employed some form of suspension on chains or leather straps, as indicated by carriage parts found in excavations.\n\nIn the kingdom of the Zhou Dynasty the Warring States were also known to have used carriages as transportation. With the decline of these civilizations these techniques almost disappeared.\n\nThe medieval carriage was typically a four-wheeled wagon type, with a rounded top ('tilt') similar in appearance to the Conestoga Wagon familiar from the United States. Sharing the traditional form of wheels and undercarriage known since the Bronze Age, it very likely also employed the pivoting fore-axle in continuity from the ancient world. Suspension (on chains) is recorded in visual images and written accounts from the 14th century ('chars branlant' or rocking carriages), and was in widespread use by the 15th century. Carriages were largely used by royalty, aristocrats (and especially by women), and could be elaborately decorated and gilded. These carriages were on four wheels often and were pulled by two to four horses depending on how they were decorated (elaborate decoration with gold lining made the carriage heavier). Wood and iron were the primary requirements needed to build a carriage and carriages that were used by non-royalty were covered by plain leather.\n\nAnother form of carriage was the pageant wagon of the 14th century. Historians debate on the structure and size of pageant wagons; however, they are generally miniature house-like structures that rest on four to six wheels depending on the size of the wagon. The pageant wagon is significant because up until the 14th century most carriages were on two or 3 wheels; the chariot, rocking carriage, and baby carriage are two examples of carriages which pre-date the pageant wagon. Historians also debate whether or not pageant wagons were built with pivotal axle systems, which allowed the wheels to turn. Whether it was a four- or six-wheel pageant wagon, most historians maintain that pivotal axle systems were implemented on pageant wagons because many roads were often winding with some sharp turns. Six wheel pageant wagons also represent another innovation in carriages; they were one of the first carriages to use multiple pivotal axles. Pivotal axles were used on the front set of wheels and the middle set of wheels. This allowed the horse to move freely and steer the carriage in accordance with the road or path.\n\nOne of the great innovations of the carriage was the invention of the suspended carriage or the \"chariot branlant\" (though whether this was a Roman or medieval innovation remains uncertain). The 'chariot branlant' of medieval illustrations was suspended by chains rather than leather straps as had been believed. Chains provided a smoother ride in the chariot branlant because the compartment no longer rested on the turning axles. In the 15th century, carriages were made lighter and needed only one horse to haul the carriage. This carriage was designed and innovated in Hungary. Both innovations appeared around the same time and historians believe that people began comparing the chariot branlant and the Hungarian light coach. However, the earliest illustrations of the Hungarian 'Kochi-wagon' do not indicate any suspension, and often the use of three horses in harness.\n\nUnder King Mathias Corvinus (1458–90), who enjoyed fast travel, the Hungarians developed fast road transport, and the town of Kocs between Budapest and Vienna became an important post-town, and gave its name to the new vehicle type. The Hungarian coach was highly praised because it was capable of holding 8 men, used light wheels, could be towed by only one horse (it may have been suspended by leather straps, but this is a topic of debate). Ultimately it was the Hungarian coach that generated a greater buzz of conversation than the chariot branlant of France because it was a much smoother ride. Henceforth, the Hungarian coach spread across Europe rather quickly, in part due to Ippolito d'Este of Ferrara (1479–1529), nephew of Mathias' queen Beatrix of Aragon, who as a very junior Archbishopric of Esztergom developed a liking of Hungarian riding and took his carriage and driver back to Italy. Around 1550 the 'coach' made its appearance throughout the major cities of Europe, and the new word entered the vocabulary of all their languages. However, the new 'coach' seems to have been a concept (fast road travel for men) as much as any particular type of vehicle, and there is no obvious change that accompanied the innovation. As it moved throughout Europe in the late 16th century, the coach’s body structure was ultimately changed, from a round-top to the 'four-poster' carriages that became standard by c.1600.\n\nThe coach had doors in the side, with an iron step protected by leather that became the \"boot\" in which servants might ride. The driver sat on a seat at the front, and the most important occupant sat in the back facing forwards. The earliest coaches can be seen at Veste Coburg, Lisbon, and the Moscow Kremlin, and they become a commonplace in European art. It was not until the 17th century that further innovations with steel springs and glazing took place, and only in the 18th century, with better road surfaces, was there a major innovation with the introduction of the steel C-spring.\n\nIt was not until the 18th century that steering systems were truly improved. Erasmus Darwin was a young English doctor who was driving a carriage about 10,000 miles a year to visit patients all over England. Darwin found two essential problems or shortcomings of the commonly used light carriage or Hungarian carriage. First, the front wheels were turned by a pivoting front axle, which had been used for years, but these wheels were often quite small and hence the rider, carriage and horse felt the brunt of every bump on the road. Secondly, he recognized the danger of overturning.\n\nA pivoting front axle changes a carriage’s base from a rectangle to a triangle because the wheel on the inside of the turn is able to turn more sharply than the outside front wheel. Darwin proposed to fix these insufficiencies by proposing a principle in which the two front wheels turn about a centre that lies on the extended line of the back axle. This idea was later patented as Ackerman Steering. Darwin argued that carriages would then be easier to pull and less likely to overturn.\n\nCarriage use in North America came with the establishment of European settlers. Early colonial horse tracks quickly grew into roads especially as the colonists extended their territories southwest. Colonists began using carts as these roads and trading increased between the north and south. Eventually, carriages or coaches were sought to transport goods as well as people. As in Europe, chariots, coaches and/or carriages were a mark of status. The tobacco planters of the South were some of the first Americans to use the carriage as a form of human transportation. As the tobacco farming industry grew in the southern colonies so did the frequency of carriages, coaches and wagons. Upon the turn of the 18th century wheeled vehicle use in the colonies was at an all-time high. Carriages, coaches and wagons were being taxed based on the number of wheels they had. These taxes were implemented in the South primarily as the South had superior numbers of horses and wheeled vehicles when compared to the North. Europe, however, still used carriage transportation far more often and on a much larger scale than anywhere else in the world.\nCarriages and coaches began to disappear as use of steam propulsion began to generate more and more interest and research. Steam power quickly won the battle against animal power as is evident by a newspaper article written in England in 1895 entitled \"Horseflesh vs. Steam\". The article highlights the death of the carriage as the means of transportation.\n\nNowadays, carriages are still used for day-to-day transport in the United States by some minority groups such as the Amish. They are also still used in the tourism as vehicles for sightseeing in cities such as Bruges, Vienna, New Orleans, and Little Rock, Arkansas.\n\nThe most complete working collection of carriages can be seen at the Royal Mews in London where a large selection of vehicles is in regular use. These are supported by a staff of liveried coachmen, footmen and postillions. The horses earn their keep by supporting the work of the Royal Household, particularly during ceremonial events. Horses pulling a large carriage known as a \"covered brake\" collect the Yeoman of the Guard in their distinctive red uniforms from St James's Palace for Investitures at Buckingham Palace; High Commissioners or Ambassadors are driven to their audiences with The Queen in landaus; visiting heads of state are transported to and from official arrival ceremonies and members of the Royal Family are driven in Royal Mews coaches during Trooping the Colour, the Order of the Garter service at Windsor Castle and carriage processions at the beginning of each day of Royal Ascot.\n\nCarriages may be enclosed or open, depending on the type. The top cover for the body of a carriage, called the \"head\" or \"hood\", is often flexible and designed to be folded back when desired. Such a folding top is called a \"bellows top\" or \"calash\". A \"hoopstick\" forms a light framing member for this kind of hood. The top, roof or second-story compartment of a closed carriage, especially a diligence, was called an \"imperial\". A closed carriage may have side windows called \"quarter lights\" (British) as well as windows in the doors, hence a \"glass coach\". On the forepart of an open carriage, a screen of wood or leather called a \"dashboard\" intercepts water, mud or snow thrown up by the heels of the horses. The dashboard or carriage top sometimes has a projecting sidepiece called a \"wing\" (British). A \"foot iron\" or \"footplate\" may serve as a carriage step.\n\nA carriage driver sits on a \"box\" or \"perch\", usually elevated and small. When at the front it is known as a \"dickey box\", a term also used for a seat at the back for servants. A footman might use a small platform at the rear called a \"footboard\" or a seat called a \"rumble\" behind the body. Some carriages have a moveable seat called a \"jump seat\". Some seats had an attached backrest called a \"lazyback\".\n\nThe shafts of a carriage were called \"limbers\" in English dialect. \"Lancewood\", a tough elastic wood of various trees, was often used especially for carriage shafts. A \"holdback\", consisting of an iron catch on the shaft with a looped strap, enables a horse to back or hold back the vehicle. The end of the tongue of a carriage is suspended from the collars of the harness by a bar called the \"yoke\". At the end of a trace, a loop called a \"cockeye\" attaches to the carriage.\n\nIn some carriage types the body is suspended from several leather straps called \"braces\" or \"thoroughbraces\", attached to or serving as springs.\n\nBeneath the carriage body is the \"undergear\" or \"undercarriage\" (or simply \"carriage\"), consisting of the running gear and chassis. The wheels and axles, in distinction from the body, are the \"running gear\". The wheels revolve upon bearings or a spindle at the ends of a bar or beam called an \"axle\" or \"axletree\". Most carriages have either one or two axles. On a four-wheeled vehicle, the forward part of the running gear, or \"forecarriage\", is arranged to permit the front axle to turn independently of the fixed rear axle. In some carriages a 'dropped axle', bent twice at a right angle near the ends, allows a low body with large wheels. A guard called a \"dirtboard\" keeps dirt from the axle arm.\n\nSeveral structural members form parts of the chassis supporting the carriage body. The fore axletree and the splinter bar above it (supporting the springs) are united by a piece of wood or metal called a \"futchel\", which forms a socket for the pole that extends from the front axle. For strength and support, a rod called the \"backstay\" may extend from either end of the rear axle to the reach, the pole or rod joining the hind axle to the forward bolster above the front axle.\n\nA skid called a \"drag\", \"dragshoe\", \"shoe\" or \"skidpan\" retards the motion of the wheels. A London patent of 1841 describes one such apparatus: An iron-shod beam, slightly longer than the radius of the wheel, is hinged under the axle so that when it is released to strike the ground the forward momentum of the vehicle wedges it against the axle. The original feature of this modification was that, instead of the usual practice of having to stop the carriage to retract the beam and so lose useful momentum, the chain holding it in place is released (from the driver's position) so that it is allowed to rotate further in its backwards direction, releasing the axle. A system of \"pendant-levers\" and straps then allows the beam to return to its first position and be ready for further use.\n\nA catch or block called a \"trigger\" may be used to hold a wheel on a declivity.\n\nA horizontal wheel or segment of a wheel called a \"fifth wheel\" sometimes forms an extended support to prevent the carriage from tipping; it consists of two parts rotating on each other about the kingbolt above the fore axle and beneath the body. A block of wood called a \"headblock\" might be placed between the fifth wheel and the forward spring.\n\nMany of these fittings were carried over to horseless carriages and evolved into the modern elements of automobiles. During the Brass Era they were often the same parts on either type of carriage (i.e., horse-drawn or horseless). \n\nA trap, pony trap or horse trap is a light, often sporty, two-wheeled or sometimes four-wheeled horse-drawn carriage, accommodating usually two to four persons in various seating arrangements, such as face-to-face or back-to-back.\n\nA tanga (Hindi: टाँगा, Urdu: ٹانگہ, Bengali: টাঙ্গা) or Tonga is a light horse-drawn carriage used for transportation in India, Pakistan, and Bangladesh. Tangas are a popular mode of transportation because they are fun to ride in, and are usually cheaper to hire than a taxi or rickshaw. However, in many cities, tangas are not allowed to use highways because of their slow pace. In Pakistan, tangas are mainly found in the older parts of cities and towns, and are becoming less popular for utilitarian travel and more popular for pleasure. Tangas have become a traditional feature of weddings and other social functions in Pakistan, as well as in other nations. They are usually pulled by two horses, though some require only one. Others are designed for farm work. The room under the seats is sometimes used by the coachman (locally called \"coach-waan\") to keep his horse's food and sometimes to keep luggage if required.\n\nTangas are used for economic activity, mainly to carry heavy goods within the city limits.\n\nTangas were the most common means of transport in urban India and Pakistan until the early 1980s. Although autorickshaws have overtaken them in popularity, tangas are still common today in many cities and villages.\n\nA volante is a two-wheeled, one- or two-passenger Spanish carriage formerly much used in Cuba. The axle was behind an open, hooded body. The carriage was driven by a rider on the horse.\n\nAn araba (from Arabic: عربة, \"araba\" or ) (also arba or aroba) is a carriage (such as a cabriolet or coach), wagon or cart drawn by horses or oxen, used in Turkey and neighboring Middle Eastern countries. It is usually heavy and without springs, and often covered.\n\nThe names of many of these have now passed into obscurity but some have been adopted to describe automotive car body styles: \"coupé,\" \"victoria,\" \"brougham,\" \"landau\" and \"landaulet\", \"cabriolet\" (giving us our \"cab\"), \"phaeton,\" and \"limousine\" – all these once denoted particular types of carriages.\n\nA person whose business was to drive a carriage was a \"coachman\". A servant in livery called a \"footman\" or \"piquer\" formerly served in attendance upon a rider or was required to run before his master's carriage to clear the way. An attendant on horseback called an \"outrider\" often rode ahead of or next to a carriage. A \"carriage starter\" directed the flow of vehicles taking on passengers at the curbside. A \"hackneyman\" hired out horses and carriages. When hawking wares, a \"hawker\" was often assisted by a carriage.\n\nUpper-class people of wealth and social position, those wealthy enough to keep carriages, were referred to as \"carriage folk\" or \"carriage trade\".\n\nCarriage passengers often used a \"lap robe\" as a blanket or similar covering for their legs, lap and feet. A \"buffalo robe\", made from the hide of an American bison dressed with the hair on, was sometimes used as a carriage robe; it was commonly trimmed to rectangular shape and lined on the skin side with fabric. A \"carriage boot\", fur-trimmed for winter wear, was made usually of fabric with a fur or felt lining. A \"knee boot\" protected the knees from rain or splatter.\n\nA horse especially bred for carriage use by appearance and stylish action is called a \"carriage horse\"; one for use on a road is a \"road horse\". One such breed is the \"Cleveland Bay\", uniformly bay in color, of good conformation and strong constitution. Horses were broken in using a bodiless carriage frame called a \"break\" or \"brake\".\n\nA \"carriage dog\" or \"coach dog\" is bred for running beside a carriage.\n\nA roofed structure that extends from the entrance of a building over an adjacent driveway and that shelters callers as they get in or out of their vehicles is known as a \"carriage porch\" or \"porte cochere\". An outbuilding for a carriage is a \"coach house\", which was often combined with accommodation for a groom or other servants.\n\nA \"livery stable\" kept horses and usually carriages for hire. A range of stables, usually with \"carriage houses\" (\"remises\") and living quarters built around a yard, court or street, is called a \"mews\".\n\nA kind of dynamometer called a \"peirameter\" indicates the power necessary to haul a carriage over a road or track.\n\nIn most European and English-speaking countries, driving is a competitive equestrian sport. Many horse shows host driving competitions for a particular style of driving, breed of horse, or type of vehicle. Show vehicles are usually carriages, carts, or buggies and, occasionally, sulkies or wagons. Modern high-technology carriages are made purely for competition by companies such as Bennington Carriages. in England.\nTerminology varies: the simple, lightweight two- or four-wheeled show vehicle common in many nations is called a \"cart\" in the USA, but a \"carriage\" in Australia.\n\nInternationally, there is intense competition in the all-round test of driving: combined driving, also known as \"horse-driving trials\", an equestrian discipline regulated by the Fédération Équestre Internationale (International Equestrian Federation) with national organizations representing each member country. World championships are conducted in alternate years, including single-horse, horse pairs and four-in-hand championships. The World Equestrian Games, held at four-year intervals, also includes a four-in-hand competition.\n\nFor pony drivers, the World Combined Pony Championships are held every two years and include singles, pairs and four-in-hand events.\n\nAn almost bewildering variety of horse-drawn carriages existed. Arthur Ingram's \"Horse Drawn Vehicles since 1760 in Colour\" lists 325 types with a short description of each. By the early 19th century one's choice of carriage was only in part based on practicality and performance; it was also a status statement and subject to changing fashions. The types of carriage included the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "9675622", "url": "https://en.wikipedia.org/wiki?curid=9675622", "title": "Cleaner", "text": "Cleaner\n\nA cleaner or cleaning operative is a type of industrial or domestic worker who cleans homes or commercial premises for payment. Cleaning Operatives may specialise in cleaning particular things or places, such as window cleaners. Cleaning operatives often work when the people who otherwise occupy the space are not around. They may clean offices at night or houses during the workday. \n\nThe 2000 film \"Bread and Roses\" by British director Ken Loach depicted the struggle of cleaners in Los Angeles, California to fight for better pay and working conditions, and the right to join a union. In an interview with the BBC in 2001, Loach stated that thousands of cleaners from around 30 countries have since contacted him with tales similar to the one told in the film.\n\nThe cleaning industry is quite big as there are different types of cleaning required depending on the shape and size of object or property. If you want to get an office space cleaned then you would require the services of a commercial cleaner whereas if you want to clean your house then you would require domestic cleaning services. Depending on the task, even these categories can be sub divided such as Bond cleaners, carpet cleaners, upholstery cleaners, window cleaners, car cleaning services etc. Different types of cleaners specialize in their own industry, you cannot ask a window cleaner to do a marvelous job at carpet cleaning. Some of the types of cleaners are mentioned below.\n\nThese cleaning operatives are quite easy to find due to their large numbers, they are usually denoted as maid service providers, janitors and domestic cleaning operatives. These type of operatives specialize in house cleaning services such as spring cleaning, bond (damage deposit) cleaning etc. Most of the house cleaning operatives are known to give good attention to details, but due to their large numbers it can be quite hard to find the right cleaning operative who is trustworthy and works effectively. The cost of this type of service depends on the demand for the service and the number of providers. In Australia, you can hire a household cleaner from anywhere between $20–$30 an hour. In countries like India, China, Philippines etc., where the labor rates are quite low, people can afford a full-time maid or domestic cleaner.\n\nThe following are some items used by cleaning staff:\n\n\n\nhttps://www.bics.org.uk/A cleaner is a product used to clean, A Cleaning Operative is a skilled worker who cleans as a profession. https://www.bics.org.uk/\n"}
{"id": "3012047", "url": "https://en.wikipedia.org/wiki?curid=3012047", "title": "Combined sewer", "text": "Combined sewer\n\nA combined sewer is a sewage collection system of pipes and tunnels designed to simultaneously collect surface runoff water in a shared system. This type of gravity sewer design is no longer used in almost every instance worldwide when constructing new sewer systems. Modern-day sewer designs exclude surface runoff from sanitary sewers, but many older cities and towns continue to operate previously constructed combined sewer systems.\n\nCombined sewers can cause serious water pollution problems during combined sewer overflow (CSO) events when combined sewage and surface runoff flows exceed the capacity of the sewage treatment plant, or of the maximum flow rate of the system which transmits the combined sources. In instances where exceptionally high surface runoff occurs (such as large rainstorms), the load on individual tributary branches of the sewer system may cause a back-up to a point where raw sewage flows out of input sources such a toilets, causing inhabited buildings to be flooded with a toxic sewage-runoff mixture, incurring massive financial burdens for cleanup and repair. When combined sewer systems experience these higher than normal throughputs, relief systems cause discharges containing human and industrial waste to flow into rivers, streams, or other bodies of water. Such events frequently cause both negative environmental and lifestyle consequences, including beach closures, contaminated shellfish unsafe for consumption, and contamination of drinking water sources, rendering them temporarily unsafe for drinking and requiring boiling before uses such as bathing or washing dishes.\n\nRecent archaeological discoveries have shown that some of the earliest sewer systems were developed 2500 BC in the ancient city of Harappa. The primitive sewers were carved in the ground alongside buildings. This discovery reveals the conceptual understanding of waste disposal by the early civilizations.\n\nThe earliest sewers were designed to carry street runoff away from inhabited areas and into surface waterways without treatment. Open sewers, consisting of gutters and urban streambeds, were common worldwide before the 20th century. In the majority of developed countries, large efforts were made during the late 19th and early 20th centuries to cover the formerly open sewers, converting them to closed systems with cast iron, steel, or concrete pipes, masonry, and concrete arches. Most sewage collection systems of the 19th and early to mid 20th century used single-pipe systems that collect both sewage and urban runoff from streets and roofs. This type of collection system is referred to as a combined sewer system. The rationale for combining the two was that it would be cheaper to build just a single system. Most cities at that time did not have sewage treatment plants, so there was no perceived public health advantage in constructing a separate \"surface water sewerage\" (UK terminology) or \"storm sewer\" (US terminology) system.\n\nWhen constructed, combined sewer systems were typically sized to carry three to 160 times the average dry weather sewage flows. It is generally infeasible to treat the volume of mixed sewage and surface runoff flowing in a combined sewer during peak runoff events caused by snowmelt or convective precipitation. As cities built sewage treatment plants, those plants were typically built to treat only the volume of sewage flowing during dry weather. Relief structures were installed in the collection system to bypass untreated sewage mixed with surface runoff during wet weather, protecting sewage treatment plants from damage caused if peak flows reached the headworks.\n\nThese relief structures, called storm-water regulators (in American English - or combined sewer overflows in British English) are constructed in combined sewer systems to divert flows in excess of the peak design flow of the sewage treatment plant. Combined sewers are built with control sections establishing stage-discharge or pressure differential-discharge relationships which may be either predicted or calibrated to divert flows in excess of sewage treatment plant capacity. A leaping weir may be used as a regulating device allowing typical dry-weather sewage flow rates to fall into an interceptor sewer to the sewage treatment plant, but causing a major portion of higher flow rates to leap over the interceptor into the diversion outfall. Alternatively, an orifice may be sized to accept the sewage treatment plant design capacity and cause excess flow to accumulate above the orifice until it overtops a side-overflow weir to the diversion outfall.\n\nCSO statistics may be confusing because the term may describe either the number of events or the number of relief structure locations at which such events may occur. A CSO event, as the term is used in American English, occurs when mixed sewage and stormwater are bypassed from a combined sewer system control section into a river, stream, lake, or ocean through a designed diversion outfall, but without treatment. Overflow frequency and duration varies both from system to system, and from outfall to outfall, within a single combined sewer system. Some CSO outfalls discharge infrequently, while others activate every time it rains.\n\nThe storm water component contributes pollutants to CSO; but a major faction of pollution is the first foul flush of accumulated biofilm and sanitary solids scoured from the dry weather wetted perimeter of combined sewers during peak flow turbulence. Each storm is different in the quantity and type of pollutants it contributes. For example, storms that occur in late summer, when it has not rained for a while, have the most pollutants. Pollutants like oil, grease, fecal coliform from pet and wildlife waste, and pesticides get flushed into the sewer system. In cold weather areas, pollutants from cars, people and animals also accumulate on hard surfaces and grass during the winter and then are flushed into the sewer systems during heavy spring rains.\n\nCSO discharges during heavy storms can cause serious water pollution problems. The discharges contain human and industrial waste, and can cause beach closings, restrictions on shellfish consumption and contamination of drinking water sources.\n\nCSOs should not be confused with sanitary sewer overflows. Sanitary sewer overflows are caused by sewer system obstructions, damage, or flows in excess of sewer capacity (rather than treatment plant capacity.) Sanitary sewer overflows may occur at any low spot in the sewer system rather than at the CSO relief structures. Absence of a diversion outfall often causes sanitary sewer overflows to flood residential structures and/or flow over traveled road surfaces before reaching natural drainage channels. Sanitary sewer overflows may cause greater health risks and environmental damage than CSOs if they occur during dry weather when there is no precipitation runoff to dilute and flush away sewage pollutants.\n\nAbout 860 communities in the US have combined sewer systems, serving about 40 million people. Pollutants from CSO discharges can include bacteria and other pathogens, toxic chemicals, and debris. These pollutants have also been linked with antimicrobial resistance, posing serious public health concerns. The U.S. Environmental Protection Agency (EPA) issued a policy in 1994 requiring municipalities to make improvements to reduce or eliminate CSO-related pollution problems. It is managed by the National Pollutant Discharge Elimination System (NPDES) permit program. The policy defined water quality parameters for the safety of an ecosystem; it allowed for action that are site specific to control CSOs in most practical way for community; it made sure the CSO control is not beyond a community’s budget; and allowed water quality parameters to be flexible, based upon the site specific conditions. The CSO Control Policy required all states to have ″nine minimum controls″ in place by January 1, 1997, to decrease the effects of sewage overflow by making small improvements in existing processes. In 2000 Congress amended the Clean Water Act to require the municipalities to comply with the EPA policy.\n\nThe United Kingdom Environment Agency identified unsatisfactory intermittent discharges and issued an Urban Wastewater Treatment Directive requiring action to limit pollution from combined sewer overflows. In 2009 the Canadian Council of Ministers of the Environment adopted a Canada-wide Strategy for the Management of Municipal Wastewater Effluent including national standards to (1) remove floating material from combined sewer overflows, (2) prevent combined sewer overflows during dry weather, and (3) prevent development or redevelopment from increasing frequency of combined sewer overflows.\n\nMunicipalities in the US have been undertaking projects to mitigate CSO since the 1990s. For example, prior to 1990, the quantity of untreated combined sewage discharged annually to lakes, rivers and streams in southeast Michigan was estimated at more than per year. In 2005, with nearly $1 billion of a planned $2.4 billion CSO investment put into operation, untreated discharges have been reduced by more than per year. This investment that has yielded an 85 percent reduction in CSO has included numerous sewer separation, CSO storage and treatment facilities and wastewater treatment plant improvements constructed by local and regional governments.\n\nMany other areas in the US are undertaking similar projects (see, for example, in the Puget Sound of Washington). Cities like Pittsburgh, Seattle, Philadelphia, and New York are focusing on these projects partly because they are under federal consent decrees to solve their CSO issues. Both up-front penalties and stipulated penalties are utilized by EPA and state agencies to enforce CSO-mitigating initiatives and the efficiency of their schedules. Municipalities' sewage departments, engineering and design firms, and environmental organizations offer different approaches to potential solutions.\n\nSome US cities have undertaken sewer separation projects — building a second piping system for all or part of the community. In many of these projects, cities have been able to separate only portions of their combined systems. High costs or physical limitations may preclude building a completely separate system. In 2011 Washington, D.C. separated its sewers in four small neighborhoods at a cost of $11 million. (The project cost also includes improvements to the drinking water piping system.)\n\nAnother solution is to build a CSO storage facility, such as a tunnel that can store flow from many sewer connections. Because a tunnel can share capacity among several outfalls, it can reduce the total volume of storage that must be provided for a specific number of outfalls. Storage tunnels store combined sewage but do not treat it. When the storm is over, the flows are pumped out of the tunnel and sent to a wastewater treatment plant. One of the main concerns with CSO storage is the time it is stored before it is released. Without careful management of this time the water in the CSO storage facility runs the risk of going septic.\n\nWashington, D.C. is building underground storage capacity as its primary strategy to address CSOs. In 2011 the city began construction on a system of four deep storage tunnels, adjacent to the Anacostia River, that will reduce overflows to the river by 98 percent, and 96 percent system-wide. The system will comprise over 18 miles of tunnels with a storage capacity of 157 million gallons. (The city's overall \"Clean Rivers\" project, projected to cost $2.6 billion, includes other components, such as reducing stormwater flows.) The South Boston CSO Storage Tunnel is a similar project, completed in 2011.\n\nSome cities have expanded their basic sewage treatment capacity to handle some or all of the CSO volume. In 2002 litigation forced the city of Toledo, Ohio to double its treatment capacity and build a storage basin in order to eliminate most overflows. The city also agreed to study ways to reduce stormwater flows into the sewer system. (\"See\" Reducing stormwater flows.)\n\nRetention treatment basins or large concrete tanks that store and treat combined sewage are another solution. These underground structures can range in storage and treatment capacity from to of combined sewage. While each facility is unique, a typical facility operation is as follows. Flows from the overloaded sewers are pumped into a basin that is divided into compartments. The first flush compartment captures and stores flows with the highest level of pollutants from the first part of a storm. These pollutants include motor oil, sediment, road salt, and lawn chemicals (pesticides and fertilizers) that are picked up by the stormwater as it runs off roads and lawns. The flows from this compartment are stored and sent to the wastewater treatment plant when there is capacity in the interceptor sewer after the storm. The second compartment is a treatment or flow-through compartment. The flows are disinfected by injecting sodium hypochlorite, or bleach, as they enter this compartment. It then takes about 20‑30 minutes for the flows to move to the end of the compartment. During this time, bacteria are killed and large solid materials settle out. At the end of the compartment, any remaining sanitary trash is skimmed off the top and the treated flows are discharged into the river or lake.\n\nScreening and disinfection facilities treat CSO without ever storing it. Called \"flow-through\" facilities, they use fine screens to remove solids and sanitary trash from the combined sewage. Flows are injected with sodium hypochlorite for disinfection and mixed as they travel through a series of fine screens to remove debris. The fine screens have openings that range in size from 4 to 6 mm, or a little less than a quarter inch. The flow is sent through the facility at a rate that provides enough time for the sodium hypochlorite to kill bacteria. All of the materials removed by the screens are then sent to the sewage treatment plant through the interceptor sewer.\n\nCommunities may implement low impact development techniques to reduce flows of stormwater into the collection system. This includes:\n\nCSO mitigating initiatives that are solely composed of sewer system reconstruction are referred to as gray infrastructure, while techniques like permeable pavement and rainwater harvesting are referred to as green infrastructure. Conflict often occurs between a municipality's sewage authority and its environmentally active organizations between gray and green infrastructural plans.\n\nThe 2004 EPA \"Report to Congress\" on CSO's provides a review of available technologies to mitigate CSO impacts.\n\nRecent technological advances in sensing and control have enabled the implementation of Real Time Decision Support Systems (RT-DSS) for CSO mitigation. Through the use of internet of things technology and cloud computing, CSO events can now be mitigated by dynamically adjusting setpoints for movable gates, pump stations, and other actuated assets in sewers and storm water management systems. Similar technology, called adaptive traffic control is used to control the flow of vehicles through traffic lights. RT-DSS systems take advantage of storm temporal and spatial variability as well as varying concentration times due to diverse land uses across the sewershed to coordinate and optimize control assets. By maximizing storage and conveyance RT-DSS are able to minimize overflows using existing infrastructure. Successful implementations of RT-DSS have been carried out throughout the United States and Europe.\n\nAs a product of the Industrial Revolution, many cities in Europe and North America grew in the 19th century, frequently leading to crowding and increasing concerns about public health. As part of a trend of municipal sanitation programs in the late 19th and 20th centuries, many cities constructed extensive sewer systems to help control outbreaks of disease such as typhoid and cholera. Initially these systems discharged sewage directly to surface waters without treatment. As pollution of water bodies became a concern, cities added sewage treatment plants to their systems. Most cities in the Western world added more expensive systems for sewage treatment in the early 20th century.\n\nAs Britain was the first country to industrialize, it was also the first to experience the disastrous consequences of major urbanisation and was the first to construct a sewerage system as we know it today to mitigate the resultant unsanitary conditions. Joseph Bazalgette designed an extensive underground sewerage system that diverted waste to the Thames Estuary, downstream of the main centre of population. Six main interceptor sewers, totalling almost 135 miles (217 km) in length, were constructed. The intercepting sewers, constructed between 1859 and 1865, were fed by 450 miles (720 km) of main sewers that, in turn, conveyed the contents of some 13,000 miles (21,000 km) of smaller local sewers. With only minor modifications, Bazalgette's engineering achievement remains the basis for sewerage design up into the present day.\n\nIn France, the Paris cholera epidemic of 1832 sharpened the public awareness of the necessity for some sort of drainage system to deal with sewage and waster water in a better and healthier way since the Seine received up to 100,000 cubic meters of wastewater per day. Between 1865 and 1920 Eugene Belgrand led the development of a large scale system for water supply and wastewater management. By 1894 laws were passed which made drainage mandatory. The treatment of Paris sewage, though, was left to natural devices as 5,000 hectares of land were used to spread the waste out to be naturally purified.\n\nThe image of the sewer recurs in European culture as they were often used as hiding places or routes of escape by the scorned or the hunted, including partisans and resistance fighters in World War II. Fighting erupted in the sewers during the Battle of Stalingrad. The only survivors from the Warsaw Uprising and Warsaw Ghetto made their final escape through city sewers. Some have commented that the engravings of imaginary prisons by Piranesi were inspired by the Cloaca Maxima, one of the world's earliest sewers.\n\nThere is in the UK a legal difference between a storm sewer and a surface water sewer. You do not have a right of connection to a storm-water overflow sewer under section 106 of the Water Industry Act.\n\nThese are normally the pipe line that discharges to a watercourse, downstream of a combined sewer overflow. It takes the excess flow from a combined sewer. A surface water sewer conveys rainwater; legally you have a right of connection for your rainwater to this public sewer. A public storm water sewer can discharge to a public surface water, but not the other way around, without a legal change in sewer status by the water company.\n\nThe theme of traveling through, hiding, or even residing in combined sewers is a common plot device in media. Famous examples of sewer dwelling are the Teenage Mutant Ninja Turtles, Stephen King's \"It\", \"Les Miserables\", \"The Third Man\", \"Ladyhawke,\" \"Mimic\", \"The Phantom of the Opera\", \"Beauty and the Beast\", and \"Jet Set Radio Future\". The Todd Strasser novel \"\" is centered on a dog thwarting terroristic threats to electronically sabotage American sewage treatment plants.\n\nA well-known urban legend, the sewer alligator, is that of giant alligators or crocodiles residing in combined sewers, especially of major metropolitan areas. Two public sculptures in New York depict an alligator dragging a hapless victim into a manhole.\n\nAlligators have been known to get into combined storm sewers in the southeastern United States. Closed-circuit television by a sewer repair company captured an alligator in a combined storm sewer on tape.\n\n\n"}
{"id": "22554733", "url": "https://en.wikipedia.org/wiki?curid=22554733", "title": "Customer to customer", "text": "Customer to customer\n\nCustomer to customer (C2C) markets provide an innovative way to allow customers to interact with each other. Traditional markets require business to customer relationships, in which a customer goes to the business in order to purchase a product or service. In customer to customer markets, the business facilitates an environment where customers can sell goods or services to each other. Other types of markets include business to business (B2B) and business to customer (B2C).\n\nConsumer to consumer (or citizen-to-citizen) electronic commerce involves the electronically facilitated transactions between consumers through some third party. A common example is an online auction, in which a consumer posts an item for sale and other consumers bid to purchase it; the third party generally charges a flat fee or commission. The sites are only intermediaries, just there to match consumers. They do not have to check quality of the products being offered.\n\nConsumer to consumer (C2C) marketing is the creation of a product or service with the specific promotional strategy being for consumers to share that product or service with others as brand advocates based on the value of the product. The investment into conceptualising and developing a top of the line product or service that consumers are actively looking for is equitable to a retail pre-launch product awareness marketing.\n\nThere are many different classifications of marketing. From Government to Business (G2B), Business to Business (B2B), Business to Consumer (B2C), to Customer to Customer (C2C). While many companies usually operate in one or more of these areas, Customer to Customer businesses operate only within that specific area. Customer to Customer marketing has become more popular recently with the advent of the internet. Companies such as Craigslist, eBay, and other classified and auction based sites have allowed for greater interaction between consumers, facilitating the Customer to Customer model. Furthermore, as it becomes more economical for individuals to network on the internet via social websites and individual content creation, this marketing model has been greatly leveraged by businesses and individuals alike.\n\nThere are two implementations of customer to customer markets that are credited with its origin. These are classifieds and auctions.\n\nNewspapers and other similar publications were in frequent circulation and therefore were able to be used to facilitate a common need. Some people wanted things, other people had things and wanted to sell them. This was the birth of classifieds. The use of classifieds is referred to as classified advertisement. Normally used in text based print, classified advertisement is a now a strong vertical market that allows customers to communicate their needs with each other. In 2003 US classifieds market totaled $30.00 billion for both newspapers and online classified ad services.\n\nThe oldest auction house is Stockholm Auction House (Stockholms Auktionsverk), which was established in Sweden in 1674. Auctions however, have been recorded as far back as 500 B.C. Deriving from the Latin word augēre, which means to \"'increase' (or 'augment')\". Auctions have since widely used a method of liquidating assets, and has evolved into many different variations. The most successful current form of auctions is based on the internet, such as eBay.\n\nMost C2C websites, such as eBay, have both streamlined and globalized traditional person-to-person trading, which was usually conducted through such forms as garage sales, collectibles shows, flea markets and more, with their web interface. This facilitates easy exploration for buyers and enables the sellers to immediately list an item for sale within minutes of registering.\n\nWhen an item is listed on a C2C site, a nonrefundable insertion fee is charged based on the seller's opening bid on the item. Once the auction is completed, a final value fee is charged. This fee generally ranges from 1.25 percent to 5 percent of the final sale price.\n\nAfter the C2C site sets up the system in which bids could be placed, items can be put up for sale, transactions can be completed, seller fees are charged, and feedback can be left, while the C2C site stays in the background. For example, at the end of an auction, the C2C site notifies the buyer via e-mail that he or she has won. The C2C site also e-mails the seller to report who won and at what price the auction finished. At that point it's up to the seller and buyer to finish the transaction independently of the C2C site.\n\nC2C sites make money by charging fees to sellers. Although it's free to shop and place bids, sellers place fees to list items for sale, add on promotional features, and successfully complete transactions.\n\nMany C2C sites have expanded and developed existing product categories by introducing category-specific bulletin boards and chat rooms, integrating category-specific content, advertising its service in targeted publications and participating in targeted trade shows. eBay specifically has also broadened the range of products that it offers to facilitate trading on the site, including payment services, shipping services, authentication, appraisal, vehicle inspection and escrow services.\n\nSpecialty marketplaces have also been added to serve the specialized needs of buyers and sellers. For example, eBay Motors serves the automotive marketplace, including vehicles, parts and accessories; and Half.com (now closed) was focused on providing a fixed-price trading environment, initially for books music, videos and video games.\n\nMany online auction sites use a system called PayPal for sellers to receive online payments securely and quickly. A traditional credit card is not required to use this site because PayPal can be linked directly to you bank account.\n\nThere are various platforms that Consumer-to-consumer e-commerce is taking place on, such as social media (e.g. Facebook), advertisement websites (e.g. Craigslist) and online auction sites (e.g. eBay). \n\nConsumer to Consumer transactions often involve products sold via either a classified or auction-like system. As such, the products and services bought and sold are usually varied in type and have a short development and sale cycle. Products sold may often be used or second-hand, since consumer to consumer sales are often facilitated through auction or classified sites.\n\nSince products are usually second-hand, surplus, or used there is seldom a long development cycle associated with the products that are marketed via this method. However, in the case of individuals who are looking to sell a product or service they have developed to be sold on the small-scale, there is a product development life cycle. However, even when a product goes through a development life cycle when marketed in this manner, seldom does traditional marketing research occur. Oftentimes individuals are looking to make a quick profit, and simply place their product in the market place in hopes that it will be sold.\n\nAdvertising is essential towards the success of any business. In the case of customer to customer marketing, advertising often relates to online auctions and listings. As opposed to the pricey costs to advertise in media such as newspapers and magazines, products are already being promoted and publicized once users decide to officially put them on the internet. Potential buyers will become aware of products or services by conducting searches on the websites. Aside from possible fees and commissions imposed by the auction or listing site, advertising in this market does not require a substantial amount of money.\n\nCustomer to Customer marketing has become very popular in the recent years. Customers can directly contact sellers and eliminate the middle man. Moreover, anyone can now sell and advertise a product in the convenience of one's home – enabling one to easily start a business. Therefore, a wide variety of products can often be found on auction sites such as eBay, including second-hand goods. Since majority of these sales occur over the internet, sellers can reach both national and international customers and greatly increase their market. Feedback on the purchased product is often requested to aid both the seller and potential customers. The actual buying and searching process is simplified and search costs, distribution costs, and inventory costs are all reduced. Moreover, the transactions occur at a swift rate with the use of online payment systems such as PayPal.\n\nAlthough online auctions allow sellers to display their products, there is often a fee associated with such exhibitions. Other times, websites may charge a commission when products are sold. With the growing use of online auctions, the number of internet-related auction frauds have also increased. For instance, a seller may create two accounts on an auction site. When an interested buyer bids for an item, the seller will use another account to bid on the same item and thus, increasing the price. Consequently, many users have purchased products at unnecessarily inflated prices.\n\nIdentity theft has become a rising issue. Scam artists often create sites with popular domain names such as \"ebay\" in order to attract unknowing eBay customers. These sites will ask for personal information including credit card numbers. Numerous cases have been documented in which users find unknown charges on their credit card statements and withdrawals in their bank statements after purchasing something online. Unfortunately, websites often have a liability statement claiming that they are not responsible for any losses or damages. Furthermore, illegal or restricted products and services have been found on auction sites. Anything from illegal drugs, pirated works, prayers, and even sex have appeared on such sites. Although most of these items are blacklisted, some still find their way onto the internet.\n\nDespite the success of eBay, numerous other online auction sites have either shut down or consolidated with other similar sites. Creating an innovative and efficient business model is vital towards success. Online auctions can be categorized into five main models: C2C, B2C, B2B, B2G, and G2P. C2C refers to customer to customer, B2C signifies business to customer, B2B refers to business to business, B2G signifies business to government, and G2P refers to government to public. In recent years, online auctions have even appealed to major businesses. For instance, Sears has reported selling items at higher prices on these auctions when compared to discounting them in stores.\n\nThe success of an online auction site largely depends on six variables: interactivity, product offering, level of trust, rate of growth and adoption, networking, level of commitment, and payment options. Interactions among users are crucial and thus, websites must be accessible and easily navigable. E-mails, community boards, and feedback all aid in increasing the interactivity. With the growing need for convenience, the variety of products offered can greatly attribute to the client basis. Especially with the growing number of online frauds, trust is essential in auction sites. Users must be guaranteed that their personal information will remain secured and that they will receive their purchased product in a perfect condition and in a timely manner. With the fast-paced advancements in technology, auction sites must respond to these changes by staying updated. Moreover, sites also need to constantly search for business opportunities in order to expand their market. A large network of users is also crucial. Having an array of different sellers, buyers, suppliers, and delivery agents will increase the number of users, which would also raise the level of interactivity. In addition, forming alliances with different partners will also aid in the site's success. The level of commitment in buyers and sellers also plays a role in the auction's success. Similar to the level of trust, buyers must be ensured that they receive their purchased item, and sellers must actually receive payment. Although most prefer speedy online transactions, it is beneficial to offer different payment options that will accommodate different buyers.\n\nInternet classifieds are another example of customer to customer marketing. An example of an internet classified company, is Craigslist. Craigslist utilizes the internet to attract a wide customer and buyer base which employs the website to list and sell items.\n\nSince the customer to customer marketing strategy is strongly focused on serving the customer, the business model of Craigslist is simple: serve the customer first. Utilizing this model, Craigslist has developed into a prime example of a customer to customer driven 'machine', which focuses on the customer selling to the customer.\n\nRevenues which support the company are derived through subsidiary channels, while maintaining the model and convenience of the site. In fact, Craigslist makes no money off the customer to customer interactions that occur on the classifieds of the website. All of their revenue is derived from portion of the website targeted at businesses. Thus, in other words, their revenue is derived solely from their business to customer model utilized by businesses to post jobs and hire new workers.\n\nAs such, it becomes apparent that companies who focus on this particular model and, specifically classifieds, whether online or off, are often not focused on profit; but rather, on delivery of the service or product to ensure customer to customer interaction.\n\nInternet classifieds sites such as OLX, Quikr, Loogga etc. are gaining prominence in emerging economies such as India, Brazil and Nigeria. OLX and Quikr recently enabled their users to sell cows and buffaloes in rural India \n\nC2C marketing is of critical importance to retailers. When a shopper buys a product, if it can be shared with the shopper's friends, that drives significant traffic back to the customer site. Additionally, shoppers trust user generated recommendations much higher than recommendations pushed by the retailer. Retailers like CafePress have implemented C2C marketing on their website and companies like ShopSocially are building C2C marketing platforms for retailers. Recent trends by Facebook and Wavespot that leverage free WIFI at a local business are indicative of C2C marketing's importance in SMB space.\n\nMost companies think of C2C marketing as the use of social media channels such as Facebook and Twitter. However, in many cases, the messaging tends to be business to consumer.\n\n\n"}
{"id": "20592740", "url": "https://en.wikipedia.org/wiki?curid=20592740", "title": "DO-160", "text": "DO-160\n\nDO-160, Environmental Conditions and Test Procedures for Airborne Equipment is a standard for the environmental testing of avionics hardware. It is published by the Radio Technical Commission for Aeronautics (RTCA) and supersedes DO-138.\n\nThe DO-160 document was first published on February 28, 1975 to specify test conditions for the design of avionics electronic hardware in airborne systems. Since then the standard has undergone subsequent revisions up through Revision G.\n\nThis document outlines a set of minimal standard environmental test conditions (categories) and corresponding test procedures for airborne equipment for the entire spectrum of aircraft from light general aviation aircraft and helicopters through the jumbo jets and supersonic transport categories of aircraft. The purpose of these tests is to provide a controlled (laboratory) means of assuring the performance characteristics of airborne equipment in environmental conditions similar of those which may be encountered in airborne operation of the equipment.\nThe standard environmental test conditions and test procedures contained within the standard, may be used in conjunction with applicable equipment performance standards, as a minimum specification under environmental conditions, which can ensure an adequate degree of confidence in performance during use aboard an air vehicle.\nThe Standard Includes Sections on:\nThe user of the standard must also decide interdependently of the standard, how much additional test margin to allow for uncertainty of test conditions and measurement in each test.\n\n\n\n\n\n\n"}
{"id": "75987", "url": "https://en.wikipedia.org/wiki?curid=75987", "title": "Daylighting", "text": "Daylighting\n\nDaylighting is the practice of placing windows, skylights, other openings, and reflective surfaces so that sunlight (direct or indirect) can provide effective internal lighting. Particular attention is given to daylighting while designing a building when the aim is to maximize visual comfort or to reduce energy use. Energy savings can be achieved from the reduced use of artificial (electric) lighting or from passive solar heating. Artificial lighting energy use can be reduced by simply installing fewer electric lights where daylight is present or by automatically dimming/switching off electric lights in response to the presence of daylight – a process known as daylight harvesting.\n\nThe amount of daylight received in an internal space can be analyzed by measuring illuminance on a grid or undertaking a daylight factor calculation. Computer programs such as Radiance allow an architect or engineer to quickly calculate benefits of a particular design. The human eye's response to light is non-linear, so a more even distribution of the same amount of light makes a room appear brighter. \n\nThe source of all daylight is the Sun. The proportion of direct to diffuse light impacts the amount and quality of daylight. \"Direct sunlight\" reaches a site without being scattered within Earth's atmosphere. Light that is scattered in the atmosphere is \"diffused daylight\". Ground reflected light also contributes to the daylight. Each climate has different composition of these daylights and different cloud coverage, so daylighting strategies vary with site locations and climates. There is no direct sunlight on the polar-side wall (north-facing wall in the Northern Hemisphere and south-facing wall in the Southern Hemisphere) of a building from the autumnal equinox to the spring equinox at latitudes north of the Tropic of Cancer and south of the Tropic of Capricorn.\n\nTraditionally, houses were designed with minimal windows on the polar side, but more and larger windows on the equatorial-side (south-facing wall in the Northern Hemisphere and north-facing wall in the Southern Hemisphere). Equatorial-side windows receive at least some direct sunlight on any sunny day of the year (except in the tropics in summertime), so they are effective at daylighting areas of the house adjacent to the windows. In higher latitudes during midwinter, light incidence is highly directional and casts long shadows. This may be partially ameliorated through light diffusion, light pipes or tubes, and through somewhat reflective internal surfaces. In fairly low latitudes in summertime, windows that face east and west and sometimes those that face toward the pole receive more sunlight than windows facing toward the equator.\n\nWindows are the most common way to admit daylight into a space. Their vertical orientation means that they selectively admit sunlight and diffuse daylight at different times of the day and year. Therefore, windows on multiple orientations must usually be combined to produce the right mix of light for the building, depending on the climate and latitude. There are three ways to improve the amount of light available from a window:<ref name=\"Building for Energy Independence: Sun/Earth Buffering and Superinsulation\"></ref> (a) placing the window close to a light colored wall, (b) slanting the sides of window openings so the inner opening is larger than the outer opening, or (c) using a large light colored window-sill to project light into the room.\n\nDifferent types and grades of glass and different window treatments can also affect the amount of light transmission through the windows. The type of glazing is an important issue, expressed by its VT coefficient (Visual Transmittance), also known as visual light transmittance (VLT). As the name suggests, this coefficient measures how much visible light is admitted by the window. A low VT (below 0.4) can reduce by half or more the light coming into a room. But be also aware of high VT glass: high VT numbers (say, above 0.60) can be a cause of glare. On the other hand, you should also take into account the undesirable effects of large windows.\n\nWindows grade into translucent walls (below).\n\nAnother important element in creating daylighting is the use of clerestory windows. These are high, vertically placed windows. They can be used to increase direct solar gain when oriented towards the equator. When facing toward the sun, clerestories and other windows may admit unacceptable glare. In the case of a passive solar house, clerestories may provide a direct light path to polar-side (north in the northern hemisphere; south in the southern hemisphere) rooms that otherwise would not be illuminated. Alternatively, clerestories can be used to admit diffuse daylight (from the north in the northern hemisphere) that evenly illuminates a space such as a classroom or office.\n\nOften, clerestory windows also shine onto interior wall surfaces painted white or another light color. These walls are placed so as to reflect indirect light to interior areas where it is needed. This method has the advantage of reducing the directionality of light to make it softer and more diffuse, reducing shadows.\n\nAnother roof-angled glass alternative is a sawtooth roof (found on older factories). Sawtooth roofs have vertical roof glass facing away from the equator side of the building to capture diffused light (not harsh direct equator-side solar gain). The angled portion of the glass-support structure is opaque and well insulated with a cool roof and radiant barrier. The sawtooth roof's lighting concept partially reduces the summer \"solar furnace\" skylight problem, but still allows warm interior air to rise and touch the exterior roof glass in the cold winter, with significant undesirable heat transfer.\n\nSkylights are light transmitting fenestration (products filling openings in a building envelope which also includes windows, doors, etc.) forming all, or a portion of, the roof of a building space. Skylights are widely used in daylighting design in residential and commercial buildings, mainly because they are the most effective source of daylight on a unit area basis.\n\nAn alternative to a skylight is a roof lantern. A roof lantern is a daylighting cupola that sits above a roof, as opposed to a skylight which is fitted into a roof's construction. Roof lanterns serve as both an architectural feature and a method of introducing natural light into a space, and are typically wooden or metal structures with a number of glazed glass panels.\n\nAn atrium is a large open space located within a building. It is often used to light a central circulation or public area by daylight admitted through a glass roof or wall. Atria provide some daylight to adjacent working areas, but the amount is often small and does not penetrate very far. The main function of an atrium is to provide a visual experience and a degree of contact with the outside for people in the working areas. The daylighting of successive storeys of rooms adjoining an atrium is interdependent and requires a balanced approach. Light from the sky can easily penetrate the upper storeys but not the lower, which rely primarily on light reflected from internal surfaces of the atrium such as floor-reflected light. The upper stories need less window area than the lower ones, and if the atrium walls are light in color the upper walls will reflect light toward the lower stories.\n\nWalls made of glass brick are translucent to transparent. Traditionally, they are hollow and grouted with a fine concrete grout, but some modern glass brick walls are solid cast glass grouted with a transparent glue. If the glue matches the refractive index of the glass, the wall can be fairly transparent.\n\nIncreasing the amount of concrete, bottle walls embed bottles that run right through the wall, transmitting light. Concrete walls with glass prisms running through them have also been made. With the advent of cheaper optical fibers, fiber-optic concrete walls. Daylight (and shadow images) can then pass directly through a solid concrete wall, making it translucent; fiber optics will lead light around bends and over tens of meters. Typically only a few percent of the light is transmitted (the percent transmittance is about half the percent of the surface that is fibers, and usually only ~5% fibers are used).\n\nBoth glass and concrete conduct heat fairly well, when solid, so none of these walls insulate well. They are therefore often used outdoors, as a divider between two heated spaces (see images), or in very temperate climates.\n\nGreenhouse walls (and rooves) are made to transmit as much light and as little heat as possible. They use a variety of materials, and may be transparent or translucent.\n\n\"See anidolic lighting\"\n\nIt is possible to provide some daylight into spaces that have low possibility of windows or skylights through remote distribution devices such as mirrors, prisms, or light tubes. This is called anidolic lighting, from anidolic (non-image-forming) optics. The non-linear response of the human eye to light means that spreading light to a broader area of a room makes the room appear brighter, and makes more of it usefully lit.\n\nRemote daylight distribution systems have losses, and the further they have to transmit the daylight and the more convoluted the path, the greater the inefficiency. The efficiency of many remote distribution systems can also vary dramatically from clear to overcast skies. Nonetheless, where there is no other possibility of providing daylight to a space, remote distribution systems can be appreciated.\n\nOnce used extensively in office buildings, the manually adjustable light reflector is seldom in use today having been supplanted by a combination of other methods in concert with artificial illumination. The reflector had found favor where the choices of artificial light provided poor illumination compared to modern electric lighting.\n\nLight shelves are an effective way to enhance the lighting from windows on the equator-facing side of a structure, this effect being obtained by placing a white or reflective metal light shelf outside the window. Usually the window will be protected from direct summer season sun by a projecting eave. The light shelf projects beyond the shadow created by the eave and reflects sunlight upward to illuminate the ceiling. This reflected light can contain little heat content and the reflective illumination from the ceiling will typically reduce deep shadows, reducing the need for general illumination.\n\nIn the cold winter, a natural light shelf is created when there is snow on the ground which makes it reflective. Low winter sun (see Sun path) reflects off the snow and increases solar gain through equator-facing glass by one- to two-thirds which brightly lights the ceiling of these rooms. Glare control (drapes) may be required.\n\nThe oldest use of prisms for daylighting may well be deck prisms, let into the decks of ships to transmit light below. Later, pavement lights or vault lights were used to light basement areas under sidewalks.\n\nPrisms that used total internal reflection to throw light sideways, lighting the deeper portions of a room, later became popular. Early thick, slow-cooling cast glass prism tiles were often known as \"luxfer tiles\" after a major manufacturer. They were and are used in the upper portions of windows, and some believe that they contributed to the trend from dark, subdivided Victorian interiors to open-plan, light-coloured ones.\n\nDaylight redirecting window film (DRF) is a thin plastic version of the old glass prism tiles. It can be used as a substitute for opaque blinds.\n\nAnother type of device used is the light tube, also called a tubular daylighting device (TDD), which is placed into a roof and admits light to a focused area of the interior. These somewhat resemble recessed ceiling light fixtures. They do not allow as much heat transfer as skylights because they have less surface area.\n\nTDDs use modern technology to transmit visible light through opaque walls and roofs. The tube itself is a passive component consisting of either a simple reflective interior coating or a light conducting fiber optic bundle. It is frequently capped with a transparent, roof-mounted dome \"light collector\" and terminated with a diffuser assembly that admits the daylight into interior spaces and distributes the available light energy evenly (or else efficiently if the use of the lit space is reasonably fixed, and the user desired one or more \"bright-spots\").\n\nThe tubular daylighting device was invented by Solatube International in 1986 and brought to market first in Australia in 1991.\n\nSmart glass is the name given to a class of materials and devices that can be switched between a transparent state and a state which is opaque, translucent, reflective, or retro-reflective. The switching is done by applying a voltage to the material, or by performing some simple mechanical operation. Windows, skylights, etc., that are made of smart glass can be used to adjust indoor lighting, compensating for changes of the brightness of the light outdoors and of the required brightness indoors.\n\nThe use of heliostats, mirrors which are moved automatically to reflect sunlight in a constant direction as the sun moves across the sky, is gaining popularity as an energy-efficient method of lighting. A heliostat can be used to shine sunlight directly through a window or skylight, or into any arrangement of optical elements, such as light tubes, that distribute the light where it is needed. The image shows a mirror that rotates on a computer-controlled, motor-driven altazimuth mount.\n\nSolar street lights raised light sources which are powered by photovoltaic panels generally mounted on the lighting structure. The solar array of such off-grid PV system charges a rechargeable battery, which powers a fluorescent or LED lamp during the night. Solar street lights are stand-alone power systems, and have the advantage of savings on trenching, landscaping, and maintenance costs, as well as on the electric bills, despite their higher initial cost compared to conventional street lighting. They are designed with sufficiently large batteries to ensure operation for at least a week and even in the worst situation, they are expected to dim only slightly.\n\nOak Ridge National Laboratory (ORNL) has developed a new alternative to skylights called hybrid solar lighting. This design uses a roof-mounted light collector, large-diameter optical fiber, and modified efficient fluorescent lighting fixtures that have transparent rods connected to the optical fiber cables. Essentially no electricity is needed for daytime natural interior lighting.\n\nField tests conducted in 2006 and 2007 of the new HSL technology were promising, but the low-volume equipment production is still expensive. HSL should become more cost effective in the near future. A version that can withstand windstorms could begin to replace conventional commercial fluorescent lighting systems with improved implementations in 2008 and beyond. The U.S. 2007 Energy Bill provides funding for HSL R&D, and multiple large commercial buildings are ready to fund further HSL application development and deployment.\n\nAt night, ORNL HSL uses variable-intensity fluorescent lighting electronic control ballasts. As the sunlight gradually decreases at sunset, the fluorescent fixture is gradually turned up to give a near-constant level of interior lighting from daylight until after it becomes dark outside.\n\nHSL may soon become an option for commercial interior lighting. It can transmit about half of the direct sunlight it receives.\n\nIn a well-designed isolated solar gain building with a solarium, sunroom, greenhouse, etc., there is usually significant glass on the equator side. A large area of glass can also be added between the sun room and the interior living quarters. Low-cost, high-volume-produced patio door safety glass is an inexpensive way to accomplish this goal.\n\nThe doors used to enter a room should be opposite the sun room interior glass, so that a user can see outside immediately when entering most rooms. Halls should be minimized with open spaces used instead. If a hall is necessary for privacy or room isolation, inexpensive patio door safety glass can be placed on both sides of the hall. Drapes over the interior glass can be used to control lighting. Drapes can optionally be automated with sensor-based electric motor controls that are aware of room occupancy, daylight, interior temperature, and time of day. Passive solar buildings with no central air conditioning system need control mechanisms for hourly, daily, and seasonal, temperature-and-daylight variations. If the temperature is correct, and a room is unoccupied, the drapes can automatically close to reduce heat transfer in either direction.\n\nTo help distribute sun room daylight to the sides of rooms that are farthest from the equator, inexpensive ceiling-to-floor mirrors can be used.\n\nBuilding codes require a second means of egress, in case of fire. Most designers use a door on one side of bedrooms, and an outside window, but west-side windows provide very-poor summer thermal performance. Instead of a west-facing window, designers use an R-13 foam-filled solid energy-efficient exterior door. It may have a glass storm door on the outside so that light can pass through when the inner door is opened. East/west glass doors and windows should be fully shaded top-to-bottom or a spectrally selective coating can be used to reduce solar gain.\n\nArchitects and interior designers often use daylighting as one of design elements. Good daylighting requires attention to both qualitative and quantitative aspects of design.\n\nUtilizing natural light is one of the design aspects in architecture; In 1929, the French architect, Le Corbusier said that \"The history of architectural material... has been the endless struggle for light... in other words, the history of windows.\" As he emphasized in his architecture (such as Notre Dame du Haut), daylighting has been a major architectural design elements (See MIT Chapel and Church of the Light for examples). Not only the aesthetic aspects, the impact of daylighting on human health and work performance is also considered as qualitative daylighting. The current studies show that lighting conditions in workplaces contribute to a variety of factors related to work satisfaction, productivity and well-being and significantly higher visual acceptance scores under daylighting than electrical lighting. Studies have also shown that light has a direct effect on human health because of the way it influences the circadian rhythms.\n\nA well daylit space needs both adequate lighting levels and light that is well distributed. In the current building industry, daylighting is considered a building performance measure in green building certification programs such as LEED. Illumination Engineering Society (IES) and the society of Light and Lighting (SLL) provide illuminance recommendation for each space type. How much daylighting contributes to the recommended lighting level determines daylighting performance of a building. There are two metrics that IES has approved to evaluate daylighting performance: Spatial Daylight Autonomy(sDA) and Annual Sunlight Exposure (ASE). sDA is a metric describing annual sufficiency of ambient daylight levels in interior environments. See Daylight autonomy and LEED documentation sections for more details.\n\nIn existing buildings, field measurements can be undertaken to evaluate daylighting performance. Illuminance measurements on a grid is a basic level to derive an average illuminance of a space. The spacing of the measurement points vary with project purposes. The height of these points depends on where the primary task is performed. In most office spaces, desk level (0.762m above the floor) will be measured. Based on measurements, average illuminance, maximum-to-minimum uniformity ratio, and average-to-minimum uniformity ratio will be calculated and compared to the recommended lighting level. A diagnostic survey specific to lighting can be conducted to analyse the satisfaction of building occupants.\n\nComputational simulations can predict daylighting condition of a space much faster and more detailed than hand calculations or scale model testing. The simulations allow for the effects of climate with hourly weather data from typical meteorological year. Computer models are available which can predict variations in internally reflected light. Radiosity and ray-tracing are methods can deal with complex geometry, allow complex sky distributions and potentially produce photorealistic images. Radiosity methods assume all surfaces are perfectly diffusing to reduce computational times. Ray-tracing techniques have accuracy and image rendering capacity.\n\nDaylight autonomy is the percentage of time that daylight levels are above a specified target illuminance within a physical space or building. The calculation is based on annual data and the predetermined lighting levels. The goal of the calculation is to determine how long an individual can work in a space without requiring electrical lighting, while also providing optimal visual and physical comfort.\n\nDaylight autonomy is beneficial when determining how daylight enters and illuminates a space. The drawback, however, is that there is no upper limit on luminance levels. Therefore, a space with a high internal heat gain deemed uncomfortable by occupants, would still perform well in the analysis. Achieving daylight autonomy requires an integrated design approach that guides the building form, siting, climate considerations, building components, lighting controls, and lighting design criteria.\n\nContinuous daylight autonomy, is similar to daylight autonomy but partial credit is attributed to time steps when the daylight illuminance lies below the minimum illuminance level. For example, if the target illuminance is 400 lux and the calculated value is 200 lux, daylight autonomy would give zero credit, while continuous daylight autonomy would give 0.5 credit (200/400 = 0.5). The benefit of continuous daylight autonomy is that it does not give a hard threshold of acceptable illuminance. Instead, it addresses the transition area—allowing for realistic preferences within any given space. For example, office occupants usually prefer to work at daylight below the illuminance threshold since this level avoids potential glare and excessive contrast.\n\nUseful daylight illuminance focuses on the direct sunlight that falls into a space. The useful daylight illuminance calculation is based on three factors—the percentage of time a point is below, between, or above an illuminance value. The range for these factors is typically 100-2,000 lux. Useful daylight illuminance is similar to daylight autonomy but has the added benefit of addressing glare and thermal discomfort. The upper threshold is used to determine when glare or thermal discomfort is occurring and may need resolution.\n\nThe LEED 2009 daylighting standards were intended to connect building occupants with the outdoors through use of optimal daylighting techniques and technologies. According to these standards, the maximum value of 1 point can be achieved through four different approaches. The first approach is a computer simulation to demonstrate, in clear sky conditions, the daylight illuminance levels 108-5,400 lux on, September 21 between 9:00 a.m. and 3:00 p.m. Another prescriptive approach is a method that uses two types of side-lighting, and three types of top-lighting to determine if a minimum of 75% daylighting is achieved in the occupied spaces. A third approach uses indoor light measurements showing that between 108-5,400 lux have been achieved in the space. The last approach is a combination of the other three calculation methods to prove that the daylight illumination requirements are achieved.\n\nThe LEED 2009 documentation is based upon the daylight factor calculation. The daylight factor calculation is based on uniform overcast skies. It is most applicable in Northern Europe and parts of North America. Daylight factor is “the ratio of the illuminance at a point on a plane, generally the horizontal work plane, produced by the luminous flux received directly or indirectly at that point from a sky whose luminance distribution is known, to the illuminance on a horizontal plane produced by an unobstructed hemisphere of this same sky.\"\n\nLEED v4 daylighting standards are the most current as of 2014. The new standards are similar to the old standards, but also intend to “reinforce circadian rhythms, and reduce the use of electrical lighting by introducing daylight in the space. Two options exist for achieving the maximum value of these two most recent points. One option is to use a computer simulation to demonstrate that a spatial daylight autonomy of 300 lux for at least 50% of the time, and an annual sunlight exposure of 1,000 lux for 250 occupied hours per year, exists in the space. Another option is to show that illuminance levels are between 300 lux and 3,000 lux between 9:00 a.m. and 3:00 p.m. on a clear day at the equinox for 75% or 90% of the floor area in the space. The overall goal of the LEED v4 daylighting metrics is to analyze both the quantity and quality of the light, as well as to balance the use of glazing to ensure more light and less cooling load.\n\n\n"}
{"id": "22033128", "url": "https://en.wikipedia.org/wiki?curid=22033128", "title": "DelSolar", "text": "DelSolar\n\nDelSolar Co., Ltd. (Traditional Chinese:旺能光電; Simplified Chinese: 旺能光电(pinyin: wàng néng guāng diàn), commonly abbreviated 旺能)) (3599, TW) is solar company from Delta group(台達電子/台達集團), engaging in the research, design, manufacture and distribution of solar cells, solar modules, as well as the development of Photovoltaic system. The company’s headquarters is in Hsinchu, Science-based industrial Park, Phrase II, Taiwan.\n\nFounded in 2004 by the alliance between the parent company, Delta Electronics, Inc., the world's largest provider of switching power supplies and brushless fans, and the Industrial Technology Research Institute (ITRI), a non-profit research institute located in Taiwan under the supervision of the Republic of China Ministry of Economic Affairs. DelSolar has started its initial solar cell production in 2005 and launched the first solar module series in 2009. \nThe company is listed on Taiwan’s Emerging Board in 2007.\n\nThe name DelSolar is a merger of “Delta” and “solar”, the name of the parent company and DelSolar’s main focused industry. The company’s vision is to enable ubiquitous solar systems for a greener world. By being a part of Delta Group, DelSolar is able to do more with less because of its seamless vertical integration with Delta Electronics and strategic alliances with its business partners. The company is dedicated to providing clean and effective solar energy so to power as well as empower the world.\n\nDelSolar has started its initial solar cell production in 2005 with the capacity of 25MW and launched the first solar module series in 2009 with up to 230W power output. The company had 11 production lines with total capacity of 436 MW by the end of 2010. DelSolar is executing its capacity expansion plans given the robust trend in demand. On 4 June 2010, DelSolar announced its ground breaking ceremony for Jhunan plant, which is expected to be completed and begin its mass production in Q2, 2011. The estimated solar cell production capacity could reach 3 GW. The second manufacturing plant, which is planned as a module plant, is expected to be completed at the end of 2012. The module capacities of DelSolar are 45 MW and 70 MW in Hsinchu plant and Wujiang plant respectively. Wujiang plant is expected to reach the total solar module capacity at 1 GW in year 2013.\n\nIn 2009, the total shipment of DelSolar is 90.8 MW. According to the company’s 2009 financial report, total revenues was US$135 M. Sales has been primarily in Europe, approximately 60%. DelSolar has started their module business in US and is optimistic about US market as the company will leverage their market resources in US.\n\nDelSolar’s solar cell includes monocrystalline and polycrystalline silicon-based cells, with size of 6 inch, 2 busbars or 3 busbars and the wafer thickness of 180 to 200 mm.\n\nDelSolar’s solar module includes monocrystalline and polycrystalline silicon solar modules and black modules. DelSolar’s modules can provide positive power tolerances of up to +3%, which offer a stable and high-energy system output. \nAll DelSolar module series are certified to IEC 61215 ed. 2 and IEC 61730. The fabrication is also certified in accordance to ISO 9001/14001 and OHSAS 18001. Product series with IEC and additional UL certificates are available for the European and US markets.\n\nThe company also offers a full set of Photovoltaic system design, including, feasibility study, financing, site planning, engineering, procurement, construction, operation and maintenance. DelSolar has supplied or installed solar modules in many Photovoltaic systems worldwide, such as Kaohsiung National Stadium in Taiwan and River of Life Christian Church in US.\n\nDelSolar has 3 manufacturing facilities, located at Hsinchu and Jhunan, Taiwan and Wujiang, China, as well as the representative offices in Taiwan, China, United States, the Netherlands.\n"}
{"id": "35221180", "url": "https://en.wikipedia.org/wiki?curid=35221180", "title": "Flexibility–usability tradeoff", "text": "Flexibility–usability tradeoff\n\nThe flexibility–usability tradeoff is a design principle maintaining that, as the flexibility of a system increases, its usability decreases. The tradeoff exists because accommodating flexibility requires satisfying a larger set of requirements, which results in complexity and usability compromises.\n\nDesign theory maintains that over their lifecycle, systems shift from supporting multiple uses inefficiently, towards efficiently supporting a single use as users' needs become more defined and better understood, both by themselves and designers.\n\nWhen weighting the relative importance of flexibility versus usability, designers are advised to consider how well the needs of the user are understood. If user needs are well understood, designers should bias toward simple less-flexible systems. Otherwise, designers should create flexible designs that support multiple future applications.\n"}
{"id": "36369795", "url": "https://en.wikipedia.org/wiki?curid=36369795", "title": "Fuel Cells (journal)", "text": "Fuel Cells (journal)\n\n\"Fuel Cells—From Fundamentals to Systems\" is a bimonthly peer-reviewed scientific journal covering fundamental and applied research on fuel cell technology. Disciplines of interest are chemistry, materials science, physics, chemical engineering, electrical engineering, and mechanical engineering. Publishing formats include original research papers and reviews. It is published by Wiley-VCH and the editor-in-chief is Ulrich Stimming (TUM CREATE Center for Electromobility).\n\nThe journal is abstracted and indexed by:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2011 impact factor of 3.149.\n"}
{"id": "28022473", "url": "https://en.wikipedia.org/wiki?curid=28022473", "title": "GPS Tuner", "text": "GPS Tuner\n\nGPS Tuner is the name of a software company based in Budapest, Hungary, and of a software product they make. The software is a tool intended for off-road navigation based on the Global Positioning System (GPS), where it is used to determine position while cycling, hiking, geocaching, boating, flying, driving, and many other outdoor sporting activities. It can be used by those with a GPS receiver alone, or in a smartphone, Pocket PC, or iPhone. Recently, the firm released a new version of its navigation software dedicated to electric bicycles named eBike Navigation. This new version includes a variety of battery management features like display for battery capacity, range, gear, power and battery consumption. EBike Navigation also offers assistance recommendations for optimizing battery capacity along with a real rage calculation based on terrain elevation and other variable conditions. In mid-2010, the firm released an iPhone app version of the software that included technology allowing off-road turn-by-turn navigation.\n\nLength units and map grids are selectable. Units include metric, imperial or nautical. Grids include 24 national grids, such as WGS-84, British (OSGB36), NAD27, NAD83.\n\nMapping supports JPG files as maps, calibration desktop software program, auto map loading feature, zoom and pan by pen or joystick, flexible layer and track/route display, distance measuring and route defining manually by drawing on maps. The map can also be blank.\n\nFor navigation, the digital compass has a real time rotating compass with a target arrow. The target arrow points to a defined target coordinate. Any waypoint, trackpoint, or point of map can be selected as target, with six different types of compass views, 38 selectable compass tools, voice navigation and next turn indication for routes. The trip computer monitors whole trips, with options to save, load and reset. It has 31 selectable trip computer tools, a graph for altitude and speed, selectable tool window size, and can push track data into trip computer to get a full track analysis.\n\nWaypoints, point of interest (POI) support GPS eXchange Format (GPX), LOC, Keyhole Markup Language (KML) files. Track, route supports GPX, TRK, KML files, and tracking recording by configurable auto, time, or distance interval. Waypoints and tracks can be viewed in Google Earth directly, with KML export/import. Flexible waypoint management, convert track to route and reverse route direction.\n\nIt can do NMEA recording, playback (with selectable speed), and can jump to any position in NMEA files, skipping unneeded data.\n\nPosition sharing allows user positions to be shared in real time with other users, which position is also displayed. All user positions can be monitored in a web browser, suitable for precise personal and vehicle tracking.\n\nGPS navigation applications use vector-based maps called vector graphics. This map type contains objects such as points of interest, city names, and street names, which are stored in data tables inside the map itself. GPS Tuner uses such vector map data, combined with raster graphics based maps.\n\nCustom user maps can be created and calibrated by users, or pre-calibrated maps can be downloaded from many free websites. As pictures, the maps need no updating until any elements of the picture map change, such as street positions, new roads are added, etc. POIs are not built into the pictures, and are instead maintained in a separate vector file database.\n\n"}
{"id": "27802692", "url": "https://en.wikipedia.org/wiki?curid=27802692", "title": "German Institute of Food Technologies", "text": "German Institute of Food Technologies\n\nThe German Institute of Food Technologies (ger. \"Deutsches Institut für Lebensmitteltechnik e.V.\" - DIL) is a non-governmental and self-financing research institute supported by more than 120 members from fields of food production, mechanical engineering, process engineering and metrology.\nThe DIL was founded in 1983 and is situated in the county of Osnabrück, one of the regions with the highest numbers of full-time employees and highest density of food industry enterprises in Germany.\n\nThe research focus of the DIL is to develop innovative methods for food production, quality assurance and process optimization. Beneficiaries of new insights in food technology are small and medium-sized enterprises. Stated as main objective is the improvement of efficiency and increase of competitiveness of companies from the food industry.\n\nAccording to own sources, the institute finances itself by 90% through external funds, out of which 55% are used to cooperate directly with members of the DIL.\n\nIn May 1985, a new building was planned for the DIL in the Professor-von-Klitzing Street, where the foundation stone was laid on September 1985 by Birgit Breuel, who was Secretary of Commerce then. The Minister-President of Lower Saxony at that time, Ernst Albrecht, inaugurated the new building at the turn of the year 1986/1987.\n\nIn 1988, the institute was renamed into its present name, \"German Institute of Food Technologies\", and its legal form was changed from membership cooperation to registered association.\n\nOn 20 Mai 2009, the foundation stone for the expansion of the DIL building was laid by Friedrich-Otto Ripke, the Secretary of State for agriculture, nutrition, and consumer protection.\nThe former Minister-President of Lower Saxony Christian Wulff and Hans-Heinrich Ehlen were thanked for their personal contribution to the grant which enables the German Institute of Food Technologies to cooperate and link itself with further research institutes in the county.\n\nAs part of DIL's agenda for 2015, the current building had to be expanded. The state of Lower Saxony had agreed to support the Artland municipality with a subsidy of €15 million to enable local agribusinesses and food production companies to develop their international competitiveness.\n\nNew trends in markets and direct orders from companies serve as drive and guideline to develop appealing, healthy food products for all market segments: confectionery, baked goods, dairy and meat products, snacks and convenience products. The development process is also governed by a comprehensive analytical characterization of the structure of ingredients. Physical analysis in thermal, rheological and physiochemical tests enable possibilities to identify and characterize microstructures such as foams, emulsions and suspensions.\n\nThe focus of this business field is set to investigate in more efficient methods to process food production. Main goals include energy reduction, automation of technical processes and the use of new technologies to transform materials.\nThe following areas are highlights of the process development:\n\nProducts are tested upon their safety to avoid unforeseen incidents such as contamination of food. Raw material but also final products are being tested according to legal requirements and regulations in order to be safe for the consumer.\nMain areas of focus are quality management, chemical- and microbiological analysis. Although the food producer alone carries the responsibility for the production of safe food and the implementation of the necessary measures, the DIL can support food producers through quality controls and seminars about hygiene management.\n\nThe Center of Food Physics (ZLP) is the latest business division of the DIL ; it started operation in mid-2011. On behalf of its customers, the ZLP investigates the relationship between structure and function and transfers its findings on structure formation mechanisms into process technology. The scale-up of technical and industrial applications is conducted in cooperation with other DIL divisions in order to be able to process complex R&D tasks for its customers effectively and to a high level. While chemical analysis is concerned with the determination of the quality and quantity of substances and material systems, physical analysis applies expert knowledge, advanced measuring and examination methods and tailormade\nanalysis concepts to characterize materials and material systems. Since structures are the key to understanding properties, this area is of particular importance. The work of the ZLP is methodology-oriented, which means that the central elements are the development and application of measuring techniques for physical characterization. The measuring methods will then be adapted to the respective material systems. The existing expertise and the available measuring and examination systems of the industry and other research institutes can thereby be offered as a service to minimize the gap between research and application.\n\nSince 2012, the Bio-Economy division has been a new research focus at DIL. It serves as an interface between research platforms and business divisions. Cross-thematic and cross-sectoral research activities have been established to promote the structural change from an oil-based to a bio-based industry along with major opportunities for growth and employment. At the same time, international responsibility needs to be taken on the issues of the global food supply, the biomass-based supply of raw materials and energy, and climate and environmental protection. DIL is continually working to pave the way for bio-based food production. As part of these efforts, natural resources shall be used and new ways of using renewable raw materials are to be developed. This will be made possible through the development of new, bio-based production processes. This objective not only serves to preserve our living space, but also improves product quality and is good for our health. The division has its own research projects planned for 2013.\n\nThe quality of food is determined by their structure. Therefore, an optimization of food structures leads an improvement of long-term quality of food. Based on test on structure generation and the interdependency with the quality of food the DIL tries to increase the shelf-life of products.\n\nWhen the Bio-Economy division was founded in 2012, it was accompanied by an extensive realignment and expansion of the research platforms. As a result, meat technology was removed from the Process Technology business division, the biotechnology focus was placed on enzyme and raw material research, and a new research platform was founded for feed technology. Together, these three areas comprise the Protein Technology research division. These newly created capacities are the key to an integrated approach to global problems in the area of Bio-Economy.\n\nNew application areas for robotics are being researched, far away from the traditional image of robotics where robots are mostly used for packing processes. Safe and hygienic handling of unpacked and packed food products are main highlight in this field. Through cooperation with food producers, robotics engineers and manufacturers the DIL establishes new functions for robotics.\n\nThe optimization of existing (production) processes always increases the competitiveness of the entrepreneur. The implementation of advanced process analysis into existing systems allows the performance of the plants to be optimized further. On a daily basis sensor technology provides new and improved options for the collection of measuring data. Processing data can consequently be compiled with much higher precision and at shorter intervals. This allows the optimization of production parameters such as quality, pieces/minute, reject rate – no matter whether it's anticipatory (e.g. by assessing the products introduced into the process) or downstream (e.g. control at the end of each process stage). This is where the value of the information becomes most visible. Many production plants are still not yet operating to their performance capabilities. There are many possibilities for process analysis to further improve the quality achieved so far.\n\nThe \"Lower Saxonian competency center for the food industry\" (NieKE) is a network of cooperation between the German Institute of Food Technologies, \"The Institute of Spatial Analysis and Planning in Areas of Intensive Agriculture\" (ISPA) of the University of Vechta, DIALOG - \"The Center for Transfer of Information and Technology\" at the University of Oldenburg, \"the Competency Center Functional Food\" (KKF) and \"The Research and Technology Contact Center\" (unitransfer) at the University of Hanover.\nThe aims of this cooperative network are knowledge sharing between the members and the support of small and medium-sized enterprises.\n\nIn May 2009 the University of Applied Sciences in Osnabrück and the DIL signed an agreement for a long-term cooperation for future research and education. As result of this cooperation a statewide new academic course was established for the fall semester 2008; the Bachelor of Engineering “Food Production Engineering and Business”. The German Institute of Food Technologies functions as one of the main coordinating bodies.\n\nAlong with the University of Groningen, the Netherlands and the Netherlands Organisation for Applied Scientific Research (TNO), the DIL founded the ICCF (International Competence Center Food and Research Development) as research collaboration. This project was established to link closer cooperation and integration between the German and Dutch parts of the Ems Dollart Region.\nThe German Institute of Food Technologies operates worldwide with customers from Japan, China, India, Australia and the USA. Among DIL's customers are global players such as Mars, General Mills and Barilla.\n\nHighTech Europe is an initiative of European research organizations, industrial associations and enterprises with the goal of setting up a European Institute of Food Technology in order to share competences and knowledge of research available in Europe and to strengthen the resident industry for global competition. Long-term goal of the network of excellence is to achieve a long-lasting integration of the European R&D activities into high-tech food processing.\n\nThe network is coordinated by the DIL and sponsored by the 7th EU Framework Programme EU research sponsoring. Currently, the network counts 22 members, 21 being from Europe and one from Oceania. Project aims have been implemented by creating and applying various knowledge transfer tools. Network partners identified potential innovations from the biotechnology, nanotechnology, information- and communication technology and other areas to be used by food industry especially small and medium enterprises. Result is a unique database for the food processing sector, the Food Technology Innovation Portal (www.foodtech-portal.eu). Further results and network activities are available on the project website.\n\n"}
{"id": "57506816", "url": "https://en.wikipedia.org/wiki?curid=57506816", "title": "Hall circles", "text": "Hall circles\n\nHall circles (also known as M-circles and N-circles) are a graphical tool in control theory used to obtain values of a closed-loop transfer function from the Nyquist plot (or the Nichols plot) of the associated open-loop transfer function. Hall circles have been introduced in control theory by Albert C. Hall in his thesis.\n\nConsider a closed-loop linear control system with open-loop transfer function given by transfer function formula_1 and with a unit gain in the feedback loop. The closed-loop transfer function is given by formula_2. \n\nTo check the stability of \"T\"(\"s\"), it is possible to use the Nyquist stability criterion with the Nyquist plot of the open-loop transfer function \"G\"(\"s\"). Note, however, that only the Nyquist plot of \"G\"(\"s\") does not give the actual values of \"T\"(\"s\"). To get this information from the G(s)-plane, Hall proposed to construct the locus of points in the \"G\"(\"s\")-plane such that \"T\"(\"s\") has constant magnitude and the also the locus of points in the \"G\"(\"s\")-plane such that \"T\"(\"s\") has constant phase angle.\n\nGiven a positive real value \"M\" representing a fixed magnitude, and denoting G(s) by \"z\", the points satisfying formula_3are given by the points \"z\" in the \"G\"(\"s\")-plane such that the ratio of the distance between \"z\" and 0 and the distance between \"z\" and -1 is equal to \"M\". The points \"z\" satisfying this locus condition are circles of Apollonius, and this locus is known in the context of control systems as \"M-circles\".\n\nGiven a positive real value \"N\" representing a phase angle, the points satisfying formula_4are given by the points z in the \"G\"(\"s\")-plane such that the angle between -1 and z and the angle between 0 and z is constant. In other words, the angle opposed to the line segment between -1 and 0 must be constant. This implies that the points z satisfying this locus condition are arcs of circles, and this locus is known in the context of control systems as \"N-circles\".\n\nTo use the Hall circles, a plot of M and N circles is done over the Nyquist plot of the open-loop transfer function. The points of the intersection between these graphics give the corresponding value of the closed-loop transfer function.\n\nHall circles are also used with the Nichols plot and in this setting, are also known as Nichols chart. Rather than overlaying directly the Hall circles over the Nichols plot, the points of the circles are transferred to a new coordinate system where the ordinate is given by formula_5 and the abscissa is given by formula_6. The advantage of using Nichols chart is that adjusting the gain of the open loop transfer function directly reflects in up and down translation of the Nichols plot in the chart.\n\n\n"}
{"id": "39198232", "url": "https://en.wikipedia.org/wiki?curid=39198232", "title": "High-field domain", "text": "High-field domain\n\nA high-field domain is a band of elevated field orthogonal to the equi-current lines and seen in photoconductive and monochromatic light at the band edge as dark band was discovered by Böer, using the Franz-Keldysh effect. Such domains must appear whenever the conductivity decreases stronger than linearly. This can be caused by the field dependence of the carrier density, as observed in copper-doped caused by Frenkel Poole excitation of holes, causing faster electron recombination, known as field quenching. These high-field domains, now referred to as Böer domains, or by field dependence of the mobility, caused by excitation of electrons into higher conduction bands with lower mobility as observed in , called the Gunn effect. The high-field domains can be identified by periodical field oscillations between high (the domain) and low values, as shown in Fig. 1.\nMany other crystals show such domains by typical current oscillations. The high-field domains in copper doped can be easily observed by the Franz-Keldysh effect as stationary, adjacent to the cathode or moving. These are analyzed as another example below.\n\nTheory: Stationary high-field domains can be analyzed from the transport- and Poisson equations:\n\nformula_1 and formula_2\n\nThe projection of any solution curves into an arbitrary plane can be filled with direction arrows at any point of this plane. Two auxiliary curves for which , called and called divide this plane into four quadrants with the same type of directions. This is shown in Fig. 2(left) in a double logarithmic representation.\nAny solution of an -type semiconductor with blocking cathode must start at the boundary density that is below the density in the bulk, and approaches the singular point at which , that is in the bulk where both and are constant. The solution curve represents a Schottky-blocking contact as shown in Fig. 2(B), curve (a).\nWhen decreases at higher fields due to field-quenching cause by Poole-Frenkel excitation of holes from Coulomb attractive hole traps, that consequently enhances electron recombination through recombination centers, and thereby deforms the curve at higher fields as shown in Fig. 2(B). When the bias is increased the current curve is shifted upwards and to the right, and when it crosses again, it produces a second singular point II. With further increased bias this singular point II reaches the value of the boundary density , and the solution curve changes from a monotonic increasing Schottky-solution, to a high-field domain, curve (b): that remains constant near the cathode, and then changes within a few Debye lengths to approach the constant value in the bulk, near the singular point I. The width of the domain increases with bias (Fig. 3a), while the current remains constant (Fig. 3c). The domain is visible as dark part in the transmission picture through the platelett, extending from the cathode as shown in Fig. 3a. The field in the domain can be obtained from the slope of the domain that increases with bias (Fig. 3b).\n\nWhen, with further increased bias the domain fills the entire sample,then it flips to an anode adjacent high-field domain (Fig. 4b). The field at the cathode is now much higher than for the cathode adjacent domain (Fig. 4b and c), while the current still remains essentially constant (Fig. 4c).\n\nHigh-field domains to determine the work function of blocking contacts\n\nSince the high field domain starts at the electron density given by the work-function at the cathode and pulls the Schottky barrier open to a constant field in the domain, this work function can be determined precisely, and it can be used as a tool to determine the changes of the work function, as it varies depending on external parameters. As an example, it can be shown that it depends on the optical excitation in a photoconductor (See Fig. 5).\nHigh-field domains as tools to measure the electron density in the field quenched branch and of the electron mobility as a function of the temperature\nThe high-field domain is determined by the boundary density at the cathode and the field within the domain. A shadow band in front of the cathode acts as a pseudo-cathode, as it reduces the electron density within the shadow (Fig. 6). This can be used as an experimental tool to change the boundary density as a function of the light intensity within the shadow.\n\nThis permits to measure directly the electron density in the field-quenched range, using different pseudo-electron densities, causing a shift of the singular point, and measuring the domain field.\n\nThe electron Hall mobility can be measured by placing the platelett in a magnet and applying a bias sufficient to create a high-field domain. When the domain is extended to include the Hall electrodes one can determine the Hall mobility within the domain. Different fields in the high-field domain are achieved by using different samples or different cathode metals.\n\nStationary domains to explain efficiency improvement of solar cells with a thin cover layer\n\nThe application of a 200Å thick copper doped layer of on top of a typically 2 μm thick solar cell, increases the open circuit voltage substantially, so that it can reach the theoretical limit of the band gap of the -emitter when extrapolated to 0 .\nThis improvement can be explained by limiting the field in the side of the junction when it reaches the critical value for a high-field domain to appear and thereby limits the maximum junction field to the domain field of typical 50 . This field is below a field in which electron leaking from the into occurs, resulting in an increase of the open circuit voltage and thereby an increase of the solar cell conversion efficiency.\n\nMoving high-field domains in copper-doped with a small circular cathode\n\nThe domains started from the cathode, separates and, with increasing bias increases its radius. When the anode is reached the ring disappeared and a new domain grows from the cathode. The process repeated itself with a period of 10 sec (Fig. 7).\nSuch undeformed moving domains in crystals with slit electrodes are bands parallel to the electrodes, and seen by the oscillating field, when plotted in a diagram give the optical impression of bifurcation (Fig. 8).\n\nSuch moving high-field domains are measured in p-Ge with (a) local voltage (b) field- and (c) carrier density oscillation (Fig. 9).\nUndeformed moving high-field domains and domains with deformation (Chaos) are observed in many other crystals, and also in nanocrystals or superlatices. However, because of the small size they can only be analyzed by the changing shape of the current-voltage characteristics.\n\nBöer domains\n\nThe high-field domains were renamed Böer domains at the 50th anniversary of their discovery.\n\nThe Benefits of the High-Field Domains\n\nCopper-doped photoconductive CdS shows high-field domains when at sufficient bias and with blocking contacts these domains remain attached to the contacts. The field within the domain is constant and the current is by drift only. With increased bias the domain width increases. When it reaches both electrodes the entire crystal becomes space-charge free (this is one more example at which an interaction vanishes: for the interaction of electrons with phonons, superconductivity results; from the interaction of photons with phonons, lasers can be produced). This gives the opportunity to measure the spectral distribution of defect levels free of interaction of the broadening electric field surrounding the defects. A first example is shown by the extremely sharp quenching spectrum of a CdS crystal that has been inverted to become p-type with a high-field-domain (Fig. 10)\n\nAnother benefit of the high-field domain is the direct connection of any p-type emitter through a copper-doped thin CdS layer directly to an electron-blocking electrode through which the holes are extracted, and the open circuit voltage is increased to approach the theoretical limit of the band gap or the emitter at 0K. Because in the CdS the hole current is now carried by drift only, we can finally draw the band model of a typical solar cell. e.g. the CdS/CdTe cell as given in Fig. 11.\nFor the first time we can replace the estimate of the band connection of abrupt hetero-junctions from the difference of electron affinities by calculating it from the continuity of the majority carrier current, with the only small discontinuity left by the difference of the effective masses of the carriers at the different carrier bands.\n"}
{"id": "22754200", "url": "https://en.wikipedia.org/wiki?curid=22754200", "title": "IPtronics", "text": "IPtronics\n\nIPtronics was a fabless semiconductor company headquartered in Copenhagen, Denmark. Its products include integrated circuits for parallel optical interconnect applications intended for the computer, storage and communication industries. IPtronics' design center is certified by STMicroelectronics, which is also their semiconductor foundry partner. In June 2013, IPtronics was acquired by Mellanox Technologies.\n\nIPtronics was founded in 2003 and built up by former directors, managers and engineers from Giga A/S, which was acquired by Intel Corporation in 2000 for $1.25 billion USD. On June 4, 2013, it was announced that IPtronics was acquired by Mellanox Technologies at a total cash purchase price of approximately $47.5 million, subject to certain adjustments.\n\nThree former Giga employees, Niels Finseth, Steen Bak Christensen, and Eivind Johansen, co-founded IPtronics. Giga A/S specialized in products for telecommunication and data communication applications, as for example OC-48 and OC-192. Finseth was previously an engineering manager, responsible for all 10 Gbit/s IC product development. Mr. Christensen was also previously an engineering manager, responsible for all 2.5 Gbit/s development. Johansen was a co-founder of Giga (1987) as well, serving as the technical director until the acquisition by Intel, followed by a CTO position at Intel's Optical Component Division (OCD) and being appointed Intel Fellow in 2001, a corporate VP position for his technical leadership in optical communication. In May 2004, Henning Lysdal was recruited as COO, previously high-speed PHY development manager at Intel OCD. Lysdal later became VP of engineering after hiring a dedicated director of operations. Two former colleagues from Giga/Intel, who were at that time CEOs in their respective Danish electronic companies, were recruited to IPtronics. Steen Gundersen came from a position as the CEO of Alight Technologies and Jesper Wolf Bek came from a position as the CEO of Kaleido Technology.\n\nIn the beginning, the founders worked together in the garage of Steen Bak Christensen in Roskilde. As the first employee was hired in February 2005, IPtronics moved into new premises outside Roskilde.\nGiga had customer support.\n\nIn 2006, Intel closed its Danish office, which resulted in even more new electronics start-ups in Copenhagen metropolitan area as well as many employees joining already existing companies such as IPtronics. However, several new additions to the staff from 2008 and beyond have a different background than from Giga or Intel, such as Navid Ostadian-Binai.\nDuring 2006, the company appointed Jørgen Bardenfleth as chairman of the board of directors. Bardenfleth is the country general manager of Microsoft Denmark. In November 2011, IPtronics announced Martin Rofheart as chairman.\n\nThe company's first customer was CERN, the European Organization for Nuclear Research. The IPtronics chips were produced with TriQuint Semiconductor's GaAs foundry process technology. These devices are being used as front-end electronics for Resistive Plate Chambers (RPC), a gaseous\nparticle detector capable of sub-nanosecond time resolution on very large areas.\n\nIn October 2005, IPtronics started developing optical interconnects in a collaboration with NEC Corporation.\n\nDuring the summer of 2008, IPtronics opened its North American office in Silicon Valley.\n\nIn April 2010, IPtronics joined the InfiniBand Trade Association, which promoted InfiniBand technology. \nIPtronics announced it would offer low power and high volume products. IPtronics later also joined the associations, Optical Internetworking Forum and Peripheral Component Interconnect Special Interest Group.\n\nThis technology enables parallel optical interconnect systems that computer manufactures have begun to adopt in order to overcome the physical constraints from using copper-based connections over high speed interfaces and backplanes. Parallel optics is introduced to be able to simultaneously transmit and receive data at high bandwidths over multiple fibers, initially implemented in supercomputers and servers followed by an upcoming introduction into consumer electronics. In June 2011, IPtronics announced it had reached a shipment milestone, passing 1 million ICs, and the company states the majority is shipped to Asia.\n\nLate 2007, IPtronics started shipping 4-channel and 12-channel chipsets capable of operating at a minimum of 10 Gbit/s per channel, primarily targeting data center and supercomputer applications. A chipset consists of a VCSEL driver and a Transimpedance amplifier (TIA). The company also states to have qualified solder bump versions of the same two chipsets to be used for flip chip mounting, the preferred assembly technology in high-volume production. Early 2010, IPtronics announced 16 Gbit/s versions of their 4- and 12-channel VCSEL drivers and TIAs.\n\nThe company announced in June 2010 it would address the market for \"lower rates\", especially driven by HDMI cables, at higher volumes, and a lower cost structure than active optical cables for InfiniBand. At the 2010 China International Optoelectronic Exposition and European Conference and Exhibition on Optical Communication, the company presented a demonstration of their optical HDMI by transmitting signals from a Blu-ray DVD player to a LCD television, targeted for 2011. In January 2011, IPtronics released a new 4-channel chipset for pluggable module applications, and the company claims to have reduced the power consumption compared to their first-generation 4-channel chipset.\n\nIn March 2012, IPtronics announced 28 Gbit/s/channel parts.\n\nIPtronics first announced it would develop Thunderbolt technology (original code-name Light Peak) in 2009. Thunderbolt was brought to market by Apple in February 2011, and Light Peak is Intel's code-name for the new high-speed cable technology designed to connect consumer electronic devices to each other using copper or optical interconnect. IPtronics is a supplier of driver and receiver ICs that go into the optical module, performing the conversion from electricity to light and vice versa, using miniature lasers and photodetectors. The ICs from IPtronics are dual-channel, where each channel operates at a minimum of 10 Gbit/s.\n\nIn October 2010, the company announced a new silicon revision. They claim the same cost competitiveness, enabling optical module and -cable applications such as Thunderbolt implementation, though now also single channel optical links up to 14 Gbit/s. Besides Thunderbolt, the devices are claimed to be used for data center and other kinds of cables.\n"}
{"id": "19634316", "url": "https://en.wikipedia.org/wiki?curid=19634316", "title": "Institute of Electronics, Information and Communication Engineers", "text": "Institute of Electronics, Information and Communication Engineers\n\nThe (IEICE) is a Japanese institute specializing in the areas of electronic, information and communication engineering and associated fields. It was established in May, 1917. Its headquarters are located in Tokyo, Japan.\n\n"}
{"id": "7321060", "url": "https://en.wikipedia.org/wiki?curid=7321060", "title": "Interferometric synthetic-aperture radar", "text": "Interferometric synthetic-aperture radar\n\nInterferometric synthetic aperture radar, abbreviated InSAR (or deprecated IfSAR), is a radar technique used in geodesy and remote sensing. This geodetic method uses two or more synthetic aperture radar (SAR) images to generate maps of surface deformation or digital elevation, using differences in the phase of the waves returning to the satellite or aircraft. The technique can potentially measure millimetre-scale changes in deformation over spans of days to years. It has applications for geophysical monitoring of natural hazards, for example earthquakes, volcanoes and landslides, and in structural engineering, in particular monitoring of subsidence and structural stability.\n\nSynthetic aperture radar (SAR) is a form of radar in which sophisticated processing of radar data is used to produce a very narrow effective beam. It can be used to form images of relatively immobile targets; moving targets can be blurred or displaced in the formed images. SAR is a form of active remote sensing – the antenna transmits radiation that is reflected from the image area, as opposed to passive sensing, where the reflection is detected from ambient illumination. SAR image acquisition is therefore independent of natural illumination and images can be taken at night. Radar uses electromagnetic radiation at microwave frequencies; the atmospheric absorption at typical radar wavelengths is very low, meaning observations are not prevented by cloud cover.\n\nSAR makes use of the amplitude and the absolute phase of the return signal data. In contrast, interferometry uses differential phase of the reflected radiation, either from multiple passes along the same trajectory and/or from multiple displaced phase centers (antennas) on a single pass. Since the outgoing wave is produced by the satellite, the phase is known, and can be compared to the phase of the return signal. The phase of the return wave depends on the distance to the ground, since the path length to the ground and back will consist of a number of whole wavelengths plus some fraction of a wavelength. This is observable as a phase difference or phase shift in the returning wave. The total distance to the satellite (i.e., the number of whole wavelengths) is known based on the time that it takes for the energy to make the round trip back to the satellite—but it is the extra fraction of a wavelength that is of particular interest and is measured to great accuracy.\n\nIn practice, the phase of the return signal is affected by several factors, which together can make the absolute phase return in any SAR data collection essentially arbitrary, with no correlation from pixel to pixel. To get any useful information from the phase, some of these effects must be isolated and removed. Interferometry uses two images of the same area taken from the same position (or, for topographic applications, slightly different positions) and finds the difference in phase between them, producing an image known as an interferogram. This is measured in radians of phase difference and, because of the cyclic nature of phase, is recorded as repeating fringes that each represent a full 2π cycle.\n\nThe most important factor affecting the phase is the interaction with the ground surface. The phase of the wave may change on reflection, depending on the properties of the material. The reflected signal back from any one pixel is the summed contribution to the phase from many smaller 'targets' in that ground area, each with different dielectric properties and distances from the satellite, meaning the returned signal is arbitrary and completely uncorrelated with that from adjacent pixels. Importantly though, it is consistent – provided nothing on the ground changes the contributions from each target should sum identically each time, and hence be removed from the interferogram.\n\nOnce the ground effects have been removed, the major signal present in the interferogram is a contribution from orbital effects. For interferometry to work, the satellites must be as close as possible to the same spatial position when the images are acquired. This means that images from two satellite platforms with different orbits cannot be compared, and for a given satellite data from the same orbital track must be used. In practice the perpendicular distance between them, known as the \"baseline\", is often known to within a few centimetres but can only be controlled on a scale of tens to hundreds of metres. This slight difference causes a regular difference in phase that changes smoothly across the interferogram and can be modelled and removed.\n\nThe slight difference in satellite position also alters the distortion caused by topography, meaning an extra phase difference is introduced by a stereoscopic effect. The longer the baseline, the smaller the topographic height needed to produce a fringe of phase change – known as the \"altitude of ambiguity\". This effect can be exploited to calculate the topographic height, and used to produce a digital elevation model (DEM).\n\nIf the height of the topography is already known, the topographic phase contribution can be calculated and removed. This has traditionally been done in two ways. In the \"two-pass\" method, elevation data from an externally derived DEM is used in conjunction with the orbital information to calculate the phase contribution. In the \"three-pass\" method two images acquired a short time apart are used to create an interferogram, which is assumed to have no deformation signal and therefore represent the topographic contribution. This interferogram is then subtracted from a third image with a longer time separation to give the residual phase due to deformation.\n\nOnce the ground, orbital and topographic contributions have been removed the interferogram contains the deformation signal, along with any remaining noise (see Difficulties below). The signal measured in the interferogram represents the change in phase caused by an increase or decrease in distance from the ground pixel to the satellite, therefore only the component of the ground motion parallel to the satellite line of sight vector will cause a phase difference to be observed. For sensors like ERS with a small incidence angle this measures vertical motion well, but is insensitive to horizontal motion perpendicular to the line of sight (approximately north-south). It also means that vertical motion and components of horizontal motion parallel to the plane of the line of sight (approximately east-west) cannot be separately resolved.\n\nOne fringe of phase difference is generated by a ground motion of half the radar wavelength, since this corresponds to a whole wavelength increase in the two-way travel distance. Phase shifts are only resolvable relative to other points in the interferogram. Absolute deformation can be inferred by assuming one area in the interferogram (for example a point away from expected deformation sources) experienced no deformation, or by using a ground control (GPS or similar) to establish the absolute movement of a point.\n\nA variety of factors govern the choice of images which can be used for interferometry. The simplest is data availability – radar instruments used for interferometry commonly don't operate continuously, acquiring data only when programmed to do so. For future requirements it may be possible to request acquisition of data, but for many areas of the world archived data may be sparse. Data availability is further constrained by baseline criteria. Availability of a suitable DEM may also be a factor for two-pass InSAR; commonly 90 m SRTM data may be available for many areas, but at high latitudes or in areas of poor coverage alternative datasets must be found.\n\nA fundamental requirement of the removal of the ground signal is that the sum of phase contributions from the individual targets within the pixel remains constant between the two images and is completely removed. However, there are several factors that can cause this criterion to fail. Firstly the two images must be accurately co-registered to a sub-pixel level to ensure that the same ground targets are contributing to that pixel. There is also a geometric constraint on the maximum length of the baseline – the difference in viewing angles must not cause phase to change over the width of one pixel by more than a wavelength. The effects of topography also influence the condition, and baselines need to be shorter if terrain gradients are high. Where co-registration is poor or the maximum baseline is exceeded the pixel phase will become incoherent – the phase becomes essentially random from pixel to pixel rather than varying smoothly, and the area appears noisy. This is also true for anything else that changes the contributions to the phase within each pixel, for example changes to the ground targets in each pixel caused by vegetation growth, landslides, agriculture or snow cover.\n\nAnother source of error present in most interferograms is caused by the propagation of the waves through the atmosphere. If the wave travelled through a vacuum it should theoretically be possible (subject to sufficient accuracy of timing) to use the two-way travel-time of the wave in combination with the phase to calculate the exact distance to the ground. However, the velocity of the wave through the atmosphere is lower than the speed of light in a vacuum, and depends on air temperature, pressure and the partial pressure of water vapour. It is this unknown phase delay that prevents the integer number of wavelengths being calculated. If the atmosphere was horizontally homogeneous over the length scale of an interferogram and vertically over that of the topography then the effect would simply be a constant phase difference between the two images which, since phase difference is measured relative to other points in the interferogram, would not contribute to the signal. However, the atmosphere is laterally heterogeneous on length scales both larger and smaller than typical deformation signals. This spurious signal can appear completely unrelated to the surface features of the image, however, in other cases the atmospheric phase delay is caused by vertical inhomogeneity at low altitudes and this may result in fringes appearing to correspond with the topography.\n\nPersistent or Permanent Scatterer techniques are a relatively recent development from conventional InSAR, and rely on studying pixels which remain coherent over a sequence of interferograms. In 1999, researchers at Politecnico di Milano, Italy, developed a new multi-image approach in which one searches the stack of images for objects on the ground providing consistent and stable radar reflections back to the satellite. These objects could be the size of a pixel or, more commonly, sub-pixel sized, and are present in every image in the stack. That specific implementation is patented.\n\nSome research centres and companies, were inspired to develop variations of their own algorithms which would also overcome InSAR's limitations. In scientific literature, these techniques are collectively referred to as Persistent Scatterer Interferometry or PSI techniques. The term Persistent Scatterer Interferometry (PSI) was proposed by European Space Agency (ESA) to define the second generation of radar interferometry techniques. This term is nowadays commonly accepted by scientific and the end user community.\n\nCommonly such techniques are most useful in urban areas with lots of permanent structures, for example the PSI studies of European geohazard sites undertaken by the Terrafirma project. The Terrafirma project provides a ground motion hazard information service, distributed throughout Europe via national geological surveys and institutions. The objective of this service is to help save lives, improve safety, and reduce economic loss through the use of state-of-the-art PSI information. Over the last 9 years this service has supplied information relating to urban subsidence and uplift, slope stability and landslides, seismic and volcanic deformation, coastlines and flood plains.\n\nThe processing chain used to produce interferograms varies according to the software used and the precise application but will usually include some combination of the following steps.\n\nTwo SAR images are required to produce an interferogram; these may be obtained pre-processed, or produced from raw data by the user prior to InSAR processing. The two images must first be co-registered, using a correlation procedure to find the offset and difference in geometry between the two amplitude images. One SAR image is then re-sampled to match the geometry of the other, meaning each pixel represents the same ground area in both images. The interferogram is then formed by cross-multiplication of each pixel in the two images, and the interferometric phase due to the curvature of the Earth is removed, a process referred to as flattening. For deformation applications a DEM can be used in conjunction with the baseline data to simulate the contribution of the topography to the interferometric phase, this can then be removed from the interferogram.\n\nOnce the basic interferogram has been produced, it is commonly filtered using an adaptive power-spectrum filter to amplify the phase signal. For most quantitative applications the consecutive fringes present in the interferogram will then have to be \"unwrapped\", which involves interpolating over the 0 to 2π phase jumps to produce a continuous deformation field. At some point, before or after unwrapping, incoherent areas of the image may be masked out. The final processing stage involves geocoding the image, which resamples the interferogram from the acquisition geometry (related to direction of satellite path) into the desired geographic projection.\n\nEarly exploitation of satellite-based InSAR included use of Seasat data in the 1980s, but the potential of the technique was expanded in the 1990s, with the launch of ERS-1 (1991), JERS-1 (1992), RADARSAT-1 and ERS-2 (1995). These platforms provided the stable, well-defined orbits and short baselines necessary for InSAR. More recently, the 11-day NASA STS-99 mission in February 2000 used a SAR antenna mounted on the space shuttle to gather data for the Shuttle Radar Topography Mission. In 2002 ESA launched the ASAR instrument, designed as a successor to ERS, aboard Envisat. While the majority of InSAR to date has utilised the C-band sensors, recent missions such as the ALOS PALSAR, TerraSAR-X and COSMO-SkyMed are expanding the available data in the L- and X-band.\n\nMost recently, ESA launched Sentinel-1A and Sentinel-1B – two C-band sensors. Together, they provide InSAR coverage on a global scale and on a 6-day repeat cycle.\n\nAirborne InSAR data acquisition systems are built by companies such as the American Intermap, the German AeroSensing, and the Brazilian OrbiSat.\n\nTerrestrial or ground-based SAR Interferometry (GBInSAR or TInSAR) is a remote sensing technique for the displacement monitoring of slopes, rock scarps, volcanoes, landslides, buildings, infrastructures etc. This technique is based on the same operational principles of the Satellite SAR Interferometry, but the Synthetic Aperture of the Radar (SAR) is obtained by an antenna moving on a rail instead of a satellite moving around an orbit. SAR technique allows 2D radar image of the investigated scenario to be achieved, with a high range resolution (along the instrumental line of sight) and cross-range resolution (along the scan direction). Two antennas respectively emit and receive microwave signals and, by calculating the phase difference between two measurements taken in two different times, it is possible to compute the displacement of all the pixels of the SAR image. The accuracy in the displacement measurement is of the same order of magnitude as the EM wavelength and depends also on the specific local and atmospheric conditions.\n\nInSAR can be used to measure tectonic deformation, for example ground movements due to earthquakes. It was first used for the 1992 Landers earthquake, but has since been utilised extensively for a wide variety of earthquakes all over the world. In particular the 1999 Izmit and 2003 Bam earthquakes were extensively studied. InSAR can also be used to monitor creep and strain accumulation on faults.\n\nInSAR can be used in a variety of volcanic settings, including deformation associated with eruptions, inter-eruption strain caused by changes in magma distribution at depth, gravitational spreading of volcanic edifices, and volcano-tectonic deformation signals. Early work on volcanic InSAR included studies on Mount Etna, and Kilauea, with many more volcanoes being studied as the field developed. The technique is now widely used for academic research into volcanic deformation, although its use as an operational monitoring technique for volcano observatories has been limited by issues such as orbital repeat times, lack of archived data, coherence and atmospheric errors. Recently InSAR has been used to study rifting processes in Ethiopia.\n\nGround subsidence from a variety of causes has been successfully measured using InSAR, in particular subsidence caused by oil or water extraction from underground reservoirs, subsurface mining and collapse of old mines. Thus, InSAR has become an indispensable tool to satisfactorily address many subsidence studies. Tomás et al. performed a cost analysis that allowed to identify the strongest points of InSAR techniques compared with other conventional techniques: (1) higher data acquisition frequency and spatial coverage; and (2) lower annual cost per measurement point and per square kilometre.\n\nAlthough InSAR technique can present some limitations when applied to landslides, it can also be used for monitoring landscape features such as landslides.\n\nGlacial motion and deformation have been successfully measured using satellite interferometry. The technique allows remote, high-resolution measurement of changes in glacial structure, ice flow, and shifts in ice dynamics, all of which agree closely with ground observations.\n\nInSAR can also be used to monitor the stability of built structures. Especially Very High Resolution SAR data (such as derived from the TerraSAR-X StripMap mode or COSMO-Skymed HIMAGE mode) are suitable for this task. InSAR is used for monitoring highway and railway settlements, dike stability, forensic engineering and many other uses.\n\nInterferograms can be used to produce digital elevation maps (DEMs) using the stereoscopic effect caused by slight differences in observation position between the two images. When using two images produced by the same sensor with a separation in time, it must be assumed other phase contributions (for example from deformation or atmospheric effects) are minimal. In 1995 the two ERS satellites flew in tandem with a one-day separation for this purpose. A second approach is to use two antennas mounted some distance apart on the same platform, and acquire the images at the same time, which ensures no atmospheric or deformation signals are present. This approach was followed by NASA's SRTM mission aboard the space shuttle in 2000. InSAR-derived DEMs can be used for later two-pass deformation studies, or for use in other geophysical applications.\n\n\n\n"}
{"id": "26552149", "url": "https://en.wikipedia.org/wiki?curid=26552149", "title": "International Association for Engineering and Food", "text": "International Association for Engineering and Food\n\nThe International Association for Engineering and Food (IAEF) is a global body of around 25 delegates representing professional engineering societies including food engineering activities. This organization is mainly in charge of identifying the sites for ICEF events. ICEF, International Congress on Engineering and Food, is the most important congress in the field of food engineering. It is usually held in a four year cycle at different locations. The other main assignment of IAEF is to elect its President who is the organiser of the next ICEF event.\n\n\n\n"}
{"id": "1198338", "url": "https://en.wikipedia.org/wiki?curid=1198338", "title": "Japan Remote Control", "text": "Japan Remote Control\n\nJapan Remote Control Co., Ltd. (日本遠隔制御株式会社; \"Nippon Enkaku Seigyo Kabushiki Gaisha\") (commonly called saurce Propo, source Racing, or hum) was a Japanese manufacturer of popular radio control devices including transmitters, receivers, servos, electronics, programmable robots and model aircraft. JR has ceased production of RC equipment. http://www.jramericas.com/news_feed.php?#newsid70\n\nUnique to JR Propo's radios is the company's patented ABC&W technology, or Automatic Blocking Circuit with Window. Simply put, this system rapidly reroutes incoming signals through a series of ever-smaller electronic \"windows,\" effectively blocking out spurious signals which cause radio \"glitching\". Any signal that does get through the system is filtered before being sent to the servos all without noticeable lag or delay. If the signal is unable to be processed, it is rejected, again without noticeable reaction.\n\nCompeting brands of radio control systems include, Spektrum RC, Sanwa, Futaba, Hitec, KO Propo, Acoms and Multiplex Modelsport.\n\n"}
{"id": "7494162", "url": "https://en.wikipedia.org/wiki?curid=7494162", "title": "KeeLoq", "text": "KeeLoq\n\nKeeLoq is a proprietary hardware-dedicated block cipher that uses a non-linear feedback shift register (NLFSR). The uni-directional command transfer protocol was designed by Frederick Bruwer of Nanoteq (Pty) Ltd., the cryptographic algorithm was created by Gideon Kuhn at the University of Pretoria, and the silicon implementation was by Willem Smit at Nanoteq Pty Ltd (South Africa) in the mid-1980s. KeeLoq was sold to Microchip Technology Inc in 1995 for $10 million. It is used in \"code hopping\" encoders and decoders such as NTQ105/106/115/125D/129D, HCS101/2XX/3XX/4XX/5XX and MCS31X2. KeeLoq is or was used in many remote keyless entry systems by such companies as Chrysler, Daewoo, Fiat, GM, Honda, Toyota, Volvo, Volkswagen Group, Clifford, Shurlok, and Jaguar.\n\nKeeLoq \"code hopping\" encoders encrypt a 0-filled 32-bit block with KeeLoq cipher to produce a 32-bit \"hopping code\". A 32-bit initialization vector is linearly added (XORed) to the 32 least significant bits of the key prior to encryption and after decryption.\n\nKeeLoq cipher accepts 64-bit keys and encrypts 32-bit blocks by executing its single-bit NLFSR for 528 rounds. The NLFSR feedback function is codice_1 or\nKeeLoq uses bits 1, 9, 20, 26 and 31 of the NLFSR state as its inputs during encryption and bits 0, 8, 19, 25 and 30 during decryption. Its output is linearly combined (XORed) with two of the bits of the NLFSR state (bits 0 and 16 on encryption and bits 31 and 15 on decryption) and with a key bit (bit 0 of the key state on encryption and bit 15 of the key state on decryption) and is fed back into the NLFSR state on every round.\n\nFor simplicity, individual \"code hopping\" implementations do not use cryptographic nonces, and clock drift excludes the possibility of using timestamping. This makes the protocol inherently vulnerable to replay attacks: For example, by jamming the channel while intercepting the code, a thief can obtain a code that may still be usable at a later stage. This sort of \"code grabber,\" while theoretically interesting, does not appear to be widely used by car thieves.\n\nA detailed description of an inexpensive prototype device designed and built by Samy Kamkar to exploit this technique appeared in 2015. The device about the size of a wallet could be concealed on or near a locked vehicle to capture a single keyless entry code to be used at a later time to unlock the vehicle. The device transmits a jamming signal to block the vehicle's reception of rolling code signals from the owner's fob, while recording these signals from both of his two attempts needed to unlock the vehicle. The recorded first code is forwarded to the vehicle only when the owner makes the second attempt, while the recorded second code is retained for future use. A demonstration was announced for DEF CON 23.\n\nKeeLoq was first cryptanalyzed by Andrey Bogdanov using sliding techniques and efficient linear approximations. Nicolas Courtois attacked KeeLoq using sliding and algebraic methods. The attacks by Bogdanov and Courtois do not pose any threat to the actual implementations that seem to be much more vulnerable to simple brute-force of the key space that is reduced in all the code-hopping implementations of the cipher known to date. Some KeeLoq \"code grabbers\" use FPGA-based devices to break KeeLoq-based keys by brute force within about two weeks due to the reduced key length in the real world implementations.\n\nIn 2007, researchers in the COSIC group at the university at Leuven, Belgium, (K.U.Leuven) in cooperation with colleagues from Israel found a new attack against the system.\nUsing the details of the algorithm that were leaked in 2006, the researchers started to analyze the weaknesses. After determining the part of the key common to cars of a specific model, the unique bits of the key can be cracked with only sniffed communication between the key and the car.\n\nMicrochip introduced in 1996 a version of KeeLoq ICs which use a 60-bit seed. If a 60-bit seed is being used, an attacker would require approximately 100 days of processing on a dedicated parallel brute force attacking machine before the system is broken.\n\nIn March 2008, researchers from the Chair for Embedded Security of Ruhr University Bochum, Germany, presented a complete break of remote keyless entry systems based on the KeeLoq RFID technology. Their attack works on all known car and building access control systems that rely on the KeeLoq cipher.\n\nThe attack by the Bochum team allows recovering the secret cryptographic keys embedded in both the receiver and the remote control. It is based on measuring the electric power consumption of a device during an encryption. Applying what is called side-channel analysis methods to the power traces, the researchers can extract the manufacturer key from the receivers, which can be regarded as a master key for generating valid keys for the remote controls of one particular manufacturer. Unlike the cryptanalytic attack described above which requires about 65536 chosen plaintext-ciphertext pairs and days of calculation on a PC to recover the key, the side-channel attack can also be applied to the so-called KeeLoq Code Hopping mode of operation (a.k.a. rolling code) that is widely used for keyless entry systems (cars, garages, buildings, etc.).\n\nThe most devastating practical consequence of the side-channel analysis is an attack in which an attacker, having previously learned the system's master key, can clone any legitimate encoder by intercepting only two messages from this encoder from a distance of up to . Another attack allows one to reset the internal counter of the receiver (garage door, car door, etc.), which makes it impossible for a legitimate user to open the door.\n\n"}
{"id": "12284919", "url": "https://en.wikipedia.org/wiki?curid=12284919", "title": "Mark 21 nuclear bomb", "text": "Mark 21 nuclear bomb\n\nThe Mark 21 nuclear bomb was a United States nuclear gravity bomb first produced in 1955. It was based on the TX-21 \"Shrimp\" prototype that had been detonated during the Castle Bravo test in March 1954. While most of the Operation Castle tests were intended to evaluate weapons intended for immediate stockpile, or which were already available for use as part of the Emergency Capability Program, Castle Bravo was intended to test a design which would drastically reduce the size and costs of the first generation of air-droppable atomic weapons (the Mk 14, Mk 17 & Mk 24). At long, in diameter, and weighing , the Mk-21 was half the length and one-third the weight of the Mk-17/24 weapons it replaced. Its minimum yield was specified at four megatons.\n\nQuantity production of the Mk-21 started in December 1955 and ran until July 1956. Three marks were produced; the Mk-21C was proof tested as the Operation Redwing Navajo shot, with a yield of 4.5 megatons. Starting in June 1957 all Mk-21 bombs were converted to the more powerful Mk-36, which was removed from service in 1962.\n\n"}
{"id": "41355", "url": "https://en.wikipedia.org/wiki?curid=41355", "title": "Master station", "text": "Master station\n\nIn telecommunication, a master station is a station that controls or coordinates the activities of other stations in the system.\n\nExamples:\n\nIn data transmission, a master station can be set to not wait for a reply from a slave station after transmitting each message or transmission block. In this case the station is said to be in \"continuous operation\".\n"}
{"id": "579750", "url": "https://en.wikipedia.org/wiki?curid=579750", "title": "Methanogenesis", "text": "Methanogenesis\n\nMethanogenesis or biomethanation is the formation of methane by microbes known as methanogens. Organisms capable of producing methane have been identified only from the domain Archaea, a group phylogenetically distinct from both eukaryotes and bacteria, although many live in close association with anaerobic bacteria. The production of methane is an important and widespread form of microbial metabolism. In anoxic environments, it is the final step in the decomposition of biomass. Methanogenesis is responsible for significant amounts of natural gas accumulations, the remainder being thermogenic.\n\nMethanogenesis in microbes is a form of anaerobic respiration. Methanogens do not use oxygen to respire; in fact, oxygen inhibits the growth of methanogens. The terminal electron acceptor in methanogenesis is not oxygen, but carbon. The carbon can occur in a small number of organic compounds, all with low molecular weights. The two best described pathways involve the use of acetic acid or inorganic carbon dioxide as terminal electron acceptors:\n\nDuring anaerobic respiration of carbohydrates, H and acetate are formed in a ratio of 2:1 or lower, so H contributes only ca. 33% to methanogenesis, with acetate contributing the greater proportion. In some circumstances, for instance in the rumen, where acetate is largely absorbed into the bloodstream of the host, the contribution of H to methanogenesis is greater.\n\nHowever, depending on pH and temperature, methanogenesis has been shown to use carbon from other small organic compounds, such as formic acid (formate), methanol, methylamines, tetramethylammonium, dimethyl sulfide, and methanethiol. The catabolism of the methyl compounds is mediated by methyl transferases to give methyl coenzyme M.\n\nThe biochemistry of methanogenesis involves the following coenzymes and cofactors: F420, coenzyme B, coenzyme M, methanofuran, and methanopterin.\n\nThe mechanism for the conversion of bond into methane involves a ternary complex of methyl coenzyme M and coenzyme B fit into a channel terminated by the axial site on nickel of the cofactor F430. One proposed mechanism invokes electron transfer from Ni(I) (to give Ni(II)), which initiates formation of . Coupling of the coenzyme M thiyl radical (RS) with HS coenzyme B releases a proton and re-reduces Ni(II) by one-electron, regenerating Ni(I).\n\nSome organisms can oxidize methane, functionally reversing the process of methanogenesis, also referred to as the anaerobic oxidation of methane (AOM). Organisms performing AOM have been found in multiple marine and freshwater environments including methane seeps, hydrothermal vents, coastal sediments and sulfate-methane transition zones. These organisms may accomplish reverse methanogenesis using a nickel-containing protein similar to methyl-coenzyme M reductase used by methanogenic archaea. Reverse methanogenesis occurs according to the reaction:\n\nMethanogenesis is the final step in the decay of organic matter. During the decay process, electron acceptors (such as oxygen, ferric iron, sulfate, and nitrate) become depleted, while hydrogen (H) and carbon dioxide accumulate. Light organics produced by fermentation also accumulate. During advanced stages of organic decay, all electron acceptors become depleted except carbon dioxide. Carbon dioxide is a product of most catabolic processes, so it is not depleted like other potential electron acceptors.\n\nOnly methanogenesis and fermentation can occur in the absence of electron acceptors other than carbon. Fermentation only allows the breakdown of larger organic compounds, and produces small organic compounds. Methanogenesis effectively removes the semi-final products of decay: hydrogen, small organics, and carbon dioxide. Without methanogenesis, a great deal of carbon (in the form of fermentation products) would accumulate in anaerobic environments.\n\nEnteric fermentation occurs in the gut of some animals, especially ruminants. In the rumen, anaerobic organisms, including methanogens, digest cellulose into forms nutritious to the animal. Without these microorganisms, animals such as cattle would not be able to consume grasses. The useful products of methanogenesis are absorbed by the gut, but methane is released from the animal mainly by belching (eructation). The average cow emits around 250 liters of methane per day. In this way, ruminants contribute about 25% of anthropogenic methane emissions. One method of methane production control in ruminants is by feeding them 3-nitrooxypropanol.\n\nSome humans produce flatus that contains methane. In one study of the feces of nine adults, five of the samples contained archaea capable of producing methane. Similar results are found in samples of gas obtained from within the rectum.\n\nEven among humans whose flatus does contain methane, the amount is in the range of 10% or less of the total amount of gas.\n\nMany experiments have suggested that leaf tissues of living plants emit methane. Other research has indicated that the plants are not actually generating methane; they are just absorbing methane from the soil and then emitting it through their leaf tissues.\n\nMethanogens are observed in anoxic soil environments, contributing to the degradation of organic matter. This organic matter may be placed by humans through landfill, buried as sediment on the bottom of lakes or oceans as sediments, and as residual organic matter from sediments that have formed into sedimentary rocks.\n\nAtmospheric methane is an important greenhouse gas with a global warming potential 25 times greater than carbon dioxide (averaged over 100 years), and methanogenesis in livestock and the decay of organic material is thus a considerable contributor to global warming. It may not be a net contributor in the sense that it works on organic material which used up atmospheric carbon dioxide when it was created, but its overall effect is to convert the carbon dioxide into methane which is a much more potent greenhouse gas.\n\nMethanogenesis can also be beneficially exploited, to treat organic waste, to produce useful compounds, and the methane can be collected and used as biogas, a fuel. It is the primary pathway whereby most organic matter disposed of via landfill is broken down.\n\nThe presence of atmospheric methane has a role in the scientific search for extra-terrestrial life. The justification is that methane in the atmosphere will eventually dissipate, unless something is replenishing it. If methane is detected (by using a spectrometer for example) this may indicate that life is, or recently was, present.\nThis was debated when methane was discovered in the Martian atmosphere by M.J. Mumma of NASA's Goddard Flight Center, and verified by the Mars Express Orbiter (2004) and in Titan's atmosphere by the Huygens probe (2005). This debate was furthered with the discovery of 'transient', 'spikes of methane' on Mars by the Curiosity Rover.\n\nIt is also argued that atmospheric methane can come from volcanoes or other fissures in the planet's crust and that without an isotopic signature, the origin or source may be difficult to identify.\n\nOn 13 April 2017, NASA confirmed that the dive of the Cassini orbiter spacecraft on 28 October 2015 discovered the Enceladus plume which has all the ingredients for methanogenesis-based life forms to feed from. Previous results, published in March 2015, suggested hot water is interacting with rock beneath the sea; the new findings support that conclusion and add that the rock appears to be reacting chemically. From these observations scientists have determined that nearly 98 percent of the gas in the plume is water, about 1 percent is hydrogen and the rest is a mixture of other molecules including carbon dioxide, methane and ammonia.\n\n"}
{"id": "42505078", "url": "https://en.wikipedia.org/wiki?curid=42505078", "title": "Moon Ribas", "text": "Moon Ribas\n\nMoon Ribas (born 24 May 1985) is a Spanish avant-garde artist and cyborg activist best known for developing and implanting an online seismic sensor in her elbow that allows her to feel earthquakes through vibrations. Her choreography works are based on the exploration of new movements developed by the addition of new senses or sensory extensions to the dancer. Since 2007, international media have described her as the world's first cyborg woman or the world's first female cyborg artist. She is the co-founder of the [[Cyborg Foundation]], an international organisation that encourages humans to become cyborgs and promotes cyborgism as an art movement and the co-founder of the Transpecies Society, an association that gives voice to people with non-human identities and offers the development of new senses and organs in community.\n\nMoon Ribas grew up in [[Mataró]] (Spain) and moved to England at the age of 18 where she studied experimental dance and graduated in choreography at [[Dartington College of Arts]] (England) and Movement Research at SNDO [[Theaterschool]] (Amsterdam). During her studies she began to explore the possibilities of sensory extensions by applying technology to her body.\n\nIn 2013, Moon developed a sensor that vibrates whenever there's an earthquake in the planet. The sensor, which is permanently attached to her elbow, vibrates in different levels depending on the intensity of each earthquakes and is wirelessly connected to online seismographs, which means she can feel earthquakes from all over the world regardless of where she is. Moon has been wearing the sensor permanently since March 2013 and has used her seismic sense to create dance pieces. \"Waiting for Earthquakes\" is a solo dance performance where the dancer stands still until an earthquake is felt. The choreography depends on the earthquakes felt during the duration of the performance and the intensity of the dancer's movements depend on the magnitude of each earthquake (which can be felt from 1.0 in the [[Richter Scale]]). If there are no earthquakes during the time of performance the dancer will not dance. The piece was premiered on 28 March 2013 at Nau Ivanow, Barcelona.\n\nMoon's first sensory experiment was in 2007 when she created and wore a pair of kaleidoscopic glasses for three months. The glasses only allowed her to see colour, no shape. The lack of shape perception increased not only her sense of colour discrimination but also her detection of movement. Any slight change of colour in her field of vision indicated that something had moved. During the three-month period, Moon visited several cities in Europe and met people without ever seeing their faces.\n\nIn 2008, Moon created a speedometer glove that allowed her to perceive the exact speed of any movement around her through vibrations on her hand. She wore the glove for several months and was able to sense different speeds depending on the vibration intervals. She later transformed the glove into a pair of earrings that vibrated whenever there was presence around her. Moon travelled around Europe with her speedborg earrings to find out what the average walking speed of citizens was in different cities. \"The Speeds of Europe\" is a video dance that shows the results of her research; Londoners and Stockholm citizens for example walk at a similar average speed of approximately 6.1 km/h whereas people in Rome and Oslo walk at an average speed of 4 km/h.\n\nBy 2009, Moon was able to detect not only the exact speed of any person walking in front of her but also her own speed. Knowing her own speed allowed her to create \"Green Lights\" a piece choreographed in relation to a set of 8 traffic lights: by learning the traffic light timings of Barcelona's [[Rambla de Catalunya]] avenue and by measuring the distance between each traffic light, she calculated the speed she had to walk to avoid red traffic lights and was able to get from one end to the other end of the avenue without stops.\n\nIn 2010, Moon explored the possibilities of sensing movement behind her by turning the speedborg earrings around. The earrings allowed her to perceive if presence was behind her. The earrings were developed further by students from [[La Salle (Barcelona)|La Salle]] (Barcelona) by adding 4 extra sensors in order to gain 360° perception of movement through vibrations around the head.\n\nIn 2010, Moon Ribas and [[Neil Harbisson]] created the [[Cyborg Foundation]] (and an offshoot of it called the Cyborg Arts organization), an international organisation that encourages humans to become cyborgs. The aims of the organisation are: to extend human senses and abilities by creating and applying cybernetic extension to the body, to promote cyborgism as an art movement, and to defend cyborg rights. In 2010, the foundation won the Cre@tic Award, awarded by Tecnocampus Mataró. In 2012 a short film about the foundation was awarded at [[Sundance Film Festival]]. \n\n[[Category:1985 births]]\n[[Category:Living people]]\n[[Category:Cyborgs]]\n[[Category:Transhumanists]]\n[[Category:Vegetarians]]\n[[Category:Catalan women artists]]\n[[Category:Catalan artists|+Women]]\n[[Category:Spanish women artists| Catalan]]\n[[Category:Spanish artists]]\n[[Category:Spanish dancers]]\n[[Category:Spanish choreographers]]\n[[Category:Alumni of Falmouth University]]\n[[Category:People associated with Falmouth University]]\n[[Category:Spanish transhumanists]]\n[[Category:21st-century Spanish dancers]]"}
{"id": "51237323", "url": "https://en.wikipedia.org/wiki?curid=51237323", "title": "Mosel Vitelic Inc", "text": "Mosel Vitelic Inc\n\nMosel Vitelic Inc. manufactures power discrete, power management IC and analog IC technologies. Mosel Vitelic Inc. (TWSE:2342) joined the Taiwan Stock Exchange in September 1995.\n\nIts products include power MOSFETs, insulated-gate bipolar transistors, diodes, analog ICs, transient voltage suppressors (TVS), solar energy products, wide bandgap layout products, as well as other components and modules.\n\nThe company offers its products in China, Europe, the Americas, Japan, and South Korea.\n\nMosel Vitelic Inc. is based in Hsinchu, Taiwan.\n\n\nThe founding and incorporation of Mosel Vitelic was in 1991.\n\n\n\n"}
{"id": "66904", "url": "https://en.wikipedia.org/wiki?curid=66904", "title": "Nixie tube", "text": "Nixie tube\n\nA Nixie tube ( ), or cold cathode display, is an electronic device for displaying numerals or other information using glow discharge.\nThe glass tube contains a wire-mesh anode and multiple cathodes, shaped like numerals or other symbols. Applying power to one cathode surrounds it with an orange glow discharge. The tube is filled with a gas at low pressure, usually mostly neon and often a little mercury or argon, in a Penning mixture.\n\nAlthough it resembles a vacuum tube in appearance, its operation does not depend on thermionic emission of electrons from a heated cathode. It is therefore called a cold-cathode tube (a form of gas-filled tube), and is a variant of the neon lamp. Such tubes rarely exceed 40 °C (104 °F) even under the most severe of operating conditions in a room at ambient temperature. Vacuum fluorescent displays from the same era use completely different technology—they have a heated cathode together with a control grid and shaped phosphor anodes; Nixies have no heater or control grid, typically a single anode (in the form of a wire mesh, not to be confused with a control grid), and shaped bare metal cathodes.\n\nThe early Nixie displays were made by a small vacuum tube manufacturer called Haydu Brothers Laboratories, and introduced in 1955 by Burroughs Corporation, who purchased Haydu. The name \"Nixie\" was derived by Burroughs from \"NIX I\", an abbreviation of \"Numeric Indicator eXperimental No. 1\", although this may have been a backronym designed to justify the evocation of the mythical creature with this name. Hundreds of variations of this design were manufactured by many firms, from the 1950s until the 1990s. The Burroughs Corporation introduced \"Nixie\" and owned the name \"Nixie\" as a trademark. Nixie-like displays made by other firms had trademarked names including \"Digitron\", \"Inditron\" and \"Numicator\". A proper generic term is \"cold cathode neon readout tube\", though the phrase \"Nixie tube\" quickly entered the vernacular as a generic name.\n\nBurroughs even had another Haydu tube that could operate as a digital counter and directly drive a Nixie tube for display. This was called a \"Trochotron\", in later form known as the \"Beam-X Switch\" counter tube; another name was \"magnetron beam-switching tube\", referring to their derivation from a split-anode magnetron. Trochotrons were used in the UNIVAC 1101 computer, as well as in clocks and frequency counters.\n\nThe first trochotrons were surrounded by a hollow cylindrical magnet, with poles at the ends. The field inside the magnet had essentially-parallel lines of force, parallel to the axis of the tube. It was a thermionic vacuum tube; inside were a central cathode, ten anodes, and ten \"spade\" electrodes. The magnetic field and voltages applied to the electrodes made the electrons form a thick sheet (as in a cavity magnetron) that went to only one anode. Applying a pulse with specified width and voltages to the spades made the sheet advance to the next anode, where it stayed until the next advance pulse. Count direction was determined by the direction of the magnetic field, and as such was not reversible. A later form of trochotron called a Beam-X Switch replaced the large, heavy external cylindrical magnet with ten small internal metal-alloy rod magnets which also served as electrodes.\nGlow-transfer counting tubes, similar in essential function to the trochotrons, had a glow discharge on one of a number of main cathodes, visible through the top of the glass envelope. Most used a neon-based gas mixture and counted in base-10, but faster types were based on argon, hydrogen, or other gases, and for timekeeping and similar applications a few base-12 types were available. Sets of \"guide\" cathodes (usually two sets, but some types had one or three) between the indicating cathodes moved the glow in steps to the next main cathode. Types with two or three sets of guide cathodes could count in either direction. A well-known trade name for glow-transfer counter tubes in the United Kingdom was Dekatron. Types with connections to each individual indicating cathode, which enabled presetting the tube's state to any value (in contrast to simpler types which could only be directly reset to zero or a small subset of their total number of states), were trade named \"Selectron\" tubes.\n\nDevices that functioned in the same way as Nixie tubes were patented in the 1930s, and the first mass-produced display tubes were introduced in 1954 by National Union Co. under the brand name Inditron. However, their construction was cruder, their average lifetime was shorter, and they failed to find many applications due to their complex periphery.\n\nThe most common form of Nixie tube has ten cathodes in the shapes of the numerals 0 to 9 (and occasionally a decimal point or two), but there are also types that show various letters, signs and symbols. Because the numbers and other characters are arranged one behind another, each character appears at a different depth, giving Nixie based displays a distinct appearance. A related device is the pixie tube, which uses a stencil mask with numeral-shaped holes instead of shaped cathodes. Some Russian Nixies, e.g. the IN-14, used an upside-down digit 2 as the digit 5, presumably to save manufacturing costs as there is no obvious technical or aesthetic reason.\n\nEach cathode can be made to glow in the characteristic neon red-orange color by applying about 170 volts DC at a few milliamperes between a cathode and the anode. The current limiting is normally implemented as an anode resistor of a few tens of thousands of ohms. Nixies exhibit negative resistance and will maintain their glow at typically 20 V to 30 V below the strike voltage. Some color variation can be observed between types, caused by differences in the gas mixtures used. Longer-life tubes that were manufactured later in the Nixie timeline have mercury added to reduce sputtering resulting in a blue or purple tinge to the emitted light. In some cases, these colors are filtered out by a red or orange filter coating on the glass.\n\nOne advantage of the Nixie tube is that its cathodes are typographically designed, shaped for legibility. In most types, they are not placed in numerical sequence from back to front, but arranged so that cathodes in front obscure the lit cathode minimally. One such arrangement is 6 7 5 8 4 3 9 2 0 1 from front (6) to back (1). Russian NH-12A & NH-12B tubes use the number arrangement 1 6 2 7 5 0 4 9 8 3 from back to front, with the 5 being an upside down 2. The 12B tubes feature a bottom far left decimal point between the numbers 8 and 3.\n\nNixies were used as numeric displays in early digital voltmeters, multimeters, frequency counters and many other types of technical equipment. They also appeared in costly digital time displays used in research and military establishments, and in many early electronic desktop calculators, including the first: the Sumlock-Comptometer \"ANITA Mk VII\" of 1961 and even the first electronic telephone switchboards. Later alphanumeric versions in fourteen segment display format found use in airport arrival/departure signs and stock ticker displays. Some elevators used Nixies to display floor numbers.\n\nAverage longevity of Nixie tubes varied from about 5,000 hours for the earliest types, to as high as 200,000 hours or more for some of the last types to be introduced. There is no formal definition as to what constitutes \"end of life\" for Nixies, mechanical failure excepted. Some sources suggest that incomplete glow coverage of a glyph (\"cathode poisoning\") or appearance of glow elsewhere in the tube would not be acceptable.\n\nNixie tubes are susceptible to multiple failure modes, including\n\nDriving Nixies outside of their specified electrical parameters will accelerate their demise, especially excess current, which increases sputtering of the electrodes. A few extreme examples of sputtering have even resulted in complete disintegration of Nixie-tube cathodes.\n\nCathode poisoning can be abated by limiting current through the tubes to significantly below their maximum rating, through the use of Nixie tubes constructed from materials that avoid the effect (e.g. by being free of silicates and aluminum), or by programming devices to periodically cycle through all digits so that seldom-displayed ones get activated.\n\nAs testament to their longevity, and that of the equipment which incorporated them, several suppliers still provide common Nixie tube types as replacement parts, new in original packaging. Equipment with Nixie-tube displays in excellent working condition is still plentiful, though much of it has been in frequent use for 30–40 years or more. Such items can easily be found as surplus and obtained at very little expense. In the former Soviet Union, Nixies were still being manufactured in volume in the 1980s, so Russian and Eastern European Nixies are still available.\n\nOther numeric-display technologies concurrently in use included backlit columnar transparencies (\"thermometer displays\"), light pipes, rear-projection and edge-lit lightguide displays (all using individual incandescent or neon light bulbs for illumination), Numitron incandescent filament readouts, Panaplex seven-segment displays, and vacuum fluorescent display tubes. Before Nixie tubes became prominent, most numeric displays were electromechanical, using stepping mechanisms to display digits either directly by use of cylinders bearing printed numerals attached to their rotors, or indirectly by wiring the outputs of stepping switches to indicator bulbs. Later, a few vintage clocks even used a form of stepping switch to drive Nixie tubes.\n\nNixie tubes were superseded in the 1970s by light-emitting diodes (LEDs) and vacuum fluorescent displays (VFDs), often in the form of seven-segment displays. The VFD uses a hot filament to emit electrons, a control grid and phosphor-coated anodes (similar to a cathode ray tube) shaped to represent segments of a digit, pixels of a graphical display, or complete letters, symbols, or words. Whereas Nixies typically require 180 volts to illuminate, VFDs only require relatively low voltages to operate, making them easier and cheaper to use. VFDs have a simple internal structure, resulting in a bright, sharp, and unobstructed image. Unlike Nixies, the glass envelope of a VFD is evacuated rather than being filled with a specific mixture of gases at low pressure.\n\nSpecialized high-voltage driver chips such as the 7441/74141 were available to drive Nixies. LEDs are better suited to the low voltages that integrated circuits used, which was an advantage for devices such as pocket calculators, digital watches, and handheld digital measurement instruments. Also, LEDs are much smaller and sturdier, without a fragile glass envelope. LEDs use less power than VFDs or Nixie tubes with the same function.\n\nCiting dissatisfaction with the aesthetics of modern digital displays and a nostalgic fondness for the styling of obsolete technology, significant numbers of electronics enthusiasts in recent years have shown interest in reviving Nixies. Unsold tubes that have been sitting in warehouses for decades are being brought out and used, the most common application being in homemade digital clocks. This is somewhat ironic, since during their heyday, Nixies were generally considered too expensive for use in mass-market consumer goods such as clocks. This recent surge in demand has caused prices to rise significantly, particularly for large tubes. The largest Nixie tubes known to be in the hands of collectors, the Rodan CD47/GR-414 (220 mm [8.7 in] tall), have been sold for hundreds of dollars each, but said Nixies are extremely rare. Prices for other large Nixies displaying digits over 25 mm (1 in) tall have risen by double, triple or more between 1998 and 2005.\n\nThere have also been some attempts to make new Nixie tubes, the most successful of which is that of Dalibor Farny, a programmer and electrical engineer from the Czech Republic. His efforts began in February 2012, and by the end of 2014, Dalibor had created his first marketable Nixie tube, the R|Z568M (whose \"R\" stands for \"resurrection\"), which he still sells on his website for $145 USD.\n\nIn addition to the tube itself, another important consideration is the relatively high-voltage circuitry necessary to drive the tube. The original 7400 series drivers integrated circuits such as the 74141 BCD decoder driver have long since been out of production and are rarer than NOS tubes. Only \"Integral\" in Belarus lists the 74141 and its Soviet equivalent, the K155ID1 as still in production. However modern bipolar transistors with high voltage ratings are now available cheaply, such as MPSA92 or MPSA42 – an unusual example where an original IC design has been replaced by discrete transistors.\n\n\n\n"}
{"id": "21876751", "url": "https://en.wikipedia.org/wiki?curid=21876751", "title": "Optical sorting", "text": "Optical sorting\n\nOptical sorting (sometimes called digital sorting) is the automated process of sorting solid products using cameras and/or lasers.\n\nDepending on the types of sensors used and the software-driven intelligence of the image processing system, optical sorters can recognize objects' color, size, shape, structural properties and chemical composition. The sorter compares objects to user-defined accept/reject criteria to identify and remove defective products and foreign material (FM) from the production line, or to separate product of different grades or types of materials.\nOptical sorters are in widespread use in the food industry worldwide, with the highest adoption in processing harvested foods such as potatoes, fruits, vegetables and nuts where it achieves non-destructive, 100 percent inspection in-line at full production volumes. The technology is also used in pharmaceutical manufacturing and nutraceutical manufacturing, tobacco processing, waste recycling and other industries. Compared to manual sorting, which is subjective and inconsistent, optical sorting helps improve product quality, maximize throughput and increase yields while reducing labor costs.\n\nIn general, optical sorters feature four major components: the feed system, the optical system, image processing software and the separation system. The objective of the feed system is to spread product into a uniform monolayer so products are presented to the optical system evenly, without clumps, at a constant velocity. The optical system includes lights and sensors housed above and/or below the flow of the objects being inspected. The image processing system compares objects to user-defined accept/reject thresholds to classify objects and actuate the separation system. The separation system, usually compressed air for small products and mechanical devices for larger products like whole potatoes, pinpoints objects while in-air and deflects the objects to remove into a reject chute while good product continues along its normal trajectory.\n\nThe ideal sorter depends on the application, with the product's characteristics and the user's objectives determining the ideal sensors, software-driven capabilities and mechanical platform.\n\nOptical sorters require a compatible combination of light and sensors to illuminate objects and capture images of the objects before the images can be processed and accept/reject decisions made.\n\nThere are camera sorters, laser sorters and sorters that feature a combination of cameras and lasers on one platform. Lights, cameras, lasers and laser sensors can be designed to function within visible light wavelengths as well as the infrared (IR) and ultraviolet (UV) spectrums. The optimal wavelengths for each application maximize the contrast between the objects to be separated. Cameras and laser sensors can differ in spatial resolution, with higher resolutions enabling the sorter to detect and remove smaller defects.\n\n Monochromatic cameras detect shades of gray from black to white and can be effective when sorting products with high-contrast defects.\n\nSophisticated color cameras with high color resolution are capable of detecting millions of colors to better distinguish more subtle color defects. Trichromatic color cameras (also called three-channel cameras) divide light into three bands, which can include red, green and/or blue within the visible spectrum as well as IR and UV.\n\nCoupled with intelligent software, sorters that feature cameras are capable of recognizing each object's color, size and shape as well as the color, size, shape and location of a defect on a product. Some intelligent sorters even allow the user to define a defective product based on the total defective surface area of any given object. They use high beam infrared laser beams to show the laser on the beam.\n\nWhile cameras capture product information based primarily on material reflectance, lasers and their sensors are able to interrogate a material's structural properties in addition to determining differences in color. This structural property inspection capability makes lasers ideal for detecting a wide range of organic and inorganic foreign material such as insects, glass, metal, sticks, rocks and plastic, even if they are the same color as the good product, and for separating various materials at waste recycling facilities.\n\nLasers can be designed to operate within specific wavelengths of light, within the visible spectrums and beyond. For example, lasers can detect chlorophyll by stimulating fluorescence using specific wavelengths, a process that is very effective for removing foreign material from green vegetables.\n\nSorters equipped with cameras and lasers on one platform are generally capable of identifying the widest variety of attributes. Cameras are often better at recognizing color, size and shape while laser sensors identify differences in structural properties to maximize foreign material detection and removal. \n\n Driven by the need to solve previously impossible sorting challenges, a new generation of sorters that feature multispectral and hyperspectral imaging systems are being developed.\n\nLike trichromatic cameras, multispectral and hyperspectral cameras collect data from across the electromagnetic spectrum. Unlike trichromatic cameras, which divide light into three bands, hyperspectral systems can divide light into hundreds of narrow bands over a continuous range that covers a vast portion of the electromagnetic spectrum. Compared to the three data points per pixel collected by trichromatic cameras, hyperspectral cameras can collect hundreds of data points per pixel, which are combined to create a unique spectral signature (also called a fingerprint) for each object. When complemented by capable software intelligence, a hyperspectral sorter processes those fingerprints to enable sorting on the chemical composition of the product. This is an emerging area of chemometrics.\n\nOnce the sensors capture the object's response to the energy source, image processing takes over to manipulate the raw data to extract and categorize information about specific features. As raw data flows from the sensors, the definitions of good and bad flow from the user who sets the accept/reject thresholds. The art and science of image processing lies in developing algorithms that maximize the effectiveness of the sorter while presenting a simple user-interface to the operator.\n\nObject-based recognition is a classic example of software-driven intelligence. It allows the user to define a defective product based on where a defect lies on the product and/or the total defective surface area of an object. It offers more control in defining a wider range of defective products, and if used to control the sorter's ejection system, it improves the accuracy of ejecting defective products, which improves product quality and increases yields.\n\nNew software-driven capabilities are constantly being developed to address the specific needs of various applications. As computing hardware becomes more powerful, new software-driven advancements become possible. Some of these advancements enhance the effectiveness of sorters to achieve better results while others enable completely new sorting decisions to be made.\n\nThe considerations that determine the ideal platform for a specific application include the nature of the product – large or small, wet or dry, fragile or unbreakable, round or easy to stabilize – and the user's objectives. In general, products smaller than a grain of rice and as large as whole potatoes can be sorted. Throughputs range from less than 2 metric tons of product per hour on low-capacity sorters to more than 35 metric tons of product per hour on high-capacity sorters.\n\nThe simplest optical sorters are channel sorters, a type of color sorter that can be effective for products that are small, hard and dry with a consistent size and shape such as rice and seeds. For these products, channel sorters offer an affordable solution and ease of use with a small footprint. Channel sorters feature monochromatic or color cameras and remove defects and foreign material based only on differences in color.\n\nFor products that are soft or wet or non-homogenous, which cannot be handled by a channel sorter, and for processors that want more control over the quality of their product, freefall sorters (also called waterfall or gravity-fed sorters), chute-fed sorters or belt sorters are ideal. These more sophisticated sorters often feature advanced cameras and/or lasers that, when complemented by capable software intelligence, detect objects' size, shape, color, structural properties and chemical composition.\n\nAs the names imply, freefall sorters inspect product in-air during the freefall, while chute-fed sorters stabilize product on a chute prior to in-air inspection. The major advantages of freefall and chute-fed sorters, compared to belt sorters, are a lower price point, smaller footprint and the absence of moving parts, which contributes to low maintenance. These sorters are often most suitable for nuts and berries as well as a frozen and dried fruits, vegetables, potato strips and seafood, in addition to waste recycling applications that require mid-volume throughputs.\n\n Belt sorting platforms are often preferred for higher capacity applications such as vegetable and potato products prior to canning, freezing or drying, most fresh cut produce and wet fruits as well as waste recycling. Belt sorters stabilize product on a conveyor belt prior to inspection. Some belt sorters inspect product on the belt from the top only while others also launch product off the belt for in-air inspection from the bottom. Belt sorters can be designed to achieve traditional two-way sorting or can be equipped with two ejector systems and three outfeed streams to achieve three-way sorting.\n\nA fifth type of sorting platform, called an automated defect removal (ADR) system, is specifically for potato strips (French fries). Unlike other sorters that eject products with defects from the production line, ADR systems identify defects and actually cut the defects from the strips. In conjunction with a mechanical nubbin grader that follows the ADR, this combination is essentially another type of optical sorting system because it uses optical sensors to identify and remove defects.\n\nThe platforms described above all operate in bulk mode, without needing to single-file product prior to inspection. In contrast, a sixth type of platform, used in the pharmaceutical industry, is a single-file optical inspection system. Although these sorters are effective in detecting and removing foreign tablets and capsules and defects based on differences in size, shape and color, they have not been widely adopted due to the high capital costs, low throughput and slow changeover, compared to the belt sorters for tablets, capsules and softgels.\n\nFor products that require sorting only by size, mechanical grading systems that don't utilize sensors or image processing systems are often highly effective. These mechanical grading systems are sometimes referred to as sorting systems, but should not be confused with optical sorters that feature sensors and image processing systems.\n\n"}
{"id": "3237641", "url": "https://en.wikipedia.org/wiki?curid=3237641", "title": "Phu Phong Glass Joint Stock Company", "text": "Phu Phong Glass Joint Stock Company\n\nPhu Phong Glass Joint Stock Company (CTCP Sản xuất Thương mại Dịch vụ Phú Phong) is a company based in the outskirts of Ho Chi Minh City that makes architectural glass and float glass for use in furniture and construction materials. Phu Phong's main offices are in Ho Chi Minh City. Its stock is listed at the Hanoi Securities Trading Center, symbol is PPG.\n\n"}
{"id": "1265417", "url": "https://en.wikipedia.org/wiki?curid=1265417", "title": "Pohang University of Science and Technology", "text": "Pohang University of Science and Technology\n\nPohang University of Science and Technology (POSTECH) is a private research university in Pohang, South Korea dedicated to research and education in science and technology.\n\nIn 2012-2014, the \"Times Higher Education\" ranked POSTECH 1st in its \"100 Under 50 Young Universities\" rankings. Since its establishment, POSTECH has been maintaining its prestige and reputation as one of the most prominent young research universities in the world, being 83rd overall in the QS World University Rankings 2019 and 12th best in QS Asia University Ranking 2017-2018.\n\nPOSTECH was established in 1986 in Pohang, Korea by POSCO, one of the world's leading steel companies, for the purpose of providing advanced education for budding engineers and laying the groundwork for future technological development.\n\nThe founder of POSCO and the founding chairman of POSTECH, Park Tae-joon realized the need for Korea to educate their youth in science and technology to ensure Korea's position in the high technology arena. Park wanted to use the California Institute of Technology (Caltech) as a model for POSTECH and visited the university on a business trip to Los Angeles in 1985. He noted characteristics of Caltech and requested to the POSTECH founding team to establish a contemporary research university that had: a low student-faculty ratio, a greater proportion of graduate students to undergraduates, a low net education cost, student on-campus housing, and a high-quality campus environment. These features represented a drastic departure from the Korean universities of the 1980s.\n\nPOSCO organized a task force on February 21, 1984 made up of POSCO employees selected by the company. Construction work began on August 17, 1985. On May 4, 1986, British Prime Minister Margaret Thatcher visited POSTECH and donated an Inmos transputer, one of the leading edge computer parts at the time. The first matriculation ceremony was held on March 5, 1987. A group of 249 freshmen were selected from the top one percent of all graduating seniors in Korea to be taught by an international faculty recruited by POSTECH founding president Hogil Kim and POSTECH task force head manager Dai Kong Lee. The first degree awarding ceremony was on February 20, 1991. Diplomas were awarded to 146 graduating seniors, 123 (84%) of whom went on to pursue graduate studies. The remaining 23 graduates were employed by the nation's major corporations including POSCO, Samsung, LG, and Hyundai.\n\nTo facilitate translational research and active academia-industry collaboration, POSTECH hosted POSCO's Research Institute of Science and Technology (RIST) on campus. In 1994, POSTECH set up the Pohang Accelerator Laboratory (PAL), a 3rd-generation synchrotron light source and now a national facility. Currently, the 4th-generation light source X-ray free electron laser (XFEL) is under construction (to be completed in 2015) at the cost of US$400 million, which will be the third one in the world and will open up new frontiers and research areas in life sciences, materials, chemistry, and physics.\n\nIn 1998, POSTECH was ranked by \"Asiaweek\" as the best science and technology university in Asia. From 2002 to 2006, one of Korea's most circulated daily newspapers, \"JoongAng Ilbo\", ranked POSTECH as the leading university in Korea. In 2010, the Times Higher Education ranked POSTECH 28th in the world. In 2011, the \"Times Higher Education\" ranked the university as the 53rd best university in the world, the 6th best in Asia, and the best in South Korea. In 2017-2018, QS World University Rankings ranked POSTECH 71st overall in the world. It remains third best ranked in Korea, after Seoul National University and KAIST, in the QS Asian University Rankings. However, in the \"Times Higher Education\" rankings, it scored highly after compilers placed less emphasis on \"reputation and heritage\" and gave more weight to objective measures including the influence of universities' research, placing 53rd. In 2012 and 2013, the \"Times Higher Education\" ranked POSTECH 1st in its \"100 Under 50 Young Universities\" rankings. \"The New York Times\" and the \"International Herald Tribune\" cited POSTECH's rapid ascent as a young university to top the world rankings in less than 50 years.\n\nPOSTECH is a 400-acre campus located twenty minutes by car from downtown Pohang, an hour by bus from Busan, and approximately two and half hours by train (KTX) from Seoul.\nCompleted in 2003, the Tae-Joon Park Library is 24,420 square meters with 352,977 volumes and 8,324 digital and paper journals. As of 2005, the library collection consists of approximately 320,000 books, 3,500 journals, 7,000 e-journals 25 databases, and 4,400 multimedia materials. The Library shares materials with industrial-educational-research cooperation and is part of an intercollegiate data exchange program with approximately 150 other research and educational institutions throughout the nation.\n\nPOSTECH operates a Smart Campus where the scientific and technological information of the world is accessible anywhere on campus using different types of smart phones and mobile devices as well as laptops. In 2010, for the first time among Korean universities, POSTECH implemented a Desktop Cloud Service, providing a convenient and secure computing environment. However, as of 2018, many of the previously implemented technological services (e.g. campus smartphone applications, university website, university online portal, etc) are defective as they have not been since updated, and are only accessible through Internet Explorer. \n\nPOSTECH offers students living on-campus in affordable dormitories and apartments for married graduate students. The student housing complex is composed of 23 five-story student dormitories, one 13-story undergraduate student dormitory, and 4 graduate student apartments. All POSTECH undergraduate students are required to live in one of the on-campus dormitories and many graduate students prefer to stay on campus, either in the student dormitories or in one of the four high-rise graduate student apartments.\n\nPOSTECH has a range of sports facilities, from a gymnasium equipped with racquetball, basketball and badminton courts to POSPLEX, a sports center with a swimming pool, a fitness center and an indoor golf driving range, and outdoor sports fields for tennis, futsal, and soccer.\n\nIn March 2010, POSTECH declared the initiation of a bilingual campus. However, as of 2018, the vast majority of notices and announcements are made only in Korean. Most major official events (e.g., commencement) are also only run in Korean. While course catalogs list courses as taught in English, many professors decide to teach in Korean because international members represent only a minority of the classroom. The rising consensus from international students reveals the difficulty of living and studying as a non-Korean speaker at POSTECH.\n\nPOSTECH admits approximately 300 undergraduate students each year. POSTECH received 1,987 applicants for freshman admission and admitted 323 for the 2014 school year. POSTECH provides the highest educational investment and the most per-student scholarship support in Korea, allowing students from all economic backgrounds the opportunity to obtain a POSTECH education.\n\nA growing number of international students attend POSTECH as it expands its recruiting efforts abroad. POSTECH offers full tuition fellowships to excellent graduate students from the following countries: Afghanistan, Bhutan, Cambodia, China, Democratic Republic of the Congo, India, Indonesia, Japan, Kazakhstan, Malaysia, Mongolia, Pakistan, Philippines, Singapore, Taiwan, Tajikistan, Thailand, Uzbekistan, Vietnam, and Zimbabwe. However, the international program still lacks proper care and attention to the well-being and integration of international students into POSTECH, as the university's cultural and social norms are not suited for an international community. Growing reports indicate the wide spread presence of racism, sexism, and discrimination against international students. \n\nAll graduate students receive a small teaching assistant scholarships and the university provides the Tae-Joon Park Graduate Fellowship, which provides the highest level of scholarship (approximately 25 million South Korean won) to about 50 top graduate students each year. The POSCO Fellowship is available for students recommended by a POSTECH departmental committee and then selected by the POSCO TJ Park Foundation. POSTECH offers a one-time settlement grant of approximately $1,500 USD to all incoming international graduate students during the first semester of enrollment. As of 2018, graduate students will receive a sum balance of approximately $153 USD per month from the teaching assistant scholarship to pay for food and living expenses. \n\nGraduate students have the option of enrolling in the MS-Ph.D. integrated program. Under this program, students can work towards a Ph.D. degree after passing the Ph.D. qualifying exam without going through the MS program. Ph.D. applicants must have earned a master's degree before enrolling at POSTECH.\n\nThe undergraduate research program provides students with opportunities to participate in research under the guidance and mentorship of professors. Scholarships are offered to encourage student participation in research. In several departments such as Chemistry and Mathematics, it is mandatory for all students to participate in the undergraduate research program.\n\nCourses and activities on entrepreneurship are available, and efforts to strengthen the entrepreneurial spirit and start-up support are being developed including club activities, start-up prep group, training on patents, and leave of absence for creating a new venture.\n\nThrough the Science & Technology Core (STC), courses that extend beyond departmental boundaries, students are allowed to take classes in different departments for an interdisciplinary and broad academic foundation.\n\nPOSTECH operates the Residential College program for all freshmen and sophomores. Opportunities for personal development, social and cultural experiences, extracurricular group activities, and volunteer opportunities are occasionally available for students to participate in and enhance their undergraduate experience.\n\nPOSTECH's partnership with Korea National University of Arts allows students to take courses on both campuses and organize performances.\n\n11 undergraduate departments: \n\n11 graduate departments, 4 divisions, 5 graduate schools, and 4 specialized graduate schools: \n\nFour research centers of the Institute for Basic Science (IBS), which were recently established by the Korean government to promote research in basic sciences, are located at POSTECH. Four scientists and their teams in the fields of mathematics, physics, chemistry and life sciences are currently carrying out research on campus.\n\nThe 3rd generation light source at POSTECH is the 5th in the world and the only synchrotron radiation accelerator in Korea. This national research facility enables studies on various structural characteristics of materials using light. The facility is utilized in various basic science to high-tech industrial research. Its performance improvement to PLS-II was successfully completed in 2012. Thousands of users visit yearly to conduct their research using PLS-ll.\n\nThe 4th generation light source (PAL-XFEL), which is operational since 2015, is 10 billion times brighter than the 3rd generation light source. PAL-XFEL will generate new knowledge and discoveries in almost all technical and scientific disciplines including medicine, pharmaceutics, chemistry, material science, nanotechnology, power engineering and electronics.\n\nThe Max Planck Society of Germany and POSTECH established two Max Planck Centers at POSTECH: the Max Planck-POSTECH Center for Attosecond Science and the Max Planck-POSTECH Center for Complex Phase Materials. This collaboration is not only a testament to POSTECH's internationally renowned research excellence but, with the introduction of advanced research institute operation and management methods, also lays a foundation for the further strengthening Korea's competitiveness in basic science research.\n\nThe POSTECH Biotech Center was founded in 2000 with the goal of becoming the hub of biotechnology research and business development (R&BD). The primary goal of the POSTECH Biotech Center is to incubate disruptive technologies in biotechnology areas for the development of immunology, pharmaceuticals, and nanobiotechnology. The Center has been participating actively in academia-industry collaboration both nationally and internationally.\n\nThe National Institute for Nanomaterials Technology, a core research center for advancement and commercialization of nanotechnology, is developing the next-generation semiconductor and display materials, etc. The Institute provides technical support and assistance to industry, from research and development to commercialization. The Institute also supports and enables POSTECH researchers to actively carry out nanotechnology and other related areas.\n\nC plays a pivotal role in catalyzing world-first convergence education and research, and fostering future global leaders with a creative and challenging spirit. C was named after Creative, Collaborative, Cultivating, Convergence, and Center. C has 7-stories with a size of 16,000 square meters per floor. The construction of C came at an investment of 31.5 billion won. Construction began in October 2013 and its inauguration ceremony was held on January 30, 2015.\n\nThe first three floors of C is filled by the Department of Creative IT Engineering (CiTE) and the POSETCH Future IT Innovation Laboratory (i-Lab). CiTE and i-Lab use interdisciplinary-oriented education and innovative research. The remaining three floors house POSTECH research teams that work for POSTECH's strategic promotion of research areas and subjects with huge growth potential. A modern open-space design allows various research teams to rearrange work areas freely to facilitate and promote dynamic collaboration.\n\nThe POSCO Pohang Center for Creative Economy occupies the 5th floor of C. The Center, led by POSCO, was established to maintain the competitiveness of the steel industry, promote source technology development, and nurture regionally-based \"hidden champions\" to vitalize venture businesses.<br>\n\nSince 1987, POSTECH has hosted the annual Sunrise Festival to celebrate the spirit of unity and harmony of the POSTECH members and the local community. It is run in mid-May for three days immediately after the midterm of the spring semester. This festival is financially supported by the university, and is prepared by the entire student body and the POSTECH Student Club Association. Participants at the festival can enjoy movies, performances, singing contests, quiz shows, as well as traditional Korean drinks sold at cafes, beer gardens, and food stalls set up by student clubs.\n\nScience War is an event held together by KAIST and Pohang University of Science and Technology every year in the fall. The first tournament was held in 2002 at KAIST. Every year since, both schools held it alternately. The competition name is determined depending by the home university. If held at KAIST, it is called \"Pocajeon\"; at the Pohang University of Science and Technology, it is called \"Kapojeon\". Events include football, baseball, basketball, athletics and hacking competitions, League of Legend, and a science quiz.\n\nThe Hyungsan Academic Festival, organized by the Federation of Student Circles, takes place at the end of October after the midterm of the second semester. Originally designed to provide venues for academic presentations by students' circles and departments, this festival now provides various programs including sports competition, plays and movie festivals, as well as academic conferences.\n\n\n\n"}
{"id": "56916379", "url": "https://en.wikipedia.org/wiki?curid=56916379", "title": "Process-Architecture-Optimization model", "text": "Process-Architecture-Optimization model\n\nProcess–architecture-optimization is a processor development model adopted in 2016 by Intel. Under this three-phase model, every die shrink is followed by a microarchitecture change and then by an optimizaton. It replaced the two-phase Tick–tock model, adopted by Intel in 2006, because according to Intel the previous model is no longer sustainable.\n"}
{"id": "14586564", "url": "https://en.wikipedia.org/wiki?curid=14586564", "title": "Process control monitoring", "text": "Process control monitoring\n\nIn the application of integrated circuits, process control monitoring (PCM) is the procedure followed to obtain detailed information about the process used.\n\nPCM is associated with designing and fabricating special structures that can monitor technology specific parameters such as V in CMOS and V in bipolars. These structures are placed across the wafer at specific locations along with the chip produced so that a closer look into the process variation is possible.\n"}
{"id": "2722905", "url": "https://en.wikipedia.org/wiki?curid=2722905", "title": "Replication (computing)", "text": "Replication (computing)\n\nReplication in computing involves sharing information so as to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance, or accessibility.\n\nReplication in computing can refer to:\n\nReplication in space or in time is often linked to scheduling algorithms.\n\nAccess to a replicated entity is typically uniform with access to a single non-replicated entity. The replication itself should be transparent to an external user. In a failure scenario, a failover of replicas should be hidden as much as possible with respect to quality of service (QoS).\n\nComputer scientists further describe replication as being either:\n\nWhen one master replica is designated to process all the requests, the system is using a primary-backup or master-slave scheme, which is predominant in high-availability clusters. In comparison, if any replica can process a request and distribute a new state, the system is using a multi-primary or multi-master scheme. In the latter case, some form of distributed concurrency control must be used, such as a distributed lock manager.\n\nLoad balancing differs from task replication, since it distributes a load of different computations across machines, and allows a single computation to be dropped in case of failure. Load balancing, however, sometimes uses data replication (especially multi-master replication) internally, to distribute its data among machines.\n\nBackup differs from replication in that the saved copy of data remains unchanged for a long period of time. Replicas, on the other hand, undergo frequent updates and quickly lose any historical state. Replication is one of the oldest and most important topics in the overall area of distributed systems.\n\nData replication and computation replication both require processes to handle incoming events. Processes for data replication are passive and operate only to maintain the stored data, reply to read requests and apply updates. Computation replication is usually performed to provide fault-tolerance, and take over an operation if one component fails. In both cases, the underlying needs are to ensure that the replicas see the same events in equivalent orders, so that they stay in consistent states and any replica can respond to queries.\n\nThree widely cited models exist for data replication, each having its own properties and performance:\n\nDatabase replication can be used on many database management systems (DBMS), usually with a master-slave relationship between the original and the copies. The master logs the updates, which then ripple through to the slaves. Each slave outputs a message stating that it has received the update successfully, thus allowing the sending of subsequent updates.\n\nIn Multi-master replication, updates can be submitted to any database node, and then ripple through to other servers. This is often desired but introduces substantially increased costs and complexity which may make it impractical in some situations. The most common challenge that exists in multi-master replication is transactional conflict prevention or resolution. Most synchronous (or eager) replication solutions perform conflict prevention, while asynchronous (or lazy) solutions have to perform conflict resolution. For instance, if the same record is changed on two nodes simultaneously, an eager replication system would detect the conflict before confirming the commit and abort one of the transactions. A lazy replication system would allow both transactions to commit and run a conflict resolution during resynchronization. The resolution of such a conflict may be based on a timestamp of the transaction, on the hierarchy of the origin nodes or on much more complex logic, which decides consistently across all nodes.\n\nDatabase replication becomes more complex when it scales up, which is usually described in two dimensions: horizontal and vertical. Horizontal scale-up has more data replicas, while vertical scale-up has data replicas located at greater physical distances. Problems raised by horizontal scale-up can be alleviated by a multi-layer multi-view access protocol. The early problems of vertical scale-up have largely been addressed by improving Internet reliability and performance.\n\nWhen data is replicated between database servers, so that the information remains consistent throughout the database system and users cannot tell or even know which server in the DBMS they are using, the system is said to exhibit replication transparency.\n\nActive (real-time) storage replication is usually implemented by distributing updates of a block device to several physical hard disks. This way, any file system supported by the operating system can be replicated without modification, as the file system code works on a level above the block device driver layer. It is implemented either in hardware (in a disk array controller) or in software (in a device driver).\n\nThe most basic method is disk mirroring, typical for locally connected disks. The storage industry narrows the definitions, so \"mirroring\" is a local (short-distance) operation. A replication is extendable across a computer network, so that the disks can be located in physically distant locations, and the master-slave database replication model is usually applied. The purpose of replication is to prevent damage from failures or disasters that may occur in one location – or in case such events do occur, to improve the ability to recover data. For replication, latency is the key factor because it determines either how far apart the sites can be or the type of replication that can be employed.\n\nThe main characteristic of such cross-site replication is how write operations are handled, through either \"synchronous\" or \"asynchronous\" replication. The main difference is that synchronous replication needs to wait for the destination server in any write operation.\n\nSynchronous replication guarantees \"zero data loss\" by the means of atomic write operations, where the write operation is not considered complete until acknowledged by both the local and remote storage. Most applications wait for a write transaction to complete before proceeding with further work, hence overall performance decreases considerably. Inherently, performance drops proportionally to distance, as minimum latency is dictated by the speed of light. For 10 km distance, the fastest possible roundtrip takes 67 μs, whereas an entire local cached write completes in about 10–20 μs.\n\nIn Asynchronous replication, the write operation is considered complete as soon as local storage acknowledges it. Remote storage is updated with a small lag. Performance is greatly increased, but in case of a local storage failure, the remote storage is not guaranteed to have the current copy of data (the most recent data may be lost).\n\nSemi-synchronous replication typically considers a write operation complete when acknowledged by local storage and received or logged by the remote server. The actual remote write is performed asynchronously, resulting in better performance but remote storage will lag behind the local storage, so that there is no guarantee of durability (i.e.: seamless transparency) in the case of local storage failure.\n\nPoint-in-time replication produces periodic snapshots which are replicated instead of primary storage. This is intended to replicate only the changed data instead of the entire volume. As less information is replicated using this method, replication can occur over less-expensive bandwidth links such as iSCSI or T1 instead of fiber optic lines.\n\nMany distributed filesystems use replication to ensure fault tolerance and avoid a single point of failure.\n\nMany commercial synchronous replication systems do not freeze when the remote replica fails or loses connection – behaviour which guarantees zero data loss – but proceed to operate locally, losing the desired zero recovery point objective.\n\nTechniques of wide-area network (WAN) optimization can be applied to address the limits imposed by latency.\n\nFile-based replication conducts data replication at the logical level (i.e.: individual data files) rather than at the storage block level. There are many different ways of performing this, which almost exclusively rely on software.\n\nA kernel driver (specifically a filter driver) can be used to intercept calls to the filesystem functions, capturing any activity as it occurs. This utilises the same type of technology that real-time active virus checkers employ. At this level, logical file operations are captured like file open, write, delete, etc. The kernel driver transmits these commands to another process, generally over a network to a different machine, which will mimic the operations of the source machine. Like block-level storage replication, the file-level replication allows both synchronous and asynchronous modes. In synchronous mode, write operations on the source machine are held and not allowed to occur until the destination machine has acknowledged the successful replication. Synchronous mode is less common with file replication products although a few solutions exist.\n\nFile-level replication solutions allow for informed decisions about replication based on the location and type of the file. For example, temporary files or parts of a filesystem that hold no business value could be excluded. The data transmitted can also be more granular; if an application writes 100 bytes, only the 100 bytes are transmitted instead of a complete disk block (generally 4096 bytes). This substantially reduces the amount of data sent from the source machine and the storage burden on the destination machine.\n\nDrawbacks of this software-only solution include the requirement for implementation and maintenance on the operating system level, and an increased burden on the machine's processing power.\n\nSimilarly to database transaction logs, many file systems have the ability to journal their activity. The journal can be sent to another machine, either periodically or in real time by streaming. On the replica side, the journal can be used to play back file system modifications.\n\nOne of the notable implementations is Microsoft's System Center Data Protection Manager (DPM), released in 2005, which performs periodic updates but does not offer real-time replication.\n\nThis is the process of comparing the source and destination file systems and ensuring that the destination matches the source. The key benefit is that such solutions are generally free or inexpensive. The downside is that the process of synchronizing them is quite system-intensive, and consequently this process generally runs infrequently.\n\nOne of the notable implementations is rsync.\n\nAnother example of using replication appears in distributed shared memory systems, where many nodes of the system share the same page of memory. This usually means that each node has a separate copy (replica) of this page.\n\nMany classical approaches to replication are based on a primary-backup model where one device or process has unilateral control over one or more other processes or devices. For example, the primary might perform some computation, streaming a log of updates to a backup (standby) process, which can then take over if the primary fails. This approach is common for replicating databases, despite the risk that if a portion of the log is lost during a failure, the backup might not be in a state identical to the primary, and transactions could then be lost.\n\nA weakness of primary-backup schemes is that only one is actually performing operations. Fault-tolerance is gained, but the identical backup system doubles the costs. For this reason, starting , the distributed systems research community began to explore alternative methods of replicating data. An outgrowth of this work was the emergence of schemes in which a group of replicas could cooperate, with each process acting as a backup while also handling a share of the workload.\n\nComputer scientist Jim Gray analyzed multi-primary replication schemes under the transactional model and published a widely cited paper skeptical of the approach \"The Dangers of Replication and a Solution\". He argued that unless the data splits in some natural way so that the database can be treated as \"n\" disjoint sub-databases, concurrency control conflicts will result in seriously degraded performance and the group of replicas will probably slow as a function of \"n\". Gray suggested that the most common approaches are likely to result in degradation that scales as \"O(n³)\". His solution, which is to partition the data, is only viable in situations where data actually has a natural partitioning key.\n\nIn the 1985–1987, the virtual synchrony model was proposed and emerged as a widely adopted standard (it was used in the Isis Toolkit, Horus, Transis, Ensemble, Totem, Spread, C-Ensemble, Phoenix and Quicksilver systems, and is the basis for the CORBA fault-tolerant computing standard). Virtual synchrony permits a multi-primary approach in which a group of processes cooperates to parallelize some aspects of request processing. The scheme can only be used for some forms of in-memory data, but can provide linear speedups in the size of the group.\n\nA number of modern products support similar schemes. For example, the Spread Toolkit supports this same virtual synchrony model and can be used to implement a multi-primary replication scheme; it would also be possible to use C-Ensemble or Quicksilver in this manner. WANdisco permits active replication where every node on a network is an exact copy or replica and hence every node on the network is active at one time; this scheme is optimized for use in a wide area network (WAN).\n"}
{"id": "40388885", "url": "https://en.wikipedia.org/wiki?curid=40388885", "title": "Russian Union of Engineers", "text": "Russian Union of Engineers\n\nThe Russian Union of Engineers (RUE) (Russian: Российский союз инженеров (РСИ)) claims to be an all-Russian nongovernmental organization of engineers, design-engineers, builders, inventors, rationalizers, researchers, scientists, scientific and technical employees, and managers of industrial production. It has published several studies on economics, energy and housing related subjects and a paper on Malaysia Airlines Flight 17.\n\nOn January 21, 2012, at the Polytechnical Museum of Moscow, the Russian Union of Engineers presented the general rating of city appeal of Russian cities for 2011.\n\nOn May 20, 2013, the Ministry of Regional Development of the Russian Federation, the Federal Agency of Construction and Housing of the Russian Federation, Russian Union of Engineers and the experts of Lomonosov Moscow State University developed a rating and methodology of evaluation of urban environment and analyzed the 50 largest Russian cities. For this work, the Union designed the methods of urban environment quality assessment and a city appeal threshold.\n\nThe rating has been developed:\n\n\n\n"}
{"id": "45574176", "url": "https://en.wikipedia.org/wiki?curid=45574176", "title": "SIM box", "text": "SIM box\n\nA SIM box (also called a SIM bank) is device used as part of a VoIP gateway installation. It contains a number of SIM cards, which are linked to the gateway but housed and stored separately from it. A SIM box can have SIM cards of different mobile operators installed, permitting it to operate with several GSM gateways located in different places.\n\nThe SIM box operator can route international calls through the VoIP connection and connect the call as local traffic, allowing the box's operator to bypass international rates and often undercut prices charged by local mobile network operators. \n\nThis business model of operation is very common around the world and is usually legal, as it simply uses the internet to piggyback calls to a local area and using a private exchange to route the call back to the mobile network. However, many companies, such as Verizon, provide strong lobbying forces to local governments to ban the use of SIM pools. Another reason is the lack of control of the users, i.e. Knowing who called whom? Creating a serious privacy challenge to the users, where (local) governments want to know who made certain calls to whom and from where. One such example is the country of Ghana, where government has banned the use of SIM boxes. \n\nAs the normal mobile network are sometimes using a very outdated and low quality sound codecs (such as the GSM HR and FR codecs), a SIM box may provide local quality with superior sound quality.\n"}
{"id": "26298245", "url": "https://en.wikipedia.org/wiki?curid=26298245", "title": "Safe household water storage", "text": "Safe household water storage\n\nSafe household water storage is a critical component of a Household Water Treatment and Safe Storage (HWTS) system being promoted by the World Health Organization (WHO) worldwide in areas that do not have piped drinking water. In these areas it is not uncommon for drinking water to be stored in a pot, jar, crock or other container in the home. Even if this drinking water was of acceptable microbiological quality initially, it can become contaminated from dirty hands and utensils, such as dirty dippers and cups. Drinking water containers with \"narrow dispensers are key\" to keeping water from being contaminated while being stored in the home.\n\nAll types of 'safe household water storage must be used with water from known clean sources or with water having received prior efficacious treatment.\n\n\nThe United Nations' \"Millennium Declaration\" adopted by its General Assembly in September 2000 set Millennium Development Goals (MDG) that have a purpose of significantly reducing the proportion of people in the world in extreme poverty. Resolution 19 specifically states with respect to drinking water, \"To halve, by the year 2015...the proportion of the world's people who are unable to reach or to afford safe drinking water\". In 2009 the United Nations published \"The Millennium Development Goals Report\" that states: \"The world is well on its way to meeting the drinking water target, though some countries still face enormous challenges.\"\nOne way that the World Health Organization (WHO) has supported the safe drinking water goal is with its Household Water Treatment and Safe Storage (HWTS) program which targets people that are not connected to community water systems. Their website states that improved HWTS techniques can dramatically improve drinking water quality and reduce diarrhoeal diseases for those that must rely on unsafe water supplies. It reminds us that there are 1.6 million diarrhoeal deaths per year related to unsafe water, sanitation, and hygiene and that these are mostly of children under 5 years old.\n\n\n"}
{"id": "39594285", "url": "https://en.wikipedia.org/wiki?curid=39594285", "title": "Spherical surface acoustic wave (SAW) sensor", "text": "Spherical surface acoustic wave (SAW) sensor\n\nSpherical surface acoustic wave sensors use a type of surface acoustic wave (SAW) that travels along the surface of a medium exhibiting elasticity with exponentially decaying amplitude along depth. MEMS-IDT technology allows the use of SAW waves to sense various gases. Sensitivity up to 10 ppm of hydrogen using a spherical Ball SAW device is obtained.\n\nConventional planar SAW sensors are based on principle that the parameters such as amplitude, speed and phase of Surface acoustic wave changes on adsorption of gas molecules. Limitation of planar SAW based sensors is that the change in above mentioned parameters is very small due to limited path offered to Surface acoustic wave by planar sensor. In case of Spherical sensors surface acoustic wave make several round trips along the equator of a ball as shown in fig, which offer longer paths to Surface acoustic wave hence even smaller change in parameters is amplified with multiple turns, which increases the sensitivity of the sensor considerably.\n"}
{"id": "54204845", "url": "https://en.wikipedia.org/wiki?curid=54204845", "title": "Spoon and chopstick rest", "text": "Spoon and chopstick rest\n\nSpoon and chopstick rest is a piece of is tableware for resting a spoon and chopsticks without touching the table. In Korean cuisine context, it can be referred to as \"sujeo\" rest as \"sujeo\" is a paired set of spoon and chopsticks.\n\n"}
{"id": "6542979", "url": "https://en.wikipedia.org/wiki?curid=6542979", "title": "Steorn", "text": "Steorn\n\nSteorn Ltd was a small, private technology development company based in Dublin, Ireland. In August 2006, it announced that it had developed a technology to provide \"free, clean, and constant energy\" via an apparent perpetual motion machine, something which is contrary to the law of conservation of energy, a fundamental principle of physics.\n\nSteorn challenged the scientific community to investigate their claim and, in December 2006, said that it had chosen a jury of scientists to do so. In June 2009 the jury gave its unanimous verdict that Steorn had not demonstrated the production of energy.\n\nSteorn gave two public demonstrations of their technology. In the first demonstration, in July 2007 at the Kinetica Museum in London, the device failed to work. The second demonstration, which ran from December 2009 to February 2010 at the Waterways Visitor Centre in Dublin, involved a motor powered by a battery and provided no independent evidence that excess energy was being generated. It was dismissed by the press as an attempt to build a perpetual motion machine, and a publicity stunt.\n\nIn November 2016, the company laid off its staff, closed its facility, and prepared for liquidation.\n\nSteorn was founded in 2000 and, in October 2001, their website stated that they were a \"specialist service company providing programme management and technical assessment advice for European companies engaging in e-commerce projects\". Steorn is a Norse word meaning to guide or manage.\n\nIn May 2006, \"The Sunday Business Post\" reported that Steorn was a former dot-com company which was developing a microgenerator product based on the same principle as self-winding watches, as well as creating e-commerce websites for customers. The company had also recently raised about €2.5 million from investors and was three years into a four-year development plan for its microgenerator technology. Steorn later stated that the account given in this interview was intended to prevent a leak regarding their free energy technology.\n\nThe company's investment history shows several share allotments for cash between August 2000 and October 2005, the investments totalling €3 million. In 2006, Steorn secured €8.1 million in loans from a range of investors in order to continue their research, and these funds were also converted into shares. Steorn said that they would seek no further funding while attempting to prove their free-energy claim in order to demonstrate their genuine desire for validation.\n\nIn June 2016, the company informed shareholders that it had failed to meet expectations, that company founder Shaun (Seán) McCarthy was being replaced as CEO, and that operating costs were nearly €1 million per year. After investments totaling nearly €23 million over a ten-year period, in November 2016 the company shut down and laid off its staff, due to a lack of additional funding to continue operations.\n\nIn August 2006, Steorn placed an advertisement in \"The Economist\" saying that they had developed a technology that produced \"free, clean and constant energy\". Called Orbo, the technology was said to violate conservation of energy but had been validated by eight independent scientists. None of these scientists would talk to the media, and Steorn suggested that this was because they did not want to become embroiled in a controversy.\n\nNo specific details of the workings of the claimed technology were made public. McCarthy stated in a 2006 RTÉ radio interview, \"What we have developed is a way to construct magnetic fields so that when you travel round the magnetic fields, starting and stopping at the same position, you have gained energy\". In 2011, Steorn's website was updated to suggest that the Orbo is based on magnetic fields which vary over time. Barry Williams of the Australian Skeptics has pointed out that Steorn is \"not the first company to claim they have suddenly discovered the miraculous property of magnetism that allows you to get free energy\" while Martin Fleischmann says that it is not credible that positioning of magnetic fields could create energy.\n\nFollowing a meeting between McCarthy and Professor Sir Eric Ash in July 2007, Ash reported that \"the \"Orbo\" is a mechanical device which uses powerful magnets on the rim of a rotor and further magnets on an outer shell.\" During this meeting, McCarthy referred to the law of conservation of energy as scientific dogma. However, conservation of energy is a fundamental principle of physics, more specifically a consequence of the unchanging nature of physical laws with time by Noether's Theorem. Ash said that there was no comparison with religious dogma since there is no flexibility in choosing to accept that energy is always conserved. Rejecting conservation of energy would undermine all science and technology. Ash also formed the opinion that McCarthy was truly convinced in the validity of his invention but that this conviction was a case of \"prolonged self-deception.\"\n\nMany people have accused Steorn of engaging in a publicity stunt although Steorn denied such accusations. Eric Berger, writing on the \"Houston Chronicle\" website, commented: \"Steorn is a former e-business company that saw its market vanish during the dot.com bust. It stands to reason that Steorn has retooled as a Web marketing company and is using the \"free energy\" promotion as a platform to show future clients how it can leverage print advertising and a slick Web site to promote their products and ideas\". Thomas Ricker at \"Engadget\" suggested that Steorn's free-energy claim was a ruse to improve brand recognition and to help them sell Hall probes, while Josh Catone, features editor for \"Mashable\", believes that it was merely an elaborate hoax.\n\nIn its advertisement in \"The Economist\", Steorn challenged scientists to form an independent jury to test their technology and publish the results. Within 36 hours of the advertisement being published, 420 scientists contacted Steorn and, on 1 December 2006, Steorn announced it had selected a jury. It was headed by Ian MacDonald, emeritus professor of electrical engineering at the University of Alberta, and the process began in February 2007.\n\nIn June 2009 the jury announced its unanimous verdict that \"Steorn's attempts to demonstrate the claim have not shown the production of energy. The jury is therefore ceasing work\". Dick Ahlstrom, writing in the \"Irish Times\", concluded from this that Steorn's technology did not work. Steorn responded by saying that because of difficulties in implementing the technology the focus of the process had been on providing the jury with test data on magnetic effects for study. Steorn also said that these difficulties had been resolved and disputed its jury's findings.\n\nOn 4 July 2007, the technology was to be displayed at the Kinetica Museum, Spitalfields Market, London. A unit constructed of clear plastic was prepared so that the arrangement of magnets could be seen and to demonstrate that the device operated without external power sources. The public demonstration was delayed and then cancelled because of technical difficulties. Steorn initially said that the problems had been caused by excessive heat from the lighting.\n\nA second demonstration ran between 15 December 2009 and February 2010 at the Waterways Visitor Centre in Dublin, and was streamed via Steorn's website. The demonstration was of a device powered by a rechargeable battery. Steorn said that the device produced more energy than it consumed and recharged the battery. No substantive details of the technology were revealed and no independent evidence of Steorn's claim was provided.\n\nOn 1 April 2010 Steorn opened an online development community, called the Steorn Knowledge Development Base (SKDB), which they said would explain their technology. Access was available only under licence on payment of a fee.\n\nIn May 2015, Steorn put an \"Orbo PowerCube\" on display behind the bar of a pub in Dublin. The PowerCube was a small box which the pub website claimed contained a \"perpetual motion motor\" which required no external power source. The cube was shown charging a mobile phone. Steorn claimed to be performing some \"basic field trials\" in undisclosed locations.\n\nBeginning in December 2015, Steorn began accepting orders for two products, including a phone charger, through email only. The announcement was posted only to a Facebook page titled \"Orbo\" and a Steorn YouTube channel. In early December, McCarthy said that he was waiting for the first shipment of the two products, the Orbo Phone and the Orbo Cube, from a manufacturer in China. Steorn described the Orbo Cube as a showcase for the technology rather than a mass-market product, with the Cube retailing at €1,200.\n\n"}
{"id": "57681488", "url": "https://en.wikipedia.org/wiki?curid=57681488", "title": "Stored Energy at Sea", "text": "Stored Energy at Sea\n\nThe Stored Energy at Sea (StEnSEA) project is a new pump storage system designed to store significant quantities of electrical energy offshore. After research and development, it was tested on a model scale in November 2016. It is designed to link in well with offshore wind platforms and their issues caused by electrical production fluctuations. It works by water flowing into a container, at significant pressure, thus driving a turbine. If there is spare electricity the water can be pumped out, allowing electricity to be generated at a time of increased need.\n\nIn 2011, the physics Prof. Dr Horst Schmidt-Böcking (Goethe University Frankfurt) and Dr Gerhard Luther (Saarland University) had the idea of a new pump storage system, which should be placed on the sea bed. This system would use the high water pressure at great water depths to store electricity in hollow bodies.\n\nShortly, after their idea was published on the first of April 2011 in the newspaper Frankfurter Allgemeine Zeitung, a consortium of the \"Fraunhofer Institute for Energy Economics and Energy System Technology\" and the construction company \"Hochtief AG\" was set up. In collaboration they conducted a first preliminary sketch, which proved the feasibility of the pump storage concept. Subsequently, the German Federal Ministry for Economic Affairs and Energy supported the development and testing of the new concept.\nThe functionality of a seawater pressure storage power plant is based on usual pumped-hydro storage plants. A hollow concrete sphere with an integrated pump-turbine will be installed on the bottom of the sea. Compared to well known pumped-hydro storage plants, the sea that surrounds the sphere, represents the upper water basin. The hollow sphere represents the lower water basin. The StEnSea concept uses the high water pressure difference between the hollow sphere and the surrounding sea, which is about 75 bar (≈1 bar per 10 meters).\n\nIn case of overproduction of adjacent energy sources such as wind turbines or photovoltaic systems, the pump-turbine will be enabled to pump water from the cavity against the pressure into the surrounding sea. An empty hollow sphere means a fully charged storage system. When electricity is needed, water from the surrounding sea is guided through the turbine into the cavity, generating electricity. The higher the pressure difference between hollow sphere and the surrounding sea, the higher the energy yield during discharging. While discharging the hollow sphere a vacuum will be created inside. To avoid cavitation, the pump turbines and all other electrical components are placed in a centrally mounted cylinder. An auxiliary feed pump in the bottom of the cylinder is required to fill the cylinder with water and produces an inside pressure.\n\n\"Both pumps require an input pressure above the net positive suction head to avoid cavitation while pumping water from the inner volume into the cylinder or from the cylinder out of the sphere. As the pressure difference for the additional pump is much lower than for the pump turbine the required input pressure is lower as well. The input pressure of both pumps is given by the water column above them. For the additional pump this is the water column in the sphere and for the pump turbine it is the water column in the cylinder.\"\n\nThe maximum capacity for the hollow concrete sphere depends on the total pump-turbine efficiency, the installation depth and the inner volume.\n\nformula_1\n\nThe stored energy is proportional to the ambient pressure in the depths of the sea. Problems considered during the construction of the hollow sphere were choosing a construction-type that withstands the high water-pressure and which is heavy enough to keep the buoyancy force lower than the gravitational force. This resulted in the spherical construction with an inner diameter of 28.6 meter and a 2.72 meter thick wall made of normal watertight concrete.\nFor the proof of feasibility under real conditions and the acquisition of measurement data, the Fraunhofer engineers started the implementation of a pilot project. Therefore, a needed pilot hollow sphere was constructed by Hochtief Solutions AG in a scale of 1:10, with an outer diameter of three meters, an inner volume of eight m and is made of concrete. On November 9, 2016 it was installed in the lake Constance at a depth of 100 meters and tested for four weeks.\n\nDuring the test phase, the engineers were able to successfully store energy and operate the system in different operating modes. Another important question, that has been investigated, is whether or not a pressure equalization line to the surface is required. In case of application without the compensating cable, a reduction of costs and expense would be possible. The pilot test revealed, that both operation variants work and would be possible to run.\n\nIn the next step, a possible test location in the sea for the carrying out of a demonstration project is to be scrutinized. Then a sphere with the planned demonstration diameter of 30 meters should be built and installed at a suitable location in the sea. Possible places of installation situated near a coast would be for example the Norwegian trench or some Spanish sea areas.\n\nFurthermore, partners from the industry financing half of the project must be found, in order to receive further public funding from the BMWi. Because the total costs for the demonstration project are estimated at a low double-digit million euro amount.\n\nThe identification of potential installation sites was undertaken in three consecutive steps. At first, the designation of several arguments depicting the quality of a potential location were determined. Besides the installation depth, which is the main factor involved, variables like slope, geomorphology, distance to a possible grid connection point as well as to bases for servicing and set-up, marine reserves and the requirement for power storage in the surroundings were taken into account.\n\nIn the following step, specific values were assigned to the hard parameters, which are required for the use of the technology. Many of these values were determined in a previous feasibility analysis, a few had to be assessed by using comparable applications from different offshore industries. The installation depth of the concrete sphere should be 600-800m below the sea level and have an angle of inclination of less than or equal to 1°. In addition, it is required to reach the next grid connection point within one hundred kilometres as well as a basis, from which maintenance and repair measures can be carried out. Furthermore, an installation basis should not be more than 500 km away and areas with inappropriate geomorphology for example canyons were excluded.\n\nFinally, a global location analysis, based on geo-datasets and the above defined restrictions, was carried out with a Geographical Information System (GIS). In order to make a statement about the potential storage capacities, the resulting areas were assigned to the Exclusive Economic Zones (EEZ) of the affected states. Those and the corresponding capacities for storing electricity are displayed in the table below.\nStEnSea is a modular high capacity energy storage technology. It's profitability depends on installed units (concrete hollows) per facility (causing scale effects), on the realized arbitrages on the energy market and it depends on the operating hours per year. As well as on the investment and operation cost.\n\nIn the following chart the relevant economic parameters for an economic assessment are pictured. 800 to 1000 full operation cycles per annum are required.\n\nFor the operation and management of a storage farm, personal expenditure is based on of 0.5 - 2 staff per storage farm, depending on the farm capacity. Labor costs of 70 k€ per year and member of staff are used for the calculation. The price arbitrage is set to be 6 €ct per kWh for the economic assessment, resulting from an average electricity purchase price of 2 €ct per kWh and an average sale price of 8 €ct kWh. This price arbitrage includes the purchase of other services such as the provision of positive or negative balance power, frequency control or reactive power, all of which are not separately considered in the calculations. Planning and approval costs include costs for the site evaluation (as prerequisite for the permission), power plant certification, as well as the project development and management. \nDepending on the number of storage units per farm, the unit specific costs for planning and approval vary in the range from 1,070 mio.€ at 120 units to 1,74 mio.€ at 5 units. Also the annuities depend directly on the number of installed units. With 120 units an annuity of 544k€ can be achieved, while only a 232k€ annuity with 5 installed units only is possible.\nThis energy storage solution is not only a chance for the problem of the wind industry in storing energy but is also ecologically harmless. Due to the main components of the construction (primary steal, concrete for the hollow and cables for the connection) the risks for the eco-system are minimal. To avoid sea animals being sucked into the turbine a fine meshed grid is installed. In addition the flow speed of the water rushing into the hollow is kept low.\n\nA video post on the public television station ZDF called the hollow concrete balls a “possible solution to store solar and wind energy”. The gained data helped to understand the project better. For further tests on a bigger scale Christian Dick, also member oft the Fraunhofer IEE team, thinks about constructing a big concrete hollow upon the sea.\n\nThe TV station ZDF nano produced a documentary about the field study StEnSea in lake “Bodensee”. Christian Dick was cited that “the ball exactly worked like it was supposed to work”. The most important finding was that an air-connection to the surface is not needed, reducing the technical effort significantly. Project leader Matthias Puchta from Fraunhofer IEE said “by pumping out the water we created a nearly total vacuum. Demonstrating that was very exciting, because nobody was able to do that before by using this technology. We showed it works.” For maintenance and possible technical problems the technology will be located in a cylinder, easy to recover and maintain with a robotic submarine. After all this technology could be “a mosaic of our future energy supply\".\n\nThis opinion was shared by Swiss radio channel SRF as they reported about the project as a “potentially path-breaking experiment”. Thanks to the successful project in lake “Bodensee”, where energy was fed in a test grid and drawn from it, the team is looking forward into taking the next step: Installing a concrete ball of a diameter 10 times higher than the pilot project (30 meters). Due to Germany's too shallow coastlines the country drops out as a site for further projects. In return the Spanish coastline offers good conditions for a long-term-project. This long-term-project should last between three and five years under real-life conditions and is supposed to gain the data for the subsequent commercialization. This energy storage solution is not only a chance for the problem of the wind industry in storing energy but is also ecologically harmless.\n\n\"Der Spiegel\" reported that the technology of StEnSea could be also interesting for Offshore-Wind-Parks. The economically efficient storage of surplus energy is one of the key tasks for the grid and the energy market, as more and more renewables are taken into the system. Therefore, the technology's role in reorganizing the energy system can be crucial.\n"}
{"id": "48991154", "url": "https://en.wikipedia.org/wiki?curid=48991154", "title": "Stratoscale", "text": "Stratoscale\n\nStratoscale is a software company offering software-defined data center technology, marketed with the term hyper-converged infrastructure and cloud computing capabilities. Stratoscale combines compute, storage, and networking hardware with no additional third party software.\n\nStratoscale was founded in 2013 by Ariel Maislos. Stratoscale is headquartered in Israel with offices in Herzliya and Haifa, and offices in North America in Sunnyvale, California, Boston, Massachusetts, and New York City, New York. Stratoscale announced Stratoscale Symphony, in December 2015, selling through channel partners.\n\nStratoscale raised $70 million from Battery Ventures, Bessemer Venture Partners, Intel Capital, Cisco, Leslie Ventures, Qualcomm Ventures, and SanDisk.\n\nStratoscale Symphony is marketed for software-defined data centers or hyper-converged infrastructure. The software is intended to work on customers' hardware. Stratoscale Symphony is available on subscription basis. The Symphony suite may be deployed on commodity x86 servers to provide an Amazon Web Services (AWS) capability with the capacity to augment legacy VMware. In 2016, Stratoscale released Symphony 3.\n\nStratoscale has channel partners, technology partners, and system partners. Channel partners consist of resellers, integrators, and distributors. Technology partners include CloudEndure, Cloudera, Docker, Hortonworks, Intel, Mellanox Technologies, Midokura, OpenStack, and SanDisk. System partners adjust for server configurations and environments. System partners include Cisco, Hewlett Packard Enterprise (HPE), Infinidat, Lenovo, and Supermicro.\n\n"}
{"id": "43148907", "url": "https://en.wikipedia.org/wiki?curid=43148907", "title": "Types of e-commerce", "text": "Types of e-commerce\n\nVarious types of e-commerce platforms fall into several industry classifications based upon their licensing model, sales scenario and data exchange.\n\nOn-premises e-commerce software usually requires initial one time purchase investment in terms of licensing fees. Also, it implies extra costs related to hardware and installation services as well as data migration and on-going maintenance fees that are usually charged on a yearly basis for software updates and support.\n\nAdvantages:\n\nDisadvantages:\n\nSoftware as a Service (SaaS)- is a cloud based delivery model in which applications are hosted and managed in a service provider's data center, paid for on a subscription basis and accessed via a browser over an internet connection.\n\nAdvantages:\n\nDisadvantages:\n\nFully Managed (FM) E-commerce - is the next step of Platform as a Service (PaaS). As a basis, PaaS consists of e-commerce software and hardware hosting. In addition to this, fully managed e-commerce solutions provide services like product picture taking, image editing, data management, customer support, marketing consulting. FM E-Commerce is offered to brick-and-mortar stores as a B2B solution to help them start selling online quickly and at low cost. The licensing model is usually based on the sales volume.\n\nOpen source e-Commerce is a free of charge platform that doesn’t imply licenses fee. Furthermore, open source users are also responsible for installing, maintaining, securing and configuring the software on their own servers. In order to set up an open source platform, basic technical expertise is required in the areas of web design and development. Software products that are distributed as open source are generally free, and users can access and modify the source code.\n\nAdvantages:\n\nDisadvantages:\n"}
{"id": "39224245", "url": "https://en.wikipedia.org/wiki?curid=39224245", "title": "YDS algorithm", "text": "YDS algorithm\n\nYDS is a scheduling algorithm for dynamic speed scaling processors which minimizes the total energy consumption. It was named after and developed by Yao et al. There is both an online and an offline version of the algorithm.\n\nDefinitions:\n\nThe algorithm then works as follows:\ncodice_1\n\nIn other terms it's a recursive algorithm that will follow these steps until all jobs are scheduled:\n\n\nFor any Job instance, the algorithm computes an optimal schedule minimizing the total energy consumption.\n\n"}
