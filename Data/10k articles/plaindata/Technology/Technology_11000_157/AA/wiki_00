{"id": "40983077", "url": "https://en.wikipedia.org/wiki?curid=40983077", "title": "Arnan Azaryahu", "text": "Arnan Azaryahu\n\nArnan \"Sini\" Azaryahu(26 June 1917 – 26 November 2008) was a long-time political insider within the Israeli government, where he served as a trusted aide and confidant to Minister Yisrael Galili, a close ally and advisor to Israeli prime minister Golda Meir.\n\nArnan \"Sini\" Azaryahu was an officer in the general staff of the Palmach, a leader in the Kibbutz Hameuchad movement, and later a senior aide and confidant to Minister Yisrael Galili.\nSini was born in 1917 in Haifa to Sarah and Yosef Azaryahu, teachers and Zionists who were among the founders of Tel Aviv. In 1941, Sini was recruited by the Palmach, a Mandate-era Jewish fighting force. He was cultural and education officer just two years later, a position that he held until the Palmach's dissolution in 1949. Sini also served as Galili's adjutant officer while he was head of the national headquarters of the Haganah, a Jewish defense organization during the British Mandate that became the backbone of the Israeli Defense Force after independence. He held the same position under General Yigal Allon along the southern front of the War of 1948. After independence, Sini joined the secretariat of the Hakibbutz Hameuchad, a part of the Kibbutz Movement associated with the Achdut Ha'Avodah party. He became an envoy to the United States in this capacity in the late 1950s.\nThough Sini was not a member of the narrow group around Prime Minister Ben Gurion that had given birth to the nuclear project, his close relationship with many top Israeli security officials, especially Munya Mardor and Galili, gave him an inside look at some key junctions in Israel's nuclear history. In 1962 Sini prepared a memo for Galili for the first closed-door Israeli top-level strategic conference on the nuclear program that Ben Gurion conducted. The compromise policy that Ben Gurion ultimately adopted in the wake of that conference may have planted the seeds of Israel's nuclear opacity policy. Sini also often discussed nuclear issues in meetings with officials, such as his conversations with Munya Mardor, the director of RAFAEL, after the Six-Day War and with Galili before and after the Yom Kippur War of 1973.\nAfter the Likud party gained power in 1977, Sini left his role in the government. Two years later, he founded the Galilee Center in Yad Tabenkin, which focused on security issues. Sini died in November 2008.\n\n\n"}
{"id": "41780028", "url": "https://en.wikipedia.org/wiki?curid=41780028", "title": "Aroma lamp", "text": "Aroma lamp\n\nAroma lamps, or diffusers, are used to diffuse essential oils in aromatherapy and esoterics.\n\nThere are four different types of diffusers:\n\nIs a device that doesn't use water or heat as other diffusers. The bottle of essential oil is attached directly to the nebulizer. As a result, the unit uses 100% pure essential oil. The benefit of using this device is a strong concentration of the essential oil. However, it can be noisier than the other devices.\n\nBasically, it is the same as Nebulizer, but it uses water. This makes the mist not so concentrated. This device can be used as a humidifier, that can be very helpful especially in the winter. \n\nThese types of diffusers contain an ultrasonic transducer, which vaporizes the water/oil mixture through cavitation and creates instantaneous vapor.\n\nThis is a device that uses a pad or a filter to diffuse the essential oils. One of the disadvantages of this tool is that the light elements of the essential oils will be circulated around the ceiling first and will only come down at the end of the process.\n\nUsually, a candle driven lamps that use a small candle under a bowl to vaporize a mixture of water and oil. This device is very cheap and doesn't need a special maintenance, however, the heat can change the chemical structure of the oils.\nCertain plug-in nightlight units made by companies such as Air Wick are used in combination with glass bulbs of scented oil, filling a room with fragrance and light.\n\n"}
{"id": "42562538", "url": "https://en.wikipedia.org/wiki?curid=42562538", "title": "Artificial marbling", "text": "Artificial marbling\n\nArtificial marbling is the injection of animal fat and/or vegetable oil into lean meat in order to simulate the appearance of marbling and attempt to improve the palatability of inexpensive cuts by preventing them from drying out or losing flavour during the freezing or cooking process. Lean cuts of beef are one common target of artificial marbling. The process may also be performed on pork. It has been described as a more technologically advanced form of larding.\n\nThe Agri-Food and Veterinary Authority of Singapore labels artificially marbled beef as \"beef tallow-injected further processed beef\". Hokubee Australia markets its fat-injected beef product under the brand name Meltique, though its halal products use canola oil instead of fat, due to concerns about cross-contamination with pork tallow.\n\nThe United States Department of Agriculture requires that when fats and oils are added to red meat products such as roast beef and steaks, the product indicate this prominently, for example as part of the product name or as a product name qualifier. Additionally, products that appear to be of a higher quality as a result of fat injections must include a statement to indicate this, such as \"injected with beef fat\", \"artificially marbled—simulated fat covered\", or \"product may appear to be of a higher quality than the actual grade“.\n\nSome unscrupulous purveyors have taken advantage of the improved appearance in an attempt to pass it off as the higher-quality naturally marbled meat, especially in environments such as restaurants, where the meat's packaging and labelling are not visible to customers.\n\nA variety of injectants may be used to create artificial marbling. The injectant may be pure fat (such as tallow) heated to a high temperature to melt it while sufficiently cool so as not to cook the meat when injected, fat suspended in an emulsifier, fat blended with vegetable oils, or fatty acids such as conjugated linoleic acid (CLA) in powder form.\n\nResearch into artificial marbling through fat injection was carried out as early as the 1960s. A 1999 Polish study found that fat-injected horse meat tended to retain more flavour than non-injected control samples after two-week and three-month periods of cold storage. Another 2007 U.S. study by Tyson Foods and the University of Arkansas used beef strip loin and injected it with conjugated linoleic acid (CLA) in powder form, or oil with high concentration of CLAs. Under visual assessment by independent panelists, the USDA marbling score of the powder-treated meat increased by two levels. The powder-treated meat tended to be juicier than the oil-treated meat after cooking, but otherwise the injected meat had similar colour and flavour to the untreated meat. A 2017 study from West Texas A&M University found that the injection of beef loins with pork back fat resulted in higher consumer preference scores for juiciness and overall preference. Cooked steaks injected with pork fat had a lower moisture content but higher fat content, and trained panelists could detect an \"off aroma.\" \n\n"}
{"id": "47148792", "url": "https://en.wikipedia.org/wiki?curid=47148792", "title": "August Home", "text": "August Home\n\nAugust, Inc is a San Francisco home automation company, focusing on Wi-Fi connected door locks and doorbell cameras. The company was founded in November 2012 by Yves Béhar and Jason Johnson.\n\nOn October 19, 2017, Swedish lock manufacturer Assa Abloy announced an acquisition of August Home. The deal closed in December 2017.\n\nAs of July 2018, August Home had sold over one million smart locks and cameras.\n\nIn May 2013, August released their first smart lock. The lock had a metal frame and was controlled using Bluetooth 4.0 with a smartphone app. As with other August door locks, the device clips on to an existing deadbolt on the inside portion of a door, still allowing the use of a traditional key. A Wi-Fi bridge was later released allowing remote access to the lock, and the use of virtual assistants (such as Amazon Alexa).\n\nIn October 2015, the company debuted a suite of new products including a second generation smart lock, a video doorbell, and a keypad for users without a phone. The company also announced August Access, a platform to let couriers from Postmates, Handy, and other services get access to the lock through a one time code. The service was later expanded to include Walmart in select U.S. markets. A HomeKit compatible version of the lock was also released the following year.\n\nA cheaper version of the smart lock was released in 2017, along with a Z-Wave compatible version designed for professional installation. The new locks also added motion sensors to know whether the door is open or closed.\n\nThe August Access platform expanded to include locks for Yale and Emtek in January 2018.\n\nIn August 2016, a white-hat hacker showcased a vulnerability with August's one-time access codes allowing someone to use them once expired at DEF CON. The company patched the issue later that month.\n"}
{"id": "28634992", "url": "https://en.wikipedia.org/wiki?curid=28634992", "title": "Augustine's laws", "text": "Augustine's laws\n\nAugustine's laws were a series of tongue in cheek aphorisms put forth by Norman Ralph Augustine, an American aerospace businessman who served as Under Secretary of the Army from 1975 to 1977. In 1984 he published his laws. The book and several of the laws were the topic of an article in Sound and Vibration magazine in March 2012.\n\nLaw Number I: The best way to make a silk purse from a sow's ear is to begin with a silk sow. The same is true of money.\n\nLaw Number II: If today were half as good as tomorrow is supposed to be, it would probably be twice as good as yesterday was.\n\nLaw Number III: There are no lazy veteran lion hunters.\n\nLaw Number IV: If you can afford to advertise, you don't need to.\n\nLaw Number V: One-tenth of the participants produce over one-third of the output. Increasing the number of participants merely reduces the average output.\n\nLaw Number VI: A hungry dog hunts best. A hungrier dog hunts even better.\n\nLaw Number VII: Decreased business base increases overhead. So does increased business base.\n\nLaw Number VIII: The most unsuccessful four years in the education of a cost-estimator is fifth grade arithmetic.\n\nLaw Number IX: Acronyms and abbreviations should be used to the maximum extent possible to make trivial ideas profound...Q.E.D.\n\nLaw Number X: Bulls do not win bullfights; people do. People do not win people fights; lawyers do.\n\nLaw Number XI: If the Earth could be made to rotate twice as fast, managers would get twice as much done. If the Earth could be made to rotate twenty times as fast, everyone else would get twice as much done since all the managers would fly off.\n\nLaw Number XII: It costs a lot to build bad products.\n\nLaw Number XIII: There are many highly successful businesses in the United States. There are also many highly paid executives. The policy is not to intermingle the two.\n\nLaw Number XIV: After the year 2015, there will be no airplane crashes. There will be no takeoffs either, because electronics will occupy 100 percent of every airplane's weight.\n\nLaw Number XV: The last 10 percent of performance generates one-third of the cost and two-thirds of the problems.\n\nLaw Number XVI: In the year 2054, the entire defense budget will purchase just one aircraft. This aircraft will have to be shared by the Air Force and Navy 3-1/2 days each per week except for leap year, when it will be made available to the Marines for the extra day.\n\nLaw Number XVII: Software is like entropy. It is difficult to grasp, weighs nothing, and obeys the Second Law of Thermodynamics; i.e., it always increases.\n\nLaw Number XVIII: It is very expensive to achieve high unreliability. It is not uncommon to increase the cost of an item by a factor of ten for each factor of ten degradation accomplished.\n\nLaw Number XIX: Although most products will soon be too costly to purchase, there will be a thriving market in the sale of books on how to fix them.\n\nLaw Number XX: In any given year, Congress will appropriate the amount of funding approved the prior year plus three-fourths of whatever change the administration requests, minus 4-percent tax.\n\nLaw Number XXI: It's easy to get a loan unless you need it.\n\nLaw Number XXII: If stock market experts were so expert, they would be buying stock, not selling advice.\n\nLaw Number XXIII: Any task can be completed in only one-third more time than is currently estimated.\n\nLaw Number XXIV: The only thing more costly than stretching the schedule of an established project is accelerating it, which is itself the most costly action known to man.\n\nLaw Number XXV: A revised schedule is to business what a new season is to an athlete or a new canvas to an artist.\n\nLaw Number XXVI: If a sufficient number of management layers are superimposed on each other, it can be assured that disaster is not left to chance.\n\nLaw Number XXVII: Rank does not intimidate hardware. Neither does the lack of rank.\n\nLaw Number XXVIII: It is better to be the reorganizer than the reorganizee.\n\nLaw Number XXIX: Executives who do not produce successful results hold on to their jobs only about five years. Those who produce effective results hang on about half a decade.\n\nLaw Number XXX: By the time the people asking the questions are ready for the answers, the people doing the work have lost track of the questions.\n\nLaw Number XXXI: The optimum committee has no members.\n\nLaw Number XXXII: Hiring consultants to conduct studies can be an excellent means of turning problems into gold, your problems into their gold.\n\nLaw Number XXXIII: Fools rush in where incumbents fear to tread.\n\nLaw Number XXXIV: The process of competitively selecting contractors to perform work is based on a system of rewards and penalties, all distributed randomly.\n\nLaw Number XXXV: The weaker the data available upon which to base one's conclusion, the greater the precision which should be quoted in order to give the data authenticity.\n\nLaw Number XXXVI: The thickness of the proposal required to win a multimillion dollar contract is about one millimeter per million dollars. If all the proposals conforming to this standard were piled on top of each other at the bottom of the Grand Canyon it would probably be a good idea.\n\nLaw Number XXXVII: Ninety percent of the time things will turn out worse than you expect. The other 10 percent of the time you had no right to expect so much.\n\nLaw Number XXXVIII: The early bird gets the worm. The early worm...gets eaten.\n\nLaw Number XXXIX: Never promise to complete any project within six months of the end of the year, in either direction.\n\nLaw Number XL: Most projects start out slowly, and then sort of taper off.\n\nLaw Number XLI: The more one produces, the less one gets.\n\nLaw Number XLII: Simple systems are not feasible because they require infinite testing.\n\nLaw Number XLIII: Hardware works best when it matters the least.\n\nLaw Number XLIV: Aircraft flight in the 21st century will always be in a westerly direction, preferably supersonic, crossing time zones to provide the additional hours needed to fix the broken electronics.\n\nLaw Number XLV: One should expect that the expected can be prevented, but the unexpected should have been expected.\n\nLaw Number XLVI: A billion saved is a billion earned.\n\nLaw Number XLVII: Two-thirds of the Earth's surface is covered with water. The other third is covered with auditors from headquarters.\n\nLaw Number XLVIII: The more time you spend talking about what you have been doing, the less time you have to spend doing what you have been talking about. Eventually, you spend more and more time talking about less and less until finally you spend all your time talking about nothing.\n\nLaw Number XLIX: Regulations grow at the same rate as weeds.\n\nLaw Number L: The average regulation has a life span one-fifth as long as a chimpanzee's and one-tenth as long as a human's, but four times as long as the official's who created it.\n\nLaw Number LI: By the time of the United States Tricentennial, there will be more government workers than there are workers.\n\nLaw Number LII: People working in the private sector should try to save money. There remains the possibility that it may someday be valuable again.\n\nHis most cited law is number 16, which shows that defense budgets grow linearly but the unit cost of a new military aircraft grows exponentially:\nIn the year 2054, the entire defense budget will purchase just one tactical aircraft. This aircraft will have to be shared by the Air Force and Navy 3½ days each per week except for leap year, when it will be made available to the Marines for the extra day.\"\n\n"}
{"id": "27795700", "url": "https://en.wikipedia.org/wiki?curid=27795700", "title": "Automatic test switching", "text": "Automatic test switching\n\n\"Automatic test system switching\"' test equipment allows for high-speed testing of a device or devices in a test situation, where strict sequences and combinations of switching must be observed. By automating the process in this way, the possibility of test errors and inaccuracies is minimized, and only systematic errors would generally be encountered due to such as an incorrect programmed test condition. This eliminates error due to human factors and allows application of a standard test sequence repetitively. The design of a test system’s switching configuration is governed by the test specification, which is derived from the functional tests to be performed.\n\nA typical test system would involve the connection of input and outputs of the device under test to the test equipment, which is usually controlled by an electronic program generated by a computer or a Programmable Logic Controller.\n\nThe simplest definition of a switch is “a device that opens or closes a circuit.” \n\nA relay is an electronically operated switch. Three relay types are commonly used in automated test system switching:\n\n\nThe ideal switch:\n\n\nIt’s important to recognize, however, that real-life switches are not ideal, so when calculating the overall system accuracy, the effects of the switch itself and all the switching hardware in the system must be factored in.\n\nAs a signal travels from its source to its destination, it can encounter various forms of interference and sources of error, so whenever a signal passes through a connecting cable or switch point, it may be degraded. For example, in low current and high resistance applications, unshielded cabling can introduce leakage currents that will degrade measurement accuracy. Unshielded cable can result in noisy readings for low current and high resistance applications, especially if the cabling runs adjacent to equipment generating electromagnetic interference.\n\nThree terms are used to describe the configuration of a relay: pole, throw, and form.\n\nPole refers to the number of common terminals within a given switch. Throw refers to the number of positions in which the switch may be placed to create a signal path or connection. Figure lA illustrates a single-pole, single-throw normally open switch (SPST NO). Figure 1B shows a single-pole, double-throw (SPDT) switch. One terminal is normally-open (NO) and the other is normally-closed (NC). Depending on the state of the switch, one or the other position is connected to the common terminal (COM). One signal path is broken before the other is connected, so this is referred to as a break-before-make configuration.\n\nWhen more than one common terminal is used, the number of poles increases. Figure 1C shows a double-pole, single-throw (DPST) switch. Both poles are actuated simultaneously when the relay is energized. In this case, both poles are either always closed or always open. Figure 1D illustrates a double-pole, double throw (DPDT) switch.\n\nContact form, or simply form, is the term relay manufacturers use to describe a relay's contact configuration. \"Form A” refers to a single-pole, normally open switch. \"Form B\" indicates a single-throw, normally closed switch, and \"Form C\" indicates a single-pole, double-throw switch. Virtually any contact configuration can be described using this format.\n\nA variety of switching configurations are commercially available for test system development:\n\n\nThe scanner (Figure 2) is used to connect multiple inputs to a single output in sequential order. Only one relay is closed at any one time. In its most basic form, relay closure proceeds from the first channel to the last, but some scanner systems allow skipping channels. Typical scanner switching applications include component burn-in testing, monitoring time and temperature drift in circuits, and taking data on system variables like temperature, pressure, flow, etc.\n\n\nLike a scan configuration, multiplex switching can be used to connect one instrument to multiple devices (1:N) or multiple instruments to a single device (N:1), but it offers much more flexibility than the scanner configuration because it permits multiple simultaneous connections and both sequential and non-sequential switch closures. Typical applications of multiplex switching include capacitor leakage, insulation resistance, and contact resistance test systems for multiple devices.\n\n\nThe matrix switch configuration is the most versatile because it can connect multiple inputs to multiple outputs. A matrix is useful when connections must be made between several signal sources and a multipin device, such as an integrated circuit or resistor network.\n\nUsing a matrix switch card allows connecting any input to any output by closing the switch at the intersection (crosspoint) of a given row and column. The most common terminology to describe the matrix size is M rows by N columns (MxN). Matrix switch cards generally have two or three poles per crosspoint. As shown in Figure 3, a 5VDC source can be connected to any two terminals of the device under test (DUT). A function generator supplies pulses between another two terminals. Operation of the DUT can be verified by connecting an oscilloscope between two other terminals. The DUT pin connections can easily be programmed, so this system can be used to test a variety of components.\n\nSome performance tradeoffs are typically necessary when choosing a matrix card for use with mixed signals. For example, if both high frequency and low current signals must be switched, take extra care when reviewing the specifications of the card. The card chosen must have wide bandwidth, as well as good isolation and low offset current. A single matrix card may not satisfy both requirements completely, so the system builder must decide which switched signal is more critical.\n\nIn a system with multiple cards, card types should not be mixed if their outputs are connected together. For example, a general-purpose matrix card with its output connected in parallel with a low current matrix card will degrade the performance of the low current card.\n\nA large test system may require more rows and/or columns than a single switch card can accommodate, but it’s possible to expand a matrix by joining the rows and/or columns of several cards together. Depending on the switch card and mainframe selected, the rows of the cards may be connected together through the backplane of the mainframe or the rows may be wired externally.\n\nThe isolated, or independent, switch configuration consists of individual relays, often with multiple poles, with no connections between relays. Isolated relays are commonly used in power and control applications to open and close different parts of a circuit that are at substantially different voltage levels. Applications for isolated relays include controlling power supplies, turning on motors and annunciator lamps, and actuating pneumatic or hydraulic valves. Figure 4 illustrates a single isolated relay or actuator, in which a single-pole normally open relay is controlling the connection of the voltage source to the lamp. This relay connects one input to one output. An isolated relay can have more than one pole and can have normally closed contacts as well as normally open contacts.\n\nIsolated relays are not connected to any other circuit, so the addition of some external wiring makes them suitable for building very flexible and unique combinations of input/output configurations.\n\nGiven that the relays are isolated from each other, the terminals of each channel on the switch card are independent from the terminals of the other channels. As shown in Figure 5, each isolated Form A relay has two terminals. Two-pole isolated relays would have four terminals (two inputs and two outputs). A Form C isolated relay would have three terminals.\n\nThe term cold switching indicates that a switch is activated with no signal applied. Therefore, no current will flow when the switch is closed, and no current will be interrupted when the switch is opened. In contrast, in hot switching, voltage is present and current will flow immediately once the contacts close. When the switch is opened, this current will be interrupted and can cause arcing.\n\nCold switching allows power to be applied to the device under test in a controlled manner. Its primary advantage is a longer switch life than with hot switching (up to a thousand times more cycles than with hot switching). Cold switching also eliminates arcing at the relay contacts and any radio frequency interference that might be caused by arcing. Hot switching might be necessary if close control must be exercised in the period between the application of power and the making of the measurement. For example, hot switching is typically used where digital logic is involved, because devices might change state if power is interrupted even for a moment.\n\nWith relatively large relays, hot switching may be necessary to ensure good contact closure. The connection may not be reliable without the “wetting” action of the current through the contacts.\n\n"}
{"id": "53774008", "url": "https://en.wikipedia.org/wiki?curid=53774008", "title": "Avid Larizadeh Duggan", "text": "Avid Larizadeh Duggan\n\nAvid Larizadeh-Duggan (born ) is an Iranian-French-American entrepreneur and venture capitalist. She is the EVP Group, Chief Strategy and Business Officer at Kobalt Music Group and a non executive director of Barclays Bank UK.\n\nShe is a Mayfield Fellow and a Kauffman Fellow. In 2016 she was named one of the World Economic Forum's \"Young Global Leaders\".\n\nLarizadeh Duggan was born in Dallas Texas right before the Iranian Revolution. She has a sister, Roxanna Larizadeh. They are the grandchildren of Kazem Khosrowshahi, the cofounder of pharmaceutical conglomerate KBC – a company nationalized in 1980 after the Iranian Revolution. Kazem Khosrowshahi entered politics in August 1977 as Minister of Commerce, but his tenure was short lived due to the Revolution. Larizadeh Duggan’s father, also an entrepreneur, and her mother had to leave Iran in 1979 and start over in Europe where they immigrated. Larizadeh Duggan and her sister grew up in Paris, France.\n\nLarizadeh Duggan attended L’Ecole Active Bilingue Jeannine Manuel before studying at Stanford University, where she graduated with a bachelor’s degree in 2000 and master’s degree in 2001 both in engineering. While at Stanford, Larizadeh Duggan was a Mayfield Fellow and a Member of Stanford’s Women Honor Society, Cap and Gown.\n\nLarizadeh Duggan also has an MBA from Harvard Business School where she graduated in 2006.\n\nWhile studying at Stanford University, she worked for Linkexchange (acquired by Microsoft in 1998) and Tellme Networks (Acquired by Microsoft in 2007).\n\nAfter graduating from Stanford, she joined eBay on the product management team. She launched eBay’s first suite of global online selling tools, Selling Manager and Selling Manager Pro, for which she holds a patent. She also led the design and launch of eBay’s picture services.\n\nWhile at Harvard Business School, she also worked in product management at Skype Technologies. In 2006 after graduating from Harvard Business School, she joined Accel Partners in London as an Associate where she focused on software and Internet investments.\n\nShe left Accel Partners in 2009 to cofound Boticca, a global marketplace for independent brands of fashion accessories, where she was the COO, running product, engineering and editorial. Boticca was featured in Vogue, selected by Lady Gaga for her 2011 European tour, and worn by Kate Moss, Cameron Diaz and Jessica Alba. Boticca was acquired by Wolf & Badger.\n\nIn 2014 Larizadeh Duggan joined Google Ventures (now GV) as the only female General Partner when GV launched its European fund. She was an investor, director and advisor to a variety of businesses, most in the software and Internet domain, including Kobalt, Lost My Name, Yieldify, Resolution Games and Breather. \n\nIn February 2018, She left GV to join Kobalt, one of her portfolio companies as EVP Group, Chief Strategy and Business Officer. \n\nLarizadeh Duggan was appointed as a non-executive director of Barclays UK in 2017\n\nLarizadeh Duggan has made a number of angel investments in companies including Jampp and Okta. In 2017 Okta went public on the NASDAQ stock exchange (OKTA).\n\nIn 2016 she was named one of the World Economic Forum's \"Young Global Leaders\".\n\nLarizadeh Duggan has been named one of the 50 most inspiring female influencers, entrepreneurs, business leaders, academics and policy makers from across Europe three years in a row from 2015 to 2017. She has been named as “one of the most influential European female VCs” by Tech.eu “one of the 50 most influential women in UK IT” by Computer Weekly.\n\nLarizadeh Duggan has been involved with various organizations linked to education. She is on the Harvard Business School European Advisory Board. She is also on the Board of Trustees of Founders4Schools. She led Code.org in the UK , founded by her cousins Hadi and Ali Partovi, and helped over 13 million people try an hour of code in the UK. She has contributed to Forbes and the World Economic Forum.\n\nShe lives in London with her husband and daughter.\n"}
{"id": "53063942", "url": "https://en.wikipedia.org/wiki?curid=53063942", "title": "Bedside sleeper", "text": "Bedside sleeper\n\nA bedside sleeper, also referred to as a sidecar sleeper or bedside bassinet, is a bassinet or baby cot that attaches to the parents' bed, allowing newborns to sleep next to their parents safely. This is a form of safe co-sleeping, and has little risks associated with sudden infant death syndrome, unlike bedsharing. Bedside sleepers are a component of rooming-in, a practice followed in hospitals to keep the baby by the mother's bed, giving her time to establish a stronger bond with her baby.\n\nA bedside sleeper is defined by the United States government as \"a rigid frame assembly secured to an adult bed that is intended to provide a sleeping environment for infants.\" Usually, one wall of the bedside sleeper is lower than the others, which allows the parent to easily reach for the child at night. Most bedside sleepers are multi-mode, meaning that they can be converted into bassinets and/or play yards.\n\nA bedside bassinet tends to have four sides, like a regular baby crib. It can be positioned near the parents' bed as an unattached bedside bassinet, or attached to the bed. This arrangement allows parents to more easily attend to their baby during the night. Because bedside bassinets have four rails, quick, easy access to the occupant can still be limited.\n\nA bedside sleeper or sidecar is similar to a bedside bassinet in that it attaches to the parents' bed, but only has three crib walls, which allows the baby to sleep at the same height as the parents, and there is no obstruction to reaching out for the baby. Bedside sleepers allow parents to keep the baby close without it sleeping in the dimensional space of the family bed.\n\nCo-sleeping is an ancient practice whereby babies sleep close to their parents and not in a different room, where they can sense another's presence. According to the Natural Child Project, co-sleeping is an unquestioned practice in much of southern Europe, Asia, Africa and Central and South America. However, one of the most common types of co-sleeping is bedsharing, which can be dangerous.\n\nThe American Academy of Pediatrics encourages room-sharing (sleeping in the same room but on separate surfaces), but it recommends against bed-sharing with infants, due to instances of SIDS. In a study of 321 SIDS cases, the \"British Medical Journal\" indicated that the largest percentage of SIDS cases arose from babies who slept in a different room than the parents, suggesting that co-sleeping on a separate surface is the safest method of infant sleep. Co-sleeping—sleeping with a baby nearby—is gaining popularity in the United States. Bedside sleepers were created to allow parents and babies to gain the benefits of co-sleeping while minimizing instances of SIDS.\n\n\nLike other infant sleep products, bedside sleepers may also pose various risks to babies of all shapes and sizes. The main issue that most bedside sleeper users and manufacturers must consider is the risk that a baby might fall into a gap between the bedside sleeper and the adult bed mattress, which could cause entrapment injuries and/or strangulation.\n"}
{"id": "27755111", "url": "https://en.wikipedia.org/wiki?curid=27755111", "title": "British Electrotechnical Approvals Board", "text": "British Electrotechnical Approvals Board\n\nThe British Electrotechnical Approvals Board was an electrical safety and certification organisation in the UK, until the introduction of Harmonised European Standards meant that local certification of electrical products was no longer permitted. The BEAB Mark is now owned by Intertek Group.\n\nIt was formed in 1960 by Regional Electricity Companies, manufacturers and retailer as the British Electrical Approvals Board for Domestic Appliances. It was based on \"Cockspur Street\" in London and test facilities at a laboratory owned by the British Electrical Development Association in Leatherhead; this later became the Appliance Testing Laboratories of the Electricity Council. In 1961 its headquarters moved to London Road,Kingston upon Thames. In 1974 it moved to The Green, Hersham near Walton-on-Thames in Surrey. In 1962 it extended the range of appliances it tested.\n\nIn 1971 it changed its name to the British Electrotechnical Approvals Board for Household Equipment. They joined forces with the British Approvals Board for Telecommunications. The headquarters moved to \"Station View\" in Guildford.\n\nIn 2004 it merged with ASTA Certification Services to form ASTA BEAB Certification Services. In June 2007, this company was acquired by Intertek. The testing is done in Leatherhead. Intertek was formed from ITS Testing & Certification Ltd, which became Intertek in September 2003.\n\nDomestic electrical appliances with the BEAB Mark of Approval meant that they had reached a required safety standard (BS 3456).\n\nThis mark means the item has been tested and deemed to be safe - it is a safety guarantee. That does not mean the item will always be safe (product quality may vary, and wear and tear will affect inherent safety), but the standard product should be. Electrical equipment is important to be safe because apart from the obvious danger of causing electric shocks, faulty electrical equipment can cause fires, such as electric blankets.\n\nWhere fires have been caused by domestic electrical appliances, they are examined at the laboratory to determine what caused the fire.\n\nThere is also the BEAB Controls Mark for controls and switches.\n\n\n"}
{"id": "241799", "url": "https://en.wikipedia.org/wiki?curid=241799", "title": "Chlordane", "text": "Chlordane\n\nChlordane is a chemical compound and also part of a similarly named pesticide mixture resulting from synthesis (main components- heptachlor, chlordane, and nonachlor). These highly chlorinated cyclodienes were classified as organic pollutants hazardous for human health. They are both resistant to degradation in the environment and in humans/animals and readily accumulate in lipids (fats) of human/animals. Exposure to these compounds has been linked to cancers and many other diseases.\n\nIn the United States, chlordane was used for termite-treatment of approximately 30 million homes until it was banned in 1988. Chlordane was banned 10 years earlier for food crops like corn and citrus, and on lawns and domestic gardens.\n\nTechnical chlordane development was by chance by Julius Hyman in 1948, during a search for possible uses of a by-product of synthetic rubber manufacturing. By chlorinating this by-product, persistent and potent insecticides were easily and cheaply produced. The chlorine atoms, 7 in the case of heptachlor and 8 in chlordane, and 9 in the case of nonachlor, surround and stabilize the cyclodiene ring and thus these compounds are referred to as cyclodienes. Other members of the cyclodiene family of organochorine insecticides are aldrin and its epoxide, dieldrin, as well as endrin, which is a stereoisomer of dieldrin. Cyclodiene derives its name from hexachlorocyclopentadiene a precursor in its production.\n\nHexachlorocyclopentadiene forms a Diels-Alder adduct with cyclopentadiene to give Chlordene intermediate [3734-48-3]; chlorination of this adduct gives predominantly two chlordane isomers, α and β, in addition to other products such as \"trans\"-nonachlor and heptachlor. The β-isomer is popularly known as gamma and is more bioactive. The mixture that is composed of 147 components is called technical chlordane.\n\nChlordane appears as a white or off-white crystals when synthesized, but it was more commonly sold in various formulations as oil solutions, emulsions, sprays, dusts, and powders. These products were sold in the United States from 1948 to 1988.\nBecause of concern for harm to human health and to the environment, the United States Environmental Protection Agency (EPA) banned all uses of chlordane in 1983, except termite control in wooden structures (e.g. houses). After many reports of chlordane in the indoor air of treated homes, EPA banned the remaining use of chlordane in 1988. The EPA recommends that children should not drink water with more than 60 parts of chlordane per billion parts of drinking water (60 ppb) for longer than 1 day. EPA has set a limit in drinking water of 2 ppb.\n\nChlordane is very persistent in the environment because it does not break down easily. Tests of the air in the residence of U.S. government housing, 32 years after chlordane treatment, showed levels of chlordane and heptachlor 10-15 times the Minimal Risk Levels (20 nanograms/cubic meter of air) published by the Centers for Disease Control. It has an environmental half-life of 10 to 20 years.\n\n In the years 1948–1988 chlordane was a common pesticide for corn and citrus crops, as well as a method of home termite control. Pathways of exposure to chlordane include ingestion of crops grown in chlordane-contaminated soil, inhalation of air in chlordane-treated homes and from landfills, and ingestion of high-fat foods such as meat, fish, and dairy, as chlordane builds up in fatty tissue. The United States Environmental Protection Agency reported that over 30 million homes were treated with technical chlordane or technical chlordane with heptachlor. Depending on the site of home treatment, the indoor air levels of chlordane can still exceed the Minimal Risks Levels (MRLs) for both cancer and chronic disease by orders of magnitude. Chlordane is excreted slowly through feces, urine elimination, and through breast milk in nursing mothers. It is able to cross the placenta and become absorbed by developing fetuses in pregnant women. A breakdown product of chlordane, the metabolite oxychlordane, accumulates in blood and adipose tissue with age.\n\nBeing hydrophobic, chlordane adheres to soil particles and enters groundwater only slowly, owing to its low solubility (0.009 ppm). It requires many years to degrade. Chlordane bioaccumulates in animals. It is highly toxic to fish, with an of 0.022–0.095 mg/kg (oral).\n\nOxychlordane (CHClO), the primary metabolite of chlordane, and heptachlor epoxide, the primary metabolite of heptachlor, along with the two other main components of the chlordane mixture, \"cis\"-nonachlor and \"trans\"-nonachlor, are the main bioaccumulating constituents. \"trans\"-Nonachlor is more toxic than technical chlordane and \"cis\"-nonachlor is less toxic.\n\nChlordane and heptachlor are known as persistent organic pollutants (POP), classified among the \"dirty dozen\" and banned by the 2001 Stockholm Convention on Persistent Organic Pollutants.\n\nExposure to chlordane/heptachlor and/or its metabolites (oxychlordane, heptachlor epoxide) are risk factors for type-2 diabetes, for lymphoma, for prostate cancer, for obesity, for testicular cancer, for breast cancer,\n\nAn epidemiological study conducted by the National Cancer Institute reported that higher levels of chlordane in dust on the floors of homes were associated with higher rates of non-Hodgkin lymphoma in occupants. Breathing chlordane in indoor air is the main route of exposure for these levels in human tissues. Currently, EPA has defined a concentration of 24 nanogram per cubic meter of air (ng/M) for chlordane compounds over a 20-year exposure period as the concentration that will increase the probability of cancer by 1 in 1,000,000 persons. This probability of developing cancer increases to 10 in 1,000,000 persons with an exposure of 100 ng/m and 100 in 1,000,000 with an exposure of 1000 ng/m.\n\nThe non-cancer health effects of chlordane compounds, which include diabetes, insulin resistance, migraines, respiratory infections, immune-system activation, anxiety, depression, blurry vision, confusion, intractable seizures as well as permanent neurological damage, probably affects more people than cancer. Trans-nonachlor and oxychlordane in serum of mothers during gestation has been linked with behaviors associated with autism in offspring at age 4-5. The Agency for Toxic Substances and Disease Registry (ATSDR) has defined a concentration of chlordane compounds of 20 ng/m as the Minimal Risk Level (MRLs). ATSDR defines Minimal Risk Level as an estimate of daily human exposure to a dose of a chemical that is likely to be without an appreciable risk of adverse non-cancerous effects over a specific duration of exposure.\n\nChlordane was applied under the home/building during treatment for termites and the half-life can be up to 30 years. Chlordane has a low vapor pressure and volatilizes slowly into the air of home/building above. To remove chlordane from indoor air requires either ventilation (Heat Exchange Ventilation) or activated carbon filtration. \nChemical remediation of chlordane in soils was attempted by the US Army Corps of Engineers by mixing chlordane with aqueous lime and persulfate. In a phytoremediation study, Kentucky bluegrass and Perennial ryegrass were found to be minimally affected by chlordane, and both were found to take it up into their roots and shoots. Mycoremediation of chlordane in soil have found that contamination levels were reduced. The fungus \"Phanerochaete chrysosporium\" has been found to reduce concentrations by 21% in water in 30 days and in solids in 60 days.\n\n"}
{"id": "21686501", "url": "https://en.wikipedia.org/wiki?curid=21686501", "title": "Chromatron", "text": "Chromatron\n\nThe Chromatron is a color television cathode ray tube design invented by Nobel prize-winner Ernest Lawrence and developed commercially by Paramount Pictures, Sony, Litton Industries and others. The Chromatron offered brighter images than conventional color television systems using a shadow mask, but a host of development problems kept it from being widely used in spite of years of development. Sony eventually abandoned it in favor of their famous Trinitron system using an aperture grille.\n\nColor television had been studied even before commercial broadcasting became common, but it was only in the late 1940s that the problem was seriously considered. At the time, a number of systems were being proposed that used separate red, green and blue signals (RGB), broadcast in succession. Most systems broadcast entire frames in sequence, with a colored filter (or \"gel\") that rotated in front of an otherwise conventional black and white television tube. Because they broadcast separate signals for the different colors, all of these systems were incompatible with existing black and white sets. Another problem was that the mechanical filter made them flicker unless very high refresh rates were used. In spite of these problems, the US Federal Communications Commission selected a sequential-frame 144 frame/s standard from CBS as their color broadcast in 1950.\n\nRCA worked along different lines entirely, using the luminance-chrominance system. This system did not directly encode or transmit the RGB signals; instead it combined these colors into one overall brightness figure, the \"luminance\". Luminance closely matched the black and white signal of existing broadcasts, allowing it to be displayed on existing televisions. This was a major advantage over the mechanical systems being proposed by other groups. Color information was then separately encoded and folded into the signal as a high-frequency modification to produce a composite video signal – on a black and white television this extra information would be seen as a slight randomization of the image intensity, but the limited resolution of existing sets made this invisible in practice. On color sets the signal would be noticed, decoded back into RGB, and displayed.\n\nAlthough RCA's system had enormous benefits, it had not been successfully developed because it was difficult to produce the display tubes. Black and white TVs used a continuous signal and the tube could be coated with an even painting of phosphor. With RCA's system, the color was changing continually along the line, which was far too fast for any sort of mechanical filter to follow. Instead, the phosphor had to be broken down into a discrete pattern of colored spots. Focusing the right signal on each of these tiny spots was beyond the capability of electron guns of the era. RCA's early experiments used three-tube projectors, or mirror-based systems known as \"Triniscope\".\n\nRCA eventually solved the problem of displaying the color images with their introduction of the shadow mask. The shadow mask consists of a thin sheet of aluminum with tiny holes photo etched into it, placed just behind the front surface of the picture tube. Three guns, arranged in a triangle, were all aimed at the holes. Stray electrons at the edge of the beam were cut off by the mask, creating a sharply focused spot that was small enough to hit a single colored phosphor on the screen. Since each of the guns aimed at the hole from a slightly different angle, the spots of phosphor on the tube could be separated slightly to prevent overlap.\n\nThe disadvantage of this approach was that for any given amount of gun power, the shadow mask filtered out the majority of the signal. To ensure there was no overlap of the signal on the screen, the dots had to be separated and covered perhaps 25% of its surface. This led to very dim images, requiring much greater power in order to provide a useful picture. Moreover, the system was highly dependent on the relative angles of the beams between the three guns, which required constant adjustment by the user to ensure the guns hit the correct colors. In spite of this, the technical superiority of the RCA system was overwhelming compared to the CBS system, and was selected as the new NTSC standard in 1953. The first broadcast using the new standard occurred on New Year's Day in 1954, when NBC broadcast the Tournament of Roses Parade.\n\nIn spite of this early start, only a few years after regularly scheduled television broadcasting had begun, consumer uptake of color televisions was very slow to start. The dim images, constant adjustments and high costs had kept them in a niche of their own. Low consumer acceptance led to a lack of color programming, further reducing the demand for the sets in a chicken or the egg situation. In the United States in 1960, only 1 color set was sold for every 50 sets sold in total.\n\nIn 1951 Ernest Lawrence, a professor at University of California, Berkeley best known as the father of the cyclotron, patented a new solution to the color decoding problem. This system, the \"Chromatron\" or simply \"Lawrence Tube\", used an electronic focusing system in place of RCA's mechanical solution. The system consisted of a series of thin metal wires or plates placed about 1/2 an inch behind the phosphor screen. The wires were used to electrically focus the beams and bend them onto the correct phosphors, which were arranged in vertical stripes. The phosphor covered over 50% of the screen's area, whereas the contemporary shadow masks covered about 25%. This led to much brighter images using the same amount of power.\n\nEach focusing element consisted of a pair of wires, and a conductive aluminum coating on the back of the phosphors. The screen was normally charged with a potential of 3000 to 4500 V between the wires and the aluminum, resulting in a curved electric field between the grid and the screen. When the electron beam from the gun entered the region between the grid and the screen it was accelerated and focused down to a tiny spot, normally impinging on the green phosphor. By varying the relative voltage between the two wires in each pair, the beam would be bent one direction or the other, allowing control over the color. Unlike a shadow mask, all of the signal eventually reached the screen, further reducing power requirements.\n\nIf the chrominance signal was missing, or deliberately ignored, the focusing system was disconnected and its power added to the gun. This produced a slightly stronger and unfocused beam, which hit all three colored strips and produced a B&W image. The spaces between the stripes meant the overall image would be about as bright as a conventional B&W set. A shadow mask set required all three guns to be powered to produce a B&W image, and since the color spots were small, their power had to be very high.\n\nYet another advantage of the near-screen focusing was that the electron beam was bent to hit the phosphors on the tube's faceplate at right angles no matter what the angle of the beam was behind the focuser. This allowed the tubes to be built with much higher deflection angles than conventional tubes – 72 degrees as opposed to a more typical 45. Chromatron tubes thus had much less depth for any given horizontal size.\n\nThe Chromatron also had several disadvantages. One was that there was a fundamental ratio between the acceleration provided by the grid and the electron gun at the back of the tube; in order to ensure that the grid could successfully control the beam, it had to have a significant proportion of the overall power. Unfortunately the mechanical layout of the grid limited it to voltages of about 5000 V or less, which in turn limited the electron gun to relatively low voltages around 8000 V. Thus the overall power in the Chromatron was less than in conventional tubes, offsetting its natural brightness to some degree.\n\nThe more pressing concern was the mechanical layout of the grid. Getting the fine wires to stay aligned with the strips of color on the screen proved to be the design's Achilles heel.\n\nThe University of California, Berkeley set up \"Chromatic Television Laboratories\" to commercially develop the system, in partnership with Paramount Pictures who provided development funding. They started producing the PDF 22-4 22 inch prototype tubes in 1952 and 1953, with a display area of 14 by 11 inches.\n\nIn practice the design proved to have serious problems. Since the focusing system had to quickly move the beam to generate the correct colors, very high voltages and powers had to be used, leading to arcing problems and radio frequency (RF) noise. The latter was particularly annoying when used as the basis of a television, as the noise interfered with the radio receivers that picked up the broadcasts. The University eventually abandoned their interest in Chromatron, but Paramount continued development as a system for displaying film during editing, which meant that the RF noise did not present a problem. Development was still continuing in the early 1960s when their work was bought by Sony.\n\nIn spite of these problems, the promise of the Chromatron system was so great that a number of companies continued development of the system throughout the 1950s. The Chromatron design was also licensed for a variety of other uses; Litton Industries used the Chromatron with a two-color display (blue-red) as the basis for an Identification Friend or Foe system.\n\nBy 1961 Sony was a major Japanese manufacturer of black and white sets, but had no color television technology at all. Sony dealers were asking when they could expect a color set, and the sales division started putting pressure on engineering to simply license a shadow mask design from another maker and start production. Masaru Ibuka refused, apparently displaying an intense personal feeling that the shadow mask design was fundamentally flawed.\n\nIn March 1961 Ibuka, Akio Morita and Nobutoshi Kihara attended the IEEE trade show at the New York Coliseum. This was Kihara's first visit to the U.S., and he spent considerable time wandering the show floor. At the small Autometric booth he saw the Chromatron being displayed, and hurried to find Morita and Ibuka to show them. When Morita saw the display he immediately started negotiating a meeting for the next morning to visit the Chromatic labs in Manhattan. By the end of the meeting the next day, Morita had secured a license to produce \"a Chromatron tube and color television receiver utilizing it.\"\n\nIn early 1963 Senri Miyaoka was sent to the Chromatic labs to arrange the transfer of the technology to Sony, which would lead to the closing of Chromatic. He was unimpressed with the labs, describing the windowless basement as \"squalor\". The American team was quick to point out the flaws in the Chromatron design, telling Miyaoka that the design was hopeless. By September 1964, a 17 inch prototype had been built in Japan, but mass-production test runs were demonstrating serious problems.\n\nIbuka remained a staunch supporter of the technology, and pressed ahead with the construction of a new factory to produce them at Osaki Station in Tokyo. This proved unwise, and in early runs only 1 to 3 tubes would be usable out of every 1,000 produced. The rest suffered from alignment problems, with the colors fading from one to another across the screen, impossible to fix after the tube was sealed. Usable tubes were quickly rushed to Sony showrooms in spite of the low yields, and Ibuka make the product Sony's top sales priority. This too proved unwise; the low yields meant that the production cost was about 400,000 yen, but Sony was forced to sell them at 198,000 yen ($500) in order to be competitive.\n\nThe production problems were never solved, and led to increasing tension between Ibuka and Morita. In November 1966 Kazuo Iwama told Susumu Yoshida that the company was close to ruin, and that the team had to improve the yields by the end of the year, or the product would have to be cancelled. Meanwhile, RCA was making great progress improving their shadow mask technology, and new entrants like General Electric's \"Porta-Color\" offered other advantages. Sony was clearly falling behind the rest of the market by following the Chromatron approach.\n\nIbuka finally announced that he would personally lead the search for an alternative system. His team of 30 engineers and physicists explored a wide variety of approaches in the search for a uniquely Sony system. After reading several of the reports, Ibuka called 29-year-old physicist Miyaoka into his office along with Yoshida, and asked him if his single-gun approach could be made to work. Miyaoka was attempting to leave work for a cello rehearsal, and rashly stated that it would work. The result was the famed Trinitron system, which went on sale in 1968 to wide acclaim.\n\nIn spite of Sony's move to Trinitron, a limited number of 7-inch Chromatrons were built and offered for sale in the United States starting in April 1968 as the KV 7010U. These were replaced three months later by the KV 7010UA using a Trinitron tube.\n\nThe basic concept that defined the Chromatron was the near-screen focusing system, which provided the beam resolution needed to accurately hit the individual colored phosphor strips. The grid both focused the signal as well as guided it to the correct colors.\n\nThe phosphors were silk screened onto the back of the tube in strips 2 mils wide with 2 mil wide gaps between them, and then coated in aluminum to make the screen conductive. Since the grid had to be charged to relatively high voltages, the aluminum coating was fairly thick, which dimmed the image to some extent.\n\nThe phosphors were patterned in an RGB-BGR-RGB pattern. The focusing grid was aligned so the beam would normally focus down onto the green strips in the middle of each pair of wires. To produce different colors, say blue, the beam would have to be pulled to the right for one pixel, and then to the left for the next. Since the adjacent stripes of phosphors shared one of the wires, this meant that a single voltage setting would produce the blue color on two adjacent pixels. Since a single frame of color television does not consist of a single color, the deflection system had to be continually varied as the beam moved across the screen.\n\n\n\n"}
{"id": "2161214", "url": "https://en.wikipedia.org/wiki?curid=2161214", "title": "Clay pit", "text": "Clay pit\n\nA clay pit is a quarry or mine for the extraction of clay, which is generally used for manufacturing pottery, bricks or Portland cement. Quarries where clay is mined to make bricks are sometimes called brick pits.\n\nA brickyard or brickworks is often located alongside a clay pit to reduce the transport costs of the raw material. Today, pottery producers are often not sited near the source of their clay and usually do not own the clay deposits. In these industries, the other essential raw material is fuel for firing and potteries may be located near to fuel sources. \n\nFormer claypits are sometimes filled with water and used for recreational purposes such as sailing and scuba diving. The Eden Project at Bodelva near St Austell, Cornwall, UK is a major redevelopment of a former china clay (kaolin) pit for educational and environmental purposes.\n\n"}
{"id": "261135", "url": "https://en.wikipedia.org/wiki?curid=261135", "title": "Cookware and bakeware", "text": "Cookware and bakeware\n\nCookware and bakeware are types of food preparation containers, commonly found in a kitchen. Cookware comprises cooking vessels, such as saucepans and frying pans, intended for use on a stove or range cooktop. Bakeware comprises cooking vessels intended for use inside an oven. Some utensils are considered both cookware and bakeware.\n\nThe choice of material for cookware and bakeware items has a significant effect on the item's performance (and cost), particularly in terms of thermal conductivity and how much food sticks to the item when in use. Some choices of material also require special pre-preparation of the surface—known as seasoning—before they are used for food preparation.\n\nBoth the cooking pot and lid handles can be made of the same material but will mean that, when picking up or touching either of these parts, oven gloves will need to be worn. In order to avoid this, handles can be made of non-heat-conducting materials, for example bakelite, plastic or wood. It is best to avoid hollow handles because they are difficult to clean or to dry.\n\nA good cooking pot design has an \"overcook edge\" which is what the lid lies on. The lid has a dripping edge that avoids condensation fluid from dripping off when handling the lid (taking it off and holding it 45°) or putting it down.\n\nThe history of cooking vessels before the development of pottery is minimal due to the limited archaeological evidence. The earliest pottery vessels, dating from , were discovered in Xianrendong Cave, Jiangxi, China. The pottery may have been used as cookware, manufactured by hunter-gatherers. Harvard University archaeologist Ofer Bar-Yosef reported that \"When you look at the pots, you can see that they were in a fire.\" It is also possible to extrapolate likely developments based on methods used by latter peoples. Among the first of the techniques believed to be used by stone age civilizations were improvements to basic roasting. In addition to exposing food to direct heat from either an open fire or hot embers it is possible to cover the food with clay or large leaves before roasting to preserve moisture in the cooked result. Examples of similar techniques are still in use in many modern cuisines.\n\nOf greater difficulty was finding a method to boil water. For people without access to natural heated water sources, such as hot springs, heated stones (\"pot boilers\") could be placed in a water-filled vessel to raise its temperature (for example, a leaf-lined pit or the stomach from animals killed by hunters). In many locations the shells of turtles or large mollusks provided a source for waterproof cooking vessels. Bamboo tubes sealed at the end with clay provided a usable container in Asia, while the inhabitants of the Tehuacan Valley began carving large stone bowls that were permanently set into a hearth as early as 7,000 BC.\n\nAccording to Frank Hamilton Cushing, Native American cooking baskets used by the Zuni (Zuñi) developed from mesh casings woven to stabilize gourd water vessels. He reported witnessing cooking basket use by Havasupai in 1881. Roasting baskets covered with clay would be filled with wood coals and the product to be roasted. When the thus fired clay separated from the basket, it would become a usable clay roasting pan in itself. This indicates a steady progression from use of woven gourd casings to waterproof cooking baskets to pottery. Other than in many other cultures, Native Americans used and still use the heat source inside the cookware. Cooking baskets are filled with hot stones and roasting pans with wood coals. Native Americans would form a basket from large leaves to boil water, according to historian and novelist Louis L'Amour. As long as the flames did not reach above the level of water in the basket, the leaves would not burn through.\n\nThe development of pottery allowed for the creation of fireproof cooking vessels in a variety of shapes and sizes. Coating the earthenware with some type of plant gum, and later glazes, converted the porous container into a waterproof vessel. The earthenware cookware could then be suspended over a fire through use of a tripod or other apparatus, or even be placed directly into a low fire or coal bed as in the case of the pipkin. Ceramics conduct heat poorly, however, so ceramic pots must cook over relatively low heats and over long periods of time. However, most ceramic pots will crack if used on the stovetop, and are only intended for the oven.\n\nThe development of bronze and iron metalworking skills allowed for cookware made from metal to be manufactured, although adoption of the new cookware was slow due to the much higher cost. After the development of metal cookware there was little new development in cookware, with the standard Medieval kitchen utilizing a cauldron and a shallow earthenware pan for most cooking tasks, with a spit employed for roasting.\n\nBy the 17th century, it was common for a Western kitchen to contain a number of skillets, baking pans, a kettle and several pots, along with a variety of pot hooks and trivets. Brass or copper vessels were common in Asia and Europe, whilst iron pots were common in the American colonies. Improvements in metallurgy during the 19th and 20th centuries allowed for pots and pans from metals such as steel, stainless steel and aluminium to be economically produced.\n\nAt the 1968 Miss America protest, protestors symbolically threw a number of feminine products into a \"Freedom Trash Can\", which included pots and pans.\n\nPottery has been used to make cookware from before dated history. Pots and pans made with this material are durable (some could last a lifetime or more)and are inert and non-reactive. Heat is also conducted evenly in this material. They can be used for both cooking in a fire pit surrounded with coals and for baking in the oven.\n\nMetal pots are made from a narrow range of metals because pots and pans need to conduct heat well, but also need to be chemically unreactive so that they do not alter the flavor of the food. Most materials that are conductive enough to heat evenly are too reactive to use in food preparation. In some cases (copper pots, for example), a pot may be made out of a more reactive metal, and then tinned or clad with another.\n\nAluminium is a lightweight metal with very good thermal conductivity. It is resistant to many forms of corrosion. Aluminium is commonly available in sheet, cast, or anodized forms, and may be physically combined with other metals (see below).\n\nSheet aluminium is spun or stamped into form. Due to the softness of the metal it may be alloyed with magnesium, copper, or bronze to increase its strength. Sheet aluminium is commonly used for baking sheets, pie plates, and cake or muffin pans. Deep or shallow pots may be formed from sheet aluminium.\n\nCast aluminium can produce a thicker product than sheet aluminium, and is appropriate for irregular shapes and thicknesses. Due to the microscopic pores caused by the casting process, cast aluminium has a lower thermal conductivity than sheet aluminium. It is also more expensive. Accordingly, cast aluminium cookware has become less common. It is used, for example, to make Dutch ovens lightweight and bundt pans heavy duty, and used in ladles and handles and woks to keep the sides at a lower temperature than the center.\n\nAnodized aluminium has had the naturally occurring layer of aluminium oxide thickened by an electrolytic process to create a surface that is hard and non-reactive. It is used for sauté pans, stockpots, roasters, and Dutch ovens.\n\nUncoated and un-anodized aluminium can react with acidic foods to change the taste of the food. Sauces containing egg yolks, or vegetables such as asparagus or artichokes may cause oxidation of non-anodized aluminium.\n\nAluminium exposure has been suggested as a risk factor for Alzheimer's disease. The Alzheimer's Association states that \"studies have failed to confirm any role for aluminum in causing Alzheimer's.\" The link remains controversial.\n\nCopper provides the highest thermal conductivity among non-noble metals and is therefore fast heating with unparalleled heat distribution \"(see: Copper in heat exchangers).\" Pots and pans are formed from copper sheets of various thicknesses, with those in excess of 2.5 mm considered commercial (or \"extra-fort\") grade. Between 1 mm and 2.5 mm wall thickness is considered utility (\"fort\") grade, with thicknesses below 1.5 mm often requiring tube beading or edge rolling to reinforce structural rigidity in circular configurations. Less than 1mm wall thickness is generally considered decorative, with exception made for the case of .75–1 mm planished copper, which is work-hardened by hammering and therefore expresses performance and strength characteristic of thicker material.\n\nCopper thickness of less than .25 mm is, in the case of cookware, referred to as foil and must be formed to a more structurally rigid metal to produce a serviceable vessel. Such applications of copper are purely aesthetic and do not materially contribute to cookware performance.\n\nCopper is reactive with acidic foods which can result in corrosion, the byproducts of which can foment copper toxicity. In certain circumstances, however, unlined copper is recommended and safe, for instance in the preparation of meringue, where copper ions prompt proteins to denature (unfold) and enable stronger protein bonds across the sulfur contained in egg whites. Unlined copper is also used in the making of preserves, jams and jellies. Copper does not store (\"bank\") heat, and so thermal flows reverse almost immediately upon removal from heat. This allows precise control of consistency and texture while cooking sugar and pectin-thickened preparations. Alone, fruit acid would be sufficient to cause leaching of copper byproducts, but naturally occurring fruit sugars and added preserving sugars buffer copper reactivity. Unlined pans have thereby been used safely in such applications for centuries.\n\nLining copper pots and pans prevents copper from contact with acidic foods. The most popular lining types are tin, stainless steel, nickel and silver.\n\nThe use of tin dates back many centuries and is the original lining for copper cookware. Although the patent for canning in sheet tin was secured in 1810 in England, legendary French chef Auguste Escoffier experimented with a solution for provisioning the French army while in the field by adapting the tin lining techniques used for his cookware to more robust steel containers (then only lately introduced for canning) which protected the cans from corrosion and soldiers from lead solder and botulism poisoning.\n\nTin linings sufficiently robust for cooking are wiped onto copper by hand, producing a .35–45-mm-thick lining. Decorative copper cookware, i.e., a pot or pan less than 1 mm thick and therefore unsuited to cooking, will often be electroplate lined with tin. Should a wiped tin lining be damaged or wear out the cookware can be re-tinned, usually for much less cost than the purchase price of the pan. Tin presents a smooth crystalline structure and is therefore relatively non-stick in cooking applications. As a relatively soft metal abrasive cleansers or cleaning techniques can accelerate wear of tin linings. Wood, silicone or plastic implements are to preferred over harder stainless steel types.\n\nFor a period following the Second World War, pure nickel was electroplated as a lining to copper cookware. Nickel had the advantage of being harder and more thermally efficient than tin, with a higher melting point. Despite its hardness nickel's wear characteristics were similar to that of tin, as nickel would be plated only to a thickness of <20 microns, and often even less owing to nickel's tendency to plate somewhat irregularly, requiring milling to produce an even cooking surface, albeit sticky compared to tin and silver. Copper cookware with aged or damaged nickel linings is eligible for retinning, or possibly replating with nickel, although this service is difficult if not impossible to find in the US and Europe in the early 21st century. Nickel linings began to fall out of favor in the 1980s owing to the isolation of nickel as an allergen.\n\nSilver is also applied to copper by means of electroplating, and provides an interior finish that is at once smooth, more durable than either tin or nickel, relatively non-stick and extremely thermally efficient. Copper and silver bond extremely well owing to their shared high electro-conductivity. Lining thickness varies widely by maker, but averages between 7 and 10 microns. The disadvantages of silver are expense and the tendency of sulfurous foods, especially brassicas, to discolor. Worn silver linings on copper cookware can be restored by stripping and re-electroplating.\n\nCopper cookware lined with a thin layer of stainless steel is available from most modern European manufacturers. Stainless steel is 25 times less thermally conductive than copper, and is sometimes critiqued for compromising the efficacy of the copper with which it is bonded. Among the advantages of stainless steel are its durability and corrosion resistance, and although relatively sticky and subject to food residue adhesions, stainless steel is tolerant of most abrasive cleaning techniques and metal implements. Stainless steel forms a pan's structural element when bonded to copper and is irreparable in the event of wear or damage.\n\nUsing modern metal bonding techniques, such as cladding, copper is frequently incorporated into cookware constructed of primarily dissimilar metal, such as stainless steel, often as an enclosed diffusion layer (see Coated and Composite Cookware below).\n\nCast iron cookware is slow to heat, but once at temperature provides even heating. Cast iron can also withstand very high temperatures, making cast iron pans ideal for searing. Being a reactive material, cast iron can have chemical reactions with high acid foods such as wine or tomatoes. In addition, some foods (such as spinach) cooked on bare cast iron will turn black.\n\nCast iron is a porous material that rusts easily. As a result, it typically requires seasoning before use. Seasoning creates a thin layer of oxidized fat over the iron that coats and protects the surface, and prevents sticking.\n\nEnameled cast iron cookware was developed in the 1920s. In 1934, the French company Cousances designed the enameled cast iron Doufeu to reduce excessive evaporation and scorching in cast iron Dutch ovens. Modeled on old braising pans in which glowing charcoal was heaped on the lids (to mimic two-fire ovens), the Doufeu has a deep recess in its lid which instead is filled with ice cubes. This keeps the lid at a lower temperature than the pot bottom. Further, little notches on the inside of the lid allow the moisture to collect and drop back into the food during the cooking. Although the Doufeu (literally, \"gentlefire\") can be used in an oven (without the ice, as a casserole pan), it is chiefly designed for stove top use.\n\nStainless steel is an iron alloy containing a minimum of 11.5% chromium. Blends containing 18% chromium with either 8% nickel, called 18/8, or with 10% nickel, called 18/10, are commonly used for kitchen cookware. Stainless steel's virtues are resistance to corrosion, non-reactivity with either alkaline or acidic foods, and resistance to scratching and denting. Stainless steel's drawbacks for cooking use is that it is a relatively poor heat conductor and its non-magnetic property, although recent developments have allowed the production of magnetic 18/10 alloys, and which thereby provides compatibility with induction cooktops, which require magnetic cookware. Since the material does not adequately spread the heat itself, stainless steel cookware is generally made as a cladding of stainless steel on both sides of an aluminum or copper core to conduct the heat across all sides, thereby reducing \"hot spots\", or with a disk of copper or aluminum on just the base to conduct the heat across the base, with possible \"hot spots\" at the sides. In so-called \"tri-ply\" cookware, the central aluminum layer is obviously non-magnetic, and the interior 18/10 layer need not be magnetic, but the exterior 18/10 layer must be magnetic to be compatible with induction cooktops.\n\nCarbon steel cookware can be rolled or hammered into relatively thin sheets of dense material, which provides robust strength and improved heat distribution. Carbon steel accommodates high, dry heat for such operations as dry searing. Carbon steel does not conduct heat efficiently, but this may be an advantage for larger vessels, such as woks and paella pans, where one portion of the pan is intentionally kept at a different temperature than the rest. Like cast iron, carbon steel must be seasoned before use, usually by rubbing a fat or oil on the cooking surface and heating the cookware on the stovetop or in the oven. With proper use and care, seasoning oils polymerize on carbon steel to form a low-tack surface, well-suited to browning, Maillard reactions and easy release of fried foods. Carbon steel will easily rust if not seasoned and should be stored seasoned to avoid rusting. Carbon steel is traditionally used for crêpe and fry pans, as well as woks.\n\nSteel or aluminum cooking pans can be coated with a substance such as polytetrafluoroethylene (PTFE, often referred to with the genericized trademark Teflon®) in order to minimize food sticking to the pan surface. There are advantages and disadvantages to such a coating. Coated pans are easier to clean than most non-coated pans, and require little or no additional oil or fat to prevent sticking, a property that helps to produce lower fat food. On the other hand, some sticking is required to cause sucs to form, so a non-stick pan cannot be used where a pan sauce is desired. Non-stick coatings tend to degrade over time and are susceptible to damage. Using metal implements, harsh scouring pads, or chemical abrasives can damage or destroy cooking surface.\n\nNon-stick pans must not be overheated. The coating is stable at normal cooking temperatures, even at the smoke point of most oils. However, if a non-stick pan is heated while empty its temperature may quickly exceed , above which the non-stick coating may begin to deteriorate, changing color and losing its non-stick properties. Above , the non-stick coating will rapidly decompose and emit toxic fumes, which are especially dangerous to birds, and may cause polymer fume fever in human beings.\n\nThe main difference in coating quality is due to the formulas of the liquid coating, the thickness of each layer and the number of layers used. Higher-quality non-stick cookware uses powdered ceramic or titanium mixed with the non-stick material to strengthen the coating and make it more resistant to abrasion and deterioration. Some non-stick coatings contain hardening agents. Some coatings are high enough in quality that they pass the strict standards of the National Sanitation Foundation for approval for restaurant use.\n\nEnameled cast iron cooking vessels are made of cast iron covered with a porcelain surface. This creates a piece that has the heat distribution and retention properties of cast iron combined with a non-reactive, low-stick surface.\nThe enamel over steel technique creates a piece that has the heat distribution of carbon steel and a non-reactive, low-stick surface. Such pots are much lighter than most other pots of similar size, are cheaper to make than stainless steel pots, and do not have the rust and reactivity issues of cast iron or carbon steel. Enamel over steel is ideal for large stockpots and for other large pans used mostly for water-based cooking. Because of its light weight and easy cleanup, enamel over steel is also popular for cookware used while camping.\nCladding is a technique for fabricating pans with a layer of efficient heat conducting material, such as copper or aluminum, covered on the cooking surface by a non-reactive material such as stainless steel, and often covered on the exterior aspect of the pan (\"dual-clad\") as well. Some pans feature a copper or aluminum interface layer that extends over the entire pan rather than just a heat-distributing disk on the base. Generally, the thicker the interface layer, especially in the base of the pan, the more improved the heat distribution. Claims of thermal efficiency improvements are, however, controversial, owing in particular to the limiting and heat-banking effect of stainless steel on thermal flows.\n\nAluminum is typically clad on both the inside and the exterior pan surfaces, providing both a stainless cooking surface and a stainless surface to contact the cooktop. Copper of various thicknesses is often clad on its interior surface only, leaving the more attractive copper exposed on the outside of the pan (see Copper above).\n\nSome cookware use a dual-clad process, with a thin stainless layer on the cooking surface, a thick core of aluminum to provide structure and improved heat diffusion, and a foil layer of copper on the exterior to provide the \"look\" of a copper pot at a lower price.\n\nNon-metallic cookware can be used in both conventional and microwave ovens. Non-metallic cookware typically can not be used on the stovetop, although Corningware and Pyroflam are some exceptions.\n\n\n\n\n\n\nThe size and shape of a cooking vessel is typically determined by how it will be used. Intention, application, technique and configuration also have a bearing on whether a cooking vessel is referred to as a pot or a pan. Generally (but not consistently) within the classic batterie de cuisine a vessel designated \"pot\" is round, has \"ear\" handles in diagonal opposition with a relatively low height to cooking surface ratio, and is intended for liquid cooking such as stewing, stocking, brewing or boiling. Vessels with a long handle or ear handles, a relatively high height to cooking surface ratio, used for frying, searing, reductions, braising and oven work take the designation \"pan\". Additionally, while pots are round, pans may be round, oval, squared, or irregularly shaped.\n\n\n\n\nBakeware is designed for use in the oven (for baking), and encompasses a variety of different styles of baking pans as cake pans, pie pans, and Bread pans.\n\n\n\n\n"}
{"id": "47818759", "url": "https://en.wikipedia.org/wiki?curid=47818759", "title": "Creative Visualization (design)", "text": "Creative Visualization (design)\n\nIn design, Creative Visualization refers to the process by which computer generated imagery, digital animation, three-dimensional models, and two-dimensional representations, such as architectural blueprints, engineering drawings, and sewing patterns are created and used in order to visualize a potential product prior to production. Such products include prototypes for vehicles in automotive engineering, apparel in the fashion industry, and buildings in architectural design.\n"}
{"id": "35095526", "url": "https://en.wikipedia.org/wiki?curid=35095526", "title": "Gable CAD", "text": "Gable CAD\n\nGable CAD, or Gable 4D Series, was a British architectural computer-aided design package initially developed in the early 1980s.\n\nGable CAD was developed at the University of Sheffield in the mid-1980s under the leadership of Professor Bryan Lawson. It was spun out into Gable CAD Systems Limited (incorporated in 1984) and retained links with the university until its demise in 1996 when a court order was made for compulsory winding up.\n\nAn early building information modeling application, Gable CAD was an advanced 2D and 3D design package with different modules, and was operated via a Windows-style interface and mouse running on UNIX. It was possible to create detailed 3D models and then generate 2D drawings or rendered visualisations from the data.\n\nThe assets of the company were acquired by Auxer in 1997 and aimed to complete the conversion of Gable CAD to Windows NT but this does not appear to have ever been released.\n"}
{"id": "44818019", "url": "https://en.wikipedia.org/wiki?curid=44818019", "title": "Green engineering", "text": "Green engineering\n\nGreen engineering approaches the design of products and processes by applying financially and technologically feasible processes and products in a manner that simultaneously decreases the amount of pollution that is generated by a source, minimizes exposures to potential hazards (including reducing toxicity and improved uses of matter and energy throughout the life cycle of the product and processes) as well as protecting human health without relinquishing the economic efficiency and viability. As such, green engineering is not actually an engineering discipline in itself, but an overarching engineering framework for all design disciplines.\n\nGreen engineering adheres to nine guiding principles. A designer must strive to:\n\n\nThe American Chemical Society has expanded these to twelve principles:\n\n\nTo varying extents, all engineering disciplines engage in green engineering. This includes sustainable design, life cycle analysis (LCA), pollution prevention, design for the environment (DfE), design for disassembly (DfD), and design for recycling (DfR). As such, green engineering is a subset of sustainable engineering.\nGreen engineering involves four basic approaches to improve processes and products to make them more efficient from an environmental standpoint.\n\n\nGreen engineering approaches design from a systematic perspective, which means that numerous professional disciplines must be integrated. In addition to all engineering disciplines, green engineering includes land use planning, architecture, landscape architecture, and other design fields, as well as the social sciences(e.g. to determine how various groups of people use products and services. Designers have always been concerned with space. Architects consider the sense of place. Engineers view the site map as a set of fluxes across the boundary. Planners consider the combinations of these systems over larger regions, e.g. urban areas. \nThe life cycle analysis is an important green engineering tool, which provides a holistic view of the entirety of a product, process or activity, encompassing raw materials, manufacturing, transportation, distribution, use, maintenance, recycling, and final disposal. In other words, assessing its life cycle should yield a complete picture of the product. The first step in a life cycle assessment is to gather data on the flow of a material through an identifiable society. Once the quantities of various components of such a flow are known, the important functions and impacts of each step in the production, manufacture, use, and recovery/disposal are estimated. Thus, in sustainable design, engineers must optimize for variables that give the best performance in temporal frames.\n\nThe systems approach employed in green engineering is similar to value engineering (VE). Daniel A. Vallero considers green engineering to be a form of VE because both systems require that all elements and linkages within the overall project be considered to enhance the value of the project. Every component and step of the system must be challenged. Ascertaining overall value is determined not only be a project's cost-effectiveness, but other values, including environmental and public health factors. Thus, the broader sense of VE is compatible with and can be identical to green engineering, since VE is aimed at effectiveness, not just efficiency, i.e. a project is designed to achieve multiple objectives, without sacrificing any important values. Efficiency is an engineering and thermodynamic term for the ratio of an input to an output of energy and mass within a system. As the ratio approaches 100%, the system becomes more efficient. Effectiveness requires that efficiencies be met for each component, but also that the integration of components lead to an effective, multiple value-based design.\nGreen engineering is also a type of concurrent engineering, since tasks must be parallelized to achieve multiple design objectives.\n\n\n"}
{"id": "42361206", "url": "https://en.wikipedia.org/wiki?curid=42361206", "title": "Hypothetical technology", "text": "Hypothetical technology\n\nHypothetical technology is technology that does not exist yet, but that could exist in the future. This article presents examples of technologies that have been hypothesized or proposed, but that have not been developed yet.\n\nArtificial general intelligence (AGI) is a hypothetical artificial intelligence that demonstrates a human-like ability to learn. AGI is a machine which could do all human activities with the efficiency of a machine. It is a primary goal of artificial intelligence research and a common topic among science fiction writers and futurists. Artificial general intelligence is also referred to as \"strong AI\", \"full AI\" or one that has the ability to perform \"general intelligent action\". AGI is associated with traits such as consciousness, sentience, sapience, and self-awareness, which are observed in living beings. Some examples of hypothetical technology are flying cars, jetpacks, teleportation or robot helpers.\n\nWhole brain emulation (WBE) or mind uploading (sometimes called mind copying or mind transfer) is the hypothetical process of copying mental content (including long-term memory and \"self\") from a particular brain substrate and copying it to a computational or storage device, such as a digital, analog, quantum-based, or software-based artificial neural network. The computational device could then run a simulation model of the brain information processing, such that it responds in essentially the same way as the original brain (i.e., indistinguishable from the brain for all relevant purposes) and experiences having a conscious mind.\n\nMind uploading may potentially be accomplished by at least two methods: Copy-and-Transfer or Gradual Replacement of neurons. In the former method, mind uploading would be achieved by scanning and mapping the salient features of a biological brain, and then by copying, transferring and storing that information state into a computer system or another computational device. The simulated mind could be within a virtual reality or simulated world, supported by an anatomic 3D body simulation model. Alternatively, the simulated mind could reside in a computer that's inside (or connected to) a humanoid robot or a biological body.\n\nThere are many forms of spaceflight that have been proposed that have not, so far, been developed but are thought to be possible. Some, like the space elevator are under active development. Others, like Project Orion, a nuclear bomb propulsion system, are entirely paper exercises. As it happens, Orion is thought to be entirely achievable (the obstacles to it are environmental and political rather than technological), whereas the space elevator depends on the development of a material for the cable with a very high specific strength.\n\nA space elevator is a proposed type of space transport system. Its main component is a ribbon-like cable (also called a tether) starting at or near a planetary surface and extending into space. It is designed to permit vehicle transport along the cable directly into space or orbit without the use of large rockets. An Earth-based space elevator would consist of a cable with one end attached to the surface near the equator and the other end in space beyond geostationary orbit (35,800 km altitude). The competing forces of gravity, which are stronger at the lower end, and the outward/upward centrifugal force, which is stronger at the upper end, would result in the cable staying up under tension, and stationary over a single position on Earth. Once deployed, the tether would be ascended repeatedly by mechanical means to orbit, and descended to return to the surface from orbit.\n\nOn Earth, with its relatively strong gravity, current technology is not capable of manufacturing tether materials that are sufficiently strong and light enough to build a space elevator. However, recent concepts for a space elevator are notable for their plans to use carbon nanotube or boron nitride nanotube-based materials as the tensile element in the tether design.\n\nThe rotating skyhook, or momentum-exchange tether, is an idea related to the space elevator concept. It is one of the many proposed applications of space tethers, which include some propulsion systems. The tether is rotated from a heavy orbiting vehicle such that the far end, weighted with a docking station, periodically enters Earth's atmosphere. With the right timing, a fast aircraft can transfer cargo and passengers during the brief time the skyhook is at the bottom of its cycle and stationary relative to Earth's surface.\n\nA light sail is a proposed propulsion system that uses the momentum transferred to a sail by light impinging on it. A light sail could use sunlight to achieve interplanetary travel without carrying large quantities of onboard fuel. Just as a sailboat on Earth can tack into the wind, the light sail can be tacked against the direction of light for a return journey from the outer planets.\n\n"}
{"id": "48323672", "url": "https://en.wikipedia.org/wiki?curid=48323672", "title": "IMC Process Guide", "text": "IMC Process Guide\n\nIMC Process Guide is the name of an electronic performance support system (EPSS). The software is developed by e-learning company IMC AG, Saarbrücken.\n\nProcess Guide provides context-sensitive help and up-to-date information for software users in case of difficulties with applications. It operates as an employee's personal navigation system that guides users through company and role specific processes.\n\nIn contrast to conventional formal learning, the user is not uniquely trained before using software, but receives hints, tips and help as soon as a problem occurs when applying a software in everyday work.\n\nThe didactic concept takes into account the so-called forgetting curve, or also known as \"Ebbinghaus curve\" named after the psychologist Hermann Ebbinghaus. The curve describes the high degree of forgetfulness in conventional, formal learning, where only a small part of information is actually moved and therefore saved into the long-term memory of the participants.\n\nThe concept is based on the technique of micro learning, where information is divided into small units and the informal learning, which takes place on a day-to-day basis outside the formal educational systems.\n\nAdvanced context-sensitive assistance programs like EPSS should not be confused with simple help applications that are usually keyword-triggered. An example for the latter is the well-known, but highly unpopular Clippy. This animated paperclip popped up whenever users typed a certain word or term in Microsoft Office. Clippy was often seen as annoying and removed by Microsoft in 2007. A new generation of virtual assistants, like Microsoft's Cortana uses smarter methods of assessing when and how to offer what help. Help options can appear for instance after a pre-defined time of inaction between two usually connected process-steps, instead of popping up whenever a certain word is typed.\n\nThe boundary between online-help from advanced virtual assistants like Microsoft's Cortana or Apple's Siri and EPSS is not always clear. Both can work like a knowledge navigator or GPS for software. But while Cortana and Siri are more software-specific, an EPPS like Process Guide is more organization-specific. The program does not explain how a certain software works, but instead offers assistance in handling different programs that are in use in an organization as a whole.\n\nThe application is unusual for a software made in Germany. Even if the company now has development studios in the US, the UK, Australia and other countries, the developer has its roots in the e-learning and training industry in Germany. Process Guide supports informal learning and training on the job, while training in Germany traditionally relies heavily on formal learning, especially in manufacturing.\n\nBasic vocational training as an apprentice can last three years and certificates in vocational training have a high reputation in Germany, even compared with graduations. U.S. President Barack Obama recommended the German dual education system as a standard for vocational training in his State of the Union address in 2012. While the so-called dual education system provides \"technicians, engineers and skilled workers through a … apparatus of vocational training and technical apprenticeships\", it has downfalls when it comes to lifelong learning, as recommended by the OECD and the European Commission.\n\nAs well it does not prepare for rapidly changing working conditions, like technological innovations or changes in the legal framework of a profession. In addition, informal training has proven to be efficient. According to Cross, organizations spend 80% of their training budget on formal learning and 20% on informal learning, while informal learning accounts for 80% of the learning success. Unlike in formal training, usually no certificates or diplomas are handed out though.\n\nAuthors use the so-called \"designer component\" to create help texts for applications and functions and define process steps. These help texts can be updated easily with a few clicks if software or processes are changing. On user side, the contents are automatically retrieved via a \"Guide\", as soon as a function is used with a help text archived. The \"Guide\" can also be opened selectively to lead through an application or process. This context sensitive help through texts, links and videos is intended to reduce the number of errors in everyday work, especially rarely run processes. In particular, this applies for complex processes or software updates.\n\nThe software was used in 2013 by the Saarland Ministry for Finances and European Affairs as a part of information technology restructuring of all Saarland ministries. Another example is the usage by the Lutheran parish of Lahti, Finland at training social workers on case management software.\n\nIn addition to the usage in software training, the area of compliance is referred to as exemplary area of application. The need for exact compliance with processes and communication channels, specific information requirements as well as specific data protection requirements and legal obligations coincide here and are of particularly serious consequences. Using the \"Process Guide\" allows for warnings to be displayed during the execution of compliance related processes. The users can be prompted to confirm their understanding, which along with any process deviations, are automatically documented and made available as compliance reports. This is necessary to ensure that users are not only aware of the rules of compliance but also apply them.\n\nThe user interface is based on the Programme Office 2013 by Microsoft. An inbuilt social feedback function allows users to record their screen and send it directly to the help desk. This makes it easier for help desk staff to deal with software problems and to provide help in the right context. In addition to the traditional \"On-Premise model\", the Process Guide is also available via software as a Service, a part of Cloud Computing.\n"}
{"id": "21402762", "url": "https://en.wikipedia.org/wiki?curid=21402762", "title": "Ironing", "text": "Ironing\n\nIroning is the use of a heated tool (an iron) to remove wrinkles from fabric. The heating is commonly done to a temperature of 180–220 °Celsius, depending on the fabric. Ironing works by loosening the bonds between the long-chain polymer molecules in the fibers of the material. While the molecules are hot, the fibers are straightened by the weight of the iron, and they hold their new shape as they cool. Some fabrics, such as cotton, require the addition of water to loosen the intermolecular bonds. Many modern fabrics (developed in or after the mid-twentieth century) are advertised as needing little or no ironing. Permanent press clothing was developed to reduce the ironing necessary by combining wrinkle-resistant polyester with cotton.\n\nThe first known use of heated metal to \"iron\" clothes is known to have occurred in China. The electric iron was invented in 1882, by Henry W. Seeley. Seeley patented his \"electric flatiron\" on June 6, 1882 (U.S. Patent no. 259,054).\n\nThe iron is the small appliance used to remove wrinkles from fabric. It is also known as a clothes iron, flat iron, or smoothing iron. The piece at the bottom is called a sole plate. Ironing uses heat energy, chemical energy, electrical energy, and mechanical energy.\n\nMost ironing is done on an ironing board, a small, portable, foldable table with a heat-resistant surface. Some commercial-grade ironing boards incorporate a heating element and a pedal-operated vacuum to pull air through the board and dry the garment.\n\nOn 15 February 1858 W. Vandenburg and J. Harvey patented an ironing table that facilitated pressing sleeves and pant legs. A truly portable folding ironing board was first patented in Canada in 1875 by John B. Porter. The invention also included a removable press board used for sleeves. In 1892 Sarah Boone obtained a patent in the United States for improvements to the ironing board, allowing for better quality ironing for shirt sleeves.\n\nA tailor's ham or \"dressmakers ham\" is a tightly stuffed pillow in the shape of a ham used as a mold when pressing curves such as sleeves or collars.\n\nCommercial dry cleaning and full-service laundry providers usually use a large appliance called a steam press to do most of the work of ironing clothes. Alternatively, a rotary iron may be used.\nHistorically, larger tailors' shops included a tailor's stove, used to quickly and efficiently heat multiple irons. In many developing countries a cluster of solid irons, heated alternatively from a single heating source, are used for pressing clothes at small commercial outlets.\n\nAnother source suggests slightly higher temperatures, for example, 180-220 °C for cotton\n\nWhen the fabric is heated, the molecules are more easily reoriented. In the case of cotton fibres, which are derivatives of cellulose, the hydroxyl groups that crosslink the cellulose polymer chains are reformed at high temperatures, and become somewhat \"locked in place\" upon cooling the item. In permanent press pressed clothes, chemical agents such as dimethylol ethylene urea are added as crosslinking agents.\n\n\n"}
{"id": "52410622", "url": "https://en.wikipedia.org/wiki?curid=52410622", "title": "Islamic Association of Engineers of Iran", "text": "Islamic Association of Engineers of Iran\n\nIslamic Association of Engineers of Iran () is an Iranian political party of engineers affiliated with the Council for Coordinating the Reforms Front.\n"}
{"id": "1248425", "url": "https://en.wikipedia.org/wiki?curid=1248425", "title": "Jib (camera)", "text": "Jib (camera)\n\nIn cinematography, a jib is a boom device with a camera on one end, and a counterweight and camera controls on the other. It operates like a see-saw, but with the balance point located close to the counterweight, so that the camera end of the arm can move through an extended arc. A jib permits the camera to be moved vertically, horizontally, or a combination of the two. A jib is often mounted on a tripod or similar support.\n\nA jib is useful for getting high shots, or shots which need to move a great distance horizontally or vertically, without the expense and safety issues of putting a camera operator on a crane for a crane shot or laying track for a camera dolly. A jib can even be mounted on a dolly for shots in which the camera moves over obstacles such as furniture, when a normal dolly shot could not be used.\n\nA jib is somewhat more complicated than a simple lever, since almost always the camera's aim needs to be controlled independently of the swing of the jib arm. This can be done by relatively simple mechanical means or by the use of remotely controlled electric servo motors.\n\nSince the camera operator is often not able to use the camera's controls directly or look through the camera's viewfinder, a jib is often used in conjunction with a remote camera control for focus and zoom and with a portable video monitor.\n\nA device known as a \"hot head\" or \"remote head\" is attached to the camera end of larger jibs. It supports the camera and enables remote pan/tilt functions with focus/zoom control. This setup can be operated by one person, or the circumstance may require two operators. In a two-operator situation, one person operates the jib arm/boom while another operates the pan/tilt/zoom functions of the remote head.\n\n"}
{"id": "5332296", "url": "https://en.wikipedia.org/wiki?curid=5332296", "title": "Kank-A", "text": "Kank-A\n\nKank-A is a liquid pharmaceutical product marketed by Blistex used primarily to treat canker sores. It is applied directly on the sore and works by numbing the local area (with benzocaine) to prevent discomfort and forming a protective mucous barrier with a compound benzoin tincture that protects the ulcer from further irritation, allowing it to heal more rapidly.\n\nAs of 2014, Kank-A comes in two varieties, the original providing oral pain relief against canker sores, and a soft gel that provides oral pain relief of toothaches.\n"}
{"id": "19870711", "url": "https://en.wikipedia.org/wiki?curid=19870711", "title": "Laser-based angle-resolved photoemission spectroscopy", "text": "Laser-based angle-resolved photoemission spectroscopy\n\nLaser-based angle-resolved photoemission spectroscopy is a form of angle-resolved photoemission spectroscopy that uses a laser as the light source. Photoemission spectroscopy is a powerful and sensitive experimental technique to study surface physics. It is based on the photoelectric effect originally observed by Heinrich Hertz in 1887 and later explained by Albert Einstein in 1905 that when a material is shone by light, the electrons can absorb photons and escape from the material with the kinetic energy: formula_1, where formula_2 is the incident photon energy, formula_3 the work function of the material. Since the kinetic energy of ejected electrons are highly associated with the internal electronic structure, by analyzing the photoelectron spectroscopy one can realize the fundamental physical and chemical properties of the material, such as the type and arrangement of local bonding, electronic structure and chemical composition.\n\nIn addition, because electrons with different momentum will escape from the sample in different directions, angle-resolved photoemission spectroscopy is widely used to provide the dispersive energy-momentum spectrum. The photoemission experiment is conducted using synchrotron radiation light source with typical photon energy of 20 – 100 eV. Synchrotron light is ideal for investigating two-dimensional surface systems and offers unparalleled flexibility to continuously vary the incident photon energy. However, due to the high costs to construct and maintain this accelerator, high competition for beam time, as well as the universal minimum electron mean free path in the material around the operating photon energy (20–100 eV) which leads to the fundamental hindrance to the three-dimensional bulk materials sensitivity, an alternative photon source for angle-resolved photoemission spectroscopy is desirable.\n\nIf femtosecond lasers are used, the method can easily be extended to access excited electronic states and electron dynamics by introducing a pump-probe scheme, see also two-photon photoelectron spectroscopy.\n\nTable-top laser-based angle-resolved photoemission spectroscopy had been developed by some research groups. Daniel Dessau of University of Colorado, Boulder, made the first demonstration and applied this technique to explore superconducting system. The achievement not only greatly reduces the costs and size of facility, but also, most importantly, provides the unprecedented higher bulk sensitivity due to the low photon energy, typically 6 eV, and consequently the longer photoelectron mean free path (2–7 nm) in the sample. This advantage is extremely beneficial and powerful for the study of strongly correlated materials and high-Tc superconductors in which the physics of photoelectrons from the topmost layers might be different from the bulk. \nIn addition to about one-order-of-magnitude improvement in the bulk sensitivity, the advance in the momentum resolution is also very significant: the photoelectrons will be more broadly dispersed in emission angle when the energy of incident photon decreases. In other words, for a given angular resolution of the electron spectrometer, the lower photon energy leads to higher momentum resolution. The typical momentum resolution of a 6 eV laser-based ARPES is approximately 8 times better than that of a 50 eV synchrotron radiation ARPES. Besides, the better momentum resolution due to low photon energy also results in less k-space accessible to ARPES which is helpful to the more precise spectrum analysis. For instance, in the 50 eV synchrotron ARPES, electrons from the first 4 Brillouin zones will be excited and scattered to contribute to the background of photoelectron analysis. However, the small momentum of 6 eV ARPES will only access some part of the first Brillouin zone and therefore only those electrons from small region of k-space can be ejected and detected as the background. The reduced inelastic scattering background is desirable while doing the measurement of weak physical quantities, in particular the high-Tc superconductors.\n\nThe first 6 eV laser-based ARPES system used a Kerr mode-locked Ti: sapphire oscillator is used and pumped with another frequency doubled Nd:Vanadate laser of 5 W and then generates 70 fs and 6 nJ pulses which are tunable around 840 nm (1.5 eV) with the 1 MHz repetition rate. Two stages of non-linear second harmonic generation of light are carried out through type Ι phase matching in β-barium borate and then the quadruple light with 210 nm (~ 6 eV) is generated and finally focused and directed into the ultra-high vacuum chamber as the low-energy photon source to investigate the electronic structure of the sample.\n\nIn the first demonstration, Dessau’s group showed that the typical forth harmonic spectrum fits very well with the Gaussian profile with a full width at half maximum of 4.7 meV as well as presents a 200 μW power. The performance of high flux (~ 10- 10 photons/s) and narrow bandwidth makes the laser-based ARPES overwhelm the synchrotron radiation ARPES even though the best undulator beamlines are used. Another noticeable point is that one can make the quadruple light pass through either 1/4 wave plate or 1/2 wave plate which produces the circular polarization or any linear polarization light in the ARPES. Because the polarization of light can influence the signal to background ratio, the ability to control the polarization of light is a very significant improvement and advantage over the synchrotron ARPES. With the aforementioned favorable features, including lower costs for operating and maintenance, better energy and momentum resolution, and higher flux and ease of polarization control of photon source, the laser-based ARPES undoubtedly is an ideal candidate to be employed to conduct more sophisticated experiments in condensed matter physics.\n\nOne way to show the powerful ability of laser-based ARPES is to study high Tc superconductors. The following figure references refer to this publication. Fig. 1 shows the experimental dispersion relation, binding energy vs. momentum, of the superconducting BiSrCaCuO along the nodal direction of the Brillouin zone. Fig. 1 (b) and Fig. 1 (c) are taken by the synchrotron light source of 28 eV and 52 eV, respectively, with the best undulator beamlines. The significantly sharper spectral peaks, the evidence of quasiparticles in the cuprate superconductor, by the powerful laser-based ARPES are shown in Fig. 1 (a). This is the first comparison of dispersive energy-momentum relation at low photon energy from table-top laser with higher energy from synchrotron ARPES. The much clearer dispersion in (a) indicates the improved energy-momentum resolution as well as many important physical features, such as overall band dispersion, Fermi surface, superconducting gaps, and a kink by electron-boson coupling, are successfully reproduced. It is foreseeable that in the near future the laser-based ARPES will be widely used to help condensed matter physicists get more detailed information about the nature of superconductivity in the exotic materials as well as other novel properties that cannot be observed by the state-of-the-art conventional experimental techniques.\n\nFemtosecond laser-based ARPES can be extended to give spectroscopic access to excited states in time-resolved photoemission and two-photon photoelectron spectroscopy. By pumping an electron to a higher level excited state with the first photon, the subsequent evolution and interactions of electronic states as a function of time can be studied by the second probing photon. The traditional pump-probe experiments usually measure the changes of some optical constants, which might be too complex to obtain the relevant physics. Since the ARPES can provide a lot of detailed information about the electronic structures and interactions, the pump-probe laser-based ARPES may study more complicated electronic systems with sub-picosecond resolution.\n\nEven though the angle-resolved synchrotron radiation source is widely used to investigate the surface dispersive energy-momentum spectrum, the laser-based ARPES can even provide more detailed and bulk-sensitive electronic structures with much better energy and momentum resolution, which are critically necessary for studying the strongly correlated electronic system, high-T superconductor, and phase transition in exotic quantum system. In addition, the lower costs for operating and higher photon flux make laser-based ARPES easier to be handled and more versatile and powerful among other modern experimental techniques for surface science.\n\n"}
{"id": "23721217", "url": "https://en.wikipedia.org/wiki?curid=23721217", "title": "LivePerson", "text": "LivePerson\n\nLivePerson is a publicly held American technology company that develops products for online messaging, marketing, and analytics.\n\nHeadquartered in New York City, LivePerson is best known as the developer of LiveEngage- a messaging platform that allows companies to talk with visitors in real time on websites, mobile, and social networks, enabling both customer service and conversational commerce (e-commerce via chat).\n\nLivePerson was founded in 1995 by Robert LoCascio after a disastrous customer service experience online. In April 2000, the company completed an initial public offering on the NASDAQ, in March 2011 its shares started trading also on the Tel Aviv Stock Exchange and are included in the TA-100 Index and the TA BlueTech Index.\n\nSince its founding, LivePerson has expanded to include offices in London and Reading, England; Tel Aviv, Israel; Melbourne, Australia; Alpharetta, Georgia; Mannheim and Berlin, Germany; Amsterdam, Netherlands; Milan, Italy; Paris, France; Tokyo, Japan.\n\nLivePerson provides a web based engagement service, also referred to as click to chat.\n\n\n\n\n"}
{"id": "645581", "url": "https://en.wikipedia.org/wiki?curid=645581", "title": "Local number portability", "text": "Local number portability\n\nLocal number portability (LNP) for fixed lines, and full mobile number portability (FMNP) for mobile phone lines, refers to the ability of a \"customer of record\" of an existing fixed-line or mobile telephone number assigned by a local exchange carrier (LEC) to reassign the number to another carrier (\"service provider portability\"), move it to another location (\"geographic portability\"), or change the type of service (\"service portability\"). In most cases, there are limitations to transferability with regards to geography, service area coverage, and technology. Location Portability and Service Portability are not consistently defined or deployed in the telecommunication industry.\n\nIn the United States and Canada, mobile number portability is referred to as WNP or WLNP (Wireless LNP). In the rest of the world it is referred to as mobile number portability (MNP). Wireless number portability is available in some parts of Africa, Asia, Australia, Latin America and most European countries including Britain; however, this relates to transferability between mobile phone lines only. Canada, South Africa and the United States are the only countries that offer full number portability transfers between both fixed lines and mobile phone lines, because mobile and fixed line numbers are mixed in the same area codes, and are billed identically for the calling party, the mobile user usually pays for incoming calls and texts; in other countries all mobile numbers are placed in higher priced mobile-dedicated area codes and the originator of the call to the mobile phone pays for the call. The government of Hong Kong has tentatively approved fixed-mobile number portability; however, as of July 2012, this service is not yet available.\n\nSome cellular telephone companies will charge for this conversion as a regulatory cost recovery fee.\n\nLNP was invented by Edward Sonnenberg while working for Siemens.\n\nThough it was introduced as a tool to promote competition in the heavily monopolized wireline telecommunications industry, number portability became popular with the advent of mobile telephones, since in most countries different mobile operators are provided with different area codes and, without portability, changing one's operator would require changing one's number. Some operators, especially incumbent operators with large existing subscriber bases, have argued against portability on the grounds that providing this service incurs considerable overhead, while others argue that it prevents vendor lock-in and allows them to compete fairly on price and service. Due to this conflict of interest, number portability is usually mandated for all operators by telecommunications regulatory authorities. In the US, LNP was mandated by the Federal Communications Commission (FCC) in 1996. The mandate required all carriers in the top 100 metropolitan statistical areas (MSAs) to be \"LNP-capable\" and port numbers to any carriers sending a BFR (bona fide request). The ability to keep a number while switching providers is thought to be attractive to consumers. It was also a major point made by CLECs (competitive local exchange carriers) preventing customers from leaving ILECs (incumbent local exchange carriers), thus hindering competition. Details regarding the reasons for LNP and how it is to be implemented can be found in the First Report and Order referenced above.\n\nIn the US, the FCC has mandated this in order to increase competition among providers. As of late November 2003, LNP was required for all landline and wireless common carriers, so long as the number is being ported to the same geographical area or telephone exchange. This latest mandate included carriers outside the top 100 MSAs that previously enjoyed a rural carrier exemption.\n\nThere are four main methods to route a number whose operator has changed.\nThe operator that originates the call always check a centralized database and obtains the route to the call.\nThe originating operator then routes the call to Serving Network.\n\nThe operator that originates the call first checks with the operator to which the number initially belonged, the donor operator. The donor operator verifies the call and informs that it no longer possesses the number. The operator that originates the call then checks the centralized database, as is done with ACQ.\n\nAlso known as Return to Pivot (RoP). The operator that originates the call first checks with the donor operator. The donor operator checks its own database and provides a new route. The operator that originates the call then uses this route to forward the call. No central database is consulted.\n\nThe operator that originates the call routes the call to the donor operator. The donor operator checks its own database and obtains a new route. The operator to which the number was designated routes the call to the new operator. This model is called indirect routing.\n\nComplexity for number portability can come from many sources. Historically, numbers were assigned to various operators in blocks. The operators, who were often also service providers, then provided these numbers to the subscribers of telephone services. Numbers were also recycled in blocks. With number portability, it is envisioned that the size of these blocks may grow smaller or even to single numbers. Once this occurs the granularity of such operations will represent a greater workload for the telecommunications provider.\nWith phone numbers assigned to various operators in blocks, the system worked quite well in a fixed line environment since everyone was attached to the same infrastructure. The situation becomes somewhat more complex in a wireless environment such as that created by cellular communications.\n\nIn number portability the “donor network” provides the number and the “recipient network” accepts the number. The operation of donating a number requires that a number be “snapped out” from a network and “snapped into” the receiving network. If the subscriber ceases to need the number then it is normal that the original donor receive the number back and “snaps back” the number to its network. The situation is slightly more complex if the user leaves the first operator for a second and then subsequently elects to use a third operator. In this case the second operator will return the number to the first and then it is assigned to the third.\n\nIn cellular communications the concept of a location registry exists to tie a “mobile station” (such as a cellular phone) to the number. If a number is dialed it is necessary to be able to determine where in the network the mobile station exists. Some mechanism for such forwarding must exist. (For an example of such a system, see the article on the GSM network.)\n\nIn the US, there are standards for portability defined by the FCC, the LNPA, NANPA and the ATIS which are agreed upon by all member providers to help make LNP as cost-efficient and expedient as possible while still retaining a healthy level security for all providers and in respect of the highest level of customer service. These rules, first defined in the 1st, 2nd and 3rd Reports and Orders by the FCC (publicly available at fcc.gov), are further detailed by the LNPA in order to ensure any provider can successfully port numbers to any other provider. iconectiv provides a national database called the NPAC (National Portability Administration Center) which contains the correct routing information for all ported and pooled numbers in the US and Canada. The NANC maintains detailed documentation of the procedure common among US carriers to port numbers as described here.\n\nProviders use SS7 to route calls throughout the US/Canada network. SS7 accesses databases for various services such as CNAM, LIDB, Custom Local Area Signaling Services (CLASS) and LNP. Calls to ported numbers are completed when a customer who calls a ported number sends the dialed number to a provider's SSP (Service Switch Point), where it is identified either as a local call or not. If the call is local, the switch has the NPA-NXX in its routing table as portable, so it sends a routing request to the Signal Transfer Point (STP) which accesses a local database that is updated by an LSMS (Local Service Management System) which holds all routing for all ported numbers to which the carrier is responsible for completing calls. If routing information is found, a response is sent to the \"query\" containing the information necessary to properly route the call. If it is not a local number, the call is passed on to the STP and routed until it gets to a local carrier who will perform the \"query\" mentioned earlier and route the call accordingly.\n\nThe routing information necessary to complete these calls is known as a Location Routing Number (LRN). The LRN is no more than a simple 10-digit telephone number that resides in the switch of the service provider currently providing service for the ported telephone number.\n\nWhen a provider receives a request to port a telephone number from a new customer, that provider sends an industry-standard Local Service Request (LSR) to the existing (or \"old\") provider. When the Old Provider receives this request, it sends back Firm Order Confirmation (FOC) and the process of porting the number(s) begins. Either provider can initiate the port using a Service Order Activator (SOA or LSOA) which directly edits the NPAC database mentioned before. Providers can also make these requests within the NPAC database directly. If the new provider initiates the port, it is called a \"pull,\" and if the old provider initiates, it is a \"push.\" Once the number is pulled or pushed, the providers must concur the request and the new provider must \"activate\" the number using the LRN of the switch serving the customer on the agreed due date. At the point this is completed, the number is ported.\n\nMuch of this process is duplicated in intermodal portability (porting between wireline and wireless providers). There are a few technical differences, however, in WLNP—Especially with concern to the time intervals allowed.\n\nSome service providers, especially related to fax services, do not qualify as a \"local exchange carrier\" or other form of telecommunications carrier. Such service providers may be the \"customer of record\" from the LEC's perspective. As a result, the applicable law may not require that such service provider port out the number to another provider. Users and providers often negotiate portability and port out fees. eFax is one vendor that claims it is not a telecommunications company and does not allow porting out of numbers originally assigned by them to their customers; however, numbers ported by customers into eFax may be ported out.\n\nA fax machine connected to its own physical telephone line at the subscriber's premises is portable in the same manner as any other standard wireline service. Distinctive ring sometimes poses problems, as one landline may have two or three numbers with a fax or dial-up modem programmed to answer just one of the secondary numbers on the line. Porting out the main number will usually unsubscribe the entire line, disconnecting the secondary numbers without moving them to the new provider.\n\nIn Canada, pocket pager answering services are exempted from all local number portability requirements. The same is not true of mobile telephones, which are fully portable to another carrier or another service type (such as landline or voice over IP) within the same local interconnection region.\n\nThe Communications Commission of Kenya announced in 2004 that mobile number portability would be available as of July 1, 2005 and fixed-line number portability as of July 1, 2006. Mobile Number Portability was officially launched in April 1, 2011.\n\nNumber Portability Company (Pty) Ltd (Reg. No. 2005/040348/07) was established in 2005 and Mobile Number Portability was introduced on 10 November 2006. Geographic Number Portability (between fixed operators) was introduced on 26 April 2010. The Mobile Number Portability Company is jointly owned by the mobile and fixed operators including Vodacom, MTN, Cell C, Telkom and Neotel.\n\nIn Argentina, full mobile number portability is available since March 2012, being a law approved in 2000. It originally took up to ten working days to be effective. Since July 2017, however, takes up to a 24-hour period to be effective.\n\nIn Brazil, number portability (both fixed and mobile) is available nationwide since March, 2009. However, it's not possible to port a fixed line number to a mobile line number and vice versa\n. It's possible to carry the fixed line number within the same municipality and for mobile line number within the same area code (comprising from parts of a state to an entire state).\n\nIn Canada, wireline/competitive local exchange carriers must provide portability. As of March 14, 2007, wireless carriers must provide portability in most of Canada.\n\nNumbers are only portable within a LIR (local interconnection region), regions defined by the ILEC and approved by the Canadian Radio-television and Telecommunications Commission (CRTC), each of which cover a number of exchanges. Each LIR has a Point of Interconnection (POI) exchange through which calls are routed, and if a number is ported out to a different LIR then calls to that destination will be rejected by the POI switch.\n\nNot all exchanges support LNP, typically there needs to exist competition within an exchange before an ILEC will enable portability, and then only by request. Most small local independent telephone company exchanges are exempted from competition and local number portability requirements. Numbers in the rarely used non-geographic area code 600 are not portable.\n\nIn the Dominican Republic, number portability in both mobile and local telephony was launched September 30, 2009. In March, 2009, the Dominican Telecommunications Institute (INDOTEL) selected Informática El Corte Inglés to administer the number portability.\n\nIn Ecuador, Mobile Number Portability has been available since 12. October 2009.\n\nMexico is the first Latin American country to have number portability in both mobile and local telephony. The Federal Commission of Telecommunications (COFETEL) applied this law, in defense and regulation of the great monopoly that Telmex has. It was also one condition for Telmex, if the Mexican company wanted to enter to the video market Triple play (telecommunications). The number portability is available since July 5 of 2008. The handling of the service is handled by Telcordia Technologies.\n\nIn the United States, (b)(2), added by the Telecommunications Act of 1996, requires all local exchange carriers (LECs) to offer number portability in accordance with the regulations of the Federal Communications Commission (FCC). The FCC implemented regulations on 27 June 1996, with LECs required to implement them in the 100 largest Metropolitan Statistical Areas by 1 October 1997 and elsewhere by 31 December 1998. (The regulations are currently located at , \"et seq.\") The North American Numbering Council (NANC) was directed to select the Local Number Portability Administrators (LNPAs), akin to the North American Numbering Plan Administrator (NANPA) which administers the North American Numbering Plan.\n\nLNP was first implemented in the U.S. upon the establishment of the original Number Portability Administration Center (NPAC) in Chicago, Illinois in 1998. This service covered select rate centers in the Ameritech region. Thereafter, as switches and telephone networks were upgraded with location routing number (LRN) capability, LNP was deployed sequentially to the remaining Regional Bell Operating Company (RBOC) areas. The U.S. FCC since has mandated \"Wireless Local Number Portability\" starting November 24, 2003 (in metropolitan areas), and allowed operators to charge an additional monthly \"Long-Term Telephone Number Portability End-Use Charge\" as compensation. On November 10, 2003, the FCC additionally ruled that number portability applies to landline numbers moving to mobile telephones and, on October 31, 2007, the FCC made clear that the obligation to provide LNP extends to VoIP providers.\n\nToll-free telephone numbers (area code +1-800) have been portable through the RespOrg system since 1993 in the US and 1994 in Canada.\n\nIn Hong Kong, fixed line number portability is available since July 1, 1995, the same day of fixed line telephone market liberalization (i.e., reversal of franchised monopoly), which was a requirement from the government. Mobile number portability is available since March 1, 1999. Although the government allowed porting a fixed line number to a mobile carrier or vice versa, the introduction of this service shall be decided by the fixed/mobile carriers in a voluntary basis. As of October 2009, fixed-mobile number portability is not available.\n\nMobile number portability launched in India in Haryana state on November 25, 2010. It was finally launched all over India on January 20, 2011.\nIn Israel, number portability is free and takes 15 minutes. All cellular lines can be ported, Landline numbers may be ported, except between regions (area codes). Wireless and VoIP companies each have a single area code for the whole country. Within it, numbers may be ported with no regard to geographic area.\n\nThere is no porting between landline and cellular lines.\n\nIn Japan, fixed line portability began in March, 2001. (bangō portability seido – commonly referred to as portability or MNP) began on October 24, 2006. Users are able to change cellular phone carriers without changing their number for a fee of 5000 yen. However, e-mail addresses are subject to change, and music/data downloaded may become unusable.\n\nThe Japanese Ministry of Internal Affairs and Communications (MIC) spent three years to put mobile number portability into practice, since its initial workgroup started in November, 2003. As a result, NTT DoCoMo, KDDI and Softbank accelerated the price battle, but it was of little effect due to already competitive price plans and customer loyalty. Overall, mobile number portability in Japan was not very successful, because of high transition costs for the customer due to SIM lock, the long time it took to establish mobile number portability, allowing operators to fence in subscribers with price plans, and the significance of mobile Internet mail.\n\nIn Malaysia, mobile number portability plan to start by mid-2008, according to an article on the National News Agency Bernama\n\nIn Saudi Arabia, MNP was launched in July 8, 2006, to be the first country to launch this service in the ME region. A centralized number portability clearinghouse (NPC) solution was implemented by CITC(the telecom regulation authority) and the two mobile phone operators were obliged to implement the MNP solution in their networks and to interface with the NPC. the service was provided to the mobile subscribers for free.\n\nIn Oman, Mobile Number Portability was mandated on the Public Mobile Operators, Nawras and Oman Mobile, via the licenses issued to them by the Telecommunications Regulatory Authority (TRA). Mobile number portability was launched on August 26, 2006. Users are able to change cellular phone carriers without changing their number for a nominal fee of 3 OMR.\n\nIn Pakistan, () the PTA mandated mobile number portability on March 23, 2007. Users are able to change their cellular phone service for free. They just have to pay for new sim cards depending upon the provider they are migrating. Some companies even do not charge anything.\n\nSingapore was one of the first countries to introduce number portability for mobile telephones in 1997. This is currently implemented through voice call & SMS forwarding. True number portability was realized from June 13, 2008, with the implementation of a Centralised Number Portability Database Solution, as proposed by the Infocomm Development Authority (IDA) of Singapore.\nIn South Korea, mobile number portability service started from January 1, 2004. One thing different from \nother countries is that it started from SK Telecom, the dominant operator which has over 50% of market share. To prevent users' churning to the dominant operator, the government gave six months' and one year's delay to the second and the third operator, respectively. As a result, only SK Telecom's subscribers could move to other operators during the first six months.\n\nThe Sri Lankan government made a policy decision in August 2007 to introduce number portability for cellular phones in Sri Lanka. This is supported by Sri Lanka Telecom owned Mobitel Lanka and other cellular operators.\n\nIn the European Union, all telephone providers are required to provide number portability under the \"Universal Services Directive (2002/22/EU)\".\n\nAlbania\n\nIn Albania, mobile number portability was implemented in 04.05.2011 (AKEP). While for Fixed line numbers it started on some geographical areas in September 2012 and was available in all country by 01.04.2013 (AKEP).\n\nIn Austria, number portability was implemented in Oct 2004.\n\nIn Austria, number portability was implemented in Oct 2004.\n\nIn Belgium, number portability was implemented in Oct 2002.\n\nIn Cyprus, geographic, non-geographic and mobile number portability is required as of July 12, 2004.\n\nIn Denmark, portability of fixed line numbers and ISDN was implemented on January 1, 2001. Mobile number portability was implemented on July 1, 2001. In 2006, 238,293 fixed lines were ported, along with 456,159 mobile lines. Considering that the number of fixed lines by the end of 2006 was 2,974,000 and the number of mobile lines was 5.828.000, roughly 7.9% of lines were ported in 2006.\n\nIn Estonia, number portability is required from fixed operators since January 1, 2004 and should be required from mobile operators as from January 1, 2005.\n\nIn Finland, mobile number portability was implemented on July 25, 2003. The impact of mobile number portability in Finland exceeds that of other countries. In one year (June 2003 – June 2004), the combined market share of TeliaSonera, Elisa and DNA fell from 98.7% to 87.9%.\n\nIn France, geographic number portability has been available since January 1, 1998. As of January 1, 2001, it became possible to change geographic location or operator while keeping the same number. Mobile number portability was introduced on June 30, 2003. However, due to its lack of effectiveness, a new system was launched on May 21, 2007 with two objectives: having a single contact for the customer (the new operator should take all the steps towards mobile number portability) and a maximum period of ten days for mobile number portability to have effect.\n\nIn Germany, fixed number portability was introduced on January 1, 1998, for geographic numbers and numbers for non-geographic services. Mobile number portability was implemented on November 1, 2002.\n\nIn Greece, fixed number portability is available since January 1, 2003. Mobile number portability was implemented on March 1, 2004.\n\nIn Hungary, portability exists for geographic numbers since January 1, 2004. Portability for non-geographic numbers (including mobile numbers) is available since May 1, 2004. There has been added a special area code +36 21, which legally allows the phone number to be anywhere in the world, beside having the +36 country code prefix.\n\nIn Ireland, local number portability was implemented in 2000, using an IN solution with a shared routing database. Partial mobile number portability was introduced in 1997 with full portability becoming available in 2003.\n\nIn Italy, mobile number portability is available since April 30, 2002.\n\nMNP was introduced in Luxembourg in June 2004. The Mobile Number Portability Central (MNPC) managed by the G.I.E Telcom E.I.G. operator group and developed, installed and operated by Systor Trondheim AS of Norway, was put into commercial operations from February 2005.\n\nFixed number portability was introduced in Norway in 2000, one year before the introduction of mobile number portability. The administrative solution for fixed and mobile number portability in Norway, the National Reference Database (NRDB), was put into service in 2000. The NRDB is owned and managed by the 8 largest network operators in Norway through the company NRDB AS. The reference database was developed, installed and is presently operated by Systor Trondheim AS.\n\nIn Portugal, fixed number portability was implemented on June 30, 2001. Mobile number portability has been available since January 1, 2002.\nThe administrative Reference Entity (Entidade de Referencia (ER)) interconnecting all network operators and service providers is operated by a local third party, Portabil S.A., a joint venture between the internationally well known companies Logica and Systor Trondheim AS.\n\nIn Slovakia, number portability was implemented in May 2004.\n\nIn Spain, number portability among cell phone carriers is available since October 1, 2000, without any cost to the end user. The technical details for the process are regulated by the CMT \"(Comisión del Mercado de las Telecomunicaciones\" or Telecoms Market Commission) and all carriers are obliged to comply with their requirements. As of August 2007, cell number portability must complete in 5 business days (i.e. excluding weekends) from the moment the request is confirmed by the customer, with the actual switch occurring late at night to avoid missing any calls. The user wakes up using a new SIM-card from the new cell provider while keeping the number.\n\nIn the mature Spanish cell phone market (as of June 2007, with 107 lines per 100 inhabitants ), portability has been widely used by the competing carriers as a way to steal each other's customers, usually offering them free handsets or extra credit. From June 2006 to June 2007 alone, 3,957,556 cell phone lines switched carriers via this proceeding, about 10% of all cellular lines in use. Spain is the one country in the European Union where more customers have switched cell phone providers, with more than 9 million carrier switches completed as of April 2007.\n\nAs for the fixed line market, number portability is also available since year 2000, but weaker competition meant that actual adoption of the fixed number portability process was quite sluggish. As of August 2004, 1,041,246 fixed line switches were completed.\n\nFixed line market is peculiar in Spain, since only two local loop providers can operate at each particular region (or \"demarcación\" as regulated by the CMT): a cable carrier (such as Ono, R and many others) and the former State monopoly (Telefónica). The sole of them operating statewide—Telefónica—is obliged to provide other firms with access to their exchange facilities or rental/transfer of their copper last-mile loops, at fees regulated by the CMT (practice known as local loop unbundling). As cable providers do not have a statewide footprint, many users have no actual chance of applying for \"true\" fixed number portability, that is, giving up Telefónica's service altogether. Some of them can however get their service from a third company who will bill the service and then pay Telefónica for the copper pair rental and maintenance fees, with the customer receiving a single bill. In the end, as Telefónica set up a reselling program for their fixed lines and DSL internet access, the former monopoly is still much in control of the fixed line market, including profitable broadband access. In fact, Telefónica was fined in excess of €152 million by the European Commission on July 4, 2007 on ground of \"impeding competition on the Spanish broadband internet access market for more than five years, and so depriving consumers and business of a choice of broadband suppliers\".\n\nDue to the billing scheme used throughout Europe and most of the world, where the calling party assumes the full cost of the call, and calling a cellphone is usually more expensive than calling a fixed line, a distinction must be made between cellphone numbers (beginning with \"6\" or, from October 2011, \"71\", \"72\", \"73\" or \"74\", ) and fixed numbers (usually beginning with 9 or 8). Full number portability in which a customer transfers a cell to a fixed number or vice versa is thus not possible. See Telephone numbering in Spain for more information.\n\nIn Sweden, fixed line portability was implemented in 1999 and mobile number portability was implemented on September 1, 2001. At the introduction of mobile number portability the Swedish operators joined forces and procured a central solution, SNPAC CRDB, which is a central reference database now containing both the fixed and mobile portings.\n\nIn Switzerland, mobile number portability is available since November 1, 2015.\n\nIn Turkey, mobile number portability was implemented in Nov 2008. Fixed number portability was initially planned to take place exactly 6 months following the mobile number portability, on May 9, 2009. However, it was not until September 9, 2009 that the regulator approved the procedure for fixed number portability. Since then, fixed and mobile operators, and the incumbent, are working to get the process going and performing interoperability tests. However, there is still progress to be made and the progress for fixed number portability has not proved to be going ahead as in-time as the mobile number portability.\n\nIn the United Kingdom, Ofcom directs fixed-line telephone network providers, mobile phone providers and broadband service providers to provide number portability under the Porting Authorisation Code rules and Migration Authorisation Code code of practice respectively. As the UK is an EU member country, the Ofcom direction is intended to reflect the requirements of EU Directive 2002/22/EU.\n\nThe number portability service on public telephone networks at a fixed location is available as of 1 April 2014.\n\nIn Australia, local telephone numbers have been portable since 1999. The porting process is based on a peer-to-peer file exchange between fixed line operators. According to ACMA, local number portability came into full effect at the start of 2000. Mobile number portability has been available as of September 25, 2001.\n\nFor service providers who require knowledge of porting activity to enable them to deliver voice calls directly to the current \"network owner\", they can either form agreements with all of the fixed-line operators, or use a third-party LNP provider, such as Paradigm.One.\n\nIn New Zealand, local and mobile number portability (LMNP) began on April 1, 2007. The rules governing LMNP originate in the Number Portability Determination. Ports are authorised, scheduled, and coordinated via a centralised number portability system called IPMS (Industry Portability Management System). All networks update their own routing and confirm this to IPMS. There are now 26 carriers and service providers that participate in LMNP in New Zealand, over a million numbers have been ported.\n\n\n\n"}
{"id": "39654004", "url": "https://en.wikipedia.org/wiki?curid=39654004", "title": "Magritek", "text": "Magritek\n\nMagritek is a scientific instrument company based in Wellington, New Zealand, and Aachen, Germany, that was established in 2004 and specialises in compact, portable and benchtop nuclear magnetic resonance (NMR) and magnetic resonance imaging (MRI) products. The technology was originally developed to enable NMR measurements in Antarctica by scientists at Massey and Victoria Universities in New Zealand, including Dr Robin Dykstra This was combined with compact, handheld NMR magnet technology developed by researchers at RWTH University in Aachen\n\nMagritek is well known in New Zealand as an example of successful commercialisation of university developed IP and in 2010 the team behind the company won the Prime Minister's Science Prize led by famous New Zealand scientist Sir Paul Callaghan\n\nMagritek uses novel magnetic resonance techniques such as Earth's field NMR and Halbach array permanent magnets to create products such as the Spinsolve benchtop NMR spectrometer which enables both scientists and students to access high resolution NMR spectroscopy where they are working.\n\nIn 2009 they released a series of popular free videos explaining and demonstrating the principles of NMR and MRI.\n\n"}
{"id": "37495602", "url": "https://en.wikipedia.org/wiki?curid=37495602", "title": "Military drums", "text": "Military drums\n\nMilitary drums or war drums are all kinds of drums and membranophones that have been used for martial music, including military communications, as well as drill, honors music and military ceremonies.\n\nAmong ancient war drums that can be mentioned, junjung was used by the Serer people in West Africa. The Rigveda describes the war drum as the fist of Indra.\n\nIn early medieval Europe, the use of the drum for military purposes did not begin until the crusades. (p. 19) The European armies first encountered them used by the Islamic military forces, who used primarily their traditional kettledrums, and found that the sound would particularly affect the Crusader's horses, who had not previously encountered them. By the early 13th century the Crusaders used them also.\n\nThe snare drum was taken into use in 13th century Europe, to rally troops, and to demoralize the enemy.\n\nA military tattoo was originally a drum signal for soldiers' curfew. Other uses for military drums have been recruiting and calling for parley.\n\nAncient Fife and Drum Corps, as well as modern drum corps have been used by early modern armies for signalling and ceremonies, occasionally played by drummer boys in conflicts such as the American Civil War.\n\nOver a period of time, Snare drums, as well as timpani, have been adopted into civilian classical and popular music.\n\nIn modern times, the term \"war drums\" is used as a metaphor for preparation for war.\n\n"}
{"id": "16233420", "url": "https://en.wikipedia.org/wiki?curid=16233420", "title": "Mulch-till", "text": "Mulch-till\n\nIn agriculture mulch tillage or mulch-till fall under the umbrella term of conservation tillage in the United States and refer to seeding methods where a hundred percent of the soil surface is disturbed by tillage whereby crop residues are mixed with the soil and a certain amount of residues remain on the soil surface. A great variety of cultivator implements are used to perform mulch-till. \n\nMulch is material to regulate heat. This is done by covering it with any material like wood chips, straw, leaves or food waste.\n\n"}
{"id": "52462968", "url": "https://en.wikipedia.org/wiki?curid=52462968", "title": "NIRCam", "text": "NIRCam\n\nNIRCam is an instrument aboard the to-be-launched James Webb Space Telescope. It has two major tasks, as an imager from 0.6 to 5 micron light wavelength, and as a wavefront sensor to keep the 18-section mirrors functioning as one. In other words, it is a camera and is also used to provide information to align the 18 segments of the primary mirror. It is an infrared camera with ten mercury-cadmium-telluride (HgCdTe) detector arrays, and each array has an array of 2048x2048 pixels. The camera has a field of view of 2.2x2.2 arc minutes with an angular resolution of 0.07 arcsec at 2 microns. NIRCam is also equipped with coronagraphs, which helps to collect data on exoplanets near stars. It helps with imaging anything next to a much brighter object, because the coronagraph blocks that light. \nNIRCam is housed in the Integrated Science Instrument Module, to which it is attached physically by struts. It is designed to operate at 37 Kelvin (roughly minus 400 degrees Fahrenheit), so it can detect infrared light at this wavelength. It is connected to the ISIM by struts and thermal straps connect to heat radiators, which helps maintain its temperature. The Focal Plane Electronics operated at 290 kelvin.\n\nNIRCam should be able to observe as faint as magnitude +29 with a 10000-second exposure (about 2.8 hours). It makes these observations in light from 0.6 (600 nm) to 5 microns (5000 nm) wavelength. It can observe in two fields of view, and either side can do imaging, or from the capabilities of the wave-front sensing equipment, spectroscopy. The wavefront sensing is much finer than the thickness of an average human hair. It must perform at an accuracy of at least 93 nanometers and in testing it has even achieved between 32 and 52 nm. A human hair is thousands of nanometers across.\n\nWavefront sensors components include: \n\nParts of NIRCam:\n\nNIRCam has two complete optical systems for redundancy. The two sides can operate at the same time, and view two separate patches of sky; the two sides are called side A and side B. The lenses used in the internal optics are triplet refractors. The lens materials are lithium fluoride (LiF), a barium fluoride (BaF) and zinc selenide (ZnSe). The triplet lenses are collimating optics. The biggest lens as 90 mm of clear apature.\n\nThe observed wavelength range is broken up into a high and low band. The low band goes from 0.6 to 2.3 microns and the high band goes from 2.4 to 5 microns; both have the same field of view and access to a coronagraph. The lower band has finer plate scale with twin quad arrays of 2048x2048 arrays, while the high band has two 2048x2048 arrays. Each side of the NIRCam views a 2.2 arcminute by 2.2 arcminute patch of sky in both the high and the low band; however, the low band has twice the resolution. The high band has one array per side (two overall), and the low band has four arrays per side, or 8 overall. Side A and Side B have a unique field of view, but they are adjacent to each other. In other words, the camera looks at two 2.2 arcminute wide fields of view that are next to each other, and each of these views is observed in the high and low band, but the low band has twice the resolution of the higher wavelength band.\n\nThe builders of NIRCam are the University of Arizona and company Lockheed Martin, in cooperation with the U.S. Space agency, NASA. NIRCam was completed in July 2013 and it was shipped to Goddard Spaceflight Center, which is the NASA center managing the JWST project. NIRCam was also delivered to NASA in March 2014.\n\nNIRCam's four major science goals include:\n\nData from the image sensors (Focal Plane Arrays) is collected by the Focal Plane Electronics and sent to the ISIM computer. The data between the FPE and the ISIM computer is transferred by SpaceWire connection. There are also Instrument Control Electronics (ICE). The Focal Plane Arrays contain 40 million pixels.\n\nThe FPE provides or monitors the following for the FPA:\n\nNIRcam includes filter wheels that allow the light coming in from the optics to be sent through a filter before it is recorded by the sensors. The filters have a certain range in which they allow light to pass, blocking the other frequencies; this allows operators of NIRCam some control over what frequencies are observed when making an observation with the telescope.\n\nBy using multiple filters the redshift of distant galaxies can be estimated by photometry.\n\nNIRcam filters:\n\n\n"}
{"id": "12368774", "url": "https://en.wikipedia.org/wiki?curid=12368774", "title": "Nanosubmarine", "text": "Nanosubmarine\n\nNanosubmarines, or nanosubs, are synthetic microscopic devices that can navigate and perform specific tasks within the human body. Most of the self-propelled devices will be used to detect substances, decontaminate the environment, perform targeted drug delivery, conduct microsurgery and destroy malicious cells. Nanosubmarines use a variety of methods to navigate through the body; currently the preferred method uses the electrochemical properties of molecules. There have been multiple successful tests using this technology to heal mice with inflammatory bowel diseases. The general goal of nanosubmarines is to be able to produce a machine which can sense and respond autonomously, all while being fueled by its environment.\n\nThe main purpose of a nanosubmarine is to navigate the body and perform a specific task. The most speculated task is the treatment and diagnosis of diseases from within the body. This is supported by the task of detecting substances, as most diseases cause a specific type of protein or other molecule to be made in abundance within the bloodstream. Another speculated task is microsurgery. With this technology, doctors will be able to perform surgery on specific locations from within the body. One example of this could be a treatment for cancer. A nanosubmarine could be built to detect specific cancer cells within the body; after locating the cells, the nanosub would be able to kill only the mutated cells and ignore healthy cells.\n\nNavigation is one of the most difficult aspects to develop in nanosubmarines. The goal is to be able to travel throughout the bloodstream without getting stuck in even the smallest of capillaries. However, this is difficult because the smallest capillaries are 2 μm across (2.0 x 10m); blood cells are about 7μm but they are easily pliable and can squeeze through the capillaries. Another challenge with navigation is the fact that physics restricts the amount of propulsion such a small device can output. The blood flow is simply too strong for any device even compete with the flow, therefore the nanosubmarine would have to be carried by the blood.\n\nOne form of propulsion nanosubmarines could use is electrochemical. One example of a motor is a nanorod which is platinum on one side and gold on the other. When submerged in hydrogen peroxide the platinum oxidizes the HO into 2H and O. This process occurs because platinum takes two electrons from the molecule. On the other side of the rod, the gold reduces hydrogen peroxide into water, in doing so an electron is pulled from the gold. This causes a steady electron flow from the platinum side of the rod towards the gold side. Since the rod is so small, Newton's third law of physics applies. For any action there is a reaction, when the electrons are pulled across the surface of the rod, so too is the rod pulled in the opposite direction.\n\nThe first recorded success of a nanosubmarine was performed by a team of students led by Dan Peer from Tel Aviv University in Israel. This was a continuation to Peer's work at Harvard on nanosubmarines and targeted drug delivery. Tests have proven successful in delivering drugs to heal mice with ulcerative colitis. Tests will continue and the team plans to experiment on the human body soon.\n\n"}
{"id": "56502274", "url": "https://en.wikipedia.org/wiki?curid=56502274", "title": "Parlux", "text": "Parlux\n\nParlux is an Italian manufacturer of hair dryers and other related hair care electrical appliances, which sells largely in the commercial side of the hair electrical appliance market, and less so the household market.\nIt was founded in 1977 by Paolo Parodi in Corsico. In 1991 it moved to Trezzano sul Naviglio. From 2000, the appliances were no longer hand-made, but made on an automated assembly line. \nIt is situated in the Lombardy region of Italy.\n\n\n"}
{"id": "3158873", "url": "https://en.wikipedia.org/wiki?curid=3158873", "title": "Pothook", "text": "Pothook\n\nA pothook (or pot hook) is an S-shaped metal hook for suspending a pot over a fire.\nWhile one extremity of the pothook is hooked to the handle of the pot, the other is caught upon an iron crane moving on a pivot over the fire. Later stoves obviated the necessity for this arrangement, but in the early twentieth century it was still to be seen in great numbers of country cottages and farmhouse kitchens all over England, and in small artisan's houses in the West Midlands and the North.\n\nIn the elementary teaching of writing, a glyph of similar shape is called a pothook.\n\n"}
{"id": "49002717", "url": "https://en.wikipedia.org/wiki?curid=49002717", "title": "ProntoForms", "text": "ProntoForms\n\nProntoForms (TSXV: PFM) is a Canadian software and mobile app developer whose eponymous app provides users with mobile forms for use largely in small and medium businesses and enterprise management. The application allows workers in the field to send data, forms, and other information to management personnel, back office systems, and analytics tools. The company went public in 2005 and trades on the TSX Venture Exchange under the stock ticker symbol, PFM. In 2015, Frost & Sullivan honored ProntoForms with the \"North American Mobile Forms Competitive Strategy Innovation and Leadership\" Award.\n\nProntoForms was founded in 2001 in Ottawa, Canada as TrueContext Mobile Solutions by Alvaro Pombo (who is also the company's CEO). Pombo had immigrated to Canada from Colombia to work in the oil industry. He then worked for Palm before starting TrueContext. In 2002, the company received $5.4 million in seed funding from Skypoint Capital and Venture Coaches. The company went public on September 15, 2005 and began trading on the TSX Venture Exchange under the stock ticker symbol, PFM. One month later, TrueContext raised $7.6 million in Series B funding from Skypoint Capital and Jefferson Partners.\n\nIn 2007, the company launched their Pronto app in the United States. At the time, the app was available for BlackBerry and Windows Mobile devices. The app would later be made available for Android and iOS devices. By 2011, clients who had used the ProntoForms software included, Louisiana's St. John the Baptist Parish School Board, Promarket, and Amaco Equipment. The firm also entered into a strategic partnership with AT&T that made the telecommunications company a distributor of the ProntoForms software. TrueContext had around 1,800 business clients by the end of 2012. Other strategic partnerships that ProntoForms has formed include those with Nextel, Rogers, and Bell.\n\nIn 2013, the company officially changed its name to ProntoForms while also selling off several patents. In 2014, the company raised $1 million in funding through private placement. By the end of 2014, the company had around 2,500 business clients. In 2015, ProntoForms established a partnership with Apple, allowing the latter company to distribute the ProntoForms technology. In September 2015, the company raised an additional $3 million through private placement.\n\nProntoForms' flagship product is an eponymous mobile app and software solution that allows workers in the field to collect, send, and receive data using mobile devices. The app is available for iOS, Android, BlackBerry, and Windows Mobile devices. The app uses mobile forms to facilitate data transfers between devices in the field and a range of back office applications, cloud services, and data analytics tools. Data can be exported to a number of different formats, including PDF, CSV, Excel, raw data files, and others. ProntoForms provides clients with several default forms, but forms can also be customized for each set of circumstances. Clients can use the app for a number of different purposes, including environmental, health & safety inspections, timesheet automation, delivery dispatch coordination, tracking time on service calls, indicating which parts or products are required, billing, and others.\n\n"}
{"id": "81231", "url": "https://en.wikipedia.org/wiki?curid=81231", "title": "Pyrometer", "text": "Pyrometer\n\nA pyrometer is a type of remote-sensing thermometer used to measure the temperature of a surface. Various forms of pyrometers have historically existed. In the modern usage, it is a device that from a distance determines the temperature of a surface from the amount of the thermal radiation it emits, a process known as pyrometry and sometimes radiometry.\n\nThe word pyrometer comes from the Greek word for fire, \"πυρ\" (\"pyro\"), and \"meter\", meaning to measure. The word pyrometer was originally coined to denote a device capable of measuring the temperature of an object by its incandescence, visible light emitted by a body which is at least red-hot. Modern pyrometers or infrared thermometers also measure the temperature of cooler objects, down to room temperature, by detecting their infrared radiation flux.\n\nA modern pyrometer has an optical system and a detector. The optical system focuses the thermal radiation onto the detector. The output signal of the detector (temperature \"T\") is related to the thermal radiation or irradiance \"j\" of the target object through the Stefan–Boltzmann law, the constant of proportionality σ, called the Stefan-Boltzmann constant and the emissivity ε of the object.\nThis output is used to infer the object's temperature from a distance, with no need for the pyrometer to be in thermal contact with the object; most other thermometers (e.g. thermocouples and resistance temperature detectors (RTDs)) are placed in thermal contact with the object, and allowed to reach thermal equilibrium.\n\nPyrometry of gases presents difficulties. These are most commonly overcome by using thin filament pyrometry or soot pyrometry. Both techniques involve small solids in contact with hot gases.\n\nThe potter Josiah Wedgwood invented the first pyrometer to measure the temperature in his kilns, which first compared the color of clay fired at known temperatures, but was eventually upgraded to measuring the shrinkage of pieces of clay, which depended on kiln temperature. Later examples used the expansion of a metal bar.\n\nThe first disappearing filament pyrometer was built by L. Holborn and F. Kurlbaum in 1901. This device had a thin electrical filament between an observer's eye and an incandescent object. The current through the filament was adjusted until it was of the same colour (and hence temperature) as the object, and no longer visible; it was calibrated to allow temperature to be inferred from the current.\n\nThe temperature returned by the vanishing filament pyrometer and others of its kind, called brightness pyrometers, is dependent on the emissivity of the object. With greater use of brightness pyrometers, it became obvious that problems existed with relying on knowledge of the value of emissivity. Emissivity was found to change, often drastically, with surface roughness, bulk and surface composition, and even the temperature itself.\n\nTo get around these difficulties, the ratio or two-color pyrometer was developed. They rely on the fact that Planck's law, which relates temperature to the intensity of radiation emitted at individual wavelengths, can be solved for temperature if Planck's statement of the intensities at two different wavelengths is divided. This solution assumes that the emissivity is the same at both wavelengths and cancels out in the division. This is known as the gray body assumption. Ratio pyrometers are essentially two brightness pyrometers in a single instrument. The operational principles of the ratio pyrometers were developed in the 1920s and 1930s, and they were commercially available in 1939.\n\nAs the ratio pyrometer came into popular use, it was determined that many materials, of which metals are an example, do not have the same emissivity at two wavelengths. For these materials, the emissivity does not cancel out and the temperature measurement is in error. The amount of error depends on the emissivities and the wavelengths where the measurements are taken. Two-color ratio pyrometers cannot measure whether a material’s emissivity is wavelength dependent.\n\nTo more accurately measure the temperature of real objects with unknown or changing emissivities, multiwavelength pyrometers were envisioned at the US National Institute of Standards and Technology and described in 1992. Multiwavelength pyrometers use three or more wavelengths and mathematical manipulation of the results to attempt to achieve accurate temperature measurement even when the emissivity is unknown, changing, and different at all wavelengths.\n\nPyrometers are suited especially to the measurement of moving objects or any surfaces that can not be reached or can not be touched.\n\nTemperature is a fundamental parameter in metallurgical furnace operations. Reliable and continuous measurement of the metal temperature is essential for effective control of the operation. Smelting rates can be maximized, slag can be produced at the optimum temperature, fuel consumption is minimized and refractory life may also be lengthened. Thermocouples were the traditional devices used for this purpose, but they are unsuitable for continuous measurement because they melt and degrade.\n\nSalt bath furnaces operate at temperatures up to 1300 °C and are used for heat treatment. At very high working temperatures with intense heat transfer between the molten salt and the steel being treated, precision is maintained by measuring the temperature of the molten salt. Most errors are caused by slag on the surface which is cooler than the salt bath.\n\nThe \"tuyère pyrometer\" is an optical instrument for temperature measurement through the tuyeres which are normally used for feeding air or reactants into the bath of the furnace.\n\nA steam boiler may be fitted with a pyrometer to measure the steam temperature in the superheater.\n\nA hot air balloon is equipped with a pyrometer for measuring the temperature at the top of the envelope in order to prevent overheating of the fabric.\n\nPyrometers may be fitted to experimental gas turbine engines to measure the surface temperature of turbine blades. Such pyrometers can be paired with a tachometer to tie the pyrometer output with the position of an individual turbine blade. Timing combined with a radial position encoder allows engineers to determine the temperature at exact points on blades moving past the probe.\n\n\n"}
{"id": "253849", "url": "https://en.wikipedia.org/wiki?curid=253849", "title": "R-value (insulation)", "text": "R-value (insulation)\n\nIn building and construction, the R-value is a measure of how well an object, per unit of its exposed area, resists conductive flow of heat: the greater the R-value, the greater the resistance, and so the better the thermal insulating properties of the object. R-values are used in describing effectiveness of insulation and in analysis of heat flow across assemblies (such as walls, roofs, and windows) under steady-state conditions. Heat flow through an object is driven by temperature difference (e.g. formula_1) between two sides of the object, and the R-value quantifies how effectively the object resists this drive: formula_1 divided by the R-value and then multiplied by the surface area of the object's side gives the \"total rate of heat flow through the object\" (as measured in Watts or in BTUs per hour). Moreover, as long as the materials involved are dense solids in direct mutual contact, R-values are additive; for example, the total R-value of an object composed of several layers of material is the sum of the R-values of the individual Note that the R-value is the building industry term for what is in other contexts called ″thermal resistance per unit It is sometimes denoted RSI-value if the SI (metric) units are used.\n\nAn R-value can be given for a material (e.g. for polyethylene foam), or for an assembly of materials (e.g. a wall or a window). In the case of materials, it is often expressed in terms of R-value per unit length (e.g. per inch of thickness). The latter can be misleading in the case of low-density building thermal insulations, for which R-values are not additive: their R-value per inch is not constant as the material gets thicker, but rather usually decreases. \n\nThe units of an R-value (see ) are usually not explicitly stated, and so it is important to decide from context which units are being used: an R-value expressed in I-P (inch-pound) units is about 5.68 times larger than when expressed in SI units, so that, for example, a window that is R-2 in I-P units has an RSI of 0.35 (since 2/5.68=0.35). For R-values there is no difference between US customary units and imperial units. As far as how R-values are reported, all of the following mean the same thing: ″this is an R-2 window″; ″this is an R2 ″this window has an R-value of 2″; ″this is a window with R=2″ (and similarly with RSI-values, which also include the possibility ″this window provides RSI 0.35 of resistance to heat flow″).\n\nThe more a material is intrinsically able to conduct heat, as given by its thermal conductivity, the lower its R-value. On the other hand, the thicker the material, the higher its R-value. Sometimes heat transfer processes \"other\" than conduction (namely, \nconvection and radiation) significantly contribute to heat transfer within the material. In such cases, it is useful to introduce an ″apparent thermal conductivity″, which captures the effects of all three kinds of processes, and to define the R-value in general as formula_3. This comes at a price, however: R-values that include non-conductive processes may no longer be additive and may have significant temperature dependence. In particular, for a loose or porous material, the R-value per inch generally depends on the thickness, almost always so that it decreases with increasing thickness (polyisocyanurate (″polyso″) being an exception; its R-value/inch \"increases\" with thickness). For similar reasons, the R-value per inch also depends on the temperature of the material, usually increasing with decreasing temperature (polyso again being an exception); a nominally R-13 fiberglass batt may be R-14 at -12° C (10° F) and R-12 at +43° C (110° F). Nevertheless, in construction it is common to treat R-values as independent of temperature. Note that an R-value may not account for radiative or convective processes at the material's \"surface\", which may be an important factor for some applications.\n\nThe R-value is the reciprocal of the thermal transmittance (U-factor) of a material or assembly. The U.S. construction industry preferes to use R-values, however, because they are additive and because bigger values mean better insulation, neither of which is true for U-factors.\n\nThe \"U-factor\" or \"U-value\" is the overall heat transfer coefficient that describes how well a building element conducts heat or the rate of transfer of heat (in watts) through one square metre of a structure divided by the difference in temperature across the structure. The elements are commonly assemblies of many layers of components such as those that make up walls/floors/roofs etc. It measures the rate of heat transfer through a building element over a given area under standardised conditions. The usual standard is at a temperature gradient of 24 °C (75.2 °F), at 50% humidity with no wind (a smaller \"U-factor\" is better at reducing heat transfer). It is expressed in watts per meter squared kelvin (W/m²K). This means that the higher the U-value the worse the thermal performance of the building envelope. A low U-value usually indicates high levels of insulation. They are useful as it is a way of predicting the composite behavior of an entire building element rather than relying on the properties of individual materials.\n\nIn most countries the properties of specific materials (such as insulation) are indicated by the thermal conductivity, sometimes called a k-value or lambda-value (lowercase λ). The thermal conductivity (k-value) is the ability of a material to conduct heat; hence, the lower the k-value, the better the material is for insulation. Expanded polystyrene (EPS) has a k-value of around 0.033 W/mK. For comparison, phenolic foam insulation has a k-value of around 0.018 W/mK, while wood varies anywhere from 0.15 to 0.75 W/mK, and steel has a k-value of approximately 50.0 W/mK. These figures vary from product to product, so the UK and EU have established a 90/90 standard which means that 90% of the product will conform to the stated k-value with a 90% confidence level so long as the figure quoted is stated as the 90/90 lambda-value.\n\n\"U\" is the inverse of \"R\" with SI units of W/(mK) and U.S. units of BTU/(hr °F ft);\n\nwhere formula_5 is the heat flux, formula_6 is the temperature difference across the material, \"k\" is the material's coefficient of thermal conductivity and \"L\" is its thickness. In some contexts, \"U\" is referred to as unit surface conductance.\n\nSee also: tog (unit) or Thermal Overall Grade (where 1 tog = 0.1 m·K/W), used for duvet rating.\n\nNote that the term \"U-factor\" (which redirects here) is usually used in the U.S. and Canada to express the heat flow through entire assemblies (such as roofs, walls, and windows). For example, energy codes such as ASHRAE 90.1 and the IECC prescribe U-values. However, R-value is widely used in practice to describe the thermal resistance of insulation products, layers, and most other parts of the building enclosure (walls, floors, roofs). Other areas of the world more commonly use U-value/U-factor for elements of the entire building enclosure including windows, doors, walls, roof, and ground slabs.\n\nThe SI (metric) unit of R-value is \n\nwhereas the I-P (inch-pound) unit is \n\nFor R-values there is no difference between US customary units and imperial units, so the same I-P unit is used in both. \n\nSome sources use ″RSI″ when referring to R-values in SI units.\n\nR-values expressed in I-P units are approximately 5.68 times as large as R-values expressed in SI units. For example, a window that is R-2 in the I-P system is about RSI 0.35, since 2/5.68 ≈ 0.35.\n\nIn countries where the SI system is generally in use, the R-values will also normally be given in SI units. This includes the U.K., Australia, and New Zealand.\n\nI-P values are commonly given in the U.S. and Canada, though in Canada normally both I-P and RSI values are listed. \n\nBecause the units are usually not explicitly stated, one must decide from context which units are being used. In this regard, it helps to keep in mind that I-P R-values are 5.68 times larger than the corresponding SI R-values.\n\nMore precisely,\n\nThe Australian Government explains that the required total R-values for the building fabric vary depending on climate zone. \"Such materials include aerated concrete blocks, hollow expanded polystyrene blocks, straw bales and rendered extruded polystyrene sheets.\"\n\nIn Germany, after the law Energieeinsparverordnung (EnEv) introduced in 2009 (October 10) regarding energy savings, all new buildings must demonstrate an ability to remain within certain boundaries of the U-value for each particular building material. Further, the EnEv describes the maximum coefficient for each new material if parts are replaced or added to standing structures.\n\nThe U.S. Department of Energy has recommended R-values for given areas of the USA based on the general local energy costs for heating and cooling, as well as the climate of an area. There are four types of insulation: rolls and batts, loose-fill, rigid foam, and foam-in-place. Rolls and batts are typically flexible insulators that come in fibers, like fiberglass. Loose-fill insulation comes in loose fibers or pellets and should be blown into a space. Rigid foam is more expensive than fiber, but generally has a higher R-value per unit of thickness. Foam-in-place insulation can be blown into small areas to control air leaks, like those around windows, or can be used to insulate an entire house.\n\nIncreasing the thickness of an insulating layer increases the thermal resistance. For example, doubling the thickness of fiberglass batting will double its R-value, perhaps from 2.0 mK/W for 110 mm of thickness, up to 4.0 mK/W for 220 mm of thickness. Heat transfer through an insulating layer is analogous to adding resistance to a series circuit with a fixed voltage. However, this only holds approximately because the effective thermal conductivity of some insulating materials depends on thickness. The addition of materials to enclose the insulation such as sheetrock and siding provides additional but typically much smaller R-value.\n\nThere are many factors that come into play when using R-values to compute heat loss for a particular wall. Manufacturer R-values apply only to properly installed insulation. Squashing two layers of batting into the thickness intended for one layer will increase but not double the R-value. (In other words, compressing a fiberglass batt decreases the R-value of the batt but increases the R-value per inch.) Another important factor to consider is that studs and windows provide a parallel heat conduction path that is unaffected by the insulation's R-value. The practical implication of this is that one could double the R-value of insulation installed between framing members and realize substantially less than a 50 percent reduction in heat loss. When installed between wall studs, even perfect wall insulation only eliminates conduction through the insulation but leaves unaffected the conductive heat loss through such materials as glass windows and studs. Insulation installed between the studs may reduce, but usually does not eliminate, heat losses due to air leakage through the building envelope. Installing a continuous layer of rigid foam insulation on the exterior side of the wall sheathing will interrupt thermal bridging through the studs while also reducing the rate of air leakage.\n\nThe R-value is a measure of an insulation sample's ability to reduce the rate of heat flow under specified test conditions.\nThe primary mode of heat transfer impeded by insulation is conduction, but insulation also reduces heat loss by all three heat transfer modes: conduction, convection, and radiation.\nThe primary heat loss across an uninsulated air-filled space is natural convection, which occurs because of changes in air density with temperature. Insulation greatly retards natural convection making conduction the primary mode of heat transfer. Porous insulations accomplish this by trapping air so that significant convective heat loss is eliminated, leaving only conduction and minor radiation transfer.\nThe primary role of such insulation is to make the thermal conductivity of the insulation that of trapped, stagnant air. However this cannot be realized fully because the glass wool or foam needed to prevent convection increases the heat conduction compared to that of still air. \nThe minor radiative heat transfer is obtained by having many surfaces interrupting a \"clear view\" between the inner and outer surfaces of the insulation such as visible light is interrupted from passing through porous materials. Such multiple surfaces are abundant in batting and porous foam. Radiation is also minimized by low emissivity (highly reflective) exterior surfaces such as aluminum foil.\nLower thermal conductivity, or higher R-values, can be achieved by replacing air with argon when practical such as within special closed-pore foam insulation because argon has a lower thermal conductivity than air.\n\nHeat transfer through an insulating layer is analogous to electrical resistance. The heat transfers can be worked out by thinking of resistance in series with a fixed potential, except the resistances are thermal resistances and the potential is the difference in temperature from one side of the material to the other. The resistance of each material to heat transfer depends on the specific thermal resistance [R-value]/[unit thickness], which is a property of the material (see table below) and the thickness of that layer. A thermal barrier that is composed of several layers will have several thermal resistors in the analogous with circuits, each in series. Analogous to a set of resistors in parallel, a well insulated wall with a poorly insulated window will allow proportionally more of the heat to go through the (low-R) window, and additional insulation in the wall will only minimally improve the overall R-value. As such, the least well insulated section of a wall will play the largest role in heat transfer relative to its size, similar to the way most current flows through the lowest resistance resistor in a parallel array. Hence ensuring that windows, service breaks (around wires/pipes), doors, and other breaks in a wall are well sealed and insulated is often the most cost effective way to improve the insulation of a structure, once the walls are sufficiently insulated.\n\nLike resistance in electrical circuits, increasing the physical length (for insulation, thickness) of a resistive element, such as graphite for example, increases the resistance linearly; double the thickness of a layer means double the R-value and half the heat transfer; quadruple, quarters; etc. In practice, this linear relationship does not hold for compressible materials such as glass, wool, and cotton batting whose thermal properties change when compressed. \n\nTo find the average heat loss per unit area, simply divide the temperature difference by the R-value for the layer.\n\nIf the interior of a home is at 20 °C and the roof cavity is at 10 °C then the temperature difference is 10 °C (or 10 K). Assuming a ceiling insulated to RSI 2.0 (R = 2 mK/W), energy will be lost at a rate of 10 K / (2 K·m/W) = 5 watts for every square meter of ceiling. The RSI-value used here is for the actual insulating layer (and not per unit thickness of insulation).\n\nR-value should not be confused with the intrinsic property of thermal resistivity and its inverse, thermal conductivity. The SI unit of thermal resistivity is K·m/W. Thermal conductivity assumes that the heat transfer of the material is linearly related to its thickness.\n\nIn calculating the R-value of a multi-layered installation, the R-values of the individual layers are added:\n\nTo account for other components in a wall such as framing, first calculate the U-value (=1/R-value) of each component, then the area-weighted average U-value. The average R-value will be 1/(this average U-value). For example, if 10% of the area is 4 inches of softwood (R-value 5.6) and 90% is 2 inches of silica aerogel (R-value 20), the area-weighted U-value is 0.1/5.6 + 0.9/20 = 0.0629 and the weighted R-value is 1/0.0629 = 15.9.\n\nThermal conductivity is conventionally defined as the rate of thermal conduction through a material per unit area per unit thickness per unit temperature differential (ΔT). The inverse of conductivity is resistivity (or R per unit thickness). Thermal conductance is the rate of heat flux through a unit area at the installed thickness and any given ΔT.\n\nExperimentally, thermal conduction is measured by placing the material in contact between two conducting plates and measuring the energy flux required to maintain a certain temperature gradient.\n\nFor the most part, testing the R-value of insulation is done at a steady temperature, usually about with no surrounding air movement. Since these are ideal conditions, the listed R-value for insulation will almost certainly be higher than it would be in actual use, because most situations with insulation are under different conditions\n\nA definition of R-value based on apparent thermal conductivity has been proposed in document C168 published by the American Society for Testing and Materials. This describes heat being transferred by all three mechanisms—conduction, radiation, and convection.\n\nDebate remains among representatives from different segments of the U.S. insulation industry during revision of the U.S. FTC's regulations about advertising R-values illustrating the complexity of the issues.\n\nThere are weaknesses to using a single laboratory model to simultaneously assess the properties of a material to resist conducted, radiated, and convective heating. Surface temperature varies depending on the mode of heat transfer.\n\nIn the absence of radiation or convection, the surface temperature of the insulator should equal the air temperature on each side.\n\nIn response to thermal radiation, surface temperature depends on the thermal emissivity of the material. Light, reflective, or metallic surfaces that are exposed to radiation tend to maintain lower temperatures than dark, non-metallic ones.\n\nConvection will alter the rate of heat transfer (and surface temperature) of an insulator, depending on the flow characteristics of the gas or fluid in contact with it.\n\nWith multiple modes of heat transfer, the final surface temperature (and hence the observed energy flux and calculated R-value) will be dependent on the relative contributions of radiation, conduction, and convection, even though the total energy contribution remains the same.\n\nThis is an important consideration in building construction because heat energy arrives in different forms and proportions. The contribution of radiative and conductive heat sources also varies throughout the year and both are important contributors to thermal comfort\n\nIn the hot season, solar radiation predominates as the source of heat gain. According to the Stefan–Boltzmann law, radiative heat transfer is related to the fourth power of the absolute temperature (measured in kelvins: \"T\" [K] = \"T\" [°C] + 273.16). Therefore, such transfer is at its most significant when the objective is to cool (i.e. when solar radiation has produced very warm surfaces). On the other hand, the conductive and convective heat loss modes play a more significant role during the cooler months. At such lower ambient temperatures the traditional fibrous, plastic and cellulose insulations play by far the major role: the radiative heat transfer component is of far less importance, and the main contribution of the radiation barrier is in its superior air-tightness contribution.\nIn summary: claims for radiant barrier insulation are justifiable at high temperatures, typically when minimizing summer heat transfer; but these claims are not justifiable in traditional winter (keeping-warm) conditions.\n\nUnlike bulk insulators, radiant barriers resist conducted heat poorly. Materials such as reflective foil have a high thermal conductivity and would function poorly as a conductive insulator.\nRadiant barriers retard heat transfer by two means: by reflecting radiant energy away from its surface and by reducing the emission of radiation from its opposite side.\n\nThe question of how to quantify performance of other systems such as radiant barriers has resulted in controversy and confusion in the building industry with the use of R-values or 'equivalent R-values' for products which have entirely different systems of inhibiting heat transfer. (In the U.S., the federal government's R-Value Rule establishes a legal definition for the R-value of a building material; the term 'equivalent R-value' has no legal definition and is therefore meaningless.) According to current standards, R-values are most reliably stated for bulk insulation materials. All of the products quoted at the end are examples of these.\n\nCalculating the performance of radiant barriers is more complex. With a good radiant barrier in place, most heat flow is by convection, which depends on many factors other than the radiant barrier itself. Although radiant barriers have high reflectivity (and low emissivity) over a range of electromagnetic spectra (including visible and UV light), their thermal advantages are mainly related to their emissivity in the infra-red range. Emissivity values are the appropriate metric for radiant barriers. Their effectiveness when employed to resist heat gain in limited applications is established,\neven though R-value does not adequately describe them.\n\nR-values of products may deteriorate over time. For instance the compaction of loose fill cellulose creates voids that reduce overall performance; this may be avoided by densely packing the initial installation. Some types of foam insulation, such as polyurethane and polyisocyanurate are blown with heavy gases such as chlorofluorocarbons (CFC) or hydrochlorofluorocarbons (HFCs). However, over time a small amount of these gases diffuse out of the foam and are replaced by air, thus reducing the effective R-value of the product. There are other foams which do not change significantly with aging because they are blown with water or are open-cell and contain no trapped CFCs or HFCs (e.g., half-pound low density foams). On certain brands, twenty-year tests have shown no shrinkage or reduction in insulating value.\n\nThis has led to controversy as how to rate the insulation of these products. Many manufacturers will rate the R-value at the time of manufacture; critics argue that a more fair assessment would be its settled value. The foam industry adopted the LTTR (Long-Term Thermal Resistance) method, which rates the R-value based on a 15-year weighted average. However, the LTTR effectively provides only an eight-year aged R-value, short in the scale of a building that may have a lifespan of 50 to 100 years.\n\nCorrect attention to air sealing measures and consideration of vapor transfer mechanisms are important for the optimal function of bulk insulators. Air infiltration can allow convective heat transfer or condensation formation, both of which may degrade the performance of an insulation.\n\nOne of the primary values of spray-foam insulation is its ability to create an airtight (and in some cases, watertight) seal directly against the substrate to reduce the undesirable effects of air leakage.\n\nThe deterioration of R-values is especially a problem when defining the energy efficiency of an existing building. Especially in older or historic buildings the R-values defined before construction might be very different than the actual values. This greatly affects energy efficiency analysis. To obtain reliable data, R-values are therefore often determined via U-value measurements at the specific location (in situ). There are several potential methods to this, each with their specific trade-offs: thermography, multiple temperature measurements, and the heat flux method.\n\nThermography is applied in the building sector to assess the quality of the thermal insulation of a room or building. By means of a thermographic camera thermal bridges and inhomogeneous insulation parts can be identified. However, it does not produce any quantitative data. This method can only be used to approximate the U-value or the inverse R-value.\n\nThis approach is based on three or more temperature measurements inside and outside of a building element. By synchronizing these measurements and making some basic assumptions, it is possible to calculate the heat flux indirectly, and thus deriving the U-value of a building element. The following requirements have to be fulfilled for reliable results:\n\n\nThe U-value can be calculated as well by using a heat flux sensor in combination with two temperature sensors. By measuring the heat that is flowing through a building element and combining this with the inside and outside temperature, it is possible to define the U-value precisely. A measurement that lasts at least 72 hours with a temperature difference of at least 5 °C is required for a reliable result according to ISO 9869 norms, but shorter measurement durations give a reliable indication of the U-value as well. The progress of the measurement can be viewed on the laptop via corresponding software and obtained data can be used for further calculations. Measuring devices for such heat flux measurements are offered by companies like FluxTeq, Ahlborn, greenTEG and Hukseflux.\n\nVacuum insulated panels have the highest R-value, approximately R-45 (in U.S. units) per inch; aerogel has the next highest R-value (about R-10 to R-30 per inch), followed by polyurethane (PUR) and phenolic foam insulations with R-7 per inch. They are followed closely by polyisocyanurate (PIR) at R-5.8, graphite impregnated expanded polystyrene at R-5, and expanded polystyrene (EPS) at R-4 per inch. Loose cellulose, fibreglass (both blown and in batts), and rock wool (both blown and in batts) all possess an R-value of roughly R-2.5 to R-4 per inch.\n\nStraw bales perform at about R-1.5 per inch. However, typical straw bale houses have very thick walls and thus are well insulated. Snow is roughly R-1 per inch. Brick has a very poor insulating ability at a mere R-0.2 per inch; however it does have a relatively good thermal mass.\n\nNote that the above examples all use the U.S. (non-SI) definition for R-value.\n\nWhen determining the overall thermal resistance of a building assembly such as a wall or roof, the insulating effect of the surface air film is added to the thermal resistance of the other materials.\nIn practice the above surface values are used for floors, ceilings, and walls in a building, but are not accurate for enclosed air cavities, such as between panes of glass. The effective thermal resistance of an enclosed air cavity is strongly influenced by radiative heat transfer and distance between the two surfaces. See insulated glazing for a comparison of R-values for windows, with some effective R-values that include an air cavity.\n\nThe Federal Trade Commission (FTC) governs claims about R-values to protect consumers against deceptive and misleading advertising claims. \"The Commission issued the R-Value Rule\n\nThe primary purpose of the rule is to ensure that the home insulation marketplace provides this essential pre-purchase information to the consumer. The information gives consumers an opportunity to compare relative insulating efficiencies, to select the product with the greatest efficiency and potential for energy savings, to make a cost-effective purchase and to consider the main variables limiting insulation effectiveness and realization of claimed energy savings.\n\nThe rule mandates that specific R-value information for home insulation products be disclosed in certain ads and at the point of sale. The purpose of the R-value disclosure requirement for advertising is to prevent consumers from being misled by certain claims which have a bearing on insulating value. At the point of transaction, some consumers will be able to get the requisite R-value information from the label on the insulation package. However, since the evidence shows that packages are often unavailable for inspection prior to purchase, no labeled information would be available to consumers in many instances. As a result, the Rule requires that a fact sheet be available to consumers for inspection before they make their purchase.\n\nThe R-value Rule specifies:\n\n\n"}
{"id": "55359", "url": "https://en.wikipedia.org/wiki?curid=55359", "title": "SIMD", "text": "SIMD\n\nSingle instruction, multiple data (SIMD) is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. Such machines exploit data level parallelism, but not concurrency: there are simultaneous (parallel) computations, but only a single process (instruction) at a given moment. SIMD is particularly applicable to common tasks such as adjusting the contrast in a digital image or adjusting the volume of digital audio. Most modern CPU designs include SIMD instructions to improve the performance of multimedia use. Not to be confused with SIMT which utilizes threads.\n\nThe first use of SIMD instructions was in the ILLIAC IV, which was completed in 1966.\n\nSIMD was the basis for vector supercomputers of the early 1970s such as the CDC Star-100 and the Texas Instruments ASC, which could operate on a \"vector\" of data with a single instruction. Vector processing was especially popularized by Cray in the 1970s and 1980s. Vector-processing architectures are now considered separate from SIMD computers, based on the fact that vector computers processed the vectors one word at a time through pipelined processors (though still based on a single instruction), whereas modern SIMD computers process all elements of the vector simultaneously.\n\nThe first era of modern SIMD computers was characterized by massively parallel processing-style supercomputers such as the Thinking Machines CM-1 and CM-2. These computers had many limited-functionality processors that would work in parallel. For example, each of 65,536 single-bit processors in a Thinking Machines CM-2 would execute the same instruction at the same time, allowing, for instance, to logically combine 65,536 pairs of bits at a time, using a hypercube-connected network or processor-dedicated RAM to find its operands. Supercomputing moved away from the SIMD approach when inexpensive scalar MIMD approaches based on commodity processors such as the Intel i860 XP became more powerful, and interest in SIMD waned.\n\nThe current era of SIMD processors grew out of the desktop-computer market rather than the supercomputer market. As desktop processors became powerful enough to support real-time gaming and audio/video processing during the 1990s, demand grew for this particular type of computing power, and microprocessor vendors turned to SIMD to meet the demand. Hewlett-Packard introduced MAX instructions into PA-RISC 1.1 desktops in 1994 to accelerate MPEG decoding. Sun Microsystems introduced SIMD integer instructions in its \"VIS\" instruction set extensions in 1995, in its UltraSPARC I microprocessor. MIPS followed suit with their similar MDMX system.\n\nThe first widely deployed desktop SIMD was with Intel's MMX extensions to the x86 architecture in 1996. This sparked the introduction of the much more powerful AltiVec system in the Motorola PowerPC's and IBM's POWER systems. Intel responded in 1999 by introducing the all-new SSE system. Since then, there have been several extensions to the SIMD instruction sets for both architectures.\n\nAll of these developments have been oriented toward support for real-time graphics, and are therefore oriented toward processing in two, three, or four dimensions, usually with vector lengths of between two and sixteen words, depending on data type and architecture. When new SIMD architectures need to be distinguished from older ones, the newer architectures are then considered \"short-vector\" architectures, as earlier SIMD and vector supercomputers had vector lengths from 64 to 64,000. A modern supercomputer is almost always a cluster of MIMD computers, each of which implements (short-vector) SIMD instructions. A modern desktop computer is often a multiprocessor MIMD computer where each processor can execute short-vector SIMD instructions.\n\nAn application that may take advantage of SIMD is one where the same value is being added to (or subtracted from) a large number of data points, a common operation in many multimedia applications. One example would be changing the brightness of an image. Each pixel of an image consists of three values for the brightness of the red (R), green (G) and blue (B) portions of the color. To change the brightness, the R, G and B values are read from memory, a value is added to (or subtracted from) them, and the resulting values are written back out to memory.\n\nWith a SIMD processor there are two improvements to this process. For one the data is understood to be in blocks, and a number of values can be loaded all at once. Instead of a series of instructions saying \"retrieve this pixel, now retrieve the next pixel\", a SIMD processor will have a single instruction that effectively says \"retrieve n pixels\" (where n is a number that varies from design to design). For a variety of reasons, this can take much less time than retrieving each pixel individually, as with traditional CPU design.\n\nAnother advantage is that the instruction operates on all loaded data in a single operation. In other words, if the SIMD system works by loading up eight data points at once, the codice_1 operation being applied to the data will happen to all eight values at the same time. This parallelism is separate from the parallelism provided by a superscalar processor; the eight values are processed in parallel even on a non-superscalar processor, and a superscalar processor may be able to perform multiple SIMD operations in parallel.\n\n\nExamples of SIMD supercomputers (not including vector processors):\n\n\nSmall-scale (64 or 128 bits) SIMD became popular on general-purpose CPUs in the early 1990s and continued through 1997 and later with Motion Video Instructions (MVI) for Alpha. SIMD instructions can be found, to one degree or another, on most CPUs, including the IBM's AltiVec and SPE for PowerPC, HP's PA-RISC Multimedia Acceleration eXtensions (MAX), Intel's MMX and iwMMXt, SSE, SSE2, SSE3 SSSE3 and SSE4.x, AMD's 3DNow!, ARC's ARC Video subsystem, SPARC's VIS and VIS2, Sun's MAJC, ARM's NEON technology, MIPS' MDMX (MaDMaX) and MIPS-3D. The IBM, Sony, Toshiba co-developed Cell Processor's SPU's instruction set is heavily SIMD based. NXP founded by Philips developed several SIMD processors named Xetal. The Xetal has 320 16bit processor elements especially designed for vision tasks.\n\nModern graphics processing units (GPUs) are often wide SIMD implementations, capable of branches, loads, and stores on 128 or 256 bits at a time.\n\nIntel's AVX SIMD instructions now process 256 bits of data at once. Intel's Larrabee prototype microarchitecture includes more than two 512-bit SIMD registers on each of its cores (VPU: Wide Vector Processing Units), and this 512-bit SIMD capability is being continued in Intel's Many Integrated Core Architecture (Intel MIC) and Skylake-X.\n\nSIMD instructions are widely used to process 3D graphics, although modern graphics cards with embedded SIMD have largely taken over this task from the CPU. Some systems also include permute functions that re-pack elements inside vectors, making them particularly useful for data processing and compression. They are also used in cryptography. The trend of general-purpose computing on GPUs (GPGPU) may lead to wider use of SIMD in the future.\n\nAdoption of SIMD systems in personal computer software was at first slow, due to a number of problems. One was that many of the early SIMD instruction sets tended to slow overall performance of the system due to the re-use of existing floating point registers. Other systems, like MMX and 3DNow!, offered support for data types that were not interesting to a wide audience and had expensive context switching instructions to switch between using the FPU and MMX registers. Compilers also often lacked support, requiring programmers to resort to assembly language coding.\n\nSIMD on x86 had a slow start. The introduction of 3DNow! by AMD and SSE by Intel confused matters somewhat, but today the system seems to have settled down (after AMD adopted SSE) and newer compilers should result in more SIMD-enabled software. Intel and AMD now both provide optimized math libraries that use SIMD instructions, and open source alternatives like libSIMD, SIMDx86 and SLEEF have started to appear.\n\nApple Computer had somewhat more success, even though they entered the SIMD market later than the rest. AltiVec offered a rich system and can be programmed using increasingly sophisticated compilers from Motorola, IBM and GNU, therefore assembly language programming is rarely needed. Additionally, many of the systems that would benefit from SIMD were supplied by Apple itself, for example iTunes and QuickTime. However, in 2006, Apple computers moved to Intel x86 processors. Apple's APIs and development tools (XCode) were modified to support SSE2 and SSE3 as well as AltiVec. Apple was the dominant purchaser of PowerPC chips from IBM and Freescale Semiconductor and even though they abandoned the platform, further development of AltiVec is continued in several Power Architecture designs from Freescale and IBM. On WWDC '15, Apple announced SIMD Vectors support for version 2.0 of their new programming language Swift.\n\n\"SIMD within a register\", or SWAR, is a range of techniques and tricks used for performing SIMD in general-purpose registers on hardware that doesn't provide any direct support for SIMD instructions. This can be used to exploit parallelism in certain algorithms even on hardware that does not support SIMD directly.\n\nMicrosoft added SIMD to .NET in RyuJIT. Use of the libraries that implement SIMD on .NET are available in NuGet package Microsoft.Bcl.Simd\n\nIn 2013 John McCutchan announced that he had created a performant interface to SIMD instruction sets for the Dart programming language, bringing the benefits of SIMD to web programs for the first time. The interface consists of two types:\n\n\nInstances of these types are immutable and in optimized code are mapped directly to SIMD registers. Operations expressed in Dart typically are compiled into a single instruction with no overhead. This is similar to C and C++ intrinsics. Benchmarks for 4×4 matrix multiplication, 3D vertex transformation, and Mandelbrot set visualization show near 400% speedup compared to scalar code written in Dart.\n\nMcCutchan's work on Dart has been adopted by ECMAScript and Intel announced at IDF 2013 that they are implementing McCutchan's specification for both V8 and SpiderMonkey.\n\nEmscripten, Mozilla’s C/C++-to-JavaScript compiler, with extensions can enable compilation of C++ programs that make use of SIMD intrinsics or gcc style vector code to SIMD API of JavaScript resulting in equivalent speedups compared to scalar code.\n\nThough it has generally proven difficult to find sustainable commercial applications for SIMD-only processors, one that has had some measure of success is the GAPP, which was developed by Lockheed Martin and taken to the commercial sector by their spin-off Teranex. The GAPP's recent incarnations have become a powerful tool in real-time video processing applications like conversion between various video standards and frame rates (NTSC to/from PAL, NTSC to/from HDTV formats, etc.), deinterlacing, image noise reduction, adaptive video compression, and image enhancement.\n\nA more ubiquitous application for SIMD is found in video games: nearly every modern video game console since 1998 has incorporated a SIMD processor somewhere in its architecture. The PlayStation 2 was unusual in that one of its vector-float units could function as an autonomous DSP executing its own instruction stream, or as a coprocessor driven by ordinary CPU instructions. 3D graphics applications tend to lend themselves well to SIMD processing as they rely heavily on operations with 4-dimensional vectors. Microsoft's Direct3D 9.0 now chooses at runtime processor-specific implementations of its own math operations, including the use of SIMD-capable instructions.\n\nOne of the recent processors to use vector processing is the Cell Processor developed by IBM in cooperation with Toshiba and Sony. It uses a number of SIMD processors (a NUMA architecture, each with independent local store and controlled by a general purpose CPU) and is geared towards the huge datasets required by 3D and video processing applications. It differs from traditional ISAs by being SIMD from the ground up with no separate scalar registers.\n\nZiilabs produced an SIMD type processor for use on mobile devices, such as media players and mobile phones.\n\nLarger scale commercial SIMD processors are available from ClearSpeed Technology, Ltd. and Stream Processors, Inc. ClearSpeed's CSX600 (2004) has 96 cores each with 2 double-precision floating point units while the CSX700 (2008) has 192. Stream Processors is headed by computer architect Bill Dally. Their Storm-1 processor (2007) contains 80 SIMD cores controlled by a MIPS CPU.\n\n\n"}
{"id": "6297074", "url": "https://en.wikipedia.org/wiki?curid=6297074", "title": "Self-steering gear", "text": "Self-steering gear\n\nSelf-steering gear is equipment used on ships and boats to maintain a chosen course without constant human action. It is also known by several other terms, such as autopilot (borrowed from aircraft and considered incorrect by some) and autohelm (technically a Raymarine trademark, but often used generically). Several forms of self-steering gear exist, divided into two categories: electronic and mechanical.\n\nElectronic self-steering is controlled by electronics operating according to one or more input sensors, invariably at least a magnetic compass and sometimes wind direction or GPS position versus a chosen waypoint. The electronics module calculates the required steering movement and a drive mechanism (usually electrical, though possibly hydraulic in larger systems) causes the rudder to move accordingly.\n\nThere are several possibilities for the interface between the drive mechanism and the conventional steering system. On yachts, the three most common systems are:\nDepending on the sophistication of the control unit (e.g. tiller pilot, steering wheel attached Chartplotter, ...), electronic self-steering gear can be programmed to hold a certain compass course, to maintain a certain angle to the wind (so that sailing boats need not change their sail trim), to steer towards a certain position, or any other function which can reasonably be defined. However, the amount of power required by electrical actuators, especially if constantly in action because of sea and weather conditions, is a serious consideration. Long-distance cruisers, which have no external source of electricity and often do not run their engines for propulsion, typically have relatively strict power budgets and do not use electrical steering for any length of time. As the electronic autopilot systems require electricity to operate, many vessels also make use of PV solar panels or small wind turbines on the boat. This eliminates extra pollution and cuts costs.\n\nThe main goal of a mechanical self-steering gear is to keep a sailboat on a given course towards the apparent wind and to free the helmsman from the steering job. An advantageous side effect is that the sails are kept in optimal angle towards the apparent wind and deliver optimal propulsion force by that. Even in sailboats running under engine, the self steering gear can be used to keep the boat heading into the wind to easily set or change sails (exception: sheet-to-tiller principle).\nAs wind direction sensors are used \na) a wind vane mounted on an axis being tilted more or less towards the horizon (wind vane self-steering)\nb) the pressure of the wind in the sail(s) and by that the force on the sheet (sheet to tiller self-steering).\n\nThe different mechanical principles of coupling a change in apparent wind direction mechanically with a course changing actuator (rudder) can be roughly grouped:\n\nMechanical or \"wind vane\" self-steering started out as a way to keep model sail boats on course.Before the advent of Radio Control, model yacht racing (started before WW1) was typically contested on long narrow ponds, and the number of stops along the banks was counted as a penalty in the final result. Initially a system of counterweight on the tillers was devised to compensate for the weather helm when the model boat heeled in a gust. These crude systems evolved in a more sophisticated system called Braine Gear after its inventor, George Braine. The Braine steering gear was a fine-tuned system of quadrant on the rudder stock driven by the tension of the mainsail sheet and damped by a rubber band. A more sophisticated system called the vane gear was later devised, it relied on a small vane or airfoil driving the main rudder via an adjustable system of clockwork gears. It was very similar to the later vane driven autopilots seen on transatlantic yachts such as Blondie Hasler's self steering rudder.\nSome transatlantic singlehanded sailors used a crude form of self steering devices to cross the Atlantic Ocean in the 1920s and 1930s, the most notable being Frenchman Marin Marie (Pierre Durand Coupel de Saint Front) who crossed the Atlantic twice in the 1930s, first on a sailing yacht called \"Winnibelle II\" and secondly on a motor pinnace called \"Arielle\".\n\nSelf steering aboard \"Winnibelle II\" on its Atlantic crossing from Douarnenez, France, to New York in 1933 was somewhat similar to a Braine gear, using twin jibs (Trinquettes jumelles) with their sheets connected to the rudder via an array of blocks and lines. The long keeled Winnibelle II was perfectly stable on course on close-hauled or beam reach points of sailing but the self steering twin jib system could take over in the trickier downwind broad reaches and running points of sailing.\n\nOn the small motor pinnace \"Arielle\", a 13-metre boat propelled by a 65HP French made Baudouin diesel engine which sailed from New York to Le Havre in 1936, the task of steering a motor boat in the Atlantic swells was more daunting. \"Arielle\" had two rudders; the main one under the hull, in the propeller race, was for manual steering and the smaller auxiliary rudder was transom mounted. This auxiliary rudder could be mechanically driven by a special wind vane mounted atop of the coachroof consisting of two rectangular airfoils set at an angle on a vertical axle and balanced by a counterweight. It was simple and worked quite well, but could not steer the boat in very light breezes or flat calm.\n\nWhile Marin Marie was fitting out \"Arielle\" in New York he was approached by a French inventor named Casel who offered to fit an electrical autopilot of his invention, free of charge. The Casel autopilot was using the then revolutionary photoelectric cells and a system of light and reflecting mirrors on the magnetic compass rose. Its principle is somewhat similar to modern day electronic autohelms, excepting the modern flux-gate sensor for autopilots system. The Casel autopilot, which included an array of green, red and white telltale control lights, used an electric motor to act on the main rudder. Though its basic principle was sound and was useful in some sections of the passage, it proved to be somewhat too lightly built for a wet vibrating little boat and was trouble ridden. Marin Marie, though appreciative in some occasions generally loathed the temperamental device, specially when he discovered that Casel had inadvertently hidden his stores of Bordeaux wine in the autopilot compartment, unwillingly condemning him to a teetotal Atlantic crossing of some 20 days.\n\nMechanical self-steering gear is made by a number of manufacturers, but most systems produced today share the same principle (servo pendulum rudder, see below).\nAs well as their requirement for electric power, many long-distance cruisers observe that electronic self-steering machinery is complex and unlikely to be repairable without spare parts in remote areas. By contrast the vane gear offers at least the possibility of an improvised repair at sea, and can usually be rebuilt on land using non-specific parts (sometimes plumbing parts) by a local welder or machinist.\nTo minimize the speed loss by the self steering gear it is essential to have the vessel's sails balanced with little load on the rudder before any attempt is made to engage the self steering. With the sails are trimmed correctly, the force-balance of the servo oar and the main or auxiliary rudder is minimized that way, that the lowest angles of attack of rudder and servo oar towards the water flow are achieved. Some experimentation and judgement is usually needed, however, to determine the proper settings for a given vessel and steering mechanism.\nA popular source on contemporary windvane technology is \"The Windvane Self-Steering Handbook\". One particularly valuable contribution of Morris's book is his coverage of the variety of alloys used in vane gear manufacturing. Morris admits to his practice of setting a kitchen timer for a half-hour at a time and sleeping while the windvane steering device controls the helm, even in head winds of 25 to 35 knots. In a recent interview, he said he once narrowly missed being hit by a huge freighter while sleeping on his sail up the Red Sea. Morris points out, \"An autopilot wouldn't have made any difference in this case. If I had been using an electronic autopilot, that freighter still would have been there. I made a choice to sail two-thirds of my circumnavigation single-handed, and I accepted the risks that came with that decision. I guess fate was on my side.\"\n\nIn former Trim-Tab servo systems, the pivot movement of the servo blade around its vertical axis has been carried out by a trim tab Servo tab, which however costs some force due to the fact, that the trim tab is moved in the opposite direction to turn the servo blade.\nThe same holds for a trim trim tab, which is mounted at a big distance behind the ship's rudder, connected to it at its upper and lower end. This construction is called \"The Saye's Rigg\". \nAnother version of wind vane self steering on sail boats is known as the vertical axis vane and usually, because of the inferior steering force output compared to Servo Pendulum devices it makes use of a trim tab hung off the rudder to control the course of the boat. The vane spins at right angles to the ground and can lock to the trim tab in any desired position, as the boat falls off the wind the vane will be turned by the wind and will take the trim tab with it which in turn causes the rudder to move in the opposite direction and thus corrects course. Generally self steering like this, with a trim tab can only be used on boats with transom (or aft hung double enders) rudders as the trim tab needs to be mounted directly to and aft of the rudder to produce the desired effect, and of course has to be controlled even as the rudder swings side to side. This is typically accomplished by use of a slotted bar in which the connection to the vane assembly can slide in as the rudder turns. These self steering systems are generally simpler and are thus easier to set and adjust course as they don't make use of lines controlling the rudder but control it more directly through solid linkages.\nA related device has been used on some windmills, the fantail, a small windmill mounted at right angles to the main sails which automatically turns the heavy cap and main sails into the wind, (invented in England in 1745). (When the wind is already directly into the main vanes, the fantail remains essentially motionless.)\n\nOnly few manufacturers have been successful with systems that operate an auxiliary rudder directly from the windvane (Non-servo systems: Windpilot Atlantik, Hydrovane); the picture of the windvane shown uses this principle with the large fabric vane on a vertical axis (the use of wind vanes with a nearly horizontal axis is used predominantly).\n\nThe most widespread form of self-steering, the servo pendulum, was introduced to cope with the power required to operate a larger rudder and was a successor to the servo trim tab principle (introduced by Herbert \"Blondie\" Hasler). \nCommon to all servo pendulum rudder(oar, blade) systems is the fact, that the speed of the boat through the water is used to amplify the small force coming from the wind vane in order to be able to turn the rudder. The servo blade can be turned in its vertical axis and is hung like a pendulum. When it is turned around its vertical axis, the water flow initiates a sideways force on the blade area, and the forceful swing movement to the side is used to act on a rudder (ship's rudder or auxiliary rudder being integrated in the system).\nA narrow upright board, the wind vane, is mounted on a nearly horizontal axis carrier that is itself rotated around its vertical axis so that with the boat traveling in the desired direction the vane is vertical and edge-on to the wind. The wind vane is balanced by a small weight below the pivot, but if the boat turns so that the board is no longer edge-on to the wind it will be blown over to one side as the extra surface area is revealed. This movement is transmitted by a series of linkages to a blade (or oar) in the water, so that the oar is turned around its vertical axis, when the wind vane rotates from its neutral position.\nAs the blade described above turns, the pressure of water moving past it causes it to swing out sideways on the end of a pivoted rod. An immersed area of 0.1 m at 1 m lever length at a boat speed of 2.5 m/s (about 5 knots) and 5° angle of attack already generates a moment of 180 N⋅m, when the oar has a NACA0012 profile. The steering force of the servo oar is transmitted to the main rudder typically involving an arrangement of two lines and four or more rolls to guide the steering ropes to the helm or the steering wheel.\n\nModern servo pendulum self-steering devices with optimized transmission and low friction mechanics are more and more used for day sailing and cruising; formerly being used mainly for long distance ocean passages. The increased low wind capabilities of optimized, modern devices enable downwind steering down to 1.3 m/s apparent wind and 1.5 kn of boat speed – properties that make an electronic steering device nearly redundant and enable crossing the doldrums under wind vane self-steering. An increasing number of long distance regatta sailors are using wind vane self-steering because of the fact, that the sails are always kept in optimal angle towards the wind, and hence the speed of the boat is kept at the possible maximum.\n\nThe mathematical description of the horizontal windvane servo self-steering covers the relation of a course error to a steady-state rudder angle to correct for the course error. The dynamics are described by force and momentum coupling equations. Mainly three different mechanical transmission principles are in use: Murray slide-block joint, 90° bevel gear, Z-shaft, which due to their geometry have different steering force changes by course error change.\n\nIn cases, when a pure servo pendulum self-steering gear is not usable (Hydraulic rudder gear, very big force needed to turn the rudder), auxiliary rudder systems are used. They consist of a servo pendulum rudder coupled directly to an auxiliary rudder which is part of the self-steering system. The main rudder in such case is used to \"trim\" the main course and the self-steering gear steers \"around\" that main course according to the changes of the apparent wind.\n\nAside of the widespread mechanical self-steering through a wind vane being mechanically coupled to the rudder or a servo pendulum rudder, there is a mechanical self steering principle called \"sheet-to-tiller\". Rollo Gebhard crossed the Atlantic in his 5.6 m long Solveig using such a method. The sheet-to-tiller self-steering consists of a connection between the spring-loaded tiller and a sheet using the force of the wind in the sail to steer the boat.\n\nFor quite a long time there was little development in the self steering systems that were available commercially. \nMost new developments came in the form of self-build systems. Crucial roles were played by Walt Murray, an American who published his designs on his website. and Dutchman Jan Alkema who developed a new windvane, the so-called Up Side Down windvane (USD for short, commercially available from only two brands) and a new kind of servo pendulum system that could be fitted to boats with a transom hung rudder. For this last invention Jan Alkema was rewarded the John Hogg-Price from the AYRS ( Amateur Yacht Research Society) in 2005. Jan Alkema published a lot of his inventions on Walt Murray's website.\n\nJoern Heinrich added in 2010 a mechanism using the roll angle of the boat in downwind situation for a correctional servo oar angle of attack which increases course stability and lowers the risk of broaching in following seas. Joern Heinrich also published a mechanism which uses a fin in the water to compensate for the apparent wind change during the acceleration/deceleration of multihull yachts with larger speed potential like Catamarans and Trimarans in gusts. Heinrich applies his own parametric simulation software VaneSim to optimize windvane self-steering devices according to boat properties.\n\nIn 2002 Robert Chicken patented a sheet to tiller system in the UK, known as The Steersman. It consists of two swinging platforms fitted on to the cockpit coamings either side of the boat. The normal jib sheet winches are moved from their normal position and then re-bolted down on to the top of these platforms.\nWith the sails set, the leeward jib sheet is cleated to the winch in the normal way, and the wind pressure in the jib, transmitted through the jib sheet, swings the platform forward. To balance this movement, a shock cord spring tensioned between the platform and a point at the stern of the boat, keeps the platform in a central neutral position. Once set, any slight changes in wind strength or direction, causes the platform to swing forwards or backwards. A simple linkage then transmits this movement to the helm to keep the boat on course.\nThe pressure in the sails can vary hugely depending on the wind strength and the direction that the boat is travelling relative to the wind. To accommodate this, the spring is arranged in a ‘block-and-tackle’ form with a double block fitted to the stern of the boat, and a single block clipped to the platform. The fixed end, and tail end are also attached to the platform; the fixed end is clipped, and the tail end passes through a jamb cleat for fine adjustment. With this in place, the maximum spring tension is now made up of four lengths of shock cord. For a lower wind pressure in the jib, the fixed end, and the single block can be re-clipped to an attachment point on the base of the platform. This then gives a range of spring strengths from one to four shock cord lengths. For very light wind pressures, a single lighter length of shock cord is used instead.\nThe claimed benefit of this design over a windvane system that it \"is far more sensitive because it uses the larger jib area to sense any changes in the wind\" is questionable. When the jib is in laminar flow, i.e. optimally trimmed and delivers maximum propulsion, the force at the sheet is biggest and decreases to both sides of course deviation from this optimum. Concluding from that, the ship must be sailed with sub-optimal sail trim in order to have the proper steering correction at the tiller. \nIts position in the cockpit area however leaves the stern of the boat clear for other purposes such as dinghy davits, stern ladders etc. \nIn 2012, the invention won the Haven Academy Award in the UK. The chairman of the judging committee was Sir Robin Knox-Johnston, the first person to complete a circumnavigation non-stop, single handed.\n\nSome of the most famous self-steering boats include:\n\n"}
{"id": "863268", "url": "https://en.wikipedia.org/wiki?curid=863268", "title": "SubEthaEdit", "text": "SubEthaEdit\n\nSubEthaEdit is a collaborative real-time editor designed for Mac OS X. The name comes from the Sub-Etha communication network in \"The Hitchhiker's Guide to the Galaxy\" series.\n\nApart from the usual text-editing capabilities, collaborative editing is one of SubEthaEdit's marquee features. The collaboration is document-based, non-locking, and non-blocking. Anyone participating in the collaborative edit can type in the document anywhere at any time. Using Bonjour (formerly Rendezvous) and BEEP, SubEthaEdit works without any configuration on the LAN but can also coordinate collaborative editing over the Internet. SubEthaEdit can be used for distributed pair programming and collaborative note-taking in conferences. In 2007, TheCodingMonkeys licensed this \"Subetha Engine\" to Panic for use in Coda.\n\nOther SubEthaEdit features include:\nSubEthaEdit was first released under the name Hydra in early 2003. For legal reasons, the name was changed to SubEthaEdit in late 2004. The first version of Hydra was built in just a few months with the intent of winning an Apple Design Award, which it did at Apple's Worldwide Developers Conference 2003. In June 2014, SubEthaEdit 4 was released, distributed exclusively in the Mac App Store.\n\nWith the version 5 release the application became free and open source, under the MIT licence.\n\n\n"}
{"id": "1988772", "url": "https://en.wikipedia.org/wiki?curid=1988772", "title": "Tatung Company", "text": "Tatung Company\n\nTatung Company () (Tatung; ) is a multinational corporation established in 1918 and headquartered in Zhongshan, Taipei, Taiwan.\n\nEstablished in 1918 and headquartered in Taipei, Tatung Company has evolved into a conglomerate from its substantial heritage. Tatung Company holds 3 business groups, which include 8 business units such as Industrial Appliance BU, Motor BU, Wire & Cable BU, Solar BU, Smart Meter BU, System Integration BU, Appliance BU, Advanced Electronics BU. Being a leader in the field of energy saving and green energy creation, Tatung has pioneered in the development of national smart grid in Taiwan and many smart IoT solutions. As a conglomerate, Tatung's investees involve in some major industries such as optoelectronics, energy, system integration, industrial system, branding retail channel, and asset development.\n\nHistory\n\nFounding Period\n\n1918\n\n   Establishment of Xie Zhi Business Enterprise, the forerunner of Tatung Company, by founder and chairman, Mr. Shang-Zhi Lin\n\n   Completed over 600 constructions, including the Danshuei River embankment project and the Executive Yuan building\n\n1939\n\n   Tatung Iron Works was established. (When Taiwan was retroceded to the Republic of China in 1945, Tatung Iron Works was renamed \"Tatung Steel and Machinery Manufacturing Company.\")\n\n1942\n\n   Mr. T. S. Lin succeeded as chairman of Tatung and principal of both Tatung High School and Tatung University\n\n   Establishment of Tatung High School\n\n1949\n\n   Mass production of electric fans & motors (Pioneering in Home Appliance & Motor industries)\n\n1960\n\n   Mass production of Tatung rice cookers, a revolutionary step for housewives in Taiwan\n\nFledgling Period\n\n1962\n\n   The Company became publicly listed on the Taiwan Stock Exchange\n\n1966\n\n   Establishment of Wire & Cable Plant in Taoyuan County\n\n1968\n\n  The Company renamed from Tatung Steel and Machinery Company to Tatung Company and officially registered as so \n\n1969\n\n   Company mascot (Tatung Boy)and song launched\n\n   Mass production of coloured TVs\n\n1970\n\n  Revenues exceeded NT$2.2 billion, making Tatung Taiwan's foremost private company\n\n1972\n\n  Mr. W. S. Lin appointed as president of Tatung\n\n1977\n\n   Participated in the Ten Major Infrastructure Projects with the construction of a slag treatment facility for China Steel Corp. and provision of the turnkey solution for CKS International Airport's power control station\n\n1985\n\n  Became the first Taiwanese company to win the “Premier’s Gold Award” as the country's No. 1 exporter\n\n1991\n\n   Ranked No. 1 in domestic and export sales of information products by the Institute for Information Industry\n\n2000\n\n   Chunghwa Picture Tubes was listed on the OTC market\n\nPositioning for the New Millennium\n\n2001\n\n   Global Management Division set up at the company's headquarters\n\n   Chunghwa Picture Tubes was listed on the Taiwan Stock Exchange\n\n2003\n\n   Mass production of LCD & PDP TVs (Pioneering in Digital Display products)\n\n2004\n\n   Set up a new subsidiary Toes Opto-Mechatronics Company\n\n   Established SeQual Technologies Co. to produce the oxygen generator for clinical therapy and home health care uses\n\n   Established Tatung Compressors (Zhongshan) Co. in China\n\n2005\n\n   Consolidated Tatung's Desktop PC Business Unit with Elitegroup Computer Systems (ECS), making Tatung the largest shareholder of ECS\n\n   Established Tatung Wire & Cable Technology (Wujiang) Co. in China\n\n2006\n\n   Mr. W. S. Lin was elected as chairman and president of Tatung\n\n   Green Energy Technology started trading on the emerging stock market\n\n2007\n\n   Forward Electronics invested in Apollo Solar Energy Co. to expand its scope into the market of solar cell module\n\n2008\n\n   Green Energy Technology was listed on the Taiwan Stock Exchange on 25th January\n\n   Established Tatung Electric Technology (Vietnam) Co. for the manufacturing and sales of wires and cables\n\n2009\n\n   Tatung University launched WiMAX network, the first wireless broadband network ever built in campus alike\n\n   To help the victims of typhoon Morakot, Tatung initiated a Special Service Project in which 1,000 technicians and 70 service trucks were mobilized in and around the affected areas to help handle damaged home appliance items. The employees of Tatung Group together with the staff of Tatung University and Tatung High School also donated their one-day earnings totaling to 10 million Taiwanese dollars to those in need\n\n2010\n\n   New Energy Business Unit set up a crystal growth center to manufacture multi-crystalline silicon bricks, a milestone for HQ's involvement in the crystal growing business\n\n   Luxury condominium, “Tatung Noble Residences (Phase Ⅱ of Tatung Tomorrow World)”, the 2nd project in Nangang by Shan Chih Asset Development, was under construction\n\n2011\n\n   Mrs. W.Y. Lin was appointed President of Tatung\n\n2012\n\n   Lithium iron phosphate cathode material by Tatung Fine Chemicals successfully entered into Japanese market of energy storage\n\n2013\n\n   Tatung Consumer Products Co.(TCPC), Tatung's brand channel, set up an official account on LINE along with the release of Tatung Boy character stickers and emoticons which, within 24 hours of online introduction, attracted more than one million active users and the download volume it created broke the record to become No.1 in the official account category of LINE\n\n   Tatung-branded AI rice cookers were introduced to the market to mark the first rice cooker of artificial intelligence by Taiwan own brand maker\n\n2014\n\n   The Company, as an important supplier of high efficiency motors and cables, was invited by Steel Asia Manufacturing Corporation, the largest steel company in the Philippines, to its new plant's inauguration ceremony in Davao\n\n   Tatung Group donated 12 million dollars to Kaohsiung City Government to help the victims in the disaster of explosion\n\n   All new brand --“in fresh”— introduces hydroponic vegetables that are fresh, healthy, and pollution free to provide customers a new healthy choice by utilizing optoelectronic technology in agriculture\n\n   Co-organizing “Smart City Summit and Expo” to promote Tatung's unique total solution for smart energy saving system\n\n2015\n\n   Establishing eTungGo, Tatung's online shop, to involve in e-commerce business\n\n   Tatung Healthy Life Store began its official operation\n\n   Tatung utilized the technology of Internet of Things on smart appliances and cooperated with SIGMU to create all-rounded smart living\n\n   Tatung won the bidding of solar PV roofing plan for the public buildings in Kinmen County. The project enables Kinmen County Government to get the trends in electricity consumption and gauge reportable events on real-time power generation via Tatung's smart meters and energy saving monitoring system so that electricity losses can be reduced and efficiency on power generation can be enhanced\n\n   Tatung rice cooker, an important cultural asset representing Taiwan's everyday life, was exhibited and demonstrated as a cultural & creative artifact in Tsutaya Books(Japan), one of the twenty most beautiful bookstores in the world\n\n   Tatung won the bidding of solar PV power generation system for Pratas Island. The project includes 40kWp of solar energy and fuel control system to monitor the operation of generator and load status so that output of solar PV can be controlled and uninterrupted power supply in Pratas Island can be expected\n\n   Tatung cooperated with ITRI and Toshiba to promote microgrid within smart grid in Penghu. The project not only implements regional application of microgrid in Taiwan but also raises the proportion of renewable energy hoping to maintain stable power supply and optimize the regulation for demand and supply\n\n2016\n\n   Tatung won the bidding of solar PV roofing system for the public buildings in New Taipei City. System of microgrid is introduced to the project to enable the emergency supply of electricity when without power supply from state grid in an event of natural disasters\n\n\n"}
{"id": "2028016", "url": "https://en.wikipedia.org/wiki?curid=2028016", "title": "Technology alignment", "text": "Technology alignment\n\nBusiness and technology alignment, or just technology alignment, corrects terminology and assumptions used in business to better match those of technology and standards anticipated in the technology strategy and technology roadmaps. \n\nWhen technology is changing very rapidly in an industry, the aligning of business terms to the distinctions that the technology requires tends to dominate any enterprise taxonomy development effort. In such circumstances, consultants or specific technology training is usually required, as the organization lacks the internal skills or experience with the technologies that it expects to be using soon.\n\nIn government, for example, citizen use of the Internet and the increased availability of teleworkers has presented special challenges and opportunities, typically called \"e-government\". At the same time, internal operational efficiencies have become more of a priority due to rising competition between jurisdictions. Often the first step is to limit the number of different departments or agencies involved. By \"consolidating the technology operations of 91 state agencies into the Virginia Information Technology Agency, the State of Virginia estimates an eventual savings of nearly $100 million a year.\" - \n\n\"Similarly, the U.S. National Performance Review recommended a data processing consolidation and modernization initiative citing industry experience suggesting operational savings of between 30% and 50%.\" - \n\nWhile \"California is the cradle of the information technology industry\" its own state government claims that \"collaborative exploration and exploitation of emerging technologies is extremely rare within state government\", accordingly it seeks to \"improve customer relationships through online services.\" However such efforts tend to rely very much on driving personalities and leadership. After one resignation, the state \"lost the vision and executive sponsorship that contributed to its success and the national recognition of California's emerging eGovernment activities.\" This is a major problem in all technology alignment.\n\nIn Canada, a similar nationwide effort called Service Canada has similar goals, and has run into similar problems: \"The big complaints are that departments fight over their turf and are organized to serve the bureaucracy, not Canadians. They don’t share data, information, common infrastructure, technology or integrate their business processes. Senior bureaucrats are often accused of being out of touch with the needs of Canadians.\" - The government claims that it \"is expected to save C$3 billion over five years by automating manual operations, consolidating call centres and reducing overpayments in Canada Pension Plan and employment service.\" It \"will need to spend about C$500 million for technology, consolidate or move offices and retrain the thousands of workers whose jobs were eliminated by automation.\"\n\nWhen, as in California or Canada, new leadership and massive change to operations is required, technology alignment may simply excuse a massive business process reengineering and downsizing exercise. This too is a common situation in technology alignment: using the fact of new technology as a pretext for other large changes.\n\nHowever, as with all such exercises, there are claims that better service will result, by (in Canada) \"opening new offices and creating more front-line jobs in local communities\" or (in California) \"a 20% reduction in the workforce performing shared services\" and of \"nearly 9,000 state employees... about 3,600 are engaged in common core functions. An eventual 20% reduction in this workforce segment is possible through attrition when phased in over 5 years.\" - \n\nThese claims also are fairly typical: despite a longstanding admission among experts that there is a \"productivity paradox\", the introduction of new information technology and more automated work processes are always assumed to be \"more efficient\" than what they replace. Accordingly, technology alignment is probably not a passing fad, but, seems to be driven by factors built into business and technology culture.\n\n"}
{"id": "51180029", "url": "https://en.wikipedia.org/wiki?curid=51180029", "title": "Timeline of online video", "text": "Timeline of online video\n\nThis is a timeline of online video.\n"}
{"id": "2144540", "url": "https://en.wikipedia.org/wiki?curid=2144540", "title": "Toxaphene", "text": "Toxaphene\n\nToxaphene was an insecticide used primarily for cotton in the southern United States during the late 1960s and 1970s. Toxaphene is a mixture of over 670 different chemicals and is produced by reacting chlorine gas with camphene. It can be most commonly found as a yellow to amber waxy solid.\n\nToxaphene was banned in the United States in 1990 and was banned globally by the 2001 Stockholm Convention on Persistent Organic Pollutants. It is a very persistent chemical that can remain in the environment for 1–14 years without degrading, particularly in the soil.\n\nTesting performed on animals, mostly rats and mice, has demonstrated that toxaphene is harmful to animals. Exposure to toxaphene has proven to stimulate the central nervous system, as well as induce morphological changes in the thyroid, liver, and kidneys.\n\nToxaphene has been shown to cause adverse health effects in humans. The main sources of exposure are through food, drinking water, breathing contaminated air, and direct contact with contaminated soil. Exposure to high levels of toxaphene can cause damage to the lungs, nervous system, liver, kidneys, and in extreme cases, may even cause death. It is thought to be a potential carcinogen in humans, though this has not yet been proven.\n\nToxaphene is a synthetic organic mixture composed of over 670 chemicals, formed by the chlorination of camphene (CH) to an overall chlorine content of 67–69% by weight. The bulk of the compounds (mostly chlorobornanes, chlorocamphenes, and other bicyclic chloroorganic compounds) found in toxaphene have chemical formulas ranging from CHCl to CHCl, with a mean formula of CHCl. The formula weights of these compounds range from 308 to 551 grams/mole; the theoretical mean formula has a value of 414 grams/mole. Toxaphene is usually seen as a yellow to amber waxy solid with a piney odor. It is highly insoluble in water but freely soluble in aromatic hydrocarbons and readily soluble in aliphatic organic solvents. It is stable at room temperature and pressure. It is volatile enough to be transported for long distances through the atmosphere.\n\nToxaphene was primarily used as a pesticide for cotton in the southern United States during the late 1960s and 1970s. It was also used on corn, small grains, vegetables, and soybeans to control ectoparasites such as lice, flies, ticks, mange, and scam mites on livestock. In some cases it was used to kill undesirable fish species in lakes and streams. The breakdown of usage can be summarized: 85% on cotton, 7% to control insect pests on livestock and poultry, 5% on other field crops, 3% on soybeans, and less than 1% on sorghum.\n\nThe first recorded usage of toxaphene was in 1966 in the United States and by the early to mid 1970’s, toxaphene was the United States' most heavily used pesticide. Over 34 million pounds of toxaphene were used annually from 1966 to 1976. As a result of Environmental Protection Agency restrictions, annual toxaphene usage fell to 6.6 million pounds in 1982. In 1990, the EPA banned all usage of toxaphene in the United States. Toxaphene is still used in countries outside the United States but much of this usage has been undocumented. Between 1970 and 1995, global usage of toxaphene was estimated to be 670 million kilograms (1.5 billion pounds).\n\nToxaphene was first produced in the United States in 1947 although it was not heavily used until 1966. By 1975, toxaphene production reached its peak at 59.4 million pounds annually. Production decreased more than 90% from this value by 1982 due to Environmental Protection Agency restrictions. Overall, an estimated 234,000 metric tons (over 500 million pounds) have been produced in the United States. Between 25% and 35% of the toxaphene produced in the United States has been exported. There are currently 11 toxaphene suppliers worldwide.\n\nWhen released into the environment, toxaphene can be quite persistent and exists in the air, soil, and water. In water, it can evaporate easily and is fairly insoluble. Its solubility is 3 mg/L of water at 22 degrees Celsius. Toxaphene breaks down very slowly and has a half-life of up to 12 years in the soil. It is most commonly found in air, soil, and sediment found at the bottom of lakes or streams. It can also be present in many parts of the world where it was never used because toxaphene is able to evaporate and travel long distances through air currents. Toxaphene can eventually be degraded, through dechlorination, in the air using sunlight to break it down. The degradation of toxaphene usually occurs under aerobic conditions. The levels of toxaphene have decreased since its ban, however, due to its persistence can still be found in the environment today.\n\nThe three main paths of exposure to toxaphene are ingestion, inhalation, and absorption. For humans, the main source of toxaphene exposure is through ingested seafood. When toxaphene enters the body, it usually accumulates in fatty tissues. It is broken down through dechlorination and oxidation in the liver, and the byproducts are eliminated through feces.\n\nPeople that live near an area that has high toxaphene contamination are at high risk to toxaphene exposure through inhalation of contaminated air or direct skin contact with contaminated soil or water. Eating large quantities of fish on a daily basis also increases susceptibility to toxaphene exposure. Finally, exposure is rare, yet possible through drinking water when contaminated by toxaphene runoff from the soil. However, toxaphene has been rarely seen at high levels in drinking water due to toxaphene's high levels of insolubility in water.\n\nShellfish, algae, fish and marine mammals have all been shown to exhibit high levels of toxaphene. People in the Canadian Arctic, where a traditional diet consists of fish and marine animals, have been shown to consume ten times the accepted daily intake of toxaphene. Also, blubber from beluga whales in the Arctic were found to have unhealthy and toxic levels of toxaphene.\n\nWhen inhaled or ingested, sufficient quantities of toxaphene can damage the lungs, nervous system, and kidneys, and may cause death. The major health effects of toxaphene involve central nervous system stimulation leading to convulsive seizures. The dose necessary to induce nonfatal convulsions in humans is about 10 milligrams per kilogram body weight per day. Several deaths linked to toxaphene have been documented in which an unknown quantity of toxaphene was ingested intentionally or accidentally from food contamination. The deaths are attributed to respiratory failure resulting from seizures. Chronic inhalation exposure in humans results in reversible respiratory toxicity.\n\nA study conducted between 1954 and 1972 of male agricultural workers and agronomists exposed to toxaphene and other pesticides showed that there are higher proportions of bronchial carcinoma in the test group than in the unexposed general population. However, toxaphene may not have been the main pesticide responsible for tumor production. Tests on lab animals show that toxaphene causes liver and kidney cancer, so the EPA has classified it as a Group B2 carcinogen, meaning it is a probable human carcinogen. The International Agency for Research on Cancer has classified it as a Group 2B carcinogen.\n\nToxaphene can be detected in blood, urine, breast milk, and body tissues if a person has been exposed to high levels, but it is removed from the body quickly, so detection has to occur within several days of exposure.\n\nIt is not known whether toxaphene can affect reproduction in humans.\n\nToxaphene was used to treat mange in cattle in California in the 1970s and there were reports of cattle deaths following the toxaphene treatment.\n\nChronic oral exposure in animals affects the liver, the kidney, the spleen, the adrenal and thyroid glands, the central nervous system, and the immune system. Toxaphene stimulates the central nervous system by antagonizing neurons leading to hyperpolarization of neurons and increased neuronal activity.\n\nToxaphene has been found on at least 68 of the 1,699 National Priorities List sites identified by the United States Environmental Protection Agency. Toxaphene has been forbidden in Germany since 1980. Most uses of toxaphene were cancelled in the U.S. in 1982 with the exception of use on livestock in emergency situations, and for controlling insects on banana and pineapple crops in Puerto Rico and the U.S. Virgin Islands. All uses of toxaphene were cancelled in the U.S. in 1990.\n\nToxaphene has been banned in 37 countries, including Austria, Belize, Brazil, Costa Rica, Dominican Republic, Egypt, the EU, India, Ireland, Kenya, Korea, Mexico, Panama, Singapore, Thailand and Tonga. Its use has been severely restricted in 11 other countries, including Argentina, Columbia, Dominica, Honduras, Nicaragua, Pakistan, South Africa, Turkey, and Venezuela.\n\nIn the Stockholm Convention on POPs, which came into effect on 17 May 2004, twelve POPs were listed to be eliminated or their production and use restricted. The OCPs or pesticide-POPs identified on this list have been termed the \"dirty dozen\" and include aldrin, chlordane, DDT, dieldrin, endrin, heptachlor, hexachlorobenzene, mirex, and toxaphene.\n\nThe EPA has determined that lifetime exposure to 0.01 milligrams per liter of toxaphene in the drinking water is not expected to cause any adverse noncancer effects if the only source of exposure is drinking water, and has established the maximum contaminant level (MCL) of toxaphene at 0.003 mg/L. The United States Food and Drug Administration (FDA) uses the same level for the maximum level permissible in bottled water.\n\nThe FDA has determined that the concentration of toxaphene in bottled drinking water should not exceed 0.003 milligrams per liter.\n\nThe United States Department of Transportation lists toxaphene as a hazardous material and has special requirements for marking, labeling, and transporting the material.\n\nIt is classified as an extremely hazardous substance in the United States as defined in Section 302 of the U.S. Emergency Planning and Community Right-to-Know Act (42 U.S.C. 11002), and is subject to strict reporting requirements by facilities which produce, store, or use it in significant quantities.\n\nTrade names and synonyms include Chlorinated camphene, Octachlorocamphene, Camphochlor, Agricide Maggot Killer, Alltex, Allotox, Crestoxo, Compound 3956, Estonox, Fasco-Terpene, Geniphene, Hercules 3956, M5055, Melipax, Motox, Penphene, Phenacide, Phenatox, Strobane-T, Toxadust, Toxakil, Vertac 90%, Toxon 63, Attac, Anatox, Royal Brand Bean Tox 82, Cotton Tox MP82, Security Tox-Sol-6, Security Tox-MP cotton spray, Security Motox 63 cotton spray, Agro-Chem Brand Torbidan 28, and Dr Roger's TOXENE.\n\n"}
{"id": "653173", "url": "https://en.wikipedia.org/wiki?curid=653173", "title": "Wallet", "text": "Wallet\n\nA wallet is a small, flat case that can be used to carry such personal items as cash, credit cards, and identification documents (driver's license, identification card, club card, etc.), photographs, transit pass, gift cards, business cards and other paper or laminated cards. Wallets are generally made of leather or fabrics, and they are usually pocket-sized but not always foldable.\n\nThe word “wallet” has been in use since the late 14th century to refer to a bag or a knapsack for carrying articles (see for example knapzak in Dutch and Frisian), indeed its early usage by Shakespeare described something that we would recognise as more like a backpack today. The word may derive from Proto-Germanic. The ancient Greek word \"kibisis\", said to describe the sack carried by the god Hermes and the sack in which the mythical hero Perseus carried the severed head of the monster Medusa, has been typically translated as \"wallet\". Usage of the term \"wallet\" in its modern meaning of \"flat case for carrying paper currency\" in American English dates to 1834 but this meaning was one of many in the 19th century and early 20th century.\n\nThe classicist A. Y. Campbell set out to answer the question, \"What...in ancient literature, are the uses of a wallet?\" He deduced, as a Theocritean scholar, that \"the wallet was the poor man's portable larder; or, poverty apart, it was a thing that you stocked with provisions.\" He found that sometimes a man may be eating out of it directly but the most characteristic references allude to its being \"replenished as a store\", not in the manner of a lunch basket but more as a survival pack.\n\nWallets were developed after the introduction of paper currency to the West in the 1600s. (The first paper currency was introduced in the New World by the Massachusetts Bay Colony in 1690.) Prior to the introduction of paper currency, coin purses (usually simple drawstring leather pouches) were used for storing coins. Early wallets were made primarily of cow or horse leather and included a small pouch for printed calling cards. \n\nIn recounting the life of the Elizabethan merchant, John Frampton, Lawrence C. Wroth describes the merchant as, \"a young English-man of twenty-five years, decently dressed, ..., wearing a sword, and carrying fixed to his belt something he called a 'bowgett' (or budget), that is, a leathern pouch or wallet in which he carried his cash, his book of accounts, and small articles of daily necessity\".\n\nIn addition to money or currency, a wallet would also be used for carrying dried meat, victuals, \"treasures\", and \"things not to be exposed\". Wallets originally were used by early Industrial Americans. It was considered \"semi-civilized\" in 19th century America to carry one's wallet on one's belt. At this time, carrying goods or a wallet in one's pocket was considered uncivilized and uncommon.\n\nIn Spain, a wallet was a case for smoking paraphernalia: \"Every man would carry a small sheaf of white paper in addition to a small leather wallet which would contain a flint and steel along with a small quantity of so-called \"yesca\", being a dried vegetable fibre which a spark would instantly ignite.\"\n\nThe modern bi-fold wallet with multiple \"card slots\" became standardized in the early 1950s with the introduction of the first credit cards. Some innovations include the introduction of the velcro-closure wallet in the 1970s. Pocket-sized wallets remain popular to this day.\n\nFor cryptocurrencies that only exist in cyberspace as entries in some online ledger a \"cryptocurrency wallet\" is a computing tool whose purpose is to securely keep the owners secret key, authenticate the owner, and let the owner sign transactions securely. A \"hardware wallet\" is a single purpose computer to do this even more safely.\n\nWallets are usually designed to hold banknotes and credit cards and fit into a pocket (or handbag). Small cases for securing banknotes which do not have space for credit cards or identification cards may be classified as money clips: this may also be used to describe small cases designed to hold ISO/IEC 7810 cards alone.\n\n\nMost major designers offer seasonal and perennial wallet collections of black and brown leather. Major retailers also sell a wide selection of men's wallets, including branded and house-name wallets.\n\nThe traditional material for wallets is leather or fabric, but many other flexible flat sheet materials can be used in their fabrication. Non-woven textiles such as Tyvek are used, sometimes including reuse of waterproof maps printed on that material. Woven metals, such as fine mesh made of copper or stainless steel have been incorporated into wallets that are promoted as having electromagnetic shielding properties to protect against unauthorized scanning of embedded NFC & RFID tags. Do-it-yourself websites such as Instructables feature many projects for making wallets out of materials such as denim, Kevlar, or duct tape.\n\nSome wallets, particularly in Europe where larger value coins are prevalent, contain a coin purse compartment. Some wallets have built-in clasps or bands to keep them closed. As European banknotes, such as Euros and Pounds, are typically larger than American banknotes in size, they do not fit in some smaller American wallets.\n\n"}
{"id": "12314226", "url": "https://en.wikipedia.org/wiki?curid=12314226", "title": "Webster's reagent", "text": "Webster's reagent\n\nWebster's reagent is a solution of sodium hydroxide and ethyl alcohol used to test surfaces for the presence of TNT and Tetryl explosive compounds. Specifically Webster's reagent responds to the presence of the trinitrophenyl group. Although it is normally used in testing for explosives, presence of other nitrated organic ring compounds such as benzenes and naphthalenes will also give positive Webster reactions.\n\nWebster's reagent stock solution is made by saturating absolute ethyl alcohol with pellets of sodium hydroxide. The stock solution is then diluted with a 9:1 ratio of absolute ethyl alcohol.\n\n"}
{"id": "21503105", "url": "https://en.wikipedia.org/wiki?curid=21503105", "title": "WikID", "text": "WikID\n\nWikID is a semantic industrial design engineering reference wiki, originally started in 2008 by the Faculty of Industrial Design Engineering at the Delft University of Technology. WikID is a design tool, offering information in a compact manner tailored to its user group, being the Industrial Designers.\nIn WikID the domain of Industrial Design Engineering may be browsed from three angles: Design Methods, Design Aspects, and Product Domains.\n\nThe category Design Methods include design theories, design methods and design techniques. These techniques are for example creativity techniques or techniques to create the design goal or techniques to evaluate product features in a product design.\n\nThe category Design Aspects include the aspects usually found in lists of requirements for product designs. These aspects are (not limited to) ergonomics, production techniques, aesthetics, product safety, sustainability, energy techniques, costs, materials, logistics, marketing, interaction, quality.\n\nA Product Domain is the domain in which the to be designed product will be used. This may be an office environment, kitchen environment, a medical domain, and more.\n\nVroom, R.W., Van 't Ende, J.J., Jelierse, R., Olieman, A.M., and Kooijman, A. (2009)\nCreating a Community Base for WikID; an Industrial Design Engineering Wiki.\nIn: J. Malins (Ed.), \"Design Connexity : Proceedings of the Eighth International Conference of the European Academy of Design\" (pp. 464–469). (AED 2009), Aberdeen: Gray's School of Art, The Robert Gordon University.\n\n"}
{"id": "7442709", "url": "https://en.wikipedia.org/wiki?curid=7442709", "title": "William V. Cruess Award", "text": "William V. Cruess Award\n\nThe William V. Cruess Award has been awarded every year since 1970. It is awarded for excellence in teaching in food science and technology and is the only award in which student members in the Institute of Food Technologists (IFT) can nominate. This award is named after William V. Cruess (1886-1968), a food science professor at the University of California, Berkeley and later at the University of California, Davis who was also the first ever IFT Award winner when he won the Nicholas Appert Award in 1942.\n\nAward winners receive a bronze medal showing a side view of Cruess from the Northern California Section of IFT and a USD 3000 honorarium from the IFT office in Chicago, Illinois.\n\n"}
{"id": "3409888", "url": "https://en.wikipedia.org/wiki?curid=3409888", "title": "YIG sphere", "text": "YIG sphere\n\nYttrium iron garnet spheres (YIG spheres) serve as magnetically tunable filters and resonators for microwave frequencies. These filters are used for their high Q factors, typically between 100 and 200. A sphere made from a single crystal of synthetic yttrium iron garnet acts as a resonator. These spheres are on the order of 0.5 mm in diameter and are manufactured from slightly larger cubes of diced material by tumbling, as is done in the manufacture of jewelry. The garnet is mounted on a ceramic rod, and a pair of small loops around the sphere couple fields into and out of the sphere; the loops are half-turns, positioned at right-angles to each other to prevent direct electromagnetic coupling between them and each is grounded at one end.\n\nThe field from an electromagnet changes the resonance frequency of the sphere and hence the frequency it will allow to pass. The advantage of this type of filter is that the garnet can be tuned over a very wide frequency range by varying the strength of the magnetic field. Some filters can be tuned from 3 GHz up to 50 GHz. YIG filters usually consist of several coupled stages, each stage consisting of a sphere and a pair of loops.\n\nThe input and output coils are oriented at right angles to one another around the YIG crystal. They are cross-coupled when energized by the ferrimagnetic resonance frequency, which depends on the external magnetic field supplied by an electromagnet.\n\nYIG filters are often used as preselectors. YIG filters tuned by a sweep current are used in spectrum analyzers.\n\nAnother YIG application is YIG oscillators, where the sphere acts as a tunable frequency-determining element. It is coupled to an amplifier which provides the required feedback for oscillation.\n\n\n"}
{"id": "19337973", "url": "https://en.wikipedia.org/wiki?curid=19337973", "title": "Z-drag", "text": "Z-drag\n\nA Z-Drag or Z-Rig is an arrangement of lines and pulleys commonly used in rescue situations. The basic arrangement provides a theoretical mechanical advantage of \"three\". The name comes from the fact that the arrangement of lines is roughly Z shaped. Besides the mechanical advantage to pulling, it also uses only part of the total length of the rope for the block and tackle arrangement.\n\nThe typical configuration (see diagram) uses two single pulleys and two Prusik knot loops or other suitable friction hitches. These Prusiks provide fixed attach-points on the rope that can be moved when slightly loosened. The first Prusik knot provides the mechanical advantage. The second Prusik knot can be used to hold the position of the rope and is sometimes referred to as a 'progress capture device' or ratchet. It is also advisable to attach a towel or soft object (such as a life vest) to the end of the line near the connection to the object being pulled since the line is under high tension and could break free and present a dangerous flying hazard.\n\nBorrowed from rock climbing, the Z-Drag is considered an important tool in whitewater rescue and is used primarily for the recovery of pinned boats.\n\nThe Z-Drag is considered an important tool in mountain rescue because of its simplicity and is used for nearly all lifting systems.\n\nIt also serves as an excellent method for tightening the rope to be crossed in a Tyrolean traverse, where the other end is also fixed to a stable object.\n\n\n"}
