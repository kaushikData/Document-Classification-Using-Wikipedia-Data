{"id": "54099280", "url": "https://en.wikipedia.org/wiki?curid=54099280", "title": "2GIS", "text": "2GIS\n\n2GIS is a Russian local search company that develops digital maps and guides of cities in Russia, Kazakhstan, Italy, Czech Republic, Chile, The UAE, Kyrgyzstan, Cyprus and Ukraine. Their headquarters are located in Novosibirsk.\n\nAs of 2018, according to company data, 2GIS digital maps covered 350 cities in operating countries, processing more than 2,2 million search queries daily. Also company claims a global audience of more than 40 million monthly users. According to Forbes, 2GIS is one of the largest internet-based companies in Russia.\n\nAll versions of the application are free of charge, and the company's main source of income is advertising on their website. Digital maps come in three versions: PC, Web and Mobile, including iOS and Android platforms.\n\nIn 2013, the company earned 3 billion rubles. Mashable included 2GIS in its list of Russia's “20 hottest startups\".\n\nIn 2015, company raised $40 million from two major investment funds, Baring Vostok Capital Partners and Ru-Net, to expand its presence in the European part of Russia, to extend a foreign franchises network, as well as to develop new products, stated Vera Garmash, 2GIS President.\n\nIn 2016, the Wall Street Journal named 2GIS as a recognized Russian startup targeting global markets amid Russian economic crisis and Western sanctions.\n\nIn 2018, Government of Dubai and 2GIS signed an agreement to improve maps and navigation in Dubai Now — application allowing the city's residents to access government services.\n"}
{"id": "1009286", "url": "https://en.wikipedia.org/wiki?curid=1009286", "title": "Actinometer", "text": "Actinometer\n\nActinometers are instruments used to measure the heating power of radiation. They are used in meteorology to measure solar radiation as pyranometers, pyrheliometers and net radiometers.\n\nAn actinometer is a chemical system or physical device which determines the number of\nphotons in a beam integrally or per unit time. This name is commonly\napplied to devices used in the ultraviolet and visible wavelength ranges.\nFor example, solutions of iron(III) oxalate can be used as a chemical\nactinometer, while bolometers, thermopiles, and photodiodes are physical\ndevices giving a reading that can be correlated to the number of photons\ndetected.\n\nThe actinometer was invented by John Herschel in 1825; he introduced the term \"actinometer\", the first of many uses of the prefix \"actin\" for scientific instruments, effects, and processes.\n\nThe actinograph is a related device for estimating the actinic power of lighting for photography.\n\nChemical actinometry involves measuring radiant flux via the yield from a chemical reaction. It requires a chemical with a known quantum yield and easily analyzed reaction products.\n\nPotassium ferrioxalate is commonly used, as it is simple to use and sensitive over a wide range of relevant wavelengths (254 nm to 500 nm). Other actinometers include malachite green leucocyanides, vanadium(V)–iron(III) oxalate and monochloroacetic acid, however all of these undergo dark reactions, that is, they react in the absence of light. This is undesirable since it will have to be corrected for. Organic actinometers like butyrophenone or piperylene are analysed by gas chromatography. Other actinometers are more specific in terms of the range of wavelengths at which quantum yields have been determined. Reinecke’s salt K[Cr(NH)(NCS)] reacts in the near-UV region although it is thermally unstable. \n\nRecent investigations into nitrate photolysis\nhave used 2-nitrobenzaldehyde and benzoic acid as a radical scavenger for hydroxyl radicals produced in the photolysis of hydrogen peroxide and sodium nitrate. However, they originally used ferrioxalate actinometry to calibrate the quantum yields for the hydrogen peroxide photolysis. Radical scavengers proved a viable method of measuring production of hydroxyl radical.\n\nMeso-diphenylhelianthrene can be used for chemical actinometry in the visible range (400–700 nm). This chemical measures in the 475–610 nm range, but measurements in wider spectral ranges can be done with this chemical if the emission spectrum of the light source is known.\n"}
{"id": "20605998", "url": "https://en.wikipedia.org/wiki?curid=20605998", "title": "ActiveReports", "text": "ActiveReports\n\nActiveReports is a .NET reporting tool used by developers of WinForms, ASP.NET, and HTML5 applications. It was originally developed by Data Dynamics, which was then acquired by GrapeCity. ActiveReports is a set of components and tools that facilitates the production of reports to display data in documents and web-based formats. It is written in managed C# code and allows Visual Studio programmers to leverage their knowledge of C# or Visual Basic.NET when programming with ActiveReports.\n\nAmong the components included with ActiveReports are exports to file formats such as PDF, Excel, RTF, and TIFF. The main components are a Visual Studio integrated report designer, and an API that developers use to create customized reports from a variety of data sources. ActiveReports Standard Edition also includes a Visual Query Designer, a customizable Windows Viewer control, an HTML5 Viewer control, and a WPF Viewer control.\n\nThe integrated report designer handles three types of reports:\n\n\nThe Professional Edition of ActiveReports includes the Standard Edition tools plus an End-User Report Designer control that developers use to host the report designer in their own Microsoft Windows applications to let end users create and modify reports. It also includes a server-side ASP.NET web viewer with Flash, PDF, and HTML viewer types; ASP.NET HTTP Handlers that export reports to HTML or PDF format without custom code; a Silverlight viewer; and advanced PDF encryption and rendering features.\n\nActiveReports Server is a scalable server platform with built in load balancing. It provides a secure and extensible report server to publish reports designed in ActiveReports. A comprehensive RESTful API enables integration of the responsive HTML5 Report Portal for end users to view, schedule and export reports.\n\n\n\n\n\n\n\nLatest Service Releases\n\nStandard Edition\nalso support for various things\n\nActiveReports Designer\n\nWindows Forms Viewer\n\nReporting Engine\n\nIncludes all of the Standard Edition features, and adds the following:.\n\nEnd-User Report Designer\n\nASP.NET\n\n\n\nMay 15, 2015 - SD Times 100 GrapeCity wins Best In Show in the User Experience category\n\nFebruary 27, 2015 - Ranked #6 in the ComponentSource Bestselling Product Awards for 2014-2015\n\nMay 15, 2014 - SD Times 100 GrapeCity (dba ComponentOne) wins Best In Show in the User Experience category\n\nFebruary 28, 2014 - Ranked #5 in the ComponentSource Bestselling Product Awards for 2013-2014\n\nOctober 15, 2013 - Other Hot Products recognition in Best Microsoft Windows Development Printing/Reporting Tool of 2013\n\nMay 15, 2013 - SD Times 100 GrapeCity wins Best In Show in the User Experience category\n\nMay 15, 2013 - SD Times 100 GrapeCity wins Best In Show in the APIs, Libraries & Frameworks category\n\nFebruary 27, 2013 - Ranked #4 in the ComponentSource Bestselling Product Awards for 2012-2013\n\nMay 15, 2012 - SD Times 100 GrapeCity wins Best In Show in the Libraries & Frameworks category\n\nApril 30, 2012 - Ranked #11 in the ComponentSource Bestselling Product Awards for 2011-2012\n\nMay 15, 2011 - SD Times 100 GrapeCity wins Best In Show in the Components category\n\nFebruary 28, 2011 - Ranked #6 in the ComponentSource Bestselling Product Awards for 2010-2011\n\nNovember 29, 2010 - Finalist in the 2010 Best of Connections Awards: Visual Studio Developer Products\n\nMay 15, 2010 - SD Times 100 GrapeCity wins Best In Show in the Components & Libraries category\n\nMarch 26, 2010 - Ranked #7 in the ComponentSource Bestselling Product Awards for 2009-2010\n\nMay 7, 2009 - Finalist in the 2009 asp.netPRO Readers' Choice Awards\n\nMarch 13, 2009 - Ranked #4 and #5 in the 2008 ComponentSource Bestselling Publishers List\n\nMarch 13, 2009 - Ranked #2 in the 2008 ComponentSource Bestselling Products List\n\nFebruary 9, 2009 - ActiveReports Customers Give .NET Reporting and BI Tools High Ratings\nJune 2, 2008 - SD Times 100 Finalist in the Components category\n\nMay 15, 2007 - SD Times 100 Finalist in the Components category\n\nFebruary 9, 2007 - 2006 ComponentSource Bestselling Publisher\n\nFebruary 9, 2007 - 2006 ComponentSource Bestselling Product\n\nMay 15, 2006 - SD Times 100 Winner in the Components category\n\nIn the past, ActiveReports was known to be unable to handle large reports. This issue was ongoing across years and versions. Since that time, development efforts have focused on improving large report handling in every release.\n\nActiveReports can be used in many ways, so each project can have a number of reasons for consuming memory. In newer versions, CacheToDisk and CacheToDiskLocation properties were added for PDF exports. Some other considerations that may cause too much memory use in section reports include:\n\n\n"}
{"id": "23715864", "url": "https://en.wikipedia.org/wiki?curid=23715864", "title": "Active Bat", "text": "Active Bat\n\nActive Bat is a low-power, wireless indoor location system accurate up to 3 cm. It is based on a principle of trilateration, and relies on multiple ultrasonic receivers embedded in the ceiling and measures time-of-flight to them.\n\nActive Bat is an indoor localization system which gives an accuracy within centimeters. This is a range based technique, which works by finding the distance to minimum of three reference nodes and then using multilateration technique to find the exact position. The distance to reference nodes is find out using the time of arrival of ultrasonic signals from the reference nodes. \n\n\n"}
{"id": "51793263", "url": "https://en.wikipedia.org/wiki?curid=51793263", "title": "Air-jet loom", "text": "Air-jet loom\n\nAn air-jet loom is a shuttleless loom that uses a jet of air to propel the weft yarn through the warp shed. It is one of two types of fluid-jet looms, the other being a water-jet loom, which was developed previously. Fluid-jet looms can operate at a faster speed than predecessor looms such as rapier looms, but they are not as common. The machinery used in airjet weaving consists of a main nozzle, auxiliary nozzles or relay nozzles, and a profile reed.\n\nAir-jet looms are capable of producing standard household and apparel fabrics for items such as shirts, denim, sheets, towels, and sports apparel, as well as industrial products such as printed circuit board cloths. Heavier yarns are more suitable for air-jet looms than lighter yarns. Air-jet looms are capable of weaving plaids, as well as dobby and jacquard fabrics.\n\nCompanies that produce air-jet looms include Toyota Industries and Tsudakoma, both based in Japan; Picanol, based in Belgium; Dornier, based in Germany; RIFA, based in China; and Itema, based in Italy.\n\nIn an air-jet loom, yarn is pulled from the supply package, and the measuring disc removes a length of yarn of the width of fabric being woven. A clamp holds the yarn and an auxiliary air nozzle forms it into the shape of a hairpin. The main nozzle blows the yarn, the clamp opens, and the yarn is carried through the shed. At the end of the insertion cycle, the clamp closes, the yarn is beaten in and cut, and the shed is closed.\n\nResearch has been done to analyze factors that contribute to compressed air use, a major source of energy consumption, in air-jet looms.\n\nThe air-jet loom was invented in Czechoslovakia in the 20th century and was later refined by Swiss, Dutch, and Japanese companies.\n"}
{"id": "173407", "url": "https://en.wikipedia.org/wiki?curid=173407", "title": "Analog sampled filter", "text": "Analog sampled filter\n\nAn analog sampled filter an electronic filter that is a hybrid between an analog and a digital filter. The input signal is analog, and usually stored in capacitors. The time domain is discrete, however. Distinct analog samples are shifted through an array of holding capacitors as in a bucket brigade. Analog adders and amplifiers do the arithmetic in the signal domain, just as in an analog computer.\n\nNote that these filters are subject to aliasing phenomena just like a digital filter, and anti-aliasing filters will usually be required. See: Filter design\n\nCompanies such as Linear Technology and Maxim produce integrated circuits that implement this functionality. Filters up to the 8th order may be implemented using a single chip. Some are fully configurable; some are pre-configured, usually as low-pass filters.\n\nDue to the high filter order that can be achieved in an easy and stable manner, single chip analog sampled filters are often used for implementing anti-aliasing filters for digital filters. The analog sampled filter will in its turn need yet another anti-aliasing filter, but this can often be implemented as a simple 1st order low-pass analog filter consisting of one series resistor and one capacitor to ground.\n"}
{"id": "6368297", "url": "https://en.wikipedia.org/wiki?curid=6368297", "title": "Binary explosive", "text": "Binary explosive\n\nA binary explosive or two-component explosive is an explosive consisting of two components, neither of which is explosive by itself, which have to be mixed in order to become explosive. Examples of common binary explosives include Oxyliquit (liquid oxygen/combustible powder), ANFO (ammonium nitrate/fuel oil), Kinestik (ammonium nitrate/nitromethane), Tannerite and ammonal (ammonium nitrate/aluminum), and FIXOR (nitroethane/physical sensitizer).\n\nBinary explosives are often used in commercial applications because of their greater handling safety. \n\nIn the United States, ATF regulations allow the components of some binary explosives to be legally purchased, when neither one is an explosive by itself. ATF advises: \"Persons manufacturing explosives for their own personal, non-business use only (e.g., personal target practice) are not required to have a Federal explosives license or permit.\" A prohibited person (a person barred by federal law from buying or owning a firearm) cannot legally possess mixed explosives. Explosives for lawful target practice must be used once mixed: any transport, storage or commercial use of mixed explosives falls under federal explosives laws, and cannot be transported in mixed form without following strict regulations including insurance, packaging, signage on the transport vehicle, storage magazines, etc.\n\nVarious regulations also govern the storage of unmixed explosives. As oxidizers and combustibles, the unmixed components still have some shipping restrictions in the United States.\n\nA Maryland law intended specifically to ban the sale or ownership of consumer products containing binary explosive components (such as Tannerite brand rifle targets) became effective on October 1, 2012, and expanded the definition of an explosive to include, in addition to \"bombs and destructive devices designed to operate by chemical, mechanical, or explosive action\", \"two or more components that are advertised and sold together with instructions on how to combine the components to create an explosive\".\n\nOn August 5, 2013, the United States Forest Service (USFS) and the U.S. Attorney's office in Denver announced that the USFS is implementing a closure order to prohibit the use of unpermitted explosives, particularly exploding targets, on all USFS lands in the Rocky Mountain Region. This region includes national forests and grasslands in the states of Colorado, Wyoming, Kansas, Nebraska, and South Dakota. According to the USFS, at least 16 wildfires in the Western states had been associated with exploding targets. It cost more than $33 million to extinguish the fires. Such a ban has already been implemented by the USFS in Washington, Oregon and Montana. The Bureau of Land Management has banned the use of all exploding targets on BLM land in Utah.\n\n\n"}
{"id": "20207968", "url": "https://en.wikipedia.org/wiki?curid=20207968", "title": "BodyKom", "text": "BodyKom\n\nBodyKom is a mobile heart monitoring service that enable care personnel to receive the ECG of heart patients via the mobile network. The patients can be mobile and perform their everyday activities yet remain under observation. The caregiver receives diagnosis data immediately when the patient’s heart starts acting abnormally. Through a backend system the clinician can set patient individual limits to support finding the abnormal heart functionality for the patient. Measures can be initiated automatically, for example by notifying the clinician and informing relatives.\n\nThe use of BodyKom does not require any change in the care routines at the hospital, other than the patient getting a new type of patient kit which includes ECG electrodes, a small portable sensor and a smart cell phone. Monitoring data is collected by the sensor and is wirelessly transferred to the cell phone. The ECG data is then transferred via the mobile network to a decision support system for adjustment and then forwarded to the caregivers system for analysis. From a medical and technical point of view the patient is connected in the same way as with conventional ECG methods, like using a Holter monitor.\n\nBodyKom was developed by Kiwok in cooperation with Karolinska University Hospital in Sweden.\n\n"}
{"id": "191574", "url": "https://en.wikipedia.org/wiki?curid=191574", "title": "Boot", "text": "Boot\n\nA boot is a type of footwear and not a specific type of shoe. Most boots mainly cover the foot and the ankle, while some also cover some part of the lower calf. Some boots extend up the leg, sometimes as far as the knee or even the hip. Most boots have a heel that is clearly distinguishable from the rest of the sole, even if the two are made of one piece. Traditionally made of leather or rubber, modern boots are made from a variety of materials. Boots are worn both for their functionality – protecting the foot and leg from water, extreme cold, mud or hazards (e.g., work boots may protect wearers from chemicals or use a steel toe) or providing additional ankle support for strenuous activities with added traction requirements (e.g., hiking), or may have hobnails on their undersides to protect against wear and to get better grip; and for reasons of style and fashion.\n\nIn some cases, the wearing of boots may be required by laws or regulations, such as the regulations in some jurisdictions requiring workers on construction sites to wear steel-toed safety boots. Some uniforms include boots as the regulated footwear. Boots are recommended as well for motorcycle riders. High-top athletic shoes are generally not considered boots, even though they do cover the ankle, primarily due to the absence of a distinct heel. In Britain, the term may be used to refer to football (soccer) cleats.\n\nEarly boots consisted of separate leggings, soles, and uppers worn together to provide greater ankle protection than shoes or sandals. Around 1000 BC, these components were more permanently joined to form a single unit that covered the feet and lower leg, often up to the knee. A type of soft leather ankle boots were worn by nomads in eastern Asia and carried to China to India and Russia around AD 1200 to 1500 by Mongol invaders. The Inuit and Aleut natives of Alaska developed traditional winter boots of caribou skin or sealskin featuring decorative touches of seal intestine, dog hair and suchlike. The early Dutch Masters were the first to define the boot in European iconography, in spite of the fact that the Chinese had been using footwear that the average Frenchman or Portuguese sailor of the day would have recognized as a boot for centuries at that time. Most historians agree, though, that the first codified definition of the boot was entered into law by Royal decree during the Hundred Years' War, when the Duke of Wales wrote, \"that sturdy, stiff shyue off a type ne'er seent heretofore wi' high scuppers and ye nailes on the souyle.\" Sporadic wars were fought among city states during this time as the Protestants rejected that definition, but history vindicated the Duke eventually, and the Roche family of Nantucket actually rose to prominence more as a result if their trade in these boots in the colonies than from their whaling endeavors. European boots were influenced by military styles, featuring thick soles and turnover tops that were originally designed to protect horse mounted soldiers. In the 1700s, distinctive, thigh-high boots worn by Hessian soldiers fighting in the American Revolutionary War influenced the development of the iconic heeled cowboy boots worn by cattlemen in the American west.\n\nBoots which are designed for walking through snow, shallow water and mud may be made of a single closely stitched design (using leather, rubber, canvas, or similar material) to prevent the entry of water, snow, mud or dirt through gaps between the laces and tongue found in other types of shoes. Waterproof gumboots are made in different lengths of uppers. In extreme cases, thigh-boots called waders, worn by anglers, extend to the hip. Such boots may also be insulated for warmth. With the exception of gum boots, boots sold in general retail stores may be considered \"water resistant,\" as they are not usually fully waterproof, compared to high-end boots for fishers or hikers.\nSpeciality boots have been made to protect steelworkers' feet and calves if they accidentally step in puddles of molten metal, to protect workers from a variety of chemical exposure, to protect workers from construction site hazards and to protect feet from extreme cold (e.g., with insulated or inflatable boots for use in Antarctica). Most work boots are \"laceups\" made from leather. Formerly they were usually shod with hobnails and heel- and toe-plates, but now can usually be seen with a thick rubber sole, and often with steel toecaps.\n\nBoots are normally worn with socks to prevent chafes and blisters, to absorb sweat, to improve the foot's grip inside the boot, or to insulate the foot from the cold. Before socks became widely available, footwraps were worn instead.\n\nSpecialty boots have been designed for many different types of sports, particularly riding, skiing, snowboarding, ice-skating, and sporting in wet/damp conditions.\n\nBovver boots, Doc Martens boots and army boots were adopted by skinheads and punks as part of their typical dress and have migrated to more mainstream fashion, including women's wear. As a more rugged alternative to dress shoes, dress boots may be worn (though these can be more formal than shoes). Fashionable boots for women may exhibit all the variations seen in other fashion footwear: tapered or spike heels, platform soles, pointed toes, zipper closures and the like. The popularity of boots as fashion footwear ebbs and flows. Singer Nancy Sinatra popularized the fad of women wearing boots in the late 1960s with her song \"These boots are made for walking\". They were popular in the 1960s and 1970s (particularly knee-high boots), but diminished in popularity towards the end of the 20th century. In the 2010s, they are experiencing a resurgence in popularity, especially designs with a long bootleg. Boot bolos, boot bracelets, boot straps, boot chains, and boot harnesses are used to decorate boots. Sandal boots also exist.\n\nBoots have become the object of sexual attraction for some people and they have become a standard accessory in the BDSM scene (where leather, latex and PVC boots are favoured) and a fashion accessory in music videos. Knee- or thigh-high leather boots are worn by some strippers and pornography models and actresses. Boots have even become a sexual fetish for devotees known as boot fetishists and foot fetishists.\n\n\nAs boots have been used by riders for millennia, they were used by knights. As a consequence, albeit not common, boots came to be used as charges in heraldry.\n\nBecause of the origin of heraldry as insignia used by mounted warriors like the medieval knights, when boots are used in heraldry, they are often displayed as riding boots, even if the blazon might not specify it as such. They are sometimes adorned with spurs, which may or may not have another tincture (colour) than the boot and the background field.\n\nBoots were also used in coats of arms of shoemakers' guilds and in shop signs outside their shops.\n\n\n"}
{"id": "4672384", "url": "https://en.wikipedia.org/wiki?curid=4672384", "title": "Brightstar Corporation", "text": "Brightstar Corporation\n\nBrightstar Corp., a U.S.-based privately held corporation founded in 1997, distributes mobile phones and other devices, serving more than 200 carriers, 40,000+ retailers, and 15,000+ enterprise customers in more than 100 countries. It provides specialized global wireless distribution and services, serving mobile device manufacturers, wireless operators and retailers. Brightstar offers value-added device and accessories distribution, supply chain, handset protection and insurance, buyback and trade-in and omnichannel retail, and mobile digital products within the wireless telecommunications industry.\n\nIn July 2014, Brightstar Corp. and Bharti Enterprises announced that they entered into an agreement to provide various mobile technologies and services to the Indian market. As part of the agreement, a Brightstar subsidiary acquired a majority interest in Beetel Teletech Limited, a leading company in the distribution and sale of communication and media devices, enterprise and IT products in India.\n\nIn February 2014, Brightstar Corp. completed its acquisition of 20:20 Mobile, a leading provider of distribution and integrated supply chain to the European mobile industry. Brightstar acquired facilities in the UK, Spain, Hungary, Portugal, Denmark, Finland, Norway, and Sweden, and provide products and services across 13 European countries.\n\nIn October 2013, Japanese SoftBank paid $1.26 billion for a 57% stake in Brightstar. Over the next 5 years, or upon certain events, SoftBank's ownership will accrete to 70%.\n\nThe company's headquarters is located in suburban unincorporated Miami-Dade County, Florida in Miami, Florida.\n\nBrightstar has operations centers and/or sales offices in four geographic regions:\n\n\n\n"}
{"id": "1369226", "url": "https://en.wikipedia.org/wiki?curid=1369226", "title": "Brix", "text": "Brix\n\nDegrees Brix (symbol °Bx) is the sugar content of an aqueous solution.\nOne degree Brix is 1 gram of sucrose in 100 grams of solution and represents the strength of the solution as percentage by mass. If the solution contains dissolved solids other than pure sucrose, then the °Bx only approximates the dissolved solid content. The °Bx is traditionally used in the wine, sugar, carbonated beverage, fruit juice, maple syrup and honey industries.\n\nComparable scales for indicating sucrose content are the degree Plato (°P), which is widely used by the brewing industry, and the degree Balling, which is the oldest of the three systems and therefore mostly found in older textbooks, but also still in use in some parts of the world.\n\nA sucrose solution with an apparent specific gravity (20°/20 °C) of 1.040 would be 9.99325 °Bx or 9.99359 °P while the representative sugar body, the International Commission for Uniform Methods of Sugar Analysis (ICUMSA), which favors the use of mass fraction, would report the solution strength as 9.99249%. Because the differences between the systems are of little practical significance (the differences are less than the precision of most common instruments) and wide historical use of the Brix unit, modern instruments calculate mass fraction using ICUMSA official formulas but report the result as °Bx.\n\nIn the early 1800s, Karl Balling, followed by Adolf Brix, and finally the \"Normal-Commissions\" under Fritz Plato, prepared pure sucrose solutions of known strength, measured their specific gravities and prepared tables of percent sucrose by mass vs. measured specific gravity. Balling measured specific gravity to 3 decimal places, Brix to 5, and the Normal-Eichungs Kommission to 6 with the goal of the Commission being to correct errors in the 5th and 6th decimal place in the Brix table.\n\nEquipped with one of these tables, a brewer wishing to know how much sugar was in his wort could measure its specific gravity and enter that specific gravity into the Plato table to obtain °Plato, which is the concentration of sucrose by percentage mass. Similarly, a vintner could enter the specific gravity of his must into the Brix table to obtain the °Bx, which is the concentration of sucrose by percent mass. It is important to point out that neither wort nor must is a solution of pure sucrose in pure water. Many other compounds are dissolved as well but these are either sugars, which behave very similarly to sucrose with respect to specific gravity as a function of concentration, or compounds which are present in small amounts (minerals, hop acids in wort, tannins, acids in must). In any case, even if °Bx are not representative of the exact amount of sugar in a must or fruit juice they can be used for comparison of relative sugar content.\n\nAs specific gravity was the basis for the Balling, Brix and Plato tables, dissolved sugar content was originally estimated by measurement of specific gravity using a hydrometer or pycnometer. In modern times, hydrometers are still widely used, but where greater accuracy is required, an electronic oscillating U-tube meter may be employed. Whichever means are used, the analyst enters the tables with specific gravity and takes out (using interpolation if necessary) the sugar content in percent by mass. If the analyst uses the Plato tables (maintained by the American Society of Brewing Chemists) he or she reports in °P. If using the Brix table (the current version of which is maintained by NIST and can be found on their website), he or she reports in °Bx. If using the ICUMSA tables, he or she would report in mass fraction (m.f.). It is not, typically, actually necessary to consult tables as the tabulated °Bx or °P value can be printed directly on the hydrometer scale next to the tabulated value of specific gravity or stored in the memory of the electronic U-tube meter or calculated from polynomial fits to the tabulated data. Both ICUMSA and ASBC have published suitable polynomials; in fact, the ICUMSA tables are calculated from the polynomials. The opposite is true with the ASBC polynomial. Also note that the tables in use today are not those published by Brix or Plato. Those workers measured true specific gravity reference to water at 4 °C using, respectively, 17.5 °C and 20 °C, as the temperature at which the density of a sucrose solution be measured. Both NBS and ASBC converted to apparent specific gravity at 20 °C/20 °C. The ICUMSA tables are based on more recent measurements on sucrose, fructose, glucose and invert sugar, and they tabulate true density and weight in air at 20 °C against mass fraction.\n\nDissolution of sucrose and other sugars in water changes not only its specific gravity but its optical properties in particular its refractive index and the extent to which it rotates the plane of linearly polarized light. The refractive index, \"n\", for sucrose solutions of various percentage by mass has been measured and tables of \"n\" vs. °Bx published. As with the hydrometer, it is possible to use these tables to calibrate a refractometer so that it reads directly in °Bx. Calibration is usually based on the ICUMSA tables, but the user of an electronic refractometer should verify this.\n\nSugars also have known infrared absorption spectra and this has made it possible to develop instruments for measuring sugar concentration using NIR (Near Infra Red) and FT-IR (Fourier Transform Infrared Spectrometry) techniques. In the former case, in-line instruments are available which allow constant monitoring of sugar content in sugar refineries, beverage plants, wineries, etc. As with any other instruments, NIR and FT-IR instruments can be calibrated against pure sucrose solutions and thus report in °Bx, but there are other possibilities with these technologies, as they have the potential to distinguish between sugars and interfering substances.\n\nApproximate values of °Bx can be computed from 231.61 × (S − 0.9977), where S is the apparent specific gravity of the solution at 20 °C/20 °C. More accurate values are available from:\n\n°formula_1, \n\nderived from the NBS table with S as above. This should not be used above S = 1.17874 (40 °Bx). RMS disagreement between the polynomial and the NBS table is 0.0009 °Bx. The Plato scale can be approximated by the Lincoln Equation: \n\n°formula_2 \n\nor values obtained with high accuracy with respect to the ASBC table from the ASBC polynomial:\n\n°formula_3\n\nThe difference between the °Bx and °P as calculated from the respective polynomials is: \n\n°formula_4°formula_5 \n\nThe difference is generally less than ±0.0005 °Bx or °P with the exception being for weak solutions. As 0 °Bx is approached °P tend towards as much as 0.002 °P higher than the °Bx calculated for the same specific gravity. Disagreements of this order of magnitude can be expected as the NBS and the ASBC used slightly different values for the density of air and pure water in their calculations for converting to apparent specific gravity. It should be clear from these comments that Plato and Brix are, for all but the most exacting applications, the same. Note: all polynomials in this article are in a format that can be pasted directly into a spreadsheet.\n\nWhen a refractometer is used, the Brix value can be obtained from the polynomial fit to the ICUMSA table: \n\n°formula_6,\n\nwhere formula_7 is the refractive index measured at the wavelength of the sodium D line (589.3 nm) at 20 °C. Temperature is very important as refractive index changes dramatically with temperature. Many refractometers have built in \"Automatic Temperature Compensation\" (ATC) which is based on knowledge of the way the refractive index of sucrose changes. For example, the refractive index of a sucrose solution of strength less than 10 °Bx is such that a 1 °C change in temperature would cause the Brix reading to shift by about 0.06 °Bx. Beer, conversely, exhibits a change with temperature about three times this much. It is important, therefore, that users of refractometers either make sure the sample and prism of the instrument are both at very close to 20 °C or, if that is difficult to ensure, readings should be taken at 2 temperatures separated by a few degrees, the change per degree noted and the final recorded value referenced to 20 °C using the Bx vs. Temp slope information.\n\nThe four scales are often used interchangeably since the differences are minor.\n\nBrix is used in the food industry for measuring the approximate amount of sugars in fruits, vegetables, juices, wine, soft drinks and in the starch and sugar manufacturing industry. Different countries use the scales in different industries: In brewing, the UK uses specific gravity X 1000; Europe uses Plato degrees; and the US use a mix of specific gravity, degrees Brix, degrees Baumé, and degrees Plato. For fruit juices, 1.0 degree Brix is denoted as 1.0% sugar by mass. This usually correlates well with perceived sweetness.\n\nModern optical Brix meters are divided into two categories. In the first are the Abbe-based instruments in which a drop of the sample solution is placed on a prism; the result is observed through an eyepiece. The critical angle (the angle beyond which light is totally reflected back into the sample) is a function of the refractive index and the operator detects this critical angle by noting where a dark-bright boundary falls on an engraved scale. The scale can be calibrated in Brix or refractive index. Often the prism mount contains a thermometer which can be used to correct to 20 °C in situations where measurement cannot be made at exactly that temperature. These instruments are available in bench and handheld versions.\n\nDigital refractometers also find the critical angle, but the light path is entirely internal to the prism. A drop of sample is placed on its surface, so the critical light beam never penetrates the sample. This makes it easier to read turbid samples. The light/dark boundary, whose position is proportional to the critical angle, is sensed by a CCD array. These meters are also available in bench top (laboratory) and portable (pocket) versions. This ability to easily measure Brix in the field makes it possible to determine ideal harvesting times of fruit and vegetables so that products arrive at the consumers in a perfect state or are ideal for subsequent processing steps such as vinification.\n\nDue to higher accuracy and the ability to couple it with other measuring techniques (%CO2 and %alcohol), most soft drink companies and breweries use an oscillating U-tube density meter. Refractometers are still commonly used for fruit juice.\n\nWhen a sugar solution is measured by refractometer or density meter, the °Bx or °P value obtained by entry into the appropriate table only represents the amount of dry solids dissolved in the sample if the dry solids are exclusively sucrose. This is seldom the case. Grape juice (must), for example, contains little sucrose but does contain glucose, fructose, acids, and other substances. In such cases, the °Bx value clearly cannot be equated with the sucrose content, but it may represent a good approximation to the total sugar content. For example, an 11.0% by mass D-Glucose (\"grape sugar\") solution measured 10.9 °Bx using a hand held instrument. For these reasons, the sugar content of a solution obtained by use of refractometry with the ICUMSA table is often reported as \"Refractometric Dry Substance\" (RDS) which could be thought of as an equivalent sucrose content. Where it is desirable to know the actual dry solids content, empirical correction formulas can be developed based on calibrations with solutions similar to those being tested. For example, in sugar refining, dissolved solids can be accurately estimated from refractive index measurement corrected by an optical rotation (polarization) measurement.\n\nAlcohol has a higher refractive index (1.361) than water (1.333). As a consequence, a refractometer measurement made on a sugar solution once fermentation has begun will result in a reading substantially higher than the actual solids content. Thus, an operator must be certain that the sample they are testing has not begun to ferment. Brix or Plato measurements based on specific gravity are also affected by fermentation, but in the opposite direction; as ethanol is less dense than water, an ethanol/sugar/water solution gives a Brix or Plato reading which is artificially low.\n\n\n"}
{"id": "4505820", "url": "https://en.wikipedia.org/wiki?curid=4505820", "title": "Cloth diaper", "text": "Cloth diaper\n\nA cloth diaper (American English) or a cloth nappy (Australian English and British English) is a reusable diaper made from natural fibers, man-made materials, or a combination of both. They are often made from industrial cotton which may be bleached white or left the fiber’s natural color. Other natural fiber cloth materials include wool, bamboo, and unbleached hemp. Man-made materials such as an internal absorbent layer of microfiber toweling or an external waterproof layer of polyurethane laminate (PUL) may be used. Polyester fabrics microfleece or suedecloth are often used inside cloth diapers as a \"stay-dry\" wicking liner because of the non-absorbent properties of those synthetic fibers.\n\nModern cloth diapers come in a host of shapes, including preformed cloth diapers, all-in-one diapers with waterproof exteriors, fitted diaper with covers and pocket or \"stuffable\" diapers, which consist of a water-resistant outer shell sewn with an opening for insertion of absorbent material inserts. Many design features of modern cloth diapers have followed directly from innovations initially developed in disposable diapers, such as the use of the hour glass shape, materials to separate moisture from skin and the use of double gussets, or an inner elastic band for better fit and containment of waste material.\n\nTraditionally, cloth diapers consisted of a folded square or rectangle of linen cloth, cotton flannel, or stockinet, which was fastened with safety pins. Today, this type of diaper is referred to as a flat. The flat was commonly used in the late 1800s in Europe and North America.\n\nIn the early part of the 20th century, cloth users were boiling diapers as they became aware of bacteria. During World War II, the increase of working mothers brought the need for the \"diaper service\". Fresh cotton diapers would be delivered on an as-needed basis.\n\nIn 1946, a Westport housewife named Marion Donovan, invented the \"Boater\", a waterproof covering for cloth diapers. Marion was granted 4 patents for her designs, including the use of plastic snaps that replaced the traditional and dangerous \"safety pins\".\n\nIn 1950, the prefold diaper was invented by a diaper service owner and produced by Curity. The prefold diaper consisted of a standard \"flat\" diaper, but pre-folded and sewn together. Also in 1950, the Safe-T Di-Dee diaper was invented. The diaper was preformed and was the first pinless, snap-on diaper- this was the first fitted diaper. It was invented by Sybil Geeslin (Kennedy) who subsequently sold the patent. They were then sold as Keystone Safe-T Di-Dee Diapers and were nationally distributed.\n\nIn the 1960s, the disposable diaper rapidly took hold and cloth diaper use fell out of favor. The \"Safe Diaper Clip,\" an alternative to traditional safety pins, was invented and patented in 1961 by Edward Moonan of Boonville, NY. but never took off, due to drop in use of cloth diapers.\n\nIn the late 1980s, cloth diaper users re-emerged with environmental issues concerning the use of disposables. By the late 1990s and the beginning of the next decade, many large cloth diaper manufacturing companies were well established.\n\nIn 1987 Bummis, a Canadian company, invented the modern diaper cover to go over flats and prefolds. Also in 1987, the Snappi diaper fastener was invented in South Africa to be used with prefolds or flats and is still widely used today.\n\nIn 1995, the Motherease company was formed and began selling by mail order in the US, although the owner (Erika Froese) had been developing and selling her diapers since 1991 (mostly in Canada) and using cloth since 1981. Motherease was .\n\nCatherine McDiarmid's BornToLove.com, a Canadian site, was one of the first major diaper websites to emerge in 1997 as the web took off.\nIn 1999, Poochies and HoneyBoy! diapers were developed and , some selling for over $200.00 per diaper. Cuddlebuns Cloth Diapers were introduced to the diapering community in 1999 and were a one-size diaper that could fit babies from infancy to potty training with adjustable sizing. This is known today as a one-size diaper. All three of these diapers brand patterns eventually became available for the general public to sew through cottage licensing, and are now owned by BabyByYou! Also in 1999, Kissaluvs started an online business to sell fitted and contour diapers. These years also saw a tremendous amount of growth in the amount of moms who begin sewing their own diapers, referred to as \"Work at Home Moms\" or WAHMs.\n\nThe UK market was slower to get started than the USA, with a few domestic manufacturers such as Earthwise and Snugglenaps being established in the late 1990s, alongside importers of brands such as Motherease, Bummis and Kooshies. The first multi-brand ecommerce online shop was opened by Twinkleontheweb, with others following on. The Real Nappy Association was formed by members of WEN the Women's Environmental Network, with the first Real Nappy Week being held in 1997 to promote the use of cloth diapers in the UK.\n\nIn 2000, Fuzzi Bunz opened an online store selling the first pocket diaper. Originally, they were fleece on the outside and the inside. Later they were made with PUL (a waterproof laminated polyester fabric used in virtually all Cloth diaper covers today) outers and fleece inners. Both looks were different from what you see today. Other companies quickly followed suit, included TotsBots and Nature Babies in the UK, Baby Kicks, and Stacinator fleece diaper covers.\n\nThe phrase \"hyena\" was also coined around this time, or shortly thereafter, when Kendell from Freshies commented on how all the diapering mamas were so ruthless in their stalking of hard-to-get diapers. She said, in a joking way, that \"they're just like a pack of hyenas, closing in for the kill\" and the term stuck. So now hyena diapers are the hard to get, but much sought after diaper brands available today.\n\nIn 2003, \"Wahm Boutique\" and \"Tuesday Bear\" (the first WAHM congos for diapers) and other WAHM items were opened to the public.\n\nAlso in 2003, wool soakers became , which resulted in a slew of WAHMs selling hand knit wool covers/soakers. In 2004 and 2005, the soakers evolved into wool longies or pants.\n\n2002 and 2003 are also known by many experienced cloth diapers WAHMS as the diaper war years. Larger cloth diaper companies began threatening smaller ones with legal action and accusations of pattern or patent infringement. Motherease threatened legal action as they tried to protect their pattern for the one size diaper, and Fuzzi Bunz threatened legal action to protect their patent on pocket diapers. Many small WAHM diaper makers closed, deciding that the industry was too cutthroat.\n\nIn 2004, Karen Fegelman, the owner of Kool Sheep Soakers, figured out how to program a simple shopping-cart system that would not oversell, and that would be super-affordable and easy for a WAHM to use. Hyena Cart was born. It is still used by WAHMS everywhere today as their only method of selling or as a supplement system.\n\n2004 and 2005 were also the host to an explosion of growth in the cloth diapering community. Many new cloth diaper sewing and retail businesses were started. The presence of WAHM congos also saw a tremendous amount of growth.\n\nAlso, in 2004 and 2005, designer diapers were the rage. Some more sought after brands can be sold at auction for $200–300.00, with some charity auctions bringing in over $500.00 for a single cloth diaper. Online forums exploded with cloth diapering communities (MDC taking the lead) and many new web sites dedicated to cloth diaper information. Diaper sewing is a hot pastime and many online resources, patterns, and diaper fabric stores were opened.\n\nIn 2006, the bumGenius diaper by Cotton Babies was introduced, as was the Thirsties brand of diaper covers. A flushable diaper called a gDiaper was introduced in the US.\n\nIn 2009, the GroVia Hybrid Diaper was introduced by The Natural Baby Company. A major component of this modern cloth diapering system, the BioSoaker, was awarded US patent 8115050.\n\nIn 2010 Boingo Baby Diaper Fastener(Us Patent Pending) hit the market as the first innovation for diaper fasteners in 25 years. An Alternative for diaper pins it and regularly sells out due to supply and demand.\n\nIn 2011 Namaste Mama was awarded a patent for their adjustable elastic one size Evolution Diaper. They were awarded a second patent in 2013 granting further coverage for the adjustable elastic design. The Evolution Diaper is now manufactured under their sister brand, Boingo Baby.\n\n2012 saw explosion of cloth diaper \"co-ops\" on the website Facebook where groups of people would purchase cloth diapers in bulk from the manufacturer and distribute them to the members.\n\nIn 2013, the Cotton Babies company was awarded a patent for their bumGenius diaper. This caused a resurgence of the \"diaper wars\" in 2002, with Cotton Babies pursuing Facebook co-ops, small business, and WAHM for patent violation.\n\nIn 2014 Diaper Diamond received a patent for their cloth diaper sprayer shield that makes rinsing cloth diapers easier and cleaner.\n\n\n\n"}
{"id": "17171834", "url": "https://en.wikipedia.org/wiki?curid=17171834", "title": "Computer-aided assessment", "text": "Computer-aided assessment\n\nComputer-aided assessment (or \"computer-assisted assessment\") is a term that covers all forms of assessments, whether summative (i.e. tests that will contribute to formal qualifications) or formative (i.e. tests that promote learning but are not part of a course's marking), delivered with the help of computers. This covers both assessments delivered on computer, either online or on a local network, and those that are marked with the aid of computers, such as those using Optical Mark Reading (OMR). There are number of open source online tools to handle exams conducted on OMR sheets.\n\nComputer-aided assessment can be viewed in a few different ways. Technically, assignments that are written on a computer and researched online are computer-aided assessments. One of the most common forms of computer-aided assessment (in terms of e-learning) is online quizzes or exams. These can be implemented online, and also marked by the computer by putting the answers in. Many content management systems will have easy to set up and use systems for online exams.\n\nIt is also envisaged that computer-based formative assessment, in particular, will play an increasingly important role in learning, with the increased use of banks of question items for the construction and delivery of dynamic, on-demand assessments. This can be witnessed by current projects such as the SQA's SOLAR Project.\n\nThe effectiveness of these assessments has been frequently demonstrated in studies, both in the form of positive student feedback and improvement in student performance (see, for example, Einig (2013) or Marriott and Lau (2008)).\n\n\n"}
{"id": "1558421", "url": "https://en.wikipedia.org/wiki?curid=1558421", "title": "Cropping (image)", "text": "Cropping (image)\n\nCropping is the removal of unwanted outer areas from a photographic or illustrated image. The process usually consists of the removal of some of the peripheral areas of an image to remove extraneous trash from the picture, to improve its framing, to change the aspect ratio, or to accentuate or isolate the subject matter from its background. Depending on the application, this can be performed on a physical photograph, artwork, or film footage, or it can be achieved digitally by using image editing software. The process of cropping is common to the photographic, film processing, broadcasting, graphic design, and printing businesses.\n\nIn the printing, graphic design and photography industries, cropping is the removal of unwanted areas from the periphery of a photographic or illustrated image. Cropping is one of the most basic photo manipulation processes, and it is carried out to remove an unwanted object or irrelevant noise from the periphery of a photograph, to change its aspect ratio, or to improve the overall composition. \nIn telephoto photography, most commonly in avian and aviation photography, an image is cropped to magnify the primary subject and further reduce the angle of view -- when a lens of sufficient focal length to achieve the desired magnification directly was not available. It is considered one of the few editing actions permissible in modern photojournalism along with tonal balance, color correction and sharpening. A cropping made by trimming off the top and bottom margins of a photograph, or a film, produces an view that mimics the panoramic format (in photography) or the widescreen format in cinematography and broadcasting. Neither of these formats is cropped as such, but rather they are products of highly specialized optical configurations and camera designs.\n\nCropping in order to emphasize the subject:\nCropping in order to remove unwanted details/objects:\n\nIn certain circumstances, film footage may be cropped to change it from one aspect ratio to another, without stretching the image or filling the blank spaces with letterbox bars (fig. 2).\n\nConcerns about aspect ratios are a major issue in filmmaking. Rather than cropping, the cinematographer usually uses mattes to increase the latitude for alternative aspect ratios in projection and broadcast. Anamorphic optics (such as Panavision lenses) produce a full-frame, horizontally compressed image from which broadcasters and projectionists can matte a number of alternative aspect ratios without cropping relevant image detail. Without this, widescreen reproduction, especially for television broadcasting, is dependent upon a variety of soft matting techniques such as letterboxing, which involves varying degrees of image cropping (see figures 2, 3 and 4)\n\nSince the advent of widescreen television, a similar process removes large chunks from the top & bottom to make a standard 4:3 image fit a 16:9 one, losing 25 percent of the original image. This process has become standard in the United Kingdom, for television programs in which many archive clips are used. This gives them a zoomed-in, cramped image with reduced ??. Another option is a process called pillarboxing, where black bands are placed down the sides of the screen, allowing the original image to be shown full-frame within the wider aspect ratio (fig. 6). See this article for a fuller description of the problem.\n\nVarious methods may be used following cropping or may be used on the original image.\n\nIt is not possible to \"uncrop\" a cropped image unless the original still exists or undo information exists: if an image is cropped and saved (without undo information), it cannot be recovered without the original.\n\nHowever, using texture synthesis, it is possible to artificially add a band around an image, synthetically \"uncropping\" it. This is effective if the band smoothly blends with the existing image, which is relatively easy if the edge of the image has low detail or is a chaotic natural pattern such as sky or grass, but does not work if discernible objects are cut off at the boundary, such as half a car. An uncrop plug-in exists for the GIMP image editor.\n"}
{"id": "34496910", "url": "https://en.wikipedia.org/wiki?curid=34496910", "title": "Danielle Fong", "text": "Danielle Fong\n\nDanielle Fong (born October 30, 1987) is a Canadian entrepreneur and the co-founder and Chief Scientist of LightSail Energy.\n\nFong was born in Halifax, Nova Scotia, and was raised in the Dartmouth community there. At age 12, she dropped out of junior high school, and enrolled in Dalhousie University, where she got her Bachelor of Science in Physics and Computer Science in 2005 at age 17. She joined the Plasma Physics program at Princeton University as a Ph.D candidate, but later dropped out.\n\nIn 2009 at Berkeley, California, Fong co-founded LightSail Energy with entrepreneur Stephen Crane and Edwin P. Berlin, Jr. LightSail Energy is developing a form of compressed air energy storage, which they term regenerative air energy storage (RAES). The company was initially backed by Khosla Ventures.\n\nIn 2011, Fong was featured in \"Forbes\" \"30 under 30\" entrepreneurs under the Energy category. and interviewed by Forbes.com in a video titled \"Danielle Fong May Save the World\". She was named by the MIT Technology Review as one of the top 35 innovators under 35 in 2012.\n\nShe is a regular guest contributor to the Women 2.0 blog and was a featured speaker at the \"Women 2.0 PITCH Conference & Competition\" in 2012.\n"}
{"id": "2445044", "url": "https://en.wikipedia.org/wiki?curid=2445044", "title": "Deep reactive-ion etching", "text": "Deep reactive-ion etching\n\nDeep reactive-ion etching (DRIE) is a highly anisotropic etch process used to create deep penetration, steep-sided holes and trenches in wafers/substrates, typically with high aspect ratios. It was developed for microelectromechanical systems (MEMS), which require these features, but is also used to excavate trenches for high-density capacitors for DRAM and more recently for creating through silicon vias (TSVs) in advanced 3D wafer level packaging technology.\n\nThere are two main technologies for high-rate DRIE: cryogenic and Bosch, although the Bosch process is the only recognised production technique. Both Bosch and cryo processes can fabricate 90° (truly vertical) walls, but often the walls are slightly tapered, e.g. 88° (\"reentrant\") or 92° (\"retrograde\").\n\nAnother mechanism is sidewall passivation: SiOF functional groups (which originate from sulphur hexafluoride and oxygen etch gases) condense on the sidewalls, and protect them from lateral etching. As a combination of these processes deep vertical structures can be made.\n\nIn cryogenic-DRIE, the wafer is chilled to −110 °C (163 K). The low temperature slows down the chemical reaction that produces isotropic etching. However, ions continue to bombard upward-facing surfaces and etch them away. This process produces trenches with highly vertical sidewalls. The primary issues with cryo-DRIE is that the standard masks on substrates crack under the extreme cold, plus etch by-products have a tendency of depositing on the nearest cold surface, i.e. the substrate or electrode.\n\nThe Bosch process, named after the German company Robert Bosch GmbH which patented the process, also known as pulsed or time-multiplexed etching, alternates repeatedly between two modes to achieve nearly vertical structures:\n\nEach phase lasts for several seconds. The passivation layer protects the entire substrate from further chemical attack and prevents further etching. However, during the etching phase, the directional ions that bombard the substrate attack the passivation layer at the bottom of the trench (but not along the sides). They collide with it and sputter it off, exposing the substrate to the chemical etchant.\n\nThese etch/deposit steps are repeated many times over resulting in a large number of very small isotropic etch steps taking place only at the bottom of the etched pits. To etch through a 0.5 mm silicon wafer, for example, 100–1000 etch/deposit steps are needed. The two-phase process causes the sidewalls to undulate with an amplitude of about 100–500 nm. The cycle time can be adjusted: short cycles yield smoother walls, and long cycles yield a higher etch rate.\n\nRIE \"deepness\" depends on application:\n\nWhat distinguishes DRIE from RIE is etch depth: Practical etch depths for RIE (as used in IC manufacturing) would be limited to around 10 µm at a rate up to 1 µm/min, while DRIE can etch features much greater, up to 600 µm or more with rates up to 20 µm/min or more in some applications.\n\nDRIE of glass requires high plasma power, which makes it difficult to find suitable mask materials for truly deep etching. Polysilicon and nickel are used for 10–50 µm etched depths. In DRIE of polymers, Bosch process with alternating steps of SF etching and CF passivation take place. Metal masks can be used, however they are expensive to use since several additional photo and deposition steps are always required. Metal masks are not necessary however on various substrates (Si [up to 800 µm], InP [up to 40 µm] or glass [up to 12 µm]) if using chemically amplified negative resists.\n\nGallium ion implantion can be used as etch mask in cryo-DRIE. Combined nanofabrication process of focused ion beam and cryo-DRIE was first reported by N Chekurov \"et al\" in their article \"The fabrication of silicon nanostructures by local gallium implantation and cryogenic deep reactive ion etching\" (Nanotechnology, 2009).\n\nDRIE has enabled the use of silicon mechanical components in high-end wristwatches. According to an engineer at Cartier, “There is no limit to geometric shapes with DRIE,”. With DRIE it is possible to obtain an aspect ratio of 30 or more, meaning that a surface can be etched with a vertical-walled trench 30 times deeper than its width.\n\nThis has allowed for silicon components to be substituted for some parts which are usually made of steel, such as the hairspring. Silicon is lighter and harder than steel, which carries benefits but makes the manufacturing process more challenging.\n\n"}
{"id": "41062453", "url": "https://en.wikipedia.org/wiki?curid=41062453", "title": "ERating", "text": "ERating\n\neRating is a certification, education, and labeling program for the passenger transportation sector developed by the Certification for Sustainable Transport (CST) at the University of Vermont. The eRating certification system is used by the Certification for Sustainable Transportation to rate vehicles based on criteria such as greenhouse gas emissions per passenger mile, emissions, alternative fuels, purchase of carbon offsets, and any training programs that the driver has undergone that may help with energy efficient driving. If enough standards are met, the vehicle is given an eRating certification.\n\nThe Certification for Sustainable Transportation's (CST) eRating certification program is an independent, third party certification, education, and labeling initiative for owners and operators, manufacturers, and passengers of transportation vehicles. The eRating certification program also functions as a sustainability index that weighs factors such as greenhouse gasses per passenger mile, environmental impacts, and even the use of alternative fuels and technologies in the transportation industry. Two major works were used in developing the CST eRating certification program: the Federal Trade Commission’s Part 26-Guide for the Use of Environmental Marketing Claims, and the International Social and Environmental Accreditation and Labeling (ISEAL) Alliance planning framework. In May 2012, the CST finalized Step B-3, and transitioned to step E-1 to launch the CST eRating certification program. The CST was designed to help improve economic, environmental, and energy efficiency within the passenger transportation sector. The program uses research-based criteria to evaluate vehicles and includes the driver training programs \"Idle Free\" and \"Eco-Driving 101\" to improve efficiency.\n\nThis course teaches drivers about the health, environmental, and financial impacts of idling a vehicle. Health experts, vehicle manufacturers, and vehicle operators have all given their testimony about the advantages of idle-free driving. Upon completion of the course, drivers are then able to take an \"idle-free pledge\", in which they promise to follow the idling guidelines set forth in the course. Completion of this course earns 20 points towards an eRating certification.\n\nThis course teaches drivers about eco-driving. Eco-driving is a set of simple driving habits that result in using less fuel, generating fewer emissions, and increasing safety. The course first explains the science behind eco-driving, as well as the environmental and mechanical benefits of doing so. Drivers are taught techniques such as avoiding aggressive acceleration, speeding and braking monitoring speed to maintain efficient and consistent speed; keeping RPM levels as low as possible for the speed and keeping the vehicle properly maintained, that they can use in their everyday driving in order to cut back on fuel consumption. The typical eco-driver can increase fuel efficiency by 10-30%. For organizations putting their drivers through this online training program, 20 points will be awarded towards the organizations eRating certification once they reach the 80% driver completion threshold.\n\nThe program benefits owners, operators, and manufacturers by helping them reduce vehicle operation costs, save energy, and promote their businesses. The program also benefits consumers by helping them identify and choose the highest performing, lowest impact forms of transportation. Whether displayed on a bus, boat, train, car, bicycle, or plane, an eRating certification label signifies that CST has thoroughly evaluated and certified the vehicle.\n\nVarious features of the vehicle being considered for certification are examined. There are four levels of certification for a vehicle(s) in the eRating program: e1, e2, e3, and e4. The CST uses a point system, or sustainability index, to determine the certification level of a given vehicle. Points are given for more efficient features and attributes of the vehicle, with 100 being required for entry level, or e1, certification. e1 certification represents entry-level certification and e4 certification indicates the highest level. Only the most energy efficient, lowest impacts forms of transportation are eligible for certification; a certification label on a vehicle, be it e1, e2, e3, or e4, lets consumers know that the vehicle has met a set of rigorous sustainability criteria.\n\nThe CST eRating certification program aims to provide recognition through certification to transportation systems, fleets, operators and individual vehicles that help the passenger transportation sector:\n\n· Reduce greenhouse gas and other harmful emissions\n\n· Increase energy efficiency\n\n· Utilize alternative fuels and new technologies\n\nThe CST eRating certification program will offer four levels of certification on a per vehicle basis to qualifying operators: e1, e2, e3 and e4 certification; e1 certification represents entry-level certification and e4 certification indicates the highest level of certification available. The application process determines which level of certification an operator qualifies. Points are earned based on the following:\n\n· Vehicle technology\n\n· Operation of the vehicle(s) at certain efficiency levels\n\n· Use of particular operating procedures within a company\n\n· Use of specific policies and educational programs within a company\n\nPoints are earned if the vehicle's greenhouse gas emission levels are at least 50% below the U.S. average for 2000-2009 (.274 kg per passenger mile). Minimum qualifications for vehicles must be greater than or equal to an average of 148 passenger miles per gallon. Higher levels of efficiency earn greater points.\n\nPoints are awarded for the use of technologies that reduce emissions such as carbon monoxide, sulfur dioxide, volatile organic compounds and nitrogen oxide. Pollutant producing vehicles, such as those with leaky exhaust systems or that produce excessive amounts of smoke, are automatically disqualified for certification.\n\nThe use of alternative fuels besides gasoline or diesel earns a vehicle points ranging from 5-100 towards certification. Qualifying fuels must be used a minimum of 80% of the time.\n\nOptional points can be earned by purchasing greenhouse gas offsets from endorsed carbon-trading programs. These GHG reduction credits must be purchased through the Climate Action Reserve or Verified Carbon Standard or from another verified organization. These credits must be purchased in the region of intended use and must not be more than two years old. Generally 1 point will be awarded toward certification for every 5% of emissions offset.\n"}
{"id": "13856398", "url": "https://en.wikipedia.org/wiki?curid=13856398", "title": "Emiran", "text": "Emiran\n\nEmiran culture was a culture that existed in the Levant (Syria, Lebanon, Israel, Palestine) between the Middle Paleolithic and the Upper Paleolithic periods.\n\nEmiran culture apparently developed from the local Mousterian without rupture, keeping numerous elements of the Levalloise-Mousterian, together with the locally typical Emireh point. The Emireh point is the type tool of stage one of the Upper Paleolithic, first identified in the Emiran culture. Numerous stone blade tools were used, including curved knives similar to those found in the Chatelperronian culture of Western Europe.\n\nThe Emiran eventually evolved into the Antelian culture, still of Levalloise tradition but with some Aurignacian influences.\n\nAccording to Dorothy Garrod, the Emireh point, known from several sites in Palestine, is the hallmark of this culture.\n\n\n"}
{"id": "57195675", "url": "https://en.wikipedia.org/wiki?curid=57195675", "title": "Emmons problem", "text": "Emmons problem\n\nIn combustion, Emmons problem describes the flame structure which develops inside the boundary layer, created by a flowing oxidizer stream on flat fuel (solid or liquid) surfaces. The problem was first studied by Howard Wilson Emmons in 1956. The flame is of diffusion flame type because it separates fuel and oxygen by a flame sheet. The corresponding problem in a quiescent oxidizer environment is known as Clarke–Riley diffusion flame.\n\nConsider a semi-infinite fuel surface with leading edge located at formula_1 and let the free stream oxidizer velocity be formula_2. Through the solution formula_3 of Blasius equation formula_4 (formula_5 is the self-similar Howarth–Dorodnitsyn coordinate), the mass flux formula_6 (formula_7 is density and formula_8 is vertical velocity) in the vertical direction can be obtained\n\nwhere \n\nIn deriving this, it is assumed that the density formula_11 and the viscosity formula_12, where formula_13 is the temperature. The subscript formula_14 describes the values far away from the fuel surface. The main interest in combustion process is the fuel burning rate, which is obtained by evalutaing formula_6 at formula_16, as given below,\n\n"}
{"id": "3997844", "url": "https://en.wikipedia.org/wiki?curid=3997844", "title": "European medieval architecture in North America", "text": "European medieval architecture in North America\n\nMedieval architecture in North America is an anachronism. Some structures in North America can however be classified as medieval, either by age or origin. In some rare cases these structures are seen as evidence on pre-Columbian trans-oceanic contact. Although much of this is pseudoscience, these buildings are of interest to American scholars of medieval architecture.\n\n\nMedieval building that have been transported to North America in modern times.\n\nOther later period buildings were also transported like the Cotswold Cottage, built in the early 17th century in Chedworth, Gloucestershire, England, now in The Henry Ford museum in Dearborn, Michigan. The Church of St. Mary the Virgin, Aldermanbury, London, which was designed by Sir Christopher Wren in 1677 is now in Fulton, Missouri. It includes a spiral staircase which probably dates to the 15th century.\n"}
{"id": "10432337", "url": "https://en.wikipedia.org/wiki?curid=10432337", "title": "Farmer-managed natural regeneration", "text": "Farmer-managed natural regeneration\n\nFarmer-managed natural regeneration (FMNR) is a low-cost, sustainable land restoration technique used to combat poverty and hunger amongst poor subsistence farmers in developing countries by increasing food and timber production, and resilience to climate extremes. It involves the systematic regeneration and management of trees and shrubs from tree stumps, roots and seeds. \nFMNR is especially applicable, but not restricted to, the dryland tropics. As well as returning degraded croplands and grazing lands to productivity, it can be used to restore degraded forests, thereby reversing biodiversity loss and reducing vulnerability to climate change. FMNR can also play an important role in maintaining not-yet-degraded landscapes in a productive state, especially when combined with other sustainable land management practices such as conservation agriculture on cropland and holistic management on rangelands.\nFMNR adapts centuries-old methods of woodland management, called coppicing and pollarding, to produce continuous tree-growth for fuel, building materials, food and fodder without the need for frequent and costly replanting. On farmland, selected trees are trimmed and pruned to maximise growth while promoting optimal growing conditions for annual crops (such as access to water and sunlight). When FMNR trees are integrated into crops and grazing pastures there is an increase in crop yields, soil fertility and organic matter, soil moisture and leaf fodder. There is also a decrease in wind and heat damage, and soil erosion.\n\nIn the Sahel region of Africa, FMNR has become a potent tool in increasing food security, resilience and climate change adaptation in poor, subsistence farming communities where much of sub-Saharan Africa's poverty exists. FMNR is also being promoted in East Timor, Indonesia and Myanmar.\n\nFMNR complements the evergreen agriculture, conservation agriculture and agroforestry movements. It is considered a good entry point for resource-poor and risk-averse farmers to adopt a low-cost and low-risk technique. This in turn has acted as a stepping stone to greater agricultural intensification as farmers become more receptive to new ideas.\n\nThroughout the developing world, immense tracts of farmland, grazing lands and forests have become degraded to the point they are no longer productive. Deforestation continues at an alarming rate. In Africa's drier regions, 74 percent of rangelands and 61 percent of rain-fed croplands are damaged by moderate to very severe desertification. In some African countries deforestation rates exceed planting rates by 300:1.\n\nDegraded land has an extremely detrimental effect on the lives of subsistence farmers who depend on it for their food and livelihoods. Subsistence farmers often make up to 70-80 percent of the population in these regions and they regularly suffer from hunger, malnutrition and even famine as a consequence.\nIn the Sahel region of Africa, a band of savannah which runs across the continent immediately south of the Sahara Desert, large tracts of once-productive farmland are turning to desert. In tropical regions across the world, where rich soils and good rainfall would normally assure bountiful harvests and fat livestock, some environments have become so degraded they are no longer productive.\nSevere famines across the African Sahel in the 1970s and 80s led to a global response, and stopping desertification became a top priority. Conventional methods of raising exotic and indigenous tree species in nurseries were used – planting out, watering, protecting and weeding. However, despite investing millions of dollars and thousands of hours labour, there was little overall impact. Conventional approaches to reforestation in such harsh environments faced insurmountable problems and were costly and labour-intensive. Once planted out, drought, sand storms, pests, competition from weeds and destruction by people and animals negated efforts. Low levels of community ownership were another inhibiting factor.\n\nExisting indigenous vegetation was generally dismissed as 'useless bush', and it was often cleared to make way for exotic species. Exotics were planted in fields containing living and sprouting stumps of indigenous vegetation, the presence of which was barely acknowledged, let alone seen as important.\n\nThis was an enormous oversight. In fact, these living tree stumps are so numerous they constitute a vast 'underground forest' just waiting for some care to grow and provide multiple benefits at little or no cost –and each stump can produce between 10 and 30 stems each. During the process of traditional land preparation, farmers saw the stems as weeds and slashed and burnt them before sowing their food crops. The net result was a barren landscape for much of the year with few mature trees remaining. To the casual observer, the land was turning to desert. Most concluded that there were no trees present and that the only way to reverse the problem was through tree planting.\n\nMeanwhile, established indigenous trees continued to disappear at an alarming rate. In Niger, from the 1930s until 1993, forestry laws took tree ownership and responsibility for the care of trees out of the hands of the people; and even though ineffective and uneconomic, reforestation through conventional tree planting seemed to be the only way to address desertification at the time.\n\nIn the early 1980s, in the Maradi region of the Republic of Niger, the missionary organisation, Serving in Mission (SIM), was unsuccessfully attempting to reforest the surrounding districts using conventional means. In 1983, SIM began experimenting and promoting FMNR amongst about 10 farmers. During the severe famine of 1984, a food-for-work program was introduced that saw some 70,000 people exposed to FMNR and its practice on around 12,500 hectares of farmland. From 1985 to 1999, FMNR continued to be promoted locally and nationally as exchange visits and training days were organised for various NGOs, government foresters, Peace Corps Volunteers and farmer and civil society groups. Additionally, SIM project staff and farmers visited numerous locations across Niger to provide training.\n\nBy 2004 it was ascertained that FMNR was being practiced on over five million hectares or 50 percent of Niger's farmland – an average reforestation rate of 250,000 hectares per year over a 20-year period. This transformation prompted Senior Fellow of the World Resources Institute, Chris Reij to comment that \"this is probably the largest positive environmental transformation in the Sahel and perhaps all of Africa”.\n\nAlso in 2004, World Vision Australia and World Vision Ethiopia initiated a forestry-based carbon sequestration project as a potential means to stimulate community development while engaging in environmental restoration. An innovative partnership with the World Bank, the Humbo Community-based Natural Regeneration Project involved the regeneration of 2,728 hectares of degraded native forests. This brought social, economic and ecological benefits to the participating communities. Within two years, communities were collecting wild fruits, firewood and fodder, and reported that wildlife had begun to return and erosion and flooding had been reduced. In addition, the communities are now receiving payments for the sale of carbon credits through the Clean Development Mechanism (CDM) of the Kyoto protocol.\n\nFollowing the success of the Humbo project, FMNR spread to the Tigray region of northern Ethiopia where 20,000 hectares have been set aside for regeneration, including 10-hectare FMNR model sites for research and demonstration in each of 34 sub-districts. In addition, the Government of Ethiopia has committed to reforest 15 million hectares of degraded land using FMNR as part of a climate change and renewable energy plan to become carbon neutral by 2025.\n\nIn Talensi, northern Ghana, FMNR is being practiced on 2,000-3,000 hectares and new projects, initiated by World Vision, are introducing FMNR into three new districts. In the Kaffrine and Diourbel regions of Senegal, FMNR has spread across 50,000 hectares in four years. World Vision is also promoting FMNR in Indonesia, Myanmar and East Timor. There are also examples of both independently promoted and spontaneous FMNR movements occurring. In Burkina Faso, for example, an increasing part of the country is being transformed into agroforestry parkland. And in Mali, an ageing agroforestry parkland of about 6 million hectares is showing signs of regeneration.\n\nFMNR depends on the existence of living tree stumps or roots in crop fields, grazing pastures, woodlands or forests. Each season bushy growth will sprout from the stumps/roots often appearing like small shrubs. Continuous grazing by livestock, regular burning and/or regular harvesting for fuel wood results in these 'shrubs' never attaining tree stature. On farmland, standard practice has been for farmers to slash this regrowth in preparation for planting crops, but with a little attention this growth can be turned into a valuable resource without jeopardising, but in fact, enhancing crop yields.\n\nFor each stump, a decision is made as to how many stems will be chosen to grow. The tallest and straightest stems are selected and the remaining stems culled. Best results are obtained when the farmer returns regularly to prune any unwanted new stems and side branches as they appear. Farmers can then grow other crops between and around the trees. When farmers want wood they can cut the stem(s) they want and leave the rest to continue growing. The remaining stems will increase in size and value each year, and will continue to protect the environment. Each time a stem is harvested, a younger stem is selected to replace it.\n\nVarious naturally occurring tree species can be used which may also provide berries, fruits and nuts or have medicinal qualities. In Niger, commonly used species include: \"Strychnos spinosa\", \"Balanites aegyptiaca\", \"Boscia senegalensis\", \"Ziziphus\" spp., \"Annona senegalensis\", \"Poupartia birrea\" and \"Faidherbia albida\". However, the most important determinants are whatever species are locally available, their ability to re-sprout after cutting, and the value local people place on those species.\n\n\"Faidherbia albida\", also known as the 'fertiliser tree', is popular for intercropping across the Sahel as it fixes nitrogen into the soil, provides fodder for livestock, and shade for crops and livestock. By shedding its leaves in the wet season, \"Faidherbia\" provides beneficial light shade to crops when high temperatures would otherwise damage crops or retard growth. Leaf fall contributes useful nutrients and organic matter to the soil.\n\nThe practice of FMNR is not confined to croplands. It is being practised on grazing land and in degraded communal forests as well. When there are no living stumps, seeds of naturally occurring species are used. In reality, there is no fixed way of practising FMNR and farmers are free to choose which species they will leave, the density of trees they prefer, and the timing and method of pruning.\n\nFMNR depends on the existence of living tree stumps, tree roots and seeds to be re-vegetated. These can be in crop fields, grazing lands or degraded forests. New stems, which sprout from these stumps and tree roots, can be selected and pruned for improved growth. Sprouting tree stumps and roots may look like shrubs and are often ignored or even slashed by farmers or foresters. However, with culling of excess stems and by selecting and pruning of the best stems, the re-growth has enormous potential to rapidly grow into trees.\n\nSeemingly treeless fields may contain seeds and living tree stumps and roots which have the ability to sprout new stems and regenerate trees. Even this 'bare' millet field in West Africa contains hundreds of living stumps per hectare which are buried beneath the surface like an underground forest.\n\nFMNR can restore degraded farmlands, pastures and forests by increasing the quantity and value of woody vegetation, by increasing biodiversity and by improving soil structure and fertility through leaf litter and nutrient cycling. The reforestation also retards wind and water erosion; it creates wind-breaks which decrease soil moisture evaporation, and protects crops and livestock against searing winds and temperatures. Often, dried up springs reappear and the water table rises towards historic levels; insect eating predators including insects, spiders and birds return, helping to keep crop pests in check; the trees can be a source of edible berries and nuts; and over time the biodiversity of plant and animal life is increased. FMNR can be used to combat deforestation and desertification and can also be an important tool in maintaining the integrity and productivity of land that is not yet degraded.\n\nTrials, long-running programs and anecdotal data indicate that FMNR can at least double crop yields on low fertility soils. In the Sahel, high numbers of livestock and an eight-month dry season can mean that pastures are completely depleted before the rains commence. However, with the presence of trees, grazing animals can make it through the dry season by feeding on tree leaves and seed pods of some species, at a time when no other fodder is available. In north east Ghana, more grass became available with the introduction of FMNR because communities worked together to prevent bush fires from destroying their trees.\n\nWell designed and executed FMNR projects can act as catalysts to empower communities as they negotiate land ownership or user rights for the trees in their care. This assists with self-organisation, and with the development of new agriculture-based micro-enterprises (e.g. selling firewood, timber and handcrafts made from timber or woven grasses).\n\nConventional approaches to reversing desertification, such as funding tree planting, rarely spread beyond the project boundary once external funding is withdrawn. By comparison, FMNR is cheap, rapid, locally led and implemented. It uses local skills and resources – the poorest farmers can learn by observation and teach their neighbours. Given an enabling environment, or at least the absence of a 'disabling' environment, FMNR can be done at scale and spread well beyond the original target area without ongoing government or NGO intervention.\n\nWorld Vision evaluations of FMNR conducted in Senegal and Ghana in 2011 and 2012 found that households practising FMNR were less vulnerable to extreme weather shocks such as drought and damaging rain and wind storms.\n\nThe following table summarises FMNR's benefits which fit the sustainable development model of economic, social and environmental benefits:\n\n\"Source: compiled by R Francis, Project Manager FMNR, World Vision Australia from Brown et al, Garrity et al, Haglund et al, McGahuey & Winterbottom, Reij et al, Rinaudo, World Resources Institute.\"\n\nWhile there are numerous accounts of the uptake and spread of FMNR independent of aid and development agencies, the following factors have been found to be beneficial for its introduction and spread: \n\nBrown et al. suggest that the two main reasons why FMNR has spread so widely in Niger are attitudinal change by the community of what constitutes good land management practices, and farmers' ownership of trees. Farmers need the assurance that they will benefit from their labour. Giving farmers either outright ownership of the trees they protect, or tree-user rights, has made it possible for large-scale farmer-led reforestation to take place.\n\nOver nearly 30 years, FMNR has changed the farming landscape in some of the poorest countries in the world, including parts of Niger, Burkina Faso, Mali and Senegal, providing subsistence farmers with the methods necessary to become more food secure and resilient against severe weather events.\n\nThe 2011–12 food crisis in East Africa gave a stark reminder of the importance of addressing root causes of hunger. In the 2011 State of the World Report, Bunch concludes that four major factors – lack of sustainable fertile land, loss of traditional fallowing, cost of fertiliser and climate change – are coming together all at once in a sort of “perfect storm” that will almost surely result in an African famine of unprecedented proportions, probably within the next four to five years. It will most heavily affect the lowland, semi-arid to sub-humid areas of Africa (including the Sahel, parts of eastern Africa, plus a band from Malawi across to Angola and Namibia); and unless the world does something dramatic, 10 to 30 million people could die from famine between 2015 and 2020. Restoration of degraded land through FMNR is one way of addressing these major contributors to hunger.\n\nIn recent years FMNR has come to the attention of global development agencies and grass roots movements alike. The World Bank, World Resources Institute, World Agroforestry Center, USAID and the Permaculture movement are amongst those either actively promoting or advocating for the uptake of FMNR and FMNR has received recognition from a number of quarters including:\n\n\nIn April 2012, World Vision Australia – in partnership with the World Agroforestry Center and World Vision East Africa – held an international conference in Nairobi called Beating Famine to analyse and plan how to improve food security for the world's poor through the use of FMNR and Evergreen Agriculture. The conference was attended by more than 200 participants, including world leaders in sustainable agriculture, five East African ministers of agriculture and the environment, ambassadors and other government representatives from Africa, Europe and Australia, and leaders from non-government and international organisations.\n\nTwo major outcomes of the conference were:\n\nThe conference acted as a catalyst for media coverage of FMNR in some of the world's leading outlets and a noticeable increase in momentum for an FMNR global movement. This heightened awareness of FMNR has created an opportunity for it to spread exponentially worldwide.\n\nWorld Vision and the World Agroforestry Centrer are currently exploring opportunities for conducting conferences and workshops in new regions where FMNR is not yet established in order to stimulate further awareness and adoption.\n\n\n"}
{"id": "12726821", "url": "https://en.wikipedia.org/wiki?curid=12726821", "title": "Gas detector", "text": "Gas detector\n\nA gas detector is a device that detects the presence of gases in an area, often as part of a safety system. This type of equipment is used to detect a gas leak or other emissions and can interface with a control system so a process can be automatically shut down. A gas detector can sound an alarm to operators in the area where the leak is occurring, giving them the opportunity to leave. This type of device is important because there are many gases that can be harmful to organic life, such as humans or animals.\n\nGas detectors can be used to detect combustible, flammable and toxic gases, and oxygen depletion. This type of device is used widely in industry and can be found in locations, such as on oil rigs, to monitor manufacture processes and emerging technologies such as photovoltaic. They may be used in firefighting.\n\nGas leak detection is the process of identifying potentially hazardous gas leaks by sensors. These sensors usually employ an audible alarm to alert people when a dangerous gas has been detected. Exposure to toxic gases can also occur in operations such as painting, fumigation, fuel filling, construction, excavation of contaminated soils, landfill operations, entering confined spaces, etc. Common sensors include combustible gas sensors, photoionization detectors, infrared point sensors, ultrasonic sensors, electrochemical gas sensors, and semiconductor sensors. More recently, infrared imaging sensors have come into use. All of these sensors are used for a wide range of applications and can be found in industrial plants, refineries, pharmaceutical manufacturing, fumigation facilities, paper pulp mills, aircraft and shipbuilding facilities, hazmat operations, waste-water treatment facilities, vehicles, indoor air quality testing and homes.\n\nGas leak detection methods became a concern after the effects of harmful gases on human health were discovered. Before modern electronic sensors, early detection methods relied on less precise detectors. Through the 19th and early 20th centuries, coal miners would bring canaries down to the tunnels with them as an early detection system against life-threatening gases such as carbon dioxide, carbon monoxide and methane. The canary, normally a very songful bird, would stop singing and eventually die if not removed from these gases, signaling the miners to exit the mine quickly.\n\nThe first gas detector in the industrial age was the \"flame safety lamp\" (or Davy lamp) was invented by Sir Humphry Davy (of England) in 1815 to detect the presence of methane (firedamp) in underground coal mines. The flame safety lamp consisted of an oil flame adjusted to specific height in fresh air. To prevent ignition with the lamps flame was contained within a glass sleeve with a mesh flame arrestor. The flames height varied depending on the presence of methane (higher) or the lack of oxygen (lower). To this day, in certain parts of the world flame safety lamps are still in service.\n\nThe modern era of gas detection started in 1926-1927 with the development of the catalytic combustion (LEL) sensor by Dr.Oliver Johnson. Dr Johnson was an employee of Standard Oil Company in California (now Chevron), he begun research and development on a method to detect combustible mixtures in air to help prevent explosions in fuel storage tanks. A demonstration model was developed in 1926 and denoted as the Model A. The first practical \"electric vapor indicator\" meter begun production in 1927 with the release of the Model B.\n\nThe worlds first gas detection company, Johnson-Williams Instruments (or J-W Instruments) was formed in 1928 in Palo Alto, CA by Dr Oliver Johnston and Phil Williams. J-W Instruments is recognized as the first electronics company in Silicon Valley. Over the next 40 years J-W Instruments pioneered many \"firsts\" in the modern age of gas detection, including making instruments smaller and more portable, development of a portable oxygen detector as well as the first combination instrument that could detect both combustible gases/vapors as well as oxygen.\n\nBefore the development of electronic household carbon monoxide detectors in the 1980s and 1990s, carbon monoxide presence was detected with a chemically infused paper that turned brown when exposed to the gas. Since then, many electronic technologies and devices have been developed to detect, monitor, and alert the leak of a wide array of gases.\n\nAs the cost and performance of electronic gas sensors improved, they have been incorporated into a wider range of systems. Their use in automobiles was initially for engine emissions control, but now gas sensors may also be used to ensure passenger comfort and safety. Carbon dioxide sensors are being installed into buildings as part of demand-controlled ventilation systems. Sophisticated gas sensor systems are being researched for use in medical diagnostic, monitoring, and treatment systems, well beyond their initial use in operating rooms. Gas monitors and alarms for carbon monoxide and other harmful gases are increasingly available for office and domestic use, and are becoming legally required in some jurisdictions.\n\nOriginally, detectors were produced to detect a single gas. Modern units may detect several toxic or combustible gases, or even a combination. Newer gas analyzers can break up the component signals from a complex aroma to identify several gases simultaneously.\n\nGas detectors can be classified according to the operation mechanism (semiconductors, oxidation, catalytic, photoionization, infrared, etc.). Gas detectors come packaged into two main form factors: portable devices and fixed gas detectors.\n\nPortable detectors are used to monitor the atmosphere around personnel and are either hand-held or worn on clothing or on a belt/harness. These gas detectors are usually battery operated. They transmit warnings via audible and visible signals, such as alarms and flashing lights, when dangerous levels of gas vapors are detected.\n\nFixed type gas detectors may be used for detection of one or more gas types. Fixed type detectors are generally mounted near the process area of a plant or control room, or an area to be protected, such as a residential bedroom. Generally, industrial sensors are installed on fixed type mild steel structures and a cable connects the detectors to a SCADA system for continuous monitoring. A tripping interlock can be activated for an emergency situation.\n\nElectrochemical gas detectors work by allowing gases to diffuse through a porous membrane to an electrode where it is either chemically oxidized or reduced. The amount of current produced is determined by how much of the gas is oxidized at the electrode, indicating the concentration of the gas. Manufactures can customize electrochemical gas detectors by changing the porous barrier to allow for the detection of a certain gas concentration range. Also, since the diffusion barrier is a physical/mechanical barrier, the detector tended to be more stable and reliable over the sensor's duration and thus required less maintenance than other early detector technologies.\n\nHowever, the sensors are subject to corrosive elements or chemical contamination and may last only 1–2 years before a replacement is required. Electrochemical gas detectors are used in a wide variety of environments such as refineries, gas turbines, chemical plants, underground gas storage facilities, and more.\n\nCatalytic bead sensors are commonly used to measure combustible gases that present an explosion hazard when concentrations are between the lower explosion limit (LEL) and upper explosion limit (UEL). Active and reference beads containing platinum wire coils are situated on opposite arms of a Wheatstone bridge circuit and electrically heated, up to a few hundred degrees C. The active bead contains a catalyst that allows combustible compounds to oxidize, thereby heating the bead even further and changing its electrical resistance. The resulting voltage difference between the active and passive beads is proportional to the concentration of all combustible gases and vapors present. The sampled gas enters the sensor through a sintered metal frit, which provides a barrier to prevent an explosion when the instrument is carried into an atmosphere containing combustible gases. Pellistors measure essentially all combustible gases, but they are more sensitive to smaller molecules that diffuse through the sinter more quickly. The measureable concentration ranges are typically from a few hundred ppm to a few volume percent. Such sensors are inexpensive and robust, but require a minimum of a few percent oxygen in the atmosphere to be tested and they can be poisoned or inhibited by compounds such as silicones, mineral acids, chlorinated organic compounds, and sulfur compounds.\n\nPhotoionization detectors (PIDs) use a high-photon-energy UV lamp to ionize chemicals in the sampled gas. If the compound has an ionization energy below that of the lamp photons, an electron will be ejected, and the resulting current is proportional to the concentration of the compound. Common lamp photon energies include 10.0 eV, 10.6 eV and 11.7 eV; the standard 10.6 eV lamp lasts for years, while the 11.7 eV lamp typically last only a few months and is used only when no other option is available. A broad range of compounds can be detected at levels ranging from a few ppb to several thousand ppm. Detectable compound classes in order of decreasing sensitivity include: aromatics and alkyl iodides; olefins, sulfur compounds, amines, ketones, ethers, alkyl bromides and silicate esters; organic esters, alcohols, aldehydes and alkanes; H2S, NH3, PH3 and organic acids. There is no response to standard components of air or to mineral acids. Major advantages of PIDs are their excellent sensitivity and simplicity of use; the main limitation is that measurements are not compound-specific. Recently PIDs with pre-filter tubes have been introduced that enhance the specificity for such compounds as benzene or butadiene. Fixed, hand-held and miniature clothing-clipped PIDs are widely used for industrial hygiene, hazmat, and environmental monitoring.\n\nInfrared (IR) point sensors use radiation passing through a known volume of gas; energy from the sensor beam is absorbed at certain wavelengths, depending on the properties of the specific gas. For example, carbon monoxide absorbs wavelengths of about 4.2-4.5 μm. The energy in this wavelength is compared to a wavelength outside of the absorption range; the difference in energy between these two wavelengths is proportional to the concentration of gas present.\n\nThis type of sensor is advantageous because it does not have to be placed into the gas to detect it and can be used for remote sensing. Infrared point sensors can be used to detect hydrocarbons and other infrared active gases such as water vapor and carbon dioxide. IR sensors are commonly found in waste-water treatment facilities, refineries, gas turbines, chemical plants, and other facilities where flammable gases are present and the possibility of an explosion exists. The remote sensing capability allows large volumes of space to be monitored.\n\nEngine emissions are another area where IR sensors are being researched. The sensor would detect high levels of carbon monoxide or other abnormal gases in vehicle exhaust and even be integrated with vehicle electronic systems to notify drivers.\n\nInfrared image sensors include active and passive systems. For active sensing, IR imaging sensors typically scan a laser across the field of view of a scene and look for backscattered light at the absorption line wavelength of a specific target gas. Passive IR imaging sensors measure spectral changes at each pixel in an image and look for specific spectral signatures that indicate the presence of target gases. The types of compounds that can be imaged are the same as those that can be detected with infrared point detectors, but the images may be helpful in identifying the source of a gas.\n\nSemiconductor sensors detect gases by a chemical reaction that takes place when the gas comes in direct contact with the sensor. Tin dioxide is the most common material used in semiconductor sensors, and the electrical resistance in the sensor is decreased when it comes in contact with the monitored gas. The resistance of the tin dioxide is typically around 50 kΩ in air but can drop to around 3.5 kΩ in the presence of 1% methane. This change in resistance is used to calculate the gas concentration. Semiconductor sensors are commonly used to detect hydrogen, oxygen, alcohol vapor, and harmful gases such as carbon monoxide. One of the most common uses for semiconductor sensors is in carbon monoxide sensors. They are also used in breathalyzers. Because the sensor must come in contact with the gas to detect it, semiconductor sensors work over a smaller distance than infrared point or ultrasonic detectors.\n\nUltrasonic gas leak detectors are not gas detectors per se. They detect the acoustic emission created when a pressured gas expands in a low pressure area through a small orifice (the leak). They use acoustic sensors to detect changes in the background noise of its environment. Since most high-pressure gas leaks generate sound in the ultrasonic range of 25 kHz to 10 MHz, the sensors are able to easily distinguish these frequencies from background acoustic noise which occurs in the audible range of 20 Hz to 20 kHz. The ultrasonic gas leak detector then produces an alarm when there is an ultrasonic deviation from the normal condition of background noise. Ultrasonic gas leak detectors cannot measure gas concentration, but the device is able to determine the leak rate of an escaping gas because the ultrasonic sound level depends on the gas pressure and size of the leak.\n\nUltrasonic gas detectors are mainly used for remote sensing in outdoor environments where weather conditions can easily dissipate escaping gas before allowing it to reach leak detectors that require contact with the gas to detect it and sound an alarm. These detectors are commonly found on offshore and onshore oil/gas platforms, gas compressor and metering stations, gas turbine power plants, and other facilities that house a lot of outdoor pipeline.\n\nHolographic gas sensors use light reflection to detect changes in a polymer film matrix containing a hologram. Since holograms reflect light at certain wavelengths, a change in their composition can generate a colorful reflection indicating the presence of a gas molecule. However, holographic sensors require illumination sources such as white light or lasers, and an observer or CCD detector.\n\nAll gas detectors must be calibrated on a schedule. Of the two form factors of gas detectors, portables must be calibrated more frequently due to the regular changes in environment they experience. A typical calibration schedule for a fixed system may be quarterly, bi-annually or even annually with more robust units. A typical calibration schedule for a portable gas detector is a daily \"bump test\" accompanied by a monthly calibration. Almost every portable gas detector requires a specific calibration gas which is available from the manufacturer. In the US, the Occupational Safety and Health Administration (OSHA) may set minimum standards for periodic recalibration.\n\nBecause a gas detector is used for employee/worker safety, it is very important to make sure it is operating to manufacturer's specifications. Australian standards specify that a person operating any gas detector is strongly advised to check the gas detector's performance each day and that it is maintained and used in accordance with the manufacturers instructions and warnings.\n\nA challenge test should consist of exposing the gas detector to a known concentration of gas to ensure that the gas detector will respond and that the audible and visual alarms activate. It is also important inspect the gas detector for any accidental or deliberate damage by checking that the housing and screws are intact to prevent any liquid ingress and that the filter is clean, all of which can affect the functionality of the gas detector. The basic calibration or challenge test kit will consist of calibration gas/regulator/calibration cap and hose (generally supplied with the gas detector) and a case for storage and transport. Because 1 in every 2,500 untested instruments will fail to respond to a dangerous concentration of gas, many large businesses use an automated test/calibration station for bump tests and calibrate their gas detectors daily.\n\nOxygen deficiency gas monitors are used for employee and workforce safety. Cryogenic substances such as liquid nitrogen (LN2), liquid helium (He), and liquid argon (Ar) are inert and can displace oxygen (O) in a confined space if a leak is present. A rapid decrease of oxygen can provide a very dangerous environment for employees, who may not notice this problem before they suddenly lose consciousness. With this in mind, an oxygen gas monitor is important to have when cryogenics are present. Laboratories, MRI rooms, pharmaceutical, semiconductor, and cryogenic suppliers are typical users of oxygen monitors.\n\nOxygen fraction in a breathing gas is measured by electro-galvanic oxygen sensors. They may be used stand-alone, for example to determine the proportion of oxygen in a nitrox mixture used in scuba diving, or as part of feedback loop which maintains a constant partial pressure of oxygen in a rebreather.\n\nDetection of hydrocarbons can be based on the mixing properties of gaseous hydrocarbons – or other volatile organic compounds (VOCs) – and the sensing material incorporated in the sensor. The selectivity and sensitivity depends on the molecular structure of the VOC and the concentration; however, it is difficult to design a selective sensor for a single VOC. Many VOC sensors detect using a fuel-cell method.\n\nVOCs in the environment or certain atmospheres can be detected based on different principles and interactions between the organic compounds and the sensor components. There are electronic devices that can detect ppm concentrations despite not being particularly selective. Others can predict with reasonable accuracy the molecular structure of the volatile organic compounds in the environment or enclosed atmospheres and could be used as accurate monitors of the chemical fingerprint and further as health monitoring devices.\n\nSolid-phase microextraction (SPME) techniques are used to collect VOCs at low concentrations for analysis.\n\nDirect injection mass spectrometry techniques are frequently utilized for the rapid detection and accurate quantification of VOCs. PTR-MS is among the methods that have been used most extensively for the on-line analysis of biogenic and antropogenic VOCs. Recent PTR-MS instruments based on time-of-flight mass spectrometry have been reported to reach detection limits of 20 pptv after 100 ms and 750 ppqv after 1 min measurement (signal integration) time. The mass resolution of these devices is between 7000 and 10,500 m/Δm, thus it is possible to separate most common isobaric VOCs and quantify them independently.\n\n\nGaseous ammonia is continuously monitored in industrial refrigeration processes and biological degradation processes, including exhaled breath. Depending on the required sensitivity, different types of sensors are used (e.g., flame ionization detector, semiconductor, electrochemical, photonic membranes). Detectors usually operate near the lower exposure limit of 25ppm; however, ammonia detection for industrial safety requires continuous monitoring above the fatal exposure limit of 0.1%.\n\n\n\nThere are several different sensors that can be installed to detect hazardous gases in a residence. Carbon monoxide is a very dangerous, but odorless, colorless gas, making it difficult for humans to detect. Carbon monoxide detectors can be purchased for around US$20–60. Many local jurisdictions in the United States now require installation of carbon monoxide detectors in addition to smoke detectors in residences.\n\nHandheld flammable gas detectors can be used to trace leaks from natural gas lines, propane tanks, butane tanks, or any other combustible gas. These sensors can be purchased for US$35–100.\n\nThe European Community has supported research called the MINIGAS project that was coordinated by VTT Technical Research Center of Finland. This research project aims to develop new types of photonics-based gas sensors, and to support the creation of smaller instruments with equal or higher speed and sensitivity than conventional laboratory-grade gas detectors.\n\n\n\n\n"}
{"id": "57622", "url": "https://en.wikipedia.org/wiki?curid=57622", "title": "Helianthus annuus", "text": "Helianthus annuus\n\nHelianthus annuus, the common sunflower, is a large annual forb of the genus \"Helianthus\" grown as a crop for its edible oil and edible fruits. This sunflower species is also used as wild bird food, as livestock forage (as a meal or a silage plant), in some industrial applications, and as an ornamental in domestic gardens. The plant was first domesticated in the Americas. Wild \"Helianthus annuus\" is a widely branched annual plant with many flower heads. The domestic sunflower, however, often possesses only a single large inflorescence (flower head) atop an unbranched stem. The name \"sunflower\" may derive from the flower's head's shape, which resembles the sun, or from the impression that the blooming plant appears to slowly turn its flower towards the sun as the latter moves across the sky on a daily basis.\n\nSunflower seeds were brought to Europe from the Americas in the 16th century, where, along with sunflower oil, they became a widespread cooking ingredient.\n\nThe plant has an erect rough-hairy stem, reaching typical heights of . The tallest sunflower on record achieved . Sunflower leaves are broad, coarsely toothed, rough and mostly alternate. What is often called the \"flower\" of the sunflower is actually a \"flower head\" or pseudanthium of numerous small individual five-petaled flowers (\"florets\"). The outer flowers, which resemble petals, are called ray flowers. Each \"petal\" consists of a ligule composed of fused petals of an asymmetrical ray flower. They are sexually sterile and may be yellow, red, orange, or other colors. The flowers in the center of the head are called disk flowers. These mature into fruit (sunflower \"seeds\"). The disk flowers are arranged spirally. Generally, each floret is oriented toward the next by approximately the golden angle, 137.5°, producing a pattern of interconnecting spirals, where the number of left spirals and the number of right spirals are successive Fibonacci numbers. Typically, there are 34 spirals in one direction and 55 in the other; however, in a very large sunflower head there could be 89 in one direction and 144 in the other. This pattern produces the most efficient packing of seeds mathematically possible within the flower head.\n\nMost cultivars of sunflower are variants of \"Helianthus annuus\", but four other species (all perennials) are also domesticated. This includes \"H. tuberosus\", the Jerusalem Artichoke, which produces edible tubers.\n\nA model for the pattern of florets in the head of a sunflower was proposed by H. Vogel in 1979. This is expressed in polar coordinates\nwhere θ is the angle, \"r\" is the radius or distance from the center, and \"n\" is the index number of the floret and \"c\" is a constant scaling factor. It is a form of Fermat's spiral. The angle 137.5° is related to the golden ratio (55/144 of a circular angle, where 55 and 144 are Fibonacci numbers) and gives a close packing of florets. This model has been used to produce computer graphics representations of sunflowers.\n\nThe sunflower, \"Helianthus annuus\", genome is diploid with a base chromosome number of 17 and an estimated genome size of 2871–3189 Mbp. Some sources claim its true size is around 3.5 billion base pairs (slightly larger than the human genome).\n\nTo grow best, sunflowers need full sun. They grow best in fertile, moist, well-drained soil with heavy mulch. In commercial planting, seeds are planted apart and deep.\nSunflower \"whole seed\" (fruit) are sold as a snack food, raw or after roasting in ovens, with or without salt and/or seasonings added. Sunflowers can be processed into a peanut butter alternative, sunflower butter. In Germany, it is mixed with rye flour to make \"Sonnenblumenkernbrot\" (literally: sunflower whole seed bread), which is quite popular in German-speaking Europe. It is also sold as food for birds and can be used directly in cooking and salads. Native Americans had multiple uses for sunflowers in the past, such as in bread, medical ointments, dyes and body paints.\n\nSunflower halva is popular in countries in Eastern Europe, including Belarus, Bulgaria, Romania, Moldova, Latvia, Lithuania, Estonia, Russia, and Ukraine as well as other countries of the former Soviet Union. It is made of sunflower seeds instead of sesame.\n\nSunflower oil, extracted from the seeds, is used for cooking, as a carrier oil and to produce margarine and biodiesel, as it is cheaper than olive oil. A range of sunflower varieties exist with differing fatty acid compositions; some 'high oleic' types contain a higher level of monounsaturated fats in their oil than even olive oil.\n\nThe cake remaining after the seeds have been processed for oil is used as a livestock feed. The hulls resulting from the dehulling of the seeds before oil extraction can also be fed to domestic animals. Some recently developed cultivars have drooping heads. These cultivars are less attractive to gardeners growing the flowers as ornamental plants, but appeal to farmers, because they reduce bird damage and losses from some plant diseases. Sunflowers also produce latex, and are the subject of experiments to improve their suitability as an alternative crop for producing hypoallergenic rubber.\n\nTraditionally, several Native American groups planted sunflowers on the north edges of their gardens as a \"fourth sister\" to the better known three sisters combination of corn, beans, and squash. Annual species are often planted for their allelopathic properties.\n\nHowever, for commercial farmers growing commodity crops other than sunflowers, the wild sunflower, like any other unwanted plant, is often considered a weed. Especially in the Midwestern US, wild (perennial) species are often found in corn and soybean fields and can have a negative impact on yields.\n\nSunflowers can be used in phytoremediation to extract toxic ingredients from soil, such as lead, arsenic and uranium, and used in rhizofiltration to neutralize radionuclides and other toxic ingredients and harmful bacteria from water. They were used to remove caesium-137 and strontium-90 from a nearby pond after the Chernobyl disaster, and a similar campaign was mounted in response to the Fukushima Daiichi nuclear disaster.\n\nSunflowers are grown as ornamentals in a domestic setting. Being easy-to-grow and producing spectacular results in any good, moist soil in full sun, they are a favourite subject for children. A large number of cultivars, of varying size and colour, are now available to grow from seed. The following are cultivars of sunflowers (those marked have gained the Royal Horticultural Society's Award of Garden Merit):- \nA common misconception is that flowering sunflower heads track the Sun across the sky. Although immature flower buds exhibit this behaviour, the mature flowering heads point in a fixed (and typically easterly) direction throughout the day. This old misconception was disputed in 1597 by the English botanist John Gerard, who grew sunflowers in his famous herbal garden: \"[some] have reported it to turn with the Sun, the which I could never observe, although I have endeavored to find out the truth of it.\" The uniform alignment of sunflower heads in a field might give some people the false impression that the flowers are tracking the sun.\n\nThis alignment results from heliotropism in an earlier development stage, the young flower stage, before full maturity of flower heads (anthesis). Young sunflowers orient themselves in the direction of the sun. At dawn the head of the flower faces east and moves west throughout the day. When sunflowers reach full maturity they no longer follow the sun, and continuously face east. Young flowers reorient overnight to face east in anticipation of the morning. Their heliotropic motion is a circadian rhythm, synchronized by the sun, which continues if the sun disappears on cloudy days or if plants are moved to constant light. They are able to regulate their circadian rhythm in response to the blue-light emitted by a light source. If a sunflower plant in the bud stage is rotated 180°, the bud will be turning away from the sun for a few days, as resynchronization by the sun takes time.\n\nWhen growth of the flower stalk stops and the flower is mature, the heliotropism also stops and the flower faces east from that moment onward. This eastward orientation allows rapid warming in the morning and, as a result, an increase in pollinator visits. Sunflowers do not have a pulvinus below their inflorescence. A pulvinus is a flexible segment in the leaf stalks (petiole) of some plant species and functions as a 'joint'. It effectuates leaf motion due to reversible changes in turgor pressure, which occurs without growth. The sensitive plant's closing leaves are a good example of reversible leaf movement via pulvinuli.\n\nAlthough it was commonly accepted that the sunflower was first domesticated in what is now the southeastern US, roughly 5000 years ago, there is evidence that it was first domesticated in Mexico around 2600 BC. These crops were found in Tabasco, Mexico at the San Andres dig site. The earliest known examples in the United States of a fully domesticated sunflower have been found in Tennessee, and date to around 2300 BC. Other very early examples come from rockshelter sites in Eastern Kentucky. Many indigenous American peoples used the sunflower as the symbol of their solar deity, including the Aztecs and the Otomi of Mexico and the Incas in South America. In 1510 early Spanish explorers encountered the sunflower in the Americas and carried its seeds back to Europe. Of the four plants known to have been domesticated in what is now the eastern continental United States and to have become important agricultural commodities, the sunflower is currently the most economically important.\n\nDuring the 18th century, the use of sunflower oil became very popular in Russia, particularly with members of the Russian Orthodox Church, because sunflower oil was one of the few oils that was allowed during Lent, according to some fasting traditions. In the early 19th century it was first commercialized in the village of Alexeyevka in Voronezh Governorate by the merchant named Daniil Bokaryov, who developed a technology suitable for its large-scale extraction, and quickly spread around. The town's coat of arms includes an image of a sunflower ever since.\n\nAmong the Zuni people, the fresh or dried root is chewed by the medicine man before sucking venom from a snakebite and applying a poultice to the wound. This compound poultice of the root is applied with much ceremony to rattlesnake bites. Blossoms are also used ceremonially for anthropic worship.\n\nThere are many species in the sunflower genus \"Helianthus\", and many species in other genera that may be called sunflowers.\n\n\nIn today's market, most of the sunflower seeds provided or grown by farmers are hybrids. Hybrids or hybridized sunflowers are produced by crossbreeding different types and species of sunflower, for example crossbreeding cultivated sunflowers with wild species of sunflowers. By doing so, new genetic recombinations are obtained ultimately leading to the production of new hybrid species. These hybrid species generally have a higher fitness and carry properties or characteristics that farmers look for, such as resistance to pathogens.\n\nHybrid, \"Helianthus annuus dwarf2\" does not contain the hormone gibberellin and does not display heliotropic behavior. Plants treated with an external application of the hormone display a temporary restoration of elongation growth patterns. This growth pattern diminished by 35% 7–14 days after final treatment.\n\nHybrid Male Sterile and Male Fertile flowers that display heterogeneity have a low crossover of honeybee visitation. Sensory cues such as pollen odor, diameter of seed head, and height may influence pollinator visitation of pollinators that display constancy behavior patterns.\n\nOne of the major threats that sunflowers face today is Fusarium, a filamentous fungi that is found largely in soil and plants. It is a pathogen that over the years has caused an increasing amount of damage and loss of sunflower crops, some as extensive as 80 percent of damaged crops.\n\nDowny mildew is another disease to which sunflowers are susceptible. Its susceptibility to downy mildew is particular high due to the sunflower's way of growth and development. Sunflower seeds are generally planted only an inch deep in the ground. When such shallow planting is done in moist and soaked earth or soil, it increases the chances of diseases such as downy mildew.\n\nAnother major threat to sunflower crops is broomrape, a parasite that attacks the root of the sunflower and causes extensive damage to sunflower crops, as high as 100 percent.\n\n\n\n"}
{"id": "659359", "url": "https://en.wikipedia.org/wiki?curid=659359", "title": "Industrial processes", "text": "Industrial processes\n\nIndustrial processes are procedures involving chemical, physical, electrical or mechanical steps to aid in the manufacturing of an item or items, usually carried out on a very large scale. Industrial processes are the key components of heavy industry.\n\nThese may be applied on their own, or as part of a larger process.\n\n\n\nThe availability of electricity and its effect on materials gave rise to several processes for plating or separating metals.\n\n\nThere are several physical processes for reshaping a material by cutting, folding, joining or polishing, developed on a large scale from workshop techniques.\n\nThe physical shaping of materials by forming their liquid form using a mould.\n\nMany materials exist in an impure form, purification, or separation provides a usable product.\n\n\n\nEarly production of iron was from meteorites, or as a by-product of copper refining.\n\nThe nature of an organic molecule means it can be transformed at the molecular level to create a range of products.\n\nOrganized by product:\n\n\nA list by process:\n\n"}
{"id": "1410986", "url": "https://en.wikipedia.org/wiki?curid=1410986", "title": "Joseph Day (inventor)", "text": "Joseph Day (inventor)\n\nJoseph Day (1855 in London – 1946) is a little-known English engineer who developed the extremely widely used crankcase-compression two-stroke petrol engine,\nas used for small engines from lawnmowers to mopeds and small motorcycles. He trained as an engineer at the Crystal Palace School of Engineering at Crystal Palace in London, began work at Stothert & Pitt in Bath, and in 1889 designed the crankcase-compression two-stroke engine as it is widely known today (in contrast to the two-stroke engine designed by Dugald Clark), the Valve-less Two-Stroke Engine. In 1878 he started his own business, an iron foundry making cranes, mortar mills and compressors amongst other things. \n\nHe advertised a new design of \"valveless air compressor\" which he made on licence from the patentee, Edmund Edwards. By 1889, he was working on an engine design which would not infringe the patents that Otto had on the four-stroke, and that he eventually called the Valveless Two-Stroke Engine. In fact there were two flap valves in Joseph Day's original design, one in the inlet port, where you would probably find a reed valve on a modern two stroke, and one in the crown of the piston, because he did not come up with the idea of the transfer ports until a couple of years later. He made about 250 of these first two-port motors, fitting them to small generating sets, which won a prize at the International Electrical Exhibition in 1892.\n\nIt was one of Joseph Day's workmen, Frederick Cock, who made the modification which allowed the skirt of the piston to control the inlet port and do away with valves altogether, giving rise to the classic piston ported two stroke. Only two of these original engines have survived, one in the Deutsches Museum in Munich, the other in the Science Museum in London.\n\nThe first American patent was taken out in 1894, and by 1906, a dozen American companies had taken licences. One of these, Palmers of Connecticut, managed by entrepreneur Julius Briner, had produced over 60,000 two-stroke engines before 1912.\nMany of these early engines found their way into motorcycles, or onto the back of boats.\n\nHis company in Bath was a general engineering one, and his engines were a sideline. Much of his money came from the manufacture of bread making machinery, and the prices of wheat were very turbulent around the turn of the Century. The profitability of Day’s factory fluctuated just as wildly. These were early days for the idea of the limited company, and shareholders, then as now, could panic and bring down a company that they thought to be under threat. The problem was made worse by the publication of rumours, or the deliberate orchestration of publicity campaigns in the press.\n\nJoseph Day suffered from his involvement with both of the aforementioned, with the result that his firm was driven into bankruptcy. A flurry of lawsuits followed, with Day as either plaintiff or defendant. The Treasury Solicitor even tried to have him extradited from the USA where he had gone to try to sell his US patents in order to raise money. The case was eventually settled when the jury found that Day had no case to answer, but it all came too late, and he went into virtual retirement by the seaside. The development of his engine then passed to his licence holders in America, whose royalties restored his finances sufficiently to allow him to launch a spectacular new venture after the First World War. This new enterprise was the exploration for oil.\n\nDay lost most of his fortune exploring for oil in Norfolk in the east of England. A second financial disaster was the last straw, and Joseph Day disappeared from public view between 1925 and his death in 1946. His obscurity was so complete that a mere five years after his death, the Science Museum made a public appeal for biographical information about him – with no apparent result.\n"}
{"id": "12675001", "url": "https://en.wikipedia.org/wiki?curid=12675001", "title": "List of LNG terminals", "text": "List of LNG terminals\n\nLiquefied natural gas (LNG) is the liquefied form of natural gas, which has a much smaller volume than natural gas in its gaseous form. This liquefied condition is used to facilitate the carriage of natural gas over long distances, often by sea, in specialized tanks. \n\nLNG port terminals are purpose-built port terminals designed to accommodate large LNG carrier ships designed to load, carry and unload LNG. These LNG terminals are located adjacent to a gas liquefaction and storage plant (export), or to a gas regasification and storage plant (import), which are themselves connected to gas pipelines connected to on-shore or off-shore gas fields (export) or to storage and distribution plants (import).\n\n\n\n\n\n\n\n\nLahari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following LNG off-loading and regasification terminals are located in the United States and Gulf of Mexico:\n\n\n\nNeed to add Annova LNG and Texas LNG which along with Rio Grande LNG are seeking FERC approval to build and operate at the Port of Brownsville, TX.\n\nAlso, NextDecade, parent company of Rio Grande LNG, is seeking DOE permission for a proposed Galveston Bay LNG project. \n\n\n\n\n\nThe United States has had a massive shift in LNG terminal planning and construction starting in 2010-2011 due to rapid increase in domestic supply with the widespread adoption of hydraulic fracking. Many brand-new LNG import terminals are planning or have begun addition of liquefaction facilities to operate as export terminals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe country also has liquefaction terminals in more remote areas for export, and imports from the Middle East in areas with dense population.\n\n\n\n\n\n\n\n\n\nKaliningrad LNG Terminal\n\n\n\nProposed LNG Terminals:\n\n\n\n11.http://www.lngglobal.com/latest/sabine-pass-progress-report-train-2-first-cargo-mid-august-2016.html\n"}
{"id": "3151313", "url": "https://en.wikipedia.org/wiki?curid=3151313", "title": "LocationFree Player", "text": "LocationFree Player\n\nSony's LocationFree is the marketing name for a group of products and technologies for timeshifting and placeshifting streaming video. The LocationFree Player is an internet-based multifunctional device used to stream live television broadcasts (including digital cable and satellite), DVDs and DVR content over a home network or the internet. It is in essence a remote video streaming server product (similar to the Slingbox). It was first announced by Sony in Q1 2004 and launched early in Q4 2004 alongside a co-branded wireless tablet TV. The last LocationFree product was the LF-V30 released in 2007.\nThe LocationFree base station connects to a home network via a wired Ethernet cable, or for newer models, via a wireless connection. Up to three attached video sources can stream content through the network to local content provision devices or across the internet to remote devices. A remote user can connect to the internet at a wireless hotspot or any other internet connection anywhere in the world and receive streamed content. Content may only be streamed to one computer at a time. In addition, the original LocationFree Player software contained a license for only one client computer. Additional (paid) licenses were required to connect to the base station for up to a total of 4 clients. \n\nOn November 29, 2007 Sony modified its LocationFree Player policy to provide free access to the latest LocationFree Player LFA-PC30 software for Windows XP/Vista. In addition, the software no longer requires a unique serial number in order to pair it with a LocationFree base station. In December, 2007 Sony Dropped the $30 license fee for the LocationFree client. However, the software still requires registration to Sony's servers after 30 days.\nNote (2016)...When attempting to register on-line, you are presented with a failed message and told that there is no internet connection. This may be operating system version related (Win 7, 8, 10) as the software was written for older Windows versions. The \"free\" version of the player, which can still be downloaded, will work for a time (30 days?) but then wants to be registered or have a key input. There does not seem to be a key available for the free version so the software must be removed and re-installed to continue using it. Any new units came with a CD of the software (usually an older version) along with a key, which can't be used on the free version. Two devices using the same key could not be registered to the same device. The same limitation is not present with the \"free\" software.\n\nThe player (server) can stream content to the following (client) devices:\n\n\nSony (Client) Products:\n\n\nThese products do not act as DVRs, since they do not allow content to be recorded to a hard drive.\n\nA user can also access and control from anywhere in the world any device connected to the unit, and switch between multiple devices.\n\nBase stations packaged with LocationFree Player installation disc and instructional DVD\n\n\"Notes:\" - Wired models can be used via normal wireless routers. Access via internet\nvia firewall provided necessary ports are opened. Can be used with DDNS.\n\nClient box - enables users to watch streamed content on a television set, without the need for a PC or laptop.\n\nIn October 2004 Sony unveiled a portable, wireless and rechargeable SVGA 12.1\" LCD tablet screen with dualband Wi-Fi technology (IEEE 802.11a/b/g)\nwhich can receive pictures from the LocationFree player up to 100 feet from the source signal. The TV also has web-browsing and email functions, a Memory Stick Duo slot and an on-screen hand-drawing function for use as a drawing tablet. The screen can also be used as an intelligent universal AV remote control. These tablets were bundled with Base Stations.\n\nThree versions have been released:\n\n\n\n"}
{"id": "43909683", "url": "https://en.wikipedia.org/wiki?curid=43909683", "title": "Maggie Orth", "text": "Maggie Orth\n\nMaggie Orth (born 1964, Columbus, OH) is an American artist and technologist who helped create the field of E-textiles. \nHer 2001 MIT Media Lab PhD thesis, Sculpted computational objects with smart and active computing materials and associated publications and patents are among the early work in this field. She was named a 2007 United States Artists Target Fellow. The United States Artists foundation describes her as \"A pioneer of electronic textiles, interactive fashions, wearable computing, and interface design\". She founded , which created e-textile products.\n\nThe team of Gorbett+Banerjee and Maggie Orth were commissioned to create an interactive robotic sculpture, for the Mineta San Jose International airport.\n\n"}
{"id": "28043082", "url": "https://en.wikipedia.org/wiki?curid=28043082", "title": "Maize milling", "text": "Maize milling\n\nA maize milling machine is a machine for processing maize or corn for safe consumption, such as for daily food use. \n\nThe grain is first cleaned and then conditioned (dampening the maize with water and then allowing it to condition for some time to stay in a temper bin).\n\nCleaning and conditioning of the maize is an important step in the process and refers to the removal of foreign material and all that is not maize kernels from the to-be milled grain that lowers the quality of the product such as husk, straw, dust, sand, and everything too big or too small and lighter than a maize kernel.\n\nIt also refers to the removal of poisonous seeds, and material harmful to the milling equipment such as metal and stones. Conditioning refers to the addition of moisture to the maize to allow the bran to be peeled off in flakes during milling with plate or roller mills, allowing easy separation in a sifter and, most importantly, to add mass to the meal.\n\nFollowing this process, milling can commence and may take several forms:\n\n1. Adopt the Roller Mill to grinding the grain, the roller mill has Single roller mill, Double roller mill and Pneumatic roller mill, the mill adopt the grinding roller to work. In a complete maize milling plant, there are several Roller Mill to work together, they have different functions, the first mill mainly peeling the maize skin, the second and third will grinding the maize into granular size, and meanwhile to get some super fine flour, and the granular sized product will go to the next mill to continue grinding.\n\n2.While grinding time, use Double Bin sifter or Square Plan sifter to sift the meal from the miller,classification and sifting more super flour.In general,the sifting used to separate the flour and bran, also separate large size and small size to ensure flour quality.\n\nAfter the maize is processed, it will come out in different final products like flour and grits. They are different from their granular size. For the packing, a Full-auto Flour Packing Machine is adopted, and the flour is packed into 5kg, 10kg, 25kg or 50kg bags.\n\nThe best quality maize meal is therefore obtained by:\n\nDe-germinating the maize prior to milling\nMilling with rollers rather than hammer mills or plate mills/disc mills.\nAlternatively, if the budget is small, the whole maize, after cleaning and conditioning, may be milled by means of the plate mill only and then sifted (without de-germination). A significant part of the bran and germ meal would then be sifted off, resulting in a Special Sifted meal - of lower quality than roller milled meal and higher quality than hammer milled meal.\n\nA further element of major importance is sifting. Milling and sifting, form the very essence of grain processing.\nThe sifters in every system should be one or more of the following:\n\nTurbo sifters - two separation horizontally shafted with steel screens - high capacity but not sifting very fine - used in small capacity plate mill systems and as graders for samp and other primary grading after de-germinators.\n\nRotary sifters - three separation horizontally shafted with nylon screens - lower capacity but very fine screening - used in small capacity plate mill systems.\n\nMini plan sifters - relatively high capacity and very fine screening used as primary sifters for all mills of 1 ton per hour to 2.5 tons per hour.\n\nPlan sifters in various sizes and numbers of passages: Very high capacity and fine screening - used in all mills with capacity of 2.5 ton per hour and more.\n\nIndustrial type mills can be constructed over one level, two levels, three levels or more, as existing buildings determine or as practical as the solution may be. High capacity mills normally require more levels to make use of gravity in moving product between mills and sifters.\n\nAnother option to consider, is to place a small mill (500-1,000 kg/h) in a container. This is only recommended for special applications where buildings are problematic, where the mill needs to be moved from time to time to another location, where temporary power is used and where the mill is located in very remote areas.\n\n"}
{"id": "37844", "url": "https://en.wikipedia.org/wiki?curid=37844", "title": "Mass driver", "text": "Mass driver\n\nA mass driver or electromagnetic catapult is a proposed method of non-rocket spacelaunch which would use a linear motor to accelerate and catapult payloads up to high speeds. All existing and contemplated mass drivers use coils of wire energized by electricity to make electromagnets. Sequential firing of a row of electromagnets accelerates the payload along a path. After leaving the path, the payload continues to move due to momentum. \n\nAlthough any device used to propel a ballistic payload is technically a mass driver, in this context a mass driver is essentially a coilgun that magnetically accelerates a package consisting of a magnetizable holder containing a payload. Once the payload has been accelerated, the two separate, and the holder is slowed and recycled for another payload.\n\nMass drivers can be used to propel spacecraft in three different ways: A large, ground-based mass driver could be used to launch spacecraft away from Earth, the Moon, or another body. A small mass driver could be on board a spacecraft, flinging pieces of material into space to propel itself. Another variation would have a massive facility on a moon or asteroid send projectiles to assist a distant craft.\n\nMiniaturized mass drivers can also be used as weapons in a similar manner as classic firearms or cannon using chemical combustion. Hybrids between coilguns and railguns such as helical railguns are also possible.\n\nMass drivers need no physical contact between moving parts because they guide their projectiles by dynamic magnetic levitation, allowing extreme reusability in the case of solid-state power switching, and a functional life of - theoretically - up to millions of launches. While marginal costs tend to be accordingly low, initial development and construction costs are highly dependent on performance, especially the intended mass, acceleration, and velocity of projectiles. For instance, while Gerard O'Neill built his first mass driver in 1976–1977 with a $2000 budget, a short test model firing a projectile at 40 m/s and 33 g,\nhis next model had an order-of-magnitude greater acceleration\nafter a comparable increase in funding, and, a few years later, researchers at the University of Texas estimated that a mass driver firing a 10 kilogram projectile at 6000 m/s would cost $47 million.\n\nFor a given amount of energy involved, heavier objects go proportionally slower. Light objects may be projected at 20 km/s or more. The limits are generally the expense of energy storage able to be discharged quickly enough and the cost of power switching, which may be by semiconductors or by gas-phase switches (which still often have a niche in extreme pulse power applications). However, energy can be stored inductively in superconducting coils. A 1 km long mass driver made of superconducting coils can accelerate a 20 kg vehicle to 10.5 km/s at a conversion efficiency of 80%, and average acceleration of 5,600 g.\n\nEarth-based mass drivers for propelling vehicles to orbit, such as the StarTram concept, would require considerable capital investment.\n\nThe Earth's relatively strong gravity and relatively thick atmosphere make such an installation difficult, thus many proposals feature installing mass drivers on the moon, where the lower gravity and lack of atmosphere greatly reduce the required velocity to reach lunar orbit.\n\nMost serious mass-driver designs use superconducting coils to achieve reasonable energetic efficiency (often 50% to 90+%, depending on design). Equipment may include a superconducting bucket or aluminum coil as the payload. The coils of a mass driver can induce eddy currents in a payload's aluminum coil, and then act on the resulting magnetic field. There are two sections of a mass driver. The maximum acceleration part spaces the coils at constant distances, and synchronizes the coil currents to the bucket. In this section, the acceleration increases as the velocity increases, up to the maximum that the bucket can take. After that, the constant acceleration region begins. This region spaces the coils at increasing distances to give a fixed amount of velocity increase per unit of time.\n\nBased on this mode, a major proposal for the use of mass drivers involved transporting lunar-surface material to space habitats for processing using solar energy. The Space Studies Institute showed that this application was reasonably practical.\n\nIn some designs, the payload would be held in a bucket and then released, so that the bucket can be decelerated and reused. A disposable bucket, on the other hand, would avail acceleration along the whole track.\n\nIn contrast to cargo-only chemical space-gun concepts, a mass driver could be any length, affordable, and with relatively smooth acceleration throughout, optionally even lengthy enough to reach target velocity without excessive g forces for passengers. It can be constructed as a very long and mainly horizontally aligned launch track for spacelaunch, targeted upwards at the end, partly by bending of the track upwards and partly by Earth's curvature in the other direction.\n\nNatural elevations, such as mountains, may facilitate the construction of the distant, upwardly targeted part. The higher up the track terminates, the less resistance from the atmosphere the launched object will encounter.\n\nThe 40 megajoules per kilogram or less kinetic energy of projectiles launched at up to 9000 m/s velocity (if including extra for drag losses) towards low Earth orbit is a few kilowatt-hours per kilogram if efficiencies are relatively high, which accordingly has been hypothesized to be under $1 of electrical energy cost per kilogram shipped to LEO, though total costs would be far more than electricity alone. By being mainly located slightly above, on or beneath the ground, a mass driver may be easier to maintain compared with many other structures of non-rocket spacelaunch. Whether or not underground, it needs to be housed in a pipe that is vacuum pumped in order to prevent internal air drag, such as with a mechanical shutter kept closed most of the time but a plasma window used during the moments of firing to prevent loss of vacuum.\n\nA mass driver on Earth would usually be a compromise system. A mass driver would accelerate a payload up to some high speed which would not be enough for orbit. It would then release the payload, which would complete the launch with rockets. This would drastically reduce the amount of velocity needed to be provided by rockets to reach orbit. Well under a tenth of orbital velocity from a small rocket thruster is enough to raise perigee if a design prioritizes minimizing such, but hybrid proposals optionally reduce requirements for the mass driver itself by having a greater portion of delta-v by a rocket burn (or orbital momentum exchange tether). On Earth, a mass-driver design could possibly use well-tested maglev components.\n\nTo launch a space vehicle with humans on board, a mass driver's track would need to be several hundreds of kilometers long if providing almost all the velocity to Low Earth Orbit, though a lesser length could provide major launch assist. Required length, if accelerating mainly at near a constant maximum acceptable g-force for passengers, is proportional to velocity squared. For instance, half of the velocity goal could correspond to a quarter as long of a tunnel needing to be constructed, for the same acceleration. For rugged objects, much higher accelerations may suffice, allowing a far shorter track, potentially circular or helical (spiral). Another concept involves a large ring design whereby a space vehicle would circle the ring numerous times, gradually gaining speed, before being released into a launch corridor leading skyward.\n\nMass drivers have been proposed for the disposal of nuclear waste in space: a projectile launched at much above Earth's escape velocity would escape the Solar System, with atmospheric passage at such speed calculated as survivable through an elongated projectile and a very substantial heatshield.\n\nA spacecraft could carry a mass driver as its primary engine. With a suitable source of electrical power (probably a nuclear reactor) the spaceship could then use the mass driver to accelerate pieces of matter of almost any sort, boosting itself in the opposite direction. At the smallest scale of reaction mass, this type of drive is called an ion drive.\n\nNo absolute theoretical limit is known for the size, acceleration or muzzle energy of linear motors. However, practical engineering constraints apply for such as the power-to-mass ratio, waste heat dissipation, and the energy intake able to be supplied and handled. Exhaust velocity is best neither too low nor too high.\n\nThere is a mission-dependent limited optimal exhaust velocity and specific impulse for any thruster constrained by a limited amount of onboard spacecraft power. Thrust and momentum from exhaust, per unit mass expelled, scales up linearly with its velocity (momentum = mv), yet kinetic energy and energy input requirements scale up faster with velocity squared (kinetic energy = mv). Too low exhaust velocity would excessively increase propellant mass needed under the rocket equation, with too high a fraction of energy going into accelerating propellant not used yet. Higher exhaust velocity has both benefit and tradeoff, increasing propellant usage efficiency (more momentum per unit mass of propellant expelled) but decreasing thrust and the current rate of spacecraft acceleration if available input power is constant (less momentum per unit of energy given to propellant).\n\nElectric propulsion methods like mass drivers are systems where energy does not come from the propellant itself. (Such contrasts to chemical rockets where propulsive efficiency varies with the ratio of exhaust velocity to vehicle velocity at the time, but near maximum obtainable specific impulse tends to be a design goal when corresponding to the most energy released from reacting propellants). Although the specific impulse of an electric thruster itself optionally could range up to where mass drivers merge into particle accelerators with fractional-lightspeed exhaust velocity for tiny particles, trying to use extreme exhaust velocity to accelerate a far slower spacecraft could be suboptimally low thrust when the energy available from a spacecraft's reactor or power source is limited (a lesser analogue of feeding onboard power to a row of spotlights, photons being an example of an extremely low momentum to energy ratio).\n\nFor instance, if limited onboard power fed to its engine was the dominant limitation on how much payload a hypothetical spacecraft could shuttle (such as if intrinsic propellant economic cost was minor from usage of extraterrestrial soil or ice), ideal exhaust velocity would rather be around 62.75% of total mission delta v if operating at constant specific impulse, except greater optimization could come from varying exhaust velocity during the mission profile (as possible with some thruster types, including mass drivers and variable specific impulse magnetoplasma rockets).\n\nSince a mass driver could use any type of mass for reaction mass to move the spacecraft, a mass driver or some variation seems ideal for deep-space vehicles that scavenge reaction mass from found resources.\n\nOne possible drawback of the mass driver is that it has the potential to send solid reaction mass travelling at dangerously high relative speeds into useful orbits and traffic lanes. To overcome this problem, most schemes plan to throw finely-divided dust. Alternatively, liquid oxygen could be used as reaction mass, which upon release would boil down to its molecular state. Propelling the reaction mass to solar escape velocity is another way to ensure that it will not remain a hazard.\n\nA mass driver on a spacecraft could be used to \"reflect\" masses from a stationary mass driver. Each deceleration and acceleration of the mass contributes to the momentum of the spacecraft. The lightweight, fast spacecraft need not carry reaction mass, and does not need much electricity beyond the amount needed to replace losses in the electronics, while the immobile support facility can run off power plants able to be much larger than the spacecraft if needed. This could be considered a form of beam-powered propulsion (a macroscopic-scale analogue of a particle beam propelled magsail). A similar system could also deliver pellets of fuel to a spacecraft to power another propulsion system.\n\nAnother theoretical use for this concept of propulsion can be found in space fountains, a system in which a continuous stream of pellets in a circular track holds up a tall structure.\n\nSmall to moderate size high-acceleration electromagnetic projectile launchers are currently undergoing active research by the US Navy for use as ground-based or ship-based weapons (most often railguns but coilguns in some cases). On larger scale than weapons currently near deployment but sometimes suggested in long-range future projections, a sufficiently high velocity linear motor, a mass driver, could in theory be used as intercontinental artillery (or, if built on the Moon or in orbit, used to attack a location on Earth's surface). As the mass driver would be located further up the gravity well than the theoretical targets, it would enjoy a significant energy imbalance in terms of counter-attack.\n\nOne of the first engineering descriptions of an \"Electric Gun\" appears in the technical supplement of the 1937 science fiction novel \"Zero to Eighty\" by \"Akkad Pseudoman\", a pen name for the Princeton physicist and electrical entrepreneur Edwin Fitch Northrup. Dr. Northrup built prototype coil guns powered by kHz-frequency three phase electrical generators, and the book contains photographs of some of these prototypes. The book describes a fictional circumnavigation of the moon by a two-person vehicle launched by a Northrup electric gun.\n\nLater prototype mass drivers have been built since 1976 (Mass Driver 1), some constructed by the U.S. Space Studies Institute in order to prove their properties and practicality. Military R&D on coilguns is related, as are maglev trains.\n\n\n\n"}
{"id": "2649730", "url": "https://en.wikipedia.org/wiki?curid=2649730", "title": "Measuring cup", "text": "Measuring cup\n\nA measuring cup or measuring jug is a kitchen utensil used primarily to measure the volume of liquid or bulk solid cooking ingredients such as flour and sugar, especially for volumes from about 50 mL (2 fl oz) upwards. Measuring cups are also used to measure washing powder, liquid detergents and bleach for clothes washing. The cup will usually have a scale marked in cups and fractions of a cup, and often with fluid measure and weight of a selection of dry foodstuffs. \n\nMeasuring cups may be made of plastic, glass, or metal. Transparent (or translucent) cups can be read from an external scale; metal ones only from a dipstick or scale marked on the inside.\n\nMeasuring cups usually have capacities from 250 mL (approx. 1 cup) to 1000 mL (approx. 4 cups = 2 pints = 1 quart), though larger sizes are also available for commercial use. They usually have scale markings at different heights: the substance being measured is added to the cup until it reaches the wanted level. Dry measure cups without a scale are sometimes used, in sets typically of 1/4, 1/3, 1/2, and 1 cup. The units may be milliliters or fractions of a liter, or the cup (unit) with its fractions (typically 1/4, 1/3, 1/2, 2/3, and 3/4), pints, and often fluid ounces. Dry measure cups are distinguished from liquid measure cups in that they are meant to be filled to the top so that excess may be scraped off and shallow for easy cleaning. Liquid measure cups tend to be microwave safe for heating and clear to more easily judge the meniscus. \n\nSometimes multiples of teaspoons and tablespoons are included. There may also be scales for the approximate weight for particular substances, such as flour and sugar.\n\nMany dry ingredients, such as granulated sugar, are not very compressible, so volume measures are consistent. Others, notably flour, are more variable. For example, 1 cup of all-purpose flour sifted into a cup and leveled weighs about 100 grams, whereas 1 cup of all-purpose flour scooped from its container and leveled weighs about 140 grams.\n\nUsing a measuring cup to measure bulk foods which can be compressed to a variable degree such as chopped vegetables or shredded cheese leads to large measurement uncertainties. It is easier to chop down the units for a better measure.\n\n\n"}
{"id": "8886314", "url": "https://en.wikipedia.org/wiki?curid=8886314", "title": "Nicholas Kemmer", "text": "Nicholas Kemmer\n\nProf Nicholas Kemmer, FRS FRSE (7 December 1911 – 21 October 1998), was a Russian-born nuclear physicist working in Britain, who played an integral and leading edge role in United Kingdom's nuclear programme, and was known as a mentor of Abdus Salam – a Nobel laureate in physics.\n\nHe was the son of Nicholas P. Kemmer and his wife, Barbara Statzer, and was born in Saint Petersburg, his family moved to Germany in 1922, where he was educated at Bismarckschule Hanover and then at the University of Göttingen. He received his doctorate in nuclear physics at the University of Zurich and worked as an assistant to Wolfgang Pauli, who had to give strong arguments in 1936, before being allowed to employ a non-Swiss national. Later on, Kemmer moved to the Beit Fellowship at Imperial College London.\n\nHe moved to Trinity College, Cambridge in 1940 to work on Tube Alloys, the wartime atomic energy project. In 1940, when Egon Bretscher and Norman Feather showed that a slow neutron reactor fuelled with uranium would in theory produce substantial amounts of plutonium-239 as a by-product, Kemmer (who was lodging at the Bretschers') proposed the names Neptunium for the new element 93 and Plutonium for 94 by analogy with the outer planets Neptune and Pluto beyond Uranus (uranium being element 92). The Americans Edwin M. McMillan and Philip Abelson at the Berkeley Radiation Laboratory, who had made the same discovery, fortuitously suggested the same names.\n\nHe spent 1944–1946 in Canada. At the University of Edinburgh from 1953 to 1979, he was Tait Professor of Mathematical Physics, creating the Tait Institute of Mathematical Physics in 1955. He was elected as a Fellow of the Royal Society of Edinburgh in 1954. His proposers were Norman Feather, Max Born, Sir Edmund Whittaker and Alexander Aitken. He served as the Society's Vice-President from 1971 to 1974.\n\nHe was elected a Fellow of the Royal Society in 1956 and won its Hughes Medal in 1966. Kemmer was awarded the J. Robert Oppenheimer Memorial Prize in 1975. Nicholas Kemmer was also a mentor and a teacher of the only Pakistani Nobel laureate, Dr. Abdus Salam. Kemmer is credited to trained and work with Salam in Neutron scattering by using relativity equations. Salam later passed his research work to Pakistani physics students who went on to contribute in this field.\n\nThe Duffin–Kemmer–Petiau equation (DKP equation, also called \"Duffin–Kemmer equation\" or \"Kemmer equation\") plays a role in the description of the standard model of particles, together with the Yang-Mills field. The Duffin–Kemmer–Petiau equation is closely linked to the Proca equation and the Klein–Gordon equation. The DKP equation suffers the same drawback as the Klein–Gordon equation in that it calls for negative probabilities. The equation involves matrices which obey the Duffin–Kemmer–Petiau algebra. The work leading to the DKP equation, culminating in Kemmer's article, has been quoted as \"the first attempt at writing down a satisfactory relativistic theory of elementary particles beyond the electron\", and these equations have later been brought in unified form with the Dirac equation by Homi J. Bhabha.\n\n"}
{"id": "24684701", "url": "https://en.wikipedia.org/wiki?curid=24684701", "title": "Outline of robotics", "text": "Outline of robotics\n\nThe following outline is provided as an overview of and topical guide to robotics:\n\nRobotics is a branch of mechanical engineering, electrical engineering and computer science that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behaviour, and or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics.\n\nThe word \"robot\" was introduced to the public by Czech writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), published in 1920. The term \"robotics\" was coined by Isaac Asimov in his 1941 science fiction short-story \"Liar!\"\n\nRobotics can be described as:\n\n\n\nRobotics incorporates aspects of many disciplines including electronics, engineering, mechanics, software and arts. The design and control of robots relies on many fields knowledge, including:\n\n\n\nAutonomous robots – robots that are not controlled by humans:\n\nMobile robots may be classified by:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistory of robots\n\nFuture of robotics\n\n\n\n\n\nRobot competition\n\n\n\n\n\n"}
{"id": "57083442", "url": "https://en.wikipedia.org/wiki?curid=57083442", "title": "Paul Aebersold", "text": "Paul Aebersold\n\nPaul C. Aebersold (1910-1967) was an American nuclear physicist and pioneer of the biologic and medical application of radioactive materials. He worked on the Manhattan Project and became the first director of the United States Atomic Energy Commission's Division of Isotope Development.\n\nIn the 1930s, Aebersold participated in the first production and application of radioactive materials administered to humans (sodium and phosphorus).\n\nWhile working as an assistant to Ernest Lawrence, he administered the growth of the Radiation Laboratory at the University of California, Berkeley. He later worked at Oak Ridge and Los Alamos. Aebersold took measurements and conducted radiation-related research prior to and after the Trinity nuclear weapons test. After World War II, Aebersold returned to Oak Ridge where he rose to the position of Director of the Division of Isotopes Development.\n\nIn the 1960s, Aebersold's health began to decline, and he was hospitalised for a year and a half prior to his retirement. He committed suicide on 31 May 1967 by throwing himself from a 17-story building.\n\nAn award issued by the Society of Nuclear Medicine and Molecular Imaging was named in his memory. It was first presented in 1973 for \"Outstanding Achievement in Basic Nuclear Medicine Science\".\n"}
{"id": "39288501", "url": "https://en.wikipedia.org/wiki?curid=39288501", "title": "Planar transmission line", "text": "Planar transmission line\n\nPlanar transmission lines are transmission lines with conductors, or in some cases dielectric (insulating) strips, that are flat, ribbon-shaped lines. They are used to interconnect components on printed circuits and integrated circuits working at microwave frequencies because the planar type fits in well with the manufacturing methods for these components. Transmission lines are more than normal simple interconnections. With normal interconnections the propagation of the electromagnetic wave along the wire is fast enough to be considered instantaneous, and the voltage at each end of the wire can be considered identical. If the wire is longer than a large fraction of a wavelength (one tenth is often used as a rule of thumb) these assumptions are no longer true and transmission line theory must be used instead. At lower frequencies, these considerations are only necessary for the cables connecting different pieces of equipment, but at microwave frequencies the distance at which transmission line theory becomes necessary is measured in millimetres. Hence the need for transmission lines \"within\" a circuit.\n\nThere are several different types of planar transmission line. The earliest type of planar transmission line was conceived during World War II by Robert M. Barrett. It is known as stripline, and is one of the four main types in modern use, along with microstrip, suspended stripline, and coplanar waveguide*. All four of these types consist of a pair of conductors (although in three of them, the return conductor is through the ground plane). Consequently, they have a dominant mode of transmission (the mode is the field pattern of the electromagnetic wave) that is identical, or near-identical, to the mode found in a pair of wires. Other planar types, such as slotline, finline, and imageline, transmit along a strip of dielectric, and substrate integrated waveguide forms a dielectric waveguide within the substrate with rows of posts. These types cannot support the same mode as a pair of wires, and consequently they have different transmission properties. Many of these types have a narrower bandwidth and in general they produce more signal distortion than pairs of conductors. Their advantages depend on the exact types being compared, but can include low loss and a better range of characteristic impedance.\n\nPlanar transmission lines can be used for constructing components as well as interconnecting them. At microwave frequencies it is often the case that individual components in a circuit are themselves larger than a significant fraction of a wavelength. This means they can no longer be treated as lumped components, that is, treated as if they existed at a single point. Lumped passive components are often impractical at microwave frequencies for this reason, or because the values required are impractically small to manufacture. A pattern of transmission lines can be used for the same function as these components. Whole circuits, called distributed element circuits, can be built this way. The method is often used for filters. This method is particularly appealing for use with printed and integrated circuits because these structures can be manufactured with the same processes as the rest of the assembly simply by applying patterns to the existing substrate. This gives the planar technologies a big economic advantage over other types, such as coaxial cable.\n\nPlanar transmission lines are those transmission lines in which the conductors are essentially flat. The conductors consist of flat strips, and there are usually one or more ground planes parallel to the flat surface of the conductors. Transmission lines can also be constructed in non-planar formats such as wires or coaxial cable. As well as interconnections, there are a wide range of circuits that can be implemented in transmission lines. These include filters, power dividers, directional couplers, impedance matching networks, and choke circuits to deliver biasing to active components. The principal advantage of the planar types is that they can be manufactured using the same processes used to make printed circuits and integrated circuits, particularly through the photolithography process. The planar technologies are thus particularly well suited to mass production of such components.\n\nMaking circuit elements out of transmission lines is most useful at microwave frequencies. At lower frequencies the longer wavelength makes these components too bulky. At the higher microwave frequencies planar transmission line types are generally too lossy and waveguide is used instead. Waveguide, however, is more bulky and more expensive to manufacture. At still higher frequencies dielectric waveguide (such as optical fibre) becomes the technology of choice. However, there are planar types of dielectric waveguide. The most widely used planar types are stripline, microstrip, suspended stripline, and coplanar waveguide.\n\nAn important parameter for transmission lines is the mode of transmission employed. The mode describes the electromagnetic field patterns caused by the geometry of the transmission structure. It is possible for more than one mode to exist simultaneously on the same line. Usually, steps are taken to suppress all modes except the operational one. However, some devices, such as the dual-mode filter, rely on the transmission of more than one mode.\n\nThe mode found on ordinary conductive wires and cables is the transverse electromagnetic mode (TEM mode). This is also the dominant mode on some planar transmission lines. In the TEM mode the field strength vectors for the electric and magnetic field are both transverse to the direction of travel of the wave and orthogonal to each other. An important property of the TEM mode is that it can be used at low frequencies; all the way down to zero (DC).\n\nAnother feature of the TEM mode is that on an ideal transmission line (one that meets the Heaviside condition) there is no change of line transmission parameters (characteristic impedance and signal group velocity) with the frequency of transmission. Because of this, ideal TEM transmission lines do not suffer from a form of distortion called dispersion. Dispersion is where different frequency components travel at different velocities resulting in the wave shape (which may represent the transmitted information) becoming \"smeared out\" in the direction of the line length. All other modes suffer from dispersion which puts a limit on the bandwidth achievable.\n\nSome planar types, notably microstrip, do not have a homogeneous dielectric; it is different above and below the line. Such geometries cannot support a true TEM mode; there is some component of the electromagnetic field parallel to the direction of the line. However, the transmission can be \"nearly\" TEM. Such a mode is referred to as quasi-TEM. Discontinuities placed on a TEM line, such as gaps and posts, which are used to construct filters and other devices, have an impedance that is purely reactive. That is, they can store energy, but do not dissipate it. In most quasi-TEM lines, these structures additionally have a resistive component to the impedance. This resistance is a result of radiation from the structure and causes the circuit to be lossy. The same problem occurs at bends and corners of the line. These problems can be mitigated by using a high permittivity material as the substrate. This causes a higher proportion of the wave to be contained in the dielectric, making for a more homogeneous transmission medium and a mode closer to TEM.\n\nIn hollow metal waveguides and optical waveguides there are an unlimited number of other transverse modes that can occur. However, the TEM mode cannot be supported since it requires conductors to propagate. The transverse modes are classified as either \"transverse electric\" (TE) or \"transverse magnetic\" (TM) (also called respectively H and E modes) according to whether, respectively, all of the electric field, or all of the magnetic field is transverse. There is always a longitudinal component of one field or the other. The exact mode is identified by a pair of indices counting the number of wavelengths or half-wavelengths along specified transverse dimensions. These indices are usually written without a separator, for instance TE. The exact definition depends on whether the waveguide is rectangular, circular, or elliptical. For waveguide resonators a third index is introduced to the mode for wavelengths in the longitudinal direction.\n\nA feature of TE and TM modes is that there is a definite cutoff frequency below which transmission will not take place. The cutoff frequency depends on mode and the mode with the lowest cutoff frequency is called the \"dominant mode\". Multi-mode propagation is generally undesirable. Because of this, circuits are often designed to operate in the dominant mode at frequencies below the cutoff of the next highest mode. Only one mode, the dominant mode, can exist in this band.\n\nSome planar types that are designed to operate as TEM devices can also support TE and TM modes unless steps are taken to suppress them. The ground planes or shielding enclosures can behave as hollow waveguides and propagate these modes. Suppression can take the form of shorting screws between the ground planes or designing the enclosure to be too small to support frequencies as low as the operational frequencies of the circuit. This is similar to coaxial cable which can support circular TE and TM modes that do not require the centre conductor to propagate. Likewise, these modes can be suppressed by reducing the diameter of the cable.\n\nLongitudinal-section electric (LSE) modes and longitudinal-section magnetic (LSM) modes are hybrid modes that can occur in planar types with non-homogeneous transmission media. When determining whether a structure can support a particular TE mode, one sets the electric field in the direction (the longitudinal direction of the line) to zero and then solves Maxwell's equations for the boundary conditions set by the physical structure of the line. One can just as easily set the electric field in the direction to zero and ask what modes that gives rise to. Such modes are designated LSE modes. Similarly there can be LSE modes and, analogously for the magnetic field, LSM and LSM modes. It turns out that the LSE modes are a linear superposition of the corresponding TE and TM modes. In other words, in general, they have a longitudinal component of both electric and magnetic field. Likewise the LSM modes are found by setting one of the transverse components of magnetic field to zero with analogous results.\n\nWhen dealing with longitudinal-section modes, the TE and TM modes are sometimes written as LSE and LSM for a consistent set of notations. The LSE and LSM modes are subsets of hybrid electromagnetic (HEM) modes. There are some structures that are unable to support a pure TE or TM mode and consequently the transmission mode must necessarily be hybrid.\n\nThe characteristic impedance of a line is the impedance encountered by a wave travelling along the line. It depends only on the line geometry and materials and is not changed by the line termination. It is necessary to match the characteristic impedance of the planar line to the impedance of the systems to which it is connected. Furthermore, many filter designs require lines with a number of different characteristic impedances. Thus, it is an advantage for a technology to have a good range of achievable impedances. Narrow lines have a higher impedance than broad lines. The highest impedance achievable is limited by the resolution of the manufacturing process which imposes a limit on how narrow the lines can be made. The lower limit is determined by the width of line at which unwanted transverse resonance modes might arise.\n\n\"Q\" factor (or just \"Q\") is the ratio of energy stored to energy dissipated per cycle. It is the main parameter characterising the quality of resonators. In transmission line circuits, resonators are frequently constructed of transmission line sections to build filters and other devices. Their \"Q\" factor limits the steepness of the filter skirts and its selectivity. The main factors determining \"Q\" of a planar type are the permittivity of the dielectric (high permittivity increases \"Q\") and the dielectric losses, which decrease \"Q\". Other factors detracting from \"Q\" are the resistance of the conductor and radiation losses.\n\nThere are a wide range of substrates that are used with planar technologies. For printed circuits, glass-reinforced epoxy (FR-4 grade) is commonly used. High permittivity ceramic-PTFE laminates (e.g. Rogers Corporation 6010 board) are expressly intended for microwave applications. At the higher microwave frequencies, a ceramic material such as aluminium oxide (alumina) might be used for hybrid microwave integrated circuits (MICs). At the very highest microwave frequencies, in the millimetre band, a crystalline substrate might be used such as sapphire or quartz. Monolithic microwave integrated circuits (MMICs) will have substrates composed of the semiconductor material of which the chip is built such as silicon or gallium arsenide, or an oxide deposited on the chip such as silicon dioxide.\n\nThe electrical properties of the substrate of most interest are the relative permittivity (ε) and the loss tangent (). The relative permittivity determines the characteristic impedance of a given line width and the group velocity of signals travelling on it. High permittivity results in smaller printed components, aiding miniaturisation. In quasi-TEM types, permittivity determines how much of the field will be contained within the substrate and how much is in the air above it. The loss tangent is a measure of the dielectric losses. It is desirable to have this as small as possible, especially in circuits which require high \"Q\".\n\nMechanical properties of interest include the thickness and mechanical strength required of the substrate. In some types, such as suspended stripline and finline, it is advantageous to make the substrate as thin as possible. Delicate semiconductor components mounted on a flexing substrate can become damaged. A hard, rigid material such as quartz might be chosen as the substrate to avoid this problem, rather than an easier-to-machine board. In other types, such as homogeneous stripline, it can be much thicker. For printed antennae, that are conformal to the device shape, flexible, hence very thin, substrates are required. Thickness required for electrical performance depends on the permittivity of the material. Surface finish is an issue; some roughness may be required to ensure adhesion of the metallisation, but too much causes conductor losses. Thermal properties can be important. Thermal expansion changes the electrical properties of lines and can break plated through holes.\n\nStripline is a strip conductor embedded in a dielectric between two ground planes. It is usually constructed as two sheets of dielectric clamped together with the stripline pattern on one side of one sheet. The main advantage of stripline over its principal rival, microstrip, is that transmission is purely in the TEM mode and is free of dispersion, at least over the distances encountered in stripline applications. Stripline is capable of supporting TE and TM modes but these are not generally used. The main disadvantage is that it is not as easy as microstrip to incorporate discrete components. For any that are incorporated, cutouts have to be provided in the dielectric and they are not accessible once assembled.\n\nSuspended stripline is a type of air stripline in which the substrate is suspended between the ground planes with an air gap above and below. The idea is to minimise dielectric losses by having the wave travel through air. The purpose of the dielectric is only for mechanical support of the conductor strip. The mixed media of air and dielectric leads, in theory, to the transmission mode not being pure TEM. However, a thin dielectric renders this effect negligible. Suspended stripline is used in the mid microwave frequencies where it is superior to microstrip with respect to losses, but not as bulky or expensive as waveguide.\n\nThe idea of two conductor stripline is to compensate for air gaps between the two substrates. Small air gaps are inevitable because of manufacturing tolerances and the thickness of the conductor. These gaps can promote radiation away from the line between the ground planes. Printing identical conductors on both boards ensures the fields are equal in both substrates and the electric field in the gaps due to the two lines cancels out. Usually, one line is made slightly undersize to prevent small misalignments effectively widening the line, and consequently reducing the characteristic impedance.\n\nThe bilateral suspended stripline has more of the field in the air and almost none in the substrate leading to higher \"Q\", compared to standard suspended stripline. The disadvantage of doing this is that the two lines have to be bonded together at intervals less than a quarter wavelength apart. The bilateral structure can also be used to couple two independent lines across their broad side. This gives much stronger coupling than side-by-side coupling and allows coupled-line filter and directional coupler circuits to be realised that are not possible in standard stripline.\n\nMicrostrip consists of a strip conductor on the top surface of a dielectric layer and a ground plane on the bottom surface of the dielectric. The electromagnetic wave travels partly in the dielectric and partly in the air above the conductor resulting in quasi-TEM transmission. Despite the drawbacks of the quasi-TEM mode, microstrip is often favoured for its easy compatibility with printed circuits. In any case, these effects are not so severe in a miniaturised circuit.\n\nAnother drawback of microstrip is that it is more limited than other types in the range of characteristic impedances that it can achieve. Some circuit designs require characteristic impedances of or more. Microstrip is not usually capable of going that high so either those circuits are not available to the designer or a transition to another type has to be provided for the component requiring the high impedance.\n\nThe tendency of microstrip to radiate is generally a disadvantage to the type, but when it comes to creating antennae it is a positive advantage. It is very easy to make a patch antenna in microstrip, and a variant of the patch, the planar inverted-F antenna, is the most widely used antenna in mobile devices.\n\nSuspended microstrip has the same aim as suspended stripline; to put the field into air rather than the dielectric to reduce losses and dispersion. The reduced permittivity results in larger printed components, which detracts from miniaturisation, but is easier to manufacture. Suspending the substrate increases the maximum frequency at which the type can be used.\n\nInverted microstrip has similar properties to suspended microstrip with the additional benefit that most of the field is contained in the air between the conductor and the groundplane. There is very little stray field above the substrate available to link to other components. Trapped inverted microstrip shields the line on three sides preventing some higher order modes that are possible with the more open structures. Placing the line in a shielded box avoids completely any stray coupling but the substrate must now be cut to fit the box. Fabricating a complete device on one large substrate is not possible using this structure.\n\nCoplanar waveguide (CPW) has the return conductors on top of the substrate in the same plane as the main line, unlike stripline and microstrip where the return conductors are ground planes above or below the substrate. The return conductors are placed either side of the main line and made wide enough that they can be considered to extend to infinity. Like microstrip, CPW has quasi-TEM propagation.\n\nCPW is simpler to manufacture; there is only one plane of metallization and components can be surface mounted whether they are connected in series or shunt (that is, in parallel across the line). Shunt components in stripline and microstrip require a connection through to the bottom of the substrate. CPW is also easier to miniaturise; its characteristic impedance depends on the ratio of the line width to the distance between return conductors rather than the absolute value of line width.\n\nDespite its advantages, CPW has not proved popular. Its disadvantages include that return conductors take up a large amount of board area that cannot be used for mounting components. However, it is possible in some designs to achieve a greater density of components than microstrip. More seriously, there is a second mode in CPW that has zero frequency cutoff called the slotline mode. Since this mode cannot be avoided by operating below it, and multiple modes are undesirable, it needs to be suppressed. It is an odd mode, meaning that the electric potentials on the two return conductors are equal and opposite. Thus, it can be suppressed by bonding the two return conductors together. This can be achieved with a bottom ground plane (conductor backed coplanar waveguide, CBCPW) and periodic plated through holes, or periodic air bridges on the top of the board. Both these solutions detract from the basic simplicity of CPW.\n\nCoplanar strips (also \"coplanar stripline\" or \"differential line\") are usually used only for RF applications below the microwave band. The lack of a ground plane leads to a poorly defined field pattern and the losses from stray fields are too great at microwave frequencies. On the other hand, the lack of ground planes means that the type is amenable to embedding in multi-layer structures.\n\nA slotline is a slot cut in the metallisation on top of the substrate. It is the dual of microstrip, a dielectric line surrounded by conductor instead of a conducting line surrounded by dielectric. The dominant propagation mode is hybrid, quasi-TE with a small longitudinal component of electric field.\n\nSlotline is essentially a balanced line, unlike stripline and microstrip, which are unbalanced lines. This type makes it particularly easy to connect components to the line in shunt; surface mount components can be mounted bridging across the line. Another advantage of slotline is that high impedance lines are easier to achieve. Characteristic impedance increases with line width (compare microstrip where it decreases with width) so there is no issue with printing resolution for high impedance lines.\n\nOn the disadvantages of slotline, both characteristic impedance and group velocity vary strongly with frequency, resulting in slotline being more dispersive than microstrip. Slotline also has a relatively low \"Q\".\n\nAntipodal slotline is used where very low characteristic impedances are required. With dielectric lines, low impedance means narrow lines (the opposite of the case with conducting lines) and there is a limit to the thinness of line that can be achieved because of the printing resolution. With the antipodal structure, the conductors can even overlap without any danger of short-circuiting. Bilateral slotline has similar advantages to bilateral air stripline.\n\nSubstrate integrated waveguide (SIW), also called \"laminated waveguide\" or \"post-wall waveguide\", is a waveguide formed in the substrate dielectric by constraining the wave between two rows of posts or plated through holes and ground planes above and below the substrate. The dominant mode is a quasi-TE mode. SIW is intended as a cheaper alternative to hollow metal waveguide while retaining many of its benefits. The greatest benefit is that, as an effectively enclosed waveguide, it has considerably less radiation losses than microstrip. There is no unwanted coupling of stray fields to other circuit components. SIW also has high \"Q\" and high power handling. Additionally, as a planar technology, SIW is easier to integrate with other components.\n\nSIW can be implemented on printed circuit boards or as low temperature co-fired ceramic (LTCC). The latter is particularly suited to implementing SIW. Active circuits are not directly implemented in SIW, the usual technique is to implement the active part in stripline through a stripline to SIW transition. Antennae can be created directly in SIW by cutting slots in the ground planes. A horn antenna can be made by flaring the rows of posts at the end of a waveguide.\n\nThere is a SIW version of ridge waveguide. Ridge waveguide is a rectangular hollow metal waveguide with an internal longitudinal wall part way across the E-plane. The principal advantage of ridge waveguide is that it has a very wide bandwidth. Ridge SIW is not very easy to implement in printed circuit boards because the equivalent of the ridge is a row of posts that only go partway through the board. However, the structure can be created more easily in LTCC.\n\nFinline consists of a sheet of metallised dielectric inserted into the E-plane of a rectangular metal waveguide. However, the design is not intended to generate waveguide modes in the rectangular waveguide as such. Instead, a line is cut in the metallisation exposing the dielectric and it is this that acts as a transmission line. Finline is thus a type of dielectric waveguide and can be viewed as a shielded slotline.\n\nFinline is similar to ridge waveguide in that the metallisation of the substrate represents the ridge (the \"fin\") and the finline represents the gap. Filters can be constructed in ridge waveguide by varying the height of the ridge in a pattern. A common way of manufacturing these is to take a thin sheet of metal with pieces cut out (typically, a series of rectangular holes) and insert this in the waveguide in much the same way as finline. However, a finline filter is able to implement patterns of arbitrary complexity whereas the metal insert filter is limited by the need for mechanical support and integrity.\n\nFinline has been used at frequencies up to and experimentally tested to at least . At these frequencies it has a considerable advantage over microstrip for its low loss and it can be manufactured with similar low cost printed circuit techniques. It is also free of radiation due to being completely enclosed in the rectangular waveguide. A metal insert device has even lower loss because it is air dielectric, but has very limited circuit complexity. A full waveguide solution for a complex design retains the low loss of air dielectric, but it would be much more bulky than finline and significantly more expensive to manufacture. A further advantage of finline is that it can achieve a particularly wide range of characteristic impedances. Biasing of transistors and diodes cannot be achieved in finline by feeding bias current down the main transmission line, as is done in stripline and microstrip, since the finline is not a conductor. Separate arrangements have to be made for biasing in finline.\n\nUnilateral finline is the simplest design and easiest to manufacture but bilateral finline has lower loss for similar reasons to the advantages of bilateral suspended stripline. The high \"Q\" of bilateral finline often makes it the choice for filter applications. Antipodal finline is used where very low characteristic impedance is required. The stronger the coupling between the two planes, the lower the impedance. Insulated finline is used in circuits that contain active components needing bias lines. However, the \"Q\" of insulated finline is lower than other finline types so it is otherwise not usually used.\n\nImageline, also \"image line\" or \"image guide\", is a planar form of dielectric slab waveguide. It consists of a strip of dielectric, often alumina, on a metal sheet. In this type, there is no dielectric substrate extending in all horizontal directions, only the dielectric line. It is so called because the ground plane acts as a mirror resulting in a line that is equivalent to a dielectric slab without the ground plane of twice the height. It shows promise for use at the higher microwave frequencies, around , but it is still largely experimental. For instance \"Q\" factors in the thousands are theoretically possible but radiation from bends and losses in the dielectric-metal adhesive severely detract from this figure. A disadvantage of imageline is that the characteristic impedance is fixed at a single value of about .\n\nImageline supports TE and TM modes. The dominant TE and TM modes have a cutoff frequency of zero, unlike hollow metal waveguides whose TE and TM modes all have a finite frequency below which propagation cannot occur. As the frequency approaches zero, the longitudinal component of field diminishes and the mode asymptotically approaches the TEM mode. Imageline thus shares the property of being able to propagate waves at arbitrarily low frequencies with the TEM type lines, although it cannot actually support a TEM wave. However, imageline is not a suitable technology at lower frequencies. Imageline must be precisely machined as surface roughness increases radiation losses.\n\nIn insular imageline a thin layer of low permittivity insulator is deposited over the metal ground plane and the higher permittivity imageline is set on top of this. The insulating layer has the effect of reducing conductor losses. This type also has lower radiation losses on straight sections, but like the standard imageline, radiation losses are high at bends and corners. Trapped imageline overcomes this drawback, but is more complex to manufacture since it detracts from the simplicity of the planar structure.\n\nRibline is a dielectric line machined from the substrate as a single piece. It has similar properties to insular imageline. Like imageline, it must be precisely machined. Strip dielectric guide is a low permittivity strip (usually plastic) placed on a high permittivity substrate such as alumina. The field is largely contained in the substrate between the strip and the ground plane. Because of this, this type does not have the precise machining requirements of standard imageline and ribline. Inverted strip dielectric guide has lower conductor losses because the field in the substrate has been moved away from the conductor, but it has higher radiation losses.\n\nMultilayer circuits can be constructed in printed circuits or monolithic integrated circuits, but LTCC is the most amenable technology for implementing planar transmission lines as multilayers. In a multilayer circuit at least some of the lines will be buried, completely enclosed by dielectric. The losses will not, therefore, be as low as with a more open technology. However, very compact circuits can be achieved with multilayer LTCC.\n\nDifferent parts of a system may be best implemented in different types. Transitions between the various types are therefore required. Transitions between types using unbalanced conductive lines are straightforward enough: this is mostly just a matter of providing continuity of the conductor through the transition and ensuring a good impedance match. The same can be said for transitions to non-planar types such as coaxial. A transition between stripline and microstrip needs to ensure that both ground planes of the stripline are adequately electrically bonded to the microstrip ground plane. One of these groundplanes can be continuous through the transition, but the other ends at the transition. There is a similar issue with the microstrip to CPW transition shown at C in the diagram. There is only one ground plane in each type but it changes from one side of the substrate to the other at the transition. This can be avoided by printing the microstrip and CPW lines on opposite sides of the substrate. In this case the ground plane is continuous on one side of the substrate but a via is required on the line at the transition.\n\nTransitions between conductive lines and dielectric lines or waveguides are more complex. In these cases a change of mode is required. Transitions of this sort consist of forming some kind of antenna in one type that acts as a launcher into the new type. Examples of this are coplanar waveguide (CPW) or microstrip converted to slotline or substrate integrated waveguide (SIW). For wireless devices, transitions to the external antennae are also required.\n\nTransitions to and from finline can be treated in a similar way to slotline. However, it is more natural for finline transitions to go to waveguide; the waveguide is already there. A simple transition into waveguide consists of a smooth exponential taper (Vivaldi antenna) of the finline from a narrow line to the full height of the waveguide. The earliest application of finline was to launch into circular waveguide.\n\nA transition from a balanced to an unbalanced line requires a balun circuit. An example of this is CPW to slotline. Example D in the diagram shows this kind of transition and features a balun consisting of a dielectric radial stub. The component shown thus in this circuit is an air bridge bonding the two CPW ground planes together. All transitions have some insertion loss and add to the complexity of the design. It is sometimes advantageous to design with a single integrated type for the whole device to minimise the number of transitions even when the compromise type is not optimal for each of the component circuits.\n\nThe development of planar technologies was driven at first by the needs of the US military, but today it can be found in household mass-produced items such as mobile phones and satellite TV receivers. According to Thomas H. Lee, Harold A. Wheeler may have experimented with coplanar lines as early as the 1930s. However, the first documented planar transmission line was stripline, invented by Robert M. Barrett and published by Barrett and Barnes in 1951. Although publication did not occur until the 1950s, stripline had actually been used during World War II. According to Barrett, the first stripline power divider was built by V. H. Rumsey and H. W. Jamieson during this period. Microstrip followed soon after in 1952 due to Grieg and Englemann. However, the quality of common dielectric materials was at first not good enough for microwave circuits and consequently their use did not become widespread until the 1960s. Stripline and microstrip were commercial rivals. \"Stripline\" was the brand name of Airborne Instruments Laboratory Inc. (AIL) who made air stripline. Microstrip was made by ITT. Later, dielectric filled stripline under the brand name \"triplate\" was manufactured by Sanders Associates. \"Stripline\" became a generic term for dielectric filled stripline and \"air stripline\" or \"suspended stripline\" is now used to distinguish the original type.\n\nStripline was initially preferred to its rival because of the dispersion issue. However, in the 1960s, the need to incorporate miniature solid-state components in MICs swung the balance to microstrip. Miniaturisation also leads to favouring microstrip because its disadvantages are not so severe in a miniaturised circuit. However, stripline is still chosen where operation over a wide band is required. The first planar slab dielectric line, imageline, is due to King in 1952. Slotline, the first printed planar dielectric line type, is due to Cohn in 1968. Coplanar waveguide is due to Wen in 1969. Finline, as a printed technology, is due to Meier in 1972. However, Robertson created finline-like structures much earlier (1955–56) with metal inserts. Robertson fabricated circuits for diplexers and couplers and coined the term \"finline\".\n\nAt first, components made in planar types were made as discrete parts connected together, usually with coaxial lines and connectors. However, it was quickly realised that the size of circuits could be hugely reduced by directly connecting components together with planar lines within the same housing. This led to the concept of hybrid MICs: \"hybrid\" because lumped components were included in the designs connected together with planar lines. From the 1970s, there has been a great proliferation in new variations of the basic planar types to aid miniaturisation and mass production. Further miniaturisation became possible with the introduction of MMICs. In this technology, the planar transmission lines are directly incorporated in the semiconductor slab in which the integrated circuit components have been manufactured. The first MMIC, an X band amplifier, is due to Pengelly and Turner of Plessey in 1976.\n\nA small selection of the many circuits that can be constructed with planar transmission lines are shown in the figure. Such circuits are a class of distributed element circuits. Microstrip and slotline types of directional couplers are shown at A and B respectively. Generally, a circuit form in conducting lines like stripline or microstrip has a dual form in dielectric line such as slotline or finline with the roles of the conductor and insulator reversed. The line widths of the two types are inversely related; narrow conducting lines result in high impedance, but in dielectric lines the result is low impedance. Another example of dual circuits is the bandpass filter consisting of coupled lines shown at C in conductor form and at D in dielectric form.\n\nEach section of line acts as a resonator in the coupled lines filters. Another kind of resonator is shown in the SIW bandpass filter at E. Here posts placed in the centre of the waveguide act as resonators. Item F is a slotline hybrid ring featuring a mixture of both CPW and slotline feeds into its ports. The microstrip version of this circuit requires one section of the ring to be three-quarters wavelength long. However, in the slotline/CPW version all sections are one-quarter wavelength because there is a 180° phase inversion at the slotline junction.\n\n"}
{"id": "1970574", "url": "https://en.wikipedia.org/wiki?curid=1970574", "title": "Potato ricer", "text": "Potato ricer\n\nA potato ricer (also called a ricer) is a kitchen implement used to process potatoes or other food by forcing it through a sheet of small holes, which are typically about the diameter of a grain of rice.\n\nA common variety of potato ricer resembles a large garlic press. It has two long handles, one with a perforated basket at the end, the other with a flat surface that fits into the basket. The food is placed in the basket, then the flat surface is pushed down into the basket by pressing the handles together, forcing the food through the holes.\n\nAnother form, sometimes called a rotary ricer, is cone-shaped with small perforations all around the cone. It comes with a wooden pestle that is used to push the food through the holes.\n\nA food mill can be used as a substitute for a ricer.\n\nThis tool is commonly used to rice potatoes, a process that forces cooked potatoes through the ricer and turns the potatoes into fine, thin shreds. The resulting potatoes are lighter and fluffier. The process allows the full starch cells of high-starch potatoes to maintain their integrity and stay separate, giving the potatoes a fluffy, full texture. At this point, milk, butter, and other additives can easily be blended to maintain the starch structure, the result being consistent mashed potatoes. The process works since uniform texture is created due to the passing of potatoes through evenly sized holes, which ensures that the potatoes are smashed only once. With this method, the cell walls are much less likely to break open.\n\nPressing cooked vegetables and fruits through the small holes produces a puree comparable to using a drum sieve. Many foods can now be pureed more easily in a food processor; however, a manual method such as ricing is best for potatoes, which are starchy and become glutinous when over-processed. Ricers are often used to puree food for babies.\n\nA ricer can be used to remove excess water from foods such as cooked greens that are to be added to quiche, thawed frozen spinach, and sliced or grated potatoes to improve the quality of potato chips or hash browns made from them.\n\nRicers are also used to make \"Mont Blanc\" (a dessert of chestnut puree), \"lefse\" (a Norwegian) flatbread, spätzle (German noodles), \"passatelli\" (a type of Italian pasta), and process ice cream when making the German dish \"spaghettieis\".\n\nPotters and ceramicists use ricers to extrude \"hair\" for human heads or animal sculptures such as sheep.\n\nPotato ricers are seen by some to be too bulky, and an unnecessary kitchen utensil. Many opponents of potato ricers will point out that mashing potatoes does a similar job.\n\n"}
{"id": "51230831", "url": "https://en.wikipedia.org/wiki?curid=51230831", "title": "Professional Engineers Day (U.S.)", "text": "Professional Engineers Day (U.S.)\n\nProfessional Engineers Day was launched by the National Society of Professional Engineers in 2016 to celebrate and raise public awareness of the contributions of licensed professional engineers in the United States. As of 2015, there were 474,777 licensed professional engineers in the U.S. The first Professional Engineers Day was celebrated on August 3, 2016.\n\nThe idea for Professional Engineers Day came from Tim Austin, PE, a professional engineer from Kansas who served as president of the National Society of Professional Engineers in 2015-16. While promotion of engineering in the US is common, such as the attention given to STEM fields and events such as the USA Science and Engineering Festival and National Engineers Week (U.S.), which was also founded by NSPE in 1951, Austin believed attention should be paid specifically to the contributions of licensed professional engineers because of NSPE's core principle which states, \"Being a licensed professional engineer means more than just holding a certificate and possessing technical competence. It is a commitment to hold the public health, safety, and welfare above all other considerations.\"\n\nLicensing of professional engineers in the US began in 1907, when Clarence Johnston, the state engineer of Wyoming, presented a bill to the Wyoming legislature that would require registration for those representing themselves to the public as an engineer or land surveyor. The bill was later enacted, making the state the first in the US to register engineers and land surveyors.\n\nOn August 8, 1907, Charles Bellamy of Wyoming received the first professional engineering license. Professional Engineers Day is held the first Wednesday in August to mark that occasion. Incidentally, Bellamy’s wife, Mary Godot Bellamy, is also known for a first: the first woman elected to the Wyoming legislature.\n\nCharles Bellamy founded Bellamy & Sons Engineers in 1913.\n\nThe Bellamy Chapter of the Wyoming Society of Professional Engineers is named in his honor.\n\nToday, for the purpose of protecting the public, all US states and territories license professional engineers.\n\nTo mark the inaugural Professional Engineers Day in 2016, governors from Kansas, Ohio, Oklahoma, Louisiana, and Wisconsin signed proclamations recognizing the contributions licensed professional engineers make to their states and society.\n\n"}
{"id": "4481763", "url": "https://en.wikipedia.org/wiki?curid=4481763", "title": "Renewable natural gas", "text": "Renewable natural gas\n\nRenewable Natural Gas (RNG), also known as Sustainable Natural Gas (SNG) or biomethane, is a biogas which has been upgraded to a quality similar to fossil natural gas and having a methane concentration of 90% or greater. A biogas is a gaseous form of methane obtained from biomass. By upgrading the quality to that of natural gas, it becomes possible to distribute the gas to customers via the existing gas grid within existing appliances. Renewable natural gas is a subset of synthetic natural gas or substitute natural gas (SNG).\n\nRenewable natural gas can be produced and distributed via the existing gas grid, making it an attractive means of supplying existing premises with renewable heat and renewable gas energy, while requiring no extra capital outlay of the customer. Renewable natural gas can be converted into liquefied natural gas (LNG) for direct use as fuel in transport sector. LNG pricing compete with gasoline or diesel as it can replace these fuels in the transport sector.\n\nThe existing gas network allows distribution of gas energy over vast distances at a minimal cost in energy. Existing networks would allow biogas to be sourced from remote markets that are rich in low-cost biomass (Russia or Scandinavia for example).\n\nThe UK National Grid believes that at least 15% of all gas consumed could be made from matter such as sewage, food waste such as food thrown away by supermarkets and restaurants and organic waste created by businesses such as breweries.\n\nIn the US, analysis conducted in 2011 by the Gas Technology Institute determined that renewable gas from waste biomass including agricultural waste has the potential to add up to 2.5 quadrillion Btu annually, being enough to meet the natural gas needs of 50% of American homes. \n\nIn combination with power-to-gas, whereby the carbon dioxide and carbon monoxide fraction of biogas are converted to methane using electrolyzed hydrogen, the renewable gas potential of raw biogas is approximately doubled. Many ways of methanising carbon dioxide/monoxide and hydrogen exist, including biomethanation, the sabatier process and a new electrochemical process pioneered in the United States currently undergoing trials.\n\nA biomass to SNG efficiency of 70% can be achieved. Costs are minimised by maximising production scale and by locating an anaerobic digestion plant next to transport links (e.g. a port or river) for the chosen source of biomass. The existing gas storage infrastructure would allow the plant to continue to manufacture gas at the full utilisation rate even during periods of weak demand, helping minimise manufacturing capital costs per unit of gas produced.\n\nRenewable gas can be produced through three main processes\n\nGöteborg Energi opened the first demonstration plant for large scale production of bio-SNG through gasification of forest residues in Gothenburg, Sweden within the GoBiGas project. The plant had the capacity to produce 20 megawatts-worth of bioSNG from about 30 MW-worth of biomass, aiming at a conversion efficiency of 65%. From December 2014 the bioSNG plant was fully operational and supplied gas to the Swedish natural gas grid, reaching the quality demands with a methane content of over 95%. The plant was permanently closed due to economic problems in April 2018. Göteborg Energi had invested 175 million euro in the plant and intensive attempts for a year to sell the plant to new investors had failed.\n\nIt can be noted that the plant was a technical success, and performed as intended. However, natural gas is currently at a very low price given market conditions globally. It is expected the plant is to re-emerge around 2030 when economic conditions may be more favourable, with the possibility of a higher carbon price.\n\nSNG is of particular interest in countries with extensive natural gas distribution networks. Core advantages of SNG include compatibility with existing natural gas infrastructure, higher efficiency that Fisher-Tropsch fuels production and smaller-production scale than other second generation biofuel production systems. The Energy Research Centre of the Netherlands has conducted extensive research on large-scale SNG production from woody biomass, based on the importation of feedstocks from abroad.\n\nRenewable natural gas plants based on wood can be categorised into two main categories, one being allothermal, which has the energy provided by a source outside of the gasifier. One example is the double-chambered fluidised bed gasifiers consisting of a separate combustion and gasification chambers. Autothermal systems generate the heat within the gasifier, but require the use of pure oxygen to avoid nitrogen dilution.\n\nIn the UK, NNFCC found that any UK bioSNG plant built by 2020 would be highly likely to use ‘clean woody feedstocks' and that there are several regions with good availability of that source.\n\nIn the UK, using anaerobic digestion is growing as a means of producing renewable biogas, with nearly 90 biomethane injection sites built across the country. Ecotricity announced plans to supply green gas to UK consumers via the national grid. Centrica also announced that it would begin injecting gas, manufactured from sewage, into the gas grid. In Canada, FortisBC, a gas provider in British Columbia, injects renewably created natural gas into its existing gas distribution system.\n\nSustainable SNG is produced by high temperature oxygen blown slagging co-gasification at 70 to 75 bar pressure of biomass or waste residue. The advantage of a wide range of feedstocks is that much larger quantities of renewable SNG can be produced compared with biogas, with fewer supply chain limitations. A wide range of fuels with an overall biogenic carbon content of 50 to 55% is technically and financially viable. Hydrogen is added to the fuel mix during the gasification process, and carbon dioxide is removed by capture from the purge gas 'slip stream' syngas clean-up and catalytic methanation stages.\n\nLarge scale sustainable SNG will enable the UK gas and electricity grids to be substantially de-carbonised in parallel at source, while maintaining the existing operational and economic relationship between the gas and electricity grids. Carbon capture and sequestration can be added at little additional cost, thereby progressively achieving deeper de-carbonisation of the existing gas and electricity grids at low cost and operational risk. Cost benefit studies indicate that large scale 50% biogenic carbon content sustainable SNG can be injected into the high pressure gas transmission grid at a cost of around 65p/therm. At this cost, it is possible to re-process fossil natural gas, used as an energy input into the gasification process, into 5 to 10 times greater quantity of sustainable SNG. Large scale sustainable SNG, combined with continuing natural gas production from UK continental shelf and unconventional gas, will potentially enable the cost of UK peak electricity to be de-coupled from international oil denominated 'take or pay' gas supply contracts.\n\nApplications:\n\n\nIn contrast to being renewable, biogas creates similar environmental pollutants as ordinary natural gas fuel, such as carbon monoxide, sulfur dioxide, nitrogen oxide, hydrogen sulfide and particulates. Any unburned gas that escapes contains methane, a long lived greenhouse gas. The key difference from ordinary natural gas is that it is often considered partly or fully carbon neutral, the carbon dioxide contained in the biomass is naturally renewed in each generation of plants, rather than being released from fossil stores and increasing atmospheric carbon dioxide.\n\n\n"}
{"id": "21853209", "url": "https://en.wikipedia.org/wiki?curid=21853209", "title": "Revere Ware", "text": "Revere Ware\n\nRevere Ware is a line of consumer and commercial kitchen wares introduced in 1939 by the Revere Brass & Copper Corp. Focusing primarily on consumer cookware such as (but not limited to) skillets, sauce pans, stock pots, and tea kettles. Initially Revere Ware was the culmination of various innovative techniques developed during the 30's, the most popular being construction of Stainless Steel with rivetlessly attached bakelite handles, copper clad bases and rounded interiors for ease of cleaning. Over the next 40+ years, Revere Ware would introduce new series to position itself in competition with other manufacturers at various price points, or for specific specialty markets. In the early 60's the profitability of Revere Ware began to level off. Coinciding with new series introductions, cost cutting measures were implemented in the manufacture of the traditional cookware. The bakelite handles were changed from two piece to one, and the thickness of utensil walls and copper cladding were reduced.\n\nWhile the cookware division remained profitable, the seventies saw parent company Revere Brass & Copper Corp. experience a shift of fortune. Transferring of its aluminum production from domestic to overseas manufacturing marked the beginning of the end. By 1982 financial issues due to these failing aluminum operations forced Revere Brass & Copper to enter solvency. By 1985 the cookware subsidiary Revere Ware Incorporated, which had remained profitable, had been sold to Corning Glass Inc.. Within ten years Corning Glass inc. had expanded Revere Ware from the four lines at their acquisition, to over dozen lines, while ceasing any domestic manufacturing. In 1998 World Kitchen became the controlling parent company of Corning. During this period Revere Ware suffered from branding incoherency, with nearly a dozen new \"Lines\" introduced by 2006 before briefly leaving the market. Revere Ware has since been reintroduced, as World Kitchen currently (as of 2016) offers select variations; Copper-cored Stainless Steel, traditional copper-clad bottomed cookware and anodized non-stick aluminum.\n\nRevere Ware has introduced several lines since its inception. The main series has always been the 1400 line, featuring the classic curved, smooth knurled bakelite handles, stainless steel walls and copper bottoms. The Patriot Ware series functioned as Revere Wares economy line, briefly being marketed directly by mail order through J.C. Penney, with an exclusive \"Made Expressly for Penney's by Revere\" trademark. Copper Maid was another economy series introduced in 1957. In 1959, in response to leveling sales, the Designers' Group series was introduced, intended to reestablish Revere Ware as the premiere consumer cookware. The series would be sold until 1973, having some cross over with the Designer's Group products introduced in 1969. While historians and aficionados cite the 6000 Line Designers' Group pieces as the finest cookware manufactured by Revere Ware, the 6500 Line Designer's Group was a confusing addition that sold poorly.\n\nOther lines introduced to expand the Revere Ware brand were the heavy duty Institutional line, substituting Stainless Steel handles for the bakelites. A selection of this series was reintroduced as Patio Ware in 1956, attempting to cash in on the barbecue craze of 50's. The larger, more rugged handles of the 1800 line were marketed as made specifically for men's larger hands. Various aluminum products were also introduced, such as the Galaxy line. The Galaxy line featured Stainless Steel construction with a bonded aluminum lining. The 500 line was a series of miniature 1400 series cookware, marketed as toys for children, but manufactured to the same standards as all their consumer cookware. The mini sets, which were available through 1983, have become quite collectible.\n\nPrior to the sale of Revere Ware to Corning Glass inc in 1985, the brand offered or had in development four series. The traditional 1400 series continued to sell well. The newly developed 2000 line, for use on the then-new 80's technology of Ceramic and Glass top stoves, featured heavy aluminum bottoms for good heat transfer. Both the ONXY line and the Micro-Fryer line were released after the sale, but were discontinued almost immediately as they were perceived as being in direct competition with existing products from subsidiaries of Corning.\n\n\nThe first series developed and introduced after the Corning Glass inc acquisition was the Proline, a heavy gauge Stainless Steel product that would go through many arbitrary, minor changes during its run. Minor changes would happen frequently across all their lines, undermining consumer confidence. The Vista series was a reintroduction of the 1400 series, again with minor changes and now sold with glass lids taken from other Corning product lines. The Spectrum series featured aluminum non-stick pans with colored enamel bodies, and were imported from Thailand. only four years after the Vista Line hit store shelves, the Centura series was yet another reintroduction of the traditional 1400 line. The Centura series sold with less expensive materials, being made overseas, and were offered exclusively from Wal-Mart. Most other series introduced were contemporaries of other lines from Corning subsidiaries.\n\nIn 1998 Revere Ware, along with all of the divisions of Corning Consumer Products, were reorganized under the World Kitchen inc banner. While a handful of unique lines emerged after this restructure, it was three lines in particular that were significant as reintroductions, despite being produced overseas, and using less expensive materials. These series were the Tri-Ply Stainless series, based on the 7000 Stainless Revere Line introduced in 1974, the Copper Select series based on the Paul Revere Ware line of 1967, and finally the Copper Clad series of 2003, which was yet another reintroduction of the 1400 line.\n\n\nThe 1400 line is manufactured with all of Revere Wares defining features. Copper-clad bases, rounded corners for ease of cleaning, bakelite handles, and Vapor Seal lids. Early pans and skillets featured two piece handles, held together by rivets and screws. The earliest style of bakelite handles feature two screws, just a little more than an inch apart, near the pan side of the handle. Later, a screw was located at either end of the handle. By 1968, in a cost cutting measure, the bakelite was made from one piece and pressed onto the attached metal handle.\n\nWhen identifying various utensils, know that skillets are always of a larger diameter, with a shallow depth and a single long handle. Sauce pans will have a relative depth, and also a single long handle. Stock Pots are generally four quarts or larger, and have two smaller handles for grasping at each side. Dutch Ovens are similarly constructed, but feature a flared edge at the top of the pot with a narrow seat for their larger, more pronouncedly domed lid to seal. \"The domed lids used for Dutch Ovens (and square Skillets) are distinctly different in shape than that of the traditional bell-lipped Stainless Steel lids. The walls of these Domed lids are noticeably taller than standard utensil lids.\"\n\nRevere Ware double boilers and steamers come in three variations. The first (marked (1) below) is an insert, which nestles into the pan, supported by a lip and two small \"ears\", or flairs at the mouth of the insert. This variety remains almost entirely within the heating utensil while in use. The second (marked (2) below)is a ribbed design with a single handle, similar to a Sauce Pan in construction. Only a small portion of the Steamer/Double Boiler rests within the Heating utensil, to create a seal, while the remainder protrudes. The last variety (marked (3) below) is also ribbed, but features two small handles (much like stock pots) on either side, to facilitate ease of use.\n\nBeing the longest running & cornerstone line of wares produced by Revere Ware, the 1400 series is an iconic part of American kitchens. At the height of 1400's popularity, nearly 5 and a half million pieces were shipping a year. It was the \"Gold Standard\" of American cookware, at its peak offering 39 items simultaneously (counting lids as separate pieces) across 12 distinct utensil types. While specialty items and minor revisions were occasionally made to the line, the 1400 series existed with a relative consistency before the sale to Corning Glass inc. in '85. The most iconic pieces are listed below.\n\nNotes on Specialty items: The 1 qt. Combination Pan can be identified by its straight walled construction, opposed to a Skillet's flared walls, and will be stamped \"1 qt.\" on post-1968 pieces.\n\nEgg Poaching inserts and removable cups (1515 and 1520), either four or six, are placed into correspondingly sized Skillets. The poaching cup handles are designed with an exclusive \"lock on\" handle, and accept most any household fork for lifting.\n\nOmelette Skillets have a specially designed domed shape, with nearly transparent transitions from pan-base to walls. This surface facilitates the ease of omelette preparing. As the primary purpose of the pan is omelettes, it is not intended for use with a lid.\n\nHow to properly measure Revere Ware diameters: \"Pot and pan diameters\" are measured from the interior of the walls. Measurements taken from the exterior of the walls often include the extra material of the rolled lip, giving the illusion of an additional quarter inch of width. As an example, re-sellers and collectors of vintage Revere Ware may measure a utensil as being 9.25\" or 9 1/4\", when in fact the pot should properly be measured 9\". Skillets may also be improperly measured, as the cooking surface is slightly smaller than the marked or measured size. The marked skillet dimensions refer to interior circumference at the top most part of the flared walls, as the pans are designed to use a lid that coincides with this diameter. \"Lid Diameters\" can also lend to confusion, as the correct Stainless Steel lids of vintage (pre-Corning) Revere Ware have belled lips. These belled lips facilitate the oft-advertised \"Vapor Seal\", a method of locking in moisture, which some believe retain nutrients. Re-sellers and collectors, when measuring the inner circumference of the bell flange, often measure the lids an eighth to a quarter of an inch narrower than proper size, or conversely, measure the outside of the bell flair, adding an additional 1/4\" to 3/8\" of an inch of diameter. A good rule of thumb is knowing all lids and utensils are designed to be measured at whole-inch increments, with the exception of 5.5\" sauce pans and lids.\n\nRevere Ware is considered a highly collectible brand of cookware, much like certain cast iron cookware brands. Like all collectibles, some pieces have become more desirable than others. There have been many different lines and subtle changes incurred since 1939, yet most utensils retain a particularly universal look. Because of this, it can be tricky to identify specific years of pieces, especially within the 1400 line. Thankfully, there are easy ways to identify the most desirable periods.\n\nFeaturing far thicker copper cladding and steel walls, items produced prior to 1968 are the pieces most collectors seek. Most are identifiable by a \"Double Ring\" Trademark. The earliest Revere Ware products, produced in 1939, may have a Trademark that includes the name Riding Revere, as initially Revere Ware had not decided on the branding of their new product. Despite securing a patent for their Copper Cladding process in 1942, pieces from 1939–1946 featured a \"Pat. Pend.\" stamp underneath the logo. From 1946–1968 pieces feature the most prominent Trademark logo, the large double ring containing the Revere head outline, Copper Clad – Stainless Steel and Revere Ware. Below are the words \"Made under Process Patent\". Production initially all took place in Rome, NY, but starting in 1949 expanded to California and then Illinois. Items made at these plants included additions to the Trademark; \"Riverside, CA\" or \"Clinton, IL\". Pieces made in Rome, NY are identified by not including a place of manufacture, prior to 1974.\n\nStarting in 1968, the trademarking on the bottom of Revere Ware Utensils became simplified. Gone was the distinct \"Double Ring\" marking, and any mention of patent processes. This coincided with the decision to reduce the thickness of both the Copper Cladding, and the stainless steel walls of their products. While reducing the material used for manufacture proved a resourceful way to cut costs, it has been recognized as noticeably lowering quality and durability. Still, Revere Ware remained in demand, and pieces from the 1968–86 era continue to be popular entry items for collectors. The new, simplified trademark featured the familiar Revere silhouette, flanked by 1801, the year of the companies origin. Below this, in stark lettering, was printed \"REVERE WARE\", with an identifying marker for the size of the utensil (a new feature) and the location of manufacture.\n"}
{"id": "2383902", "url": "https://en.wikipedia.org/wiki?curid=2383902", "title": "Safety lamp", "text": "Safety lamp\n\nA safety lamp is any of several types of lamp that provides illumination in coal mines and is designed to operate in air that may contain coal dust or gases both of which are potentially flammable or explosive. Until the development of effective electric lamps in the early 1900s miners used flame lamps to provide illumination. Open flame lamps could ignite flammable gases which collected in mines, causing explosions and so safety lamps were developed to enclose the flame and prevent it from igniting the surrounding atmosphere. Flame safety lamps have been replaced in mining with sealed explosion-proof electric lights.\n\nMiners have traditionally referred to the various gases encountered during mining as damps, from the Middle Low German word \"dampf\" (meaning \"vapour\"). Damps are variable mixtures and are historic terms.\n\n\nBefore the invention of safety lamps, miners used candles with open flames. This gave rise to frequent explosions. For example, at one colliery (Killingworth) in the north east of England, 10 miners were killed in 1806, 12 in 1809. In 1812, 90 men and boys were suffocated or burnt to death in the Felling Pit near Gateshead and 22 in the following year.\n\nWhen they came into regular use, barometers were used to tell if atmospheric pressure was low which could lead to more firedamp seeping out of the coal seams into the mine galleries. Even after the introduction of safety lamps this was still essential information, see Trimdon Grange for details of an accident where pressure was involved.\n\nThe lack of good lighting was a major cause of the eye affliction nystagmus. Miners working in thin seams or when undercutting the coal had to lie on their side in cramped conditions. The pick was swung horizontally to a point beyond the top of their head. In order to see where they were aiming (and accurate blows were needed), the eyes needed to be straining in what would normally be the upwards and slightly to one side direction. This straining led first to temporary nystagmus and then to a permanent disability. Mild nystagmus would self-correct if a miner ceased to perform this work but if left untreated would force a man to give up mining. The lower levels of light associated with safety lamps caused an increase in the incidence of nystagmus.\n\nBoth on the continent of Europe and in the UK dried fish skins were used. From them a faint bioluminescence (often called phosphorescence) occurs. Another safe source of illumination in mines was bottles containing fireflies.\n\nFlint and steel mills introduced by Carlisle Spedding (1696-1755) before 1733 had been tried with limited success. An example of a Spedding steel mill may be seen in the museum at Whitehaven where Spedding was manager of the collieries of Sir James Lowther, 4th Baronet. A steel disk was rotated at high speed by a crank mechanism. Pressing a flint against the disk produced a shower of sparks and dim illumination. These mills were troublesome to use and were often worked by a boy, whose only task was to provide light for a group of miners. It was assumed that the sparks had insufficient energy to ignite firedamp until a series of explosions at Wallsend colliery in 1784; a further explosion in June 1785 which the operator of the mill (John Selkirk) survived showed that ignition was possible.\n\nThe first safety lamp made by William Reid Clanny used a pair of bellows to pump air through water to a candle burning in a metal case with a glass window. Exhaust gases passed out through water. The lamp gave out only a weak light though it was intrinsically safe provided it was kept upright. It was heavy and ungainly and required a man to pump it continuously. It was not a practical success and Clanny subsequently changed the basis of operation of later lamps in the light of the Davy and Stephenson lamps.\n\nSafety lamps have to address the following issues:\n\nFire requires three elements to burn: fuel, oxidant and heat; the triangle of fire. Remove one element of this triangle and the burning will stop. A safety lamp has to ensure that the triangle of fire is maintained inside the lamp, but cannot pass to the outside.\n\n\nIn the Geordie lamp the inlet and exhausts are kept separate. Restrictions in the inlet ensure that only just enough air for combustion passes through the lamp. A tall chimney contains the spent gases above the flame. If the percentage of firedamp starts to rise, less oxygen is available in the air and combustion is diminished or extinguished. Early Geordie lamps had a simple pierced copper cap over the chimney to further restrict the flow and to ensure that the vital spent gas did not escape too quickly. Later designs used gauze for the same purpose and also as a barrier in itself. The inlet is through a number of fine tubes (early) or through a gallery (later). In the case of the gallery system air passes through a number of small holes into the gallery and through gauze to the lamp. The tubes both restrict the flow and ensure that any back flow is cooled. The flame front travels more slowly in narrow tubes (a key Stephenson observation) and allows the tubes to effectively stop such a flow.\n\nIn the Davy system a gauze surrounds the flame and extends for a distance above forming a cage. All except the very earliest Davy lamps have a double layer at the top of the cage. Rising hot gases are cooled by the gauze, the metal conducting the heat away and being itself cooled by the incoming air. There is no restriction on the air entering the lamp and so if firedamp is entrained it will burn within the lamp itself. Indeed, the lamp burns brighter in dangerous atmospheres thus acting as a warning to miners of rising firedamp levels. The Clanny configuration uses a short glass section around the flame with a gauze cylinder above it. Air is drawn in and descends just inside the glass, passing up through the flame in the centre of the lamp.\n\nThe outer casings of lamps have been made of brass or tinned steel. If a lamp bangs against a hard piece of rock a spark could be generated if iron or untinned steel were employed.\n\nWithin months of Clanny's demonstration of his first lamp, two improved designs had been announced: one by George Stephenson, which later became the Geordie lamp, and the Davy lamp, invented by Sir Humphry Davy. Subsequently, Clanny incorporated aspects of both lamps and produced the ancestor of all modern oil safety lamps.\n\nGeorge Stephenson came from a mining family and by 1804 had secured the post of brakesman at Killingworth colliery. He was present at both the 1806 and 1809 explosions in the pit. By 1810, he was engineman and responsible for machinery both above and below ground. The pit was a gassy pit and Stephenson took the lead in work to extinguish a fire in 1814. For some years prior to 1815 he had been experimenting on the \"blowers\" or fissures from which gas erupted. He reasoned that a lamp in a chimney could create a sufficient updraft that firedamp would not penetrate down the chimney. Further observations of the speed of flame fronts in fissures and passageways led him to design a lamp with fine tubes admitting the air.\n\nSir Humphry Davy was asked to consider the problems of a safety lamp following the Felling explosion. Previous experimenters had used coal gas (chiefly carbon monoxide) incorrectly, believing it to be the same as firedamp. Davy however performed his experiments with samples of firedamp collected from pits. As an experimental chemist, he was familiar with flames not passing through gauze; his experiments enabled him to determine the correct size and fineness for a miner's lamp.\n\nDavy was awarded the Rumford Medal and £1,000 by the Royal Society in 1816 and a £2,000 prize by the country's colliery owners, who also awarded 100 guineas (£105) to Stephenson. However, the Newcastle committee also awarded Stephenson a £1,000 prize collected by subscription. Dr. Clanny was awarded a medal by the Royal Society of Arts in 1816.\n\nBoth the Davy and Stephenson lamps were fragile. The gauze in the Davy lamp rusted in the damp air of a coal pit and became unsafe, while the glass in the Stephenson lamp was easily broken, and allowed the flame to ignite firedamp in the mine. Later Stephenson designs also incorporated a gauze screen as a protection against glass breakage. Developments, including the Gray, Mueseler and Marsaut lamps, tried to overcome these problems by using multiple gauze cylinders, but glass remained a problem until toughened glass became available.\n\nWere the flame to go out in a lamp, then there was a temptation for the collier to relight it. Some opened the lamps to light tobacco pipes underground. Both of these practices were strictly forbidden; they clearly negated the whole point of the safety lamp. The miner was expected to return to the shaft to perform relighting, a round trip of up to a few miles. For men on piece work, this meant a loss of income (perhaps 10% of their day's pay) and so was unpopular. From the mid-century onwards, and particularly after the 1872 act, lamps had to have a lock mechanism which prevented the miner opening the lamp. Two schemes existed; either a special tool was required which kept at the pit head or else opening the lamp extinguished the flame. The latter mechanism can be seen in the Mueseler, Landau and Yates lamps below. Such a lamp was known as a \"protector\" lamp, a term picked up and used as a company name. Only on the return to the bank could the lamp man open the lamp for refilling and service. Various different locking mechanisms were developed; miners tended to be ingenious in finding ways of circumventing them. A number of additional lamps were supposed to accompany each gang of men, but restricting the number was an obvious economy for the pit owners.\n\nThe light given out by these lamps was poor (particularly the Davy where it passed through the gauze); indeed, in early lamps worse than candles. The problem not solved until the introduction of electric lighting around 1900 and the introduction of battery-powered helmet lamps in 1930. The poor light provided yet another reason for miners to try to circumvent the locks.\n\nEarly lamps (the Davy, Geordie and Clanny) had the gauze exposed to air currents. It was quickly discovered that an air current could cause the flame to pass through the gauze. The flame playing directly on the gauze heats it faster than the heat can be conducted away, eventually igniting the gas outside the lamp.\n\nThe following table is compiled from :\nFollowing accidents such as Wallsend (1818), Trimdon Grange (1882) and the Bedford Colliery Disaster (1886), lamps had to be shielded against such currents. In the case of the Davy, a \"Tin-can Davy\" was developed which had a metal cylinder with perforations at the bottom and a glass window for the light from the gauze. Clanny derived lamps had a metal shield (typically tinned iron) in the shape of a truncated cone, called a bonnet covering the gauze above the glass cylinder. The important principle is that no direct current of air can impinge on the gauze. The shield had the disadvantage of not allowing the collier or the deputy to check that the gauze was in place and clean. Lamps were therefore made so that they could be inspected and then the bonnet placed on and locked.\n\nIn the Davy lamp a standard oil lamp is surrounded by fine wire gauze, the top being closed by a double layer of gauze.\n\nIf firedamp is drawn into the flame it will burn more brightly and if the proportions are correct may even detonate. The flame on reaching the gauze fails to pass through and so the mine atmosphere is not ignited. However, if the flame is allowed to play on the gauze for a significant period, then it will heat up, sometimes as far as red heat. At this point it is effective, but in a dangerous state. Any further increase in temperature to white heat will ignite the external atmosphere. A sudden draught will case a localised hot spot and the flame will pass through. At a draught of between 4 and 6 feet per second the lamp becomes unsafe. At Wallsend in 1818 lamps were burning red hot (indicating significant firedamp). A boy (Thomas Elliott) was employed to carry hot lamps to the fresh air and bring cool lamps back. For some reason he stumbled; the gauze was damaged and the damaged lamp triggered the explosion. At Trimdon Grange (1882) a roof fall caused a sudden blast of air and the flame passed through the gauze with fatal results (69 killed).\n\nPoor copies and ill-advised \"improvements\" were known, but changing dimensions either reduced the illumination or the safety. The poor light compared to either the Geordie or Clanny eventually led to the Davy being regarded as \"not a lamp but a scientific instrument for detecting the presence of firedamp\". Some pits continued to use candles for illumination, relying on the Davy to warn men when to extinguish them.\n\nIn the earlier Geordie lamps an oil lamp is surrounded by glass. The top of the glass has a perforated copper cap with a gauze screen above that. The glass is surrounded by a perforated metal tube to protect it.\n\nLater versions had an annular chamber around the base of the lamp into which air entered through small (\") holes and from which air passed through gauze into the lamp. The glass was surrounded by gauze so that in the event of a glass breakage the Geordie became a Davy.\n\nIf the current of air forced in through the tubes (later holes and gallery) by the lamp's situation was too strong, the flame grew, and the lamp could get red-hot. The lamp becomes unsafe in a current of from 8 to 12 feet per second, about twice the limit of the Davy's safe operating conditions. However, this variation in the flame was not entirely bad. The intensity of the flame, air currents being equal, gives an indication of firedamp levels, so it is both a source of light and a sensor.\n\nA development of the Geordie lamp was the Purdy. A galley with gauze provided the inlet, above the glass was a chimney with perforated copper cap and gauze outer. A brass tube protected the upper works, shielded them and kept them locked in position. A sprung pin locked the whole together. The pin could only be released by applying a vacuum to a captive hollow screw; not something that a nicotine starved miner could do at the coal face.\n\nClanny abandoned his pumps and candles and developed a safety lamp which combined features of both the Davy and Geordie. The oil lamp was surrounded by a glass chimney with no ventilation from below. Above the chimney is a gauze cylinder with a double top. Air enters from the side and spent gases exit from the top. In the presence of firedamp the flame intensifies. The flame must be kept fairly high in normal use, a small flame permits the enclosed space to fill with firedamp/air mixture and the subsequent detonation may pass through the gauze. A larger flame will keep the upper part full of burnt gas. The Clanny gives more light than the Davy and can be carried more easily in a draught. Lupton notes however \"it is superior in no other respect\", particularly as a test instrument.\n\nThe glass on a Clanny was secured by a large diameter brass ring which could be hard to tighten securely. If a splinter occurred at the end of a crack, or indeed any other unevenness, then the seal might be compromised. Such an incident occurred at Nicholson Pit in 1856 on a lamp being used by an overman to test for firedamp. The mines inspector recommended that only Stephenson lamps were used for illumination and Davys for testing. In particular \"overmen ... whose lamps are mostly used to detect the presence gas, should avoid such [Clanny] lamps\".\n\nThe lamp is a modified Clanny designed by the Belgian Mathieu-Louis Mueseler. The flame is surrounded by a glass tube surmounted by a gauze capped cylinder. Air enters from the side above the glass and flows down to the flame before rising to exit at the top of the lamp. So far this is just a Clanny, but in the Mueseler a metal chimney supported on an internal gauze shelf conducts the combustion products to the top of the lamp. Some Mueseler lamps were fitted with a mechanism which locked the base of the lamp. Turning down the wick eventually released the base, but by then the flame was extinguished and therefore safe.\n\nThe lamp was patented in 1840 and in 1864 the Belgian government made this type of lamp compulsory.\n\nIn the presence of firedamp the explosive mixture is drawn through two gauzes (cylinder and shelf), burnt and then within the chimney are only burnt gases, not explosive mixture. Like a Clanny, and the Davy before it, it acts as an indicator of firedamp, burning more brightly in its presence. Later models had graduated shields by which the deputy could determine the concentration of firedamp from the heightening of the flame. Whilst the Clanny will continue to burn if laid on its side, potentially cracking the glass; the Mueseler will extinguish itself due to the stoppage of convection currents. The lamp is safe in currents up to 15 feet per second.\n\nThe Marsaut lamp is a Clanny with multiple gauzes. Two or three gauzes are fitted inside each other which improves the safety in a draught. Multiple gauzes will however interfere with the flow of air. The Marsaut was one of the first lamps to be fitted with a shield, in the illustration (right) the bonnet can be seen surrounding the gauzes. A shielded Marsaut lamp can resist a current of 30 feet per second.\n\nThe Bainbridge is a development of the Stephenson. A tapered glass cylinder surrounds the flame, and above that the body is a brass tube. The top of the tube is closed by a horizontal gauze attached to the body of the lamp by small bars to conduct heat away. Air enters through a series of small holes drilled in the lower brass ring supporting the glass.\n\nThe lamp is in part a development of the Geordie. Air enters into a ring near the base which is protected by gauze or perforated plate. The air passes down the side of the lamp passing through a series of gauze covered holes and enters the base through another yet another series of gauze covered holes. Any attempt to unscrew the base causes the lever (shown at \"f\" in the illustration) to extinguish the flame. The gauze covered holes and passageways restrict the flow to that required for combustion, so if any part of the oxygen is replaced by firedamp, then the flame is extinguished for want of oxidant.\n\nThe upper portion of the lamp uses a chimney like Mueseler and Morgan lamps. The rising gases pass up the chimney and through a gauze. At the top of the chimney a dished reflector diverts the gases out sideways through a number of holes in the chimney. The gases then start to rise up the \"intermediate chimney\" before exiting through another gauze. Gas finally passes down between the outermost chimney and the intermediate chimney, exiting a little above the glass. The outer chimney is therefore effectively a shield.\n\nThe Yates lamp is a development of the Clanny. Air enters through the lower part of the gauze top and leaves through the upper part; there is no chimney. The lower glass part of the lamp has seen some development however. It is replaced by a silvered reflector having a \"strong lens or bull's-eye\" in it to allow the light out. The result was a claimed 20 fold improvement in lighting over the Davy. Yates claimed \"the temptation to expose the flame to obtain more light is removed\".\n\nThe base also contained an interlocking mechanism to ensure that the wick was lowered and the lamp extinguished by any attempt to open it.\n\nThe lamp was \"much more expensive than the forms of lamp now in general use, but Mr, Yates states that the saving of oil effected by its use will in one year pay the additional cost\".\n\nThe Evan–Thomas lamp is similar to a shielded Clanny, but there is a brass cylinder outside the gauze above the glass. It resists draughts well but the flame is dull.\n\nThe Morgan is a cross between the Mueseler and the Marsaut. It is a shielded lamp with a series of disks at the top to allow spent fumes out and a series of holes lower down the shield to allow air in. There is an inner and outer shield so that air cannot blow directly on the gauze but must first find its way through a slim chamber. There are multiple gauzes, like the Mersaut, and there is an internal chimney like the Mueseler. There is no \"shelf\" supporting the chimney, instead it hangs from an inverted cone of gauze.\n\nThe Morgan will resist air up to 53 feet per second and is \"sufficiently safe for every practical purpose\".\n\nThe Clifford also has a double shield, but with a plain flat top. The chimney is quite narrow with gauze covering the top. The bottom of the chimney has a glass bell covering the flame. The chimney is supported on a gauze shelf. Air enters through the lower part of the outer shield, through the passage and into the lamp through the inner shield. It is drawn down through the gauze then passes the flame and ascends the chimney. At the top it leaves through gauze and the top of the double shield. The inner chimney is made of copper coated with a fusible metal. If the lamp gets too hot the metal melts and closes up the air holes, extinguishing the lamp.\n\nThe lamp has been tested and according to Lupton \"successfully resisted every effort to explode it up to a velocity of more than 100 feet per second\".\n\nIt was not until tungsten filaments replaced carbon that a portable electric light became a reality. An early pioneer was Joseph Swan who exhibited his first lamp in Newcastle upon Tyne in 1881 and improved ones in subsequent years. The Royal Commission on Accidents in Mines set up in 1881 carried out extensive tests of all types of lamps and the final report in 1886 noted that there had been good progress made in producing electric lamps giving a light superior to that of oil lamps and expected economic and efficient lamps to become available soon. This turned out not to be the case and progress was slow in attaining reliability and economy. The Sussmann lamp was introduced into Britain in 1893 and following trials at Murton Colliery in Durham it became a widely used electric lamp with 3000 or so reported by the company in use in 1900 However, by 1910 there were only 2055 electric lamps of all types in use - about 0.25% of all safety lamps. In 1911, an anonymous colliery owner, through the British government, offered a prize of £1000 () for the best lamp to specified requirements. There were 195 entries. It was won by a German engineer with the CEAG lamp, which was hand-held and delivered twice the illumination of oil lamps, with a battery life of 16 hours. Awards were made to 8 other lamps that met the judges' criteria. Clearly this stimulated development and over the next few years there was a marked increase in the use of electric lamps, especially the CEAG, Gray-Sussmann and Oldham, so by 1922 there were 294593 in use in Britain.\n\nIn 1913, Thomas Edison won the \"Ratheman medal\" for inventing a lightweight storage battery that could be carried on the back, powering a parabolic reflector that could be mounted on the miner's helmet. After extensive testing, 70,000 robust designs were in use in the US by 1916.\n\nEarly electric lamps in Britain were hand held as miners were used to this and helmet lamps became common much later than in countries like the USA where helmet (cap) lamps had been the norm.\n\nNowadays, safety lamps are mainly electric, and traditionally mounted on miners' helmets (such as the wheat lamp) or the Oldham headlamp, sealed to prevent gas penetrating the casing and being ignited by electrical sparks.\n\nAlthough its use as a light source was superseded by electric lighting, the flame safety lamp has continued to be used in mines to detect methane and blackdamp, although many modern mines now also use sophisticated electronic gas detectors for this purpose.\n\nAs a new light source, LED has many advantages for safety lamps, including longer illumination times and reduced energy requirements. Combined with new battery technologies, such as the lithium battery, it gives much better performance in safety lamp applications. It is replacing conventional safety lamps.\n\nThe Office of Mine Safety and Health (OMSHR), a part of the National Institute for Occupational Safety and Health (NIOSH) (itself part of Centers for Disease Control and Prevention) in the United States has been investigating the benefits of LED headlamps. A problem in mining is that the average age is increasing: 43.3 years in 2013 (in the USA) and as a person ages vision degenerates. LED technology is physically robust compared to a filament light bulb, and has a longer life: 50,000 hours compared to 1,000 – 3,000. Extended life reduces light maintenance and failures; according to OMSHR an average of 28 accidents per year occur in US mines involving lighting. NIOSH has sponsored the development of cap lamp systems which they claim improve the \"ability of older subjects to detect moving hazards by 15% and trip hazards by 23.7%, and discomfort glare was reduced by 45%\". Conventional lights are strongly focussed in a beam, NIOSH LED lamps are designed to produce a wider more diffuse beam which is claimed to improve the perception of objects by 79.5%.\n\n\n\n"}
{"id": "1724209", "url": "https://en.wikipedia.org/wiki?curid=1724209", "title": "Sensor web", "text": "Sensor web\n\nThe concept of the \"sensor web\" is a type of sensor network that is especially well suited for environmental monitoring.\nThe phrase the \"sensor web\" is also associated with a sensing system which heavily utilizes the World Wide Web. OGC's Sensor Web Enablement (SWE) framework defines a suite of web service interfaces and communication protocols abstracting from the heterogeneity of sensor (network) communication.\n\nThe term \"sensor web\" was first used by Kevin Delin of NASA in 1997,\nto describe a novel wireless sensor network architecture where the individual pieces could act and coordinate as a whole. In this sense, the term describes a specific type of sensor network: an amorphous network of spatially distributed sensor platforms (pods) that wirelessly communicate with each other. This amorphous architecture is unique since it is both synchronous and router-free, making it distinct from the more typical TCP/IP-like network schemes. A pod as a physical platform for a sensor can be orbital or terrestrial, fixed or mobile and might even have real time accessibility via the Internet. Pod-to-pod communication is both omni-directional and bi-directional where each pod sends out collected data to every other pod in the network. Hence, the architecture allows every pod to know what is going on with every other pod throughout the sensor web at each measurement cycle. The individual pods (nodes) were all hardware equivalent and Delin's architecture did not require special gateways or routing to have each of the individual pieces communicate with one another or with an end user. Delin's definition of a sensor web was an \"autonomous, stand-alone, sensing entity – capable of interpreting and reacting to the data measured – that does not necessarily require the presence of the World Wide Web to function\". \nAs a result, on-the-fly data fusion, such as false-positive identification and plume tracking, can occur within the sensor web itself and the system subsequently reacts as a coordinated, collective whole to the incoming data stream. For example, instead of having uncoordinated smoke detectors, a sensor web can react as a single, spatially dispersed, fire locator.\n\nThe term \"sensor web\" has also morphed into sometimes being associated with an additional layer connecting sensors to the World Wide Web.\nThe Sensor Web Enablement (SWE) initiative of the Open Geospatial Consortium (OGC) defines service interfaces which enable an interoperable usage of sensor resources by enabling their \"discovery\", \"access\", \"tasking\", as well as \"eventing and alerting\". By defining standardized service interfaces, a sensor web based on SWE services hides the heterogeneity of an underlying sensor network, its communication details and various hardware components, from the applications built on top of it.\nOGC's SWE initiative defines the term \"sensor web\" as \"an infrastructure enabling access to sensor networks and archived sensor data that can be discovered and accessed using standard protocols and application programming interfaces\". Through this abstraction from sensor details, their usage in applications is facilitated.\n\nDelin designed a sensor web as a web of interconnected pods. All pods in a sensor web are equivalent in hardware (there are no special \"gateway\" or \"slave\" pods). Nevertheless, there are additional functions that pods can perform besides participating in the general sensor web function. Any pod of a sensor web can be a \"portal pod\" and provides users access to the sensor web (both input and output). \nAccess can be provided by RF modem, cell phone connections, laptop connections, or even an Internet Server. In some cases, a pod will have an attached removable memory unit (such as a USB stick or a laptop) that stores collected data.\n\nThe term of \"mother pod\" refers to the pod that contains the master clock of the synchronous sensor web system. The mother pod has no special hardware associated with it, its designation as a mother is merely based on the ID number associated with the pod. Often the mother pod serves as a primary portal point to the Internet, but this is done only for deployment convenience. Early papers referenced the mother pod as \"a prime node\" if it additionally contained special hardware for a particular type of input/output device (say an RF modem).\n\nBecause of the inherent hopping of data within a sensor web, a pod with no attached sensors can be deployed as a \"relay\" with the single purpose of facilitating communication between the other pods and to expand the communication range to a particular end-point (such as a mother pod). Sensors can be attached to relay pods at a later time and relays can also serve as portal pods.\n\nEach pod usually contains:\n\nEach pod also typically requires a support such as a pole or tripod. \nThe number of pods may vary, with examples of sensor webs with 12 to 30 pods. \nThe shape of a sensor web may impact its usefulness, for instance a particular deployment \nmade sure each pod was in range to communicate with at least two other pods. Sensor web measurement cycles have typically been between 30 seconds and 15 minutes for deployed systems thus far.\n\nSensor webs consisting of pods have been deployed that have spanned miles and run continuously for years. \nSensor webs have been fielded in harsh environments (including deserts, mountain snowpacks, and Antarctica) \nfor the purposes of environmental science and have also proved valuable in urban search and rescue and infrastructure protection. \nThe technology is not only monitoring the environment but sometimes also controlling the environment by actuating devices.\n\n\n\n"}
{"id": "32557566", "url": "https://en.wikipedia.org/wiki?curid=32557566", "title": "Shaherose Charania", "text": "Shaherose Charania\n\nShaherose Charania is a Canadian-born tech entrepreneur. \n\nCharania is the founder of Founder Labs and Women 2.0.\n\nCharania is also an advisor at Opinno, an open innovation network for growth stage startups across web, mobile, cleantech and biotech. She is also a mentor with 500Startups.\n\nShe consulted Ribbit (acquired by British Telecom) on various consumer products and market launches. Previously, she was Director of Product Management at Talenthouse where she led the company’s product from concept to initial alpha launch.\n\nBefore that she was at JAJAH (acquired by Telefónica/O2) as the Director of Product Management and Marketing. She began her career in Silicon Valley at TiE Global overseeing governance and operations of 40+ entrepreneurship centers.\n\nIn parallel, Shaherose is perhaps best known as the CoFounder and CEO of Women 2.0, a media company offering content, community and conferences for aspiring and current women innovators in technology. She has led its growth to include tens of thousands of participants, hosting monthly educational and networking workshops and engaging entrepreneurs and investors from top Venture Capital firms and Angel circles. Over the years she has seen over 300 early stage startups through Women 2.0 and has a strong interest in supporting the entrepreneurial ecosystem.\n\nShe applies her commitment to positive social change to her role as a Board member for Good World Solutions that builds mobile apps to empowering factory workers in China, India, and other global markets to enable safety, communication and responsibility between employers and workers.\n\nShaherose holds a B.A. in Business Admin from the University of Western Ontario’s Richard Ivey School of Business and completed an international exchange program at ESADE in Barcelona, Spain.\n\n\n"}
{"id": "636089", "url": "https://en.wikipedia.org/wiki?curid=636089", "title": "Spot Image", "text": "Spot Image\n\nSpot Image, a public limited company created in 1982 by the French Space Agency, Centre National d'Etudes Spatiales (CNES), the IGN, and Space Manufacturers (Matra, Alcatel, SSC, etc.) is a subsidiary of Airbus Defence and Space (99%). The company is the commercial operator for the SPOT Earth observation satellites.\n\nSpot Image is a worldwide distributor of products and services using imagery from Earth observation satellites and works through a network of subsidiaries, local offices (Australia, Brazil, China, United States, Japan, Peru, Singapore), and partners. The goal is to provide on-the-spot service with worldwide availability.\n\nSpot Image works with a network of more than 30 direct receiving stations handling images acquired by the SPOT satellites.\n\nSpot Image collaborates with ESA’s GMES programme, shares geographic information with the OGC and contributes to the interoperability of web services; with Infoterra Global it continues to offer services for precision-agriculture.\n\nThe 3 SPOT satellites in orbit (Spot 5, 6, and 7) provide images with a large choice of resolutions – from 2.5 m to 10 m. Spot Image also distributes multiresolution data from other optical satellites, in particular from Formosat-2 (Taiwan) and Kompsat-2 (South Korea), for which it has exclusive distribution rights, and from radar satellites (TerraSAR-X, ERS, Envisat, Radarsat). Spot Image is also the exclusive distributor of data from the very-high resolution Pleiades satellites. The launch of Pléiades-1, via a Soyuz rocket, was confirmed on December 16, 2011. The second (Pléiades-2) was launched mid-2012. Spot Image also offers infrastructures for receiving and processing, as well as added value options.\n\nIn addition to images, Spot Image proposes innovative added-value products to meet new user-needs:\n\nAmong the services offered by Spot File:\nOn-line services:\n\nIts goal is to encourage the Earth observation industry and geographic information professionals to support local projects searching for adaptive solutions to climate change problems. The products and materials available to these professionals – satellite images, geographic information systems, image processing and display software – are indispensable for studying the impact of global warming on our planet, on local and global scales.\nPlanet Action supports projects that study:\n\nPlanet Action Web Site\n\n\n\n"}
{"id": "90910", "url": "https://en.wikipedia.org/wiki?curid=90910", "title": "Stockholm syndrome", "text": "Stockholm syndrome\n\nStockholm syndrome is a condition that causes hostages to develop a psychological alliance with their captors as a survival strategy during captivity. These alliances, resulting from a bond formed between captor and captives during intimate time spent together, are generally considered irrational in light of the danger or risk endured by the victims. The FBI's Hostage Barricade Database System and Law Enforcement Bulletin shows that roughly 8% of victims show evidence of Stockholm syndrome.\n\nThis term was first used by foreign media in 1973 as eponym when four hostages were taken during a bank robbery in Stockholm, Sweden. The hostages defended their captors after being released and would not agree to testify in court against them. Stockholm syndrome is ostensibly paradoxical because the sympathetic sentiments captives feel towards their captors are the opposite of the fear and disdain an onlooker may feel towards the captors.\n\nThere are four key components that generally lead to the development of Stockholm syndrome: \n\nStockholm syndrome is considered a \"contested illness\", due to many law enforcement officers' doubt about the legitimacy of the condition. Stockholm syndrome has also come to describe the reactions of some abuse victims beyond the context of kidnappings or hostage-taking. Actions and attitudes similar to those suffering from Stockholm syndrome have also been found in victims of sexual abuse, human trafficking, discrimination, terror, and political and religious oppression.\n\nIn 1973, Jan-Erik Olsson, a convict on parole, took four employees of the bank (three women and one man) hostage during a failed bank robbery in Kreditbanken, one of the largest banks in Stockholm, Sweden. He negotiated the release from prison of his friend Clark Olofsson to assist him. They held the hostages captive for six days (23–28 August) in one of the bank’s vaults. When they were released, none of them would testify against either captor in court; instead they began raising money for their defense. \n\nNils Bejerot, a Swedish criminologist and psychiatrist coined the term after Stockholm police asked him for assistance with analyzing the victims' reactions to the 1973 bank robbery and their status as hostages. As the idea of brainwashing was not a new concept, Bejerot, speaking on \"a news cast after the captives' release\" instinctively reduced the hostages' reactions to a result of being brainwashed by their captors. He called it \"Norrmalmstorgssyndromet\", meaning \"The Norrmalmstorg Syndrome\"; it later became known outside of Sweden as the Stockholm syndrome. It was originally defined by psychiatrist Frank Ochberg to aid the management of hostage situations.\n\nOlsson later said in an interview:It was the hostages' fault. They did everything I told them to. If they hadn't, I might not be here now. Why didn't any of them attack me? They made it hard to kill. They made us go on living together day after day, like goats, in that filth. There was nothing to do but get to know each other.\n\nMary McElroy was abducted from her home at age 25 by four men who held a gun to her, demanded her compliance, took her to an abandoned farmhouse, and chained her to a wall. She defended her kidnappers when she was released, explaining that they were only businessmen. She then continued to visit her captors while they were in jail. She eventually committed suicide and left the following note: “My four kidnappers are probably the only people on Earth who don't consider me an utter fool. You have your death penalty now – so, please, give them a chance.\"\n\nNatascha Kampusch was kidnapped at age ten and kept in an insulated, dark room under the garage of Wolfgang Priklopil. She would receive a variation of kind, physically and sexually abusive, controlling, and permissive treatment from her captor. Eight years after her kidnapping, Natascha left and Priklopil committed suicide. After her kidnapper's death, Natascha lamented and kept a picture of him in her wallet.\n\nKampusch now owns the house in which she was imprisoned, saying, \"I know it's grotesque – I must now pay for electricity, water and taxes on a house I never wanted to live in\". It was reported that she claimed the house from Přiklopil's estate because she wanted to protect it from vandals and being torn down; she also noted that she has visited it since her escape. When the third anniversary of her escape approached, it was revealed she had become a regular visitor at the property and was cleaning it out possibly to move in herself.\n\nIn January 2010, Kampusch said she had retained the house because it was such a big part of her formative years, also stating that she would fill in the cellar if it is ever sold, adamant that it will never become a macabre museum to her lost adolescence. The cellar was indeed filled in, though Kampusch still owns the house.\n\nPatty Hearst, the granddaughter of publisher William Randolph Hearst, was taken and held hostage by the Symbionese Liberation Army, \"an urban guerilla group\", in 1974. She was recorded denouncing her family as well as the police under her new name, \"Tania\", and was later seen working with the SLA to rob banks in San Francisco. She publicly asserted her sympathetic feelings towards the SLA and their pursuits as well. After her 1975 arrest, pleading Stockholm syndrome did not work as a proper defense in court, much to the chagrin of her defense lawyer, F. Lee Bailey. Her seven-year prison sentence was later commuted, and she was eventually presidentially pardoned by Bill Clinton, who was informed that she was not acting under her own free will.\n\nIn 1977, Colleen Stan was hitchhiking to visit a friend in southern California when she was kidnapped by Cameron Hooker and his wife Janice and forced to live in a wooden restraining box underneath their bed. For seven years she was repeatedly raped and tortured by Cameron and forced to live life as a sort of domestic/sex slave. Even though she was allowed to socialize with Janice and even visit her mother, she still continued to live in the box and did not attempt to escape. She was eventually freed by Janice, who asked Stan to not disclose her abuse as Janice was attempting to reform Cameron. Stan remained silent until Janice finally decided to turn Cameron over to the police.\n\nThere is evidence that some victims of \"childhood sexual abuse\" come to feel a connection with their abuser. They often feel flattered by the adult attention or are afraid that disclosure will create family disruption. In adulthood, they resist disclosure for emotional and personal reasons.\n\nA similar form of Stockholm syndrome called \"Lima syndrome\" has been proposed, in which abductors develop sympathy for their hostages. An abductor may also have second thoughts or experience empathy towards their victims.\n\nLima syndrome was named after an abduction at the Japanese embassy in Lima, Peru, in 1996, when members of a militant movement took hostage hundreds of people attending a party at the official residence of Japan's ambassador. Within a few hours, the abductors had set free most of the hostages, including the most valuable ones, because of sympathy towards them.\n\nVictims of the formal definition of Stockholm syndrome develop \"positive feelings toward their captors and sympathy for their causes and goals, and negative feelings toward the police or authorities\". These symptoms often follow escaped victims back into their previously ordinary lives.\n\n\nFrom a psychoanalytic lens, it can be argued that Stockholm syndrome arises strictly as a result of survival instincts. Strentz states, \"the victim’s need to survive is stronger than his impulse to hate the person who has created the dilemma.\" A positive emotional bond between captor and captive is a \"defense mechanism of the ego under stress\". These sentimental feelings are not strictly for show, however. Since captives often fear that their affection will be perceived as fake, they eventually begin to believe that their positive sentiments are genuine. The conception of Stockholm syndrome has grown to include victims of kidnappings or hostage instances, domestic or child abuse, human trafficking, incest, prisoners of war, political terrorism, cult members, concentration camp prisoners, slaves, and prostitutes. It is believed that women are especially prone to developing the condition.\n\nTypically, Stockholm syndrome develops in captives when they engage in \"face-to-face contact\" with their captors, and when captors make captives doubt the likelihood of their survival by terrorizing them into \"helpless, powerless, and submissive\" states. This enables captors to appear merciful when they perform acts of kindness or fail to \"beat, abuse, or rape\" the victims. Ideas like \"dominance hierarchies and submission strategies\" assist in devising explanations for the illogical reasoning behind the symptoms of those suffering from Stockholm syndrome as a result of any oppressive relationship. Partial activation of the capture-bonding psychological trait may lie behind battered woman syndrome, military basic training, fraternity hazing, and sex practices such as sadism/masochism or bondage/discipline.\n\nEvolutionarily speaking, research evidence exists to support the genuine scientific nature of Stockholm syndrome. Responses similar to those in human captives have been detected in some reptiles and mammals, primates in particular. Abuse and subsequent submission and appeasement by the victim have been observed among chimpanzees, leading to the theory that the Stockholm syndrome may have its roots in evolutionary needs.\n\nLife in the \"environment of evolutionary adaptiveness\" (EEA) is thought by researchers such as Israeli military historian Azar Gat to be similar to that of the few remaining hunter-gatherer societies, who asserts that war and abductions were typical of human pre-history. Being captured by neighbouring tribes was a relatively common event for women. In some of those tribes (Yanomamo, for instance), practically everyone in the tribe is descended from a captive within the last three generations. As high as one in ten of females were abducted and incorporated into the tribe that captured them.Being captured and having their children killed may have been common; women who resisted capture risked being killed. When selection is intense and persistent, adaptive traits (such as capture-bonding) become universal to the population or species.\n\nFirst published in 1994, author Dee Graham uses the Stockholm syndrome label to describe group or collective responses to trauma, rather than individual reactions. Graham focuses specifically on the impact of Stockholm syndrome on battered and abused women as a community.She claimed that in both the psychological and societal senses, these women are defined by their sense of fear surrounding the threat of male violence. This constant fear is what drives these women to perform actions that they know will be pleasing to men in order to avoid emotional, physical, or sexual assault as a result of male anger. Graham draws parallels between women and kidnapping victims in the sense that these women bond to men to survive, as captives bond to their captors to survive.\n\nRecovering from Stockholm syndrome ordinarily involves \"psychiatric or psychological counseling,\" where the patient is helped to realize that their actions and feelings stemmed from inherent human survival techniques. The process of recovery includes reinstating normalcy into the lives of victims, including helping the victim learn how to decrease their survival-driven behaviors.\n\nThis book is widely used as the \"classification system for psychological disorders\" by the American Psychiatric Association. Stockholm syndrome has not historically appeared in the manual, as many believe it falls under posttraumatic stress disorder. Before the fifth edition (DSM5) was released, Stockholm syndrome was under consideration to be included under 'Disorders of Extreme Stress, Not Otherwise Specified'. The work was updated in 2013, but Stockholm syndrome was not present.\n\nResearchers have found that although there is a lot of media coverage of Stockholm syndrome, there has not been a lot of professional research into the phenomena. What little research has been done is often contradictory and does not always agree on what Stockholm syndrome is. The term has grown beyond kidnappings to all kinds of abuse. Also, there is no clear definition of symptoms to ‘diagnose’ the ‘syndrome’.\n\nA report by the FBI found that only 8% of kidnapping victims showed signs of Stockholm syndrome. The sensational nature of dramatic cases causes the public to perceive this phenomenon as the rule rather than the exception. For Stockholm syndrome to happen, FBI researchers have identified three key factors: the passage of time, continual contact, and small acts of kindness without direct and persistent abuse.\n\nRobbins and Anthony, who had historically studied a condition similar to Stockholm syndrome, known as destructive cult disorder, observed in their 1982 study that the 1970s were rich with apprehension surrounding the potential risks of brainwashing. They assert that brainwashing’s media attention during this time resulted in the fluid reception of Stockholm syndrome as a psychological condition.\n\nIt is possible that the label Stockholm syndrome is used too freely in cases in which it may not apply. Elizabeth Smart has been held as a classic example of Stockholm syndrome; however, she denies that she ever had any emotional attachment to her abusers. Although she chose not to run away when she had the chance, she emphasized that the threats from her captors to her and her family, and the direct presence of her captors influenced her decision to stay. Once freed from her captors, she gladly reunited with her family and felt no empathy for her abusers.\n\n\n"}
{"id": "6106010", "url": "https://en.wikipedia.org/wiki?curid=6106010", "title": "Storey pole", "text": "Storey pole\n\nA storey pole (or story pole, storey rod, story stick, jury stick, scantling, scantillon) is a length of narrow board usually cut to the height of one storey. It is used as a layout tool for any kind of repeated work in carpentry including stair-building, framing, timber framing, siding, brickwork, and setting tiles. The pole is marked for the heights from (usually) the floor platform of a building for dimensions such as window sill heights, window top heights (or headers), exterior door heights (or headers), interior door heights, wall gas jet heights (for gas lamps) and the level of the next storey joists. It makes for quick, repeatable measurements without the need of otherwise calibrated measuring devices or workers skilled in using them.\n\nCraftsmen use them to mark clapboard and brick courses so that, for example, a course ends neatly below a window sill or at a door's architrave. They are used in remodelling so that, for example, the new coursing of exterior siding on a wing will match the existing.\n\nThere is evidence of 'boning-rods' being used in building Egypt's Great Pyramid as counterparts of modern storey poles.\n"}
{"id": "1518706", "url": "https://en.wikipedia.org/wiki?curid=1518706", "title": "Techne Ltd.", "text": "Techne Ltd.\n\nTechne Ltd. was a company founded in Cambridge, UK by Norman de Bruyne in 1948 to manufacture gelation timers and thermal cycler equipment. The company was sold after the founder's death by his son John. After changing hands many times, the company finally closed in 2005. Techne branded products are now manufactured by the parent company Barloworld Scientific Ltd. From the time of its formation until 1999 Techne was located in Duxford vicarage near Cambridge. At this point, however, the property was sold for housing development and the company moved to the industrial estate up the road.\n"}
{"id": "12293209", "url": "https://en.wikipedia.org/wiki?curid=12293209", "title": "Terminal cleaning", "text": "Terminal cleaning\n\nTerminal cleaning is a cleaning method used in healthcare environments to control the spread of infections.\n\nNosocomial infections claim approximately 90,000 lives in the United States annually. When patients are hospitalized and identified as having methicillin-resistant Staphylococcus aureus or infections that can be spread to other patients, best practices isolate these patients in rooms that are subjected to terminal cleaning when the patient is discharged.\n\nTerminal cleaning reduces the spread of \"C. difficile\" infections.\n\nTerminal cleaning methods vary, but usually include removing all detachable objects in the room, cleaning lighting and air duct surfaces in the ceiling, and cleaning everything downward to the floor. Items removed from the room are disinfected or sanitized before being returned to the room.\n\n"}
{"id": "12915657", "url": "https://en.wikipedia.org/wiki?curid=12915657", "title": "Trickling filter", "text": "Trickling filter\n\nA trickling filter is a type of wastewater treatment system. It consists of a fixed bed of rocks, coke, gravel, slag, polyurethane foam, sphagnum peat moss, ceramic, or plastic media over which sewage or other wastewater flows downward and causes a layer of microbial slime (biofilm) to grow, covering the bed of media. conditions are maintained by splashing, diffusion, and either by forced-air flowing through the bed or natural convection of air if the filter medium is porous.\n\nThe terms trickle filter, trickling biofilter, biofilter, biological filter and biological trickling filter are often used to refer to a trickling filter. These systems have also been described as roughing filters, intermittent filters, packed media bed filters, alternative septic systems, percolating filters, attached growth processes, and fixed film processes.\n\nTypically, sewage flow enters at a high level and flows through the primary settlement tank. The supernatant from the tank flows into a dosing device, often a tipping bucket which delivers flow to the arms of the filter. The flush of water flows through the arms and exits through a series of holes pointing at an angle downwards. This propels the arms around distributing the liquid evenly over the surface of the filter media.\nMost are uncovered (unlike the accompanying diagram) and are freely ventilated to the atmosphere.\n\nThe removal of pollutants from the waste water stream involves both absorption and adsorption of organic compounds and some inorganic species such as nitrite and nitrate ions by the layer of microbial bio film. The filter media is typically chosen to provide a very high surface area to volume. Typical materials are often porous and have considerable internal surface area in addition to the external surface of the medium. Passage of the waste water over the media provides dissolved oxygen which the bio-film layer requires for the biochemical oxidation of the organic compounds and releases carbon dioxide gas, water and other oxidized end products. As the bio film layer thickens, it eventually sloughs off into the liquid flow and subsequently forms part of the secondary sludge. Typically, a trickling filter is followed by a clarifier or sedimentation tank for the separation and removal of the sloughed film. Other filters utilizing higher-density media such as sand, foam and peat moss do not produce a sludge that must be removed, but require forced air blowers and backwashing or an enclosed anaerobic environment.\n\nThe bio-film that develops in a trickling filter may become several millimetres thick and is typically a gelatinous matrix that contains many species of bacteria, cilliates and amoeboid protozoa, annelids, round worms and insect larvae and many other micro fauna. This is very different from many other bio-films which may be less than 1 mm thick. Within the thickness of the biofilm both aerobic and anaerobic zones can exist supporting both oxidative and reductive biological processes. At certain times of year, especially in the spring, rapid growth of organisms in the film may cause the film to be too thick and it may slough off in patches leading to the \"spring slough\".\n\nA typical trickling filter is circular and between 10 metres and 20 metres across and between 2 metres to 3 metres deep. A circular wall, often of brick, contains a bed of filter media which in turn rests on a base of under-drains. These under-drains function both to remove liquid passing through the filter media but also to allow the free passage of air up through the filter media. Mounted in the center over the top of the filter media is a spindle supporting two or more horizontal perforated pipes which extend to the edge of the media. The perforations on the pipes are designed to allow an even flow of liquid over the whole area of the media and are also angled so that when liquid flows from the pipes the whole assembly rotates around the central spindle. Settled sewage is delivered to a reservoir at the centre of the spindle via some form of dosing mechanism, often a tipping bucket device on small filters.\n\nLarger filters may be rectangular and the distribution arms may be driven by hydraulic or electrical systems.\n\nSingle trickling filters may be used for the treatment of small residential septic tank discharges and very small rural sewage treatment systems. Larger centralized sewage treatment plants typically use many trickling filters in parallel.\n\nSystems can be configured for single-pass use where the treated water is applied to the trickling filter once before being disposed of, or for multi-pass use where a portion of the treated water is cycled back and re-treated via a closed loop. Multi-pass systems result in higher treatment quality and assist in removing Total Nitrogen (TN) levels by promoting nitrification in the aerobic media bed and denitrification in the anaerobic septic tank. Some systems use the filters in two banks operated in series so that the wastewater has two passes through a filter with a sedimentation stage between the two passes. Every few days the filters are switched round to balance the load. This method of treatment can improve nitrification and de-nitrification since much of the carbonaceous oxidative material is removed on the first pass through the filters.\n\nTrickling may have a variety of types of filter media used to support the biofilm. Types of media most commonly used include coke, pumice, plastic matrix material, open-cell polyurethane foam, clinker, gravel, sand and geotextiles. Ideal filter medium optimizes surface area for microbial attachment, wastewater retention time, allows air flow, resists plugging is mechanically robust in all weathers allowing walking access across the filter and does not degrade. Some residential systems require forced aeration units which will increase maintenance and operational costs.\n\nThe treatment of industrial wastewater may involve specialized trickling filters which use plastic media and high flow rates. Wastewaters from a variety of industrial processes have been treated in trickling filters. Such industrial wastewater trickling filters consist of two types:\n\n\nThe availability of inexpensive plastic tower packings has led to their use as trickling filter beds in tall towers, some as high as 20 meters. As early as the 1960s, such towers were in use at: the Great Northern Oil's Pine Bend Refinery in Minnesota; the Cities Service Oil Company Trafalgar Refinery in Oakville, Ontario and at a kraft paper mill.\n\nThe treated water effluent from industrial wastewater trickling filters is typically processed in a clarifier to remove the sludge that sloughs off the microbial slime layer attached to the trickling filter media as for other trickling filter applications.\n\nSome of the latest trickle filter technology involves aerated biofilters of plastic media in vessels using blowers to inject air at the bottom of the vessels, with either downflow or upflow of the wastewater.\n\n"}
{"id": "41191060", "url": "https://en.wikipedia.org/wiki?curid=41191060", "title": "Wash rack", "text": "Wash rack\n\nA wash rack is a partly enclosed platform that is used to wash vehicles, heavy equipment, tools, and parts by removing dirt, grime, chemicals, invasive species, and other contaminants with a pressure washer in order to prevent corrosion and promote longer equipment lifespan. Cars, trucks, boats, construction and maintenance equipment, and even aircraft and military vehicles can all be cleaned in a wash rack. Wash racks are usually mobile and constructed from steel, although plastic wash racks exist as well as concrete formed alternatives which are sometimes installed in permanent facilities.\n\nThe term wash rack is also used to describe pads for washing horses. Wash racks for vehicle washing may be called closed-loop washing systems, heavy duty wash systems and degreasing pads.\n\nWash racks are often called \"containment racks\" or \"containment pads\" because their purpose is to contain effluent which often consists of hazardous materials that have been dislodged from the wash subject, and prevent it from draining off into the ground and causing stormwater contamination. Instead, the waste water is pumped out of the wash rack and into some sort of filtration system which removes the contaminates then sends the water either back into the pressure washer for reuse or into a sanitary sewer.\n\nThe main purpose of a wash rack is to clean equipment while protecting the environment from contaminates commonly found on construction, maintenance and military vehicles or equipment. To comply with U.S. Department of Agriculture (USDA) regulations, which are intended to prevent soil-borne insects or other potentially harmful organisms from entering the United States, U.S. military vehicles and equipment must be thoroughly washed before being shipped home. As such, wash racks are commonly used by the US military to ensure vehicles are clean and safe before they are brought back into the country.\n\nAccording to statistics, wash racks help recycle thousands of gallons of water on a daily basis and are already part of many military construction projects around the world. Industrial water use was estimated at 18.5 billion gallons per day in the United States in 2005, a number that could be greatly reduced by using wash racks to recycle and reuse water.\n\nAccording to the United States Department of Agriculture, wash racks are also important in controlling the spread of noxious weeds and invasive species in the agricultural industry, as well as forestry.\n"}
{"id": "46182752", "url": "https://en.wikipedia.org/wiki?curid=46182752", "title": "YourMechanic", "text": "YourMechanic\n\nYourMechanic is a privately held startup company based in Mountain View, California. The company provides mobile car repair and maintenance services to car owners at their location (home or office).\nThe company currently operates in over 700 cities within 50+ major metro areas in the United States. Services are performed by certified mechanics.\n\nThe company has raised $32 million to date, most recently with $24 million provided by SoftBank Capital, Lerer Hippeau Ventures, Data Point Capital, Andreessen Horowitz, SAIC, Verizon Ventures, American Family Insurance, PG Ventures, Promus Ventures, and Silicon Valley Bank.\n\nMost mechanics are certified by National Institute for Automotive Service Excellence (ASE) and some are certified by similar organizations and/or have dealer or factory training. A new customer can review mechanics’ certifications during the booking process in addition to experience, work history, jobs completed for other car owners, fees paid by other car owners, and customers ratings and reviews. Mechanics undergo background checks and other screenings.\n\nThe company has a mobile app that provides consumers a means to obtain price quotes and schedule repairs and services.\n\nIn 2012, Art Agrawal (Co-founder) and Dongyi Liao (CTO) founded YourMechanic.\n\nIn September 2012, the company raised $1.8 million in seed funding from venture firms and investors including Andreessen Horowitz, SV Angel, Crunch Fund, Promus Ventures, Greylock Partners, DFJ, Lerer Ventures, PG Ventures, Ashton Kutcher, Jawed Karim, Dave Gilboa, Justin Waldron, Joshua Schachter, Hector Hulian, Paige Craig, Mark Friedgan, Alex Goldstein, Sam Angus, Kevin Freedman, Jeremy Wenokur and Rob Wang.\n\nIn September 2012, the company won the TechCrunch Battlefield Disrupt San Francisco competition against other finalists Saya, Lit Motors, Prior Knowledge, Zumper, Gyft and Expert Labs.\n\nIn March 2016, the company announced $24 million in funding, bringing its total funding to $32 million.\n\n\n"}
