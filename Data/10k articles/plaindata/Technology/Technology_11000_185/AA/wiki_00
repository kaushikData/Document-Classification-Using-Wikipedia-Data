{"id": "6570377", "url": "https://en.wikipedia.org/wiki?curid=6570377", "title": "123 (interbank network)", "text": "123 (interbank network)\n\nEgyptian Banks Co. for Technological Advancement (EBC) provides the banking community in Egypt with a shared cash network commercially called \"123\".\n\nThe \"123\" network links more than 30 Egyptian Banks supporting more than 1500 ATMs distributed all over Egypt. This network provides the banks' clients with direct access to their different accounts at any time and from anywhere through the ATMs carrying the \"123\" logo.\nThis network is available 24 hours a day 7 days a week.\n\nIn addition, the \"123\" network is a gateway to MasterCard, Diners Club and American Express International networks. Moreover, it is linked to regional networks in the Persian Gulf states, NAPS in State of Qatar, Benefit in Kingdom of Bahrain and CSCBank SAL in Lebanon.\n\nBenefits:\n\n"}
{"id": "45021102", "url": "https://en.wikipedia.org/wiki?curid=45021102", "title": "Acousto-electronics", "text": "Acousto-electronics\n\nAcousto-electronics (also spelled 'Acoustoelectronics') is a branch of physics, acoustics and electronics that studies interactions of ultrasonic and hypersonic waves in solids with electrons and with electro-magnetic fields. Typical phenomena studied in acousto-electronics are acousto-electric effect and also amplification of acoustic waves by flows of electrons in piezoelectric semiconductors, when the drift velocity of the electrons exceeds the velocity of sound. The term 'acousto-electronics' is often understood in a wider sense to include numerous practical applications of the interactions of electro-magnetic fields with acoustic waves in solids. In particular, these are signal processing devices using surface acoustic waves (SAW), different sensors of temperature, pressure, humidity, acceleration, etc. \n\n\n \n"}
{"id": "3421228", "url": "https://en.wikipedia.org/wiki?curid=3421228", "title": "Anastase Dragomir", "text": "Anastase Dragomir\n\nAnastase Dragomir (1896–1966) was a distinguished Romanian inventor, most famous for his \"catapultable cockpit\" patent (with Tănase Dobrescu) as an early form of ejection seat, although preceded by Everard Calthrop's 1916 compressed air ejection seat, and others.\n\nAnastase Dragomir, born 6 February 1896 in Brăila, Romania, was the sixth child of his family. He worked in France at several aircraft factories where he perfected a system to save pilots and passengers in case of accidents. On 3 November 1928 he applied for French patent #678566, \"Nouveau système de montage des parachutes dans les appareils de locomotion aérienne\". Issued on 2 April 1930, the invention was, \"a new system of parachuting from the apparatus for air locomotion, each passenger having his own parachute that allows, in critical moments, the assembly detaching from the plane, so the parachute with seated passenger passes through an opening.\"\n\nAfter several attempts, Dragomir managed to obtain financing and began construction of his \"catapulted cockpit\". The invention was tested in a Farman airplane piloted by at Paris-Orly, France airport on 28 August 1929. French newspapers later reported on the invention's success. Dragomir returned home to Romania after the Paris-Orly experiment where, with Romanian aviation engineer captain Constantin Nicolau, he successfully repeated the experiment in an Avia airplane at Băneasa Airport in Bucharest, Romania on 26 October 1929. He continued to refine his invention and obtained Romanian patent #40658 in 1950 for his \"parachuted cell\". In 1960, he received Romanian patent #41424 for a transport aircraft equipped with ejection cabins. Anastase Dragomir died in Bucharest, Romania in June 1966.\n\nThe photograph shown on the stamp is of a British-designed and built Martin-Baker ejection seat. Founded by Sir James Martin, Martin-Baker ejection seats are used by more than 80 air forces around the world today with over 16,500 seats currently in service. The Martin-Baker Aircraft Company is still run by the Martin family today.\n\n"}
{"id": "37982021", "url": "https://en.wikipedia.org/wiki?curid=37982021", "title": "Automatic trip", "text": "Automatic trip\n\nAn automatic trip is an action performed by some system, usually a Safety Instrumented System, Programmable Logic Controller, or Distributed Control System, to put an industrial process into a safe state. It is triggered by some parameter going into a pre-determined unsafe state. It is usually preceded by an alarm to give a process operator a chance to correct the condition to prevent the trip, since trips are usually costly because of lost production.\n"}
{"id": "2905316", "url": "https://en.wikipedia.org/wiki?curid=2905316", "title": "Bahut", "text": "Bahut\n\nA bahut is a portable chest, with a rounded lid covered in leather, garnished with nails, once used for the transport of clothes or other personal luggage, it was, in short, the original portmanteau. This ancient receptacle, of which mention is made as early as the 14th century — its traditional form is still preserved in many varieties of the travelling trunk — sometimes had its leather covering richly ornamented, and occasionally its interior was divided into compartments; but whatever the details of its construction it was always readily portable.\n\nTowards the end of the 17th century the name fell into disuse, and was replaced by \"coffer\", which probably accounts for its misuse by the French romantic writers of the early 19th century. They applied it to almost any antique sideboard, cupboard or wardrobe, and its use became hopelessly confused.\n\nIn architecture, this term is also used for a dwarf-wall of plain masonry, carrying the roof of a cathedral or church and masked or hidden behind the balustrade.\n\nAccording to the Shorter Oxford Dictionary (3rd edition revised) \"bahut\" also means \"a dress for masquerading\".\n"}
{"id": "8207827", "url": "https://en.wikipedia.org/wiki?curid=8207827", "title": "Bass worms", "text": "Bass worms\n\nA bass worm is an artificial fishing lure which comes in a variety of different colors and body types and is usually scented with a salty, garlic residue. Bass worms are more effective than other rubber worms primarily because bass are attracted to a particular worm depending on the environment in which they are being used.\n\nThe grub is meant to resemble that of the larvae that appear naturally all around the world. Most artificial grubs have a curly tail on the end of a fat body that flaps rapidly when being pulled through the water. This action mimics the true movement of a living worm and helps to entice a bass to strike. These worms vary anywhere from 2 to 4 inches in length.\n\nThe ringworm has somewhat of a nightcrawler profile, equipped with a smooth, aerodynamic nose and a body that has circular ridges (or rings) that continue down the body until reaching a flat or sometimes curly tail. These worms are typically 4 to 6 inches in length.\n\nA jerkbait resembles a small bait fish (example: shad); usually hard-bodied, sometimes soft plastic, usually with two (or sometimes three) treble hooks. The fisherman imparts a stop-and-go, twitching-type motion to the lure to resemble an injured or dying bait fish.\n\nThese worms tend to range between 7 and 10 inches in length; the majority of which is the tail that is sinuously twisted so it can displace water while causing as much visual pleasure to the bass as possible. The front of the body accounts for about 3 inches of the overall length and is cylindrical in shape, sometimes having circular rings. \n\nFloating worms have the nightcrawler profile but instead of staying submerged, these baits hover near the surface. These worms work well for fishing through heavy cover and over grass beds where topwater action is essential. These worms tend to be 6 to 8 inches in length.\n\nChartreuse - a tint consisting of, but not limited to, pale green and yellow. This color is widely used in the sport of bass fishing because of its versatility when adding other colors to the lure.\n\nWatermelon- a light green tint that is also very versatile when adding other colors. Watermelon lures usually come either with or without the \"seed.\" The watermelon seed lures will be speckled with black dots.\n"}
{"id": "9938867", "url": "https://en.wikipedia.org/wiki?curid=9938867", "title": "Blanking (video)", "text": "Blanking (video)\n\nIn raster scan equipment, an image is built up by scanning an electron beam from left to right across a screen to produce a visible trace of one scan line, reducing the brightness of the beam to zero (\"horizontal blanking\"), moving it back as fast as possible to the left of the screen at a slightly lower position (the next scan line), restoring the brightness, and continuing until all the lines have been displayed and the beam is at the bottom right of the screen. Its intensity is then reduced to zero again (vertical blanking), and it is rapidly moved to the top left to start again, creating the next Film frame.\n\nIn television, in particular, the vertical blanking interval is long to accommodate the slow equipment available at the time the standard was set. Fast modern electronics allows digital information to be encoded into the signal during the vertical blanking interval; it is not displayed on screen as the beam is blanked, but can be processed by appropriate circuitry.\n"}
{"id": "1902795", "url": "https://en.wikipedia.org/wiki?curid=1902795", "title": "Christmas tree (oil well)", "text": "Christmas tree (oil well)\n\nIn petroleum and natural gas extraction, a Christmas tree, or \"tree\", is an assembly of valves, spools, and fittings used to regulate the flow of pipes in an oil well, gas well, water injection well, water disposal well, gas injection well, condensate well and other types of wells. It was named for its resemblance to the series of starting lights at a drag racing strip (Christmas tree (drag racing)).\n\nChristmas trees are used on both surface and subsea wells. It is common to identify the type of tree as either \"subsea tree\" or \"surface tree\". Each of these classifications has a number of variations. Examples of subsea include conventional, dual bore, mono bore, TFL (through flow line), horizontal, mudline, mudline horizontal, side valve, and TBT (through-bore tree) trees. The deepest installed subsea tree is in the Gulf of Mexico at approximately . (Current technical limits are up to around 3000 metres and working temperatures of -50°F to 350°F with a pressure of up to 15,000 psi.)\n\nThe primary function of a tree is to control the flow, usually oil or gas, out of the well. (A tree may also be used to control the injection of gas or water into a non-producing well in order to enhance production rates of oil from other wells.) When the well and facilities are ready to produce and receive oil or gas, tree valves are opened and the formation fluids are allowed to go through a flow line. This leads to a processing facility, storage depot and/or other pipeline eventually leading to a refinery or distribution center (for gas). Flow lines on subsea wells usually lead to a fixed or floating production platform or to a storage ship or barge, known as a floating storage offloading vessel (FSO), or floating processing unit (FPU), or floating production, storage and offloading vessel (FPSO).\n\nA tree often provides numerous additional functions including chemical injection points, well intervention means, pressure relief means, monitoring points (such as pressure, temperature, corrosion, erosion, sand detection, flow rate, flow composition, valve and choke position feedback), and connection points for devices such as down hole pressure and temperature transducers (DHPT). On producing wells, chemicals or alcohols or oil distillates may be injected to preclude production problems (such as blockages).\n\nFunctionality may be extended further by using the control system on a subsea tree to monitor, measure, and react to sensor outputs on the tree or even down the well bore. The control system attached to the tree controls the downhole safety valve (SCSSV, DHSV, SSSV) while the tree acts as an attachment and conduit means of the control system to the downhole safety valve.\n\nTree complexity has increased over the last few decades. They are frequently manufactured from blocks of steel containing multiple valves rather than being assembled from individual flanged components. This is especially true in subsea applications where the resemblance to Christmas trees no longer exists given the frame and support systems into which the main valve block is integrated.\n\nNote that a tree and wellhead are separate pieces of equipment not to be mistaken as the same piece. The Christmas tree is installed on top of the wellhead. A wellhead is used without a Christmas tree during drilling operations, and also for riser tie-back situations that later would have a tree installed at riser top. Wells being produced with rod pumps (pump jacks, nodding donkeys, grasshopper pumps, and so on) frequently do not utilize any tree owing the absence of a pressure-containment requirement.\n\nSubsea and surface trees have a large variety of valve configurations and combinations of manual and/or actuated (hydraulic or pneumatic) valves. Examples are identified in API Specifications 6A and 17D.\n\nA basic surface tree consists of two or three manual valves (usually gate valves because of their flow characteristics, i.e. low restriction to the flow of fluid when fully open).\n\nA typical sophisticated surface tree will have at least four or five valves, normally arranged in a crucifix type pattern (hence the endurance of the term \"Christmas tree\"). The two lower valves are called the master valves (upper and lower respectively). Master valves are normally in the fully open position and are \"never\" opened or closed when the well is flowing (except in an emergency) to prevent erosion of the valve sealing surfaces. The lower master valve will normally be manually operated, while the upper master valve is often hydraulically actuated, allowing it to be used as a means of remotely shutting in the well in the event of emergency. An actuated wing valve is normally used to shut in the well when flowing, thus preserving the master valves for positive shut off for maintenance purposes. Hydraulic operated wing valves are usually built to be fail safe closed, meaning they require active hydraulic pressure to stay open. This feature means that if control fluid fails the well will automatically shut itself in without operator action.\n\nThe right hand valve is often called the flow wing valve or the production wing valve, because it is in the flowpath the hydrocarbons take to production facilities (or the path water or gas will take from production to the well in the case of injection wells). \n\nThe left hand valve is often called the kill wing valve (KWV). It is primarily used for injection of fluids such as corrosion inhibitors or methanol to prevent hydrate formation. In the North Sea, it is called the non-active side arm (NASA). It is typically manually operated.\n\nThe valve at the top is called the swab valve and lies in the path used for well interventions like wireline and coiled tubing. For such operations, a lubricator is rigged up onto the top of the tree and the wire or coil is lowered through the lubricator, past the swab valve and into the well. This valve is typically manually operated.\n\nSome trees have a second swab valve, the two arranged one on top of the other. The intention is to allow rigging down equipment from the top of the tree with the well flowing while still preserving the two-barrier rule. With only a single swab valve, the upper master valve is usually closed to act as the second barrier, forcing the well to be shut in for a day during rig down operations. However, avoiding delaying production for a day is usually too small a gain to be worth the extra expense of having a Christmas tree with a second swab valve.\n\nSubsea trees are available in either vertical or horizontal configurations with further speciality available such as dual bore, monobore, concentric, drill-through, mudline, guidlineless or guideline. Subsea trees may range in size and weight from a few tons to approximately 70 tons for high pressure, deepwater (>3000 feet) guidelineless applications. Subsea trees contain many additional valves and accessories compared to surface trees. Typically a subsea tree would have a choke (permits control of flow), a flowline connection interface (hub, flange or other connection), subsea control interface (direct hydraulic, electro hydraulic, or electric) and sensors for gathering data such as pressure, temperature, sand flow, erosion, multiphase flow, single phase flow such as water or gas.\n\n\n"}
{"id": "28916166", "url": "https://en.wikipedia.org/wiki?curid=28916166", "title": "Clarence Seamans", "text": "Clarence Seamans\n\nClarence Walker Seamans (June 5, 1854 – May 30, 1915) was an American typewriter manufacturer and executive of several organizations involved in the production and sale of the Remington typewriter, including the Union Typewriter Company and the Remington Typewriter Company.\n\nSeamans was born June 5, 1854 in Ilion, New York to Abner Clark Seamans and Caroline Matilda Williams. Seamans began work as a clerk at E. Remington and Sons, the firm at which his father was a purchasing agent, at the age of fifteen. In 1875, he began a three year stint of overseeing a silver mine in Bingham Canyon, Utah. Upon returning to the state of New York, Seamans became a bookkeeper and salesman at Fairbanks & Company, a scale manufacturer. Fairbanks had become the sole marketer of the Sholes and Glidden typewriter, produced by Seaman's former employer, E. Remington and Sons. After 1879, Seamans lived in Brooklyn with his wife Ida Gertrude Watson and their two daughters, Mabel and Dorothy.\nIn 1881, marketing of the typewriter returned to Remington. Seamans had been a \"star typewriter salesman\" and was retained and made manager of sales. The following year, Seamans partnered with Harry H. Benedict, a Remington director, and William O. Wyckoff, a Remington sales agent, to form the firm of Wyckoff, Seamans & Benedict. In 1886, Wyckoff, Seamans & Benedict purchased the typewriter business from Remington and, in 1892, formed the Remington Standard Typewriter Company with Seamans as the treasurer and general manager. A year later, Seamans was made president of the Union Typewriter Company, a trust formed from the merger of Remington Standard with several prominent typewriter manufacturers. Seamans presided over the acquisition of the Wahl Adding Machine Company, which made Union the world's largest typewriter company. Seamans remained president until being elected to chairman of the board in 1913. During this time, Seamans held a director position at several trust companies and an insurance company.\n\nSeamans died at his summer home in Pigeon Cove, Massachusetts on May 30, 1915.\n\n"}
{"id": "29879589", "url": "https://en.wikipedia.org/wiki?curid=29879589", "title": "Committee on Commercial and Industrial Policy", "text": "Committee on Commercial and Industrial Policy\n\nThe Committee on Commercial and Industrial Policy was chaired by Lord Balfour of Burleigh from 1916 to 1918. Balfour instructed its members to \"cast aside any abstract fiscal dogmas\".\n\nThe Paris Economic Conference of the Allied Powers resolved to damage the Central Powers economically. The Prime Minister H. H. Asquith appointed the committee in July 1916 in order to implement the Paris Resolutions.\n\nThe committee included W. A. S. Hewins (Conservative), Lord Faringdon (Conservative), Alfred Mond (Liberal), Lord Rhondda (Liberal), J. A. Pease (Liberal), George Wardle (Labour), Sir Henry Birchenough and Richard Hazleton (Irish Nationalist). \n\nThe committee's interim report on certain essential industries argued for a Special Industries Board to scrutinise industrial development and promote the manufacture of strategically essential products. This Board should offer state support for efficient businesses but \"failing efficient and adequate output, the Government should itself undertake the manufacture of such articles as may be essential for national safety\".\n\nThe committee's final report dealt with the future of British industry both in commercial competitiveness and capacity for war:\n\nIt is in our opinion a matter of vital importance that, alike in the old-established industries and in the new branches of manufacture which have arisen during the war, both employer and employed should make every effort to attain the largest possible volume of production, by the increased efficiency of industrial organisation and processes, by more intensive working, and by the adoption of the best and most economical methods of distribution.[And] it is only by the attainment of this maximum production and efficiency that we can hope to secure a speedy recovery of the industrial and financial position of the United Kingdom and assure its economic stability and progress.\n\n\n"}
{"id": "55238726", "url": "https://en.wikipedia.org/wiki?curid=55238726", "title": "David Zeiger (industrialist)", "text": "David Zeiger (industrialist)\n\nDavid Zeiger (February 21, 1918 - May 10, 1981) Brazilian industrialist in the area of clothing and fashion.\n\nZeiger's family immigrated from Ukraine (the part of Ukraine that was then part of Poland) to Brazil in the early 1920s. David, with his siblings and mother, met with his father Shaja Zeiger, who had left Poland a year earlier. Shaja Zeiger was a tailor, and in a few years founded Goomtex, a factory for raincoats and overcoats. As a young man, David Zeiger worked with his father, and soon became the manager of the business. After his marriage, David Zeiger began to run Pullsport—a women's fashion factory—along with his wife Mila.\n\nDavid Zeiger also dedicated his energy to the areas of culture, philanthropy, and publicity.\n\nZeiger was born in Kovel, Poland, which is now part of Ukraine. Although raised and educated within the community of Jewish immigrants in the São Paulo neighborhood of Bom Retiro, the adolescent Zeiger already had a good command of Portuguese and a curiosity for Brazilian culture that led him to seek friendships outside the small European community where he lived.\n\nHe became a member of the Espéria club, and of the Tietê regatta club in which he participated as a rower in competitions. He joined the Portugália club, which published a newspaper with texts written by various members. His news stories and fiction began to be published under the guidance of the poet Mario Gallo. Zeiger finished his education at the São Paulo Technical School of Accounting.\n\nAfter World War II, Zeiger began his philanthropic activities. He helped raise funds and contributed his own resources to the newly created state of Israel.\n\nMost of his efforts were for social causes focused on Brazil. He assisted Carmem Prudente in the establishment of the Hospital do Câncer, and in the charitable campaigns that followed. \nHe was an initial donor of the Hospital Israelita Albert Einstein. He gave medical access to his employees through the network of SERMA clinics.\n\nZeiger believed in a Brazil based on the market economy of the United States—with a vast middle class, general access to education and medical services. During the Juscelino Kubitschek administration (1956 to 1961), he found a favorable climate for this view, which would help Brazil to move out of its agrarian economy to become an industrial giant. At that time Zeiger met congressman Antônio Sílvio Cunha Bueno, with whom he developed a long friendship, and through which he began to participate more actively in the political life of the country. Zeiger promoted Cunha Bueno within the Jewish community. The friendship with the Cunha Bueno family resulted in two homages: David Zeiger Public School, and David Zeiger Street, both in São Paulo.\n\nDuring the years of the \"Economic Miracle\" in the 1960s and 1970s, David Zeiger's business expanded quickly. His factory (Cia Pullsport of Malharia) grew to occupy two buildings of seven floors with 500 employees.\n\nIn 1967, Time Magazine published a cover story with the photo of President Costa e Silva. The subject was the surge of progress in the economy of Brazil. It quoted examples of leaders in various fields of activity, and presented Zeiger as an example of an industrialist. Zeiger had an unpretentious and communicative character, which he demonstrated equally with both his company employees and important personalities.\n\nZeiger participated from the beginning in the textile fairs established by Caio de Alcântara Machado (FENIT), the board of directors of the International Art Biennial of São Paulo , and sponsored a project aimed at integrating a cultural center with theaters and concert halls in the Ibirapuera Park. He was also an art collector. In his view, the city of São Paulo had the potential to become the most important metropolis in Latin America.\n"}
{"id": "9715544", "url": "https://en.wikipedia.org/wiki?curid=9715544", "title": "EAN-8", "text": "EAN-8\n\nAn EAN-8 is an EAN/UPC symbology barcode and is derived from the longer International Article Number (EAN-13) code. It was introduced for use on small packages where an EAN-13 barcode would be too large; for example on cigarettes, pencils, and chewing gum packets. It is encoded identically to the 12 digits of the UPC-A barcode, except that it has 4 (rather than 6) digits in each of the left and right halves.\n\nEAN-8 barcodes may be used to encode GTIN-8 (8-digit Global Trade Identification Numbers) which are product identifiers from the GS1 System. A GTIN-8 begins with a 2- or 3-digit GS1 prefix (which is assigned to each national GS1 authority) followed by a 5- or 4-digit item reference element depending on the length of the GS1 prefix), and a checksum digit.\n\nEAN-8 codes are common throughout the world, and companies may also use them to encode RCN-8 (8-digit Restricted Circulation Numbers), and use them to identify own-brand products sold only in their stores. RCN-8 are a subset of GTIN-8 which begin with a first digit of 0 or 2.\n"}
{"id": "44084888", "url": "https://en.wikipedia.org/wiki?curid=44084888", "title": "EVRYTHNG", "text": "EVRYTHNG\n\nEVRYTHNG is an internet of things software company based in London, San Francisco and New York City. The company manages digital identity data in an intelligent IoT \"smart products\" cloud to connect consumer products to the Web and drive real-time applications. Niall Murphy, Andy Hobsbawm, Dominique Guinard and Vlad Trifa founded the company in January 2011. Murphy is the company's founding CEO.\n\nNiall Murphy, Andy Hobsbawm, Dominique Guinard and Vlad Trifa founded the company in January 2011. In November 2011, EVRYTHNG closed a $1 million seed funding round led by Atomico, a technology investment firm.\nEVRYTHNG announced Diageo, a British alcoholic beverages company, as its first customer in October 2012. EVRYTHNG created a Digital Identity for Diageo's whiskey brands in Brazil, Chile, Venezuela, Australia, Eastern Europe and Vietnam. In 2013, global analyst group Frost & Sullivan awarded EVRYTHNG’s IoT platform a New Product Innovation Best Practice award.\n\nIn April 2014, EVRYTHNG secured US$7 million in a series-A investment round from Atomico, BHLP LLC, Dawn Capital and Cisco Investments. It was Cisco’s first investment in a company based in both Europe and the United States. Later that year, the company received an undisclosed amount of funding from Samsung Ventures Investment Corporation. In 2014, EVRYTHNG received the best IoT start-up of the year from The Europas, a subsidiary event from TechCrunch. The company was voted IoT Breakout Startup of the Year in 2014 and 2015 Postcapes awards.\n\nEVRYTHNG's offers an IoT Smart Products Platform-as-a-Service that connects consumer products to the Web and manages real-time data to drive applications. It operates Active Digital Identities, unique digital identities for individual products in the cloud to deliver real-time interactive experiences and support services to consumers, and connect via \"cloud-to-cloud\" integration with the growing ecosystem of other connected applications and \"smart\" products. It has been referred to as a \"Product Relationship Management\" (PRM) platform. EVRYTHNG’s real-time IoT data platform works with both \"tagged\", non-electronic products (using technologies like RFID, beacons, NFC, 1D and 2D barcodes and image recognition) and natively \"connected\" products via embedded controllers, sensors and chip sets. In March 2017, EVRYTHNG raised $24.8 million Series B round of funding.  The Series B round was led by Sway Ventures and also included participation from new investors BLOC Ventures and Generation Ventures.\n\nEVRYTHNG partnered with ThinFilm, a Norway-based printed electronics company in July 2014. Additionally, EVRYTHNG partnered with Avery Dennison, a global manufacturer and distributor of packaging solutions, in September 2014. In 2012, EVRYTHNG engaged in a three-year EU COMPOSE project with IBM,the World Wide Web Consortium, universities from across Europe, several other technology companies and the Barcelona Supercomputing Centre to develop an open technical framework for Internet of Things applications and services. EVRYTHNG announced a partnership with Gooee, an LED lighting technology company, in 2015.EVERYTHING and Zappar became partners in 2017.\n"}
{"id": "1490017", "url": "https://en.wikipedia.org/wiki?curid=1490017", "title": "Electroactive polymers", "text": "Electroactive polymers\n\nElectroactive polymers, or EAPs, are polymers that exhibit a change in size or shape when stimulated by an electric field. The most common applications of this type of material are in actuators and sensors. A typical characteristic property of an EAP is that they will undergo a large amount of deformation while sustaining large forces.\n\nThe majority of historic actuators are made of ceramic piezoelectric materials. While these materials are able to withstand large forces, they commonly will only deform a fraction of a percent. In the late 1990s, it has been demonstrated that some EAPs can exhibit up to a 380% strain, which is much more than any ceramic actuator. One of the most common applications for EAPs is in the field of robotics in the development of artificial muscles; thus, an electroactive polymer is often referred to as an artificial muscle.\nThe field of EAPs emerged back in 1880, when Wilhelm Röntgen designed an experiment in which he tested the effect of an electrostatic field on the mechanical properties of a stripe of natural rubber. The rubber stripe was fixed at one end and was attached to a mass at the other. Electric charges were then sprayed onto the rubber, and it was observed that the length changed. It was in 1925 that the first piezoelectric polymer was discovered (Electret). Electret was formed by combining carnauba wax, rosin and beeswax, and then cooling the solution while it is subject to an applied DC electrical bias. The mixture would then solidify into a polymeric material that exhibited a piezoelectric effect.\n\nPolymers that respond to environmental conditions, other than an applied electric current, have also been a large part of this area of study. In 1949 Katchalsky \"et al.\" demonstrated that when collagen filaments are dipped in acid or alkali solutions, they would respond with a change in volume. The collagen filaments were found to expand in an acidic solution and contract in an alkali solution. Although other stimuli (such as pH) have been investigated, due to its ease and practicality most research has been devoted to developing polymers that respond to electrical stimuli in order to mimic biological systems.\n\nThe next major breakthrough in EAPs took place in the late 1960s. In 1969 Kawai demonstrated that polyvinylidene fluoride (PVDF) exhibits a large piezoelectric effect. This sparked research interest in developing other polymers systems that would show a similar effect. In 1977 the first electrically conducting polymers were discovered by Hideki Shirakawa \"et al.\" Shirakawa along with Alan MacDiarmid and Alan Heeger demonstrated that polyacetylene was electrically conductive, and that by doping it with iodine vapor, they could enhance its conductivity by 8 orders of magnitude. Thus the conductance was close to that of a metal. By the late 1980s a number of other polymers had been shown to exhibit a piezoelectric effect or were demonstrated to be conductive.\n\nIn the early 1990s, ionic polymer-metal composites (IPMCs) were developed and shown to exhibit electroactive properties far superior to previous EAPs. The major advantage of IPMCs was that they were able to show activation (deformation) at voltages as low as 1 or 2 volts. This is orders of magnitude less than any previous EAP. Not only was the activation energy for these materials much lower, but they could also undergo much larger deformations. IPMCs were shown to exhibit anywhere up to 380% strain, orders of magnitude larger than previously developed EAPs.\n\nIn 1999, Yoseph Bar-Cohen proposed the Armwrestling Match of EAP Robotic Arm Against Human Challenge. This was a challenge in which research groups around the world competed to design a robotic arm consisting of EAP muscles that could defeat a human in an arm wrestling match. The first challenge was held at the Electroactive Polymer Actuators and Devices Conference in 2005. Another major milestone of the field is that the first commercially developed device including EAPs as an artificial muscle was produced in 2002 by Eamex in Japan. This device was a fish that was able to swim on its own, moving its tail using an EAP muscle. But the progress in practical development has not been satisfactory.\n\nDARPA-funded research in the 1990s at SRI International and led by Ron Pelrine developed an electroactive polymer using silicone and acrylic polymers; the technology was spun off into the company Artificial Muscle in 2003, with industrial production beginning in 2008. In 2010, Artificial Muscle became a subsidiary of Bayer MaterialScience.\n\nEAP can have several configurations, but are generally divided in two principal classes: Dielectric and Ionic.\n\nDielectric EAPs are materials in which actuation is caused by electrostatic forces between two electrodes which squeeze the polymer. Dielectric elastomers are capable of very high strains and are fundamentally a capacitor that changes its capacitance when a voltage is applied by allowing the polymer to compress in thickness and expand in area due to the electric field. This type of EAP typically requires a large actuation voltage to produce high electric fields (hundreds to thousands of volts), but very low electrical power consumption. Dielectric EAPs require no power to keep the actuator at a given position. Examples are electrostrictive polymers and dielectric elastomers.\n\nFerroelectric polymers are a group of crystalline polar polymers that are also ferroelectric, meaning that they maintain a permanent electric polarization that can be reversed, or switched, in an external electric field. Ferroelectric polymers, such as polyvinylidene fluoride (PVDF), are used in acoustic transducers and electromechanical actuators because of their inherent piezoelectric response, and as heat sensors because of their inherent pyroelectric response.\nElectrostrictive graft polymers consist of flexible backbone chains with branching side chains. The side chains on neighboring backbone polymers cross link and form crystal units. The backbone and side chain crystal units can then form polarized monomers, which contain atoms with partial charges and generate dipole moments, shown in Figure 2. When an electrical field is applied, a force is applied to each partial charge and causes rotation of the whole polymer unit. This rotation causes electrostrictive strain and deformation of the polymer.\n\nMain-chain liquid crystalline polymers have mesogenic groups linked to each other by a flexible spacer. The mesogens within a backbone form the mesophase structure causing the polymer itself to adopt a conformation compatible with the structure of the mesophase. The direct coupling of the liquid crystalline order with the polymer conformation has given main-chain liquid crystalline elastomers a large amount of interest. The synthesis of highly oriented elastomers leads to have a large strain thermal actuation along the polymer chain direction with temperature variation resulting in unique mechanical properties and potential applications as mechanical actuators.\n\n\nElectrorheological fluids change the viscosity of a solution with the application of an electric field. The fluid is a suspension of polymers in a low dielectric-constant liquid. With the application of a large electric field the viscosity of the suspension increases. Potential applications of these fluids include shock absorbers, engine mounts and acoustic dampers.\n\nIonic polymer-metal composites consist of a thin ionomeric membrane with noble metal electrodes plated on its surface. It also has cations to balance the charge of the anions fixed to the polymer backbone. They are very active actuators that show very high deformation at low applied voltage and show low impedance. Ionic polymer-metal composites work through electrostatic attraction between the cationic counter ions and the cathode of the applied electric field, a schematic representation is shown in Figure 3. These types of polymers show the greatest promise for bio-mimetic uses as collagen fibers are essentially composed of natural charged ionic polymers. Nafion and Flemion are commonly used ionic polymer metal composites.\n\nStimuli-responsive gels (hydrogels, when the swelling agent is an aqueous solution) are a special kind of swellable polymer networks with volume phase transition behaviour. These materials change reversibly their volume, optical, mechanical and other properties by very small alterations of certain physical (e.g. electric field, light, temperature) or chemical (concentrations) stimuli. \n\nDielectric polymers are able to hold their induced displacement while activated under a DC voltage. This allows dielectric polymers to be considered for robotic applications. These types of materials also have high mechanical energy density and can be operated in air without a major decrease in performance. However, dielectric polymers require very high activation fields (>10 V/µm) that are close to the breakdown level.\n\nThe activation of ionic polymers, on the other hand, requires only 1-2 volts. They however need to maintain wetness, though some polymers have been developed as self-contained encapsulated activators which allows their use in dry environments. Ionic polymers also have a low electromechanical coupling. They are however ideal for bio-mimetic devices.\n\nWhile there are many different ways electroactive polymers can be characterized, only three will be addressed here: stress–strain curve, dynamic mechanical thermal analysis, and dielectric thermal analysis.\n\nStress strain curves provide information about the polymer's mechanical properties such as the brittleness, elasticity and yield strength of the polymer. This is done by providing a force to the polymer at a uniform rate and measuring the deformation that results. An example of this deformation is shown in Figure 4. This technique is useful for determining the type of material (brittle, tough, etc.), but it is a destructive technique as the stress is increased until the polymer fractures.\n\nBoth dynamic mechanical analysis is a non destructive technique that is useful in understanding the mechanism of deformation at a molecular level. In DMTA a sinusoidal stress is applied to the polymer, and based on the polymer's deformation the elastic modulus and damping characteristics are obtained (assuming the polymer is a damped harmonic oscillator). Elastic materials take the mechanical energy of the stress and convert it into potential energy which can later be recovered. An ideal spring will use all the potential energy to regain its original shape (no damping), while a liquid will use all the potential energy to flow, never returning to its original position or shape (high damping). A viscoeleastic polymer will exhibit a combination of both types of behavior.\n\nDETA is similar to DMTA, but instead of an alternating mechanical force an alternating electric field is applied. The applied field can lead to polarization of the sample, and if the polymer contains groups that have permanent dipoles (as in Figure 2), they will align with the electrical field. The permittivity can be measured from the change in amplitude and resolved into dielectric storage and loss components. The electric displacement field can also be measured by following the current. Once the field is removed, the dipoles will relax back into a random orientation.\n\nEAP materials can be easily manufactured into various shapes due to the ease in processing many polymeric materials, making them very versatile materials. One potential application for EAPs is that they can potentially be integrated into microelectromechanical systems (MEMS) to produce smart actuators.\n\nAs the most prospective practical research direction, EAPs have been used in artificial muscles. Their ability to emulate the operation of biological muscles with high fracture toughness, large actuation strain and inherent vibration damping draw the attention of scientists in this field.\n\nIn recent years, \"electro active polymers for refreshable Braille displays\" has emerged to aid the visually impaired in fast reading and computer assisted communication. This concept is based on using an EAP actuator configured in an array form. Rows of electrodes on one side of an EAP film and columns on the other activate individual elements in the array. Each element is mounted with a Braille dot and is lowered by applying a voltage across the thickness of the selected element, causing local thickness reduction. Under computer control, dots would be activated to create tactile patterns of highs and lows representing the information to be read.\n\nVisual and tactile impressions of a virtual surface are displayed by a high resolution tactile display, a so-called \"artificial skin\" (Fig.6) . These monolithic devices consist of an array of thousands of multimodal modulators (actuator pixels) based on stimuli-responsive hydrogels. Each modulator is able to change individually their transmission, height and softness. Besides their possible use as graphic displays for visually impaired such displays are interesting as free programmable keys of touchpads and consoles.\n\nEAP materials have huge potential for microfluidics e.g. as drug delivery systems, microfluidic devices and lab-on-a-chip. A first microfluidic platform technology reported in literature is based on stimuli-responsive gels. To avoid the electrolysis of water hydrogel-based microfluidic devices are mainly based on temperature-responsive polymers with lower critical solution temperature (LCST) characteristics, which are controlled by an electrothermic interface. Two types of micropumps are known, a diffusion micropump and a displacement micropump. Microvalves based on stimuli-responsive hydrogels show some advantageous properties such as particle tolerance, no leakage and outstanding pressure resistance. Besides these microfluidic standard components the hydrogel platform provides also chemical sensors and a novel class of microfluidic components, the chemical transistors (also referred as chemostat valves). These devices regulate a liquid flow if a threshold concentration of certain chemical is reached. Chemical transistors form the basis of microchemomechanical fluidic integrated circuits. \"Chemical ICs\" process exclusively chemical information, are energy-self-powered, operate automatically and are able for large-scale integration.\n\nAnother microfluidic platform is based on ionomeric materials. Pumps made from that material could offer low voltage (battery) operation, extremely low noise signature, high system efficiency, and highly accurate control of flow rate.\n\nAnother technology that can benefit from the unique properties of EAP actuators is optical membranes. Due to their low modulus, the mechanical impedance of the actuators, they are well-matched to common optical membrane materials. Also, a single EAP actuator is capable of generating displacements that range from micrometers to centimeters. For this reason, these materials can be used for static shape correction and jitter suppression. These actuators could also be used to correct for optical aberrations due to atmospheric interference.\n\nSince these materials exhibit excellent electroactive character, EAP materials show potential in biomimetic-robot research, stress sensors and acoustics field, which will make EAPs become a more attractive study topic in the near future. They have been used for various actuators such as face muscles and arm muscles in humanoid robots.\n\nThe field of EAPs is far from mature, which leaves several issues that still need to be worked on. The performance and long-term stability of the EAP should be improved by designing a water impermeable surface. This will prevent the evaporation of water contained in the EAP, and also reduce the potential loss of the positive counter ions when the EAP is operating submerged in an aqueous environment. Improved surface conductivity should be explored using methods to produce a defect-free conductive surface. This could possibly be done using metal vapor deposition or other doping methods. It may also be possible to utilize conductive polymers to form a thick conductive layer. Heat resistant EAP would be desirable to allow operation at higher voltages without damaging the internal structure of the EAP due to the generation of heat in the EAP composite. Development of EAPs in different configurations (e.g., fibers and fiber bundles), would also be beneficial, in order to increase the range of possible modes of motion.\n\n\n"}
{"id": "20306638", "url": "https://en.wikipedia.org/wiki?curid=20306638", "title": "Energy &amp; Environmental Science", "text": "Energy &amp; Environmental Science\n\nEnergy & Environmental Science is a monthly peer-reviewed scientific journal publishing original (primary) research and review articles. The journal covers work of an interdisciplinary nature in the biochemical and biophysical sciences and chemical engineering disciplines. \"Energy & Environmental Science\" is published by the Royal Society of Chemistry and the editor-in-chief is Nathan Lewis (Caltech).\n\nAccording to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 30.067, ranking it 4th out of 163 journals in the category \"Chemistry, Multidisciplinary\", first out of 88 journals in the category \"Energy & Fuels\", first out of 135 journals in the category \"Engineering, Chemical\", and first among 225 journals in the category \"Environmental Sciences\".\n\n\"Energy & Environmental Science\" publishes the following types of articles: Research Papers (original scientific work); Review Articles, Perspectives, and Minireviews (feature review-type articles of broad interest); Communications (original scientific work of an urgent nature), Opinions (personal, often speculative, viewpoints or hypotheses on a current topic), and Analysis Articles (in-depth examination of energy and environmental technologies, strategies, policies, and general conceptual frameworks of general interest).\n\nAccording to the Thomson Reuters Master Journal List and CASSI, this journal is indexed by the following services:\n"}
{"id": "50238474", "url": "https://en.wikipedia.org/wiki?curid=50238474", "title": "Exit scam", "text": "Exit scam\n\nAn exit scam is a confidence trick where an established business stops shipping orders while continuing to receive payment for new orders. If the entity had a good reputation, then it can take some time before it is widely recognized that orders are not shipping, and the entity can then make off with the money paid for unshipped orders. Customers that trusted the business don't realize that no orders are being fulfilled until the business has already disappeared.\n\nIndividual vendors often reach a point of reputation maturity whereby they have sold sufficient product to have accumulated both significant reputation and escrowed funds, that many may choose to exit with those funds rather than compete at the higher-volume higher-priced matured product level.\n\nExit scams can be a tempting alternative to a non-fraudulent shutdown of illegal operations if the operation was going to shut down anyway for other reasons. If an illegal entity thrives by selling drugs, for example, it is usually not an option for the cheated buyers to notify law enforcement. The examples mentioned in news articles are online sellers, where the buyer does not know the identity or physical location of the scammer, and therefore has little recourse.\n\nThe online black market Evolution is cited as the biggest exit scam yet as of 2016. The administrators apparently made off with $12 million in bitcoin, which was held in escrow on the marketplace.\n"}
{"id": "521753", "url": "https://en.wikipedia.org/wiki?curid=521753", "title": "Flashover", "text": "Flashover\n\nA flashover is the near-simultaneous ignition of most of the directly exposed combustible material in an enclosed area. When certain organic materials are heated, they undergo thermal decomposition and release flammable gases. Flashover occurs when the majority of the exposed surfaces in a space are heated to their autoignition temperature and emit flammable gases (see also flash point). Flashover normally occurs at or for ordinary combustibles, and an incident heat flux at floor level of .\n\nAn example of flashover is ignition of a piece of furniture in a domestic room. The fire involving the initial piece of furniture can produce a layer of hot smoke which spreads across the ceiling in the room. The hot buoyant smoke layer grows in depth, as it is bounded by the walls of the room. The radiated heat from this layer heats the surfaces of the directly exposed combustible materials in the room, causing them to give off flammable gases via pyrolysis. When the temperatures of the evolved gases become high enough, these gases will ignite throughout their extent.\n\nThe original Swedish terminology related to the term 'flashover' has been altered in its translation to conform with current European and North American accepted [scientific] definitions as follows:\n\nFlashover is one of the most-feared phenomena among firefighters. Firefighters are taught to recognize the signs and symptoms of rollovers and flashovers and to avoid backdrafts. For example, they have certain routines for opening closed doors to buildings and compartments on fire, known as door entry procedures, ensuring fire crew safety where possible.\n\nThe following are some of the signs that firefighters are looking for when they attempt to determine whether a flashover is likely to occur.\n\nFirefighters memorize a chant to help remember these during training: \"Thick dark smoke, high heat, rollover, free burning.\"\n\nThe colour of the smoke is often considered, as well, but there is no connection between the colour of the smoke and the risk of flashovers. Traditionally, black, dense smoke was considered particularly dangerous, but history shows this to be an unreliable indicator. For example, there was a fire in a rubber mattress factory in London in 1975 which produced white smoke. The white smoke wasn't considered dangerous, so firefighters decided to ventilate, which caused a smoke explosion and killed two firefighters. The white smoke from the pyrolysis of the rubber turned out to be extremely flammable.\n\n"}
{"id": "18420686", "url": "https://en.wikipedia.org/wiki?curid=18420686", "title": "Fly fishing tackle", "text": "Fly fishing tackle\n\nFly fishing tackle comprises the fishing tackle or equipment typically used by fly anglers. Fly fishing tackle includes:\n\nFly rods normally vary between 2 m (6 ft) and 4 m (13 ft) in length with the most common length sold being 2.74 m (9 ft). Rod lengths are typically given in imperial measurements of feet and inches. Fly rods and lines are designated as to their \"weight\", typically written as Nwt where 'N' is the number (\"e.g.\" 8wt, 9wt, 10wt).\n\nRods are matched to the line according to weight. The rod's manufacturer will mark on the rod the fly line weights for which a rod has been designed. One-weight (1wt) rods and lines are the lightest; the weight designations increase in whole number increments as the rod becomes heavier. The heaviest rods and lines readily available currently are 16-weight (16wt). In general, 1wt through 2wt rods would be used for the cast small flies for small trout and panfish; 3wt and 4wt rods are popular for small-stream fishing; 5wt is often considered the all-around rod for trout and general freshwater fishing; 6wt and 7wt rods are used on large rivers and for fishing with streamers, for larger warmwater species, and occasionally in calm inshore conditions for smaller saltwater species; 8wt to 9wt rods and lines might be used for steelhead or salmon in medium rivers, as well as for bass fishing with large flies, fishing for large carp, and general inshore saltwater use; and 10-11wt rods and lines would be used for pursuing large saltwater gamefish (tarpon, snook) under conditions of high wind or surf. The heaviest rods (12–16wt) are mostly used for bluewater species (billfish, tuna) while fishing from a boat. The characteristics of these rods reflect the fact that only short casts are needed during this type of fly fishing, while lifting ability is at a premium.\n\nThe species pursued, under which conditions, will largely determine the weight of rod selected. Next, it is important to match the line to the weight of the rod. Using too heavy a line on too light on a rod, or \"vice versa\", will dramatically affect casting performance. It may also permanently warp the rod blank. Generally speaking, you can safely go one line weight more or less (i.e. using an 8wt or 10wt line on a 9wt rod). There are also rods stamped with a range of weights. For example, a rod may be rated 7-8wt. This indicates the rod is designed for either a 7 or 8 weight fly line. There are also some rods rated for wider ranges (e.g. 8-9-10wt). The drawback to multi-rated rods is that compromises in flexibility or action are made in order to accommodate a wider range of line weights. For example, a rod rated for 8-9 weight line will be slightly stiffer than a straight 8wt but slightly softer than a straight 9wt rod. In general, the more expensive the rod, the more likely it'll be designated for a single line weight rather than a range.\n\nSaltwater fly rods are built to handle powerful fish and to cast large, bulky flies over longer distances or into strong winds. Saltwater fly rods are normally fitted with heavier, corrosion-resistant fittings. The reel seat may also be equipped with a short extension often called a \"fighting butt\". Rods for saltwater fishing fall into the 8 to 15 weight class, with 12-weight being typical for most larger species like tuna, dorado (mahi-mahi) and wahoo (ono).\n\nNote that the line weight generalities described above hold for both single-handed fly rods as well as double-handed fly rods used for Spey Casting, but the length and usage of double-handed rods often varies significantly.\n\nThe earliest fly rods were made from greenheart, a tropical wood, and later bamboo originating in the Tonkin area of Guangdong Province in China. The mystical appeal of handmade split-cane rods has endured despite the emergence over the last 50 years of cheaper rod-making materials that offer more durability and performance: fiberglass and carbon fiber.\n\nSplit-cane bamboo fly rods combine sport, history and art. It may take well over 100 hours for an experienced rod builder to select and split the raw cane and then to cure, flame, plane, file, taper, glue, wrap and finish each rod. Quality rods made by famous rod makers may sell for prices well beyond US$2,000; a new rod from a competent, contemporary (though not famous) builder may sell for nearly as much. These rods offer grace, form, and, with their solid mass, surprising strength. Bamboo rods vary in action from slow to fast depending on the taper of the rod. In competent hands, they provide the pinnacle in performance.\n\nToday, fly rods are mainly made from carbon fiber/graphite with cork or, less frequently, hypalon being favored for the grip. Such rods generally offer greater stiffness than bamboo, are much more consistent and less expensive to manufacture, and require less maintenance. Fiberglass was popular for rods constructed in the years following World War II and was the \"material of choice\" for many years. However, by the late 1980s, carbon/graphite composite rods (including premium graphite/boron and graphite/titanium blends) had emerged as the materials used by most fly rod manufacturers. These premium rods offer a stiffness, sensitivity, and feel unmatched by any other synthetic material. Graphite composites are especially well-suited to the construction of multi-piece rods since the joints, known as ferrules, in better-quality graphite rods do not significantly affect overall flex or rod action. Today's modern carbon graphite composite fly rods are available in a wide range of sizes and types, from ultralight trout rods to bass fishing rods and two-handed \"spey\" rods.\n\n\"Fly reels\", or \"fly casting reels\", with a few exceptions, are really little more than line-storage devices. In use, a fly angler strips line off the reel with one hand while casting and manipulating the rod with the other. Slack line is picked up by rotating the reel spool. Even today, the vast majority of fly reels are manually operated, single-action reels of rather simple construction, with a simple click-pawl drag system. However, in recent years, more advanced fly reels have been developed for larger fish and more demanding conditions. These newer reels feature disc-type mechanical, adjustable drag systems to permit the use of lighter leaders and tippets, or to successfully capture fish that undertake long, powerful runs. Many newer fly reels have large arbors to increase the speed of the retrieve and to improve drag performance during long runs. In order to prevent corrosion, saltwater fly reels often use aluminum frames and spools or stainless steel components with sealed bearing and drive mechanisms.\n\nSome reels with simple click drags are designed to be \"palmed\" when a fish runs with the line. Palming allows the angler to add additional drag with a light touch of the palm to the rim of the reel. On some reels, palming is difficult or impossible because the spool is fully skirted. With such reels, the only drag that an angler can apply to the line is with one or more fingers directly pinching the line.\n\nThe fly line can be retrieved using either hand. Most modern fly reels can be converted to or from left-hand or right-hand retrieve. Many fly anglers who have come to the sport after spending some years as spin casting anglers are more comfortable with a left-hand retrieve. Right-handed \"big game\" fishers may find the right hand retrieve more efficient. In either case, modern large-arbor reels can be retrieved with fair efficiency using either hand to retrieve.\n\nFly reels are often rated for a specific weight and type of fly line in combination with a specific strength and length of backing. For example, the documentation supplied with a reel may state that the reel can take of 50 pound-test backing and of fly line. An angler should be able to \"load\" the reel with the specified length of line and backing and still have sufficient room between the line and the spool's edge. As well, many modern reels are designed to take interchangeable spools. Such spools can be quickly switched, thus allowing an angler to change the type of line in a matter of minutes.\n\nFly line is a specialized fishing line that supplies the weight or mass necessary to cast an artificial fly with a fly rod. The first fly lines were constructed of woven horsehair that eventually evolved into woven silk fiber lines. As plastics technologies improved, synthetic materials gradually replaced natural materials in the construction of fly lines. Today’s fly lines are generally constructed of an outside synthetic layer that determines the line’s slickness, buoyancy, shape and weight over an inside core material which determines the line’s strength and flexibility. The typical fly line is long although longer fly lines are manufactured. Fly lines have several characteristics which can be used to describe any given fly line. Some of these characteristics are based on industry standards and norms while others vary considerably between manufacturers.\n\nFly line manufacturers design and formulate their fly lines with other characteristics as well. Some fly lines are specifically formulated for warm water and cold water conditions, fresh and salt water conditions as well as designs that target a specific type or fish or fishing.\n\nFly line is typically attached to a length of braided or gelspun line wound on the fly reel known as backing. The length and breaking strength of the backing required depends on the overall line capacity of the reel and the type of fish being sought. Backing may be as short as a few yards up to hundreds of yards if the reel has the capacity. Backing can serve two purposes. One is to create a larger diameter spooling surface that allows the fly line to fill the entire fly reel. The other is to provide additional line for fighting heavy or hard fighting fish. A fast running or hard fighting fish may take line from the reel and get \"into the backing.\"\n\nTerminal fly fishing tackle connects the fly line to the artificial fly. This is typically a tapered or level ‘’leader’’ with a ‘’tippet’’ section. Other terminal tackle may include small ‘’strike indicators’’ or weights added to the leader to assist in strike detection and presentation. The leader is a section of fishing line that is attached directly to the end of the fly line. The ‘’tippet’’ section is a section of fishing line attached to the leader to which the artificial fly is attached. Leaders and tippets play a key role in the presentations of the fly to the fish and the subsequent landing of a fish when caught. Leaders and tippets are generally constructed of monofilament or fluorocarbon fishing line. In some fly fishing situations involving toothy fish, tippets are constructed of braided or single strand stainless steel wire.\n\nThe tip of a fly line is usually more than 0.030\" thick and the eye of a fly hook\nmay be less than a tenth that size. The two must be joined by a \"leader,\" usually 7 to 10 feet long, nowadays of nylon or similar monofilament, either extruded in a continuous taper or made by knotting together several lengths of nylon of diminishing thickness. These taper from about 0.020\" diameter to 0.010\" for a large fly or 0.007\" for a size 14 trout fly. The right size and stiffness of nylon also helps the leader \"turn over\" when cast, so as to present the fly naturally, as if not connected with a fishing rod. Anglers usually carry spools of extra fine nylon, to replace the tippet or other sections of a leader as required.\n\nLevel leaders are a single diameter of line that connects the fly line to the tippet or fly. Level leaders are typically much shorter than tapered leaders and used with sinking fly lines and heavy flies. Level leaders when used with sinking lines help get the fly deeper faster.\n\nThe tippet connects the leader to the fly. Tippet sizes were traditionally expressed as \"X\" sizes in a scale based on silkworm gut leader material, but nowadays gut has been superseded by a variety of synthetics, mainly monofilaments. Monofilament is calibrated in thousandths of an inch from 0.020\" and larger (used for leader butts or in saltwater fishing) to 0.011\" (old size 0X) and as small as 0.003\" (8X.) Fly fishing records are classed by tippet diameter, not breaking strength, which varies between material and manufacturers. Choice of tippet involves a tradeoff: finer tippets are less visible to the fish, resulting in more strikes, but are more easily abraded and broken. Stiffer or softer tippets may be used depending on water temperature, visibility, and need for abrasion resistance. Some toothy species require specially strong and durable tippets so they will not be bitten through, called \"shock tippets,\" made of thick monofilament or stainless steel wire.\n\n\nAccessories include an abundance of different tools and gadgets used by fly anglers to maintain and prepare their tackle, deal with fish being caught and personal clothing and apparel specifically designed for fly fishing comfort and safety. Accessories include fly boxes designed to store and carry artificial flies.\n\n\n\nFly boxes are designed to store and carry artificial flies in an organized manner. The typical fly angler carries one or more fly boxes while fly fishing. Fly boxes are available in a wide variety of sizes, styles and configurations. Fly boxes store flies using a variety of foam, plastic, clip, metal and containing mechanisms. Probably the most famous fly boxes are made in England by Richard Wheatley who have been manufacturing these since 1860 and maybe the oldest continuous makers of fly fishing tackle in the world.\n"}
{"id": "54633736", "url": "https://en.wikipedia.org/wiki?curid=54633736", "title": "Football Live", "text": "Football Live\n\nFootball Live was the name given to the project and computer system created and utilised by PA Sport to collect Real Time Statistics from major English & Scottish Football Matches and distribute to most leading media organisations. At the time of its operation, more than 99% of all football statistics displayed across Print, Internet, Radio & TV Media outlets would have been collected via Football Live.\n\nPrior to implementation of Football Live, the collection process consisted of a news reporter or press officer at each club calling the Press Association, relaying information on Teams, Goals and Half-Time & Full Time.\n\nThe basis for Football Live was to have a representative of the Press Association (FBA - Football Analyst) at every ground. Throughout the whole match they would stay on an open line on a mobile phone, constantly relaying in real time statistical information for every :\n\nThis information would be entered in real time and passed to our media customers.\n\nThe Football Live project was in use from Season 2001/02 until the service was taken over by Opta in 2013/14\n\nThe most famous use for the Football Live data was for the Vidiprinter services on BBC & Sky Sports, allowing goals to be viewed on TV screens within 20 seconds of the event happening.\n\nFrom its inception in 2001/02 season, the following leagues/competitions were fully covered by Football live\n\nDuring the early development stages, the initial idea was to employee ex-referees to act as Football Analysts, but this was soon dismissed in favour of ex-professional Footballers. The most famous of which were Brendon Ormsby, Mel Sterland, Jimmy Case, Imre Varadi. All the FBA's were supplied and managed by the Professional Football Association (PFA), with day-to-day responsibility lying with Paul Allen and Chris Joslin from the PFA.\n"}
{"id": "1531568", "url": "https://en.wikipedia.org/wiki?curid=1531568", "title": "General-purpose input/output", "text": "General-purpose input/output\n\nA general-purpose input/output (GPIO) is an uncommitted digital signal pin on an integrated circuit or electronic circuit board whose behavior—including whether it acts an input or output—is controllable by the user at run time.\n\nGPIOs have no predefined purpose and are unused by default. If used, the purpose and behavior of a GPIO is defined and implemented by the designer of higher assembly-level circuitry: the circuit board designer in the case of integrated circuit GPIOs, or system integrator in the case of board-level GPIOs.\n\nIntegrated circuit (IC) GPIOs are implemented in a variety of ways. Some ICs provide GPIOs as a primary function whereas others include GPIOs as a convenient \"accessory\" to some other primary function. Examples of the former include the Intel 8255, which interfaces 24 GPIOs to a parallel bus, and various GPIO \"expander\" ICs, which interface GPIOs to serial buses such as I²C and SMBus. An example of the latter is the Realtek ALC260 IC, which provides eight GPIOs in addition to its primary function of audio codec.\n\nMicrocontroller ICs typically include GPIOs. Depending on the application, a microcontroller's GPIOs may comprise its primary interface to external circuitry or they may be just one type of I/O used among several, such as analog I/O, counter/timer, and serial communication.\n\nIn some ICs, particularly microcontrollers, a GPIO pin may be capable of alternate functions. Often in such cases, it is necessary to configure the pin to operate as a GPIO (vs. its alternate functions) in addition to configuring the GPIO's behavior. Some microcontroller devices (e.g., Microchip dsPIC33 family) incorporate internal signal routing circuitry that allows GPIOs to be programmatically mapped to device pins. FPGAs extend this capability by allowing GPIO pin mapping, instantiation and architecture to be programmatically controlled.\n\nMany circuit boards expose board-level GPIOs to external circuitry through integrated electrical connectors. Typically, each such GPIO is accessible via a dedicated connector pin.\n\nLike IC-based GPIOs, some boards merely include GPIOs as a convenient, auxiliary resource that augments the board's primary function, whereas in other boards the GPIOs are the central, primary function of the board. Some boards, which typically are classified as multi-function I/O boards, are a combination of both; such boards provide GPIOs along with other types of general-purpose I/O. GPIOs are also found on embedded controller boards such as Arduino, BeagleBone and Raspberry Pi.\n\nBoard-level GPIOs are often endowed with capabilities which typically are not found in IC-based GPIOs. For example, schmitt-trigger inputs, high-current output drivers, optical isolators, or combinations of these may be used to buffer and condition the GPIO signals and to protect board circuitry. Also, higher-level functions are sometimes implemented, such as input debounce, input signal edge detection, and pulse-width modulation (PWM) output.\n\nGPIOs are used in a diverse variety of applications, limited only by the electrical and timing specifications of the GPIO interface and the ability of software to interact with GPIOs in a sufficiently timely manner.\n\nGPIOs typically employ standard logic levels and cannot supply significant current to output loads. When followed by an appropriate high-current output buffer (or mechanical or solid-state relay), a GPIO may be used to control high-power devices such as lights, solenoids, heaters, and motors (e.g., fans and blowers). Similarly, an input buffer, relay or optoisolator is often used to translate an otherwise incompatible signal (e.g., high voltage) to the logic levels required by a GPIO.\n\nIntegrated circuit GPIOs are commonly used to control or monitor other circuitry (including other ICs) on a board. Examples of this include enabling and disabling the operation of (or power to) other circuitry, reading the states of on-board switches and configuration shunts, and driving LED status indicators. In the latter case, a GPIO can, in many cases, supply enough output current to directly power an LED without using an intermediate buffer.\n\nMultiple GPIOs are sometimes used together as a bit-banged communication interface. For example, two GPIOs may be used to implement a serial communication bus such as I²C, and four GPIOs can be used to implement an SPI bus; these are typically used to facilitate serial communication with ICs and other devices which have compatible serial interfaces, such as sensors (e.g., temperature sensors, pressure sensors, accelerometers) and motor controllers. Taken to the extreme, this technique may be used to implement an entire parallel bus, thus allowing communication with bus-oriented ICs or circuit boards.\n\nAlthough GPIOs are fundamentally digital in nature, they are often used to control linear processes. For example, a GPIO may be used to control motor speed, light intensity, or temperature. Typically, this is accomplished via PWM, in which the duty cycle of the GPIO output signal determines the effective magnitude of the process control signal. For example, when controlling light intensity, the light may be dimmed by reducing the GPIO duty cycle. Some linear processes require a linear control voltage; in such cases it may be feasible to connect a GPIO -- which is operated as a PWM output -- to an RC filter to create a simple and inexpensive digital-to-analog converter.\n\nGPIO interfaces vary widely. In some cases, they are simple—a group of pins that can switch as a group to either input or output. In others, each pin can be set up to accept or source different logic voltages, with configurable drive strengths and pull ups/downs. Input and output voltages are typically—though not always—limited to the supply voltage of the device with the GPIOs, and may be damaged by greater voltages.\n\nA GPIO pin's state may be exposed to the software developer through one of a number of different interfaces, such as a memory mapped peripheral, or through dedicated IO port instructions. Some GPIOs have 5 V tolerant inputs: even when the device has a low supply voltage (such as 2 V), the device can accept 5 V without damage.\n\nA GPIO port is a group of GPIO pins (typically 8 GPIO pins) arranged in a group and controlled as a group.\n\nGPIO capabilities may include:\n\n\n"}
{"id": "43097160", "url": "https://en.wikipedia.org/wiki?curid=43097160", "title": "Hetzner", "text": "Hetzner\n\nHetzner Online GmbH is an Internet hosting company and data center operator based in Gunzenhausen, Germany.\n\nIt should not be confused with its South African namesake and partner company, Hetzner (Pty) Ltd — the two are separate companies registered and incorporated in their own rights under the applicable country laws. They do not have the same shareholders.\n\nHetzner Online GmbH (Gesellschaft mit beschränkter Haftung) began operations in Germany in 1997 under the name Hetzner Online Services. Between 2000 and 2015, Hetzner Online in Germany operated under the legal status AG (\"Aktiengesellschaft\"). In 2015, it changed its legal status to GmbH. \n\nThe company is named after its founder Martin Hetzner. Hetzner Online owns and operates three data center parks in Nuremberg and Falkenstein/Vogtland (Germany), and Helsinki (Finland). In addition, Hetzner Online is a co-investor in the Cinia C-Lion project, which connected Helsinki and Rostock, Germany together with a 1,100 km long submarine fiberglass cable. This cable will provide a high-speed connection between Hetzner's German data centers and the one under construction in Finland.\n\nIn 2017, Hetzner Online celebrated its 20th anniversary with a multi-day event at its data center park in Falkenstein. The event, \"Willkommen im Internet\", included technical workshops, a tour of the facility, and a public, open-air concert by Austrian singer, Christina Stürmer.\n\nThe head office of Hetzner Online is based in Gunzenhausen, Bavaria, in Germany. It has a partner company in South Africa: Hetzner South Africa (Pty.) Ltd. which operates data centers in Midrand and Cape Town (South Africa).\n\nHetzner Online provides dedicated hosting, shared web hosting, virtual private servers, managed servers, domain names, SSL certificates, storage boxes, and cloud solutions. At the data center parks located in Nuremberg, Falkenstein and Helsinki/Finnland, customers can also connect their hardware to Hetzner Online's energy-efficient and state of the art infrastructure and network with the company's colocation services. The company operates a server auction site online where older servers are auctioned off in the form of a Dutch auction.\n\nHetzner Online has a domain name registrar arrangement with ICANN (for registering domains under .com, .net and .org and others), DENIC (for .de), and nic.at (for .at).\n\nHetzner Online's datacenter projects are coordinated and implemented in-house with as little outsourcing as possible. Data center units served by multiple redundant uplinks, including 300 Gbit/s to DE-CIX and fiber optic links to Nuremberg and Frankfurt. Colocation facilities are sited at all data center parks in Nuremberg, Falkenstein/Vogtland in Germany and Helsinki in Finland.\n\nEnergy for normal operations at the data center parks is used exclusively from renewable sources, in Germany from hydropower and in Finland from wind power. In the German data center parks, this energy is provided by Energiedienst AG. In addition the company strives to conserver energy with its use of direct free cooling, double-height raised floor, components selected on the basis of energy-consumption, waste heat recycling, and intelligent lighting solutions. In 2011, TÜV SÜD certified Hetzner's data centers as an \"Energy Efficient Enterprise\".\n\nTo prevent possible faults caused by problems with the electrical network, Hetzner Online obtains its power in a redundant manner from the network provider. The current is fed from the neighbouring transformer station to the redundant transformers via two separate lines. The A/B busbar concept, in conjunction with the use of static transfer switches (STS), ensures a higher availability of the power supply. The redundant modular UPS system is able to secure the complete power supply in case of a module failure. In the event of a longer power failure, a diesel generator takes over the entire power supply of the data center. In addition, a battery monitoring system permanently monitors the condition of the battery blocks.\n\nDirect free cooling is used for cooling IT hardware. In contrast to conventional systems, this method uses outside air for cooling. If the outdoor temperature rises, the system automatically switches to mixed operation and cools the air via compressors. The cold air is then directed to the air intakes of the servers via the raised floors. This is made possible by cold aisle containment, in which the currents of cold and warm air are separated and deliberately channeled in specific directions. The warm air in the corridors and near the ceiling of the data center are channeled out through the roof in a concentrated manner. The remaining \"waste heat\" is not wasted. In fact, it is used to heat offices. This system, which was designed in house, not only effecitvely cools the hardware; it also reduces the use of cold air and increases the efficiency of cooling, thereby reducing energy consumption.\n\nThe backbone is set up in the form of a ring network between the datacenter locations Nuremberg and Falkenstein as well as the most important Internet location, Frankfurt. All locations are connected to central exchange nodes such as DE-CIX, AMS-IX, DATA-IX and V-IX via the company's own fiber optic network.\n\nAn extensive array of measuring devices enables the Hetzner Online team to closely monitor energy consumption and analyze the quality of the power network in real time. The system measures power consumption 24/7 at different intervals at all levels in the data center and sends warnings if it finds of anomalies. A remote-readable software calculates important key figures, such as PUE/EER, and identifies optimization potentials.\n\nThe internationally recognized standard for information security certifies that Hetzner Online GmbH has established and implemented an appropriate information security management system (ISMS). Hetzner Online utilizes the ISMS in its infrastructure and complete operations for the data center parks in the Nuremberg and Falkenstein locations. FOX Certification, a third party certification authority, audited Hetzner Online's data center parks for the certification process. The datacenter parks are thus certified in accordance to ISO/IEC 27001.\n\nThis certification confirms that Hetzner Online will uphold strict information security standards. It states that Hetzner Online provides up-to-standard systems for security management, data security, data confidentiality and availability of IT systems. The certificate proves adequate security management, data security, confidentiality of information and availability of IT systems. It also confirms that the safety standards are continuously improved and sustainably monitored.\n\nHetzner Online is a sponsor of a variety of humanitarian and educational charity organizations. Sponsoring activities in the past include Habitat for Humanity Germany and their building projects in Africa. In 2010, Hetzner Online donated setup fees received from new orders during the football World Cup 2010 to the Youth Build Project 2010. In 2012 Habitat for Humanity Germany patron, actress Alexandra Neldel travelled to Mozambique and South Africa to learn more about Hetzner Online's projects. The company received the Habitat for Humanity Corporate Partnership Award in 2012. In addition, Hetzner Online has sponsored several non-profit organizations located in the communities near its data center parks.\n\nIn June 2013, Hetzner Online suffered from a security breach where customer information was exposed to attackers who had compromised Hetzner Online's monitoring systems.\n\nIn early August 2014, the Russian Federal Service for Supervision of Communications, Information Technology and Mass Media (Roskomnadzor) sent a demand to many news agencies, including BBC, prohibiting any mention of the demonstration that was being arranged in the Siberian city of Novosibirsk in support of the federalization of Siberia.\nA number of such messages were sent to Ukraine, which was in the midst of undeclared war with Russian paramilitary in Donetsk region. Since the Ukrainian online newspapers did not remove the article, Roskomnadzor sent letters to their Internet providers demanding removal of the news item. Hetzner Online complied with the demands and sent a notice to glavcom.ua, saying \"Please solve the problem and reply within the next 24 hours to avoid suspension. This is the final deadline.\"\n\nThis story was widely reprinted in news sources. Ukrainian Ministry of Foreign Affairs\nissued a statement expressing solidarity with glavcom.ua owners and journalists. \nVassily Zvarych, vice-head of Communications Department of Foreign Ministry, gave a press conference saying that he was surprised by Hetzner Online's compliance with the Russian complaint. Also, a German chapter of Reporters Without Borders issued a statement condemning Roskomnadzor.\n\nThe notices to suspend Glavcom.ua were issued by Hetzner Online August 6, 2014; on August 10 Hetzner Online issued apologies, denying that any censorship was planned and that their technical support made a wrong decision, which they regret. However, by that time the story was widely published in German mass-media, and Glavcom.ua already migrated from Hetzner Online to another hosting provider.\n\nIn 2013, an Estonian anti-spam activist Tõnu Samuel posted a blog entry about an alleged spammer Silver Teede on his website no.spam.ee. In retaliation, Teede wrote a complaint to the blog's service provider, Hetzner Online, who decided to terminate services for the blog. In an ensuing court case, Estonian courts found the complaints to be baseless and awarded Samuel damages for the loss of his servers.\n\nOn January 11, 2016, Hetzner blocked the St. Petersburg site of Novaya Gazeta, the leading oppositional, non-governmental newspaper in Russia. The newspaper marked the act as political censorship without any legal procedure.\n\n\n\n"}
{"id": "9963332", "url": "https://en.wikipedia.org/wiki?curid=9963332", "title": "History of the petroleum industry", "text": "History of the petroleum industry\n\nThe petroleum industry is not of recent origin, but petroleum's current status as the key component of politics, society, and technology has its roots in the early 20th century. The invention of the internal combustion engine was the major influence in the rise in the importance of petroleum.\n\nAccording to Herodotus, more than four thousand years ago natural asphalt was employed in the construction of the walls and towers of Babylon, great quantities of it were found on the banks of the river Issus, one of the tributaries of the Euphrates, and this fact confirmed by Diodorus Siculus. Herodotus mentioned pitch spring on Zacynthus (Ionian islands, Greece). Also, Herodotus described a well for bitumen and oil near Ardericca in Cessia. Ancient Persian tablets indicate the medicinal and lighting uses of petroleum in the upper levels of their society.\n\nOil was exploited in the Roman province of Dacia, now in Romania, where it was called \"picula\".\n\nThe use of petroleum dates back to ancient China more than 2000 years ago. In I Ching, one of the earliest Chinese writings cites the use of oil in its raw state without refining was first discovered, extracted, and used in China in the first century BCE. In addition, the Chinese were the first to use petroleum as fuel as the early as the fourth century BCE.\n\nThe earliest known oil wells were drilled in China in 347 AD or earlier. They had depths of up to about and were drilled using bits attached to bamboo poles. The oil was burned to evaporate brine and produce salt. By the 10th century, extensive bamboo pipelines connected oil wells with salt springs. The ancient records of China and Japan are said to contain many allusions to the use of natural gas for lighting and heating. Petroleum was known as \"burning water\" in Japan in the 7th century. In his book \"Dream Pool Essays\" written in 1088, the polymathic scientist and statesman Shen Kuo of the Song Dynasty coined the word 石油 (\"Shíyóu\", literally \"rock oil\") for petroleum, which remains the term used in contemporary Chinese and Japanese (\"Sekiyū\").\n\nThe first streets of Baghdad were paved with tar, derived from petroleum that became accessible from natural fields in the region. In the 9th century, oil fields were exploited in the area around modern Baku, Azerbaijan. These fields were described by the Arab geographer Abu al-Hasan 'Alī al-Mas'ūdī in the 10th century, and by Marco Polo in the 13th century, who described the output of those wells as hundreds of shiploads. Distillation of Petroleum was described by the Persian alchemist, Muhammad ibn Zakarīya Rāzi (Rhazes). There was production of chemicals such as kerosene in the alembic (\"al-ambiq\"), which was mainly used for kerosene lamps. Arab and Persian chemists also distilled crude oil in order to produce flammable products for military purposes. Through Islamic Spain, distillation became available in Western Europe by the 12th century. It has also been present in Romania since the 13th century, being recorded as păcură.\n\nThe earliest mention of petroleum in the Americas occurs in Sir Walter Raleigh's account of the Trinidad Pitch Lake in 1595; while thirty-seven years later, the account of a visit of a Franciscan, Joseph de la Roche d'Allion, to the oil springs of New York was published in Gabriel Sagard's \"Histoire du Canada\". A Finnish born Swede, scientist and student of Carl Linnaeus, Peter Kalm, in his work \"Travels into North America\" published first in 1753 showed on a map the oil springs of Pennsylvania.\n\nIn 1710 or 1711 (sources vary) the Russian-born Swiss physician and Greek teacher Eirini d'Eyrinys (also spelled as Eirini d'Eirinis) discovered asphaltum at Val-de-Travers, (Neuchâtel). He established a bitumen mine \"de la Presta\" there in 1719 that operated until 1986.\n\nIn 1745 under the Empress Elizabeth of Russia the first oil well and refinery were built in Ukhta by Fiodor Priadunov. Through the process of distillation of the \"rock oil\" (petroleum) he received a kerosene-like substance, which was used in oil lamps by Russian churches and monasteries (though households still relied on candles).\n\nOil sands were mined from 1745 in Merkwiller-Pechelbronn, Alsace under the direction of Louis Pierre Ancillon de la Sablonnière, by special appointment of Louis XV.\nThe Pechelbronn oil field was active until 1970, and was the birthplace of companies like Antar and Schlumberger. The first modern refinery was built there in 1857.\n\nThe modern history of petroleum began in the 19th century with the refining of paraffin from crude oil. The Scottish chemist James Young in 1847 noticed a natural petroleum seepage in the Riddings colliery at Alfreton, Derbyshire from which he distilled a light thin oil suitable for use as lamp oil, at the same time obtaining a thicker oil suitable for lubricating machinery.\nIn 1846, Baku (settlement Bibi-Heybat) the first ever well drilled with percussion tools to a depth of 21 meters for oil exploration, based on data of Nicolay Voskoboynikov; it was 13 years before the Drake's well was drilled in Pennsylvania. \nThe new oils were successful, but the supply of oil from the coal mine soon began to fail (eventually being exhausted in 1851). Young, noticing that the oil was dripping from the sandstone roof of the coal mine, theorized that it somehow originated from the action of heat on the coal seam and from this thought that it might be produced artificially.\n\nFollowing up this idea, he tried many experiments and eventually succeeded, by distilling cannel coal at a low heat, a fluid resembling petroleum, which when treated in the same way as the seep oil gave similar products. Young found that by slow distillation he could obtain a number of useful liquids from it, one of which he named \"paraffine oil\" because at low temperatures it congealed into a substance resembling paraffin wax.\n\nThe production of these oils and solid paraffin wax from coal formed the subject of his patent dated 17 October 1850. In 1850 Young & Meldrum and Edward William Binney entered into partnership under the title of E.W. Binney & Co. at Bathgate in West Lothian and E. Meldrum & Co. at Glasgow; their works at Bathgate were completed in 1851 and became the first truly commercial oil-works and oil refinery in the world, using oil extracted from locally mined torbanite, shale, and bituminous coal to manufacture naphtha and lubricating oils; paraffin for fuel use and solid paraffin were not sold till 1856.\nAbraham Pineo Gesner, a Canadian geologist developed a process to refine a liquid fuel from coal, bitumen and oil shale. His new discovery, which he named kerosene, burned more cleanly and was less expensive than competing products, such as whale oil. In 1850, Gesner created the Kerosene Gaslight Company and began installing lighting in the streets in Halifax and other cities. By 1854, he had expanded to the United States where he created the North American Kerosene Gas Light Company at Long Island, New York. Demand grew to where his company’s capacity to produce became a problem, but the discovery of petroleum, from which kerosene could be more easily produced, solved the supply problem.\n\nIn 1846 the first oil well in the world was drilled in Asia, on the Aspheron Peninsula north-east of Baku (in settlement Bibi-Heybat), by Major Alekseev based on data of N. Voskoboynikov.\n\nIgnacy Łukasiewicz improved Gesner's method to develop a means of refining kerosene from the more readily available \"rock oil\" (\"petr-oleum\") seeps, in 1852, and the first rock oil mine was built in Bóbrka, near Krosno in central European Galicia (Poland) in 1854. These discoveries rapidly spread around the world, and Meerzoeff built the first modern Russian refinery in the mature oil fields at Baku in 1861. At that time Baku produced about 90% of the world's oil.\n\nThe question of what constituted the first commercial oil well is a difficult one to answer. Edwin Drake's 1859 well near Titusville, Pennsylvania, discussed more fully below, is popularly considered the first modern well. Drake's well is probably singled out because it was drilled, not dug; because it used a steam engine; because there was a company associated with it; and because it touched off a major boom. However, the first well ever drilled anywhere in the world, which produced oil, was drilled in 1857 to a depth of 280 feet by the American Merrimac Company in La Brea (Spanish for “Pitch”) in southeast Trinidad in the Caribbean. Additionally, there was considerable activity before Drake in various parts of the world in the mid-19th century. A group directed by Major Alexeyev of the Bakinskii Corps of Mining Engineers hand-drilled a well in the Baku region in 1846. There were engine-drilled wells in West Virginia in the same year as Drake's well. An early commercial well was hand dug in Poland in 1853, and another in nearby Romania in 1857. At around the same time the world's first, but small, oil refineries were opened at Jasło, in Poland, with a larger one being opened at Ploiești, in Romania, shortly after. Romania is the first country in the world to have its crude oil output officially recorded in international statistics, namely 275 tonnes. In 1875, crude oil was discovered by David Beaty at his home in Warren, Pennsylvania. This led to the opening of the Bradford oil field, which, by the 1880s, produced 77 percent of the global oil supply. However, by the end of the 19th century, the Russian Empire, particularly the Branobel company in Azerbaijan, had taken the lead in production.\n\nSamuel Kier established America's first oil refinery in Pittsburgh on Seventh avenue near Grant Street, in 1853. In addition to the activity in West Virginia and Pennsylvania, an important early oil well in North America was in Oil Springs, Ontario, Canada in 1858, dug by James Miller Williams. The discovery at Oil Springs touched off an oil boom which brought hundreds of speculators and workers to the area. New oil fields were discovered nearby throughout the late 19th century and the area developed into a large petrochemical refining centre and exchange. The modern US petroleum industry is considered to have begun with Edwin Drake's drilling of a oil well in 1859, on Oil Creek near Titusville, Pennsylvania, for the Seneca Oil Company (originally yielding , by the end of the year output was at the rate of ). The industry grew through the 1800s, driven by the demand for kerosene and oil lamps. It became a major national concern in the early part of the 20th century; the introduction of the internal combustion engine provided a demand that has largely sustained the industry to this day. Early \"local\" finds like those in Pennsylvania and Ontario were quickly outpaced by demand, leading to \"oil booms\" in Ohio, Texas, Oklahoma, and California.\n\nBy 1910, significant oil fields had been discovered in the Dutch East Indies (1885, in Sumatra), Persia (1908, in Masjed Soleiman), Peru (1863, in Zorritos District), Venezuela (1914, in Maracaibo Basin), and Mexico, and were being developed at an industrial level. Significant oil fields were exploited in Alberta (Canada) from 1947.\nFirst offshore oil drilling at Oil Rocks (Neft Dashlari) in the Caspian Sea off Azerbaijan eventually resulted in a city built on pylons in 1949.\nAvailability of oil and access to it, became of \"cardinal importance\" in military power before and after World War I, particularly for navies as they changed from coal, but also with the introduction of motor transport, tanks and airplanes. Such thinking would continue in later conflicts of the twentieth century, including World War II, during which oil facilities were a major strategic asset and were extensively bombed.\nIn 1938, vast reserves of oil were discovered in the Al-Ahsa region along the coast of the Persian Gulf.\n\nUntil the mid-1950s coal was still the world's foremost fuel, but after this time oil quickly took over. Later, following the 1973 and 1979 energy crises, there was significant media coverage on the subject of oil supply levels. This brought to light the concern that oil is a limited resource that will eventually run out, at least as an economically viable energy source. Although at the time the most common and popular predictions were quite dire, a period of increased production and reduced demand in the following years caused an oil glut in the 1980s. This was not to last, however, and by the first decade of the 21st century discussions about peak oil had returned to the news.\n\nToday, about 90% of vehicular fuel needs are met by oil. Petroleum also makes up 40% of total energy consumption in the United States, but is responsible for only 2% of electricity generation. Petroleum's worth as a portable, dense energy source powering the vast majority of vehicles and as the base of many industrial chemicals makes it one of the world's most important commodities.\n\nThe top three oil producing countries are Saudi Arabia, Russia, and the United States. About 80% of the world's readily accessible reserves are located in the Middle East, with 62.5% coming from the Arab 5: Saudi Arabia (12.5%), UAE, Iraq, Qatar and Kuwait. However, with high oil prices (above $100/barrel), Venezuela has larger reserves than Saudi Arabia due to its crude reserves derived from bitumen.\n\n\n\n"}
{"id": "19790637", "url": "https://en.wikipedia.org/wiki?curid=19790637", "title": "Home fuel cell", "text": "Home fuel cell\n\nA home fuel cell or a residential fuel cell is a scaled down version of industrial stationary fuel cell for primary or backup power generation. These fuel cells are usually based on combined heat and power-CHP or micro combined heat and power MicroCHP technology, generating both power and heated water or air. \n\nA commercially working cell is called Ene-Farm in Japan and is supported by the regional government which uses natural gas to power up the fuel cell to produce electricity and heated water.\n\n\nMost home fuel cells fit either inside a mechanical room or outside a home or business, and can be discreetly sited to fit within a building's design. The system operates like a furnace, water heater, and electricity provider—all in one compact unit. Some of the newer home fuel cells can generate anywhere between 1–5 kW—optimal for larger homes ( or more), especially if pools, spas, and radiant floor heating are in plans. Other uses include sourcing of back-up power for essential loads like refrigerator/freezers and electronics/computers.\n\nDeploying the system's heat energy efficiently to a home or business' hot water applications displaces the electricity or gas otherwise burned to create that heat, which further reduces overall energy bills. Retail outlets like fast food chains, coffee bars, and health clubs gain operational savings from hot water heating.\n\nSince it is in general not possible for a fuel cell to produce at all times exactly the needed amount of both electricity and heat, home fuel cells are typically not standalone installations. Instead they may rely on the grid when the electricity production is above or below what is needed. Additionally, a home fuel cell may be combined with a traditional furnace that produces only heat. For example, the German company Viessmann produces a home fuel cell with an electric power of 0.75 kW and a thermal power of 1 kW, integrated with a traditional 19 kW heat producing furnace, using the grid for electricity need below and above the fuel cell production.\n\nBecause the home fuel cell generates electricity and heat that are both used on site, theoretical efficiency approaches 100%. This is in contrast to traditional or fuel cell non-domestic electricity production with both a transmission loss and useless heat, requiring extra energy consumption for the domestic heating. As noted above however, the home fuel cell can in general not at all times generate exactly the needed amount of both heat and electricity, and is therefore typically not a standalone installation but rather combined with a traditional furnace and connected to the grid for electricity need above or below that produced by the fuel cell. As such, the overall efficiency is below 100%. The optimum efficiency of home fuels cell has caused some countries such as Germany to economically support their installation as part of a policy reacting to climate change.\n\nHome fuel cells are designed and built to fit in either an interior mechanical room or outside—running quietly in the background 24/7. Connected to the utility grid through the home's main service panel and using net metering, the home fuel cells easily integrate with existing electrical and hydronic systems and are compliant with utility interconnection requirements. In the event of grid interruption the system automatically switches to operate in a grid-independent mode to provide continuous backup power for dedicated circuits in the home while the grid is down. It can also be modified to run off-the-grid, if desired.\n\nTwenty companies have installed Bloom Energy fuel cells in their buildings, including Google, eBay, and FedEx. The eBay CEO said to \"60 Minutes\", that they have saved $100,000 in electricity bills in the 9 months they have been installed.\n\nOregon-based ClearEdge Power has installations of its 5 kW system at the home of Jackie Autry, Bay Area Wealth Manager Bruce Raabe and VC investor Gary Dillabough.\n\nUpdate – ClearEdge Power went out of business in 2014 and no longer supports any of the 5kW units in the field.\n\nDelta-ee consultants stated in 2013 that with 64% of global sales the fuel cell micro-combined heat and power passed the conventional systems in sales in 2012.\n\nMost home fuel cells are comparable to residential solar energy photovoltaic systems on a dollar per watt-installed basis. Some natural gas driven home fuel cells can generate eight times more energy per year than the same size solar installation, even in the best solar locations. For example, a 5 kW home fuel cell produces about 80 MWh of annual combined electricity and heat, compared to approximately 10MWh generated by a 5 kW solar system. However, these systems are not directly comparable because solar power is a renewable resource with basically no operating cost while natural\nOperating costs for home fuel cells can be as low as 6.0¢ per kWh based on $1.20 per therm for natural gas, assuming full electrical and heat load utilization.\n\nIn the U.S.A., home fuel cells are eligible for substantial incentives and rebates at both the state and federal levels as a part of renewable energy policy. For example, the California Self Generation Incentive Program (SGIP) rebate ($2,500 per kW) and Federal Tax Credits ($1,000 per kW residential and $3,000 per kW commercial) will significantly reduce the net capital cost to the customer. For businesses, additional cash advantages can be realized from bonus and accelerated depreciation of fuel cells.\n\nIn addition, home fuel cells receive net metering credit in many service areas for any excess electricity generated, but not used, by putting it back on the utility grid.\n\nThe Database of State Incentives for Renewables & Efficiency (DSIRE) provides comprehensive information on state, local, utility, and federal incentives that promote renewable energy and energy efficiency.\n\nIn California in particular, utilities charge higher per kWh rates as energy consumption rises above established baselines – with the top tier set at the highest rates to discourage consumption at those levels. Home fuel cells reduce customer exposure to the top tier rates, saving homeowners as much as 45% in reduced annual energy costs.\n\nHome fuel cells is a new market and represents a fundamental shift in the sourcing of energy. An individual home fuel cell system installed in a US home becomes a part of the bigger picture of U.S. energy independence. An ultimate benefit of home fuel cells will be to eventually create networks of micro-CHP systems distributed throughout communities and business parks. This self-generation of energy in a distributed generation approach that will secure and increase US power generating capacity, enabling unused electricity to be sent back to the grids without having to add new power plants and transmission lines. Putting a home fuel cell system into homes has the potential to get people off-the-grid, play a significant role in energy efficiency, and reduce US dependence on foreign energy imports.\n\n\n"}
{"id": "58468005", "url": "https://en.wikipedia.org/wiki?curid=58468005", "title": "IPhone XR", "text": "IPhone XR\n\niPhone XR (stylized as iPhone X🅁, Roman numeral \"X\" pronounced \"ten\") is a smartphone designed and manufactured by Apple, Inc. It is the twelfth generation of the iPhone. It was announced by Apple CEO Tim Cook on September 12, 2018, at Steve Jobs Theater in the Apple Park campus, alongside the higher-priced iPhone XS and iPhone XS Max. Pre-orders began on October 19, 2018, with an official release on October 26, 2018.\n\nThe phone has a 6.1-inch LCD display, and is the least expensive device in Apple's iPhone X line of devices with a starting price of $749 in the US, $1029 in Canada, £749 in the UK, €849 in the Eurozone countries and 6499 yuan in China. It features the same processor as the XS and XS Max, the Apple A12 Bionic chip built with a 7 nanometer process, which Apple claims is the \"smartest and most powerful chip\" ever put into a smartphone.\n\nIt is available in six colors: black, white, blue, yellow, coral (a shade of pink and orange), and Product Red. The XR is the second iPhone device to be released in yellow and blue, the first being the iPhone 5C.\n\nInternationally, the phone will support dual SIMs through a Nano-SIM and an eSIM. In mainland China, Hong Kong and Macau, dual Nano-SIM (in a single tray) will be offered instead.\n\nThe XR has a similar design to the iPhone X and iPhone XS. However, it has larger bezels, an aluminum frame and is available in a wide variety of colors. Similar to other X-branded iPhones, all models come with a black front.\n\nThe XR has similar hardware to the XS but with some features removed to reduce the price. Instead of 3D Touch, the XR comes with Haptic Touch where the user long presses until they feel a vibration from the Taptic Engine. The XR also has an LCD display known as Liquid Retina instead of the OLED screen used on the X, XS and XS Max. The display on the XR has a resolution of 1792×828 pixels and a pixel density of 326 PPI compared with 458 PPI on other X-branded iPhones. The screen-to-body ratio of the XR is 79.3%, much higher than the 67.5% of the iPhone 8 Plus but still lower than most other phones in its price category. Unlike most other phones in the X-series, the XR ships with a single camera on the back on the phone, featuring very similar specs to the main camera on the XS and XS Max. Unlike the XS, it does not have optical zoom.\n\nDespite the rear single-camera setup, a modified version of Portrait Mode is included. It works unaltered while using the TrueDepth front camera, but with the rear camera it attempts to calculate depth of field using a combination of the focus pixels on the image sensor and AI, resulting in more limitations including lower resolution depth data and subjects not being close enough due to the wide-angle lens being used instead of the missing telephoto lens. At present, only human faces can be recognized in the iOS camera app, however, third party camera apps including Halide are working to allow Portrait Mode to work on other subjects as well.\n\nThe XR is shipped with iOS 12 installed out of the box. For reasons unknown, the XR received an updated version of iOS 12.1 one week after its initial release for devices that did not have the update installed prior.\n\n\n"}
{"id": "3488118", "url": "https://en.wikipedia.org/wiki?curid=3488118", "title": "Joas Electronics", "text": "Joas Electronics\n\nJoas Electronics is an electronics company headquartered in Seoul and Namyangju Gyeonggi-do, Korea. It was established in 1982 under the name SungJin Electronics but the name was changed in 1999 to Joas Electronics. It manufactures many electronic beauty products.\n\n\n\n"}
{"id": "4067253", "url": "https://en.wikipedia.org/wiki?curid=4067253", "title": "Light-addressable potentiometric sensor", "text": "Light-addressable potentiometric sensor\n\nA light-addressable potentiometric sensor (LAPS) is a sensor that uses light (e.g. LEDs) to select what will be measured. Light can activate carriers in semiconductors.\n\nAn example is the pH-sensitive LAPS (range pH4 to pH10) that uses LEDs in combination with (semi-conducting) silicon and pH-sensitive TaO (SiO; SiN) insulator. The LAPS has several advantages over other types of chemical sensors. The sensor surface is completely flat, no structures, wiring or passivation are required. At the same time, the \"light-addressability\" of the LAPS makes it possible to obtain a spatially resolved map of the distribution of the ion concentration in the specimen. The spatial resolution of the LAPS is an important factor and is determined by the beam size and the lateral diffusion of photocarries in the semiconductor substrate. By illuminating parts of the semiconductor surface, electron-hole pairs are generated and a photocurrent flows. \nThe LAPS is a semiconductor based chemical sensor with an electrolyte-insulator-semiconductor (EIS) structure. Under a fixed bias voltage, the AC (kHz range) photocurrent signal varies depending on the solution. \nA two-dimensional mapping of the surface from the LAPS is possible by using a scanning laser beam.\n\n"}
{"id": "6916771", "url": "https://en.wikipedia.org/wiki?curid=6916771", "title": "MVDS", "text": "MVDS\n\nMVDS is an acronym for terrestrial \"Multipoint Video Distribution System.\"\n\nMVDS currently is a part of broader MWS (Multimedia Wireless System) standards.\nIn European Union MWS works in 10.7 - 13.5 and 40.5 - 43.5 GHz frequency bands.\n\nResearch for 42 GHz frequency has been done under the European Commition EMBRACE (Efficient Millimetre Broadband Radio Access for Convergence and Evolution) initiative.\n\n\n\n\n"}
{"id": "3185281", "url": "https://en.wikipedia.org/wiki?curid=3185281", "title": "Mammoty", "text": "Mammoty\n\nA mammoty (Tamil: \"மண்வெட்டி\"; Sinhala: \" උදැල්ල \") is a special type of garden hoe common in India and Sri Lanka. \n\nThe Mammoty's blade is about four times as large as the average garden hoe. It is the gardening implement of choice in these countries. Its name, \"mammoty\" comes from the word \"Man Vetty\" (\"Man\" rhymes with \"one\") in Tamil, a language spoken in Tamil Nadu in India and some northern parts of Sri Lanka. In the Sinhala language, it is called \" Udalla \". \"Mann\" in Tamil, language means earth or soil and \"Vetti\" refers to a cutter. Colloquially it is called as \" Mambutty or Mambty \".\n\nThere are two types of Manvetti; the Manvetti with the shorter handle and the Manvetti with the longer handle. The short handled Manvetti exerts more pressure but tiresome to use continuously. The long handled Manvetti is less strenuous and useful for long and continued work. The short handled Manvetti is widely used in wet lands for trenching, bunding etc. \nIts important to mention that though the long handled Manvetti is used in states like Kerala in India, the shorter one is preferred or widely used.\n"}
{"id": "13383351", "url": "https://en.wikipedia.org/wiki?curid=13383351", "title": "Modulated neutron initiator", "text": "Modulated neutron initiator\n\nA modulated neutron initiator is a neutron source capable of producing a burst of neutrons on activation. It is a crucial part of some nuclear weapons, as its role is to \"kick-start\" the chain reaction at the optimal moment when the configuration is prompt critical. It is also known as an internal neutron initiator. The initiator is typically placed in the center of the plutonium pit, and is activated by impact of the converging shock wave.\n\nOne of the key elements in the proper operation of a nuclear weapon is initiation of the fission chain reaction at the proper time. To obtain a significant nuclear yield, sufficient neutrons must be present within the supercritical core at the right time. If the chain reaction starts too soon (\"predetonation\"), the result will be only a 'fizzle yield', well below the design specification, therefore low spontaneous neutron emission of the pit material is crucial. If it occurs too late, the core will have begun to expand and disassemble into a less-dense state, leading to a lowered yield (less of the core material undergoes fission) or no yield at all (the core is no longer a critical mass).\n\nAs a general rule, 80 generations of neutron multiplication, in a nuclear (e.g. heavy water) fission reactor, are required in order to achieve explosion-yield equivalent; it must be achieved cca. yield of first 40 neutron generations, in order for a nuclear weapon to be \"properly activated.\" \n\nFor boosted fission weapons, the size of the centrally placed initiator is critical and has to be as small as possible. The use of an external neutron source allows more flexibility, such as variable yields.\n\nThe usual design is based on a combination of beryllium-9 and polonium-210, separated until activation, then placed in intimate contact by the shock wave. Polonium-208 and actinium-227 were also considered as alpha sources. The isotope used must have strong alpha emissions and weak gamma emissions, as gamma photons can also knock neutrons loose and cannot be so efficiently shielded as alpha particles. Several variants were developed, differing by the dimensions and mechanical configuration of the system ensuring proper mixing of the metals.\n\nUrchin was the code name for the internal neutron initiator, a neutron generating device that triggered the nuclear detonation of the earliest plutonium atomic bombs such as The Gadget and Fat Man, once the critical mass had been 'assembled' by the force of conventional explosives.\n\nThe initiator used in the early devices, located at the center of the bomb's plutonium pit, consisted of a beryllium pellet, and a beryllium shell with polonium between the two. The pellet, 0.8 cm in diameter, was coated with nickel and then a layer of gold. The beryllium shell was of 2 cm outer diameter with wall thickness of 0.6 cm. The inner surface of that shell had 15 concentric, wedge-shaped latitudinal grooves and was, like the inner sphere, coated with gold and nickel. A small amount of polonium-210 (50 curies, 11 mg) was deposited in the grooves of the shell and on the central sphere: the layers of gold and nickel served to shield the beryllium from alpha particles emitted by the polonium. The whole urchin weighed about 7 grams and was attached to mounting brackets in a 2.5 cm diameter inner cavity in the pit.\n\nWhen the shock wave from the implosion of the plutonium core arrives, it crushes the initiator. Hydrodynamic forces acting on the grooved shell thoroughly and virtually instantly mix the beryllium and polonium, allowing the alpha particles from the polonium to impinge on the beryllium atoms. Reacting to alpha particle bombardment, the beryllium atoms emit neutrons at a rate of about 1 neutron each 5–10 nanoseconds (See Beryllium#Nuclear properties). These neutrons trigger the chain reaction in the compressed supercritical plutonium. Placing the polonium layer between two large masses of beryllium ensures contact of the metals even if the shock wave turbulence performs poorly.\n\nThe 50 curies of polonium generated about 0.1 watts of decay heat, noticeably warming the small sphere.\n\nThe grooves in the inner surface of the shell shaped the shock wave into jets by the Munroe effect, similar to a shaped charge, for fast and thorough mixing of the beryllium and polonium. As the Munroe effect is less reliable in linear geometry, later designs used a sphere with conical or pyramidal inner indentations instead of linear grooves. Some initiator designs omit the central sphere, being hollow instead. The advantage of a hollow design is possibly managing a smaller size while retaining reliability.\n\nThe short half-life of polonium (138.376 days) required frequent replacement of initiators and a continued supply of polonium for their manufacture, as their shelf life was only about 4 months. Later designs had shelf life as long as 1 year.\n\nThe US government used Postum as a code name for polonium.\n\nUse of polonium for the neutron initiator was proposed in 1944 by Edward Condon. The initiator itself was designed by James L. Tuck, and its development and testing was carried out at Los Alamos National Laboratory in \"Gadget\" division's initiator group led by Charles Critchfield.\n\nA different initiator (code named ABNER) was used for the Little Boy uranium bomb. Its design was simpler and it contained less polonium. It was activated by the impact of the uranium projectile to the target. It was added to the design as an afterthought and was not essential for the weapon's function.\n\nAn improved construction of the initiator, probably based on conical or pyramidal indentations, was proposed in 1948, put into production by Los Alamos in January 1950, and tested in May 1951. The TOM design used less polonium, as the number of neutrons per milligram of polonium was higher than of the Urchin. Its outer diameter was only 1 cm. The first live fire test of a TOM initiator occurred on 28-Jan-1951 during the Baker-1 shot of Operation Ranger. A series of calibration experiments for initiation time vs yield data of the TOM initiators was done during the Operation Snapper, during the Fox test on 25 May 1952.\n\nIn 1974, India performed the Smiling Buddha nuclear test. The initiator, codenamed \"Flower\", was based on the same principle as the Urchin. It is believed the polonium was deposited on lotus-shaped platinum gauze to maximize its surface and enclosed in a tantalum sphere surrounded by uranium shell with embedded beryllium pellets. According to other sources, the design was yet more similar to the Urchin, with a beryllium shell shaped to create beryllium jets upon implosion. The initiator outer diameter is reported as 1.5 cm, or \"about 2 cm\".\n\nUranium deuteride (UD) can be used for construction of a neutron multiplier.\n\nBoosted fission weapons and weapons using external neutron generators offer the possibility of variable yield, allowing selection of the weapon's power depending on the tactical needs.\n\nThe polonium used in the urchin initiator was created at Oak Ridge and then extracted and purified as part of the Dayton Project under the leadership of Charles Allen Thomas. The Dayton Project was one of the various sites comprising the Manhattan Project.\n\nIn 1949 Mound Laboratories in nearby Miamisburg, Ohio opened as a replacement for the Dayton Project and the new home of nuclear initiator research & development. Polonium-210 was produced by neutron irradiation of bismuth. Production and research of polonium at Mound was phased out in 1971.\n\nPolonium from Dayton was used by the G Division of Los Alamos in initiator design studies at a test site in Sandia Canyon. The initiator group built test assemblies by drilling holes in large turbine ball bearings, inserting the active material, and plugging the holes with bolts. These test assemblies were known as screwballs. The test assemblies were imploded and their remains studied to examine how well the polonium and beryllium mixed.\n\nThe production of the beryllium-polonium TOM initiators ended in 1953. The initiators were replaced with a different design, which slightly reduced the weapon yield but its longer shelf life reduced the complexity of the logistics. The sealed neutron initiator, brought into inventory in late 1954, still required a periodic disassembly to access its capsule for maintenance checks. The capsules were phased out completely in 1962.\n\nUrchin style initiators were later superseded by other means of generating neutrons such as pulsed neutron emitters that do not use polonium. Having no radioactive materials, they didn't require frequent replacement on that account. These were generally mounted outside the pit, since neutrons easily pass through considerable mass without interactions. Also, the hollow core was needed to support a boosted fission weapon which injected gaseous deuterium and tritium mixture into the center. These initiators were more controllable and led to much improved weapon reliability.\n\n"}
{"id": "471158", "url": "https://en.wikipedia.org/wiki?curid=471158", "title": "Multi-angle imaging spectroradiometer", "text": "Multi-angle imaging spectroradiometer\n\nThe multi-angle imaging spectroradiometer (MISR) is a scientific instrument on the Terra satellite launched by NASA on 18 December 1999. This device is designed to measure the intensity of solar radiation reflected by the Earth system (planetary surface and atmosphere) in various directions and spectral bands; it became operational in February 2000. Data generated by this sensor have been proven useful in a variety of applications including atmospheric sciences, climatology and monitoring terrestrial processes.\n\nThe MISR instrument consists of an innovative configuration of nine separate digital cameras that gather data in four different spectral bands of the solar spectrum. One camera points toward the nadir, while the others provide forward and aftward view angles at 26.1°, 45.6°, 60.0°, and 70.5°. As the instrument flies overhead, each region of the Earth's surface is successively imaged by all nine cameras in each of four wavelengths (blue, green, red, and near-infrared).\n\nThe data gathered by MISR are useful in climatological studies concerning the disposition of the solar radiation flux in the Earth's system. MISR is specifically designed to monitor the monthly, seasonal, and long-term trends of atmospheric aerosol particle concentrations including those formed by natural sources and by human activities, upper air winds and cloud cover, type, height, as well as the characterization of land surface properties, including the structure of vegetation canopies, the distribution of land cover types, or the properties of snow and ice fields, amongst many other biogeophysical variables.\n\n\n"}
{"id": "57914571", "url": "https://en.wikipedia.org/wiki?curid=57914571", "title": "PFCP", "text": "PFCP\n\nPacket Forwarding Control Protocol (PFCP) is a 3GPP protocol used on the Sx/N4 interface between the control plane and the user plane function, specified in TS 29.244. It is one of the main protocols introduced in the 5G Next Generation Mobile Core Network (aka 5GC), but also used in the 4G/LTE EPC to implement the Control and User Plane Separation (CUPS). PFCP and the associated interfaces seek to formalize the interactions between different types of functional elements used in the Mobile Core Networks as deployed by most operators providing 4G, as well as 5G, services to mobile subscribers. These 2 types of components are: \n\n\nPFCP's scope is similar to that of OpenFlow, however it was engineered to serve the particular use-case of Mobile Core Networks. PFCP lacks the same general-purpose targets, describing well the 3GPP-specific functional basic blocks of packet forwarding used in 2G, 3G, 4G/LTE, Non-3GPP WiFi and 5G networks.\n\nAlbeit similar to GTP in concepts and implementation, PFCP is complementary to it. It provides the control means for a signaling component of the Control-Plane to manage packet processing and forwarding performed by an User-Plane component. Typical EPC or 5G Packet Gateways are split by the protocol in 2 functional parts, allowing for a more natural evolution and scalability.\n\nThe PFCP protocol is used on the following 3GPP mobile core interfaces:\n\n\nNote: Sxa and Sxb can be combined, in case a merged SGW/PGW is implemented.\n\nThe Control-Plane functional element (e.g. PGW-C, SMF) controls the packet processing and forwarding in the User-Plane functional elements (e.g. PGW-U, UPF), by establishing, modifying or deleting PFCP Sessions. Per session multiple PDRs, FARs, QERs, URR and/or BARs are sent.\n\nHere are the main concepts used, organized in their logical association model:\n\n\nIEs are defined either as having a proprietary encoding, or as grouped. Grouped IEs are simply a list of other IEs, encoded one after the other like in the PFCP Message Payload.\n\nIE Types 0..32767 are 3GPP specific and do not have an Enterprise-ID set. IE Types 32768..65535 can be used by custom implementation and the Enterprise-ID must be set to the IANA SMI Network Management Private Enterprise Codes of the issuing party.\n\nVery similar to GTP-C, PFCP uses UDP. Port 8805 is reserved. \n\nFor reliability, a similar re-transmission strategy as for GTP-C is employed, lost messages being sent N1-times at T1-intervals. Transactions are identified by the 3-byte long Sequence Number, the IP address and port of the communication peer.\n\nThe protocol includes an own Heart-beat Request/Response model, which allows monitoring the availability of communication peers and detecting restarts (by use of a Recovery-Timestamp Information Element).\n\nFor User-Plane packet exchanges between the Control and User Plane functional elements, GTP-U for the Sx-u interface, or alternatively a simpler UDP or Ethernet encapsulation for the N4-u interface (to be confirmed, as standards are still incomplete).\n\n"}
{"id": "11810505", "url": "https://en.wikipedia.org/wiki?curid=11810505", "title": "Polymerase cycling assembly", "text": "Polymerase cycling assembly\n\nPolymerase cycling assembly (or PCA, also known as Assembly PCR) is a method for the assembly of large DNA oligonucleotides from shorter fragments. The process uses the same technology as PCR, but takes advantage of DNA hybridization and annealing as well as DNA polymerase to amplify a complete sequence of DNA in a precise order based on the single stranded oligonucleotides used in the process. It thus allows for the production of synthetic genes and even entire synthetic genomes.\n\nMuch like how primers are designed such that there is a forward primer and a reverse primer capable of allowing DNA polymerase to fill the entire template sequence, PCA uses the same technology but with multiple oligonucleotides. While in PCR the customary size of oligonuleotides used is 18 base pairs, in PCA lengths of up to 50 are used to ensure uniqueness and correct hybridization.\n\nEach oligonucleotide is designed to be either part of the top or bottom strand of the target sequence. As well as the basic requirement of having to be able to tile the entire target sequence, these oligonucleotides must also have the usual properties of similar melting temperatures, hairpin free, and not too GC rich to avoid the same complications as PCR.\n\nDuring the polymerase cycles, the oligonucleotides anneal to complementary fragments and then are filled in by polymerase. Each cycle thus increases the length of various fragments randomly depending on which oligonucleotides find each other. It is critical that there is complementarity between all the fragments in some way or a final complete sequence will not be produced as polymerase requires a template to follow.\n\nAfter this initial construction phase, additional primers encompassing both ends are added to perform a regular PCR reaction, amplifying the target sequence away from all the shorter incomplete fragments. A gel purification can then be used to identify and isolate the complete sequence.\n\nA typical reaction consists of oligonucleotides ~50 base pairs long each overlapping by about 20 base pairs. The reaction with all the oligonucleotides is then carried out for ~30 cycles followed by an additional 23 cycles with the end primers.\n\nA modification of this method, Gibson assembly, described by Gibson et al. allows for single-step isothermal assembly of DNA with up to several hundreds kb. By using T5 exonuclease to 'chew back' complementary ends, an overlap of about 40bp can be created. The reaction takes place at 50 °C, a temperature where the T5 exonuclease is unstable. After a short timestep it is degraded, the overlaps can anneal and be ligated. Cambridge University IGEM team made a video describing the process. Ligation independent cloning (LIC) is a new variant of the method for compiling several DNA pieces together and needing only exonuclease enzyme for the reaction. However, the method requires even number of DNA-pieces to be joined together and (usually PCR mediated) synthesis of suitable adapters. LIC cannot thus be regarded as a \"non-scarring\" sub-cloning method.\n\n"}
{"id": "8767868", "url": "https://en.wikipedia.org/wiki?curid=8767868", "title": "Portable Content Format", "text": "Portable Content Format\n\nThe DVB's Portable Content Format (PCF) is a data format designed by the DVB project for the description of interactive digital television (iTV) services. It is intended to support the business-to-business interchange of interactive content and to enable deployment on multiple target platforms with a minimum amount of re-authoring.\n\nToday's digital television platforms offer a wide variety of interactive services. However, different television platforms use different technologies for interactive services and so a large amount of interactive content has to be developed bespoke for each platform. This is time consuming and results in high production costs limiting interactive content to high-profile programming and revenue generating propositions.\n\nThe DVB PCF is a response to this situation. The PCF provides a standard format for the description of an author’s intended viewer experience of an interactive service, which can be translated as required for each target platform.\n\nPCF is a platform-independent description of \"what\" the viewer experience should be, rather than \"how\" it should be achieved. This description must be transformed into a platform-specific format by a \"transcoder\". This transformation step uses the available features of a particular platform to create the viewer experience described in PCF.\nThe PCF supports independent description of different aspects of the interactive service, i.e. content, presentation (layout and style), behaviour and navigation. Furthermore, the PCF does not require all aspects of a service to be described as one physical unit, such as a file. For example, service descriptions can be arbitrarily distributed across files located on the Internet, using references represented as Uniform Resource Identifiers (URIs).\n\nThe PCF embodies a high-level declarative model that is based on industry standard formats, including XML syntax, MIME types and UML.\n\n\n\nThe PCF specification was published by ETSI in September 2006.\n"}
{"id": "51192759", "url": "https://en.wikipedia.org/wiki?curid=51192759", "title": "Progress in Energy and Combustion Science", "text": "Progress in Energy and Combustion Science\n\nProgress in Energy and Combustion Science is a bimonthly peer-reviewed review journal published by Elsevier. Established in 1975, the journal publishes review articles on all aspects of energy and combustion science. All contributions are by invitation of the editors-in-chief. The founding editor is Norman Chigier (Carnegie Mellon University). The current editors-in-chief are Hai Wang (Stanford University) and Christof Schulz (University of Duisburg-Essen). \n\nAccording to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 25.242.\n"}
{"id": "6121100", "url": "https://en.wikipedia.org/wiki?curid=6121100", "title": "Pyrotechnic composition", "text": "Pyrotechnic composition\n\nA pyrotechnic composition is a substance or mixture of substances designed to produce an effect by heat, light, sound, gas/smoke or a combination of these, as a result of non-detonative self-sustaining exothermic chemical reactions. Pyrotechnic substances do not rely on oxygen from external sources to sustain the reaction.\n\nBasic types of pyrotechnic compositions are:\n\nSome pyrotechnic compositions are used in industry and aerospace for generation of large volumes of gas in gas generators (e.g. in airbags), in pyrotechnic fasteners, and in other similar applications. They are also used in military pyrotechnics, when production of large amount of noise, light, or infrared radiation is required; e.g. missile decoy flares, flash powders, and stun grenades. A new class of reactive material compositions is now under investigation by military.\n\nMany pyrotechnic compositions – especially involving aluminium and perchlorates – are often highly sensitive to friction, impact, and static electricity. Even as little as 0.1–10 millijoules spark can set off certain mixtures.\n\nPyrotechnic compositions are usually homogenized mixtures of small particles of fuels and oxidizers. The particles can be grains or flakes. Generally, the higher the surface area of the particles, the higher the reaction rate and burning speed. For some purposes, binders are used to turn the powder into a solid material.\n\nTypical fuels are based on metal or metalloid powders. A flash powder composition may specify multiple different fuels. Some fuels can also serve as binders. Common fuels include:\n\n\nWhen metallic fuels are used, the metal particle size is important. A larger surface area to volume ratio leads to a faster reaction; this means that smaller particle sizes produce a faster-burning composition. The shape also matters. Spherical particles, like those produced by atomizing molten metal, are undesirable. Thin and flat particles, like those produced by milling metal foil, have higher reaction surface and therefore are ideal when faster reaction is desired. Using nanoparticles can drastically affect the reaction rates; metastable intermolecular composites exploit this.\n\nA suitable metal fuel may be dangerous on its own, even before it is mixed with an oxidizer. Careful handling is required to avoid the production of pyrophoric metal powders.\n\nPerchlorates, chlorates and nitrates are the most commonly used oxidizers for flash powders. Other possibilities include permanganates, chromates, and some oxides. Generally, the less the oxidizer, the slower the burning and the more light produced. For use at very high temperatures, sulfates can be used as oxidizers in combination with very strongly reducing fuels.\n\nOxidizers in use include:\n\nCorresponding sodium salts can be substituted for potassium ones.\n\n\n\n"}
{"id": "2351529", "url": "https://en.wikipedia.org/wiki?curid=2351529", "title": "Quadruplex videotape", "text": "Quadruplex videotape\n\n2-inch quadruplex video tape (also called 2″ quad, or just quad, for short) was the first practical and commercially successful analog recording video tape format. It was developed and released for the broadcast television industry in 1956 by Ampex, an American company based in Redwood City, California. The first videotape recorder using this format was built and created in the same year. This format revolutionized broadcast television operations and television production, since the only recording medium available to the TV industry before then was film used for kinescopes, which was much more costly to utilize and took time to develop at a film laboratory. In addition, kinescope images were usually of obviously inferior quality to the live television broadcast images they recorded, whereas quadruplex videotape preserved almost all the image detail of a live broadcast.\n\nSince most United States West Coast network broadcast delays done by the television networks at the time were done with film kinescopes that needed time for developing, the networks wanted a more practical, cost-effective, and quicker way to time-shift television programming for later airing on the West Coast than the expense and time consumption of the processing and editing of film caused. Faced with these challenges, broadcasters sought to adapt magnetic tape recording technology (already in use for recording audio) for use with television as well.\n\nThe term \"quadruplex\" refers to the use of four magnetic record/reproduce heads mounted on a headwheel spinning transversely (width-wise) across the tape at a rate of 14,400 (for 960 recorded stripes per second) rpm for NTSC 525 lines/30fps-standard quad decks, and at 15,000 (for 1,000 stripes per second) rpm for those using the PAL 625 lines/25fps video standard. This method is called quadrature scanning, as opposed to the helical scan transport used by later videotape formats. The tape ran at a speed of either per second for NTSC 525/30 video recording, or per second for PAL 625/25 video; the audio, control, and cue tracks were recorded in a standard linear fashion near the edges of the tape. The cue track was used either as a second audio track, or for recording cue tones or time code for linear video editing.\n\nA typical reel of quad tape holds approximately one hour of recorded material at 15 inches per second.\n\nThe quadruplex format employs segmented recording; each transversely recorded video track on a 2-inch quad videotape holds one-sixteenth (NTSC) or one-twentieth (PAL) of a field of video. (For NTSC systems, the math suggests 15 transverse head passes, each consisting of 16 lines of video, are required to complete one field.) This meant that 2-inch quad did not support \"trick-play\" functions, such as still, shuttle, and reverse or variable-speed playback. (In fact, the quadruplex format could only reproduce recognizable pictures when the tape was playing at normal speed.)) However, it was capable of producing extremely high-quality images containing about 400 horizontal lines of video resolution, and remained the \"de facto\" industry standard for television broadcasting from its inception in 1956 to the mid-1980s, when newer, smaller, and lower-maintenance videotape formats superseded it.\n\nThere were three different variations of 2-inch quad:\n\n\nMost quad machines made later in the 1960s and 1970s by Ampex can play back both low and high-band 2-inch quad tape.\n\nTime-shifting of television programming for the West Coast of the United States by the networks in the 1950s (in order to broadcast their programming at the same local time on the East and West Coasts) using kinescope films was quite a rushed and perilous ordeal. This was because there were only three hours for the West Coast branches of the TV networks to receive video for the programming from the East Coast (live via leased microwave relay or coaxial cable circuits provided by the phone company (AT&T) at the time), and then to record such to kinescope films, and finally to develop the film to be aired three hours later on the West Coast. This usually meant the kinescope was aired almost immediately after it came straight out of the developing equipment, still warm from the film dryer. These were referred to by the networks as \"hot kines\". By 1954, the networks used more raw film stock for kinescopes than all of the Hollywood film studios combined, spending up to $4,000 per half hour. They were desperate to obtain a quicker, less expensive, and more practical solution.\n\nIn the early 1950s, Ampex and several other companies such as Bing Crosby Enterprises (BCE) and RCA were competing to release a videotape format. RCA and BCE did release working prototypes of their recorders, but their downfall was that they all used a longitudinal (stationary-head) method of recording, much like audio tape recorders. This meant that the tape had to be recorded at an extremely high speed (around ) in order to accommodate sufficient bandwidth to reproduce an adequate video image (at least 2–3 MHz for a watchable image), in turn requiring large amounts of tape on large reels. At the same time, the BBC developed a similar stationary-head video tape recorder (VTR) system that saw some on-air use, called VERA (Vision Electronic Recording Apparatus).\n\nAmpex, seeing the impracticality of the prototype BCE and RCA VTRs, started to develop a more practical videotape format with tape economy in mind, as well as providing a solution to the networks' West Coast delay woes. Starting in 1952, Ampex built the Mark I prototype VTR, using -wide tape. Ampex decided that instead of having the tape move at high speed past a stationary head to record enough bandwidth for video, the head would be made to move rapidly across the relatively slow moving tape. This resulted in the Mark I using arcuate scanning, which consisted of a spinning disk with a face (where the heads were mounted) which contacted the tape (as opposed to the edge of the headwheel with transverse quadrature scanning). This resulted in an arc-shaped track being recorded across the width of the tape. Arcuate scanning resulted in a head-to-tape speed of about , but problems with timebase stability of the reproduced video signal from the tape led Ampex to abandon arcuate scanning in favor of the more reliable transverse scanning system.\n\nAmpex continued through the mid-1950s with the Mark II and Mark III prototype recorders, which now used transverse scanning. The Mark II used frequency modulation for recording video to tape, resulting in a much-improved, but still noisy, video image (the Mark I had used amplitude modulation, which resulted in a very poor-quality video signal reproduced from the tape, compounded as well by the shortcomings of the machine's arcuate scanning). The Mark III had improved signal-processing and servo electronics, resulting in much better video reproduction.\n\nThe Mark III worked well, but its appearance was quite that of a prototype, and not a finished, saleable product. It was in a makeshift wooden case, with several parts of its chassis externally mounted in partially filled racks. Ampex then built the Mark IV by putting Mark III components into a sleek metal console and fully populated rack-mount cases.\n\nThe Mark IV was the machine first publicly demonstrated at the National Association of Radio and Television Broadcasters (now the NAB) convention (the NAB Show) in Chicago on April 14, 1956. After William Lodge of CBS finished his speech, the Mark IV replayed his image and words almost immediately, causing \"pandemonium\" among the astonished attendees. The earlier Mark III was given some cosmetic improvements, and was also demonstrated at Ampex headquarters in Redwood City the same day. Both demonstrations were a success, and Ampex took $2 million in orders for the machine in four days.\n\nAmpex later released the first manufactured models of quad VTR based on the Mark IV which were also prototypes, the VRX-1000, of which 16 were made. Machines made afterward were the final production models, and were designated as the VR-1000. The advertised price for the Ampex Videotape Recorder in late 1956 was $45,000 ().\n\nIn 1957, shortly after Ampex's introduction of the 2-inch quad format, RCA introduced a quad-compatible VTR, the TRT-1A. RCA referred to it as a \"Television Tape Recorder\", since the word \"videotape\" was a trademark of Ampex at the time.\n\nRCA was able to make the TRT-1A and its later machines compatible with 2-inch quad because Ampex assisted RCA in doing so, as an expression of gratitude for RCA assisting Ampex with making their later quad machines after the VR-1000 color-capable. Initially, the VR-1000 was only natively capable of recording and playing back black and white video, but RCA had modified several VR-1000s to record color video for the NBC TV network (which RCA owned at the time) in the late 1950s, since NTSC color video programming was already underway at NBC.\n\nAmpex developed and released updated and improved models of their quad decks, beginning with the VR-1000B in mid-1959. At that time, Ampex advertised that some 360-plus VR-1000s had been sold worldwide, more than 250 in the U.S.—roughly 30 at each network, 100 by independent stations, and 20 by production companies. The second-generation VR-2000 appeared in 1964. followed by a scaled-down economy version, the VR-1200, in 1966 and the AVR series of VTRs, AVR-1, AVR-2, and AVR-3 in the 1970s. The AVR-2 was the most compact of quad VTRs, using conventional 120 volt (V) single-phase household-type AC power to operate, rather than the 208 or 220 V three-phase AC power required by larger quad machines.\n\nRCA released later models of quad VTRs as well, such as the TR-22, TR-70, and TR-600.\n\nThe Fernseh division of Bosch in Germany released the BCM-40 quadruplex VTR in the 1970s. It was only marketed in Europe, and was not sold in the U.S.\n\nCBS was the first television network to use 2-inch quad videotape, using it for a West Coast delay of \"Douglas Edwards and the News\" on November 30, 1956. The CBS show \"Arthur Godfrey's Talent Scouts\" on December 24, 1956 became the first entertainment program to be broadcast live to the nation from New York and taped for a time-delayed rebroadcast in the Pacific Time Zone. On January 22, 1957, the NBC game show \"Truth or Consequences\", produced in Hollywood, became the first program to be broadcast in all time zones from a prerecorded videotape. \"The Edsel Show\", on October 13, 1957, was the first CBS entertainment program to be broadcast live to the nation from Hollywood, then tape-delayed for rebroadcast in the Pacific time zone.\n\nThe engineers at Ampex who worked on the development of 2-inch quadruplex videotape from the Mark I to the VR-1000 were Charles Ginsburg, Alex Maxey, Fred Pfost, Shelby Henderson, Charlie Anderson, and Ray Dolby (who later went on to found Dolby Laboratories).\n\nAs two inch machines became more reliable, they began to see use in outside broadcast production. The massive machines required their own truck to house and were incredibly labour intensive requiring considerable on site maintenance. Despite this, these machines allowed for OB video engineers to provide instant replays and generate opening sequences over which captions could be added. \n\nAt first, editing was accomplished by physically cutting and splicing the 2\" magnetic tape. The tape was \"developed\" using a solution of fine iron powder suspended in a liquid solvent, marketed as \"Edivue\" Magnetic Developer. Using a special splicing block (such as the then-industry standard Smith Splicer) equipped with a microscope to view the developed tracks, the editor could then see their patterns and then cut between them. Some 2\" splicing blocks instead used a read-only tape head connected to an oscilloscope that enabled the editor to electronically view cue tones or the control track pulses on the tape to determine where the tape should be cut. See linear video editing for details.\n\nAmpex developed and introduced all-electronic videotape editing, making the physical splicing of videotape practically obsolete, with its breakthrough Editec system in 1963; by recording cue tones on the tape, the editor could make frame-accurate edits.\n\nRCA had an \"electronic splicer\" in their TR-4/5 and TR-22 VTRs for frame-accurate edits.\n\nIn 1967 EECO created and introduced the EECO-900 edit controller, which used their proprietary On-Time time code (the later and more standard SMPTE time code had not yet been developed), which was used in conjunction with the quad machines of the time, and was the successor to Ampex's Editec. The EECO-900 and On-Time timecode were developed from EECO's previous work and products developed for NASA for logging and timecoding of their telemetry tapes during space missions.\n\nIn 1971, CMX, a collaborative between CBS and Memorex, introduced the first computer-based edit controller using SMPTE time code for editing. The CMX 200 could control both the source (A-roll) and record (B-roll) quad VTRs. CMX continued to make more powerful edit controllers capable of controlling more VTRs and peripheral devices, such as switchers, DVEs and character generators.\n\nIn 1976, Robert Bosch GmbH introduced the Mach One list-management edit controller, a lower-cost (and less powerful) post-production alternative to CMX edit controllers. At the time, both CMX and Bosch edit controllers utilized similar DEC computers as their basic hardware.\n\nAs 1\" Type B and 1\" Type C VTRs came on the market, list-management editing bays sometimes used a combination of both 1\" and 2\" VTRs; however, 2\" VTRs soon began to disappear from both broadcast and post-production facilities, as the newer 1\" machines were smaller, more dependable, used tape that was far less expensive to purchase, and were capable of recording stereo audio tracks.\n\n\nThe VR-2000 & VR-1200 (and the VR-1100E & VR-1195, as well as some updated VR-1000 VTRs) used modules to correct the playback time base errors of the videotape. The list of modules are:\n\n\n\n\n\n\nBecause the Ampex VR-3000 model was self-contained portable, the U.S. military used it in a wide variety of reconnaissance applications in various vehicles and aircraft. Its ability to accurately record a wide bandwidth of signals, especially high-frequency signals, was a definite advantage for signals intelligence applications.\n\n2-inch quad is no longer used as a mainstream format in TV broadcasting and video production, having long ago been supplanted by easier-to-use, more practical and lower-maintenance analog tape formats like 1\" Type C (1976), U-matic and Betacam. Television and video industry changes to digital video tape (DVCAM, DVCPro and Digital Betacam) and high-definition (HDCAM) are making analog tape formats increasingly obsolete.\n\nWhen it was in use, 2-inch quad VTRs required ongoing maintenance, usually 3-phase power or one phase 220 V to operate, plus an air compressor to provide air pressure for the air bearing that the spinning transverse headwheel rode on due to its high rotational speed (some quad VTRs, such as the portable Ampex VR-3000, used ball bearings instead due to the lack of availability of compressed air, but these wore out quickly).\n\nOperation of VR-1000-era machines required the skills of a highly trained video engineer. When a tape was changed, the operator spent as much as half-an-hour, \"lining-up\" the VTR — that is, carrying out specialized technical adjustments to calibrate the machine to the tape before it was ready for playback. From VR-1200/2000 onward, improvements in head manufacturing/refurbishing tolerances, timebase correction, and greater thermal stability of solid-state electronics made tape changes possible in under a minute and servo calibrations needed only once per shift. From AVR-1 onward, servos were self calibrating and tape changes as fast as the operator could articulate threading.\n\nThe few quadruplex VTRs which remain in service are used for the transfer and/or restoration of archival 2-inch quad videotape material to newer data storage formats, although mainstream TV serials from the 1950s to late 1960s have mostly already been remastered onto more modern media some years ago, even digitized within the last decade.\n\n\n\n\n"}
{"id": "21306150", "url": "https://en.wikipedia.org/wiki?curid=21306150", "title": "Random-access memory", "text": "Random-access memory\n\nRandom-access memory (RAM ) is a form of computer data storage that stores data and machine code currently being used. A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory. In contrast, with other direct-access data storage media such as hard disks, CD-RWs, DVD-RWs and the older magnetic tapes and drum memory, the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.\n\nRAM contains multiplexing and demultiplexing circuitry, to connect the data lines to the addressed storage for reading or writing the entry. Usually more than one bit of storage is accessed by the same address, and RAM devices often have multiple data lines and are said to be \"8-bit\" or \"16-bit\", etc. devices.\n\nIn today's technology, random-access memory takes the form of integrated circuits. RAM is normally associated with volatile types of memory (such as DRAM modules), where stored information is lost if power is removed, although non-volatile RAM has also been developed. Other types of non-volatile memories exist that allow random access for read operations, but either do not allow write operations or have other kinds of limitations on them. These include most types of ROM and a type of flash memory called \"NOR-Flash\".\n\nIntegrated-circuit RAM chips came into the market in the early 1970s, with the first commercially available DRAM chip, the Intel 1103, introduced in October 1970.\n\nEarly computers used relays, mechanical counters or delay lines for main memory functions. Ultrasonic delay lines could only reproduce data in the order it was written. Drum memory could be expanded at relatively low cost but efficient retrieval of memory items required knowledge of the physical layout of the drum to optimize speed. Latches built out of vacuum tube triodes, and later, out of discrete transistors, were used for smaller and faster memories such as registers. Such registers were relatively large and too costly to use for large amounts of data; generally only a few dozen or few hundred bits of such memory could be provided.\n\nThe first practical form of random-access memory was the Williams tube starting in 1947. It stored data as electrically charged spots on the face of a cathode ray tube. Since the electron beam of the CRT could read and write the spots on the tube in any order, memory was random access. The capacity of the Williams tube was a few hundred to around a thousand bits, but it was much smaller, faster, and more power-efficient than using individual vacuum tube latches. Developed at the University of Manchester in England, the Williams tube provided the medium on which the first electronically stored program was implemented in the Manchester Baby computer, which first successfully ran a program on 21 June 1948. In fact, rather than the Williams tube memory being designed for the Baby, the Baby was a testbed to demonstrate the reliability of the memory.\n\nMagnetic-core memory was invented in 1947 and developed up until the mid-1970s. It became a widespread form of random-access memory, relying on an array of magnetized rings. By changing the sense of each ring's magnetization, data could be stored with one bit stored per ring. Since every ring had a combination of address wires to select and read or write it, access to any memory location in any sequence was possible.\n\nMagnetic core memory was the standard form of memory system until displaced by solid-state memory in integrated circuits, starting in the early 1970s. Dynamic random-access memory (DRAM) allowed replacement of a 4 or 6-transistor latch circuit by a single transistor for each memory bit, greatly increasing memory density at the cost of volatility. Data was stored in the tiny capacitance of each transistor, and had to be periodically refreshed every few milliseconds before the charge could leak away. The Toshiba Toscal BC-1411 electronic calculator, which was introduced in 1965, used a form of DRAM built from discrete components. DRAM was then developed by Robert H. Dennard in 1968.\n\nPrior to the development of integrated read-only memory (ROM) circuits, \"permanent\" (or \"read-only\") random-access memory was often constructed using diode matrices driven by address decoders, or specially wound core rope memory planes. \n\nThe two widely used forms of modern RAM are static RAM (SRAM) and dynamic RAM (DRAM). In SRAM, a bit of data is stored using the state of a six transistor memory cell. This form of RAM is more expensive to produce, but is generally faster and requires less dynamic power than DRAM. In modern computers, SRAM is often used as cache memory for the CPU. DRAM stores a bit of data using a transistor and capacitor pair, which together comprise a DRAM cell. The capacitor holds a high or low charge (1 or 0, respectively), and the transistor acts as a switch that lets the control circuitry on the chip read the capacitor's state of charge or change it. As this form of memory is less expensive to produce than static RAM, it is the predominant form of computer memory used in modern computers.\n\nBoth static and dynamic RAM are considered \"volatile\", as their state is lost or reset when power is removed from the system. By contrast, read-only memory (ROM) stores data by permanently enabling or disabling selected transistors, such that the memory cannot be altered. Writeable variants of ROM (such as EEPROM and flash memory) share properties of both ROM and RAM, enabling data to persist without power and to be updated without requiring special equipment. These persistent forms of semiconductor ROM include USB flash drives, memory cards for cameras and portable devices, and solid-state drives. \nECC memory (which can be either SRAM or DRAM) includes special circuitry to detect and/or correct random faults (memory errors) in the stored data, using parity bits or error correction codes.\n\nIn general, the term \"RAM\" refers solely to solid-state memory devices (either DRAM or SRAM), and more specifically the main memory in most computers. In optical storage, the term DVD-RAM is somewhat of a misnomer since, unlike CD-RW or DVD-RW it does not need to be erased before reuse. Nevertheless, a DVD-RAM behaves much like a hard disc drive if somewhat slower.\n\nThe memory cell is the fundamental building block of computer memory. The memory cell is an electronic circuit that stores one bit of binary information and it must be set to store a logic 1 (high voltage level) and reset to store a logic 0 (low voltage level). Its value is maintained/stored until it is changed by the set/reset process. The value in the memory cell can be accessed by reading it.\n\nIn SRAM, the memory cell is a type of flip-flop circuit, usually implemented using FETs. This means that SRAM requires very low power when not being accessed, but it is expensive and has low storage density.\n\nA second type, DRAM, is based around a capacitor. Charging and discharging this capacitor can store a \"1\" or a \"0\" in the cell. However, the charge in this capacitor slowly leaks away, and must be refreshed periodically. Because of this refresh process, DRAM uses more power, but it can achieve greater storage densities and lower unit costs compared to SRAM.\nTo be useful, memory cells must be readable and writeable. Within the RAM device, multiplexing and demultiplexing circuitry is used to select memory cells. Typically, a RAM device has a set of address lines A0... An, and for each combination of bits that may be applied to these lines, a set of memory cells are activated. Due to this addressing, RAM devices virtually always have a memory capacity that is a power of two.\n\nUsually several memory cells share the same address. For example, a 4 bit 'wide' RAM chip has 4 memory cells for each address. Often the width of the memory and that of the microprocessor are different, for a 32 bit microprocessor, eight 4 bit RAM chips would be needed.\n\nOften more addresses are needed than can be provided by a device. In that case, external multiplexors to the device are used to activate the correct device that is being accessed.\n\nOne can read and over-write data in RAM. Many computer systems have a memory hierarchy consisting of processor registers, on-die SRAM caches, external caches, DRAM, paging systems and virtual memory or swap space on a hard drive. This entire pool of memory may be referred to as \"RAM\" by many developers, even though the various subsystems can have very different access times, violating the original concept behind the \"random access\" term in RAM. Even within a hierarchy level such as DRAM, the specific row, column, bank, rank, channel, or interleave organization of the components make the access time variable, although not to the extent that access time to rotating storage media or a tape is variable. The overall goal of using a memory hierarchy is to obtain the highest possible average access performance while minimizing the total cost of the entire memory system (generally, the memory hierarchy follows the access time with the fast CPU registers at the top and the slow hard drive at the bottom).\n\nIn many modern personal computers, the RAM comes in an easily upgraded form of modules called memory modules or DRAM modules about the size of a few sticks of chewing gum. These can quickly be replaced should they become damaged or when changing needs demand more storage capacity. As suggested above, smaller amounts of RAM (mostly SRAM) are also integrated in the CPU and other ICs on the motherboard, as well as in hard-drives, CD-ROMs, and several other parts of the computer system.\n\nIn addition to serving as temporary storage and working space for the operating system and applications, RAM is used in numerous other ways.\n\nMost modern operating systems employ a method of extending RAM capacity, known as \"virtual memory\". A portion of the computer's hard drive is set aside for a \"paging file\" or a \"scratch partition\", and the combination of physical RAM and the paging file form the system's total memory. (For example, if a computer has 2 GB of RAM and a 1 GB page file, the operating system has 3 GB total memory available to it.) When the system runs low on physical memory, it can \"swap\" portions of RAM to the paging file to make room for new data, as well as to read previously swapped information back into RAM. Excessive use of this mechanism results in thrashing and generally hampers overall system performance, mainly because hard drives are far slower than RAM.\n\nSoftware can \"partition\" a portion of a computer's RAM, allowing it to act as a much faster hard drive that is called a RAM disk. A RAM disk loses the stored data when the computer is shut down, unless memory is arranged to have a standby battery source.\n\nSometimes, the contents of a relatively slow ROM chip are copied to read/write memory to allow for shorter access times. The ROM chip is then disabled while the initialized memory locations are switched in on the same block of addresses (often write-protected). This process, sometimes called \"shadowing\", is fairly common in both computers and embedded systems.\n\nAs a common example, the BIOS in typical personal computers often has an option called “use shadow BIOS” or similar. When enabled, functions that rely on data from the BIOS’s ROM instead use DRAM locations (most can also toggle shadowing of video card ROM or other ROM sections). Depending on the system, this may not result in increased performance, and may cause incompatibilities. For example, some hardware may be inaccessible to the operating system if shadow RAM is used. On some systems the benefit may be hypothetical because the BIOS is not used after booting in favor of direct hardware access. Free memory is reduced by the size of the shadowed ROMs.\n\nSeveral new types of \"non-volatile\" RAM, which preserve data while powered down, are under development. The technologies used include carbon nanotubes and approaches utilizing Tunnel magnetoresistance. Amongst the 1st generation MRAM, a 128 KiB ( bytes) chip was manufactured with 0.18 µm technology in the summer of 2003. In June 2004, Infineon Technologies unveiled a 16 MiB (16 × 2 bytes) prototype again based on 0.18 µm technology. There are two 2nd generation techniques currently in development: thermal-assisted switching (TAS) which is being developed by Crocus Technology, and spin-transfer torque (STT) on which Crocus, Hynix, IBM, and several other companies are working. Nantero built a functioning carbon nanotube memory prototype 10 GiB (10 × 2 bytes) array in 2004. Whether some of these technologies can eventually take significant market share from either DRAM, SRAM, or flash-memory technology, however, remains to be seen.\n\nSince 2006, \"solid-state drives\" (based on flash memory) with capacities exceeding 256 gigabytes and performance far exceeding traditional disks have become available. This development has started to blur the definition between traditional random-access memory and \"disks\", dramatically reducing the difference in performance.\n\nSome kinds of random-access memory, such as \"EcoRAM\", are specifically designed for server farms, where low power consumption is more important than speed.\n\nThe \"memory wall\" is the growing disparity of speed between CPU and memory outside the CPU chip. An important reason for this disparity is the limited communication bandwidth beyond chip boundaries, which is also referred to as \"bandwidth wall\". From 1986 to 2000, CPU speed improved at an annual rate of 55% while memory speed only improved at 10%. Given these trends, it was expected that memory latency would become an overwhelming bottleneck in computer performance.\n\nCPU speed improvements slowed significantly partly due to major physical barriers and partly because current CPU designs have already hit the memory wall in some sense. Intel summarized these causes in a 2005 document.\n\nFirst of all, as chip geometries shrink and clock frequencies rise, the transistor leakage current increases, leading to excess power consumption and heat... Secondly, the advantages of higher clock speeds are in part negated by memory latency, since memory access times have not been able to keep pace with increasing clock frequencies. Third, for certain applications, traditional serial architectures are becoming less efficient as processors get faster (due to the so-called Von Neumann bottleneck), further undercutting any gains that frequency increases might otherwise buy. In addition, partly due to limitations in the means of producing inductance within solid state devices, resistance-capacitance (RC) delays in signal transmission are growing as feature sizes shrink, imposing an additional bottleneck that frequency increases don't address.\n\nThe RC delays in signal transmission were also noted in \"Clock Rate versus IPC: The End of the Road for Conventional Microarchitectures\" which projected a maximum of 12.5% average annual CPU performance improvement between 2000 and 2014.\n\nA different concept is the processor-memory performance gap, which can be addressed by 3D integrated circuits that reduce the distance between the logic and memory aspects that are further apart in a 2D chip. Memory subsystem design requires a focus on the gap, which is widening over time. The main method of bridging the gap is the use of caches; small amounts of high-speed memory that houses recent operations and instructions nearby the processor, speeding up the execution of those operations or instructions in cases where they are called upon frequently. Multiple levels of caching have been developed to deal with the widening gap, and the performance of high-speed modern computers relies on evolving caching techniques. These can prevent the loss of processor performance, as it takes less time to perform the computation it has been initiated to complete. There can be up to a 53% difference between the growth in speed of processor speeds and the lagging speed of main memory access.\n\nIn contrast, RAM can be as fast as 5766 MB/s vs 477 MB/s for an SSD.\n"}
{"id": "8148733", "url": "https://en.wikipedia.org/wiki?curid=8148733", "title": "SANAKO", "text": "SANAKO\n\nSanako is a technology company developing language teaching solutions, teaching and classroom management software, virtual learning and professional development solutions.\n\nSanako's multimedia learning solutions are used by universities, schools, educational institutions and corporations in more than 100 countries. Based in Finland, the company has sales offices in China, India, UAE, UK and US. Sanako supports its over 30,000 classroom installations through a network of over 200 business partners worldwide. \n\n\nMultimedia learning solutions have been designed for use on PCs, providing tutors live interaction with students during cross-curricular teaching, and making multimedia, Internet resources and classroom management tools available. Products for multimedia learning include Sanako Study 1200, Sanako Study 700, Sanako Study 500, Sanako Lab 300.\n\nLanguage learning solutions include traditional language labs, PC-based multimedia suites, and mobile and wireless systems. Current products for language learning are Sanako Lab 100, Sanako Lab 300, and Sanako Study 1200.\n\nClassroom management solutions allow tutors manage the technology resources used during live learning sessions. Products for classroom management include Sanako Study 500, Sanako Study 700, Sanako Study 1200, and Sanako Study Modules.\n\nProfessional services provided by Sanako and its global partner network include training, system installation, helpdesk, maintenance, and remote system support.\n\nSanako products have been designed for the Windows software platform. The company is a Microsoft Certified Partner.\n\nExternal analog and digital devices used during classes can be connected to a number of Sanako products for integrated teaching environment. The company has integrated some of its products with [Blackboard Inc.|Blackboard Academic Suite]. Sanako is co-operating with SIVECO in order to introduce integrated content products to the technology platform.\n\nSanako's learning systems can be extended to wireless handheld devices. The Nokia N810 Internet Tablet has been integrated with the classroom management solution, allowing students to personalise their learning environment and teachers to monitor classroom use of the devices.\n\n\n"}
{"id": "23363551", "url": "https://en.wikipedia.org/wiki?curid=23363551", "title": "Single-ended recuperative burner", "text": "Single-ended recuperative burner\n\nA single-ended recuperative (SER) burner is a type of gas burner used in high-temperature industrial kilns and furnaces. These burners are used where indirect heating is required, e.g. where the products of combustion are not allowed to combine with the atmosphere of the furnace. The typical design is a tubular (pipe) shape with convoluted pathways on the interior, closed on the end pointed into the furnace. A gas burner fires a flame down the center of these pathways, and the hot combustion gases are then forced to change direction and travel along the shell of the tube, heating it to incandescent temperatures and allowing efficient transfer of thermal energy to the furnace interior. Exhaust gas is collected back at the burner end where it is eventually discharged to the atmosphere. The hot exhaust can be used to pre-heat the incoming combustion air and fuel gas (recuperation) to boost efficiency.\n\nSuch burners compete with electrical heating and can be more economical to operate than electric heat, particularly in larger furnaces and in most areas where the price of natural gas per unit of available energy is lower than that of electricity.\n\nThese particular furnaces are claimed to have a significant increase in efficiency over other types of furnaces, and are said to achieve efficiency up to 80 percent and higher.\n"}
{"id": "11057027", "url": "https://en.wikipedia.org/wiki?curid=11057027", "title": "Technological nationalism", "text": "Technological nationalism\n\nTechnological nationalism is a way of understanding how technology affects the society and culture of a nation. One common example is the use of technology as the key subject in a Nationalist project, with the goal of promoting connectedness and a stronger national identity. This idea establishes the belief that the success of a nation can be determined by how well that nation innovates and diffuses technology across its people. Technological Nationalists believe that the presence of national R&D efforts, and the effectiveness of these efforts, are key drivers to the overall growth, sustainability, and prosperity of a nation.\n\nTechnological nationalism is consistently tied to specific countries that are known for their innovative nature. These countries and regions such as Great Britain, Germany and North America have become known for being leaders in technological growth. When identifying leaders in technological innovation it has been affirmed \"technologies are associated with particular nations. Cotton textiles and steam power are seen as British, chemicals as German, mass production as North American, consumer electronics as Japanese.\" These countries have grown to be prosperous due to their strong economic ties to technological growth, \"Historians and others have assumed that Germany and America grew fast in the early years of the twentieth century because of rapid national innovation.\" Because of the effect that technology has on economic growth there is an implicit tie between economic growth and nationalism. Britain became an example of this tie between economic prosperity and technological innovation when they invested heavily in technological research and development to match the innovation standards of other countries.\n\nIndonesia isn't often thought of as an area where excessive innovation occurs. However, in relation to technological nationalism, Indonesia is a front-runner. In 1976, Indonesia established the Industri Pesawat Terban Nusantara or IPTN, which is a government issued company that specializes in air and space travel. The IPTN would soon receive a 2 billion dollar investment from the government, making it among the largest companies in a third world country, let alone one of the only aircraft manufacturers. Because of its overwhelming success, Indonesians felt immense pride for their country and became \"a prominent symbol of Indonesian national esteem and pride.\" Because of this newfound confidence, Indonesians began to consider themselves \"equal to Westerners,\" showing that having pride in one's country can be a direct result of technological investments.\n\nCanada's greatest challenge in the 19th century was to unite the country across a continent. The construction of the CPR (from 1881 to 1885) was a deliberate political and economic attempt to unite Canada's regions and link Eastern and Western Canada, the heartland and hinterland respectively. Charland identified this project as based on the nation's faith in technology's ability to overcome physical obstacles. As the technology was adapted to suit Canadian needs, it fed the national rhetoric that railroads were an integral part of nation building. This spirit of technological nationalism also fuelled the development of broadcasting in the country and thus further served in the development of a national identity. Paradoxically however, these technologies, which historian Harold Innis termed \"space-binding,\" simultaneously supported and undermined the development of a Canadian nation. Based in connection rather than content, they did not favour any particular set of values, except those arising from trade and communication themselves, and so they also contributed to Canada's integration into first the British, and then the American empire.\n\n\n"}
{"id": "6967890", "url": "https://en.wikipedia.org/wiki?curid=6967890", "title": "Timeline of virtualization development", "text": "Timeline of virtualization development\n\nNote: This timeline is missing data for important historical systems, including: Atlas Computer (Manchester), GE 645, Burroughs B5000\n\nOpen source kvm released which is integrated with linux kernel and provides virtualization on only linux system, it needs hardware support.\n\nIn the mid-1960s, IBM's Cambridge Scientific Center developed CP-40, the first version of CP/CMS. It went into production use in January 1967. From its inception, CP-40 was intended to implement full virtualization. Doing so required hardware and microcode customization on a S/360-40, to provide the necessary address translation and other virtualization features. Experience on the CP-40 project provided input to the development of the IBM System/360-67, announced in 1965 (along with its ill-starred operating system, TSS/360). CP-40 was reimplemented for the S/360-67 as CP-67, and by April 1967, both versions were in daily production use. CP/CMS was made generally available to IBM customers in source code form, as part of the unsupported IBM Type-III Library, in 1968.\n\nIBM announced the System/370 in 1970. To the disappointment of CP/CMS users – as with the System/360 announcement – the series would not include virtual memory. In 1972, IBM changed direction, announcing that the option would be made available on all S/370 models, and also announcing several virtual storage operating systems, including VM/370. By the mid-1970s, CP/CMS, VM, and the maverick VP/CSS were running on a numerous large IBM mainframes. By the late 80s, there were reported to be more VM licenses than MVS licenses.\n\nOn February 8, 1999, VMware introduced the first x86 virtualization product, \"VMware Virtual Platform\", based on earlier research by its founders at Stanford University.\n\nPreviously, a substantial licensing fee was required for the use of VMware's Workstation product. VMware decided to provide high quality virtualization technology to everyone for free. They omitted the ability to create virtual machines and did not distribute the acceleration tools that come with VMware workstation. This early corporate play to encourage consumer applications of virtualization went largely unnoticed.\n\nThis year virtualization has a new level of playing field in application virtualization and application streaming.\n\nVMware releases VMware Workstation 6.5 beta, the first program for Windows and Linux to enable DirectX 9 accelerated graphics on Windows XP guests .\n\nAs an overview, there are three levels of virtualization:\n\nApplication virtualization solutions such as VMware ThinApp, Softricity, and Trigence attempt to separate application specific files and settings from the host operating system, thus allowing them to run in more-or-less isolated sandboxes without installation and without the memory and disk overhead of full machine virtualization. Application virtualization is tightly tied to the host OS and thus does not translate to other operating systems or hardware. VMware ThinApp and Softricity are Intel Windows centric, while Trigence supports Linux and Solaris. Unlike machine virtualization, Application virtualization does not use code emulation or translation so CPU related benchmarks run with no changes, though fileystem benchmarks may experience some performance degradation. On Windows, VMware ThinApp and Softricity essentially work by intercepting filesystem and registry requests by an application and redirecting those requests to a preinstalled isolated sandbox, thus allowing the application to run without installation or changes to the local PC. Though VMware ThinApp and Softricity both began independent development around 1998, behind the scenes VMware ThinApp and Softricity are implemented using different techniques:\nBecause Application Virtualization runs all application code natively, it can only provide security guarantees as strong as the host OS is able to provide. Unlike full machine virtualization, Application virtualization solutions currently do not work with device drivers and other code that runs at ring0 such as virus scanners. These special applications must be installed normally on the host PC in order to function.\n\nAnother technique sometimes referred to as virtualization, is portable byte code execution using a standard portable native runtime (aka Managed Runtimes). The two most popular solutions today include Java and .NET. These solutions both use a process called JIT (Just in time) compilation to translate code from a virtual portable machine language into the local processor’s native code. This allows applications to be compiled for a single architecture and then run on many different machines. Beyond machine portable applications, an additional advantage to this technique includes strong security guarantees. Because all native application code is generated by the controlling environment, it can be checked for correctness (possible security exploits) prior to execution. Programs must be originally designed for the environment in question or manually rewritten and recompiled to work for these new environments. For example, one cannot automatically convert or run a Windows / Linux native app on .NET or Java. Because portable runtimes try to present a common API for applications for a wide variety of hardware, applications are less able to take advantage of OS specific features. Portable application environments also have higher memory and CPU overheads than optimized native applications, but these overheads are much smaller compared with full machine virtualization. Portable Byte Code environments such as Java have become very popular on the server where a wide variety of hardware exist and the set of OS-specific APIs required is standard across most Unix and Windows flavors. Another popular feature among managed runtimes is garbage collection, which automatically detects unused data in memory and reclaims the memory without the developer having to explicitly invoke free(ing) operations.\n\nGiven the industry-biased in the past, to be more neutral, there are also two other ways to look at the Application Level:\n\nMicrosoft bought Softricity on July 17, 2006 and popularized Application Streaming, giving traditional Windows applications a level playing field with Web and Java applications with respect to the ease of distribution (i.e. no more setup required, just click and run). Soon every JRE and CLR can run virtually in user mode, without kernel mode drivers being installed, such that there can even be multiple versions of JRE and CLR running concurrently in RAM.\n\nThe integration of the Linux Hypervisor into the Linux Kernel and that of the Windows Hypervisor into the Windows Kernel may make rootkit techniques such as the filter driver obsolete.\nThis may take a while as the Linux Hypervisor is still waiting for the Xen Hypervisor and VMware Hypervisor to be fully compatible with each other as Oracle impatiently pounding at the door to let the Hypervisor come into the Linux Kernel so that it can full steam ahead with its Grid Computing life. Meanwhile, Microsoft have decided to be fully compatible with the Xen Hypervisor\n. IBM, of course, doesn't just sit idle as it is working with VMware for the x86 servers, and possibly helping Xen to move from x86 into Power Architecture using the open source rHype.\nNow, to make the Hypervisor party into a full house, Intel VT-x and AMD-V are hoping to ease and speed up paravirtualization so that a guest OS can be run unmodified. \n\n\n"}
{"id": "19627280", "url": "https://en.wikipedia.org/wiki?curid=19627280", "title": "Tom Gruber", "text": "Tom Gruber\n\nThomas Robert \"Tom\" Gruber (born 1959) is an American computer scientist, inventor, and entrepreneur with a focus on systems for knowledge sharing and collective intelligence. He did foundational work in ontology engineering and is well known for his definition of ontologies in the context of artificial intelligence.\n\nIn 2007 Gruber co-founded Siri Inc., which created the Siri intelligent personal assistant and knowledge navigator. Siri Inc. was acquired by Apple in 2010, and Siri is now an integral part of iOS.\n\nGruber studied psychology and computer science at the Loyola University New Orleans, where he received a double major B.S. in 1981 and graduated summa cum laude. He designed and implemented a computer-assisted instruction (CAI) system for programmed-curriculum courses. It was the first of its kind at the university, and is used routinely by the Psychology department for introductory courses. In 1984 he received a M.S. in Computer and Information Science at the University of Massachusetts Amherst. For his Master's research, Gruber designed and implemented an intelligent communication prosthesis assistant, a computer system which enables people with severe physical disabilities who cannot otherwise speak to communicate in natural language presented in displayed, written, or spoken form. Four years later in 1988 at the University of Massachusetts Amherst he received a Ph.D. in Computer and Information Science with the dissertation \"The Acquisition of Strategic Knowledge\". His dissertation research addressed a critical problem for Artificial Intelligence—knowledge acquisition—with a computer assistant that acquires strategic knowledge from experts.\n\nFrom 1988 to 1994 Gruber was a Research Associate at the Knowledge Systems Laboratory of the Computer Science Department at Stanford University. He worked on the How Things Work, SHADE, and Knowledge Sharing Technology projects. In 1994 he became Senior Project Leader, Enterprise Integration Technologies and proposed and designed several projects using the Internet to create shared, virtual environments for collaborative learning and work (for ARPA, NASA, and NIST). During this time he also proposed a business plan for corporate training. In 1995, he founded and became Chief Technology Officer of Intraspect Software, an enterprise software company that did early commercial work on collaborative knowledge management. Intraspect applications help professional people collaborate in large distributed communities, continuously contributing to a collective body of knowledge.\n\nGruber has been a member of journal editorial boards of the \"Knowledge Acquisition\", \"IEEE Expert\" and \"International Journal of Human-Computer Studies\".\n\nGruber's research interests in the 1990s were in the field of developing intelligent networked software to support human collaboration and learning. Areas of specialty include: knowledge acquisition, knowledge representation, computer-supported collaborative work, computer-mediated communication for design, and knowledge sharing technology.\n\nIn 1994 he was responsible for the creation of hypermail, an email to web gateway software that saw extensive use after a rewrite by a different programmer.\n\nIn 2007 Gruber co-founded Siri Inc., which created the Siri intelligent personal assistant and knowledge navigator. Siri Inc. was acquired by Apple in 2010, and Siri is now an integral part of iOS. In 2016, Siri was added to macOS in macOS Sierra.\n\nIn April 2017 Gruber spoke on the TED stage about his vision for the future of \"humanistic AI\" especially in regards to augmentation of human capacities such as memory. He is quoted saying, \"We are in the middle of a renaissance in AI. Every time a machine gets smarter, we get smarter.\"\n\nGruber published several articles and some books, most notably:\n\n\n"}
{"id": "50437164", "url": "https://en.wikipedia.org/wiki?curid=50437164", "title": "TrekkSoft", "text": "TrekkSoft\n\nTrekkSoft is an online booking and payment software provider for tour and activity companies. The software as a service company is headquartered in Interlaken, Switzerland.\n\nTrekkSoft was founded in Interlaken in 2010 by Jon Fauver, Valentin Binnendijk and Philippe Willi.\n\nAfter working as a raft guide in Nepal, Jon Fauver co-founded Outdoor Interlaken in Switzerland in 2001 and become Co-Owner of Bus2Alps in 2006.\n\nPhilippe Willi joined Outdoor Interlaken in 2007 and Bus2Alps as Co-Owner in 2009. Philippe Willi knew Valentin Binnendijk from studying at University of St. Gallen, which put the three founders in contact.\n\nTrekkSoft was founded in 2010.\n\nIn 2012, the TrekkSoft platform launched and began offering booking software to tour and activity companies. In June 2012, 55 tour operators were using TrekkSoft booking software.\n\nIn June 2013, TrekkSoft launched its mobile application.\n\nAs of May 2016, TrekkSoft has been used by tour and activity providers in 125 countries. In TrekkSoft's current management structure, Jon Fauver is CEO, Valentin Binenndijk is CTO, and Philippe Willi is COO/CFO.\n\nIn August 2013, TrekkSoft raised $800,000 in an investment seed round.\n\nIn January 2015, TrekkSoft raised $1.1 million in investment from Redalpine and a group of independent investors including Armin Meier (former CEO of Kuoni), and Walter Güntensperger (former CEO of Hotelplan Switzerland).\n\nAs of January 2015, the total invested in the company is $2.8 million.\n\nIn January 2014, TrekkSoft signed a partnership with TripAdvisor to integrate TripAdvisor content across TrekkSoft’s network of vendors.\n\nIn November 2015, TrekkSoft acquired Acteavo, the Irish-based booking software company.\n\nIn December 2015, TrekkSoft and Diviac, the online booking website for scuba holidays, announced a partnership for dive centers to manage bookings and increase online distribution.\n\nIn May 2016, TrekkSoft announced a strategic partnership with Myobis Booking Systems, a Germany-based software provider for the tourism and events industry. The same month, TrekkSoft also announced a partnership with Musement, the digital travel companion and marketplace.\n\nIn 2017, TrekkSoft Group was founded as a collective of TrekkSoft, TrekkPay, TrekkConnect. The group aim to build and develop software companies to better serve medium to large organisations in the Tourism Industry. \n\nIn November 2018, TrekkSoft Group acquired Digitickets,a leading software company for theme parks and attractions with headquarters in the UK. \n\nIn September 2014, TrekkSoft was awarded ninth place in the \"Top 100 Startup Award\" 2014\n\nIn September 2015, Business Insider named TrekkSoft as one of \"The 12 hottest startups in Switzerland\". TrekkSoft was also ranked seventh in the \"Top 100 Startups Award\" 2015.\n"}
{"id": "57951641", "url": "https://en.wikipedia.org/wiki?curid=57951641", "title": "VR Systems", "text": "VR Systems\n\nVR Systems is a provider of elections technology systems and software founded in Florida in 1992 by Jane and David Watson. VR Systems is based in Tallahassee, Florida. The company's products are used in elections in eight U.S. states. The CEO and President is currently Mindy Perkins.\n\nVR Systems was founded in Florida in 1992 and grew its voter registration system, VoterFocus, in the years following the passage of the Help America Vote Act in 2002. In 2004, in response to devastation caused by Hurricane Charley in South Florida, VR created the EViD electronic pollbook designed to check in voters at central locations as many of the precincts in the area had been destroyed. Today, 64 of 67 counties in Florida use VR products. In 2010, VR became a 100% employee owned company.\n\nVR Systems was reportedly targeted by operatives of the Russian Main Intelligence Directorate (GRU) in and around August 2016. Russian actors also attempted to impersonate VR Systems by creating a false email address as part of a spear phishing campaign targeting state electoral officials. There are no reports that the spearphishing campaign was successful.\n\nVR Systems offers the EViD electronic pollbook, Voter Focus voter registration software, ELM online training and website services specifically designed for the elections community. \n\n"}
{"id": "4721358", "url": "https://en.wikipedia.org/wiki?curid=4721358", "title": "Variable gauge", "text": "Variable gauge\n\nA variable gauge system allows railway vehicles in a train to travel across a break of gauge caused by two railway networks with differing track gauges.\n\nFor through-operation, a train must be equipped with special trucks holding variable gauge wheelsets containing a variable gauge axle (VGA). The gauge is altered by driving the train through a gauge changer or gauge changing facility.\n\nAs the train passes through the gauge changer, the wheels are unlocked, moved closer together, or further apart, and are then re-locked. Installed variable gauge systems exist within the internal network of Spain, and are installed on international links between Spain/France (Spanish train), Sweden/Finland (Swedish train), Poland/Lithuania (Polish train) and Poland/Ukraine (Polish train).\n\nA system for changing gauge, without need for stopping is widespread for passenger traffic in Spain, used in services run on a mix of dedicated high-speed lines (using Standard gauge) and older lines (using Iberian gauge). Similar systems for freight traffic are still rather incipient, as the higher axle weight increases the technological challenge. Although several alternatives exist, including transferring freight, replacing individual wheels and axles, truck exchange, transporter flatcars or the simple transshipment of freight or passengers, they are impractical, thus a cheap and fast system for changing gauge would be beneficial for cross-border freight traffic.\n\nAlternative names include Gauge Adjustable Wheelsets (GAW), Automatic Track Gauge Changeover System (ATGCS/TGCS), Rolling Stock Re-Gauging System (RSRS), Rail Gauge Adjustment System (RGAS), Shifting wheelset, Variable Gauge Rolling Truck, track gauge change and track change wheelset.\n\nVariable gauge axles help solve the problem of a break-of-gauge without having to resort to dual gauge tracks or transshipment. Systems allow the adjustment between two gauges. No gauge changer designs supporting more than two gauges are used.\n\nThere are several variable gauge axle systems:\n\n\nThe variable gauge systems are not themselves all compatible. Only the SUW 2000 and Rafil Type V systems are interoperable.\n\nIn 2009, at Roda de Barà near Tarragona, a Unichanger capable of handle four different VGA systems was under development.\n\nVGA is particularly important with international railway traffic because gauge changes tend to occur more often at international borders.\n\nThe maximum speed of the trains equipped with the different technologies vary. Only CAF and Talgo produce high-speed VGA, allowing speeds up to 250 km/h.\n\nA gauge changer is a device which forces the gauge adjustment in the wheels. Designs consist of a pair of running rails that gradually vary in width between the two gauges, combined with other rails and levers to unlock, move, support and re-lock the adjustable axles.\n\nIn the Spanish Talgo-RD system, a constant spray of water is used to lubricate the metal surfaces, to reduce heat and wear. A Talgo-RD gauge changer is 20 metres long and 6 metres wide.\n\nVariable gauge multiple units, or a train including a variable gauge locomotive (e.g. Talgo 250) and rolling stock, may drive straight across a gauge changer. Normally the locomotive will not be able to change gauge, meaning that it must move out of the way whilst the remainder of the train itself passes through. On the opposite side, a new locomotive of the other gauge will couple to the train.\n\nA train (or an individual car) can be pushed halfway across the gauge-changer, uncoupled, and then (once far enough across) coupled to the new locomotive and pulled the rest of the way. A long length of wire-rope with hooks on the end means that the process can be asynchronous, with the rope used to bridge across the length of the gauge changer (to temporarily couple the arriving cars and receiving locomotive, although without braking control from the locomotive to the train vehicles).\n\nOn long-distance trains in Spain and night trains crossing from Spain into France, the arriving locomotive stops just short of the gauge changer, uncouples and moves into a short siding out of the way. Gravity then moves the train through the gauge changer at a controlled low speed. The new locomotive is coupled onto the front only after the full train has finished passing through the changer.\n\nIn 1933, as many as 140 inventions were offered to Australia railways to overcome the breaks of gauge between the different states. None was accepted. About 20 of these devices were adjustable wheels/axles of some kind or another, which may be analogous to the modern VGA. VGA systems were mostly intended for Broad Gauge and Standard Gauge lines.\n\nVariable gauge axles were used for a while on the Grand Trunk Railway in the 1860s in Canada to connect and standard gauge without transshipment. Five-hundred vehicles were fitted with \"adjustable gauge trucks\" but following heavy day-in, day-out use the system proved unsatisfactory, particularly in cold and snowy weather. The system used telescoping axles with wide hubs that allowed the wheels to be squeezed or stretched apart through a gauge-changer, after holding pins had been manually released.\n\nRailway operations over the Niagara Bridge were also complicated.\n\n\nIn 1999, a gauge-changer was installed at Tornio at the Finnish end of the dual-gauge section between Haparanda and Tornio, for use with variable gauge freight wagons. The Tornio gauge changer is a \"Rafil\" design from Germany; a similar \"Talgo-RD\" gauge changer at the Haparanda end used to exist, but was removed as it required de-icing in winter.\n\nTrain ferry traffic operated by SeaRail and arriving from Germany and Sweden by sea used bogie exchange facilities in the Port of Turku.\n\nA new gauge changer has been put in place in Akhalkalaki for Baku-Tbilisi-Kars railway. Northwestern end has rails apart, southeastern end has rails apart. Both bogie exchhange and variable gauge adapters are provided.\n\n\nThe \"Gauge Change Train\" is a project started in Japan in the 1990s to investigate the feasibility of producing an electric multiple unit (EMU) train capable of operating both the Shinkansen high-speed network at 270–300 km/h and the original network at 130–140 km/h. See .\n\nThe first-generation train was tested from 1998 to 2006, including on the US High-speed Test Track in 2002. The second-generation train, intended to run at a maximum speed of , was test-run in various locations in Japan between 2006 and 2013. A third-generation train has been undergoing reliability trials since 2014 in preparation for potential introduction to service on the planned Kyushu Shinkansen extension to Nagasaki.\n\nPoland has SUW 2000 gauge changers installed on international lines to Lithuania and Ukraine used for daily night-trains and some freight transport.\n\nSpain is the largest user of variable gauge systems. This is because of the need to connect older mainlines built to Iberian gauge and extensive new high-speed railway lines and connections to France, using the standard gauge. Two gauge changes are installed on lines to France and at all entrances/exits leading between the high-speed network and older lines. There are also significant lengths of secondary lines but these are not connected to the main network. \n\nIn February 2004, RENFE placed orders for:\n\nVariable gauge axles are going to be implemented on the Montreux–Gstaad–Zweisimmen–Spiez–Interlaken line. Trains will automatically switch from to at Zweisimmen. A trial bogie has been built and tested. It has no axles which allow the bogie half frames holding the wheels on both sides to slide sideways to each other.\n\nAs part of a joint bid for the 2012 European Football cup with Poland VGA trains would be introduced across their border.\n\nJohn Fowler mentions in 1886 at attempt by the GWR to develop a \"telescopical\" axle.\n\nTrams ran between Leeds () and Bradford ( gauge) following a successful trial in 1906 using Bradford tram car number 124. The system was later patented by – GB190601695 (A) of 1906. This system was improved again in patent GB190919655 (A) of 1909 by introducing a locking system acting on the wheel and axle rather than just the wheel rim. This provided a more effective grip where the wheel was free to move along the splined axle.\n\nIn VGA, the train is pulled through the \"adjuster\" at about 10 km/h (2.77 m/s) without any need to uncouple the wagons or disconnect (and test) the brake equipment.\n\nVGA always has the exact number of wheels of each gauge and they are always at hand, whereas with truck exchange, there must be a complete set of trucks of the right gauge and with wheels of the same diameter at the depot.\n\nTruck exchange needs a stock of spare trucks of each gauge, which need to be shunted as required. If there is an influx of traffic one might run out of trucks of the other gauge. Depending on the sophistication of the exchange facility, the cars may need to be uncoupled for the truck exchange to take place.\n\nSteam locomotive are generally not gauge convertible on-the-fly. While diesel locomotives can be truck exchanged, this is not normally done owing to the complexity in the reconnection of cables and hoses. In Australia, some locomotives are transferred between gauges. The transfer might happen every few months, but not for an individual trip.\n\nBy 2004, variable gauge electric passenger locomotives were available from Talgo. It is not clear if variable gauge freight locomotives are available.\n\n\n\n\n\n"}
{"id": "16227046", "url": "https://en.wikipedia.org/wiki?curid=16227046", "title": "Warrant canary", "text": "Warrant canary\n\nA warrant canary is a method by which a communications service provider aims to inform its users that the provider has been served with a secret government subpoena despite legal prohibitions on revealing the existence of the subpoena. The warrant canary typically informs users that there has \"not\" been a secret subpoena as of a particular date. If the canary is not updated for the time period specified by the host or if the warning is removed, users are to assume that the host has been served with such a subpoena. The intention is to allow the provider to warn users of the existence of a subpoena passively, without disclosing to others that the government has sought or obtained access to information or records under a secret subpoena.\n\nSecret subpoenas, such as those covered under 18 U.S.C. §2709(c) of the USA Patriot Act, provide criminal penalties for disclosing the existence of the warrant to any third party, including the service provider's users. \n\nUnited States secret subpoenas or national security letters originated in the 1986 Electronic Communications Privacy Act to be used only against those suspected of being agents of a foreign power. This was revised in 2001 under the Patriot Act so that secret subpoenas can be used against anyone who may have information deemed relevant to counter-intelligence or terrorism investigations. The idea of using negative pronouncements to thwart the nondisclosure requirements of court orders and served secret warrants was first proposed by Steven Schear on the cypherpunks mailing list, mainly to uncover targeted individuals at ISPs. It was also suggested for and used by public libraries in 2002 in response to the USA Patriot Act, which would force librarians to disclose the circulation history of any of their patrons.\n\nThe first commercial use of a warrant canary was by the US cloud storage provider rsync.net, which began publishing its canary in 2006. In addition to a digital signature, it provides a recent news headline as proof that the warrant canary was recently posted as well as mirroring the posting internationally.\n\nOn November 5, 2013, Apple became the most prominent company to publicly state that it had never received an order for user data under Section 215 of the Patriot Act. On September 18, 2014, GigaOm reported that the warrant canary statement did not appear anymore in the next two Apple Transparency Reports, covering July–December 2013 and January–June 2014. Tumblr also included a warrant canary in the transparency report that it issued on February 3, 2014. In August 2014, the online cloud service Spider Oak implemented an encrypted warrant canary that publishes an \"All Clear!\" message every 6 months. Three PGP signatures from geographically distributed signers must sign each message — so if a government agency forced SpiderOak to update the page, they would need to enlist the help of all three signers.\n\nIn September 2014, US security researcher Moxie Marlinspike wrote that \"every lawyer I've spoken to has indicated that having a 'canary' you remove or choose not to update would likely have the same legal consequences as simply posting something that explicitly says you've received something.\"\n\nAustralia outlawed the use of a certain kind of warrant canary in March 2015, making it illegal for a journalist to \"disclose information about the existence or non-existence\" of a warrant issued under new mandatory data retention laws. It is unlikely a journalist could give a correct canary in this situation anyway, as under this legislation the agency obtaining the warrant is not compelled to inform the journalist of the warrant. Afterwards, computer security and privacy specialist Bruce Schneier wrote in a blog post that \"[p]ersonally, I have never believed [warrant canaries] would work. It relies on the fact that a prohibition against speaking doesn't prevent someone from not speaking. But courts generally aren't impressed by this sort of thing, and I can easily imagine a secret warrant that includes a prohibition against triggering the warrant canary. And for all I know, there are right now secret legal proceedings on this very issue.\"\n\nThat said, case law specific to the United States would render the covert continuance of warrant canaries subject to constitutionality challenges. West Virginia State Board of Education v. Barnette and Wooley v. Maynard rule the Free Speech Clause prohibits compelling someone to speak against one's wishes; this can easily be extended to prevent someone from being compelled to lie. New York Times Co. v. United States protects one exercising the First Amendment to publish government information, even if it is against the wishes of the government, except under grave and exceptional circumstances previously set by act and precedent. The latter also carries the weight of acting against a direct government intervention similar to a government intervention against a warrant canary.\n\nThe following is a list of notable companies and organizations that have published warrant canaries:\nThe following is a list of companies and organizations whose warrant canaries no longer appear in transparency reports:\n\nIn 2015, a coalition of organizations consisting of the EFF, Freedom of the Press Foundation, NYU Law, the Calyx Institute, and the Berkman Center created a website called Canary Watch in order to provide a compiled list of all companies providing warrant canaries. Its mission was to provide prompt updates of any changes in a canary's state. It is often difficult for users to ascertain a canary's validity on their own and thus Canary Watch aimed to provide a simple display of all active canaries and any blocks of time that they were not active. In May 2016, it was announced that Canary Watch \"will no longer accept submissions of new canaries or monitor the existing canaries for changes or take downs\". The coalition of organizations which created Canary Watch explained their decision to discontinue the project by stating that it has achieved its goals to raise awareness about \"illegal and unconstitutional national security process, including National Security Letters and other secret court processes.\" The Electronic Frontier Foundation also noted that \"the fact that canaries are non-standard makes it difficult to automatically monitor them for changes or takedowns\".\n\n"}
{"id": "1213458", "url": "https://en.wikipedia.org/wiki?curid=1213458", "title": "Willgodt Theophil Odhner", "text": "Willgodt Theophil Odhner\n\nWillgodt Theophil Odhner (in Cyrillic, Вильгодт Теофил Однер) was a Swedish engineer and entrepreneur, working in St. Petersburg, Russia. He was the inventor of the Odhner Arithmometer, which by the 1940s was one of the most popular type of portable mechanical calculator in the world.\n\nAccording to a brochure distributed by Odhner's company at the Paris World exposition of 1900 \"...Odhner had, in 1871, an opportunity to repair a Thomas calculating machine and then became convinced that it is possible to solve the problem of mechanical calculation by a simpler and more appropriate way\". It took him 19 years to perfect the design of this new machine so it could be manufactured effectively.\n\nOdhner studied at the Royal Institute of Technology in Stockholm from 1864 to 1867 but left before graduating. At age 23, in 1868, he moved to Saint Petersburg, Russia, even though he didn't speak any Russian. As soon as he arrived, he went to the Swedish consulate, which found him a job in a local mechanical workshop. A few months later he joined the Nobel's mechanical factory owned by a Swede named Ludvig Nobel (1831–1888), brother of Alfred Nobel of Nobel Prize fame, where he worked until 1877. In 1878 he joined the Expedition, a large paper mill and printing house, and worked there until 1892.\n\nWhile working at the Expedition, Odhner started his own workshop in 1885, building high quality production machines for local manufacturing businesses. One of his biggest project was the manufacturing of printing presses, he also made cigarette-making machines and all kind of scientific instruments. Odhner officially started the production of his arithmometer in this workshop in 1890. Early on, Mr. F. N. Hill, a British citizen, became his associate but he left the company around 1897, making Odhner the sole proprietor until his death in 1905. \n\nAfter Odhner's death, his sons Alexander and Georg and son-in-law Karl Siewert continued the production and about 23,000 calculators were made before the factory was forced to close down in 1918.\n\nIndependent clone makers from all over the world, including Russia, carried the design from 1893 well into the 1970s.\nIn 1871 he married Alma Skånberg the daughter of Fredrik Skånberg, a Nobel coworker. They had seven children. Unfortunately Emilia, their second child, died at age 2 in 1877, the same year Alma, their third child was born. \n\nW. T. Odhner died of a heart attack on 15 September 1905 (Gregorian calendar) in Saint Petersburg, Russia.\n\n\n"}
{"id": "55946527", "url": "https://en.wikipedia.org/wiki?curid=55946527", "title": "Yangshupu Waterworks", "text": "Yangshupu Waterworks\n\nThe Yangshupu Waterworks (Chinese: 杨树浦水厂) is a waterworks built in 1883, and located at 830 Yangshupu Road (杨树浦路830号) in the district of Yangpu, Shanghai, China. The waterworks was the first of its kind to be built in China and provided running water for the first time to some of the cities' residents. It belongs to the Shanghai Water Company and occupies a site of 12.9 hectares and has four major lines of tap water allowing for a maximum capacity of around 1.5 million cubic metres a day. In 2009 it supplied 400 million cubic metres of water, about 20% of the total water supply of Shanghai.\n\nIn 2013, the site was designated as a Major Historical and Cultural Site Protected at the National Level by the State Council of China, one of 28 sites in Shanghai. The site is also of importance due to the effect it had on China's early modern industries in the 1880s and 1890s, with many factories being built around the waterworks providing electricity, gas, vehicles and textiles. It is regarded as the birthplace of China's modern industry by the Shanghai municipal government. Furthermore, the site is protected due to its unique architecture.\n\nTo the east and west of the waterworks used to lie the docks. The waterworks lie to the south west of the Yangshupu Power Station.\n\nIn 1880 the Shanghai Waterworks Co. Ltd were founded in London, United Kingdom. British and Chinese engineers provided expertise with British investors providing the funds, to built a waterworks in Shanghai in what was at that time the Shanghai International Settlement, a merging of the previous British and American settlement areas which had existed before 1863. Building started in August 1881 and the design was carried out by the British architect J.W. Hart (赫德), in the style of an English castle.\n\nOn 29 June 1883, Li Hongzhang the Viceroy of Zhili of the Qing Dynasty, opened the water gate valve and thus opened the waterworks. It was the first waterworks to be built in China, and at the time of its construction the largest in East Asia. During the 1930s the site was much larger spanning 25.9 hectares.\n\nIn 1911, Zhabei Waterworks (闸北水厂) was built to the west of Yangshupu Waterworks on Hengfeng Road. It supplied water to residents and factories on that side of the city, decreasing the usage of the Yangshupu Waterworks. In 1926 the waterworks in Zhabei were moved to their present location on Zhayin Road.\n\nBy the 1950s, the waterworks was providing around 300,000 cubic metres of water a day.\n\nOn 10 March 2001, for the first time in its history the waterworks opened its gates to the general public. Several thousand Shanghai residents applied for tickets.\n\nThe buildings and its perimeter wall were designed to resemble English castles featuring pink and grey brickwork and mock turrets. At the top of the walls and on the roofs are crenellation to resemble a castle's parapet. \n\nIn 1989, the site was designated a monument of importance for the city of Shanghai, and became protected and preserved as a historical site by the Shanghai city government.\n\nIn 2013, the site was designated (7-1694) a Major Historical and Cultural Site Protected at the National Level by the State Council of China, giving it national protection.\n\nSome of the buildings of the site now host the Shanghai Waterworks Science and Technology Museum (上海自来水科技馆) which was opened in Autumn 2003. It has three exhibitions detailing the history, technology and future of the waterworks.\n\nDue to the importance of its role in supplying running water, numerous factories and workshops were rapidly built around the Yangshupu Road, many of which were likewise the first of their kind in China. The most important are listed below:\n\n\nOpposite the waterworks are the sites of:\n\n\nTo the south of the waterworks, facing the Huangpu River where once the docks stood, is the Yangpu Riverside (杨浦滨江) park and promenade opened in 2017.\n\n\n"}
{"id": "48896023", "url": "https://en.wikipedia.org/wiki?curid=48896023", "title": "Zi Corporation", "text": "Zi Corporation\n\nZi Corporation was a software company based in Calgary, Canada. The company was founded on 4 December 1987 as Cancom Ventures Inc, owning an Edmonton secretarial college and an industrial equipment rental business. On 30 August 1989 the name was changed to Multi-Corp Inc. In 1993, board member Michael Lobsinger took control of the company, became CEO, and turned the company towards the telecommunications industry, purchasing several privately held companies involved in the telecommunications businesses, and in November 1993, Multi-Corp entered into an exclusive licensing agreement with Eric Chappell for a stroke-based Chinese text entry system which they referred to as the \"Jiejing Licenses\". A wholly owned subsidiary, Ziran Developments Inc, was formed to handle the Chinese text entry business. \nThe company was listed on the Toronto Stock Exchange on 9 June 1995 (symbol MCU). Under Lobsinger, the strategic importance of the Chinese text entry grew, and Ziran was renamed Zi Corporation in 1996; \"Zi\" meaning \"character\" in Chinese. \nThe telecommunications businesses were disposed of by 1997 In June 1997, the parent company was renamed Zi Corporation to reflect the importance of the new business. In 1999, co-founder Gary Kovacs joined as COO.\n\nHaving started with Chinese on personal computers, toward the end of the 1990s Zi's focus moved to text entry for all languages for mobile phones and other handheld devices, where efficient entry of text traditionally has been challenging due to limitations of a physically small device. This new business area was quickly quite successful, especially in the Chinese market, and in 1998 Zi entered into licensing agreements with Ericsson (later Sony Ericsson), who at that time was a very significant player in the Chinese market. Further license agreements with Ericsson and other companies were to follow, for a total of more than 1000 different device models being shipped with Zi text entry software,\nin more than 100 million devices.\n\nThe introduction of touch screen technology for mobile devices made the company follow up with software for handwriting recognition, with focus on languages such as Chinese and Japanese.\nIn 2003, Mike Donnell took over as CEO from Lobsinger.\nZi Corporation was listed on NASDAQ from September 2007 .\n\nText entry on devices with limited number of keys was early subject to a number of software patents, and Zi was engaged in several costly patent disputes with Tegic Corporation, later Nuance Communications.\nIn August 2008, Nuance made a takeover bid to acquire the company for million, which the company declined. Shortly thereafter, Nuance filed a patent infringement lawsuit against Zi. The company was finally acquired by Nuance for million on 26 February 2009, and the acquisition was completed on 9 April 2009.\n\nZi's main product was \"EZiText\" predictive text entry software, in competition with solutions such as the original T9 product from Tegic, later Nuance.\nOther products include \"eZiType\", a predictive text entry product for mobile email users, for keyboard-based mobile devices, \"Qix\", a service discovery engine that provides a quick and easy method for accessing a phone's features, applications, and services, and \"Decuma\" which delivers an input method for pen-based devices by fusing handwriting recognition with predictive text technology.\n\nThere was a series of patent infringement lawsuits between Tegic, the holder of the T9 patents, and Zi:\n\n\n"}
