{"id": "8762412", "url": "https://en.wikipedia.org/wiki?curid=8762412", "title": "22.2 surround sound", "text": "22.2 surround sound\n\n22.2 or Hamasaki 22.2 (named after Kimio Hamasaki, a senior research engineer at NHK Science & Technology Research Laboratories in Japan) is the surround sound component of Super Hi-Vision (a new television standard with 16 times the pixel resolution of HDTV). It has been developed by NHK Science & Technical Research Laboratories. It uses 24 speakers (including two subwoofers) arranged in three layers.\n\nThe channel numbers and labels are: \n\nChannel numbers up to 6 represent the same as those in a 5.1 channel system. There are then a further 5 listener-plane channels, 9 overhead channels, arranged in a square, and a row of 3 further channels at the bottom front, plus the additional LFE channel.\n\n\n\n\n"}
{"id": "7402901", "url": "https://en.wikipedia.org/wiki?curid=7402901", "title": "A-MAC", "text": "A-MAC\n\nA-MAC carries the digital information: sound, data-teletext on an FM subcarrier at 7 MHz. Since the vision bandwidth of a standard MAC signal is 8.4 MHz, the horizontal resolution on A-MAC has to be reduced to make room for the 7 MHz carrier. \"A-MAC has not been used in service.\"\n\nMAC transmits luminance and chrominance data separately in time rather than separately in frequency (as other analog television formats do, such as composite video).\n\nAudio and Scrambling (selective access)\n\n\n"}
{"id": "28760304", "url": "https://en.wikipedia.org/wiki?curid=28760304", "title": "Aeroplane (magazine)", "text": "Aeroplane (magazine)\n\nAeroplane (formerly Aeroplane Monthly) is a British magazine devoted to aviation, with a focus on aviation history and preservation. Issue 1 of \"Aeroplane Monthly\" was published in May 1973 at a cover price of 30p, in association with \"Flight International\", by IPC Media. The founder was Richard T. Riding. The magazine is now owned by Key Publishing Ltd and headquartered in Stamford, Lincolnshire.\n\nThe magazine is the successor to an earlier, weekly publication called \"The Aeroplane\", founded in 1911.\n\n"}
{"id": "2218788", "url": "https://en.wikipedia.org/wiki?curid=2218788", "title": "Aerospaceplane", "text": "Aerospaceplane\n\nThe US Air Force's aerospaceplane project encompassed a variety of projects from 1958 until 1963 to study a fully reusable spaceplane. A variety of designs were studied during the lifetime of the project, including most of the early efforts on liquid air cycle engines (LACE) and even a nuclear-powered ramjet.\n\nThe effort was started largely due to the work of Weldon Worth at the Wright-Patterson AFB, who published a short work outlining a manned spaceplane. AF officials were interested enough to start SR-89774 (\"SR\" stands for \"study requirement\") for a reusable spaceplane in 1957. By 1959 this work had resulted in the Recoverable Orbital Launch System, or ROLS, based around a LACE engine, known at the time as a \"Liquid Air Collection System\", or \"LACES\".\n\nFurther work showed that more performance could be gained by extracting only the oxygen from the liquid air, a system they referred to as \"Air Collection and Enrichment System\", or \"ACES\". A contract to develop an ACES testbed was placed with Marquardt and General Dynamics, with Garrett AiResearch building the heat exchanger for cooling the air. The original ACES design was fairly complex; the air was first liquified in the heat exchanger cooled by liquid hydrogen fuel, then pumped into a low-pressure tank for short-term storage. From there it was then pumped into a high-pressure tank, where the oxygen was separated and the rest (mostly nitrogen) was dumped overboard. In late 1960 and early 1961 a 125 N demonstrator engine was being operated for up to 5 minutes at a time.\n\nIn early 1960 Air Force offered a development contract to build a spaceplane with a crew of three that could take off from any runway and fly directly into orbit and return. They wanted the design to be in operation in 1970 for a total development cost of only $5 billion. Boeing, Douglas, Convair, Lockheed, Goodyear, North American, and Republic all responded. Most of these designs ignored the ACES system and instead used a scramjet for power. The scramjet had first been outlined at about the same time as the original LACES design in a NASA paper of 1958, and many companies were highly interested in seeing it develop, perhaps none more than Marquardt, whose ramjet business was dwindling with the introduction of newer jet engines and who had already started work on the scramjet. Both Alexander Kartveli and Antonio Ferri were proponents of the scramjet approach. Ferri successfully demonstrated a scramjet producing net thrust in November 1964, eventually producing , about 80% of his goal.\n\nLater that year a review suggested that the basic concepts of the aerospaceplane were far too new for development of an operational system to begin. They pointed out that far too much was being spent on development of the aircraft, and not nearly enough on basic research. Moreover, the designs were all extremely sensitive to weight, and any increase (and there always is some) could result in all of the designs not working. In 1963 the Air Force changed their priorities in SR-651 and focused entirely on development of a variety of high-speed engines. Included were LACES and ACES engines, as well as scramjets, turboramjets and a \"normal\" (subsonic-combustion) ramjet with an intake suitable for use up to Mach 8. In October a further review concluded that the technology was simply too new for anyone to predict when any such aerospaceplane could ever be built, and funding was wound down in 1964.\n\n"}
{"id": "17401970", "url": "https://en.wikipedia.org/wiki?curid=17401970", "title": "Airplane mode", "text": "Airplane mode\n\nAirplane mode, aeroplane mode, flight mode, offline mode, or standalone mode is a setting available on smartphones and other portable computers that, when activated, suspends radio-frequency signal transmission by the device, thereby disabling Bluetooth, telephony, and Wi-Fi. GPS may or may not be disabled, because it does not involve transmitting radio waves.\n\nThe name comes from the prohibition by most of the airlines of using equipment transmitting radio-frequency signal while in flight; using airplane mode prevents devices from transmitting.\n\nWhen the \"aeroplane mode\" is activated, it disables all voice, text, telephone, and other signal-transmitting technologies such as Wi-Fi and Bluetooth. Wi-Fi and Bluetooth can be enabled separately even while the device is in airplane mode; this is acceptable on some aircraft. Receiving radio-frequency signals, as by radio receivers and satellite navigation services, is not inhibited. However, even receiving telephone calls and messages without responding would require the phone to transmit.\n\nIn a revised review in October 2013, the United States Federal Aviation Administration (FAA) made a recommendation on the use of electronic devices in \"airplane mode\"—cellular telephony is disabled, while Wi-Fi may be used if the carrier offers it. Short-range transmission such as Bluetooth is always permissible. The statement cites the common practice of aircraft operators whose aircraft can tolerate use of these personal electronic devices, but use may still be prohibited on some models of aircraft.\n\nWhile in airplane mode, most devices allow the user to continue to use their email client or other program to write text or E-mail messages which are saved in memory to send when airplane mode is disabled.\n\nAlthough it is not possible to make normal calls or send text in airplane mode, devices such as some Nokia smartphones allow the user to make calls to emergency services even in airplane mode, while others do not.\n\nAs a side-effect, airplane mode reduces power consumption and increases battery endurance by shutting down the device's transmitters and receivers.\n\n\n"}
{"id": "24034128", "url": "https://en.wikipedia.org/wiki?curid=24034128", "title": "Assistive listening device", "text": "Assistive listening device\n\nAn Assistive listening device (ALD) is used to improve hearing ability for people in a variety of situations where they are unable to distinguish speech in noise. Often in a noisy or crowded room it is almost impossible for an individual who is hard of hearing to distinguish one voice among many. The hard of hearing listener has to distinguish between background noise, noise between them and the speaker and then there will be the effect of room acoustics on the quality of sound reaching their ears. Hearing aids are able to amplify and process these sounds and improve the speech to noise ratio but if the sound is too distorted by the time it reaches the listener even the best hearing aids will struggle to unscramble the signal.\n\nA common usage is to aid people who are hard of hearing (HOH). by amplification and better sound to noise ratio (SNR). The ALD may be used to help HOH people hear televisions and other audio devices and also to help people hear speech through public address (PA) systems such as in synagogue or at a lecture.\n\nThe assistive listening device usually uses a microphone to capture an audio source near to its origin and broadcast it wirelessly over an FM (Frequency Modulation) transmission, IR (Infra Red) transmission, IL (Induction Loop) transmission, or other transmission method. The person who is listening may use an FM/IR/IL Receiver to tune into the signal and listen at his/her preferred volume. There are also other consumer ALD such as alarm clocks with bed shakers, amplified stethoscopes, baby monitors and flashing door bell indicators. Most FM assistive listening devices operate on seventeen channels between 72.1000 and 75.800 MHz.\n\nThe use of a wireless microphone placed next to the speaker helps to reduce the effects of reverberation from poor room acoustics, bypasses the sounds between the speaker and the listener, eliminates the background noise by the listener, and by using a directional microphone reduces the background noise by the speaker.\n"}
{"id": "18950033", "url": "https://en.wikipedia.org/wiki?curid=18950033", "title": "BITNET", "text": "BITNET\n\nBITNET was a co-operative U.S. university computer network founded in 1981 by Ira Fuchs at the City University of New York (CUNY) and Greydon Freeman, Inc. at Yale University. The first network link was between CUNY and Yale.\n\nThe name BITNET originally meant \"Because It's There Network\", but it eventually came to mean \"Because It's Time Network\". \n\nA college or university wishing to join BITNET was required to lease a data circuit (phone line) from a site to an existing BITNET node, buy modems for each end of the data circuit, sending one to the connecting point site, and allow other institutions to connect to its site free of charge.\n\nBitnet's NJE (Network Job Entry) network protocols, called RSCS, were used for the huge IBM internal network known as VNET. BITNET links originally ran at 9600 baud. The BITNET protocols were eventually ported to non-IBM mainframe operating systems, and became particularly widely implemented under VAX/VMS, in addition to DECnet.\n\nBITNET featured email and LISTSERV software, but predated the World Wide Web, the common use of FTP, and Gopher. Gateways for the lists made them available on Usenet. BITNET also supported interactive transmission of files and messages to other users. A gateway service called TRICKLE enabled users to request files from Internet FTP servers in 64 Kb UUencoded chunks. The \"Interchat Relay Network\", popularly known as Bitnet Relay, was the network's instant messaging feature.\n\nBITNET differed from the Internet in that it was a point-to-point \"store and forward\" network. That is, email messages and files were transmitted in their entirety from one server to the next until reaching their destination. From this perspective, BITNET was more like UUCPNET.\n\nBITNET’s first electronic magazine, VM/COM, began as a University of Maine newsletter and circulated broadly in early 1984. Two email newsletters that began as Bitnet newsletters in the fall of 1987 are known to still be transmitting. They are the Electronic Air and SCUP Email News (formerly SCUP Bitnet News).\n\nBITNET's eligibility requirements limited exchange with commercial entities, including IBM itself, which made technical assistance and bug fixes difficult. This became a particular problem when trying to communicate on heterogeneous networks with graphical workstation vendors such as Silicon Graphics.\n\nAt its zenith around 1991, BITNET extended to almost 500 organizations and 3,000 nodes, all educational institutions. It spanned North America (in Canada it was known as NetNorth), Europe (as EARN), Israel (as ISRAEARN), India (VIDYANET) and some Persian Gulf states (as GulfNet). BITNET was also very popular in other parts of the world, especially in South America, where about 200 nodes were implemented and heavily used in the late 1980s and early 1990s. With the rapid growth of TCP/IP systems and the Internet in the early 1990s, and the rapid abandonment of the base IBM mainframe platform for academic purposes, BITNET's popularity and use diminished quickly.\n\nIn 1984, a text-based BITNET game called MAD became the first global Multi-User Dungeon (MUD). Players connected from the United States, Europe or Israel to a single server running in France.\n\nIn 1996, CREN ended their support for BITNET. The individual nodes were free to keep their phone lines up as long as they wished, but as nodes dropped out, the network splintered into parts that were inaccessible from each other. As of 2007, BITNET has essentially ceased operation. However, a successor, BITNET II, which transmits information via the Internet using BITNET protocols, still has some users.\n\n\n"}
{"id": "34883656", "url": "https://en.wikipedia.org/wiki?curid=34883656", "title": "Bell mouth", "text": "Bell mouth\n\nIn building services engineering and HVAC, a bell mouth is a tapered expanding or reducing opening in the end of a ventilation duct, so named because the taper can resemble that of a bell shape. They are primarily designed and used for return air or extract air purposes within building ventilation systems, more commonly located within ceiling voids or other similar plenum. The bellmouth cross-sectional area is normally double that of the duct area so that the air velocity entering the bellmouth is low (to reduce noise, turbulence and pressure drop), and gradually increases to the normal design velocity of the ductwork. The angle of the bellmouth is normally tapered at about 45° as a balance between keeping the bellmouth short without causing too much turbulence or excessive pressure drop. Bellmouths can be manufactured to suit either circular or rectangular ductwork sections.\n\nThe bell-mouth shape allows the maximum amount of air to be drawn into the duct with minimum loss.\nA bell-mouth inlet duct is a form of convergent inlet air duct used to direct air into the inlet of a gas turbine engine. The area of a convergent duct gets smaller as the air flows into the engine. A bell-mouth inlet duct is extremely efficient and is used where there is little ram pressure available to force the air into the engine. Bell-mouth ducts are used in engine test cells and on engines installed in helicopters.\n\n"}
{"id": "551450", "url": "https://en.wikipedia.org/wiki?curid=551450", "title": "Bioship", "text": "Bioship\n\nA bioship is a type of spacecraft or starship described in science fiction. Bioships differ from other types of spacecraft in that they are composed, either predominantly or totally, of biological components, rather than being constructed from manufactured materials. Because of this, they nearly always have a distinctively organic look.\n\nBioships are usually quite powerful, and can often regenerate or heal damaged parts. Some bioships are intelligent or sentient, and some are considered to be lifeforms. Like most organic beings, many bioships contain large amounts of \"scaffolding\" materials to keep their shape, such as the xylem in trees or bone and chitin in animals.\n\nIn the science fiction short story \"Specialist\" by Robert Sheckley, published in 1953 in \"Galaxy\" magazine, it is revealed that many galactic races are actually capable of symbiotic cooperation to become bioships, with each race forming a different part. Earth, apparently, is one of the planets inhabited by creatures that are supposed to function as FTL drives (Pushers), and, it is stated that all the conflicts and discontent of humanity are due to the fact that, while they have matured, they have nowhere to apply their true purpose. This story is perhaps the first mention of a bioship in science fiction.\n\nVolume 322 of the German Perry Rhodan magazine series, first published in November 1967, marks another very early appearance of the bioship concept in science fiction. The Dolans are powerful bioengineered combat spaceships that are grown from the same synthetic genetic material as their extraterrestrial commanders. Different types of bioships are a recurrent feature in later stages of the Perry Rhodan universe.\n\n\"The Night's Dawn Trilogy\": the Edenist Voidhawk and Mercenary Blackhawk are both advanced bioships (the latter being a genetic tailoring for combat of the former). Both types employ mental bonding to the captain. In the case of Voidhawks this is done by both the craft and captain gestating together and maintaining mental contact during their formative years. Blackhawks however are purchased as eggs and are bonded to the buyer who will become captain when the Blackhawk matures.\n\nIn the first novel of Julian May's Pliocene series, \"The Many-Colored Land\" (1982), the backstory of two races of alien refugees living in the Earth's Pliocene epoch describes their hard landing in a bioship. The bioship was emotionally bonded to one of the aliens (the \"shipwife\") and sacrificed its own life to safely deliver its passengers to the planet surface.\n\n"}
{"id": "163261", "url": "https://en.wikipedia.org/wiki?curid=163261", "title": "Boeing YAL-1", "text": "Boeing YAL-1\n\nThe Boeing YAL-1 Airborne Laser Testbed (formerly Airborne Laser) weapons system was a megawatt-class chemical oxygen iodine laser (COIL) mounted inside a modified Boeing 747-400F. It was primarily designed as a missile defense system to destroy tactical ballistic missiles (TBMs) while in boost phase. The aircraft was designated YAL-1A in 2004 by the U.S. Department of Defense.\n\nThe YAL-1 with a low-power laser was test-fired in flight at an airborne target in 2007. A high-energy laser was used to intercept a test target in January 2010, and the following month, successfully destroyed two test missiles. Funding for the program was cut in 2010 and the program was canceled in December 2011. It made its final flight on February 14, 2012 to Davis–Monthan Air Force Base in Tucson, Arizona to be kept in storage at the \"Boneyard\" by the 309th Aerospace Maintenance and Regeneration Group. It was ultimately scrapped in September 2014 after all usable parts were removed.\n\nThe Airborne Laser Laboratory was a less-powerful prototype installed in a Boeing NKC-135A. It shot down several missiles in tests conducted in the 1980s.\n\nThe Airborne Laser program was initiated by the US Air Force in 1996 with the awarding of a product definition risk reduction contract to Boeing's ABL team. In 2001, the program was transferred to the MDA and converted to an acquisition program.\n\nThe development of the system was being accomplished by a team of contractors. Boeing Defense, Space & Security provides the aircraft, the management team and the systems integration processes. Northrop Grumman was supplying the COIL, and Lockheed Martin was supplying the nose turret and the fire control system.\n\nIn 2001, a retired Air India 747-200 was acquired by the air force, and trucked without its wings from the Mojave Airport to Edwards Air Force Base where the airframe was incorporated into the System Integration Laboratory (SIL) building at Edwards' Birk Flight Test Center, to be used to fit check and test the various components. The SIL was built primarily to test the COIL at a simulated operational altitude, and during that phase of the program, the laser was operated over 50 times, achieving lasing durations representative of actual operational engagements. These tests fully qualified the system so that it could be integrated into the actual aircraft. Following the completion of the tests, the laboratory was dismantled, and the 747-200 fuselage was removed.\n\nBoeing completed initial modifications to a new 747-400F off the production line in 2002, culminating in its first flight on July 18, 2002 from Boeing's Wichita, Kansas facility. Ground testing of the COIL resulted in its successful firing in 2004. The YAL-1 was assigned to the 417th Flight Test Squadron Airborne Laser Combined Test Force at Edwards AFB.\n\nBesides the COIL, the system also includes two kilowatt-class Target Illuminator Lasers for target tracking. On March 15, 2007, the YAL-1 successfully fired this laser in flight, hitting its target. The target was an NC-135E \"Big Crow\" test aircraft that has been specially modified with a \"signboard\" target on its fuselage. The test validated the system's ability to track an airborne target and measure and compensate for atmospheric distortion.\n\nThe next phase in the test program involved the \"surrogate high-energy laser\" (SHEL), a stand-in for the COIL, and demonstrated the transition from target illumination to simulated weapons firing. The COIL system was installed in the aircraft and undergoing ground testing by July 2008.\n\nIn an April 6, 2009 press conference, the Secretary of Defense Robert Gates recommended the cancellation of the planned second ABL aircraft and said that the program should return to a Research and Development effort. \"The ABL program has significant affordability and technology problems and the program's proposed operational role is highly questionable,\" Gates said in making the recommendation.\n\nThere was a test launch off the California coast on June 6, 2009. At that time it was anticipated that the new Airborne Laser Aircraft could be ready for operation by 2013 after a successful test. On August 13, 2009 the first in-flight test of the YAL-1 culminated with a successful firing of the SHEL at an instrumented test missile.\n\nThe U.S. Missile Defense Agency (MDA) on August 18, 2009 successfully fired the high-energy laser aboard the aircraft in flight for the first time. The YAL-1 took off from Edwards Air Force Base and fired its high-energy laser while flying over the California High Desert. The laser was fired into an onboard calorimeter, which captured the beam and measured its power.\n\nIn January 2010, the high-energy laser was used in-flight to intercept, although not destroy, a test \"Missile Alternative Range Target Instrument\" (MARTI) in the boost phase of flight. On February 11, 2010 in a test at Point Mugu Naval Air Warfare Center-Weapons Division Sea Range off the central California coast, the system successfully destroyed a liquid-fuel boosting ballistic missile. Less than an hour after that first missile had been destroyed, a second missile—a solid-fuel design—had, as announced by the MDA, been \"successfully engaged\", but not destroyed, and that all test criteria had been met. The MDA announcement also noted that ABL had destroyed an identical solid-fuel missile in flight eight days earlier. This test was the first time that a directed-energy system destroyed a ballistic missile in any phase of flight. It was later reported that the first February 11 engagement required 50% less dwell time than expected to destroy the missile, the second engagement on the solid-fuel missile, less than an hour later, had to be cut short before it could be destroyed because of a \"beam misalignment\" problem.\n\nSecretary of Defense Gates summarized fundamental concerns with the practicality of the program concept: \"I don't know anybody at the Department of Defense, Mr. Tiahrt, who thinks that this program should, or would, ever be operationally deployed. The reality is that you would need a laser something like 20 to 30 times more powerful than the chemical laser in the plane right now to be able to get any distance from the launch site to fire...So, right now the ABL would have to orbit inside the borders of Iran in order to be able to try and use its laser to shoot down that missile in the boost phase. And if you were to operationalize this you would be looking at 10 to 20 747s, at a billion and a half dollars apiece, and $100 million a year to operate. And there's nobody in uniform that I know who believes that this is a workable concept.\"\n\nThe Air Force did not request further funds for the Airborne Laser for 2010; Air Force Chief of Staff Schwartz has said that the system \"does not reflect something that is operationally viable\".\n\nIn December 2011, it was reported that the project was to be ended after 16 years of development and a cost of over US$5 billion. While in its current form, a relatively low power laser mounted on an unprotected airliner may not be a practical or defensible weapon, the YAL-1 testbed is considered to have proven that air mounted energy weapons with increased range and power could be another viable way of destroying otherwise very difficult to intercept sub-orbital ballistic missiles and rockets. On 12 February 2012, the YAL-1 flew its final flight and landed at Davis-Monthan AFB, Arizona, where it was placed in storage at the AMARG until it was ultimately scrapped in September 2014 after all usable parts were removed.\n\nAs of 2013, studies were underway to apply the lessons of the YAL-1 by mounting laser anti-missile defenses on unmanned combat aerial vehicles that could fly above the altitude limits of the converted jetliner.\n\nBy 2015, the Missile Defense Agency had started efforts to deploy a laser on a high-altitude UAV. Rather than a manned jetliner containing chemical fuels flying at , firing a megawatt laser from a range of \"tens of kilometers\" at a boost-phase missile, the new concept envisioned an unmanned aircraft carrying an electric laser flying at , firing the same power level at targets potentially up to \"hundreds of kilometers\" away for survivability against air defenses. While the ABL's laser required to generate one kW, the MDA wanted to reduce that to per kW, totaling for a megawatt. Unlike the ABL, which required its crew to rest and chemical fuel to be reloaded, an electric laser would need only power generating from fuel to fire, so a UAV with in-flight refueling could have near-inexhaustible endurance and armament. A \"low-power demonstrator\" has been planned to fly sometime in or around 2021.\n\nBoeing's Jim Albaugh sees it as a pretty good deterrent for the North Korean missile program if it was operational.\n\nThe heart of the system was the COIL, comprising six interconnected modules, each as large as an SUV. Each module weighed about 6,500 pounds (3,000 kg). When fired, the laser produced enough energy in a five-second burst to power a typical American household for more than an hour.\n\nThe ABL was designed for use against tactical ballistic missiles (TBMs). These have a shorter range and fly more slowly than ICBMs. The MDA has recently suggested the ABL might be used against ICBMs during their boost phase. This could require much longer flights to get in position, and might not be possible without flying over hostile territory. Liquid-fueled ICBMs, which have thinner skins, and remain in boost phase longer than TBMs, might be easier to destroy.\n\nIf the ABL had achieved its design goals, it could have destroyed liquid-fueled ICBMs up to 600 km away. Tougher solid-fueled ICBM destruction range would likely have been limited to 300 km, too short to be useful in many scenarios, according to a 2003 report by the American Physical Society on National Missile Defense.\n\nThe ABL system used infrared sensors for initial missile detection. After initial detection, three low power tracking lasers calculated missile course, speed, aimpoint, and air turbulence. Air turbulence deflects and distorts lasers. The ABL adaptive optics use the turbulence measurement to compensate for atmospheric errors. The main laser, located in a turret on the aircraft nose, could be fired for 3 to 5 seconds, causing the missile to break up in flight near the launch area. The ABL was not designed to intercept TBMs in the terminal, or descending, flight phase. Thus, the ABL would have had been within a few hundred kilometers of the missile launch point. All of this would have occurred in approximately 8 to 12 seconds.\n\nThe ABL did not burn through or disintegrate its target. It heated the missile skin, weakening it, causing failure from high speed flight stress. The laser used chemical fuel similar to rocket propellant to generate the high laser power. Plans called for each 747 to carry enough laser fuel for about 20 shots, or perhaps as many as 40 low-power shots against fragile TBMs. To refuel the laser, YAL-1 would have to land. The aircraft itself could have been refueled in flight, which would have enabled it to stay aloft for long periods. Preliminary operational plans called for the ABL to be escorted by fighters and possibly electronic warfare aircraft. The ABL aircraft would likely have had to orbit near potential launch sites (located in hostile countries) for long periods, flying a figure-eight pattern that allows the aircraft to keep the laser aimed toward the missiles.\n\nIn theory, an airborne laser could be used against hostile fighter aircraft, cruise missiles, or even low-earth-orbit satellites (see anti-satellite weapon). However, the YAL-1 infrared target acquisition system was designed to detect the hot exhaust of TBMs in boost phase. Satellites and other aircraft have a much lower heat signature, making them more difficult to detect. Aside from the difficulty of acquiring and tracking a different kind of target, ground targets such as armored vehicles and possibly even aircraft are not fragile enough to be damaged by a megawatt-class laser.\n\nAn analysis by the Union of Concerned Scientists discusses potential airborne laser use against low earth orbit satellites. Another program, the Advanced Tactical Laser, envisions air-to-ground use of a megawatt-class laser mounted on an aircraft better suited for low altitude flight.\n\n\n\n"}
{"id": "7954287", "url": "https://en.wikipedia.org/wiki?curid=7954287", "title": "Butterworm", "text": "Butterworm\n\nThe Chilean moth (\"Chilecomadia moorei\") is a moth of the Cossidae family. The butterworm is the larval form and is commonly used as fishing bait in South America.\n\nButterworms, like mealworms, are used as food for insectivore pets, such as geckos and other reptiles, as their scent and bright color help attract the more stubborn eaters. They are also called tebo worms or trevo worms, and are high in fat and calcium. They are difficult to breed in captivity, and most are imported directly from Chile. They are usually irradiated to kill bacteria and prevent pupation as the moth is an invasive species.\n"}
{"id": "306686", "url": "https://en.wikipedia.org/wiki?curid=306686", "title": "Detonation", "text": "Detonation\n\nDetonation () is a type of combustion involving a supersonic exothermic front accelerating through a medium that eventually drives a shock front propagating directly in front of it. Detonations occur in both conventional solid and liquid explosives, as well as in reactive gases. The velocity of detonation in solid and liquid explosives is much higher than that in gaseous ones, which allows the wave system to be observed with greater detail (higher resolution).\n\nA very wide variety of fuels may occur as gases, droplet fogs, or dust suspensions. Oxidants include halogens, ozone, hydrogen peroxide and oxides of nitrogen. Gaseous detonations are often associated with a mixture of fuel and oxidant in a composition somewhat below conventional flammability ratios. They happen most often in confined systems, but they sometimes occur in large vapor clouds. Other materials, such as acetylene, ozone, and hydrogen peroxide are detonable in the absence of dioxygen.\n\nDetonation was discovered in 1881 by two pairs of French scientists Marcellin Berthelot and P. Vieille and Ernest-François Mallard and Henry Louis Le Chatelier. The mathematical predictions of propagation were carried out first by David Chapman in 1899 and by Émile Jouguet in 1905, 1906 and 1917. The next advance in understanding detonation was made by Zel'dovich, von Neumann, and W. Doering in the early 1940s.\n\nThe simplest theory to predict the behaviour of detonations in gases is known as Chapman-Jouguet (CJ) theory, developed around the turn of the 20th century. This theory, described by a relatively simple set of algebraic equations, models the detonation as a propagating shock wave accompanied by exothermic heat release. Such a theory confines the chemistry and diffusive transport processes to an infinitesimally thin zone.\n\nA more complex theory was advanced during World War II independently by Zel'dovich, von Neumann, and W. Doering. This theory, now known as ZND theory, admits finite-rate chemical reactions and thus describes a detonation as an infinitesimally thin shock wave followed by a zone of exothermic chemical reaction. With a reference frame of a stationary shock, the following flow is subsonic, so that an acoustic reaction zone follows immediately behind the lead front, the Chapman-Jouguet condition.\nThere is also some evidence that the reaction zone is semi-metallic in some explosives.\n\nBoth theories describe one-dimensional and steady wave fronts. However, in the 1960s, experiments revealed that gas-phase detonations were most often characterized by unsteady, three-dimensional structures, which can only in an averaged sense be predicted by one-dimensional steady theories. Indeed, such waves are quenched as their structure is destroyed. The Wood-Kirkwood detonation theory can correct for some of these limitations.\n\nExperimental studies have revealed some of the conditions needed for the propagation of such fronts. In confinement, the range of composition of mixes of fuel and oxidant and self-decomposing substances with inerts are slightly below the flammability limits and for spherically expanding fronts well below them. The influence of increasing the concentration of diluent on expanding individual detonation cells has been elegantly demonstrated. Similarly their size grows as the initial pressure falls. Since cell widths must be matched with minimum dimension of containment, any wave overdriven by the initiator will be quenched.\n\nMathematical modeling has steadily advanced to predicting the complex flow fields behind shocks inducing reactions. To date, none has adequately described how structure is formed and sustained behind unconfined waves.\n\nWhen used in explosive devices, the main cause of damage from a detonation is the supersonic blast front (a powerful shock wave) in the surrounding area. This is a significant distinction from deflagrations where the exothermic wave is subsonic and maximum pressures are at most one eighth as great. Therefore, detonation is most often used for explosives and the acceleration of projectiles. However, detonation waves may also be used for less destructive purposes, including deposition of coatings to a surface or cleaning of equipment (e.g. slag removal) and even explosively welding together metals that would otherwise fail to fuse. Pulse detonation engines use the detonation wave for aerospace propulsion. The first flight of an aircraft powered by a pulse detonation engine took place at the Mojave Air & Space Port on January 31, 2008.\n\nUnintentional detonation when deflagration is desired is a problem in some devices. In internal combustion engines it is called engine knocking or pinging or pinking, and it causes a loss of power, excessive heating, and eventual engine failure in some cases. In firearms, it may cause catastrophic and potentially lethal failure.\n\nIn classical Latin, \"detonare\" means \"to stop thundering\", as in weather. The modern meaning developed later.\n\n\n"}
{"id": "5289750", "url": "https://en.wikipedia.org/wiki?curid=5289750", "title": "Diode-or circuit", "text": "Diode-or circuit\n\nA diode-OR circuit is used in electronics to isolate two or more voltage sources. There are two typical implementations:\n\nWhen a DC supply voltage needs to be generated from one of a number of different sources, for example when terminating a parallel SCSI bus, a very simple circuit like this can be used:\n\nIn digital electronics a diode-OR circuit is used to derive a simple Boolean logic function. This kind of circuit was once very common in diode–transistor logic but has been largely replaced by CMOS in modern electronics:\n"}
{"id": "8736181", "url": "https://en.wikipedia.org/wiki?curid=8736181", "title": "Electronic nose", "text": "Electronic nose\n\nAn electronic nose is a device intended to detect odors or flavors.\n\nOver the last decades, \"electronic sensing\" or \"e-sensing\" technologies have undergone important developments from a technical and commercial point of view. The expression \"electronic sensing\" refers to the capability of reproducing human senses using sensor arrays and pattern recognition systems.\nSince 1982, research has been conducted to develop technologies, commonly referred to as electronic noses, that could detect and recognize odors and flavors. The stages of the recognition process are similar to human olfaction and are performed for identification, comparison, quantification and other applications, including data storage and retrieval. However, hedonic evaluation is a specificity of the human nose given that it is related to subjective opinions. These devices have undergone much development and are now used to fulfill industrial needs.\n\nIn all industries, odor assessment is usually performed by human sensory analysis, by chemosensors, or by gas chromatography. The latter technique gives information about volatile organic compounds but the correlation between analytical results and meme odor perception is not direct due to potential interactions between several odorous components.\n\nIn the Wasp Hound odor detector, the mechanical element is a video camera and the biological element is five parasitic wasps who have been conditioned to swarm in response to the presence of a specific chemical.\n\nScientist Alexander Graham Bell popularized the notion that it was difficult to measure a smell, and in 1914 said the following:\n\nIn the decades since Bell made this observation, no such science of odor materialised, and it was not until the 1950s and beyond that any real progress was made.\n\nThe electronic nose was developed in order to mimic human olfaction that functions as a non-separative mechanism: i.e. an odor / flavor is perceived as a global fingerprint.\nEssentially the instrument consists of head space sampling, sensor array, and pattern recognition modules, to generate signal pattern that are used for characterizing odors.\n\nElectronic noses include three major parts: a sample delivery system, a detection system, a computing system.\n\nThe sample delivery system enables the generation of the headspace (volatile compounds) of a sample, which is the fraction analyzed. The system then injects this headspace into the detection system of the electronic nose. The sample delivery system is essential to guarantee constant operating conditions.\n\nThe detection system, which consists of a sensor set, is the \"reactive\" part of the instrument. When in contact with volatile compounds, the sensors react, which means they experience a change of electrical properties.\n\nIn most electronic noses, each sensor is sensitive to all volatile molecules but each in their specific way. However, in bio-electronic noses, receptor proteins which respond to specific odor molecules are used. Most electronic noses use sensor arrays that react to volatile compounds on contact: the adsorption of volatile compounds on the sensor surface causes a physical change of the sensor. A specific response is recorded by the electronic interface transforming the signal into a digital value. Recorded data are then computed based on statistical models.\n\nBio-electronic noses use olfactory receptors - proteins cloned from biological organisms, e.g. humans, that bind to specific odor molecules. One group has developed a bio-electronic nose that mimics the signaling systems used by the human nose to perceive odors at a very high sensitivity: femtomolar concentrations.\n\nThe more commonly used sensors for electronic noses include \n\nSome devices combine multiple sensor types in a single device, for example polymer coated QCMs. The independent information leads to vastly more sensitive and efficient devices.\n\nIn recent years, other types of electronic noses have been developed that utilize mass spectrometry or ultra-fast gas chromatography as a detection system.\n\nThe computing system works to combine the responses of all of the sensors, which represents the input for the data treatment. This part of the instrument performs global fingerprint analysis and provides results and representations that can be easily interpreted. Moreover, the electronic nose results can be correlated to those obtained from other techniques (sensory panel, GC, GC/MS). Many of the data interpretation systems are used for the analysis of results. These systems include artificial neural network (ANN), fuzzy logic, pattern recognition modules, etc.\n\nAs a first step, an electronic nose needs to be trained with qualified samples so as to build a database of reference. Then the instrument can recognize new samples by comparing a volatile compound's fingerprint to those contained in its database. Thus they can perform qualitative or quantitative analysis. This however may also provide a problem as many odors are made up of multiple different molecules, which may be wrongly interpreted by the device as it will register them as different compounds, resulting in incorrect or inaccurate results depending on the primary function of a nose.\n\nElectronic nose instruments are used by research and development laboratories, quality control laboratories and process & production departments for various purposes:\n\n\n\n\n\n\nVarious application notes describe analysis in areas such as flavor and fragrance, food and beverage, packaging, pharmaceutical, cosmetic and perfumes, and chemical companies. More recently they can also address public concerns in terms of olfactive nuisance monitoring with networks of on-field devices. Since emission rates on a site can be extremely variable for some sources, the electronic nose can provide a tool to track fluctuations and trends and assess the situation in real time. It improves understanding of critical sources, leading to pro-active odor management. Real time modeling will present the current situation, allowing the operator to understand which periods and conditions are putting the facility at risk. Also, existing commercial systems can be programmed to have active alerts based on set points (odor concentration modeled at receptors/alert points or odor concentration at a nose/source) to initiate appropriate actions.\n\nThe Cyranose 320 is a handheld \"electronic nose\" developed by Cyrano Sciences of Pasadena, California in 2000. Cyrano Sciences was founded in 1997, 9 years after the concept of an \"electronic nose\" based on using multiple semi-selective sensors combined with electronic computation was first proposed by Gardner and Bartlett. The Cyranose 320 is based on sensor research performed by Professor Nathan Lewis of the California Institute of Technology.\nApplications researched using the Cyranose 320 include the detection of COPD, and other medical conditions as well as industrial applications generally related to quality control or contamination detection.\n\n\n"}
{"id": "37463545", "url": "https://en.wikipedia.org/wiki?curid=37463545", "title": "Energy-shaping control", "text": "Energy-shaping control\n\nEnergy-shaping control considers the plant and its controller as energy-transformation devices. The control strategy is formulated in terms of interconnection (in a power-preserving manner) in order to achieve a desired behavior.\n\n\n"}
{"id": "23648658", "url": "https://en.wikipedia.org/wiki?curid=23648658", "title": "Environmental Impact of the Big Cypress Swamp Jetport", "text": "Environmental Impact of the Big Cypress Swamp Jetport\n\nThe \"Environmental Impact of the Big Cypress Swamp Jetport\", unofficially known as the \"Leopold Report\" or the \"Leopold-Marshall Report\", was a report authored by hydrologist Luna Leopold of the United States Geological Service for the Department of the Interior and officially released on September 17, 1969. Arthur R. Marshall, formerly of the United States Fish and Wildlife Service, helped draft the report. It is considered the first ecological impact report in the state of Florida.\n\nOn June 2, 1969, Walter Hickel, Secretary of the Interior in the Nixon administration, created a select committee to conduct an inquiry into a proposed jetport for supersonic transport in what is now known as the Big Cypress National Preserve on the border of the Everglades National Park in Florida. The proposed Everglades Jetport would have had six runways for supersonic aircraft, making it the largest airport in the world at the time. Russell E. Train, then undersecretary of the Department of the Interior, appointed Luna Leopold of the USGS to direct the environmental impact assessment. Before the report was published, Wisconsin Senator Gaylord Nelson leaked key conclusions from the report in defense of the Everglades: \"Either we stop the jetport at the present site, or we publicly admit that we are going to destroy the park.\"\n\nWhen the report was finally released, Leopold began by stating his strong opposition to the plan: \"Development of the proposed jetport and its attendant facilities will lead to land drainage and development for agriculture, transportation, and services in the Big Cypress Swamp which will inexorably destroy the south Florida ecosystem and thus the Everglades National Park.\" A report from the National Academy of Sciences was subsequently published the next day supporting the findings of the Leopold report.\n\nBecause the jetport did not meet the necessary standards, Walter Hickel opposed it. Hickel successfully defeated the construction of the jetport by preventing it from being listed by the Department of Transportation for funding under an airport development program. Although construction of only one runway was completed, the remains of the Everglades Jetport was later opened as the Dade-Collier Training and Transition Airport and is sometimes used as an aviation training facility.\n"}
{"id": "58419997", "url": "https://en.wikipedia.org/wiki?curid=58419997", "title": "Gambia Film Unit", "text": "Gambia Film Unit\n\nGambia Film Unit was an agency of the Gambian government that produced short films and showed them in remote parts of The Gambia. It was founded in 1967 and led by Ebrima Sagnia from 1972. It was merged into Gam TV, a predecessor to the Gambia Radio & Television Service (GRTS), in 1995.\n\nThe Gambia Film Unit was founded in 1967 as a cinema projection unit in the Department of Agriculture. Oxfam donated a camera and projection equipment, and the Department acquired a Land Rover with a built-in power generator. The unit produced short films on various topics that it then showed to people in villages. One film, 'Two Years of Progress', highlighted the government's achievements in the two years following independence. In 1969, the unit was transferred to the control of the Department of Information. Following this transfer, it began producing films on topics such as health, nutrition, the 1973 census, and the 1978 state visit of the President of Nigeria.\n\nIn 1969, Ebrima Sagnia was sent on a production course at the London Film School, returning in 1972 to head the unit. The unit acted in the place of a television service in remote parts of The Gambia. Early production was hampered by a lack of laboratory facilities and editing equipment, and the lack of trained Gambians in this field. Due to these conditions, the government requested assistance from the UN Development Programme for the expansion of the unit. An expert from the UNDP began working with the unit in July 1972. In 1975, a black and white film laboratory and the training of technicians were also provided for. In 1981, the unit was able to capture footage of the coup d'etat attempt that was used by news media across the world.\n\nThe unit held an important role in Gambian media, alongside Radio Gambia and the Gambia News Bulletin. The unit became part of Gam TV, The Gambia's first TV station, which was shortlived before it became itself part of Gambia Radio & Television Service (GRTS) in 1995. Sagnia and others involved in the unit have since been described as \"pioneers\" of Gambian film by Gambian filmmaker Prince Bubacarr Aminata Sankanu.\n"}
{"id": "54136572", "url": "https://en.wikipedia.org/wiki?curid=54136572", "title": "Glass breaker", "text": "Glass breaker\n\nA glass breaker is a small hand tool designed to aid in the emergency extrication of occupants from a vehicle. Most glass breakers contain a sharp pointed metal tip for breaking tempered glass and a sharp knife for slicing through seatbelts. Glass breakers are also built into other tools, such as flashlights or multitools. Unlike bus mallets, glass breakers are smaller and generally privately owned.\n"}
{"id": "37811523", "url": "https://en.wikipedia.org/wiki?curid=37811523", "title": "Glen Davis Shale Oil Works", "text": "Glen Davis Shale Oil Works\n\nThe Glen Davis Shale Oil Works was a shale oil extraction plant in Glen Davis, New South Wales, Australia which operated from 1940 until 1952 and was the last oil-shale operation in Australia until the Stuart Oil Shale Project in the late 1990s. For the period of 1865–1952, it provided one fifth of the shale oil produced in Australia.\n\nThe shale oil industry at Glen Davis was developed for production of shale oil for national defence purposes, although the basis of this project was the 1934 report of the Newnes Investigation Committee, which looked at ways to decrease the number of unemployed miners in the region. The project was operated by National Oil Proprietary Pty. Ltd., a company created as a special purpose vehicle by G. F. Davis of Davis Gelantine. A public notice in the \"Commonwealth of Australia Gazette\", on 28 May 1936, invited offers for developing the oil industry in the Glen Davis area. The company was established by private interests with financial support from the Commonwealth of Australia and New South Wales governments.\n\nConstruction of the shale oil works started in 1938 and the plant was commissioned in 1939, with operations starting on 3 January 1940. During World War II, shale oil produced by the Glen Davis Shale Oil Works was considered to be a strategic resource. In 1941, of shale oil were produced.\n\nIn 1942, under the National Security Act, the government took over the company and in August 1949 acquired the private shareholdings. After expansion in 1946, a shortage of mined shale reduced its output. In December 1950, it was decided to end the project. In 1951, the last full year before closure, it produced only . Government funding ceased in 1952, and Glen Davis was closed on 30 May. Although some syndicates had an interest to the facility, no deal was concluded. The closure caused a strike by miners, which ended after 26 days without success when the Australian Council of Trade Unions decided not to support the strike.\n\nThe mining and shale oil extraction complex was located in Gindantherie, Goolloinboin, Barton, Glen Alice, and Capertee parishes of Cook and Hunter counties. The plant used room-and-pillar mining techniques, and employed 170 miners. The shale was crushed by a Pennsylvania single-roll type crusher and was then conveyed into the retorts.\n\nThe company planned to use two tunnel ovens, each with a daily capacity of 336 tons, designed by \"AS Franz Krull\" of Estonia and \"Lurgi AG\" of Germany, similar to those used by some oil shale industries in Estonia. However, for economic reasons, it was decided in March 1939 to instead use a technology that had been employed in the closed Newnes Shale Oil Works, and 64 modified Pumpherston retorts were transferred from Newnes. Other equipment was imported from the United States, including a second bench of 44 retorts added in 1946. Retorts were heated by coal obtained from nearby coal mines.\n\nThe oil was treated to create motor oil and was then transported by a pipeline to storage tanks at Newnes Junction.\n"}
{"id": "42417769", "url": "https://en.wikipedia.org/wiki?curid=42417769", "title": "Grant Prideco", "text": "Grant Prideco\n\nNOV Grant Prideco is a supplier of drill pipe and drill stem accessories. Its head office is situated in Houston, Texas. The company is among the 1000 top companies of Fortune magazine in 2012.\n"}
{"id": "52997550", "url": "https://en.wikipedia.org/wiki?curid=52997550", "title": "Hemp hurds", "text": "Hemp hurds\n\nHemp hurds, also known as shives or hemp wood, are an agricultural product made from hemp. Hurds consist of \"the woody inner portion of the hemp stalk, broken into pieces and separated from the fiber in the processes of breaking and scutching\" and \"correspond to the shives in flax, but are coarser and usually softer in texture\". Hurds have traditionally been a by-product of fiber production.\n\nDecortication separates bast fiber from the hurd. The fine bast fiber is used to make textiles. Decortication is accomplished by manually crushing, a labor-intensive process, or by a hemp decorticator machine.\n\nHurds can be used as stock in papermaking; particleboard, hempcrete, and other construction composites; or bedding for animals, particularly in horse stalls, which is the most common use (by weight) in Europe.\n\nHemp shiv is a renewable organic raw material used for the production of insulation boards.\nThe use of biomaterials for insulation boards in walls is attracting increasing interest, but wider acceptance in the construction industry has so far been slowed-down by adverse material properties. Because such materials consist mainly from cellulose, they are flammable and their hydrophilic surface leads to high water uptake, which can lead to mold or rot.\nTherefore, engineering studies have investigated whether a coating with a water-repellent thin film provides the desired benefits. For the hydrophobic coating with silica particles, a colloidal sol-gel dispersion was successfully synthesized by the Stöber process, characterized and deposited on hemp shiv. These samples passed a 72-hour test in the humidity chamber without loss of their hydrophobic property and no signs of mold growth.\n\nWhen the hemp shiv was coated several times with functionalized silica particles, uniform and complete coverage of the surface was achieved. Such a treatment provided the hemp shiv with a comprehensive water repellency: The hydrophilic character of the untreated hemp shiv was modified to durably hydrophobic, once the bio-materials were coated with functionalised silica particles. Mould growth was delayed when exposed to humidity, whilst the liquid water repellent property of the treated hemp shiv was maintained despite the humid conditions. The treatment developed in this study could be a viable solution for use on bio-materials that need to repel liquid water, whilst preserving the integrity of the insulation panel in common environmental conditions. There are of course still further tests to be carried out before adopting this treatment in the construction sector, such as additional moisture tests, mechanical tests, biodegradation tests, etc. However, the promising results described here give a good first assessment of feasibility. Before being fully adopted in the construction sector, these treatments and materials must be fully tested in accordance with the building codes. In addition, with the recent recent use of biomaterials, new standards and test methods for these specific materials should be developed.\n\n"}
{"id": "1569276", "url": "https://en.wikipedia.org/wiki?curid=1569276", "title": "High-end audio", "text": "High-end audio\n\nHigh-end audio is a class of consumer home audio equipment marketed to audiophiles on the basis of high price or quality, and esoteric or novel sound reproduction technologies. The term can refer simply to the price, to the build quality of the components, or to the subjective or objective quality of sound reproduction.\n\nThe distinction between the terms high-end and high fidelity is not well defined. According to one industry commentator, high-end could be defined as \"gear below which’s price and performance one could not go without compromising the music and the sound.\" Harry Pearson, founder of \"The Absolute Sound\" magazine, is widely acknowledged to have coined the term \"high-end audio\".\n\nHigh-end audio equipment can be extremely expensive. It is sometimes referred to as cost-no-object equipment. Audiophile equipment can encompass the full range from budget to high-end in terms of price.\n\nAssessing the fidelity of sound reproduction may be done aurally or using dedicated measurement equipment.\n\nThe human sense of hearing is subjective and difficult to define. Psychoacoustics is a division of acoustics that studies this field.\n\nMeasurements can be deceiving; high or low figures of certain technical characteristics do not necessarily offer a good representation of how the equipment sounds to each person. For example, some valve (vacuum tube) amplifiers produce greater amounts of total harmonic distortion, but this type of distortion (2nd harmonic) is not as disturbing to the ear as the higher order distortions produced by poorly designed transistor equipment. The terms \"high-end audio\" and \"audiophile\" are typically used disparagingly by Audio Engineering Society members, who feel that the term \"high end audio\" is vague, concerning the widely varying performance of the products sold in this premium price segment, and that an \"audiophile\" is too often a person who is overly suggestible to the marketing claims from manufacturers and sellers.\n\nThe validity of certain products are often questioned by those outside the industry include accessories such as speaker wires utilizing exotic materials (such as Oxygen-Free Copper (OFC)) and construction geometries, cable stands for lifting them off the floor (as a way to control mechanically induced vibrations), connectors, sprays and other tweaks.\n\n"}
{"id": "1643583", "url": "https://en.wikipedia.org/wiki?curid=1643583", "title": "Hydraulic analogy", "text": "Hydraulic analogy\n\nThe electronic–hydraulic analogy (derisively referred to as the drain-pipe theory by Oliver Lodge) is the most widely used analogy for \"electron fluid\" in a metal conductor. Since electric current is invisible and the processes at play in electronics are often difficult to demonstrate, the various electronic components are represented by hydraulic equivalents. Electricity (as well as heat) was originally understood to be a kind of fluid, and the names of certain electric quantities (such as current) are derived from hydraulic equivalents. As with all analogies, it demands an intuitive and competent understanding of the baseline paradigms (electronics and hydraulics).\n\nThere is no unique paradigm for establishing this analogy. Two paradigms can be used to introduce the concept to students using pressure induced by gravity or by pumps.\n\nIn the version with pressure induced by gravity, large tanks of water are held up high, or are filled to differing water levels, and the potential energy of the water head is the pressure source. This is reminiscent of electrical diagrams with an up arrow pointing to +V, grounded pins that otherwise are not shown connecting to anything, and so on. This has the advantage of associating electric potential with gravitational potential.\n\nA second paradigm is a completely enclosed version with pumps providing pressure only and no gravity. This is reminiscent of a circuit diagram with a voltage source shown and the wires actually completing a circuit. This paradigm is further discussed below.\n\nOther paradigms highlight the similarities between equations governing the flow of fluid and the flow of charge. Flow and pressure variables can be calculated in both steady and transient fluid flow situations with the use of the hydraulic ohm analogy. Hydraulic ohms are the units of hydraulic impedance, which is defined as the ratio of pressure to volume flow rate. The pressure and volume flow variables are treated as phasors in this definition, so possess a phase as well as magnitude.\n\nA slightly different paradigm is used in acoustics, where acoustic impedance is defined as a relationship between pressure and air speed. In this paradigm, a large cavity with a hole is analogous to a capacitor that stores compressional energy when the time-dependent pressure deviates from atmospheric pressure. A hole (or long tube) is analogous to an inductor that stores kinetic energy associated with the flow of air.\n\nA circuit was used to model feedback stabilization of a hydrodynamic plasma instability in a magnetic mirror In this application, the effort was to keep the plasma column centered by applying voltages to the plates, and except for the presence of turbulence and non-linear effects, the plasma was an actual electric circuit element (not really an analog).\n\nIn general, electric potential is equivalent to hydraulic head. This model assumes that the water is flowing horizontally, so that the force of gravity can be ignored. In this case electric potential is equivalent to pressure. The voltage (or voltage drop or \"potential difference\") is a difference in pressure between two points. Electric potential and voltage are usually measured in volts. \nElectric current is equivalent to a hydraulic volume flow rate; that is, the volumetric quantity of flowing water over time. Usually measured in amperes.\n\nElectric charge is equivalent to a quantity of water.\n\nA relatively wide pipe completely filled with water is equivalent to conducting wire. When comparing to a piece of wire, the pipe should be thought of as having semi-permanent caps on the ends. Connecting one end of a wire to a circuit is equivalent to un-capping one end of the pipe and attaching it to another pipe. With few exceptions (such as a high-voltage power source), a wire with only one end attached to a circuit will do nothing; the pipe remains capped on the free end, and thus adds nothing to the circuit.\n\nA resistor is equivalent to a constriction in the bore of the pipe which requires more pressure to pass the same amount of water. All pipes have some resistance to flow, just as all wires have some resistance to current.\n\nA node (or junction) in Kirchhoff's junction rule is equivalent to a pipe tee. The net flow of water into a piping tee (filled with water) must equal the net flow out.\nA capacitor is equivalent to a tank with one connection at each end and a rubber sheet dividing the tank in two lengthwise (a hydraulic accumulator). When water is forced into one pipe, equal water is simultaneously forced out of the other pipe, yet no water can penetrate the rubber diaphragm. Energy is stored by the stretching of the rubber. As more current flows \"through\" the capacitor, the back-pressure (voltage) becomes greater, thus current \"leads\" voltage in a capacitor. As the back-pressure from the stretched rubber approaches the applied pressure, the current becomes less and less. Thus capacitors \"filter out\" constant pressure differences and slowly varying, low-frequency pressure differences, while allowing rapid changes in pressure to pass through. \n\nAn ideal voltage source (ideal battery) or ideal current source is a dynamic pump with feedback control. A pressure meter on both sides shows that regardless of the current being produced, this kind of pump produces constant pressure difference. If one terminal is kept fixed at ground, another analogy is a large body of water at a high elevation, sufficiently large that the drawn water does not affect the water level. To create the analog of an ideal current source, use a positive displacement pump: A current meter (little paddle wheel) shows that when this kind of pump is driven at a constant speed, it maintains a constant speed of the little paddle wheel.\n\nA diode is equivalent to a one-way check valve with a slightly leaky valve seat. As with a diode, a small pressure difference is needed before the valve opens. And like a diode, too much reverse bias can damage or destroy the valve assembly.\n\nA transistor is a valve in which a diaphragm, controlled by a low-current signal (either constant current for a BJT or constant pressure for a FET), moves a plunger which affects the current through another section of pipe.\n\nCMOS is a combination of two MOSFET transistors. As the input pressure changes, the pistons allow the output to connect to either zero or positive pressure.\n\nA memristor is a needle valve operated by a flow meter. As water flows through in the forward direction, the needle valve restricts flow more; as water flows the other direction, the needle valve opens further providing less resistance.\n\nEM wave speed (velocity of propagation) is equivalent to the speed of sound in water. When a light switch is flipped, the electric wave travels very quickly through the wires.\n\nCharge flow speed (drift velocity) is equivalent to the particle speed of water. The moving charges themselves move rather slowly.\n\nDC is equivalent to the a constant flow of water in a circuit of pipes.\n\nLow frequency AC is equivalent to water oscillating back and forth in a pipe\n\nHigher-frequency AC and transmission lines is somewhat equivalent to sound being transmitted through the water pipes, though this does not properly mirror the cyclical reversal of alternating electric current. As described, the fluid flow conveys pressure fluctuations, but fluids do not reverse at high rates in hydraulic systems, which the above \"low frequency\" entry does accurately describe. A better concept (if sound waves are to be the phenomenon) is that of direct current with high-frequency \"ripple\" superimposed.\n\nInductive spark used in induction coils is similar to water hammer, caused by the inertia of water\n\nSome examples of analogous electrical and hydraulic equations:\n\nIf the differential equations have the same form, the response will be similar.\n\nIf taken too far, the water analogy can create misconceptions. For it to be useful, one must remain aware of the regions where electricity and water behave very differently.\n\nFields (Maxwell equations, Inductance): Electrons can push or pull other distant electrons via their fields, while water molecules experience forces only from direct contact with other molecules. For this reason, waves in water travel at the speed of sound, but waves in a sea of charge will travel much faster as the forces from one electron are applied to many distant electrons and not to only the neighbors in direct contact. In a hydraulic transmission line, the energy flows as mechanical waves through the water, but in an electric transmission line the energy flows as fields in the space surrounding the wires, and does not flow inside the metal. Also, an accelerating electron will drag its neighbors along while attracting them, both because of magnetic forces.\n\nCharge: Unlike water, movable charge carriers can be positive or negative, and conductors can exhibit an overall positive or negative net charge. The mobile carriers in electric currents are usually electrons, but sometimes they are charged positively, such as the positive ions in an electrolyte, the H ions in proton conductors or holes in p-type semiconductors and some (very rare) conductors.\n\nLeaking pipes: The electric charge of an electrical circuit and its elements is usually almost equal to zero, hence it is (almost) constant. This is formalized in Kirchhoff's current law, which does not have an analogy to hydraulic systems, where amount of the liquid is not usually constant. Even with incompressible liquid the system may contain such elements as pistons and open pools, so the volume of liquid contained in a part of the system can change. For this reason, continuing electric currents require closed loops rather than hydraulics' open source/sink resembling spigots and buckets.\n\nFluid velocity and resistance of metals: As with water hoses, the carrier drift velocity in conductors is directly proportional to current. However, water only experiences drag via the pipes' inner surface, while charges are slowed at all points within a metal, as with water forced through a filter. Also, typical velocity of charge carriers within a conductor is less than centimeters per minute, and the \"electrical friction\" is extremely high. If charges ever flowed as fast as water can flow in pipes, the electric current would be immense, and the conductors would become incandescently hot and perhaps vaporize. To model the resistance and the charge-velocity of metals, perhaps a pipe packed with sponge, or a narrow straw filled with syrup, would be a better analogy than a large-diameter water pipe. Resistance in most electrical conductors is a linear function: as current increases, voltage drop increases proportionally (Ohm's Law). Liquid resistance in pipes is not linear with volume, varying as the square of volumetric flow (see Darcy–Weisbach equation).\n\nQuantum Mechanics: Solid conductors and insulators contain charges at more than one discrete level of atomic orbit energy, while the water in one region of a pipe can only have a single value of pressure. For this reason there is no hydraulic explanation for such things as a battery's charge pumping ability, a diode's depletion layer and voltage drop, solar cell functions, Peltier effect, etc., however equivalent devices can be designed which exhibit similar responses, although some of the mechanisms would only serve to regulate the flow curves rather than to contribute to the component's primary function.\n\nIn order for the model to be useful, the reader or student must have a substantial understanding of the model (hydraulic) system's principles. It also requires that the principles can be transferred to the target (electrical) system. Hydraulic systems are deceptively simple: the phenomenon of pump cavitation is a known, complex problem that few people outside of the fluid power or irrigation industries would understand. For those who do, the hydraulic analogy is amusing, as no \"cavitation\" equivalent exists in electrical engineering. The hydraulic analogy can give a mistaken sense of understanding that will be exposed once a detailed description of electrical circuit theory is required.\n\nOne must also consider the difficulties in trying to make an analogy match reality completely. The above \"electrical friction\" example, where the hydraulic analog is a pipe filled with sponge material, illustrates the problem: the model must be increased in complexity beyond any realistic scenario.\n\n\n"}
{"id": "19489821", "url": "https://en.wikipedia.org/wiki?curid=19489821", "title": "Illinois Section American Water Works Association", "text": "Illinois Section American Water Works Association\n\nThe Illinois Section American Water Works Association (ISAWWA) is part of the national American Water Works Association. Established in 1909, the ISAWWA currently represents over 1770 public water supplies of all sizes. ISAWWA members are administrators, utility operators, professional engineers, contractors, manufacturers, scientists, professors, health professionals, regulators, and ordinary citizens. \n\nThe mission of ISAWWA is the advancement and dissemination of knowledge concerning the practices in the design, construction, operation, and management of water works, and the promotion of public health, safety, and welfare.\n\n"}
{"id": "11025494", "url": "https://en.wikipedia.org/wiki?curid=11025494", "title": "Internationalization Tag Set", "text": "Internationalization Tag Set\n\nThe Internationalization Tag Set (ITS) is a set of attributes and elements designed to provide internationalization and localization support in XML documents.\n\nThe ITS specification identifies concepts (called \"ITS data categories\") which are important for internationalization and localization. It also defines implementations of these concepts through a set of elements and attributes grouped in the ITS namespace. XML developers can use this namespace to integrate internationalization features directly into their own XML schemas and documents.\n\nITS v1.0 includes seven data categories:\n\n\nThe vocabulary is designed to work on two different fronts: First by providing markup usable directly in the XML documents. Secondly, by offering a way to indicate if there are parts of a given markup that correspond to some of the ITS data categories and should be treated as such by ITS processors.\n\nITS applies to both new document types as well as existing ones. It also applies to both markups without any internationalization features as well as the class of documents already supporting some internationalization or localization-related functions.\n\nITS can be specified using global rules and local rules.\n\n\nIn the following ITS markup example, the elements and attributes with the codice_2 prefix are part of the ITS namespace. The codice_1 element lists the different rules to apply to this file. There is one codice_4 rule that indicates that any content inside the codice_5 element should not be translated.\n\nThe codice_6 attributes used in some elements are utilised to override the global rule. Here, to make translatable the content of codice_7 and to make non-translatable the text \"faux pas\".\n\nIn the following ITS markup example, the codice_8 element specifies that any node corresponding to the XPath expression codice_9 has an associated note. The location of that note is expressed by the codice_10 attribute, which holds a relative XPath expression pointing to the node where the note is, here codice_11.\n\nNote also the use of the codice_6 attribute to mark the codice_13 elements as non-translatable.\n\nITS does not have a solution to all XML internationalization and localization issues.\n\nOne reason is that version 1.0 does not have data categories for everything. For example, there is currently no way to indicate a relation source/target in bilingual files where some parts of a document store the source text and some other parts the corresponding translation.\n\nThe other reason is that many aspects of internationalization cannot be resolved with a markup. They have to do with the design of the DTD or the schema itself. There are best practices, design and authoring guidelines that are necessary to follow to make sure documents are correctly internationalized and easy to localize. For example, using attributes to store translatable text is a bad idea for many different reasons, but ITS cannot prevent an XML developer from making such choice.\n\nSome of the ITS 1.0 limitations are being addressed in the version 2.0: See http://www.w3.org/TR/its20/ for more details.\n\n"}
{"id": "24402636", "url": "https://en.wikipedia.org/wiki?curid=24402636", "title": "Learning store", "text": "Learning store\n\nA learning store is a specialised retailer of materials expressly for the education and development of children of all ages. These materials include toys, books, games, building blocks and music, along with traditional teaching resources and curriculum materials. Some learning stores offer materials for developmentally disabled adults as well. The term \"learning store\" was coined in 1984 by the Golden Apple Learning Store. Large chains such as Early Learning Centre now exist and learning stores can now be found in many urban and suburban areas.\n\nLearning stores have traditionally offered less mainstream toys such as BRIO, Playmobil, Melissa & Doug, or toys sold primarily in the speciality toy market. Other resources found at Learning Stores are often published by small educational publishers outside of the mainstream textbook publishing world. Learning stores and their suppliers are generally members of either the American Specialty Toy Retailing Association (ASTRA) or the National School Supply and Equipment Association (NSSEA).\n"}
{"id": "20370598", "url": "https://en.wikipedia.org/wiki?curid=20370598", "title": "Life Technologies (Thermo Fisher Scientific)", "text": "Life Technologies (Thermo Fisher Scientific)\n\nLife Technologies Corporation was a biotech company founded in November 2008 through a US$6.7 billion merger of Invitrogen Corporation and Applied Biosystems Inc. The joint sales of the combined companies were about $3.5 billion; they had about 9,500 employees, and owned more than 3,600 licenses and patents.\n\nThermo Fisher Scientific Corporation acquired the company in 2014 and used the Life Technologies brand name for a family of biotechnology products and services from Feb 2014 to July 2015. Thermo Fisher retired the Life Technologies brand name and logos in late July 2015, following a world-wide release letter to all customers in five languages.\n\nThe name \"Life Technologies\" was an old name from the history of Invitrogen. GIBCO (Grand Island Biological Company) had been founded around 1960 in New York; in 1983 GIBCO merged with a reagent company called Bethesda Research Laboratories and the merged company was named Life Technologies. In 2000, Invitrogen acquired Life Technologies and discontinued that name. When Invitrogen and Applied Biosystems merged, the companies revived the name.\n\nThe use of the \"Life Technologies\" brand name is disputed. Life Technologies (India) Private Limited, a company founded in 2002, operating in this corporate name claims ownership of the brand name.\n\nIn January 2014, a legal dispute on the use of the mark \"Life Technologies\" in India concluded as the Delhi High Court denied Life Technologies' request to prevent the similarly named biotechnology company Life Technologies India Pvt. Limited from using the \"Life Technologies\" mark and various related web domains.\n\nIn 2013 Thermo Fisher agreed to buy Life Technologies for $13.6 billion. They completed the sale on 3 February 2014, and the Life Technologies brand became part of the Life Sciences Solutions Group of Thermo Fisher Scientific.\n\nBetween its formation in 2008 and its acquisition by Thermo Fisher Scientific in 2014, Life Technologies Corporation was an independent multinational corporation based in Carlsbad, California, United States.\n\nA court case involving Life Technologies (as the former Applera Corp) ended in January 2014, as the Connecticut District Court penalized Life Technologies Corp over $60 million for patent infringements by its parent companies prior to the merger. The jury awarded $48 million in royalty damages to the plaintiffs Enzo Biochem, Inc, Enzo Life Sciences, and Yale University.\n"}
{"id": "44255279", "url": "https://en.wikipedia.org/wiki?curid=44255279", "title": "List of BlackBerry 10 devices", "text": "List of BlackBerry 10 devices\n\nThis is a list of all devices running the BlackBerry 10 operating system. The company's later devices, starting in the fall of 2015 with the BlackBerry Priv, use the Android operating system instead.\n\nMid-range devices are targeted at emerging markets and budget-conscious customers. They use dual-core processors. Between 8 GB and 16 GB of internal storage is included, though more can be added with a separate microSD card.\nFlagship devices are featured the most in BlackBerry advertisements. The Passport features a quad-core processor. All other devices use dual-core processors.\nPorsche Design smartphones are luxury offerings that are based, as the name suggests, on the Porsche automobile brand. These devices use luxury components on the exterior. The internal hardware is the same as the corresponding flagship devices, but the internal storage is upgraded to 64 GB.\n"}
{"id": "30444895", "url": "https://en.wikipedia.org/wiki?curid=30444895", "title": "List of brazing alloys", "text": "List of brazing alloys\n"}
{"id": "50452924", "url": "https://en.wikipedia.org/wiki?curid=50452924", "title": "List of tallest oil platforms", "text": "List of tallest oil platforms\n\nThis is a list of the tallest oil platforms over in height. The current highest oil platform is the Petronius platform operated by Chevron Corporation and Marathon Oil in the Gulf of Mexico, 210 km southeast of New Orleans, United States.\n"}
{"id": "70311", "url": "https://en.wikipedia.org/wiki?curid=70311", "title": "Maraschino cherry", "text": "Maraschino cherry\n\nA maraschino cherry ( or ) is a preserved, sweetened cherry, typically made from light-colored sweet cherries such as the Royal Ann, Rainier, or Gold varieties. In their modern form, the cherries are first preserved in a brine solution usually containing sulfur dioxide and calcium chloride to bleach the fruit, then soaked in a suspension of food coloring (common red food dye is FD&C Red 40), sugar syrup, and other components.\n\nMaraschino cherries are an ingredient in many cocktails, giving them the nickname \"cocktail cherries\". As a garnish, they often are used to decorate frozen yogurt, baked ham, cakes, pastry, parfaits, milkshakes, ice cream sundaes, and ice cream sodas. They are an integral part of an American ice cream sundae. The term \"cherry on top\" refers to the Maraschino cherries on top of the ice cream sundae. They are frequently included in canned fruit cocktail. They are also used as an accompaniment to sweet paan. Sometimes the cherries, along with some of the maraschino \"juice\", are put into a glass of Coca-Cola to make an old-fashioned or homemade \"Cherry Coke\".\n\nThe name \"maraschino\" originates from the Marasca cherry of Croatian origin and the maraschino liqueur made from it, in which Marasca cherries were crushed and preserved after being pickled. Whole cherries preserved in this liqueur were known as \"maraschino cherries\". These had been a local means of preserving the fruit in Dalmatia.\n\nIn the 19th century, these became popular in the rest of Europe, but the supply in Dalmatia was too small for the whole continent, so they came to be seen as a delicacy for royalty and the wealthy.\n\nBecause of the relative scarcity of the Marasca, other cherries came to be preserved in various ways and sold as \"maraschino\".\n\nThe cherries were first introduced in the United States in the late 19th century, where they were served in fine bars and restaurants. Because they were scarce and expensive, by the turn of the century American producers were experimenting with other processes for preserving cherries, with flavors such as almond extract and substitute fruit like Queen Anne cherries. Among these, alcohol was already becoming less common.\n\nIn response, the USDA in 1912 defined \"maraschino cherries\" as \"Marasca cherries preserved in maraschino\" under the authority of the Food and Drugs Act of 1906. The artificially-colored and sweetened Royal Anne variety were required to be called \"Imitation Maraschino Cherries\" instead. Food Inspection Decision 141 defined Marasca cherries and maraschino themselves. It was signed on Feb. 17, 1912.\n\nDuring Prohibition in the United States as of 1920, the decreasingly popular alcoholic variety was illegal as well. Ernest H. Wiegand, a professor of horticulture at Oregon State University, developed the modern method of manufacturing maraschino cherries using a brine solution rather than alcohol. Accordingly, most modern maraschino cherries have only a historical connection with maraschino liqueur.\n\nAccording to Bob Cain, Cliff Samuels, and Hoya Yang, who worked with Wiegand at OSU, Prohibition had nothing to do with Wiegand's research: his intention was to develop a better brining process for cherries that would not soften them. When Wiegand began his research, there were several ways to preserve maraschino cherries without alcohol, long before Prohibition went into effect. Wiegand took a process that people had their own recipes for—\"and who knows what they were putting in there\" (frequently not alcohol)—and turned it into a science, something replicable.\n\nWhen Wiegand began his research, sodium metabisulfite was being used to preserve maraschino cherries. Some accounts indicate that this preservation method was being used long before Prohibition. Some manufacturers used maraschino or imitation liqueurs to flavor the cherries, but newspaper stories from the early part of the century suggest that many manufacturers stopped using alcohol and artificial dyes before Prohibition.\n\nAfter Prohibition was repealed lobbying by the non-alcoholic preserved cherry industry encouraged the Food and Drug Administration to revise federal policy toward canned cherries. It held a hearing in April 1939 to establish a new standard of identity. Since 1940, \"maraschino cherries\" have been defined as \"cherries which have been dyed red, impregnated with sugar, and packed in a sugar syrup flavored with oil of bitter almonds or a similar flavor.\"\n\nFD&C Red Number 1 and 4, and FD&C Yellow Number 1 through 4 were removed from the approved list in 1960. The ban on Red Number 4 was lifted in 1965 to allow the coloring of maraschino cherries, which by then were considered mainly decorative and not a foodstuff. In 1975, William F. Randolph of the FDA ruled that if an \"artificial bitter almond flavor or any synthetic flavor is used, the product must be labeled artificial or artificially flavored.\" The following year, the ban on Red #4 was reinstated. \n\nAs of 2010, modern American Maraschino cherries typically use FD&C Red 40 as a colorant.\n\nThey are associated with ice cream sundaes and ice cream desserts.\n\n"}
{"id": "3315351", "url": "https://en.wikipedia.org/wiki?curid=3315351", "title": "Mechanical engineering technology", "text": "Mechanical engineering technology\n\nMechanical Engineering Technology is the application of engineering principles and technological developments for the creation of useful products and production machinery.\n\nMechanical engineering technologists are expected to apply current technologies and principles from machine and product design, production and material and manufacturing processes.\n\nExpandable specialties may include aerospace, automotive, energy, nuclear, petroleum, manufacturing, product development, and industrial design. \n\nMechanical engineering technologists can have many different titles, including in the United States: \n\nMechanical Engineering Technology coursework is less theoretical, and more application based than a mechanical engineering degree. This is evident through the additional laboratory coursework required for a degree. The ability to apply concepts from the chemical engineering and electrical engineering fields is important. \n\nSome university Mechanical Engineering Technology degree programs require mathematics through differential equations and statistics. Most courses involve algebra and calculus.\n\nOftentimes, a MET graduate could get hired as an engineer; job titles may include Mechanical Engineer and Manufacturing Engineer. In the U.S. it is possible to get an associates or bachelor's degree. Individuals with a bachelor's degree in engineering technology may continue on to complete the E.I.T. (Engineer in Training) examination to eventually become Professional Engineers if the program is ABET accredited.\n\nSoftware tools such as Finite Element Analysis (FEA) and Computational Fluid Dynamics (CFD) are often used to analyze parts and assemblies. 3D models can be made to represent parts and assemblies with computer-aided design (CAD) software. \n\nThrough the application of computer-aided manufacturing (CAM), models may also be used directly by software to create \"instructions\" for the manufacture of objects represented by the models, through computer numerically controlled (CNC) machining or other automated processes.\n\n"}
{"id": "49154103", "url": "https://en.wikipedia.org/wiki?curid=49154103", "title": "PayCash", "text": "PayCash\n\nPayCash is a Russian electronic payment platform for making anonymous Internet purchases with cash via kiosks. \n\nThe system is based on the technology proposed in the eighties by Dutch David Chaum. PayCash is developed by CJSC \"Processing Technologies\" and JSC «Aerospace Equipment Corporation», with the financial support from Tavrichesky Bank. PayCash owns 4 patents on technologies used in the system.\n\nSeveral major projects are implemented based on the PayCash technology:\n\n"}
{"id": "42743995", "url": "https://en.wikipedia.org/wiki?curid=42743995", "title": "Phnom Penh Institute of Technology", "text": "Phnom Penh Institute of Technology\n\nPhnom Penh Institute of Technology (), informally Phnom Penh Tech () or PPIT, is a national top-tier research university in Phnom Penh, Cambodia. PPIT is the largest private institution for higher education in Cambodia dedicated to science technology and dngineering. PPIT enrolled 246 undergraduates students in 2012–2013. It employs around 21 faculty members.\n\nPhnom Penh Tech is organised into 4 schools, within which there are over 8 departments and research centres.\n\nPPIT has one campus: the Sensok campus in Phnom Penh Thmey, Sensok, Phnom Penh.\n\nPhnom Penh Institute of Technology was founded by Japan's Graduated Cambodia Students and Yamada Osamitsu Scholarship Foundation as the Phnom Penh Vocational School on April 1, 2011, three years after the Great Recession. To accomplish the quick catch-up to the future economic recovering, the government expected this school to cultivate new modernized craftsmen and engineers. In 2013, it was renamed Phnom Penh Institute of Technology.\n\nAfter forming a new government, the new education policy was promulgated in late 2013 with the Industrial Development Policy and Science Technology and Innovation Policy. Phnom Penh Institute of Technology was recognized by the Royal Government of Cambodia.\n\nMany three-year courses were turned into four-year courses with the start of the School of Engineering this year. The university started graduate programmes in engineering in 2013.\n\nThe following year, the four centers were integrated and reorganized. The Center for the Promotion of Science Technology and Innovation; the Center for Continuing Education and Professional Development; the Center for Research Development and Industrial Liaison; the Center for Global Communication and International Language; and the School of Engineering were merged to create School of Science and Engineering.\n\nSince January 2014, it has been privatized into the National University Incorporation of Phnom Penh Institute of Technology under a sub-decree.\n\nPhnom Penh Tech is a member of SAKURA Exchange Program in Science, an international network of leading universities in Japan and Asia exchanging students and senior scholars.\n\nThe University College of Technology Sarawak (UCTS) welcomes applications from people of at least 21 years of age who do not qualify for admission into undergraduate and postgraduate programmes on the basis of formal educational attainment through Non-formal Entry via APEL.\n\n"}
{"id": "57931046", "url": "https://en.wikipedia.org/wiki?curid=57931046", "title": "Planar Crown connector", "text": "Planar Crown connector\n\nPlanar Crown is a series of Blind mate electrical RF connectors manufactured by API Weinchel. It is fitted for example to the RF input connector of the Tektronix Real Time Spectrum Analyzer RTSA-5000 series. An adapter is required to connect the more common N-type or 3.5 mm connectors.\n"}
{"id": "22499640", "url": "https://en.wikipedia.org/wiki?curid=22499640", "title": "Reason washing machine", "text": "Reason washing machine\n\nThe Reason washing machine is an eco friendly design of the washing machine, with an emphasis on ease of use, invented by the architect Andrew Reason.\n\nMr Reason suffered a back injury when playing rugby. He then read a Sunday supplements article that asked 40,000 people what they wanted from a washing machine and the most requested feature was a larger door for easier loading and unloading. Reason sketched plans of a machine with a drum which slides out when you open the washing machine door to allow easy access. He added green features; the machine automatically weighed the washing to dispense exactly the right amount of water and detergent, and instead of a concrete base, the water used for the wash acted as ballast. Reason approached the major manufacturers but they did not want to incorporate it at that time. Reason and his team in Pembrokeshire, Wales claimed that the machine could clean clothes using less energy and water, and was easy to load.\n\nThe Reason Washing Machine Kickstarter page claims an increased load capacity of 12 kg as opposed to 10 kg in its previous launch in 2009. Other changes claimed include powered opening and closing of the sliding drum.\nThe product was originally launched in January 2009, with the slogan \"The best washing machine ever made?\", and by March 2009 they employed 11 people. The first 200 machines were auctioned on the internet, and the company aimed to build 100,000 more by the end of 2010. A production line was created to meet demand, which the company anticipated would require 150 people. The company is located in Pembrokeshire, Wales.\n\n"}
{"id": "11152831", "url": "https://en.wikipedia.org/wiki?curid=11152831", "title": "Recovery boiler", "text": "Recovery boiler\n\nRecovery boiler is the part of Kraft process of pulping where chemicals for white liquor are recovered and reformed from black liquor, which contains lignin from previously processed wood. The black liquor is burned, generating heat, which is usually used in the process or in making electricity, much as in a conventional steam power plant. The invention of the recovery boiler by G.H. Tomlinson in the early 1930s was a milestone in the advancement of the kraft process.\n\nRecovery boilers are also used in the (less common) sulfite process of wood pulping; this article deals only with recovery boiler use in the Kraft process.\n\nConcentrated black liquor contains organic dissolved wood residue in addition to sodium sulfate from the cooking chemicals added at the digester. Combustion of the organic portion of chemicals produces heat. In the recovery boiler heat is used to produce high pressure steam, which is used to generate electricity in a turbine. The turbine exhaust, low pressure steam is used for process heating.\n\nCombustion of black liquor in the recovery boiler furnace needs to be controlled carefully. High concentration of sulfur requires optimum process conditions to avoid production of sulfur dioxide and reduced sulfur gas emissions. In addition to environmentally clean combustion, reduction of inorganic sulfur must be achieved in the char bed.\n\nSeveral processes occur in the recovery boiler:\n\n\nSome features of the original recovery boiler have remained unchanged to this day. It was the first recovery equipment type where all processes occurred in a single vessel. The drying, combustion and subsequent reactions of black liquor all occur inside a cooled furnace. This is the main idea in Tomlinson’s work.\n\nSecondly the combustion is aided by spraying the black liquor into small droplets. Controlling process by directing spray proved easy. Spraying was used in early rotary furnaces and with some success adapted to stationary furnace by H. K. Moore. Thirdly one can control the char bed by having primary air level at char bed surface and more levels above. Multiple level air system was introduced by C. L. Wagner.\n\nRecovery boilers also improved the smelt removal. It is removed directly from the furnace through smelt spouts into a dissolving tank. Some of the first recovery units employed the use of Cottrell’s electrostatic precipitator for dust recovery.\n\nBabcock & Wilcox was founded in 1867 and gained early fame with its water tube boilers. The company built and put into service the first black liquor recovery boiler in the world in 1929. This was soon followed by a unit with completely water cooled furnace at Windsor Mills in 1934. After reverberatory and rotating furnaces the recovery boiler was on its way.\n\nThe second early pioneer, Combustion Engineering based its recovery boiler design on the work of William M. Cary, who in 1926 designed three furnaces to operate with direct liquor spraying and on work by Adolph W. Waern and his recovery units.\n\nRecovery boilers were soon licensed and produced in Scandinavia and Japan. These boilers were built by local manufacturers from drawings and with instructions of licensors. One of the early Scandinavian Tomlinson units employed an 8.0 m high furnace that had 2.8*4.1 m furnace bottom which expanded to 4.0*4.1 m at superheater entrance.\n\nThis unit stopped production for every weekend. In the beginning economizers had to be water washed twice every day, but after installation of shot sootblowing in the late 1940s the economizers could be cleaned at the regular weekend stop.\n\nThe construction utilized was very successful. One of the early Scandinavian boilers 160 t/day at Korsnäs, operated still almost 50 years later.\n\nThe use of Kraft recovery boilers spread fast as functioning chemical recovery gave Kraft pulping an economic edge over sulfite pulping.\n\nThe first recovery boilers had horizontal evaporator surfaces, followed by superheaters and more evaporation surfaces. These boilers resembled the state-of-the-art boilers of some 30 years earlier. This trend has continued until today. Since a halt in the production line will cost a lot of money the adopted technology in recovery boilers tends to be conservative.\n\nThe first recovery boilers had severe problems with fouling.\n\nTube spacing wide enough for normal operation of a coal-fired boiler had to be wider for recovery boilers. This gave satisfactory performance of about a week before a water wash. Mechanical sootblowers were also quickly adopted. To control chemical losses and lower the cost of purchased chemicals electrostatic precipitators were added. Lowering dust losses in flue gases has more than 60 years of practice.\n\nOne should also note square headers in the 1940 recovery boiler. The air levels in recovery boilers soon standardized to two: a primary air level at the char bed level and a secondary above the liquor guns.\n\nIn the first tens of years, the furnace lining was of refractory brick. The flow of smelt on the walls causes extensive replacement and soon designs that eliminated the use of bricks were developed.\n\nTo achieve solid operation and low emissions the recovery boiler air system needs to be properly designed. Air system development continues and has been continuing as long as recovery boilers have existed. As soon as the target set for the air system has been met new targets are given. Currently the new air systems have achieved low NOx, but are still working on lowering fouling. Table 1 visualizes the development of air systems.\n\nTable 1: Development of air systems. \nThe first generation air system in the 1940s and 1950s consisted of a two level arrangement; primary air for maintaining the reduction zone and secondary air below the liquor guns for final oxidation. The recovery boiler size was 100 – 300 TDS (tons of dry solids) per day. and black liquor concentration 45 – 55%. Frequently to sustain combustion auxiliary fuel needed to be fired. Primary air was 60 – 70% of total air with secondary the rest. In all levels openings were small and design velocities were 40 – 45 m/s. Both air levels were operated at 150C. Liquor gun or guns were oscillating. Main problems were high carryover, plugging and low reduction. But the function, combustion of black liquor, could be filled.\n\nThe second generation air system targeted high reduction. In 1954 CE moved their secondary air from about 1 m below the liquor guns to about 2 m above them. The air ratios and temperatures remained the same, but to increase mixing 50 m/s secondary air velocities were used. CE changed their frontwall/backwall secondary to tangential firing at that time. In tangential air system the air nozzles are in the furnace corners. The preferred method is to create a swirl of almost the total furnace width. In large units the swirl caused left and right imbalances. This kind of air system with increased dry solids managed to increase lower furnace temperatures and achieve reasonable reduction. B&W had already adopted the three-level air feeding by then.\n\nThird generation air system was the three level air. In Europe the use of three levels of air feeding with primary and secondary below the liquor guns started about 1980. At the same time stationary firing gained ground. Use of about 50% secondary seemed to give hot and stable lower furnace. Higher black liquor solids 65 – 70% started to be in use. Hotter lower furnace and improved reduction were reported. With three level air and higher dry solids the sulfur emissions could be kept in place.\n\nFourth generation air systems are the multilevel air and the vertical air. As the feed of black liquor dry solids to the recovery boiler have increased, achieving low sulfur emissions is not anymore the target of the air system. Instead low NOx and low carryover are the new targets.\n\nThe three-level air system was a significant improvement, but better results were required. Use of CFD models offered a new insight of air system workings. The first to develop a new air system was Kvaerner (Tampella) with their 1990 multilevel secondary air in Kemi, Finland, which was later adapted to a string of large recovery boilers. Kvaerner also patented the four level air system, where additional air level is added above the tertiary air level. This enables significant NOx reduction.\n\nVertical air mixing was invented by Erik Uppstu. His idea is to turn traditional vertical mixing to horizontal mixing. Closely spaced jets will form a flat plane. In traditional boilers this plane has been formed by secondary air. By placing the planes to 2/3 or 3/4 arrangement improved mixing results. Vertical air has a potential to reduce NOx as staging air helps in decreasing emissions. In vertical air mixing, primary air supply is arranged conventionally. Rest of the air ports are placed on interlacing 2/3 or 3/4 arrangement.\n\nAs fired black liquor is a mixture of organics, inorganics and water. Typically the amount of water is expressed as mass ratio of dried black liquor to unit of black liquor before drying. This ratio is called the black liquor dry solids.\nIf the black liquor dry solids is below 20% or water content in black liquor is above 80% the net heating value of black liquor is negative. This means that all heat from combustion of organics in black liquor is spent evaporating the water it contains. The higher the dry solids, the less water the black liquor contains and the hotter the adiabatic combustion temperature.\n\nBlack liquor dry solids have always been limited by the ability of available evaporation. Virgin black liquor dry solids of recovery boilers is shown as a function of purchase year of that boiler.\n\nWhen looking at the virgin black liquor dry solids we note that on average dry solids has increased. This is especially true for latest very large recovery boilers. Design dry solids for green field mills have been either 80 or 85% dry solids. 80% (or before that 75%) dry solids has been in use in Asia and South America. 85% (or before that 80%) has been in use in Scandinavia and Europe.\n\nDevelopment of recovery boiler main steam pressure and temperature was rapid at the beginning. By 1955, not even 20 years from birth of recovery boiler highest steam pressures were 10.0 MPa and 480C. The pressures and temperatures used then backed downward somewhat due to safety. By 1980 there were about 700 recovery boilers in the world.\nDevelopment of recovery boiler pressure, temperature and capacity.\n\nOne of the main hazards in operation of recovery boilers is the smelt-water explosion. This can happen if even a small amount of water is mixed with the solids in high temperature. Smelt-water explosion is purely a physical phenomenon. The smelt water explosion phenomena have been studied by Grace. By 1980 there were about 700 recovery boilers in the world. The liquid - liquid type explosion mechanism has been established as one of the main causes of recovery boiler explosions.\n\nIn the smelt water explosion even a few liters of water, when mixed with molten smelt can violently turn to steam in few tenths of a second. Char bed and water can coexist as steam blanketing reduces heat transfer. Some trigger event destroys the balance and water is evaporated quickly through direct contact with smelt. This sudden evaporation causes increase of volume and a pressure wave of some 10 000 – 100 000 Pa. The force is usually sufficient to cause all furnace walls to bend out of shape. Safety of equipment and personnel requires an immediate shutdown of the recovery boiler if there is a possibility that water has entered the furnace. All recovery boilers have to be equipped with special automatic shutdown sequence.\n\nThe other type of explosions is the combustible gases explosion. For this to happen the fuel and the air have to be mixed before the ignition. Typical conditions are either a blackout (loss of flame) without purge of furnace or continuous operation in a substoichiometric state. To detect blackout flame monitoring devices are installed, with subsequent interlocked purge and startup. Combustible gas explosions are connected with oil/gas firing in the boiler. As also continuous O monitoring is practiced in virtually every boiler the noncombustible gas explosions have become very rare.\n\nThe modern recovery boiler is of a single drum design, with vertical steam generating bank and wide spaced superheaters. This design was first proposed by Colin MacCallum in 1973 in a proposal by Götaverken (now Metso Power inc.) for a large recovery boiler having a capacity of 4,000,000 lb of black liquor solids per day for a boiler in Skutskär, Sweden, but this design was rejected as being too advanced at that time by the prospective owner. MacCallum presented the design at BLRBAC and in a paper \"The Radiant Recovery Boiler\" printed in Tappi magazine in December 1980. The first boiler of this single-drum design was sold by Götaverken at Leaf River in Mississippi in 1984. The construction of the vertical steam generating bank is similar to the vertical economizer. Vertical boiler bank is easy to keep clean. The spacing between superheater panels increased and leveled off at over 300 but under 400 mm. Wide spacing in superheaters helps to minimize fouling. This arrangement, in combination with sweetwater attemperators, ensures maximum protection against corrosion. There have been numerous improvements in recovery boiler materials to limit corrosion.\n\nThe effect of increasing dry solids concentration has had a significant effect on the main operating variables. The steam flow increases with increasing black liquor dry solids content. Increasing closure of the pulp mill means that less heat per unit of black liquor dry solids will be available in the furnace. The flue gas heat loss will decrease as the flue gas flow diminishes. Increasing black liquor dry solids is especially helpful since the recovery boiler capacity is often limited by the flue gas flow.\n\nA modern recovery boiler consists of heat transfer surfaces made of steel tube; furnace-1, superheaters-2, boiler generating bank-3 and economizers-4. The steam drum-5 design is of single-drum type. The air and black liquor are introduced through primary and secondary air ports-6, liquor guns-7 and tertiary air ports-8. The combustion residue, smelt exits through smelt spouts-9 to the dissolving tank-10.\n\nThe nominal furnace loading has increased during the last ten years and will continue to increase. Changes in air design have increased furnace temperatures. This has enabled a significant increase in hearth solids loading (HSL) with only a modest design increase in hearth heat release rate (HHRR). The average flue gas flow decreases as less water vapor is present. So the vertical flue gas velocities can be reduced even with increasing temperatures in lower furnace.\n\nThe most marked change has been the adoption of single drum construction. This change has been partly affected by the more reliable water quality control. The advantages of a single drum boiler compared to a bi drum are the improved safety and availability. Single drum boilers can be built to higher pressures and bigger capacities. Savings can be achieved with decreased erection time. There is less tube joints in the single drum construction so drums with improved startup curves can be built.\n\nThe construction of the vertical steam generating bank is similar to the vertical economizer, which based on experience is very easy to keep clean. Vertical flue gas flow path improves the cleanability with high dust loading. To minimize the risk for plugging and maximize the efficiency of cleaning both the generating bank and the economizers are arranged on generous side spacing. Plugging of a two drum boiler bank is often caused by the tight spacing between the tubes.\n\nThe spacing between superheater panels has increased. All superheaters are now wide spaced to minimize fouling. This arrangement, in combination with sweetwater attemperators, ensures maximum protection against corrosion. With wide spacing plugging of the superheaters becomes less likely, the deposit cleaning is easier and the sootblowing steam consumption is lower. Increased number of superheaters facilitates the control of superheater outlet steam temperature especially during start ups.\n\nThe lower loops of hottest superheaters can be made of austenitic material, with better corrosion resistance. The steam velocity in the hottest superheater tubes is high, decreasing the tube surface temperature. Low tube surface temperatures are essential to prevent superheater corrosion. A high steam side pressure loss over the hot superheaters ensures uniform steam flow in tube elements.\n\nRecovery boilers have been the preferred mode of Kraft mill chemical recovery since the 1930s and the process has been improved considerably since the first generation. There have been attempts to replace the Tomlinson recovery boiler with recovery systems yielding higher efficiency. The most promising candidate appears to be gasification, where Chemrec's technology for entrained flow gasification of black liquor could prove to be a strong contender.\n\nEven if new technology is able to compete with traditional recovery boiler technology the transition will most likely be gradual. First, manufacturers of recovery boilers such as Metso, Andritz and Mitsubishi, can be expected to continue development of their products. Second, Tomlinson recovery boilers have a long life span, often around 40 years, and will probably not be replaced until the end of their economic lifetime, and may in the meantime be upgraded at intervals of 10 – 15 years.\n\n"}
{"id": "33582951", "url": "https://en.wikipedia.org/wiki?curid=33582951", "title": "Shear grab", "text": "Shear grab\n\nA shear grab or also known as a \"silo grab\" or \"block cutter\", is an implement used to cut blocks of silage from a silage pit/clamp. It is connected to a tractor via the three-point linkage or some are connected to a front end loader. It is powered by the tractors hydraulic system.\n\nIt usually has spikes on the bottom and a large rectangular section with blades on it that cut the block using two hydraulic rams. Before the invention of shear grabs farmers had to use silage knives, and a link box to get the silage into the livestock.\n\nList of agricultural machinery\n"}
{"id": "56887415", "url": "https://en.wikipedia.org/wiki?curid=56887415", "title": "Solidarność: Menedżer Konspiracji", "text": "Solidarność: Menedżer Konspiracji\n\nSolidarność: Menadżer Konspiracji is a 2013 Polish educational game.\n"}
{"id": "8868426", "url": "https://en.wikipedia.org/wiki?curid=8868426", "title": "Sonar technician", "text": "Sonar technician\n\nSonar technician (abbreviated as ST) is a United States Navy occupational rating.\n\nSTs are responsible for underwater surveillance. They assist in safe navigation and aid in search, rescue and attack operations. They operate and repair sonar equipment. STs track underwater threats and send tracks to fire control (antisubmarine warfare controlling station) operator (ASWCS) for further evaluation and or destruction.\n\nSonar technicians are separated into two categories, STG (sonar technician surface) who are on surface ships and STS (sonar technician submarine) who operate on submarines.\n\nSonar technicians are colloquially referred to as \"ping jockeys\" on board vessels, after the sound of active sonar.\n\nSonar technicians, surface fleet (manipulate, control, evaluate, and interpret data) surface sonar, Towed array, and other oceanographic systems; operate surface ship underwater fire control systems (with associated equipment) for the solution of antisubmarine warfare problems, operate underwater communications, torpedo countermeasure equipment, depth finders for navigation, collect and disseminate bathythermograph data, calculate optimum performance; perform organizational and intermediate maintenance on surface sonar and allied equipment. Attached to WEAPONS Dept, aboard US NAVY ships.\n\nSonar technician submarines operate (control, evaluate, and interpret data) submarine sonar, oceanographic equipment and submarine auxiliary sonar; coordinate submarine sonar and underwater interface; perform organizational and intermediate maintenance on submarine and allied equipment. During battle stations, sonarmen track targets, passing bearing information to fire control to refine speed, range and course. The result is a firing solution used to launch and guide torpedoes to their intended target(s).\n\nThe majority of the sonar used aboard submarines is passive, since active emissions give away a vessel's position. Active sonar is mainly used for under ice operations.\n\n"}
{"id": "38157733", "url": "https://en.wikipedia.org/wiki?curid=38157733", "title": "Superette (radio)", "text": "Superette (radio)\n\nIn 1931 RCA introduced a new line of Superette radio receivers. These used the superheterodyne principle but were lower cost than earlier products,in an attempt to maintain sales during the onset of the Great Depression. \n\nEdwin Howard Armstrong invented the superheterodyne receiver in 1918. Armstrong and RCA (under David Sarnoff) had a business and technical relationship, that would last into the 1940s. \n\nFunded by RCA, Armstrong designed a radio that can receive stations easily without complex tuning or interference from other stations. Early radio designs by Armstrong and others produced radios that were very sensitive but hard to keep under control due to the nature of radio waves operating at higher frequencies. Armstrong's superheterodyne receiver converted these high frequencies into one lower frequency. This allow the radio to be more stable or easier to tune, with less interference. \n\nThe result was the RCA Radiola AR-812 and Radiola VIII Superheterodynes in 1924, the world's first consumer superheterodyne receivers. In 1924, these cost $224 and $475 respectively. Up to 1930, RCA controlled the superheterodyne patent, and any radio manufacturer that wanted to build one had to pay royalties to RCA. In 1928 RCA launched their first AC operated superheterodyne radio, the Radiola 60 ($147 in 1928 dollars). \n\nAll these superhets were large and expensive. In the 1930s the depression was in full force. The trend in radios were smaller, more compact and lower cost. RCA introduced the Superette line in 1931 with the R7 table and R9 console.\n\nFrom 1931 RCA produced a range of small mantel radios called the Superette, which at introduction sold for $57.50 not including the vacuum tubes. \"Super\" was derived from superheterodyne. Probably the most well known is the Model R-7, which was produced in several versions.\n\nRCA also produced a console version, the model R-9. The R-7 and R-9 share identical chassis (using RCA tubes 280, 227, 235, 245 and 224). There were several versions of the R7 table (mantel) version: the R-7A using pentode output tubes (RCA 247), R-7DC and R-9DC for 110 VDC power, and the R-7 LW for long wave listening. These early superheterodynes had no AVC so stronger stations were louder than weaker ones.\n\nRCA produced spinoffs of the Superette during the 1931-32 model year. These models are based on the R-7 design but are not called Superette in RCA's literature. \"Superette\" was reserved for the R-7 and R-9 models. \n\n"}
{"id": "3200021", "url": "https://en.wikipedia.org/wiki?curid=3200021", "title": "Synthetic molecular motor", "text": "Synthetic molecular motor\n\nSynthetic molecular motors are molecular machines capable of continuous directional rotation under an energy input. Although the term \"molecular motor\" has traditionally referred to a naturally occurring protein that induces motion (via protein dynamics), some groups also use the term when referring to non-biological, non-peptide synthetic motors. Many chemists are pursuing the synthesis of such molecular motors.\n\nThe basic requirements for a synthetic motor are repetitive 360° motion, the consumption of energy and unidirectional rotation. The first two efforts in this direction, the chemically driven motor by Dr. T. Ross Kelly of Boston College with co-workers and the light-driven motor by Feringa and co-workers, were published in 1999 in the same issue of Nature. \n\nAn example of a prototype for a synthetic chemically driven rotary molecular motor was reported by Kelly and co-workers in 1999. Their system is made up from a three-bladed triptycene rotor and a helicene, and is capable of performing a unidirectional 120° rotation.\n\nThis rotation takes place in five steps. The amine group present on the triptycene moiety is converted to an isocyanate group by condensation with phosgene (a). Thermal or spontaneous rotation around the central bond then brings the isocyanate group in proximity of the hydroxyl group located on the helicene moiety (b), thereby allowing these two groups to react with each other (c). This reaction irreversibly traps the system as a strained cyclic urethane that is higher in energy and thus energetically closer to the rotational energy barrier than the original state. Further rotation of the triptycene moiety therefore requires only a relatively small amount of thermal activation in order to overcome this barrier, thereby releasing the strain (d). Finally, cleavage of the urethane group restores the amine and alcohol functionalities of the molecule (e).\n\nThe result of this sequence of events is a unidirectional 120° rotation of the triptycene moiety with respect to the helicene moiety. Additional forward or backward rotation of the triptycene rotor is inhibited by the helicene moiety, which serves a function similar to that of the pawl of a ratchet. The unidirectionality of the system is a result from both the asymmetric skew of the helicene moiety as well as the strain of the cyclic urethane which is formed in c. This strain can be only be lowered by the clockwise rotation of the triptycene rotor in d, as both counterclockwise rotation as well as the inverse process of d are energetically unfavorable. In this respect the preference for the rotation direction is determined by both the positions of the functional groups and the shape of the helicene and is thus built into the design of the molecule instead of dictated by external factors.\n\nThe motor by Kelly and co-workers is an elegant example of how chemical energy can be used to induce controlled, unidirectional rotational motion, a process which resembles the consumption of ATP in organisms in order to fuel numerous processes. However, it does suffer from a serious drawback: the sequence of events that leads to 120° rotation is not repeatable. Kelly and co-workers have therefore searched for ways to extend the system so that this sequence can be carried out repeatedly. Unfortunately, their attempts to accomplish this objective have not been successful and currently the project has been abandoned. In 2016 David Leigh's group invented the first autonomous chemically-fuelled synthetic molecular motor.\n\nSome other examples of synthetic chemically driven rotary molecular motors that all operate by sequential addition of reagents have been reported, including the use of the stereoselective ring opening of a racemic biaryl lactone by the use of chiral reagents, which results in a directed 90° rotation of one aryl with respect to the other aryl. Branchaud and co-workers have reported that this approach, followed by an additional ring closing step, can be used to accomplish a non-repeatable 180° rotation. Feringa and co-workers used this approach in their design of a molecule that can repeatably perform 360° rotation. The full rotation of this molecular motor takes place in four stages. In stages A and C rotation of the aryl moiety is restricted, although helix inversion is possible. In stages B and D the aryl can rotate with respect to the naphthalene with steric interactions preventing the aryl from passing the naphthalene. The rotary cycle consists of four chemically induced steps which realize the conversion of one stage into the next. Steps 1 and 3 are asymmetric ring opening reactions which make use of a chiral reagent in order to control the direction of the rotation of the aryl. Steps 2 and 4 consist of the deprotection of the phenol, followed by regioselective ring formation. \n\nIn 1999 the laboratory of Prof. Dr. Ben L. Feringa at the University of Groningen, The Netherlands reported the creation of a unidirectional molecular rotor. Their 360° molecular motor system consists of a bis-helicene connected by an alkene double bond displaying axial chirality and having two stereocenters.\n\nOne cycle of unidirectional rotation takes 4 reaction steps. The first step is a low temperature endothermic photoisomerization of the trans (\"P\",\"P\") isomer 1 to the \"cis\" (\"M\",\"M\") 2 where \"P\" stands for the right-handed helix and \"M\" for the left-handed helix. In this process, the two axial methyl groups are converted into two less sterically favorable equatorial methyl groups.\n\nBy increasing the temperature to 20 °C these methyl groups convert back exothermally to the (\"P\",\"P\") \"cis\" axial groups (3) in a helix inversion. Because the axial isomer is more stable than the equatorial isomer, reverse rotation is blocked. A second photoisomerization converts (\"P\",\"P\") cis 3 into (\"M\",\"M\") trans 4, again with accompanying formation of sterically unfavorable equatorial methyl groups. A thermal isomerization process at 60 °C closes the 360° cycle back to the axial positions.\nA major hurdle to overcome is the long reaction time for complete rotation in these systems, which does not compare to rotation speeds displayed by motor proteins in biological systems. In the fastest system to date, with a fluorene lower half, the half-life of the thermal helix inversion is 0.005 seconds. This compound is synthesized using the Barton-Kellogg reaction. In this molecule the slowest step in its rotation, the thermally induced helix-inversion, is believed to proceed much more quickly because the larger \"tert\"-butyl group makes the unstable isomer even less stable than when the methyl group is used. This is because the unstable isomer is more destabilized than the transition state that leads to helix-inversion. The different behaviour of the two molecules is illustrated by the fact that the half-life time for the compound with a methyl group instead of a \"tert\"-butyl group is 3.2 minutes.\n\nThe Feringa principle has been incorporated into a prototype nanocar. The car synthesized has a helicene-derived engine with an oligo (phenylene ethynylene) chassis and four carborane wheels and is expected to be able to move on a solid surface with scanning tunneling microscopy monitoring, although so far this has not been observed. The motor does not perform with fullerene wheels because they quench the photochemistry of the motor moiety. Feringa motors have also been shown to remain operable when chemically attached to solid surfaces. The ability of certain Feringa systems to act as an asymmetric catalyst has also been demonstrated.\n\nIn 2016 Feringa was awarded with a Nobel prize for his work on molecular motors. \n\nA single-molecule electrically operated motor made from a single molecule of \"n\"-butyl methyl sulfide (CHS) has been reported. The molecule is adsorbed onto a copper (111) single-crystal piece by chemisorption.\n\n"}
{"id": "25383505", "url": "https://en.wikipedia.org/wiki?curid=25383505", "title": "T5 retrofit conversion", "text": "T5 retrofit conversion\n\nT5 retrofit conversion is a means of converting light fittings designed to use T8 format lamps, so that they can use more energy-efficient T5 lamps. This is done by electronically converting the luminaires to high frequency operation.\n\nT5 lamps are approximately 40% smaller than T8 Lamps. T5 lamps have a miniature bi-pin base while T8 lamps use a medium bi-pin base.\n\nConversion kits are available which will work in existing fittings containing switch start, mains frequency fluorescent lamp ballasts. The kits convert the fittings to use energy efficient, high frequency ballasts and accommodate the smaller diameter T5 lamp.\n\nThe magnetic ballast remains in place but it is bypassed, rendering it ineffective as a conductor. The new high-frequency ballast draws only 2 W, rather than the 6-10 W of the old ballast, increasing the efficiency of the system. Changing to this type of lamp without taking the ballast out of operation (rather than simply bypassing it) results in an increased power factor for the fitting. This increase in power is a result of the separate coils used in an electric ballast, as opposed to the single coil in a magnetic ballast, because it allows the electricity to flow more consistently.\n\nThere are tree main types of conversion kits:\n\nT5 retrofit conversion can maintain existing lighting levels at up to 65% reduction in energy use.\n\n"}
{"id": "5197382", "url": "https://en.wikipedia.org/wiki?curid=5197382", "title": "Test CD", "text": "Test CD\n\nA Test CD usually refers to a compact disc containing tracks of musical and technical tests and demonstrations. Most of the tracks are made of electronic signals and pure frequencies. The purpose of these specialized compact discs is to make accurate tests and calibrate audio equipment.\n\nA wide variety of CD-DA test discs have been produced in the past, and a few are still in production:\n\n\n"}
{"id": "2382627", "url": "https://en.wikipedia.org/wiki?curid=2382627", "title": "Tolt pipeline", "text": "Tolt pipeline\n\nThe Tolt pipeline runs from the Tolt Reservoir in the Cascade Range to the Lake Forest Park Reservoir, owned by the City of Seattle, supplying the city with about 30% of its water supply. It passes through Seattle's northern Eastside suburbs and also supplies several suburban cities and water districts.\n\nThe pipeline was originally built of wood slats wrapped with iron hoops. A section of the original pipeline can be viewed at the city of Kirkland's public works department.\n\n\n"}
{"id": "31522657", "url": "https://en.wikipedia.org/wiki?curid=31522657", "title": "Tredegar Iron and Coal Company", "text": "Tredegar Iron and Coal Company\n\nTredegar Iron and Coal Company was an important 19th century ironworks in Tredegar, Wales, which due to its need for coke became a major developer of coal mines and particularly the Sirhowy Valley of South Wales. It is most closely associated with the Industrial Revolution and coal mining in the South Wales Valleys.\n\nIn 1778 an iron furnace was built in the upper Sirhowy Valley by Thomas Atkinson and William Barrow, who came to the area from London. Fuel was needed for the furnace so men were employed to dig coal at Bryn Bach and Nantybwch, the first small scale coal mining operation in the area. The furnace failed in 1794, and hence also the business.\n\nIn 1797, Samuel Homfray, with partners Richard Fothergill and the Matthew Monkhouse built a new furnace which they called the Sirhowy Ironworks, leasing the land in Bedwellty, Newport from the Tredegar Estate. \n\nIn 1800, the company was renamed the Tredegar Iron Company, named in honour of the Tredegar Estate at Tredegar House and Tredegar Park in Newport. The company was taken over by the Harfords of Ebbw Vale in 1818.\n\nIt was expanded in the late 1830s and early 1840s, producing significant volumes of rails, largely for export. The works was purchased by the Tredegar Iron Company Limited in 1873 and nine years later began to produce steel.\n\nThe company ironworks were developed on a single site, which later became known as Whiteheads, after that company took over the southern section of the site in 1907. By 1850, TICC employed between 2000 and 3000 people at its 9 furnaces, mills shops and ancillary plants.\n\nHowever, all of this production on such a vast scale had a price. Adrian Vaughn, in his 1985 book \"Grub, Water & Relief,\" mentions that in 1832 John Gooch took a managerial post in the Tredegar iron works:\nWith many people in such a small area, and with poor sanitation provision, there were several cholera epidemics in the town in the 19th century. A dedicated cholera burial ground was later established at Cefn Golau.\n\nIn 1875, the company renamed itself the Tredegar Iron and Coal Company, to allow development of additional coal mining capacity.\n\nIn 1891, the company ceased production of iron, but continued to develop coal mines and produce coal. The former Tredegar Ironworks were effectively abandoned, with Whiteheads taking over the southern section of the site from 1907. In 1931, they also closed down their operations, moving everything to their Newport works.\n\nTICC continued to develop coal mines and work pitts, until it was nationalised in 1946, becoming part of the National Coal Board. Its last chairman was Henry McLaren, 2nd Baron Aberconway.\n\nThough now almost entirely redeveloped, traces of the terracing of the valley sides at the site can still be noted at OS grid reference SO 155093.\n\n\n"}
{"id": "580863", "url": "https://en.wikipedia.org/wiki?curid=580863", "title": "Trowel", "text": "Trowel\n\nA trowel is a small hand tool used for digging, applying, smoothing, or moving small amounts of viscous or particulate material. Common varieties include the masonry trowel, garden trowel, and float trowel.\n\nA power trowel is a much larger gasoline or electrically powered walk-behind device with rotating paddles used to finish concrete floors.\n\nNumerous forms of trowel are used in masonry, concrete, and drywall construction, as well as applying adhesives such as those used in tiling and laying synthetic flooring. Masonry trowels are traditionally made of forged carbon steel, but some newer versions are made of cast stainless steel, which has longer wear and is rust-free. These include:\n\n\n\n\n\n\n\n\n\nOther forms of trowel include:\n\n\n\n\n"}
{"id": "47834322", "url": "https://en.wikipedia.org/wiki?curid=47834322", "title": "Waste House", "text": "Waste House\n\nWaste House is a building on the University of Brighton campus in the centre of Brighton on the south coast of England. It was built between 2012 and 2014 as a project involving hundreds of students and apprentices and was designed by Duncan Baker-Brown, an architect who also lectures at the university. The materials consist of a wide range of construction industry and household waste—from toothbrushes and old jeans to VHS cassettes and bicycle inner tubes—and it is the first public building in Europe to be built primarily of such products. \"From a distance [resembling] an ordinary contemporary town house\", Waste House is designed to be low-energy and sustainable, and will be in continuous use as a test-bed for the university's design, architecture and engineering students. The building has won several awards and was shortlisted for the Royal Institute of British Architects' Stephen Lawrence Prize in September 2015.\n\nWaste House has its origins in an earlier project by Duncan Baker-Brown, a senior lecturer at the university and a director of architecture firm BBM Sustainable Design based at Cooksbridge railway station in East Sussex. On Channel 4 television programme \"The House That Kevin Built\" (2008), he and designer and television presenter Kevin McCloud assembled a \"low-energy ... ecologically friendly\" prefabricated house made of organic materials. It was the first such building to be constructed in the United Kingdom. Baker-Brown was responsible for the design of this building, which was in turn referred to as \"The House that Kevin Built\". Originally Baker-Brown planned simply to take down the building and re-erect it at the University. Instead, the temporary building formed the prototype for Waste House, and since 2008 work has been undertaken at the University of Brighton Faculty of Arts to develop the ideas and techniques used on the programme in order to build a permanent house which can be used to test materials, construction theories and new techniques. It is intended to be \"a real live research project\" as well as a building which the university and other groups can use.\n\nWork was planned to begin in 2012 in time for an end-of-year completion date and a public opening in February 2013. These dates were not achieved, but preparation started on 26 November 2012 and by May 2013 construction was underway. Work finished in April 2014, the building was featured as part of the Brighton Festival the following month, the public were able to view the interior during June 2014, and by August 2014 Waste House was complete. Construction cost £140,000 and the total value of the contract was £200,000. Waste House is the first permanent public building in Europe made from waste material.\n\nOver 300 students from the University of Brighton, City College Brighton & Hove (CCB) and apprentices from housing provider Mears Group were involved in the project. The construction work was mainly undertaken by CCB students, the Mears apprentices and some volunteers, led by a project manager from Mears Group. Students learning carpentry at CCB designed the timber-framed structure and the \"fine timber staircase, finished with a decorative flourish of offcuts\". Many of the structural elements were created in the college's own workshops. University of Brighton Faculty of Arts students worked with Baker-Brown on the design, the selection of materials and on interior features such as furniture. Overall, 97.5% of the time taken to build Waste House—2,507 person-days—was provided by students and volunteers, and about 700 people worked on the project. A major aim of the project was to introduce students and apprentices to sustainable building techniques and to allow them to continue testing their ideas by retrofitting new fixtures.\n\nThe purpose of building Waste House was to demonstrate that material considered to be waste, and therefore destined for landfill, could be used to create a viable permanent building. Baker-Brown stated that in the United Kingdom, 20% of construction materials go to waste—so the equivalent of one house worth of unwanted material is generated for every five houses built. Between 85% and 90% of the materials used in Waste House are building waste of this type or ordinary household and consumer products which are no longer needed.\nStructurally, the building is timber-framed and uses a mixture of reclaimed wood and plywood from various sources around Brighton. It has been constructed on foundations of ground granulated blast-furnace slag. The \"rather unusual\" walls consist of a mixture of waste chalk and clay left over or reclaimed from building sites. These are compressed into rammed earth-style walls using pneumatic equipment—a technique which improves the building's energy conservation, because such walls store solar energy for a long time. On the outside the walls are covered with \"a scaly surface of rubbery black shingles\"—2,000 carpet tiles from an old office building in Brighton. Their fire-retardant, waterproof underlay faces outwards, providing weatherproof cladding and insulation. Their mostly black appearance is varied in places by being laminated with plastic bags in various colours. Between the carpet tiles and the walls themselves, some new material was used: DuPont supplied about of \"breathable membrane\", to further weatherproof the building, and \"Housewrap\" moisture seal. Inside, the walls are laid with reclaimed plasterboard coated with surplus paint from construction sites. The main load-bearing wall is formed of 10 tonnes of compressed chalk spoil from a building site nearby. Throughout the building, the space between the boarding and the exterior clay and chalk blocks has been filled with household rubbish which will act as insulation. Sensors have been installed to monitor how well heat is kept in, which will form part of a PhD project for a University of Brighton student. The insulation materials are revealed in various places by \"little peephole windows\" (transparent panels in the walls): as well as some secondhand conventional (polyurethane) insulation material, there are floppy disks, 4,000 VHS cassettes, 4,000 DVD and video cases, two tonnes of denim offcuts from pairs of jeans and denim jackets, cycle inner tubes to insulate windows, and 20,000 toothbrushes. The cassettes and other media came from the stock of rental shops which were closing down; an aeroplane cleaning company at nearby Gatwick Airport donated most of the toothbrushes, which were provided to First and Business Class passengers and discarded after one use, and some others were provided by Brighton schoolchildren; and the denim came from textile traders (in particular, one company which turned imported jeans into denim shorts by cutting off the legs).\n\nMany materials were obtained on the Freegle website—a free reuse and recycling service developed from The Freecycle Network. These include the clay tiles on the roof, the kitchen and some of the insulation material. Cat Fletcher, one of the founders of Freegle UK, was part of the Waste House project team and was responsible for obtaining many of the materials, both from Freegle members (numbering 18,000 in the Brighton area) and from other sources. Other than the specialist cladding materials supplied by DuPont, the only new products used in the building are the triple-glazed windows and the material used in the wiring and plumbing.\n\nIn September 2015, Waste House was nominated for that year's Stephen Lawrence Prize. This Royal Institute of British Architects (RIBA) award, named after the aspiring architect who was murdered in 1993, is for projects with a budget of up to £1 million. In its shortlisting for the award, which was judged in mid-October 2015, judges from the RIBA stated that the house \"has sufficient scientific integrity to be taken seriously by the construction industry\" and the potential to alter political attitudes to recycling. Waste House was one of seven buildings nominated for the prize, which was won by a fishing hut designed by Niall McLaughlin Architects.\n\nWaste House was one of several buildings in Sussex, along with the Chichester Festival Theatre and St Botolph's Church at Botolphs, to win an award at the RIBA South East Regional Awards in April 2015. It was described as \"a project with an interesting agenda\" and \"a collective of experiments in which students learn by application\". Duncan Baker-Brown also won an Argus Achievement Award from local newspaper \"The Argus\".\n\nRIBA awarded Waste House a Sustainability Award in 2014. It described \"some of the experiments [as] extraordinary\", commenting specifically on the toothbrush insulation and the vinyl banner vapour control layers, and noted that \"the continually evolving [design] brief\" would \"continue to question important issues of recycling that affect everyone\".\n\nThe 2Degrees Network, a worldwide collaborative association of sustainable businesses, awarded the building a Champions Award in 2014. Waste House was described as \"an investment in educating the next generation of designers and builders to think and build more sustainably\".\n\nIn view of its low energy use, Waste House has a building energy rating of A. When work started, Baker-Brown stated the aim of making it one of the United Kingdom's first A*-rated buildings.\n\n"}
{"id": "12066045", "url": "https://en.wikipedia.org/wiki?curid=12066045", "title": "Well integrity", "text": "Well integrity\n\nWell integrity, in regard to oil wells, is defined by NORSOK D-010 as the \"“Application of technical, operational and organizational solutions to reduce risk of uncontrolled release of formation fluids throughout the life cycle of a well”. There are various facets to well integrity, including accountability/responsibility, well operating processes, well service processes, tubing/annulus integrity, tree/wellhead integrity and testing of safety systems.\n\nAccountability is the position that ensures a particular task is being done. They typically control the budget and organizational chart structure.\n\nResponsibility is the position that actually does the task. There are a variety of tasks associated with well integrity and responsibility can reside with a field's well operations engineer, operators, well service technicians, etc.\n\nThis includes processes such as personnel competency, well startup-operating-shutdown procedures, process to report anomalies, corrosion/erosion control, etc.\n\nThis includes processes such as personnel competency, intervention procedures, etc.\n\nThis refers to the integrity of the production tubing. Common threats to tubing integrity are erosion and corrosion by production fluids, which denude the steel. The other threat is pressure differential between the tubing and the 'A' annulus. If it exceeds the rated pressure of the tubing, there is a risk of tubing burst or tubing collapse.\n\nThis refers to the integrity of the casing strings, which bound the annuli. All annuli will naturally be filled to some level with brine or other fluids from the completion, though the 'A' annulus may have gas down to the lowest gas lift valve if the well is gas lifted. The major integrity issues comes from the differential pressure across the casings due to the weight of brine on either side. To protect well integrity, each annulus will have a Maximum Allowable Annulus Surface Pressure.\n\nThis refers to the integrity of the surface (or subsea) equipment. The wellhead and the tree are typically suitably engineered to be able to withstand the normal operating pressures. The major integrity issues for these components surround the operation of the valves, which are prone to leaking. For this reason, valves must be routinely maintained.\n\nThis refers to testing of surface and subsurface safety systems.\n"}
{"id": "435858", "url": "https://en.wikipedia.org/wiki?curid=435858", "title": "ZDNet", "text": "ZDNet\n\nZDNet is a business technology news website published by CBS Interactive, along with TechRepublic. The brand was founded on April 1, 1991, as a general interest technology portal from Ziff Davis and evolved into an enterprise IT-focused online publication owned by CNET Networks.\n\nZDNet began as a subscription-based digital service called \"ZiffNet\" that offered computing information to users of CompuServe. It featured computer industry forums, events, features and searchable archives.\n\nInitially, ZiffNet was intended to serve as a common place to find content from all Ziff-Davis print publications. As such, ZiffNet was an expansion on an earlier online service called PCMagNet for readers of PC Magazine. Launched in 1988, PCMagNet in turn was the evolution of Ziff Davis' first electronic publishing venture, a bulletin board, which launched in 1985.\n\nOn June 20, 1995, Ziff-Davis announced the consolidation of its online information services under a single name, \"ZD Net.\" The service had grown its membership to 275,000 subscribers across six platforms: CompuServe, Prodigy, AT&T Interchange, the Microsoft Network, AppleLink and eWorld.\n\nA few months prior to the name consolidation, Ziff-Davis expanded onto the World Wide Web under the name \"ZD Net.\" Dan Farber, former editor-in-chief of \"PC Week\" and \"MacWeek\", was named editor-in-chief of the property. By June, the site was recording web traffic of 2.5 million pageviews per week.\n\nBy its fifth anniversary in 1996, the collective \"ZD Net\" brand—now on the Web, America Online, Microsoft Network and Prodigy—counted 300,000 subscribers and was named the second-highest grossing advertising site on the Web. The site also expanded overseas: initially to France, Germany and the U.K.; later to China, Australia, Hong Kong, Italy, Korea, Malaysia, Russia, Spain, Taiwan and India.\n\nIn 1997, the website—now the brand's flagship property—underwent another redesign that featured topical \"channels\" of content. It also marked the change in name from \"ZD Net\" to \"ZDNet.\"\n\nTwo months prior, the company launched ZDNet News, or \"ZDNN,\" the site's first dedicated section to original reportage. Among the journalists hired to staff the department were former Computer Shopper executive editor Charlie Cooper, San Jose Mercury News business editor Steve Hamm, PC Week Inside senior editor Bill Snyder, PC Week editor John Dodge, Computerworld editor Michael Fitzgerald and PC Week editorial director Jim Louderback.\n\nThe appointment of digital publishing executive Dan Rosensweig as ZDNet's first president capped a year of significant change for the brand.\n\nIn 1998, ZDNet launched \"Inter@active Investor,\" or ZDII, a spin-off website for investors that offered financial news and information on technology companies.\n\nOn May 11, 1998, Ziff-Davis launched ZDTV as the first cable television channel and website to offer 24-hour programming about computing and the Internet. The venture, which was partly owned by Vulcan Enterprises, was supported with a staff of 170 and incorporated ZDNet content on its website, ZDTV.com. The channel would later become Tech TV.\n\nBy the end of 1998, ZDNet was the dominant technology brand online. It led its closest rival, CNET, by a 26 percent margin and was the 13th most popular site on the Web, reaching 8.4 million users, or 13.4 percent of all users on the Web. The site would reach an additional 600,000 users within a year.\n\nIn 1999, Ziff-Davis spun ZDNet off as a separate company and offered it as a tracking stock, ZDZ, to accompany the parent stock, ZD. An initial public offering raised $190 million, but the tracking stock was eliminated in early 2000 and revived as common stock. The new company soon acquired Updates.com, a software upgrade service. It was incorporated into the site's \"Help Channel.\"\n\nIn 1999, ZDNet also launched \"Tech Life,\" a network of six consumer-focused tech sites intended to attract parents (\"FamilyPC\"), music listeners (\"ZDNet Music\"), gadget enthusiasts (\"ZDNet Equip\"), gamers (\"ZDNet GameSpot\") and basic users (\"Internet Life\" with Yahoo).\n\nIt also launched \"Computer Stew,\" a web-based comedy show about technology that featured John Hargrave and Jay Stevens, as well as the first ZDNet Holiday Gift Guide.\n\nOn December 30, 1999, ZDNet launched a $25 million branding campaign in response to a $100 million advertising campaign launched by rival CNET.\n\nZDNet's lead over the competition narrowed by 2000. Despite a record 10.7 million unique users in January, it managed only a 13 percent lead over the next competitor. By mid-2000, ZDNet had expanded to 23 countries in 14 languages on six continents.\n\nOn July 19, 2000, CNET Networks—parent company of CNET, ZDNet's largest rival—announced that it would acquire ZDNet for about $1.6 billion. Some analysts thought that the merger of CNET and ZDNet would lead to redundancy in their product offerings, but research revealed that their target audiences had just 25 percent overlap.\n\nIn 2001, Ziff Davis Media Inc. reached an agreement with CNET Networks Inc. and ZDNet to regain the URLs lost in the 2000 sale of Ziff Davis Inc, to Softbank Corp.\n\nIn 2002, CNET Networks launched ZDNet sister site Builder.com, a site intended for enterprise software developers. On July 7, 2002, CNET Networks acquired Newmediary for its database of more than 30,000 enterprise IT white papers. ZDNet had integrated its services into its \"Business & Technology\" channel as early as January 2001.\n\nIn 2003, CNET Networks redesigned and relaunched ZDNet as an enterprise-focused publication intended to help business executives make better technology decisions.\n\nThe entire site was realigned as part of a CNET Networks B2B portfolio that included CNET News.com, Builder.com and TechRepublic.\n\nA \"Tech Update\" section was created to serve as a directory of proprietary IT research (dubbed \"IT Priorities\"), and a new \"Power Center\" was implemented to prominently feature webcasts, white papers and case studies from partners. ZDNet also offered eight enterprise-targeted newsletters, as well launched its first blogs.\n\nIn 2005, ZDNet Government was launched as the brand's first industry vertical, with a mission to cater to IT professionals in the public sector. Editorial features included writing by former Utah CIO Phil Windley, TechRepublic columnist Ramon Padilla and CNET News reporter Declan McCullagh. ZDNet also launched its first original podcasts in 2005.\n\nIn 2006, ZDNet experienced another redesign that reduced its editorial focus on traditional news articles and product reviews and emphasized a growing network of expert bloggers, now totaling more than 30. The blogs covered topics such as enterprise IT, open source, Web 2.0, Google, Apple and Microsoft, and featured journalists David Berlind, Mary Jo Foley and Larry Dignan.\n\nOn February 19, 2008, Larry Dignan was appointed editor-in-chief of ZDNet and editorial director of TechRepublic, replacing Dan Farber, who became editor-in-chief of CNET News.com.\n\nOn May 17, 2008, CBS Corporation announced that it would acquire CNET Networks for approximately $1.8 billion. The entire company would be organized under its CBS Interactive division.\n\nIn May 2010, ZDNet redesigned its site to place emphasis on the topics its blog network covers—now \"Companies,\" \"Hardware,\" \"Software,\" \"Mobile,\" \"Security\" and \"Research\"—and de-emphasize the downloads and reviews it imported from CNET post-merger.\n\nZDNet currently operates a network of about 50 blogs loosely aligned by its major verticals: companies, hardware, software, mobile, security and IT research. Within those general areas are blogs on gadgets, management strategy, social media, datacenters, technology law, SOA, healthcare, CRM, virtualization and sustainability.\n\nThe site still offers product reviews and software downloads, which are mostly imported from CNET. It maintains an extensive database of enterprise-focused white papers.\n\nAt the 14th Annual Computer Press Awards in 1999, ZDNet was adjudged the Best Overall Online Site.\n\nIn 2007, the Association of Online Publishers awarded ZDNet UK under the Business Website category for its contribution to innovation in incorporating Web 2.0 and community features effectively on its site.\n\nA Japanese news publishing company called Asahi Interactive owns the ZDNet Japan website. According to alexa.com statistics, the Japan.zdnet.com subdomain is the second most visited on ZDNet, after the blogs subdomain. Also, the ZDNet website has an overall traffic rank of 558 in Japan.\n\nThe ZDNet UK Live feature displays real time news updates and comments on the website and on social media including Twitter.\n\nOther country editions include Australia, Asia, Belgium, China, Germany, Netherlands, UK and France, in their native languages.\n"}
