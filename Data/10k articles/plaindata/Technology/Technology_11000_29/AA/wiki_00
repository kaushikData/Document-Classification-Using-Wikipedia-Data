{"id": "29555753", "url": "https://en.wikipedia.org/wiki?curid=29555753", "title": "ADS Group", "text": "ADS Group\n\nADS Group Limited, informally known as ADS, is the trade organisation representing the aerospace, defence, security and space industries in the United Kingdom. It has more than 1000 member companies across its sectors, including some of the UK's largest manufacturers, like Airbus, Rolls Royce, BAE Systems, and Bombardier.\n\nIts subsidiary Farnborough International Limited runs the biennial Farnborough Air Show, one of the world's biggest air shows, and other events.\n\nADS Group Limited was incorporated on 11 September 2009 as a private, limited by guarantee, no share capital company registered in the United Kingdom with company number 01765250.\n\nIt was formed from the merger of the Association of Police and Public Security Suppliers (APPSS), the Defence Manufacturers Association (DMA) and the Society of British Aerospace Companies (SBAC).\n\nADS activities are focussed on contributing to policy debates of importance to its industries, supporting UK manufacturing and its industries supply chains, encouraging investments in technology and innovation, supporting business development, and enhancing the profile of its sectors. Its current published priority issues are around developing industrial strategy, productivity supply chain improvements, industrial innovation, public procurement, and sectoral priorities for negotiations on the UK's exit from the European Union.\n\nFarnborough International Limited, which runs the Farnborough Airshow, is a wholly owned subsidiary of ADS. ADS also encompasses the British Aviation Group (BAG).\n\nThe organisation participates in joint initiatives between industry and Government, including the Aerospace Growth Partnership and Defence Growth Partnership. It leads the National Aerospace Technology Exploitation Programme (NATEP), supported by funding from the Aerospace Technology Institute, aimed at helping UK supply chain companies develop innovative technologies.\n\nADS is the largest member and contributor to the Aerospace and Defence Industries Association of Europe (ASD), the trade association representing Aerospace, Defence and Space in Europe.\n\nIt publishes a quarterly magazine covering its four sectors, ADS Advance, and annual economic outlook reports on the UK aerospace, defence and security sectors.\n\nADS coordinates UK Pavilions for UK companies exhibiting at international shows such as Paris Air Show and Singapore Air Show. The 2017 Paris Air Show saw 88 UK companies exhibiting in the UK Pavilion, while 150 companies exhibited in the 2017 UK Pavilion at DSEI.\n\nIt also runs the annual United Kingdom Aerospace Youth Rocketry Challenge, a competition for secondary school student teams. The UK national champions compete in the International Rocketry Challenge against winners of the US, French and Japanese competitions. The International Rocketry Challenge final is held at the Farnborough Airshow or Paris Air Show in alternating years.\n\nADS holds an Annual Dinner for member companies and invited guests, and since 2015 has held an annual reception in Parliament for apprentices from across its member companies.\n\nADS has more than 30 Special Interest Groups formed of members with similar professional specialisms. All groups deliver activities that are led by members, and cover areas including aerospace exports, aircraft interiors, maritime, training and simulation, and environmental issues.\n\nThe head office is situated on the Albert Embankment (A3036), on the opposite side of the Thames to Millbank Tower, and next to the headquarters of the London Fire Brigade and the UN's International Maritime Organization, south of Lambeth Bridge.\n\nIt also has offices at \n\nADS is overseen by the ADS Board, comprising the President, Chief Executive and representatives of member companies. Day-to-day operations are the responsibility of the Executive Team. A number of Boards and Committees oversee specific areas of work, such as membership, business ethics, and civil aviation safety.\n\nThe current Chief Executive is Paul Everitt, who succeeded Ian Godden in 2013. The current President is Colin Smith, Chairman of the Aerospace Growth Partnership and former Director of Engineering and Technology at Rolls Royce, who took up the post in January 2018.\n\nMembership is made up of more than 1000 UK registered businesses. Companies include:\n\nEach year ADS hosts an Annual Dinner for member companies from across its four sectors, and invited guests from across industry, politics, the armed forces, government and the media.\n\nThe 2018 Annual Dinner, the 9th edition of the event, was held on 15 January in London. It drew more than 900 attendees, with speeches from ADS President Colin P. Smith, and Airbus Chief Executive Tom Enders. A charity collection for SSAFA – the Armed Forces Charity, raised more than £13,000 in donations.\n\nThe 8th ADS Annual Dinner in 2017 was addressed by ADS & Airbus President Paul Kahn - who spoke about the impact of Brexit on the ADS industries and about the Single Source Regulatory Office (SSRO) - and by former UK foreign secretary William Hague.\n\n\n\n"}
{"id": "35754785", "url": "https://en.wikipedia.org/wiki?curid=35754785", "title": "ASTech", "text": "ASTech\n\nASTech Paris Région is a French cluster of aerospace engineering companies and research centres. It is in the region of Île-de-France in the middle of France and is mainly concentrated in and around the city of Paris.\n\nThere are over 500 companies, including Dassault Aviation, EADS and Air France Industries. About 100,000 people work there in the aviation and space flight industries.\n\nThe headquarters of ASTech is located in Meudon, a town in Hauts-de-Seine department. The chairman of the cluster is Gérard Laruelle.\n\n"}
{"id": "7393754", "url": "https://en.wikipedia.org/wiki?curid=7393754", "title": "Advanced CCD Imaging Spectrometer", "text": "Advanced CCD Imaging Spectrometer\n\nThe Advanced CCD Imaging Spectrometer (ACIS), formerly the AXAF CCD Imaging Spectrometer, is an instrument built by a team from the Massachusetts Institute of Technology's Center for Space Research and the Pennsylvania State University for the \"Chandra X-ray Observatory\".\n\nACIS is a focal plane instrument that uses an array of charge-coupled devices. It serves as an X-ray integral field spectrograph for \"Chandra\". The instrument is capable of measuring both the position and energy of incoming X-rays.\n\nThe CCD sensors of ACIS operate at and its filters at . It carries a special heater that allows contamination from \"Chandra\" to be baked off; the spacecraft contains lubricants, and the ACIS design took this into account in order to clean its sensors. Contamination buildup can reduce the instrument's sensitivity. Radiation in space is another potential danger to the sensor.\n\n, after 15 years of operation, there was no indication of a limit to the lifetime of ACIS. Another design feature of the instrument was a calibration source that can be used to understand its health. This allows for a measurement of the level of contamination, if present, as well as any degree of charge transfer inefficiency.\n\n"}
{"id": "37342352", "url": "https://en.wikipedia.org/wiki?curid=37342352", "title": "Anca Mosoiu", "text": "Anca Mosoiu\n\nAnca Mosoiu (born 1973) is a technical consultant and web developer who established the Oakland, California co-working space Tech Liminal in 2009. She is credited with helping to build the tech industry in Oakland. She has been described as a \"tech guru\".\n\nMosoiu moved to the United States from Romania with her family in 1983 when she was 9 years old. Her family had been granted political asylum \"before the revolution toppled the Soviet-era autocrat Nicolae Ceausescu\". She grew up in Oakland attending local middle schools.\n\nMosoiu attended Massachusetts Institute of Technology, and earned her bachelor's degree in computer science in 1995.\n\nAs a technical consultant Mosoiu spent a lot of time traveling and in hotel rooms. Because she wanted to work with people and be part of a community, Mosoiu founded Tech Liminal. Tech Liminal was the first technology co-working space in Oakland.\n\n\"It's easier to have more impact in Oakland with little more than gumption,\" Mosoiu said. \"It feels like we're all in this together.\"\n\n"}
{"id": "529449", "url": "https://en.wikipedia.org/wiki?curid=529449", "title": "Audio control surface", "text": "Audio control surface\n\nIn the domain of digital audio, a control surface is a human interface device (HID) which allows the user to control a digital audio workstation or other digital audio application. Generally, a control surface will contain one or more controls that can be assigned to parameters in the software, allowing tactile control of the software. As digital audio software is complex and can play any number of functions in the audio chain, control surfaces can be used to control many aspects of music production, including virtual instruments, samplers, signal processors, mixers, DJ software, and music sequencers.\n\nSince control surfaces are designed to perform different functions, they vary widely in size, shape and number and type of controls. A basic control surface for mixing resembles a traditional analogue mixing console, featuring faders, knobs (rotary encoders), and buttons that can be assigned to parameters in the software. Other control surfaces are designed to give a musician control over the sequencer while recording, and thus provide transport controls (remote control of record, playback and song position). Control surfaces are often incorporated into MIDI controllers to give the musician more control over an instrument. Control surfaces with motorized faders can read and write mix automation.\n\nThe control surface connects to the host computer via many different interfaces. MIDI was the first major interface created for this purpose, although many devices now use USB, FireWire, or Ethernet.\n\n\n"}
{"id": "1422141", "url": "https://en.wikipedia.org/wiki?curid=1422141", "title": "Common traffic advisory frequency", "text": "Common traffic advisory frequency\n\nCommon traffic advisory frequency (CTAF) is the name given to the VHF radio frequency used for air-to-air communication at US, Canadian and Australian non-towered airports.\n\nMany towered airports close their towers overnight, keeping the airport itself open for cargo operations and other activity. Pilots use the common frequency to coordinate their arrivals and departures safely, giving position reports and acknowledging other aircraft in the airfield traffic pattern.\n\nIn many locations, smaller airports use pilot-controlled lighting systems when it is uneconomical or inconvenient to have automated systems or staff to turn on the taxiway and runway lights. In Canada, the lighting system is accessed through an aircraft radio control of aerodrome lighting (ARCAL) frequency which is often shared with the CTAF.\n\nTwo common CTAF allocations are UNICOM, a licensed non-government base station that provides air-to-ground communications (and vice versa) and may also serve as a CTAF when in operation, and MULTICOM, a frequency allocation (without a physical base station) that is reserved as a CTAF for airports without other facilities.\n\nIn Australia, there are many landing strips in remote locations that have CTAF operations 24 hours a day, seven days a week. There are also CTAF(R) landing strips which require the aircraft intending to enter the area of operation to be fitted with a radio. The most common CTAF frequency is 126.7 MHz at non towered aerodromes except for when two CTAF airports are near each other. Aerodromes using CTAF outside tower hours typically nominate a frequency that is used during tower hours.\n\nUNICOM and a CTAF may be mutually exclusive, but this is not always the case. In the United States, many non-towered airports use the same frequency for both Unicom and CTAF purposes. Pilots are advised to check their sectional charts and/or Chart Supplement (formerly Airport/Facilities Directory) guides to determine the appropriate frequency for CTAF prior to operating at any given airport.\n\nUnlicensed aerodromes in the UK often recommend pilots communicate with each other using SAFETYCOM, currently 135.475 MHz. However, most gliding clubs use the Glider Ground Station Common Field Frequency, currently 129.975 MHz.\n\n"}
{"id": "58573677", "url": "https://en.wikipedia.org/wiki?curid=58573677", "title": "Compost heater", "text": "Compost heater\n\nA compost heater (or Biomeiler) is a structure for the energetic use of biomass for the heating of buildings.\n\nThe method was developed by Jean Pain in the 1970s. Compost heaters are used primarily for demonstration purposes as small systems for heating a house. Local waste can be converted to energy.\n\nThe traditional compost heater exploits the heat of a large compost heap to warm a house. This type requires a big heap, intertwined with a spiral water hose. The circulating water conducts heat to the building, where it can be fed to a heating circuit.\n\nThe heap must contain at least 8,000 liters of biomass to maintain a temperature during the winter.\n\nFor this purpose, chipped wood is usually piled up and a water hose is passed through it. A microbiological degradation process generates heat for up to 24 months. The heat produces hot water, which is then fed to a heating circuit. With sufficient oxygen supply, the biomass is degraded by aerobic decomposition. The activity of the microorganisms can be regulated by the moisture content.\n\nPain's 'Biomeiler' combines composting with biogas.\n\nThe raw materials of Pain's compost heap were saplings, branches and underbrush. He developed the machines that grind these materials to the proper size. One of his machines, a tractor-driven model, earned fourth prize in the 1978 Grenoble Agricultural Fair. After Pain had ground the raw materials, Pain would construct a heap three metres high and six metres across (10 × 20 feet). The heap weighed approximately , and was mounted over a steel tank. This tank was 3/4 full of compost, which had first been steeped in water for two months. The hermetically sealed tank was connected by tubing to 24 truck tire inner tubes, gathered nearby to collect the methane gas. The gas was distilled by washing and compressing it through small stones in water. Pain used the gas for cooking and producing electricity. He also fueled a light van. Pain estimated that of brushwood would supply the gas equivalent of . of petrol.\n\nIt took about 90 days to produce of gas - enough to keep two ovens and three burner stoves going for a year. The methane-fueled combustion engine drove a generator that produced 100 watts of electricity. This charged a battery, providing the light needed. Skepticism has been leveled at Pain's estimates for methane extraction and it is not known if anyone has been able to reproduce his results.\n\nPain's compost heaps generated hot water via of pipe buried inside the compost mound. The pipe was wrapped around the methane generator with an inlet for cold water and an outlet for hot water. The heat from the decomposing mass produced of hot water heated to — enough to meet central heating, bathroom and kitchen requirements. The heap composted for nearly 18 months, after which it was dismantled. The humus was used to mulch soils. \n\nThe composting process runs in an airtight container inside a house. The heat can be radiated directly to the interiors of the house or distributed by a heating circuit. An additional water pipe can be vertically built into the silo for warming water.\n\nThe silo is the central part of an in-house compost heater. In autumn the silo is filled up with fresh biomass, after which the silo delivers comfortable heat throughout the winter.\n\nInlet air and outlet air provide the necessary oxygen. The outlet air goes out of the house. Silo moisture is higher than in a regular compost heap. The decay process produces additional water, which is drained at the bottom of the silo. A part of this water enters the top of the silo for better distribution in the processing volume. If pumped periodically or continuously to the top to rinse through the silo, the whole system becomes a \"wet composting\" system.\n"}
{"id": "44891333", "url": "https://en.wikipedia.org/wiki?curid=44891333", "title": "Cool warehouse", "text": "Cool warehouse\n\nA cool warehouse or cold storage warehouse is a warehouse were perishable goods are stored and refrigerated. Products stored can be, amongst other things, food, especially meat, other agricultural products, pharmaceutical drugs, other chemicals and blood.\n"}
{"id": "183554", "url": "https://en.wikipedia.org/wiki?curid=183554", "title": "Cucoloris", "text": "Cucoloris\n\nIn lighting for film, theatre and still photography, a cucoloris (occasionally also spelled cuculoris, kookaloris, cookaloris or cucalorus) is a device for casting shadows or silhouettes to produce patterned illumination. It is normally referred to as a \"cookie\" or sometimes as a \"kook\" or a \"coo-koo\". The cucoloris is used to create a more natural look by breaking up the light from a man-made source. It can be used to simulate movement by passing shadows or light coming through a leafy canopy.\n\nThe etymology of the word is opaque, bearing a number of plausible origins. Grant Barett, a co-host of the radio show \"A Way with Words\", suggested that the phrase is an eponym of George Cukor. Moreover, a specious claim cited by \"Directing and Producing Television\" maintained that the term arose from the Greek 'kukaloris', breaking of light, 'loris' conceivably cognate with 'luo', 'I break'.\n\nGenerally, cookies fall into three groups: hard cookies, made from thin plywood or heavy poster board with random shapes cut out of the body; soft cookies (often called \"celo\" cookies), made from plastic impregnated screen (the same screen one might find in a storm window), also with random shapes cut or burned out; and \"brancholorises\" or \"dingles\", which are simply tree limbs or other available things that can be placed between the light and the subject.\n\nMany \"old-school\" grips would say that any unnatural pattern used to create a shadow is a cookie.\n\nCucolorises are sometimes thought of as a subset of the gobo category. Cucolorises differ from standard gobos in that they are used farther away from the lighting instrument, and therefore do not need to be as heat resistant. Cuculorises generally produce softer edges than gobos.\n\nA similar technique to using a cookie is simulated in 3D computer graphics, where using an alpha map as a cookie (sometimes called a \"light texture\") to cast shadows on 3D objects is simulated by applying an alpha texture to an emitting light source in the 3D scene, typically a spot light type or a directional light type, to serve as a virtual cookie that projects shadows onto 3D object(s) by emitting light only through the transparent or translucent parts of the alpha texture, thus simulating the effect of a cucoloris as used in its real-world counterpart. This effect is commonly used in both 3D computer-generated animation and video games.\n\nCinematographer George J. Folsey, ASC thus recounted the history: \nWhile shooting a scene with an actor who was wearing a white shirt, he wanted to separate the skin tones on the actor’s face from the hue of the shirt. Folsey told a grip to hold a stepladder in front of a key light to create a shadow on the actor’s shirt. The closer that the ladder was held to the light, the softer and less defined the shadow became. The grip eventually tired of holding the ladder, so he cut a grill with the same pattern in a sheet of light wood. One day, Folsey visited Hal Rosson, ASC, who was shooting on another set. In the scene, an actress was lying on a bed swathed in white sheets. Rosson used Folsey’s wooden grill to create some shadows, which made the scene more dramatic. Later, while shooting a similar situation, Rosson asked Folsey, “Where’s that kookaloris thing?” Kodak: The Essential Reference Guide for Filmmakers\n\nGobo (lighting)\n\n \n"}
{"id": "47959004", "url": "https://en.wikipedia.org/wiki?curid=47959004", "title": "DFM analysis for stereolithography", "text": "DFM analysis for stereolithography\n\nIn design for additive manufacturing (DFAM), there are both broad themes (which apply to many additive manufacturing processes) and optimizations specific to a particular AM process. Described here is DFM analysis for stereolithography, in which design for manufacturability (DFM) considerations are applied in designing a part (or assembly) to be manufactured by the stereolithography (SLA) process. In SLA, parts are built from a photocurable liquid resin that cures when exposed to a laser beam that scans across the surface of the resin (photopolymerization). Resins containing acrylate, epoxy, and urethane are typically used. Complex parts and assemblies can be directly made in one go, to a greater extent than in earlier forms of manufacturing such as casting, forming, metal fabrication, and machining. Realization of such a seamless process requires the designer to take in considerations of manufacturability of the part (or assembly) by the process. In any product design process, DFM considerations are important to reduce iterations, time and material wastage.\n\nExcessive setup specific material cost and lack of support for 3rd party resins is a major challenge with SLA process:. The choice of material (a design process) is restricted by the supported resin. Hence, the mechanical properties are also fixed. When scaling up dimensions selectively to deal with expected stresses, post curing is done by further treatment with UV light and heat. Although advantageous to mechanical properties, the additional polymerization and cross linkage can result in shrinkage, warping and residual thermal stresses. Hence, the part shall be designed in its 'green' stage i.e. pre-treatment stage.\n\nSLA process is an additive manufacturing process. Hence, design considerations such as orientation, process latitude, support structures etc. have to be considered.\nOrientation affects the support structures, manufacturing time, part quality and part cost. Complex structures may fail to manufacture properly due to orientation which is not feasible resulting in undesirable stresses. This is when the DFM guidelines can be applied. Design feasibility for stereolithography can be validated by analytical as well as on the basis of simulation and/or guidelines \n\nRule-based considerations in DFM refer to certain criteria that the part has to meet in order to avoid failures during manufacturing. Given the layer-by-layer manufacturing technique the process follows, there isn't any constraint on the overall complexity that the part may have. But some rules have been developed through experience by the printer developer/academia which must be followed to ensure that the individual features that make up the part are within certain 'limits of feasibility'.\n\nConstraints/limitations in SLA manufacturing comes from the printer's accuracy, layer thickness, speed of curing, speed of printing etc. Various printer constraints are to be considered during design such as:\n\nA point needs support if:\nWhile printing, support structures act as a part of design hence, their limitations and advantages are kept in mind while designing. Major considerations include:\n\nPart orientation is a very crucial decision in DFM analysis for SLA process. The build time, surface quality, volume/number of support structures etc. depend on this. In many cases, it is also possible to address the manufacturability issues just by reorienting the part. For example, an overhanging geometry with shallow angle may be oriented to ensure steep angles. Hence, major considerations include:\n\nPlan-based considerations in DFM refer to criteria that arise due to process plan. These are to be met in order to avoid failures during manufacturing of a part that may be satisfy the rule-based criteria but may have some manufacturing difficulties due to sequence in which features are produced.\n\nGeometric Tailoring bridges the mismatch of material properties and process differences described above. Both functionality and manufacturability issues are addressed. Functionality issues are addressed through 'tailoring' of dimensions of the part to compensate the stress and deflection behavior anomalies. Manufacturability issues are tackled through identification of difficult to manufacture geometric attributes (an approach used in most DFM handbooks) or through simulations of manufacturing processes. For RP-produced parts (as in SLA), the problem formulations are called material-process geometric tailoring (MPGT)/RP.\nFirst, the designer specifies information such as: Parametric CAD model of the part; constraints and goals on functional, geometry, cost and time characteristics; analysis models for these constraints and goals; target values of goals; and preferences for the goals.\nDFM problem is then formulated as the designer fills in the MPGT template with this information and sends to the manufacturer, who fills in the remaining 'manufacturing relevant' information. With the completed formulation, the manufacturer is now able to solve the DFM problem, performing GT of the part design. Hence, the MPGT serves as the digital interface between the designer and the manufacturer.\nVarious Process Planning (PP) strategies have been developed for geometric tailoring in SLA process.\n\nThe constraints imposed by the manufacturing process are mapped onto the design. This helps in identification of DFM problems while exploring process plans by acting as a retrieval method. Various DFM frameworks are developed in literature. These frameworks help in various decision making steps such as:\n\n\n"}
{"id": "2596487", "url": "https://en.wikipedia.org/wiki?curid=2596487", "title": "Diamond plate", "text": "Diamond plate\n\nDiamond plate, also known as checker plate, tread plate and Durbar floor plate, is a type of metal stock with a regular pattern of raised diamonds or lines on one side, with the reverse side being featureless. Diamond plate is usually steel, stainless steel or aluminum. Steel types are normally made by hot rolling, although modern manufacturers also make a raised and pressed diamond design.\n\nThe added texture reduces the risk of slipping, making diamond plate a solution for stairs, catwalks, walkways, and ramps in industrial settings. Its non-skid properties mean that diamond plate is frequently used on the interior of ambulances and on the footplates of firetrucks. Additional applications include truck beds and trailer floors. \n\nDiamond plate can also be used decoratively, particularly highly polished aluminum variants. Manufactured in plastic, diamond plate is marketed as an interlocking tile system to be installed on garage floors, trailers, and exercise rooms.\n\nDiamond plate may be used for surface protection against damage from foot traffic or harmful chemicals. Manufactured with polymer variants, inter-locking diamond plate tile is used in areas with high surface-erosive traffic.\n\nTata Steel marketed the plate as Durbar Plate.\n\n\"Diamond plate\" can also refer to similar anti-slip textures.\n\n"}
{"id": "2279628", "url": "https://en.wikipedia.org/wiki?curid=2279628", "title": "Dynamic currency conversion", "text": "Dynamic currency conversion\n\nDynamic currency conversion (DCC) or cardholder preferred currency (CPC) is a process whereby the amount of a Visa or MasterCard transaction is converted by a merchant or ATM to the currency of the payment card's country of issue at the point of sale.\n\nDCC allows the merchant, merchant's bank or ATM operator to charge a markup on the exchange rate used, sometimes by as much as 18%. Where the DCC markup is less than the card issuer's currency conversion fee, DCC can benefit card holders by allowing them to see the amount that their card will be charged expressed in the currency of the card's country of issue. However, in most cases, DCC markups are higher than the card issuer's currency conversion fee (which can be zero), thereby negating this benefit.\n\nDCC services are generally provided by third party operators in association with the merchant, and not by a card issuer. Card issuers do not provide cardholders with a DCC option at the point of sale, but these two card networks permit DCC operators to offer currency conversion in accordance with their card processing rules.\n\nWithout DCC, the currency conversion is carried out by the card issuer when the transaction is charged to the card holder's statement, usually a day or two later, but for an increasing number of cards in real time. Even though the card issuer will publish the exchange rate used for conversion on the statement, most do not disclose the exchange rate used to convert a transaction at the time of payment. Both Visa and MasterCard state that the rates they publish in advance of a transaction posting to a cardholder's statement are indicative, since the rates they use for conversion correspond to the date and time \"they\" process the transaction, as opposed to the actual transaction date.\n\nWith DCC, the currency conversion is done by the merchant or their card processor at the point of sale. Unlike a credit card company, a DCC operator must disclose the exchange rate used for conversion at the time of the transaction according to credit card company rules which govern how DCC is offered. The DCC exchange rate must be based on a wholesale interbank rate, to which any additional markup is then applied. Visa requires this markup be disclosed to the cardholder. The credit card company may still charge an additional fee for charges made outside the card holder's home country, even when the transaction has been processed in their home currency with DCC.\n\nProponents of this service believe that customers can better understand prices in their home currency, and this makes it easier for business travelers to keep track of their expenses. They also point out that the customer has full transparency inclusive of conversion fees, and can make an informed choice whether or not to use DCC. The financial benefit to the merchant or their card processor may be an incentive for the merchant to offer DCC even when it would be disadvantageous to the customer.\n\nOpponents of the service believe that customers do not understand DCC, and point out that DCC exchange rate markups are mostly higher than the card issuers' currency conversion fees that DCC avoids, and therefore, in almost all cases, opting for DCC will result in a higher charge to the cardholder.\n\nA dynamic currency conversion service was offered in 1996 and commercialized by a number of companies including Monex financial services and FEXCO.\n\nPrior to the card schemes (Visa and MasterCard) imposing rules relating to DCC, cardholder transactions were converted without the need to disclose that the transaction was being converted into a customer's home currency, in a process known as \"back office DCC\". Visa and MasterCard now prohibit this practice and require the customer's consent for DCC, although many travelers have reported that this is not universally followed.\n\nVisa Chargeback reason code 76 explicitly covers situations where the \"Cardholder was not advised that Dynamic Currency Conversion (DCC) would occur\" or \"Cardholder was refused the choice of paying in the merchant’s local currency\". Customers have a strong chance of successfully disputing such transactions, especially in situations where they pay with a credit card and where Verified by Visa or Securecode is not involved.\nMastercard DCC Compliance team investigates the case \nand then notifies the Acquirer of their findings. If appropriate, the Acquirer \nwill also be asked to take action to remedy the complaint.\n\nMasterCard take seriously any complaint from a customer by investigating the case for compliance with DCC rules, and if the customer was not given a choice in a DCC transaction, the bank's customer has the possibility to refund the customer with a chargeback sent to the bank's merchant.\n\nDCC is only available at merchants that have signed up for the facility with a DCC provider.\n\nWhen a customer is ready to pay for a transaction and chooses to pay with a payment card, the point-of-sale terminal of a DCC merchant will determine the card's country of issue from the card's issuer identification number (first 6 digits of the card number). If it detects a foreign card is being used, then the transaction will be routed through the DCC provider, which may offer DCC to the customer. \n\nIf DCC is being offered, the terminal will also display the transaction amount in the customer’s home currency. Visa and MasterCard require the DCC provider to disclose the exchange rate and margin to the cardholder, but not all merchants comply with this obligation, and other card issuers do not have that obligation. The cardholder can then select whether the transaction is to be processed in the local or home currency.\n\nIf the cardholder chooses to pay in their home currency, the DCC provider will cause the cardholder’s account to be debited by the transaction amount in the home currency, and the merchant's account to be credited with the amount in the local currency. At regular periods, usually monthly, the merchant would also be credited with the commission for DCC transactions. The exchange rate risk is borne by the DCC provider, which may carry that risk or set up some hedging arrangement to minimise or transfer that risk. \n\nSome card issuers impose an additional foreign transaction fee on DCC transactions, even though they are denominated in the card’s home currency.\n\nAn example of the difference with DCC can be seen in the following image, where the same GBP purchase is made twice just after each other: one with DCC and one without DCC. In both cases, the original amount is GB£6.90 and is paid with a Visa card denominated in EUR. \n\nThe difference in charges can be seen on the customer’s card statement. With DCC (left part of the above image), the amount becomes EUR 8.20, at an exchange rate of 1.1882. The DCC provider, in this example the merchant itself, also disclosed that it is using the Reuters Wholesale Interbank exchange rate plus 2.95%. Without DCC, the amount can vary with fluctuations between the GBP and EUR currencies, but on the date of this transaction it was EUR 8.04.\n\nIn this example, difference is just over 2%. Though this difference may seem a small amount for the customer, it can result in a big income stream for the DCC provider and merchant. One should also realise that even without DCC the card issuer converts the transaction amount using its own exchange rates and margins, which in this example was 1.16522.\n\nThe merchant’s point-of-sale terminal can only detect the card’s country of issue and not the currency of the account that is to be selected. The DCC makes an assumption that the account home currency is the currency of the card's country of issue. This assumption can result in DCC being offered incorrectly. For example, a DCC-enabled terminal in the Eurozone will offer DCC to a customer paying with a debit card issued in the United Kingdom on a euro bank account. If the customer mistakenly chooses DCC, then the transaction will first be converted from EUR to GBP by the DCC provider, and then from GBP back to EUR by the UK card issuer, often with its markup.\n\nThere have reported cases of point-of-sale terminals allowing merchants to change the transaction amount and currency after the cardholder has entered their PIN and handed the terminal back to the merchant. In this scenario, DCC is carried out without the cardholder's consent, even though the receipt subsequently printed states falsely that the cardholder has given their consent.\n\nDCC operates similarly with Internet transactions. When payment card information is entered to finalize payment, the system can detect the home country of the cardholder and offer the cardholder the option of paying in their home currency. \n\nMany commercial websites can detect the country from which a query has come and quote prices in terms of the enquirer’s country. Often the prices in the local currency of the supplier are not indicated, and the exchange rate used to convert prices is often also not disclosed.\n\nDCC is also sometimes available for cash withdrawals at ATMs.\n\nDCC has proved popular with merchants because it enables them to profit from the foreign exchange conversion that occurs during the payment process for a foreign denominated credit card.\n\nCredit card acquirers and payment gateways will also take a profit on the foreign exchange conversion that occurs during the payment process for foreign denominated credit card when DCC is used. DCC revenue has been important for them because it offsets increasing international interchange fees.\n\nThe main advantage of DCC is that for a non-DCC transaction the customer does not know exactly the exchange rate that the credit card company will apply (and the final cost) until the transaction is cleared, so the actual rate is not known to the customer until it appears on a monthly statement.\n\nOther advantages to customers, according to proponents, are:\n\nFor the merchant which normally accepts credit cards, DCC offers an opportunity to earn a margin on the transaction with no exchange rate risk, which is borne by the DCC operator.\n\nThe main objection to DCC is the unfavorable exchange rates and fees being applied on the transaction, resulting in a higher charge on their credit card, and that in many cases the customer is not aware of the additional and often unnecessary cost of the DCC transaction.\n\nThe size of the foreign exchange margin added using DCC varies depending on the DCC operator, card acquirer or payment gateway and merchant. This margin is in addition to any charges levied by the customer's bank or credit card company for a foreign purchase. In most cases, customers are charged more using DCC than they would have been if they had simply paid in the foreign currency.\n\n\nThe main DCC providers are:\n\n\n"}
{"id": "3245297", "url": "https://en.wikipedia.org/wiki?curid=3245297", "title": "Egyptian pyramid construction techniques", "text": "Egyptian pyramid construction techniques\n\nEgyptian pyramid construction techniques are the controversial subject of many hypotheses. These techniques seem to have developed over time; later pyramids were not constructed in the same way as earlier ones. Most of the construction hypotheses are based on the belief that huge stones were carved from quarries with copper chisels, and these blocks were then dragged and lifted into position. Disagreements chiefly concern the methods used to move and place the stones. \n\nIn addition to the many unresolved arguments about the construction techniques, there have been disagreements as to the kind of workforce used. The Greeks, many years after the event, believed that the pyramids must have been built by slave labor. Archaeologists now believe that the Great Pyramid of Giza (at least) was built by tens of thousands of skilled workers who camped near the pyramids and worked for a salary or as a form of tax payment (levy) until the construction was completed, pointing to workers' cemeteries discovered in 1990 by archaeologists Zahi Hawass and Mark Lehner. For the Middle Kingdom Pyramid of Amenemhat II, there is evidence from the annal stone of the king that foreigners from Canaan were used.\n\nDuring the earliest period, pyramids were constructed wholly of stone. Locally quarried limestone was the material of choice for the main body of these pyramids, while a higher quality of limestone quarried at Tura (near modern Cairo) was used for the outer casing. Granite, quarried near Aswan, was used to construct some architectural elements, including the portcullis (a type of gate) and the roofs and walls of the burial chamber. Occasionally, granite was used in the outer casing as well, such as in the Pyramid of Menkaure. In the early pyramids, the layers of stone (called \"courses\") forming the pyramid body were laid sloping inwards; however, this configuration was found to be less stable than simply stacking the stones horizontally on top of each other. The Bent Pyramid at Dahshur seems to indicate acceptance of a new technique at a transition between these two building techniques. Its lower section is built of sloping courses while in its upper section the stones are laid horizontally.\n\nDuring the Middle Kingdom, pyramid construction techniques changed again. Most pyramids built then were little more than mountains of mud brick encased in a veneer of polished limestone. In several cases, later pyramids were built on top of natural hills to further reduce the volume of material needed in their construction. The materials and methods of construction used in the earliest pyramids have ensured their survival in a generally much better state of preservation than for the pyramid monuments of the later pharaohs.\n\nOne of the major problems faced by the early pyramid builders was the need to move huge quantities of stone. The Twelfth Dynasty tomb of Djehutihotep has an illustration of 172 men pulling an alabaster statue of him on a sledge. The statue is estimated to weigh 60 tons and Denys Stocks estimated that 45 workers would be required to start moving a lubricated block, or eight workers to move a block. Dr R H G Parry has suggested a method for rolling the stones, using a cradle-like machine that had been excavated in various new kingdom temples. Four of those objects could be fitted around a block so it could be rolled easily. Experiments done by the Obayashi Corporation, with concrete blocks square by long and weighing , showed how 18 men could drag the block over a 1-in-4 incline ramp, at a rate of . This idea was previously described by John Bush in 1977, and is mentioned in the \"Closing Remarks\" section of Parry's book. Vitruvius in \"De architectura\" described a similar method for moving irregular weights. It is still not known whether the Egyptians used this method but the experiments indicate it could have worked using stones of this size. Egyptologists generally accept this for the 2.5 ton blocks mostly used but do not agree over the methods used for the 15+ ton and several 70 to 80 ton blocks.\n\nAs the stones forming the core of the pyramids were roughly cut, especially in the Great Pyramid, the material used to fill the gaps was another problem. Huge quantities of gypsum and rubble were needed. The filling has almost no binding properties, but it was necessary to stabilize the construction. To make the gypsum mortar, it had to be dehydrated by heating which requires large quantities of wood. According to Egyptologists, the findings of both the 1984 and 1995 David H. Koch Pyramids Radiocarbon Projects may suggest that Egypt had to strip its forest and scrap every bit of wood it had to build the pyramids of Giza and other even earlier 4th Dynasty pyramids. Carbon dating samples from core blocks and other materials revealed that dates from the 1984 study averaged 374 years earlier than currently accepted and the 1995 dating averaging 100–200 years. As suggested by team members, \"We thought that it was unlikely that the pyramid builders consistently used centuries-old wood as fuel in preparing mortar. The 1984 results left us with too little data to conclude that the historical chronology of the Old Kingdom was wrong by nearly 400 years, but we considered this at least a possibility\". To explain this discrepancy, Egyptologists proposed the \"old wood\" theory claiming the earlier dates were possibly derived from recycling large amounts of centuries old wood and other earlier materials.\n\nThere is good information concerning the location of the quarries, some of the tools used to cut stone in the quarries, transportation of the stone to the monument, leveling the foundation, and leveling the subsequent tiers of the developing superstructure. Workmen probably used copper chisels, drills, and saws to cut softer stone, such as most of the limestone. The harder stones, such as granite, granodiorite, syenite, and basalt, cannot be cut with copper tools alone; instead they were worked with time-consuming methods like pounding with dolerite, drilling, and sawing with the aid of an abrasive, such as quartz sand. Blocks were transported by sledge likely lubricated by water. Leveling the foundation may have been accomplished by use of water-filled trenches as suggested by Mark Lehner and I.E.S. Edwards or through the use of a crude square level and experienced surveyors.\n\nThe diary of Merer, logbooks written more than 4,500 years ago by an Egyption official and found in 2013 by a French archeology team under the direction of in a cave in Wadi al-Jarf, describes the transportation of limestone from the quarry in Tora to Giza.\n\nThe unknowns of pyramid construction chiefly center on the question of how the blocks were moved up the superstructure. There is no known accurate historical or archaeological evidence that definitively resolves the question. Therefore, most discussion on construction methods involves functional possibilities that are supported by limited historical and archaeological evidence.\n\nHistorical accounts for the construction of the Egyptian pyramids do little to point definitively to methods to lift the blocks; yet most Egyptologists refer to these accounts when discussing this portion of pyramid construction. Thales, according to the philosopher Hieronymus (3rd century BC) visited the Egyptian pyramids during the 7th century BC and by using the intercept theorem, also known as Thales's theorem, measured their height and thus their volume. The first historical accounts of the construction of these monuments came centuries after the era of pyramid construction, by Herodotus in the 5th century BC and Diodorus Siculus in the 1st century BC. Herodotus's account states:\nDiodorus Siculus's account states:\nDiodorus Siculus's description of the shipment of the stone from Arabia is correct since the term \"Arabia\" those days implied the land between the Nile and the Red Sea where the limestone blocks have been transported from quarries across the river Nile. Both Herodotus's and Diodorus Siculus's writings are known to contain gross errors of fact, and Siculus is routinely accused of borrowing from Herodotus. Herodotus's description of slave labor is one of the most persistent myths of the construction process. However, these documents do give credit to both the levering and ramp methods.\n\nMost Egyptologists acknowledge that ramps are the most tenable of the methods to raise the blocks, yet they acknowledge that it is an incomplete method that must be supplemented by another device. Archaeological evidence for the use of ramps has been found at the Great Pyramid of Giza and other pyramids. The method most accepted for assisting ramps is levering (Lehner 1997: 222). The archaeological record gives evidence of only small ramps and inclined causeways, not something that could have been used to construct even a majority of the monument. To add to the uncertainty, there is considerable evidence demonstrating that non-standardized or \"ad hoc\" construction methods were used in pyramid construction (Arnold 1991: 98, Lehner 1997: 223).\n\nTherefore, there are many proposed ramps and there is a considerable amount of discrepancy regarding what type of ramp was used to build the pyramids. One of the widely discredited ramping methods is the large straight ramp, and it is routinely discredited on functional grounds for its massive size, lack of archaeological evidence, huge labor cost, and other problems (Arnold 1991: 99, Lehner 1997: 215, Isler 2001: 213).\n\nOther ramps serve to correct these problems of ramp size, yet either run into critiques of functionality and limited archaeological evidence. There are zig-zagging ramps, straight ramps using the incomplete part of the superstructure (Arnold 1991), spiraling ramps supported by the superstructure and spiraling ramps leaning on the monument as a large accretion are proposed. Mark Lehner speculated that a spiraling ramp, beginning in the stone quarry to the southeast and continuing around the exterior of the pyramid, may have been used. The stone blocks may have been drawn on sleds along the ramps lubricated by water or milk.\n\nLevering methods are considered to be the most tenable solution to complement ramping methods, partially due to Herodotus's description; and partially to the Shadoof; an irrigation device first depicted in Egypt during the New Kingdom, and found concomitantly with the Old Kingdom in Mesopotamia. In Lehner's (1997: 222) point of view, levers should be employed to lift the top 3% of the material of the superstructure. It is important to note that the top 4% of this material comprises of the total height of the monument. In other words, in Lehner's view, levers should be employed to lift a small amount of material and a great deal of vertical height of the monument.\n\nIn the milieu of levering methods, there are those that lift the block incrementally, as in repeatedly prying up alternating sides of the block and inserting a wooden or stone shims to gradually move the stone up one course; and there are other methods that use a larger lever to move the block up one course in one lifting procedure. Since the discussion of construction techniques to lift the blocks attempts to resolve a gap in the archaeological and historical record with a plausible functional explanation, the following examples by Isler, Keable, and Hussey-Pailos list experimentally tested methods. Isler's method (1985, 1987) is an incremental method and, in the Nova experiment (1992), used wooden shims or cribbing. Isler was able to lift a block up one tier in approximately one hour and 30 minutes. Peter Hodges's and Julian Keable's method is similar to Isler's method and instead used small manufactured concrete blocks as shims, wooden pallets, and a pit where their experimental tests were performed. Keable was able to perform his method in approximately 2 minutes. Scott Hussey-Pailos's (2005) method uses a simple levering device to lift a block up a course in one movement. This method was tested with materials of less strength than historical analogs (tested with materials weaker than those available in ancient Egypt), a factor of safety of 2, and lifted a 2500-pound block up one course in under a minute. This method is presented as a levering device to work complementary with Mark Lehner's idea of a combined ramp and levering techniques.\n\nHoudin's father was an architect who, in 1999, thought of a construction method that, it seemed to him, made more sense than any existing method proposed for building pyramids. To develop this hypothesis, Jean-Pierre Houdin, also an architect, gave up his job and set about drawing the first fully functional CAD architectural model of the Great Pyramid. His/their scheme involves using a regular external ramp to build the first 30% of the pyramid, with an \"internal ramp\" taking stones up beyond that height. The stones of the external ramp are re-cycled into the upper stories, thus explaining the otherwise puzzling lack of evidence for ramps.\n\nAfter 4 years working alone, Houdin was joined by a team of engineers from the French 3D software company Dassault Systemes, who used the most modern computer-aided design technology available to further refine and test the hypothesis, making it (according to Houdin) the only one proven to be a viable technique. In 2006 Houdin announced it in a book: \"Khufu: The Secrets Behind the Building of the Great Pyramid\", and in 2008 he and Egyptologist Bob Brier wrote a second one: \"The Secret of the Great Pyramid\"\n\nIn Houdin's method, each ramp inside the pyramid ended at an open space, a notch temporarily left open in the edge of the construction.(see diagram) This 10-square-meter clear space housed a crane that lifted and rotated each 2.5-ton block, to ready it for eight men to drag up the next internal ramp. There is a notch of sorts in one of the right places, and in 2008 Houdin's co-author Bob Brier, with a National Geographic film crew, entered a previously unremarked chamber that could be the start of one of these internal ramps. In 1986 a member of the French team (see below) saw a desert fox at this notch, rather as if it had ascended internally.\n\nHoudin's thesis remains unproven and in 2007, UCL Egyptologist David Jeffreys described the internal spiral hypothesis as \"far-fetched and horribly complicated\", while Oxford University's John Baines, declared he was \"suspicious of any theory that seeks to explain only how the Great Pyramid was built\". \n\nHoudin has another hypothesis developed from his architectural model, one that could finally explain the internal \"Grand Gallery\" chamber that otherwise appears to have little purpose. He believes the gallery acted as a trolley chute/guide for counterbalance weights. It enabled the raising of the five 60-ton granite beams that roof the King's Chamber. Houdin and Brier and the Dassault team are already credited with proving for the first time that cracks in beams appeared during construction, were examined and tested at the time and declared relatively harmless.\n\nMaterials scientist Joseph Davidovits has claimed that the blocks of the pyramid are not carved stone, but mostly a form of limestone concrete and that they were \"cast\" as with modern concrete. According to this hypothesis, soft limestone with a high kaolinite content was quarried in the wadi on the south of the Giza Plateau. The limestone was then dissolved in large, Nile-fed pools until it became a watery slurry. Lime (found in the ash of cooking fires) and natron (also used by the Egyptians in mummification) were mixed in. The pools were then left to evaporate, leaving behind a moist, clay-like mixture. This wet \"concrete\" would be carried to the construction site where it would be packed into reusable wooden moulds and in a few days would undergo a chemical reaction similar to the curing of concrete. New blocks, he suggests, could be cast in place, on top of and pressed against the old blocks. Proof-of-concept tests using similar compounds were carried out at a geopolymer institute in northern France and it was found that a crew of five to ten, working with simple hand tools, could agglomerate a structure of five, 1.3 to 4.5 ton blocks in a couple of weeks. He also claims that the Famine Stele, along with other hieroglyphic texts, describe the technology of stone agglomeration.\n\nDavidovits's method is not accepted by the academic mainstream. His method does not explain the granite stones, weighing well over 10 tons, above the King's Chamber, which he agrees were carved. Geologists have carefully scrutinized Davidovits's suggested technique and concluded his concrete came from natural limestone quarried in the Mokattam Formation. However, Davidovits alleges that the bulk of the soft limestone came from the same natural Mokkatam Formation quarries found by geologists, and insists that ancient Egyptians used the soft marly layer instead of the hard layer to re-agglomerate stones.\n\nDavidovits's hypothesis gained support from Michel Barsoum, a materials science researcher. Michel Barsoum and his colleagues at Drexel University published their findings supporting Davidovits's hypothesis in the \"Journal of the American Ceramic Society\" in 2006. Using scanning electron microscopy, they discovered in samples of the limestone pyramid blocks mineral compounds and air bubbles that do not occur in natural limestone.\n\nDipayan Jana, a petrographer, made a presentation to the ICMA (International Cement Microscopy Association) in 2007 and gave a paper in which he discusses Davidovits's and Barsoum's work and concludes \"we are far from accepting even as a remote possibility a 'man-made' origin of pyramid stones.\"\n\nIn 1997, Mark Lehner and stonemason Roger Hopkins conducted a three-week pyramid-building experiment for a \"NOVA\" television episode. They built a pyramid high by wide, consisting of a total of , or about 405 tons. It was made out of 186 stones weighing an average of 2.2 tons each. Twelve quarrymen carved 186 stones in 22 days, and the structure was erected using 44 men. They used iron hammers, chisels and levers (this is a modern shortcut, as the ancient Egyptians were limited to using copper and later bronze and wood). But Lehner and Hopkins did experiments with copper tools, noting that they were adequate for the job in hand, provided that additional manpower was available to constantly resharpen the ancient tools. They estimated they would have needed around 20 extra men for this maintenance. Another shortcut taken was the use of a front-end loader or fork lift truck, but modern machinery was not used to finish the construction. They used levers to lift the capstone to a height of . Four or five men were able to use levers on stones less than one ton to flip them over and transport them by rolling, but larger stones had to be towed. Lehner and Hopkins found that by putting the stones on wooden sledges and sliding the sledges on wooden tracks, they were able to tow a two-ton stone with 12 to 20 men. The wood for these sledges and tracks would have to have been imported from Lebanon at great cost since there was little, if any, wood in ancient Egypt. While the builders failed to duplicate the precise jointing created by the ancient Egyptians, Hopkins was confident that this could have been achieved with more practice.\n\nSome research suggests alternate estimates to the accepted workforce size. For instance, mathematician Kurt Mendelssohn calculated that the workforce may have been 50,000 men at most, while Ludwig Borchardt and Louis Croon placed the number at 36,000. According to Miroslav Verner, a workforce of no more than 30,000 was needed in the Great Pyramid's construction. Evidence suggests that around 5,000 were permanent workers on salaries with the balance working three- or four-month shifts in lieu of taxes while receiving subsistence \"wages\" of ten loaves of bread and a jug of beer per day. Zahi Hawass believes that the majority of workers may have been volunteers. It is estimated that only 4,000 of the total workforce were labourers who quarried the stone, hauled blocks to the pyramid and set the blocks in place. The vast majority of the workforce provided support services such as scribes, toolmakers and other backup services. The tombs of supervisors contain inscriptions regarding the organisation of the workforce. There were two crews of approximately 2,000 workers sub-divided into named gangs of 1,000. The gangs were divided into five phyles of 200 which were in turn split into groups of around 20 workers grouped according to their skills, with each group having their own project leader and a specific task.\n\nA construction management study carried out by the firm Daniel, Mann, Johnson, & Mendenhall in association with Mark Lehner, and other Egyptologists, estimates that the total project required an average workforce of 14,567 people and a peak workforce of 40,000. Without the use of pulleys, wheels, or iron tools, they used critical path analysis to suggest the Great Pyramid was completed from start to finish in approximately 10 years. Their study estimates the number of blocks used in construction was between 2 and 2.8 million (an average of 2.4 million), but settles on a reduced finished total of 2 million after subtracting the estimated volume of the hollow spaces of the chambers and galleries. Most sources agree on this number of blocks somewhere above 2.3 million. Their calculations suggest the workforce could have sustained a rate of 180 blocks per hour (3 blocks/minute) with ten-hour work days for putting each individual block in place. They derived these estimates from modern third-world construction projects that did not use modern machinery, but conclude it is still unknown exactly how the Great Pyramid was built. As Dr. Craig Smith of the team points out:\n\nThe entire Giza Plateau is believed to have been constructed over the reign of five pharaohs in less than a hundred years, which generally includes: the Great Pyramid, Khafre and Menkaure's pyramids, the Great Sphinx, the Sphinx and Valley Temples, 35 boat pits cut out of solid bedrock, and several causeways, as well as paving nearly the entire plateau with large stones. This does not include Khafre's brother Djedefre's northern pyramid, Abu Rawash, which would have also been built during this time frame of 100 years. In the hundred years prior to Giza—beginning with Djoser, who ruled from 2687–2667 BC, and amongst dozens of other temples, smaller pyramids, and general construction projects—four other massive pyramids were built: the Step pyramid of Saqqara (believed to be the first Egyptian pyramid), the pyramid of Meidum, the Bent Pyramid, and the Red Pyramid. Also during this period (between 2686 and 2498 BC) the Sadd el-Kafara dam, which used an estimated 100,000 cubic meters of rock and rubble, was built.\n\nIn October 2018, a team of archaeologists from the Institut Français d'Archéologie Orientale and University of Liverpool had announced the discovery of the remains of a 4,500-year-old ramp contraption at Hatnub, excavated since 2012. This method which aided in lifting the heavy alabaster stones up from their quarries, may have been used to build Egypt's Great Pyramid as well. Yannis Gourdon, co-director of the joint mission at Hatnub, said:\n\n\n"}
{"id": "55186888", "url": "https://en.wikipedia.org/wiki?curid=55186888", "title": "Enamelled glass", "text": "Enamelled glass\n\nEnamelled glass is glass which has been decorated with vitreous enamel (powdered glass, possibly mixed with a binder) and then fired to fuse the glasses. It can produce brilliant and long-lasting colours, and be transparent, translucent or opaque. Generally the desired colours only appear when the piece is fired, adding to the artist's difficulties.\n\nIt is similar to vitreous enamel on metal surfaces, but the supporting surface is glass. It is also close to \"enamelled\" overglaze decoration on pottery, especially on porcelain, and it is thought likely that the technique passed from metal to glass (probably in the Islamic world), and then in the Renaissance from glass to pottery (perhaps in Bohemia or Germany).\n\nGlass may be enamelled by sprinkling a loose powder on a flat surface, painting or printing a slurry, or painting or stamping a binder and then sprinkling it with powder, which will adhere. As with enamel on metal, gum tragacanth may be used to make sharp edges.\n\nSome modern techniques are much simpler than historic ones. For instance, there now exist glass enamel pens.\n\nEnamelled glass is often used in combination with gilding. Mica may also be added for sparkle.\n\nEnamelled Venetian glass was called \"smalto\".\n\nMosque lamps are made of enamelled glass. They generally have lugs, from which they are suspended to light not only mosques, but also similar spaces such as madrassas and mausoleums. They have a religious symbolism based on the Quranic verse of light, with which they are often calligraphed.\n\nDuring the European Renaissance, expensive enamelled goblets were used as courtship and marriage gifts. These goblets were rarely used, and some have survived.\n\nGlass painting involves painting on glass, with glass, making the finished work transparent. Glass fusing is similar, but powders are not mixed into a paintable paste first; however, the result is similar.\n\n\n"}
{"id": "46540543", "url": "https://en.wikipedia.org/wiki?curid=46540543", "title": "Envision Energy", "text": "Envision Energy\n\nEnvision Energy() headquartered in Shanghai provides wind turbines, energy management software, and energy technology services.\n\nEnvision was founded by Lei Zhang in 2007 in Jiangyin, Jiangsu in the east region of China. Zhang was named Top 10 Chinese innovators in 2014 by China Daily. The company started full operations since 2009.\n\nIn 2013, Envision installed five 2.1 MW, 110-metre-rotor turbines at the Ucuquer wind farm in central Chile. It also signed software contracts with US developer Pattern Energy’s fleet and compatriot Atlantic Power’s Canadian Hills wind farm in Oklahoma\n\nIn 2014, Envision is partnering with New Zealand’s infrastructure fund manager Infratil to build smart infrastructure in Christchurch, New Zealand.\n\nIn 2015, Envision launched its office in London to handle business in Europe, the Middle East, and Africa Its entry into the European market was the purchase of a 25MW onshore project near Eskilstuna, Sweden. It also acquired ViveEnergia's 600 MW wind energy projects in Mexico in the same year.\nIn early 2016, Envision launched a new operational head office in Hamburg, Germany, which will provide service to its international clients in European countries, and a Global Blade Innovation Center in Boulder, Colorado, which will lead the R&D of blade design in the US. Envision's Ucuquer wind farm project in Chile has been selected as finalist in Inter-American Development Bank’s 360 2016 Infrastructure Awards. The company also made a new investment in a renewable energy project in Montenegro alongside with Enemalta, Shanghai Electric Power, and Vestigo. Currently, Envision is in the process of installing 5 more wind turbines in La Esperanza Wind Farm in Negrete Municipality, Chile by the end of March 2016.\n\nIn 2018, Nissan Motors has entered into a definitive agreement with Envision for the sale of Nissan’s electric battery operations and production facilities to Envision. However, the proposed transaction between Zhang and Nissan has yet to be approved by The Committee on Foreign Investment in the United States (\"CFIUS\"). In August 2018, GSR Capital has submitted summon to The High Court of Hong Kong SAR, Envision Energy facing the potential legal injunction and legal penalties over the AESC deal for breaching of Limited Partnership Agreement basic rules. \n\nEnvision's R&D operations are based in its headquarter in Shanghai, in a factory complex in Jiangyin, and in an innovation center in Silkeborg, Denmark, staffed by 40 engineers focusing on advanced turbine technology. There is a battery-storage R&D center in Osaka, Japan, a cloud service center in Houston, and a digital innovation center in Silicon Valley, California. The company has installed over 2400 wind turbines globally. It also provides software that is used in over 6,000 wind turbines in North America, Europe, Latin America and China.\n"}
{"id": "19260930", "url": "https://en.wikipedia.org/wiki?curid=19260930", "title": "Fault-tolerant computer system", "text": "Fault-tolerant computer system\n\nFault-tolerant computer systems are systems designed around the concepts of fault tolerance. In essence, they must be able to continue working to a level of satisfaction in the presence of errors or breakdowns.\n\nFault tolerance is not just a property of individual machines; it may also characterise the rules by which they interact. For example, the Transmission Control Protocol (TCP) is designed to allow reliable two-way communication in a packet-switched network, even in the presence of communications links which are imperfect or overloaded. It does this by requiring the endpoints of the communication to \"expect\" packet loss, duplication, reordering and corruption, so that these conditions do not damage data integrity, and only reduce throughput by a proportional amount.\n\nRecovery from errors in fault-tolerant systems can be characterised as either 'roll-forward' or 'roll-back'. When the system detects that it has made an error, roll-forward recovery takes the system state at that time and corrects it, to be able to move forward. Roll-back recovery reverts the system state back to some earlier, correct version, for example using checkpointing, and moves forward from there. Roll-back recovery requires that the operations between the checkpoint and the detected erroneous state can be made idempotent. Some systems make use of both roll-forward and roll-back recovery for different errors or different parts of one error.\n\nMost fault-tolerant computer systems are designed to handle several possible failures, including hardware-related faults such as hard disk failures, input or output device failures, or other temporary or permanent failures; software bugs and errors; interface errors between the hardware and software, including driver failures; operator errors, such as erroneous keystrokes, bad command sequences or installing unexpected software and physical damage or other flaws introduced to the system from an outside source.\n\nHardware fault-tolerance is the most common application of these systems, designed to prevent failures due to hardware components. Most basically, this is provided by redundancy, particularly dual modular redundancy. Typically, components have multiple backups and are separated into smaller \"segments\" that act to contain a fault, and extra redundancy is built into all physical connectors, power supplies, fans, etc. There are special software and instrumentation packages designed to detect failures, such as fault masking, which is a way to ignore faults by seamlessly preparing a backup component to execute something as soon as the instruction is sent, using a sort of voting protocol where if the main and backups don't give the same results, the flawed output is ignored.\n\nSoftware fault-tolerance is based more around nullifying programming errors using real-time redundancy, or static \"emergency\" subprograms to fill in for programs that crash. There are many ways to conduct such fault-regulation, depending on the application and the available hardware.\n\nThe first known fault-tolerant computer was SAPO, built in 1951 in Czechoslovakia by Antonín Svoboda. Its basic design was magnetic drums connected via relays, with a voting method of memory error detection (triple modular redundancy). Several other machines were developed along this line, mostly for military use. Eventually, they separated into three distinct categories: machines that would last a long time without any maintenance, such as the ones used on NASA space probes and satellites; computers that were very dependable but required constant monitoring, such as those used to monitor and control nuclear power plants or supercollider experiments; and finally, computers with a high amount of runtime which would be under heavy use, such as many of the supercomputers used by insurance companies for their probability monitoring.\n\nMost of the development in the so-called LLNM (Long Life, No Maintenance) computing was done by NASA during the 1960s, in preparation for Project Apollo and other research aspects. NASA's first machine went into a space observatory, and their second attempt, the JSTAR computer, was used in Voyager. This computer had a backup of memory arrays to use memory recovery methods and thus it was called the JPL Self-Testing-And-Repairing computer. It could detect its own errors and fix them or bring up redundant modules as needed. The computer is still working today.\n\nHyper-dependable computers were pioneered mostly by aircraft manufacturers, nuclear power companies, and the railroad industry in the USA. These needed computers with massive amounts of uptime that would fail gracefully enough with a fault to allow continued operation, while relying on the fact that the computer output would be constantly monitored by humans to detect faults. Again, IBM developed the first computer of this kind for NASA for guidance of Saturn V rockets, but later on BNSF, Unisys, and General Electric built their own.\n\nThe 1970 F14 CADC had built-in self-test and redundancy.\n\nIn general, the early efforts at fault-tolerant designs were focused mainly on internal diagnosis, where a fault would indicate something was failing and a worker could replace it. SAPO, for instance, had a method by which faulty memory drums would emit a noise before failure. Later efforts showed that, to be fully effective, the system had to be self-repairing and diagnosing – isolating a fault and then implementing a redundant backup while alerting a need for repair. This is known as N-model redundancy, where faults cause automatic fail safes and a warning to the operator, and it is still the most common form of level one fault-tolerant design in use today.\n\nVoting was another initial method, as discussed above, with multiple redundant backups operating constantly and checking each other's results, with the outcome that if, for example, four components reported an answer of 5 and one component reported an answer of 6, the other four would \"vote\" that the fifth component was faulty and have it taken out of service. This is called M out of N majority voting.\n\nHistorically, motion has always been to move further from N-model and more to M out of N due to the fact that the complexity of systems and the difficulty of ensuring the transitive state from fault-negative to fault-positive did not disrupt operations.\n\nTandem and Stratus were among the first companies specializing in the design of fault-tolerant computer systems for online transaction processing.\n\nThe most important requirement of design in a fault tolerant computer system is making sure it actually meets its requirements for reliability. This is done by using various failure models to simulate various failures, and analyzing how well the system reacts. These statistical models are very complex, involving probability curves and specific fault rates, latency curves, error rates, and the like. The most commonly used models are HARP, SAVE, and SHARPE in the USA, and SURF or LASS in Europe.\n\nResearch into the kinds of tolerances needed for critical systems involves a large amount of interdisciplinary work. The more complex the system, the more carefully all possible interactions have to be considered and prepared for. Considering the importance of high-value systems in transport, public utilities and the military, the field of topics that touch on research is very wide: it can include such obvious subjects as software modeling and reliability, or hardware design, to arcane elements such as stochastic models, graph theory, formal or exclusionary logic, parallel processing, remote data transmission, and more.\n\n\"Failure-oblivious computing\" is a technique that enables computer programs to continue executing despite memory errors. The technique handles attempts to read invalid memory by returning a manufactured value to the program, which in turn, makes use of the manufactured value and ignores the former memory value it tried to access. This is a great contrast to typical memory checkers, which inform the program of the error or abort the program. In failure-oblivious computing, no attempt is made to inform the program that an error occurred. Furthermore, it happens that the execution is modified several times in a row, in order to prevent cascading failures.\nThe approach has performance costs: because the technique rewrites code to insert dynamic checks for address validity, execution time will increase by 80% to 500%.\n\nRecovery shepherding is a lightweight technique to enable software programs to recover from otherwise fatal errors such as null pointer dereference and divide by zero. Comparing to the failure oblivious computing technique, recovery shepherding works on the compiled program binary directly and does not need to recompile to program. \nIt uses the just-in-time binary instrumentation framework Pin. It attaches to the application process when an error occurs, repairs the execution, \ntracks the repair effects as the execution continues, contains the repair effects within the application process, and detaches from the process after all repair effects are flushed \nfrom the process state. It does not interfere with the normal execution of the program and therefore incurs negligible overhead. For 17 of 18 systematically collected real world null-dereference and divide-by-zero errors, a prototype implementation enables the application to continue to execute to provide acceptable output and service to its users on the error-triggering inputs.\n\n\n"}
{"id": "19496254", "url": "https://en.wikipedia.org/wiki?curid=19496254", "title": "GOG.com", "text": "GOG.com\n\nGOG.com (formerly Good Old Games) is a digital distribution platform for video games and films. It is operated by GOG Sp. z o.o., a wholly owned subsidiary of CD Projekt based in Warsaw, Poland. GOG.com delivers DRM-free video games through its digital platform for Microsoft Windows, OS X and Linux. In March 2012, it began selling more recent titles such as \"Alan Wake\", \"Assassin's Creed\" and the \"Metro Redux series\", among many others.\n\nPoland, where CD Projekt and Good Old Games were founded, had previously been under Communist rule but in 1990, the old government had fallen in favor of a more democratic government which spurred economic growth. While under Communism, copyright laws in Poland were virtually non-existent and unenforceable, and copyright infringement, in the form of piracy by stripping out any digital rights management (DRM), was rampant across electronic media. The consumer perception of copyright in Poland remained the same after the change of government, making it difficult for legitimate sales of electronic media; pirated and bootlegged versions were sold in open markets next to boxed copies of the legitimate productions for a fraction of the cost.\n\nCD Projekt was founded by Marcin Iwiński and Michał Kiciński in 1994 for the purposes to trying to bring legitimate sales of foreign game titles into Poland, knowing they would have no easy way to compete against pirated copies. They would obtain import rights from foreign publishers, and where possible, provide in-game localization for text and voice lines, typically through reverse engineering to decompile the game's code. They would then package the game with localized instruction manuals and other physical goodies, hoping that the added features would draw buyers away from pirated copies. Their first major success was with \"Baldur's Gate\" (1998) which with they had 18,000 units sold on its first day of release in Poland. Due to this success, Interplay, the publisher of \"Baldur's Gate\", asked CD Projekt if they could do a similar treatment to \"\", a console title released in 2001. As their past work had been strictly on personal computers, the company accepted to try to port it, but the project fell through before it was completed. However, as a result, CD Projekt realized they had the ability to make their own games, and moved into games development. This eventually proved fruitful, as it ultimately landed the company with rights to \"The Witcher\" video game series. The company's interest in game distribution has declined since then.\n\nDigital distribution grew in the 2000s, along with the use of DRM to control access to games, which raised some resentment with players. CD Projekt saw potential to look back at their distribution days to offer DRM-free versions of classic games through digital distribution, using their past experience in reverse engineering to make the games work on modern platforms and provide a wide array of localization options. In this manner, they would have a reason to draw players to buy their product instead of simply downloading it for free from pirate game websites and services. They founded a new subsidiary, Good Old Games, to serve this purpose in early 2008. Their first challenge was to find a publisher that would be willing to work with them; they spoke to several who were generally unaware of CD Projekt; their first big break was from Interplay, who knew of the company's past work, and allowed them to offer their games on the service. After some time, Good Old Games was approached by Ubisoft, who were interested in selling their older titles through the service as well. Once Ubisoft was signed, it became easier for Good Old Games to convince other publishers to allow them to offer older titles on the service.\n\nDuring a period of days from 19 to 22 September 2010, the GOG.com website was disabled, leaving behind messages on the web site and their Twitter accounts that the site had been closed. A spokesman for Good Old Games reiterated that the site was not being shut down, and confirmed news would be forthcoming about changes to the service. A clarification posted on the site on 20 September 2010 said they had to shut down the site temporarily \"due to business and technical reasons\", with industry journalists believing the shutdown may be related to the nature of DRM-free strategy, based on Twitter messages from the company. On 22 September 2010, GOG.com revealed that this shutdown was a marketing hoax as part of the site coming out of beta. The site's management, aware of the reactions to the fake closure, stated: \"First of all we'd like to apologize to everyone who felt deceived or harmed in any way by the closedown of GOG.com. As a small company we don't have a huge marketing budget and this is why we could not miss a chance to generate some buzz around an event as big as launching a brand new version of our website and even more important, bringing back \"Baldur's Gate\" to life!\"\n\nThe site returned on 23 September 2010, with an improved storefront and additional benefits, as outlined during a webcast presentation. During the presentation, GOG.com's co-founder Marcin Iwinski and managing director Guillaume Rambourg had dressed as monks to atone for their sins. The relaunch of the site was considered by Rambourg to have been successful, having brought new customers that were previously unaware of GOG.com. As promised after its relaunch, GOG.com was able to offer several Black Isle Studios games such as \"Baldur's Gate\", \"\" and \"Icewind Dale\" which have previously been unreleased through any download service due to legal issues about the ownership of \"Dungeons & Dragons\"-related games between Atari, Hasbro, and other companies.\n\nOn 27 March 2012, Good Old Games announced that it was branching out to feature \"AAA\" and independent titles in addition to older games. The site was rebranded to GOG.com.\n\nIn October 2012, GOG.com was announced to be bringing DRM-free games to OS X. This included the previously Steam exclusive (OS X version) \"The Witcher\" and \"The Witcher 2\", both made by CD Projekt Red. GOG.com gathered user feedback in a community wishlist, and one of the most demanded feature requests was support for native Linux games, which gathered close to 15,000 votes before it was marked as \"in progress\". Originally GOG.com representatives said, that there are technical and operational issues which make it harder than it seems, however it's something they would love to do, and they have been looking at. On 18 March 2014, GOG.com officially announced that they would be adding support for Linux, initially targeting Ubuntu and Linux Mint in the fall of 2014. On 25 July 2014, Linux support was released early, and 50 games were released compatible with the operating system. Several of the launch titles included games that were newly compatible with Linux, while most of the games already supported downloads made for the operating system on other distribution platforms.\n\nOn 27 August 2014 GOG.com announced the launch of the new addition to their service – distribution of DRM-free films. GOG.com offers DRM-free downloading in mp4 format and streaming of video in standard and DRM-free HTML fashion which doesn't bind users to any specific platforms or devices. The movies are made available in Full HD 1080p, 720p and 576p for limited bandwidth or download quotas; however, a few titles do not have the Full HD 1080p format available. GOG.com started with adding 21 documentaries about Internet culture and gaming. They also have plans for adding fiction films and series; according to GOG.com's managing director Guillaume Rambourg, they were in talks with many major studios. While studios' representatives liked the idea, they also were reluctant to let go of their current DRM approach until some other major studio would make the first step. Still GOG.com plan to work on overcoming the initial reluctance and moving DRM-free video forward.\n\nOn 9 December 2013, GOG.com introduced a money-back guarantee for the first 30 days if a customer faces unresolvable technical problems with a bought game. Beginning 2 April 2015, GOG.com began to offer DRM-free downloads to holders of game keys from boxed copies of select games whose DRM validation systems no longer operate; examples are the \"S.T.A.L.K.E.R.\" series and the \"Master of Orion\" series. Over $1,700,000 of retail game purchases had been redeemed through this system by November 2017.\n\nIn August 2018, GOG created an anti-DRM program called \"FCK DRM\". The homepage of the initiative offers links to the websites of Defective by Design, the EFF, Bandcamp, itch.io, Wikisource, Project Gutenberg and other projects that promote free culture.\n\nGOG.com works to offer older games as well as new releases to users, with the product lacking any type of digital rights management to give consumers the ability to install the game anywhere and as many times as they want.\n\nPrior to any development work to bring an older game for use on modern computers, legal experts within GOG.com need to track down all ownership rights to games and make sure that all necessary parties agree to their redistribution. This can be difficult for many games of the late 1990s and early 2000s, where very few publishers and developers kept digital records of their legal documentation, and there were large numbers of acquisitions and dissolutions that make tracking down rights difficult and take years to complete. One difficult case was acquiring the rights for the Strategic Simulations \"Gold Box\" series games, due to the number of acquisitions that Strategic Simulations went through since the 1990s. GOG.com offers users a means to request back-catalog games they would like to see, and the company uses this list to identify games that may require more extensive licensing research. Some of this work has been done in coordination with Nightdive Studios, who were able to find and acquire the rights to \"System Shock 2\", one of the most requested games at GOG.com for years, and have since found and relicensed other older games thought lost to licensing issues.\n\nIn order to ensure compatibility with newer versions of operating systems and current PC hardware, games are pre-patched and restored by GOG.com. Whenever possible, GOG.com attempts to acquire the game's original source code, which can prove as difficult as determining the legal rights to games. From this, they can work to make the game compatible with modern and future hardware, directly apply compatibility fixes, and sometimes incorporate well-established community-made patches from a game's fan-community. They also will seek external help with some of the code issues, approaching developers that may have previously worked on the title for assistance. They may also need to reverse engineer the game's code if it is not available. In cases where it is impossible to recode the game, they will instead package the game with open-source emulation or compatibility software, such as ScummVM and DOSBox \n\nFor newer titles, particularly for indie games, GOG.com offers the ability to publish their games on the site starting 2013. GOG.com offers indie developers a typical 70/30 split on revenue (with GOG.com taking 30% of the sale), as well as an option for an upfront payment to the developer, with then GOG.com taking 40% of the sales until the upfront payment has been covered, reverting the cost back to 30%. Such games are still distributed DRM-free.\n\nOn 26 March 2009, GOG.com announced it had signed a deal with Ubisoft to publish games from their back catalogue; this was the first deal with a major publisher to offer DRM-free downloads. The deal to publish through GOG.com also included games that were not available through any other online distribution channel. On 5 September 2014, GOG.com started to sell Amiga games from Cinemaware's catalogue, starting with Defender of the Crown. This was technically made possible through Cinemaware's own written emulator called \"Rocklobster\". On 28 October 2014, GOG.com was able to secure another major publisher as a DRM-free partner, Disney Interactive / LucasArts. With this new partnership, GOG.com began to re-release several often-requested game titles from LucasArts, starting with six titles (\"\", \"\", \"Sam & Max Hit the Road\", \"\", \"Indiana Jones and the Fate of Atlantis\" and \"\"). On 5 May 2015, GOG.com released \"Pacific General\" and \"Fantasy General\" and named itself, GOG Ltd, as the publisher. The company revealed that it had acquired the copyright to these titles and that it intends to acquire more in the future. On 26 August 2015, Bethesda Softworks joined GOG.com with classical titles as id Software's \"Doom\" and \"Quake\", \"Fallout\" (which had been sold on GOG by Interplay before the rights changed hands), and also some classic \"Elder Scrolls\" titles. GOG.com has more than 2,000 DRM-free games in its catalogue and new ones are added several times a week.\n\nThe offered digital goods (video games and movies) can be purchased and downloaded online and they are distributed without digital rights management. The prices of products typically range from about $5 to $10 for older games, along with special offers in sales held several times a week. Some newer titles have a higher price. GOG.com's digital products can also be given to other persons via redeemable gift certificates.\n\nThe user does not have to install special client software to download or run the games, although a download manager, which is due to be phased out, and the GOG Galaxy client, which is currently in beta, are available. After downloading, the customer is free to use the software for any personal use like installing on multiple devices, archiving on any personal storage media for unlimited time, modding and patching; with the restriction that reselling and sharing is not permitted. The software installers are technically independent of the customer's GOG.com account, although still subject to GOG.com's EULA, where a \"licensed, not sold\" formulation is used. The \"licensed, not sold\" model frequently raises questions of ownership of digital goods. In the European Union, the European Court of Justice held that a copyright holder cannot oppose the resale of a digitally sold software, in accordance with the rule of copyright exhaustion on first sale as ownership is transferred, and questions therefore the \"licensed, not sold\" EULA.\n\nAlong with the games, customers are also able to download numerous extra materials relating to the game they purchased. Often these extras include the game's soundtrack (partly as FLAC), wallpapers, avatars, and manuals. GOG.com also offers full customer support for all purchases and a money-back guarantee for the first 30 days.\n\nPromotions are organized regularly. The style of these promotions varies from a discount for products that are bought in bundles, to thematic competitions like riddles, \"guess a game from a picture\" contests or \"best time on a specific level\". Also, GOG.com gives away promotion codes for a game with review contests.\n\nIn the CD Projekt Red company update in June 2014, GOG.com announced that it would be bringing a Steam-like client, GOG Galaxy, to Windows, Mac, and Linux platforms. The client is designed as a storefront, software delivery, and social network client, allowing players to buy and play games from GOG.com and share them with friends. Galaxy is also going to include an original multiplayer API, allowing developers to include the same kind of multiplayer functionality in GOG.com versions of games as on Steam. The client is optional and will retain the DRM-free objective of the GOG.com website. On 15 October 2014 the open multiplayer beta of the GOG Galaxy client was started, accompanied by the giveaway of \"Alien vs Predator\". In July 2015 the GOG Galaxy beta client was reviewed favorably by the PC Gamer magazine, especially noting the focus on user respect in comparison to Steam. On 22 March 2017, the client added in cloud saves for 29 games from its catalog. GOG Galaxy is currently available for Microsoft Windows and macOS, and a Linux version is planned.\n\nRevealed in June 2016, GOG Connect enables users with both GOG.com and Steam accounts to claim certain games they already own on Steam as part of their GOG.com library, allowing them to download the DRM-free version and other bonus items for that game offered by GOG.com. Not all such games are part of this offer, as it requires GOG.com to work with the game publishers to enable this. Further, the time to claim such games will be limited, though once a user has claimed their game on GOG.com, it otherwise remains in their library indefinitely.\n\nAs GOG.com does not typically release absolute game selling numbers, market share considerations of GOG.com among the digital distributors are a challenge. But, sometimes an individual game developer releases their internal statistics about the selling performance on different game distribution channels for their specific game.\n\nIn an article dated 11 November 2011, PC Gamer reported statistics for online sales of \"The Witcher 2\". According to PC Gamer, Direct2Drive, Impulse and Gamersgate's combined sales were a total of 10,000 (4%), GOG.com sold 40,000 copies (16%), while Steam sold in the same time period 200,000 copies (80%).\n\nOn 20 February 2013, \"Defender's Quest\" developer Lars Doucet revealed the first three months of revenue following his game's release across 6 different digital distribution platforms, including 4 major digital game distributors and 2 methods of purchasing and downloading the game directly from the developer. The results showed that GOG.com generated 8.5% of the revenue – second only to Steam's 58.6% among the digital distribution platforms used. Doucet noted that \"for the major [digital game distributors], GOG's star is clearly rising. Even under direct competition, GOG generated 14.5 percent as much revenue as Steam. [...] Steam enjoys a captive market of ardent loyalists, but GOG is swiftly becoming an attractive alternative and gaining loyalists of its own, especially in the anti-DRM crowd.\"\n\n, GOG.com had seen 690,000 units of CD Projekt Red's game \"\" redeemed through the service, more than the second largest digital seller Steam (approx. 580,000 units) and all other PC digital distribution services combined.\n"}
{"id": "43566143", "url": "https://en.wikipedia.org/wiki?curid=43566143", "title": "General Software", "text": "General Software\n\nGeneral Software was a Washington, USA based creator and supplier of system software headquartered in Bellevue, Washington. It was founded in 1989 by Steve Jones and later incorporated in 1990 as General Software, Inc. In 2008, the company was purchased by Phoenix Technologies, Inc.\n\nGeneral Software developed firmware to support telecommunications, data communications, UMPC (Ultra Mobile Personal Computer), general consumer electronics, and support models designed for embedded ODMs (Original Design Manufacturer) and OEMs (Original Equipment Manufacturer).\n\nSome of the products included General Software Embedded DOS, Embedded DOS-ROM, Embedded DOS-XL and Embedded BIOS.\n\nIn 1998 they partnered with Caldera Thin Clients, Inc. in order to ship their Embedded BIOS with Caldera DR-DOS.\n\n"}
{"id": "42071140", "url": "https://en.wikipedia.org/wiki?curid=42071140", "title": "Gourmand (fragrance)", "text": "Gourmand (fragrance)\n\nA gourmand fragrance is a perfume consisting primarily of synthetic edible (gourmand) notes, such as honey, chocolate, vanilla or candy. These top and middle notes may be blended with non-edible base notes such as patchouli or musk. They have been described as olfactory desserts. They are also called \"foodie\" fragrances and can be both feminine and masculine.\n\nThierry Mugler's \"Angel\", launched in 1992, is credited as the first modern gourmand scent. Additional examples include Mugler's \"A*Men\", Lolita Lempicka's \"Lolita Lempicka\" and \"Au Masculin\", Hanae Mori's \"Butterfly\", Calvin Klein's \"Euphoria\", Burberry's \"Brit for Men\", Rochas' \"Rochas Man\", Chopard's \"Wish\" and Viktor & Rolf's \"Antidote\". \n\nThe gourmand trend has steadily increased in popularity since 1992 but is not a new concept. In 1956, Edmond Roundnitska created \"Diorissimo\" to counter the contemporary preference for heavy and sweet notes which are common in gourmand perfumes.\n\nMost gourmand scents, such as those in the form of bath products, perfume or cologne, are not intended for human consumption. Other gourmand scents are not only scented but flavoured, such as Amoretti's line of products or Jessica Simpson's discontinued \"Dessert Treats\". These scented products include other forms such as lip glosses, dusting powders and lotions.\n\nGourmand is considered a subcategory of the modern family of fragrance. Gourmands may also work in combination with the fruity family of fragrance.\n"}
{"id": "24224889", "url": "https://en.wikipedia.org/wiki?curid=24224889", "title": "Hedonometer", "text": "Hedonometer\n\nA hedonometer or hedonimeter is a device used to gauge happiness or pleasure. Conceived of at least as early as 1880, the term was used in 1881 by the economist Francis Ysidro Edgeworth to describe \"an ideally perfect instrument, a psychophysical machine, continually registering the height of pleasure experienced by an individual.\" \n\nMore recently, it has been used to refer to a tool developed by Peter Dodds and Chris Danforth to gauge the valence of various corpora, including historical State of the Union addresses, song lyrics, and online tweets and blogs. A version of the tool is available at hedonometer.org, which they call a sort of \"Dow Jones Index of Happiness\", and hope will be used by government officials in conjunction with other metrics as a gauge of the population's well-being.\n\n\n"}
{"id": "10908329", "url": "https://en.wikipedia.org/wiki?curid=10908329", "title": "Institute of Scientific and Technical Communicators", "text": "Institute of Scientific and Technical Communicators\n\nThe Institute of Scientific and Technical Communicators (ISTC) is the UK's largest professional association for those involved in technical communication and information design. It encourages professional education and standards, provides guidance about the value of using professional communicators, and provides research resources and networking opportunities for its members and affiliates.\n\nThe ISTC as it is today was created in 1972 from an amalgamation of three associations: the Presentation of Technical Information Group (founded in 1948), the Technical Publications Association and the Institute of Technical Publicity & Publications. The institute was incorporated as a limited company on 19 July 1972.\n\nThe ISTC is managed by its Council, with day-to-day running handled by a professional administration company. Members have the opportunity to contribute their ideas, skills and time to specific Steering Groups and can stand for Council.\n\nThe ISTC is a member of the Professional Associations Research Network (PARN). This organisation provides a network for professional bodies and offers services and events on subjects such as governance, Continuing Professional Development, ethics and standards, and member relations. The ISTC was one of the sponsors of a project in 2006 to explore the issues affecting professional associations as they grow.\n\nThe ISTC has a Code of Professional Practice, with which it expects its members to comply. This was developed in accordance with research carried out by PARN and its author was cited in PARN's third book on ethics.\n\nTechnical communication encompasses a wide range of activities, all with the common thread of communicating complex or important information in the most effective way for the reader. Technical communicators create information that affects people in virtually every industry and area of society. Examples of roles that may be involved in technical communication are:\n\n\nThe ISTC offers several grades of individual membership. There are two corporate grades, Member (MISTC) and Fellow (FISTC), and two non-corporate grades, Associate and Student. Corporate grades are entitled to vote at the ISTC's AGM. Members cite a sense of community as an important factor in their continued membership.\n\nOrganisations can join the ISTC as Business Affiliates.\n\nThe ISTC publishes periodicals and books:\n\nThe ISTC organises an annual conference, Technical Communication UK, which is held in September.\n\nThe ISTC makes annual awards:\n\nThe ISTC participates in the international technical communication profession through its membership of two umbrella organisations:\n\nThe ISTC was one of the founder members of both organisations, INTECOM in 1970 and TCeurope in 2002.\n\n"}
{"id": "18306738", "url": "https://en.wikipedia.org/wiki?curid=18306738", "title": "Instruments used in endocrinology", "text": "Instruments used in endocrinology\n\nInstruments used specially in Endocrinology are as follows:\n\n\"Vide\":\n"}
{"id": "48567843", "url": "https://en.wikipedia.org/wiki?curid=48567843", "title": "Learning relationship management", "text": "Learning relationship management\n\nA learning relationship management (LRM) software system manages and facilitates student-led instruction to maximize student engagement, achievement, outcome and long-term success (allows learners to assemble and manage their own sociotechnical system). Unlike learning management systems (LMS) in which elements are organized around specific courses, LRMs are student-centric in design, facilitate personalized learning, and provide individualized learning paths, a central point for analytics data and a way of tracking interventions and related results. The LRM system provides a comprehensive foundation for end-to-end student support\", which may include communication with and/or support from a learner network consisting of educators, administrators, parents/guardians, mentors, advisors/guidance counselors, etc.\n\nExamples of LRMs include Fidelis Education, Epiphany Learning, Project Foundry, MyLC, Fishtree, and Motivis Learning.\n"}
{"id": "42312637", "url": "https://en.wikipedia.org/wiki?curid=42312637", "title": "List of Australian mobile virtual network operators", "text": "List of Australian mobile virtual network operators\n\nMobile virtual network operators (MVNOs) in Australia lease wireless telephone and data services from major carriers such as Optus, Telstra, Vodafone and TPG for resale.\n\n"}
{"id": "5046331", "url": "https://en.wikipedia.org/wiki?curid=5046331", "title": "List of Dutch inventions and discoveries", "text": "List of Dutch inventions and discoveries\n\nThe Netherlands had a considerable part in the making of modern society. The Netherlands and its people have made numerous seminal contributions to the world's civilization, especially in art, science, technology and engineering, economics and finance, cartography and geography, exploration and navigation, law and jurisprudence, thought and philosophy, medicine, and agriculture. Dutch-speaking people, in spite of their relatively small number, have a significant history of invention, innovation, discovery and exploration. The following list is composed of objects, (largely) unknown lands, breakthrough ideas/concepts, principles, phenomena, processes, methods, techniques, styles etc., that were discovered or invented (or pioneered) by people from the Netherlands and Dutch-speaking people from the former Southern Netherlands (\"Zuid-Nederlanders\" in Dutch). Until the fall of Antwerp (1585), the Dutch and Flemish were generally seen as one people.\n\nThe De Stijl school proposed simplicity and abstraction, both in architecture and painting, by using only straight horizontal and vertical lines and rectangular forms. Furthermore, their formal vocabulary was limited to the primary colours, red, yellow, and blue and the three primary values, black, white and grey. De Stijl's principal members were painters Theo van Doesburg (1883–1931), Piet Mondrian (1872–1944), Vilmos Huszár (1884–1960), and Bart van der Leck (1876–1958) and architects Gerrit Rietveld (1888–1964), Robert van 't Hoff (1888–1979) and J.J.P. Oud (1890–1963).\n\nBrabantine Gothic, occasionally called Brabantian Gothic, is a significant variant of Gothic architecture that is typical for the Low Countries. It surfaced in the first half of the 14th century at Saint Rumbold's Cathedral in the City of Mechelen. The Brabantine Gothic style originated with the advent of the Duchy of Brabant and spread across the Burgundian Netherlands.\n\nThe Dutch gable was a notable feature of the Dutch-Flemish Renaissance architecture (or Northern Mannerist architecture) that spread to northern Europe from the Low Countries, arriving in Britain during the latter part of the 16th century. Notable castles/buildings including Frederiksborg Castle, Rosenborg Castle, Kronborg Castle, Børsen, Riga's House of the Blackheads and Gdańsk's Green Gate were built in Dutch-Flemish Renaissance style with sweeping gables, sandstone decorations and copper-covered roofs. Later Dutch gables with flowing curves became absorbed into Baroque architecture. Examples of Dutch-gabled buildings can be found in historic cities across Europe such as Potsdam (Dutch Quarter), Friedrichstadt, Gdańsk and Gothenburg. The style spread beyond Europe, for example Barbados is well known for Dutch gables on its historic buildings. Dutch settlers in South Africa brought with them building styles from the Netherlands: Dutch gables, then adjusted to the Western Cape region where the style became known as Cape Dutch architecture. In the Americas and Northern Europe, the West End Collegiate Church (New York City, 1892), the Chicago Varnish Company Building (Chicago, 1895), Pont Street Dutch-style buildings (London, 1800s), Helsingør Station (Helsingør, 1891), and Gdańsk University of Technology's Main Building (Gdańsk, 1904) are typical examples of the Dutch Renaissance Revival (Neo-Renaissance) architecture in the late 19th century.\n\nAntwerp Mannerism is the name given to the style of a largely anonymous group of painters from Antwerp in the beginning of the 16th century. The style bore no direct relation to Renaissance or Italian Mannerism, but the name suggests a peculiarity that was a reaction to the classic style of the early Netherlandish painting. Antwerp Mannerism may also be used to describe the style of architecture, which is loosely Mannerist, developed in Antwerp by about 1540, which was then influential all over Northern Europe. The Green Gate (Brama Zielona) in Gdańsk, Poland, is a building which is inspired by the Antwerp City Hall. It was built between 1568 and 1571 by Regnier van Amsterdam and Hans Kramer to serve as the formal residence of the Polish monarchs when visiting Gdańsk.\n\nCape Dutch architecture is an architectural style found in the Western Cape of South Africa. The style was prominent in the early days (17th century) of the Cape Colony, and the name derives from the fact that the initial settlers of the Cape were primarily Dutch. The style has roots in medieval Netherlands, Germany, France and Indonesia. Houses in this style have a distinctive and recognisable design, with a prominent feature being the grand, ornately rounded gables, reminiscent of features in townhouses of Amsterdam built in the Dutch style.\n\nThe Amsterdam School (Dutch: \"Amsterdamse School\") flourished from 1910 through about 1930 in the Netherlands. The Amsterdam School movement is part of international Expressionist architecture, sometimes linked to German Brick Expressionism.\n\nThe Rietveld Schröder House or Schröder House (Rietveld Schröderhuis in Dutch) in Utrecht was built in 1924 by Dutch architect Gerrit Rietveld. It became a listed monument in 1976 and a UNESCO World Heritage Site in 2000. The Rietveld Schröder House constitutes both inside and outside a radical break with tradition, offering little distinction between interior and exterior space. The rectilinear lines and planes flow from outside to inside, with the same colour palette and surfaces. Inside is a dynamic, changeable open zone rather than a static accumulation of rooms. The house is one of the best known examples of \"De Stijl\" architecture and arguably the only true \"De Stijl\" building.\n\nThe Van Nelle factory was built between 1925 and 1931. Its most striking feature is its huge glass façades. The factory was designed on the premise that a modern, transparent and healthy working environment in green surroundings would be good both for production and for workers' welfare. The factory had a huge impact on the development of modern architecture in Europe and elsewhere. The Van Nelle Factory is a Dutch national monument (Rijksmonument) and since 2014 has the status of UNESCO World Heritage Site. The Justification of Outstanding Universal Value was presented in 2013 to the UNESCO World Heritage Committee.\n\nAn architectural movement started by a generation of new architects during the 1990, among this generation of architects were OMA, MVRDV, UNStudio, Mecanoo, Meyer en Van Schooten and many more. They started with buildings, which became internationally known for their new and refreshing style. After which Super Dutch Architecture spread out across the globe.\n\nThe Dutch door (also known as \"stable door\" or \"half door\") is a type of door divided horizontally in such a fashion that the bottom half may remain shut while the top half opens. The initial purpose of this door was to keep animals out of farmhouses, while keeping children inside, yet allowing light and air to filter through the open top. This type of door was common in the Netherlands in the seventeenth century and appears in Dutch paintings of the period. They were commonly found in Dutch areas of New York and New Jersey (before the American Revolution) and in South Africa.\n\nThe Red and Blue Chair was designed in 1917 by Gerrit Rietveld. It represents one of the first explorations by the De Stijl art movement in three dimensions. It features several Rietveld joints.\n\nThe Zig-Zag Chair was designed by Rietveld in 1934. It is a minimalist design without legs, made by 4 flat wooden tiles that are merged in a Z-shape using Dovetail joints. It was designed for the Rietveld Schröder House in Utrecht.\n\nAlthough oil paint was first used for Buddhist paintings by Indian and Chinese painters sometime between the fifth and tenth centuries, it did not gain notoriety until the 15th century. Its practice may have migrated westward during the Middle Ages. Oil paint eventually became the principal medium used for creating artworks as its advantages became widely known. The transition began with Early Netherlandish painting in northern Europe, and by the height of the Renaissance oil painting techniques had almost completely replaced tempera paints in the majority of Europe. Early Netherlandish painting (Jan van Eyck in particular) in the 15th century was the first to make oil the default painting medium, and to explore the use of layers and glazes, followed by the rest of Northern Europe, and only then Italy.\n\nGlazing is a technique employed by painters since the invention of modern oil painting. Early Netherlandish painters in the 15th century were the first to make oil the usual painting medium, and explore the use of layers and glazes, followed by the rest of Northern Europe, and only then Italy.\n\nTwo aspects of realism were rooted in at least two centuries of Dutch tradition: conspicuous textural imitation and a penchant for ordinary and exaggeratedly comic scenes. Two hundred years before the rise of literary realism, Dutch painters had already made an art of the everyday – pictures that served as a compelling model for the later novelists. By the mid-1800s, 17th-century Dutch painting figured virtually everywhere in the British and French fiction we esteem today as the vanguard of realism.\n\nHieronymus Bosch is considered one of the prime examples of Pre-Surrealism. The surrealists relied most on his insights. In the 20th century, Bosch's paintings (e.g. The Garden of Earthly Delights, The Haywain, The Temptation of St. Anthony and The Seven Deadly Sins and the Four Last Things) were cited by the Surrealists as precursors to their own visions.\n\nStill-life painting as an independent genre or specialty first flourished in the Netherlands in the last quarter of the 16th century, and the English term derives from \"stilleven\": \"still life\", which is a calque, while Romance languages (as well as Greek, Polish, Russian and Turkish) tend to use terms meaning \"dead nature\".\n\nThe term \"landscape\" derives from the Dutch word \"landschap\" (and the German \"Landschaft\"), which originally meant \"region, tract of land\" but acquired the artistic connotation, \"a picture depicting scenery on land\" in the early 16th century. After the fall of the Roman Empire, the tradition of depicting pure landscapes declined and the landscape was seen only as a setting for religious and figural scenes. This tradition continued until the 16th century when artists began to view the landscape as a subject in its own right. The Dutch Golden Age painting of the 17th century saw the dramatic growth of landscape painting, in which many artists specialized, and the development of extremely subtle realist techniques for depicting light and weather.\n\nThe Flemish Renaissance painter Pieter Brueghel the Elder chose peasants and their activities as the subject of many paintings. Genre painting flourished in Northern Europe in his wake. Adriaen van Ostade, David Teniers, Aelbert Cuyp, Jan Steen, Johannes Vermeer and Pieter de Hooch were among many painters specializing in genre subjects in the Netherlands during the 17th century. The generally small scale of these artists' paintings was appropriate for their display in the homes of middle class purchasers.\n\nMarine painting began in keeping with medieval Christian art tradition. Such works portrayed the sea only from a bird's eye view, and everything, even the waves, was organized and symmetrical. The viewpoint, symmetry and overall order of these early paintings underlined the organization of the heavenly cosmos from which the earth was viewed. Later Dutch artists such as Hendrick Cornelisz Vroom, Cornelius Claesz, Abraham Storck, Jan Porcellis, Simon de Vlieger, Willem van de Velde the Elder, Willem van de Velde the Younger and Ludolf Bakhuizen developed new methods for painting, often from a horizontal point of view, with a lower horizon and more focus on realism than symmetry.\n\nThe term vanitas is most often associated with still life paintings that were popular in seventeenth-century Dutch art, produced by the artists such as Pieter Claesz. Common vanitas symbols included skulls (a reminder of the certainty of death); rotten fruit (decay); bubbles, (brevity of life and suddenness of death); smoke, watches, and hourglasses, (the brevity of life); and musical instruments (the brevity and ephemeral nature of life). Fruit, flowers and butterflies can be interpreted in the same way, while a peeled lemon, as well as the typical accompanying seafood was, like life, visually attractive but with a bitter flavor.\n\nGroup portraits were produced in great numbers during the Baroque period, particularly in the Netherlands. Unlike in the rest of Europe, Dutch artists received no commissions from the Calvinist Church which had forbidden such images or from the aristocracy which was virtually non-existent. Instead, commissions came from civic and businesses associations. Dutch painter Frans Hals used fluid brush strokes of vivid color to enliven his group portraits, including those of the civil guard to which he belonged. Rembrandt benefitted greatly from such commissions and from the general appreciation of art by bourgeois clients, who supported portraiture as well as still-life and landscape painting. Notably, the world's first significant art and dealer markets flourished in Holland at that time.\n\nIn the 17th century, Dutch painters (especially Frans Hals, Rembrandt, Jan Lievens and Johannes Vermeer) began to create uncommissioned paintings called \"tronies\" that focused on the features and/or expressions of people who were not intended to be identifiable. They were conceived more for art's sake than to satisfy conventions. The tronie was a distinctive type of painting, combining elements of the portrait, history, and genre painting. This was usually a half-length of a single figure which concentrated on capturing an unusual mood or expression. The actual identity of the model was not supposed to be important, but they might represent a historical figure and be in exotic or historic costume. In contrast to portraits, \"tronies\" were painted for the open market. They differ from figurative paintings and religious figures in that they are not restricted to a moral or narrative context. It is, rather, much more an exploration of the spectrum of human physiognomy and expression and the reflection of conceptions of character that are intrinsic to psychology's pre-history.\n\nRembrandt lighting is a lighting technique that is used in studio portrait photography.\nIt can be achieved using one light and a reflector, or two lights, and is popular because\nit is capable of producing images which appear both natural and compelling with a minimum\nof equipment. Rembrandt lighting is characterized by an illuminated triangle under the eye\nof the subject, on the less illuminated side of the face. It is named for the Dutch painter\nRembrandt, who often used this type of lighting in his portrait paintings.\n\nThe first known mezzotint was done in Amsterdam in 1642 by Utrecht-born German artist Ludwig von Siegen. He lived in Amsterdam from 1641 to about 1644, when he was supposedly influenced by Rembrandt.\n\nThe painter and printmaker Jan van de Velde is often credited to be the inventor of the aquatint technique, in Amsterdam around 1650.\n\nPronkstilleven (\"pronk still life\" or \"ostentatious still life\") is a type of banquet piece whose distinguishing feature is a quality of ostentation and splendor. These still lifes usually depict one or more especially precious objects. Although the term is a post-17th century invention, this type is characteristic of the second half of the seventeenth century. It was developed in the 1640s in Antwerp from where it spread quickly to the Dutch Republic. Flemish artists such as Frans Snyders and Adriaen van Utrecht started to paint still lifes that emphasized abundance by depicting a diversity of objects, fruits, flowers and dead game, often together with living people and animals. The style was soon adopted by artists from the Dutch Republic. A leading Dutch representative was Jan Davidsz. de Heem, who spent a long period of his active career in Antwerp and was one of the founders of the style in Holland.\n\nVincent van Gogh's work is most often associated with Post-Impressionism, but his innovative style had a vast influence on 20th-century art and established what would later be known as Expressionism, also greatly influencing fauvism and early abstractionism. His impact on German and Austrian Expressionists was especially profound. \"Van Gogh was father to us all,\" the German Expressionist painter Max Pechstein proclaimed in 1901, when Van Gogh's vibrant oils were first shown in Germany and triggered the artistic reformation, a decade after his suicide in obscurity in France. In his final letter to Theo, Van Gogh stated that, as he had no children, he viewed his paintings as his progeny. Reflecting on this, the British art historian Simon Schama concluded that he \"did have a child of course, Expressionism, and many, many heirs.\"\n\nDutch graphic artist Maurits Cornelis Escher, usually referred to as M. C. Escher, is known for his often mathematically inspired woodcuts, lithographs, and mezzotints. These feature impossible constructions, explorations of infinity, architecture and tessellations. His special way of thinking and rich graphic work has had a continuous influence in science and art, as well as permeating popular culture. His ideas have been used in fields as diverse as psychology, philosophy, logic, crystallography and topology. His art is based on mathematical principles like tessellations, spherical geometry, the Möbius strip, unusual perspectives, visual paradoxes and illusions, different kinds of symmetries and impossible objects. \"Gödel, Escher, Bach\" by Douglas Hofstadter discusses the ideas of self-reference and strange loops, drawing on a wide range of artistic and scientific work, including Escher's art and the music of J. S. Bach, to illustrate ideas behind Gödel's incompleteness theorems.\n\nMiffy (Nijntje) is a small female rabbit in a series of picture books drawn and written by Dutch artist Dick Bruna.\n\nIn music, the Franco-Flemish School or more precisely the Netherlandish School refers to the style of polyphonic vocal music composition in the Burgundian Netherlands in the 15th and early 16th centuries, and to the composers who wrote it.\n\nThe Venetian School of polychoral music was founded by the Netherlandish composer Adrian Willaert.\n\nHardcore or hardcore techno is a subgenre of electronic dance music originating in Europe from the emergent raves in the 1990s. It was initially designed at Rotterdam in Netherlands, derived from techno.\n\nHardstyle is an electronic dance genre mixing influences from hardtechno and hardcore. Hardstyle was influenced by gabber. Hardstyle has its origins in the Netherlands where artists like DJ Zany, Lady Dana, DJ Isaac, DJ Pavo, DJ Luna and The Prophet, who produced hardcore, started experimenting while playing their hardcore records.\n\nHolsteins or Holstein-Friesians are a breed of cattle known today as the world's highest-production dairy animals. Originating in Europe, Holstein-Friesians were bred in the two northern provinces of North Holland and Friesland, and Schleswig-Holstein in what became Germany. The animals were the regional cattle of the Frisians and the Saxons. The origins of the breed can be traced to the black cows and white cows of the Batavians and Frisians – migrant tribes who settled the coastal Rhine region more than two thousand years ago.\n\nForerunners to modern Brussels sprouts were likely cultivated in ancient Rome. Brussels sprouts as we now know them were grown possibly as early as the 13th century in the Low Countries (may have originated in Brussels). The first written reference dates to 1587. During the 16th century, they enjoyed a popularity in the Southern Netherlands that eventually spread throughout the cooler parts of Northern Europe.\n\nThrough history, carrots weren’t always orange. They were black, purple, white, brown, red and yellow. Probably orange too, but this was not the dominant colour. Orange-coloured carrots appeared in the Netherlands in the 16th century. Dutch farmers in Hoorn bred the color. They succeeded by cross-breeding pale yellow with red carrots. It is more likely that Dutch horticulturists actually found an orange rooted mutant variety and then worked on its development through selective breeding to make the plant consistent. Through successive hybridisation the orange colour intensified. This was developed to become the dominant species across the world, a sweet orange.\n\nBelle de Boskoop is an apple cultivar which, as its name suggests, originated in Boskoop, where it began as a chance seedling in 1856. There are many variants: Boskoop red, yellow or green. This rustic apple is firm, tart and fragrant. Greenish-gray tinged with red, the apple stands up well to cooking. Generally Boskoop varieties are very high in acid content and can contain more than four times the vitamin C of 'Granny Smith' or 'Golden Delicious'.\n\nKarmijn de Sonnaville is a variety of apple bred by Piet de Sonnaville, working in Wageningen in 1949. It is a cross of Cox's Orange Pippin and Jonathan, and was first grown commercially beginning in 1971. It is high both in sugars (including some sucrose) and acidity. It is a triploid, and hence needs good pollination, and can be difficult to grow. It also suffers from fruit russet, which can be severe. In Manhart's book, “apples for the 21st century”, Karmijn de Sonnaville is tipped as a possible success for the future. Karmijn de Sonnaville is not widely grown in large quantities, but in Ireland, at The Apple Farm, it is grown for fresh sale and juice-making, for which the variety is well suited.\n\nElstar apple is an apple cultivar that was first developed in the Netherlands in the 1950s by crossing Golden Delicious and Ingrid Marie apples. It quickly became popular, especially in Europe and was first introduced to America in 1972. It remains popular in Continental Europe. The Elstar is a medium-sized apple whose skin is mostly red with yellow showing. The flesh is white, and has a soft, crispy texture. It may be used for cooking and is especially good for making apple sauce. In general, however, it is used in desserts due to its sweet flavour.\n\nThe Groasis Waterboxx is a device designed to help grow trees in dry areas. It was developed by former flower exporter Pieter Hoff, and won \"Popular Science's\" \"Green Tech Best of What's New\" Innovation of the year award for 2010.\n\nThe Dutch-Frisian geographer Gemma Frisius was the first to propose the use of a chronometer to determine longitude in 1530. In his book \"On the Principles of Astronomy and Cosmography\" (1530), Frisius explains for the first time how to use a very accurate clock to determine longitude. The problem was that in Frisius’ day, no clock was sufficiently precise to use his method. In 1761, the British clock-builder John Harrison constructed the first marine chronometer, which allowed the method developed by Frisius.\n\nTriangulation had first emerged as a map-making method in the mid sixteenth century when the Dutch-Frisian mathematician Gemma Frisius set out the idea in his \"Libellus de locorum describendorum ratione\" (\"Booklet concerning a way of describing places\"). Dutch cartographer Jacob van Deventer was among the first to make systematic use of triangulation, the technique whose theory was described by Gemma Frisius in his 1533 book.\n\nThe modern systematic use of triangulation networks stems from the work of the Dutch mathematician Willebrord Snell (born Willebrord Snel van Royen), who in 1615 surveyed the distance from Alkmaar to Bergen op Zoom, approximately 70 miles (110 kilometres), using a chain of quadrangles containing 33 triangles in all – a feat celebrated in the title of his book \"Eratosthenes Batavus\" (\"The Dutch Eratosthenes\"), published in 1617.\n\nThe Mercator projection is a cylindrical map projection presented by the Flemish geographer and cartographer Gerardus Mercator in 1569. It became the standard map projection for nautical purposes because of its ability to represent lines of constant course, known as rhumb lines or loxodromes, as straight segments which conserve the angles with the meridians.\n\nFlemish geographer and cartographer Abraham Ortelius generally recognized as the creator of the world's first modern atlas, the \"Theatrum Orbis Terrarum\" (\"Theatre of the World\"). Ortelius's \"Theatrum Orbis Terrarum\" is considered the first true atlas in the modern sense: a collection of uniform map sheets and sustaining text bound to form a book for which copper printing plates were specifically engraved. It is sometimes referred to as the summary of sixteenth-century cartography.\n\nThe first printed atlas of nautical charts (\"De Spieghel der Zeevaerdt\" or \"The Mirror of Navigation\" / \"The Mariner's Mirror\") was produced by Lucas Janszoon Waghenaer in Leiden. This atlas was the first attempt to systematically codify nautical maps. This chart-book combined an atlas of nautical charts and sailing directions with instructions for navigation on the western and north-western coastal waters of Europe. It was the first of its kind in the history of maritime cartography, and was an immediate success. The English translation of Waghenaer's work was published in 1588 and became so popular that any volume of sea charts soon became known as a \"waggoner\", the Anglicized form of Waghenaer's surname.\n\nGerardus Mercator was the first to coin the word \"atlas\" to describe a bound collection of maps through his own collection entitled \"Atlas sive Cosmographicae meditationes de fabrica mvndi et fabricati figvra\". He coined this name after the Greek god who held the earth in his arms.\n\nThe Dutch Republic's explorers and cartographers like Pieter Dirkszoon Keyser, Frederick de Houtman, Petrus Plancius and Jodocus Hondius were the pioneers in first systematic charting/mapping of largely unknown southern hemisphere skies in the late 16th century.\n\nThe constellations around the South Pole were not observable from north of the equator, by Babylonians, Greeks, Chinese or Arabs. The modern constellations in this region were defined during the Age of Exploration, notably by Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman at the end of sixteenth century. These twelve Dutch-created represented flora and fauna of the East Indies and Madagascar. They were depicted by Johann Bayer in his star atlas \"Uranometria\" of 1603. Several more were created by Nicolas Louis de Lacaille in his star catalogue, published in 1756. By the end of the Ming Dynasty, Xu Guangqi introduced 23 asterisms of the southern sky based on the knowledge of western star charts. These asterisms have since been incorporated into the traditional Chinese star maps. Among the IAU's 88 modern constellations, there are 15 Dutch-created constellations (including Apus, Camelopardalis, Chamaeleon, Columba, Dorado, Grus, Hydrus, Indus, Monoceros, Musca, Pavo, Phoenix, Triangulum Australe, Tucana and Volans).\n\nThe speculation that continents might have 'drifted' was first put forward by Abraham Ortelius in 1596. The concept was independently and more fully developed by Alfred Wegener in 1912. Because Wegener's publications were widely available in German and English and because he adduced geological support for the idea, he is credited by most geologists as the first to recognize the possibility of continental drift. During the 1960s geophysical and geological evidence for seafloor spreading at mid-oceanic ridges established continental drift as the standard theory or continental origin and an ongoing global mechanism.\n\nWhile making a coloured liquid for a thermometer, Cornelis Drebbel dropped a flask of Aqua regia on a tin window sill, and discovered that stannous chloride makes the color of carmine much brighter and more durable. Though Drebbel himself never made much from his work, his daughters Anna and Catharina and his sons-in-law Abraham and Johannes Sibertus Kuffler set up a successful dye works. One was set up in 1643 in Bow, London, and the resulting color was called bow dye.\n\nDutch chemical company DSM invented and patented the Dyneema in 1979. Dyneema fibres have been in commercial production since 1990 at their plant at Heerlen. These fibers are manufactured by means of a gel-spinning process that combines extreme strength with incredible softness. Dyneema fibres, based on ultra-high-molecular-weight polyethylene (UHMWPE), is used in many applications in markets such as life protection, shipping, fishing, offshore, sailing, medical and textiles.\n\nIn 1962 Philips invented the compact audio cassette medium for audio storage, introducing it in Europe in August 1963 (at the Berlin Radio Show) and in the United States (under the \"Norelco\" brand) in November 1964, with the trademark name \"Compact Cassette\".\n\nLaserdisc technology, using a transparent disc, was invented by David Paul Gregg in 1958 (and patented in 1961 and 1990). By 1969, Philips developed a videodisc in reflective mode, which has great advantages over the transparent mode. MCA and Philips decided to join forces. They first publicly demonstrated the videodisc in 1972. Laserdisc entered the market in Atlanta, on 15 December 1978, two years after the VHS VCR and four years before the CD, which is based on Laserdisc technology. Philips produced the players and MCA made the discs.\n\nThe compact disc was jointly developed by Philips (Joop Sinjou) and Sony (Toshitada Doi). In the early 1970s, Philips' researchers started experiments with \"audio-only\" optical discs, and at the end of the 1970s, Philips, Sony, and other companies presented prototypes of digital audio discs.\n\nBluetooth, a low-energy, peer-to-peer wireless technology was originally developed by Dutch electrical engineer Jaap Haartsen and Swedish engineer Sven Mattisson in the 1990s, working at Ericsson in Lund, Sweden. It became a global standard of short distance wireless connection.\n\nIn 1991, NCR Corporation/AT&T Corporation invented the precursor to 802.11 in Nieuwegein. Dutch electrical engineer Vic Hayes chaired IEEE 802.11 committee for 10 years, which was set up in 1990 to establish a wireless networking standard. He has been called the father of Wi-Fi (the brand name for products using IEEE 802.11 standards) for his work on IEEE 802.11 (802.11a & 802.11b) standard in 1997.\n\nThe DVD optical disc storage format was invented and developed by Philips and Sony in 1995.\n\nAmbilight, short for \"ambient lighting\", is a lighting system for televisions developed by Philips in 2002.\n\nPhilips and Sony in 1997 and 2006 respectively, launched the Blu-ray video recording/playback standard.\n\nDijkstra's algorithm, conceived by Dutch computer scientist Edsger Dijkstra in 1956 and published in 1959, is a graph search algorithm that solves the single-source shortest path problem for a graph with non-negative edge path costs, producing a shortest path tree. Dijkstra's algorithm is so powerful that it not only finds the shortest path from a chosen source to a given destination, it finds all of the shortest paths from the source to all destinations. This algorithm is often used in routing and as a subroutine in other graph algorithms.\n\nDijkstra's algorithm is considered as one of the most popular algorithms in computer science. It is also widely used in the fields of artificial intelligence, operational research/operations research, network routing, network analysis, and transportation engineering.\n\nThrough his fundamental contributions Edsger Dijkstra helped shape the field of computer science. His groundbreaking contributions ranged from the engineering side of computer science to the theoretical one and covered several areas including compiler construction, operating systems, distributed systems, sequential and concurrent programming, software engineering, and graph algorithms. Many of his papers, often just a few pages long, are the source of whole new research areas. Several concepts that are now completely standard in computer science were first identified by Dijkstra and/or bear names coined by him.\n\nEdsger Dijkstra's foundational work on concurrency, semaphores, mutual exclusion, deadlock, finding shortest paths in graphs, fault-tolerance, self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built. The Edsger W. Dijkstra Prize in Distributed Computing (sponsored jointly by the ACM Symposium on Principles of Distributed Computing and the EATCS International Symposium on Distributed Computing) is given for outstanding papers on the principles of distributed computing, whose significance and impact on the theory and/or practice of distributed computing has been evident for at least a decade.\n\nThe academic study of concurrent programming (concurrent algorithms in particular) started in the 1960s, with Edsger Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion. A pioneer in the field of concurrent computing, Per Brinch Hansen considers Dijkstra's \"Cooperating Sequential Processes\" (1965) to be the first classic paper in concurrent programming. As Brinch Hansen notes: ‘Here Dijkstra lays the conceptual foundation for abstract concurrent programming.’\n\nComputer programming in the 1950s to 1960s was not recognized as an academic discipline and unlike physics there were no theoretical concepts or coding systems. Dijkstra was one of the moving forces behind the acceptance of computer programming as a scientific discipline. In 1968, computer programming was in a state of crisis. Dijkstra was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs. Dijkstra coined the phrase \"structured programming\" and during the 1970s this became the new programming orthodoxy. As Bertrand Meyer remarked: \"The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming.\"\n\nDijkstra's ideas about structured programming helped lay the foundations for the birth and development of the professional discipline of software engineering, enabling programmers to organize and manage increasingly complex software projects.\n\nIn computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the \"shunting yard\" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report.\n\nIn 1963/64, during an extended stay at SLAC, Dutch theoretical physicist Martinus Veltman designed the computer program \"Schoonschip\" for symbolic manipulation of mathematical equations, which is now considered the very first computer algebra system.\n\nIn computer science, mutual exclusion refers to the requirement of ensuring that no two concurrent processes are in their critical section at the same time; it is a basic requirement in concurrency control, to prevent race conditions. The requirement of mutual exclusion was first identified and solved by Edsger W. Dijkstra in his seminal 1965 paper titled \"Solution of a problem in concurrent programming control\", and is credited as the first topic in the study of concurrent algorithms.\n\nThe semaphore concept was invented by Dijkstra in 1965 and the concept has found widespread use in a variety of operating systems.\n\nIn computer science, the sleeping barber problem is a classic inter-process communication and synchronization problem between multiple operating system processes. The problem is analogous to that of keeping a barber working when there are customers, resting when there are none and doing so in an orderly manner. The Sleeping Barber Problem was introduced by Edsger Dijkstra in 1965.\n\nThe Banker's algorithm is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an \"s-state\" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue. The algorithm was developed in the design process for the THE multiprogramming system and originally described (in Dutch) in EWD108. The name is by analogy with the way that bankers account for liquidity constraints.\n\nIn computer science, the dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them. It was originally formulated in 1965 by Edsger Dijkstra as a student exam exercise, presented in terms of computers competing for access to tape drive peripherals.\nSoon after, Tony Hoare gave the problem its present formulation.\n\nDekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming. Dijkstra attributed the solution to Dutch mathematician Theodorus Dekker in his manuscript on cooperating sequential processes. It allows two threads to share a single-use resource without conflict, using only shared memory for communication. Dekker's algorithm is the first published software-only, two-process mutual exclusion algorithm.\n\nThe THE multiprogramming system was a computer operating system designed by a team led by Edsger W. Dijkstra, described in monographs in 1965–66 and published in 1968.\n\nVan Wijngaarden grammar (also vW-grammar or W-grammar) is a two-level grammar that provides a technique to define potentially infinite context-free grammars in a finite number of rules. The formalism was invented by Adriaan van Wijngaarden to rigorously define some syntactic restrictions that previously had to be formulated in natural language, despite their formal content. Typical applications are the treatment of gender and number in natural language syntax and the well-definedness of identifiers in programming languages. The technique was used and developed in the definition of the programming language ALGOL 68. It is an example of the larger class of affix grammars.\n\nIn 1968, computer programming was in a state of crisis. Dijkstra was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs. Dijkstra coined the phrase \"structured programming\" and during the 1970s this became the new programming orthodoxy. Structured programming is often regarded as “goto-less programming”. But as Bertrand Meyer notes, “As the first book on the topic [\"Structured Programming\" by Dijkstra, Dahl, and Hoare] shows, structured programming is about much more than control structures and the goto. Its principal message is that programming should be considered a scientific discipline based on mathematical rigor.” , structured programming – especially in the 1970s and 1980s – significantly influenced the birth of many modern programming languages such as Pascal, C, Modula-2, and Ada. The Fortran 77 version which incorporates the concepts of structured programming, was released in 1978. The C++ language was a considerably extended and enhanced version of the popular C (see also: list of C-based programming languages). Since C++ was developed from a more traditional , it is a 'hybrid language', rather than a pure object-oriented programming language.\n\nAn \"EPROM\" or \"erasable programmable read only memory\", is a type of memory chip that retains its data when its power supply is switched off. Development of the EPROM memory cell started with investigation of faulty integrated circuits where the gate connections of transistors had broken. Stored charge on these isolated gates changed their properties. The EPROM was invented by the Amsterdam-born Israeli electrical engineer Dov Frohman in 1971, who was awarded US patent 3660819 in 1972.\n\nSelf-stabilization is a concept of fault-tolerance in distributed computing. A distributed system that is self-stabilizing will end up in a correct state no matter what state it is initialized with. That correct state is reached after a finite number of execution steps. Many years after the seminal paper of Edsger Dijkstra in 1974, this concept remains important as it presents an important foundation for self-managing computer systems and fault-tolerant systems. Self-stabilization became its own area of study in distributed systems research, and Dijkstra set the stage for the next generation of computer scientists such as Leslie Lamport, Nancy Lynch, and Shlomi Dolev. As a result, Dijkstra's paper received the 2002 ACM PODC Influential-Paper Award (later renamed as \"Dijkstra Prize\" or \"Edsger W. Dijkstra Prize in Distributed Computing\" since 2003).\n\nPredicate transformer semantics were introduced by Dijkstra in his seminal paper \"Guarded commands, nondeterminacy and formal derivation of programs\".\n\nThe Guarded Command Language (GCL) is a language defined by Edsger Dijkstra for predicate transformer semantics. It combines programming concepts in a compact way, before the program is written in some practical programming language.\n\nA \"Van Emde Boas tree\" (or \"Van Emde Boas priority queue\", also known as a \"vEB tree\", is a tree data structure which implements an associative array with \"m\"-bit integer keys. The vEB tree was invented by a team led by Dutch computer scientist Peter van Emde Boas in 1975.\n\nABC is an imperative general-purpose programming language and programming environment developed at CWI, Netherlands by Leo Geurts, Lambert Meertens, and Steven Pemberton. It is interactive, structured, high-level, and intended to be used instead of BASIC, Pascal, or AWK. It is not meant to be a systems-programming language but is intended for teaching or prototyping.\n\nThe language had a major influence on the design of the Python programming language (as a counterexample); Guido van Rossum, who developed Python, previously worked for several years on the ABC system in the early 1980s.\n\nThe Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.\n\nSmoothsort is a comparison-based sorting algorithm. It is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort's upper bound is O(\"n\" log \"n\"). The advantage of smoothsort is that it comes closer to O(\"n\") time if the input is already sorted to some degree, whereas heapsort averages O(\"n\" log \"n\") regardless of the initial sorted state.\n\nThe Amsterdam Compiler Kit (ACK) is a fast, lightweight and retargetable compiler suite and toolchain developed by Andrew Tanenbaum and Ceriel Jacobs at the Vrije Universiteit in Amsterdam. It is MINIX's native toolchain. The ACK was originally closed-source software (that allowed binaries to be distributed for MINIX as a special case), but in April 2003 it was released under an open-source BSD license. It has frontends for programming languages C, Pascal, Modula-2, Occam, and BASIC. The ACK's notability stems from the fact that in the early 1980s it was one of the first portable compilation systems designed to support multiple source languages and target platforms.\n\nEFM (Eight-to-Fourteen Modulation) was invented by Dutch electrical engineer Kees A. Schouhamer Immink in 1985. EFM is a data encoding technique – formally, a channel code – used by CDs, laserdiscs and pre-Hi-MD MiniDiscs.\n\nMINIX (from \"mini-Unix\") is a Unix-like computer operating system based on a microkernel architecture. Early versions of MINIX were created by Andrew S. Tanenbaum for educational purposes. Starting with MINIX 3, the primary aim of development shifted from education to the creation of a highly reliable and self-healing microkernel OS. MINIX is now developed as open-source software. MINIX was first released in 1987, with its complete source code made available to universities for study in courses and research. It has been free and open-source software since it was re-licensed under the BSD license in April 2000. Tanenbaum created MINIX at the Vrije Universiteit in Amsterdam to exemplify the principles conveyed in his textbook, \"\" (1987), that Linus Torvalds described as \"the book that launched me to new heights\".\n\nAmoeba is a distributed operating system developed by Andrew S. Tanenbaum and others at the Vrije Universiteit in Amsterdam. The aim of the Amoeba project was to build a timesharing system that makes an entire network of computers appear to the user as a single machine. The Python programming language was originally developed for this platform.\n\nPython is a widely used general-purpose, high-level programming language. Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java. The language provides constructs intended to enable clear programs on both a small and large scale. Python supports multiple programming paradigms, including object-oriented, imperative and functional programming or procedural styles. It features a dynamic type system and automatic memory management and has a large and comprehensive standard library.\n\nPython was conceived in the late 1980s and its implementation was started in December 1989 by Guido van Rossum at CWI in the Netherlands as a successor to the ABC language (itself inspired by SETL) capable of exception handling and interfacing with the Amoeba operating system. Van Rossum is Python's principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, \"benevolent dictator for life\" (BDFL).\n\nVim is a text editor written by the Dutch free software programmer Bram Moolenaar and first released publicly in 1991. Based on the Vi editor common to Unix-like systems, Vim carefully separated the user interface from editing functions. This allowed it to be used both from a command line interface and as a standalone application in a graphical user interface.\n\nBlender is a professional free and open-source 3D computer graphics software product used for creating animated films, visual effects, art, 3D printed models, interactive 3D applications and video games. Blender's features include 3D modeling, UV unwrapping, texturing, raster graphics editing, rigging and skinning, fluid and smoke simulation, particle simulation, soft body simulation, sculpting, animating, match moving, camera tracking, rendering, video editing and compositing. Alongside the modelling features it also has an integrated game engine. Blender has been successfully used in the media industry in several parts of the world including Argentina, Australia, Belgium, Brazil, Russia, Sweden, and the United States.\n\nThe Dutch animation studio Neo Geo and Not a Number Technologies (NaN) developed Blender as an in-house application, with the primary author being Ton Roosendaal. The name \"Blender\" was inspired by a song by Yello, from the album \"Baby\".\n\nEFMPlus is the channel code used in DVDs and SACDs, a more efficient successor to EFM used in CDs. It was created by Dutch electrical engineer Kees A. Schouhamer Immink, who also designed EFM. It is 6% less efficient than Toshiba's SD code, which resulted in a capacity of 4.7 gigabytes instead of SD's original 5 GB. The advantage of EFMPlus is its superior resilience against disc damage such as scratches and fingerprints.\n\nThe Dutch East India Company (Verenigde Oostindische Compagnie, or VOC), founded in 1602, was the world's first multinational, joint-stock, limited liability corporation – as well as its first government-backed trading cartel. It was the first company to issue shares of stock and what evolved into corporate bonds. The VOC was also the first company to actually issue stocks and bonds through a stock exchange. In 1602, the VOC issued shares that were made tradable on the Amsterdam Stock Exchange. This invention enhanced the ability of joint-stock companies to attract capital from investors as they could now easily dispose their shares. The company was known throughout the world as the VOC thanks to its logo featuring those initials, which became the first global corporate brand. The company's monogram also became the first global logo.\n\nThe Dutch East India Company was arguably the first megacorporation, possessing quasi-governmental powers, including the ability to wage war, imprison and execute convicts, negotiate treaties, coin money and establish colonies. Many economic and political historians consider the Dutch East India Company as the most valuable, powerful and influential corporation in the world history.\n\nThe VOC existed for almost 200 years from its founding in 1602, when the States-General of the Netherlands granted it a 21-year monopoly over Dutch operations in Asia until its demise in 1796. During those two centuries (between 1602 and 1796), the VOC sent almost a million Europeans to work in the Asia trade on 4,785 ships, and netted for their efforts more than 2.5 million tons of Asian trade goods. By contrast, the rest of Europe combined sent only 882,412 people from 1500 to 1795, and the fleet of the English (later British) East India Company, the VOC's nearest competitor, was a distant second to its total traffic with 2,690 ships and a mere one-fifth the tonnage of goods carried by the VOC. The VOC enjoyed huge profits from its spice monopoly through most of the 17th century.\n\nA Dutch auction is also known as an \"open descending price auction\". Named after the famous auctions of Dutch tulip bulbs in the 17th century, it is based on a pricing system devised by Nobel Prize–winning economist William Vickrey. In the traditional Dutch auction, the auctioneer begins with a high asking price which is lowered until some participant is willing to accept the auctioneer's price. The winning participant pays the last announced price. Dutch auction is also sometimes used to describe online auctions where several identical goods are sold simultaneously to an equal number of high bidders. In addition to cut flower sales in the Netherlands, Dutch auctions have also been used for perishable commodities such as fish and tobacco.\n\nThe Dutch Republic was the birthplace of the first modern art market (\"open art market\" or \"free art market\"). The seventeenth-century Dutch were the pioneering arts marketers, successfully combining art and commerce together as we would recognise it today. Until the 17th century, commissioning works of art was largely the preserve of the church, monarchs and aristocrats. The emergence of a powerful and wealthy middle class in Holland, though, produced a radical change in patronage as the new Dutch bourgeoisie bought art. For the first time, the direction of art was shaped by relatively broadly-based demand rather than religious dogma or royal whim, and the result was a market which today's dealers and collectors would find familiar. With the creation of the first large-scale open art market, prosperous Dutch merchants, artisans, and civil servants bought paintings and prints in unprecedented numbers. Foreign visitors were astonished that even modest members of Dutch society such as farmers and bakers owned multiple works of art.\n\nThe seventeenth-century Dutch businessmen were the pioneers in laying the basis for modern corporate governance. Isaac Le Maire, an Amsterdam businessman and a sizeable shareholder of the VOC, became the first recorded investor to actually consider the corporate governance's problems. In 1609, he complained of the VOC's shoddy corporate governance. On January 24, 1609, Le Maire filed a petition against the VOC, marking the first recorded expression of shareholder activism. In what is the first recorded corporate governance dispute, Le Maire formally charged that the directors (the VOC's board of directors – the Heeren XVII) sought to “retain another’s money for longer or use it ways other than the latter wishes” and petitioned for the liquidation of the VOC in accordance with standard business practice.\n\nThe first shareholder revolt happened in 1622, among Dutch East India Company (VOC) investors who complained that the company account books had been “smeared with bacon” so that they might be “eaten by dogs.” The investors demanded a “reeckeninge,” a proper financial audit. The 1622 campaign by the shareholders of the VOC is a testimony of genesis of CSR (Corporate Social Responsibility) in which shareholders staged protests by distributing pamphlets and complaining about management self enrichment and secrecy.\n\nThe construction in 1619 of a train-oil factory on Smeerenburg in the Spitsbergen islands by the Noordsche Compagnie, and the acquisition in 1626 of Manhattan Island by the Dutch West India Company are referred to as the earliest cases of outward foreign direct investment (FDI) in Dutch and world history. Throughout the seventeenth century, the Dutch East India Company (VOC) and the Dutch West India Company (GWIC/WIC) also began to create trading settlements around the globe. Their trading activities generated enormous wealth, making the Dutch Republic one of the most prosperous countries of that time. The Dutch Republic's extensive arms trade occasioned an episode in the industrial development of early-modern Sweden, where arms merchants like Louis de Geer and the Trip brothers, invested in iron mines and iron works, another early example of outward foreign direct investment.\n\nIt was in the Dutch Republic that some important industries (economic sectors) such as shipbuilding, shipping, printing and publishing were developed on a large-scale export-driven model for the first time in history. The ship building district of Zaan, near Amsterdam, became the first industrialized area in the world, with around 900 industrial windmills at the end of the 17th century, but there were industrialized towns and cities on a smaller scale also. Other industries that saw significant growth were papermaking, sugar refining, printing, the linen industry (with spin-offs in vegetable oils, like flax and rape oil), and industries that used the cheap peat fuel, like brewing and ceramics (brickworks, pottery and clay-pipe making).\n\nThe Dutch shipbuilding industry was of modern dimensions, inclining strongly toward standardised, repetitive methods. It was highly mechanized and used many labor-saving devices-wind-powered sawmills, powered feeders for saw, block and tackles, great cranes to move heavy timbers-all of which increased productivity. Dutch shipbuilding benefited from various design innovations which increased carrying capacity and cut costs.\n\nEconomic historians consider the Netherlands as the first predominantly capitalist nation. The development of European capitalism began among the city-states of Italy, Flanders, and the Baltic. It spread to the European interstate system, eventually resulting in the world's first capitalist nation-state, the Dutch Republic of the seventeenth century. The Dutch were the first to develop capitalism on a nationwide scale (as opposed to earlier city states). They also played a pioneering role in the emergence of the capitalist world-system. Simon Schama aptly titled his work \"The Embarrassment of Riches\", capturing the astonishing novelty and success of the commercial revolution in the Dutch Republic.\n\nWorld-systems theorists (including Immanuel Wallerstein and Giovanni Arrighi) often consider the economic primacy of the Dutch Republic in the 17th century as the first capitalist hegemony in world history (followed by hegemonies of the United Kingdom in the 19th century and the United States in the 20th century).\n\nThe Dutch economic transition from a possession of the Holy Roman Empire in the 1590s to the foremost maritime and economic power in the world has been called the “Dutch Miracle” (or “Dutch Tiger”) by many economic historians, including K. W. Swart. Until the 18th century, the economy of the Dutch Republic was the most advanced and sophisticated ever seen in history.\nDuring their Golden Age, the provinces of the Northern Netherlands rose from almost total obscurity as the poor cousins of the industrious and heavily urbanised southern regions (Southern Netherlands) to become the world leader in economic success.\nThe Netherlands introduced many financial innovations that made it a major economic force – and Amsterdam became the world center for international finance. Its manufacturing towns grew so quickly that by the middle of the century the Netherlands had supplanted France as the leading industrial nation of the world.”\n\nDutch economist Jan Tinbergen developed the first national comprehensive macroeconomic model, which he first built for the Netherlands and after World War II later applied to the United States and the United Kingdom.\n\nThe concept of \"fair trade\" has been around for over 40 years, but a formal labelling scheme emerged only in the 1980s. At the initiative of Mexican coffee farmers, the world's first Fairtrade labeling organisation, Stichting Max Havelaar, was launched in the Netherlands on 15 November 1988 by Nico Roozen, Frans van der Hoff and Dutch ecumenical development agency Solidaridad. It was branded \"Max Havelaar\" after a fictional Dutch character who opposed the exploitation of coffee pickers in Dutch colonies.\n\nAn \"exchange\", or \"bourse\", is a highly organized market where (especially) tradable securities, commodities, foreign exchange, futures, and options contracts are sold and bought. The term \"bourse\" is derived from the 13th-century inn named \"Huis ter Beurze\" in Bruges, Low Countries, where traders and foreign merchants from across Europe conducted business in the late medieval period. The building, which was established by Robert van der Buerze as a hostelry, had operated from 1285. Its managers became famous for offering judicious financial advice to the traders and merchants who frequented the building. This service became known as the \"Beurze Purse\" which is the basis of \"bourse\", meaning an organised place of exchange.\n\nThe seventeenth-century Dutch merchants laid the foundations for modern stock market that now influences greatly the global economy. It was in the Dutch Republic that a fully-fledged stock market was established and developed for the first time in history. The Dutch merchants were also the pioneers in developing the basic techniques of stock trading. Although bond sales by municipalities and states can be traced to the thirteenth century, the origin of modern stock exchanges that specialize in creating and sustaining secondary markets in corporate securities goes back to the formation of the Dutch East India Company in the year 1602. Dutch investors were the first to trade their shares at a regular stock exchange. The Amsterdam Stock Exchange is considered the oldest in the world. It was established in 1602 by the Dutch East India Company for dealings in its printed stocks and bonds. Here, the Dutch also pioneered stock futures, stock options, short selling, debt-equity swaps, merchant banking, bonds, unit trusts and other speculative instruments. Unlike the competing companies, the VOC allowed anyone (including housemaids) to purchase stock in the trading at the fully operational Amsterdam Bourse. The practice of naked short selling was also invented in the Dutch Republic. In 1609, Isaac Le Maire, an Amsterdam merchant and a sizeable shareholder of the Dutch East India Company (VOC), became the first recorded short seller in history. The first recorded ban on short selling also took place in the Dutch Republic in the same year. In the early 17th century, Dutch merchants invented the common stock – that of the VOC. Also, the Dutch experienced the first recorded stock market crash in history, the Tulip Mania of 1636–1637. Since 1602, stock market trading has come a long way. But basically, the concept and principle of stock market trading is still upheld and is still being implemented up to now.\n\nThe Dutch Republic (Amsterdam in particular) was the birthplace of the world's first fully functioning financial market, with the birth of a fully fledged capital market. Capital markets for debt and equity shares are used to raise long-term funds. New stocks and bonds are sold in primary markets (including initial public offerings) and secondary markets (including stock exchanges). While the Italian city-states produced the first transferable municipal bonds, they didn't develop the other ingredient necessary to produce a fully fledged capital market: corporate shareholders. The Dutch East India Company (VOC) became the first company to offer shares of stock to the general public. Dutch investors were the first to trade their shares at a regular stock exchange. In 1602 the Dutch East India Company (VOC) established an exchange in Amsterdam where the VOC stocks and bonds could be traded in a secondary market. The buying and selling of the VOC's securities (including shares and bonds) became the basis of the first official stock market. The Dutch were also the first to use a fully-fledged capital market (including bond market and stock market) to finance companies (such as the VOC and the WIC). It was in seventeenth-century Amsterdam that the global securities market began to take on its modern form.\n\nWhat is now known as corporate finance has its modern roots in financial management policies of the Dutch East India Company (VOC) in the 17th century and some basic aspects of modern corporate finance began to appear in financial activities of Dutch businessmen in the early 17th century.\n\nThe earliest form of a company which issued \"public shares\" was the \"publicani\" during the Roman Republic. In 1602, the Dutch East India Company (Vereenigde Oost-Indische Compagnie or VOC) became the first modern company to issue shares to the public, thus launching the first modern initial public offering (IPO). The VOC held the first public offering of shares in history shortly after its founding. With this first recorded initial public offering (IPO), the VOC brought in 6,424,588 guilders and the company subsequently grew to become the first true transnational corporation in the world.\n\nThe Dutch were the pioneers in laying the basis for investment banking, allowing the risk of loans to be distributed among thousands of investors in the early seventeenth century.\n\nPrior to the 17th century most money was commodity money, typically gold or silver. However, promises to pay were widely circulated and accepted as value at least five hundred years earlier in both Europe and Asia. The Song Dynasty was the first to issue generally circulating paper currency, while the Yuan Dynasty was the first to use notes as the predominant circulating medium. In 1455, in an effort to control inflation, the succeeding Ming Dynasty ended the use of paper money and closed much of Chinese trade. The medieval European Knights Templar ran an early prototype of a central banking system, as their promises to pay were widely respected, and many regard their activities as having laid the basis for the modern banking system. As the first public bank to \"offer accounts not directly convertible to coin\", the Bank of Amsterdam (\"Amsterdamsche Wisselbank\" or literally Amsterdam Exchange Bank) established in 1609 is considered to be the precursor to modern central banks, if not the first true central bank. The Wisselbank's innovations helped lay the foundations for the birth and development of modern central banking systems. There were earlier banks, especially in the Italian city-states, but the Wisselbank, with its public backing, provided for a scale of operations and stability hitherto unmatched. Along with a number of subsidiary local banks, it performed many of modern-day central banking functions. The model of the Wisselbank as a state bank was adapted throughout Europe, including the Bank of Sweden (1668) and the Bank of England (1694). It occupied a central position in the financial world of its day, providing an effective, efficient and trusted system for national and international payments. The establishment of the Wisselbank led to the introduction of the concept of bank money – the bank guilder. Lucien Gillard (2004) calls it the \"European guilder\" (\"le florin européen\"), and Adam Smith devotes many pages to explaining how the bank guilder works (Smith 1776: 446–55). Considered by many experts to be the first internationally dominant reserve currency of modern times, the Dutch guilder was the dominant currency during the 17th and 18th centuries. It was just replaced by British pound sterling in the 19th century and the US dollar took the lead just after World War Two and has held it until this day.\n\nFinancial innovation in Amsterdam took many forms. In 1609, investors led by Isaac Le Maire formed history's first bear syndicate to engage in short selling, but their coordinated trading had only a modest impact in driving down share prices, which tended to be robust throughout the 17th century.\n\nIn the first decades of the 17th century, the VOC was the first recorded company ever to pay regular dividends. To encourage investors to buy shares, a promise of an annual payment (called a dividend) was made. An investor would receive dividends instead interest and the investment was permanent in the form of shares in the company. Between 1600 and 1800 the Dutch East India Company (VOC) paid annual dividends worth around 18 percent of the value of the shares.\n\nIn 1656, King Charles X Gustav of Sweden signed two charters creating two private banks under the directorship of Johan Palmstruch (though before having been ennobled he was called Johan Wittmacher or Hans Wittmacher), a Riga-born merchant of Dutch origin. Palmstruch modeled the banks on those of Amsterdam where he had become a burgher. The first real European banknote was issued in 1661 by the Stockholms Banco of Johan Palmstruch, a private bank under state charter (precursor to the Sveriges Riksbank, the central bank of Sweden).\n\nJoseph de la Vega, also known as Joseph Penso de la Vega, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman. His 1688 book \"Confusion de Confusiones\" (\"Confusion of Confusions\") explained the workings of the city's stock market. It was the earliest book about stock trading, taking the form of a dialogue between a merchant, a shareholder and a philosopher. The book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment. The book has been described as the first precursor of modern behavioural finance, with its descriptions of investor decision-making still reflected in the way some investors operate today, and in 2001 was still rated by the \"Financial Times\" as one of the ten best investment book ever written.\n\nThe principles of technical analysis are derived from hundreds of years of financial market data. These principles in a raw form have been studied since the seventeenth century. Some aspects of technical analysis began to appear in Joseph de la Vega's accounts of the Dutch markets in the late 17th century. In Asia, technical analysis is said to be a method developed by Homma Munehisa during the early 18th century which evolved into the use of candlestick techniques, and is today a technical analysis charting tool.\n\nJosseph de la Vega was in 1688 the first person to give an account of irrational behaviour in financial markets. His 1688 book \"Confusion of Confusions\", has been described as the first precursor of modern behavioural finance, with its descriptions of investor decision-making still reflected in the way some investors operate today.\n\nBy the first decades of the 18th century, Amsterdam had become the world’s leading financial centre for more than a century, having developed a sophisticated financial system with central banking, fully-fledged capital markets, certain kinds of financial derivatives, and publicly traded multinational corporations. Amsterdam was the first modern model of an international (global) financial centre that now operated in several countries around the world.\n\nIn the early 17th century, the Dutch revolutionized domestic and international finance by inventing common stock – that of the Dutch East India Company and founding a proto-central bank, the Wisselbank or Bank of Amsterdam. In 1609, the Dutch had already had a government bond market for some decades. Shortly thereafter, the Dutch Republic had in place, in one form or another, all of the key components of a modern financial system: formalized public credit, stable money, elements of a banking system, a central bank of sorts and securities markets. The Dutch Republic went on to become that century's leading economy.\n\nThe first investment fund has its roots back in 1774. A Dutch merchant named Adriaan van Ketwich formed a trust named \"Eendragt Maakt Magt\". The name of Ketwich's fund translates to \"unity creates strength\". In response to the financial crisis of 1772–1773, Ketwich's aim was to provide small investors an opportunity to diversify (Rouwenhorst & Goetzman, 2005). This investment scheme can be seen as the first near-mutual fund. In the years following, near-mutual funds evolved and become more diverse and complex.\n\nThe first mutual funds were established in 1774 in the Netherlands. Amsterdam-based businessman Abraham van Ketwich (a.k.a. Adriaan van Ketwich) is often credited as the originator of the world's first mutual fund. The first mutual fund outside the Netherlands was the Foreign & Colonial Government Trust, which was established in London in 1868.\n\nGibbing is the process of preparing salt herring (or soused herring), in which the gills and part of the gullet are removed from the fish, eliminating any bitter taste. The liver and pancreas are left in the fish during the salt-curing process because they release enzymes essential for flavor. The fish is then cured in a barrel with one part salt to 20 herring. Today many variations and local preferences exist on this process. The process of gibbing was invented by Willem Beuckelszoon (aka Willem Beuckelsz, William Buckels or William Buckelsson), a 14th-century Zealand Fisherman. The invention of this fish preservation technique led to the Dutch becoming a seafaring power. This invention created an export industry for salt herring that was monopolized by the Dutch.\n\nMany people believe it was the Dutch who invented doughnuts. A Dutch snack made from potatoes had a round shape like a ball, but, like Gregory's dough balls, needed a little longer time when fried to cook the inside thoroughly. These potato-balls developed into doughnuts when the Dutch finally made them into ring-shapes reduce frying time.\n\nGin is a spirit which derives its predominant flavour from juniper berries (\"Juniperus communis\"). From its earliest origins in the Middle Ages, gin has evolved over the course of a millennium from a herbal medicine to an object of commerce in the spirits industry. Gin was developed on the basis of the older Jenever, and become widely popular in Great Britain when William III of Orange, leader of the Dutch Republic, occupied the British throne with his wife Mary. Today, the gin category is one of the most popular and widely distributed range of spirits, and is represented by products of various origins, styles, and flavour profiles that all revolve around juniper as a common ingredient.\n\nThe Dutch physician Franciscus Sylvius is often credited with the invention of gin in the mid 17th century, although the existence of genever is confirmed in Massinger's play \"The Duke of Milan\" (1623), when Dr. Sylvius would have been but nine years of age. It is further claimed that British soldiers who provided support in Antwerp against the Spanish in 1585, during the Eighty Years' War, were already drinking genever (jenever) for its calming effects before battle, from which the term \"Dutch Courage\" is believed to have originated. The earliest known written reference to genever appears in the 13th century encyclopaedic work \"Der Naturen Bloeme\" (Bruges), and the earliest printed genever recipe from 16th century work \"Een Constelijck Distileerboec\" (Antwerp).\n\nA stroopwafel (also known as \"syrup waffle\", \"treacle waffle\" or \"caramel waffle\") is a waffle made from two thin layers of baked batter with a caramel-like syrup filling the middle. They were first made in Gouda in the 1780s. The traditional way to eat the stroopwafel is to place it atop of a drinking vessel with a hot beverage (coffee, tea or chocolate) inside that fits the diameter of the waffle. The heat from the rising steam warms the waffle and slightly softens the inside and makes the waffle soft on one side while still crispy on the other.\n\nIn 1815, Dutch chemist Coenraad Van Houten introduced alkaline salts to chocolate, which reduced its bitterness. In the 1820s, Casparus van Houten, Sr. patented an inexpensive method for pressing the fat from roasted cocoa beans. He created a press to remove about half the natural fat (cacao butter) from chocolate liquor, which made chocolate both cheaper to produce and more consistent in quality. This innovation introduced the modern era of chocolate. Van Houten developed the first cocoa powder producing machine in the Netherlands. Van Houten's machine – a hydraulic press – reduced the cocoa butter content by nearly half. This created a \"cake\" that could be pulverized into cocoa powder, which was to become the basis of all chocolate products. The press separated the greasy cocoa butter from cacao seeds, leaving a purer chocolate powder behind. This powder, much like the instant cocoa powder used today, was easier to stir into milk and water. As a result, another very important discovery was made: solid chocolate. By using cocoa powder and low amounts of cocoa butter, it was then possible to manufacture chocolate bar. The term \"chocolate\" then came to mean solid chocolate, rather than hot chocolate.\n\nDutch-processed chocolate or Dutched chocolate is chocolate that has been treated with an alkalizing agent to modify its color and give it a milder taste compared to \"natural cocoa\" extracted with the Broma process. It forms the basis for much of modern chocolate, and is used in ice cream, hot cocoa, and baking. The Dutch process was developed in the early 19th century by Dutch chocolate maker Coenraad Johannes van Houten, whose father Casparus is responsible for the development of the method of removing fat from cacao beans by hydraulic press around 1828, forming the basis for cocoa powder.\n\nIn 1609, Hugo Grotius, the Dutch jurist who is generally known as the father of modern international law, published his book \"Mare Liberum\" (\"The Free Sea\"), which first formulated the notion of \"freedom of the seas\". He developed this idea into a legal principle. It is said to be 'the first, and classic, exposition of the doctrine of the freedom of the seas' which has been the essence and backbone of the modern law of the sea. It is generally assumed that Grotius first propounded the principle of freedom of the seas, although all countries in the Indian Ocean and other Asian seas accepted the right of unobstructed navigation long before Grotius wrote his \"De Jure Praedae\" (\"On the Law of Spoils\") in the year of 1604. His work sparked a debate in the seventeenth century over whether states could exclude the vessels of other states from certain waters. Grotius won this debate, as \"freedom of the seas\" became a universally recognized legal principle, associated with concepts such as communication, trade and peace. Grotius's notion of the freedom of the seas would persist until the mid-twentieth century, and it continues to be applied even to this day for much of the high seas, though the application of the concept and the scope of its reach is changing.\n\nThe publication of \"De jure belli ac pacis\" (\"On the Laws of War and Peace\") by Hugo Grotius in 1625 had marked the emergence of international law as an 'autonomous legal science'. Grotius's \"On the Law of War and Peace\", published in 1625, is best known as the first systematic treatise on international law, but to thinkers of the seventeenth and eighteenth centuries, it seemed to set a new agenda in moral and political philosophy across the board. Grotius developed pivotal treatises on freedom of the seas, the law of spoils, the laws of war and peace and he created an autonomous place for international law as its own discipline. Jean Barbeyrac’s \"Historical and Critical Account of the Science of Morality\", attached to his translation of Samuel von Pufendorf’s \"Law of Nature and Nations\" in 1706, praised Grotius as “the first who broke the ice” of “the \"Scholastic Philosophy\"; which [had] spread itself all over Europe” (1749: 67, 66). Grotius' truly distinctive contribution to jurisprudence and philosophy of law (public international law or law of nations in particular) was that he secularized natural law. Grotius had divorced natural law from theology and religion by grounding it solely in the social nature and natural reason of man. When Grotius, considered by many to be the founder of modern natural law theory (or \"secular natural law\"), said that natural law would retain its validity 'even if God did not exist' (\"etiamsi daremus non esse Deum\"), he was making a clear break with the classical tradition of natural law. Adam Smith, in lectures delivered in 1762 on the subject of moral philosophy and the law of nations, said that: “Jurisprudence is that science which inquires into the general principles which ought to be the foundation of laws of all nations. Grotius seems to have been the first who attempted to give the world anything like a regular system of natural jurisprudence, and his treatise, 'On the Laws of War and Peace, ' with all its imperfections, is perhaps at this day the most complete work on this subject.”\n\nThe Grotian conception of international society became the most distinctive characteristic of the internationalist (or rationalist) tradition in international relations. This is why it is also called the \"Grotian tradition\". According to it international politics takes place within international society in which states are bound not only by rules of prudence or expediency but also of morality and law. Grotius was arguably not the first to formulate such a doctrine. However, he was first to clearly define the idea of one society of states, governed not by force or warfare but by laws and mutual agreement to enforce those laws. As many international law scholars noted, the spirit of the Peace of Westphalia (1648) was preceded with the thoughts and ideas of Grotius. Thomas Franck observed: ‘Since the Reformation, the Peace of Westphalia, and the writings of Hugo Grotius, there has been an explicit assumption that the international system is an association of sovereign states.’ As Hedley Bull declared: ‘The idea of international society which Grotius propounded was given concrete expression in the Peace of Westphalia’, affirming that ‘Grotius must be considered the intellectual father of this first general peace settlement of modern times’.\n\nBy the end of the seventeenth century, support was growing for some limitation to the seaward extent of territorial waters. What emerged was the so-called \"cannon shot rule\", which acknowledged the idea that property rights could be acquired by physical occupation and in practice to the effective range of shore-based cannon: about three nautical miles. The rule was long associated with Cornelis van Bijnkershoek, a Dutch jurist who, especially in his \"De Dominio Maris Dissertatio\" (1702), advocated a middle ground between the extremes of \"Mare Liberum\" and John Selden's \"Mare Clausum\", accepting both the freedom of states to navigate and exploit the resources of the high seas and a right of coastal states to assert wide-ranging rights in a limited marine territory.\n\nThe Permanent Court of Arbitration (PCA) is an international organization based in The Hague in the Netherlands. The court was established in 1899 as one of the acts of the first Hague Peace Conference, which makes it the oldest global institution for international dispute resolution. Its creation is set out under Articles 20 to 29 of the 1899 Hague Convention for the pacific settlement of international disputes, which was a result of the first Hague Peace Conference. The most concrete achievement of the Conference was the establishment of the PCA as the first institutionalized global mechanism for the settlement of disputes between states. The PCA encourages the resolution of disputes that involve states, state entities, intergovernmental organizations, and private parties by assisting in the establishment of arbitration tribunals and facilitating their work. The court offers a wide range of services for the resolution of international disputes which the parties concerned have expressly agreed to submit for resolution under its auspices. Dutch-Jew legal scholar Tobias Asser's role in the creation of the PCA at the first Hague Peace Conference (1899) earned him the Nobel Peace Prize in 1911.\n\nThe International Opium Convention, sometimes referred to as the Hague Convention of 1912, signed on 23 January 1912 at The Hague, was the first international drug control treaty and is the core of the international drug control system. The adoption of the Convention was a turning point in multilateralism, based on the recognition of the transnational nature of the drug problem and the principle of shared responsibility.\n\nDenmark was the first state to recognize a legal relationship for same-sex couples, establishing \"registered partnerships\" very much like marriage in 1989. In 2001, the Netherlands became the first nation in the world to grant same-sex marriages. The first laws enabling same-sex marriage in modern times were enacted during the first decade of the 21st century. , sixteen countries (Argentina, Belgium, Brazil, Canada, Denmark, France, Iceland, Netherlands, New Zealand, Norway, Portugal, Spain, South Africa, Sweden, United Kingdom, Uruguay) and several sub-national jurisdictions (parts of Mexico and the United States) allow same-sex couples to marry. Polls in various countries show that there is rising support for legally recognizing same-sex marriage across race, ethnicity, age, religion, political affiliation, and socioeconomic status.\n\nThe first mechanical clocks, employing the verge escapement mechanism with a foliot or balance wheel timekeeper, were invented in Europe at around the start of the 14th century, and became the standard timekeeping device until the pendulum clock was invented in 1656. The pendulum clock remained the most accurate timekeeper until the 1930s, when quartz oscillators were invented, followed by atomic clocks after World War 2.\n\nA pendulum clock uses a pendulum's arc to mark intervals of time. From their invention until about 1930, the most accurate clocks were pendulum clocks. Pendulum clocks cannot operate on vehicles or ships at sea, because the accelerations disrupt the pendulum's motion, causing inaccuracies. The pendulum clock was invented by Christian Huygens, based on the pendulum introduced by Galileo Galilei. Although Galileo studied the pendulum as early as 1582, he never actually constructed a clock based on that design. Christiaan Huygens invented pendulum clock in 1656 and patented the following year. He contracted the construction of his clock designs to clockmaker Salomon Coster, who actually built the clock.\n\nVarious authors have credited the invention of the thermometer to Cornelis Drebbel, Robert Fludd, Galileo Galilei or Santorio Santorio. The thermometer was not a single invention, however, but a development. However, each inventor and each thermometer was unique – there was no standard scale. In 1665 Christiaan Huygens suggested using the melting and boiling points of water as standards. The Fahrenheit scale is now usually defined by two fixed points: the temperature at which water freezes into ice is defined as 32 degrees Fahrenheit (°F), and the boiling point of water is defined to be 212 °F, a 180 degree separation, as defined at sea level and standard atmospheric pressure. In 1742, Swedish astronomer Anders Celsius created a temperature scale which was the reverse of the scale now known by the name \"Celsius\": \"0\" represented the boiling point of water, while \"100\" represented the freezing point of water. From 1744 until 1954, 0 °C was defined as the freezing point of water and 100 °C was defined as the boiling point of water, both at a pressure of one standard atmosphere with mercury being the working material.\n\nThe invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century. Some dispute remains as to whether British scientist Robert Hooke (his was a straight spring) or Dutch scientist Christiaan Huygens was the actual inventor of the balance spring. Huygens was clearly the first to successfully implement a spiral balance spring in a portable timekeeper. This is significant because up to that point the pendulum was the most reliable. This innovation increased watches' accuracy enormously, reducing error from perhaps several hours per day to perhaps 10 minutes per day, resulting in the addition of the minute hand to the face from around 1680 in Britain and 1700 in France.\n\nLike the invention of pendulum clock, Huygens' spiral hairspring (balance spring) system of portable timekeepers, helped lay the foundations for the modern watchmaking industry. The application of the spiral balance spring for watches ushered in a new era of accuracy for portable timekeepers, similar to that which the pendulum had introduced for clocks. From its invention in 1675 by Christiaan Huygens, the spiral hairspring (balance spring) system for portable timekeepers, still used in mechanical watchmaking industry today.\n\nVarious authors have credited the invention of the thermometer to Cornelis Drebbel, Robert Fludd, Galileo Galilei or Santorio Santorio. The thermometer was not a single invention, however, but a development. Though Galileo is often said to be the inventor of the thermometer, what he produced were thermoscopes. The difference between a thermoscope and a thermometer is that the latter has a scale. The first person to put a scale on a thermoscope is variously said to be Francesco Sagredo or Santorio Santorio in about 1611 to 1613.\n\nBefore there was the thermometer, there was the earlier and closely related thermoscope, best described as a thermometer without a temperature scale. A thermoscope only showed the differences in temperatures, for example, it could show something was getting hotter. However, the thermoscope did not measure all the data that a thermometer could, for example an exact temperature in degrees. What can be considered the first modern thermometer, the mercury thermometer with a standardized scale, was invented by German-Dutch scientist Daniel Gabriel Fahrenheit (who had settled in Amsterdam in 1701) in 1714. Fahrenheit invented the first truly accurate thermometer using mercury instead of alcohol and water mixtures. He began constructing his own thermometers in 1714, and it was in these that he used mercury for the first time.\n\nVarious authors have credited the invention of the thermometer to Cornelis Drebbel, Robert Fludd, Galileo Galilei or Santorio Santorio. The thermometer was not a single invention, however, but a development. However, each inventor and each thermometer was unique – there was no standard scale. In 1665 Christiaan Huygens suggested using the melting and boiling points of water as standards, and in 1694 Carlo Renaldini proposed using them as fixed points on a universal scale. In 1701 Isaac Newton proposed a scale of 12 degrees between the melting point of ice and body temperature. Finally in 1724 Daniel Gabriel Fahrenheit produced a temperature scale which now (slightly adjusted) bears his name. He could do this because he manufactured thermometers, using mercury (which has a high coefficient of expansion) for the first time and the quality of his production could provide a finer scale and greater reproducibility, leading to its general adoption. The Fahrenheit scale was the first widely used temperature scale. By the end of the 20th century, most countries used the Celsius scale rather than the Fahrenheit scale, though Canada retained it as a supplementary scale used alongside Celsius. Fahrenheit remains the official scale for Jamaica, the Cayman Islands, Belize, the Bahamas, Palau and the United States and associated territories.\n\nThe Snellen chart is an eye chart used by eye care professionals and others to measure visual acuity. Snellen charts are named after Dutch ophthalmologist Hermann Snellen who developed the chart in 1862. Vision scientists now use a variation of this chart, designed by Ian Bailey and Jan Lovie.\n\nPrevious to the string galvanometer, scientists used a machine called the capillary electrometer to measure the heart's electrical activity, but this device was unable to produce results at a diagnostic level. Dutch physiologist Willem Einthoven developed the string galvanometer in the early 20th century, publishing the first registration of its use to record an electrocardiogram in a Festschrift book in 1902. The first human electrocardiogram was recorded in 1887, however only in 1901 was a quantifiable result obtained from the string galvanometer.\n\nIn 1922, Dutch astronomer Jan Schilt invented the Schilt photometer, a device that measures the light output of stars and, indirectly, their distances.\n\nIn the 19th century it became clear that the heart generated electric currents. The first to systematically approach the heart from an electrical point-of-view was \"Augustus Waller\", working in St Mary's Hospital in Paddington, London. In 1911 he saw little clinical application for his work. The breakthrough came when Einthoven, working in Leiden, used his more sensitive string galvanometer, than the \"capillary electrometer\" that Waller used. Einthoven assigned the letters P, Q, R, S and T to the various deflections that it measured and described the electrocardiographic features of a number of cardiovascular disorders. He was awarded the 1924 Nobel Prize for Physiology or Medicine for his discovery.\n\nEinthoven's triangle is an imaginary formation of three limb leads in a triangle used in electrocardiography, formed by the two shoulders and the pubis. The shape forms an inverted equilateral triangle with the heart at the center that produces zero potential when the voltages are summed. It is named after Willem Einthoven, who theorized its existence.\n\nWhen German bombers attacked The Hague in 1940 while Willem Johan Kolff was there, he organised the first blood bank in continental Europe. It was located in the \"Zuidwal\" hospital in The Hague. Eleven patients were given blood transfusions in The Hague, six of whom survived. Donated blood was also used for victims of the bombardment of Rotterdam, whither it was transported by civilian car.\n\nAn artificial kidney is a machine and its related devices which clean blood for patients who have an acute or chronic failure of their kidneys. The first artificial kidney was developed by Dutchman Willem Johan Kolff. The procedure of cleaning the blood by this means is called \"dialysis\", a type of renal replacement therapy that is used to provide an artificial replacement for lost kidney function due to renal failure. It is a life support treatment and does not treat disease.\n\nOn 12 December 1957, Kolff implanted an artificial heart into a dog at Cleveland Clinic. The dog lived for 90 minutes. In 1967, Dr. Kolff left Cleveland Clinic to start the Division of Artificial Organs at the University of Utah and pursue his work on the artificial heart. Under his supervision, a team of surgeons, chemists, physicists and bioengineers developed an artificial heart and made it ready for industrial production. To help manage his many endeavors, Dr. Kolff assigned project managers. Each project was named after its manager. Graduate student Robert Jarvik was the project manager for the artificial heart, which was subsequently renamed the \"Jarvik-7\". Based on lengthy animal trials, this first artificial heart was successfully implanted into the thorax of patient Barney Clark in December 1982. Clark survived 112 days with the device.\n\nThe Dutch Republic has been considered by many political and military historians as the first modern (global) sea power. The United Provinces of the Netherlands was the first state to possess the full triad of foreign commerce, forward bases and merchant and naval fleets. In the middle of the 17th century the Dutch navy was the most powerful navy in the world. The Dutch Republic had a commercial fleet that was larger than that of England, France, Germany, Portugal, and Spain combined. According to Walter Russell Mead, the “modern version of sea power was invented by the Dutch. The system of global trade, investment, and military power the Dutch built in the seventeenth century was the envy and the wonder of the world at the time, and many of its basic features were adopted by the British and the Americans in subsequent years.” When the Peter the Great determined to achieve sea power for Imperial Russia, he came to the Dutch Republic to learn about shipbuilding, seamanship and nautical sciences. During his stay in Holland (1697) the Tsar engaged, with the help of Russian and Dutch assistants, many skilled workers such as builders of locks, fortresses, shipwrights and seamen. They had to help him with his modernization of Russia. The best-known sailor who made the journey from the Dutch Republic to Russia was Norwegian-Dutch Cornelius Cruys. Cruys performed well in Russia and came be regarded as the architect of the Russian Navy. He became the first commander of the Russian Baltic Fleet and the vice admiral of the Imperial Russian Navy. Peter the Great designed his new capital on the model of Amsterdam and gave it a Dutch name, Sint Pieterburgh (later Germanized into Sankt Peterburg). In St. Petersburg, there is an island which is still called Novaya Gollandiya (literally “New Holland”). The triangular man-made island took its name after a number of canals and shipbuilding facilities that rendered its appearance similar to Amsterdam. The Tsar chose to call his island “New Holland”, commemorating his enthusiasm for all things Dutch.\n\nThe early modern Military Revolution began with reforms inaugurated by Prince Maurice of Nassau with his cousins Count Willem Lodewijk of Nassau-Dillenburg and Count John VII of Nassau during the 1590s. Maurice developed a system of linear formations (linear tactics), discipline, drill and volley fire based on classical Roman methods that made his army more efficient and his command and control more effective. He also developed a 43-step drill for firing the musket that was included in an illustrated weapons manual by Jacob de Gheyn II in 1607 (\"Wapenhandelinghe\" or \"Exerise of Arms\"). This became known as the \"Dutch drill\". It was widely read and emulated in the rest of Europe. Adopting and perfecting the techniques pioneered by Maurice of Nassau several decades earlier, Gustavus Adolphus repeatedly proved his techniques by defeating the armies of the Holy Roman empire(1630–1632), an adversary with resources fantastically larger than Sweden's during the Thirty Years' War. Descartes served for a while in the army of the Dutch military leader Prince Maurice of Orange-Nassau, and developed a fascination for practical technology. Maurice' s military innovations had considerable influences on Descartes' system of philosophy.\n\nThe Norden bombsight was designed by Carl Norden, a Dutch engineer educated in Switzerland who emigrated to the U.S. in 1904. In 1920, he started work on the Norden bombsight for the United States Navy. The first bombsight was produced in 1927. It was essentially an analog computer, and bombardiers were trained in great secrecy on how to use it. The device was used to drop bombs accurately from an aircraft, supposedly accurate enough to hit a 100-foot circle from an altitude of 21,000 feet – but under actual combat situations, such an accuracy was never achieved.\n\nA submarine snorkel is a device that allows a submarine to operate submerged while still taking in air from above the surface. It was invented by the Dutchman J.J. Wichers shortly before World War II and copied by the Germans during the war for use by U-Boats. Its common military name is snort.\n\nGoalkeeper is a close-in weapon system (CIWS) still in use as of 2015. It is autonomous and completely automatic short-range defense of ships against highly maneuverable missiles, aircraft and fast maneuvering surface vessels. Once activated the system automatically performs the entire process from surveillance and detection to destruction, including selection of priority targets.\n\nThe first (mechanical) metronome was invented by Dietrich Nikolaus Winkel in Amsterdam in 1812, but named (patented) after Johann Maelzel, who took the idea and popularized it.\n\nDutch musician-physicist Adriaan Fokker designed and had built keyboard instruments capable of playing microtonal scales via a generalized keyboard. The best-known of these is his 31-tone equal-tempered organ, which was installed in Teylers Museum in Haarlem in 1951. It is commonly called the Fokker organ.\n\nThe Kraakdoos or Cracklebox is a custom-made battery-powered noise-making electronic device. It is a small box with six metal contacts on top, which when pressed by fingers generates unusual sounds and tones. The human body becomes a part of the circuit and determines the range of sounds possible – different players generate different results. The concept was first conceived by Michel Waisvisz and Geert Hamelberg in the 1960s, and developed further in the 1970s when Waisvisz joined the STEIM foundation in Amsterdam.\n\nThe Moodswinger is a twelve-string electric zither with an additional third bridge designed by Dutch luthier Yuri Landman. The rod functions as the third bridge and divides the strings into two sections to add overtones, creating a multiphonic sound.\n\nThe Springtime is an experimental electric guitar with seven strings and three outputs. Landman created the instrument in 2008.\n\nNeostoicism was a syncretic philosophical movement, joining Stoicism and Christianity. Neostoicism was founded by Dutch-Flemish humanist Justus Lipsius, who in 1584 presented its rules, expounded in his book \"De Constantia\" (\"On Constancy\"), as a dialogue between Lipsius and his friend Charles de Langhe. The eleven years (1579–1590) that Lipsius spent in Leiden (Leiden University) were the period of his greatest productivity. It was during this time that he wrote a series of works designed to revive ancient Stoicism in a form that would be compatible with Christianity. The most famous of these is \"De Constantia\" (1584). Neostoicism had a direct influence on many seventeenth and eighteenth-century writers including Montesquieu, Bossuet, Francis Bacon, Joseph Hall, Francisco de Quevedo and Juan de Vera y Figueroa.\n\nThe rise of modern rationalism in the Dutch Republic, had a profound influence on the 17th-century philosophy. Descartes is often considered to be the first of the modern rationalists. Descartes himself had lived in the Dutch Republic for some twenty years (1628–1649) and served for a while in the army of the Dutch military leader Prince Maurice of Orange-Nassau. The Dutch Republic was the first country in which Descartes' rationalistic philosophy (Cartesianism) succeeded in replacing Aristotelianism as the academic orthodoxy. Fritz Berolzheimer considers Hugo Grotius the Descartes of legal philosophy and notes Grotian rationalism's influence on the 17th-century jurisprudence: \"As the Cartesian \"cogito ergo sum\" became the point of departure of rationalistic philosophy, so the establishment of government and law upon reason made Hugo Grotius the founder of an independent and purely rationalistic system of natural law.\" In the late 1650s Leiden was a place where one could study Cartesian philosophy. Sometime between 1656 and 1661 it appears that Spinoza did some formal study of philosophy at the University of Leiden. Philosophy of Spinoza (Spinozism) was an systematic answer to Descartes' famous dualist theory that the body and spirit are separate.\n\nPantheism was popularized in the modern era as both a theology and philosophy based on the work of the 17th-century Dutch Jew philosopher Baruch Spinoza, whose \"Ethics\" was an answer to Descartes' famous dualist theory that the body and spirit are separate. Spinoza is regarded as the chief source of modern pantheism. Spinoza held that the two are the same, and this monism is a fundamental quality of his philosophy. He was described as a \"God-intoxicated man,\" and used the word God to describe the unity of all substance. Although the term pantheism was not coined until after his death, Spinoza is regarded as its most celebrated advocate.\n\nEuropean liberalism, Isaiah Berlin wrote, \"wears the appearance of a single coherent movement, little altered during almost three centuries, founded upon relatively simple foundations, laid by Locke or Grotius or even Spinoza; stretching back to Erasmus and Montaigne...\"\n\nAs Bertrand Russell noted in his \"A History of Western Philosophy\" (1945): \"Descartes lived in Holland for twenty years (1629–49), except for a few brief visits to France and one to England, all on business. It is impossible to exaggerate the importance of Holland in the seventeenth century, as the one country where there was freedom of speculation. Hobbes had to have his books printed there; Locke took refuge there during the five worst years of reaction in England before 1688; Bayle (of the \"Dictionary\") found it necessary to live there; and Spinoza would hardly have been allowed to do his work in any other country.\" Russell described early liberalism in Europe: \"Early liberalism was a product of England and Holland, and had certain well-marked characteristics. It stood for religious toleration; it was Protestant, but of a latitudinarian rather than of a fanatical kind; it regarded the wars of religion as silly...\"\n\nAs Russell Shorto states: “Liberalism has many meanings, but in its classical sense it is a philosophy based on individual freedom. History has long taught that our modern sensibility comes from the eighteenth century Enlightenment. In recent decades, historians have seen the Dutch Enlightenment of the seventeenth century as the root of the wider Enlightenment. And at the center of this sits the city of Amsterdam.” Amsterdam, to Shorto, was not only the first city in Europe to develop the cultural and political foundations of what we now call liberalism – a society focused on the concerns and comforts of individuals, run by individuals acting together, and tolerant of religion, ethnicity, or other differences – but also an exporter of these beliefs to the rest of Europe and the New World.\n\nIf Descartes is still considered the father of modern philosophy, Dutch Republic can be called its cradle. Cartesianism is the name given to the philosophical doctrine of René Descartes. Descartes is often regarded as the first thinker to emphasize the use of reason to develop the natural sciences. Cartesianism had been controversial for several years before 1656. Descartes himself had lived in the Dutch Republic for some twenty years (1628–1649). Descartes served for a while in the army of the Dutch military leader Prince Maurice of Orange-Nassau, and developed a fascination for practical technology. In the 1630s, while staying in the Dutch city Deventer, Descartes worked on a text which became published as \"Traite' de l'Homme\" (1664). Throughout his writing, he used words such as clock, automaton, and self – moving machine as interchangeable constructs. He postulated an account of the physical world that was thoroughly materialistic. His mechanical view of nature replaced the organism model which had been popular since the Renaissance. His \"Discours de la méthode\" (1637) was originally published at Leiden, and his \"Principia philosophiae\" (1644) appeared from the presses at Amsterdam. In the 1630s and 1640s, Descartes's ideas gained a foothold at the Dutch universities.\n\nSpinozism is the monist philosophical system of the Dutch-Jewish philosopher Baruch Spinoza which defines \"God\" as a singular self-subsistent substance, with both matter and thought as its attributes.\n\nAffect (\"affectus\" or \"adfectus\" in Latin) is a concept used in the philosophy of Spinoza and elaborated by Henri Bergson, Gilles Deleuze and Félix Guattari that emphasizes bodily experience. The term \"affect\" is central to what became known as the \"affective turn\" in the humanities and social sciences.\n\nMandeville's paradox is named after Bernard Mandeville, who shows that actions which may be qualified as vicious with regard to individuals have benefits for society as a whole. This is already clear from the subtitle of his most famous work, \"The Fable of The Bees\": ‘Private Vices, Publick Benefits’. He states that \"Fraud, Luxury, and Pride must live; Whilst we the Benefits receive.\") (The Fable of the Bees, ‘The Moral’).\n\nMathematical intuitionism was founded by the Dutch mathematician and philosopher Luitzen Egbertus Jan Brouwer. In the philosophy of mathematics, intuitionism, or \"neointuitionism\" (opposed to preintuitionism), is an approach where mathematics is considered to be purely the result of the constructive mental activity of humans rather than the discovery of fundamental principles claimed to exist in an objective reality. That is, logic and mathematics are not considered analytic activities wherein deep properties of objective reality are revealed and applied, but are instead considered the application of internally consistent methods used to realize more complex mental constructs, regardless of their possible independent existence in an objective reality.\n\nDevotio Moderna, or Modern Devotion, was a movement for religious reform, calling for apostolic renewal through the rediscovery of genuine pious practices such as humility, obedience and simplicity of life. It began in the late fourteenth-century, largely through the work of Gerard Groote, and flourished in the Low Countries and Germany in the fifteenth century, but came to an end with the Protestant Reformation. Gerard Groote, father of the movement, founded the Brethren of the Common Life; after his death, disciples established a house of Augustinian Canons at Windesheim (near Zwolle, Overijssel). These two communities became the principal exponents of Devotio Moderna. Martin Luther studied under the Brethren of the Common Life at Magdeburg before going on to the University of Erfurt. Another famous member of the Brethren of the Common Life was Desiderius Erasmus of Rotterdam.\n\nDevotio Moderna, an undogmatic form of piety which some historians have argued helped to pave the road for the Protestant Reformation, is most known today through its influence on Thomas à Kempis, the author of \"The Imitation of Christ\" a book which proved highly influential for centuries.\n\nThe Mennonites are a Christian group based around the church communities of Anabaptist denominations named after Menno Simons (1496–1561) of Friesland. Through his writings, Simons articulated and formalized the teachings of earlier Swiss founders. The teachings of the Mennonites were founded on their belief in both the mission and ministry of Jesus Christ, which they held to with great conviction despite persecution by various Roman Catholic and Protestant states.\n\nThe Dutch Reformed Church (in Dutch: \"Nederlandse Hervormde Kerk\" or NHK) was a Reformed Christian denomination. It developed during the Protestant Reformation, with its base in what became known as the Roman Catholic Church. It was founded in the 1570s and lasted until 2004, the year it merged with the Reformed Churches in the Netherlands and the Evangelical Lutheran Church in the Kingdom of the Netherlands to form the Protestant Church in the Netherlands.\n\nArminianism is based on the theological ideas of Dutch Reformed theologian Jacobus Arminius (1560–1609) and his historic supporters known as the Remonstrants. His teachings held to the five solae of the Reformation, but they were distinct from the particular teachings of Martin Luther, Zwingli, John Calvin, and other Protestant Reformers. Arminius (Jacobus Hermanszoon) was a student of Beza (successor of Calvin) at the Theological University of Geneva.\n\nMany Christian denominations have been influenced by Arminian views on the will of man being freed by grace prior to regeneration, notably the Baptists in the 16th century, the Methodists in the 18th century and the Seventh-day Adventist Church. John Wesley was influenced by Arminianism. Also, Arminianism was an important influence in Methodism, which developed out of the Wesleyan movement. Some assert that Universalists and Unitarians in the 18th and 19th centuries were theologically linked with Arminianism.\n\nThe first synagogue of the New World, Kahal Zur Israel Synagogue, is founded in Recife, Brazil by the Dutch Jews. The Kahal Zur Israel Synagogue in Recife, Brazil, erected in 1636, was the first synagogue erected in the Americas. Its foundations have been recently discovered, and the 20th-century buildings on the site have been altered to resemble a 17th-century Dutch synagogue.\n\nJansenism was a Catholic theological movement, primarily in France, that emphasized original sin, human depravity, the necessity of divine grace, and predestination. The movement originated from the posthumously published work (\"Augustinus\") of the Dutch theologian Cornelius Jansen, who died in 1638. It was first popularized by Jansen's friend Abbot Jean Duvergier de Hauranne, of Saint-Cyran-en-Brenne Abbey, and after Duvergier's death in 1643, was led by Antoine Arnauld. Through the 17th and into the 18th centuries, Jansenism was a distinct movement within the Catholic Church. The theological centre of the movement was the convent of Port-Royal Abbey, Paris, which was a haven for writers including Duvergier, Arnauld, Pierre Nicole, Blaise Pascal, and Jean Racine.\n\nCongregation Shearith Israel, the Spanish and Portuguese Synagogue in the City of New Amsterdam, was founded in 1654, the first Jewish congregation to be established in North America. Its founders were twenty-three Jews, mostly of Spanish and Portuguese origin, who had been living in Recife, Brazil. When the Portuguese defeated the Dutch for control of Recife, and brought with them the Inquisition, the Jews of that area left. Some returned to Amsterdam, where they had originated. Others went to places in the Caribbean such as St. Thomas, Jamaica, Surinam and Curaçao, where they founded sister Sephardic congregations. One group of twenty-three Jews, after a series of unexpected events, landed in New Amsterdam. After being initially rebuffed by anti-Semitic Governor Peter Stuyvesant, Jews were given official permission to settle in the colony in 1655. These pioneers fought for their rights and won permission to remain. This marks the founding of the Congregation Shearith Israel.\n\nThe first historical records of a telescope appear in patents filed 1608 by Hans Lippershey and Jacob Metius. A description of Lippershey's instrument quickly reached Galileo Galilei, who created an improved version in 1609, with which he made the observations found in his \"Sidereus Nuncius\" of 1610.\n\nHuygens eyepieces consist of two plano-convex lenses with the plane sides towards the eye separated by an air gap. The lenses are called the eye lens and the field lens. The focal plane is located between the two lenses. It was invented by Christiaan Huygens in the late 1660s and was the first compound (multi-lens) eyepiece. Huygens discovered that two air spaced lenses can be used to make an eyepiece with zero transverse chromatic aberration. These eyepieces work well with the very long focal length telescopes (in Huygens day they were used with single element long focal length non-achromatic refracting telescopes, including very long focal length aerial telescopes). This optical design is now considered obsolete since with today's shorter focal length telescopes the eyepiece suffers from short eye relief, high image distortion, chromatic aberration, and a very narrow apparent field of view. Since these eyepieces are cheap to make they can often be found on inexpensive telescopes and microscopes. Because Huygens eyepieces do not contain cement to hold the lens elements, telescope users sometimes use these eyepieces in the role of \"solar projection\", i.e. projecting an image of the Sun onto a screen. Other cemented eyepieces can be damaged by the intense, concentrated light of the Sun.\n\nUsing an improved simple microscope, in 1673 Antonie van Leeuwenhoek becomes the first to discover, observe, describe, study and conduct scientific experiments with single-celled organisms, which he originally referred to as \"animalcules\", and which now referred to as micro-organisms or microbes. For these observations he created at least 25 simple microscopes, of differing types, of which only nine survive. His simple microscopes were made of silver or copper frames, holding specially shaped single glass sphere that acted as a small lens. The smaller the sphere, the more in magnified. Those that have survived are capable of magnification up to 275 times. It is suspected that Van Leeuwenhoek possessed units that could magnify up to 500 times.\n\nThe cycloid pendulum was invented by Christiaan Huygens in 1673. Its purpose is to eliminate the lack of isochronism of the ordinary simple pendulum. This is achieved by making the mass point move on a cycloid instead of a circular arc.\n\nThe pyrometer, invented by Pieter van Musschenbroek, is a temperature measuring device. A simple type uses a thermocouple placed either in a furnace or on the item to be measured. The voltage output of the thermocouple is read from a meter. Many different types of thermocouple are available, for measuring temperatures from −200 °C to above 1500 °C.\n\nA Leyden jar, or Leiden jar, is a device that \"stores\" static electricity between two electrodes on the inside and outside of a glass jar. It was the original form of a capacitor (originally known as a \"condenser\"). It was invented independently by German cleric Ewald Georg von Kleist on 11 October 1745 and by Dutch scientist Pieter van Musschenbroek of Leiden (Leyden) in 1745–1746. The invention was named for the city. The Leyden jar was used to conduct many early experiments in electricity, and its discovery was of fundamental importance in the study of electricity. Previously, researchers had to resort to insulated conductors of large dimensions to store a charge. The Leyden jar provided a much more compact alternative. Like many early electrical devices, there was no particular use for the Leyden jar at first, other than to allow scientists to do a greater variety of electrical experiments. Benjamin Franklin, for example, used a Leyden jar to store electricity from lightning in his famous kite experiment in 1752. By doing so he proved that lightning was really electricity.\n\nThe idea for the Leyden jar was discovered independently by two parties: German scientist and jurist Ewald Georg von Kleist, and Dutchmen Pieter van Musschenbroek and Andreas Cunaeus. These scientists developed the Leyden jar while working under a theory of electricity that saw electricity as a fluid, and hoped to develop the jar to \"capture\" this fluid. In 1744 von Kleist lined a glass jar with silver foil, and charged the foil with a friction machine. Kleist was convinced that a substantial electric charge could be collected when he received a significant shock from the device. The effects of this \"Kleistian jar\" were independently discovered around the same time by Dutch scientists Pieter van Musschenbroek and Cunaeus at the University of Leiden. Van Musschenbroek communicated on it with the French scientific community where it was called the Leyden jar.\n\nThe Eisinga Planetarium (Royal Eise Eisinga Planetarium) was built by Eise Eisinga in his home in Franeker, Friesland. It took Eisinga seven years to build his planetarium, completing it in 1781. The orrery still exists and is the world's oldest working planetarium.\n\nKipp's apparatus, also called a Kipp generator, is designed for preparation of small volumes of gases. It was invented around 1860 by Dutch pharmacist Petrus Jacobus Kipp and widely used in chemical laboratories and for demonstrations in schools into the second half of the 20th century.\n\nIn optical microscopy many objects such as cell parts in protozoans, bacteria and sperm tails are essentially fully transparent unless stained (and therefore killed). The difference in densities and composition within these objects however often gives rise to changes in the phase of light passing through them, hence they are sometimes called \"phase objects\". Using the phase-contrast technique makes these structures visible and allows the study of living specimens. This phase contrast technique proved to be such an advancement in microscopy that Dutch physicist Frits Zernike was awarded the Nobel Prize in 1953.\n\nThe magnetic horn (also known as the \"Van der Meer horn\") is a high-current, pulsed focusing device, invented by the Dutch physicist Simon van der Meer at CERN. It selects pions and focuses them into a sharp beam. Its original application was in the context of neutrino physics, where beams of pions have to be tightly focused. When the pions then decay into muons and neutrinos or antineutrinos, an equally well-focused neutrino beam is obtained. The muons were stopped in a wall of 3000 tons of iron and 1000 tons of concrete, leaving the neutrinos or antineutrinos to reach the Gargamelle bubble chamber.\n\nA golf-like game (\"kolf\" in Dutch) is recorded as taking place on 26 February 1297, in a city called Loenen aan de Vecht, where the Dutch played a game with a stick and leather ball. The winner was whomever hit the ball with the least number of strokes into a target several hundred yards away. Some scholars argue that this game of putting a small ball in a hole in the ground using clubs was also played in 17th-century Netherlands and that this predates the game in Scotland.\n\nThe Dutch played a significant role in the history of ice skating (including speed skating and figure skating). The first feature of ice skating in a work of art was made in the 15th century. The picture, depicted Saint Lidwina, patron saint of ice skaters, falling on the ice. Another important aspect is a man seen in the background, who is skating on one leg. This means that his skates must have had sharp edges similar to those found on modern ice skates. Until the 17th century, ice skating was mostly used for transportation. Some of the Stuarts (including King Charles II of England) who had fled to the Dutch Republic during the Cromwell Royal reign later returned to Britain, bringing with them the new sport. Upon his return to England in 1658, the King brought two innovations in ice skating – a pair of iron skates and the Dutch roll. The Dutch roll was the first form of a gliding or skating motion made possible by the iron skate's two edges. However, speed skating was the focus of the Dutch, while the English developed modern figure skating.\n\nSpeed skating, which had developed in the Netherlands in the 17th century, was given a boost by the innovations in skate construction. Speed skating, or speedskating, is a competitive form of skating in which skaters race each other over a certain distance. Types of speed skating are long track speed skating, short track speed skating and marathon speed skating. In the modern Olympic Games, long-track speed skating is usually referred to as just \"speed skating\", while short-track speed skating is known as \"short track\".\n\nSailing, also known as yachting, is a sport in which competitors race from point to point, or around a race course, in sail-powered boats. Yachting refers to recreational sailing or boating, the specific act of sailing or using other water vessels for sporting purposes. The invention of sailing is prehistoric, but the racing of sailing boats is believed to have started in the Netherlands some time in the 17th century. While living in the Dutch Republic, King Charles II of England fell in love with sailing and in 1660, took home the Dutch gifted 66-foot yacht he called \"Mary\". The sport's popularity spread across the British Isles. The world's first yacht club was founded in Cork, Ireland in 1720.\n\nThe International Skating Union (ISU) is the international governing body for competitive ice skating disciplines, including figure skating, synchronized skating, speed skating, and short track speed skating. It was founded in Scheveningen, Netherlands, in 1892, making it the oldest governing international winter sport federation and one of the oldest international sport federations.\n\nThe first official World Championships in Speed Skating (open to men only) directly under the auspices of the ISU were held in Amsterdam in 1893.\n\nKorfball (Korfbal in Dutch) is a mixed gender team sport, with similarities to netball and basketball. A team consists of eight players; four female and four male. A team also includes a coach. It was founded in the Netherlands in 1902 by Nico Broekhuysen.\n\nThe Cruijff Turn (also known as \"Cruyff Turn\"), is a famous dribbling trick in football, was perfected by the Dutch football player Johan Cruijff for whom the evasive trick was named. To make this move, the player first looks to pass or cross the ball. However, instead of kicking it, he drags the ball behind his planted foot with the inside of his other foot, turns through 180 degrees and accelerates away. The trick was famously employed by Cruijff in the 1974 FIFA World Cup, first seen in the Dutch match against Sweden and soon widely copied.\n\nThe foundations for Total Football (Dutch: totaalvoetbal) were laid by Englishman Jack Reynolds who was the manager of AFC Ajax. Rinus Michels, who played under Reynolds, later became manager of Ajax and refined the concept into what is known today as \"Total Football\" (\"Totaalvoetbal\" in Dutch language), using it in his training for the Ajax Amsterdam squad and the Netherlands national football team in the 1970s. It was further refined by Stefan Kovacs after Michels left for FC Barcelona. Johan Cruyff was the system's most famous exponent. Due to Cruyff's style of play, he is still referred to as \"the total footballer\". Its cornerstone was a focus on positional interchange. The invention of \"totaalvoetbal\" helped lay the foundations for the significant successes of Dutch football at both club and international level in the 1970s. During that decade, the Dutch football rose from almost total obscurity to become a powerhouse in world football. In an interview published in the 50th anniversary issue of \"World Soccer\" magazine, the captain of the Brazilian team that won the 1970 FIFA World Cup, Carlos Alberto, went on to say: “The only team I’ve seen that did things differently was Holland at the 1974 World Cup in Germany. Since then everything looks more or less the same to me…. Their ‘carousel’ style of play was amazing to watch and marvellous for the game.”\n\nFC Barcelona and the Spanish national football team play a style of football known as \"Tiki-taka\" that has its roots in Total Football. Johan Cruyff founded Tiki-taka (commonly spelled \"tiqui-taca\" in Spanish) during his time as manager of FC Barcelona (1988–1996). The style was successfully adopted by the all-conquering Spain national football team (2008–2012) and Pep Guardiola's Barcelona team (2009–2011). Tiki-taka style differs from Total Football in that it focuses on ball movement rather than positional interchange.\n\nThe Netherlands revived the construction of canals during the 13th–14th century that had generally been discontinued since the fall of the Roman Empire. They also contributed in the development of canal construction technology, such as introducing the first flash locks in Europe. The first pound lock in Europe was built by the Dutch in 1373 at Vreeswijk, where a canal from Utrecht joins the river Lek.\n\nAround the 1620s, Cornelis Drebbel developed an automatic temperature control system for a furnace, motivated by his belief that base metals could be turned to gold by holding them at a precise constant temperature for long periods of time. He also used this temperature regulator in an incubator for hatching chickens.\n\nFeedback control has been used for centuries to regulate engineered systems. In the 17th century, Drebbel invented one of the earliest devices to use feedback, an chicken incubator that used a damper controlled by a thermostat to maintain a constant temperature.\n\nThe magic lantern is an optical device, an early type of image projector developed in the 17th century. People have been projecting images using concave mirrors and pin-hole cameras (camera obscura) since Roman times. But glass lens technology wasn't sufficiently developed to make advanced optical devices (such as telescope and microscope) until the 17th century. With pinhole cameras and camera obscura it was only possible to project an image of actual scene, such as an image of the sun, on a surface. The magic lantern on the other hand could project a painted image on a surface, and marks the point where cameras and projectors became two different kinds of devices. There has been some debate about who the original inventor of the magic lantern is, but the most widely accepted theory is that Christiaan Huygens developed the original device in the late 1650s. However, other sources give credit to the German priest Athanasius Kircher. He describes a device such as the magic lantern in his book \"Ars Magna Lucis et Umbrae\". Huygens is credited because of his major innovation in lantern technology, which was the replacement of images etched on mirrors from earlier lanterns such as Kircher's with images painted on glass. This is what paved the way for the use of colour and for double-layered slide projections (generally used to simulate movement).\n\nThe first allusion to a 'magic lantern' is by Huygens in the 1650s and he is generally credited with inventing it – though he didn't want to admit it, considering it frivolous. Huygens was the first to describe a fully functioning magic lantern, one he made, and wrote about it in a work in 1659. Huygens magic lantern has been described as the predecessor of today's slide projector and the forerunner of the motion picture projector. Images were hand painted onto the glass slide until the mid-19th century when photographic slides were employed. Huygens introduced this curiosity to the Danish mathematician Thomas Walgenstein who realized its commercial value for entertainment and traveled through Europe – mostly France and Italy – demonstrating his machine to foreign princes and selling them replicas for their own amusement. The forerunner of the modern slide projector as well as moving pictures, magic lanterns retained their popularity for centuries and were also the first optical toy to be used for family entertainment in the home.\n\nIn Amsterdam, the Superintendent of the Fire Brigade, Jan van der Heyden, and his son Nicholaas took firefighting to its next step with the fashioning of the first fire hose in 1673.\n\nA gunpowder engine, also known as an \"explosion engine\" or \"Huygens' engine\", is a type of internal combustion engine using gunpowder as its fuel. It was considered essentially as the first rudimentary internal combustion piston engine. The concept was first explored during the 17th century, most notably by the Dutch scientist Christiaan Huygens. In 1678 he outlined a gunpowder engine consisting of a vertical tube containing a piston. Gunpowder was inserted into the tube and lit through a small hole at the base, like a cannon. The expanding gasses would drive the piston up the tube until the reached a point near the top. Here, the piston uncovered holes in the tube that allowed any remaining hot gasses to escape. The weight of the piston and the vacuum formed by the cooling gasses in the now-closed cylinder drew the piston back into the tube, lifting a test mass to provide power. According to sources, a single example of this sort of engine was built in 1678 or 79 using a cannon as the cylinder. The cylinder was held down to a base where the gunpowder sat, making it a breech loading design. The gasses escaped via two leather tubes attached at the top of the barrel. When the piston reached them the gasses blew the tubes open, and when the pressure fell, gravity pulled the leather down causing the tubes droop to the side of the cylinder, sealing the holes. Huygens’ presented a paper on his invention in 1680, \"A New Motive Power by Means of Gunpowder and Air\". By 1682, the device had successfully shown that a dram (1/16th of an ounce) of gunpowder, in a cylinder seven or eight feet high and fifteen or eighteen inches in diameter, could raise seven or eight boys (or about 1,100 pounds) into the air, who held the end of the rope.\n\nThe Hollander beater is a machine developed by the Dutch in 1680 to produce pulp from cellulose-containing plant fibers. It replaced stamp mills for preparing pulp because the Hollander could produce in one day the same quantity of pulp that a stamp mill could produce in eight.\n\nIn 1783, Maastricht-born chemist Jan Pieter Minckelers used coal gas for lighting and developed the first form of gas lighting.\n\nA \"meat slicer\", also called a \"slicing machine\", \"deli slicer\" or simply a \"slicer\", is a tool used in butcher shops and delicatessens to slice meats and cheeses. The first meat slicer was invented by Wilhelm van Berkel (Wilhelmus Adrianus van Berkel) in Rotterdam in 1898. Older models of meat slicer may be operated by crank, while newer ones generally use an electric motor.\n\nA pentode is an electronic device having five active electrodes. The term most commonly applies to a three-grid vacuum tube (thermionic valve), which was invented by the Dutchman Bernhard D.H. Tellegen in 1926.\n\nPhilishave was the brand name for electric shavers manufactured by the Philips Domestic Appliances and Personal Care unit of Philips (in the US, the Norelco name is used). The Philishave shaver was invented by Philips engineer Alexandre Horowitz, who used rotating cutters instead of the reciprocating cutters that had been used in previous electric shavers.\n\nA gyrator is a passive, linear, lossless, two-port electrical network element invented by Tellegen as a hypothetical fifth linear element after the resistor, capacitor, inductor and ideal transformer.\n\nDutch company \"Gatsometer BV\", founded by the 1950s rally driver \"Maurice Gatsonides\", invented the first traffic enforcement camera. Gatsonides wished to better monitor his speed around the corners of a race track and came up with the device in order to improve his time around the circuit. The company developed the first radar for use with road traffic and is the world's largest supplier of speed-monitoring camera systems. Because of this, in some countries speed cameras are sometimes referred to as \"Gatsos\". They are also sometimes referred to as \"photo radar\", even though many of them do not use radar.\n\nThe first systems introduced in the late 1960s used film cameras, replaced by digital cameras beginning in the late 1990s.\n\nVariomatic is the stepless, fully automatic transmission of the Dutch car manufacturer DAF, originally developed by Hub van Doorne. The Variomatic was introduced in 1958 (DAF 600), the first automatic gear box made in the Netherlands. It continues in use in motorscooters. Variomatic was the first commercially successful continuously variable transmissions (CVT).\n\nA Red light camera is a traffic enforcement camera that captures an image of a vehicle that enters an intersection against a red traffic light. By automatically photographing such vehicles, the camera produces evidence that assists authorities in their enforcement of traffic laws. The first red light camera system was introduced in 1965, using tubes stretched across the road to detect the violation and trigger the camera. One of the first developers of these red light camera systems was Dutch company Gatsometer BV.\n\nStochastic cooling is a form of particle beam cooling. It is used in some particle accelerators and storage rings to control the emission of particle beams. This process uses the electrical signals that the individual charged particles generate in a feedback loop to reduce the tendency of individual particles to move away from other particles in the beam. This technique was invented and applied at the Intersecting Storage Rings, and later the Super Proton Synchrotron, at CERN in Geneva, Switzerland by Dutch physicist Simon van der Meer. By increasing the particle density to close to the required energy, this technique improved the beam quality and, inter alia, brought the discovery of W and Z bosons within reach.\n\nThe clap skate (also called clapskates, slap skates, slapskates) is a type of ice skate used in speed skating. Clap skates were developed at the Faculty of Human Movement Sciences of the Vrije Universiteit of Amsterdam, led by Gerrit Jan van Ingen Schenau, although the idea is much older. van Ingen Schenau, who started work on a hinged speed skate in 1979, created his first prototype in 1980 and finished his PhD thesis on the subject in 1981 using the premise that a skater would benefit from extended movement keeping the blade on the ice, allowing the calf muscles more time to exert force.\n\nThe Cremulator is a machine developed by the Dutch company ALL Europe in 1981. The Cremulator is used after cremation, about 3 kg of ashes remain on average. These ash residues are reduced in a cremulator for subsequent scattering or in an urn. Also called asmill. The Cremulator is now further developed by DFW Europe as cremation equipment manufacturer in The Netherlands.\n\nIn the 14th century, the Dutch started using wooden platform skates with flat iron bottom runners. The skates were attached to the skater's shoes with leather straps and poles were used to propel the skater. Around 1500, the Dutch shifted to a narrow metal double edged blade, so the skater could now push and glide with his feet, eliminating the need for a pole.\n\nA herring buss () was a type of seagoing fishing vessel, used by Dutch and Flemish herring fishermen in the 15th through early 19th centuries. The \"Buis\" was first adapted for use as a fishing vessel in the Netherlands, after the invention of gibbing made it possible to preserve herring at sea. This made longer voyages feasible, and hence enabled Dutch fishermen to follow the herring shoals far from the coasts. The first herring buss was probably built in Hoorn around 1415. The last one was built in Vlaardingen in 1841.\n\nOriginally defined as a light, fast sailing vessel used by the Dutch navy to pursue pirates and other transgressors around and into the shallow waters of the Low Countries. Later, yachts came to be perceived as luxury, or recreational vessels.\n\nFluyt, a type of sailing vessel originally designed as a dedicated cargo vessel. Originating from the Netherlands in the 16th century, the vessel was designed to facilitate transoceanic delivery with the maximum of space and crew efficiency. The inexpensive ship could be built in large numbers. This ship class was credited with enhancing Dutch competitiveness in international trade and was widely employed by the Dutch East India Company in the 17th and 18th centuries. The fluyt was a significant factor in the 17th century rise of the Dutch seaborne empire.\n\nCornelis Corneliszoon was the inventor of the wind-powered sawmill. Prior to the invention of sawmills, boards were rived and planed, or more often sawn by two men with a whipsaw using saddleblocks to hold the log and a pit for the pitman who worked below and got the benefit of sawdust in his eyes. Sawing was slow and required strong and durable sawmen. The topsawer had to be the stronger of the two because the saw was pulled in turn by each man, and the lower had the advantage of gravity. The topsawyer also had to guide the saw to produce a plank of even thickness. This was often done by following a chalkline.\n\nEarly sawmills adapted the whipsaw to mechanical power, generally driven by a water wheel to speed up the process. The circular motion of the wheel was changed to back-and-forth motion of the saw blade by a \"pitman\" thus introducing a term used in many mechanical applications. A pitman is similar to a crankshaft used in reverse. A crankshaft converts back-and-forth motion to circular motion.\n\nGenerally only the saw was powered and the logs had to be loaded and moved by hand. An early improvement was the development of a movable carriage, also water powered, to steadily advance the log through the saw blade.\n\nA schooner is a type of sailing vessel with fore-and-aft sails on two or more masts, the foremast being no taller than the rear mast(s). Such vessels were first used by the Dutch in the 16th or 17th century (but may not have been called that at the time). Schooners first evolved from a variety of small two-masted gaff-rigged vessels used in the coast and estuaries of the Netherlands in the late 17th century. Most were working craft but some pleasure yachts with schooner rigs were built for wealthy merchants and Dutch nobility. Following arrival of the Dutch-born prince William III the Orange on the British throne, the British Royal Navy built a Royal yacht with a schooner rig in 1695, HMS \"Royal Transport\". This vessel, captured in a detailed Admiralty model, is the earliest fully documented schooner. \"Royal Transport\" was quickly noted for its speed and ease of handling and mercantile vessels soon adopted the rig in Europe and in European colonies in North America. Schooners were immediately popular with colonial traders and fishermen in North America with the first documented reference to a schooner in America appearing in Boston port records in 1716. North American shipbuilders quickly developed a variety of schooner forms for trading, fishing and privateering. According to the language scholar Walter William Skeat, the term \"schooner\" comes from \"scoon\", while the \"sch\" spelling comes from the later adoption of the Dutch spelling (\"schoener\"). Another study suggests that a Dutch expression praising ornate schooner yachts in the 17th century, \"een schoone Schip\", may have led to the term \"schooner\" being used by English speakers to describe the early versions of the schooner rig as it evolved in England and America.\n\nThe Wind chariot or land yacht (Zeilwagen) was designed by Flemish-born mathematician & engineer Simon Stevin for Prince Maurice of Orange. Land yacht. It offered a carriage with sails, of which a little model was preserved in Scheveningen until 2012. Around the year 1600, Stevin, Maurice and twenty-six others used it on the beach between Scheveningen and Petten. The carriage was propelled solely by force of wind, and traveled faster than horse-drawn vehicles.\n\nA replica of reduced scale of Drebbel's submarine built by the team of the TV-series \"Building the Impossible\" (2002).\nCornelius Drebbel was the inventor of the first navigable submarine, while working for the British Royal Navy. He designed and manufactured a steerable submarine with a leather-covered wooden frame. Between 1620 and 1624 Drebbel successfully built and tested two more, successively larger vessels. The third model had 6 oars and could carry 16 passengers. This model was demonstrated to King James I and several thousand Londoners. The submarine stayed submerged for three hours and could travel from Westminster to Greenwich and back, cruising at a depth of from . This submarine was tested many times in the Thames, but never used in battle.\n\nIn 2002, the British boatbuilder Mark Edwards built a wooden submarine based on the original 17th-century version by Drebbel. This was shown in the BBC TV programme \"Building the Impossible\" in November 2002. It is a scale working model of the original and was built using tools and construction methods common in 17th century boat building and was successfully tested under water with two rowers at Dorney Lake, diving beneath the surface and being rowed underwater for 10 minutes. Legal considerations prevented its use on the River Thames itself.\n\nSpyker is credited with building and racing the first ever four-wheel racing car in 1903. The first four-wheel-drive car, as well as hill-climb racer, with internal combustion engine, the \"Spyker 60 H.P.\", was presented in 1903 by Dutch brothers Jacobus and Hendrik-Jan Spijker of Amsterdam. The two-seat sports car, which was also the first ever car equipped with a six-cylinder engine, is now an exhibit in the Louwman Collection (the former \"Nationaal Automobiel Museum\") at the Hague in The Netherlands.\n\n\"Wilhelmus van Nassouwe\" (\"Het Wilhelmus\") is the national anthem of the Netherlands and is the oldest national anthem in the world. The anthem was first written down in 1574 (during the Dutch Revolt). The Japanese anthem, Kimigayo, has the oldest (9th century) lyrics, but a melody was only added in the late 19th century, making it a poem rather than an anthem for most of its lifespan. Although the Wilhelmus was not officially recognised as the Dutch national anthem until 1932, it has always been popular with parts of the Dutch population and resurfaced on several occasions in the course of Dutch history before gaining its present status.\n\nJava Man (\"Homo erectus erectus\") is the name given to hominid fossils discovered in 1891 at Trinil – Ngawi Regency on the banks of the Solo River in East Java, Indonesia, one of the first known specimens of Homo erectus. Its discoverer, Dutch paleontologist Eugène Dubois, gave it the scientific name Pithecanthropus erectus, a name derived from Greek and Latin roots meaning \"upright ape-man\".\n\nColumba is a small, faint constellation named in the late sixteenth century. Its name is Latin for dove. It is located just south of Canis Major and Lepus. Columba was named by Dutch astronomer Petrus Plancius in 1592 in order to differentiate the 'unformed stars' of the large constellation Canis Major. Plancius first depicted Columba on the small celestial planispheres of his large wall map of 1592. It is also shown on his smaller world map of 1594 and on early Dutch celestial globes.\n\nThe first person to record the Novaya Zemlya effect was Gerrit de Veer, a member of Willem Barentsz' ill-fated third expedition into the polar region. Novaya Zemlya, the archipelago where de Veer first observed the phenomenon, lends its name to the effect.\n\nPlancius defined 12 constellations created by Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman.\n\n\"Camelopardalis\" was created by Plancius in 1613 to represent the animal Rebecca rode to marry Isaac in the Bible. One year later, Jakob Bartsch featured it in his atlas. Johannes Hevelius gave it the official name of \"Camelopardus\" or \"Camelopardalis\" because he saw the constellation's many faint stars as the spots of a giraffe.\n\n\"Monoceros\" is a relatively modern creation. Its first certain appearance was on a globe created by Plancius in 1612 or 1613. It was later charted by Bartsch as \"Unicornus\" in his 1624 star chart.\n\nIn 1655, Huygens became the first person to suggest that Saturn was surrounded by a ring, after Galileo's much less advanced telescope had failed to show rings. Galileo had reported the anomaly as possibly 3 planets instead of one.\n\nIn 1655, using a 50 power refracting telescope that he designed himself, Huygens discovered the first of Saturn's moons, Titan.\n\nKapteyn's Star is a class M1 red dwarf about 12.76 light years from Earth in the southern constellation Pictor, and the closest halo star to the Solar System. With a magnitude of nearly 9 it is visible through binoculars or a telescope. It had the highest proper motion of any star known until the discovery of Barnard's Star in 1916. Attention was first drawn to what is now known as Kapteyn's Star by the Dutch astronomer Jacobus Kapteyn, in 1897.\n\nIn 1904, studying the proper motions of stars, Dutch astronomer Jacobus Kapteyn reported that these were not random, as it was believed in that time; stars could be divided into two streams, moving in nearly opposite directions. It was later realized that Kapteyn's data had been the first evidence of the rotation of our Galaxy, which ultimately led to the finding of galactic rotation by Bertil Lindblad and Jan Oort.\n\nIn 1924, Dutch astronomer Jan Oort the galactic halo, a group of stars orbiting the Milky Way but outside the main disk.\n\nThe Oort constants (discovered by Jan Oort) formula_1 and formula_2 are empirically derived parameters that characterize the local rotational properties of the Milky Way.\n\nIn 1932, Dutch astronomer Jan Oort became the first person to discover evidence of dark matter. Oort proposed the substance after measuring the motions of nearby stars in the Milky Way relative to the galactic plane. He found that the mass of the galactic plane must be more than the mass of the material that can be seen. A year later (1933), Fritz Zwicky examined the dynamics of clusters of galaxies and found their movements similarly perplexing.\n\nThe first formal proof of the existence of an atmosphere around Titan came in 1944, when Gerald Kuiper observed Titan with the new McDonald telescope and discovered spectral signatures on Titan at wavelengths longer than 0.6 μm (micrometers), among which he identified two absorption bands of methane at 6190 and 7250 Å (Kuiper1944). This discovery was significant not only because it requires a dense atmosphere with a significant fraction of methane, but also because the atmosphere needs to be chemically evolved, since methane requires hydrogen in the presence of carbon, and molecular and atomic hydrogen would have escaped from Titan's weak gravitational field since the formation of the solar system.\n\nUsing infrared spectrometry, in 1947 the Dutch-American astronomer Gerard Kuiper detected carbon dioxide in the Martian atmosphere, a discovery of biological significance because it is a principal gas in the process of photosynthesis (see also: History of Mars observation). He was able to estimate that the amount of carbon dioxide over a given area of the surface is double that on the Earth.\n\nMiranda is the smallest and innermost of Uranus's five major moons. It was discovered by Gerard Kuiper on 16 February 1948 at McDonald Observatory.\n\nNereid, also known as Neptune II, is the third-largest moon of Neptune and was its second moon to be discovered, on 1 May 1949, by Gerard Kuiper, on photographic plates taken with the 82-inch telescope at McDonald Observatory.\n\nThe \"Oort cloud\" or \"Öpik–Oort cloud\", named after Dutch astronomer Jan Oort and Estonian astronomer Ernst Öpik, is a spherical cloud of predominantly icy planetesimals believed to surround the Sun at a distance of up to . Further evidence for the existence of the Kuiper belt emerged from the study of comets. That comets have finite lifespans has been known for some time. As they approach the Sun, its heat causes their volatile surfaces to sublimate into space, gradually evaporating them. In order for comets to continue to be visible over the age of the Solar System, they must be replenished frequently. One such area of replenishment is the Oort cloud, a spherical swarm of comets extending beyond 50,000 AU from the Sun first hypothesised by Dutch astronomer in 1950. The Oort cloud is believed to be the point of origin of long-period comets, which are those, like Hale–Bopp, with orbits lasting thousands of years.\n\nThe Kuiper belt was named after Dutch-American astronomer Gerard Kuiper, regarded by many as the father of modern planetary science, though his role in hypothesising it has been heavily contested. In 1951, he proposed the existence of what is now called the Kuiper Belt, a disk-shaped region of minor planets outside the orbit of Neptune, which also is a source of short-period comets.\n\nIn the 1660s and 1670s the Dutch Republic-based scientists (in particular Leiden University-based Jan Swammerdam and Nicolas Steno, and Delft-based Regnier de Graaf and Anton van Leeuwenhoek) made key discoveries about animal and human reproduction. Their research and discoveries contributed greatly to the modern understanding of the female mammalian reproductive system. Many authors see Regnier de Graaf as the founder of modern reproductive biology (Setchell, 1974). This is due essentially to his use of convergent scientific methods: meticulous dissections, clinical observations and critical analysis of the available literature (Ankumet al., 1996).\n\nDutch physician & anatomist Regnier de Graaf may have been the first to understand the reproductive function of the Fallopian tubes. He described the hydrosalpinx, linking its development to female infertility. de Graaf recognized pathologic conditions of the tubes. He was aware of tubal pregnancies, and he surmised that the mammalian egg traveled from the ovary to the uterus through the tube.\n\nIn his \"De Mulierum Organis Generatione Inservientibus\" (1672), de Graaf provided the first thorough description of the female gonad and established that it produced the ovum. De Graaf used the terminology vesicle or egg (ovum) for what now called the ovarian follicle. Because the fluid-filled ovarian vesicles had been observed previously by others, including Andreas Vesalius and Falloppio, De Graaf did not claim their discovery. He noted that he was not the first to describe them, but to describe their development. De Graaf was the first to observe changes in the ovary before and after mating and describe the corpus luteum. From the observation of pregnancy in rabbits, he concluded that the follicle contained the oocyte. The mature stage of the ovarian follicle is called the Graafian follicle in his honour, although others, including Fallopius, had noticed it previously but failed to recognize its reproductive significance.\n\nAntonie van Leeuwenhoek is often considered to be the father of microbiology. Robert Hooke is cited as the first to record microscopic observation of the fruiting bodies of molds, in 1665. However, the first observation of microbes using a microscope is generally credited to van Leeuwenhoek. In the 1670s, he observed and researched bacteria and other microorganisms, using a single-lens microscope of his own design.\n\nIn 1981 the British microscopist Brian J. Ford found that Leeuwenhoek's original specimens had survived in the collections of the Royal Society of London. They were found to be of high quality, and were all well preserved. Ford carried out observations with a range of microscopes, adding to our knowledge of Leeuwenhoek's work.\n\nPhotosynthesis is a fundamental biochemical process in which plants, algae, and some bacteria convert sunlight to chemical energy. The process was discovered by Jan Ingenhousz in 1779. The chemical energy is used to drive reactions such as the formation of sugars or the fixation of nitrogen into amino acids, the building blocks for protein synthesis. Ultimately, nearly all living things depend on energy produced from photosynthesis. It is also responsible for producing the oxygen that makes animal life possible. Organisms that produce energy through photosynthesis are called photoautotrophs. Plants are the most visible representatives of photoautotrophs, but bacteria and algae also employ the process.\n\nPlant respiration was also discovered by Ingenhousz in 1779.\n\nMartinus Beijerinck is considered one of the founders of virology. In 1898, he published results on his filtration experiments, demonstrating that tobacco mosaic disease is caused by an infectious agent smaller than a bacterium. His results were in accordance with similar observations made by Dmitri Ivanovsky in 1892. Like Ivanovsky and Adolf Mayer, predecessor at Wageningen, Beijerinck could not culture the filterable infectious agent. He concluded that the agent can replicate and multiply in living plants. He named the new pathogen \"virus\" to indicate its non-bacterial nature. This discovery is considered to be the beginning of virology.\n\nIn 1931, Cornelis van Niel made key discoveries explaining the chemistry of photosynthesis. By studying purple sulfur bacteria and green sulfur bacteria, he was the first scientist to demonstrate that photosynthesis is a light-dependent redox reaction, in which hydrogen reduces carbon dioxide. Expressed as:\n\nwhere A is the electron acceptor. His discovery predicted that HO is the hydrogen donor in green plant photosynthesis and is oxidized to O. The chemical summation of photosynthesis was a milestone in the understanding of the chemistry of photosynthesis. This was later experimentally verified by Robert Hill.\n\nMany naturalists have studied aspects of animal behaviour throughout history. Ethology has its scientific roots in the work of Charles Darwin and of American and German ornithologists of the late 19th and early 20th century, including Charles O. Whitman, Oskar Heinroth, and Wallace Craig. The modern discipline of ethology is generally considered to have begun during the 1930s with the work of Dutch biologist Nikolaas Tinbergen and by Austrian biologists Konrad Lorenz and Karl von Frisch.\n\nTinbergen's four questions, named after Nikolaas Tinbergen, one of the founders of modern ethology, are complementary categories of explanations for behaviour. It suggests that an integrative understanding of behaviour must include both a proximate and ultimate (functional) analysis of behaviour, as well as an understanding of both phylogenetic/developmental history and the operation of current mechanisms.\n\nThe Vroman effect, named after Leo Vroman, is exhibited by protein adsorption to a surface by blood serum proteins.\n\nFlemish physician Jan Baptist van Helmont is sometimes considered the founder of pneumatic chemistry, coining the word \"gas\" and conducting experiments involving gases. Van Helmont had derived the word “gas” from the Dutch word \"geest\", which means ghost or spirit.\n\nDutch chemist Jacobus Henricus van 't Hoff is generally considered to be one of the founders of the field of stereochemistry. In 1874, Van 't Hoff built on the work on isomers of German chemist Johannes Wislicenus, and showed that the four valencies of the carbon atom were probably directed in space toward the four corners of a regular tetrahedron, a model which explained how optical activity could be associated with an asymmetric carbon atom. He shares credit for this with the French chemist Joseph Le Bel, who independently came up with the same idea. Three months before his doctoral degree was awarded Van 't Hoff published this theory, which today is regarded as the foundation of stereochemistry, first in a Dutch pamphlet in the fall of 1874, and then in the following May in a small French book entitled \"La chimie dans l'espace\". A German translation appeared in 1877, at a time when the only job Van 't Hoff could find was at the Veterinary School in Utrecht. In these early years his theory was largely ignored by the scientific community, and was sharply criticized by one prominent chemist, Hermann Kolbe. However, by about 1880 support for Van 't Hoff's theory by such important chemists as Johannes Wislicenus and Viktor Meyer brought recognition.\n\nJacobus van 't Hoff is also considered as one of the modern founders of the disciple of physical chemistry. The first scientific journal specifically in the field of physical chemistry was the German journal, \"Zeitschrift für Physikalische Chemie\", founded in 1887 by Wilhelm Ostwald and Van 't Hoff. Together with Svante Arrhenius, these were the leading figures in physical chemistry in the late 19th century and early 20th century.\n\nThe Van 't Hoff equation in chemical thermodynamics relates the change in the equilibrium constant, \"K\", of a chemical equilibrium to the change in temperature, \"T\", given the standard enthalpy change, \"ΔH\", for the process. It was proposed by Dutch chemist Jacobus Henricus van 't Hoff in 1884. The \"Van 't Hoff equation\" has been widely utilized to explore the changes in state functions in a thermodynamic system. The \"Van 't Hoff plot\", which is derived from this equation, is especially effective in estimating the change in enthalpy, or total energy, and entropy, or amount of disorder, of a chemical reaction.\n\nThe van 't Hoff factor formula_3 is a measure of the effect of a solute upon colligative properties such as osmotic pressure, relative lowering in vapor pressure, elevation of boiling point and freezing point depression. The van 't Hoff factor is the ratio between the actual concentration of particles produced when the substance is dissolved, and the concentration of a substance as calculated from its mass.\n\nIn carbohydrate chemistry, the Lobry de Bruyn–van Ekenstein transformation is the base or acid-catalyzed transformation of an aldose into the ketose isomer or vice versa, with a tautomeric enediol as reaction intermediate. The transformation is relevant for the industrial production of certain ketoses and was discovered in 1885 by Cornelis Adriaan Lobry van Troostenburg de Bruyn and Willem Alberda van Ekenstein.\n\nThe Prins reaction is an organic reaction consisting of an electrophilic addition of an aldehyde or ketone to an alkene or alkyne followed by capture of a nucleophile. Dutch chemist Hendrik Jacobus Prins discovered two new organic reactions, both now carrying the name Prins reaction. The first was the addition of polyhalogen compounds to olefins, was found during Prins doctoral research, while the others, the acid-catalyzed addition of aldehydes to olefinic compounds, became of industrial relevance.\n\nDutch physicist Dirk Coster and Hungarian-Swedish chemist George de Hevesy co-discovered \"Hafnium\" (Hf) in 1923, by means of X-ray spectroscopic analysis of zirconium ore. \"Hafnium' is named after \"Hafnia', the Latin name for Copenhagen (Denmark), where it was discovered.\n\nThe crystal bar process (also known as \"iodide process\" or the \"van Arkel–de Boer process\") was developed by Dutch chemists Anton Eduard van Arkel and Jan Hendrik de Boer in 1925. It was the first industrial process for the commercial production of pure ductile metallic zirconium. It is used in the production of small quantities of ultra-pure titanium and zirconium.\n\nKoopmans' theorem states that in closed-shell Hartree–Fock theory, the first ionization energy of a molecular system is equal to the negative of the orbital energy of the highest occupied molecular orbital (HOMO). This theorem is named after Tjalling Koopmans, who published this result in 1934.\nKoopmans became a Nobel laureate in 1975, though neither in physics nor chemistry, but in economics.\n\nIn 1889, Dutch botanist Hugo de Vries published his book \"Intracellular Pangenesis\", in which he postulated that different characters have different hereditary carriers, based on a modified version of Charles Darwin's theory of Pangenesis of 1868. He specifically postulated that inheritance of specific traits in organisms comes in \"particles\". He called these units \"pangenes\", a term shortened in 1909 to genes by Danish botanist Wilhelm Johannsen.\n\n1900 marked the \"rediscovery of Mendelian genetics\". The significance of Gregor Mendel's work was not understood until early in the twentieth century, after his death, when his research was re-discovered by Hugo de Vries, Carl Correns and Erich von Tschermak, who were working on similar problems. They were unaware of Mendel's work. They worked independently on different plant hybrids, and came to Mendel's conclusions about the rules of inheritance.\n\nThe Bushveld Igneous Complex (or BIC) is a large, layered igneous intrusion within the Earth's crust that has been tilted and eroded and now outcrops around what appears to be the edge of a great geological basin, the Transvaal Basin. Located in South Africa, the BIC contains some of Earth's richest ore deposits. The complex contains the world's largest reserves of platinum group metals (PGMs), platinum, palladium, osmium, iridium, rhodium, and ruthenium, along with vast quantities of iron, tin, chromium, titanium and vanadium. The site was discovered around 1897 by Dutch geologist Gustaaf Molengraaff.\n\nDescartes (1596–1650) was born in France, but spent most of his adult life in the Dutch Republic. As Bertrand Russell noted in his \"A History of Western Philosophy\" (1945): \"He lived in Holland for twenty years (1629–49), except for a few brief visits to France and one to England, all on business...\". In 1637, Descartes published his work on the methods of science, \"Discours de la méthode\" in Leiden. One of its three appendices was \"La Géométrie\", in which he outlined a method to connect the expressions of algebra with the diagrams of geometry. It combined both algebra and geometry under one specialty – algebraic geometry, now called analytic geometry, which involves reducing geometry to a form of arithmetic and algebra and translating geometric shapes into algebraic equations.\n\nDescartes' \"La Géométrie\" contains Descartes' first introduction of the Cartesian coordinate system.\n\nChristiaan Huygens was the first to publish in 1673 (\"Horologium Oscillatorium\") a specific method of determining the evolute and involute of a curve\n\nIn mathematics, the Korteweg–de Vries equation (KdV equation for short) is a mathematical model of waves on shallow water surfaces. It is particularly notable as the prototypical example of an exactly solvable model, that is, a non-linear partial differential equation whose solutions can be exactly and precisely specified. The equation is named for Diederik Korteweg and Gustav de Vries who, in 1895, proposed a mathematical model which allowed to predict the waves behaviour on shallow water surfaces.\n\nBrouwer fixed-point theorem is a fixed-point theorem in topology, named after Dutchman Luitzen Brouwer, who proved it in 1911.\n\nThe hairy ball theorem of algebraic topology states that there is no nonvanishing continuous tangent vector field on even-dimensional \"n\"-spheres. The theorem was first stated by Henri Poincaré in the late 19th century. It was first proved in 1912 by Brouwer.\n\nThe Debye functions are named in honor of Peter Debye, who came across this function (with \"n\" = 3) in 1912 when he analytically computed the heat capacity of what is now called the Debye model.\n\nThe Kramers–Kronig relations are bidirectional mathematical relations, connecting the real and imaginary parts of any complex function that is analytic in the upper half-plane. The relation is named in honor of Ralph Kronig and Hendrik Anthony Kramers.\n\nFormalized intuitionistic logic was originally developed by Arend Heyting to provide a formal basis for Luitzen Brouwer's programme of intuitionism. Arend Heyting introduced Heyting algebra (1930) to formalize intuitionistic logic.\n\nIn mathematics, the Zernike polynomials are a sequence of polynomials that are orthogonal on the unit disk. Named after Frits Zernike, the Dutch optical physicist, and the inventor of phase contrast microscopy, they play an important role in beam optics.\n\nIn 1941, Marcel Minnaert invented the Minnaert function, which is used in optical measurements of celestial bodies. The Minnaert function is a photometric function used to interpret astronomical observations and remote sensing data for the Earth.\n\nIn 1586, Simon Stevin (Stevinus) derived the mechanical advantage of the inclined plane by an argument that used a string of beads. Stevin's proof of the law of equilibrium on an inclined plane, known as the \"Epitaph of Stevinus\".\n\nChristiaan Huygens stated what is now known as the second of Newton's laws of motion in a quadratic form. In 1659 he derived the now standard formula for the centripetal force, exerted by an object describing a circular motion, for instance on the string to which it is attached. In modern notation:\n\nwith \"m\" the mass of the object, \"v\" the velocity and \"r\" the radius. The publication of the general formula for this force in 1673 was a significant step in studying orbits in astronomy. It enabled the transition from Kepler's third law of planetary motion, to the inverse square law of gravitation.\n\nHuygens coined the term \"centrifugal force\" in his 1659 \"De Vi Centrifiga\" and wrote of it in his 1673 \"Horologium Oscillatorium\" on pendulums.\n\nIn 1659, Christiaan Huygens was the first to derive the formula for the period of an ideal mathematical pendulum (with massless rod or cord and length much longer than its swing), in modern notation:\n\nwith \"T\" the period, \"l\" the length of the pendulum and \"g\" the gravitational acceleration. By his study of the oscillation period of compound pendulums Huygens made pivotal contributions to the development of the concept of moment of inertia.\n\nA tautochrone or isochrone curve is the curve for which the time taken by an object sliding without friction in uniform gravity to its lowest point is independent of its starting point. The curve is a cycloid, and the time is equal to π times the square root of the radius over the acceleration of gravity. Christiaan Huygens was the first to discover the tautochronous property (or isochronous property) of the cycloid. The tautochrone problem, the attempt to identify this curve, was solved by Christiaan Huygens in 1659. He proved geometrically in his \"Horologium Oscillatorium\", originally published in 1673, that the curve was a cycloid. Huygens also proved that the time of descent is equal to the time a body takes to fall vertically the same distance as the diameter of the circle which generates the cycloid, multiplied by π⁄2. The tautochrone curve is the same as the brachistochrone curve for any given starting point. Johann Bernoulli posed the problem of the brachistochrone to the readers of \"Acta Eruditorum\" in June, 1696. He published his solution in the journal in May of the following year, and noted that the solution is the same curve as Huygens's tautochrone curve.\n\nChristiaan Huygens observed that two pendulum clocks mounted next to each other on the same support often become synchronized, swinging in opposite directions. In 1665, he reported the results by letter to the Royal Society of London. It is referred to as \"an odd kind of sympathy\" in the Society's minutes. This may be the first published observation of what is now called \"coupled oscillations\". In the 20th century, \"coupled oscillators\" took on great practical importance because of two discoveries: lasers, in which different atoms give off light waves that oscillate in unison, and superconductors, in which pairs of electrons oscillate in synchrony, allowing electricity to flow with almost no resistance. \"Coupled oscillators\" are even more ubiquitous in nature, showing up, for example, in the synchronized flashing of fireflies and chirping of crickets, and in the pacemaker cells that regulate heartbeats.\n\nFlemish anatomist and physician Andreas Vesalius is often referred to as the founder of modern human anatomy for the publication of the seven-volume \"De humani corporis fabrica\" (\"On the Structure of the Human Body\") in 1543.\n\nIn 1679, van Leeuwenhoek used a microscopes to assess tophaceous material and found that gouty tophi consist of aggregates of needle-shaped crystals, and not globules of chalk as was previously believed.\n\nBoerhaave syndrome (also known as \"spontaneous esophageal perforation\" or \"esophageal rupture\") refers to an esophageal rupture secondary to forceful vomiting. Originally described in 1724 by Dutch physician/botanist Herman Boerhaave, it is a rare condition with high mortality. The syndrome was described after the case of a Dutch admiral, Baron Jan von Wassenaer, who died of the condition.\n\nFactor V Leiden is an inherited disorder of blood clotting. It is a variant of human factor V that causes a hypercoagulability disorder. It is named after the city Leiden, where it was first identified by R. Bertina, et al., in 1994.\n\nIn 1658 Dutch naturalist Jan Swammerdam was the first person to observe red blood cells under a microscope and in 1695, microscopist Antoni van Leeuwenhoek, also Dutch, was the first to draw an illustration of \"red corpuscles\", as they were called. No further blood cells were discovered until 1842 when the platelets were discovered.\n\nThe first person to observe and describe red blood cells was Dutch biologist Jan Swammerdam, who had used an early microscope to study the blood of a frog.\n\nA resident of Delft, Anton van Leeuwenhoek, used a high-power single-lens simple microscope to discover the world of micro-organisms. His simple microscopes were made of silver or copper frames, holding hand-ground lenses were capable of magnification up to 275 times. Using these he was the first to observe and describe single-celled organisms, which he originally referred to as \"animalcules\", and which now referred to as micro-organisms or microbes.\n\nVolvox (1700)- Volvox is a genus of chlorophytes, a type of green algae. It forms spherical colonies of up to 50,000 cells. They live in a variety of freshwater habitats, and were first reported by Van Leeuwenhoek in 1700.\n\nBiological nitrogen fixation was discovered by Martinus Beijerinck in 1885.\n\n\"Rhizobium\" is a genus of Gram-negative soil bacteria that fix nitrogen. Rhizobium forms an endosymbiotic nitrogen fixing association with roots of legumes and \"Parasponia\". Martinus Beijerinck in the Netherlands was the first to isolate and cultivate a microorganism from the nodules of legumes in 1888. He named it \"Bacillus radicicola\", which is now placed in \"Bergey's Manual of Determinative Bacteriology\" under the genus Rhizobium.\n\nMartinus Beijerinck discovered the phenomenon of bacterial sulfate reduction, a form of anaerobic respiration. He learned that bacteria could use sulfate as a terminal electron acceptor, instead of oxygen. He isolated and described \"Spirillum desulfuricans\" (now called \"Desulfovibrio desulfuricans\"), the first known sulfate-reducing bacterium.\n\nIn 1898 Beijerinck coined the term \"virus\" to indicate that the causal agent of tobacco mosaic disease was non-bacterial. Beijerinck discovered what is now known as the tobacco mosaic virus. He observed that the agent multiplied only in cells that were dividing and he called it a contagium vivum fluidum (\"contagious living fluid\"). Beijerinck's discovery is considered to be the beginning of virology.\n\n\"Azotobacter\" is a genus of usually motile, oval or spherical bacteria that form thick-walled cysts and may produce large quantities of capsular slime. They are aerobic, free-living soil microbes which play an important role in the nitrogen cycle in nature, binding atmospheric nitrogen, which is inaccessible to plants, and releasing it in the form of ammonium ions into the soil. Apart from being a model organism, it is used by humans for the production of biofertilizers, food additives, and some biopolymers. The first representative of the genus, \"Azotobacter chroococcum\", was discovered and described in 1901 by the Dutch microbiologist and botanist Martinus Beijerinck.\n\nBeijerinck is credited with developing the first enrichment culture, a fundamental method of studying microbes from the environment.\n\nDivision of the octave into 31 steps arose naturally out of Renaissance music theory; the lesser diesis – the ratio of an octave to three major thirds, 128:125 or 41.06 cents – was approximately a fifth of a tone and a third of a semitone. In 1666, Lemme Rossi first proposed an equal temperament of this order. Shortly thereafter, having discovered it independently, scientist Christiaan Huygens wrote about it also. Since the standard system of tuning at that time was quarter-comma meantone, in which the fifth is tuned to 5, the appeal of this method was immediate, as the fifth of 31-et, at 696.77 cents, is only 0.19 cent wider than the fifth of quarter-comma meantone. Huygens not only realized this, he went farther and noted that 31-ET provides an excellent approximation of septimal, or 7-limit harmony. In the twentieth century, physicist, music theorist and composer Adriaan Fokker, after reading Huygens's work, led a revival of interest in this system of tuning which led to a number of compositions, particularly by Dutch composers. Fokker designed the Fokker organ, a 31-tone equal-tempered organ, which was installed in Teyler's Museum in Haarlem in 1951.\n\nThrough his fundamental contributions Christiaan Huygens helped shape and lay the foundations of classical mechanics. His works cover all the fields of mechanics, from the invention of technical devices applicable to different machines to a purely rational knowledge of motion. Huygens published his results in a classic of the 17th-century mechanics, \"Horologium Oscillatorium\" (1673), that is regarded as one of the three most important work done in mechanics in the 17th century, the other two being Galileo Galilei’s \"Discourses and Mathematical Demonstrations Relating to Two New Sciences\" (1638) and Isaac Newton's \"Philosophiæ Naturalis Principia Mathematica\" (1687). It is Huygens' major work on pendulums and horology. As Domenico Bertoloni Meli (2006) notes, \"Horologium Oscillatorium\" was “a masterful combination of sophisticated mathematics and mechanics mixed with a range of practical applications culminating with a new clock aimed at resolving the vexing problem of longitude.”\n\nHuygens' groundbreaking research on the nature of light helped lay the foundations of modern optics (physical optics in particular). Huygens is remembered especially for his wave theory of light, which he first communicated in 1678 to France's Royal Académie des sciences and which he published in 1690 in his \"Treatise on light\". His argument that light consists of waves now known as the Huygens–Fresnel principle, two centuries later became instrumental in the understanding of wave–particle duality. The interference experiments of Thomas Young vindicated Huygens' s wave theory in 1801.\n\nIn 1678, Huygens discovered the polarization of light by double refraction in calcite.\n\nIn his \"Treatise on light\", Huygens showed how Snell's law of sines could be explained by, or derived from, the wave nature of light, using the Huygens–Fresnel principle.\n\nBernoulli's principle was discovered by Dutch-Swiss mathematician and physicist Daniel Bernoulli and named after him. It states that for an inviscid flow, an increase in the speed of the fluid occurs simultaneously with a decrease in pressure or a decrease in the fluid's potential energy.\n\nIn 1785, Ingenhousz described the irregular movement of coal dust on the surface of alcohol and therefore has a claim as discoverer of what came to be known as Brownian motion.\n\nThe law takes its name from Dutch meteorologist C. H. D. Buys Ballot, who published it in the \"Comptes Rendus\", in November 1857. While William Ferrel first theorized this in 1856, Buys Ballot was the first to provide an empirical validation. The law states that in the Northern Hemisphere, if a person stands with his back to the wind, the low pressure area will be on his left, because wind travels counterclockwise around low pressure zones in that hemisphere. this is approximately true in the higher latitudes and is reversed in the Southern Hemisphere.\n\nSpearheaded by Mach and Ostwald, a strong philosophical current that denied the existence of molecules arose towards the end of the 19th century. The molecular existence was considered unproven and the molecular hypothesis unnecessary. At the time Van der Waals' thesis was written (1873), the molecular structure of fluids had not been accepted by most physicists, and liquid and vapor were often considered as chemically distinct. But Van der Waals's work affirmed the reality of molecules and allowed an assessment of their size and attractive strength. By comparing his equation of state with experimental data, Van der Waals was able to obtain estimates for the actual size of molecules and the strength of their mutual attraction. The effect of Van der Waals's work on molecular science in the 20th century was direct and fundamental, as is well recognized and documented, due in large part to books by John Rowlinson (1988), and by Kipnis and Yavelov (1996). By introducing parameters characterizing molecular size and attraction in constructing his equation of state, Van der Waals set the tone for molecular physics (molecular dynamics in particular) of the 20th century. That molecular aspects such as size, shape, attraction, and multipolar interactions should form the basis for mathematical formulations of the thermodynamic and transport properties of fluids is presently considered an axiom.\n\nIn 1873, J. D. van der Waals introduced the first equation of state derived by the assumption of a finite volume occupied by the constituent molecules. The Van der Waals equation is generally regarded as the first somewhat realistic equation of state (beyond the ideal gas law). Van der Waals noted the non-ideality of gases and attributed it to the existence of molecular or atomic interactions. His new formula revolutionized the study of equations of state, and was most famously continued via the Redlich-Kwong equation of state (1949) and the Soave modification of Redlich-Kwong. While the Van der Waals equation is definitely superior to the ideal gas law and does predict the formation of a liquid phase, the agreement with experimental data is limited for conditions where the liquid forms. Except at higher pressures, the real gases do not obey Van der Waals equation in all ranges of pressures and temperatures. Despite its limitations, the equation has historical importance, because it was the first attempt to model the behaviour of real gases.\n\nThe van der Waals forces are named after the scientist who first described them in 1873. Johannes Diderik van der Waals noted the non-ideality of gases and attributed it to the existence of molecular or atomic interactions. They are forces that develop between the atoms inside molecules and keep them together. The Van der Waals forces between molecules, much weaker than chemical bonds but present universally, play a fundamental role in fields as diverse as supramolecular chemistry, structural biology, polymer science, nanotechnology, surface science, and condensed matter physics. Elucidation of the nature of the Van der Waals forces between molecules has remained a scientific effort from Van der Waals's days to the present.\n\nThe Van der Waals radius, \"r\", of an atom is the radius of an imaginary hard sphere which can be used to model the atom for many purposes. It is named after Johannes Diderik van der Waals, winner of the 1910 Nobel Prize in Physics, as he was the first to recognise that atoms were not simply points and to demonstrate the physical consequences of their size through the van der Waals equation of state.\n\nThe law of corresponding states was first suggested and formulated by van der Waals in 1880. This showed that the van der Waals equation of state can be expressed as a simple function of the critical pressure, critical volume and critical temperature. This general form is applicable to all substances. The compound-specific constants a and b in the original equation are replaced by universal (compound-independent) quantities. It was this law that served as a guide during experiments which ultimately led to the liquefaction of hydrogen by James Dewar in 1898 and of helium by Heike Kamerlingh Onnes in 1908.\n\nLorentz ether theory has its roots in Hendrik Lorentz's \"theory of electrons\", which was the final point in the development of the classical aether theories at the end of the 19th and at the beginning of the 20th century. Lorentz's initial theory created in 1892 and 1895 was based on a completely motionless aether. Many aspects of Lorentz's theory were incorporated into special relativity with the works of Albert Einstein and Hermann Minkowski.\n\nIn 1892, Hendrik Lorentz derived the modern form of the formula for the electromagnetic force which includes the contributions to the total force from both the electric and the magnetic fields. In many textbook treatments of classical electromagnetism, the Lorentz force law is used as the \"definition\" of the electric and magnetic fields E and B. To be specific, the Lorentz force is understood to be the following empirical statement:\n\nIn the physics of electromagnetism, the Abraham–Lorentz force (also \"Lorentz-Abraham force\") is the recoil force on an accelerating charged particle caused by the particle emitting electromagnetic radiation. It is also called the \"radiation reaction force\" or the \"self force\".\n\nIn physics, the Lorentz transformation (or Lorentz transformations) is named after the Dutch physicist Hendrik Lorentz. It was the result of attempts by Lorentz and others to explain how the speed of light was observed to be independent of the reference frame, and to understand the symmetries of the laws of electromagnetism. The Lorentz transformation is in accordance with special relativity, but was derived before special relativity. Early approximations of the transformation were published by Lorentz in 1895. In 1905, Poincaré was the first to recognize that the transformation has the properties of a mathematical group, and named it after Lorentz.\n\nIn physics, length contraction (more formally called Lorentz contraction or Lorentz–FitzGerald contraction after Hendrik Lorentz and George FitzGerald) is the phenomenon of a decrease in length measured by the observer, of an object which is traveling at any non-zero velocity relative to the observer. This contraction is usually only noticeable at a substantial fraction of the speed of light.\n\nThe Lorentz factor or \"Lorentz term\" is the factor by which time, length, and relativistic mass change for an object while that object is moving. It is an expression which appears in several equations in special relativity, and it arises from deriving the Lorentz transformations. The name originates from its earlier appearance in Lorentzian electrodynamics – named after the Dutch physicist Hendrik Lorentz.\n\nThe Zeeman effect, named after the Dutch physicist Pieter Zeeman, is the effect of splitting a spectral line into several components in the presence of a static magnetic field. It is analogous to the Stark effect, the splitting of a spectral line into several components in the presence of an electric field. Also similar to the Stark effect, transitions between different components have, in general, different intensities, with some being entirely forbidden (in the dipole approximation), as governed by the selection rules.\n\nSince the distance between the Zeeman sub-levels is a function of the magnetic field, this effect can be used to measure the magnetic field, e.g. that of the Sun and other stars or in laboratory plasmas.\nThe Zeeman effect is very important in applications such as nuclear magnetic resonance spectroscopy, electron spin resonance spectroscopy, magnetic resonance imaging (MRI) and Mössbauer spectroscopy. It may also be utilized to improve accuracy in atomic absorption spectroscopy.\n\nA theory about the magnetic sense of birds assumes that a protein in the retina is changed due to the Zeeman effect.\n\nWhen the spectral lines are absorption lines, the effect is called \"inverse Zeeman effect\".\n\nHelium was first liquefied (liquid helium) on 10 July 1908, by Dutch physicist Heike Kamerlingh Onnes. With the production of liquid helium, it was said that “the coldest place on Earth” was in Leiden.\n\nSuperconductivity, the ability of certain materials to conduct electricity with little or no resistance, was discovered by Dutch physicist Heike Kamerlingh Onnes.\n\nThe Einstein–de Haas effect or the \"Richardson effect\" (after Owen Willans Richardson), is a physical phenomenon delineated by Albert Einstein and Wander Johannes de Haas in the mid 1910s, that exposes a relationship between magnetism, angular momentum, and the spin of elementary particles.\n\nIn thermodynamics and solid state physics, the Debye model is a method developed by Peter Debye in 1912 for estimating the phonon contribution to the specific heat (heat capacity) in a solid. It treats the vibrations of the atomic lattice (heat) as phonons in a box, in contrast to the Einstein model, which treats the solid as many individual, non-interacting quantum harmonic oscillators. The Debye model correctly predicts the low temperature dependence of the heat capacity.\n\nThe geodetic effect (also known as geodetic precession, de Sitter precession or de Sitter effect) represents the effect of the curvature of spacetime, predicted by general relativity, on a vector carried along with an orbiting body. The geodetic effect was first predicted by Willem de Sitter in 1916, who provided relativistic corrections to the Earth–Moon system's motion.\n\nIn mathematics and physics, a de Sitter space is the analog in Minkowski space, or spacetime, of a sphere in ordinary, Euclidean space. The \"n\"-dimensional de Sitter space, denoted dS, is the Lorentzian manifold analog of an \"n\"-sphere (with its canonical Riemannian metric); it is maximally symmetric, has constant positive curvature, and is simply connected for \"n\" at least 3. The de Sitter space, as well as the anti-de Sitter space is named after Willem de Sitter (1872–1934), professor of astronomy at Leiden University and director of the Leiden Observatory. Willem de Sitter and Albert Einstein worked in the 1920s in Leiden closely together on the spacetime structure of our universe. De Sitter space was discovered by Willem de Sitter, and, at the same time, independently by Tullio Levi-Civita.\n\nIn dynamical systems, a Van der Pol oscillator is a non-conservative oscillator with non-linear damping. It was originally proposed by Dutch physicist Balthasar van der Pol while he was working at Philips in 1920. Van der Pol studied a differential equation that describes the circuit of a vacuum tube. It has been used to model other phenomenon such as human heartbeats by colleague Jan van der Mark.\n\nKramers' opacity law describes the opacity of a medium in terms of the ambient density and temperature, assuming that the opacity is dominated by bound-free absorption (the absorption of light during ionization of a bound electron) or free-free absorption (the absorption of light when scattering a free ion, also called bremsstrahlung). It is often used to model radiative transfer, particularly in stellar atmospheres. The relation is named after the Dutch physicist Hendrik Kramers, who first derived the form in 1923.\n\nIn 1925, Dutch physicists George Eugene Uhlenbeck and Samuel Goudsmit co-discovered the concept of electron spin, which posits an intrinsic angular momentum for all electrons.\n\nIn 1926, Onnes' student, Dutch physicist Willem Hendrik Keesom, invented a method\nto freeze liquid helium and was the first person who was able to solidify the noble gas.\n\nThe Ehrenfest theorem, named after the Austrian-born Dutch-Jew theoretical physicist Paul Ehrenfest at Leiden University.\n\nThe de Haas–van Alphen effect, often abbreviated to dHvA, is a quantum mechanical effect in which the magnetic moment of a pure metal crystal oscillates as the intensity of an applied magnetic field B is increased. It was discovered in 1930 by Wander Johannes de Haas and his student P. M. van Alphen.\n\nThe Shubnikov–de Haas effect (ShdH) is named after Dutch physicist Wander Johannes de Haas and Russian physicist Lev Shubnikov.\n\nIn quantum mechanics, the Kramers degeneracy theorem states that for every energy eigenstate of a time-reversal symmetric system with half-integer total spin, there is at least one more eigenstate with the same energy. It was first discovered in 1930 by H. A. Kramers as a consequence of the Breit equation.\n\nIn 1933, Marcel Minnaert published a solution for the acoustic resonance frequency of a single bubble in water, the so-called Minnaert resonance. The Minnaert resonance or Minnaert frequency is the acoustic resonance frequency of a single bubble in an infinite domain of water (neglecting the effects of surface tension and viscous attenuation).\n\nIn quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. Dutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947. After a conversation with Niels Bohr who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948; the former is called the Casimir–Polder force while the latter is the Casimir effect in the narrow sense.\n\nTellegen's theorem is one of the most powerful theorems in network theory. Most of the energy distribution theorems and extremum principles in network theory can be derived from it. It was published in 1952 by Bernard Tellegen. Fundamentally, Tellegen's theorem gives a simple relation between magnitudes that satisfy Kirchhoff's laws of electrical circuit theory.\n\nIn the early 1970s Simon van der Meer, a Dutch particle physicist at CERN, discovered this technique to concentrate proton and anti-proton beams, leading to the discovery of the W and Z particles. He won the 1984 Nobel Prize in Physics together with Carlo Rubbia.\n\nIn 1971, Gerardus 't Hooft, who was completing his PhD under the supervision of Dutch theoretical physicist Martinus Veltman, renormalized Yang–Mills theory. They showed that if the symmetries of Yang–Mills theory were to be realized in the spontaneously broken mode, referred to as the Higgs mechanism, then Yang–Mills theory can be renormalized. Renormalization of Yang–Mills theory is considered as a major achievement of twentieth century physics.\n\nThe holographic principle is a property of string theories and a supposed property of quantum gravity that states that the description of a volume of space can be thought of as encoded on a boundary to the region – preferably a light-like boundary like a gravitational horizon. In 1993, Dutch theoretical physicist Gerard 't Hooft proposed what is now known as the holographic principle. It was given a precise string-theory interpretation by Leonard Susskind who combined his ideas with previous ones of 't Hooft and Charles Thorn.\n\nDuring his first journey in 1594, Dutch explorer Willem Barentsz discovered the Orange Islands, at the northern extremity of Nova Zembla.\n\nOn 10 June 1596, Barentsz and Dutchman Jacob van Heemskerk discovered Bear Island, a week before their discovery of Spitsbergen Island.\n\nThe first undisputedly to have discovered the archipelago is an expedition led by the Dutch mariner Willem Barentsz, who was looking for the Northern Sea Route to China. He first spotted Bjørnøya on 10 June 1596 and the northwestern tip of Spitsbergen on 17 June. The sighting of the archipelago was included in the accounts and maps made by the expedition and Spitsbergen was quickly included by cartographers. The name \"Spitsbergen\", meaning \"pointed mountains\" (from the Dutch \"spits\" – pointed, \"bergen\" – mountains), was at first applied to both the main island and the Svalbard archipelago as a whole.\n\nThe search for the Northern Sea Route in the 16th century led to its exploration. Dutch explorer Willem Barentsz reached the west coast of Novaya Zemlya in 1594, and in a subsequent expedition of 1596 rounded the Northern point and wintered on the Northeast coast. Willem Barents, Jacob van Heemskerck and their crew were blocked by the pack ice in the Kara Sea and forced to winter on the east coast of Novaya Zemlya. The wintering of the shipwrecked crew in the 'Saved House' was the first successful wintering of Europeans in the High Arctic. Twelve of the 17 men managed to survive the polar winter (De Veer, 1917). Barentsz died during the expedition, and may have been buried on the northern island.\n\nIn 1600 the Dutch navigator Sebald de Weert made the first undisputed sighting of the Falkland Islands. It was on his homeward leg back to the Netherlands after having left the Straits of Magellan that Sebald De Weert noticed some unnamed and uncharted islands, at least islands that did not exist on his nautical charts. There he attempted to stop and replenish but was unable to land due to harsh conditions. The islands Sebald de Weert charted were a small group off the northwest coast of the Falkland Islands and are in fact part of the Falklands. De Weert then named these islands the “Sebald de Weert Islands” and the Falklands as a whole were known as the Sebald Islands until well into the 18th century.\n\nThe Dutch ship, Duyfken, led by Willem Janszoon, made the first documented European landing in Australia in 1606. Although a theory of Portuguese discovery in the 1520s exists, it lacks definitive evidence. Precedence of discovery has also been claimed for China, France, Spain, India, and even Phoenicia.\n\nThe Janszoon voyage of 1605–06 led to the first undisputed sighting of Australia by a European was made on 26 February 1606. Dutch vessel \"Duyfken\", captained by Janszoon, followed the coast of New Guinea, missed Torres Strait, and explored perhaps of western side of Cape York, in the Gulf of Carpentaria, believing the land was still part of New Guinea. The Dutch made one landing, but were promptly attacked by Maoris and subsequently abandoned further exploration.\n\nThe first recorded European sighting of the Australian mainland, and the first recorded European landfall on the Australian continent, are attributed to the Dutch navigator Willem Janszoon. He sighted the coast of Cape York Peninsula in early 1606, and made landfall on 26 February at the Pennefather River near the modern town of Weipa on Cape York. The Dutch charted the whole of the western and northern coastlines and named the island continent \"New Holland\" during the 17th century, but made no attempt at settlement.\n\nThe area that is now Manhattan was long inhabited by the Lenape Indians. In 1524, Florentine explorer Giovanni da Verrazzano – sailing in service of the king Francis I of France – was the first European to visit the area that would become New York City. It was not until the voyage of Henry Hudson, an Englishman who worked for the Dutch East India Company, that the area was mapped.\n\nAt the time of the arrival of the first Europeans in the 17th century, the Hudson Valley was inhabited primarily by the Algonquian-speaking Mahican and Munsee Native American people, known collectively as River Indians. The first Dutch settlement was in the 1610s at Fort Nassau, a trading post (factorij) south of modern-day Albany, that traded European goods for beaver pelts. Fort Nassau was later replaced by Fort Orange. During the rest of the 17th century, the Hudson Valley formed the heart of the New Netherland colony operations, with the New Amsterdam settlement on Manhattan serving as a post for supplies and defense of the upriver operations.\n\nThe Brouwer Route was a route for sailing from the Cape of Good Hope to Java. The Route took ships south from the Cape into the Roaring Forties, then east across the Indian Ocean, before turning northwest for Java. Thus it took advantage of the strong westerly winds for which the Roaring Forties are named, greatly increasing travel speed. It was devised by Dutch sea explorer Hendrik Brouwer in 1611, and found to halve the duration of the journey from Europe to Java, compared to the previous Arab and Portuguese monsoon route, which involved following the coast of East Africa northwards, sailing through the Mozambique Channel and then across the Indian Ocean, sometimes via India. The Brouwer Route played a major role in the discovery of the west coast of Australia.\n\nAfter unconfirmed reports of Dutch discovery as early as 1611, the island was named after Dutchman Jan Jacobszoon May van Schellinkhout, who visited the island in July 1614. As locations of these islands were kept secret by the whalers, Jan Mayen got its current name only in 1620.\n\nThe name \"Hell Gate\" is a corruption of Dutch phrase \"Hellegat\", which could mean either \"hell's hole\" or \"bright gate/passage\". It was originally applied to the entirety of the East River. The strait was described in the journals of Dutch explorer Adriaen Block, who is the first European known to have navigated the strait, during his 1614 voyage aboard the \"Onrust\".\n\nThe first European to record the existence of Long Island Sound and the Connecticut River was Dutch explorer Adriaen Block, who entered it from the East River in 1614.\n\nFishers Island was called \"Munnawtawkit\" by the Native American Pequot nation. Block named it \"Visher's Island\" in 1614, after one of his companions. For the next 25 years, it remained a wilderness, visited occasionally by Dutch traders.\n\nOn 25 December 1615, Dutch explorers Jacob le Maire and Willem Schouten aboard the Eendracht, discovered Staten Island, close to Cape Horn.\n\nOn 29 January 1616, they sighted land they called Cape Horn, after the city of Hoorn. Aboard the Eendracht was the crew of the recently wrecked ship called Hoorn.\n\nThey discovered Tonga on 21 April 1616 and the Hoorn Islands on 28 April 1616.\n\nThey discovered New Ireland around May–July 1616.\n\nThey discovered the Schouten Islands (also known as \"Biak Islands\" or \"Geelvink Islands\") on 24 July 1616.\n\nThe Schouten Islands (also known as \"Eastern Schouten Islands\" or \"Le Maire Islands\") of Papua New Guinea, were named after Schouten, who visited them in 1616.\n\nHendrik Brouwer's discovery that sailing east from the Cape of Good Hope until land was sighted, and then sailing north along the west coast of Australia was a much quicker route than around the coast of the Indian Ocean made Dutch landfalls on the west coast inevitable. The first such landfall was in 1616, when Dirk Hartog landed at Cape Inscription on what is now known as Dirk Hartog Island, off the coast of Western Australia, and left behind an inscription on a pewter plate. In 1697 the Dutch captain Willem de Vlamingh landed on the island and discovered Hartog's plate. He replaced it with one of his own, which included a copy of Hartog's inscription, and took the original plate home to Amsterdam, where it is still kept in the Rijksmuseum Amsterdam.\n\nThe first sighting of the Houtman Abrolhos by Europeans was by Dutch VOC ships \"Dordrecht\" and \"Amsterdam\" in 1619, three years after Hartog made the first authenticated sighting of what is now Western Australia, 13 years after the first authenticated voyage to Australia, that of the \"Duyfke\" in 1606. Discovery of the islands was credited to Frederick de Houtman, Captain-General of the \"Dordrecht\", as it was Houtman who later wrote of the discovery in a letter to Company directors.\n\nThe first person to spot Carstensz Pyramid (or Puncak Jaya) is reported to be the Dutch navigator and explorer Jan Carstensz in 1623, for whom the mountain is named. Carstensz was the first (non-native) to sight the glaciers on the peak of the mountain on a rare clear day. The sighting went unverified for over two centuries, and Carstensz was ridiculed in Europe when he said he had seen snow and glaciers near the equator. The snowfield of Puncak Jaya was reached as early as 1909 by a Dutch explorer, Hendrik Albert Lorentz with six of his indigenous Dayak Kenyah porters recruited from the Apo Kayan in Borneo. The now highest Carstensz Pyramid summit was not climbed until 1962, by an expedition led by the Austrian mountaineer Heinrich Harrer with three other expedition members – the New Zealand mountaineer Philip Temple, the Australian rock climber Russell Kippax, and the Dutch patrol officer Albertus (Bert) Huizenga.\n\nThe first known European explorer to visit the region was Dutch Willem Janszoon (also known as \"Willem Jansz\") on his 1605–06 voyage. His fellow countryman, Jan Carstenszoon (also known as \"Jan Carstensz\"), visited in 1623 and named the gulf in honour of Pieter de Carpentier, at that time the Governor-General of Dutch East Indies. Abel Tasman explored the coast in 1644.\n\nThe Staaten River is a river in the Cape York Peninsula, Australia that rises more than to the west of Cairns and empties into the Gulf of Carpentaria. The river was first named by Carstenszoon in 1623.\n\nIn 1623 Dutch East India Company captain Willem van Colster sailed into the Gulf of Carpentaria. Cape Arnhem is named after his ship, the \"Arnhem\", which itself was named after the city of Arnhem.\n\nGroote Eylandt was first sighted the \"Arnhem\". Only in 1644, when Abel Tasman arrived, was the island given a European name, Dutch for \"Large Island\" in an archaic spelling. The modern Dutch spelling is \"Groot Eiland\".\n\nIn February 1624, Dutch admiral Jacques l'Hermite discovered the Hermite Islands at Cape Horn.\n\nIn 1627, Dutch explorers François Thijssen and Pieter Nuyts discovered the south coast of Australia and charted about of it between Cape Leeuwin and the Nuyts Archipelago. François Thijssen, captain of the ship \"'t Gulden Zeepaert\" (The Golden Seahorse), sailed to the east as far as Ceduna in South Australia. The first known ship to have visited the area is the \"Leeuwin\" (\"Lioness\"), a Dutch vessel that charted some of the nearby coastline in 1622. The log of the \"Leeuwin\" has been lost, so very little is known of the voyage. However, the land discovered by the \"Leeuwin\" was recorded on a 1627 map by Hessel Gerritsz: Caert van't Landt van d'Eendracht (\"Chart of the Land of Eendracht\"), which appears to show the coast between present-day Hamelin Bay and Point D’Entrecasteaux. Part of Thijssen's map shows the islands St Francis and St Peter, now known collectively with their respective groups as the Nuyts Archipelago. Thijssen's observations were included as soon as 1628 by the VOC cartographer Hessel Gerritsz in a chart of the Indies and New Holland. This voyage defined most of the southern coast of Australia and discouraged the notion that \"New Holland\", as it was then known, was linked to Antarctica.\n\nSt Francis Island (originally in Dutch: \"Eyland St. François\") is an island on the south coast of South Australia near Ceduna. It is now part of the Nuyts Archipelago Wilderness Protection Area. It was one of the first parts of South Australia to be discovered and named by Europeans, along with St Peter Island. Thijssen named it after his patron saint, St. Francis.\n\nSt Peter Island is an island on the south coast of South Australia near Ceduna to the south of Denial Bay. It is the second largest island in South Australia at about 13 km long. It was named in 1627 by Thijssen after Pieter Nuyts' patron saint.\n\nThe Weibbe Hayes Stone Fort, remnants of improvised defensive walls and stone shelters built by Wiebbe Hayes and his men on the West Wallabi Island, are Australia's oldest known European structures, more than 150 years before expeditions to the Australian continent by James Cook and Arthur Phillip.\n\nIn 1642, Abel Tasman sailed from Mauritius and on 24 November, sighted Tasmania. He named Tasmania Van Diemen's Land, after Anthony van Diemen, the Dutch East India Company's Governor General, who had commissioned his voyage. It was officially renamed Tasmania in honour of its first European discoverer on 1 January 1856.\n\nMaatsuyker Islands, a group of small islands that are the southernmost point of the Australian continent. were discovered and named by Tasman in 1642 after a Dutch official. The main islands of the group are De Witt Island (354 m), Maatsuyker Island (296 m), Flat Witch Island, Flat Top Island, Round Top Island, Walker Island, Needle Rocks and Mewstone.\n\nMaria Island was discovered and named in 1642 by Tasman after Maria van Diemen (née van Aelst), wife of Anthony. The island was known as \"Maria's Isle\" in the early 19th century.\n\nTasman's journal entry for 29 November 1642 records that he observed a rock which was similar to a rock named Pedra Branca off China, presumably referring to the Pedra Branca in the South China Sea.\n\nSchouten Island is a island in eastern Tasmania, Australia. It lies 1.6 kilometres south of Freycinet Peninsula and is a part of Freycinet National Park. In 1642, while surveying the south-west coast of Tasmania, Tasman named the island after Joost Schouten, a member of the Council of the Dutch East India Company.\n\nTasman also reached Storm Bay, a large bay in the south-east of Tasmania, Australia. It is the entrance to the Derwent River estuary and the port of Hobart, the capital city of Tasmania. It is bordered by Bruny Island to the west and the Tasman Peninsula to the east.\n\nIn 1642, the first Europeans known to reach New Zealand were the crew of Dutch explorer Abel Tasman who arrived in his ships \"Heemskerck\" and \"Zeehaen\". Tasman anchored at the northern end of the South Island in Golden Bay (he named it Murderers' Bay) in December 1642 and sailed northward to Tonga following a clash with local Māori. Tasman sketched sections of the two main islands' west coasts. Tasman called them \"Staten Landt\", after the \"States General of the Netherlands\", and that name appeared on his first maps of the country. In 1645 Dutch cartographers changed the name to \"Nova Zeelandia\" in Latin, from \"Nieuw Zeeland\", after the Dutch province of \"Zeeland\". It was subsequently Anglicised as \"New Zealand\" by British naval captain James Cook\n\nVarious claims have been made that New Zealand was reached by other non-Polynesian voyagers before Tasman, but these are not widely accepted. Peter Trickett, for example, argues in \"Beyond Capricorn\" that the Portuguese explorer Cristóvão de Mendonça reached New Zealand in the 1520s, and the Tamil bell discovered by missionary William Colenso has given rise to a number of theories,\n\nIn 1643, still during the same expedition, Tasman discovered Fiji.\n\nTasman discovered Tongatapu and Haʻapai in 1643 commanding two ships, the \"Heemskerck\" and the \"Zeehaen\" commissioned by the Dutch East India Company. The expedition's goals were to chart the unknown southern and eastern seas and to find a possible passage through the South Pacific and Indian Ocean providing a faster route to Chile.\n\nThe first European known to visit Sakhalin was Martin Gerritz de Vries, who mapped Cape Patience and Cape Aniva on the island's east coast in 1643.\n\nIn the summer of 1643, the \"Castricum\", under command of Martin Gerritz de Vries sailed by the southern Kuril Islands, visiting Kunashir, Iturup and Urup, which they named \"Company Island\" and claimed for the Netherlands.\n\nVries Strait or Miyabe Line is a strait between two main islands of the Kurils. It is located between the northeastern end of the island of Iturup and the southwestern headland of Urup Island, connecting the Sea of Okhotsk on the west with the Pacific Ocean on the east. The strait is named after de Vries, the first recorded European to explore the area.\n\nThe Gulf of Patience is a large body of water off the southeastern coast of Sakhalin, Russia, between the main body of Sakhalin Island in the west and Cape Patience in the east. It is part of the Sea of Okhotsk. The first Europeans to visit the bay sailed on \"Castricum\". They named the gulf in memory of the fog that had to clear for them to continue their expedition.\n\nThe first Europeans known to land on the Rottnest Island were 13 Dutch sailors including Abraham Leeman from the \"Waeckende Boey\" who landed near Bathurst Point on 19 March 1658 while their ship was nearby. The ship had sailed from Batavia in search of survivors of the missing \"Vergulde Draeck\" which was later found wrecked north near present-day Ledge Point. The island was given the name \"Rotte nest\" (meaning \"rat nest\" in the 17th century Dutch language) by Dutch captain Willem de Vlamingh who spent six days exploring the island from 29 December 1696, mistaking the quokkas for giant rats. De Vlamingh led a fleet of three ships, \"De Geelvink\", \"De Nijptang\" and \"Weseltje\" and anchored on the northern side of the island, near The Basin.\n\nOn 10 January 1697, de Vlamingh ventured up the Swan River. He and his crew are believed to have been the first Europeans to do so. He named the \"Swan River\" (\"Zwaanenrivier\" in Dutch) after the large numbers of black swans that he observed there.\n\nOn Easter Sunday, 5 April 1722, Dutch explorer Jacob Roggeveen discovered Easter Island. Easter Island is one of the most remote inhabited islands in the world. The nearest inhabited land (50 residents) is Pitcairn Island away, the nearest town with a population over 500 is Rikitea on island Mangareva away, and the nearest continental point lies in central Chile, away.\n\nThe name \"Easter Island\" was given by the island's first recorded European visitor, the Dutch explorer Jacob Roggeveen, who encountered it on Easter Sunday (5 April) 1722, while searching for Davis or David's island. Roggeveen named it \"Paasch-Eyland\" (18th century Dutch for \"Easter Island\"). The island's official Spanish name, \"Isla de Pascua\", also means \"Easter Island\".\n\nOn 13 June Roggeveen discovered the islands of Samoa.\n\nThe Orange River was named by Colonel Robert Gordon, commander of the Dutch East India Company garrison at Cape Town, on a trip to the interior in 1779.\n\nIn 1595, Petrus Plancius, a key promoter to the East Indies expeditions, asked Pieter Dirkszoon Keyser, the chief pilot on the \"Hollandia\", to make observations to fill in the blank area around the south celestial pole on European maps of the southern sky. Plancius had instructed Keyser to map the skies in the southern hemisphere, which were largely uncharted at the time. Keyser died in Java the following year but his catalogue of 135 stars, probably measured up with the help of explorer-colleague Frederick de Houtman, was delivered to Plancius, and then those stars were arranged into 12 new southern constellations, letting them be inscribed on a 35-cm celestial globe that was prepared in late 1597 (or early 1598). This globe was produced in collaboration with the Amsterdam cartographer Jodocus Hondius.\n\nPlancius's constellations (mostly referring to animals and subjects described in natural history books and travellers' journals of his day) are Apis the Bee (later changed to Musca by Lacaille), Apus the Bird of Paradise, Chamaeleon, Dorado the Goldfish (or Swordfish), Grus the Crane, Hydrus the Small Water Snake, Indus the Indian, Pavo the Peacock, Phoenix, Triangulum Australe the Southern Triangle, Tucana the Toucan, and Volans the Flying Fish. The acceptance of these new constellations was assured when Johann Bayer, a German astronomer, included them in his \"Uranometria\" of 1603, the leading star atlas of its day. These 12 southern constellations are still recognized today by the International Astronomical Union (IAU).\n\nWithin the thirty-year period the Dutch West India Company controlled the northeast region of Brazil (1624–1654), the seven-year governorship of Count Johan Maurits van Nassau-Siegen was marked by an intense ethnographic exploration. To that end, Johan Maurits brought from Europe with him a team of artists and scientists who lived in Recife between 1637 and 1644: painter Albert Eckhout (specializing in the human figure), painter Frans Post (landscape painter), natural historian Georg Marcgraf (who also produced drawings and prints), and the physician Willem Piso. Together with Georg Marcgraf, and originally published by Joannes de Laet, Piso wrote the \"Historia Naturalis Brasiliae\" (1648), an important early western insight into Brazilian flora and fauna, also is the first scientific book about Brazil. Albert Eckhout, along with the landscape artist Frans Post, was one of two formally trained painters charged with recording the complexity of the local scene. The seven years Eckhout spent in Brazil constitute an invaluable contribution to the understanding of the European colonization of the New World. During his stay he created hundreds of oil sketches – mostly from life – of the local flora, fauna and people. These paintings by Eckhout and the landscapes by Post were among the Europeans' first, introductions to South America.\n\nIn 1641, Kiliaen van Rensselaer, the director of the Dutch West India Company, hired Adriaen van der Donck (1620–1655) to be his lawyer for his large, semi-independent estate, Rensselaerswijck, in New Netherland. Until 1645, van der Donck lived in the Upper Hudson River Valley, near Fort Orange (later Albany), where he learned about the Company's fur trade, the Mohawk and Mahican Indians who traded with Dutch, the agriculturist settlers, and the area's plants and animals. In 1649, after a serious disagreement with the new governor, Peter Stuyvesant, he returned to the Dutch Republic to petition Dutch government. In 1653, still in the Netherlands waiting for the government to decide his case, Adriaen van der Donck wrote a comprehensive description of the New Netherland's geography and native peoples based on material in his earlier \"Remonstrance\". The book, \"Beschryvinge van Nieuw-Nederlant\" or \"A Description of New Netherland\" later published in 1655. This new book was well-crafted to the interests of his audience, consisting of an extensive description of American Indians and their customs, reports on the abundance of the area's agriculture and wealth of its natural resources.\n\nJan Weltevree (1595-?) is regarded as the first naturalized Westerner to Korea. Weltevree was a Dutch sailor who arrived on the shores of an island off Joseon’s west coast in 1627 in a shipwreck. The Joseon Dynasty at that time maintained an isolation policy, so the captured foreigner could not leave the country. Weltevree took the name \"Bak Yeon\" (also \"Pak Yeon\"). He became an important government official and aided King Hyojong with his keen knowledge of modern weaponry. His adventures were recorded in the report by Dutch East India Company accountant Hendrik Hamel.\n\nDutch seafarer and VOC's bookkeeper Hendrick Hamel was the first westerner to experience first-hand and write about Korea in Joseon era (1392–1897). In 1653, Hamel and his men were shipwrecked on Jeju island, and they remained captives in Korea for more than a decade. The Joseon dynasty was often referred to as the \"Hermit Kingdom\" for its harsh isolationism and closed borders. The shipwrecked Dutchmen were given some freedom of movement, but were forbidden to leave the country. After thirteen years (1653–1666), Hamel and seven of his crewmates managed to escape to the VOC trading mission at Dejima (an artificial island in the bay of Nagasaki, Japan), and from there to the Netherlands. In 1666, three different publishers published his report (\"Journal van de Ongeluckige Voyage van 't Jacht de Sperwer\" or \"An account of the shipwreck of a Dutch vessel on the coast of the isle of Quelpaert together with the description of the kingdom of Corea\"), describing their improbable adventure and giving the first detailed and accurate description of Korea to the western world.\n\n\n"}
{"id": "985101", "url": "https://en.wikipedia.org/wiki?curid=985101", "title": "List of defunct hard disk manufacturers", "text": "List of defunct hard disk manufacturers\n\nAt least 221 companies were hard disk drive manufacturers in the past. Most of that industry has vanished through bankruptcy or mergers and acquisitions. None of the first four entrants continue in the industry today. As shown in the graphic surviving manufacturers are Seagate, Toshiba and Western Digital (WD) all of whom grew at least in part through mergers and acquisitions.\n\nSome of the defunct manufacturers include:\n\n\n\n"}
{"id": "7313339", "url": "https://en.wikipedia.org/wiki?curid=7313339", "title": "Louis Kauffman", "text": "Louis Kauffman\n\nLouis Hirsch Kauffman (born February 3, 1945) is an American mathematician, topologist, and professor of Mathematics in the Department of Mathematics, Statistics, and Computer science at the University of Illinois at Chicago. He is known for the introduction and development of the bracket polynomial and the Kauffman polynomial.\n\nKauffman was valedictorian of his graduating class at Norwood Norfolk Central High School in 1962. He received his B.S. at the Massachusetts Institute of Technology in 1966 and his Ph.D. in mathematics from Princeton University in 1972 (with the famous mathematician William Browder as thesis advisor).\n\nKauffman has worked at many places as a visiting professor and researcher, including the University of Zaragoza in Spain, the University of Iowa in Iowa City, the Institut des Hautes Études Scientifiques in Bures Sur Yevette, France, the Institut Henri Poincaré in Paris, France, the University of Bologna, Italy, the Federal University of Pernambuco in Recife, Brazil, and the Newton Institute in Cambridge England.\n\nHe is the founding editor and one of the managing editors of the \"Journal of Knot Theory and Its Ramifications\", and editor of the \"World Scientific Book Series On Knots and Everything\". He writes a column entitled Virtual Logic for the journal \"Cybernetics and Human Knowing\"\n\nFrom 2005 to 2008 he was president of the American Society for Cybernetics. He plays\nclarinet in the ChickenFat Klezmer Orchestra in Chicago.\n\nKauffman's research interests are in the fields of cybernetics, topology and foundations of mathematics and physics. His work is primarily in the topics of knot theory and connections with statistical mechanics, quantum theory, algebra, combinatorics and foundations. In topology he introduced and developed the bracket polynomial and Kauffman polynomial.\n\nIn the mathematical field of knot theory, the bracket polynomial, also known as the \"Kauffman bracket\", is a polynomial invariant of framed links. Although it is not an invariant of knots or links (as it is not invariant under type I Reidemeister moves), a suitably \"normalized\" version yields the famous knot invariant called the Jones polynomial. The bracket polynomial plays an important role in unifying the Jones polynomial with other quantum invariants. In particular, Kauffman's interpretation of the Jones polynomial allows generalization to state sum invariants of 3-manifolds. Recently the bracket polynomial formed the basis for Mikhail Khovanov's construction of a homology for knots and links, creating\na stronger invariant than the Jones polynomial and such that the graded Euler characteristic of the Khovanov homology is equal to the original\nJones polynomial. The generators for the chain complex of the Khovanov homology are states of the bracket polynomial decorated with elements\nof a Frobenius algebra.\n\nThe Kauffman polynomial is a 2-variable knot polynomial due to Louis Kauffman. It is defined as\n\nwhere formula_2 is the writhe and formula_3 is a regular isotopy invariant which generalizes the bracket polynomial.\n\nIn 1994, Kauffman and Tom Etter wrote a draft proposal for a non-commutative \"discrete ordered calculus\" (DOC), which they presented in revised form in 1996. In the mean time, the theory was presented in a modified form by Kauffman and H. Pierre Noyes together with a presentation of a derivation of free space Maxwell's equations on this basis.\n\nHe won a Lester R. Ford Award (with Thomas Banchoff) in 1978. Kauffman is the 1993 recipient of the Warren McCulloch award of the American Society for Cybernetics and the 1996 award of the Alternative Natural Philosophy Association for his work in discrete physics. He is the 2014 recipient of the Norbert Wiener award of the American Society for Cybernetics.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\nLouis H. Kauffman is author of several monographs on knot theory and mathematical physics. His publication list numbers over 170. Books:\nArticles and papers, a selection:\n\n"}
{"id": "24179727", "url": "https://en.wikipedia.org/wiki?curid=24179727", "title": "Mud cleaner", "text": "Mud cleaner\n\nA mud cleaner is a combination of desanders and/or desilters to remove drilled solids from mud.\n\nAlthough US Patent No.3,766,997 covering Mud Cleaner was issued on October 23, 1973 granted to J.K. Heilhecker and L.H. Robinson and assigned to Exxon Production Research Company, it was found invalid and efforts for re-issue was unsuccessful because of the existence of prior art filed at the British Patent Office by a German Inventor in the late 1800s.\n\nNowadays, mud cleaner used in oilfield means the equipment processing drilling mud. It is combined with desander cone and desilter cone as well as bottom shaker. It is called 2nd and 3rd phase solids control equipment.\n\nA mud cleaner is a combination of desanders and/or desilters mounted over a shaker with a fine mesh screen. A mud is fed to the inlet of the hydrocyclone (desander and/or desilter) to separate particles and the underflow passes to the fine screen mesh where in particles larger than barite are discarded and thrown away. In most drilling operations, a mud cleaner is installed in its mud systems. It is usually located in a mud tank in the same location as with the desilters.\n\nThe weighted mud flows to the inlet head section of the desander and/or desilter entering the hydrocyclones for separation of particles. Mud leaving the underflow is further screened with fine mesh to separate larger particles allowing only barite size particles to pass through the screen returning and recovering then the clean mud.\n\nThe purpose of the mud cleaner is to remove drilled solids larger than barite. Solids larger than 74–105 micrometres can be removed by the mud cleaner before the viscosity builds up.\nMud cleaner can effectively control weighted drilling fluid solid content. Prevent differential pressure sticking, adhesion stuck drill accident, reduce the drill string and the filter cake thickness on bond issues.\n"}
{"id": "733009", "url": "https://en.wikipedia.org/wiki?curid=733009", "title": "Nanopore sequencing", "text": "Nanopore sequencing\n\nNanopore sequencing is a third generation approach used in the sequencing of biopolymers- specifically, polynucleotides in the form of DNA or RNA.\n\nUsing nanopore sequencing, a single molecule of DNA or RNA can be sequenced without the need for PCR amplification or chemical labeling of the sample. At least one of these aforementioned steps is necessary in the procedure of any previously developed sequencing approach. Nanopore sequencing has the potential to offer relatively low-cost genotyping, high mobility for testing, and rapid processing of samples with the ability to display results in real-time. Publications on the method outline its use in rapid identification of viral pathogens, monitoring ebola, environmental monitoring, food safety monitoring, human genome sequencing, plant genome sequencing, monitoring of antibiotic resistance, haplotyping and other applications.\n\nNanopore sequencing uses electrophoresis to transport an unknown sample through an orifice of 10 meters in diameter. A nanopore system always contains an electrolytic solution- when a constant electric field is applied, an electric current can be observed in the system. The magnitude of the electric current density across a nanopore surface depends on the nanopore's dimensions and the composition of DNA or RNA that is occupying the nanopore. Sequencing is made possible because, when close enough to nanopores, samples cause characteristic changes in electric current density across nanopore surfaces. The total charge flowing through a nanopore channel is equal to the surface integral of electric current density flux across the nanopore unit normal surfaces between times t and t.\n\nBiological nanopore sequencing relies on the use of transmembrane proteins, called porins, that are embedded in lipid membranes so as to create size dependent porous surfaces- with nanometer scale \"holes\" distributed across the membranes. Sufficiently low translocation velocity can be attained through the incorporation of various proteins that facilitate the movement of DNA or RNA through the pores of the lipid membranes.\n\nAlpha hemolysin (αHL), a nanopore from bacteria that causes lysis of red blood cells, has been studied for over 15 years. To this point, studies have shown that all four bases can be identified using ionic current measured across the αHL pore. The structure of αHL is advantageous to identify specific bases moving through the pore. The αHL pore is ~10 nm long, with two distinct 5 nm sections. The upper section consists of a larger, vestibule-like structure and the lower section consists of three possible recognition sites (R1, R2, R3), and is able to discriminate between each base.\n\nSequencing using αHL has been developed through basic study and structural mutations, moving towards the sequencing of very long reads. Protein mutation of αHL has improved the detection abilities of the pore. The next proposed step is to bind an exonuclease onto the αHL pore. The enzyme would periodically cleave single bases, enabling the pore to identify successive bases. Coupling an exonuclease to the biological pore would slow the translocation of the DNA through the pore, and increase the accuracy of data acquisition.\n\nNotably, theorists have shown that sequencing via exonuclease enzymes as described here is not feasible. This is mainly due to diffusion related effects imposing a limit on the capture probability of each nucleotide as it is cleaved. This results in a significant probability that a nucleotide is either not captured before it diffuses into the bulk or captured out of order, and therefore is not properly sequenced by the nanopore, leading to insertion and deletion errors. Therefore, major changes are needed to this method before it can be considered a viable strategy.\n\nA recent study has pointed to the ability of αHL to detect nucleotides at two separate sites in the lower half of the pore. The R1 and R2 sites enable each base to be monitored twice as it moves through the pore, creating 16 different measurable ionic current values instead of 4. This method improves upon the single read through the nanopore by doubling the sites that the sequence is read per nanopore.\n\nMycobacterium smegmatis porin A (MspA) is the second biological nanopore currently being investigated for DNA sequencing. The MspA pore has been identified as a potential improvement over αHL due to a more favorable structure. The pore is described as a goblet with a thick rim and a diameter of 1.2 nm at the bottom of the pore. A natural MspA, while favorable for DNA sequencing because of shape and diameter, has a negative core that prohibited single stranded DNA(ssDNA) translocation. The natural nanopore was modified to improve translocation by replacing three negatively charged aspartic acids with neutral asparagines.\n\nThe electric current detection of nucleotides across the membrane has been shown to be tenfold more specific than αHL for identifying bases. Utilizing this improved specificity, a group at the University of Washington has proposed using double stranded DNA (dsDNA) between each single stranded molecule to hold the base in the reading section of the pore. The dsDNA would halt the base in the correct section of the pore and enable identification of the nucleotide. A recent grant has been awarded to a collaboration from UC Santa Cruz, the University of Washington, and Northeastern University to improve the base recognition of MspA using phi29 polymerase in conjunction with the pore.\n\nSolid state nanopore sequencing approaches, unlike biological nanopore sequencing, do not incorporate proteins into their systems. Instead, solid state nanopore technology uses various metal or metal alloy substrates with nanometer sized pores that allow DNA or RNA to pass through. These substrates most often serve integral roles in the sequence recognition of nucleic acids as they translocate through the channels along the substrates.\n\nMeasurement of electron tunneling through bases as ssDNA translocates through the nanopore is an improved solid state nanopore sequencing method. Most research has focused on proving bases could be determined using electron tunneling. These studies were conducted using a scanning probe microscope as the sensing electrode, and have proved that bases can be identified by specific tunneling currents. After the proof of principle research, a functional system must be created to couple the solid state pore and sensing devices. \nResearchers at the Harvard Nanopore group have engineered solid state pores with single walled carbon nanotubes across the diameter of the pore. Arrays of pores are created and chemical vapor deposition is used to create nanotubes that grow across the array. Once a nanotube has grown across a pore, the diameter of the pore is adjusted to the desired size. Successful creation of a nanotube coupled with a pore is an important step towards identifying bases as the ssDNA translocates through the solid state pore.\n\nAnother method is the use of nanoelectrodes on either side of a pore. The electrodes are specifically created to enable a solid state nanopore's formation between the two electrodes. This technology could be used to not only sense the bases but to help control base translocation speed and orientation.\n\nAn effective technique to determine a DNA sequence has been developed using solid state nanopores and fluorescence. This fluorescence sequencing method converts each base into a characteristic representation of multiple nucleotides which bind to a fluorescent probe strand-forming dsDNA. With the two color system proposed, each base is identified by two separate fluorescences, and will therefore be converted into two specific sequences. Probes consist of a fluorophore and quencher at the start and end of each sequence, respectively. Each fluorophore will be extinguished by the quencher at the end of the preceding sequence. When the dsDNA is translocating through a solid state nanopore, the probe strand will be stripped off, and the upstream fluorophore will fluoresce.\n\nThis sequencing method has a capacity of 50-250 bases per second per pore, and a four color fluorophore system (each base could be converted to one sequence instead of two), will sequence over 500 bases per second. Advantages of this method are based on the clear sequencing readout—using a camera instead of noisy current methods. However, the method does require sample preparation to convert each base into an expanded binary code before sequencing. Instead of one base being identified as it translocates through the pore, ~12 bases are required to find the sequence of one base.\n\n\nBiological nanopore sequencing systems have several fundamental characteristics that make them advantageous as compared with solid state systems- with each advantageous characteristic of this design approach stemming from the incorporation of proteins into their technology. Uniform pore structure, the precise control of sample translocation through pore channels, and even the detection of individual nucleotides in samples can be facilitated by unique proteins from a variety of organism types.\n\nThe use of proteins in biological nanopore sequencing systems, despite the various benefits, also brings with it some negative characteristics. The sensitivity of the proteins in these systems to local environmental stress has a large impact on the longevity of the units, overall. One example is that a motor protein may only unzip samples with sufficient speed at a certain pH range while not operating fast enough outside of the range- this constraint impacts the functionality of the whole sequencing unit. Another example is that a transmembrane porin may only operate reliably for a certain number of runs before it breaks down. Both of these examples would have to be controlled for in the design of any viable biological nanopore system- something that may be difficult to achieve while keeping the costs of such a technology as low and as competitive, to other systems, as possible.\n\nOne challenge for the 'strand sequencing' method was in refining the method to improve its resolution to be able to detect single bases. In the early papers methods, a nucleotide needed to be repeated in a sequence about 100 times successively in order to produce a measurable characteristic change. This low resolution is because the DNA strand moves rapidly at the rate of 1 to 5μs per base through the nanopore. This makes recording difficult and prone to background noise, failing in obtaining single-nucleotide resolution. The problem is being tackled by either improving the recording technology or by controlling the speed of DNA strand by various protein engineering strategies and Oxford Nanopore employs a 'kmer approach', analyzing more than one base at any one time so that stretches of DNA are subject to repeat interrogation as the strand moves through the nanopore one base at a time. Various techniques including algorithmic have been used to improve the performance of the MinION technology since it was first made available to users. More recently effects of single bases due to secondary structure or released mononucleotides have been shown.\n\nProfessor Hagan Bayley proposed in 2010 that creating two recognition sites within an alpha hemolysin pore may confer advantages in base recognition.\n\nOne challenge for the 'exonuclease approach', where a processive enzyme feeds individual bases, in the correct order, into the nanopore, is to integrate the exonuclease and the nanopore detection systems. In particular, the problem is that when an exonuclease hydrolyzes the phosphodiester bonds between nucleotides in DNA, the subsequently released nucleotide is not necessarily guaranteed to directly move into, say, a nearby alpha-hemolysin nanopore. One idea is to attach the exonuclease to the nanopore, perhaps through biotinylation to the beta barrel hemolysin. The central pore of the protein may be lined with charged residues arranged so that the positive and negative charges appear on opposite sides of the pore. However, this mechanism is primarily discriminatory and does not constitute a mechanism to guide nucleotides down some particular path.\n\nAgilent Laboratories was the first to license and develop nanopores but does not have any current disclosed research in the area. Oxford Nanopore technologies sells portable and desktop sequencers.\n\n"}
{"id": "57879382", "url": "https://en.wikipedia.org/wiki?curid=57879382", "title": "Navicula de Venetiis", "text": "Navicula de Venetiis\n\nA navicula de Venetiis or \"little ship of Venice\" was an altitude dial used to tell time and which was shaped like a little ship. The cursor (with a plumb line attached) was slid up/down the mast to the correct latitude. The user then sighted the sun through the pair of sighting holes at either end of the \"ship's deck\". The plumb line then marked what hour of the day it was. Some naviculas had additional information inscribed, such as the latitude of some common English towns, some zodiac signs, etc.\n"}
{"id": "17714766", "url": "https://en.wikipedia.org/wiki?curid=17714766", "title": "Nora Stanton Barney", "text": "Nora Stanton Barney\n\nNora Stanton Blatch Barney (September 30, 1883 – January 18, 1971) was an English-born U.S. civil engineer, architect, and suffragist. Barney was among the first women to graduate with an engineering degree in United States. She was the granddaughter of Elizabeth Cady Stanton.\n\nShe was born Nora Stanton Blatch in Basingstoke, Hampshire, England in 1883 to William Blatch and Harriot Eaton Stanton, daughter of Elizabeth Cady Stanton. She studied Latin and mathematics at the Horace Mann School in New York, beginning in 1897, returning to England in the summers. The family moved to the United States in 1902. Nora attended Cornell University, graduating in 1905 with a degree in civil engineering. She was Cornell University's first female engineering graduate. In the same year, she was accepted as a junior member of the American Society of Civil Engineers (ASCE), and began work for the New York City Board of Water Supply. She also worked for the American Bridge Company in 1905–06. \n\nFollowing the examples set by her mother and grandmother, Nora also became active in the growing women's suffrage movement. She was the first female member of the American Society of Civil Engineers, where she was allowed to be a junior member only and denied advancement to associate member in 1916 solely because of her gender. At the time, women were only admitted as junior members. In 1916, she sued the American Society of Civil Engineers (ASCE) for refusing to admit her as a full member, even though she met all requirements. Blatch lost, and no woman became a full ASCE member for a decade. In 2015, she was posthumously advanced to ASCE Fellow status.\n\nIn 1908, she married the inventor Lee de Forest, and helped to manage some of the companies he had founded to promote his invention and the new technology of wireless (radio). The couple spent their honeymoon in Europe marketing radio equipment developed by de Forest. However, the couple separated only a year later, due largely to de Forest's insistence that Nora quit her profession and become a conventional housewife. Shortly afterward, in June 1909, Nora gave birth to their daughter, Harriot. In 1909, she began working as an engineer for the Radley Steel Construction Company. She divorced de Forest in 1911. After her divorce, she continued her engineering career, working for the New York Public Service Commission.\n\nIn 1919, Nora married Morgan Barney, a marine architect. Their daughter, Rhoda Barney Jenkins, born July 12, 1920, in New York, was an architect and social activist. Nora continued to work for equal rights for women and world peace, and in 1944 authored \"World Peace Through a People's Parliament\".\n\nNora worked as a real-estate developer and political activist until her death in Greenwich, Connecticut on January 18, 1971.\n\n\n"}
{"id": "72877", "url": "https://en.wikipedia.org/wiki?curid=72877", "title": "Odometer", "text": "Odometer\n\nAn odometer or odograph is an instrument used for measuring the distance travelled by a vehicle, such as a bicycle or car. The device may be electronic, mechanical, or a combination of the two. The noun derives from the Ancient Greek word οδόμετρο from \"odós\", οδός (\"path\" or \"gateway\") and \"métro\", μέτρο (\"measure\"). Early forms of the odometer existed in the ancient Greco-Roman world as well as ancient China. In countries where Imperial units or US customary units are used, it is sometimes called a mileometer or milometer, the former name especially being prevalent in the United Kingdom and members of the Commonwealth.\n\nPossibly the first evidence for the use of an odometer can be found in the works of the ancient Roman Pliny (NH 6. 61-62) and the ancient Greek Strabo (11.8.9). Both authors list the distances of routes traveled by Alexander the Great (r. 336-323 BC) as by his bematists Diognetus and Baeton. However, the high accuracy of the bematists's measurements rather indicates the use of a mechanical device. For example, the section between the cities Hecatompylos and Alexandria Areion, which later became a part of the silk road, was given by Alexander's bematists as 529 English miles long, that is with a deviation of 0.2% from the actual distance (531 English miles). From the nine surviving bematists' measurements in Pliny's \"Naturalis Historia\" eight show a deviation of less than 5% from the actual distance, three of them being within 1%. Since these minor discrepancies can be adequately explained by slight changes in the tracks of roads during the last 2300 years, the overall accuracy of the measurements implies that the bematists already must have used a sophisticated device for measuring distances, although there is no direct mention of such a device.\n\nAn odometer for measuring distance was first described by Vitruvius around 27 and 23 BC, although the actual inventor may have been Archimedes of Syracuse (c. 287 BC – c. 212 BC) during the First Punic War. Hero of Alexandria (10 AD - 70 AD) describes a similar device in chapter 34 of his \"Dioptra\". The machine was also used in the time of Roman Emperor Commodus (c. 192 AD), although after this point in time there seems to be a gap between its use in Roman times and that of the 15th century in Western Europe. Some researchers have speculated that the device might have included technology similar to that of the Greek Antikythera mechanism.\n\nThe odometer of Vitruvius was based on chariot wheels of 4 feet (1.2 m) diameter turning 400 times in one Roman mile (about 1400 m). For each revolution a pin on the axle engaged a 400 tooth cogwheel thus turning it one complete revolution per mile. This engaged another gear with holes along the circumference, where pebbles (\"calculus\") were located, that were to drop one by one into a box. The distance traveled would thus be given simply by counting the number of pebbles. Whether this instrument was ever built at the time is disputed. Leonardo da Vinci later tried to build it himself according to the description, but failed. However, in 1981 engineer Andre Sleeswyk built his own replica, replacing the square-toothed gear designs of da Vinci with the triangular, pointed teeth found in the \"Antikythera mechanism\". With this modification, the Vitruvius odometer functioned perfectly.\n\nThe odometer was also independently invented in ancient China, possibly by the prolific inventor and early scientist Zhang Heng (78 AD – 139 AD) of the Han Dynasty. By the 3rd century (during the Three Kingdoms Period), the Chinese had termed the device as the 'jì lĭ gŭ chē' (記里鼓車), or 'li-recording drum carriage' (Note: the modern measurement of li = 500 m/1640 ft). Chinese texts of the 3rd century tell of the mechanical carriage's functions, and as one li is traversed, a mechanical-driven wooden figure strikes a drum, and when ten li is traversed, another wooden figure would strike a gong or a bell with its mechanical-operated arm.\n\nDespite its association with Zhang Heng or even the later Ma Jun (c. 200–265), there is evidence to suggest that the invention of the odometer was a gradual process in Han Dynasty China that centered around the \"huang men\" court people (i.e. eunuchs, palace officials, attendants and familiars, actors, acrobats, etc.) that would follow the musical procession of the royal 'drum-chariot'. The historian Joseph Needham asserts that it is no surprise this social group would have been responsible for such a device, since there is already other evidence of their craftsmenship with mechanical toys to delight the emperor and the court. There is speculation that some time in the 1st century BC (during the Western Han Dynasty), the beating of drums and gongs were mechanically-driven by working automatically off the rotation of the road-wheels. This might have actually been the design of one Loxia Hong (c. 110 BC), yet by 125 AD the mechanical odometer carriage in China was already known (depicted in a mural of the Xiaotangshan Tomb).\n\nThe odometer was used also in subsequent periods of Chinese history. In the historical text of the \"Jin Shu\" (635 AD), the oldest part of the compiled text, the book known as the \"Cui Bao\" (c. 300 AD), recorded the use of the odometer, providing description (attributing it to the Western Han era, from 202 BC–9 AD). The passage in the \"Jin Shu\" expanded upon this, explaining that it took a similar form to the mechanical device of the south-pointing chariot invented by Ma Jun (200–265, see also differential gear). As recorded in the \"Song Shi\" of the Song Dynasty (960-1279 AD), the odometer and south-pointing chariot were combined into one wheeled device by engineers of the 9th century, 11th century, and 12th century. The \"Sunzi Suanjing\" (Master Sun's Mathematical Manual), dated from the 3rd century to 5th century, presented a mathematical problem for students involving the odometer. It involved a given distance between two cities, the small distance needed for one rotation of the carriage's wheel, and the posed question of how many rotations the wheels would have in all if the carriage was to travel between point A and B.\n\nThe historical text of the \"Song Shi\" (1345 AD), recording the people and events of the Chinese Song Dynasty (960–1279), also mentioned the odometer used in that period. However, unlike written sources of earlier periods, it provided a much more thoroughly detailed description of the device that harkens back to its ancient form (Wade-Giles spelling):\nThe odometer. [The mile-measuring carriage] is painted red, with pictures of flowers and birds on the four sides, and constructed in two storeys, handsomely adorned with carvings. At the completion of every li, the wooden figure of a man in the lower storey strikes a drum; at the completion of every ten li, the wooden figure in the upper storey strikes a bell. The carriage-pole ends in a phoenix-head, and the carriage is drawn by four horses. The escort was formerly of 18 men, but in the 4th year of the Yung-Hsi reign-period (987 AD) the emperor Thai Tsung increased it to 30. In the 5th year of the Thien-Sheng reign-period (1027 AD) the Chief Chamberlain Lu Tao-lung presented specifications for the construction of odometers as follows:\nWhat follows is a long dissertation made by the Chief Chamberlain Lu Daolong on the ranging measurements and sizes of wheels and gears, along with a concluding description at the end of how the device ultimately functions:\nThe vehicle should have a single pole and two wheels. On the body are two storeys, each containing a carved wooden figure holding a drumstick. The road-wheels are each 6 ft in diameter, and 18 ft in circumference, one evolution covering 3 paces. According to ancient standards the pace was equal to 6 ft and 300 paces to a li; but now the li is reckoned as 360 paces of 5 ft each. \n[Note: the measurement of the Chinese-mile unit, the li, was changed over time, as the li in Song times differed from the length of a li in Han times.]\nThe vehicle wheel (li lun) is attached to the left road-wheel; it has a diameter of 1.38 ft with a circumference of 4.14 ft, and has 18 cogs (chhih) 2.3 inches apart. There is also a lower horizontal wheel (hsia phing lun), of diameter 4.14 ft and circumference 12.42 ft, with 54 cogs, the same distance apart as those on the vertical wheel (2.3 inches). (This engages with the former.) \n\nUpon a vertical shaft turning with this wheel, there is fixed a bronze \"turning-like-the-wind wheel\" (hsuan feng lun) which has (only) 3 cogs, the distance between these being 1.2 inches. (This turns the following one.) In the middle is a horizontal wheel, 4 ft in diameter, and 12 ft circumference, with 100 cogs, the distance between these cogs being the same as on the \"turning-like-the-wind wheel\" (1.2 inches).\n\nNext, there is fixed (on the same shaft) a small horizontal wheel (hsiao phing lun) 3.3 inches in diameter and 1 ft in circumference, having 10 cogs 1.5 inches apart. (Engaging with this) there is an upper horizontal wheel (shang phing lun) having a diameter of 3.3 ft and a circumference of 10 ft, with 100 cogs, the same distance apart as those of the small horizontal wheel (1.5 inches).\n\nWhen the middle horizontal wheel has made 1 revolution, the carriage will have gone 1 li and the wooden figure in the lower story will strike the drum. When the upper horizontal wheel has made 1 revolution, the carriage will have gone 10 li and the figure in the upper storey will strike the bell. The number of wheels used, great and small, is 8 inches in all, with a total of 285 teeth. Thus the motion is transmitted as if by the links of a chain, the \"dog-teeth\" mutually engaging with each other, so that by due revolution everything comes back to its original starting point (ti hsiang kou so, chhuan ya hsiang chih, chou erh fu shih).\nOdometers were first developed in the 1600s for wagons and other horse-drawn vehicles in order to measure distances traveled.\n\nLevinus Hulsius published the odometer in 1604 in his work \"Gründtliche Beschreibung deß Diensthafften und Nutzbahrn Instruments Viatorii oder Wegzählers, So zu Fuß, zu Pferdt unnd zu Fußen gebraucht werden kann, damit mit geringer mühe zu wissen, wie weit man gegangen, geritten, oder gefahren sey: als auch zu erfahren, ohne messen oder zehlen, wie weit von einem Orth zum andern. Daneben wird auch der grosse verborgene Wegweiser angezeiget und vermeldet\".\n\nIn 1645, the French mathematician Blaise Pascal invented the \"pascaline\". Though not an odometer, the \"pascaline\" utilized gears to compute measurements. Each gear contained 10 teeth. The first gear advanced the next gear one position when moved one complete revolution, the same principle employed on modern mechanical odometers.\n\nOdometers were developed for ships in 1698 with the odometer invented by the Englishman Thomas Savery. Benjamin Franklin, U.S. statesman and the first Postmaster General, built a prototype odometer in 1775 that he attached to his carriage to help measure the mileage of postal routes. In 1847, William Clayton, a Mormon pioneer, invented the \"Roadometer\", which he attached to a wagon used by American settlers heading west. The \"Roadometer\" recorded the distance traveled each day by the wagon trains. The Roadometer used two gears and was an early example of an odometer with pascaline-style gears in actual use.\n\nIn 1895, Curtis Hussey Veeder invented the \"Cyclometer\". The \"Cyclometer\" was a mechanical device that counted the number of rotations of a bicycle wheel. A flexible cable transmitted the number of rotations of the wheel to an analog odometer visible to the rider, which converted the wheel rotations into the number of miles traveled according to a predetermined formula.\n\nIn 1903 Arthur P. and Charles H. Warner, two brothers from Beloit, Wisconsin, introduced their patented \"Auto-meter\". The \"Auto-Meter\" used a magnet attached to a rotating shaft to induce a magnetic pull upon a thin metal disk. Measuring this pull provided accurate measurements of both distance and speed information to automobile drivers in a single instrument. The Warners sold their company in 1912 to the Stewart & Clark Company of Chicago. The new firm was renamed the Stewart-Warner Corporation. By 1925, Stewart-Warner odometers and trip meters were standard equipment on the vast majority of automobiles and motorcycles manufactured in the United States.\n\nBy the early 2000s, mechanical odometers would be phased out on cars from major manufacturers. The Pontiac Grand Prix was the last GM car sold in the US to offer a mechanical odometer in 2003, the Canadian-built Ford Crown Victoria and Mercury Grand Marquis were the very last Fords sold with one in 2005. \n\nWhilst an odometer is used to record distance (units can vary, usually between miles and kilometres), a mileometer specifically records only in miles. The equivalent used to record kilometres is sometimes referred to as a \"kilometreometer\".\n\nMost modern cars include a trip meter (trip odometer). Unlike the odometer, a trip meter is reset at any point in a journey, making it possible to record the distance traveled in any particular journey or part of a journey. It was traditionally a purely mechanical device but, in most modern vehicles, it is now electronic. Luxury vehicles often have multiple trip meters. Most trip meters will show a maximum value of 999.9. The trip meter may be used to record the distance traveled on each tank of fuel, making it very easy to accurately track the energy efficiency of the vehicle; another common use is resetting it to zero at each instruction in a sequence of driving directions, to be sure when one has arrived at the next turn.\n\nA form of fraud is to tamper with the reading on an mileometer/odometer and presenting the incorrect number of miles/kilometers traveled to a prospective buyer; this is often referred to as \"clocking\" in the UK and \"Busting miles\" in the US. This is done to make a car appear to have been driven less than it really has been, and thus increase its apparent market value. Most new cars sold today use digital odometers that store the mileage in the vehicle's engine control module making it difficult (but not impossible) to manipulate the mileage electronically. With mechanical odometers, the speedometer can be removed from the car dashboard and the digits wound back, or the drive cable can be disconnected and connected to another odometer/speedometer pair while on the road. Older vehicles can be driven in reverse to subtract mileage, a property that provides the premise for a classic scene late in the comedy film \"Ferris Bueller's Day Off\", but modern odometers add mileage driven in reverse to the total as if driven forward, thereby accurately reflecting the true total wear and tear on the vehicle.\n\nThe resale value of a vehicle is often strongly influenced by the total distance shown on the odometer, yet odometers are inherently insecure because they are under the control of their owners. Many jurisdictions have chosen to enact laws which penalize people who are found to commit odometer fraud. In the US (and many other countries), vehicle mechanics are also required to keep records of the odometer any time a vehicle is serviced. Companies such as Carfax then use these data to help potential car buyers detect whether odometer rollback has occurred.\n\nResearch by Irish vehicle check specialist Cartell found that 20% of vehicles imported to Ireland from Great Britain and Northern Ireland had had their mileometers altered to show a lower mileage.\n\nMost odometers work by counting wheel rotations and assume that the distance traveled is the number of wheel rotations times the tire circumference, which is a standard tire diameter times pi (3.1416). If nonstandard or severely worn or underinflated tires are used then this will cause some error in the odometer. The formula is (actual distance traveled) = ( (final odometer reading) - (initial odometer reading) ) * (actual tire diameter) / (standard tire diameter). It is common for odometers to be off by several percent. Odometer errors are typically proportional to speedometer errors.\n\n\n\n"}
{"id": "24932355", "url": "https://en.wikipedia.org/wiki?curid=24932355", "title": "Olav Tryggvason (statue)", "text": "Olav Tryggvason (statue)\n\nA statue of Olav Tryggvason is located in Trondheim, Norway. Sculpted by sculptor Wilhelm Rasmussen, it honors King Olav Tryggvason who was the city's 's founder.\nThe 18-metre (58-foot) high statue is mounted on top of an obelisk. It stands at the center of the city square (\"Torvet i Trondheim\") at the intersection of the two main streets, Munkegata and Kongens gate. The statue was unveiled in 1921. Around the statue base is a cobblestone mosaic, dating from 1930, which forms a gigantic sun dial . The sun dial is calibrated to UTC+1, meaning that the reading is inaccurate by one hour in the summer.\n\n"}
{"id": "5191394", "url": "https://en.wikipedia.org/wiki?curid=5191394", "title": "Operation Midnight Climax", "text": "Operation Midnight Climax\n\nOperation Midnight Climax was an operation initially established by Sidney Gottlieb and placed under the direction of Federal Narcotics Bureau in Boston, Massachusetts with the officer George Hunter White under the pseudonym of Morgan Hall for the CIA as a sub-project of Project MKULTRA, the CIA mind-control research program that began in the 1950s. Before the programs were shut down, hundreds of scientists would work on them.\n\nThe project consisted of a web of CIA-run safehouses in San Francisco, Marin, and New York City. It was established in order to study the effects of LSD on unconsenting individuals. Prostitutes on the CIA payroll were instructed to lure clients back to the safehouses, where they were surreptitiously plied with a wide range of substances, including LSD, and monitored behind one-way glass. Every one of these acts was blatantly illegal, but several significant operational techniques were developed in this theater, including extensive research into sexual blackmail, surveillance technology, and the possible use of mind-altering drugs in field operations.\n\nThe Operation Midnight Climax soon expanded and CIA operatives began dosing people in restaurants, bars and beaches.\n\nThe safehouses were dramatically scaled back in 1963, following a report by CIA Inspector General John Earman that strongly recommended closing the facility. The San Francisco safehouses were closed in 1965, and the New York City safehouse soon followed in 1966.\n\nIn 1974, the journalist Seymour Hersh exposed the CIA’s illegal spying on U.S. citizens and how the CIA had conducted non-consensual drug experiments. His report started the lengthy process of bringing long-suppressed details about MK-Ultra to light. Project MKULTRA came to light in the spring of 1977 during a wide-ranging survey of the CIA's technical services division. John K. Vance, a member of the CIA inspector general's staff, discovered that the agency was running a research project that included administering LSD and other drugs to unwilling human subjects.\n\n"}
{"id": "18667798", "url": "https://en.wikipedia.org/wiki?curid=18667798", "title": "Pixel artist", "text": "Pixel artist\n\nA pixel artist is a graphic designer who specializes in computer art and can refer to a number of artistic and professional disciplines which focus on visual communication and presentation. Similar to chromoluminarism used in the pointillism style of painting, in which small distinct points of primary colors create the impression of a wide selection of secondary and intermediate colors, a pixel artist works with pixels, the smallest piece of information in an image. The technique relies on the perceptive ability of the eye and mind of the viewer to mix the color spots into a fuller range of tones. Pixel art is often utilitarian and anonymous. Pixel design can refer to both the process (designing) by which the communication is created and the products (designs) which are generated.\n\nCommon uses of pixel design include print and broadcast media, web design and games. For example, a product package might include a logo or other artwork, organized text and pure design elements such as shapes and color which unify the piece. Composition is one of the most important features of design especially when utilizing pre-existing materials or using diverse elements. Pixel artists can also be a specialist in computer animation such as Computer Animation Production System users in post production of animated films and rendering (computer graphics) images like raster graphics.\n\nIn the 2000s, pixel artists such as Tyler West, Stephane Martiniere and Daniel Dociu have gained international notoriety and artistic recognition, due in part to the popularity of computer and video games. For instance the E3 Media and Business Summit, an annual trade show for the computer and video games industry, has a concurrent juried art show, \"Into the Pixel\" starting in 2003. Jurist and Getty Research Institute curator Louis Marchesano noted that most of the works were concept pieces used in the development of games.\n\nPixel artists are also used in digital forensics, an emerging field, to both create and detect fraud in all forms of media including \"the courts, politics and scientific journals\". For instance, the Federal Office of Research Integrity has said that the percent of allegations of fraud they investigated involved contested images has risen from less than 3 in 1990 to 44.1 percent in 2006.\n\nComputer art started in 1960s and by its nature is evolutionary since changes in technology and software directly affect what is possible. The term \"pixel art\" was first published by Adele Goldberg and Robert Flegal of Xerox Palo Alto Research Center in 1982. Adobe Systems, founded in 1982, developed the Postscript language and digital fonts, making drawing painting and image manipulation software popular. Adobe Illustrator, a vector drawing program based on the Bezier curve, was introduced in 1987 and Adobe Photoshop followed in 1990. Adobe Flash, a popular set of multimedia software used to add animation and interactivity to web pages, was introduced in 1996.\n\nIn the 2000s, pixel artists have been employed increasingly in video games and, to a lesser extent in music videos. These artists have remained somewhat underground until the mid-2000s. In 2006 Röyksopp released \"Remind Me\", illustrated completely by pixel art, which the New York Times cited as amazing and hypnotic. (See video here .)\n\nA pixel artist is one of the new media artists that employs technology while also utilizing traditional media and art forms. They may have a fine arts background such as photography, painting or drawing but self-taught designers and artists are also able to accomplish this work. They are often required to employ imaging and a full range of artistic and technological skills including those of conceptual artists.\n\nIn digital imaging, a pixel (\"pict\"ure \"el\"ement) is the smallest piece of information in an image. \nThe word \"pixel\" is based on a contraction of \"pix\" (for \"pictures\") and \"el\" (for \"element\"); similar formations with \"el\" for \"element\" include voxel, luxel, and texel. Pixels are normally arranged in a regular 2-dimensional grid, and are often represented using dots, squares, or rectangles. Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel has typically three or four components such as red, green, and blue, or cyan, magenta, yellow, and black.\n\nNeuroplasticity is a key element of observing many pixel images. While two individuals will observe the same photons reflecting off a photorealistic image and hitting their retinas, someone whose mind has been primed with the theory of pointillism may \"see\" a very different image as the image is interpreted in the visual cortex.\n\nThe total number of pixels (\"image resolution\"), and the amount of information in each pixel (often called \"color depth\") determine the quality of an image. For example, an image that stores 24 bits of color-information per pixel (the standard for computer displays since around 1995) can represent smoother degrees of shading than one that only stores 16 bits per pixel, but not as smooth as one that stores 48 bits. Likewise, an image sampled at 640 x 480 pixels (and therefore containing 307,200 pixels) will look rough and blocky compared to one sampled at 1280 x 1024 (1,310,720 pixels). Because it takes a large amount of data to store a high-quality image, computer software often uses data compression techniques to reduce this size for images stored on disk. Some techniques sacrifice information, and therefore image quality, in order to achieve a smaller file-size. Computer scientists refer to compression techniques that lose information as lossy compression.\n\nModern computer-monitors typically display about 72 to 130 pixels per inch (PPI), and some modern consumer printers can resolve 2400 dots per inch (DPI) or more; determining the most appropriate image resolution for a given printer-resolution can pose difficulties, since printed output may have a greater level of detail than a viewer can discern on a monitor. Typically, a resolution of 150 to 300 pixel per inch works well for 4-color process (CMYK) printing.\nDrawings usually start with what is called the line art, which is the basic line that defines the item the artist intends to create. Line arts can be either traced over scanned drawings or hand drawn on the computer itself by the use of a mouse or a graphics tablet and are often shared among other pixel artists in diverse websites in order to receive some feedback. Other techniques, some resembling painting, also exist, such as knowledge of the color theory. The limited palette often implemented into pixel art usually promotes the use of dithering in order to achieve different shades and colors (when necessary); hand-made anti-aliasing is also used for smoother purposes. A pixel artist will exponentially increase the zoom of whatever they are working on to make adjustments as needed and then view the results until desired changes are achieved.\n\n\n"}
{"id": "23679414", "url": "https://en.wikipedia.org/wiki?curid=23679414", "title": "Real-time Neutron Monitor Database", "text": "Real-time Neutron Monitor Database\n\nThe Real-time Neutron Monitor Database (or NMDB) is a worldwide network of standardized neutron monitors, used to record variations of the primary cosmic rays. The measurements complement space-based cosmic ray measurements.\n\nUnlike data from satellite experiments, neutron monitor data has never been available in high resolution from many stations in real-time. The data is often only available from the individual stations website, in varying formats, and not in real-time. To overcome this deficit, the European Commission is supporting the Real-time Neutron Monitor Database (NMDB) as an e-Infrastructures project in the Seventh Framework Programme in the Capacities section. Stations that do not have 1-minute resolution will be supported by the development of an affordable standard registration system that will submit the measurements to the database via the internet in real-time. This resolves the problem of different data formats and for the first time allows to use real-time cosmic ray measurements for space weather predictions (Steigies, Klein et al.)\n\nBesides creating a database and developing applications working with this data, a part of the project is dedicated to create a public outreach website to inform about cosmic rays and possible effects on humans, technological systems, and the environment (Mavromichalaki et al.)\n\n\n"}
{"id": "6881874", "url": "https://en.wikipedia.org/wiki?curid=6881874", "title": "Sakari Pinomäki", "text": "Sakari Pinomäki\n\nSakari Pinomäki (1933–2011) was a Finnish systems engineer and an inventor, who pioneered the mechanized forestry industry. He was the founder of PIKA Forest Machines which produced the first purpose-built forest machine in 1964 in Ylöjärvi\n, Finland. His inventions had over 50 patents.\n\nSakari Pinomäki's first company, PIKA Forest Machines, is credited with designing the first self-propelled tree length timber processor, the PIKA Model 60, in 1968, and the first fully mobile timber \"harvester\", the PIKA Model 75, in 1974. These machines differed significantly from other \"retro-fitted\" forestry machines in that they were designed from inception to be timber harvesting and processing equipment, and were not conventional farming or earth moving equipment with additional apparatus welded onto them to allow timber processing work to be possible. Pinomäki coined the term \"harvester\" to describe his Model 75 machine, which differs from a tree length processor in that a harvester grips, fells, de-limbs and sections the tree on site, while a processor simply de-limbs a tree that has been felled by chain saws and dragged to the delimbing equipment. His designs and innovations have been subsequently copied by at least five other major manufacturers of heavy timber equipment, including Timberjack, Valmet and Ponsse, and were instrumental in developing the \"Scandinavian\" system of timber harvesting, which is far more sustainable and nature conserving than the methods employed up till the mid-20th century. The two machine harvester-forwarder system consequently became the worldwide standard for sustainable forestry.\n\nOne of the most significant of PIKA's inventions has been the Paralcon Hydraulic valve system that can be used on any twin boom extending-retracting crane. This valve uses return oil flow pressure to power the extension piston, and flow oil pressure to power the retraction movement of the crane, as opposed to standard configurations that use additional pumps to power these crane movements. The result is that far less motor torque is necessary to operate the crane, which consumes far less fuel; this in turn saves operators money and reduces CO emissions for the machinery in use.\n\nAdditionally, S. Pinomäki Ky PIKA was first to market in the early 21st century with the world's first production \"Combination\" machine, a single machine that can function in both harvester and forwarder roles. This 50 percent reduction in machinery to perform the same harvesting work means far less environmental pollution from CO emissions and terrain damage from machinery operation, and has again be co-opted by every major manufacturer of heavy timber processing equipment.\n\nSakari died on July 29, 2011 after a battle with pancreatic cancer. He is survived by his two daughters and three grandchildren.\n\n\n"}
{"id": "46996599", "url": "https://en.wikipedia.org/wiki?curid=46996599", "title": "Shaker broom vise", "text": "Shaker broom vise\n\nThe Shaker broom vise is a specialized production vise that made the normally round broom flat to make it more efficient for cleaning purposes. The Shakers' invention revolutionized the production and form of brooms; in the process creating a whole new industry in New England.\n\nShaker brooms built upon the 1797 contribution of Levi Dickenson of Hadley, Massachusetts who used tassels of sorghum (Sorghum vulgare), known as broom corn, to make a better grade of broom. Brooms were essential to kitchen and hearth cleanliness. The manufacture and selling of brooms was the most widespread of all the Shaker industries. The first sorghum brooms were made by the Shakers at Watervliet. This colony is credited with being the first to grow broom corn, which was around 1800 when they first grew it on an island in the Mohawk River that was near their community.\n\nTheodore Bates (1762–1846) of the Watervliet Shaker community is credited with the innovation of the \"flat broom\" in 1798, as all brooms and brushes prior were round. He invented a unique wooden vise that pressed the round bristles flat. This allowed heavy twine to be sewed through the flattened broom to permanently hold that shape. \n\nBates' vise idea to make flat brooms and brushes was a leap in technology since it produced products that worked much more efficiently. An industry developed to sell these to the outside world. Soon the New Lebanon community ramped up to make these flat brooms. This flattening technology worked well with their woodworking technology of lathe-turned wood handles. The vise was used as part of the Shaker broom making process.\n\nMost of the Shaker villages were involved in making these flat brooms and flat brushes. The flat brooms were produced by the tens of thousands. Only a few of the original Shaker flat brooms made in the nineteenth century have survived into the twenty-first century. These were made under the auspices of a Mount Lebanon Shaker Trustee named Robert Valentine (1822–1910).\n\nThe Shakers were the pioneering inventors of the broom vise, which made today's flat broom possible. According to this was “the only major update to the broom since the introduction of the broom machine” and permitted flattening and sewing, instead of mere lashing to a round handle. This design innovation is evidenced in modern brooms (assuming they are not synthetic).\n\nThe brooms were respected and given care – such as hanging on the wall when not in use and sometimes covered with cotton hoods to keep them clean. The covered flat brooms were used to dry-polish hard wood floors and clean the last traces of dust off hard surfaces.\n\nThe flat broom led to a boom of broom making in the United States. In 1850, more than a million brooms were built in Massachusetts alone, resulting in a large export trade extending to South America.\n\nThe vise consists of two upright planks, one rigidly fixed to the base and the other hinged via a pair of short distance pieces. This can be clearly seen in the small vise on the left of the illustration \"Making Shaker brooms\". The handle of the broom can pass between the distance pieces, the length of the handle imposing a minimum size on the fixed upright. Larger vises may have the movable jaw hinged at the base and do away with the distance pieces, see the illustration \"Shaker broom vises\".\n\nIllustrations on show that there are two sets of jaws affixed to the planks. The lower set are level with the top of the planks and are circular when closed to firmly grip the head of the broom. Metal plates rising from the planks carry the upper set which are flat and force the broom corn flat. The jaws are closed by a toggle mechanism operating on horizontal links from the fixed jaw and vertical hanger links from the floor. The toggle is provided with a long wooden handle which the operator can pull down to secure the broom in the vise.\n\n"}
{"id": "40002476", "url": "https://en.wikipedia.org/wiki?curid=40002476", "title": "Smart battery charger", "text": "Smart battery charger\n\nA smart battery charger is mainly a switch mode power supply (also known as high frequency charger) that has the ability to communicate with a smart battery pack's battery management system (BMS) in order to control and monitor the charging process. This communication may be by a standard bus such as CAN bus in automobiles or System Management Bus (SMBus) in computers. The charge process is controlled by the BMS and not by the charger, thus increasing security in the system. Not all chargers have this type of communication which is commonly used for lithium batteries. \n\nBesides the usual plus (positive) and minus (negative) terminals, a smart battery charger also has multiple terminals to connect to the smart battery pack's BMS. The Smart Battery System standard is commonly used to define this connection, which includes the data bus and the communications protocol between the charger and battery. There are other \"ad-hoc\" specifications also used.\n\nSmart battery controller integrated circuits are available. For example, Linear Technology manufactures the LTC4100 and the LTC4101 Smart Battery System-compatible products.\n\nMicrochip Technology provides an application note for a smart battery charger based on the PIC16C73. The PIC16C73 source code is available for this application.\n"}
{"id": "27285310", "url": "https://en.wikipedia.org/wiki?curid=27285310", "title": "Soesterberg Principles", "text": "Soesterberg Principles\n\nThe Soesterberg Principles was adopted by the Trans-Atlantic Network for Clean Production on May 16, 1999. It was a commitment that for new technical innovation in the industry, that innovation should also include improvements in the environment, health, and social issues that follow. It is an electronic sustainability commitment that technical improvements should correspond to environmental and health improvements. The Electronic Sustainability Commitment of the principles reads:\n\n\"Each new generation of technical improvements in electronic products should include parallel and proportional improvements in environmental, health and safety as well as social attributes\" \n"}
{"id": "24170961", "url": "https://en.wikipedia.org/wiki?curid=24170961", "title": "Solids control", "text": "Solids control\n\nSolids control is a technique used in a drilling rig to separate the solids in the drilling fluids that are crushed by the drill bits and carried out of the well surface. Normally, a solid control system contains five stages: the mud tank, shale shaker, vacuum degasser, desander, desilter, and centrifuge. The shale shaker is used to separate big solids with diameter above 75μm, the desander addresses solids from 45-74μm, and the desilter segregates solids between 15-44μm. Sometimes the desander and desilter are combined as one high efficiency mud cleaner. When air enters the drilling fluids, a vacuum degasser is used to separate the air. When there is no air in the mud, the degasser works as a big agitator. All these stages are mounted on the top of the mud tank. After separating the solids, the clean mud can be pumped into the borehole again.\n\nActually, we call mud system as 4 phases solids control process or the 5 phase purification system. From shale shaker, vacuum degasser, to Desander cone, Desilter cone, and the decanting centrifuge. Different working principle, different separation point. Every well drilling request such a system to process drilling mud. However, per different well depth, different drilling condition we'll get different configuration and different result. \n\nRecent advances in solids control include the creation of a closed loop system which allows for increased environmental control and reduction in the potential for spills of drilling fluids. Several US states have either passed or are considering the implementation of closed loop systems, particularly for hydro-fracturing operations.\n"}
{"id": "15193487", "url": "https://en.wikipedia.org/wiki?curid=15193487", "title": "Stéphane Nomis", "text": "Stéphane Nomis\n\nStéphane Nomis (born 23 October 1970) is a professional French judoka.\n\nFrom 1990 to 1999, he trains as a professional judoka at INSEP and is part of the France national team under the direction of René Rambier at the French Federation of Judo. He trains with David Douillet, Djamel Bouras and Larbi Benboudaoud. Nomis earned medals in many championships, including a gold medal at the French Championships and a bronze medal at the European Championships. He managed to reach the finals of the French championships, and after a bronze medal at the tournament in Paris, he qualified for the European Championships for the second time. He brought his career to an end in 1999.\n\n"}
{"id": "49210329", "url": "https://en.wikipedia.org/wiki?curid=49210329", "title": "TNA (Airborne nuclear warhead)", "text": "TNA (Airborne nuclear warhead)\n\nThe French Airborne nuclear warhead (TNA) is a thermonuclear warhead carried by the Air-Sol Moyenne Portée Amélioré (ASMPA) medium-range air-to-surface missile, a component of the Force de frappe French nuclear deterrent. The warhead was introduced in 2010. 54 warheads had been produced replacing former TN 81 warhead carried by former Air-Sol Moyenne Portée Amélioré (ASMP) medium-range air-to-surface missile.\n\nDeployment: 54 warheads carried by the ASMPA equipping the Mirage 2000 N with the French Air Force and Dassault Rafale of Naval Aviation. The airborne nuclear warheads (TNA) that are carried on the new improved medium-range air-to-ground missiles (in French ASMPA) are now loaded underneath the Rafale aircraft. Each TNA has a yield of 300 kilotonnes.\n\nSpecifications: \n"}
{"id": "2547278", "url": "https://en.wikipedia.org/wiki?curid=2547278", "title": "Technology evangelist", "text": "Technology evangelist\n\nA technology evangelist is a person who builds a critical mass of support for a given technology, and then establishes it as a technical standard in a market that is subject to network effects. The word \"evangelism\" is taken from the context of religious evangelism due to the similarity of relaying information about a particular set of beliefs with the intention of converting the recipient. There is some element of this although some would argue it's more of showcasing the potential of a technology to lead someone to want to adopt it for themselves. \n\nPlatform evangelism is one target of technology evangelism, in which the vendor of a two-sided platform attempts to accelerate the production of complementary goods by independent developers (\"e.g.\", Facebook encourages developers to create games or develop mobile apps that can enhance users' experiences with Facebook.).\n\nProfessional technology evangelists are often employed by firms seeking to establish their technologies as \"de facto\" standards. Their work could also entail the training of personnel, including top managers so that they acquire skills and competencies necessary to adopt new technology or new technological initiative. There are even instances when technology evangelism becomes an aspect of a managerial position. \n\nOpen-source evangelists, on the other hand, operate independently. Evangelists also participate in defining open standards. Non-professional technology evangelists may act out of altruism or self-interest (\"e.g.\", to gain the benefits of early adoption or network effect).\n\nThe term \"software evangelist\" was coined by Mike Murray of Apple Computer's Macintosh computer division. It was part of Apple's drive to compete with IBM and it specifically described the initiative to win over third-party developers rhetorically to persuade them to develop software and applications for the Macintosh platform. The first so-identified technology evangelist was Mike Boich — who promoted the Macintosh computer. The job is often closely related to both sales and training but requires specific technology marketing skills. For example, convincing a potential buyer or user to change from older methods to new ones. There is also the case of adopting new products such as green IT. The marketing aspect involved in technology evangelism was strongly influenced by Geoffrey Moore and his books concerning the technology adoption lifecycle. One of his positions maintain that the role of the evangelist becomes critical when addressing what he identified as the \"chasm\" that exists between early and mainstream adoption. \n\nTechnology evangelism is sometimes associated with an internal employee assigned to encourage new practices within an organization. Methods of evangelism available include a modified STREET process (scope, track, rank, evaluate, evangelize, transfer) and the process that takes advantage of the hype cycle. Evangelism can also assume the form of a learning process and employ tools such as the Learning Management Systems (LMS). \n\nNotable technology evangelists in the commercial arena include Steve Jobs (Apple Inc.), Vint Cerf (Internet), Don Box, Guy Kawasaki, Alex St. John, Myriam Joire (Pebble), Mudasser Zaheer (Hewlett Packard Enterprise) and Dan Martin (MasterCard). Court records indicate that James Plamondon was a leading theorist, strategist, and practitioner of technology evangelism at Microsoft during its establishment of Microsoft Windows as the \"de facto\" standard PC operating system. Kawasaki, on the other hand, was credited for the remarkable growth of the software developed for the Macintosh, jumping from a few dozen products to more than 600 in less than a year of spreading the so-called Macintosh gospel. \n\n\n"}
{"id": "715886", "url": "https://en.wikipedia.org/wiki?curid=715886", "title": "Time-to-digital converter", "text": "Time-to-digital converter\n\nIn electronic instrumentation and signal processing, a time to digital converter (abbreviated TDC) is a device for recognizing events and providing a digital representation of the time they occurred. For example, a TDC might output the time of arrival for each incoming pulse. Some applications wish to measure the time interval between two events rather than some notion of an absolute time.\n\nIn electronics time-to-digital converters (TDCs) or time digitizers are devices commonly used to measure a time interval and convert it into digital (binary) output. In some cases interpolating TDCs are also called time counters (TCs).\n\nTDCs are used in many different applications, where the time interval between two signal pulses (start and stop pulse) should be determined. Measurement is started and stopped, when either the rising or the falling edge of a signal pulse crosses a set threshold. These requirements are fulfilled in many physical experiments, like time-of-flight and lifetime measurements in atomic and high energy physics, experiments that involve laser ranging and electronic research involving the testing of integrated circuits and high-speed data transfer.\n\nTDCs are used in applications where measurement events happen infrequently, such as high energy physics experiments, where the sheer number of data channels in most detectors ensures that each channel will be excited only infrequently by particles such as electrons, photons, and ions.\n\nIf the required time resolution is not high, then counters can be used to make the conversion.\n\nIn its simplest implementation, a TDC is simply a high-frequency counter that increments every clock cycle. The current contents of the counter represents the current time. When an event occurs, the counter's value is captured in an output register.\n\nIn that approach, the measurement is an integer number of clock cycles, so the measurement is quantized to a clock period. To get finer resolution, a faster clock is needed. The accuracy of the measurement depends upon the stability of the clock frequency.\n\nTypically a TDC uses a crystal oscillator reference frequency for good long term stability. High stability crystal oscillators are usually relative low frequency such as 10 MHz (or 100 ns resolution). To get better resolution, a phase-locked loop frequency multiplier can be used to generate a faster clock. One might, for example, multiply the crystal reference oscillator by 100 to get a clock rate of 1 GHz (1 ns resolution).\n\nHigh clock rates impose additional design constraints on the counter: if the clock period is short, it is difficult to update the count. Binary counters, for example, need a fast carry architecture because they essentially add one to the previous counter value. A solution is using a hybrid counter architecture. A Johnson counter, for example, is a fast non-binary counter. It can be used to count very quickly the low order count; a more conventional binary counter can be used to accumulate the high order count. The fast counter is sometime called a prescaler.\n\nThe speed of counters fabricated in CMOS-technology is limited by the capacitance between the gate and the channel and by the resistance of the channel and the signal traces. The product of both is the cut-off-frequency. Modern chip technology allows multiple metal layers and therefore coils with a large number of windings to be inserted into the chip.\nThis allows designers to peak the device for a specific frequency, which may lie above the cut-off-frequency of the original transistor.\n\nA peaked variant of the Johnson counter is the traveling-wave counter which also achieves sub-cycle resolution. Other methods to achieve sub-cycle resolution include analog-to-digital converters and vernier Johnson counters.\n\nIn most situations, the user does not want to just capture an arbitrary time that an event occurs, but wants to measure a time interval, the time between a start event and a stop event.\n\nThat can be done by measuring an arbitrary time both the start and stop events and subtracting. The measurement can be off by two counts.\n\nThe subtraction can be avoided if the counter is held at zero until the start event, counts during the interval, and then stops counting after the stop event.\n\nCoarse counters base on a reference clock with signals generated at a stable frequency formula_1. When the start signal is detected the counter starts counting clock signals and terminates counting after the stop signal is detected. The time interval formula_2 between start and stop is then\n\nwith formula_4, the number of counts and formula_5, the period of the reference clock.\n\nSince start, stop and clock signal are asynchronous, there is a uniform probability distribution of the start and stop signal-times between two subsequent clock pulses. This detuning of the start and stop signal from the clock pulses is called quantization error.\n\nFor a series of measurements on the same constant and asynchronous time interval one measures two different numbers of counted clock pulses formula_6 and formula_7 (see picture). These occur with probabilities\n\nwith formula_10 the fractional part of formula_11. The value for the time interval is then obtained by\n\nMeasuring a time interval using a coarse counter with the averaging method described above is relatively time consuming because of the many repetitions that are needed to determine the probabilities formula_13 and formula_14. In comparison to the other methods described later on, a coarse counter has a very limited resolution (1ns in case of a 1 GHz reference clock), but satisfies with its theoretically unlimited measuring range.\n\nIn contrast to the coarse counter in the previous section, fine measurement methods with much better accuracy but far smaller measuring range are presented here. Analogue methods like time interval stretching or double conversion as well as digital methods like tapped delay lines and the Vernier method are under examination. Though the analogue methods still obtain better accuracies, digital time interval measurement is often preferred due to its flexibility in integrated circuit technology and its robustness against external perturbations like temperature changes.\n\nThe counter implementation's accuracy is limited by the clock frequency. If time is measured by whole counts, then the resolution is limited to the clock period. For example, a 10 MHz clock has a resolution of 100 ns. To get resolution finer than a clock period, there are time interpolation circuits. These circuits measure the fraction of a clock period: that is, the time between a clock event and the event being measured. The interpolation circuits often require a significant amount of time to perform their function; consequently, the TDC needs a quiet interval before the next measurement.\n\nWhen counting is not feasible because the clock rate would be too high, analog methods can be used. Analog methods are often used to measure intervals that are between 10 and 200 ns. These methods often use a capacitor that is charged during the interval being measured. Initially, the capacitor is discharged to zero volts. When the start event occurs, the capacitor is charged with a constant current \"I\"; the constant current causes the voltage \"v\" on the capacitor to increase linearly with time. The rising voltage is called the fast ramp. When the stop event occurs, the charging current is stopped. The voltage on the capacitor \"v\" is directly proportional to the time interval \"T\" and can be measured with an analog-to-digital converter (ADC). The resolution of such a system is in the range of 1 to 10 ps.\n\nAlthough a separate ADC can be used, the ADC step is often integrated into the interpolator. A second constant current \"I\" is used to discharge the capacitor at a constant but much slower rate (the slow ramp). The slow ramp might be 1/1000 of the fast ramp. This discharge effectively \"stretches\" the time interval; it will take 1000 times as long for the capacitor to discharge to zero volts. The stretched interval can be measured with a counter. The measurement is similar to a dual-slope analog converter.\n\nThe dual-slope conversion can take a long time: a thousand or so clock ticks in the scheme described above. That limits how often a measurement can be made (dead time). Resolution of 1 ps with a 100 MHz (10 ns) clock requires a stretch ratio of 10,000 and implies a conversion time of 150 μs. To decrease the conversion time, the interpolator circuit can be used twice in a residual interpolator technique. The fast ramp is used initially as above to determine the time. The slow ramp is only at 1/100. The slow ramp will cross zero at some time during the clock period. When the ramp crosses zero, the fast ramp is turned on again to measure the crossing time (\"t\"). Consequently, the time can be determined to 1 part in 10,000.\n\nInterpolators are often used with a stable system clock. The start event is asynchronous, but the stop event is a following clock. For convenience, imagine that the fast ramp rises exactly 1 volt during a 100 ns clock period. Assume the start event occurs at 67.3 ns after a clock pulse; the fast ramp integrator is triggered and starts rising. The asynchronous start event is also routed through a synchronizer that takes at least two clock pulses. By the next clock pulse, the ramp has risen to .327 V. By the second clock pulse, the ramp has risen to 1.327 V and the synchronizer reports the start event has been seen. The fast ramp is stopped and the slow ramp starts. The synchronizer output can be used to capture system time from a counter. After 1327 clocks, the slow ramp returns to its starting point, and interpolator knows that the event occurred 132.7 ns before the synchronizer reported.\n\nThe interpolator is actually more involved because there are synchronizer issues and current switching is not instantaneous. Also, the interpolator must calibrate the height of the ramp to a clock period.\n\nThe vernier method is more involved. The method involves a triggerable oscillator and a coincidence circuit. At the event, the integer clock count is stored and the oscillator is started. The triggered oscillator has a slightly different frequency than the clock oscillator. For sake of argument, say the triggered oscillator has a period that is 1 ns faster than the clock. If the event happened 67 ns after the last clock, then the triggered oscillator transition will slide by −1 ns after each subsequent clock pulse. The triggered oscillator will be at 66 ns after the next clock, at 65 ns after the second clock, and so forth. A coincidence detector looks for when the triggered oscillator and the clock transition at the same time, and that indicates the fraction time that needs to be added.\n\nThe interpolator design is more involved. The triggerable clock must be calibrated to clock. It must also start quickly and cleanly.\n\nThe Vernier method is a digital version of the time stretching method. Two only slightly detuned oscillators (with frequencies formula_15 and formula_16) start their signals with the arrival of the start and the stop signal. As soon as the leading edges of the oscillator signals coincide the measurement ends and the number of periods of the oscillators (formula_6 and formula_7 respectively) lead to the original time interval formula_2:\n\nSince highly reliable oscillators with stable and accurate frequency are still quite a challenge one also realizes the vernier method via two tapped delay lines using two slightly different cell delay times formula_21. This setting is called differential delay line or vernier delay line.\n\nIn the example presented here the first delay line affiliated with the start signal contains cells of D-flip-flops with delay formula_22 which are initially set to transparent. During the transition of the start signal through one of those cells, the signal is delayed by formula_22 and the state of the flip-flop is sampled as transparent. The second delay line belonging to the stop signal is composed of a series of non-inverting buffers with delay formula_24. Propagating through its channel the stop signal latches the flip-flops of the start signal's delay line. As soon as the stop signal passes the start signal, the latter is stopped and all leftover flip-flops are sampled opaque. Analogous to the above case of the oscillators the wanted time interval formula_2 is then\n\nwith n the number of cells marked as transparent.\n\nIn general a tapped delay line contains a number of cells with well defined delay times formula_21. Propagating through this line the start signal is delayed. The state of the line is sampled at the time of the arrival of the stop signal.\nThis can be realized for example with a line of D-flip-flop cells with a delay time formula_21. The start signal propagates through this line of transparent flip-flops and is delayed by a certain number of them. The output of each flip-flop is sampled on the fly. The stop signal latches all flip-flops while propagating through its channel undelayed and the start signal cannot propagate further. Now the time interval between start and stop signal is proportional to the number of flip-flops that were sampled as transparent.\n\nCounters can measure long intervals but have limited resolution. Interpolators have high resolution but they cannot measure long intervals. A hybrid approach can achieve both long intervals and high resolution. The long interval can be measured with a counter. The counter information is supplemented with two time interpolators: one interpolator measures the (short) interval between the start event and a following clock event, and the second interpolator measure the interval between the stop event and a following clock event. The basic idea has some complications: the start and stop events are asynchronous, and one or both might happen close to a clock pulse. The counter and interpolators must agree on matching the start and end clock events. To accomplish that goal, synchronizers are used.\n\nThe common hybrid approach is the Nutt method. In this example the fine measurement circuit measures the time between start and stop pulse and the respective second nearest clock pulse of the coarse counter (\"T\", \"T\"), detected by the synchronizer (see figure). Thus the wanted time interval is\n\nwith \"n\" the number of counter clock pulses and \"T\" the period of the coarse counter.\n\nTime measurement has played a crucial role in the understanding of nature from the earliest times. Starting with sun, sand or water driven clocks we are able to use clocks today, based on the most precise caesium resonators.\n\nThe first direct predecessor of a TDC was invented in the year 1942 by Bruno Rossi for the measurement of muon lifetimes. It was designed as a time-to-amplitude-converter, constantly charging a capacitor during the measured time interval. The corresponding voltage is directly proportional to the time interval under examination.\n\nWhile the basic concepts (like Vernier methods (Pierre Vernier 1584-1638) and time stretching) of dividing time into measurable intervals are still up-to-date, the implementation changed a lot during the past 50 years. Starting with vacuum tubes and ferrite pot-core transformers those ideas are implemented in complementary metal-oxide-semiconductor (CMOS) design today.\n\nRegarding even the fine measuring methods presented, there are still errors one may wish remove or at least to consider. Non-linearities of the time-to-digital conversion for example can be identified by taking a large number of measurements of a poissonian distributed source (statistical code density test). Small deviations from the uniform distribution reveal the non-linearities.\nInconveniently the statistical code density method is quite sensitive to external temperature changes. Thus stabilizing delay or phase-locked loop (DLL or PLL) circuits are recommended.\n\nIn a similar way, offset errors (non-zero readouts at \"T\" = 0) can be removed.\n\nFor long time intervals, the error due to instabilities in the reference clock (jitter) plays a major role. Thus clocks of superior quality are needed for such TDCs.\n\nFurthermore, external noise sources can be eliminated in postprocessing by robust estimation methods.\n\nTDCs are currently built as stand-alone measuring devices in physical experiments or as system components like PCI cards. They can be made up of either discrete or integrated circuits.\n\nCircuit design changes with the purpose of the TDC, which can either be a very good solution for single-shot TDCs with long dead times or some trade-off between dead-time and resolution for multi-shot TDCs.\n\nThe time-to-digital converter measures the time between a start event and a stop event. There is also a digital-to-time converter or delay generator. The delay generator converts a number to a time delay. When the delay generator gets a start pulse at its input, then it outputs a stop pulse after the specified delay. The architectures for TDC and delay generators are similar. Both use counters for long, stable, delays. Both must consider the problem of clock quantization errors.\n\nFor example, the Tektronix 7D11 Digital Delay uses a counter architecture. A digital delay may be set from 100 ns to 1 s in 100 ns increments. An analog circuit provides an additional fine delay of 0 to 100 ns. A 5 MHz reference clock drives a phase-locked loop to produce a stable 500 MHz clock. It is this fast clock that is gated by the (fine-delayed) start event and determines the main quantization error. The fast clock is divided down to 10 MHz and fed to main counter. The instrument quantization error depends primarily on the 500 MHz clock (2 ns steps), but other errors also enter; the instrument is specified to have 2.2 ns of jitter. The recycle time is 575 ns.\n\nJust as a TDC may use interpolation to get finer than one clock period resolution, a delay generator may use similar techniques. The Hewlett-Packard 5359A High Resolution Time Synthesizer provides delays of 0 to 160 ms, has an accuracy of 1 ns, and achieves a typical jitter of 100 ps. The design uses a triggered phase-locked oscillator that runs at 200 MHz. Interpolation is done with a ramp, an 8-bit digital-to-analog converter, and a comparator. The resolution is about 45 ps.\nWhen the start pulse is received, then counts down and outputs a stop pulse. For low jitter the synchronous counter has to feed a zero flag from the most significant bit down to the least significant bit and then combine it with the output from the Johnson counter.\n\nA digital-to-analog converter (DAC) could be used to achieve sub-cycle resolution, but it is easier to either use vernier Johnson counters or traveling-wave Johnson counters.\n\nThe delay generator can be used for pulse width modulation, e.g. to drive a MOSFET to load a Pockels cell within 8 ns with a specific charge.\n\nThe output of a delay generator can gate a digital-to-analog converter and so pulses of a variable height can be generated. This allows matching to low levels needed by analog electronics, higher levels for ECL and even higher levels for TTL. If a series of DACs is gated in sequence, variable pulse shapes can be generated to account for any transfer function.\n\n\n"}
{"id": "15685361", "url": "https://en.wikipedia.org/wiki?curid=15685361", "title": "Treaty battleship", "text": "Treaty battleship\n\nA treaty battleship was a battleship built in the 1920s or 1930s under the terms of one of a number of international treaties governing warship construction. Many of these ships played an active role in the Second World War, but few survived long after it.\n\nIn the Washington Naval Treaty of 1922, the world's five naval powers agreed to abide by strict restrictions on the construction of battleships and battlecruisers, in order to prevent an arms race in naval construction such as preceded the Great War. The Treaty limited the number of capital ships possessed by each signatory, and also the total tonnage of each navy's battleships. New ships could only be constructed to replace the surviving ships as they retired after 20 years' service. Furthermore, any new ship would be limited to guns of 16-inch caliber and a displacement of 35,000 tons.\n\nThe Washington Treaty limits were extended and modified by the London Naval Treaty of 1930 and the Second London Naval Treaty of 1936. During the 1930s, however, the effectiveness of these agreements broke down, as some signatory powers (in particular Japan) withdrew from the treaty arrangements and others only paid lip service to them. By 1938, Britain and the USA had both invoked an 'escalator clause' in the Second London Treaty which allowed battleships of up to 45,000 tons displacement, and the Treaty was effectively defunct.\n\nThe strict limits on displacement forced the designers of battleships to make compromises which they might have wished to avoid given the choice. The 1920s and 1930s saw a number of innovations in battleship design, particularly in engines, underwater protection, and aircraft.\n\nThe Washington Naval Treaty was signed in 1922 by Britain, France, the United States, Japan and Italy, as the five powers which had any significant numbers of modern dreadnought battleships and battlecruisers. The Treaty was aimed at preventing an expensive arms race, principally between Britain, the USA and Japan. The Treaty established a definition of a capital ship, which was any ship with a displacement of 10,000 tons or more, or with guns above 8 in (203 mm) calibre, apart from an aircraft carrier. Carriers were specifically constrained from having guns above 8 in calibre, in order to prevent confusion between the two types. Each signatory agreed to limit its total number of capital ships, and its total tonnage of capital ships. These measures meant that several classes of battleships and battlecruisers which were planned, or had even been begun, were scrapped or cancelled.\n\nIn addition, each new ship was limited in size to 35,000 tons displacement, and to guns of calibre. Only one vessel already finished, the British battlecruiser , exceeded these limits; however, many of the new ships planned or being built were significantly larger. The Treaty permitted the improvement of existing warships, but limited the resulting increase in displacement at 3,000 tons.\n\nThe Treaty also introduced a 'building holiday'. In general, a new ship could only be begun if one of the ships allowed by the Treaty had been in service for 20 years. This meant that for most signatory powers, no new ships could be begun until the 1930s. An exception was made for Britain; the Royal Navy had no existing ships with 16-inch guns, while both the U.S. Navy and Japan had 16-inch ships already in commission which would be allowed to continue under the terms of the Treaty.\n\nThese first British treaty battleships became the , which were begun in 1922 and launched in 1925. The \"Nelson\" class solved the problem posed by the new weight restriction by placing all the heavy guns forward of the superstructure in three triple turrets, hence saving weight on the armour around them.\n\nThese limits were reiterated by the London Naval Treaty of 1930, and the Second London Naval Treaty of 1936 further limited guns to 14-inch calibre. The Second London Treaty contained a clause which allowed construction of battleships with 16-inch guns if any of the signatories of the Washington Treaty failed to ratify the new one. It contained an additional clause which allowed displacement restrictions to be relaxed if non-signatories built vessels more powerful than the treaty allowed.\n\nVirtually all battleships built subsequently obeyed the treaty limits. The Washington Naval Treaty was signed by the USA, UK, Japan, France and Italy - all the principal naval powers. At various stages Italy and France opted out of further negotiations; however, their economic resources did not permit the development of super-battleships. Germany, while not permitted any battleships by the Treaty of Versailles, developed one in the 1930s; this was legitimised by the Anglo-German Naval Agreement, which placed Germany under the same legal limits as Britain. Only Japan, which opted out of the Treaty system in 1934, actually built mammoth treaty-busting battleships - the . The collapse of the treaty system led to the wartime construction of \"post-treaty\" battleships which exceeded the limitations: the German , the U.S. and the British and (never completed) . A number of designs, never finished, shattered the treaty limits; the German H-class were scrapped on the outbreak of war, while the U.S. were canceled before being laid down.\n\nTreaty battleships were technically superior to their predecessors. Naval technology developed in the 1920s and 1930s provided improved steel, better guns, more efficient engines, and more effective protection against torpedoes. The displacement limit also encouraged naval designers to think creatively about minimising displacement, meaning that the treaty battleships significantly increased their performance.\n\n\n"}
{"id": "8414698", "url": "https://en.wikipedia.org/wiki?curid=8414698", "title": "VIA pc-1 Initiative", "text": "VIA pc-1 Initiative\n\nThe VIA pc-1 Initiative is a project of VIA Technologies, established in January 2005, to help bridge the digital divide by developing information and communication technology (ICT) systems to benefit those who currently do not have computers or Internet access.\n\nThe project sought to increase individual PC and Internet access for those who could afford it, and community access for those who couldn't, via community access deployments in schools, offices, libraries, Internet cafes, village kiosks, and telecenters. The project also sought to enable PC and Internet access in remote regions. \nIn January 2006 a development center was opened in Mumbai.\n\nThe system platform architecture includes processors built by IBM using a 90-nanometer silicon on insulator (SOI) process with a VIA digital media chipset. \n\nVIA provides reference designs, but vendors use different brand names. Examples include iDot pc-1 systems in Taiwan, Longmeng pc-1 systems in China, Geniac pc-1 systems in Nigeria, Sico pc-1 systems in Egypt, and Alaska pc-1 systems in Mexico.\n\nThe pc1000 and pc1500 platforms were described in 2006, using the VIA C3 processors. The pc3500 was introduced in August 2007 using the VIA C7.\nAs a part of the pc-1 Initiative, VIA became involved in digital inclusion projects. These include working alongside the Geekcorps in Mali to design a computer for hot dusty conditions, as well as donating low-powered computers for use in Mali desert radio stations.\n\nVIA worked with the Asia-Pacific Economic Cooperation (APEC) Digital Opportunity Center (ADOC) to bridge the digital divide in emerging markets within Pacific Rim countries. One project includes donating 20 systems for use in the VIA pc-1 ICT Center at the Thai Nguyen University of Agriculture and Forestry (TUAF) in the Northern Mountainous Area (NMA) of Vietnam. VIA worked with ADOC and the Institute for Information Industry (Taiwan) on an affordable computing advocacy project.\n\nVIA works with the Samoan Ministry of ICT to establish Samoa's first solar powered information center, arranging for solar cells and computers to be provided.\n\nIn South Africa, VIA worked with Ikamvayouth in several projects - most recently providing computers and thin clients for a tuXlab IT center at the Nazeema Isaacs Library in Khayelitsha.\n\n\n"}
{"id": "3033116", "url": "https://en.wikipedia.org/wiki?curid=3033116", "title": "Victor Glushkov", "text": "Victor Glushkov\n\nVictor Mikhailovich Glushkov (; August 24, 1923 – January 30, 1982) was a Soviet mathematician, the founding father of information technology in the Soviet Union, and one of the founders of Cybernetics.\nHe was born in Rostov-on-Don, Russian SFSR, in the family of a mining engineer. Glushkov graduated from Rostov State University in 1948, and in 1952 proposed solutions to Hilbert's fifth problem and defended his thesis in Moscow State University.\n\nIn 1956 he began working with computers and worked in Kiev as a Director of the Computational Center of the Academy of Science of Ukraine. In 1958 he became a member of the Communist Party.\n\nHe made contributions to the theory of automata. He and his followers (Kapitonova, Letichevskiy and other) successfully applied that theory to enhance construction of computers. His book on that topic \"Synthesis of Digital Automata\" became well known. For that work, he was awarded the Lenin Prize in 1964 and elected as a Member of the Academy of Science of USSR.\n\nHe greatly influenced many other fields of theoretical computer science (including the theory of programming and artificial intelligence) as well as its applications in the USSR. He published nearly 800 printed works.\n\nOne of his great practical goals was the creation of a National Automatized System of Administration of Economy (OGAS), which included the establishment of a network of computers to manage the allocation of resources and information among organizations in the national economy, which would represent a higher form of socialist planning than the extant Stalinist command economy. This ambitious project was ahead of its time, first being proposed and modeled in 1962. It received opposition from many senior Communist Party leaders who felt the system threatened Party control of the economy. By the early 1970s official interest in this system ended.\n\nGlushkov founded a Kiev-based Chair of Theoretical Cybernetics and Methods of Optimal Control at the Moscow Institute of Physics and Technology in 1967 and a Chair of Theoretical Cybernetics at Kiev State University in 1969.\nThe Institute of Cybernetics of National Academy of Science of Ukraine, which he created, is named after him.\n\n\n\n"}
{"id": "27070776", "url": "https://en.wikipedia.org/wiki?curid=27070776", "title": "Welsh dresser", "text": "Welsh dresser\n\nA Welsh dresser (British English) or a china hutch (American English), sometimes known as a kitchen dresser or pewter cupboard, is a piece of wooden furniture consisting of drawers and cupboards in the lower part, with shelves and perhaps a sideboard on top. Traditionally, it is a utilitarian piece of furniture used to store and display crockery, silverware and pewter-ware, but is also used to display general ornaments.\nOriginally, a dresser was located in the kitchen and was a utilitarian piece of furniture where meat and other food was dressed or prepared, while prepared food was placed on sideboards in the dining room ready to be served. They could be modified to suit local needs; for example, dressers in the Scottish Highlands may have a \"porridge drawer\"—a tin lined drawer into which freshly made porridge was emptied and left to cool. When cold, slices of the porridge could be cut out and taken out of the house for later consumption. Gradually the purely utilitarian function of the dresser was supplemented with other functions, such as a means of displaying the best crockery in a farmhouse. Once it became a means of display the dresser could also be found in dining rooms where it served as sideboard and a place to store and display dinner ware. In the 19th century various different styles of ceramics would evolve to fill the plate racks of the Welsh dressers of Wales and to meet the needs of the Welsh market. Furthermore, many local traditions of what constitutes the proper care and display of the items on a Welsh dresser would come to assume an important role in the culture of North Wales in particular.\n\n"}
