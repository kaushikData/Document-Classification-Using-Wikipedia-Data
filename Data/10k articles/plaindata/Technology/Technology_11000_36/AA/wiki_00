{"id": "9318591", "url": "https://en.wikipedia.org/wiki?curid=9318591", "title": "AOR (company)", "text": "AOR (company)\n\nAOR, Ltd. (\"Authority on Radio Communications, Ltd.\") is a Japanese based manufacturer of radio equipment, including transceivers, scanners, antennas and frequency monitors.\n\nEstablished in 1977 when two radio amateurs decided to go professional. Based in Tokyo, Japan, they also have offices in the United Kingdom and the United States, and manufacturing facilities in Japan and the United Kingdom.\n\n"}
{"id": "22142183", "url": "https://en.wikipedia.org/wiki?curid=22142183", "title": "Access mat", "text": "Access mat\n\nAn access mat is a portable platform used to support equipment used in construction and other resource-based activities, including drilling rigs, camps, tanks, and helipads. It may also be used as a structural roadway to provide passage over unstable ground, pipelines and more. \n\nDepending on its application, an access mat may be called a rig mat, swamp mat, industrial mat, ground protection mat, road mat, construction mat, mud mat, mobile mat, safety mat, or portable roadway. Because there is no body governing or standardizing terminology (nor design and construction), and terminology inconsistencies are compounded by regional and industry-specific vernacular, the types described below should be considered a general guide to access matting.\nThere are three general categories that describe access matting:\nConstruction mats are used to provide relatively clean, smooth, all-weather working, walking and driving surfaces in industrial or commercial construction settings where access would not otherwise be guaranteed. This category of mat reduces crew downtime, increasing the likelihood of timely task completion. \n\nConstruction mats have multiple applications, including:\n\nA subset of construction mat, pipeline mats, may be considered construction mats, though they often begin as a different type of access mat. Pipeline mats are typically mats at the end of their productive lives, used as a rough, one-time access setting.\nAccess mats are a category of matting that are generally used to provide temporary roads and worksites. Access mats are often used to access work sites in remote or environmentally sensitive areas, such as bogs, wetlands or fens. For that reason, they are often referred to as swamp, bog or wetland mats.\n\nSwamp mats are based on a design developed by Joe Penland in the late 20th century and consist of three layers of 2’ x 8’ lumber laminated together with steel bolts. Most commonly, the top and bottom layer are made up of 11 pieces and the middle layer, placed cross wise, is made from 21 pieces. In the USA, the majority of swamp mats are made from mixed hardwoods, although they are often referred to as oak. It is also usual to find hardwood mats in Canada, however the availability of durable coniferous species such as various firs, pines, and spruces make their use a more economical prospect. Common dimensions are 8' x 14' and 8' x 16'. Thicknesses vary between suppliers from 4.5” to a full 6”. Swamp mats are produced by many small and medium-sized manufacturers, and quality varies dramatically within the industry.\n\nRig mats, another variety of access mats, may also known as wood and steel mats or steel frame mats. These mats are commonly made of spruce, pine, fir or a combination thereof encased in a steel frame, though some suppliers also offer bamboo and fibreglass options. The frame is normally I Beam or HST steel. The steel is used to strengthen the mats, enabling the manufacturers of the mats to build them in larger sizes and to support more weight compared to all other types of mats. Common sizes are 8' x 20', 8' x 30', and 8' x 40'. One great advantage is the ease of repairing the wooden inserts which gives new life to an already long lasting and durable mat. This method of repair can be completed on both I Beam and HST style mats.\n\nMud mats are a combination of a reinforced member (such as metal bars or bamboo) confining geosynthetic fabric in a portable mat, that can be rolled up for ease of transport and deployment. A lightweight, light-duty flexible mat suitable for distributing loads over firm ground to avoid rutting. They are not commonly used for access matting in soft ground conditions.\n\nThe final category of matting is known as Heavy-Equipment matting. These mats are constructed from the most durable, load-bearing materials, designed to be transited by heavy equipment.\n\nOne variety of heavy-equipment mat is the Crane mat. Designed for exceptionally heavy use, Crane mats (also known as digging mats, logging mats, or bridge mats) can be used in a wide variety of applications, including:\n\n\nCrane mats are constructed of solid 8”, 10” or 12” timbers and are affixed by steel bolts, providing ground stabilization under extreme weight. The timber species used in these mats is generally Douglas fir and Hemlock as this species of wood has superior strength, durability and resiliency characteristics compared to other western softwood. The construction process of these mats allows for versatility as different size, length and quantity of timbers can be used to make different dimension mats. Most crane mat manufacturers are in the Western states and provinces. \nRepurposing a Crane mat with cable loops allows them to be relocated on site by client-owned equipment, allowing a client to minimize the number of mats they require. This repurposing transforms a Crane mat into a Digging mat.\n\nLogging mats are Crane mats with a reinforced slot which allows knuckle boom loaders or skid-steer loaders to move the mats with ease.\nOilfield Mat Combos are heavy-duty steel frame mats designed to provide spill containment and platform matting.\n\nDepending on purpose, mats may be constructed of any of the following materials:\n\nWood is the most commonly used matting material used. Wooden mats range widely in cost, depending on the type of wood used, and may be constructed of:\n\ncomposite mats are constructed of multiple materials, to improve the strength or durability of the mat. They can be more expensive upfront, but as they have a much longer lifespan than wooden mats they can be more cost-efficient. Higher quality versions of the composite mat will include anti-static and/or UV protection additives to prevent the formation of sparks from static electricity and to prevent cracking, physical breakdown & fading of the mat. Composite mats feature a variety of connection mechanisms, from complicated systems that use small parts and specialized tools to large aluminum cam lock systems. \n\nComposite mats range in size from 4’ x 4’ to 8’ x 14’. It is commonly thought that bigger is better with access mats, but it is important that the mats can be shipped by standard means. 7.5’ x 14’ composite mats, for example, will fit into an ISO container.\n\nExamples of composite materials used in access matting include:\n\nFibreglass offers high strength, long-term durability and is light weight.\n\nRubber mats evolved by industries seeking sustainable products made from recycled materials. The rubber crumb recycled from scrap tires is a primary petroleum based material.\n\nDepending on the site soil conditions, there are various thicknesses of mats that can be manufactured. The base material of the rubber mat is crumb rubber, urethane, and fibre from recycled motor vehicle tires. The production of one typical mat uses up to 350 tires which makes the product environmentally friendly. The mats are moulded into conventional 8’x14’ sections and the body has an embedded patented rigid spine which makes each mat extremely durable and virtually indestructible. The surface is textured and designed to provide excellent traction for all types of traffic and have been proven to be a viable and economical solution for long term use under some of the most unruly site conditions and usages across North America. They are effective when used as access roads, heli-pads, laydown areas, wash pads or sidewalks.\n\nRubber mats are also known as blast mats in the mining industry.\n\nEngineered, hollow rig matting systems may be made up of High-Density Polyethylene (HDPE). When compared to traditional wooden matting, composite mats are lighter in weight yet still can handle heavy loads. \n\nThis solid plastic, UHMWPE (Ultra-High Molecular Weight Polyethylene), offers the highest impact strength while being highly resistant to corrosive chemicals.\n\nSolid, one-piece compression moulded mats are made from recycled or virgin (HDPE) as well as recycled or virgin Ultra High Molecular Weight Polyethylene (UHMWPE). \n\nSolid mats are lighter in weight than hollow mats but due to the simpler connection system provide the same or similar usable working surface area per mat once connected (10’ x 8’, 13.5’ x 6.5’). Typically this means that a significantly greater number of solid mats can be loaded onto a truck resulting in a number of key benefits including larger working surface area per truckload, reduced number of transport trips per project, reduced fuel consumption and greenhouse gas emissions per project, and reduced transport costs per project.\n\nA simple connection system, using standard connectors and tools, is used which means that mats can easily be installed and connected on undulating as well as flat surfaces, avoiding the need to prepare the ground surface in advance of installation. This also results in project time and cost-savings.\nUnlike hollow mats, solid mats cannot be punctured and therefore do not take on water (which can increase mat weight) or other fluids (such as fuel or chemicals) that could have an adverse environmental impact on sensitive sites.\n\nAccess matting has a variety of industrial and commercial uses, ranging from temporary, one-time use (for example, in the construction of pipeline access, where the mats are essentially destroyed in the process), reused over multiple projects over multiple seasons, or semi-permanent.\n\nAccess mats may also be used in other, non-traditional settings, such as providing access for cattle to water troughs where muddy conditions may prove detrimental to the livestock; for home owners who need access to buildings under construction before driveways are poured; to create temporary parking; or to provide nature enthusiasts with a low-impact, environmental trailway.\n\nIn some jurisdictions, access mats must be removed when they are no longer needed due to climactic conditions. Most access mat providers contract to remove used mats, which may then be re-rented, stored, or destroyed, depending on condition. Destruction of mats includes chipping/mulching, chipping and burying in approved locations, or chipping and incinerating.\n\n"}
{"id": "479124", "url": "https://en.wikipedia.org/wiki?curid=479124", "title": "Aerial Experiment Association", "text": "Aerial Experiment Association\n\nThe Aerial Experiment Association (AEA) was a Canadian-American aeronautical research group formed on 30 September 1907, under the leadership of Dr. Alexander Graham Bell.\n\nThe AEA produced several different aircraft in quick succession, with each member acting as principal designer for at least one. The group introduced key technical innovations, notably wingtip ailerons and the tricycle landing gear.\n\nAccording to Bell, the AEA was a \"co-operative scientific association, not for gain but for the love of the art and doing what we can to help one another.\" Although the association had no significant commercial impact, one of its members, Glenn Curtiss, later established the large and successful aeronautical manufacturing company Curtiss Aeroplane and Motor Company. The AEA was disbanded on 31 March 1909.\n\nThe AEA came into being when John Alexander Douglas McCurdy and his friend Frederick W. \"Casey\" Baldwin, two recent engineering graduates of the University of Toronto, decided to spend the summer in Baddeck, Nova Scotia. McCurdy had grown up there, and his father was the personal secretary of Dr. Bell. He had grown up close to the Bell family and was well received in their home. One day, as the three sat with Dr. Bell discussing the problems of aviation, Mabel Bell, Alexander's wife, suggested they create a formal research group to exploit their collective ideas. Being independently wealthy, she provided a total of US$35,000 (equivalent to $ in ) to finance the Association, with $20,000 made available immediately by the sale of property.\n\nCurtiss, the American motorcycle designer and manufacturer and a recognized expert on gasoline engines, was recruited as a member of the association, and his associate Augustus Post assisted as representative from the Aero Club of America. Curtiss had visited the Wright brothers to discuss aeronautical engineering and offered them use of a 50 hp engine. Wilbur cordially declined, saying that a motor of their own development met their power needs, unaware that the AEA was about to become a serious competitor in powered flight. Bell wrote to U.S. President Theodore Roosevelt to have an interested young officer who had volunteered his help, US Army Lieutenant Thomas Selfridge, officially detailed to Baddeck. Selfridge was assigned to the Aeronautical Division, U.S. Signal Corps on 3 August 1907, two days after its formation, and was sent to Nova Scotia. A year later, on 17 September 1908, while riding as a passenger with Orville Wright on a demonstration flight for the U.S. Army, he became the first person killed in an aircraft accident.\n\nIn 1898, Bell experimented with tetrahedral kites and wings constructed of multiple compound tetrahedral kites covered in maroon silk. The tetrahedral wings were named \"Cygnet\" I, II and III, and were flown both unmanned and manned (\"Cygnet I\" crashed during a flight carrying Selfridge) in the period from 1907–1912. Some of Bell's kites are on display at the Alexander Graham Bell National Historic Site.\n\nThe AEA's work progressed to heavier-than-air machines, applying their knowledge of kites to gliders. The AEA collaboration led to very public success. Casey Baldwin became the first Canadian and first British subject pilot on 12 March 1908 flight of \"Red Wing\". \n\nIts successor, \"White Wing\", also of 1908, was the first airplane to have Bell's ailerons. The following design, the \"June Bug\", also of 1908 and piloted by Curtiss, won the Scientific American Trophy by making the first official one kilometer flight in North America, although, the Wrights had already accomplished this in 1904.\n\nTheir fourth flying machine, the \"Silver Dart\", also constructed in 1908, made the first controlled powered flight in Canada on 23 February 1909 when it was flown off the ice of Bras d'Or Lake near Baddeck by McCurdy, who had been one of its designers. On 10 March 1909, McCurdy set a record when he flew the \"Silver Dart\" on a circular course over a distance of more than , a feat that the Wrights had already accomplished in 1905. The Association made the first passenger flight in Canada on 2 August, also in the \"Silver Dart\".\n\nMuch development also took place in Hammondsport, New York, where in 1908 pioneering experimentation was done on seaplane carried out by Curtiss. In France Henri Fabre successfully flew the first powered seaplane in history, the Fabre Hydravion, in March 1910.\n\nBell's organization was established with a fixed term mandate, which was extended to March 31, 1909, by joint agreement of all its members, with Mrs. Bell contributing an additional $10,000 of financing. After Lt. Selfridge's death in September 1908, McCurdy became the organization's secretary and Dr. Bell's cousin, Charles J. Bell, became the Association's legal trustee.\n\nAfter Curtiss unexpectedly formed a new commercial aircraft building venture, the Herring-Curtiss Company, in March 1909, strained relations arose between Curtiss and the Association's other members. A request for him to attend the association's meeting and resolve the issue went unanswered, and the organization's mandate was not extended a second time. The AEA was disbanded on 31 March 1909.\n\n\n\n\n"}
{"id": "24376013", "url": "https://en.wikipedia.org/wiki?curid=24376013", "title": "BCT Network", "text": "BCT Network\n\nThe Army will continue the development and fielding of an incremental ground tactical network capability to all Army brigade combat teams. This network is a layered system of interconnected computers and software, radios, and sensors within the Brigade Combat Team (BCT). The BCT network is essential to enable Unified Battle Command and will be delivered to the Army's Brigade Combat Teams in increasing capability increments. The first increment is currently finishing SDD developmental and operational testing and will be delivered to Infantry Brigade Combat Teams in the form of Network Integration Kits (B-kits) with E-IBCT.\n\nThe soldier at every echelon, from Brigade to Squad, will be connected to the proper sensor data and communication relays to ensure proper battlespace situational awareness.\n\nThe Network Integration Kit (NIK) is a suite of equipment capable of being installed on many vehicles including HMMWV's and MRAP's. It provides the Network connectivity and battle command software to integrate and fuse sensor data into the common operational picture (COP) displayed on the Force XXI Battle Command Battalion/Brigade and Below (FBCB2). The Network Integration Kit consists of an integrated computer system (ICS) that hosts the Battle Command software and the Systems of Systems Common Operating Environment (SOSCOE) software, along with the JTRS GMR radio to provide the interface to the sensors and unmanned systems, as well as voice and data communications with other vehicles and soldiers.\n\nSoldiers will be able to communicate with the Battalion Tactical Operation Center (TOC), by sending reports on enemy sighting, activity and location utilizing the NIK via the Network allowing for split-time tactical decisions.\n\n\"This article incorporates work from http://www.bctmod.army.mil/systems/network/index.html, which is in the public domain as it is a work of the United States Army.\"\n"}
{"id": "47043963", "url": "https://en.wikipedia.org/wiki?curid=47043963", "title": "BigCommerce", "text": "BigCommerce\n\nBigCommerce is a privately held technology company that develops e-commerce software for businesses. The company was founded in 2009 and has 456 employees with headquarters in Austin, Texas and additional offices in San Francisco, California and Sydney, Australia. \n\nThe company reports that $16 billion in total sales have been processed by the BigCommerce platform.\n\nBigCommerce was founded in 2009 by Australians Eddie Machaalani and Mitchell Harper following a chance meeting in an online chatroom in 2003. In August 2009, the two relaunched a hosted version of Interspire Shopping Cart called “BigCommerce” and opened its first U.S. office.\n\nBigCommerce was 100% bootstrapped until July 31, 2011, when it closed $15 million in Series A funding from General Catalyst Partners. At the time, the company announced its client count had grown 680% year over year.\n\nIn January 2012, BigCommerce launched a $2 million integration fund for developers, which was used to fund 31 applications in the BigCommerce App Marketplace. The company subsequently received $20 million in Series B financing in September 2012, led by General Catalyst Partners and Floodgate Fund.\n\nIn July 2013, BigCommerce raised another $40 million in Series C financing from Revolution Growth, an investment firm led by former AOL Chief Executive Steve Case. In November 2014, the company announced $50 million in series D financing from investors SoftBank Capital, Telstra Ventures, and American Express, bringing the company’s total funding to $125 million.\n\nIn April 2015, BigCommerce made its first acquisition, buying Zing, a Texas-based company that sold checkout software to retailers.\n\nIn May 2015, the company launched a new version of its platform, BigCommerce Enterprise, for high-volume retailers.\n\nIn June 2015, the company announced a new CEO. Brent Bellm, former HomeAway president and PayPal Europe CEO, succeeded BigCommerce co-founder Eddie Machaalani, who remains with the company as executive chairman of the board. \n\nIn October 2017, BigCommerce began offering in-app shopping through Instagram to U.S.-based merchants using their platform. The offering was part of a gradual roll out of Instagram's in-app shopping feature that began in November 2016.\n\nIn April 2018, BigCommerce completed a $64 million round of funding led by Goldman Sachs.\n\n"}
{"id": "5239446", "url": "https://en.wikipedia.org/wiki?curid=5239446", "title": "Blowout preventer", "text": "Blowout preventer\n\nA blowout preventer (BOP) is a large, specialized valve or similar mechanical device, used to seal, control and monitor oil and gas wells to prevent blowouts, the uncontrolled release of crude oil and/or natural gas from a well. They are usually installed in stacks of other valves.\n\nBlowout preventers were developed to cope with extreme erratic pressures and uncontrolled flow (formation kick) emanating from a well reservoir during drilling. Kicks can lead to a potentially catastrophic event known as a blowout. In addition to controlling the downhole (occurring in the drilled hole) pressure and the flow of oil and gas, blowout preventers are intended to prevent tubing (e.g. drill pipe and well casing), tools and drilling fluid from being blown out of the wellbore (also known as bore hole, the hole leading to the reservoir) when a blowout threatens. Blowout preventers are critical to the safety of crew, rig (the equipment system used to drill a wellbore) and environment, and to the monitoring and maintenance of well integrity; thus blowout preventers are intended to provide fail-safety to the systems that include them.\n\nThe term BOP (pronounced B-O-P, not \"bop\") is used in oilfield vernacular to refer to blowout preventers. The abbreviated term preventer, usually prefaced by a type (e.g. ram preventer), is used to refer to a single blowout preventer unit. A blowout preventer may also simply be referred to by its type (e.g. ram).\n\nThe terms blowout preventer, blowout preventer stack and blowout preventer system are commonly used interchangeably and in a general manner to describe an assembly of several stacked blowout preventers of varying type and function, as well as auxiliary components. A typical subsea deepwater blowout preventer system includes components such as electrical and hydraulic lines, control pods, hydraulic accumulators, test valve, kill and choke lines and valves, riser joint, hydraulic connectors, and a support frame.\n\nTwo categories of blowout preventer are most prevalent: \"ram\" and \"annular\". BOP stacks frequently utilize both types, typically with at least one annular BOP stacked above several ram BOPs.\n\nBlowout preventers are used on land wells, offshore rigs, and subsea wells. Land and subsea BOPs are secured to the top of the wellbore, known as the wellhead. BOPs on offshore rigs are mounted below the rig deck. Subsea BOPs are connected to the offshore rig above by a drilling riser that provides a continuous pathway for the drill string and fluids emanating from the wellbore. In effect, a riser extends the wellbore to the rig. Unfortunately, blowout preventers do not always function correctly. An example of this is the Deepwater Horizon blowout, where the pipe line going through the BOP was slightly bent and the BOP failed to cut the pipe.\n\nBlowout preventers come in a variety of styles, sizes and pressure ratings. Several individual units serving various functions are combined to compose a blowout preventer stack. Multiple blowout preventers of the same type are frequently provided for redundancy, an important factor in the effectiveness of fail-safe devices.\n\nThe primary functions of a blowout preventer system are to:\n\nAdditionally, and in performing those primary functions, blowout preventer systems are used to:\n\nIn drilling a typical high-pressure well, drill strings are routed through a blowout preventer stack toward the reservoir of oil and gas. As the well is drilled, drilling fluid, \"mud\", is fed through the drill string down to the drill bit, \"blade\", and returns up the wellbore in the ring-shaped void, annulus, between the outside of the drill pipe and the casing (piping that lines the wellbore). The column of drilling mud exerts downward hydrostatic pressure to counter opposing pressure from the formation being drilled, allowing drilling to proceed.\n\nWhen a kick (influx of formation fluid) occurs, rig operators or automatic systems close the blowout preventer units, sealing the annulus to stop the flow of fluids out of the wellbore. Denser mud is then circulated into the wellbore down the drill string, up the annulus and out through the choke line at the base of the BOP stack through chokes (flow restrictors) until downhole pressure is overcome. Once “kill weight” mud extends from the bottom of the well to the top, the well has been “killed”. If the integrity of the well is intact drilling may be resumed. Alternatively, if circulation is not feasible it may be possible to kill the well by \"bullheading\", forcibly pumping, in the heavier mud from the top through the kill line connection at the base of the stack. This is less desirable because of the higher surface pressures likely needed and the fact that much of the mud originally in the annulus must be forced into receptive formations in the open hole section beneath the deepest casing shoe.\n\nIf the blowout preventers and mud do not restrict the upward pressures of a kick, a blowout results, potentially shooting tubing, oil and gas up the wellbore, damaging the rig, and leaving well integrity in question.\n\nSince BOPs are important for the safety of the crew and natural environment, as well as the drilling rig and the wellbore itself, authorities recommend, and regulations require, that BOPs be regularly inspected, tested and refurbished. Tests vary from daily test of functions on critical wells to monthly or less frequent testing on wells with low likelihood of control problems.\n\nExploitable reservoirs of oil and gas are increasingly rare and remote, leading to increased subsea deepwater well exploration and requiring BOPs to remain submerged for as long as a year in extreme conditions. As a result, BOP assemblies have grown larger and heavier (e.g. a single ram-type BOP unit can weigh in excess of 30,000 pounds), while the space allotted for BOP stacks on existing offshore rigs has not grown commensurately. Thus a key focus in the technological development of BOPs over the last two decades has been limiting their footprint and weight while simultaneously increasing safe operating capacity.\n\nBOPs come in two basic types, \"ram\" and \"annular\". Both are often used together in drilling rig BOP stacks, typically with at least one annular BOP capping a stack of several ram BOPs.\n\nThe ram BOP was invented by James Smither Abercrombie and Harry S. Cameron in 1922, and was brought to market in 1924 by Cameron Iron Works.\n\nA ram-type BOP is similar in operation to a gate valve, but uses a pair of opposing steel plungers, rams. The rams extend toward the center of the wellbore to restrict flow or retract open in order to permit flow. The inner and top faces of the rams are fitted with packers (elastomeric seals) that press against each other, against the wellbore, and around tubing running through the wellbore. Outlets at the sides of the BOP housing (body) are used for connection to choke and kill lines or valves.\n\nRams, or ram blocks, are of four common types: \"pipe\", \"blind\", \"shear\", and \"blind shear\".\n\nPipe rams close around a drill pipe, restricting flow in the annulus (ring-shaped space between concentric objects) between the outside of the drill pipe and the wellbore, but do not obstruct flow within the drill pipe. Variable-bore pipe rams can accommodate tubing in a wider range of outside diameters than standard pipe rams, but typically with some loss of pressure capacity and longevity. Pipe ram should not be closed if there is no pipe in the hole.\n\nBlind rams (also known as sealing rams), which have no openings for tubing, can close off the well when the well does not contain a drill string or other tubing, and seal it.\n\nShear rams are designed to shear the pipe in the well and seal the wellbore simultaneously. It has steel blades to shear the pipe and seals to seal the annulus after shearing the pipe.\n\nBlind shear rams (also known as shear seal rams, or sealing shear rams) are intended to seal a wellbore, even when the bore is occupied by a drill string, by cutting through the drill string as the rams close off the well. The upper portion of the severed drill string is freed from the ram, while the lower portion may be crimped and the “fish tail” captured to hang the drill string off the BOP.\n\nIn addition to the standard ram functions, variable-bore pipe rams are frequently used as test rams in a modified blowout preventer device known as a stack test valve. Stack test valves are positioned at the bottom of a BOP stack and resist downward pressure (unlike BOPs, which resist upward pressures). By closing the test ram and a BOP ram about the drillstring and pressurizing the annulus, the BOP is pressure-tested for proper function.\n\nThe original ram BOPs of the 1920s were simple and rugged manual devices with minimal parts. The BOP housing (body) had a vertical well bore and horizontal ram cavity (ram guide chamber). Opposing rams (plungers) in the ram cavity translated horizontally, actuated by threaded ram shafts (piston rods) in the manner of a screw jack. Torque from turning the ram shafts by wrench or hand wheel was converted to linear motion and the rams, coupled to the inner ends of the ram shafts, opened and closed the well bore. Such screw jack type operation provided enough mechanical advantage for rams to overcome downhole pressures and seal the wellbore annulus.\n\nHydraulic rams BOPs were in use by the 1940s. Hydraulically actuated blowout preventers had many potential advantages. The pressure could be equalized in the opposing hydraulic cylinders causing the rams to operate in unison. Relatively rapid actuation and remote control were facilitated, and hydraulic rams were well-suited to high pressure wells.\n\nBecause BOPs are depended on for safety and reliability, efforts to minimize the complexity of the devices are still employed to ensure longevity. As a result, despite the ever-increasing demands placed on them, state of the art ram BOPs are conceptually the same as the first effective models, and resemble those units in many ways.\n\nRam BOPs for use in deepwater applications universally employ hydraulic actuation. Threaded shafts are often still incorporated into hydraulic ram BOPs as lock rods that hold the ram in position after hydraulic actuation. By using a mechanical ram locking mechanism, constant hydraulic pressure need not be maintained. Lock rods may be coupled to ram shafts or not, depending on manufacturer. Other types of ram locks, such as wedge locks, are also used.\n\nTypical ram actuator assemblies (operator systems) are secured to the BOP housing by removable bonnets. Unbolting the bonnets from the housing allows BOP maintenance and facilitates the substitution of rams. In that way, for example, a pipe ram BOP can be converted to a blind shear ram BOP.\n\nShear-type ram BOPs require the greatest closing force in order to cut through tubing occupying the wellbore. Boosters (auxiliary hydraulic actuators) are frequently mounted to the outer ends of a BOP’s hydraulic actuators to provide additional shearing force for shear rams.\n\nRam BOPs are typically designed so that well pressure will help maintain the rams in their closed, sealing position. That is achieved by allowing fluid to pass through a channel in the ram and exert pressure at the ram’s rear and toward the center of the wellbore. Providing a channel in the ram also limits the thrust required to overcome well bore pressure.\n\nSingle ram and double ram BOPs are commonly available. The names refer to the quantity of ram cavities (equivalent to the effective quantity of valves) contained in the unit. A double ram BOP is more compact and lighter than a stack of two single ram BOPs while providing the same functionality, and is thus desirable in many applications. Triple ram BOPs are also manufactured, but not as common.\n\nTechnological development of ram BOPs has been directed towards deeper and higher pressure wells, greater reliability, reduced maintenance, facilitated replacement of components, facilitated ROV intervention, reduced hydraulic fluid consumption, and improved connectors, packers, seals, locks and rams. In addition, limiting BOP weight and footprint are significant concerns to account for the limitations of existing rigs.\n\nThe highest-capacity large-bore ram blowout preventer on the market, as of July 2010, Cameron’s EVO 20K BOP, has a hold-pressure rating of 20,000 psi, ram force in excess of 1,000,000 pounds, and a well bore diameter of 18.75 inches.\n\nThe annular blowout preventer was invented by Granville Sloan Knox in 1946; a U.S. patent for it was awarded in 1952. Often around the rig it is called the \"Hydril\", after the name of one of the manufacturers of such devices.\n\nAn annular-type blowout preventer can close around the drill string, casing or a non-cylindrical object, such as the kelly. Drill pipe including the larger-diameter tool joints (threaded connectors) can be \"stripped\" (i.e., moved vertically while pressure is contained below) through an annular preventer by careful control of the hydraulic closing pressure. Annular blowout preventers are also effective at maintaining a seal around the drillpipe even as it rotates during drilling. Regulations typically require that an annular preventer be able to completely close a wellbore, but annular preventers are generally not as effective as ram preventers in maintaining a seal on an open hole. Annular BOPs are typically located at the top of a BOP stack, with one or two annular preventers positioned above a series of several ram preventers.\n\nAn annular blowout preventer uses the principle of a wedge to shut in the wellbore. It has a donut-like rubber seal, known as an elastomeric packing unit, reinforced with steel ribs. The packing unit is situated in the BOP housing between the head and hydraulic piston. When the piston is actuated, its upward thrust forces the packing unit to constrict, like a sphincter, sealing the annulus or openhole. Annular preventers have only two moving parts, piston and packing unit, making them simple and easy to maintain relative to ram preventers.\n\nThe original type of annular blowout preventer uses a “wedge-faced” (conical-faced) piston. As the piston rises, vertical movement of the packing unit is restricted by the head and the sloped face of the piston squeezes the packing unit inward, toward the center of the wellbore.\n\nIn 1972, Ado N. Vujasinovic was awarded a patent for a variation on the annular preventer known as a spherical blowout preventer, so-named because of its spherical-faced head. As the piston rises the packing unit is thrust upward against the curved head, which constricts the packing unit inward. Both types of annular preventer are in common use.\n\nWhen wells are drilled on land or in very shallow water where the wellhead is above the water line, BOPs are activated by hydraulic pressure from a remote accumulator. Several control stations will be mounted around the rig. They also can be closed manually by turning large wheel-like handles.\n\nIn deeper offshore operations with the wellhead just above the mudline on the sea floor, there are five primary ways by which a BOP can be controlled. The possible means are:\n\nTwo control pods are provided on the BOP for redundancy. Electrical signal control of the pods is primary. Acoustical, ROV intervention and dead-man controls are secondary.\n\nAn emergency disconnect system/sequence, or EDS, disconnects the rig from the well in case of an emergency. The EDS is also intended to automatically trigger the deadman switch, which closes the BOP, kill and choke valves. The EDS may be a subsystem of the BOP stack’s control pods or separate.\n\nPumps on the rig normally deliver pressure to the blowout preventer stack through hydraulic lines. Hydraulic accumulators are on the BOP stack enable closure of blowout preventers even if the BOP stack is disconnected from the rig. It is also possible to trigger the closing of BOPs automatically based on too high pressure or excessive flow.\n\nIndividual wells along the U.S. coastline may also be required to have BOPs with backup acoustic control. General requirements of other nations, including Brazil, were drawn to require this method. BOPs featuring this method may cost as much as US$500,000 more than those that omit the feature.\n\nDuring the \"Deepwater Horizon\" drilling rig explosion incident on April 20, 2010, the blowout preventer should have been activated automatically, cutting the drillstring and sealing the well to preclude a blowout and subsequent oil spill in the Gulf of Mexico, but it failed to fully engage. Underwater robots (ROVs) later were used to manually trigger the blind shear ram preventer, to no avail.\n\nAs of May 2010 it was unknown why the blowout preventer failed. Chief surveyor John David Forsyth of the American Bureau of Shipping testified in hearings before the Joint Investigation of the Minerals Management Service and the U.S. Coast Guard investigating the causes of the explosion that his agency last inspected the rig's blowout preventer in 2005. BP representatives suggested that the preventer could have suffered a hydraulic leak. Gamma-ray imaging of the preventer conducted on May 12 and May 13, 2010 showed that the preventer's internal valves were partially closed and were restricting the flow of oil. Whether the valves closed automatically during the explosion or were shut manually by remotely operated vehicle work is unknown. A statement released by Congressman Bart Stupak revealed that, among other issues, the emergency disconnect system (EDS) did not function as intended and may have malfunctioned due to the explosion on the Deepwater Horizon.\n\nThe permit for the Macondo Prospect by the Minerals Management Service in 2009 did not require redundant acoustic control means. Inasmuch as the BOPs could not be closed successfully by underwater manipulation (ROV Intervention), pending results of a complete investigation, it is uncertain whether this omission was a factor in the blowout.\n\nDocuments discussed during congressional hearings June 17, 2010, suggested that a battery in the device's control pod was flat and that the rig's owner, Transocean, may have \"modified\" Cameron's equipment for the Macondo site (including incorrectly routing hydraulic pressure to a stack test valve instead of a pipe ram BOP) which increased the risk of BOP failure, in spite of warnings from their contractor to that effect. Another hypothesis was that a junction in the drilling pipe may have been positioned in the BOP stack in such a way that its shear rams had an insurmountable thickness of material to cut through.\n\nIt was later discovered that a second piece of tubing got into the BOP stack at some point during the Macondo incident, potentially explaining the failure of the BOP shearing mechanism. As of July 2010 it was unknown whether the tubing might have been casing that shot up through the well or perhaps broken drill pipe that dropped into the well. The DNV final report indicated that the second tube was the segment of the drill string that was ejected after being cut by the blow out preventer shears.\n\nOn July 10, 2010 BP began operations to install a sealing cap, also known as a capping stack, atop the failed blowout preventer stack. Based on BP's video feeds of the operation the sealing cap assembly, called Top Hat 10, included a stack of three blind shear ram BOPs manufactured by Hydril (a GE Oil & Gas company), one of Cameron's chief competitors. By July 15 the 3 ram capping stack had sealed the Macondo well, if only temporarily, for the first time in 87 days.\n\nThe U.S. government wanted the failed blowout preventer to be replaced in case of any pressure change that occurs when the relief well intersected with the well. On September 3, 2010, at 1:20 p.m. CDT the 300 ton failed blowout preventer was removed from the well and began being slowly lifted to the surface. Later that day a replacement blowout preventer was placed on the well. On September 4 at 6:54 p.m. CDT the failed blowout preventer reached the surface of the water and at 9:16 p.m. CDT it was placed in a special container on board the vessel Helix Q4000. The failed blowout preventer was taken to a NASA facility in Louisiana for examination by Det Norske Veritas (DNV).\n\nOn 20 March 2011, DNV presented their report to the US Department of Energy. Their primary conclusion was that while the rams succeeded in partly shearing through the drill pipe they failed to seal the bore because the drill pipe had buckled out of the intended line of action of the rams (because the drill string was caught at a tool joint in the upper annular BOP valve), jamming the shears and leaving the drill string shear actuator unable to deliver enough force to complete its stroke and fold the cut pipe over and seal the well. They did not suggest any failure of actuation as would be caused by faulty batteries. The upper section of the blow out preventer failed to separate as designed due to numerous oil leaks compromising hydraulic actuator operation, and this had to be cut free during recovery.\n\n\n"}
{"id": "1103547", "url": "https://en.wikipedia.org/wiki?curid=1103547", "title": "Brainstorm (1983 film)", "text": "Brainstorm (1983 film)\n\nBrainstorm is a 1983 American science fiction film directed by Douglas Trumbull, and starring Christopher Walken, Natalie Wood (in her final film role), Louise Fletcher and Cliff Robertson.\n\nIt follows a research team's efforts to perfect a system that directly records the sensory and emotional feelings of a subject, and the efforts by the company's management to exploit the device for military ends.\n\nA team of scientists invent a brain/computer interface that allows sensations to be recorded from a person's brain and converted to tape so that others may experience them. The team includes estranged husband and wife Michael and Karen Brace, as well as Michael's colleague Lillian Reynolds. At CEO Alex Terson's instruction, the team demonstrates the device to investors in order to gain financing.\n\nKaren dons the recorder while working with Michael and Lillian. When Michael plays the tape back, the group realizes that emotional experiences are also recorded. Michael tapes his memories of times with Karen, which he shares with her, and it leads to their reconciliation.\n\nLillian is pressured by backers to admit as a member of the team a former colleague, Landon Marks, whom she sees as part of the military-industrial complex. She disagrees with their plan to have the invention developed for military use.\n\nOne team member, Gordy Forbes, has sexual intercourse while wearing the recorder, and he shares the tape with colleagues, including Hal Abramson. Hal splices one section of the tape into a continuous orgasm, which results in sensory overload, leading to his forced retirement. Tensions increase as the possibilities for abuse become clear.\n\nAlready suffering from heart problems and a constant cigarette smoker, Lillian suffers a heart attack while working alone. Realizing that she is about to die, Lillian records her experience.\n\nFollowing her funeral, Michael decides to experience Lillian's recording, but he nearly dies when the playback causes his body to simulate the sensations and effects of a heart attack. Michael modifies his console to filter the physical output, and he replays the tape. Viewing Lillian's death experience, he sees \"memory bubbles\"—moments from Lillian's life. Michael experiences Lillian's memories of a humorous exchange with Michael as he plays with an industrial robot, a surprise birthday party, and being devastated when Alex tells her that an earlier project has been cancelled.\n\nA team of scientists wanting to discover the machine's military capabilities is monitoring the equipment as Michael plays Lillian's final tape. They have Gordy also experience the tape, but Landon ignored the advice of the monitoring staff that Michael has made modifications to his playback terminal. As such Gordy is killed offscreen for monitoring the tape unauthorized whilst Landon and the others watch on in surprise.\n\nMichael's playback is cut short by Hal, but having witnessed a digital near-death experience makes Michael curious to see the entire tape. Alex has the recording locked away and tells Michael he will not be allowed to view it. When he returns to work, Michael walks in on Landon Marks and a team of outsider technicians going through his research records and protests to Alex who responds by firing Michael and Karen.\n\nMichael attempts to hack into the lab's computers. Hal advises him to look under \"Project Brainstorm\", a program the military has created to re-develop their invention for torture and brainwashing. Michael and Karen's son Chris, wanting to experience the special device, is inadvertently exposed to one such tape, causing him to have a psychotic experience which results in his hospitalization—there Michael learns of Gordy's fate and deduces that Alex was behind his murder.\n\nRather than see his creation perverted, Michael vows to destroy his work and enlists the help of Karen and Hal. Michael and Karen head to the Pinehurst Resort and, realizing they are under surveillance, stage a fight that results in Karen leaving for Hal's house. As the two feign reconciliation over the phone, Michael accesses the Brainstorm computer via another phone line, while Karen hacks into the system and sabotages the robots that manufacture the interface terminals.\n\nKaren shuts down the security system, locking the staff outside and enabling Michael to load Lillian's tape and experience it uninterrupted. With the plant in chaos, Robert Jenkins orders Michael's arrest. Michael escapes their agents and drives to a phone booth at the Wright Brothers National Memorial. He reconnects with the computers and accesses the final part of the tape, beyond the point of Lillian's physical death.\n\nKaren leaves the house to meet with Michael. Hal and his wife, Wendy, send the last of Karen's commands to the company computers, shutting down the plant.\n\nKaren arrives at the Wright Brothers Memorial while the tape is playing. Michael bears witness to the afterlife, experiencing a brief vision of hell before traveling away from Earth and through the universe, even after the tape ends. He ultimately has visions of angels and departed souls flying into a great cosmic Light, which seems to be heaven. Michael then collapses. Karen sobs, believing him dead. She pleads for Michael to stay alive. Awakening from the experience, he weeps with joy and embraces Karen.\n\nTo prepare for the film, Trumbull took most of the key cast and crew up to the Esalen Institute, an experimental research facility in Northern California known for its new-age classes and workshops. In September 1981 the cast and crew traveled to North Carolina to begin six weeks of location shooting, before moving back to the Metro-Goldwyn-Mayer Studios in Culver City, California in November to film interior scenes.\n\nThe film was nearly scuttled by Natalie Wood's death during a production break in November 1981. By this time, Wood had already completed all of her major scenes, but due to mounting financial problems, MGM took Wood's death as an opportunity to shut down the already troubled production. \"When she died,\" said Trumbull, \"all the sets were locked and frozen on all the stages. No one could get in or out without special permission while all the negotiations took place.\"\n\nTrumbull believed that the financially strapped MGM simply got cold feet about putting up the rest of the money to complete \"Brainstorm\". \"MGM's problem was that insurance institution Lloyd's of London, when it took depositions from me and other people, realized that the film could be finished. Why should they pay an insurance claim for something that really wasn't damaged goods?\" When MGM refused to pay for the film to be completed, Lloyd's of London provided $2.75 million for Trumbull to complete principal photography and an additional $3.5 million towards post-production. Meanwhile, other studios showed interest in buying \"Brainstorm\" from MGM to release as their own production. \"MGM decided to allow Lloyd's of London to offer the film to many of the major studios in town,\" said Trumbull. \"Several of them made bids to MGM. And the studio suddenly realized that a lot of other people in this town were excited about \"Brainstorm\", and were ready to put up millions of dollars. MGM figured they'd look like jerks if they let it go and it turned out to be a big success. So they finally decided to work out this deal where Lloyd's of London would put up the remaining money and become a profit participant.\"\n\nTrumbull proceeded to complete the film by rewriting the script and using Natalie Wood's younger sister for Wood's few remaining scenes.\n\nThe film carries the dedication credit \"To Natalie\".\n\nThe film was conceived as an introduction to Trumbull's Showscan 60 frames-per-second 70mm film process. \"In movies people often do flashbacks and point-of-view shots as a gauzy, mysterious, distant kind of image,\" Trumbull recalled, \"And I wanted to do just the opposite, which was to make the material of the mind even more real and high-impact than 'reality'\".\n\nHowever, MGM backed out of plans to release the experimental picture in the new format. Trumbull instead shot the virtual reality sequences in 24 frames-per-second Super Panavision 70 with an aspect ratio of 2.2:1. The rest of the film was shot in conventional 35mm with an aspect ratio of approximately 1.7 to 1.\n\nThe score to \"Brainstorm\" was composed and conducted by James Horner, it won him the Saturn Award for Best Music in 1983. The \"Varèse Sarabande\" album/CD release is a re-recording with the London Symphony Orchestra, produced shortly before the original theatrical release.\n\n\"Brainstorm\" was finally released on September 30, 1983, almost two years after Wood's death. However, it opened on a small number of screens and with little publicity, despite being trumpeted unofficially as \"Natalie Wood's last movie\". Review aggregator website Rotten Tomatoes reports that 59% of 17 critics have given the film a positive review with an average rating of 5.7/10. Janet Maslin of the \"New York Times\" gave particular credit to Louise Fletcher's \"superb performance\".\n\nThe film was not a commercial success, with a production budget of $18 million and grossing only $10 million in ticket sales in North America.\n\nBecause of the immensely troubled production and disagreements with MGM, Trumbull opted never to direct a Hollywood film again. In 1983 he stated \"I have no interest...in doing another Hollywood feature film...Absolutely none. The movie business is so totally screwed up that I just don't have the energy to invest three or four years in a feature film. Moviemaking is like waging war. It destroys your personal life, too. The people who can survive the process of making films have largely given up their personal lives in order to do that, just because it's such a battle to make a movie. And in doing that, they've isolated themselves from the very audience that they're trying to reach.\" In 2013, he explained that the uncertain circumstances of Natalie Wood's death were the main reason for this decision. He has since returned to filmmaking.\n\n"}
{"id": "28739060", "url": "https://en.wikipedia.org/wiki?curid=28739060", "title": "Commercial code (communications)", "text": "Commercial code (communications)\n\nIn telecommunication, a commercial code is a code once used to save on cablegram costs. Telegraph (and telex) charged per word sent, so companies which sent large volumes of telegrams developed codes to save money on tolls. Elaborate commercial codes which encoded complete phrases into single words were developed and published as codebooks of thousands of phrases and sentences with corresponding codewords. Commercial codes were not generally intended to keep telegrams private, as codes were widely published; they were usually cost-saving measures only.\n\nMany general-purpose codes, such as the \"Acme Code\" and the \"ABC Code\", were published and widely used between the 1870s and the 1950s, before the arrival of transatlantic telephone calls and next-day airmail rendered them obsolete. Numerous special-purpose codes were also developed and sold for fields as varied as aviation, car dealerships, insurance, and cinema, containing words and phrases commonly used in those professions.\n\nThese codes turned complete phrases into single words (commonly of five letters). These were not always genuine words; for example, codes contained \"words\" such as \"BYOXO\" (\"Are you trying to weasel out of our deal?\"), \"LIOUY\" (\"Why do you not answer my question?\"), \"BMULD\" (\"You're a skunk!\"), or \"AYYLU\" (\"Not clearly coded, repeat more clearly.\").\n\nThe first telegraphic codes were developed shortly after the advent of the telegraph, and spread rapidly: in 1854, one eighth of telegrams transmitted between New York and New Orleans were written in code. Cable tolls were charged by the word, and telegraph companies counted codewords like any other words, so a carefully constructed code could reduce message lengths enormously.\n\nEarly codes were typically compilations of phrases and corresponding codewords numbering in the tens of thousands. Codewords were chosen to be pronounceable words to minimize errors by telegraphers, and telegrams composed of non-pronounceable words cost significantly more. Regulations of the International Telegraph Union evolved over time; in 1879, it mandated coded telegrams only contain words from German, English, Spanish, French, Italian, Dutch, Portuguese, or Latin, but commercial codes already frequently used nonsense words. By 1903 regulations were changed to allow any pronounceable word no more than ten letters long.\n\nAnother aim of the telegraph codes was to reduce the risk of misunderstanding by avoiding having similar words mean similar things. Codes were usually designed to avoid error by using words which could not be easily confused by telegraph operators. Telegrapher errors could sometimes cause serious monetary damages, which in one instance resulted in the Supreme Court case , in which a wool dealer argued that an error by a Western Union telegrapher cost $20,000 due to misread instructions. The Supreme Court subsequently ruled Western Union was liable only for the cost of the message, $1.15.\n\nExamples of commercial codes include the \"ABC Telegraphic Code\", \"Bentley's Second Phrase Code\", \"Lieber's Standard Telegraphic Code\" (1896), \"Slater's Telegraphy Code\" (1916), \"Western Union Universal Codebook\" (1907) and \"Unicode\" (1889).\n\nIn codes such as the \"ABC Code\", code words could contain blanks. For example, in the \"Freight and tonnage requirements\" section, \"ANTITACTE\" means \"Mozambique, loading at not more than two places, to ____, steamer for about ____ tons general cargo at ____ per ton on the d/w capacity to cargo\". The telegrapher would then fill in the three parameters: the destination, the number of tons, and the price per ton.\n\nThe regulations of the International Telegraph Convention distinguished between \"code telegrams\", which it describes as \"those composed of words the context of which has no intelligible meaning\", and \"cipher telegrams\", which it describes as \"those containing series of groups of figures or letters having a secret meaning or words not to be found in a standard dictionary of the language\". Cipher telegrams were subject to higher tolls.\n\nCodes such as the \"ABC Telegraphic Code\", therefore, included both numbers and code words so the user could choose between the two.\n\nExample code words:\n\n\n"}
{"id": "30034641", "url": "https://en.wikipedia.org/wiki?curid=30034641", "title": "Committee on Sustainability Assessment", "text": "Committee on Sustainability Assessment\n\nThe Committee on Sustainability Assessment (COSA) is a global consortium of development institutions that work collaboratively to advance the systematic and science-based measurement of sustainability in agriculture. COSA applies a pragmatic and collective approach for using scientific methods to develop indicators and tools to measure sustainability through performance monitoring, evaluation, and impact assessment. These sustainability measurements assess the distinct social, environmental and economic impacts of agricultural practices.\nCOSA’s approach and indicators have a basis in international treaties and normative references such as the International Labour Organization eight fundamental Conventions, the World Health Organization Guidelines for Water Quality and the International Finance Corporation. The indicators align with internationally recognized accords including the United Nations Global Compact, Rio Declaration on Environment and Development, UN Framework Convention on Climate Change, and the Universal Declaration of Human Rights and are benchmarked for consistency and shared use.\n\nThe concept for COSA was originally developed in 2005 as a project of the United Nations Conference on Trade and Development, the International Institute for Sustainable Development and the United Nations International Trade Centre and was focused on the coffee sector. The COSA indicator development process was inaugurated through the International Coffee Organization whose Council unanimously endorsed it, making it the first sustainability assessment system to be formally adopted by a global commodity body. The United Nations Conference on Trade and Development COSA project focused on developing a thorough and rigorous cost benefit analysis of sustainability practices in the coffee sector via two primary outputs: a tool for assessing costs and benefits according to COSA-defined criteria and indicators, and training to enable stakeholders to “measure and understand the costs and benefits of undertaking sustainable practices and adopting different sustainability initiatives.”\n\nIn 2008, the United Nations Conference on Trade and Development COSA project published \"Seeking Sustainability: COSA Preliminary Analysis of Sustainability Initiatives in the Coffee Sector\". The report summarized the findings of the pilot application of the COSA tool to collect and analyze data to facilitate understanding of environmental, social, and economic outcomes associated with sustainability initiatives in the coffee sector. The six coffee sector sustainability initiatives included were: organic, Fair Trade, Common Code for the Coffee Community (4C), UTZ certified, Rainforest Alliance, and Starbucks C.A.F.E. Practices. Three coffee growing regions were included: Africa, Asia, and Latin America.\n\nIn 2012, led by its founder and President Daniele Giovannucci, the structure and constitution of COSA was formalized as an independent non-profit organization incorporated under United States law to advance research and training in the field of sustainability. It is as this globally-focused, independent non-profit that COSA continues its work today. Core support has come primarily from the Swiss State Secretariat for Economic Affairs, the Ford Foundation and the Inter-American Development Bank. COSA is supported half by public grants and half by its advisory services and impact assessment projects.\n\nCOSA supports institutions to adopt and integrate approaches to sustainability, and includes more than 40 public and private sector organizations.\n\nThe COSA partnership with the International Institute for Tropical Agriculture, in Kenya and Uganda, led to new processes with the International Initiative for Impact Evaluation (3ie) for conducting field research to advance the understanding of the challenges faced by smallholder farmers and the roles of their cooperatives. The work, commissioned by the International Social and Environmental Accreditation and Labelling Alliance with support of the Ford Foundation, evolved methods for effectively assessing the impacts of multiple sustainability certifications on the lives of farmers, their organizations, and communities.\n\nCOSA works in development projects with sustainability labels such as Fair trade, Organic, UTZ Certified, 4C, and Rainforest Alliance. Private supply chains have utilized COSA to assess and measure the impact of their sustainability efforts, fostered by development agencies such as the International Finance Corporation, USAID, or the Swiss Government, and include firms such as Nespresso, Lindt and Sprungli, Mars Drinks, Cargill, ECOM Trading and Mondelez International.\n\nCOSA indicators and tools have been widely incorporated, adopted, and adapted by institutions, corporations, and other organizations:\nSince its creation, COSA’s reach has expanded to seventeen countries: Colombia, Costa Rica, Côte d'Ivoire, Ethiopia, Ghana, Guatemala, Honduras, India, Indonesia, Kenya, Mexico, Nicaragua, Papua New Guinea, Peru, Tanzania, Uganda, and Vietnam. In addition to coffee, COSA now also works with cocoa, cotton, sugar, and food crops.\n\nCOSA has been recognized in the international development and sustainability communities for its \"visible and impartial\" assessments. Following the 2014 publication of the COSA Measuring Sustainability Report: Coffee and Cocoa in 12 Countries, the International Society of Environmental Accreditation and Labeling Alliance called COSA a leader in the \"alignment of standards and certification initiatives, showing the potential of harmonizing metrics.\" \n\n"}
{"id": "616833", "url": "https://en.wikipedia.org/wiki?curid=616833", "title": "Cutler's resin", "text": "Cutler's resin\n\nCutler's resin is a synthetic resin made of pine pitch, beeswax, and sawdust or carnauba wax used for centuries (to today) to attach knife handles. It is used as both an adhesive and for waterproofing. The word cutler means \"one who makes knives\", hence the word \"cutlery\".\n"}
{"id": "9627438", "url": "https://en.wikipedia.org/wiki?curid=9627438", "title": "Daedeok Science Town", "text": "Daedeok Science Town\n\nDaedeok Innopolis, formerly known as Daedeok Science Town, is the research and development district in the Yuseong-gu district in Daejeon, South Korea. Daedeok Innopolis grew out of the research cluster established by President Park Chunghee in 1973 with the opening of the KAIST. Over 20 major research institutes and over 40 corporate research centers make up this science cluster. Over the last few years, a number of IT venture companies have sprung up in this region, which has a high concentration of Ph.Ds in the applied sciences. There are 232 research and educational institutions to be found in Daejeon, many in the Daedeok region, among them the Electronics and Telecommunications Research Institute and the Korea Aerospace Research Institute. The \"town\" will provide a core for the International Science and Business Belt.\n\nThe Daedeok Innopolis logo was created by the industrial design company INNO Design in Palo Alto, USA. \n\n\n\n\n"}
{"id": "11654677", "url": "https://en.wikipedia.org/wiki?curid=11654677", "title": "Fab Tree Hab", "text": "Fab Tree Hab\n\nThe Fab Tree Hab is a hypothetical ecological home design developed at MIT by Mitchell Joachim, Javier Arbona and Lara Greden. With the idea of easing the burden Humanity places on the environment with conventional housing by growing \"living, breathing\" tree homes.\n\nIt would be built by allowing native trees to grow over a computer-designed (CNC) removable plywood scaffold. Once the plants are interconnected and stable, the plywood would be removed and reused. MIT is experimenting with trees that grow quickly and develop an interwoven root structure that's soft enough to \"train\" over the scaffold, but then hardens into a more durable structure. The inside walls would be conventional clay and plaster.\n\nAn old methodology new to buildings is introduced in this design - pleaching. Pleaching is a method of weaving together tree branches to form living archways, lattices, or screens.; the technique is also named \"Aeroponic culture\". The load-bearing part of the structure uses trees that self-graft or inosculate such as Live Oak, Elm and Dogwood. The lattice frame for the walls and roof are created with the branches of the trees. Vines create a dense protective layer woven along the exterior, interspersed with soil pockets and growing plants.\n\nThe Fab Tree Hab is an experiment that would develop over time. Extra operating costs required over the life-time of the home include pest management with organic pesticides and maintenance of the living machine's water treatment system. Technical demonstration and innovation is still needed for certain components, primarily the bioplastic windows that accept growth of the structure and the management of flows across the wall section to assure that the interior remains dry and animal-free. All in all, the elapsed time to reach livability is greater than the traditional sense, but so should be the health and longevity of the home and family. Above all, building this home could be achieved at a minimal price. Depending on the surrounding climate the house is to be grown in, the team expect it will take a minimum of five years to complete its structure. Realization of these homes will begin as an experiment, and it is envisioned that thereafter, the concept of renewal will take on a new architectural form - one of inter-dependency between nature and people.\n\nAs of May 2007 Mitchell Joachim stated that there is a \"50 per cent\" organic project in California, combining natural elements and traditional construction.\n\nMain trees suggested to be used are elms and oaks. The teams hopes the homes can be grown using mainly native trees.\n\n\n"}
{"id": "45541225", "url": "https://en.wikipedia.org/wiki?curid=45541225", "title": "Games and learning", "text": "Games and learning\n\nGames and learning is a field of education research that studies what is learned by playing video games, and how the design principles, data and communities of video game play can be used to develop new learning environments. Video games create new social and cultural worlds – worlds that help people learn by integrating thinking, social interaction, and technology, all in service of doing things they care about. Computers and other technologies have already changed the way students learn. Integrating games into education has the potential to create new and more powerful ways to learn in schools, communities and workplaces. Games and learning researchers study how the social and collaborative aspects of video game play can create new kinds of learning communities. Researchers also study how the data generated by game play can be used to design the next generation of learning assessments.\n\nThe games and learning research world studies how new digital media tools shift the topic of education research from recalling and repeating information to being able to find it, evaluate it and use it compellingly at the right time and in the right context. Games and learning research explores how games and game communities can lead to 21st-century educational skills such as higher order thinking, the ability to solve complex problems, think independently, collaborate, communicate and apply digital tools to effectively gather information.\n\nResearch conducted by Shaffer, D., Squire, K., Halverson, R., & Gee, J. P. from the University of Wisconsin – Madison shows the educational and social benefits of digital games. Games do not need to be specifically geared towards education to be educational tools. Games can bring together ways of knowing, ways of doing, ways of being, and ways of caring. As John Dewey argued, schools are built on an obsession with facts. Students need to learn by doing, and with gaming, students can learn by doing something as a part of a larger community of people who share common goals and ways of achieving those common goals, making gaming a benefit for social reasons as well. Gaming has also changed the look of content-driven curriculum in schools. In content-driven media, people learn by being told and reflecting on what they are told. In gaming, game designers create digital environments and game levels that shape, facilitate and even teach problem solving.\n\nAnother experiment investigated the effects of utilizing the iPad as a learning tool in American Preschools. The results from the experiment contrasted greatly to the common notion that the increasing use of technology by children proves harmful. In their results, the use of the iPads, specifically the completion of the application's tasks within it, yielded positive results. Peer interaction, participation, and learning were all evident since the task was administered in a classroom setting that required the children to work together.\n\nGames also teach students that failure is inevitable, but not irrevocable. In school, failure is a big deal. In games, players can just start over from the last save. A low cost failure ensures that players will take risks, explore and try new things.\n\nMuch of the debate about digital games for education was based on whether or not games are good for education. But that question is overly simplistic. The National Research Council's report on laboratory activities and simulations makes clear that the design and not merely the medium of a physical or virtual learning activity determines its efficacy. Digital games are a medium with certain affordances and constraints, just as physical labs and virtual simulations are media with certain affordances and constraints. Simulations and digital games actually share many similarities in this regard. Although there are multiple definitions for games, the key characteristics differentiating games from simulations involve the explicit inclusion of (a) rules for engaging with the simulation, (b) goals for players to pursue, and (c) means for indicating players' progress toward those goals. Properly designed, features of games can provide powerful affordances for motivation and learning. Individual studies have shown, for example, that well designed games can promote conceptual understanding and process skills, can foster a deeper epistemological understanding of the nature and processes through which science knowledge is developed and can produce gains in players' willingness and ability to engage in scientific practices and discourse.\n\nIn his book \"What Video Games Have to Teach Us About Learning and Literacy\", James Paul Gee talks about the application and principles of digital learning. Gee has focused on the learning principles in video games and how these learning principles can be applied to the K-12 classroom. Successful video games are good at challenging players. They motivate players to persevere and teach players how to play. Gee's video game learning theory includes his identification of thirty-six learning principles, including: 1) Active Control, 2) Design Principle, 3) Semiotic Principle, 4) Semiotic Domain, 5) Meta-level Thinking, 6) Psychosocial Moratorium Principle, 7) Committed Learning Principle 8) Identity Principle, 9) Self-knowledge Principle, 10) Amplification of Input Principle, 11) Achievement Principle, 12) Practice Principle, 13) Ongoing Learning Principle, and 14) Regime of Competence Principle and more. Within these learning principles Gee shows the reader the various ways in which games and learning are linked and how each principle supports learning through gaming. One example would be Learning Principle 6: \"Psychosocial Moratorium\" Principle, where Gee explains that in games, learners can take risks in a space where real-world consequences are lowered. Another of Gee's principles, #8, that shows the importance of games and learning states that learning involves taking on and playing with identities in such a way that the learner has real choices (in developing the virtual identity) and ample opportunity to mediate on the relationship between new identities and old ones. There is tripartite play of identities as learners relate, and reflect on, their multiple real-world identities, a virtual identity, and a projective identity.\n\nScot Osterweil, a research director at the Massachusetts Institute of Technology's Comparative Media Studies program states that these standards and testing methods are not conducive to teaching methods that incorporate video games. Games alone will not make schools more efficient, cannot replace teachers or serve as an educational resource that can reach an infinite number of students. The extent of the roles games will play in learning remains to be seen. More research in this area is needed to determine impact of games and learning.\n\nPeter Gray, who has conducted research on early childhood learning, states that gaming is purely a beneficial activity in young children. He states that children are able to choose how to most effectively use their time and that extensive use of a particular medium of learning shows they are taking something valuable from it. He goes on to state the significance of the computer in the modern age and that not utilizing it as a learning tool is simply foolish. Video gaming has shown positive levels of improvement in areas of cognitive function. In their study \"Improving Multi-Tasking Ability through Action Videogames\". Chiappe and colleagues determined that 50 hours of gaming significantly improved results on a performance test modeled after skills used when piloting an aircraft. Aside from this, areas of attention and vigilance, as well as basic visual processes have shown to improve with allotted video game time.\n\nDigital learning tools have the potential of being customized to fit the abilities of individual students and can engage them with interactive tasks and simulate real-life situations. Games can create new social and cultural worlds that may not have been available to everyone in the past. These worlds can help people learn by integrating thinking, social interaction, and technology, all in service of doing things they care about.\n\nVideo games are important because they let people participate in and experience new worlds. They let players think, talk, and act in new ways. Indeed, players inhabit roles that are otherwise inaccessible to them. One example of a game where players are learning while playing would be \"The Sims\", a real-time strategy game where players need to make decisions that alter their character's life. They can manipulate the scenario to create digital lives where they can experience the struggles of single parenthood or poverty. Players in this game are not allowed to modify a previous decision to alter the outcome, even if the outcome is unpleasant. The goal is to survive to the best of their abilities. The game is complicated and difficult, just as it would be to live a real life. Regarding a more traditional approach to education, \"The Sims\" has been used as a platform for students to learn a language and explore world history while developing skills such as reading, math, logic and collaboration.\n\nWhile not all researchers agree, some recent studies have shown the possitive effects of using games for learning. A study carried out by professor Traci Sitzmann at the University Oregon among 6,476 students states that \"trainees in the game group had 11 percent higher factual knowledge levels, 14 percent higher skill-based knowledge levels, and 9 percent higher retention levels than trainees in the comparison group\". Some other aggregated studies also show an increase in learning performance thanks to the use of videogames.\n\nCritics suggest that lessons people learn from playing video games are not always desirable. Douglas Gentile, an associate professor of psychology at Iowa State University found that children who repeatedly play violent video games are learning thought patterns that will stick with them and influence behaviors as they grow older. Researchers from this study found that over time children started to think more aggressively, and when provoked at home, school or in other situations, children reacted much like they did when playing a violent video game. But even the harshest critics agree that people can learn something from playing video games. While research on the behavioral and cognitive impacts of video games with violence have shown mixed outcomes, games with little or no violence have shown promising results. Elizabeth Zelinski, a professor of gerontology and psychology at the University of Southern California states that some digital games have been shown to improve the function of the brain, while others have the potential to reverse cognitive loss associated with aging. Some games require players to make decisions ranging from simple to quite complex to drive its progress.\n\nSome researchers question whether a greater reliance on video games is in students' best interests, indicating there is little proof that skillful game play translates into better test scores or broader cognitive development. Emma Blakey notes very few studies have examined whether video games improve classroom performance and academic achievement.\n\nOthers, like Emma Blakey, a PhD researcher in developmental psychology at the University of Sheffield in England, question whether a greater reliance on video games is in students' best interests, indicating there is little proof that skillful game play translates into better test scores or broader cognitive development. Because schools are working to meet Common Core State Standards, which dictate what students should be able to accomplish in English and mathematics at the end of each grade (using standardized testing as a way of tracking a student's progress), game use for learning becomes obsolete.\n\n\n\n"}
{"id": "5867869", "url": "https://en.wikipedia.org/wiki?curid=5867869", "title": "Ghrsst-pp", "text": "Ghrsst-pp\n\nThe Group for High Resolution SST (GHRSST) is a follow on activity form the Global Ocean Data Assimilation Experiment (GODAE) high-resolution sea surface temperature pilot project (GHRSST-PP) provides a new generation of global high-resolution (<10 km) SST data products to the operational oceanographic, meteorological, climate and general scientific community, in real time and delayed mode. \n\nSee GHRSST for full details.\n\nSea surface temperature (SST) measured from Earth Observation Satellites in considerable spatial detail and at high frequency, is increasingly required for use in the context of operational monitoring and forecasting of the ocean, for assimilation into coupled ocean-atmosphere model systems and for applications in short-term numerical weather prediction and longer term climate change detection. Currently there are many different SST data sets available derived from satellite systems. But, scientists and operational agencies alike are presented with a bewidering set of options in terms of SST product content, coverage, spatial resolution, timeliness, format and accuracy. The international GODAE steering committee realised that SST data products were not adequate for GODAE forecast systems and initiated the GODAE High Resolution SST Pilot Project (GHRSST-PP). User Requirements were collected together to define the optimal SST data products that could be developed to suit the widest possible number of applications. In 2008 the GHRSST-PP Science Team agreed to close the Pilot Project as the GODAE project was completed. A follow on activity called the Group for High Resolution SST is now continuing the coordination of GHRSST activities.\n\nThe purpose of GHRSST is to develop an operational demonstration system and to drive all scientific aspects related to SST. The activity co-ordinates the delivery of a new generation of global coverage high-resolution (better than 10 km and ~6 hourly) SST data products. GHRSST data products are derived by combining readily available but complementary Level-2 (L2) satellite and in situ observations in real time to improve spatial coverage, temporal resolution, cross-sensor calibration stability and SST product accuracy.\n\nGHRSST is an international activity that orchestrates a wide variety of input and output data. The data are shared, indexed, processed, quality controlled, analysed and documented within an international framework. Large volumes of data and associated data services are harnessed together to deliver the new generation of global coverage high resolution SST data sets.\n\nGHRSST is based on a distributed system in which the data processing operations that are necessary to operationally generate and distribute high resolution SST data sets having global coverage are shared by Regional Data Assembly Centres (RDAC). RDAC ingest, quality control and merge existing satellite and in situ SST data sources that are then merged to generate regional coverage SST data products having the same netCDF format specification (called L2P products), in real-time. RDAC data products are then assembled together at Global Data Analysis Centres (GDAC) where they are merged to provide L4 global coverage data products free of gaps. The distributed processing system is referred to as the GHRSST Regional/Global Task Sharing (R/GTS) framework\n\nThe GHRSST Data Processing Specification (GDS) is central to the successful implementation and operation of the R/GTS. It provides a common data processing specification that must be implemented at each GHRSST RDAC and GDAC. It defines clearly the input and output data specifications, data processing procedures, algorithms and data product file formats that are used within the GDS and are thus common to each GHRSST RDAC and GDAC. Conforming to the netCDF CF1.3 specification is a prerequisite if the GHRSST Global/Regional task sharing implementation framework is to function efficiently.\n\nThere are great strengths to this approach from a community perspective. For example, a common processing description is necessary to simplify documentation of data, facilitate exchange by sharing a common data format agreed by RDAC, GDAC and users, to avoid significant duplication of effort, to minimise reformatting of different data products derived by RDAC and to ease the integration of RDAC data to provide global coverage data sets at GDAC centres. Operationally produced data products will be improved by using additional data that are only available in a delayed mode together with extensive quality control procedures as part of the GHRSST reanalysis (RAN) project (see http://ghrsst.nodc.noaa.gov).\n\nToday GHRSST is a truly international project with over $24 Million US invested across all of the project activities. A basic 'version-1.0' of the GHRSST Regional/Global Task Sharing (R/GTS) system has been implemented in an internationally distributed manner. Global and regional coverage SST data products are now produced by RDAC in the USA, Australia, France, Japan, Denmark, the United Kingdom, Italy and Canada. Products are passed in near real time to a recently operational GDAC at NASA JPL, USA. Research and development within GHRSST projects continue to tackle the problems of diurnal variability, skin temperature deviations and SST validation. Data management teams continue to refine the GHRSST data management structures to provide a functional system that conforms to federal directives (e.g., ISO, FGDC, INSPIRE). A Long Term Data Stewardship and Reanalysis Facility (http://www.nodc.noaa.gov/SatelliteData/ghrsst/ LTSRF) is operated by the NOAA National Center for Environmental Information in collaboration with the JPL GDAC (http://ghrsst.jpl.nasa.gov). The GHRSST-PP International Project Office, jointly funded by the European Space Agency and Met Office, UK, continues to manage the international co-ordination of the project. GHRSST operates a Multi Product Ensemble (GMPE) of operational SST and Sea Ice analyses of the world which contributes a formal action of the Group on Earth Observations (GEO).\n\nMost importantly, an international user community has emerged that is now testing and applying GHRSST data products and services within scientific projects and operational systems - all in real time. Over 20GB of data is exchanged within the international project each day! The challenge for GHRSST version-1 R/GTS and the Science Team that is responsible for its design and implementation is to deliver sustained production of stable, high-quality, SST data products and services and grow the user community. Only a user community can demonstrate a requirement for sustained operations. Once that point is reached, the GHRSST will have achieved its aim.\n"}
{"id": "29159801", "url": "https://en.wikipedia.org/wiki?curid=29159801", "title": "Gordion Furniture and Wooden Artifacts", "text": "Gordion Furniture and Wooden Artifacts\n\nA spectacular collection of furniture and wooden artifacts was excavated by the University of Pennsylvania at the site of Gordion (Latin: Gordium), the capital of the ancient kingdom of Phrygia in the early first millennium BC. The best preserved of these works came from three royal burials, surviving nearly intact due to the relatively stable conditions that had prevailed inside the tomb chambers. The Gordion wooden objects are now recognized as the most important collection of wooden finds recovered from the ancient Near East.\n\nThe group comprises over 100 fine wooden artifacts, including tables, a bed, a throne, serving stands, stools, footstools, plates, spoons, boxes, a parasol, and 12 carved wooden animals. Several pieces of furniture are highly ornate, profusely inlaid with geometric patterns that exhibit sophisticated types of symmetry, and featuring designs that symbolize the Phrygian Mother Goddess Matar (Kybele). The furniture from the largest tomb at Gordion, Tumulus MM, is associated with King Midas, the powerful Phrygian ruler of the eighth century BC.\n\nThe wood has been conserved over a period of 30 years using innovative methods developed by an international team of conservators; these methods are now considered standard for the treatment of dry archaeological wood. The good state of preservation of the Gordion wooden objects has allowed scientists to identify the woods used and investigate their deterioration. Chemical analyses of the residues from the associated bronze vessels have indicated how the bronzes were used with the furniture - and have even determined the menu of a royal Phrygian funerary feast. Several of the most elaborate pieces have been mounted for display in the Museum of Anatolian Civilizations, Ankara, Turkey.\n\nThe central Anatolian site of Gordion was first excavated by Gustav and Alfred Körte in 1900 and then by Professor Rodney S. Young of the University of Pennsylvania in a major campaign between 1950 and 1973. Among the many exceptional Phrygian artifacts recovered were more than one hundred wooden objects dating to the eighth century BC—a rare find, since organic materials seldom survive in buried conditions. The wood discovered by the Körte brothers consisted mainly of furniture fragments from a tumulus burial (K-III), which were significant but too fragmentary to be well understood. Young’s excavations, however, produced a spectacular collection of wooden furniture and other types of objects, many of which were in relatively good condition.These came mainly from three early royal tumuli (Tumulus MM, Tumulus P, and Tumulus W), along with carbonized remains from the destruction level of the city mound. Young died in 1974, before finishing his final report on the excavation of these tombs, but his monograph, Three Great Early Tumuli, The Gordion Excavations Final Reports, Volume 1, was published posthumously by a group of his colleagues, based on his original notes. Three Great Early Tumuli detailed the excavation of the burials and their grave goods in preliminary interpretations and working drawings. During the production of this volume, many of the early interpretations of the wooden artifacts were found to be incorrect, and a reassessment of the finds was begun by Dr. Elizabeth Simpson, now professor of ancient art and archaeology at the Bard Graduate Center, New York, NY.\n\nIn 1981, Simpson went to Turkey to photograph and draw the objects and discovered that the wood was deteriorating. A project to study and conserve the Gordion wooden artifacts was then organized under her direction with the support of the University of Pennsylvania Museum of Archaeology and Anthropology. Work has been carried out since that time by the Gordion Furniture Project team in the Museum of Anatolian Civilizations (Anadolu Medeniyetleri Müzesi), Ankara, Turkey. More than 40 archaeologists, conservators, scientists, and artists have been affiliated with the project since its inception, with funding from the National Endowment for the Humanities, the National Geographic Society, the 1984 Foundation, the Samuel H. Kress Foundation, the Getty Grant Program, and other foundations and individual donors. During the past 30 years, most of the wood has been conserved, and many of the objects have been reconstructed for display in the Museum of Anatolian Civilizations. The collection is now considered to be the largest and most important group of well-preserved ancient wooden artifacts excavated from the Near East.\n\nRodney Young named the largest burial mound at the site Tumulus MM—for “Midas Mound,” after the famous Phrygian king Midas, who ruled at Gordion during the second half of the eighth century B.C.Young eventually came to believe that the tomb’s occupant was not Midas but rather his father, although in either case the wooden finds from the burial can be associated with King Midas. When Midas took the throne on the death of his father, he surely would have officiated at the funeral, with the grave goods provided by him for the deposition. The historical King Midas was a contemporary of the Assyrian king Sargon II (r. 721–705 BC) and was also well known to the Greeks. Midas was the first foreigner to make an offering at the sanctuary of Apollo at Delphi, according to the Greek historian Herodotus, who wrote that the king had dedicated his throne, which was “well worth seeing.” Unfortunately, this tantalizing comment tells us little about the appearance of the throne of Midas. However, the magnificent wooden furniture from Tumulus MM at Gordion suggests that the throne was likely made from beautiful woods with carved and inlaid decoration, including a complex of geometric patterns that reflected the power of the Phrygian royal house and its connection to the Phrygian mother goddess Matar (Kybele).Tumulus MM produced an array of priceless artifacts: 170 bronze vessels, some with deposits of organic residues; 10 bronze-and-leather belts; more than 180 bronze fibulae (ancient safety pins); pottery vessels containing the remains of food; and 15 pieces of fine wooden furniture. The exact nature of the furniture was not clear to Young and his team when they entered the chamber in 1957, as many of the objects had broken along the lines of the joinery, with their constituent parts dispersed on the tomb floor. The first thing the excavators saw was the skeleton of the king lying on a mass of textiles, covering what Young thought was a four-poster bed. To the east of the “bed” were several furniture legs scattered among remnants of cloth. Leaning against the east wall were two ornate, inlaid objects, called “screens” by Young and initially identified as “throne backs.” To the south of these, a fancy inlaid table had collapsed with its frame intact, and nearby were the tops and legs of eight “plain” tables. Most of the bronze vessels found in the tomb had once been hung on the walls or placed on the tables, but all had fallen to the floor.\n\n The inlaid table, called the “Pagoda Table” by Young because of its exotic design and decoration, was made of 40 major wooden components.\n\nThe table had three legs, with lion-paw feet, and three structural supports that rose from the feet to prop up the frame at its corners. The four-sided frame was carved as a series of panels, connected by double bars, and inlaid with geometric patterns such as fields of squares or diamonds, configurations of hooks, rosette-like designs, and mazes. Some of these patterns appear elsewhere as markings on early figurines and clothing, and survive on women’s dowry textiles and ritual cloths in Europe and Anatolia up to the present. Evidence suggests that these designs were protective and empowering, with connotations of procreation and fertility. The inlaid table was thus decorated with apotropaic and magical imagery, suggesting that it may have had a ritual function. It was also a practical, portable banquet table, with four handles and a tray-shaped top. Boxwood was used for the frame and legs, juniper for the inlay, and walnut for the table top. In 1983, the table was reconstructed for display in the Museum of Anatolian Civilizations, where it could be seen assembled for the first time in 2700 years. The mount was refurbished and the display reinstalled in 1989.\n\nThe other eight tables found in the tomb were simpler versions of the inlaid table. Each had three curved boxwood legs and a tray-shaped top. The tops of seven of the tables were made of walnut; the wood of the top of the eighth table was probably maple or cherry. Contrary to Young’s earlier supposition, the legs were not steam bent but were made from naturally curved or trained branches. Tenons at the tops of the legs were fit into “collars” that extended down from the lower surface of the table tops. Research has shown that this system of joinery was widespread in antiquity, attested in the Middle Bronze Age tombs of Jericho and in the Pazyryk burials of the fourth century BC.\n\n Young's “screens” were made of boxwood, inlaid with juniper, with walnut top pieces and curved “legs” set into the front faces. At the center of each face was an inlaid rosette, supported by two curved legs with stylized lion-paw feet.\n\nThese elements were set within a grid of inlaid square designs surrounded by thousands of tiny diamonds and triangles. Most of the square designs were symmetrical with respect to rotations of 180 degrees, which allowed several basic designs to be turned and flipped to form derivative versions, adding complexity and obscuring the underlying patterns in which the basic designs had been arranged. This surprising play with symmetry indicates that the Phrygian woodworkers were clever, imaginative artisans with a pronounced mathematical orientation.\n\nThis impression was confirmed by an analysis of the prominent designs at the center of the screen faces, which have now been identified as religious symbols. The rosette represented the Phrygian goddess Matar and the curved legs her two attendant lions, in an abstract version of the figures in the niche of Arslan Kaya, the monumental rock-cut shrine of the Phrygian highlands. The Tumulus MM screens thus took the form of portable shrines of the goddess Matar.\n\nEach “screen” had a top piece, supported by a back leg and diagonal struts. Research has now shown that the “screens” were not “throne backs” but serving stands. The top pieces featured large wooden rings that had held small bronze cauldrons, ten of which were found nearby in the tomb. Also found near the stands were two bronze ladles, which had undoubtedly been used to transfer the contents of the cauldrons into other vessels. The Tumulus MM serving stands have been assembled for display in the Museum of Anatolian Civilizations.\n\n At the north of the chamber was the king’s “bed,” which has now been identified as an open log coffin, carved from a huge cedar log, with ledges extending out at both ends. Four large pine blocks had braced the coffin body, and inlaid rails were socketed into the sides. This new information led to a fuller understanding of the circumstances of the king’s burial. The remains of the coffin, as disposed on the tomb floor, showed that a funeral ceremony had taken place before the interment: the coffin had clearly been assembled elsewhere before the burial, then disassembled, and its parts placed in the chamber in something other than their original positions.\n\nThe contents of the tomb, which were largely banquet furnishings, must have been used for the ceremony. Analysis of the food and drink residues produced the menu of the funerary banquet. The feast included a spicy stew, made with barbecued sheep or goat, honey, wine, olive oil, and most likely lentils, seasoned with anise or fennel. The small bronze cauldrons that sat in the tops of the serving stands had contained a mixed fermented beverage of grape wine, barley beer, and honey mead. The beverage served at the king’s funeral has been reconstructed by Dogfish Head Brewery as the award-winning “Midas Touch,” which is now widely available. This collaborative research allowed the funeral ceremony before the burial to be reconstructed in a painting by Greg Harlin.\n\n This rather complete picture has aided in the interpretation of the Tumulus P and Tumulus W burials. These tombs produced wonderful wooden finds, but they are largely fragmentary because the roofs of both chambers had collapsed, crushing the contents within. Tumulus P, excavated in 1956, covered the tomb of a small child, who was buried with bronze vessels, fibulae, and belts; iron implements; painted and monochrome pottery; and a glass bowl—as well as 21 or more pieces of furniture and 49 other wooden objects.\n\nThe wood finds included a “screen,” which, like its Tumulus MM counterparts, was actually a serving stand. The top piece, back leg, and face of the stand were made of boxwood. The face was carved in openwork and inlaid with juniper and yew in a profusion of geometric patterns. As with the Tumulus MM serving stands, this stand too featured a large rosette at the center supported by abstract lion legs, suggesting that it had also represented a shrine of the Phrygian goddess Matar.\n\nFound near the stand in the southwest corner of the tomb were pieces of a carved wooden stool, inlaid in geometric patterns and studded with bronze tacks. The stool was a colorful production, assembled from alternating strips of boxwood and yew. The boxwood strips, where inlaid, were inlaid with yew, and one yew strip was inlaid with boxwood. The two faces were joined at the top by undecorated strips, and at the bottom by two stretchers, carved on their top and outer faces. The stool was reconstructed on a Plexiglas mount (1993) and is now on display in the Museum of Anatolian Civilizations. The design of this stool is unusual, and can only be understood in reference to the Tumulus MM inlaid table. The two faces of the stool were conceived with such a table in mind, representing the main features of the three-dimensional table “collapsed” into two dimensions in a clever intellectual conceit. Not content with this amusing play with form and dimension, the cabinetmakers then negated the force of the metaphor by alternating the colored woods, producing the impression of a series of dark and light stripes.\n\nMany other interesting pieces of furniture were recovered from the Tumulus P burial. The “Tripod Tray Table,” found at the north of the chamber, was named for its massive, boxwood tray-shaped top. The table had three curved legs, each with a stylized lion-paw foot and a large ring at the top. Nearby were the remains of another fancy table, called the “Mosaic Table” by Young because of its boldly inlaid table top. The top was made of boxwood boards, joined edge to edge and inlaid with strips of yew in a pattern of squares and crosses. Also found in the tomb were two plain tables, six or more stools, two footstools, a small chair or throne, and a carved and inlaid bed. Other wooden finds include a parasol, a box, eight spoons or ladles, and 23 plates and bowls. Among the most charming of the wooden objects from Tumulus P are 12 small animals, including two lions, a lion and bull in combat, a griffin eating a fish, two bulls, and a leaping ram or goat—toys for the tomb’s young royal occupant.\n\nThe Tumulus W burial, excavated in 1959, contained bronze and pottery vessels, bronze fibulae, a bronze-and-leather belt, and fragments of several wooden objects. These included the remnants of a “screen,” which was apparently a serving stand like those from the Tumulus MM and Tumulus P burials. Its front face was carved in openwork and studded with bronze tacks. Although the “screen” is only partially preserved, it had clearly been an extraordinary piece of furniture. Five or more wooden plates from the tomb are of particular interest because of their method of manufacture. Close examination has revealed tool marks indicating that the plates were made on an ancient reciprocating lathe—the earliest evidence for the lathe from a securely dated archaeological context.\n\n The destruction level of the city mound yielded carbonized fragments of what may be a serving stand, an inlaid table, and other elaborate pieces of furniture, fitted with bronze studs and brackets as well as finely carved ivory plaques. Recent scientific advances have allowed the charcoal to be identified, indicating that boxwood, maple, yew, oak, and pine are present. Research into the form of the city mound objects is currently underway; consolidation of the carbonized wood has been deemed unadvisable and will not be undertaken.\n\n Conservation of the wooden artifacts from Gordion has been ongoing since 1981, initiated by Robert Payton, now conservator at the Museum of London. Since 1990, work has been carried out by a team of conservators, conservation scientists, preparators, and graduate student interns in the Museum of Anatolian Civilizations, Ankara, under the direction of Professor Krysia Spirydowicz, Queen’s University, Kingston, Ontario. The preservation method developed by Payton was adapted and refined to treat over 40 pieces of furniture and more than 50 wooden objects from the three tumuli at Gordion. Study of the artifacts revealed that the wood was very fragile when handled. Since the wood was very light in weight and retained little of its original cellular structure, consolidation was necessary in order to strengthen the wood, using a dilute solution of Butvar B-98, a synthetic polymer based on polyvinyl butyral. The same polymer was utilized in more concentrated form to carry out repairs; for particularly fragmentary objects, glass microspheres were added to the solution to make a paste for filling gaps and reinforcing joins.\n\nA wood species analysis was begun in 1983 by Professor Burhan Aytuğ of Istanbul University, and is now under the direction of Professor Robert Blanchette, Department of Wood Pathology, University of Minnesota. Robert Blanchette is also supervising a wood pathology analysis, which aims to understand the types of degradation present in the wood and the causes of the decay. The graffiti on the serving stands from Tumulus MM have been studied by Professor Lynn Roller, University of California, Davis. The textiles associated with the furniture from Tumulus MM are the subject of a collaborative research project carried out at the Museum Conservation Institute, Smithsonian Institution, headed by Mary Ballard. The menu of the funerary feast that took place before the Tumulus MM burial was determined by Dr. Patrick McGovern and his colleagues, and the mixed fermented beverage was created through the efforts of Pat McGovern and Sam Calagione of Dogfish Head Brewery.\n\nResearch on the wooden objects from the Gordion tombs is the subject of numerous publications (see \"References\" below for a selection). Thirty years of study have yielded extensive information about the woods, tools, and techniques used by the royal cabinetmakers, who were surely among the greatest craftsmen of their time. The Phrygian artisans, while working well within ancient traditions of wood carving and joinery, were outstanding in their virtuoso use of wood inlay, their abstraction of natural forms, and their elaborate play with symmetry and design. The many fine wooden objects excavated at Gordion may serve to indicate the magnitude of our loss of most wooden artifacts from the archaeological record, and to remind us of the importance of organic materials for an appreciation of the arts of antiquity.\n\n\n"}
{"id": "176828", "url": "https://en.wikipedia.org/wiki?curid=176828", "title": "Gravity Research Foundation", "text": "Gravity Research Foundation\n\nThe Gravity Research Foundation is an organization established in 1948 by businessman Roger Babson (founder of Babson College) to find ways to implement gravitational shielding. It holds an annual contest rewarding essays by scientific researchers on gravity-related topics. The contest, which awards prizes of up to $4,000, has been won by at least five people who later won the Nobel Prize in physics.\n\nThe foundation held conferences and conducted operations in New Boston, New Hampshire through the late 1960s, but that aspect of its operation ended following Babson's death in 1967.\nIt is mentioned on stone monuments, donated by Babson, at more than a dozen American universities.\n\nThomas Edison apparently suggested the creation of the Gravity Research Foundation to Babson, who established it in several scattered buildings in the small town of New Boston, New Hampshire. Babson said he chose that location because he thought it was far enough from big cities to survive a nuclear war. Babson wanted to put up a sign declaring New Boston to be the safest place in North America if World War III came, but town fathers toned it down to say merely that New Boston was a safe place.\n\nIn an essay titled \"Gravity - Our Enemy Number One\", Babson indicated that his wish to overcome gravity dated from the childhood drowning of his sister. \"She was unable to fight gravity, which came up and seized her like a dragon and brought her to the bottom,\" he wrote.\n\nThe foundation held occasional conferences that drew such people as Clarence Birdseye of frozen-food fame and Igor Sikorsky, inventor of the helicopter. Sometimes, attendees sat in chairs with their feet higher than their heads, to counterbalance gravity. Most of the foundation's work, however, involved sponsoring essays by researchers on gravity-related topics. It had only a couple of employees in New Boston.\n\nThe physical Gravity Research Foundation disappeared some time after Babson's death in 1967. Its only remnant in New Boston is a granite slab in a traffic island that celebrates the foundation's \"active research for antigravity and a partial gravity insulator.\" The building that held the foundation's meetings has long held a restaurant, and for a time had a bar called Gravity Tavern, since renamed. \n\nThe essay award lives on, offering prizes of up to $4,000. As of 2018, it is still administered out of Wellesley, Massachusetts by George Rideout, Jr., son of the foundation's original director. \n\nOver time, the foundation shed its crankish air, turning its attention from trying to block gravity to trying to understand it. The annual essay prize has drawn respected researchers, including physicist Stephen Hawking, who won in 1971, mathematician/author Roger Penrose, who won in 1975, and astrophysicist and Nobel laureate George Smoot, who won in 1993. Other notable award winners include Jacob Bekenstein, Sidney Coleman, Bryce DeWitt, Julian Schwinger (Nobel Prize in Physics, 1965), Martin Perl (Nobel Prize in Physics, 1995), Demetrios Christodoulou, Dennis Sciama, Gerard 't Hooft (Nobel Prize in Physics, 1999), Robert Wald, John Archibald Wheeler and Frank Wilczek (Nobel Prize in Physics, 2004).\n\nIn the 1960s, Babson gave grants to a number of colleges that were accompanied by stone monuments. The monuments are inscribed with a variety of similar sayings, such as \"It is to remind students of the blessings forthcoming when a semi-insulator is discovered in order to harness gravity as a free power and reduce airplane accidents\" and \"It is to remind students of the blessings forthcoming when science determines what gravity is, how it works, and how it may be controlled.\"\n\nColleges that received monuments include:\n\nHobart College's \"H-Book\" contains a description of the circumstances surrounding the placement of its Gravity Monument: \"The location of the stone on campus was linked to a gift to the Colleges of 'gravity grant' stocks, now totaling more than $1 million, from Roger Babson, the founder of Babson College. The eccentric Babson was intrigued by the notion of anti-gravity and inclined to further scientific research in this area. The Colleges used these funds to help construct Rosenberg Hall in 1994. Two trees that shade the stone are said to be direct descendants of Newton’s famous apple tree.\"\n\nThe stone at Colby College was once in front of the Keyes Building on the main academic quadrangle but was moved to a more obscure location near the Schair-Swenson-Watson Alumni Center. Students would often knock it over in an ironic testament to gravity's power. At Tufts, the monument is the site of an \"inauguration ceremony\" for students who receive Ph.D.s in cosmology, in which a thesis advisor drops an apple on the student's head.\n\n\n\n"}
{"id": "3330632", "url": "https://en.wikipedia.org/wiki?curid=3330632", "title": "Ground effect train", "text": "Ground effect train\n\nA ground effect train is an alternative to a magnetic levitation (maglev) train. In both cases the objective is to prevent the vehicle from making contact with the ground. Whereas a maglev train accomplishes this through the use of magnetism, a ground effect train uses an air cushion; either in the manner of a hovercraft (as in hovertrains) or using the \"wing-in-ground\" design.\n\nThe advantages of a ground effect train over a maglev are lower cost due to simpler construction. Disadvantages include either constant input of energy to keep the train hovering (in the case of hovercraft-like vehicles) or the necessity to keep the vehicle moving for it to remain off the ground (in the case of Wing In Ground effect vehicles). Furthermore, these vehicles may be very drastically affected by wind, air turbulence, and weather. Whereas the magnetic levitation train can be built to operate in a vacuum to minimise air resistance, the ground effect train must operate in an atmosphere in order for the air cushion to exist.\n\nDevelopment work has been undertaken in several countries since the middle 20th century. No ground effect train has entered regular commercial service.\n\n\n\n"}
{"id": "9234492", "url": "https://en.wikipedia.org/wiki?curid=9234492", "title": "HangBoard", "text": "HangBoard\n\nA HangBoard is a device used in the snow sport of hangboarding.\n\nHangBoard development began in 2001, invented by Canadian Don Arney. Others involved in the development process were Canadian designer Peter Brooke and American designer Charles Buchwald, and Canadian snowboard and mountain biking champion Everest MacDonald.\n\nA HangBoard bolts to a standard snowboard in place of bindings, and the pilot — wearing a harness — hangs from a T-shaped bar. Arms push against handlebars in the front, and feet clamp into rudders at the back. Most control occurs by weight shifting, but pilots can also achieve directional control by using the foot rudders. Applied simultaneously, the rudders function as brakes. The HangBoard frame is made of aircraft aluminum, weighs less than 11 kg without the snowboard, and measures long, high when folded up, and wide.\n\nHangBoard pilots ride the ski lifts and descend the mountains like skiers and snowboarders. When loading onto a ski lift, the chair catches the upper part of the HangBoard, which then rests on the seat beside the pilot. A tether ensures the board cannot escape the lift.\nWhen departing the lift, the pilot holds onto the HangBoard's upper frame.\n\nHangBoard development began in 2001, invented by Canadian Don Arney. Others involved in the development process were Canadian designer Peter Brooke and American designer Charles Buchwald, and Canadian snowboard and mountain biking champion Everest MacDonald.\n\n"}
{"id": "36644955", "url": "https://en.wikipedia.org/wiki?curid=36644955", "title": "Hans Weinreich", "text": "Hans Weinreich\n\nHans Weinreich (1480/1490 – 1566) was a publisher and printer of German and Polish language books in the first half of the sixteenth century. Weinreich was originally from Danzig (Gdańsk) in Royal Prussia, Kingdom of Poland, and then moved to Königsberg (Królewiec) in Ducal Prussia at the invitation of Albert of Prussia.\n\nWeinreich was most likely born in Danzig sometime between 1480 and 1490, to a well-to-do family. His grandfather shared his name and had four sons, of whom one was Hans' father.\n\nWeinreich's printshop in Königsberg was supposedly located by the castle steps in the city's Old Town.\n\nWeinreich issued his first works in either 1512 or 1513 and moved to Königsberg in 1524. In 1524 he began issuing works in Polish. In 1530 Weinreich published the first translation of Luther's Small Catechism made by an anonymous author. However, the language of the translation was poor, and in 1533 Weinreich printed a second edition made by the Polish philologist Liboriusz Schadlika. Weinreich also issued several works by the Polish Lutheran theologian and translator Jan Seklucjan, although some of these were actually translations done by Stanisław Murzynowski. The published works were translations into Polish of the New Testament and the Catechism, as well as literature and song books. Seklucjan's works printed by Weinreich included the first Polish language hymnal.\n\nWeinreich also printed the first Lithuanian language and Old Prussian language books while in Königsberg, with the former being a translation of Martin Luther's Catechism by Martynas Mažvydas. The first print run of Mažvydas' work numbered between 200 and 300, although only one known copy has survived.\n\nBetween 1524 and 1553 Weinreich printed 103 works, of which 59 were in German, 27 in Latin, 13 in Polish and four in either Prussian or Lithuanian.\n"}
{"id": "1543683", "url": "https://en.wikipedia.org/wiki?curid=1543683", "title": "High-definition video", "text": "High-definition video\n\nHigh-definition video is video of higher resolution and quality than standard-definition. While there is no standardized meaning for \"high-definition\", generally any video image with considerably more than 480 vertical lines (North America) or 576 vertical lines (Europe) is considered high-definition. 480 scan lines is generally the minimum even though the majority of systems greatly exceed that. Images of standard resolution captured at rates faster than normal (60 frames/second North America, 50 fps Europe), by a high-speed camera may be considered high-definition in some contexts. Some television series shot on high-definition video are made to look as if they have been shot on film, a technique which is often known as filmizing.\n\nThe first electronic scanning format, 405 lines, was the first \"high definition\" television system, since the mechanical systems it replaced had far fewer. From 1939, Europe and the US tried 605 and 441 lines until, in 1941, the FCC mandated 525 for the US. In wartime France, René Barthélemy tested higher resolutions, up to 1,042. In late 1949, official French transmissions finally began with 819. In 1984, however, this standard was abandoned for 625-line color on the TF1 network.\n\nModern HD specifications date to the early 1980s, when Japanese engineers developed the HighVision 1,125-line interlaced TV standard (also called MUSE) that ran at 60 frames per second. The Sony HDVS system was presented at an international meeting of television engineers in Algiers, April 1981 and Japan's NHK presented its analog HDTV system at a Swiss conference in 1983.\n\nThe NHK system was standardized in the United States as Society of Motion Picture and Television Engineers (SMPTE) standard #240M in the early 1990s, but abandoned later on when it was replaced by a DVB analog standard. HighVision video is still usable for HDTV video interchange, but there is almost no modern equipment available to perform this function. Attempts at implementing HighVision as a 6 MHz broadcast channel were mostly unsuccessful. All attempts at using this format for terrestrial TV transmission were abandoned by the mid-1990s.\n\nEurope developed HD-MAC (1,250 lines, 50 Hz), a member of the MAC family of hybrid analogue/digital video standards; however, it never took off as a terrestrial video transmission format. HD-MAC was never designated for video interchange except by the European Broadcasting Union.\n\nThe current high-definition video standards in North America were developed during the course of the advanced television process initiated by the Federal Communications Commission in 1987 at the request of American broadcasters. In essence, the end of the 1980s was a death knell for most analog high definition technologies that had developed up to that time.\n\nThe FCC process, led by the Advanced Television Systems Committee (ATSC) adopted a range of standards from interlaced 1,080-line video (a technical descendant of the original analog NHK 1125/30 Hz system) with a maximum frame rate of 30 Hz, (60 fields per second) and 720-line video, progressively scanned, with a maximum frame rate of 60 Hz.\nIn the end, however, the DVB standard of resolutions (1080, 720, 480) and respective frame rates (24, 25, 30) were adopted in conjunction with the Europeans that were also involved in the same standardization process. The FCC officially adopted the ATSC transmission standard (which included both HD and SD video standards) in 1996, with the first broadcasts on October 28, 1998.\n\nIn the early 2000s, it looked as if DVB would be the video standard far into the future. However, both Brazil and China have adopted alternative standards for high-definition video that preclude the interoperability that was hoped for after decades of largely non-interoperable analog TV broadcasting.\n\nHigh definition video (prerecorded and broadcast) is defined threefold, by:\n\nOften, the rate is inferred from the context, usually assumed to be either 50 Hz (Europe) or 60 Hz (USA), except for 1080p, which denotes 1080p24, 1080p25, and 1080p30, but also 1080p50 and 1080p60.\n\nA frame or field rate can also be specified without a resolution. For example, 24p means 24 progressive scan frames per second and 50i means 25 progressive frames per second, consisting of 50 interlaced fields per second. Most HDTV systems support some standard resolutions and frame or field rates. The most common are noted below.\nHigh-definition signals require a high-definition television or computer monitor in order to be viewed. High-definition video has an aspect ratio of 16:9 (1.78:1). The aspect ratio of regular widescreen film shot today is typically 1.85:1 or 2.39:1 (sometimes traditionally quoted at 2.35:1). Standard-definition television (SDTV) has a 4:3 (1.33:1) aspect ratio, although in recent years many broadcasters have transmitted programs \"squeezed\" horizontally in 16:9 anamorphic format, in hopes that the viewer has a 16:9 set which stretches the image out to normal-looking proportions, or a set which \"squishes\" the image vertically to present a \"letterbox\" view of the image, again with correct proportions.\n\nNote: Image is either a frame or, in case of interlaced scanning, two fields. (EVEN and ODD)\n\nAlso, there are less common but still popular UltraWide resolutions, such as 2560×1080p (1080p UltraWide).\n\nHigh-definition image sources include terrestrial broadcast, direct broadcast satellite, digital cable, high definition disc (BD), digital cameras, Internet downloads, and video game consoles.\n\n\nBlu-ray Discs were jointly developed by 9 initial partners including Sony and Phillips (which jointly developed CDs for audio), and Pioneer (which developed its own Laser-disc previously with some success) among others. HD-DVD discs were primarily developed by Toshiba and NEC with some backing from Microsoft, Warner Bros., Hewlett Packard, and others. On February 19, 2008 Toshiba announced it was abandoning the format and would discontinue development, marketing and manufacturing of HD-DVD players and drives.\n\nThe high resolution photographic film used for cinema projection is exposed at the rate of 24 frames per second but usually projected at 48, each frame getting projected twice helping to minimise flicker. One exception to this was the 1986 National Film Board of Canada short film \"Momentum\", which briefly experimented with both filming and projecting at 48 frame/s, in a process known as IMAX HD.\n\nDepending upon available bandwidth and the amount of detail and movement in the image, the optimum format for video transfer is either 720p24 or 1080p24. When shown on television in PAL system countries, film must be projected at the rate of 25 frames per second by accelerating it by 4.1 percent. In NTSC standard countries, the projection rate is 30 frames per second, using a technique called 3:2 pull-down. One film frame is held for three video fields (1/20 of a second), and the next is held for two video fields (1/30 of a second) and then the process is repeated, thus achieving the correct film projection rate with two film frames shown in 1/12 of a second. \n\nOlder (pre-HDTV) recordings on video tape such as Betacam SP are often either in the form 480i60 or 576i50. These may be upconverted to a higher resolution format (720i), but removing the interlace to match the common 720p format may distort the picture or require filtering which actually reduces the resolution of the final output.\n\nNon-cinematic HDTV video recordings are recorded in either the 720p or the 1080i format. The format used is set by the broadcaster (if for television broadcast). In general, 720p is more accurate with fast action, because it progressively scans frames, instead of the 1080i, which uses interlaced fields and thus might degrade the resolution of fast images.\n\n720p is used more for Internet distribution of high-definition video, because computer monitors progressively scan; 720p video has lower storage-decoding requirements than either the 1080i or the 1080p. This is also the medium for high-definition broadcasts around the world and 1080p is used for Blu-ray movies.\n\nFilm as a medium has inherent limitations, such as difficulty of viewing footage while recording, and suffers other problems, caused by poor film development/processing, or poor monitoring systems. Given that there is increasing use of computer-generated or computer-altered imagery in movies, and that editing picture sequences is often done digitally, some directors have shot their movies using the HD format via high-end digital video cameras. While the quality of HD video is very high compared to SD video, and offers improved signal/noise ratios against comparable sensitivity film, film remains able to resolve more image detail than current HD video formats. In addition some films have a wider dynamic range (ability to resolve extremes of dark and light areas in a scene) than even the best HD cameras. Thus the most persuasive arguments for the use of HD are currently cost savings on film stock and the ease of transfer to editing systems for special effects.\n\nDepending on the year and format in which a movie was filmed, the exposed image can vary greatly in size. Sizes range from as big as 24 mm × 36 mm for VistaVision/Technirama 8 perforation cameras (same as 35 mm still photo film) going down through 18 mm × 24 mm for Silent Films or Full Frame 4 perforations cameras to as small as 9 mm × 21 mm in Academy Sound Aperture cameras modified for the Techniscope 2 perforation format. Movies are also produced using other film gauges, including 70 mm films (22 mm × 48 mm) or the rarely used 55 mm and CINERAMA.\n\nThe four major film formats provide pixel resolutions (calculated from pixels per millimeter) roughly as follows:\n\n\nIn the process of making prints for exhibition, this negative is copied onto other film (negative → interpositive → internegative → print) causing the resolution to be reduced with each emulsion copying step and when the image passes through a lens (for example, on a projector). In many cases, the resolution can be reduced down to 1/6 of the original negative's resolution (or worse). Note that resolution values for 70 mm film are higher than those listed above.\n\nA number of online video streaming/on demand and digital download services offer HD video, among them YouTube, Vimeo, Hulu, Amazon Video On Demand, Netflix Watch Instantly, and others. Due to heavy compression, the image detail produced by these formats are far below that of broadcast HD, and often even inferior to DVD-Video (3-9 Mbit/s MP2) upscaled to the same image size. The following is a chart of numerous online services and their HD offering:\n\nAn increasing number of manufacturers of security cameras now offer HD cameras. The need for high resolution, color fidelity, and frame rate is acute for surveillance purposes to ensure that the quality of the video output is of an acceptable standard that can be used both for preventative surveillance as well as for evidence purposes. These needs, however, must be balanced against the additional storage capacity required by HD video.\n\nBoth the PlayStation 3 game console and Xbox 360 can output native 1080p through HDMI or component cables, but the systems have few games which appear in 1080p; most games only run natively at 720p or less, but can be upscaled to 1080p. The Wii can output up to 480p (enhanced-definition) over component, which while not HD, is very useful for HDTVs as it avoids de-interlacing artifacts. The Wii can also output 576i in PAL regions.\n\nVisually, native 1080p produces a sharper and clearer picture compared to upscaled 1080p. Though only a handful of games available have the native resolution of 1080p, all games on the Xbox 360 and PlayStation 3 can be upscaled up to this resolution. Xbox 360 and PlayStation 3 games are labeled with the output resolution on the back of their packaging, although on Xbox 360 this indicates the resolution it will upscale to, not the native resolution of the game.\n\nGenerally, PC games are only limited by the display's resolution size. Drivers are capable of supporting very high resolutions, depending on the chipset of the video card. Many game engines support resolutions of 5760×1080 or 5760×1200 (typically achieved with three 1080p displays in a multi-monitor setup) and nearly all will display 1080p at minimum. 1440p and 4K are typically supported resolutions for PC gaming as well.\n\nCurrently all consoles, Nintendo's Wii U and Nintendo Switch, Microsoft's Xbox One, and PlayStation 4 display games 1080p natively. The Nintendo Switch is an unusual case, due to its hybrid nature as both a home console and a handheld: the built-in screen displays games at 720p maximum, but the console can natively display imagery at 1080p when docked. PlayStation 4 is able to display in 4K, though strictly only for displaying pictures.\n\n\n\n"}
{"id": "13232165", "url": "https://en.wikipedia.org/wiki?curid=13232165", "title": "High-redundancy actuation", "text": "High-redundancy actuation\n\nHigh-redundancy actuation (HRA) is a new approach to fault-tolerant control in the area of mechanical actuation.\n\nThe basic idea is to use a lot of small actuation elements, so that a fault of one element has only a minor effect on the overall system. This way, a High Redundancy Actuator can remain functional even after several elements are at fault. This property is also called graceful degradation.\n\nFault-tolerant operation in the presence of actuator faults requires some form of redundancy. Actuators are essential, because they are used to keep the system stable and to bring it into the desired state. Both requires a certain amount of power or force to be\napplied to the system. No control approach can work unless the actuators produce this necessary force.\n\nSo the common solution is to err on the side of safety by over-actuation: much more control action than strictly necessary is built into the system. For critical systems, the normal approach involves straightforward replication of the actuators. Often three or four actuators are used in parallel for aircraft flight control systems, even if one would be\nsufficient from a control point of view. So if one actuator fails, the remaining actuator can always keep the system operation. While this approach certainly successful, it also makes the system expensive, heavy and ineffective.\n\nThe idea of the high-redundancy actuation (HRA) is inspired by the human musculature. A muscle is composed of many individual muscle cells, each of which provides only a minute contribution to the force and the travel of the muscle. These properties allow the muscle as a whole to be highly resilient to damage of individual cells.\n\nThe aim of high redundancy actuation is not to produce man-made muscles, but to use the same principle of cooperation in technical actuator's to provide intrinsic fault tolerance. To achieve this, a high number of small actuator elements are assembled in parallel and in series to form one actuator (see Series and parallel circuits).\n\nFaults within the actuator will affect the maximum capability, but through robust control, full performance can be maintained without either adaptation or reconfiguration. Some form of condition monitoring is necessary to provide warnings to the operator calling for\nmaintenance. But this monitoring has no influence on the system itself, unlike in adaptive methods or control reconfiguration, which simplifies the design of the system significantly.\n\nThe HRA is an important new approach within the overall area of fault-tolerant control,\nusing concepts of reliability engineering on a mechanical level. When applicable, it can provide actuators that have graceful degradation, and that continue to operate at close to nominal performance even in the presence of multiple faults in the actuator elements.\n\nAn important feature of the high-redundancy actuation is that the actuator elements are connected both in parallel and in series. While the parallel arrangement is commonly used, the configuration in series is rarely employed, because it is perceived to be less efficient.\n\nHowever, there is one fault that is difficult to deal with in a parallel arrangement: the locking up of one actuator element. Because parallel actuator elements always have the same extension, one locked-up element can render the whole assembly useless. It is possible to mitigate this by guarding the elements against locking or by limiting the force exerted by a single element. But these measures reduce both the effectiveness of the system and introduce new points of failure.\n\nThe analysis of the serial configuration shows that it remains operational when one element is locked-up. This fact is important for the High Redundancy Actuator, as fault tolerance is required for different fault types. The goal of the HRA project is to use parallel and serial actuator elements to accommodate both the blocking and the inactivity (loss of force) of an element.\n\nThe basic idea of high-redundancy actuation is technology agnostic: it should be applicable to a wide range of actuator technology, including different kinds of linear actuators and rotational actuators.\n\nHowever, initial experiments are performed with electric actuators, especially with electromechanical and electromagnetic technology. Compared to pneumatic actuators, the electrical drive allow a much finer control of position and force.\n\n\n"}
{"id": "16091265", "url": "https://en.wikipedia.org/wiki?curid=16091265", "title": "HomeLink Wireless Control System", "text": "HomeLink Wireless Control System\n\nThe HomeLink Wireless Control System is a radio frequency (RF) transmitter integrated into some automobiles that can be programmed to activate devices such as garage door openers, RF-controlled lighting, gates and locks, including those with rolling codes.\n\nThe system features three buttons, most often found on the driver-side visor or on the overhead console, which can be programmed via a training sequence to replace existing remote controls. It is compatible with most RF-controlled garage door openers, as well as home automation systems such as those based on the X10 protocol.\n\nIt won the PACE Award in 1997, for supplying automotive technology to improve interaction between the consumer, the car, and home. By 2003, it had been installed on over 20,000,000 automobiles. Originally made by Johnson Controls, the HomeLink product line was sold to Gentex in 2013.\n"}
{"id": "28723408", "url": "https://en.wikipedia.org/wiki?curid=28723408", "title": "IEC 62304", "text": "IEC 62304\n\nThe international standard IEC 62304 – medical device software – software life cycle processes is a standard which specifies life cycle requirements for the development of medical software and software within medical devices. It is harmonized by the European Union (EU) and the United States (US), and therefore can be used as a benchmark to comply with regulatory requirements from both these markets.\n\nThe IEC 62304 standard calls out certain cautions on using software, particularly SOUP (software of unknown pedigree or provenance). The standard spells out a risk-based decision model on when the use of SOUP is acceptable, and defines testing requirements for SOUP to support a rationale on why such software should be used.\n\n\n\n\n\n\n\n"}
{"id": "22714344", "url": "https://en.wikipedia.org/wiki?curid=22714344", "title": "International Council for Information Technology in Government Administration", "text": "International Council for Information Technology in Government Administration\n\nThe International Council for Information Technology in Government Administration (ICA) is a non-profit making organisation which promotes the information exchange of knowledge, ideas and experiences between central government information technology authorities.\n\nThe ICA was established in 1968.\n\n"}
{"id": "57235848", "url": "https://en.wikipedia.org/wiki?curid=57235848", "title": "Janet Conrad", "text": "Janet Conrad\n\nJanet M. Conrad is an American experimental physicist, researcher, and professor at MIT studying elementary particle physics. \nHer work focuses on neutrino properties and the techniques for studying them. \nIn recognition of her efforts, Conrad has been the recipient of several highly prestigious awards during her career, including an Alfred P. Sloan Research Fellow, a Guggenheim Fellow, and the American Physical Society Maria Goeppert-Mayer Award.\n\nConrad obtained a physics B.A. from Swarthmore College in 1985. She then went to Oxford University to complete a M.Sc. in High Energy Physics as a member of the European Muon Collaboration in 1987 then to Harvard University to complete a PhD in High Energy Physics in 1993.\n\nFollowing Conrad's sophomore year at Swarthmore, she spent her summer in Cambridge, Massachusetts working with Francis Pipkin at Harvard, at her uncle's suggestion. \nThe following summer, Conrad worked with him at Fermilab.\n\nAfter graduating from Harvard in 1993, Conrad took a position as a postdoctoral research associate at the Nevis Laboratories, operated by Columbia University. \nIn 1995, she joined the Columbia Physics department as an Assistant Professor. \nIn 1996 she was awarded the DOE Outstanding Junior Investigator Award for a study entitled \"Construction of a Decay Channel for the NuTeV Experiment at Fermilab\"\nShe gained tenure at Columbia in 1999.\nIn 2002, she was nominated by the American Physical Society's Division of Particles and Fields for fellowship with the APS, citing \"her leadership in experimental neutrino physics, particularly for initiating and leading the NuTeV decay channel experiment and the Mini-BooNe neutrino oscillations experiment\". \nFrom 2005 until 2008, Conrad was a Columbia Distinguished Faculty Fellow, and was promoted to the endowed position of Walter O. Lecroy Professor in 2006. \nIn 2008, Conrad left Columbia to join the MIT Physics Department as a Professor.\n\nConrad is a member of several physics collaborations, including MicroBooNE, DAEδALUS, Short-Baseline Near Detector (SBND), and IceCube. \nShe was previously a member of \nDouble Chooz (2006-2014),\nSciBooNE (2005-2011),\nMiniBooNE (1997-2014),\nCCFR/NuTeV (1993-2001),\nE665 (1984-1996), and\nEMC (1985-1986).\n\nIn addition, she has acted as a spokesperson for IsoDAR/DAEdALUS\n\nand MiniBooNE, of which she was a founding member.\n\nIn 2012, Conrad took part in a panel with the World Science Festival, speaking to the public about neutrinos\n\nInspired by detector development efforts while working on IceCube\nConrad took part in the development of a low-cost tabletop muon detector \n\nIn 2015, Conrad and fellow MIT professor Lindley Winslow were consulted as experts in the culture and science of physics for the 2016 film Ghostbusters\n\nJanet Conrad was born October 23, 1963, in Wooster, Ohio. \nShe was a member of 4-H as a child in Ohio.\n\nConrad is the niece of chemistry Nobel Laureate William Lipscomb.\n\nConrad is married to fellow physicist Vassili Papavassiliou, a professor at New Mexico State University\n\n\n"}
{"id": "7719913", "url": "https://en.wikipedia.org/wiki?curid=7719913", "title": "Levi's Stadium", "text": "Levi's Stadium\n\nLevi's Stadium is a football stadium located in Santa Clara, California, in the San Francisco Bay Area. It has served as the home venue for the National Football League (NFL)'s San Francisco 49ers since 2014. The stadium is located approximately south of San Francisco and is named for Levi Strauss & Co., which purchased naming rights in 2013.\n\nIn 2006, the 49ers initially proposed constructing a new stadium at Candlestick Point in San Francisco, the site of their existing home, Candlestick Park. The project, which included plans for retail space and housing improvements, was claimed to be of great potential benefit to the nearby historically blighted neighborhood of Hunters Point. After negotiations with the city of San Francisco fell through, the 49ers focused their attention on a site adjacent to their administrative offices and training facility in Santa Clara.\n\nIn June 2010, Santa Clara voters approved a measure authorizing the creation of the Santa Clara Stadium Authority, a tax-exempt public authority, to build and own the new football stadium and for the city government to lease land to the Santa Clara Stadium Authority. A construction loan, raised from private investors, was secured in December 2011, allowing construction to start in April 2012. Levi's Stadium opened on July 17, 2014.\n\nLevi's Stadium hosted Super Bowl 50 on February 7, 2016. Levi's Stadium also serves as the site of the Pac-12 Football Championship Game since 2014. Previously, the game was played at the home stadium of the division winner with the better record entering the game.\n\nThe stadium was designed by HNTB, an internationally renowned architectural firm, with a focus on creating a multi-purpose venue and with the fan experience and green technology as top priorities. Civil engineering work was performed by Winzler & Kelly, which was acquired by GHD Group in 2011. Commissioning services were provided by Glumac.\n\nLevi's Stadium is designed as an open stadium with a natural grass field. It has a seating capacity of 68,500, expandable to approximately 75,000 to host major events like the Super Bowl and the FIFA World Cup. However, on June 27, 2015, The Grateful Dead Fare Thee Well Tour made history by extending the stadium to 83,000 in attendance. The seating design of the stadium places approximately two-thirds of the fans in the lower bowl, which is one of the largest of its kind in the entire NFL. The design features significantly improved accessibility and seating options for fans with special needs and disabilities when compared to Candlestick Park. The configuration is similar to Ford Field, home of the NFL's Detroit Lions, with the majority of the luxury suites on one side of the field, which puts the fans in the upper deck closer to the action.\n\nAs a multi-use facility, the stadium can be configured for special touring events including concerts, motocross events, indoor/outdoor conferences, and other community events. The stadium is also designed to meet the FIFA field geometry requirements for international soccer, which will allow it to host international friendly matches and major tournaments such as the FIFA World Cup. The stadium will also feature over of flexible premium meeting space in the club areas.\nThe stadium has created an in stadium app designed specifically for home football games for the 49ers to provide a better fan experience for fans and guests. The app can be downloaded for free off of the App Store and Google Play. Features are limited on non-football game days or if one is outside the vicinity of the stadium. However, when having the app within the stadium on game and event days one has many options including in-seat delivery, live streaming, navigation and much more. The app can be extended to other events hosted by the stadium if the third party would like to include its features for their guests.\n\nThe stadium has had repeated problems with the grass surface, including the grass collapsing under Baltimore Ravens kicker Justin Tucker during a week 6 game in 2015. This has led to concern that the stadium wasn't of a high enough caliber to host a high stakes game such as the Super Bowl. The problems with the turf were mentioned the day after the Super Bowl by Denver Broncos cornerback Aqib Talib who said \"The footing on the field was terrible. San Fran (the 49ers) has to play eight games on that field, so they better do something to get it fixed. It was terrible.\"\n\nStadium proponents and those who expect to profit from the construction of the stadium claim that the stadium is currently one of the largest buildings registered with the U.S. Green Building Council. It is also believed to be the first stadium that will have both a green roof and solar panels. The 49ers are exploring collaborative opportunities with the Environmental Protection Agency to explore environmentally friendly components including:\n\n\nLevi's Stadium received a Gold LEED (Leadership in Energy and Environmental Design) Certificate. It is the first professional football stadium in the United States to receive this certification as new construction.\n\nStadium patrons have the option of riding VTA Light Rail (Valley Transportation Authority) to the stadium. The closest light rail station is the Great America station, which is located just west of the stadium in the median of Tasman Drive.\n\nTo the east, other transit options include the VTA Lick Mill station (also in the Tasman median) as well as the Amtrak and ACE station near California's Great America.\n\nLevi's Stadium was constructed immediately east of the San Tomas Aquino Trail, a paved multi-use path installed by the City of Santa Clara in 2004 that connects to a continuous 100-mile network of off-street paths including the regional San Francisco Bay Trail. The city announced in March 2013 that the San Tomas Aquino Trail would be \"temporarily detoured between Agnew Road and Tasman Drive for approximately 1 year starting April 15 and ending when the Stadium is open,\" but this one-mile section of the trail remained closed to the public before and during stadium events since they began in August 2014, requiring the continued use of the two-mile on-street detour.\n\nThe stadium project's Final Environmental Impact Report (EIR) disclosed no such ongoing temporary closures of the trail, but stated instead that \"While there will likely be a sizeable increase in pedestrians on the San Tomas Aquino Creek trail before and after NFL events, the creek trail is open to both pedestrians and cyclists and there are no restrictions on use. Anyone at anytime can access and use the trail.\"\n\nThe stadium's official mailing address is on Marie P DeBartolo Way (formerly Centennial Boulevard), which is actually a cul-de-sac on the east side of the stadium. The primary access route to the stadium is Tasman Drive, which runs along its northern side. Tasman is a major east-west arterial road which connects to Interstate 880 several miles to the east. Both west and east of the stadium, Tasman intersects with various north-south arterial roads which connect to several important freeways, such as U.S. Route 101, California State Route 237, and Interstate 280. The closest and most important of those north-south roads is Great America Parkway to the west of the stadium, which is named after the theme park to the south.\n\nOn May 8, 2013, the 49ers announced that San Francisco-based Levi Strauss & Co. purchased the naming rights to the new stadium. The deal calls for Levi's to pay $220.3 million to the city of Santa Clara and the 49ers over 20 years, with an option to extend the deal for another five years for around $75 million. On September 14, 2015, ESPN's Chris Berman coined the name \"The Big Bellbottom\" in reference to the stadium. In a Deadspin article covering the 49ers on August 18, 2015, article writer Drew Magary coined the nickname \"the Jeanhole\" for the stadium.\n\nLevi's Stadium has been praised for its excellent sightlines, beautiful architecture, plentiful amenities, technological advancements, convenient public transportation access, and environmental sustenance. However, the stadium has been heavily criticized for its highly corporate atmosphere and lack of a football atmosphere that Candlestick Park had. With the stadium having the most expensive ticket prices in the league during its inaugural season, many long-tenured and loyal fans that had contributed to the football atmosphere at Candlestick Park could not afford to buy season tickets with the added cost of the Stadium Builders License. Also, with the several distractions inside the stadium that include multiple bars and lounges, fans would often hang out in those places while the game is going on rather than watching the game from their seats.\n\nLevi's Stadium has received some backlash from season ticket holders, who are unhappy regarding rules that won’t allow them to print their tickets until 72 hours before the game, making re-sale very difficult. In addition, older 49ers fans say that people are more segmented at Levi's Stadium in comparison to Candlestick Park, leaving tailgaters with large expanses of empty parking stalls and a more desolate tailgating experience. \n\nLevi's Stadium has also received heavy criticism for the way fans are treated on hot days during early-season afternoon games. The majority of fans are seated on the east side of the stadium and during these afternoon games, they are exposed to the sun as there is no shade provided due to the lack of overhangs. With the climate of Santa Clara being much warmer than San Francisco, it makes watching games on hot days uncomfortable for fans as they are less accommodated for exposure to the sun than are patrons at other stadiums in hot-weather climates. Several fans suffered heat exhaustion during preseason and early season afternoon games. This has led to the eastern stands being largely empty on hot days. This was the result of the stadium being designed with the Candlestick Point site in mind and when the team decided to build it in Santa Clara instead, they kept the design intact in order to quickly get started on construction not taking the differences in climate into account. Due regulations by the Federal Aviation Administration for being in close proximity to the airport, they cannot add any more height to the stadium, while any additional overhangs would have to be structures, making it very difficult to fix the problem. \n\nPilots flying into San Jose International Airport have frequently complained of being blinded or disoriented by the lights from the light towers and scoreboards. The stadium is directly in the flight path of one of the airport's runways. According to Bay Area NBC affiliate KNTV, there have been at least 43 complaints about the lights since the stadium opened. In response, Santa Clara mayor Lisa Gillmor promised to work with the 49ers and the Stadium Authority to determine if the lights need to be recalibrated.\n\nThe San Francisco 49ers played at Candlestick Park from 1971 to 2013. The stadium was a sentimental fan favorite and housed all 5 Super Bowl Championship teams. It was, however, the oldest unrenovated stadium in the NFL and was beginning to show its age.\n\nThe 49ers pursued a new stadium since 1997, when a plan for a stadium and a mall at Candlestick Point passed a public vote. When the plans failed to move forward, the San Francisco 49ers presented an alternative plan on July 18, 2006, to construct a new 68,500-seat, open air stadium as part of a mixed use development featuring housing, commercial and retail space. In November 2006 the team announced that plans for a new stadium at Candlestick Point were not feasible, “citing extensive costs for infrastructure, parking accommodations and other changes that would cost more than the stadium itself”. The 49ers turned their focus to making Santa Clara the home to their new stadium.\n\nSan Francisco voters in 1997 approved $100 million in city spending to build a new stadium and an attached shopping mall at Candlestick Point. However, even after voter approval to grant economic help for the project, the stadium was not constructed. This was because owner Eddie DeBartolo, Jr. was facing legal troubles, which led him to surrender ownership of the team to his sister Denise DeBartolo York and brother-in-law John York. Mills Corporation, the company chosen by the 49ers, was unable to put together a plan to successfully construct a new stadium for the team. NFL owners had gone as far as awarding the new stadium the rights to host Super Bowl XXXVII. When stadium plans stalled, the game went to San Diego's Qualcomm Stadium instead.\n\nFor years, the city and team ownership were embattled over attempts to gain funding and a green-light for construction of a new stadium. None of these attempts proved to be successful. \n\nThe city of San Francisco received a new incentive to get a new stadium built. Mayor Gavin Newsom wanted to bring the 2016 Summer Olympics to the city, and a new stadium would sweeten the city's proposal for selection by the United States Olympic Committee as the official US submission to the IOC. The announcement came in November 2006. It called for a new stadium that would be converted into a 68,600-seat stadium for the 49ers after the Olympics. The Olympic Village would be converted into low-income housing after the games were over.\n\nThe new stadium was to be built at Candlestick Point on land just southeast of Candlestick Park. The cost of the stadium would be $916 million. Lennar Corporation would build housing, retail, and office space around the stadium area. Originally, part of the area surrounding Candlestick Park was to be zoned for retail space and housing; the new 49ers stadium was to be combined with such elements, bringing much-needed attractions to the historically blighted neighborhood of Hunters Point.\n\nThe stadium would be stocked with 150 luxury suites, 7,500 premium club seats, and an increased number of seats lower and closer to the field, called \"bowl seating,\" potentially raising the 49ers franchise value up as much as $250 million and offering at least $300 million in advertising and concession deals, the majority of which from paid corporate naming. The architectural design would be reminiscent of San Francisco buildings.\n\nThe project planning did not get off to a good start, however, with contention between the 49ers and the city of San Francisco over viable locations for the new stadium. Initially, the idea was to build a stadium in the parking lot of Candlestick Park and later demolish the aging stadium. Team ownership feared that construction of the village and the stadium would severely limit the amount of land available in Candlestick Point, creating a parking problem for fans and increasing traffic along the roads that link the stadium to the freeway. Moreover, with residents in the low-income housing by 2016, traffic would be permanently increased, further damaging the already-limited methods of transportation to the park.\n\nWith San Francisco slow to come up with better locations for the stadium or ways to circumvent the problems posed by construction at Candlestick Point, team owners Denise DeBartolo York and John York announced on November 9, 2006, that the 49ers were shifting their efforts to create a new stadium to the city of Santa Clara, home to the team offices and training facility since 1987, approximately south of San Francisco.\n\nThe sudden removal of the planned stadium forced the San Francisco Olympics bid group to cancel its proposal, which engendered great anger not only from Mayor Newsom, but also from such 49ers legends as Joe Montana and Ronnie Lott, who were part of the effort to bring the Olympics to the Bay Area. In addition, many fans were outraged at the suggestion to move the 49ers out of the city that it had shared history with for decades. The Yorks insisted that the legacy of the franchise would be respected in the sense that the 49ers would not be renamed nor moved out of the Bay Area. This was met with much opposition from Mayor Newsom and Senator Dianne Feinstein (who was mayor of San Francisco between 1978 and 1988); the senator stated that the team should be unable to use the San Francisco name if its operations were not based in the city. On January 3, 2007, California State Senator Carole Migden introduced a bill, entitled SB49, that would bar the 49ers from building a new stadium within a radius of San Francisco, if they were to leave the city. The 49ers organization announced its strong opposition to the legislation and retorted that passing such a bill would only encourage the team to move out of the Bay Area altogether. The bill died without being acted upon.\n\nThe Santa Clara stadium project had been in the works since 2007, with negotiations beginning in 2008. Two years later the following documents were produced that were key to understanding the stadium deal that went before the voters of Santa Clara on June 8, 2010. All documents cited below are publicly available on the City of Santa Clara’s website.\n\n\nMost city council members in Santa Clara were extremely receptive to the possibility of a new stadium being constructed there for the 49ers. In 2009, the Santa Clara City Council, led by Mayor Patricia Mahan, along with city employees began negotiating in earnest with the team, who presented the city with stadium plans. On June 2, 2009, by a 5-2 vote, the Santa Clara city council agreed to preliminary terms (as detailed in a term sheet). The official term sheet stated that the team's name would not change; the team would continue to be called the San Francisco 49ers even when the move to Santa Clara was complete.\n\nIn December 2009, Cedar Fair Entertainment, Great America's owner, filed a lawsuit to stop the project from proceeding. However, the lawsuit was dismissed in court.\n\nOn December 15, 2009, the Santa Clara City Council voted 5-2 to withdraw their city-sponsored ballot measure on the stadium issue in favor of a ballot initiative, Measure J. The ballot initiative was voted on on June 8, 2010 and passed by 58% of Santa Clara voters. Santa Clara City Council members William Kennedy and Jamie McLeod had opposed the stadium project and worked (unsuccessfully) to get Measure J defeated.\n\nMeasure J is a binding, voter-initiated measure that was approved by voters in the City of Santa Clara. All documents cited below are publicly available on the City of Santa Clara’s official website.\n\n\nThere was a possibility that the Oakland Raiders might share the stadium, allowing its costs to be split between the two teams. The stadium is designed to accommodate two teams, with the exterior LEDs being programmable for alternate colors and two home-team locker rooms. The 49ers and Raiders publicly said it would be an option if possible, while NFL commissioner Roger Goodell was strongly in favor of the two sharing a stadium. Fans of both teams reacted negatively to the idea. Along with the New York metropolitan area, where the New York Giants and New York Jets shared Giants Stadium from 1984 to 2009 and currently share its successor, MetLife Stadium, the DC-Baltimore metro area and Los Angeles. The Bay Area is one of 4 NFL markets with two teams.\n\nThe 49ers and Raiders sharing a stadium would not have been a first, as the two shared Kezar Stadium for part of 1960. It would have also fulfilled the late Raiders owner Al Davis' goal of a new stadium, something he had strongly desired since the late 1980s although Davis was against sharing a new stadium with another NFL team in Los Angeles when the idea was proposed to him, prompting his move back to Oakland in 1995. The Raiders, as it stands, play at the Oakland-Alameda County Coliseum and are the only NFL team still sharing its home field with a Major League Baseball team; the Raiders' lease on the Coliseum has been on year-to-year extensions since the expiration of the last long-term lease in 2013.\n\nIn the wake of Davis' death, the possibility of the 49ers and Raiders sharing the stadium became a stronger possibility. However, by October 2011, the 49ers were far enough along on the stadium to have reportedly already sold over a quarter of the luxury suites, meaning the Raiders would be forced to be secondary tenants. In October 2012, Oakland Raiders owner Mark Davis told reporters he had no plans to share the Santa Clara stadium with the 49ers. According to the report, discussions remained open, although Davis wanted to keep the team in Oakland, or a nearby site in Dublin.\n\nWhen the stadium had its grand opening on July 17, 2014, Goodell mentioned to the live crowd that it would make a great home for the Raiders and that it was up for the team to decide whether or not it wanted to play there or build a stadium on the site of the Oakland Coliseum. While the 49ers remained open to sharing the stadium with the Raiders, the Raiders said that their personal preference was the Coliseum site.\n\nOn February 20, 2015, the Raiders announced that they would be seeking a joint stadium in Carson, with the San Diego Chargers should they not receive public funding to replace the Oakland Coliseum, reducing the likelihood of the Raiders sharing Levi's Stadium with the 49ers. In January 2016, after losing their bid to relocate to Los Angeles to the Los Angeles Rams, the Raiders withdrew their request to move to Los Angeles, and joint tenancy at Levi's again surfaced in general discussion. However, with the Raiders getting approved for a move to Las Vegas and securing a local agreement for stadium funding there with Las Vegas Stadium under construction, it seems unlikely the Raiders will ever pursue a move to Santa Clara.\n\nIn December 2011, the Santa Clara City Council voted for an agreement that calls for the city’s Stadium Authority to borrow $850 million from Goldman Sachs, Bank of America and U.S. Bank. This will cover most of the construction costs, with the remainder to be made up via funding from the NFL, a hotel tax and city redevelopment funds. Interest, fees and terms for this loan have not been disclosed. The $850 million building loan, plus interest and fees will be assumed by the City's Stadium Authority, where additional interest and fees will be applied.\nOn February 2, 2012, NFL owners approved a loan to the 49ers of $200 million for use in constructing the new stadium, and to be taken from a new G-4 stadium loan fund. Terms of the loan were not specified, but under the previous G-3 plan, money was repaid directly into the league's account from the borrowing team's share of gate receipts from road games.\n\nConstruction began soon after funding for the stadium had been confirmed. The official groundbreaking took place on April 19, 2012. On July 30, 2012, the first steel beams for the stadium were laid down. The first seats in Levi's Stadium were installed on October 1, 2013.\n\nConstruction was halted on June 11, 2013, after a mechanic working on an elevator was struck by a counterweight and then fell down the shaft to his death. Work resumed two days later after officials from the California Occupational Safety and Health Administration (Cal/OSHA) declared the site safe, but as of October 2013, the accident remained under investigation.\n\nThe Santa Clara stadium was constructed on a city-owned parking lot on Tasman Drive, located adjacent to the north of California's Great America theme park and leased to Great America for overflow parking. As with Candlestick Park, there are relatively few amenities in the stadium's immediate vicinity for sports fans, besides the 49ers headquarters and training facility. The Santa Clara Convention Center is northwest of the stadium site and there are two hotels on Tasman Drive next to the convention center, but the closest significant concentration of hotels and restaurants is on the Mission College Boulevard corridor almost a mile to the south, on the other side of Great America.\n\nThe stadium opened on July 17, 2014. It was originally scheduled to open on July 11, but was pushed back due to construction delays.\nThe first game played at the new stadium was a Major League Soccer match on August 2, 2014, where the San Jose Earthquakes defeated Seattle Sounders FC 1–0 before a crowd of 48,765. The inaugural goal was scored in the 42nd minute by Yannick Djaló.\n\nOn August 17, 2014, the 49ers lost their first preseason game, 34–0, against the Denver Broncos at Levi's Stadium. One fan at the game collapsed due to the heat and had to be rushed to a local hospital, where he died.\n\nThe first 49ers' regular-season game at the stadium was held during Week 2 on September 14, 2014, when the team hosted the Chicago Bears on \"Sunday Night Football\". The Bears won the game 28–20 in front of a 49ers home record attendance of 70,799.\n\nIn November 2013, stadium and 49ers' officials initially requested the NFL to not schedule any Monday or Thursday night home games during Levi's Stadium's inaugural season due to parking issues in the area surrounding the stadium during weekdays. Two months later, in January 2014, the Santa Clara city government was able to secure more than the 21,000 necessary parking spots by approving use of the fairways at the city-owned Santa Clara Golf and Tennis Club (which is located to the north of the stadium across Tasman Drive). This arrangement is similar to and was modeled upon a longstanding arrangement between the Rose Bowl in Pasadena, California and the adjacent Brookside Golf Course, where the golf course was modified to allow for vehicular access to the fairways; they are used for parking only when dry to minimize damage, and any damage that does occur is repaired afterward.\n\nWith access to the golf course fairways, Levi's Stadium now had 31,600 potential parking spaces, meaning that tailgating and weeknight games were now a possibility. However, the NFL decided not to schedule any weeknight home games at Levi's Stadium in 2014 until traffic flow within the area is figured out, with the exception of a Thanksgiving game between the 49ers and the Seattle Seahawks on November 27, 2014. Parking prices, which averaged $30 in the 49ers' final season at Candlestick Park, will increase to $40 at Levi's Stadium.\n\nAnticipating significant traffic from Levi's Stadium visitors, the nearby city of Mountain View instituted a three-hour parking limit on downtown streets during game days. While residents received exemptions via permit tags, stadium-goers must park in paid lots or far from Mountain View's Caltrain/VTA light rail station. This station is the closest VTA light rail station to San Francisco and receives transferring passengers heading south to San Jose via light rail (including people using the light rail to go directly to the Levi's Stadium station).\n\nIn Spring 2015, 49ers' officials offered the city of Santa Clara $15 million to take over the adjacent Santa Clara Youth Soccer Park and convert these soccer fields into additional parking lots. This money, along with a large percentage of parking fees, would have then been used for the city to build a new youth sports complex elsewhere. The plan was opposed by the youth soccer leagues that use the fields, and critics calling it a \"land grab\". After many youth soccer players attended a city council meeting on April 29 to protest the proposal, the 49ers withdrew the $15 million offer. The team then gave the city an unsolicited offer of $3 million to help improve various youth athletic fields and facilities, which was cited as a peace offering by \"San Francisco Chronicle\" columnists Phil Matier and Andrew Ross, and other Bay Area media, but the city council turned the offer down.\n\nSuper Bowl 50 was held at the stadium on February 7, 2016. The Denver Broncos defeated the Carolina Panthers 24–10. Lady Gaga performed the national anthem, and Coldplay performed with Beyoncé and Bruno Mars at halftime.\n\nLevi's Stadium hosted WWE's WrestleMania 31 on March 29, 2015. This was the first time WrestleMania was hosted in Northern California and the sixth WrestleMania to be held in California. \nThe area also hosted various activities in the week-long celebration leading up to WrestleMania itself.\n\nThe show set an attendance record for the stadium, with 76,976. It drew a $12.6 million gate.\n\nLevi's Stadium hosted the 2015 NHL Stadium Series' February 21 game between the Los Angeles Kings and San Jose Sharks. The Kings defeated the Sharks 2-1, in front of a crowd of 70,205.\n\nOn July 31, 2014, the San Jose Earthquakes agreed to play one match per year for five years at Levi's Stadium. On September 6, 2014, an international friendly between Mexico and Chile was held. The stadium also hosted a 2015 International Champions Cup match between Barcelona and Manchester United on July 25, 2015, when United won 3–1. A 2016 International Champions Cup match featured A.C. Milan and Liverpool on July 30, 2016, with Liverpool winning 2–0.\n\nIn June 2016, Levi's Stadium hosted four games at the Copa América Centenario; the opening match between United States and Colombia, two other group stage matches, and a quarter-final where Chile defeated Mexico. \n\nA 2017 International Champions Cup game was held on July 23, 2017, when Manchester United defeated Real Madrid 1-1 (2-1 in a penalty shootout). On March 23, 2018, Mexico won 3–0 against Iceland in a friendly in both teams' preparation for the 2018 FIFA World Cup.\n\nA 2018 International Champions Cup game was held on August 4, 2018, when AC Milan defeated FC Barcelona 1-0 with a last second goal by André Silva.\n\nLevi's Stadium has hosted numerous college football games. The first college game played there was a regular season game between the Oregon Ducks and the California Golden Bears, in which Oregon won. The stadium is the home of the Foster Farms Bowl and since 2014 has served as the host for the Pac-12 Championship Game. In November 2015, it was announced that Levi's Stadium would host the 2019 College Football Playoff National Championship, making it the first time the game will be played there.\n\n\n"}
{"id": "31804992", "url": "https://en.wikipedia.org/wiki?curid=31804992", "title": "Luminescent solar concentrator", "text": "Luminescent solar concentrator\n\nA luminescent solar concentrator (LSC) is a device for concentrating radiation, solar radiation in particular, to produce electricity. Luminescent solar concentrators operate on the principle of collecting radiation over a large area, converting it by luminescence (specifically by fluorescence) and directing the generated radiation into a relatively small output target.\n\nInitial designs typically comprised parallel thin, flat layers of alternating luminescent and transparent materials, placed to gather incoming radiation on their (broader) faces and emit concentrated radiation around their (narrower) edges. Commonly the device would direct the concentrated radiation onto solar cells to generate electric power.\n\nOther configurations (such as doped or coated optical fibers, or contoured stacks of alternating layers) may better fit particular applications.\n\nThe layers in the stack may be separate parallel plates or alternating strata in a solid structure. In principle, if the effective input area is sufficiently large relative to the effective output area, the output would be of correspondingly higher irradiance than the input, as measured in watts per square metre. The concentration factor is the ratio between output and input irradiance of the whole device.\n\nFor example, imagine a square glass sheet (or stack) 200 mm on a side, 5 mm thick. Its input area (e.g. the surface of one single face of the sheet oriented toward the energy source) is 10 times greater than the output area (e.g. the surface of four open sides) - 40000 square mm (200x200) as compared to 4000 square mm (200x5x4). To a first approximation, the concentration factor of such an LSC is proportional to the area of the input surfaces divided by the area of the edges multiplied by the efficiency of diversion of incoming light towards the output area. Suppose that the glass sheet could divert incoming light from the face towards the edges with an efficiency of 50%. The hypothetical sheet of glass in our example would give an output irradiance of light 5 times greater than that of the incident light, producing a concentration factor of 5.\n\nSimilarly, a graded refractive index optic fibre 1 square mm in cross section, and 1 metre long, with a luminescent coating might prove useful.\n\nThe concentration factor interacts with the efficiency of the device to determine overall output.\n\n\nMost devices (such as solar cells) for converting the incoming energy to useful output are relatively small and costly, and they work best at converting directional light at high intensities and a narrow frequency range, whereas input radiation tends to be at diffuse frequencies, of relatively low irradiance and saturation. Concentration of the input energy accordingly is one option for efficiency and economy.\n\nThe above description covers a wider class of concentrators (for example simple optical concentrators) than just luminescent solar concentrators. The essential attribute of LSCs is that they incorporate luminescent materials that absorb incoming light with a wide frequency range, and re-emit the energy in the form of light in a narrow frequency range. The narrower the frequency range, (i.e. the higher the saturation) the simpler a photovoltaic cell can be designed to convert it to electricity.\n\nSuitable optical designs trap light emitted by the luminescent material in all directions, redirecting it so that little escapes the photovoltaic converters. Redirection techniques include internal reflection, refractive index gradients and where suitable, diffraction. In principle such LSCs can use light from cloudy skies and similar diffuse sources that are of little use for powering conventional solar cells or for concentration by conventional optical reflectors or refractive devices.\n\nThe luminescent component might be a dopant in the material of some or all of the transparent medium, or it might be in the form of luminescent thin films on the surfaces of some of the transparent components.\n\nVarious articles have discussed the theory of internal reflection of fluorescent light so as to provide concentrated emission at the edges, both for doped glasses and for organic dyes incorporated into bulk polymers. When transparent plates are doped with fluorescent materials, effective design requires that the dopants should absorb most of the solar spectrum, re-emitting most of the absorbed energy as long-wave luminescence. In turn, the fluorescent components should be transparent to the emitted wavelengths. Meeting those conditions allows the transparent matrix to convey the radiation to the output area. Control of the internal path of the luminescence could rely on repeated internal reflection of the fluorescent light, and refraction in a medium with a graded refractive index.\n\nTheoretically about 75-80 % of the luminescence could be trapped by total internal reflection in a plate with a refractive index roughly equal to that of typical window glass. Somewhat better efficiency could be achieved by using materials with higher refractive indices. Such an arrangement using a device with a high concentration factor should offer impressive economies in the investment in photovoltaic cells to produce a given amount of electricity. Under ideal conditions the calculated overall efficiency of such a system, in the sense of the amount of energy leaving the photovoltaic cell divided by the energy falling on the plate, should be about 20%.\n\nThis takes into account:\n\n\nThe relative merits of various functional components and configurations are major concerns, in particular:\n\n\nLuminescent solar concentrators could be used to integrate solar-harvesting devices into building façades in cities.\n\nIn 2013, researchers at Michigan State University demonstrated the first visibly transparent luminescent solar concentrators. These devices were composed of phosphorescent metal halide nanocluster (or Quantum Dot) blends that exhibit massive Stokes shift (or downconversion) and which selectively absorb ultraviolet and emit near-infrared light, allowing for selective harvesting, improved reabsorption efficiency, and non-tinted transparency in the visible spectrum.\nThe following year, these researchers demonstrated near-infrared harvesting visibly transparent luminescent solar concentrators by utilizing luminescent organic salt derivatives. These devices exhibit a clear visible transparency similar to that of glass and a power conversion efficiency close to 0.5%. In this configuration efficiencies of over 10% are possible due to the large fraction of photon flux in the near-infrared spectrum.\n\nIn 2014 LSCs based on cadmium selenide/cadmium sulfide (CdSe/CdS) quantum dots (QD) with induced large separation between emission and absorption bands (called a large Stokes shift) were announced.\n\nLight absorption is dominated by an ultra-thick outer shell of CdS, while emission occurs from the inner core of a narrower-gap CdSe. The separation of light-absorption and light-emission functions between the two parts of the nanostructure results in a large spectral shift of emission with respect to absorption, which greatly reduces re-absorption losses. The QDs were incorporated into large slabs (sized in tens of centimeters) of polymethylmethacrylate (PMMA). The active particles were about one hundred angstroms across.\n\nSpectroscopic measurements indicated virtually no re-absorption losses on distances of tens of centimeters. Photon harvesting efficiencies were approximately 10%. Despite their high transparency, the fabricated structures showed significant enhancement of solar flux with the concentration factor of more than four.\n\n\n\n\nOther authors:\n"}
{"id": "13063487", "url": "https://en.wikipedia.org/wiki?curid=13063487", "title": "Metal corset", "text": "Metal corset\n\nMetal corsets (also known as iron corsets) are a type of historical corset or bodice made entirely out of metal, usually iron or steel. The metal corset was popularly claimed to have been introduced to France by Catherine de' Medici in the 16th century, although this is now considered a myth. The idea that such garments were worn for fashionable purposes is debatable, with fashion historians now regarding such claims sceptically. Many of the original metal bodices that have survived are now believed to have been intended for medical purposes as orthopaedic support garments and back braces. Such garments were described by the French army surgeon Ambroise Paré in the sixteenth century as a remedy for the \"crookednesse of the Bodie.\"\n\nSome of the more extreme examples of metal corsets that have survived are now generally thought to be later reproductions designed to appeal to fetishists, rather than garments intended for fashionable wear.\n\nMetal medical corsets were still being made in the twentieth century, whilst, since the late 20th century, fashion designers such as Alexander McQueen and Issey Miyake have made contemporary metal bodices and corsets from wire and aluminium coils.\n\nEarly fashion historians and writers have often attributed the introduction of fashionable corset-wearing to Catherine de' Medici, who is said to have brought metal corsets to France from Italy in the 16th century. The fashion historian Valerie Steele noted that after 19th-century writers catering to audiences for tightlacing and sexual fetishism played up the sadomasochistic idea of a \"cruel, tortuous fashion\" enforced by a dominant queen who demanded unrealistically small waists from her subjects, this mythical royal connection captured public imagination and became part of fashion mythology.\n\nIt is now widely believed that authentic metal corsets were intended as a form of orthopaedic brace to address spinal issues such as scoliosis. The 16th-century French army surgeon Ambroise Paré described metal corsets as intended \"to amend the crookednesse of the Bodie,\" recommending that the iron should be perforated in order to make the garments lighter, and that they be made to fit and padded for comfort. Paré criticised the concept of corsetry as a waist-training device, warning that such a practice risked deforming the figure.\n\nA steel corset in the Stibbert Museum, Florence, Italy, is dated to the mid-16th century, and thought to be similar to the metal stays recorded as having been made by a \"corazzaio mastro\" (master armour-maker) for Eleanor of Toledo and delivered to her on 28 February 1549. However, as Eleanor's wardrobe records do not list any boned or stiffened corsets, it is thought that her steel bodice was designed for medical or therapeutic reasons rather than worn as a fashionable garment.\n\nAlthough surviving metal bodices are usually dated to the late 16th and early 17th century, Steele has stated that some of the more extreme and elaborate examples are fakes created from the 19th century onwards to cater to fetishistic \"fantasies about women imprisoned in metal corsets.\" For example, Herbert Norris claimed in \"Tudor Costume and Fashion\" (1938) that a misbehaving wife would be locked into a metal corset by her husband until she promised to behave. One such iron corset, with a 14-inch waist, was acquired by the FIT Museum as dating from 1580–1600, but is now considered to be a forgery from the turn of the 19th and 20th centuries. Steele noted suspicious similarities between this corset and an illustration first published in 1868 in \"The Corset and the Crinoline\", a \"fetishistic\" book claiming to offer a historical overview of fashion, and draws parallels between such corsets and fake medieval chastity belts. Harold Koda, the former curator of the Metropolitan Museum of Art's Costume Institute, states that the excessive, mechanically produced regularity of the garment's structure is evidence for its being a 19th-century fabrication. Koda's take on the significant percentage of extant nineteenth-century metal corsets made in emulation of purported sixteenth-century models is that they were created to cater to a specialist market, perhaps for inclusion in collector's cabinets.\n\nThe fashion historians C. Willett Cunnington and his wife Phillis also stated firmly that surviving \"iron bodies,\" when not medical garments, were usually \"fanciful 'reproductions'\" with no proof of their having genuinely been worn. Despite the explicit scepticism of fashion historians such as Steele and the Cunningtons, scholars outside the field of dress history sometimes treat these corsets as legitimate fashion garments. The anthropologist Marianne Thesander concluded that because such bodices fit the fashionable silhouette of their alleged period, they were probably authentic, and served the same purpose as other corsets.\n\nIn \"Fashion and Fetishism\", David Kunzle noted that in Peter Rondeau's 1739 French-German dictionary, the French term \"corps de fer\" is explained in German as \"Schnürburst, mit kleinen eisernen blechen, für übel gewachsenes Frauenzimmer\" (A bodice, with small iron plates, for badly grown (i.e., deformed) girls). He reads this as implying that the iron plates would have been part of a fabric corset, rather than an all-metal garment.\n\nKunzle has noted the absence of literary evidence for showing that metal corsets were also worn for fashion purposes. He has suggested that surviving metal garments, if not specifically medical in purpose, might have served the same masochistically gratifying purpose as the deliberately uncomfortable, tortuous hair shirt, combining a fashionable silhouette with penance, and as such, might have been worn in convents. To support his \"pure speculation\", Kunzle cites a 1871 newspaper report from \"The Times\" reporting that during the Paris Commune, the National Guard found two iron corsets, a rack, and other instruments in the Convent of the White Nuns in Picpus. The claim by the Mother Superior that the instruments were for orthopaedic purposes was dismissed at the time as \"a superficial falsehood.\"\n\nMetal corsets for medical purposes continued to be used in the 18th and early 19th century, although equivalent garments made from canvas were increasingly used in their place. In 1894, A.M. Phelps of the American Orthopaedic Association recommended an aluminium corset coated with waterproof enamel for sufferers of Pott disease or curvature of the spine. Made from a cast of the patient's body, the advantages of such a garment were that aluminium was lightweight, durable, thin enough to be worn beneath clothing, and could be worn while bathing. Such corsets were still being recommended in the early 20th century as cheaper and more durable in the longer run than plaster moulds, although their initial expense was greater.\n\nSince the 20th century, actual metal corsets have occasionally been made for contemporary wear, although such instances are rare. Steele notes that alongside a 1930s metal corset made for and worn by a fetish corsetiere called Cayne, the late 20th- and early 21st-century tight-lacer Cathie Jung had a silver corset-cover made to wear over her actual laced corset. Between 1933 and 1940 Mrs. Cayne advertised a booklet describing her 14-inch waistline and offered other services in the \"Illustrated Sporting and Dramatic News\".\n\nAs a medical garment, metal corsets endured well into the 20th century. The Mexican painter Frida Kahlo was a notable wearer of such medical corsets, following ongoing problems as a result of a serious road crash she experienced as a teenager. By 1944, Kahlo's doctors had recommended that she wear a steel corset instead of the plaster ones she had mainly worn since the accident; and Kahlo, whose paintings were heavily autobiographical, used the new corset as the basis for one of her best known self-portraits, \"The Broken Column\". In the painting, Kahlo portrays herself weeping with agony, her torso split open revealing that her spine is a crumbling Ionic column, and her damaged body held together by the steel corset. A form of metal corset or orthopaedic brace used in the second half of the 20th century is called a Harris brace after its inventor, R.I. Harris. Harris braces are designed to immobilise the waist whilst healing, and are made with two bendable metal bands worn above and below the waist, and connected with rigid metal supports.\n\n20th and 21st century designers have sometimes offered metal corsets and bodices as part of their presentations, including Alexander McQueen, Issey Miyake, and Thierry Mugler. One of McQueen's most famous pieces was a 1999 aluminium corset, called the Coiled Corset, created in collaboration with the jeweller Shaun Leane and the artist Kees van der Graaf. Built around a cast of the model Laura Morgan's torso, the garment had a 15-inch waist and was composed of 97 stacked coils, which had to be screwed together onto Morgan's body. The Coiled Corset was inspired by the neck rings worn by Ndebele women, extended to encase the wearer's torso. In 2001, the corset formed part of a live presentation at the Victoria and Albert Museum showcasing McQueen and Leane's collaborations. Corsets and bustiers can also be made using wire, such as a 1983 aluminium wire bustier by Miyake which was cuffed around the torso over a feathered garment, offering a pun on the theme of birdcages.\n\nMetal corsets are found in a number of museum collections around the world. Some museums, including the Museo Stibbert, and the Kyoto Costume Institute in Japan, present their metal bodices as fashionable late 16th-century garments. The Victoria and Albert Museum in London describes their iron corset (formerly owned by the painter Talbot Hughes) as dating from the 18th century and likely intended for orthopaedic purposes. Others, such as the iron corset in the Fashion Institute of Technology, are presented as fakes.\n"}
{"id": "51582792", "url": "https://en.wikipedia.org/wiki?curid=51582792", "title": "Metals and Engineering Corporation", "text": "Metals and Engineering Corporation\n\nMetals and Engineering Corporation abbreviated as METEC, is an Ethiopian military-run corporation, which is established in 2010. It is the largest military-industrial complex in Ethiopia and responsible for the production of military equipment and civilian products. METEC works with foreign companies such as Alstom from France, and Spire Corporation from America.\n\nMETEC was once responsible for constructing the $4 billion dam project on the River Nile, expected to be Africa’s biggest hydroelectric project, but was ousted from the contract in August. Kinfe Dagnew, a Brigadier General in Ethiopia’s army and former chief executive of METEC plays a significant role in the organization. The company was assigned development of Grand Ethiopian Renaissance Dam and sugar factory, as well as well as the Jinka Sugar Bag factory. On November 12, 2018, all assigned project canceled due to fail to complete, and government arrested Kinfe Dagnew, CEO of METEC, after a trial to escape through Sudan, where he was captured by Defence force. Kinfe Dagnew arrived in helicopter to Addis Ababa.\n\nIn April 2018, Kinfe Dagnew resigned the company after serving entire eight years, when Prime Minister Abiy Ahmed took the Tigrean People's Liberation Front power. The company remained unknown when it was operated by TPLF. In June, parliamentary committee found that METEC squandered hundreds of machinery products worthing 326.4 million dollars without study of the market. \n\nSources say that the Commercial Bank of Ethiopia squandered 2.8 million dollars without completion of sugar projects, and revealed the company borrowed 436 million dollars. \nOn November 12, 2018, the Chief executive officer, Kinfe Dagnew, was attempting for escape through Sudan border. Upon arrival on the area, several Defence forces stopped and arrested him. He was founded with an empty briefcase believed to be illegally transport goods. Shortly after hours, a yellow private helicopter dispatched to handle the officers and the general; which brought him in the municipal city in Addis Ababa. 40 the company's officials were detained after they called for meeting in Imperial Hotel, Gerji, Addis Ababa.\nSome newspapers quoted that they were transferred to Addis Ababa Police Commission; they temporarily jailed by Federal Police.\n"}
{"id": "4354218", "url": "https://en.wikipedia.org/wiki?curid=4354218", "title": "Mobile commerce", "text": "Mobile commerce\n\nThe phrase mobile commerce was originally coined in 1997 by Kevin Duffey at the launch of the Global Mobile Commerce Forum, to mean \"the delivery of electronic commerce capabilities directly into the consumer’s hand, anywhere, via wireless technology.\" Many choose to think of Mobile Commerce as meaning \"a retail outlet in your customer’s pocket.\"\n\nMobile commerce is worth US$230 billion, with Asia representing almost half of the market, and has been forecast to reach US$700 billion in 2017. According to BI Intelligence in January 2013, 29% of mobile users have now made a purchase with their phones. Walmart estimated that 40% of all visits to their internet shopping site in December 2012 was from a mobile device. Bank of America predicts $67.1 billion in purchases will be made from mobile devices by European and U.S. shoppers in 2015. m-Commerce made up 11.6 per cent of total e-commerce spending in 2014, and is forecast to increase to 45 per cent by 2020, according to BI Intelligence. ComScore reported in February 2017 that mobile commerce had grown 45% in year to December 2016.\n\nThe Global Mobile Commerce Forum, which came to include over 100 organisations, had its fully minuted launch in London on 10 November 1997. Kevin Duffey was elected as the Executive Chairman at the first meeting in November 1997. The meeting was opened by Dr. Mike Short, former chairman of the GSM Association, with the very first forecasts for mobile commerce from Kevin Duffey (Group Telecoms Director of Logica) and Tom Alexander (later CEO of Virgin Mobile and then of Orange). Over 100 companies joined the Forum within a year, many forming mobile commerce teams of their own, e.g. MasterCard and Motorola. Of these one hundred companies, the first two were Logica and Cellnet (which later became O2). Member organisations such as Nokia, Apple, Alcatel, and Vodafone began a series of trials and collaborations.\n\nMobile commerce services were first delivered in 1997, when the first two mobile-phone enabled Coca-Cola vending machines were installed in the Helsinki area in Finland. The machines accepted payment via SMS text messages. This work evolved to several new mobile applications such as the first mobile phone-based banking service was launched in 1997 by Merita Bank of Finland, also using SMS. Finnair mobile check-in was also a major milestone, first introduced in 2001.\n\nThe m-Commerce(tm) server developed in late 1997 by Kevin Duffey and Andrew Tobin at Logica won the 1998 Financial Times award for \"most innovative mobile product,\" in a solution implemented with De La Rue, Motorola and Logica. The Financial Times commended the solution for \"turning mobile commerce into a reality.\" The trademark for m-Commerce was filed on 7 April 2008.\n\nIn 1998, the first sales of digital content as downloads to mobile phones were made possible when the first commercial downloadable ringtones were launched in Finland by Radiolinja (now part of Elisa Oyj).\n\nTwo major national commercial platforms for mobile commerce were launched in 1999: Smart Money in the Philippines, and NTT DoCoMo's i-Mode Internet service in Japan. i-Mode offered a revenue-sharing plan where NTT DoCoMo kept 9 percent of the fee users paid for content, and returned 91 percent to the content owner.\n\nMobile-commerce-related services spread rapidly in early 2000. Norway launched mobile parking payments. Austria offered train ticketing via mobile device. Japan offered mobile purchases of airline tickets.\n\nIn April 2002, building on the work of the Global Mobile Commerce Forum (GMCF), the European Telecommunications Standards Institute (ETSI) appointed Joachim Hoffmann of Motorola to develop official standards for mobile commerce. In appointing Mr Hoffman, ETSI quoted industry analysts as predicting \"that m-commerce is poised for such an exponential growth over the next few years that could reach US$200 billion by 2004\".\n\nAs of 2008, UCL Computer Science and Peter J. Bentley demonstrated the potential for medical applications on mobile devices.\n\nPDAs and cellular phones have become so popular that many businesses are beginning to use mobile commerce as a more efficient way to communicate with their customers.\n\nIn order to exploit the potential mobile commerce market, mobile phone manufacturers such as Nokia, Ericsson, Motorola, and Qualcomm are working with carriers such as AT&T Wireless and Sprint to develop WAP-enabled smartphones. Smartphones offer fax, e-mail, and phone capabilities.\n\n\"Profitability for device vendors and carriers hinges on high-end mobile devices and the accompanying killer applications,\" said Burchett. Perennial early adopters, such as the youth market, which are the least price sensitive, as well as more open to premium mobile content and applications, must also be a key target for device vendors.\n\nSince the launch of the iPhone in 2007, mobile commerce has moved away from SMS systems and into actual applications. SMS has significant security vulnerabilities and congestion problems, even though it is widely available and accessible. In addition, improvements in the capabilities of modern mobile devices make it prudent to place more of the resource burden on the mobile device.\n\nMore recently, brick and mortar business owners, and big-box retailers in particular, have made an effort to take advantage of mobile commerce by utilizing a number of mobile capabilities such as location-based services, barcode scanning, and push notifications to improve the customer experience of shopping in physical stores. By creating what is referred to as a 'bricks & clicks' environment, physical retailers can allow customers to access the common benefits of shopping online (such as product reviews, information, and coupons) while still shopping in the physical store. This is seen as a bridge between the gap created by e-commerce and in-store shopping, and is being utilized by physical retailers as a way to compete with the lower prices typically seen through online retailers. By mid summer 2013, \"omnichannel\" retailers (those with significant e-commerce and in-store sales) were seeing between 25% and 30% of traffic to their online properties originating from mobile devices. Some other pure play/online-only retail sites (especially those in the travel category) as well as flash sales sites and deal sites were seeing between 40% and 50% of traffic (and sometimes significantly more) originate from mobile devices.\n\nThe Google Wallet Mobile App launched in September 2011 and the m-Commerce joint venture formed in June 2011 between Vodafone, O2, Orange and T-Mobile are recent developments of note. Reflecting the importance of m-Commerce, in April 2012 the Competition Commissioner of the European Commission ordered an in-depth investigation of the m-Commerce joint venture between Vodafone, O2, Orange and T-Mobile. A recent survey states that 2012, 41% of smartphone customers have purchased retail products with their mobile devices.\n\nIn Kenya money transfer is mainly done through the use of mobile phones. This was an initiative of a multimillion shillings company in Kenya named Safaricom. Currently, the companies involved are Safaricom and Airtel. Mobile money transfer services in Kenya are now provided by the two companies under the names M-PESA and Airtel Money respectively.\n\nA similar system called MobilePay has been operated by Danske Bank in Denmark since 2013. It has gained considerable popularity with about 1.6 million users by mid-2015. Another similar system called Vipps was introduced in Norway in 2015.\n\nMobile automated teller machine (ATM) is a special type of ATM. Most ATMs are meant to be stationary, and they’re often found attached to the side of financial institutions, in stores, and in malls. A mobile ATM machine, on the other hand, is meant to be moved from location to location. This type of ATM is often found at special events for which ATM service is only needed temporarily. For example, they may be found at carnivals, fairs, and parades. They may also be used at seminars and workshops when there is no regular ATM nearby.\n\nMobile ATMs are usually self-contained units that don’t need a building or enclosure. Usually, a mobile ATM can be placed in just about any location and can transmit transaction information wirelessly, so there's no need to have a phone line handy. Mobile ATMs may, however, require access to an electrical source, though there are some capable of running on alternative sources of power. Often, these units are constructed of weather-resistant materials, so they can be used in practically any type of weather conditions. Additionally, these machines typically have internal heating and air conditioning units that help keep them functional despite the temperature of the environment.ion of mobile money services for the unbanked, operators are now looking for efficient ways to roll out and manage distribution networks that can support cash-in and cash-out. Unlike traditional ATM, sicap Mobile ATM have been specially engineered to connect to mobile money platforms and provide bank grade ATM quality.\nIn Hungary, Vodafone allows cash or bank card payments of monthly phone bills. The Hungarian market is one where direct debits are not standard practice, so the facility eases the burden of queuing for the postpaid half of Vodafone’s subscriber base in Hungary.\n\nTickets can be sent to mobile phones using a variety of technologies. Users are then able to use their tickets immediately, by presenting their mobile phone at the ticket check as a digital boarding pass. Most numbers of users are now moving towards this technology. Best example would be IRCTC where ticket comes as SMS to users. New technology such as RFID can now be used to directly provide a single association digital ticket via the mobile device hardware associated with relevant software.\n\nMobile ticketing technology can also be used for the distribution of vouchers, coupons, and loyalty cards. These items are represented by a virtual token that is sent to the mobile phone. A customer presenting a mobile phone with one of these tokens at the point of sale receives the same benefits as if they had the traditional token. Stores may send coupons to customers using location-based services to determine when the customer is nearby. Using a connected device and the networking effect can also allow for gamification within the shopping experience.\n\nCurrently, mobile content purchase and delivery mainly consist of the sale of ring-tones, wallpapers, and games for mobile phones. The convergence of mobile phones, portable audio players, and video players into a single device is increasing the purchase and delivery of full-length music tracks and video. The download speeds available with 4G networks make it possible to buy a movie on a mobile device in a couple of seconds.\n\nThe location of the mobile phone user is an important piece of information used during mobile commerce or m-commerce transactions. Knowing the location of the user allows for location-based services such as:\n\nA wide variety of information services can be delivered to mobile phone users in much the same way as it is delivered to PCs. These services include:\n\nCustomized traffic information, based on a user's actual travel patterns, can be sent to a mobile device. This customized data is more useful than a generic traffic-report broadcast, but was impractical before the invention of modern mobile devices due to the bandwidth requirements.\n\nBanks and other financial institutions use mobile commerce to allow their customers to access account information and make transactions, such as purchasing stocks, remitting money. This service is often referred to as \"mobile banking\", or m-banking.\n\nStock market services offered via mobile devices have also become more popular and are known as Mobile Brokerage. They allow the subscriber to react to market developments in a timely fashion and irrespective of their physical location.\n\nOver the past three years mobile reverse auction solutions have grown in popularity. Unlike traditional auctions, the reverse auction (or low-bid auction) bills the consumer's phone each time they place a bid. Many mobile SMS commerce solutions rely on a one-time purchase or one-time subscription; however, reverse auctions offer a high return for the mobile vendor as they require the consumer to make multiple transactions over a long period of time.\n\nUsing a mobile browser—a World Wide Web browser on a mobile device—customers can shop online without having to be at their personal computer. Many mobile marketing apps with geo-location capability are now delivering user-specific marketing messages to the right person at the right time.\n\nCatalog merchants can accept orders from customers electronically, via the customer's mobile device. In some cases, the merchant may even deliver the catalog electronically, rather than mailing a paper catalog to the customer. Consumers making mobile purchases can also receive value-add upselling services and offers. Some merchants provide mobile web sites that are customized for the smaller screen and limited user interface of a mobile device.\n\nPayments can be made directly inside of an application running on a popular smartphone operating system, such as Google Android. Analyst firm Gartner expects in-application purchases to drive 41 percent of app store (also referred to as mobile software distribution platforms) revenue in 2016. In-app purchases can be used to buy virtual goods, new and other mobile content and is ultimately billed by mobile carriers rather than the app stores themselves. Ericsson’s IPX mobile commerce system is used by 120 mobile carriers to offer payment options such as try-before-you-buy, rentals and subscriptions.\n\nIn the context of mobile commerce, mobile marketing refers to marketing sent to mobile devices. Companies have reported that they see better response from mobile marketing campaigns than from traditional campaigns. The primary reason for this is the instant nature of customer decision-making that mobile apps and websites enable. The consumer can receive a marketing message or discount coupon and, within a few seconds, make a decision to buy and go on to complete the sale - without disrupting their current real-world activity.\n\nFor example, a busy mom tending to her household chores with a baby in her arm could receive a marketing message on her mobile about baby products from a local store. She can and within a few clicks, place an order for her supplies without having to plan ahead for it. No more need to reach for her purse and hunt for credit cards, no need to log into her laptop and try to recall the web address of the store she visited last week, and surely no need to find a babysitter to cover for her while she runs to the local store.\n\nResearch demonstrates that consumers of mobile and wireline markets represent two distinct groups who are driven by different values and behaviors, and who exhibit dissimilar psychographic and demographic profiles. What aspects truly distinguish between a traditional online shopper from home and a mobile on-the-go shopper? Research shows that how individuals relate to four situational dimensions- place, time, social context and control determine to what extent they are ubiquitous or situated as consumers. These factors are important in triggering m-commerce from e-commerce. As a result, successful mobile commerce requires the development of marketing campaigns targeted to these particular dimensions and according to user segments.\n\nMobile media is a rapidly changing field. New technologies, such as WiMax, act to accelerate innovation in mobile commerce. Early pioneers in mobile advertising include Vodafone, Orange, and SK Telecom.\n\nMobile devices are heavily used in South Korea to conduct mobile commerce. Mobile companies in South Korea believed that mobile technology would become synonymous with youth lifestyle, based on their experience with previous generations of South Koreans. \"Profitability for device vendors and carriers hinges on high-end mobile devices and the accompanying killer applications,\" said Daniel Longfield.\n\nConsumers can use many forms of payment in mobile commerce, including:\n\nInteraction design and UX design has been at the core of the m-commerce experience from its conception, producing apps and mobile web pages that create highly usable interactions for users. However, much debate has occurred as to the focus that should be given to the apps. In recent research, Parker and Wang demonstrated that within fashion m-Commerce apps, the degree that the app helps the user shop (increasing convenience) was the most prominent function. Such use examples may be through design cues which help the user find their products with minimal search. \nAdditionally, shopping for others was a motivator for engaging in m-commerce apps with great preference for close integration with social media.\n\nThe popularity of apps has given rise to the latest iteration of mobile commerce: app commerce. This refers to retail transactions that take place on a native mobile app. App commerce is said to perform better than both desktop and mobile web when it comes to browsing duration and interactions. Average order value is reportedly greater with retail apps than traditional ecommerce, and conversion rates on apps are twice that of mobile websites.\n\n\n"}
{"id": "11265664", "url": "https://en.wikipedia.org/wiki?curid=11265664", "title": "PMCD", "text": "PMCD\n\nPMCD (PreMaster CD) is a specially formatted, recordable Compact Disc designed to be sent to a CD pressing plant for replication. The PreMaster CD format, developed in the early 1990s by the CD-ROM division of Sony, in cooperation with \"START Lab Inc.\" of Tokyo and Sonic Solutions, contained a hidden “PreMaster Cue Sheet” that held the metadata needed for replication that a \"Red Book\" CD-DA lacks. The PreMaster CD format made use of the fact that not all data-recording surfaces are specified for use in the \"Red Book\" CD-DA or \"Yellow Book\" CD-ROM standards. CD transports were not able to recover the data hidden in the Cue Sheet unless forced to buy proprietary software. The Cue Sheet specified a broad range of metadata, including number of channels, per track pre-emphasis status, per track copy protection bit setting, per track ISRC Codes, per disc UPC/EAN, etc.\n\nDespite claims to the contrary, only Sonic Solutions’ “Sonic System” software was able to generate PreMaster CDs. PMCDs were subsequently obsoleted by the more modern and generalized DDP specification. Several factors led to the decline of the PreMaster CD standard. First, the CD-R mechanisms that were able to read and write the hidden cue sheet metadata went out of production in the late 1990s. Second, only Laser Beam Recorders or LBRs manufactured by Sony were able to read PMCDs, which limited the formats adoption by replicators.\n\nMany CD replicators now accept regular CD-R discs in place of true PMCDs, which can be created using specialized Audio CD pre-mastering software. Unfortunately, CD-Rs formatted as “Audio CDs,” which really are Red Book-formatted Orange Book discs, are not designed for disc replication, only content distribution. The CIRC error correction used for the audio data on these discs is not as extensive as what is used for CD-ROM or DVD formats so, corruption of the audio data during readout due to dirt or mechanical damage could result in total loss of the audio data. Modern, professional pre-mastering software relies on the DDP format, which protects both the audio data and its associated metadata.\n\n"}
{"id": "233458", "url": "https://en.wikipedia.org/wiki?curid=233458", "title": "Pinking shears", "text": "Pinking shears\n\nPinking shears are scissors, the blades of which are sawtoothed instead of straight. They leave a zigzag pattern instead of a straight edge.\n\nThe pinking shears design was made by Samuel Briskman in 1931.\n\nPinking shears have a utilitarian function for cutting woven cloth. Cloth edges that are unfinished will easily fray, the weave becoming undone and threads pulling out easily. The sawtooth pattern does not prevent the fraying but limits the length of the frayed thread and thus minimizes damage.\n\nThese scissors can also be used for decorative cuts and a number of patterns (arches, sawtooth of different aspect ratios, or asymmetric teeth) are available. True dressmaker's pinking shears, however, should not be used for paper decoration because paper dulls the cutting edge.\n\nThe cut produced by pinking shears may have been derived from the plant called a pink, a flowering plant in the genus \"Dianthus\" (commonly called a carnation). The color pink may have been named after these flowers, although the origins of the name are not definitively known. As the carnation has scalloped, or \"pinked\", edges to its petals, pinking shears can be thought to produce an edge similar to the flower.\n\nThe word \"pink\" can be used as a verb dating back to 1300 meaning \"pierce, stab, make holes in\".\n"}
{"id": "38189848", "url": "https://en.wikipedia.org/wiki?curid=38189848", "title": "Rand Group", "text": "Rand Group\n\nRand Group, is an IT consulting and software company that implements business software utilizing Microsoft products. Founded in 2003, Rand Group was recognized in 2015 as 20th in Accounting Today magazine’s Top 100 VARs list. Rand Group is an atypical Microsoft Partner in that it has combined technical experts with Certified Public Accountants to provide specialized services focused on achieving positive financial outcomes. Headquartered in Houston, Texas, the company had 102 employees at the close of 2014, and four office locations – two in Houston, one in Dallas, and the fourth in Vancouver. In 2015, Rand Group briefed officials with the Mongolian government on applications of business technology to their petroleum and natural resource industry as part of a five-day workshop put on by the US Department of Commerce.\n\nRand Group was formed in 2003 by the acquisition of the technical consulting division of Hein & Associates LLP, a regional accounting firm. Between 1996 and 2003, prior to the formation, the same management team provided the same service offering at Hein. Rand Group received Gold Partner status in 2005, primarily as a result of its 90% client retention rate.\n\nSince 2004, Rand Group has consistently ranked in Accounting Today's VAR 100 list and is now considered the largest Texas-based Microsoft partner.\n\nTo-date, Rand Group has acquired the following firms:\n\nRand Group is heavily involved in the local community. In 2014, President and CEO Ron Rand was named Chairman of the Board of Houston Public Media Foundation Rand Group is the technology partner for the Museum of Fine Arts Houston and the Houston Symphony. The company is also an ongoing supporter of the Alley Theatre, Houston Symphony, Houston Zoo, and Big Brothers/Big Sisters. In past years, Rand Group has supported the National Multiple Sclerosis Society, and the Cystic Fibrosis Foundation.\n\nRand Group services center around helping people apply technology for business success. This includes implementing ERP and CRM enterprise software, such as Microsoft Dynamics products CRM, AX, GP, NAV, as well as Microsoft Azure Cloud, .NET technologies and SharePoint.\nThe company also creates intellectual property for companies that require rental management software to assist with maintenance scheduling, and inventory tracking. \nRand Group's services include application development, business intelligence, CRM and ERP implementation, intranets & portals,infrastructure services and sales and marketing services. Rand Group also provides ERP Selection services including information on NetSuite Implementation and Software Pricing , Dynamics 365, and SAP Business ByDesign Total Cost of Ownership.\n\nRand Group's technology solutions for business are tailored to the following industries:\n\n\nCEO and President Ron Rand was published in the July/August 2015 issue of CFMA Building Profits, the magazine for Construction Financial Professionals.\n\n"}
{"id": "34620959", "url": "https://en.wikipedia.org/wiki?curid=34620959", "title": "Range gate", "text": "Range gate\n\nA range gate is an electronic circuit that selects signals within a given time period; the \"gate\" allows signals to pass through only within the selected time. The term is mostly used in radar, where range gates are used to select certain targets for further processing. It is also used in lidar, time-of-flight cameras and similar roles.\n\nIn early military radars, range gates were used to select a single target and then pass on this information to other displays where more information could be seen. An example is the AI Mk. IX radar, where the radar operator would use a \"strobe\", an on-screen cursor, to select a single target. A range gate would then filter out all the other targets that might be visible to the radar. The return within that gate was then automatically tracked without further operator intervention.\n\nIn weather radar, it is common to have a series of continual range gates that separate out returns at different distances and then process them to extract Doppler shift to measure wind speed. In these cases, it is common to refer to each gate as a range bin.\n\n"}
{"id": "53382545", "url": "https://en.wikipedia.org/wiki?curid=53382545", "title": "Robot tax", "text": "Robot tax\n\nThe development of artificial intelligence (“AI”), in particular relative to robots, has gained increasing importance worldwide. First used exclusively in the industrial sector, the use of robots has now extended to all aspects of our life, including services and entertainment.\n\nThe progression of robots’ impact on our society necessarily requires new reflections, which, in addition to the ethical question, also imply social, economic, legal and job problems. Legislative resolutions will have to be adopted, which will in particular deal with the taxation of the use of robots, under penalty of serious consequences on world growth.Indeed, without legislative intervention, the scheme would be as follows:\n\n- On the one hand, the constant replacement of human activities by robots will have a major impact on employment. In the absence of adequate taxation, the situation is likely to cause serious tax losses and increase the social security deficit;\n\n- On the other hand, the loss of employment resulting from the hiring of robots instead of human beings will generate the need for additional financial resources, in particular in order to ensure a social minimum as well as professional reintegration.\n\nLeaders of the industrial and scientific communities argue that, to offset the social costs created by automation’s displacement effects, either robots should pay income tax, or their owners should pay a tax for replacing a worker with a robot. Further, \"robot tax\" should be used to finance a universal basic income or guaranteed living wage. Bill Gates said the solution to the problem is simple, the government should start taxing robots i.e. make the robot owners cough up the money needed to re-establish the defunct workforce. Such opinion is however highly criticized. For example, Mr Andrus Ansip, EU Commissioner tasked with bringing Europe to the digital age, is against such opinion. Mr Andrus Ansip considers that a robot tax would only mean someone else would take the leading position and leave Europe behind. Tshilidzi Marwala argues that many systems we use today have intelligent and robotics features and these make the task of taxing robots very difficult to achieve.\n\nThus, a new form of fiscal capacity (contributive capacity) should emerge with the development of robot autonomy.\n\nThe taxation of robots or their use presupposes a clear legal definition of their status. The idea is to envisage the creation of a new type of legal personality. Several definitions have been proposed, which generally focus on the autonomy and decision-making process of robots. On 1 January 2017, the European Union Parliament approved a report, accompanied by various recommendations, which envisages the emergence of an “electronic personality of robots\". \nIn tax matters, the central character should be the use of artificial intelligence, which will allow the robot to make its own decisions, to be autonomous and to learn, characteristics that go far beyond the state of a simple machine, and this regardless of its mechanical appearance.\n\nThe recognition of a legal personality specific to robots also implies the recognition of an electronic contributory capacity, which would allow characterising robots as legal persons, vested with various rights and obligations. Robots would consequently become fiscal subjects and become subject to special taxation.\n\nHowever, even if robots can replace a paid human activity, they do not yet have their own ability to pay (taxable income). It is not the robot as such that must be imposed but its use. A contributory capacity specific to robots could be admitted when the technology will allow robots to allocate them a capacity of payment (electronic liquidity, capital, etc.).\n\nWhen a fiscal capacity of robots has been recognized, several types of taxation could be considered. Assuming that the robot replaces a salaried human activity, one could consider a tax on the “hypothetical salary attributable\" to the robot, which would correspond to the income that the human being would earn for an equivalent job.\n\nThe World Bank opposes the robot tax, as it will reduce productivity and encourage tax avoidance by large corporations and their shareholders.\n\n\n"}
{"id": "47234448", "url": "https://en.wikipedia.org/wiki?curid=47234448", "title": "SP Nano Ltd.", "text": "SP Nano Ltd.\n\nSP Nano is an Israeli nanotechnology company commercializing the use of nanoparticles in components and products made from composite materials. Founded in 2007 as Fulcrum SP Materials Ltd. by founder Nimrod Litvak and co-founder Dr. Amnon Wolf with Prof. Arie Altman and Prof. Oded Shoseyov as scientific founders.\n\nSP Nano is the developer of SP1 - a revolutionary nano-reinforcement protein agent that will transform the composite materials and rubber industries by enabling the production of lighter, stronger and sustainable parts. SP Nano was established in 2007 is commercializing the use of carbon-based nanoparticles in the rapidly growing €140 billion mechanical rubber goods (MRG) and €84 billion composites markets. These markets includes raw materials such as carbon/glass fiber and aramid, and are commonly used in industries such as aerospace, construction, automotive and marine, wind energy and sports equipment. SP Nano’s proprietary SP1 technology can significantly improve the performance properties of composites; enabling cost-effective manufacture of stronger and lighter parts. From its headquarters in Israel, SP Nano is working in close cooperation with a number of leading multinational companies to develop a range of innovative new commercial products for various end-markets..\n\nSP Nano is the only company in the world that binds nano-particles to composites using a protein.\n\nIn March 2013, the company signed a commercial R&D agreement with Coats Plc.\n\nIn March 2015, the company changed its name from Fulcrum SP Materials Ltd. to SP Nano Ltd.\n\n"}
{"id": "58488367", "url": "https://en.wikipedia.org/wiki?curid=58488367", "title": "Slice (app)", "text": "Slice (app)\n\nSlice is an online food ordering platform for independent pizzerias. It allows local pizzeria owners to compete with large pizza chains through a mobile-optimized website and tools that enable customers to place orders through the Slice app and social media channels. The platform is used by over 9,000 independent pizzerias in more than 2,500 towns and cities across all 50 states. Slice processed more than $100 million worth of deliveries in 2017 and over 12 million orders since 2010.\n\nSlice was founded in 2010 by Ilir Sela, a third-generation family member in the pizza industry, and was originally called MyPizza. In July 2016, Slice closed on a $3 million Series A funding round. In May 2017, the company raised $15 million led by GGV Capital.\n\n"}
{"id": "1415244", "url": "https://en.wikipedia.org/wiki?curid=1415244", "title": "Smart camera", "text": "Smart camera\n\nA smart camera or intelligent camera is a machine vision system which, in addition to image capture circuitry, is capable of extracting application-specific information from the captured images, along with generating event descriptions or making decisions that are used in an intelligent and automated system. A smart camera is a self-contained, standalone vision system with built-in image sensor in the housing of an industrial video camera. It contains all necessary communication interfaces, e.g. Ethernet, as well as industry-proof 24V I/O lines for connection to a PLC, actuators, relays or pneumatic valves. It is not necessarily larger than an industrial or surveillance camera. A capability in machine vision generally means a degree of development such that these capabilities are ready for use on individual applications. This architecture has the advantage of a more compact volume compared to PC-based vision systems and often achieves lower cost, at the expense of a somewhat simpler (or omitted) user interface.\n\nAlthough often used for simpler applications, modern smart cameras can rival PCs in terms of processing power and functionalities. Smart cameras have been marketed since the mid 80s. In the 21st century they have reached widespread use, since technology allowed their size to be reduced and their processing power reached several thousand MIPS (devices with 1 GHz processors and up to 8000MIPS are available as of end of 2006).\n\nHaving a dedicated processor in each unit, smart cameras are especially suited for applications where several cameras must operate independently and often asynchronously, or when distributed vision is required (multiple inspection or surveillance points along a production line or within an assembly machine).\n\nA smart camera usually consists of several (but not necessarily all) of the following components:\n\n\nSmart cameras can in general be used for the same kind of applications where more complex vision systems are used, and can additionally be applied in some applications where volume, pricing or reliability constraints forbid use of bulkier devices and PC's.\n\nTypical fields of application are:\n\nDevelopers can purchase smart cameras and develop their own programs for special, custom made applications, or they can purchase ready made application software from the camera manufacturer or from third party sources.\nCustom programs can be developed by programming in various languages (typically C or C++) or by using more intuitive, albeit somewhat less flexible, visual development tools where existing functionalities (often called tool or blocks) can be connected in a list (a sequence or a bidimensional flowchart) that describes the desired flow of operations without any need to write program code.\nThe main advantage of the visual approach Vs. programming is in a much shorter and somewhat easier development process, available also to non-programmers.\nOther development tools are available with relatively few but comparatively high level functionalities, which can be configured and deployed with very limited effort.\n\nSmart cameras running software tailored for a single specific application are often called \"vision sensors\".\n\nSome consumer digital cameras are called \"Smart Camera\" due to features such as running a mobile operating system.\n\n"}
{"id": "22682196", "url": "https://en.wikipedia.org/wiki?curid=22682196", "title": "Smart highway", "text": "Smart highway\n\nSmart highway and smart road are terms for a number of different proposals to incorporate technologies into roads for generating solar energy, for improving the operation of autonomous cars, for lighting, and for monitoring the condition of the road.\n\n\"Intelligent transportation systems\" usually refers to the use of information and communication technologies (rather than innovations in the construction of the roadway) in the field of road transport, including infrastructure, vehicles and users, and in traffic management and mobility management, as well as for interfaces with other modes of transport.\n\nPhotovoltaic pavement is a form of pavement that generates electricity by collecting solar power with photovoltaics. Parking lots, footpaths, driveways, streets and highways are all candidate locations where this material could be used.\n\nIn 2013 Students at the Solar Institute at George Washington University installed a solar panel walking path designed by Onyx Solar, something they call \"solar pavement\".\n\n\"SolaRoad\" is a system being developed by the Netherlands Organisation for Applied Scientific Research (TNO), the Ooms Groep, Imtech and the Netherlands province of North Holland. They plan to install their panels on 100 m of cycle path in Krommenie, Netherlands in November 2014. A variant concept of a \"solar road\" installed in Avenhorn, by Ooms Avenhorn Holding AV, uses asphalt and tarmac to absorb the sun’s rays and heat water for use in domestic heating.\n\nThe Solar Roadways company of Idaho, USA, is developing a prototype system to replace current roads, parking lots, and driveways with photovoltaic solar road panels that generate electricity.\n\nSouth Korea has built a freeway with the median covered by solar panels above a bikepath.\n\nThe first photovoltaic road in the world was constructed in Tourouvre, Orne, France in 2016. Called \"Wattway\", it was built by Société Nouvelle Aeracem (SNA), and dedicated by the French Minister of Ecology, Ségolène Royal on October 25, 2016. The 1-km section of road opened to traffic on 22 December 2016. It is believed the road will provide enough power for the town's streetlights.\n\nThe Jinan solar highway opened in China in December 2017 along a 1.2 mile stretch. It uses transparent concrete on the top layer with the solar panels underneath. It was the second solar roadway in the city, the first opened in September 2017 using a different technology.\n\nThe main purpose of solar roadways is to replace asphalt roads with Solar Panels which generate energy through the sun that can be used by local houses or businesses that are connected to the system from either the house’s driveway or the businesses parking lot. The panels will also increase the number of charging stations for electric cars if that station is connected to the solar roadway. Each panel is roughly 12’ by 12’ of interlocking panels that have their own LED lights that will be used as the road lines, and can also be used to spell out words like “Reduce Speed” or “Traffic Ahead” to help the flow of traffic.\n\nThere are 3 layers that make up the solar panels:\n\n1. The Road Surface Layer - The Road Layer is the High Strength layer that has the photovoltaic cells which attracts the sun’s rays, it has traction so vehicles don’t slide off the road, and it’s waterproof to protect the layers below.\n\n2. The Electronic Layer - The Electronic Layers contain a mini microprocessor board that helps control the heating element of the panels, this technology can help melt the snow that lands on the panels so that hazardous road conditions will no longer be an issue in the more northern regions. This layer can sense how much weight is on the panels and can control the heating element to melt the snow.\n\n3. The Base Plate Layer - The Base Plate Layer is the layer that collects the energy from the sun and distributes the power to the homes or businesses that are connected to the solar roadways. This will also be used to transfer the energy to cars as they drive over the strip to recharge the battery.\n\n\"Slate\" magazine stated that solar roadways would produce less electricity than solar cells that are placed at an angle, and that less light would touch them because of shade, dirt covering the road, and cars blocking the sun from touching the panels. \n\nCritics have pointed out that solar roadways would be both more expensive, and less productive than more conventional ways of combining solar power with infrastructure, such as building shelters over roads and parking areas and putting traditional solar panels on the roofs; Elon Musk demonstrated that there is ample space in the US, apart from roads, to fulfill the power requirements of the country.\n\nThe Missouri Department of Transportation (MoDOT) began testing out “smart pavement” at a rest stop outside of Conway, Missouri along historic Route 66 late in 2016. The pilot program currently covers about 200 square feet of sidewalk at the visitor center and cost $100,000 (Landers), largely subsidized by the Federal Highway Administration. It’s all part of Missouri’s Road to Tomorrow initiative to find new innovations in their transportation infrastructure. Missouri wants to take advantage of these roadways to implement other, related technologies. The panels will heat the road and keep snow and ice from accumulating. They will also feature LED diodes that will increase the visibility of road lines. The LEDs would also double in helping prevent paint from inhibiting solar power generation. The panels have not had enough time to determine durability, energy efficiency, or cost effectiveness in a real world sense yet, so MoDOT has not reach any conclusions about feasibility and future application yet.\n\nThe Online Electric Vehicle being developed by KAIST (the  Korea Advanced Institute of Science and Technology) has electrical circuits built into the road which will power suitably adapted vehicles via contactless electromagnetic induction. A pilot system powering electric buses is under development. Germany's IAV is another company that is developing induction chargers.\n\nRoadway-powered electric vehicle system is the patent held by Howard R. Ross. It has several components. The first of which is an all electric vehicle that would be fit with electromechanical batteries that accept a charge from the road. The road is the second component and would have strategically placed charging coils as to only charge the car when needed. These cars and roads would not require gas or solar power.\n\nNowhere in the world is an invention like this currently implemented, and this is due to the cost of the infrastructure overhaul that would be needed to bring this patent into reality.\n\nThe \"Smart Highway\" concept developed by Studio Roosegaarde and the infrastructure management group Heijmans in the Netherlands incorporated photo-luminescent paint for road markings, which absorb light during the day then glow for up to 10 hours. The technology was demonstrated on a stretch of highway in Brabant, Netherlands.\n\nSnowmelt systems using electricity or hot water to heat roads and pavements have been installed in various locations.\n\nSolar Roadways has proposed including a snowmelt system with their photovoltaic road panels since the panels already have electrical power connections for harvesting photovoltaic power. Skeptics point to the energy requirements.\n\nICAX Limited of London's \"Interseasonal Heat Capture\" technology captures solar energy in thermal banks and releases it back under a roadway, heating it and keeping asphalt free of ice.\n\n\n"}
{"id": "9775906", "url": "https://en.wikipedia.org/wiki?curid=9775906", "title": "Solar cell fabric", "text": "Solar cell fabric\n\nSolar cell fabric is a fabric with embedded photovoltaic (PV) cells which generate electricity when exposed to light.\n\nTraditional silicon based solar cells are expensive to manufacture, rigid and fragile. Although less efficient, thin-film cells and organic polymer based cells can be produced quickly and cheaply. They are also flexible and can be stitched onto fabric.\n\nAccording to an article from New Scientist researchers have built a PV cell in the layers around a fiber, creating a tiny cylindrical cell. No longer limited to rooftops and poles, solar collection could work silently and unobtrusively from everyday objects.\n\nFlexible solar cells can be used in humanitarian aid. A makeshift shelter developed by PowerFilm, Inc. called the PowerShade can generate one kilowatt of power. This could help a power emergency equipment at short notice in remote places.\n\nKonarka Technologies produce a thin film polymer based PV cell, as a flexible film stitched onto a fabric. The ability to make these cells even smaller is dependent on further research into nanocrystal PV cells. In theory nanotechnology could provide a way to expand the range of photons a cell could collect, increasing its efficiency while becoming smaller. Konarka, in partner with other institutions, is working on this.\n\nShadePlex is currently developing a product that integrates thin film photovoltaic modules with architectural fabrics. They will feature a high power output (200 W, 500 W, and 1000 W), and can feed either a battery system or be tied to the grid. Integrating thin film photovoltaics with fabric structures will enable a whole class of buildings to easily integrate renewable energy solutions.\n"}
{"id": "11654392", "url": "https://en.wikipedia.org/wiki?curid=11654392", "title": "TAI Anka", "text": "TAI Anka\n\nThe TAI Anka is a family of unmanned aerial vehicles (UAV) developed by Turkish Aerospace Industries (TAI) for the requirements of the Turkish Armed Forces. Basic Anka-A is classified as a Medium Altitude Long Endurance (MALE) UAV. Envisioned in the early 2000s for tactical surveillance and reconnaissance missions, the Anka has now evolved into a modular platform with synthetic aperture radar, precise weapons and satellite communication. The drone is named after a phoenix-like mythological creature called \"Zümrüd-ü Anka\" (\"Anka kuşu\" in Turkish.)\n\nThe TUAV system consists of three air vehicles (A/V), Ground Control Station (GCS), Ground Data Terminal (GDT), Automatic Take-off, and Landing System (ATOLS), Transportable Image Exploitation System (TIES), Remote Video Terminal (RVT) and various Ground Support Equipment (GSE).\n\nThe TUAV system, which is designed for night and day missions including adverse weather conditions, performs real-time image intelligence, surveillance, reconnaissance, moving/stationery target detection, recognition, identification, and tracking missions.\n\nWhile the TUAV system has an open architecture to support other potential payloads and missions, within the context of the existing project the air vehicle is configured to carry the following payloads on board:\n\n\nThe whole composite airframe is composed of a monoblock fuselage, detachable wing and V-Tail, retractable landing gear, redundant control surfaces, avionics and payload bays and service doors. The sandwich skin structure is reinforced by composite or metallic frames, ribs, and supports. Propelled by a pusher type heavy fuel engine, the aircraft is furnished with fuselage fuel tanks and fuel system, ice protection system, environmental control system, lighting system, redundant electrical system with battery backup, and harness system.\n\nThe platform is also equipped with a digital flight control system, electro-mechanical actuators, and flight control sensor systems such as GPS, pitot-static, air data computer, navigation sensor, transducers, temperature, pressure, displacement sensors, etc. Various tasks are distributed along flight management computers and auxiliary control boxes. Identification and communication units and interface computers are employed in order to establish real time wide band communication and provide test and diagnostics functions. An air traffic radio is also integrated in the communication system for the integration of the aircraft into the civilian airspace. All flight critical equipment are dual or triple redundant and emergency modes of operational scenarios are taken into consideration for fail safe design.\n\nAll airborne and ground-based flight control software is developed by TAI while payload hardware and software items are aimed to be developed by national sub-contractors, such as Aselsan and Milsoft.\n\nUAV operations are supported by highly sophisticated ground control system with complete redundancy, developed by a domestic defence company Savronik. Whole mission segments of the air vehicle can be managed, monitored and controlled by a GCS. A pre-programmed mission plan can be loaded before the flight begins or can be altered during the flight. All the imagery stream of the payloads can be displayed and recorded in real time and all the payloads can be controlled from the GCS. ATOLS allows the air vehicle to perform its operation without operator intervention, including the most critical phases which are landing and take-off.\n\nIn TIES, valuable intelligence information can be obtained by the analysis of bulky imagery data. TIES operators can initiate intelligence missions prior to or during flight. Refined information flows to the upper command layer in order to assist the headquarters to monitor a network of TUAV systems and benefit from the gathered intelligence information. Another interface of the TUAV system is the RVT, with which other friendly units who are close to the target area can utilize the real time imagery that TUAV air vehicle broadcasts.\n\nThe contract regarding the development of an indigenous Medium Altitude Long Endurance (MALE) Unmanned Aerial Vehicle (UAV) system for the reconnaissance requirements of the Turkish Armed Forces became effective on 24 December 2004. Within the framework of the program, a total of three prototypes and ground systems will be designed, developed, manufactured, and tested by mid-2011 as part of the prototype development phase. Subsequently, in 2012, the serial production phase of Anka-A would be launched and additional 10 systems (meaning 30 air vehicles) built for the Turkish Air Force.\n\n\nOn July 19, 2012, the Turkish Defense Industry Executive Committee (SSIK) announced that Turkish Aerospace Industries had commenced research and development for the design and development of a \"hunter killer\" High-Altitude Long-Endurance version of the Anka UAV, named the Anka +A. It was planned that Anka +A will carry Cirit missiles of Turkey's Roketsan. The engines of the Anka +A UCAV had not yet been determined. It might have more powerful turbo engines or it could have gas turbine engine. The weight of Anka +A UCAV would be more than 4 tons compared to Anka Block A's 1.5 tons. It was highly expected that the UCAV would be presented to public in the events of IDEF'13 on 7–10 May 2013.\n\nTurkey's Directorate for Defence Industries has been stressing advanced variants of the Anka with larger payload capacity, extending Block A capabilities to the features like:\nAccording to the authority, the Anka will eventually have an indigenous 155 hp turbo-prop engine developed by TUSAŞ Engine Industries (TEI), with the cooperation of local companies in the future.\n\nOn January 30, 2015, the ANKA-B completed its maiden flight successfully. Anka Block B is an improved version of the Anka Block A. The UAV carries an Aselsan synthetic aperture radar/ground moving-target indicator payload in addition to the platform’s electro-optical/infrared sensor. During the maiden flight, Anka-B successfully performed \"basic shakedown\" and auto-landing. The Anka Block B also has a greater payload capacity than that of the Anka-A which includes SAR/ISAR/GMTI radar (in addition to the cameras of Anka A) that obtains and remits high resolution intelligence data back to base. The Anka Block B paved the way for weaponisation of the platform in the foreseeable future. Anka Block B passed 30.000 feet, 26 hours and 200 km radius during the test flights. Turkish Air Force ordered 10 Anka Block B in 2013 at a cost of $300 million.\n\nANKA-S is a serial production configuration of ANKA. It is equipped with ViaSat's VR-18C Highpower SATCOM antenna and a national flight control computer. Like Block A and Block B, Anka-S will be powered by Thielert Centurion 2.0S. On the other hand, Turkish Engine Industries (TEI) has been developing a national engine, capable of running with diesel and JP-8 jet fuel.\n\nOn October 25, 2013, Turkish Undersecretariat for Defence Industry (SSM) ordered 10 Anka-S UAVs and 12 ground control stations for $290 million ($220.6 million + TRY 137 million). The UAVs would be delivered in three batches (2+4+4). The first batch was planned to be delivered in March 2017. A total of 6 aircraft were planned to be delivered in 2017.\n\nIn 2016, media reported that the TAI was manufacturing 4 Anka-S UAVs for the armed forces. The first two of these aircraft were to be equipped with StarFIRE 380-HDL FLIR payload. However, these would be replaced with Aselsan CATS later on..\n\nOn August 17, 2018, Directorate for Defence Industries announced that the Anka-S completed its first live fire tests. The platform was tested with MAM-L ammunition developed by the Roketsan. In September, Ismail Demir, director of the Turkey's defence industry authority, shared a picture of the first Anka-S equipped with Aselsan CATS optical system. TAI delivered 2 more ANKA-S to Turkish Air Force in September 2018, increasing the Anka-S inventory of Turkish Air Force to 8 aircraft. TAI is planning to deliver a total of 10 Anka-S to Turkish Air Force before 2019.\n\nhttps://m.youtube.com/watch?v=pyoP8HJcJ4g\n\nThe Anka performed its first mission flight on 5 February 2016 in Turkey’s eastern province of Elazig performing a four-hour exploration and observation flight.\n\n\n\n\n\n\n"}
{"id": "33112535", "url": "https://en.wikipedia.org/wiki?curid=33112535", "title": "Tastet", "text": "Tastet\n\nTastet is a word of Catalan origin meaning 'a small taste of food'. The tastet is a variation of the tapas serving style; larger and more complex than tapas but smaller than a plate. Emphasis is placed on taste and presentation. The tastet can be served as an appetizer or as a small collection for a complete meal. The tastet serving style allows for the appreciation of several flavours within a single meal.\n\nRestaurants may favour the tastet as a specialised menu format, and may offer all their tastets at a uniform price. One such restaurant is \"El Jardi Del Tastets\" in Palamos, Spain. \n"}
{"id": "52154682", "url": "https://en.wikipedia.org/wiki?curid=52154682", "title": "Tech companies in the New York metropolitan area", "text": "Tech companies in the New York metropolitan area\n\nTech companies in the New York City metropolitan area represent a significant and growing economic component of the New York metropolitan area, the most populous combined statistical area in the United States and one of the most populous urban agglomerations in the world. In the region's Silicon Alley, new establishments include those of Israeli companies in New York City, at a rate of ten new startups per month. Below is a partial and growing list of notable New York metropolitan area tech and start-up companies:\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8838269", "url": "https://en.wikipedia.org/wiki?curid=8838269", "title": "Thermo-hygrograph", "text": "Thermo-hygrograph\n\nA thermo-hygrograph or hygrothermograph is a chart recorder that measures and records both temperature and humidity (or dew point). Similar devices that record only one parameter are a thermograph for temperature and hygrograph for humidity.\n\nThermographs where the variations are recorded using photography were described by several scientists as early as 1845, including Francis Ronalds who was Honorary Director of the Kew Observatory. An updated model of the initial machine was deployed across the national observational network set up by the new UK Met Office in 1867 and coordinated by Kew Observatory. These instruments then saw extended use around the world.\n\nAn alternative thermograph configuration has a pen that records temperature on a revolving cylinder. The pen is at the end of a lever that is controlled by a bi-metal strip of temperature-sensitive metal which bends as the temperature changes. A human hair bundle can be used for humidity in such machines.\n"}
{"id": "42331359", "url": "https://en.wikipedia.org/wiki?curid=42331359", "title": "Tomb of Ferdowsi", "text": "Tomb of Ferdowsi\n\nTomb of Ferdowsi () is a tomb complex composed of a white marble base, and a decorative edifice erected in honor of the Persian poet Ferdowsi located in Tus, Iran, in Razavi Khorasan province. It was built in the early 1930s, under the Reza Shah, and uses mainly elements of Achaemenid architecture to demonstrate Iran's rich culture and history. The construction of the mausoleum as well as its aesthetic design is a reflection of the cultural, and geo-political status of Iran at the time.\n\nFerdowsi, the influential Persian poet and author of the Persian epic, Shahnameh died in 1020AD in the Tus, Iran (Persia), in the same city in which he was born. For all his literary contribution, Ferdowsi was not recognized during his life. It was only after his death that his poems won him admiration. For hundreds of years, his resting place was nothing more than a minor dome-shrine erected by a Ghaznavid ruler of Khorasan, without any permanent edifice in place in the garden of his house where Ferdowsi's daughter had originally buried him. In the beginning years of the 20thcentury Iran started to realize his critical role in defining the identity of Iran.\nIt was not until 1934 that the Iranian government, then under the control of Reza Shah, first king of the Pahlavi dynasty, recognized the cultural and literary value of Ferdowsi and erected a permanent tomb in his honor. A millennial celebration was also held for the poet, to which were invited scholars from several countries, including Soviet Tajikistan, India, Armenia, and Europe (Germany, France, England). Funds were collected, mainly donations from Parsi scholars, to enable the building of a statue for the poet at his tomb site. The Pahlavi family used Ferdowsi to advance Iran's cultural prestige, but in doing so nearly cost Ferdowsi his tomb since, after the Islamic revolution, frustrations with the Shah of Iran nearly led to destruction of the tomb by the revolutionaries.\nThe tomb was originally designed by the Iranian architect, Haj Hossein Lurzadeh who aside from Ferdowsi's tomb also created some 842 mosques, as well as the private palace of Ramsar, part of the decoration of the Marmar palace, the Imam Hossein Mosque in Tehran, the Motahari Mosque, and various parts of the Hazrat-i-Seyyed-o-Shouhada shrine in Karbala, Iraq. The present design of the structure is credited mainly to Karim Taherzadeh, who replaced the old dome-shaped design by Lurzadeh with a modern cubical design.\n\nFerdowsi's tomb, which resembles the tomb of Cyrus the Great, is built in style of Achaemenid architecture. There is a clear link between this choice of architectural style and the politics of Iran at the time. Four years before Reza Shah came to power in 1922, a group of secular Iranian reformists had created the \"Society for National Heritage\" (SNH, or in Persian \"anjoman-e asar-e meli\"). Composed mostly of western-educated, pro-reform intellectuals such as Abdolhossein Teymourtash, Hassan Pirnia, Mostowfi ol-Mamalek, Mohammad Ali Foroughi, Firuz Mirza Firus Nosrat al-Dowleh, and Keikhosrow Shahrokh, the SNH was critical in obtaining the funds from the Iranian parliament. Keikhosrow Shahrokh, Iran's Zoroastrian representative to the Iranian parliament, was particularly active in reviving Achaemenid and Sassanid architecture in Iran in the 1930s.\n\nThe basic structure of the tomb is rectangular with a large garden surrounding the structure and interacting with the structure in the Persian style of gardening known as Char-bagh (or Chaharbagh translating to \"four gardens\" in Persian). In the center of the cross created by the legs of the garden surrounding it, is an edifice made of primarily white marble. The edifice can be divided into a \"wide chamber\" that lies at the base and a cubical erection on top, with four pillars surrounding it and scenes from the epic of Shahnameh and text ornating it. The body of the poet is actually interred in the center of the rectangular wide chamber underneath the overlying four pillars cube. There are twelve (12) steps leading from the lowest point of the wide chamber all the way to the level of the cube. The wide base has a total height of 16 m. The edifice has equal dimensions of 30 m on each side.\n\nThe following are schematic diagrams of the aerial view of the tomb's wide base and edifice section and their topography:\n\nA unique feature of the design of Ferdowsi's tomb has been its resemblance to that of Cyrus the Great in Pasargadae. Cyrus's tomb also has a rectangular structure seating atop a rectangular, gradually elevating base. This resemblance is intentional as the designer of this edifice intended to revoke the original Achaemenid style of architecture. In fact every other facet of the edifice has a Zoroastrian symbol known as Faravahar. This is not coincidental. There are multiple applications of this in the Achaemenid architecture mainly in Persepolis in Fars province today. The \"Society for National Heritage of Iran\" (SNH) heavily relied on the use of Faravahar as this was the symbolic representation of ancient Iran since Achaemenid times. Many construction in the 1930s, including the then National Bank of Iran use Faravahar which is not unexpected considering that the same architect that created Ferdowsi's tomb also created the National Bank of Iran.\n\nA closer look at the edifice points out that there are four columns each at the corner of the rectangular structure with two half-buried columns that protrude as deep friezes on each facet of the structure. Each frieze column has a box, followed by a two horn bull sign which is very much similar if not the exact imitation of the Persepolis column design. The columns are ornated with fluting 3/4 of the way down with the last portion spared. The overall effect is intended to create a grand gesture. The columns are as high as the edifice which is 30 meters high. (the edifice is also 30 m wide). Marble decorations are used to ornate the siding and the floor of the \"wide base\" structure as well the wall. Persian flower designs (concentric flower designs composed of a flower with seven (7) ovaloid pellets surrounding a central circle), and hexagonal marble designs are commonly used in the structure.\n\nComparison of the Persepolis columns, and the columns used in Ferdowsi's mausoleum:\n\nIran's history has been closely tied to geopolitical changes that has taken place since the establishment of the Achaemenid empire in Persis all the way to the modern day Iran. Two major events are of critical importance in Iran's history specially its literary history as it pertained to Ferdowsi: Arab conquest of Persia and the Mongolian invasion of Persia.\n\nFerdowsi lived his life as a poor man constantly moving from court to court, and eventually died a poor widower, having lost his only son. Tus, at one point was an opulent city in the greater Khorasan region but it was repeatedly sacked by Oguz Turks, Mongols, and Uzbeks from the steppe. This and the growing influence of Mashad as a political and religious center within Khorasan shaped Ferdowsi's experience and in many way influenced his writing as Tus lost prestige. Additionally, Arabic had found prestige in lands conquered by the Arabs and there was threat of Middle Persian being lost in favor of Arabic. Ferdowsi's role is critical in that using the least number of loan-words he transferred the Middle Persian (Pahlavi) into Modern Persian (Farsi).\n\nIn the time frame preceding the construction of the mausoleum, nationalistic feelings in Iran were high. There was a renewed sense of national identity partly due to the pressures felt by foreign powers including the constant Anglo-Persian political struggle specially over the issues of oil, and partly due to inability of the Qajar dynasty from protecting Iranian lands in central Asia to the Russians and in the east to the British. Anglo-Iranian Oil Company (AIOC) was an important source of contention for Iranians. One journalist studying during this time period reflects his and his colleagues personal experience:\n\nThe architecture of Ferdowsi's tomb is also influenced by poet's own personal life, reflecting a constant struggle between the poor poet and the lazy king, and adversity and hope. The Society for National Heritage in 1930s drawing on poet's attempt to revitalize the Persian language, also attempted to revitalize Persian culture and Iranian identity through architecture. This was in many ways taken literary with Persian poems from Shahnameh etched into the white marble facets of the edifice of the poet's mausoleum.\n\nAfter the Iranian revolution, both tomb of Ferdowsi and even mausoleum of Cyrus the Great survived the initial chaos. One of the most dangerous threats to the structure was that it would be equated with the late Pahlavi dynasty by the new regime and destroyed. It however was not and was instead embraced by the new local government since Ferdowsi was a devout Muslim.\n\nFerdowsi's Shahnameh inspires tales of heroic act by protagonists fighting against their antagonists. In that sense it is a national epic that encompasses not only fictional and literary figures but also incorporates parts of the history of pre-Islamic Iran. This has led to the interior of the edifice of Ferdowsi to reflect the same heroic scenes. The chief architect responsible for the design of the interior of the tomb of Ferdowsi is Feraydoon Sadeghi who created deep frieze scenes using three dimensional statues each depicting a scene from Shahnameh. Rostam, the hero of the book of Shahnameh is the focus of the majority of the scenes inside of the edifice. As Shahnameh is essentially a text, artistic recreation of its heroic scenes are multiple. Centered inside the edifice surrounding by the frieze scenes and other artistic endeavors is the tomb stone of the poet. Etched in the tomb stone in Farsi (Persian) is the description of Ferdowsi's contribution to the Persian-speakers and at the end it ends by denoting the poet's date of birth, date of death, and the date at which the mausoleum was built.\n\nThe Persian content of the tomb is as follows:\n\nبنام خداوند جان و خرد. این مکان فرخنده آرامگاه استاد گویندگان فارسی‌زبان و سراینده داستان‌های ملی ایران، حکیم ابوالقاسم فردوسی طوسی است که سخنان او زنده‌کننده کشور ایران و در دل مردم این سرزمین جاودان است\n\nThe English translation is roughly as follows:\n\nToday Ferdowsi's tomb is one of the most photographed in Iran. Millions of visitors from various provinces of Iran come to see the tomb every year. Foreign dignitaries, tourists, and other Persian-Speaking civilians from Europe, Asia, and Middle East also visit the site. The most recent was a visit from the Iraqi tourism minister in July 2013. The site has also inspired many Persian poets including Iranian poet Mehdi Akhavan-Sales who is actually physically buried not far from the tomb of Ferdowsi, in his own tomb in the grounds of Ferdowsi's complex.\n"}
{"id": "27797130", "url": "https://en.wikipedia.org/wiki?curid=27797130", "title": "Torpedo (petroleum)", "text": "Torpedo (petroleum)\n\nA torpedo is an explosive device used, especially in the early days of the petroleum industry, to fracture the surrounding rock at the bottom of an oil well to stimulate the flow of oil and to remove built-up paraffin wax that would restrict the flow. Earlier torpedoes used gunpowder, but the use of nitroglycerin eventually became widespread. The development of hydraulic fracturing rendered torpedoes obsolete, and is the primary fracturing process used today.\n\nA torpedo consisted of canisters that were filled with an explosive and lowered into a well via a rope or wire. Gunpowder was used in the first torpedoes, but nitroglycerin was found to work better despite its instability. The well is usually filled with water to prevent the explosion from escaping upwards. Originally, the topmost canister had a percussion cap that was to detonate the main charge. An iron weight was dropped down the well to set the torpedo off. After incidents of premature explosions, a second method was developed in which a tube of the explosive was placed in a larger tube that was packed with sand. A fuse was wound around the inner tube, connected to a blasting cap. When the torpedo was to be used, the inner tube was filled with nitroglycerin and corked; the fuse was lit and torpedo was dropped down the well.\n\nTorpedoes were generally used to remove buildup of paraffin wax from an oil well. Before the use of torpedoes caught on, boiling water or benzene was often poured down wells to try to dissolve the paraffin. Torpedoes were also used to fracture the rock to allow the oil to flow more easily.\n\nEdward A. L. Roberts developed the first torpedo and submitted a patent application in November 1864. Roberts, an American Civil War veteran, came up with the concept of using water to \"tamp\" the resulting explosion, after watching Confederate artillery rounds explode in a canal at the Battle of Fredericksburg. Roberts developed his first torpedoes in 1865 and 1866. In November 1866 he was granted a patent on his torpedo application, and founded the Roberts Petroleum Torpedo Company. William Reed also developed a torpedo design and went on to found a rival company \"for the purpose of infringing and breaking down the Roberts patent. Roberts charged $100-200 per torpedo as well as a royalty amounting to of the increased oil production. To avoid paying the exorbitant fees, an owner of a well would often hire men who illegally produced their own torpedoes and used them at night—the practice giving rise to term \"moonlighting\". Roberts spent $250,000 to protect his patent from the \"moonlighters\" by hiring the Pinkerton National Detective Agency and filing numerous lawsuits. Roberts' torpedo patents expired in 1879.\n\nTorpedoes manufactured today use modern explosives, with the last nitroglycerin torpedo being used on May 5, 1990.\n\n"}
{"id": "45638", "url": "https://en.wikipedia.org/wiki?curid=45638", "title": "Undocumented feature", "text": "Undocumented feature\n\nUndocumented features also known using the term Feature, not a bug are software features that are frequently found in software releases. Sometimes the documentation is omitted through simple oversight, but undocumented features are often essential elements of the software that are not intended for use by end users, but left available for use by the vendor for software support and development.\n\nSince the suppliers of the software usually consider the software documentation to constitute a contract for the behavior of the software, undocumented features are generally left unsupported, and may be removed or changed at will and without notice to the users.\n\nUndocumented features (for example, the ability to change the switch character in MS-DOS, usually to a hyphen) can be included for compatibility purposes (in this case with Unix utilities) or for future-expansion reasons. However; if the software provider changes their software strategy to better align with the business, the absence of documentation makes it easier to justify the feature's removal.\n\nNew versions of software might omit mention of old (possibly superseded) features in documentation but keep them implemented for users who've grown accustomed to them.\n\nWhile an incorrect use of the term, in some cases software bugs are referred to -jokingly- as undocumented features. (\"It's not a bug; it's an undocumented feature!\") This usage may have been popularised in some of Microsoft's responses to bug reports for its first Word for Windows product, but doesn't originate there. The oldest surviving reference on Usenet dates to 5 March 1984. Between 1969 and 1972, Sandy Mathes, a systems programmer for PDP-8 software at Digital Equipment Corporation (DEC) in Maynard, MA, used the terms \"bug\" and \"feature\" in her reporting of test results to distinguish between undocumented actions of delivered software products that were \"unacceptable\" and \"tolerable\", respectively. This usage may have been perpetuated.\n\nIronically, undocumented features themselves have become a major feature of computer games. Developers often include various cheats and other special features (\"easter eggs\") that are not explained in the packaged material, but have become part of the \"buzz\" about the game on the Internet and among gamers. The undocumented features of foreign games are often elements that were not localized from their native language.\n\nClosed source APIs can also have undocumented functions that are not generally known. These are sometimes used to gain a commercial advantage over third-party software by providing additional information or better performance to the application provider.\n\n"}
{"id": "11746328", "url": "https://en.wikipedia.org/wiki?curid=11746328", "title": "Vehicle regulation", "text": "Vehicle regulation\n\nVehicle regulations are requirements that automobiles must satisfy in order to be approved for sale or use in a particular country or region. They are usually mandated by legislation, and administered by a government body. The regulations concern aspects such as lighting, controls, crashworthiness, environment protection and theft protection.\n\n\nThe first steps toward harmonising vehicle regulations internationally were made in 1952 when WP.29, a working party of experts on technical requirements of vehicles was created. This resulted in the 1958 Agreement on uniform conditions of approval and mutual recognition of approvals of vehicles, components and parts. There was a new agreement in 1998 whose objectives were to improve global safety, decrease environmental pollution and consumption of energy and improve anti‐theft performance of vehicles and related components and equipment through establishing global technical regulations (GTRs) in a Global Registry based on UNECE Regulations or on national regulations listed in a Compendium of candidates, GTR harmonising them at the highest level. In 2000, WP.29 became the World Forum for Harmonization of Vehicle Regulations that is a working party of the United Nations Economic Commission for Europe(UNECE).\n\n"}
{"id": "42091943", "url": "https://en.wikipedia.org/wiki?curid=42091943", "title": "Von Karman Gas Dynamics Facility", "text": "Von Karman Gas Dynamics Facility\n\nThe von Karman Gas Dynamics Facility at Arnold Engineering Development Complex, Arnold Air Force Base, Tennessee, provide aerothermal ground test simulations of hypersonic flight over a wide range of velocities and pressure altitudes. The facility consists of three Hypersonic wind tunnels: Tunnel A, B, and C. The wind tunnels can be run for several hours at a time thanks to a 92,500 horsepower air compressor plant system. The test unit is owned by the United States Air Force and operated by National Aerospace Solutions.\n\nTunnel A is a 48-inch squared, continuous, closed-circuit, variable density, supersonic wind tunnel with a Mach number range of 1.5 to 5.5 with a maximum temperature of 290 degrees Fahrenheit. Devoted primarily to explorations of aerodynamic design, Tunnel A's unique feature is its computer controlled continuous-curvature nozzle that can vary the tunnel's Mach number.\n\nTunnel B is a 50-inch, closed-circuit hypersonic tunnel with continuous-flow capability with a Mach number capability of 6 and 8. Provided with air heated to a maximum of 900 degrees Fahrenheit with natural gas-fired heaters. Tunnel B is also primarily explores aerodynamic design.\n\nTunnel C is a continuous-flow tunnel with a Mach number capability of 4, 6, and 10. It offers an aerothermal environment for testing materials proposed for use on space vehicles and aircraft. The one-of-a-kind wind tunnel makes it possible to subject flight hardware to a combination aerodynamic and thermodynamic—or heating—effects up to 1,440 degrees Fahrenheit so engineers can study how aerospace vehicles and material response to the combined effects of external heating, internal heat conduction and pressure loading.\n\n"}
{"id": "1397151", "url": "https://en.wikipedia.org/wiki?curid=1397151", "title": "Wilkinson Sword", "text": "Wilkinson Sword\n\nWilkinson Sword is a brand owned by Edgewell Personal Care for razors and other personal care products sold in Europe. It was founded as a company in London in 1772 by Henry Nock as a manufacturer of guns, made in Shotley Bridge in County Durham.\n\nBesides swords, the company has also produced guns, bayonets and products such as typewriters, garden shears, scissors and motorcycles. Gardening equipment is still made under the Wilkinson Sword name by E.P. Barrus under a licensing arrangement. Wilkinson Sword has manufactured its products in three UK locations over the years: in London, (Chelsea and Acton), Cramlington in Northumberland and Bridgend in Wales, where it made gardening tools. In 2000, the company closed its razor plant in the UK and consolidated production in Germany.\n\nHenry Nock began trading as a gunlock smith out of Mount Pleasant in London in 1772. In 1775, he formed Nock, Jover & Co. with William Jover and John Green. The American Revolutionary War led to strong sales for the new company. In 1776, the Board of Ordnance granted Nock, Jover & Co. an advance of £200 to start producing bayonets and in 1779 the company won a contract to produce 500 seven-barreled volley guns for the Royal Navy. Although designed by James Wilson, these would become known as Nock Volley Guns or Nock guns.\n\nWhen Henry Nock died in 1804, he left the company to his foreman and adopted son-in-law, James Wilkinson. When James' son Henry Wilkinson joined the company it was renamed James Wilkinson & Son (also known as simply Wilkinson & Son). It became the Wilkinson Sword Company in 1891.\n\nWilkinson Sword produced some of the earliest motorcycles in 1903. These were two-cylinder machines with Belgian engines made by Antoine, which were marketed by a garage in Chelsea, London – one of the first motorcycle dealerships in the UK. The venture was not a success however. In 1911, Wilkinson developed and manufactured the Wilkinson TMC, a luxury touring motorcycle between 1911 and 1916, when production was stopped by World War I. The first 'Wilkinsons' were designed for military reconnaissance by P G Tacchi, who was granted a patent for the design in 1908. Demonstrated to the British military in the summer of 1908, the Wilkinson motorcycle failed to impress the authorities, despite optional accessories including a sidecar complete with Maxim machine gun – and a steering wheel instead of handlebars. Undaunted, the company continued development and exhibited a new version a year later at the Stanley Clyde Motorcycle Show at the Agricultural Hall, Islington, London in 1909. Only about 250 Wilkinsons were produced before World War I. Restrictions brought the line to its end in spring 1916, and Wilkinson had to produce thousands of bayonets for the war effort. After the war, they decided to continue to develop the in-line four engine – but in a new car called the Deemster, and they never resumed motorcycle production.\n\nIn 1962, Wilkinson Sword introduced stainless steel razor blades and soon the company's blades made rapid gains in shares of the market, because one blade, though somewhat more expensive, could be used for a week. The earlier carbon steel razor blades rusted quickly enough that many people used a new blade each day. So, although Wilkinson gained a larger percentage of the market, the demand for razor blades declined to approximately 14 percent of its previous level. This introduction gave Wilkinson a substantive market share and previous market leaders quickly responded by introducing their own stainless blades. The technology had been available for some time, but the market leaders such as Gillette, which held a patent on stainless blades, presumably knew that any gain for them in market share would be overwhelmed by the dramatic reduction in the size of the market.\n\nIn 1973, Wilkinson Sword merged with the British Match Corporation to form Wilkinson Match. This was intended to create a stronger company, with a larger advertising budget that would enable the company to fight its American rival in the consumer shaving market, the Gillette Company, and its British subsidiary, also called Gillette. In this advertising war, Wilkinson Sword loudly touted its long and proud tradition of bladesmithing in its print and electronic media advertisements. That same year Wilkinson purchased the American pen and lighter company Scripto, Inc. in an attempt to diversify its holdings.\n\nAllegheny Ludlum Industries of Pittsburgh purchased Wilkinson Match in 1978. After becoming Allegheny International, Inc., the company filed for bankruptcy reorganization in 1987. Allegheny sold Wilkinson Match in 1986 to Swedish Match, which merged with Stora Group two years later. In 1989, Gillette helped finance a buyout of the Swedish Match consumer products division, which included Wilkinson Sword, by the Netherlands-based Eemland Holdings, giving Gillette a 22% stake in Eemland. After Gillette was ordered by the European Community Commission in 1992 to sell its interest in Eemland, Eemland sold Wilkinson Sword to Warner-Lambert, owner of Schick razor brand forming Schick-Wilkinson Sword. The Schick name was used on its products in North America and Japan, and the Wilkinson Sword name in Europe. In 2000, Pfizer acquired Warner-Lambert and three-years later, divested the Wilkinson component.\n\nMost of the former Bryant and May operations of Wilkinson Match were closed or sold in the late 1970s and early 1980s, including the Bryant and May factories in Bow and Melbourne. The gardening tools division was sold to Fiskars in the 1990s.\n\nDuring this time, Wilkinson Sword produced ceremonial swords for the Household Cavalry of the British Army, and crafted the ceremonial sword for the Golden Jubilee of Elizabeth II in 2002. The sword factory combined state of the art manufacturing technology with traditional skills and 19th century machinery to produce original fighting quality swords.\n\nThe production of swords came to an end when the company's sword factory at Acton closed in September 2005. Wilkinson Sword then held an auction of the tools, equipment, sword drawings, and forging and milling machinery. Robert Pooley, who had commissioned the company to produce swords, bought many of these items and formed Pooley Sword to supply the Army in place of Wilkinson Sword. Other sword manufacturers, and in particular WKC in Germany, also bought items, including the roll forge. Many of the tools and machines are still in use today and classic knives such as the Fairbairn-Sykes are produced by the company.\n\nEnergizer Holdings bought Wilkinson Sword from Pfizer in 2003, along with Schick. In 2015, Energizer demerged its personal care business as a new company, Edgewell Personal Care, of which Wilkinson Sword and Schick became part. Both are now brands used by Edgewell; Wilkinson Sword is used in Europe and Schick is used in Edgewell's remaining markets.\n\nWilkinson Sword-branded three-, four-, and five-bladed razors for men and women have been produced in Germany since 1998, when production moved from the UK.\n\n\nWilkinson still also makes double edge razor blades for safety razors.\n\n"}
