{"id": "36845407", "url": "https://en.wikipedia.org/wiki?curid=36845407", "title": "3D cell culturing by magnetic levitation", "text": "3D cell culturing by magnetic levitation\n\n3D cell culture by the magnetic levitation method (MLM) is the application of growing 3D tissue by inducing cells treated with magnetic nanoparticle assemblies in spatially varying magnetic fields using neodymium magnetic drivers and promoting cell to cell interactions by levitating the cells up to the air/liquid interface of a standard petri dish. The magnetic nanoparticle assemblies consist of magnetic iron oxide nanoparticles, gold nanoparticles, and the polymer polylysine. 3D cell culturing is scalable, with the capability for culturing 500 cells to millions of cells or from single dish to high-throughput low volume systems. Once magnetized cultures are generated, they can also be used as the building block material, or the \"ink\", for the magnetic 3D bioprinting process.\n\nStandard monolayer cell culturing on tissue culture plastic has notably improved our understanding of basic cell biology, but it does not replicate the complex 3D architecture of in vivo tissue, and it can significantly modify cell properties. This often compromises experiments in basic life science, leads to misleading drug-screening results on efficacy and toxicity, and produces cells that may lack the characteristics needed for developing tissue regeneration therapies.\n\nThe future of cell culturing for fundamental studies and biomedical applications lies in the creation of multicellular structure and organization in three-dimensions. Many schemes for 3D culturing are being developed or marketed, such as bio-reactors or protein-based gel environments.\n\nA 3D cell culturing system known as the Bio-Assembler™ uses biocompatible polymer-based reagents to deliver magnetic nanoparticles to individual cells so that an applied magnetic driver can levitate cells off the bottom of the cell culture dish and rapidly bring cells together near the air-liquid interface. This initiates cell-cell interactions in the absence of any artificial surface or matrix. Magnetic fields are designed to rapidly form 3D multicellular structures in as little as a few hours, including expression of extracellular matrix proteins. The morphology, protein expression, and response to exogenous agents of resulting tissue show great similarity to in vivo results.\n\n3D cell culturing by magnetic levitation method (MLM) was developed from collaboration between scientists at Rice University and University of Texas MD Anderson Cancer Center in 2008. Since then, this technology has been licensed and commercialized by Nano3D Biosciences.\n\nAbove is a picture showing 3D cell culturing through magnetic levitation with the Bio-Assembler cell culturing system. (A) A magnetic iron oxide nanoparticle assembly known as Nanoshuttle is added and dispersed over cells and the mixture is incubated. (B) After incubation with Nanoshuttle, cells are detached and transferred to a petri dish. (C) A magnetic drive is then placed on top of a petri dish top. (D) The magnetic field causes cells to rise to the air–medium interface. (E) Human umbilical vein endothelial cells (HUVEC) levitated for 60 minutes (left images) and 4 hours (right images) (Scale bar, 50 μm). The onset of cell-cell interaction takes place as soon as cells levitate, and 3D structures start to form. At 1 hour, the cells are still relatively dispersed, but they are already showing some signs of stretching. Formation of 3D structures is visible after 4 hours of levitation (arrows).\n\nProtein expression in levitated cultures shows striking similarity to in vivo patterns. N-cadherin expression in levitated human glioblastoma cells was identical to the expression seen in human tumor xenografts grown in immunodeficient mice, while standard 2D culture showed much weaker expression that did not match xenograft distribution as shown in the picture below. The transmembrane protein N-cadherin is often used as an indicator of in-vivo-like tissue assembly in 3D culturing.\n\nIn the picture above, distribution of N-cadherin (red) and nuclei (blue) in human brain cancer mouse xenograft (left, human brain cancer cells grown in a mouse brain), brain cancer cells cultured by 3D magnetic levitation for 48 h. (middle), and cells cultured on a glass slide cover slip (2D, right). The 2D system shows N-cadherin in the cytoplasm and nucleus and notably absent from the membrane, while in the levitated culture and mouse, N-cadherin is clearly concentrated in the membrane, and also present in cytoplasm and cell junctions.\n\nOne of the challenges in generating in vivo like cultures or tissue in vitro is the difficulty in co-culturing different cell types. Because of the ability of 3D cell culturing by magnetic levitation to bring cells together, co-culturing different cell types is possible. Co-culturing of different cell types can be achieved at the onset of levitation, by mixing different cell types in before levitation or by magnetically guiding 3D cultures in an invasion assay format.\n\nThe unique ability to manipulate cells and shape tissue magnetically offers new possibilities for controlled co-culturing and invasion assays. Co-culturing in a realistic tissue architecture is critical for accurately modeling in vivo conditions, such as for increasing the accuracy of cellular assays as shown in the figure below.\n\nShown in the picture above is an invasion assay of magnetically levitated multicellular spheroids. Fluorescence images of human glioblastoma (GBM) cells (green; GFP-expressing cells) and normal human astrocytes (NHA) (red; mCherry-labelled) cultured separately and then magnetically guided together (left, time 0). Invasion of GBM into NHAin 3D culture provides a powerful new assay for basic cancer biology and drug screening (right, 12h to 252h).\n\nBy facilitating assembly of different populations of cells using the MLM, consistent generation of organoids termed adipospheres capable of simulating the complex intercellular interactions of endogenous white adipose tissue (WAT) can be achieved.\n\nCo-culturing 3T3-L1 pre-adipocytes in 3D with murine endothelial bEND.3 cells creates a vascular-like network assembly with concomitant lipogenesis in perivascular cells. See figure below.\n\nIn addition to cell lines, WAT organogenesis can be simulated from primary cells.\n\nAdipocyte-depleted stromal vascular fraction (SVF) containing adipose stromal cells (ASC), endothelial cells, and infiltrating leukocyte derived from mouse white adipose tissue (WAT) were cultured in 3D. This revealed organoids striking in hierarchical organization with distinct capsule and internal large vessel-like structures lined with endothelial cells, as well as perivascular localization of ASC.\n\nUpon adipogenesis induction of either 3T3-L1 adipospheres or adipospheres derived from SVF, the cells efficiently formed large lipid droplets typical of white adipocytes in vivo, whereas only smaller lipid droplet formation is achievable in 2D. This indicates intercellular signaling that better recapitulates WAT organogenesis.\n\nThis MLM for 3D co-culturing creates adipospheres appropriate for WAT modeling ex vivo and provides a new platform for functional screens to identify molecules bioactive toward individual adipose cell populations. It can also be adopted for WAT transplantation applications and aid other approaches to WAT-based cell therapy.\n\nUsing the MagPen™ (a Nano3D Biosciences, Inc. product), organized 3D co-cultures similar to native tissue architecture can be rapidly created. Endothelial cells (PEC), smooth muscle cells (SMC), fibroblasts (PF), and epithelial cells (EpiC) cultured with the Bio-Assembler™ can be sequentially layered in a drag-and-drop manner to create bronchioles that maintain phenotype and induce extracellular matrix formation.\n\nListed below are the cell types (primary and cell lines) that have been successfully cultured by the magnetic levitation method. The second table is the same but with images included. More images are available at Nano3D Biosciences, Inc.\n\nSame table as above, but with images.\n"}
{"id": "3718424", "url": "https://en.wikipedia.org/wiki?curid=3718424", "title": "Acquiring bank", "text": "Acquiring bank\n\nAn acquiring bank (also known simply as an acquirer) is a bank or financial institution that processes credit or debit card payments on behalf of a merchant. The acquirer allows merchants to accept credit card payments from the card-issuing banks within an association. The best-known (credit) card associations are Visa, MasterCard, Discover, China UnionPay, American Express, Diners Club, Japan Credit Bureau and Indian Rupay.\n\nThe acquiring bank enters into a contract with a merchant and offers it a merchant account. This arrangement provides the merchant with a line of credit. Under the agreement, the acquiring bank exchanges funds with issuing banks on behalf of the merchant, and pays the merchant for its daily payment-card activity's net balance—that is, gross sales minus reversals, interchange fees, and acquirer fees. Acquirer fees are an additional markup added to association interchange fees by the acquiring bank, and those fees vary at the acquirer's discretion.\n\nThe acquiring bank accepts the risk that the merchant will remain solvent. The main source of risk to the acquiring bank is fund reversals. Consumers can trigger the reversal of funds in three ways:\n\n\nCard associations consider a participating merchant to be a risk if more than 1% of payments received result in a chargeback. Visa and MasterCard levy fines against acquiring banks that retain merchants with a high chargeback frequency. To defray the cost of any fines received, the acquiring banks are inclined (but not required) to pass such fines on to the merchant. These fees are generally charged to the merchant.\n\nDue to the high amount of risk acquiring banks are anticipated to face, as well as their key position in the payment chain, the security of electronic payments is a great concern for these institutions. For this reason, they have been involved in the development of electronic point-of-sale security standards such as PCI-DSS.\n\n"}
{"id": "24171677", "url": "https://en.wikipedia.org/wiki?curid=24171677", "title": "Armenian printing", "text": "Armenian printing\n\nAfter the invention of the mechanical printing press by Johannes Gutenberg in Germany (circa 1439), Armenians from throughout the diaspora began to publish Armenian-language books. The first book which had Armenian letters was published in Mainz (Germany) in 1486. The first Armenian book to be published by the printing press was \"Urbatagirq\"—Book of Friday prayers—which was published by Hakob Meghapart in Venice in 1512.\n\nIn the 16th century there were published 31 books, in 17th century – 164 and in 18th there were 824 Armenian books printed.\n\n\n\nAfter the sovietization of Armenia, Yerevan becoming the center of the Armenian printing, where in 1921 organized by the State Publishing House. It assumes the functions of editing and organization publications. Makes its political, artistic, scientific, publications for children with relatively large circulations. Separated from the State Publishing House publishing house “Luys” (Light), specialized mainly in the publication of textbooks. In 1964 from publishing Armenian State Publishing House (HayPetHrap) was renamed “Hayastan” (Armenia). In 1976 have been separated from the last publishing “Sovetakan Grogh” (Soviet writer), which it published in the most artistic and literary works. Academy of Science of Armenian SSR published a monograph of scientific and other research literature, and publish works of Armenian classical and scientific texts from the Matenadaran as well. Publishing house of Yerevan State University publishes textbooks, collections and scientific monographs since 1922. From this period also involved in publishing the National Library, “Gitelik” (knowledge) and several others. In 1980 have acted in Yerevan on 20 printers. From 1922 until the end of 1970 in Armenia were published about 45 thousand titles of books. In the last years of Soviet power in Armenia each year were printed about 1,100 titles. During this period, books and periodicals published in the Armenian language as in other republics of the USSR.\n\nSince 1920 (the sovietization of Armenia) to the 1980s main centers of the Armenian printing press in the diaspora were Istanbul, Cairo and Beirut (the latter now is its main center). At this moment the Armenian Diaspora was published about 21 thousand titles. Total number of items of Armenian newspapers in 1512 and 1980, more than 80 thousand.\n\nThe following table is a list of the Armenian printing houses from 1512 to 1800.\n\n\n"}
{"id": "2097169", "url": "https://en.wikipedia.org/wiki?curid=2097169", "title": "Astrocompass", "text": "Astrocompass\n\nAn astrocompass is a navigational tool for determining the direction of true north through the positions of various astronomical bodies.\n\nThere are certain circumstances when magnetic compasses and gyrocompasses are unreliable. The most obvious is in polar regions, where the force exerted on the needle of a magnetic compass is nearly vertical and gyrocompasses become unstable due to the rotation of the Earth. Magnetic compasses are also susceptible to disruption from magnetic fields other than the Earth's, such as those produced by the hulls of some metal vehicles or craft. Before the advent of electronic navigational aids such as GPS the most reliable way to ascertain north in such circumstances was through the use of an astrocompass.\n\nThe Earth's axis of rotation remains, for all intents and purposes, stationary throughout the year. Thus, with knowledge of the current time and geographical position in the form of latitude and longitude, which are set on the instrument using dials, an astrocompass can be sighted on to any astronomical object with a known position to give an extremely accurate reading.\n\nIn its most basic form, the astrocompass consists of a base plate marked with the points of the compass, with a mechanism known as an equatorial drum mounted on it. On this drum is a set of adjustable sights and a scale of declination. More advanced versions may have built-in chronometers or default settings for bodies such as the Sun.\n\nTo use the compass, the base plate is first levelled with the horizon then pointed roughly to what the user believes to be north. The equatorial drum is then tilted in relation to this base according to the local latitude. The sights are then set using the local hour angle and the declination of whatever astronomical body is being used. Once all these settings have been made, the astrocompass is simply turned until the astronomical body is visible in the sights: it will then be precisely aligned to the points of the compass. Because of this procedure, an astrocompass requires its user to be in possession of a nautical almanac or similar astronomical tables, one of its chief disadvantages.\n\nAstrocompasses only became useful following the invention of the marine chronometer, without which it is almost useless for navigation. Even then, they saw only limited use, with first magnetic compasses and then gyrocompasses being preferred in almost all cases. Polar exploration was one of the fields in which the astrocompass saw the most use, for the reasons described above. They have also been used throughout history in other climes to check the accuracy of other forms of compasses. They saw use, for example, in the North African Campaign of World War 2.\n\nGPS and other similar forms of electronic navigation aids mean that the astrocompass is now functionally obsolete anywhere except for areas very close to the poles where GPS coverage is not available and there are no current electronic navigation aids.\n\nThe operation is as follows:\n\n\n"}
{"id": "21075669", "url": "https://en.wikipedia.org/wiki?curid=21075669", "title": "Avantix B8070", "text": "Avantix B8070\n\nAvantix B8070, more commonly known as Avantix MultiTicket was a passenger-operated railway ticket issuing system, installed at British railway stations from 1999 onwards. The machines were available as upgrades to the Ascom B8050 Quickfare or as new build. \n\nThe machine was developed by Sema Group, later SchlumbergerSema and then Atos Origin.\n\nMost machines were withdrawn during 2007 and 2008. The last Avantix B8070 was withdrawn in January 2009. \n\nA pilot Avantix B8070 was trialled at East Croydon railway station during 1998. The first machines in live use went into service on Thameslink stations, starting with St Albans. Other significant deployments followed on South West Trains, at West Anglia Great Northern stations between Kings Lynn and Peterborough to London King's Cross and on c2c. Small numbers of machines were deployed at other stations including Leeds, Manchester Piccadilly and London Paddington.\n\nThe user interface comprised an ATM style display screen with hardware buttons for selecting menu items displayed on each side of the screen. \n\nB8070 machines were fitted with an industrial PC connected to the UK rail wide area network. This offered a significant advantage over the B8050 in support terms giving the ability to connect to each machine from a central location and :\n\nA significant advantage of the Avantix B8070 over the Ascom B8050 was the ability to handle credit-card payments. Initially this was by reading the magnetic stripe of the credit-card, but this was updated to handle Chip and PIN cards with the addition of a Hypercom Chip and Pin card reader and pin pad.\n\nThe software application could handle tickets to all GB destinations, however, to reduce scrolling through lists of stations, these were in practice limited to 200 most popular destinations from the station, plus all of the London Underground destinations.\n\nAvantix B8070 offered the standard centre stripe mag encoded orange CCST stock for singles and returns, and also the green season ticket stock. Receipts were printed on a white tally roll. \n\nSeveral train operating companies took the opportunity to customise the front of the machine with branded graphics.\n"}
{"id": "261128", "url": "https://en.wikipedia.org/wiki?curid=261128", "title": "Bloom's taxonomy", "text": "Bloom's taxonomy\n\nBloom's taxonomy is a set of three hierarchical models used to classify educational learning objectives into levels of complexity and specificity. The three lists cover the learning objectives in cognitive, affective and sensory domains. The cognitive domain list has been the primary focus of most traditional education and is frequently used to structure curriculum learning objectives, assessments and activities.\n\nThe models were named after Benjamin Bloom, who chaired the committee of educators that devised the taxonomy. He also edited the first volume of the standard text, \"Taxonomy of Educational Objectives: The Classification of Educational Goals\".\n\nAlthough named after Bloom, the publication of \"Taxonomy of Educational Objectives\" followed a series of conferences from 1949 to 1953, which were designed to improve communication between educators on the design of curricula and examinations.\n\nThe first volume of the taxonomy, \"Handbook I: Cognitive\" was published in 1956, and in 1964 the second volume \"Handbook II: Affective\" was published. A revised version of the taxonomy for the cognitive domain was created in 2001.\n\nIn the original version of the taxonomy, the cognitive domain is broken into the following six levels of objectives. In the 2001 revised edition of Bloom's taxonomy, the levels are slightly different: Remember, Understand, Apply, Analyze, Evaluate, Create (rather than Synthesize).\n\nKnowledge involves recognizing or remembering facts, terms, basic concepts, or answers without necessarily understanding what they mean. Its characteristics may include:\n\n\"Example\": Name three common varieties of apple.\n\nComprehension involves demonstrating understanding of facts and ideas by organizing, comparing, translating, interpreting, giving descriptions, and stating the main ideas.\n\n\"Example\": Compare the identifying characteristics of a Golden Delicious apple with a Granny Smith apple.\n\nApplication involves using acquired knowledge—solving problems in new situations by applying acquired knowledge, facts, techniques and rules. Learners should be able to use prior knowledge to solve problems, identify connections and relationships and how they apply in new situations.\n\n\"Example\": Would apples prevent scurvy, a disease caused by a deficiency in vitamin C?\n\nAnalysis involves examining and breaking information into component parts, determining how the parts relate to one another, identifying motives or causes, making inferences, and finding evidence to support generalizations. Its characteristics include:\n\n\"Example\": List four ways of serving foods made with apples and explain which ones have the highest health benefits. Provide references to support your statements.\n\nSynthesis involves building a structure or pattern from diverse elements; it also refers to the act of putting parts together to form a whole. Its characteristics include:\n\n\"Example\": Convert an \"unhealthy\" recipe for apple pie to a \"healthy\" recipe by replacing your choice of ingredients. Explain the health benefits of using the ingredients you chose vs. the original ones.\n\nEvaluation involves presenting and defending opinions by making judgments about information, the validity of ideas, or quality of work based on a set of criteria. Its characteristics include:\n\n\"Example\": Which kinds of apples are best for baking a pie, and why?\n\nSkills in the affective domain describe the way people react emotionally and their ability to feel other living things' pain or joy. Affective objectives typically target the awareness and growth in attitudes, emotion, and feelings.\n\nThere are five levels in the affective domain moving through the lowest-order processes to the highest.\n\nThe lowest level; the student passively pays attention. Without this level, no learning can occur. Receiving is about the student's memory and recognition as well.\n\nThe student actively participates in the learning process, not only attends to a stimulus; the student also reacts in some way.\n\nThe student attaches a value to an object, phenomenon, or piece of information. The student associates a value or some values to the knowledge they acquired.\n\nThe student can put together different values, information, and ideas, and can accommodate them within his/her own schema; the student is comparing, relating and elaborating on what has been learned.\n\nThe student at this level tries to build abstract knowledge.\n\nSkills in the psychomotor domain describe the ability to physically manipulate a tool or instrument like a hand or a hammer. Psychomotor objectives usually focus on change and/or development in behavior and/or skills.\n\nBloom and his colleagues never created subcategories for skills in the psychomotor domain, but since then other educators have created their own psychomotor taxonomies. Simpson (1972) proposed the following levels:\n\nThe ability to use sensory cues to guide motor activity: This ranges from sensory stimulation, through cue selection, to translation.\n\n\"Examples\": Detects non-verbal communication cues. Estimate where a ball will land after it is thrown and then moving to the correct location to catch the ball. Adjusts heat of the stove to correct temperature by smell and taste of food. Adjusts the height of the forks on a forklift by comparing where the forks are in relation to the pallet.\n\n\"Key words\": chooses, describes, detects, differentiates, distinguishes, identifies, isolates, relates, selects.\n\nReadiness to act: It includes mental, physical, and emotional sets. These three sets are dispositions that predetermine a person's response to different situations (sometimes called mindsets). This subdivision of psychomotor is closely related with the \"responding to phenomena\" subdivision of the affective domain.\n\n\"Examples\": Knows and acts upon a sequence of steps in a manufacturing process. Recognizes his or her abilities and limitations. Shows desire to learn a new process (motivation).\n\n\"Keywords\": begins, displays, explains, moves, proceeds, reacts, shows, states, volunteers.\n\nThe early stages of learning a complex skill that includes imitation and trial and error: Adequacy of performance is achieved by practicing.\n\n\"Examples\": Performs a mathematical equation as demonstrated. Follows instructions to build a model. Responds to hand-signals of the instructor while learning to operate a forklift.\n\n\"Keywords\": copies, traces, follows, react, reproduce, responds.\n\nThe intermediate stage in learning a complex skill: Learned responses have become habitual and the movements can be performed with some confidence and proficiency.\n\n\"Examples\": Use a personal computer. Repair a leaking tap. Drive a car.\n\n\"Key words\": assembles, calibrates, constructs, dismantles, displays, fastens, fixes, grinds, heats, manipulates, measures, mends, mixes, organizes, sketches.\n\nThe skillful performance of motor acts that involve complex movement patterns: Proficiency is indicated by a quick, accurate, and highly coordinated performance, requiring a minimum of energy. This category includes performing without hesitation and automatic performance. For example, players will often utter sounds of satisfaction or expletives as soon as they hit a tennis ball or throw a football because they can tell by the feel of the act what the result will produce.\n\n\"Examples\": Maneuvers a car into a tight parallel parking spot. Operates a computer quickly and accurately. Displays competence while playing the piano.\n\n\"Key words\": assembles, builds, calibrates, constructs, dismantles, displays, fastens, fixes, grinds, heats, manipulates, measures, mends, mixes, organizes, sketches. (Note: The key words are the same as in mechanism, but will have adverbs or adjectives that indicate that the performance is quicker, better, more accurate, etc.)\n\nSkills are well developed and the individual can modify movement patterns to fit special requirements.\n\n\"Examples\": Responds effectively to unexpected experiences. Modifies instruction to meet the needs of the learners. Performs a task with a machine that was not originally intended for that purpose (the machine is not damaged and there is no danger in performing the new task).\n\n\"Key words\": adapts, alters, changes, rearranges, reorganizes, revises, varies.\n\nCreating new movement patterns to fit a particular situation or specific problem: Learning outcomes emphasize creativity based upon highly developed skills.\n\n\"Examples\": Constructs a new set or pattern of movements organized around a novel concept or theory. Develops a new and comprehensive training program. Creates a new gymnastic routine.\n\n\"Key words\": arranges, builds, combines, composes, constructs, creates, designs, initiate, makes, originates.\n\nIn the appendix to \"Handbook I\", there is a definition of knowledge which serves as the apex for an alternative, summary classification of the educational goals. This is significant as the taxonomy has been called upon significantly in other fields such as knowledge management, potentially out of context. \"Knowledge, as defined here, involves the recall of specifics and universals, the recall of methods and processes, or the recall of a pattern, structure, or setting.\"\n\nThe taxonomy is set out as follows:\n\nAs pointed out on the publication of the second volume, the classification was not a properly constructed taxonomy, as it lacked a systemic rationale of construction.\n\nThis was subsequently acknowledged in the discussion of the original taxonomy in its 2001 revision, and the taxonomy was reestablished on more systematic lines. \n\nSome critiques of the taxonomy's cognitive domain admit the existence of these six categories but question the existence of a sequential, hierarchical link. Often, educators view the taxonomy as a hierarchy and may mistakenly dismiss the lowest levels as unworthy of teaching. The learning of the lower levels enables the building of skills in the higher levels of the taxonomy, and in some fields, the most important skills are in the lower levels (such as identification of species of plants and animals in the field of natural history). Instructional scaffolding of higher-level skills from lower-level skills is an application of Vygotskian constructivism.\n\nSome consider the three lowest levels as hierarchically ordered, but the three higher levels as parallel. Others say that it is sometimes better to move to Application before introducing concepts, the idea is to create a learning environment where the real world context comes first and the theory second to promote the student's grasp of the phenomenon, concept or event. This \nthinking would seem to relate to the method of problem-based learning.\n\nFurthermore, the distinction between the categories can be seen as artificial since any given cognitive task may entail a number of processes. It could even be argued that any attempt to nicely categorize cognitive processes into clean, cut-and-dried classifications undermines the holistic, highly connective and interrelated nature of cognition. This is a criticism that can be directed at taxonomies of mental processes in general.\n\nBloom's taxonomy serves as the backbone of many teaching philosophies, in particular, those that lean more towards skills rather than content. These educators view content as a vessel for teaching skills. The emphasis on higher-order thinking inherent in such philosophies is based on the top levels of the taxonomy including analysis, evaluation, synthesis and creation. Bloom's taxonomy can be used as a teaching tool to help balance assessment and evaluative questions in class, assignments and texts to ensure all orders of thinking are exercised in students' learning, including aspects of information searching.\n\nThe skill development that takes place at these higher orders of thinking interacts well with a developing global focus on multiple literacies and modalities in learning and the emerging field of integrated disciplines. The ability to interface with and create media would draw upon skills from multiple levels of the taxonomy including analysis, application and creation. Bloom's taxonomy (and the revised taxonomy) continues to be a source of inspiration for educational philosophy and for developing new teaching strategies.\n\n"}
{"id": "8520485", "url": "https://en.wikipedia.org/wiki?curid=8520485", "title": "Bomb suit", "text": "Bomb suit\n\nA bomb suit, Explosive Ordnance Disposal (EOD) suit or a blast suit is a heavy suit of body armor designed to withstand the pressure generated by a bomb and any fragments the bomb may produce. It is usually worn by trained personnel attempting bomb disposal. In contrast to ballistic body armors, which usually focus on protecting the torso and head, a bomb suit must protect all parts of the body, since the dangers posed by a bomb's explosion affect the entire body.\n\nParts of the bomb suit overlap for maximum protection. The suit protects in several different ways. It deflects or stops projectiles that may come from an exploded device. It also stops or greatly decreases the pressure of the blast wave being transmitted to the person inside of the suit. Most bomb suits, such as the Advanced Bomb Suit use layers of Kevlar, foam, and plastic to accomplish these functions.\n\nIn order to maximize protection, bomb suits come with a pair of Interchangeable gloves and wrist guard attachments. This gives the wearer's hands mobility and protection needed for the task; as well as avoiding cross contamination of any evidence found (fingerprints). \n\nEOD technicians wear bomb suits during reconnaissance, 'render safe' or disruption procedures on potential or confirmed explosive threats. Such suits must provide a tremendous degree of protection from fragmentation, blast overpressure, thermal and tertiary effects should the threat device detonate. At the same time the suit can significantly hinder their mobility or situational awareness.\n\nModern day EOD units had their beginnings in World War II, when the German Luftwaffe greatly increased the number of bombs dropped on British soil. As the number of civilian casualties grew due to delayed explosion of bombs, which had often penetrated several feet into the ground after being dropped from planes, men were trained to defuse the unexploded devices and groups were dedicated try to keep up with that task. As fuse designs changed, many of these early UXD (unexploded device) soldiers died until more successful means to defeat a new design were developed.\n\nAs the United States saw its likely involvement in World War II, they requested help from the British to train a civilian EOD force that could defuse unexploded bombs in urban areas. The human cost of learning the variety of fuses and how to defeat them was lower for the U.S. due to this education. After it became clear that EOD tasks were best handled by the military, the U.S. tried several ways to organize EOD personnel that would allow for the need for both specialized training and diverse deployment.\nIn photos of early missions to defuse unexploded bombs, the men are not wearing any protective gear. In fact, they are often shirtless to cope with the heat generated by the manual labor of digging around the devices before they could be defused. Basically, the individual defusing the bomb succeeded - or failed with fatal results.\n\nThe first EOD suits consisted of Kevlar type material and/or armor plates made of metal or fiber-reinforced plastic. Their purpose was to protect the wearer from penetrating injuries by fragments from an exploding device. In the mid-1990s, research showed that these materials alone were not effective against the blast wave itself, which can cause blast lung and other potentially deadly internal injuries. Modern EOD suits have layers of Kevlar, plating, and foam to provide protection from both fragments and the blast wave itself.\nThe threats posed by an Improvised Explosive Device, commonly known as an IED, can also include chemical or biological agents. This has led to significant advancements since 1999 in the design of bomb disposal suits and helmets. For example, a modern bomb suit may address both conventional blast threats and chemical/biological agents by incorporating a chemical protective undergarment and a helmet compatible with a Self-contained breathing apparatus (SCBA)\n\nRecently, the U.S. National Institute of Justice supported a program to develop a national testing standard for EOD suits so that the protection afforded by a given suit can be described in a standard way. The goal is to have a means to compare the performance of different designs with each other and with expected threats, similar to the NIJ standards that are widely used to test and compare body armor or materials used to stop ballistic threats.\nDevelopers must consider more than just protection, since a person must work on a stressful task that also requires fine motor skills while wearing a bomb suit. Other factors that must be considered include\n\nThe pieces of a bomb suit overlap with other pieces for maximum protection whether the wearer is facing toward or away from an explosive device. The suit protects in several ways. It deflects or stops projectiles that may come from an exploded device. The second way it protects is by stopping the blast wave from being transmitted and injuring the wearer. Usually, Kevlar, foam, and plastic are layered and covered with fire retardant materials to accomplish these things. It is important that the fibers are strain-rate sensitive, or become more rigid if struck by an object traveling at high speeds, according to a ballistics engineer working for bomb suit manufacturer HighCom Security.\n\nUntil the mid-1990s, EOD suits consisted of Kevlar and/or armor plates to stop projectiles. However, the suits did not offer much protection against the blast wave itself. The most recognized injury due to the blast wave is called “blast lung.” The lungs (and other internal organs) can be injured by the blast wave and bleed, even when there is no penetrating injury; such internal injuries can be fatal. In the mid-1990s, research conducted in the U.K. showed that textile and rigid plate armor by themselves do not protect the lungs from blast injury. It was found that a layer with high acoustic impedance with a backing of a softer, low acoustic impedance layer (such as low density foam) would protect from blast injury. However, it was also shown that it is important to understand the frequency content of the applied blast wave and to experimentally test the way materials are put together to make sure they are effective.\n\nTo effectively stop a blast wave, thick layers of Kevlar, foam and plastic are needed to prevent serious bodily harm. Since the entire body needs protection, the resulting bomb suit can be heavy (up to 81 lbs or more), hot to the point of causing heat stress, and can impair movement. Therefore, often one individual will put on a suit to approach a device for defusing after it has been identified. The weight of the suit is often a tradeoff with the amount of protection it can provide. A range of bomb suits are thus available so that agencies can choose the needed protection without unnecessary weight when possible. A minimal suit consists of a jacket, apron and helmet that weigh as little as 11 lbs. These are listed as being suitable for demining activities but not EOD.\nThe materials needed to make bomb suits protective do not release body heat generated by the wearer. The result can be heat stress, which can lead to illness and disorientation, reducing the wearer’s ability to accomplish the task. The most recent models of bomb suits include battery-operated cooling systems to prevent heat stress. One manufacturer’s study claims that the internal cooling systems on 39 lbs and 81 lbs bomb suits helped the wearer stay at workable temperatures for up to an hour, even in a hot environment.\n\n"}
{"id": "4833231", "url": "https://en.wikipedia.org/wiki?curid=4833231", "title": "Brise soleil", "text": "Brise soleil\n\nBrise soleil, sometimes brise-soleil (, plural, \"brise-soleil\" (invariable), or \"bris-ole\", from French, \"sun breaker\"), is an architectural feature of a building that reduces heat gain within that building by deflecting sunlight.\n\nBrise-soleils can comprise a variety of permanent sun-shading structures, ranging from the simple patterned concrete walls popularized by Le Corbusier in the Palace of Assembly to the elaborate wing-like mechanism devised by Santiago Calatrava for the Milwaukee Art Museum or the mechanical, pattern-creating devices of the Institut du Monde Arabe by Jean Nouvel.\n\nIn the typical form, a horizontal projection extends from the sunside facade of a building. This is most commonly used to prevent facades with a large amount of glass from overheating during the summer. Often louvers are incorporated into the shade to prevent the high-angle summer sun falling on the facade, but also to allow the low-angle winter sun to provide some passive solar heating.\n\n\n"}
{"id": "174791", "url": "https://en.wikipedia.org/wiki?curid=174791", "title": "British Agricultural Revolution", "text": "British Agricultural Revolution\n\nThe British Agricultural Revolution, or Second Agricultural Revolution, was the unprecedented increase in agricultural production in Britain due to increases in labour and land productivity between the mid-17th and late 19th centuries. Agricultural output grew faster than the population over the century to 1770, and thereafter productivity remained among the highest in the world. This increase in the food supply contributed to the rapid growth of population in England and Wales, from 5.5 million in 1700 to over 9 million by 1801, though domestic production gave way increasingly to food imports in the nineteenth century as population more than tripled to over 32 million. The rise in productivity accelerated the decline of the agricultural share of the labour force, adding to the urban workforce on which industrialization depended: the Agricultural Revolution has therefore been cited as a cause of the Industrial Revolution.\n\nHowever, historians continue to dispute when exactly such a \"revolution\" took place and of what it consisted. Rather than a single event, G. E. Mingay states that there were a \"profusion of agricultural revolutions, one for two centuries before 1750, another emphasising the century after 1650, a third for the period 1750-1880, and a fourth for the middle decades of the nineteenth century\". This has led more recent historians to argue that any general statements about \"the Agricultural Revolution\" are difficult to sustain. \n\nOne important change in farming methods was the move in crop rotation to turnips and clover in place of fallow. Turnips can be grown in winter and are deep-rooted, allowing them to gather minerals unavailable to shallow-rooted crops. Clover fixes nitrogen from the atmosphere into a form of fertiliser. This permitted the intensive arable cultivation of light soils on enclosed farms and provided fodder to support increased livestock numbers whose manure added further to soil fertility.\n\nThe British Agricultural Revolution was the result of the complex interaction of social, economic and farming technology changes. Major developments and innovations include:\n\n\nOne of the most important innovations of the British Agricultural Revolution was the development of the Norfolk four-course rotation, which greatly increased crop and livestock yields by improving soil fertility and reducing fallow.\n\nCrop rotation is the practice of growing a series of dissimilar types of crops in the same area in sequential seasons to help restore plant nutrients and mitigate the build-up of pathogens and pests that often occurs when one plant species is continuously cropped. Rotation can also improve soil structure and fertility by alternating deep-rooted and shallow-rooted plants. Turnip roots, for example, can recover nutrients from deep under the soil. The Norfolk System, as it is now known, rotates crops so that different crops are planted with the result that different kinds and quantities of nutrients are taken from the soil as the plants grow. An important feature of the Norfolk four-field system was that it used labour at times when demand was not at peak levels.\n\nPlanting cover crops such as turnips and clover was not permitted under the common field system because they interfered with access to the fields. Besides, other people's livestock could graze the turnips.\n\nDuring the Middle Ages, the open field system had initially used a two-field crop rotation system where one field was left fallow or turned into pasture for a time to try to recover some of its plant nutrients. Later they employed a three-year, three field crop rotation routine, with a different crop in each of two fields, e.g. oats, rye, wheat, and barley with the second field growing a legume like peas or beans, and the third field fallow. Normally from 10–30% of the arable land in a three crop rotation system is fallow. Each field was rotated into a different crop nearly every year. Over the following two centuries, the regular planting of legumes such as peas and beans in the fields that were previously fallow slowly restored the fertility of some croplands. The planting of legumes helped to increase plant growth in the empty field due to the bacteria on legume roots' ability to fix nitrogen (N) from the air into the soil in a form that plants could use. Other crops that were occasionally grown were flax and members of the mustard family.\n\nConvertible husbandry was the alternation of a field between pasture and grain. Because nitrogen builds up slowly over time in pasture, ploughing up pasture and planting grains resulted in high yields for a few years. A big disadvantage of convertible husbandry was the hard work in breaking up pastures and difficulty in establishing them. The significance of convertible husbandry is that it introduced pasture into the rotation.\n\nThe farmers in Flanders (in parts of France and current day Belgium) discovered a still more effective four-field crop rotation system, using turnips and clover (a legume) as forage crops to replace the three-year crop rotation fallow year.\n\nThe four-field rotation system allowed farmers to restore soil fertility and restore some of the plant nutrients removed with the crops. Turnips first show up in the probate records in England as early as 1638 but were not widely used till about 1750. Fallow land was about 20% of the arable area in England in 1700 before turnips and clover were extensively grown. Guano and nitrates from South America were introduced in the mid-19th century and fallow steadily declined to reach only about 4% in 1900. Ideally, wheat, barley, turnips and clover would be planted in that order in each field in successive years. The turnips helped keep the weeds down and were an excellent forage crop—ruminant animals could eat their tops and roots through a large part of the summer and winters. There was no need to let the soil lie fallow as clover would re-add nitrates (nitrogen-containing salts) back to the soil. The clover made excellent pasture and hay fields as well as green manure when it was ploughed under after one or two years. The addition of clover and turnips allowed more animals to be kept through the winter, which in turn produced more milk, cheese, meat and manure, which maintained soil fertility. This maintains a good amount of crops produced.\n\nThe mix of crops also changed: the area under wheat rose by 1870 to 3.5 million acres (1.4m ha), barley to 2.25m acres (0.9m ha) and oats less dramatically to 2.75m acres (1.1m ha), while rye dwindled to 60,000 acres (25,000 ha), less than a tenth of its late medieval peak. Grain yields benefited from new and better seed alongside improved rotation and fertility: wheat yields increased by a quarter in the 18th century and nearly half in the 19th, averaging 30 bushels per acre (2,080 kg/ha) by the 1890s.\n\nThe Dutch acquired the iron-tipped, curved mouldboard, adjustable depth plough from the Chinese in the early 17th century. It had the advantage of being able to be pulled by one or two oxen compared to the six or eight needed by the heavy wheeled northern European plough. The Dutch plough was brought to Britain by Dutch contractors who were hired to drain East Anglian fens and Somerset moors. The plough was extremely successful on wet, boggy soil, but was soon used on ordinary land.\n\nBritish improvements included Joseph Foljambe's cast iron plough (patented 1730), which combined an earlier Dutch design with a number of innovations. Its fittings and coulter were made of iron and the mouldboard and share were covered with an iron plate, making it easier to pull and more controllable than previous ploughs. By the 1760s Foljambe was making large numbers of these ploughs in a factory outside of Rotherham, England, using standard patterns with interchangeable parts. The plough was easy for a blacksmith to make, but by the end of the 18th century it was being made in rural foundries. By 1770 it was the cheapest and best plough available. It spread to Scotland, America, and France.\n\nIn Europe, agriculture was feudal from the Middle Ages. In the traditional open field system, many subsistence farmers cropped strips of land in large fields held in common and divided the produce. They typically worked under the auspices of the aristocracy or the Catholic Church, who owned much of the land.\n\nAs early as the 12th century, some fields in England tilled under the open field system were enclosed into individually owned fields. The Black Death from 1348 onward accelerated the break-up of the feudal system in England. Many farms were bought by yeomen who enclosed their property and improved their use of the land. More secure control of the land allowed the owners to make innovations that improved their yields. Other husbandmen rented property they \"share cropped\" with the land owners. Many of these enclosures were accomplished by acts of Parliament in the 16th and 17th centuries.\n\nThe process of enclosing property accelerated in the 15th and 16th centuries. The more productive enclosed farms meant that fewer farmers were needed to work the same land, leaving many villagers without land and grazing rights. Many of them moved to the cities in search of work in the emerging factories of the Industrial Revolution. Others settled in the English colonies. English Poor Laws were enacted to help these newly poor.\n\nSome practices of enclosure were denounced by the Church, and legislation was drawn up against it; but the large, enclosed fields were needed for the gains in agricultural productivity from the 16th to 18th centuries. This controversy led to a series of government acts, culminating in the General Enclosure Act of 1801 which sanctioned large-scale land reform.\n\nThe process of enclosure was largely complete by the end of the 18th century.\n\nMarkets were widespread by 1500 with about 800 locations in Britain. These were regulated and not free. The most important development between the 16th century and the mid-19th century was the development of private marketing. By the 19th century, marketing was nationwide and the vast majority of agricultural production was for market rather than for the farmer and his family. The 16th-century market radius was about 10 miles, which could support a town of 10,000.\nThe next stage of development was trading between markets, requiring merchants, credit and forward sales, knowledge of markets and pricing and of supply and demand in different markets. Eventually, the market evolved into a national one driven by London and other growing cities. By 1700, there was a national market for wheat.\n\nLegislation regulating middlemen required registration, addressed weights and measures, fixing of prices and collection of tolls by the government. Market regulations were eased in 1663 when people were allowed some self-regulation to hold inventory, but it was forbidden to withhold commodities from the market in an effort to increase prices. In the late 18th century, the idea of self-regulation was gaining acceptance.\n\nThe lack of internal tariffs, customs barriers and feudal tolls made Britain \"the largest coherent market in Europe\".\n\nHigh wagon transportation costs made it uneconomical to ship commodities very far outside the market radius by road, generally limiting shipment to less than 20 or 30 miles to market or to a navigable waterway. Water transport was, and in some cases still is, much more efficient than land transport. In the early 19th century it cost as much to transport a ton of freight 32 miles by wagon over an unimproved road as it did to ship it 3000 miles across the Atlantic. A horse could pull at most one ton of freight on a Macadam road, which was multi-layer stone covered and crowned, with side drainage. But a single horse could pull a barge weighing over 30 tons.\n\nCommerce was aided by the expansion of roads and inland waterways. Road transport capacity grew from threefold to fourfold from 1500 to 1700.\n\nRailroads would eventually reduce the cost of land transport by over 95%; however they did not become important until after 1850.\n\nAnother way to get more land was to convert some pasture land into arable land and recover fen land and some pastures. It is estimated that the amount of arable land in Britain grew by 10–30% through these land conversions.\n\nThe British Agricultural Revolution was aided by land maintenance advancements in Flanders and the Netherlands. Due to the large and dense population of Flanders and Holland, farmers there were forced to take maximum advantage of every bit of usable land; the country had become a pioneer in canal building, soil restoration and maintenance, soil drainage, and land reclamation technology. Dutch experts like Cornelius Vermuyden brought some of this technology to Britain.\n\nWater-meadows were utilised in the late 16th to the 20th centuries and allowed earlier pasturing of livestock after they were wintered on hay. This increased livestock yields, giving more hides, meat, milk, and manure as well as better hay crops.\n\nWith the development of regional markets and eventually a national market, aided by improved transportation infrastructures, farmers were no longer dependent on their local market and were less subject to having to sell at low prices into an oversupplied local market and not being able to sell their surpluses to distant localities that were experiencing shortages. They also became less subject to price fixing regulations. Farming became a business rather than solely a means of subsistence.\n\nUnder free market capitalism, farmers had to remain competitive. To be successful, farmers had to become effective managers who incorporated the latest farming innovations in order to be low cost producers.\n\nIn England, Robert Bakewell and Thomas Coke introduced selective breeding as a scientific practice, mating together two animals with particularly desirable characteristics, and also using inbreeding or the mating of close relatives, such as father and daughter, or brother and sister, to stabilise certain qualities in order to reduce genetic diversity in desirable animal programmes from the mid-18th century. Arguably, Bakewell's most important breeding programme was with sheep. Using native stock, he was able to quickly select for large, yet fine-boned sheep, with long, lustrous wool. The Lincoln Longwool was improved by Bakewell, and in turn the Lincoln was used to develop the subsequent breed, named the New (or Dishley) Leicester. It was hornless and had a square, meaty body with straight top lines. \n\nBakewell was also the first to breed cattle to be used primarily for beef. Previously, cattle were first and foremost kept for pulling ploughs as oxen or for dairy uses, with beef from surplus males as an additional bonus, but he crossed long-horned heifers and a Westmoreland bull to eventually create the Dishley Longhorn. As more and more farmers followed his lead, farm animals increased dramatically in size and quality. The average weight of a bull sold for slaughter at Smithfield was reported around 1700 as 370 pounds (170 kg), though this is considered a low estimate: by 1786, weights of 840 pounds (380 kg) were reported, though other contemporary indicators suggest an increase of around a quarter over the intervening century.\n\nBesides the organic fertilisers in manure, new fertilisers were slowly discovered. Massive sodium nitrate (NaNO) deposits found in the Atacama Desert, Chile, were brought under British financiers like John Thomas North and imports were started. Chile was happy to allow the exports of these sodium nitrates by allowing the British to use their capital to develop the mining and imposing a hefty export tax to enrich their treasury. Massive deposits of sea bird guano (11–16% N, 8–12% phosphate, and 2–3% potash), were found and started to be imported after about 1830. Significant imports of potash obtained from the ashes of trees burned in opening new agricultural lands were imported. By-products of the British meat industry like bones from the knackers' yards were ground up or crushed and sold as fertiliser. By about 1840 about 30,000 tons of bones were being processed (worth about £150,000). An unusual alternative to bones was found to be the millions of tons of fossils called coprolites found in South East England. When these were dissolved in sulphuric acid they yielded a high phosphate mixture (called \"super phosphate\") that plants could absorb readily and increased crop yields. Mining coprolite and processing it for fertiliser soon developed into a major industry—the first commercial fertiliser. Higher yield per acre crops were also planted as potatoes went from about 300,000 acres in 1800 to about 400,000 acres in 1850 with a further increase to about 500,000 in 1900. Labour productivity slowly increased at about 0.6% per year. With more capital invested, more organic and inorganic fertilisers, and better crop yields increased the food grown at about 0.5%/year—not enough to keep up with population growth.\n\nGreat Britain contained about 10.8 million people in 1801, 20.7 million in 1851 and 37.1 million by 1901. This corresponds to an annual population growth rate of 1.3% in 1801-1851 and 1.2% in 1851-1901, twice the rate of agricultural output growth. In addition to land for cultivation there was also a demand for pasture land to support more livestock. The growth of arable acreage slowed from the 1830s and went into reverse from the 1870s in the face of cheaper grain imports, and wheat acreage nearly halved from 1870 to 1900.\n\nThe recovery of food imports after the Napoleonic Wars (1803–1815) and the resumption of American trade following the War of 1812 (1812–1815) led to the enactment in 1815 of the Corn Laws (protective tariffs) to protect cereal grain producers in Britain against foreign competition. These laws were only removed in 1846 after the onset of the Irish Potato Famine in which potato late blight ruined most of the Irish potato crop and brought famine to the Irish people in 1846–50. Though the blight also struck Scotland, Wales, England, and much of Europe, its effect there was far less severe since potatoes constituted a much smaller percentage of the diet than in Ireland. In addition many Britons could afford to buy specially-imported food from other countries — the famine-stricken Irish were too poor to do this. Hundreds of thousands died in the Irish famine and millions more emigrated to England, Wales, Scotland, Canada, Australia, Europe, and the United States, reducing the population from about 8.5 million in 1845 to 4.3 million by 1921.\n\nBetween 1873 and 1879 British agriculture suffered from wet summers that damaged grain crops. Cattle farmers were hit by foot-and-mouth disease, and sheep farmers by sheep liver rot. The poor harvests, however, masked a greater threat to British agriculture: growing imports of foodstuffs from abroad. The development of the steam ship and the development of extensive railway networks in Britain and the USA allowed US farmers with much larger and more productive farms to export hard grain to Britain at a price that undercut the British farmers. At the same time, large amounts of cheap corned beef started to arrive from Argentina, and the opening of the Suez Canal in 1869 and the development of refrigerator ships (reefers) in about 1880 opened the British market to cheap meat and wool from Australia, New Zealand, and Argentina. The Long Depression was a worldwide economic recession that began in 1873 and ended around 1896. It hit the agricultural sector hard and was the most severe in Europe and the United States, which had been experiencing strong economic growth fuelled by the Second Industrial Revolution in the decade following the American Civil War. By 1900 half the meat eaten in Britain came from abroad and tropical fruits such as bananas were also being imported on the new refrigerator ships.\nBefore the introduction of the seed drill, the common practice was to plant seeds by broadcasting (evenly throwing) them across the ground by hand on the prepared soil and then lightly harrowing the soil to cover the seed. Seeds left on top of the ground were eaten by birds, insects, and mice. There was no control over spacing and seeds were planted too close together and too far apart. Alternately seeds could be laboriously planted one by one using a hoe and/or a shovel. Cutting down on wasted seed was important because the yield of seeds harvested to seeds planted at that time was around four or five.\n\nThe seed drill was introduced from China to Italy in the mid-16th century where it was patented by the Venetian Senate. Jethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and at the correct depth. Tull's seed drill was very expensive and fragile and therefore did not have much of an impact. The technology to manufacture affordable and reliable machinery, including agricultural machines, improved dramatically in the last half of the nineteenth century.\n\nThe Agricultural Revolution was part of a long process of improvement, but sound advice on farming began to appear in England in the mid-17th century, from writers such as Samuel Hartlib, Walter Blith and others, and the overall agricultural productivity of Britain started to grow significantly only in the period of the Agricultural Revolution. It is estimated that total agricultural output grew 2.7-fold between 1700 and 1870 and output per worker at a similar rate.\n\nDespite its name, the Agricultural Revolution in Britain did not result in overall productivity per hectare of agricultural area as high as in China, where intensive cultivation (including multiple annual cropping in many areas) had been practised for many centuries.\n\nThe Agricultural Revolution in Britain proved to be a major turning point in history, allowing population to far exceed earlier peaks and sustain the country's rise to industrial pre-eminence. Towards the end of the 19th century, the substantial gains in British agricultural productivity were rapidly offset by competition from cheaper imports, made possible by the exploitation of new lands and advances in transportation, refrigeration, and other technologies.\n\n\n\nRobert C. Allen \"Tracking the Agricultural Revolution in England.\" \"Economic History Review\" (1999) 52#2 pp: 209-235. online\n\n"}
{"id": "1041458", "url": "https://en.wikipedia.org/wiki?curid=1041458", "title": "Chaptalization", "text": "Chaptalization\n\nChaptalization is the process of adding sugar to unfermented grape must in order to increase the alcohol content after fermentation. The technique is named after its developer, the French chemist Jean-Antoine-Claude Chaptal. This process is not intended to make the wine sweeter, but rather to provide more sugar for the yeast to ferment into alcohol.\n\nChaptalization has generated controversy and discontent in the French wine industry due to advantages that the process is perceived to give producers in poor-climate areas. In response to violent demonstrations by protesters in 1907, the French government began regulating the amount of sugar that can be added to wine.\n\nChaptalization is sometimes referred to as enrichment, for example in the European Union wine regulations specifying the legality of the practice within EU.\n\nThe legality of chaptalization varies by country, region, and even wine type. In general, it is legal in regions that produce grapes with low sugar content. Chaptalization is prohibited in Argentina, Australia, California, Italy, Spain and South Africa. Germany prohibits the practice for making Prädikatswein. It is generally permitted in regions where grapes tend to have low sugar content, including Northern regions of France, Germany, and the United States.\n\nThe technique of adding sugar to grape must has been part of the process of winemaking since the Romans added honey as a sweetening agent. While not realizing the chemical components, Roman winemakers were able to identify the benefits of added sense of body or mouthfeel.\n\nWhile the process has long been associated with French wine, the first recorded mention of adding sugar to must in French literature was the 1765 edition of \"L'Encyclopedie\", which advocated the use of sugar for sweetening wine over the previously accepted practice of using lead acetate. In 1777, the French chemist Pierre Macquer discovered that the actual chemical benefit of adding sugar to must was an increase in alcohol to balance the high acidity of underripe grapes rather than any perceived increase in sweetness. In 1801, while in the services of Napoleon, Jean-Antoine-Claude Chaptal began advocating the technique as a means of strengthening and preserving wine.\n\nIn the 1840s, the German wine industry was hard hit by severe weather that created considerable difficulty for harvesting ripened grapes in this cool region. A chemist named Ludwig Gall suggested Chaptal's method of adding sugar to the must to help wine makers compensate for the effects of detrimental weather. This process of \"Verbesserung\" (improvement) helped sustain wine production in the Mosel region during this difficult period.\n\nAt the turn of the twentieth century, the process became controversial in the French wine industry with vignerons in the Languedoc protesting the production of \"artificial wines\" that flooded the French wine market and drove down prices. In June 1907, huge demonstrations broke out across the Languedoc with over 900,000 protesters demanding that the government take action to protect their livelihood. Riots in the city of Narbonne prompted Prime Minister Georges Clemenceau to send the French army to the city. The ensuing clash resulted in the death of five protesters. The following day, Languedoc sympathizers burned the prefecture in Perpignan. In response to the protests, the French government increased the taxation on sugar and passed laws limiting the amount of sugar that can be added to wine.\n\nDifferent techniques are employed to adjust the level of sugar in the grape must. In the normal chaptalization process, cane sugar is the most common type of sugar added, although some winemakers prefer beet sugar or corn syrup. In many wine regions, brown sugar is an illegal additive, and in regions that disallow chaptalization altogether, grape concentrate may be added. After sugar is added to the must, naturally occurring enzymes break down the sucrose molecules in sugar into glucose and fructose, which are then fermented by the yeast and converted into alcohol and carbon dioxide.\n\nIn warmer regions, where overripening is a concern, the opposite process of rehydration (dilution with water) and acidification is used. This is used in jurisdictions such as areas of California, where if the must has excess sugar for normal fermentation, water may be added to lower the concentration. In acidification, tartaric acid is added to the must to compensate for the high levels of sugar and low levels of acid naturally found in ripe grapes.\n\nIn Champagne production, measured quantities of sugar, wine, and sometimes Brandy are added after fermentation and prior to corking in a process known as dosage. Chaptalization, on the other hand, involves adding sugar \"prior\" to fermentation. Champagne producers sometimes employ chaptalization in their winemaking when the wine is still in the form of must.\n\nSome wine journalists contend that chaptalization allows wine makers to sacrifice quality in favor of quantity by letting vines overproduce high yields of grapes that have not fully ripened. Also, winemakers have been using technological advances, such as reverse osmosis to remove water from the unfermented grape juice, thereby increasing its sugar concentration, but decreasing the volume of wine produced.\n\nControl of chaptalization is fairly strict in many countries, and generally only permitted in more northerly areas where grapes might not ripen enough. In the European Union, the amount of chaptalization allowed depends on the wine growing zone.\nDispensation to add another 0.5% ABV may be given in years when climatic conditions have been exceptionally unfavorable. National wine regulations may further restrict or ban chaptalization for certain classes of wine.\n\nIn some areas, such as Germany, wine regulations dictate that the wine makers must label whether or not the wines are \"natural,\" i.e. without sugar. Other areas, such as France, do not have such label requirements.\n\nIn the United States, federal law permits chaptalization when producing natural grape wine from juice with low sugar content. This allows chaptalization in cooler states such as Oregon, or in states such as Florida where the native grape (Muscadine) is naturally low in sugar. However, individual states may still create their own regulations; California, for example, prohibits chaptalization, although California winemakers may add grape concentrate.\n\n\"Countries and regions where chaptalization is permitted\"\n\n\"Countries and regions where chaptalization is not permitted\"\n\n"}
{"id": "1056915", "url": "https://en.wikipedia.org/wiki?curid=1056915", "title": "Concept art", "text": "Concept art\n\nConcept art is a form of illustration used to convey an idea for use in films, video games, animation, comic books, or other media before it is put into the final product. Concept art usually refers to world-building artwork used to inspire the development of media products, and is not the same as visual development art or concept design, though all three are often confused. \n\nConcept art is developed through several iterations. Multiple solutions are explored before settling on the final design. Concept art is not only used to develop the work, but also to show the project's progress to directors, clients and investors. Once the development of the work is complete, concept art may be reworked and used for advertising materials.\n\nThe term \"concept art\" was used by Disney as early as the 1930s. A concept artist is an individual who generates a visual design for an item, character, or area that does not yet exist. This includes, but is not limited to, film, animation, and more recently, video game production. Being a concept artist takes commitment, vision and a clear understanding of the role.\n\nA concept artist may be required for nothing more than preliminary artwork, or be part of a creative team until a project reaches fruition.\nWhile it is necessary to have the skills of a fine artist, a concept artist must also be able to work to strict deadlines in the capacity of a graphic designer. Some concept artists may start as fine artists, industrial designers, animators, or even special effects artists. Interpretation of ideas and how they are realized is where the concept artist's individual creativity is most evident, but subject matter is often beyond their control. Many concept artists work in a studio or from home via freelance. Working for a studio has the advantaged of an established salary. The average salary for a concept artist in video games is 60-$70,000 a year, although many make much less or more than that.\n\nConcept art has embraced the use of digital technology. Raster graphics editors for digital painting have become more easily available, as well as hardware such as graphics tablets, enabling more efficient working methods. Prior to this, any number of traditional mediums such as oil paints, acrylic paints, markers and pencils were used. Many modern paint packages are programmed to simulate the blending of color in the same way paint would blend on a canvas; proficiency with traditional media is often paramount to a concept artist's ability to use painting software. Popular programs for concept artists include Photoshop and Corel Painter. Others include Manga Studio, Procreate and Art Rage. Most concept artists have switched to digital media because of ease of editing and speed. A lot of concept work has tight deadlines where a highly polished piece is needed in a short amount of time.\n\nConcept art has always had to cover many subjects, being the primary medium in film poster design since the early days of Hollywood, but the two most widely covered areas are science fiction and fantasy.. Since the recent rise of its use in video game production, concept art has expanded to cover genres from football to the mafia and beyond.\n\nConcept art ranges from stylized to photorealistic depending on the needs of the IP. Artists working on a project often produce a large turnover in the early 'blue sky' stage of production. This provides a broad range of interpretations, most being in the form of sketches, speed paints, and 3D overpaints. Later pieces, such as matte paintings, are produced as realistically as required. Concept artists will often have to adapt to the style of the studio they are hired for. The ability to produce multiple styles is valued in a concept artist.\n\nThere are many concept art generalists, but there are also many specialized concept artists. The various specializations include, but are not limited to, drafting of characters, as well as creatures, as well as environments, or even industry-related designs. Specialization is regarded as better for freelancers than concept artists who want to work in-house, where flexibility is key. Knowing the foundations of art, such as anatomy, perspective, color theory, design, and lighting are essential to all specializations.\n\n"}
{"id": "26412685", "url": "https://en.wikipedia.org/wiki?curid=26412685", "title": "Conductive characteristics in coated synthetic fabrics", "text": "Conductive characteristics in coated synthetic fabrics\n\nConductive characteristics in coated synthetic fabrics demonstrates an energy current across a specific woven synthetic material, such energy generation in fabrics coated with a conductive polymer as found in polypyrrole. First validated by D.D.S. developer M.H.Webb, the discovery showed that when PET fabrics are coated by chemical synthesis using four different oxidizing agent–dopant combinations present an increase in energy when a fixed source is applied to the fabric. Several combinations of blends have proven to be successful in transfer, anthracenedione-2-sulfonic acid (AQSA) sodium salt doped polypyrrole coating was the most effective in generation whereas the sodium perchlorate dopant system was the least effective. The power density per unit area achieved in polypyrrole coated polyester–Lycra fabric with 0.027 mol/l of AQSA acting as dopant was 430 W/m2. The power density per unit area achieved for the sodium perchlorate system, using the same synthesis conditions, was 55 W/m2.\n\n"}
{"id": "47192472", "url": "https://en.wikipedia.org/wiki?curid=47192472", "title": "Creative Diagnostics", "text": "Creative Diagnostics\n\nCreative Diagnostics is an American biotechnology company that specializes in the research and manufacturing of antibodies, viral antigens, diagnostic components, critical assay reagents, and other biological services.\n\nCreative Diagnostics was founded in Shirley, New York, USA in 2005. The initial business was focused on monoclonal and polyclonal antibodies. Later, various kinds of antibodies, viral antigens, reagents, medical kits, and biological services were launched to broaden the company's activities.\n\nCreative Diagnostics has maintained a strategic commercial partner agreement with CD Genomics, Inc. since 2010. The two companies also began a platform license agreement in 2012.\n\nCreative Diagnostics provides contract research and manufacturing services. Additionally, the company conducts ELISA testing. Other products include:\n\n"}
{"id": "3808503", "url": "https://en.wikipedia.org/wiki?curid=3808503", "title": "Driver circuit", "text": "Driver circuit\n\nIn electronics, a driver is an electrical circuit or other electronic component used to control another circuit or component, such as a high-power transistor, liquid crystal display (LCD), and numerous others.\n\nThey are usually used to regulate current flowing through a circuit or to control other factors such as other components, some devices in the circuit. The term is often used, for example, for a specialized integrated circuit that controls high-power switches in switched-mode power converters. An amplifier can also be considered a driver for loudspeakers, or a voltage regulator that keeps an attached component operating within a broad range of input voltages.\n\nTypically the driver stage(s) of a circuit requires different characteristics to other circuit stages. For example in a transistor power amplifier circuit, typically the driver circuit requires current gain, often the ability to discharge the following transistor bases rapidly, and low output impedance to avoid or minimize distortion.\n\n\n"}
{"id": "57667660", "url": "https://en.wikipedia.org/wiki?curid=57667660", "title": "Elizabeth Tanner", "text": "Elizabeth Tanner\n\nKathleen Elizabeth Tanner (born 20 March 1957) is a professor of biomedical materials at the University of Glasgow. Her research focusses on developing materials with particular biological and mechanical properties for use medicine, particularly those used for bone replacement. Tanner developed HAPEX, a bone mineral composite biomaterial, which was used in over half a million middle ear transplants in the 1990s.\n\nTanner also developed Scotland’s first undergraduate degree in biomedical engineering. These degrees started in 2010 at the University of Glasgow.\n\nAfter being encouraged by her school headmistress at Wycombe Abbey to pursue engineering, Tanner attended Lady Margaret Hall at the University of Oxford where she completed a Bachelor degree in 1979, and then DPhil in Engineering Science within the Nuffield Orthopaedic Engineering Centre. Tanner’s PhD studied movement at fracture sites in patients with lower leg fractures.\n\nIn 1983 Tanner joined the Department of Materials at Queen Mary University of London. She was appointed Professor of Biomedical Materials in 1998. During this time, Tanner’s research was focussed on materials for bone replacement and augmentation. From 1998 Tanner commenced a visiting professorship in biomechanics and biomaterials position at the Department of Orthopaedics, Lund University Hospital, Sweden where she held a Hedda Anderson Adjunct Professorship. From 1998 to 2001, she was the Associate Director of the IRC in Biomedical Materials and was the Dean of Engineering from 1999 to 2000. Tanner joined the University of Glasgow in 2007 as Professor of Biomaterials.\n\nFrom 2016 Tanner led a two-year study funded by Action Medical Research to help babies with breathing difficulties by developing a biodegradable stent. During her career, she has edited three books and published over 160 papers and chapters.\n\nOne of the bone replacement materials developed by Tanner was used in middle ear transplants. This material is HAPEX, which is made of polyethylene polymers (plastic-like material) with bone-like ceramics as the filler phase. This material has been used to restore hearing in patients since 1988.\n\n\n"}
{"id": "48371359", "url": "https://en.wikipedia.org/wiki?curid=48371359", "title": "Enhanced flight vision system", "text": "Enhanced flight vision system\n\nAn Enhanced flight vision system (EFVS, sometimes EVS) is an airborne system which provides an image of the scene and displays it to the pilot, in order to provide an image in which the scene and objects in it can be better detected. In other words, an EFVS is a system which provides the pilot with an image which is better than unaided human vision. An EFVS includes imaging sensors (one or many) such as a color camera, infrared camera or radar, and typically a display for the pilot, which can be a head-mounted display or head-up display. An EFVS may be combined with a synthetic vision system to create a combined vision system.\n\nAn EFVS can be mounted on military or civilian aircraft, fixed wing (airplane) or rotary wing (helicopter). \nThe image must be displayed to the pilot conformal to the scene, i.e. the pilot must see the artificially displayed elements in exact positions relative to the real world.\nUsually along with the enhanced image, the system will display visual cues such as a horizon bar and runway location.\n\nEnhanced vision is a related to Synthetic vision system which incorporates information from aircraft based sensors (e.g., near-infrared cameras, millimeter wave radar) to provide vision in limited visibility environments.\n\nNight vision systems have been available to pilots of military aircraft for many years. More recently business jets have added similar capabilities to aircraft to enhance pilot situational awareness in poor visibility due to weather or haze, and at night. The first civil certification of an enhanced vision system on an aircraft was pioneered by Gulfstream Aerospace using a Kollsman IR camera. Originally offered as an option on the Gulfstream V aircraft, it was made standard equipment in 2003 when the Gulfstream G550 was introduced and followed on the Gulfstream G450 and Gulfstream G650. As of 2009, Gulfstream has delivered over 500 aircraft with a certified EVS installed. Other aircraft OEMs followed, with EVS now available on some Bombardier and Dassault business jet products. Boeing has begun offering EVS on its line of Boeing business jets and is likely to include it as an option on the B787 and B737 MAX.\n\nThe Gulfstream EVS and later EVS II systems use an IR camera mounted in the aircraft's nose to project a raster image on the head-up display (HUD). The IR image on the HUD is conformal to the outside scene, meaning that objects detected by the IR camera are the same size and aligned with objects outside the aircraft. Thus in poor visibility the pilot is able to view the IR camera image and is able to seamlessly and easily transition to the outside world as the aircraft gets closer.\n\nThe advantage of EVS is that safety in nearly all phases of flight are enhanced, especially during approach and landing in limited visibility. A pilot on a stabilized approach is able to recognize the runway environment (lights, runway markings, etc.) earlier in preparation for touchdown. Obstacles such as terrain, structures, and vehicles or other aircraft on the runway that might not otherwise be seen are clearly visible on the IR image.\n\nThe FAA grants some additional operating minimums to aircraft equipped with certified enhanced vision systems allowing Category I approaches to Category II minimums. Typically an operator is permitted to descend to lower altitudes closer to the runway surface (typically as low as 100 ft) in poor visibility in order to improve the chances of spotting the runway environment prior to landing. Aircraft not equipped with such systems would not be allowed to descend as low and often would be required to execute a missed approach and fly to a suitable alternate airport.\n\nOther sensor types have been flown for research purposes, including active and passive millimeter wave radar. In 2009, DARPA provided funding to develop \"Sandblaster\", a millimeter wave radar based enhanced vision system installed on helicopters which enables the pilot to see and avoid obstacles in the landing area that may be obscured by smoke, sand, or dust.\n\nThe combination of dissimilar sensor types such as long wave IR, short wave IR, and millimeter wave radar can help ensure that real time video imagery of the outside scene can be provided to the pilot in all types of visibility conditions. For example, long wave IR sensor performance can be degraded in some types of large water droplet precipitation where millimeter wave radar would be less affected.\n\nNight vision devices for military personnel have been operational since the time of World War II. Their use has been adopted also by military pilots, mainly in rotary-wing aircraft (helicopters). The use of such devices has been suggested for use by commercial pilots since the 1970s, but it was not until 1999 that the first commercial, FAA certified system, was airborne.\nStill, the pilot could not use the system to lower an aircraft below the required natural vision limit.\n\nGulfstream in 2001 became the first civilian aircraft manufacturer to develop and earn certification on its aircraft for EVS produced by Elbit's Kollsman. The FAA permitted the use of the EVS to descend down to 100 feet above Touch-down zone, if no other restrictions apply. It was not clear at the time whether an EFVS could be used for descending below that height. The situation was amended in 2004 with corrections to FAA FAR 91.175. This marks the first time an EFVS gave a concrete commercial advantage over unaided vision.\n\nThe first EVS's comprised a cooled mid-wave (MWIR) Forward looking infrared (FLIR) camera, and a HUD, certified for flight with the Gulfstream V aircraft. The camera has a cooled MWIR sensor\n\nEVSs are traditionally based on a Forward looking infrared camera which gives a thermal image of the world, and shows up heat released from airport approach lights. Most airports use incandescent Parabolic aluminized reflector lights, though energy efficiency standards (such as the \"Energy Independence and Security Act of 2007\") have caused some airports to switch to LED lighting, which has a lower thermal signature.\n\nHowever, since 2007, airports are switching to the more energy efficient LED lighting, which has a lower thermal profile. The new EVS designs are multispectral, to capture both visual light from LED lights and the thermal image of previous EVS generations. Future EVS designs focus on all-weather vision, which can be accomplished by intelligently fusing images and data from cameras operating in visible light, infrared, and millimeter-wave.\n\nAn EFVS can be mounted on any type of craft. The typical platform is a small passenger plane, since it is more cost-effective to use an EFVS than an instrumental landing system, which is used in larger passenger airplanes.\n\nNASA is developing a new supersonic airplane, the X-59 QueSST, to study technology related to better supersonic passenger planes. A key feature is an opaque nosecone, which the pilot cannot see through. NASA is considering using an EFVS to enable pilot vision on this plane. \n\nThe sensor unit of the EFVS can include a single imaging sensor, multiple cameras and also additional navigation-aiding sensors.\n\nTraditionally, the EVS sensor was a single forward looking infrared (FLIR) camera. FLIRs are of two major types: one is the high-end, cooled, MWIR band (3-5 um) camera, which has better temperature resolution and frame rate but is more expensive and bulky, and the other is uncooled microbolometers which operate in the LWIR band (8-14 um) of the light spectrum, are small and cheap but are less \"sharp\" with regards to temperature contrast.\n\nThe EVS sensor in a single FLIR EVS is usually the high-end cooled sensor. In multi-spectral applications the preferred sensor is usually uncooled since it has better atmospheric penetration in most cases (will see farther), while the fine image details will be provided by a complementary sensor.\n\nNatural unaided vision in the visible portion of the light spectrum, along with the near-infrared, can be improved by using high end cameras. Such a camera can be a high dynamic range camera for day vision, a low-light CMOS camera (sometimes called scientific CMOS or sCMOS) and night vision goggles.\n\nIn day vision and bright light it may seem that there is no need to improve the natural vision, but there are certain cases in which it may be necessary. For example, in a strong haze situation where the whole scene is very bright and features are not distinguishable, a high dynamic range camera can filter the background and present a high-contrast image, and detect the runway approach lights further away than natural vision.\n\nA SWIR (short-wavelength infrared) camera is a relatively new technology. It can offer advantages for an EFVS, such as: better haze penetration than VIS, natural scene contrast similar to VIS unlike a MWIR or LWIR. SWIR cameras aree available commercially, but there is no reported use of a SWIR camera in a commercial EFVS.\n\nA passive millimeter wave (PMMW) camera, typically between 70-250 GHZ (1-4 mm) wave band, is a promising technology for EFVS applications. It has been proposed by NASA in the 1990s, and there have been flying prototypes, but it is not yet commercially available.\n\nA PMMW camera is a thermal camera similar in principle to a FLIR: the black-body radiation from objects, which depends on their temperature and surface properties (such as emissivity), is captured through a lens and focused on the sensor, and then converted to an electrical signal. The difference from FLIR is the scales involved. The energy at millimetric wavelengths is much smaller than energy at 8-14 microns, the technology to capture the photons is different, and the physical size of the camera is much larger.\n\nAn imaging radar has also been proposed by NASA in the 1990s. It can offer the same scene resolution as a PMMW, but has different properties. It does not rely on natural radiation bu emits radio waves, which are reflected from the target and captured in the receiver. The image will be nearly the same under all conditions since it does not depend on the object temperature. An imaging radar requires very high resources for computation, since the image is formed by digital calculation and not by a lens. There have been flying prototypes, but it is not yet commercially available.\n\nA lidar is a laser system which scans the surrounding volume and provides 3D location of objects. From the data can be produced a synthetic image and also other critical flight data. The operational distance of a lidar depends on the output power. It is typically under 1 km distance, but is not limited in principle. Due to the relatively short distance it is considered more for helicopters than for airplanes. It can also aid in penetrating light to moderate atmospheric low-visibility conditions, such as fog and dust. Lidar is used in automotive applications (cars), and is being tested for helicopter landing applications.\n\nA navigational sensor may aid in complementing the image. A synthetic image can be produced based on scene data in memory and location of the aircraft, and displayed top the pilot. In principle, a pilot could land based on this synthetic image, subject to its precision and fidelity.\n\n\nThe display to the pilot is a see-through display, which means it allows both seeing the scene directly with unaided vision and seeing a projected image.\nThe display is one of two types: \nA head-down display is an LCD screen installed below the window, hence the name \"head-down\". It is generally not used as an EFVS display, since the external scene cannot be seen when looking at it.\n\nIn addition to the improved sensors image, the image displayed to the pilot will include symbology, which is a collection of visual cues displayed to a pilot regarding altitude, azimuth, horizon orientation, flight path, fuel state, other aircraft etc., and in military avionics additional friend/foe symbols, targeting system cues, weapon sights etc.\n\nThe displayed EFVS imagery and symbology must be presented so that they are aligned with and scaled to the external view. The process of alignment is called \"harmonization\". A head-up display must be harmonized with the imaging sensors. A head-mounted display moves constantly with the pilot's head, and must therefore be tracked continuously so that the image displayed conforms to the scene in real-time, see Helmet-mounted display. There is an additional issue of lag time between the image and head motion, which must be very small so as not to cause dizzyness.\n\nThe main purpose of an EVS is to permit takeoff, landing and taxiing in poor visibility conditions, \nwhere landing would not be safe otherwise. An EVS is certified for landing by the FAA only if it is combined with a HUD, in which case it is called an EFVS.\n\nThe criterion for landing is known as decision height. ICAO defines Decision Height as \"a specified altitude or height in the precision approach at which a missed approach must be initiated if the required visual reference to continue the approach has not been established.\" \nWhen a pilot in approaching the ground, they must see a visual reference to continue the approach. The visual references must be one of the following (see \"runway\"):\nIf the pilot cannot see such a reference in the decision height, they must abort the landing, and then circle for a second approach or land elsewhere.\n\nAbove the decision height, the pilot uses mostly the aircraft displays. Below decision height, the pilot must look outside to identify visual references. In this stage the pilot alternates between looking at displays and looking out the window. This switching can be avoided if a see-through display is installed to display information to the pilot while also looking out.\n\nHUDs then EVS came to business jets in 2001 and the FAA published EVFS rules in 2016 to land in poor visibility through a HUD, precluding PFD use, with combined enhanced and synthetic vision system (CVS).\nUnder current FAR 91.175 regulations, airplanes with HUDs can attain before switching to natural vision to land, permitting all-weather landing in airports without ILS Cat II/III approaches.\nAfter beginning work in 2011, Dassault was first to certify its CVS with its Elbit HUD and camera, \"FalconEye\", in October 2016 in the Falcon 2000 and 900, then in the 8X in early 2017.\n\nIn July 2018, FAA certification of the Gulfstream G500 allowed the EFVS to provide the only visual cues for landing down to runway visual range, to touchdown and rollout, after 50 test approaches, and testing to lower visibilities could allow dropping the limit, with approvals for previous Gulfstreams to follow.\nBy October 2018, the Falcon 8X FalconEye was approved by the FAA and EASA for approaches down to , before the Falcon 2000LX and 900LX by year-end.\nA dual HUD FalconEye will allow EVS-to-land in 2020, without using natural vision.\nRockwell Collins's conformal overlay of EVS and SVS is expected to enter service with the updated Global 5500/6500 around 2020.\n\nBombardier Globals use a Rockwell Collins HUD and camera while Gulfstreams have a cooled Kollsman (Elbit) camera and a Rockwell Collins HUD.\nEarly cryogenically cooled, indium antimonide (InSb) cameras could detect 1.0-5.0-micron mid-IR for hot incandescent runway lights and some background radiation from its surface, blind to visible wavelengths for LED airport lights or long-wave IR for finer environment details: the Elbit FalconEye sees in the 0.4-1.1-micron visible light and near-IR band and the 8.0-12.5-micron long-wave-IR.\n\nAn Instrument landing system, or ILS, relies on radio signals to allow operation in any weather. For an ILS landing to be allowed, the system must be installed on the ground, and a suitably equipped aircraft and appropriately qualified crew are required. Not all airports and runways are suitable for ILS installation, because of terrain conditions (hills in the way of the signal, non-straight landing slope).\n\nWhile the GPS has a very high inherent precision, the reliability is not high enough for landing. GPS signals may be intentionally jammed, or lose integrity. In such cases, it may take the GPS receiver a few seconds to detect the malfunction, which is too long for critical flight stages. GPS can be used to lower the decision height below the unaided threshold, down to cat I decision height minima, but not lower.\n\n"}
{"id": "13745915", "url": "https://en.wikipedia.org/wiki?curid=13745915", "title": "Epos Ltd", "text": "Epos Ltd\n\nEpos Ltd is a British loudspeaker company that specialises in home cinema and hi-fi speakers.\n\nEpos was founded in 1983 by Robin Marshall. In 1998 Epos was sold to Mordaunt-Short who had been bought by the TGI PLC group in 1987. When the TGI PLC group decided to close the Epos/Mordaunt-Short operations in 1999, the Epos part was sold to Michael Creek, the owner and managing director of Creek Audio Ltd.\n\nCurrent Epos speakers are split into two ranges, the budget ELS series and the high end Mi series.\n\n"}
{"id": "48648608", "url": "https://en.wikipedia.org/wiki?curid=48648608", "title": "Field of regard", "text": "Field of regard\n\nThe field of regard (abbreviated FOR) is the total area that can be captured by a movable sensor. It should not be confused with the field of view (FOV), which is the angular cone perceivable by the sensor at a particular time instant. The field of regard is the total area that a sensing system can perceive by pointing the sensor, which is typically much larger than the sensor's FOV. For a stationary sensor, the FOR and FOV coincide.\n\n"}
{"id": "11013462", "url": "https://en.wikipedia.org/wiki?curid=11013462", "title": "Green highway", "text": "Green highway\n\nA green highway is a roadway constructed per a relatively new concept for roadway design that integrates transportation functionality and ecological sustainability. An environmental approach is used throughout the planning, design, and the construction. The result is a highway that will benefit transportation, the ecosystem, urban growth, public health and surrounding communities.\n\nGreen Highways Partnership (GHP) is an alliance of Federal Highway Administration (FHWA), U.S. Environmental Protection Agency (EPA), other Federal agencies, State transportation and environmental agencies, industry, trade associations, members of academia, and contractors to encourage environmentally friendly road building.\n\nAnother effort to create greener highways is a research program named Asphalt Research Consortium(ACR) created by collaboration of FHWA, private institutions, and several universities. The program studies potential ways to make asphalt more environmentally sustainable which will result in improved traffic safety and reduced life-cycle cost.\n\nWhen built to standards of the concept, green highways have invaluable benefits to environment. Since they are built with permeable materials that provide superior watershed-driven stormwater management, leaching of metals and toxins into streams and rivers is prevented. Landfill usage is favorably reduced as construction involves recycled materials. In addition, by using cutting-edge technologies in design, critical habitats and ecosystems are protected from the encroachment of highway infrastructure.\n\nTo develop a green highway, a project can follow guidelines provided below by GHP:\n\n\nGreen highway construction can incorporate several technical elements including, but not limited to:\n\n\nU.S. Highway 301 Waldorf Transportation Improvements project is working towards becoming the nation’s first truly green highway by incorporating the principles of the Green Highways Partnership and green infrastructure in its earliest planning stages. The project encompasses an area from MD 5 and US 301 interchange in Prince George's County to the US 301 intersection with Washington Avenue and Turkey Hill Road in Charles County. It aims to improve the local traffic operation along US 301 while promoting and securing environmental stewardship.\n\nAnacostia Watershed Protection: This pilot competition is designed to support the protection and restoration of urban water resources through a holistic watershed approach to water quality management. Funding will be directed to environmentally sound, watershed projects that stress a wide range of water quality improvement strategies and targets.\n\n\n"}
{"id": "43858552", "url": "https://en.wikipedia.org/wiki?curid=43858552", "title": "High-performance Integrated Virtual Environment", "text": "High-performance Integrated Virtual Environment\n\nThe High-performance Integrated Virtual Environment (HIVE) is a distributed computing environment used for biological research, including analysis of Next Generation Sequencing (NGS) data, post market data, adverse events, metagenomic data, etc.\n\nHIVE is a massively parallel distributed computing environment where the distributed storage library and the distributed computational powerhouse are linked seamlessly. The system is both robust and flexible due to maintaining both storage and the metadata database on the same network. The distributed storage layer of software is the key component for file and archive management and is the backbone for the deposition pipeline. The data deposition back-end allows automatic uploads and downloads of external datasets into HIVE data repositories. The metadata database can be used to maintain specific information about extremely large files ingested into the system (big data) as well as metadata related to computations run on the system. This metadata then allows details of a computational pipeline to be brought up easily in the future in order to validate or replicate experiments. Since the metadata is associated with the computation, it stores the parameters of any computation in the system eliminating manual record keeping.\n\nDifferentiating HIVE from other object oriented databases is that HIVE implements a set of unified APIs to search, view, and manipulate data of all types. The system also facilitates a highly secure hierarchical access control and permission system, allowing determination of data access privileges in a finely granular manner without creating a multiplicity of rules in the security subsystem. The security model, designed for sensitive data, provides comprehensive control and auditing functionality in compliance with HIVE's designation as a FISMA Moderate system.\n\n\n\nFDA launched HIVE Open Source as a platform to support end to end needs for NGS analytics. \nhttps://github.com/FDA/fda-hive\n\nHIVE biocompute harmonization platform is at the core of High-throughput Sequencing Computational Standards for Regulatory Sciences (HTS-CSRS) project. Its mission is to provide the scientific community with a framework to harmonize biocomputing, promote interoperability, and verify bioinformatics protocols (https://hive.biochemistry.gwu.edu/htscsrs). For more information, see the project description on the FDA Extramural Research page (https://www.fda.gov/ScienceResearch/SpecialTopics/RegulatoryScience/ucm491893.htm\n\nSub-clusters of scalable high performance high density compute cores are there to serve as a powerhouse for extra-large distributed parallelized computations of NGS algorithmics. System is extremely scalable and has deployment instances ranging from a single HIVE in a box appliance to massive enterprise level systems of thousands of compute units.\n\n\n\n\n"}
{"id": "2899597", "url": "https://en.wikipedia.org/wiki?curid=2899597", "title": "History of agriculture", "text": "History of agriculture\n\nThe history of agriculture records the domestication of plants and animals and the development and dissemination of techniques for raising them productively. Agriculture began independently in different parts of the globe, and included a diverse range of taxa. At least eleven separate regions of the Old and New World were involved as independent centers of origin.\n\nWild grains were collected and eaten from at least 20,000 BC. From around 9500 BC, the eight Neolithic founder crops – emmer wheat, einkorn wheat, hulled barley, peas, lentils, bitter vetch, chick peas, and flax – were cultivated in the Levant. Rye may have been cultivated earlier but this remains controversial. Rice was domesticated in China by 6200 BC with earliest known cultivation from 5700 BC, followed by mung, soy and azuki beans. Pigs were domesticated in Mesopotamia around 11,000 BC, followed by sheep between 11,000 BC and 9000 BC. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan around 8500 BC. Sugarcane and some root vegetables were domesticated in New Guinea around 7000 BC. Sorghum was domesticated in the Sahel region of Africa by 5000 BC. In the Andes of South America, the potato was domesticated between 8000 BC and 5000 BC, along with beans, coca, llamas, alpacas, and guinea pigs. Bananas were cultivated and hybridized in the same period in Papua New Guinea. In Mesoamerica, wild teosinte was domesticated to maize by 4000 BC. Cotton was domesticated in Peru by 3600 BC. Camels were domesticated late, perhaps around 3000 BC.\n\nThe Bronze Age, from c. 3300 BC, witnessed the intensification of agriculture in civilizations such as Mesopotamian Sumer, ancient Egypt, the Indus Valley Civilisation of South Asia, ancient China, and ancient Greece. During the Iron Age and era of classical antiquity, the expansion of ancient Rome, both the Republic and then the Empire, throughout the ancient Mediterranean and Western Europe built upon existing systems of agriculture while also establishing the manorial system that became a bedrock of medieval agriculture. In the Middle Ages, both in the Islamic world and in Europe, agriculture was transformed with improved techniques and the diffusion of crop plants, including the introduction of sugar, rice, cotton and fruit trees such as the orange to Europe by way of Al-Andalus. After the voyages of Christopher Columbus in 1492, the Columbian exchange brought New World crops such as maize, potatoes, sweet potatoes, and manioc to Europe, and Old World crops such as wheat, barley, rice, and turnips, and livestock including horses, cattle, sheep, and goats to the Americas.\n\nIrrigation, crop rotation, and fertilizers were introduced soon after the Neolithic Revolution and developed much further in the past 200 years, starting with the British Agricultural Revolution. Since 1900, agriculture in the developed nations, and to a lesser extent in the developing world, has seen large rises in productivity as human labour has been replaced by mechanization, and assisted by synthetic fertilizers, pesticides, and selective breeding. The Haber-Bosch process allowed the synthesis of ammonium nitrate fertilizer on an industrial scale, greatly increasing crop yields. Modern agriculture has raised social, political, and environmental issues including water pollution, biofuels, genetically modified organisms, tariffs and farm subsidies. In response, organic farming developed in the twentieth century as an alternative to the use of synthetic pesticides.\n\nScholars have developed a number of hypotheses to explain the historical origins of agriculture. Studies of the transition from hunter-gatherer to agricultural societies indicate an antecedent period of intensification and increasing sedentism; examples are the Natufian culture in the Levant, and the Early Chinese Neolithic in China. Current models indicate that wild stands that had been harvested previously started to be planted, but were not immediately domesticated.\n\nLocalised climate change is the favoured explanation for the origins of agriculture in the Levant. When major climate change took place after the last ice age (c. 11,000 BC), much of the earth became subject to long dry seasons. These conditions favoured annual plants which die off in the long dry season, leaving a dormant seed or tuber. An abundance of readily storable wild grains and pulses enabled hunter-gatherers in some areas to form the first settled villages at this time.\n\nEarly people began altering communities of flora and fauna for their own benefit through means such as fire-stick farming and forest gardening very early. Exact dates are hard to determine, as people collected and ate seeds before domesticating them, and plant characteristics may have changed during this period without human selection. An example is the semi-tough rachis and larger seeds of cereals from just after the Younger Dryas (about 9500 BC) in the early Holocene in the Levant region of the Fertile Crescent. Monophyletic characteristics were attained without any human intervention, implying that apparent domestication of the cereal rachis could have occurred quite naturally.\n\nAgriculture began independently in different parts of the globe, and included a diverse range of taxa. At least 11 separate regions of the Old and New World were involved as independent centers of origin. Some of the earliest known domestications were of animals. Domestic pigs had multiple centres of origin in Eurasia, including Europe, East Asia and Southwest Asia, where wild boar were first domesticated about 10,500 years ago. Sheep were domesticated in Mesopotamia between 11,000 BC and 9000 BC. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan around 8500 BC. Camels were domesticated late, perhaps around 3000 BC.\n\nIt was not until after 9500 BC that the eight so-called founder crops of agriculture appear: first emmer and einkorn wheat, then hulled barley, peas, lentils, bitter vetch, chick peas and flax. These eight crops occur more or less simultaneously on Pre-Pottery Neolithic B (PPNB) sites in the Levant, although wheat was the first to be grown and harvested on a significant scale. At around the same time (9400 BC), parthenocarpic fig trees were domesticated.\n\nDomesticated rye occurs in small quantities at some Neolithic sites in (Asia Minor) Turkey, such as the Pre-Pottery Neolithic B (c. 7600 – c. 6000 BC) Can Hasan III near Çatalhöyük, but is otherwise absent until the Bronze Age of central Europe, c. 1800–1500 BC. Claims of much earlier cultivation of rye, at the Epipalaeolithic site of Tell Abu Hureyra in the Euphrates valley of northern Syria remain controversial. Critics point to inconsistencies in the radiocarbon dates, and identifications based solely on grain, rather than on chaff.\n\nBy 7000 BC, the Sumerians systematized and scaled up sowing and harvesting in Mesopotamia's fertile soil. By 8000 BC, farming was entrenched on the banks of the River Nile. About this time, agriculture was developed independently in the Far East, probably in China, with rice rather than wheat as the primary crop. Maize was domesticated from the wild grass teosinte in West Mexico by 6700 BC.\nThe potato (8000 BC), tomato, pepper (4000 BC), squash (8000 BC) and several varieties of bean (8000 BC onwards) were domesticated in the New World.\n\nAgriculture was independently developed on the island of New Guinea.\nBanana cultivation of \"Musa acuminata\", including hybridization, dates back to 5000 BC, and possibly to 8000 BC, in Papua New Guinea.\n\nBees were kept for honey in the Middle East around 7000 BC. Archaeological evidence from various sites on the Iberian peninsula suggest the domestication of plants and animals between 6,000 and 4500 BC. Céide Fields in Ireland, consisting of extensive tracts of land enclosed by stone walls, date to 3500 BC and are the oldest known field systems in the world. The horse was domesticated in the Pontic steppe around 4000 BC. In Siberia, Cannabis was in use in China in Neolithic times and may have been domesticated there; it was in use both as a fibre for ropemaking and as a medicine in Ancient Egypt by about 2350 BC.\n\nIn China, millet and rice were domesticated by 6200 BC; the earliest known cultivation of rice is from 5700 BC. They were followed by mung, soy and azuki beans. In the Sahel region of Africa, local rice and sorghum were domesticated by 5000 BC. Kola nut and coffee were domesticated in Africa. In New Guinea, ancient Papuan peoples began practicing agriculture around 7000 BC, domesticating sugarcane and taro. In the Indus Valley from the eighth millennium BC onwards at Mehrgarh, 2-row and 6-row barley were cultivated, along with einkorn, emmer, and durum wheats, and dates. In the earliest levels of Merhgarh, wild game such as gazelle, swamp deer, blackbuck, chital, wild ass, wild goat, wild sheep, boar, and nilgai were all hunted for food. These are successively replaced by domesticated sheep, goats, and humped zebu cattle by the fifth millennium BC, indicating the gradual transition from hunting and gathering to agriculture.\nMaize and squash were domesticated in Mesoamerica; potato in South America, and sunflower in the Eastern Woodlands of North America.\n\nSumerian farmers grew the cereals barley and wheat, starting to live in villages from about 8000 BC. Given the low rainfall of the region, agriculture relied on the Tigris and Euphrates rivers. Irrigation canals leading from the rivers permitted the growth of cereals in large enough quantities to support cities. The first ploughs appear in pictographs from Uruk around 3000 BC; seed-ploughs that funneled seed into the ploughed furrow appear on seals around 2300 BC. Vegetable crops included chickpeas, lentils, peas, beans, onions, garlic, lettuce, leeks and mustard. They grew fruits including dates, grapes, apples, melons, and figs. Alongside their farming, Sumerians also caught fish and hunted fowl and gazelle. The meat of sheep, goats, cows and poultry was eaten, mainly by the elite. Fish was preserved by drying, salting and smoking.\n\nThe civilization of Ancient Egypt was indebted to the Nile River and its dependable seasonal flooding. The river's predictability and the fertile soil allowed the Egyptians to build an empire on the basis of great agricultural wealth. Egyptians were among the first peoples to practice agriculture on a large scale, starting in the pre-dynastic period from the end of the Paleolithic into the Neolithic, between around 10,000 BC and 4000 BC. This was made possible with the development of basin irrigation. Their staple food crops were grains such as wheat and barley, alongside industrial crops such as flax and papyrus.\n\nJujube was domesticated in the Indian subcontinent by 9000 BC. Barley and wheat cultivation – along with the domestication of cattle, primarily sheep and goats – followed in Mehrgarh culture by 8000–6000 BC. This period also saw the first domestication of the elephant. Pastoral farming in India included threshing, planting crops in rows – either of two or of six – and storing grain in granaries. Cotton was cultivated by the 5th–4th millennium BC. By the 5th millennium BC, agricultural communities became widespread in Kashmir. Irrigation was developed in the Indus Valley Civilization by around 4500 BC. The size and prosperity of the Indus civilization grew as a result of this innovation, leading to more thoroughly planned settlements which used drainage and sewers. Archeological evidence of an animal-drawn plough dates back to 2500 BC in the Indus Valley Civilization.\n\nRecords from the Warring States, Qin Dynasty, and Han Dynasty provide a picture of early Chinese agriculture from the 5th century to 2nd century which included a nationwide granary system and widespread use of sericulture. An important early Chinese book on agriculture is the Qimin Yaoshu of 535, written by Jia Sixie. Jia's writing style was straightforward and lucid relative to the elaborate and allusive writing typical of the time. Jia's book was also very long, with over one hundred thousand written Chinese characters, and it quoted many other Chinese books that were written previously, but no longer survive. The contents of Jia's 6th century book include sections on land preparation, seeding, cultivation, orchard management, forestry, and animal husbandry. The book also includes peripherally related content covering trade and culinary uses for crops. The work and the style in which it was written proved influential on later Chinese agronomists, such as Wang Zhen and his groundbreaking \"Nong Shu\" of 1313.\n\nFor agricultural purposes, the Chinese had innovated the hydraulic-powered trip hammer by the 1st century BC. Although it found other purposes, its main function to pound, decorticate, and polish grain that otherwise would have been done manually. The Chinese also began using the square-pallet chain pump by the 1st century , powered by a waterwheel or oxen pulling an on a system of mechanical wheels. Although the chain pump found use in public works of providing water for urban and palatial pipe systems, it was used largely to lift water from a lower to higher elevation in filling irrigation canals and channels for farmland. By the end of the Han dynasty in the late 2nd century, heavy ploughs had been developed with iron ploughshares and mouldboards. These slowly spread west, revolutionizing farming in Northern Europe by the 10th century. (Thomas Glick, however, argues for a development of the Chinese plough as late as the 9th century, implying its spread east from similar designs known in Italy by the 7th century.)\n\nAsian rice was domesticated 8,200–13,500 years ago in China, with a single genetic origin from the wild rice \"Oryza rufipogon\", in the Pearl River valley region of China. Rice cultivation then spread to South and Southeast Asia.\n\nThe major cereal crops of the ancient Mediterranean region were wheat, emmer, and barley, while common vegetables included peas, beans, fava, and olives, dairy products came mostly from sheep and goats, and meat, which was consumed on rare occasion for most people, usually consisted of pork, beef, and lamb. Agriculture in ancient Greece was hindered by the topography of mainland Greece that only allowed for roughly 10% of the land to be cultivated properly, necessitating the specialized exportation of oil and wine and importation of grains from Thrace (centered in what is now Bulgaria) and the Greek colonies of southern Russia. During the Hellenistic period, the Ptolemaic Empire controlled Egypt, Cyprus, Phoenicia, and Cyrenaica, major grain-producing regions that mainland Greeks depended on for subsistence, while the Ptolemaic grain market also played a critical role in the rise of the Roman Republic. In the Seleucid Empire, Mesopotamia was a crucial area for the production of wheat, while nomadic animal husbandry was also practiced in other parts.\n\nIn the Greco-Roman world of Classical antiquity, Roman agriculture was built on techniques originally pioneered by the Sumerians, transmitted to them by subsequent cultures, with a specific emphasis on the cultivation of crops for trade and export. The Romans laid the groundwork for the manorial economic system, involving serfdom, which flourished in the Middle Ages. The farm sizes in Rome can be divided into three categories. Small farms were from 18–88 iugera (one iugerum is equal to about 0.65 acre). Medium-sized farms were from 80–500 iugera (singular iugerum). Large estates (called latifundia) were over 500 iugera. The Romans had four systems of farm management: direct work by owner and his family; slaves doing work under supervision of slave managers; tenant farming or sharecropping in which the owner and a tenant divide up a farm’s produce; and situations in which a farm was leased to a tenant.\n\nIn Mesoamerica, wild teosinte was transformed through human selection into the ancestor of modern maize, more than 6,000 years ago. It gradually spread across North America and was the major crop of Native Americans at the time of European exploration. Other Mesoamerican crops include hundreds of varieties of locally domesticated squash and beans, while cocoa, also domesticated in the region, was a major crop. The turkey, one of the most important meat birds, was probably domesticated in Mexico or the U.S. Southwest.\n\nIn Mesoamerica, the Aztecs were active farmers and had an agriculturally focused economy. The land around Lake Texcoco was fertile, but not large enough to produce the amount of food needed for the population of their expanding empire. The Aztecs developed irrigation systems, formed terraced hillsides, fertilized their soil, and developed chinampas or artificial islands, also known as \"floating gardens\". The Mayas between 400 BC to 900 AD used extensive canal and raised field systems to farm swampland on the Yucatán Peninsula.\n\nIn the Andes region of South America, with civilizations including the Inca, the major crop was the potato, domesticated approximately 7,000–10,000 years ago. Coca, still a major crop to this day, was domesticated in the Andes, as were the peanut, tomato, tobacco, and pineapple. Cotton was domesticated in Peru by 3600 BC. Animals were also domesticated, including llamas, alpacas, and guinea pigs.\n\nThe indigenous people of the Eastern U.S. domesticated numerous crops. Sunflowers, tobacco, varieties of squash and \"Chenopodium\", as well as crops no longer grown, including marsh elder and little barley, were domesticated. Wild foods including wild rice and maple sugar were harvested. The domesticated strawberry is a hybrid of a Chilean and a North American species, developed by breeding in Europe and North America. Two major crops, pecans and Concord grapes, were utilized extensively in prehistoric times but do not appear to have been domesticated until the 19th century.\n\nThe indigenous people in what is now California and the Pacific Northwest practiced various forms of forest gardening and fire-stick farming in the forests, grasslands, mixed woodlands, and wetlands, ensuring that desired food and medicine plants continued to be available. The natives controlled fire on a regional scale to create a low-intensity fire ecology which prevented larger, catastrophic fires and sustained a low-density agriculture in loose rotation; a sort of \"wild\" permaculture.\n\nA system of companion planting called the Three Sisters was developed in North America. Three crops that complemented each other were planted together: winter squash, maize (corn), and climbing beans (typically tepary beans or common beans). The maize provides a structure for the beans to climb, eliminating the need for poles. The beans provide the nitrogen to the soil that the other plants use, and the squash spreads along the ground, blocking the sunlight, helping prevent the establishment of weeds. The squash leaves also act as a \"living mulch\".\n\nFrom the time of British colonization of Australia in 1788, Indigenous Australians were characterised as nomadic hunter-gatherers who did not engage in agriculture, despite evidence to the contrary. In 1969, the archaeologist Rhys Jones proposed that Indigenous Australians engaged in systematic burning as a way of enhancing natural productivity, what has been termed fire-stick farming. In the 1970s and 1980s archaeological research in south west Victoria established that the Gunditjmara and other groups had developed sophisticated eel farming and fish trapping systems over a period of nearly 5,000 years. The archaeologist Harry Lourandos suggested in the 1980s that there was evidence of 'intensification' in progress across Australia, a process that appeared to have continued through the preceding 5,000 years. These concepts led the historian Bill Gammage to argue that in effect the whole continent was a managed landscape.\n\nIn two regions of Australia, the central west coast and eastern central Australia, forms of early agriculture may have been practiced. People living in permanent settlements of over 200 residents sowed or planted on a large scale and stored the harvested food. The Nhanda and Amangu of the central west coast grew yams (\"Dioscorea hastifolia\"), while various groups in eastern central Australia (the Corners Region) planted and harvested bush onions (\"yaua\" – \"Cyperus bulbosus\"), native millet (\"cooly, tindil\" – \"Panicum decompositum\") and a sporocarp, \"ngardu\" (\"Marsilea drummondii\").\n\nFrom 100 BC to 1600 AD, world population continued to grow along with land use, as evidenced by the rapid increase in methane emissions from cattle and the cultivation of rice.\n\nFrom the 8th century, the medieval Islamic world underwent a transformation in agricultural practice, described by the historian Andrew Watson as the Arab agricultural revolution. This transformation was driven by a number of factors including the diffusion of many crops and plants along Muslim trade routes, the spread of more advanced farming techniques, and an agricultural-economic system which promoted increased yields and efficiency. The shift in agricultural practice changed the economy, population distribution, vegetation cover, agricultural production, population levels, urban growth, the distribution of the labour force, cooking, diet, and clothing across the Islamic world. Muslim traders covered much of the Old World, and trade enabled the diffusion of many crops, plants and farming techniques across the region, as well as the adaptation of crops, plants and techniques from beyond the Islamic world. This diffusion introduced major crops to Europe by way of Al-Andalus, along with the techniques for their cultivation and cuisine. Sugar cane, rice, and cotton were among the major crops transferred, along with citrus and other fruit trees, nut trees, vegetables such as aubergine, spinach and chard, and the use of spices such as cumin, coriander, nutmeg and cinnamon. Intensive irrigation, crop rotation, and agricultural manuals were widely adopted. Irrigation, partly based on Roman technology, made use of noria water wheels, water mills, dams and reservoirs.\n\nThe Middle Ages saw further improvements in agriculture. Monasteries spread throughout Europe and became important centers for the collection of knowledge related to agriculture and forestry. The manorial system allowed large landowners to control their land and its laborers, in the form of peasants or serfs. During the medieval period, the Arab world was critical in the exchange of crops and technology between the European, Asia and African continents. Besides transporting numerous crops, they introduced the concept of summer irrigation to Europe and developed the beginnings of the plantation system of sugarcane growing through the use of slaves for intensive cultivation.\n\nBy 900, developments in iron smelting allowed for increased production in Europe, leading to developments in the production of agricultural implements such as ploughs, hand tools and horse shoes. The carruca heavy plough improved on the earlier scratch plough, with the adoption of the Chinese mouldboard plough to turn over the heavy, wet soils of northern Europe. This led to the clearing of northern European forests and an increase in agricultural production, which in turn led to an increase in population. At the same time, some farmers in Europe moved from a two field crop rotation to a three field crop rotation in which one field of three was left fallow every year. This resulted in increased productivity and nutrition, as the change in rotations permitted nitrogen-fixing legumes such as peas, lentils and beans. Improved horse harnesses and the whippletree further improved cultivation.\n\nWatermills were introduced by the Romans, but were improved throughout the Middle Ages, along with windmills, and used to grind grains into flour, to cut wood and to process flax and wool.\n\nCrops included wheat, rye, barley and oats. Peas, beans, and vetches became common from the 13th century onward as a fodder crop for animals and also for their nitrogen-fixation fertilizing properties. Crop yields peaked in the 13th century, and stayed more or less steady until the 18th century. Though the limitations of medieval farming were once thought to have provided a ceiling for the population growth in the Middle Ages, recent studies have shown that the technology of medieval agriculture was always sufficient for the needs of the people under normal circumstances, and that it was only during exceptionally harsh times, such as the terrible weather of 1315–17, that the needs of the population could not be met.\n\nAfter 1492, a global exchange of previously local crops and livestock breeds occurred. Maize, potatoes, sweet potatoes and manioc were the key crops that spread from the New World to the Old, while varieties of wheat, barley, rice and turnips traveled from the Old World to the New. There had been few livestock species in the New World, with horses, cattle, sheep and goats being completely unknown before their arrival with Old World settlers. Crops moving in both directions across the Atlantic Ocean caused population growth around the world and a lasting effect on many cultures. Maize and cassava were introduced from Brazil into Africa by Portuguese traders in the 16th century, becoming staple foods, replacing native African crops.\n\nAfter its introduction from South America to Spain in the late 1500s, the potato became a staple crop throughout Europe by the late 1700s. The potato allowed farmers to produce more food, and initially added variety to the European diet. The increased supply of food reduced disease, increased births and reduced mortality, causing a population boom throughout the British Empire, the US and Europe. The introduction of the potato also brought about the first intensive use of fertilizer, in the form of guano imported to Europe from Peru, and the first artificial pesticide, in the form of an arsenic compound used to fight Colorado potato beetles. Before the adoption of the potato as a major crop, the dependence on grain had caused repetitive regional and national famines when the crops failed, including 17 major famines in England between 1523 and 1623. The resulting dependence on the potato however caused the European Potato Failure, a disastrous crop failure from disease that resulted in widespread famine and the death of over one million people in Ireland alone.\n\nBetween the 16th century and the mid-19th century, Britain saw a large increase in agricultural productivity and net output. New agricultural practices like enclosure, mechanization, four-field crop rotation to maintain soil nutrients, and selective breeding enabled an unprecedented population growth to 5.7 million in 1750, freeing up a significant percentage of the workforce, and thereby helped drive the Industrial Revolution. The productivity of wheat went up from per acre in 1720 to around by 1840, marking a major turning point in history.\n\nAdvice on more productive techniques for farming began to appear in England in the mid-17th century, from writers such as Samuel Hartlib, Walter Blith and others. The main problem in sustaining agriculture in one place for a long time was the depletion of nutrients, most importantly nitrogen levels, in the soil. To allow the soil to regenerate, productive land was often let fallow and in some places crop rotation was used. The Dutch four-field rotation system was popularised by the British agriculturist Charles Townshend in the 18th century. The system (wheat, turnips, barley and clover), opened up a fodder crop and grazing crop allowing livestock to be bred year-round. The use of clover was especially important as the legume roots replenished soil nitrates.\nThe mechanisation and rationalisation of agriculture was another important factor. Robert Bakewell and Thomas Coke introduced selective breeding, and initiated a process of inbreeding to maximise desirable traits from the mid 18th century, such as the New Leicester sheep. Machines were invented to improve the efficiency of various agricultural operation, such as Jethro Tull's seed drill of 1701 that mechanised seeding at the correct depth and spacing and Andrew Meikle's threshing machine of 1784. Ploughs were steadily improved, from Joseph Foljambe's Rotherham iron plough in 1730 to James Small's improved \"Scots Plough\" metal in 1763. In 1789 Ransomes, Sims & Jefferies was producing 86 plough models for different soils. Powered farm machinery began with Richard Trevithick's stationary steam engine, used to drive a threshing machine, in 1812. Mechanisation spread to other farm uses through the 19th century. The first petrol-driven tractor was built in America by John Froelich in 1892.\n\nJohn Bennet Lawes began the scientific investigation of fertilization at the Rothamsted Experimental Station in 1843. He investigated the impact of inorganic and organic fertilizers on crop yield and founded one of the first artificial fertilizer manufacturing factories in 1842. Fertilizer, in the shape of sodium nitrate deposits in Chile, was imported to Britain by John Thomas North as well as guano (birds droppings). The first commercial process for fertilizer production was the obtaining of phosphate from the dissolution of coprolites in sulphuric acid.\n\nDan Albone constructed the first commercially successful gasoline-powered general purpose tractor in 1901, and the 1923 International Harvester Farmall tractor marked a major point in the replacement of draft animals (particularly horses) with machines. Since that time, self-propelled mechanical harvesters (combines), planters, transplanters and other equipment have been developed, further revolutionizing agriculture. These inventions allowed farming tasks to be done with a speed and on a scale previously impossible, leading modern farms to output much greater volumes of high-quality produce per land unit.\n\nThe Haber-Bosch method for synthesizing ammonium nitrate represented a major breakthrough and allowed crop yields to overcome previous constraints. It was first patented by German chemist Fritz Haber. In 1910 Carl Bosch, while working for German chemical company BASF, successfully commercialized the process and secured further patents. In the years after World War II, the use of synthetic fertilizer increased rapidly, in sync with the increasing world population.\n\nCollective farming was widely practiced in the Soviet Union, the Eastern Bloc countries, China, and Vietnam, starting in the 1930s in the Soviet Union; one result was the Soviet famine of 1932–33.\n\nIn the past century agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies. Other applications of scientific research since 1950 in agriculture include gene manipulation, hydroponics, and the development of economically viable biofuels such as ethanol.\n\nIn recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic movement. Famines continued to sweep the globe through the 20th century. Through the effects of climactic events, government policy, war and crop failure, millions of people died in each of at least ten famines between the 1920s and the 1990s.\n\nThe historical processes that have allowed agricultural crops to be cultivated and eaten well beyond their centers of origin continues in the present through globalization. On average, 68.7% of a nation's food supplies and 69.3% of its agricultural production are of crops with foreign origins.\n\nThe Green Revolution was a series of research, development, and technology transfer initiatives, between the 1940s and the late 1970s. It increased agriculture production around the world, especially from the late 1960s. The initiatives, led by Norman Borlaug and credited with saving over a billion people from starvation, involved the development of high-yielding varieties of cereal grains, expansion of irrigation infrastructure, modernization of management techniques, distribution of hybridized seeds, synthetic fertilizers, and pesticides to farmers.\n\nSynthetic nitrogen, along with mined rock phosphate, pesticides and mechanization, have greatly increased crop yields in the early 20th century. Increased supply of grains has led to cheaper livestock as well. Further, global yield increases were experienced later in the 20th century when high-yield varieties of common staple grains such as rice, wheat, and corn were introduced as a part of the Green Revolution. The Green Revolution exported the technologies (including pesticides and synthetic nitrogen) of the developed world to the developing world. Thomas Malthus famously predicted that the Earth would not be able to support its growing population, but technologies such as the Green Revolution have allowed the world to produce a surplus of food.\n\nAlthough the Green Revolution significantly increased rice yields in Asia, yield increases have not occurred in the past 15–20 years. The genetic \"yield potential\" has increased for wheat, but the yield potential for rice has not increased since 1966, and the yield potential for maize has \"barely increased in 35 years\". It takes only a decade or two for herbicide-resistant weeds to emerge, and insects become resistant to insecticides within about a decade, delayed somewhat by crop rotation.\nFor most of its history, agriculture has been organic, without synthetic fertilisers or pesticides, and without GMOs. With the advent of chemical agriculture, Rudolf Steiner called for farming without synthetic pesticides, and his Agriculture Course of 1924 laid the foundation for biodynamic agriculture. Lord Northbourne developed these ideas and presented his manifesto of organic farming in 1940. This became a worldwide movement, and organic farming is now practiced in many countries.\n\n\n\n\n\n\n\n"}
{"id": "56788376", "url": "https://en.wikipedia.org/wiki?curid=56788376", "title": "Intermediate moisture food", "text": "Intermediate moisture food\n\nIntermediate moisture foods (IMF) are shelf-stable products that have water activities of 0.6-0.84, with a moisture content ranging from 15% - 40% and are edible without rehydration. These food products are below the minimum water activity for most bacteria (0.90), but are susceptible to yeast and mold growth. Historically, ancient civilizations would produce IMF using methods such as sun drying, roasting over fire and adding salt to preserve food for winter months or when preparing for travel. Currently, this form of processing is achieved by using one of four methods: partial drying, osmotic drying using a humectant, dry infusion and by formulation. A variety of products are classified as IMF such as dried fruits, sugar added commodities, marshmallows, and pie fillings.\n\nThe purpose of IMF foods is to achieve a water activity that the food can be stored safely without refrigeration. However, the food is not sterile. \"Staphylococcus aureus\" is a microorganism of concern as it can grow and produce specific enterotoxins in water activities of 0.83-0.86 under aerobic conditions. Because of this, proper handling, storage, hygiene and good manufacturing practices are necessary to prevent \"Staphylococcus aureus.\" Molds of \"Aspergillis and Penicillium\" species can grow and produce harmful mycotoxins at water activity 0.77-0.85. \"Salmonella\" and \"Bacillus cereus\" are the primary pathogens of concern with low-moisture foods and IMFs. Most illnesses associated with low-moisture foods or IMFs have been caused by \"Salmonella\" spp. To reduce the risk of bacterial growth, products are treated with a combination of low pH, addition of sugar, salt and preservatives, and a thermal process that can eliminate pathogens and extend shelf-life. In the case of yeasts and molds, chemical preservatives such as sorbates and propionates are used to inhibit their growth.\n\nTo achieve 0.6-0.84 water activity in food products, partial drying is employed for raw food that naturally have a high amount of humectants such as raisins, apricots, prunes and sultanas. Humectants are solutes (such as sugar or salt) that immobilize water in food. The drying process removes free water, and the humectants in the product bind the rest of the water, not allowing it to be utilized for chemical reactions or for microbial use.\n\nOsmotic dehydration is the process of soaking food in highly concentrated solutions of humectant. Salt and sugar are commonly used humectants for this process. Water diffusion from the food to the humectant solution is caused by osmotic pressure. The water is replaced by the humectant, which results in a lowered water activity for the food product. Osmotic dehydration process results in two way mass transfer in regards to the moisture lost and the solids gained, with moisture loss being much greater than the addition of solids. Advantages of osmotic dehydration include low processing temperatures, short drying times, and 20-30% lower energy consumption than typical dehydration processes. Sugar is used as the humectant for candied intermediate moisture fruits, and salt is used for intermediate moisture vegetables and fish. Additionally, a mixture of humectants can be formulated to manipulate the sensory properties of the food product. Osmotic drying using a humectant results in a soft texture in the final product.\n\nDry infusion is the combination of partial dehydration and osmotic dehydration using a humectant. The food product is first dehydrated and then the resultant product is added to a humectant solution to reach the desired water activity. This method is desirable because it results in a higher quality and more appealing product. However, more energy is used for this method because it is two processing steps combined. Dry infusion is primarily employed by the U.S military and NASA for production of IMF to produce safe, palatable food that can be consumed much later than it is produced.\n\nMany types of food are specially formulated to achieve water activity in the IMF range. Food ingredients are mixed with salt and/or sugar, and additives (such as propylene glycol and potassium sorbate) and then subjected to processing methods such as cooking, extrusion or dehydration to result in an intermediate moisture final product. Examples of formulated IMF are confectioneries and pet food.\n\nSugar is added to fruit to protect against microbial contamination and reduce water activity in the fruit. This allows the fruit to be more stable at room temperature. Some examples are strawberries, prunes, peaches, apricots, and pineapples. IMF blueberries are prepared by osmotic dehydration. They are soaked in sugar for one to two days followed by a freeze drying process until the desired moisture level is reached.\n\nFermented meats, sausage, jerky, and corned beef can last many months without refrigeration. Pastirma is a beef product that is often eaten raw in the Middle East and Mediterranean countries. It is made from the hindquarter of beef cattle. Pastirma is a type of intermediate moisture food and can be stored for several months in humid climates. The meat is salted and dried to reduce water activity and increase microbial safety. Additionally, nitrites are added for preservation. The final product has 5% salt and a moisture content between 30-35%.\n\nSemi-moist pet food such as chewy dog treats and soft cat treats are shelf-stable, soft and do not have a high moisture content. Ingredients added to intermediate moisture pet food to achieve lower water activity are soy flakes and wheat flour in addition to solutes such as glycerol, salt, and sugar. Processing techniques such as extrusion are employed to attain the final intermediate moisture pet food. Intermediate moisture pet food are convenient products because they leave less odor and are less messy than canned wet pet food. Additionally, they have been found to be more palatable to pets than dry pet food products.\n\nCakes are considered to be intermediate moisture foods because of their moisture content (18-28%), and have low enough water activity that preserve the safety and quality. Some examples of baked goods and confectionery that come under this category are fruit cakes, pie fillings, candies, marshmallows, jams, pizza crust. Tutti Fruiti is a candy-like product that can be made from a variety of fruit, most commonly papaya. Raw pieces of unripe papaya are boiled and layered with sucrose until reaching 68 degrees brix. The solution is then air dried until a moisture content of 25.7% is reached.\nIntermediate moisture foods utilize hurdle technology by lowering water activity, reducing pH and using preservatives. Most bacteria do not grow under a water activity of 0.90 and IMF processing methods reduce water activity to 0.60-0.84. IMFs are often ready-to-eat and do not require refrigeration. This is especially important in countries with tropical climates and minimal storage and processing capacities. Nitrites and sulfites are added to food to prolong shelf life and delay flavor and color changes. Propylene glycol reduces water activity and acts as a plasticizing agent to give food its desired texture. Compared to canning, dehydration, and freezing, IMF food processing is less rigorous and results in less nutrient loss. This is because compared to other processing techniques, IMF processes are at lower temperatures, pressures, and there is no water leaching of nutrients. Additionally, IMF production is more energy efficient compared to conventional processes including canning and freezing since IMFs do not require refrigeration. The energy required for canning and freezing is costly, thus IMF are common in developing countries.\n\nSince microbes, namely \"Salmonella\" and \"Bacillus cereus,\" can persist in IMFs, other hurdles including reduction in pH and the use of preservatives is not unusual. However, additives such as nitrites and sulfites are associated with health concerns. Nitrites have a negative connotation in the food industry since they can combine with secondary amines to form nitrosamines, which are carcinogenic. Nitrites are linked to an increase risk in cancer and heart disease. Sulfite is another additive that is commonly avoided due to people having a sensitivity to sulfites. Yeast and mold are not fully inhibited by IMF processing because these microorganisms can tolerate water activity as low as 0.80. Browning can occur during storage of Intermediate moisture fruits and vegetables. Finally, sugar used commonly added as a humectant increases the caloric value of the food.\n"}
{"id": "52916912", "url": "https://en.wikipedia.org/wiki?curid=52916912", "title": "Jan Brandt", "text": "Jan Brandt\n\nJanice \"Jan\" Brandt is an American businesswoman and vice chair emeritus of America Online/Time Warner. She is known for her famous direct marketing campaign at AOL that increased the number of subscribers from 200,000 to more than 22 million.\n\nJan Brandt was born in Brooklyn, New York and moved to New Jersey at the age of eight. She graduated from Boston University's School of Public Communications. Following graduation, she began working as a copywriter at Xerox Education Publications in Middletown, Connecticut. She enrolled in night courses at the University of Connecticut and decided to switch careers to marketing. After a brief period working for Colonial Penn in Philadelphia, Brandt moved to Palo Alto to join Education Today publishing with Thomas O. Ryder, where she worked for 10 years. She then briefly returned to Xerox Education Publications, which by then had been acquired by Newfield Publications, before being hired by Steve Case as vice president of marketing of AOL in 1993.\n\nBrandt was hired with the explicit goal to grow the subscriber base and was given free rein over AOL's marketing strategies. After AOL began sending complimentary discs to people who requested them, Brandt set up a direct marketing campaign to distribute AOL installation diskettes in the mail. The trial campaign cost $250,000 and had an average response of over 10% uptake, with some mailing lists pulling as high as 16-18%. This prompted Brandt to expand the campaign beyond direct mailing and start working with nonconventional distribution partners, such as airlines and cereal companies. At one point, 50% of the CDs produced worldwide had an AOL logo. This \"carpet bombing\" strategy was instrumental in moving AOL beyond Prodigy and CompuServe to dominate the online service provider market.\n\nIn 2002, Brandt stepped down as vice chair and chief marketing officer of AOL to take a part-time consulting position.\n\nBrandt has been politically active since the 1990s, and a longtime supporter of EMILY's List. She currently sits on the board of directors for Women for Women International, a humanitarian organization for providing aid to women in war-torn regions.\n\nBrandt was named twice by Fortune Magazine as one the \"50 Most Powerful Women in American Business\", in 1999 and again in 2000. \nShe also received Direct Marketing Days New York's \"Direct Marketer of the Year\" award in 2000, and the 2001 \"Direct Marketer of the Year\" award from Target Marketing.\n\n"}
{"id": "15693143", "url": "https://en.wikipedia.org/wiki?curid=15693143", "title": "Legality of piggybacking", "text": "Legality of piggybacking\n\nLaws regarding \"unauthorized access of a computer network\" exist in many legal codes, though the wording and meaning differ from one to the next. However, the interpretation of terms like \"access\" and \"authorization\" is not clear, and there is no general agreement on whether piggybacking (intentional access of an open Wi-Fi network without harmful intent) falls under this classification. Some jurisdictions prohibit it, some permit it, and others are not well-defined.\n\nFor example, a common but untested argument is that the 802.11 and DHCP protocols operate on behalf of the owner, implicitly requesting permission to access the network, which the wireless router then authorizes. (This would not apply if the user has other reason to know that their use is unauthorized, such as a written or unwritten notice.)\n\nIn addition to laws against unauthorized access on the user side, there are the issues of breach of contract with the Internet service provider on the network owner's side. Many terms of service prohibit bandwidth sharing with others, though others allow it. The Electronic Frontier Foundation maintains a list of ISPs that allow sharing of the Wi-Fi signal.\n\nUnder Australian Law, \"unauthorized access, modification or impairment\" of data held in a computer system is a federal offence under the Criminal Code Act 1995. The act refers specifically to \"data\" as opposed to network resources (connection).\n\nIn Canadian law, unauthorized access is addressed the \"Criminal Code\", s 342.1, which provides that: \"Every one who, fraudulently and without colour of right\" obtains \"computer services\" from an access point is subject to criminal charges.\n\nSection 326(1) of the \"Criminal Code\" may also be used to address unauthorized access of a computer network: '(1) Every one commits theft who fraudulently, maliciously, or without colour of right', '(b) uses any telecommunication facility or obtains any telecommunication service.'\n\nIn Morrisburg, Ontario in 2006, a man was arrested under section 326 of the \"Criminal Code\". Ultimately the arrest was poorly reported, there does not seem to be any information available with regards to conviction.\nIn September 2016, the European Court of Justice decided in \"McFadden\" C-484/14 that \"a businessman providing a public wifi network is not responsible for copyright infringement incurred by users. But he can be ordered to protect the network with a password, to prevent copyright infringement\". The Electronic Frontier Foundation had lobbied for not requiring passwords.\n\nUnder HK Laws. Chapter 200 \"Crimes Ordinance\" Section 161 \"Access to computer with criminal or dishonest intent\":\n\nUnauthorized access to a protected system is illegal.\n\nOn April 28th, 2017 the Tokyo District Court ruled that accessing a wireless LAN network without authorization is not a crime, even if the network is protected with a password. In a case brought before the court involved a man named Hiroshi Fujita, who was accused of accessing a neighbors wi-fi network without authorization and sending virus-infected emails, and then using that to steal internet banking information and send funds to his own bank account without authorization. Hiroshi was found guilty of most of what he was accused of and sentenced to 8 years in prison. Regarding the unauthorized access of wireless networks, prosecutors argued that wi-fi passwords fall under the category of \"secrets of wireless transmission\" (無線通信の秘密) and that therefore obtaining and using passwords without permission of the network operator would fall under the category of unauthorized use of wireless transmission secrets, which is prohibited by law. However, the court ruled that the defendant is not guilty, stating in their ruling that wi-fi passwords do not fall under that category and therefore the unauthorized obtainment of passwords and subsequent accessing of protected wireless networks is not a crime.\n\nAlthough Russian criminal law does not explicitly forbid access to another person's network, there is a common judicial practice to qualify these cases as an \"unathorized access to an information\" (a broadly interpreted concept in Russian law regarding computer crimes) according to Article 272 of the Criminal Code. To construct the accusation, one considers ISP's billing data the information which has been accessed.\n\nIn addition, if the defendant have used any program (network scanner, for example) to access the network, he may also be charged by Article 273 (creation, usage and distribution of malware).\n\nPiggybacking another person's unsecured wireless network is illegal in Singapore under section 6(1)(a) of the Computer Misuse and Cybersecurity Act. The offender is liable to a fine of $10,000, imprisonment for up to 3 years, or both.\n\nIn November 2006, the 17-year-old Garyl Tan Jia Luo, was arrested for tapping into his neighbour's wireless Internet connection. On 19 December, Tan pleaded guilty to the charge, and on 16 January 2007 he became the first person in Singapore to be convicted of the offense. He was sentenced by the Community Court to 18 months' probation, half of which was to be served at a boys' home. For the remaining nine months, he had to stay indoors from 10:00 pm to 6:00 am. He was also sentenced to 80 hours of community service and banned from using the Internet for 18 months; his parents risked forfeiting a S$5,000 bond if he failed to abide by the ban. Tan was also given the option of enlisting early for National Service. If he did so, he would not have to serve whatever remained of his sentence.\n\nOn 4 January 2007, Lin Zhenghuang was charged for using his neighbour's unsecured wireless network to post a bomb hoax online. In July 2005, Lin had posted a message entitled \"Breaking News – Toa Payoh Hit by Bomb Attacks\" on a forum managed by HardwareZone. Alarmed by the message, a user reported it to the authorities through the Government of Singapore's eCitizen website. Lin faced an additional 60 charges for having used his notebook computer to repeatedly access the wireless networks of nine people in his neighborhood. Lin pleaded guilty to one charge under the Telecommunications Act and another nine under the Computer Misuse Act on 31 January. He apologised for his actions, claiming he had acted out of \"stupidness\" and not due to any \"malicious or evil intent\". On 7 February he was sentenced by District Judge Francis Tseng to three months' jail and a S$4,000 fine. The judge also set sentencing guidelines for future 'mooching' cases, stating that offenders would be liable to fines and not to imprisonment unless offences were \"committed in order to facilitate the commission of or to avoid detection for some more serious offence\", as it was in Lin's case.\n\nThe Computer Misuse Act 1990, section 1 reads:\n\nIn London, 2005, Gregory Straszkiewicz was the first person to be convicted of a related crime, \"dishonestly obtaining an electronics communication service\" (under s.125 Communications Act 2003). Local residents complained that he was repeatedly trying to gain access to residential networks with a laptop from a car. There was no evidence that he had any other criminal intent. He was fined £500 and given a 12-month conditional discharge.\n\nIn early 2006, two other individuals were arrested and received an official caution for \"dishonestly obtaining electronic communications services with intent to avoid payment.\"\n\nThere are federal and state laws (in all 50 states) addressing the issue of unauthorized access to wireless networks. The laws vary widely between states. Some criminalize the mere unauthorized access of a network, while others require monetary damages for intentional breaching of security features. The majority of state laws do not specify what is meant by \"unauthorized access\". Regardless, enforcement is minimal in most states even where it is illegal, and detection is difficult in many cases.\n\nSome portable devices, such as the Apple iPad and iPod touch, allow casual use of open Wi-Fi networks as a basic feature, and often identify the presence of specific access points within the vicinity for user geolocation.\n\nIn St. Petersburg, 2005, Benjamin Smith III was arrested and charged with \"unauthorized access to a computer network\", a third-degree felony in the state of Florida, after using a resident's wireless network from a car parked outside.\n\nAn Illinois man was arrested in January 2006 for piggybacking on a Wi-Fi network. David M. Kauchak was the first person to be charged with \"remotely accessing another computer system\" in Winnebago County. He had been accessing the Internet through a nonprofit agency's network from a car parked nearby and chatted with the police officer about it. He pleaded guilty and was sentenced to a fine of $250 and one year of court supervision.\n\nIn Sparta, Michigan, 2007, Sam Peterson was arrested for checking his email each day using a café's wireless Internet access from a car parked nearby. A police officer became suspicious, stating, \"I had a feeling a law was being broken, but I didn't know exactly what\". When asked, the man explained to the officer what he was doing, as he did not know the act was illegal. The officer found a law against \"unauthorized use of computer access\", leading to an arrest and charges that could result in a five-year felony and $10,000 fine. The café owner was not aware of the law, either. \"I didn't know it was really illegal, either. If he would have come in [to the coffee shop] it would have been fine.\" They did not press charges, but he was eventually sentenced to a $400 fine and 40 hours of community service. This case was featured on \"The Colbert Report\".\n\nIn 2007, in Palmer, Alaska, 21-year-old Brian Tanner was charged with \"theft of services\" and had his laptop confiscated after accessing a gaming website at night from the parking lot outside the Palmer Public Library, as he was allowed to do during the day. The night before, the police had asked him to leave the parking lot, which he had started using because they had asked him not to use residential connections in the past. He was not ultimately charged with theft, but could still be charged with trespassing or not obeying a police order. The library director said that Tanner had not broken any rules, and local citizens criticized police for their actions.\n\nIn 2003, the New Hampshire House Bill 495 was proposed, which would clarify that the duty to secure the wireless network lies with the network owner, instead of criminalizing the automatic access of open networks. It was passed by the New Hampshire House in March 2003 but was not signed into law. The current wording of the law provides some affirmative defenses for use of a network that is not explicitly authorized:\n\nThere are additional provisions in the NH law, Section 638:17 Computer Related Offenses, as found by searching NH RSA's in December 2009. They cover actual use of someone else's computer rather than simply \"access\":\n\nNew York law is the most permissive. The statute against unauthorized access only applies when the network \"is equipped or programmed with any device or coding system, a function of which is to prevent the unauthorized use of said computer or computer system\". In other words, the use of a network would only be considered unauthorized and illegal if the network owner had enabled encryption or password protection and the user bypassed this protection, or when the owner has explicitly given notice that use of the network is prohibited, either orally or in writing. Westchester County passed a law, taking effect in October 2006, that requires any commercial business that stores, utilizes or otherwise maintains personal information electronically to take some minimum security measures (e.g., a firewall, SSID broadcasting disabled, or using a non-default SSID) in an effort to fight identity theft. Businesses that do not secure their networks in this way face a $500 fine. The law has been criticized as being ineffectual against actual identity thieves and punishing businesses like coffee houses for normal business practices.\n\n"}
{"id": "10004859", "url": "https://en.wikipedia.org/wiki?curid=10004859", "title": "List of nanotechnology organizations", "text": "List of nanotechnology organizations\n\nThis is a list of organizations involved in nanotechnology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "20756379", "url": "https://en.wikipedia.org/wiki?curid=20756379", "title": "Logic built-in self-test", "text": "Logic built-in self-test\n\nLogic built-in self-test (or LBIST) is a form of built-in self-test (BIST) in which hardware and/or software is built into integrated circuits allowing them to test their own operation, as opposed to reliance on external automated test equipment.\n\nThe main advantage of LBIST is the ability to test internal circuits having no direct connections to external pins, and thus unreachable by external automated test equipment. Another advantage is the ability to trigger the LBIST of an integrated circuit while running a built-in self test or power-on self test of the finished product. More security will provide when compared to DES technique. It can handle the data size up to 128, 192 and 256 bits. Hackers cannot hack the data even if key and algorithm are known.\n\nLBIST that requires additional circuitry (or read-only memory) increases the cost of the integrated circuit. LBIST that only requires temporary changes to programmable logic or rewritable memory avoids this extra cost, but requires more time to first program in the BIST and then to remove it and program in the final configuration. Another disadvantage of LBIST is the possibility that the on-chip testing hardware itself can fail; external automated test equipment tests the integrated circuit with known-good test circuitry.\n\nOther, related technologies are MBIST (a BIST optimized for testing internal memory) and ABIST (either a BIST optimized for testing arrays or a BIST that is optimized for testing analog circuitry). The two uses may be distinguished by considering whether the integrated circuit being tested has an internal array or analog functions.\n\n\n"}
{"id": "8414306", "url": "https://en.wikipedia.org/wiki?curid=8414306", "title": "Megasonic cleaning", "text": "Megasonic cleaning\n\nMegasonic cleaning is a type of acoustic cleaning, related to ultrasonic cleaning. It is a gentler cleaning mechanism, less likely to cause damage, and is used in wafer, medical implant, and industrial part cleaning.\n\nSimilar to ultrasonic cleaning, megasonics utilizes a transducer that usually sits atop a piezoelectric substrate. The transducer creates an acoustic field at a much higher frequency (typically 0.8–2 MHz) compared to ultrasonic cleaning (20-200 kHz). As a result, the cavitation that occurs is gentler and on a much smaller scale. Megasonics are currently used mainly in the electronics industry for preparation of silicon wafers. \n\nThe difference between ultrasonic cleaning and megasonics cleaning lies in the frequency that is used to generate the acoustic waves. Ultrasonic cleaning uses lower frequencies; it produces random cavitation. Megasonics cleaning uses higher frequencies; it produces controlled cavitation.\n\nAn important distinction between the two methods is that the cavitation effects in a megasonic bath are much less damaging than those found with ultrasonic frequencies. This significantly reduces or eliminates cavitation erosion and the likelihood of surface damage to the product being cleaned. Parts that would be damaged by ultrasonic frequencies or cavitation effects can often be cleaned without damage in a megasonic bath using the same solution.\n\nWith ultrasonics, cavitation occurs throughout the tank, and all sides of submerged parts are cleaned. With megasonics, the acoustic wave is found only in a line of sight from the transducer surface. For this reason megasonic transducers are typically built using arrays of square or rectangular piezo devices bonded to a substrate and spaced as close together as possible. Semiconductor wafers are typically cleaned in carriers holding the substrates perpendicular to the transducer so that both front and back surfaces can be cleaned. Special carriers are sometimes used to reduce any obstructions that may prevent parts of the wafer surface from being cleaned. \n\n"}
{"id": "1399085", "url": "https://en.wikipedia.org/wiki?curid=1399085", "title": "Methods engineering", "text": "Methods engineering\n\nMethods engineering is a subspecialty of industrial engineering and manufacturing engineering concerned with human integration in industrial production processes.\n\nAlternatively it can be described as the design of the productive process in which a person is involved. The task of the Methods engineer is to decide where humans will be utilized in the process of converting raw materials to finished products and how workers can most effectively perform their assigned tasks. The terms operation analysis, work design and simplification, and methods engineering and corporate re-engineering are frequently used interchangeably.\n\nLowering costs and increasing reliability and productivity are the objectives of methods engineering. These objectives are met in a five step sequence as follows: Project selection, data acquisition and presentation, data analysis, development of an ideal method based on the data analysis and, finally, presentation and implementation of the method.\n\nMethods engineers typically work on projects involving new product design, products with a high cost of production to profit ratio, and products associated with having poor quality issues. Different methods of project selection include the Pareto analysis, fish diagrams, Gantt charts, PERT charts, and job/work site analysis guides.\n\nData that needs to be collected are specification sheets for the product, design drawings, quantity and delivery requirements, and projections as to how the product will perform or has performed in the market. The Gantt process chart can assist in the analysis of the man to machine interaction and it can aid in establishing the optimum number of workers and machines subject to the financial constraints of the operation. A flow diagram is frequently employed to represent the manufacturing process associated with the product.\n\nData analysis enables the methods engineer to make decisions about several things, including: purpose of the operation, part design characteristics, specifications and tolerances of parts, materials, manufacturing process design, setup and tooling, working conditions, material handling, plant layout, and workplace design. Knowing the specifics (who, what, when, where, why, and how) of product manufacturing assists in the development of an optimum manufacturing method.\n\nEquations of synchronous and random servicing as well as line balancing are used to determine the ideal worker to machine ratio for the process or product chosen. Synchronous servicing is defined as the process where a machine is assigned to more than one operator, and the assigned operators and machine are occupied during the whole operating cycle. Random servicing of a facility, as the name indicates, is defined as a servicing process with a random time of occurrence and need of servicing variables. Line balancing equations determine the ideal number of workers needed on a production line to enable it to work at capacity.\n\nThe industrial process or operation can be optimized using a variety of available methods. Each method design has its advantages and disadvantages. The best overall method is chosen using selection criteria and concepts involving value engineering, cost-benefit analysis, crossover charts, and economic analysis. The outcome of the selection process is then presented to the company for implementation at the plant. This last step involves \"selling the idea\" to the company brass, a skill the methods engineer must develop in addition to the normal engineering qualifications.\n\n"}
{"id": "44178766", "url": "https://en.wikipedia.org/wiki?curid=44178766", "title": "Ministry of Information and Communication (Bhutan)", "text": "Ministry of Information and Communication (Bhutan)\n\nMinistry of Information and Communication (Bhutan) is ministry of Bhutan responsible for promoting the development of reliable and sustainable information, communications and transport networks and systems and facilitating the provision of affordable and easier access to associated services.\n\n"}
{"id": "19594", "url": "https://en.wikipedia.org/wiki?curid=19594", "title": "Missile", "text": "Missile\n\nIn modern language, a missile, also known as a guided missile, is a guided self-propelled system, as opposed to an unguided self-propelled munition, referred to as a rocket (although these too can also be guided). Missiles have four system components: targeting or missile guidance, flight system, engine, and warhead. Missiles come in types adapted for different purposes: surface-to-surface and air-to-surface missiles (ballistic, cruise, anti-ship, anti-tank, etc.), surface-to-air missiles (and anti-ballistic), air-to-air missiles, and anti-satellite weapons. All known existing missiles are designed to be propelled during powered flight by chemical reactions inside a rocket engine, jet engine, or other type of engine. Non-self-propelled airborne explosive devices are generally referred to as shells and usually have a shorter range than missiles.\n\nIn ordinary British-English usage predating guided weapons, a missile is such as objects thrown at players by rowdy spectators at a sporting event.\n\nThe first missiles to be used operationally were a series of missiles developed by Nazi Germany in World War II. Most famous of these are the V-1 flying bomb and V-2 rocket, both of which used a simple mechanical autopilot to keep the missile flying along a pre-chosen route. Less well known were a series of anti-shipping and anti-aircraft missiles, typically based on a simple radio control (command guidance) system directed by the operator. However, these early systems in World War II were only built in small numbers.\n\nGuided missiles have a number of different system components:\n\nThe most common method of guidance is to use some form of radiation, such as infrared, lasers or radio waves, to guide the missile onto its target. This radiation may emanate from the target (such as the heat of an engine or the radio waves from an enemy radar), it may be provided by the missile itself (such as a radar), or it may be provided by a friendly third party (such as the radar of the launch vehicle/platform, or a laser designator operated by friendly infantry). The first two are often known as fire-and-forget as they need no further support or control from the launch vehicle/platform in order to function. Another method is to use a TV guidance, with a visible light or infrared picture produced in order to see the target. The picture may be used either by a human operator who steering the missile onto its target or by a computer doing much the same job. One of the more bizarre guidance methods instead used a pigeon to steer a missile to its target. Some missiles also have a home-on-jam capability to guide itself to a radar-emitting source. Many missiles use a combination of two or more of the methods to improve accuracy and the chances of a successful engagement.\n\nAnother method is to target the missile by knowing the location of the target and using a guidance system such as INS, TERCOM or satellite guidance. This guidance system guides the missile by knowing the missile's current position and the position of the target, and then calculating a course between them. This job can also be performed somewhat crudely by a human operator who can see the target and the missile and guide it using either cable- or radio-based remote control, or by an automatic system that can simultaneously track the target and the missile.\nFurthermore, some missiles use initial targeting, sending them to a target area, where they will switch to primary targeting, using either radar or IR targeting to acquire the target.\n\nWhether a guided missile uses a targeting system, a guidance system or both, it needs a flight system. The flight system uses the data from the targeting or guidance system to maneuver the missile in flight, allowing it to counter inaccuracies in the missile or to follow a moving target. There are two main systems: vectored thrust (for missiles that are powered throughout the guidance phase of their flight) and aerodynamic maneuvering (wings, fins, canard (aeronautics), etc.).\n\nMissiles are powered by an engine, generally either a type of rocket engine or jet engine. Rockets are generally of the solid propellant type for ease of maintenance and fast deployment, although some larger ballistic missiles use liquid-propellant rockets. Jet engines are generally used in cruise missiles, most commonly of the turbojet type, due to its relative simplicity and low frontal area. Turbofans and ramjets are the only other common forms of jet engine propulsion, although any type of engine could theoretically be used. Long-range missiles may have multiple engine stages, particularly in those launched from the surface. These stages may all be of similar types or may include a mix of engine types − for example, surface-launched cruise missiles often have a rocket booster for launching and a jet engine for sustained flight.\n\nSome missiles may have additional propulsion from another source at launch; for example, the V1 was launched by a catapult, and the MGM-51 Shillelagh was fired out of a tank gun (using a smaller charge than would be used for a shell).\n\nMissiles generally have one or more explosive warheads, although other weapon types may also be used. The warheads of a missile provide its primary destructive power (many missiles have extensive secondary destructive power due to the high kinetic energy of the weapon and unburnt fuel that may be on board). Warheads are most commonly of the high explosive type, often employing shaped charges to exploit the accuracy of a guided weapon to destroy hardened targets. Other warhead types include submunitions, incendiaries, nuclear weapons, chemical, biological or radiological weapons or kinetic energy penetrators.\nWarheadless missiles are often used for testing and training purposes.\n\nMissiles are generally categorized by their launch platform and intended target. In broadest terms, these will either be surface (ground or water) or air, and then sub-categorized by range and the exact target type (such as anti-tank or anti-ship). Many weapons are designed to be launched from both surface or the air, and a few are designed to attack either surface or air targets (such as the ADATS missile). Most weapons require some modification in order to be launched from the air or surface, such as adding boosters to the surface-launched version.\n\nAfter the boost stage, ballistic missiles follow a trajectory mainly determined by ballistics. The guidance is for relatively small deviations from that.\n\nBallistic missiles are largely used for land attack missions. Although normally associated with nuclear weapons, some conventionally armed ballistic missiles are in service, such as MGM-140 ATACMS. The V2 had demonstrated that a ballistic missile could deliver a warhead to a target city with no possibility of interception, and the introduction of nuclear weapons meant it could efficiently do damage when it arrived. The accuracy of these systems was fairly poor, but post-war development by most military forces improved the basic Inertial navigation system concept to the point where it could be used as the guidance system on Intercontinental ballistic missiles flying thousands of kilometers. Today, the ballistic missile represents the only strategic deterrent in most military forces; however, some ballistic missiles are being adapted for conventional roles, such as the Russian Iskander or the Chinese DF-21D anti-ship ballistic missile. Ballistic missiles are primarily surface-launched from mobile launchers, silos, ships or submarines, with air launch being theoretically possible with a weapon such as the cancelled Skybolt missile.\n\nThe Russian Topol M (SS-27 Sickle B) is the fastest (7,320 m/s) missile currently in service.\n\nThe V1 had been successfully intercepted during World War II, but this did not make the cruise missile concept entirely useless. After the war, the US deployed a small number of nuclear-armed cruise missiles in Germany, but these were considered to be of limited usefulness. Continued research into much longer-ranged and faster versions led to the US's SM-64 Navaho and its Soviet counterparts, the Burya and Buran cruise missile. However, these were rendered largely obsolete by the ICBM, and none were used operationally. Shorter-range developments have become widely used as highly accurate attack systems, such as the US Tomahawk missile and Russian Kh-55 . Cruise missiles are generally further divided into subsonic or supersonic weapons - supersonic weapons such as BrahMos are difficult to shoot down, whereas subsonic weapons tend to be much lighter and cheaper allowing more to be fired.\n\nCruise missiles are generally associated with land-attack operations, but also have an important role as anti-shipping weapons. They are primarily launched from air, sea or submarine platforms in both roles, although land-based launchers also exist.\n\nAnother major German missile development project was the anti-shipping class (such as the Fritz X and Henschel Hs 293), intended to stop any attempt at a cross-channel invasion. However, the British were able to render their systems useless by jamming their radios, and missiles with wire guidance were not ready by D-Day. After the war, the anti-shipping class slowly developed and became a major class in the 1960s with the introduction of the low-flying jet- or rocket-powered cruise missiles known as \"sea-skimmers\". These became famous during the Falklands War, when an Argentine Exocet missile sank a Royal Navy destroyer.\n\nA number of anti-submarine missiles also exist; these generally use the missile in order to deliver another weapon system such as a torpedo or depth charge to the location of the submarine, at which point the other weapon will conduct the underwater phase of the mission.\n\nBy the end of WWII, all forces had widely introduced unguided rockets using High-explosive anti-tank warheads as their major anti-tank weapon (see Panzerfaust, Bazooka). However, these had a limited useful range of 100 m or so, and the Germans were looking to extend this with the use of a missile using wire guidance, the X-7. After the war, this became a major design class in the later 1950s and, by the 1960s, had developed into practically the only non-tank anti-tank system in general use. During the 1973 Yom Kippur War between Israel and Egypt, the 9M14 Malyutka (aka \"Sagger\") man-portable anti-tank missile proved potent against Israeli tanks. While other guidance systems have been tried, the basic reliability of wire guidance means this will remain the primary means of controlling anti-tank missiles in the near future. Anti-tank missiles may be launched from aircraft, vehicles or by ground troops in the case of smaller weapons.\n\nBy 1944, US and British air forces were sending huge air fleets over occupied Europe, increasing the pressure on the Luftwaffe day and night fighter forces. The Germans were keen to get some sort of useful ground-based anti-aircraft system into operation. Several systems were under development, but none had reached operational status before the war's end. The US Navy also started missile research to deal with the Kamikaze threat. By 1950, systems based on this early research started to reach operational service, including the US Army's MIM-3 Nike Ajax and the Navy's \"3T's\" (Talos, Terrier, Tartar), soon followed by the Soviet S-25 Berkut and S-75 Dvina and French and British systems. Anti-aircraft weapons exist for virtually every possible launch platform, with surface-launched systems ranging from huge, self-propelled or ship-mounted launchers to man-portable systems. Subsurface-to-air missiles are usually launched from below water (usually from submarines).\n\nLike most missiles, the S-300, S-400 (missile), Advanced Air Defence and MIM-104 Patriot are for defense against short-range missiles and carry explosive warheads.\n\nHowever, in the case of a large closing speed, a projectile without explosives is used; just a collision is sufficient to destroy the target. See Missile Defense Agency for the following systems being developed:\n\n\nSoviet RS-82 rockets were successfully tested in combat at the Battle of Khalkhin Gol in 1939.\n\nGerman experience in World War II demonstrated that destroying a large aircraft was quite difficult, and they had invested considerable effort into air-to-air missile systems to do this. Their Messerschmitt Me 262's jets often carried R4M rockets, and other types of \"bomber destroyer\" aircraft had unguided rockets as well. In the post-war period, the R4M served as the pattern for a number of similar systems, used by almost all interceptor aircraft during the 1940s and 1950s. Most rockets (except for the AIR-2 Genie, due to its nuclear warhead with a large blast radius) had to be carefully aimed at relatively close range to hit the target successfully. The United States Navy and U.S. Air Force began deploying guided missiles in the early 1950s, most famous being the US Navy's AIM-9 Sidewinder and the USAF's AIM-4 Falcon. These systems have continued to advance, and modern air warfare consists almost entirely of missile firing. In the Falklands War, less powerful British Harriers were able to defeat faster Argentinian opponents using AIM-9L missiles provided by the United States as the conflict began. The latest heat-seeking designs can lock onto a target from various angles, not just from behind, where the heat signature from the engines is strongest. Other types rely on radar guidance (either on board or \"painted\" by the launching aircraft). Air-to-air missiles also have a wide range of sizes, ranging from helicopter-launched self-defense weapons with a range of a few kilometers, to long-range weapons designed for interceptor aircraft such as the R-37 (missile).\n\nIn the 1950s and 1960s, Soviet designers started work on an anti-satellite weapon, called the Istrebitel Sputnik, which literally means \"interceptor of satellites\" or \"destroyer of satellites\". After a lengthy development process of roughly twenty years, it was finally decided that testing of the Istrebitel Sputnik be canceled. This was when the United States started testing their own systems. The Brilliant Pebbles defense system proposed during the 1980s would have used kinetic energy collisions without explosives. Anti-satellite weapons may be launched either by an aircraft or a surface platform, depending on the design. To date, only a few known tests have occurred.\n\n\n"}
{"id": "17054724", "url": "https://en.wikipedia.org/wiki?curid=17054724", "title": "Mobile Alliance Against Child Sexual Abuse Content", "text": "Mobile Alliance Against Child Sexual Abuse Content\n\nThe Mobile Alliance Against Child Sexual Abuse Content was founded in 2008 by an international group of mobile operators within the GSM Association to work collectively on obstructing the use of the mobile environment by individuals or organisations wishing to consume or profit from child sexual abuse content.\n\nWhile the vast majority of child sexual abuse content (child pornography) is today accessed through conventional connections to the Internet, there is a danger that broadband networks now being rolled out by mobile operators could be misused in the same way.\n\nThe Alliance’s ultimate aim is to help stem, and ultimately reverse, the growth of non approved content around the world. Through a combination of technical measures, co-operation and information sharing, the Alliance seeks to create significant barriers to the misuse of mobile networks and services for hosting, accessing, or profiting from not approved content.\n\nMembers of the Alliance agree to, among other measures:\n· support and promote ‘hotlines’ or other mechanisms for customers to report child sexual abuse content discovered on the Internet or on mobile content services.\n· implement notice and take down processes to enable the removal of any child sexual abuse content posted on their own services\n· implement technical mechanisms to prevent access to websites identified by an appropriate agency as hosting child sexual abuse content\n\nThe Alliance encourages all mobile operators worldwide, regardless of access technology, to participate in the Alliance.\n\nThe founding members of the Mobile Alliance were:\n\n"}
{"id": "34346840", "url": "https://en.wikipedia.org/wiki?curid=34346840", "title": "Nanosensors (company)", "text": "Nanosensors (company)\n\nNanosensors is a brand of SPM and AFM probes for atomic force microscopy (AFM) and scanning probe microscopy (SPM).\n\nBasic research at IBM led to the development of the basic technologies necessary for batch processing of silicon SPM and AFM probes using bulk micromachining.\n\nIn 1993 under the brand name Nanosensors they became the first commercialized SPM and AFM probes worldwide. The development and introduction of batch processing to producing AFM probes was a crucial step to the introduction of the Atomic Force Microscope into the high tech industry. In recognition of this achievement, Nanosensors has been discerned the Dr.-Rudolf-Eberle Innovation Award of the German State of Baden-Württemberg in 1995, the Innovation Prize of the German Industry in 1995 as well as the Innovation Award of the Förderkreis für die Mikroelektronik e.V. in 1999.\n\nIn 2002, Nanosensors was acquired by and integrated into Switzerland-based NanoWorld. It continues as an independent business unit.\n\nResearchers have developed a large array of operating modes and methods for Scanning probe microscopy and Atomic Force Microscopy. Independently of the method, their use and application requires essentially a versatile SPM- or AFM-instrument which must be equipped with a method-specific SPM or AFM probe.\n\nAs Nanosensors supplies SPM- or AFM-users worldwide with the broadest choice of SPM or AFM probes, some therefore consider this company a \"giant\" of this industry.<ref name=\"doi10.1016/S1369-70210970276-7\"></ref>\n\nNanosenors is frequently cited as the supplier of the SPM or AFM probes in nanotechnology research papers (see below) – reflecting its market position and that often it is the only commercial source for these products worldwide.\n\nThe PointProbePlus series is directly based on the technology originally developed and commercialized by Nanosensors in 1993. The original PointProbe technology has been upgraded to the PointProbePlus technology in 2004 yielding a reduced variation of tip shape and increased reproducibility of images. It is manufactured from highly doped monocrystalline silicon. The tip is pointing into the <100> crystal direction.\n\nThe tip of the AdvancedTEC AFM probe series protrudes from the end of the cantilever and is visible through the optical system of the atomic force microscope. This visibility from the top allows the operator of the microscope to position the tip of this AFM probe at the point of interest.\n\n\n"}
{"id": "1002035", "url": "https://en.wikipedia.org/wiki?curid=1002035", "title": "Neowin", "text": "Neowin\n\nNeowin is a technology news website. Editorial focus is predominantly on Microsoft-related news, but the site also offers analysis and reporting on mobile news, tech trends, gadgets and new technological developments, as well as in-depth product reviews.\n\nNeowin began as a hobby in October 2000 by Marcel Klum and Steven Parker, known within the forums as \"Redmak\" and \"Neobond\" respectively, reporting news about the Windows XP alpha and beta releases (then known as \"Windows Codename Whistler\").\n\nNeowin has broken several stories, including the leak of Windows 2000 source code onto the internet.\n\nThe website offers news, technology reviews, and opinion articles, as well as an IRC server and forums. Over 345,000 users have registered for the forums, making over 11,000,000 posts as of June 2016. Two projects initiated by members of the Neowin community include a community game server for \"Team Fortress 2\" and a Folding@home team.\n\nNeowin features a vast forum of diverse topics ranging from Microsoft, Apple, gaming, general discussion and more. Neowin's forums are heavily monitored by moderators and feature stringent rules.\n"}
{"id": "3975118", "url": "https://en.wikipedia.org/wiki?curid=3975118", "title": "OBD-II PIDs", "text": "OBD-II PIDs\n\nOBD-II PIDs (On-board diagnostics Parameter IDs) are codes used to request data from a vehicle, used as a diagnostic tool.\n\nSAE standard J1979 defines many OBD-II PIDs. All on-road vehicles and trucks sold in North America are required to support a subset of these codes, primarily for state mandated emissions inspections. Manufacturers also define additional PIDs specific to their vehicles. Though not mandated, many motorcycles also support OBD-II PIDs.\n\nIn 1996, light duty vehicles (less than ) were the first to be mandated followed by medium duty vehicles (between ) in 2005. They are both required to be accessed through a standardized data link connector defined by SAE J1962.\n\nHeavy duty vehicles (greater than ) made after 2010, for sale in the US are allowed to support OBD-II diagnostics through SAE standard J1939-73 (a round diagnostic connector) according to CARB in title 13 CCR 1971.1. Some heavy duty trucks in North America use the SAE J1962 OBD-II diagnostic connector that is common with passenger cars, notably Mack and Volvo Trucks, however they use 29 bit CAN identifiers (unlike 11 bit headers used by passenger cars).\n\nThere are 10 diagnostic services described in the latest OBD-II standard SAE J1979. Before 2002, J1979 referred to these services as \"modes\". They are as follows:\n\nVehicle manufacturers are not required to support all services. Each manufacturer may define additional services above #9 (e.g.: service 22 as defined by SAE J2190 for Ford/GM, service 21 for Toyota) for other information e.g. the voltage of the traction battery in a hybrid electric vehicle (HEV).\n\nThe table below shows the standard OBD-II PIDs as defined by SAE J1979. The expected response for each PID is given, along with information on how to translate the response into meaningful data. Again, not all vehicles will support all PIDs and there can be manufacturer-defined custom PIDs that are not defined in the OBD-II standard.\n\nNote that services 01 and 02 are basically identical, except that service 01 provides current information, whereas service 02 provides a snapshot of the same data taken at the point when the last diagnostic trouble code was set. The exceptions are PID 01, which is only available in service 01, and PID 02, which is only available in service 02. If service 02 PID 02 returns zero, then there is no snapshot and all other service 02 data is meaningless.\n\nWhen using Bit-Encoded-Notation, quantities like C4 means bit 4 from data byte C. Each bit is numerated from 0 to 7, so 7 is the most significant bit and 0 is the least significant bit.\n\nService 02 accepts the same PIDs as service 01, with the same meaning, but information given is from when the freeze frame was created.\n\nYou have to send the frame number in the data section of the message.\n\nSome of the PIDs in the above table cannot be explained with a simple formula. A more elaborate explanation of these data is provided here:\n\nA request for this PID returns 4 bytes of data. Each bit, from MSB to LSB, represents one of the next 32 PIDs and specifies whether that PID is supported.\n\nFor example, if the car response is BE1FA813, it can be decoded like this:\n\nSo, supported PIDs are: 01, 03, 04, 05, 06, 07, 0C, 0D, 0E, 0F, 10, 11, 13, 15, 1C, 1F and 20\n\nA request for this PID returns 4 bytes of data, labeled A B C and D.\n\nThe first byte(A) contains two pieces of information. Bit A7 (MSB of byte A, the first byte) indicates whether or not the MIL (check engine light) is illuminated. Bits A6 through A0 represent the number of diagnostic trouble codes currently flagged in the ECU.\n\nThe second, third, and fourth bytes(B, C and D) give information about the availability and completeness of certain on-board tests. Note that test availability is indicated by set (1) bit and completeness is indicated by reset (0) bit.\n\nHere are the common bit B definitions, they are test based.\n\nThe third and fourth bytes are to be interpreted differently depending on if the engine is spark ignition (e.g. Otto or Wankel engines) or compression ignition (e.g. Diesel engines). In the second (B) byte, bit 3 indicates how to interpret the C and D bytes, with 0 being spark (Otto or Wankel) and 1 (set) being compression (Diesel).\n\nThe bytes C and D for spark ignition monitors (e.g. Otto or Wankel engines):\nAnd the bytes C and D for compression ignition monitors (Diesel engines):\n\nA request for this PID returns 4 bytes of data. \nThe first byte is always zero. The second, third, and fourth bytes give information about the availability and completeness of certain on-board tests. As with PID 01, the third and fourth bytes are to be interpreted differently depending on the ignition type (B3) – with 0 being spark and 1 (set) being compression. Note again that test availability is represented by a set (1) bit and completeness is represented by a reset (0) bit.\n\nHere are the common bit B definitions, they are test based.\nThe bytes C and D for spark ignition monitors (e.g. Otto or Wankel engines):\nAnd the bytes C and D for compression ignition monitors (Diesel engines):\n\nA request for this PID will return 9 bytes of data.\nThe first byte is a bit encoded field indicating which EGT sensors are supported:\n\nThe first byte is bit-encoded as follows:\n\nThe remaining bytes are 16 bit integers indicating the temperature in degrees Celsius in the range -40 to 6513.5 (scale 0.1), using the usual formula_1 formula (MSB is A, LSB is B). Only values for which the corresponding sensor is supported are meaningful.\n\nThe same structure applies to PID 79, but values are for sensors of bank 2.\n\nA request for this service returns a list of the DTCs that have been set. The list is encapsulated using the ISO 15765-2 protocol.\n\nIf there are two or fewer DTCs (4 bytes) they are returned in an ISO-TP Single Frame (SF). Three or more DTCs in the list are reported in multiple frames, with the exact count of frames dependent on the communication type and addressing details.\n\nEach trouble code requires 2 bytes to describe. The text description of a trouble code may be decoded as follows. The first character in the trouble code is determined by the first two bits in the first byte:\n\nThe two following digits are encoded as 2 bits. The second character in the DTC is a number defined by the following table:\n\nThe third character in the DTC is a number defined by\n\nThe fourth and fifth characters are defined in the same way as the third, but using bits B7-B4 and B3-B0. The resulting five-character code should look something like \"U0158\" and can be looked up in a table of OBD-II DTCs. Hexadecimal characters (0-9, A-F), while relatively rare, are allowed in the last 3 positions of the code itself.\n\nIt provides information about track in-use performance for catalyst banks, oxygen sensor banks, evaporative leak detection systems, EGR systems and secondary air system.\n\nThe numerator for each component or system tracks the number of times that all conditions necessary for a specific monitor to detect a malfunction have been encountered.\nThe denominator for each component or system tracks the number of times that the vehicle has been operated in the specified conditions.\n\nThe count of data items should be reported at the beginning (the first byte).\n\nAll data items of the In-use Performance Tracking record consist of two (2) bytes and are reported in this order (each message contains two items, hence the message length is 4).\n\nIt provides information about track in-use performance for NMHC catalyst, NOx catalyst monitor, NOx adsorber monitor, PM filter monitor, exhaust gas sensor monitor, EGR/ VVT monitor, boost pressure monitor and fuel system monitor.\n\nAll data items consist of two (2) bytes and are reported in this order (each message contains two items, hence message length is 4):\n\nSome PIDs are to be interpreted specially, and aren't necessarily exactly bitwise encoded, or in any scale.\nThe values for these PIDs are enumerated.\n\nA request for this PID returns 2 bytes of data.\nThe first byte describes fuel system #1.\n\nAny other value is an invalid response.\n\nThe second byte describes fuel system #2 (if it exists) and is encoded identically to the first byte.\n\nA request for this PID returns a single byte of data which describes the secondary air status.\n\nAny other value is an invalid response.\n\nA request for this PID returns a single byte of data which describes which OBD standards this ECU was designed to comply with. The different values the data byte can hold are shown below, next to what they mean:\n\nService 01 PID 51 returns a value from an enumerated list giving the fuel type of the vehicle. The fuel type is returned as a single byte, and the value is given by the following table:\n\nAny other value is reserved by ISO/SAE. There are currently no definitions for flexible-fuel vehicle.\n\nThe majority of all OBD-II PIDs in use are non-standard. For most modern vehicles, there are many more functions supported on the OBD-II interface than are covered by the standard PIDs, and there is relatively minor overlap between vehicle manufacturers for these non-standard PIDs.\n\nThere is very limited information available in the public domain for non-standard PIDs. The primary source of information on non-standard PIDs across different manufacturers is maintained by the US-based Equipment and Tool Institute and only available to members. The price of ETI membership for access to scan codes varies based on company size defined by annual sales of automotive tools and equipment in North America:\nHowever, even ETI membership will not provide full documentation for non-standard PIDs. ETI state:\n\nSome OEMs refuse to use ETI as a one-stop source of scan tool information. They prefer to do business with each tool company separately. These companies also require that you enter into a contract with them. The charges vary but here is a snapshot as of April 13th, 2015 of the per year charges:\n\nThe PID query and response occurs on the vehicle's CAN bus. Standard OBD requests and responses use functional addresses. The diagnostic reader initiates a query using CAN ID 7DFh, which acts as a broadcast address, and accepts responses from any ID in the range 7E8h to 7EFh. ECUs that can respond to OBD queries listen both to the functional broadcast ID of 7DFh and one assigned ID in the range 7E0h to 7E7h. Their response has an ID of their assigned ID plus 8 e.g. 7E8h through 7EFh.\n\nThis approach allows up to eight ECUs, each independently responding to OBD queries. The diagnostic reader can use the ID in the ECU response frame to continue communication with a specific ECU. In particular, multi-frame communication requires a response to the specific ECU ID rather than to ID 7DFh.\n\nCAN bus may also be used for communication beyond the standard OBD messages. Physical addressing uses particular CAN IDs for specific modules (e.g., 720h for the instrument cluster in Fords) with proprietary frame payloads.\n\nThe functional PID query is sent to the vehicle on the CAN bus at ID 7DFh, using 8 data bytes. The bytes are:\nThe vehicle responds to the PID query on the CAN bus with message IDs that depend on which module responded. Typically the engine or main ECU responds at ID 7E8h. Other modules, like the hybrid controller or battery controller in a Prius, respond at 07E9h, 07EAh, 07EBh, etc. These are 8h higher than the physical address the module responds to. Even though the number of bytes in the returned value is variable, the message uses 8 data bytes regardless (CAN bus protocol form Frameformat with 8 data bytes).\nThe bytes are:\n\n"}
{"id": "31547791", "url": "https://en.wikipedia.org/wiki?curid=31547791", "title": "Open Compute Project", "text": "Open Compute Project\n\nThe Open Compute Project (OCP) is an organization that shares designs of data center products among companies, including Facebook, IBM, Intel, Nokia, Google, Microsoft, Seagate Technology, Dell, Rackspace, Cisco, Goldman Sachs, Fidelity, Lenovo and Alibaba Group.\n\nThe Open Compute Project's mission is to design and enable the delivery of the most efficient server, storage and data center hardware designs for scalable computing. \"We believe that openly sharing ideas and specifications is the key to maximizing innovation and reducing operational complexity in the scalable computing space.\"\n\nAll Facebook data centers are 100% OCP.\n\nThe initiative was announced in April 2011 by Jonathan Heiliger at Facebook to openly share designs of data center products.\nThe effort came out of a redesign of Facebook's data center in Prineville, Oregon.\nAfter two years, with regards to a more modular server design, it was admitted that \"the new design is still a long way from live data centers\".\nHowever, some aspects published were used in the Prineville center to improve energy efficiency, as measured by the power usage effectiveness index defined by The Green Grid.\n\nThe Open Compute Project Foundation is a 501(c)(6)<nowiki> non-profit incorporated in the state of Delaware. Rocky Bullock serves as the Foundation's CEO. As of April 2018, there are 7 members who serve on the board of directors which is made up of one individual member and six organizational members. Mark Roenigk (</nowiki>Facebook) is the Foundation's president and chairman. Andy Bechtolsheim is the individual member. In addition to Mark Roenigk who represents Facebook, other organizations on the Open Compute board of directors include Intel (Jason Waxman), Goldman Sachs (Joshua Matheus), Rackspace (Brian Stein), and Microsoft (Bill Laing).\n\nA current list of members can be found here\n\nComponents of the Open Compute Project include:\n\n\nSeveral generations of server designs have been deployed: Freedom (Intel), Spitfire (AMD), Windmill (Intel E5-2600), Watermark (AMD), Winterfell (Intel E5-2600 v2) and Leopard (Intel E5-2600 v3)\n\n\nIn March, 2015, BladeRoom Group Limited and Bripco (UK) Limited sued Facebook, Emerson Electric Co. and others alleging that Facebook has disclosed BladeRoom and Bripco's trade secrets for prefabricated data centers in the Open Compute Project. Facebook petitioned for the lawsuit to be dismissed, but this was rejected in 2017.\n\nThe promoted vendors include:\n\n\n\n"}
{"id": "8704205", "url": "https://en.wikipedia.org/wiki?curid=8704205", "title": "Outline of energy storage", "text": "Outline of energy storage\n\nThe following outline is provided as an overview of and topical guide to energy storage:\n\nEnergy storage – accomplished by devices or physical media that store some form of energy to perform some useful operation at a later time. A device that stores energy is sometimes called an accumulator.\n\n\n1. Using a high density magnetic field. Energy formula_1 in terms of magnetic field strength formula_2 is\n\nformula_3\n\nwhere formula_4 is the inductance of the electromagnet and formula_5 is the current.\n\n\n"}
{"id": "50921584", "url": "https://en.wikipedia.org/wiki?curid=50921584", "title": "Prusa i3", "text": "Prusa i3\n\nThe Prusa i3 is an open-source fused deposition modeling 3D printer. Part of the RepRap project, it is the most used desktop 3D printer for parts ordered through the 3D Hubs fee-for-service business. The Prusa i3 was designed by Josef Průša in 2012 with the Prusa i3 MK2 being released in 2016 and the MK2S being released in 2017. The latest version, called the Prusa i3 MK3 was released in September 2017 with significant improvements over the prior models. The Prusa i3's comparable low cost and ease of construction and modification has made it popular in education and with hobbyists and professionals. Due to the printer being open source there have been many variants produced by companies and individuals worldwide, and like many other RepRap printers the Prusa i3 is capable of printing some of its own parts.\n\nThe Prusa i3 is the third printer design by Josef Průša, a core developer of the RepRap project who had previously developed the PCB heated bed. The first iteration was the Prusa Mendel produced in 2010 followed by the Prusa Mendel (iteration 2), in 2011. The printer was named Prusa Mendel by the RepRap community rather than Průša himself.\n\nThe first Prusa Mendel was released in September 2010 with the aim of simplifying the existing Mendel design, including reducing the time needed to create the 3D printed parts from 20 to 10 hours and 3D printable bushings replacing regular bearings.\n\nThe second Prusa Mendel was released in November 2011 and with upgrades including snap fit parts, reduction of the number of tools needed to construct, maintain the printer and improved belts attached to the stepper motors and use of LM8UU linear bearings.\n\nIn May 2012 designs (produced in OpenSCAD) for the Prusa i3 were released, it was a major redesign from previous versions and other RepRap printers. The design replaced the triangular threaded rod frame construction with a water jet cut aluminum frame, had a food-safe hot end called the Prusa Nozzle, and used M5 threaded screws instead of M8. The design focused on ease of construction and use rather than maximising the number of self replicated components. In 2015 Průša released a version which he called the Original Prusa i3, selling through his company Prusa Research.\n\nIn May 2016 the Prusa i3 MK2 was released, it is the first printer with automatic geometry skew correction for all three axes and includes a larger build volume, custom stepper motors with integrated lead screws, mesh bed levelling using a non-contact inductance sensor and a rewritten version of the Marlin firmware. Other new features include a polyetherimide print surface, Rambo controller board and an E3D V6 Full hotend. The Prusa MK2 became the first RepRap printer to be supported by Windows Plug-and-Play USB ID.\n\nIn March 2017, Josef Prusa announced on his blog that the Prusa i3 MK2 was now shipping as the Prusa i3 MK2S. \nEnhancements cited include U-bolts to hold the LM8UU bearings, improved LM8UU bearings, smoother rods, an improved mount for the inductance sensor, improved cable management, and a new electronics cover. While new shipments of the MK2 automatically receive the \"S\" upgrades, an upgrade kit is available to bring these improvements to earlier purchasers.\n\nIn September 2017, Prusa i3 MK3 was released, marketed as \"bloody smart\". Upgraded features included: a sturdier Y axis, a new extruder with double sided Bondtech drive-gears, quieter fans with rpm monitoring, an updated bed leveling sensor, a new electronics board named \"Einsy\", quieter stepper motors with 128 step microstepping drivers and a magnetic heatbed with interchangeable PEI-coated steel sheets. This model also includes new sensors: several temperature sensors, a filament detector, and a sensor that detects power interruptions. The MK3 runs on 24V instead of 12V, so all the electrical components were updated to 24V variants. The printer also offers dedicated sockets to connect Raspberry Pi Zero W running open source Octoprint software for wireless printing, and offers a custom Octoprint build for Prusa i3.\n\nThe idea of the MK3 update was to make the printer easier to use for casual users and less likely to lead to failed prints. The filament detector allows users to load filament just by inserting it, and it can detect and pause the print if the filament is jammed or runs out. In both cases, printing resumes normally after the issue is fixed. The new stepper motor drivers can handle skipped steps automatically and prevent layer shifts. The power panic sensor allows the printer to recover from power outages, and uses the remaining charge in the power supply's capacitors to lift the print head up to prevent it from melting the printed part while the power is out. The ambient temperature sensor is able to detect mainboard overheating caused by loose electrical connections.\n\nThe upgraded chassis and electronics also allow for faster print speeds — up to 200 mm/s.\n\nExisting MK2 and MK2S users were offered a $200 partial upgrade named MK2.5, limited to features which are cheaper to upgrade. After negative feedback from the community, Prusa made available a more expensive $500 MK2S to MK3 full upgrade.\n\nAs an open-source design the success of the Prusa i3 is underlined by the ready availability of both complete and kit-built machines that follow the various iterations of the i3 design. Rather than compete directly with these versions, Prusa Research's strategy is to pursue continual refinement of its designs.\n\nIn 2012 Josef received honours from the governor of the Vysočina Region in the Czech Republic for his accomplishments in technology. In February 2014 he was featured on the cover of Czech Forbes magazine as one of the 30 under 30 list.\n\nLike other RepRap printers the Prusa i3 is capable of creating many of its own parts, which were usually printed in ABS plastic prior to the Mk3, which uses PETG instead. The standard Prusa i3 has 26 printed parts.\n\nDepending on the hot end and heated bed installed, the Prusa i3 is capable of printing many materials including Acrylonitrile butadiene styrene (ABS), polylactic acid (PLA), high impact polystyrene (HIPS), polyetilene (PET), different flexible filaments (FLEX, TPU, TPE), polypropylene (PP) and nylon. Different materials require different working temperatures and techniques to make them adhere to the print bed.\n\nThe Prusa i3, like many RepRap printers, is made from a combination of self-replicated 3D printed parts and off-the-shelf components which are commonly referred to as \"vitamins\", as they cannot be produced by the printer itself.\n\nThe vitamins used on the Prusa i3 are a combination of common components including threaded rods, smooth rods, screws, nuts, 5 NEMA 17 stepper motors and more specialist equipment including a controller board, heated bed and hot end.\n\nDue to its popularity the Prusa i3 has many variants produced by different companies and individuals around the world including different styles of frame and extruders.\n\nThe main variant in designs of the Prusa i3 are different frames used, these include a single sheet frame cut from steel, acrylic (laser cut or CNC milled), medium-density fibreboard box frames and Lego.\n\nThere are a range of different extruders used on variants of the Prusa i3 including 1.75 mm and 3 mm filament extruders and other tool heads including a MIG welder and a laser cutter.\n\n"}
{"id": "49378265", "url": "https://en.wikipedia.org/wiki?curid=49378265", "title": "Reynolds and Branson", "text": "Reynolds and Branson\n\nReynolds & Branson Leeds was a business based at 13 Briggate and 14 Commercial Street in Leeds, England. The business lasted from 1816 to 1972. Edward Matterson managed the company in 1822, and William West F.R.S. took over the in 1833. The National Archives Records about the company include a day book, sales ledger, and prescription books. The records were created by Reynolds & Branson Ltd. Reynolds & Branson was registered in July 1898 as a limited corporation with a capital of £34,000 in shares of £10 each by Messrs. R. Reynolds, F. W. Branson. No remuneration was given to Mr. R. Reynolds, but a £700 per annum was given to each of the others. In 1890, Richard Reynold's son, Richard Freshfield (Fred) Reynolds was made a partner. \nThe firm was in the business of wholesale and retail for chemists and surgical instrument makers.\n\nThe original company can be traced back to 1816 (see Grace's Guide which is the leading source of historical information on industry and manufacturing in Britain). Edward Matterson was a druggist who ran the firm after being employed by Allen and Hanburys. He was educated at Leeds Grammar School. In 1822 the company moved to 13 Briggate, Leeds. In 1833 William West F.R.S. took over the company after Matterson went bankrupt (see The bankrupt directory: being a complete register of all the bankruptcies, with their residences, trades, and dates when they appeared in the London gazette, from December 1820 to April 1843). In 1839 Thomas Harvey joined the business, when William West left the company to pursue analytical chemistry. The firm was then renamed Thomas Harvey. Harvey was born at Barnsley into a Quaker family. His father was a linen manufacturer. The second of five children, he was educated at Barnsley Grammar in Yorkshire in 1812. From 1822 to 1825 Harvey studied at Ackworth and afterwards became a chemist apprentice for David Doncaster of Sheffield. Upon Doncaster's death he trained at Thomas Southalls in Birmingham for eight years. In 1837 Harvey settled in Leeds as a chemist. He became an anti-slavery campaigner and philanthropist.\n\nIn 1844 Richard Reynolds joined the company as an apprentice. He was born in 1829 and was the eldest son of an apothecary who died when the boy was only four years old. From 1850 to 1851 he attended the School of Pharmacy in London where he took first prizes in chemistry, materia medica and botany in a contest held by the pharmaceutical society. He then went to Mr. Henry Deane at Chapman for two years and returned to the Leeds business. In 1854 Richard Reynolds joined Thomas Harvey as a partner and the company then became Harvey & Reynolds. In 1861 the firm was joined by a Mr. Fowler and became Harvey, Reynolds & Fowler. By 1864 Thomas Harvey had retired (Noted in 1884). At the age of 72, he undertook an arduous journey to Canada on a Quaker mission but it exhausted him. He died on December 25 at his home at Ashwood, Headingley Lane. Mr. Haw then joined the business and the company became Haw & Reynolds. In 1867 the business was listed as Haw, Reynolds, & Co. In 1883 Fredrick W Branson joined the business. An 1884 advertisement listed the partnership between Reynolds & Branson (late Harvey, Reynolds & co).\n\nThe firm of Reynolds & Branson was registered in July 1898 as a limited corporation with a capital of £34,000 in shares of £10 each by Messrs. R. Reynolds, F. W. Branson. No remuneration was given to Mr. R. Reynolds, but a £700 each per annum to the others. In 1890 Richard Reynold's son, Richard Freshfield (Fred) Reynolds was made a partner. The firm was in the business of wholesale and retail chemists and surgical instrument makers.[5] Fredrick W Branson now focused on the development of scientific apparatus and chemical glassware for the business. The company was flourishing under his management. Frederick Hartridge attended the University of Leeds in 1905, and then attending the School of Pharmaceutical Society in London in 1909. In 1901, during the outbreak of lead poisoning at Morley, the company was called in. Frederick W. Branson made recommendations which freed Morley from this scourge. In collaboration with A. F. Dimmock, M.D., he contributed to the British Medical Association meeting in 1903 a paper \" A new method for the determination of uric acid in urine\" (Br. Med. J., 1903, 2, 585). For this process he devised a correction scale which was contributed to the British Pharmaceutical Conference in 1904. At the 1905 meeting of the British Medical Association a further paper by these two authors was read, \" A rapid and simple process for the estimation of uric acid \" (ibid., 1905, 2, 1104), in which uric acid was precipitated and the precipitate measured in a specially graduated tube. In 1914, in collaboration with Dr. Gordon Sharp, he contributed a paper to the British Pharmaceutical Conference on the activity of digitalis leaves and the stability and standardisation of tinctures of digitalis.\n\nDuring the First World War, he actively pursued efforts to standardize the size and shape of chemical glassware. In 1916, he was elected as an inaugural member of the Society of Glass Technology. He organized research and published works on these topics. Branson sought to secure in Great Britain the manufacturing process for the glass required for the equipment of munition factories.[8][9] Branson contributed a paper on the composition of some types of chemical glassware to the Society of Chemical Industry (J. SOC. Chem. Ind., 1915, 34, 471) in collaboration with his son Frederick Hartridge, a paper to the Transactions of the Society of Glass Technology (1919, 3, 249)\" A proposed standard formula for a glass for lamp workers. Branson was chairman until retirement in 1932. His son, Frederick Hartridge, Associate of the Royal Institute of Chemistry AIC, became chairman and managing director of Reynolds & Branson.[5][7] he run the company for 20 years, until his untimely death on 10 February 1952, Frederick Hartridge had appointed his 3 Sons and Daughter as directors of Reynolds & Branson, Frederick Norman the eldest son who attended Ilkley Grammar an all-boys school, Eileen his only daughter, Peter Orchard who as Director of Phospherade, which was the mineral water company, He attended Roundhay all-boys grammar school.\n\nIn the Second World War he was in the RAF with 54 Spitfire squadron, in 1942 married Rita Blackburn, he went to Australia with 54 Spitfire squadron at the end of 1942, he met Patricia A Grant his second wife. He married Patricia in Leeds 1948, Peter emigrated to Australia 1953, when his father died leaving the bulk of the business to his eldest brother Frederick Norman, He set up his own pharmacy in Blackburn South in 1955, He later become a Podiatrist retiring at the age of 90, Richard Orchard who attended Roundhay all-boys grammar school, Second World War Richard was also in RAF as a pilot he died on active service 1945. His eldest son Frederick Norman Branson became Chairman & Managing Director of Reynolds & Branson in 1953, he would run Reynolds & Branson for almost 20 years, at this point the company had a workforce of 150 people, In 1972 Frederick Norman Branson sold the business to Barclay, later selling to the asset strippers Slater & Walker.\n\n\nOn 24 July 1896, Reynolds and Branson attended the Photographic Convention of the United Kingdom at Leeds. The firm was represented in various sessions. During the session on Orthochromatic Photography, Branson gave a presentation on X-ray apparatus that included a well received demonstration and repeated as follows:\"... Mr. Branson, of Messrs. Reynolds and Branson, who had made a special study of X ray work, gave a demonstration which for lucidity and completeness has rarely been equalled. In the course of his remarks he fully explained the construction and exhaustion of the tubes, and showed various forms and explained his method of making calcium tungstate, which was to mix solutions of sodium tungstate and calcium chloride, collect, wash, and dry the precipitate of calcium tungstate which was formed, and then to fuse this in a small muffle furnace at the temperature of the melting point of cast-iron, and reduce to small crystals in a mortar, mix with varnish, and coat a screen. With such a screen in contact with the plate he had been able to show osseous structure of the hand, measuring only one-hundredth of an inch, with an exposure of one minute. A comparison of the fluorescent appearance of the three salts, calcium tungstate, platinocyanide of barium, and platinocyanide of potassium, was shown, the first and last being the best for photographic work, as the fluorescence was blue, and the barium salt was most satisfactory for visual work, as the fluorescence was yellow.\"\n\nAt the same convention, during the session on Photography at the Seaside the firm displayed some of their product line that included X-ray apparatus, as follows:\"Reynolds & Branson, of Commercial Street, Leeds, had a very high-class show, special prominence being given to apparatus for X ray work. A case of lenses of all the leading makers, together with a very well-made photo-micrographic outfit, a cabinet of chemicals, another of cameras, and all the little odds and ends of apparatus, made up a very fine show.\"\n\nReynolds and Branson trade catalogues listed:\n\nPatents include: #1120 in 1885, #16373 in 1893, #14102 in 1899.\n\n"}
{"id": "11555582", "url": "https://en.wikipedia.org/wiki?curid=11555582", "title": "ST-124-M3 inertial platform", "text": "ST-124-M3 inertial platform\n\nThe ST-124-M3 inertial platform was a device for measuring acceleration and attitude of the Saturn V launch vehicle. It was carried by the Saturn V Instrument Unit, a , section of the Saturn V that fit between the third stage (S-IVB) and the Apollo spacecraft. Its nomenclature means \"stable table\" (ST) for use in the moon mission (M), and it has 3 gimbals.\n\nIt was number 124 in a series of similar devices, including the ST-120 (used in the Pershing missile), the ST-90 (used on the Jupiter and on early Saturn I flights), and the ST-80 (used in the Redstone). They are descendants of the LEV-3 of the German V-2 rocket. The ST-124 was designed by Marshall Space Flight Center and manufactured by Bendix Corporation, Eclipse-Pioneer Division, in Teterboro, NJ. It took 9 men 22 to 24 weeks to assemble an ST-124, and 70 percent of that time was spent installing about 3,000 wires.\n\nThe ST-124 stabilized platform was part of the navigation, control and guidance system of the Saturn V. Data from the ST-124 were used by the Launch Vehicle Digital Computer (another Instrument Unit component) to compare actual flight data to programmed flight plans and to calculate guidance corrections. Though the ST-124 operated all during the mission, its data were not used for guidance while the vehicle was in the atmosphere, where it was subjected to high drag forces. In this region, basically the time of the first stage burn, the vehicle followed a simple preprogrammed flight plan.\n\nThe attitude of the vehicle was measured relative to a coordinate system that was fixed just prior to launch with the X coordinate vertical, the Z coordinate in the direction of the pitch maneuver (down range, roughly East), and the Y coordinate perpendicular to the other two, cross range, roughly South. At the heart of the ST-124 was a platform that was held in a fixed orientation; hence the name \"stabilized platform\". It is connected by three gimbals that allowed the vehicle to roll, pitch and yaw but the stable platform to be held fixed in space. It was being translated, of course, but did not tilt during flight.\n\nThe platform is stabilized by three gyros mounted on it. One measured any rotations about the X axis, one about the Y, and one about the Z axis. They generated signals that were shaped in feedback circuits and sent back to torquers on the inner, middle and outer gimbals that exactly countered the rotations, nulling the gyro outputs and keeping the platform stable.\n\nThe inner gimbal also carries three accelerometers, two pendulums, and a pair of prisms. The accelerometers measured vehicle acceleration along the X, Y, and Z axes. Their outputs were used by the LVDC to measure actual vehicle motion, for the purpose of navigation. The pendulums were used to set the X axis exactly vertical, and the prisms were used to align the Y and Z axes, just before launch. The prisms reflected infrared beams sent into the ST-124 by a theodolite stationed 700 feet away from the launch pad. Commands from the theodolite were transmitted via cables to the vehicle, to torquers in the ST-124 to orient the stable platform toward the correct azimuth.\n\nThe gyros, accelerometers and pendulums contain almost frictionless nitrogen gas bearings. These required very precise machining and very small gaps between the bearing surfaces. Dimensions were held to tolerances of 20 microinches (0.5 µm), and the gap filled by the nitrogen is about 600-800 microinches. Nitrogen entered the gyros at about 15 psi and was vented to space via a pressure regulator in the bottom of the ST-124 that opened at 13 psi. The large silver sphere to the left of the ST-124 held the supply of nitrogen for the bearings.\n\nThe ST-124 includes many components made of anodized beryllium. This material was chosen for its stiffness, light weight, machinability and stability. The case of the ST-124 is a short cylinder, 7.5 inches high and 21 inches in diameter, made of beryllium. The ends of the cylinder are closed by two approximately hemispherical aluminum covers. The gimbals and several parts of the gyros and accelerometers are also made of beryllium.\n\nIn contrast to beryllium, which is lightweight, the rotors of the gyros are made of a very dense, strong material, Elkonite. This is a sintered form of tungsten, with about 10 percent copper, to make it machinable.\n\nHeat generated by torquers and other electrical equipment inside the ST-124 was carried away by cooling coils built into the aluminum covers. A mixture of methanol and water at 15 degrees C was circulated through the coils. The internal temperature of the ST-124 stabilized at about 42 degrees C.\n\n"}
{"id": "29123", "url": "https://en.wikipedia.org/wiki?curid=29123", "title": "Semantic Web", "text": "Semantic Web\n\nThe Semantic Web is an extension of the World Wide Web through standards by the World Wide Web Consortium (W3C). The standards promote common data formats and exchange protocols on the Web, most fundamentally the Resource Description Framework (RDF). According to the W3C, \"The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries\". The Semantic Web is therefore regarded as an integrator across different content, information applications and systems.\n\nThe term was coined by Tim Berners-Lee for a web of data (or data web) that can be processed by machines—that is, one in which much of the meaning is machine-readable. While its critics have questioned its feasibility, proponents argue that applications in industry, biology and human sciences research have already proven the validity of the original concept.\n\nBerners-Lee originally expressed his vision of the Semantic Web as follows:\nThe 2001 \"Scientific American\" article by Berners-Lee, Hendler, and Lassila described an expected evolution of the existing Web to a Semantic Web. In 2006, Berners-Lee and colleagues stated that: \"This simple idea…remains largely unrealized\".\nIn 2013, more than four million Web domains contained Semantic Web markup.\n\nIn the following example, the text 'Paul Schuster was born in Dresden' on a Website will be annotated, connecting a person with their place of birth. The following HTML-fragment shows how a small graph is being described, in RDFa-syntax using a schema.org vocabulary and a Wikidata ID:\n<div vocab=\"http://schema.org/\" typeof=\"Person\">\n</div>\nThe example defines the following five triples (shown in Turtle Syntax). Each triple represents one edge in the resulting graph: the first element of the triple (the \"subject\") is the name of the node where the edge starts, the second element (the \"predicate\") the type of the edge, and the last and third element (the \"object\") either the name of the node where the edge ends or a literal value (e.g. a text, a number, etc.).\nThe triples result in the graph shown in the given figure.\n\nOne of the advantages of using Uniform Resource Identifiers (URIs) is that they can be dereferenced using the HTTP protocol. According to the so-called Linked Open Data principles, such a dereferenced URI should result in a document that offers further data about the given URI. In this example, all URIs, both for edges and nodes (e.g. <nowiki>http://schema.org/Person</nowiki>, <nowiki>http://schema.org/birthPlace</nowiki>, <nowiki>http://www.wikidata.org/entity/Q1731</nowiki>) can be dereferenced and will result in further RDF graphs, describing the URI, e.g. that Dresden is a city in Germany, or that a person, in the sense of that URI, can be fictional.\n\nThe second graph shows the previous example, but now enriched with a few of the triples from the documents that result from dereferencing <nowiki>http://schema.org/Person</nowiki> (green edge) and <nowiki>http://www.wikidata.org/entity/Q1731</nowiki> (blue edges).\n\nAdditionally to the edges given in the involved documents explicitly, edges can be automatically inferred: the triple\nfrom the original RDFa fragment and the triple\nfrom the document at <nowiki>http://schema.org/Person</nowiki> (green edge in the Figure) allow to infer the following triple, given OWL semantics (red dashed line in the second Figure):\n\nThe concept of the \"Semantic Network Model\" was formed in the early 1960s by the cognitive scientist Allan M. Collins, linguist M. Ross Quillian and psychologist Elizabeth F. Loftus as a form to represent semantically structured knowledge. When applied in the context of the modern internet, it extends the network of hyperlinked human-readable web pages by inserting machine-readable metadata about pages and how they are related to each other. This enables automated agents to access the Web more intelligently and perform more tasks on behalf of users. The term \"Semantic Web\" was coined by Tim Berners-Lee, the inventor of the World Wide Web and director of the World Wide Web Consortium (\"W3C\"), which oversees the development of proposed Semantic Web standards. He defines the Semantic Web as \"a web of data that can be processed directly and indirectly by machines\".\n\nMany of the technologies proposed by the W3C already existed before they were positioned under the W3C umbrella. These are used in various contexts, particularly those dealing with information that encompasses a limited and defined domain, and where sharing data is a common necessity, such as scientific research or data exchange among businesses. In addition, other technologies with similar goals have emerged, such as microformats.\n\nMany files on a typical computer can also be loosely divided into human readable documents and machine readable data. Documents like mail messages, reports, and brochures are read by humans. Data, such as calendars, addressbooks, playlists, and spreadsheets are presented using an application program that lets them be viewed, searched and combined.\n\nCurrently, the World Wide Web is based mainly on documents written in Hypertext Markup Language (HTML), a markup convention that is used for coding a body of text interspersed with multimedia objects such as images and interactive forms. Metadata tags provide a method by which computers can categorise the content of web pages, for example:\n\nWith HTML and a tool to render it (perhaps web browser software, perhaps another user agent), one can create and present a page that lists items for sale. The HTML of this catalog page can make simple, document-level assertions such as \"this document's title is 'Widget Superstore, but there is no capability within the HTML itself to assert unambiguously that, for example, item number X586172 is an Acme Gizmo with a retail price of €199, or that it is a consumer product. Rather, HTML can only say that the span of text \"X586172\" is something that should be positioned near \"Acme Gizmo\" and \"€199\", etc. There is no way to say \"this is a catalog\" or even to establish that \"Acme Gizmo\" is a kind of title or that \"€199\" is a price. There is also no way to express that these pieces of information are bound together in describing a discrete item, distinct from other items perhaps listed on the page.\n\nSemantic HTML refers to the traditional HTML practice of markup following intention, rather than specifying layout details directly. For example, the use of denoting \"emphasis\" rather than , which specifies italics. Layout details are left up to the browser, in combination with Cascading Style Sheets. But this practice falls short of specifying the semantics of objects such as items for sale or prices.\n\nMicroformats extend HTML syntax to create machine-readable semantic markup about objects including people, organisations, events and products. Similar initiatives include RDFa, Microdata and Schema.org.\n\nThe Semantic Web takes the solution further. It involves publishing in languages specifically designed for data: Resource Description Framework (RDF), Web Ontology Language (OWL), and Extensible Markup Language (XML). HTML describes documents and the links between them. RDF, OWL, and XML, by contrast, can describe arbitrary things such as people, meetings, or airplane parts.\n\nThese technologies are combined in order to provide descriptions that supplement or replace the content of Web documents. Thus, content may manifest itself as descriptive data stored in Web-accessible databases, or as markup within documents (particularly, in Extensible HTML (XHTML) interspersed with XML, or, more often, purely in XML, with layout or rendering cues stored separately). The machine-readable descriptions enable content managers to add meaning to the content, i.e., to describe the structure of the knowledge we have about that content. In this way, a machine can process knowledge itself, instead of text, using processes similar to human deductive reasoning and inference, thereby obtaining more meaningful results and helping computers to perform automated information gathering and research.\n\nAn example of a tag that would be used in a non-semantic web page:\nEncoding similar information in a semantic web page might look like this:\nTim Berners-Lee calls the resulting network of Linked Data the Giant Global Graph, in contrast to the HTML-based World Wide Web. Berners-Lee posits that if the past was document sharing, the future is data sharing. His answer to the question of \"how\" provides three points of instruction. One, a URL should point to the data. Two, anyone accessing the URL should get data back. Three, relationships in the data should point to additional URLs with data.\n\nTim Berners-Lee has described the semantic web as a component of \"Web 3.0\".\n\n\"Semantic Web\" is sometimes used as a synonym for \"Web 3.0\", though the definition of each term varies. Web 3.0 has started to emerge as a movement away from the centralisation of services like search, social media and chat applications that are dependent on a single organisation to function.\n\nSome of the challenges for the Semantic Web include vastness, vagueness, uncertainty, inconsistency, and deceit. Automated reasoning systems will have to deal with all of these issues in order to deliver on the promise of the Semantic Web.\n\nThis list of challenges is illustrative rather than exhaustive, and it focuses on the challenges to the \"unifying logic\" and \"proof\" layers of the Semantic Web. The World Wide Web Consortium (W3C) Incubator Group for Uncertainty Reasoning for the World Wide Web (URW3-XG) final report lumps these problems together under the single heading of \"uncertainty\". Many of the techniques mentioned here will require extensions to the Web Ontology Language (OWL) for example to annotate conditional probabilities. This is an area of active research.\n\nStandardization for Semantic Web in the context of Web 3.0 is under the care of W3C.\n\nThe term \"Semantic Web\" is often used more specifically to refer to the formats and technologies that enable it. The collection, structuring and recovery of linked data are enabled by technologies that provide a formal description of concepts, terms, and relationships within a given knowledge domain. These technologies are specified as W3C standards and include:\n\nThe Semantic Web Stack illustrates the architecture of the Semantic Web. The functions and relationships of the components can be summarized as follows:\n\nWell-established standards:\n\nNot yet fully realized:\n\nThe intent is to enhance the usability and usefulness of the Web and its interconnected resources by creating Semantic Web Services, such as:\n\nSuch services could be useful to public search engines, or could be used for knowledge management within an organization. Business applications include:\n\nIn a corporation, there is a closed group of users and the management is able to enforce company guidelines like the adoption of specific ontologies and use of semantic annotation. Compared to the public Semantic Web there are lesser requirements on scalability and the information circulating within a company can be more trusted in general; privacy is less of an issue outside of handling of customer data.\n\nCritics question the basic feasibility of a complete or even partial fulfillment of the Semantic Web, pointing out both difficulties in setting it up and a lack of general-purpose usefulness that prevents the required effort from being invested. In a 2003 paper, Marshall and Shipman point out the cognitive overhead inherent in formalizing knowledge, compared to the authoring of traditional web hypertext:\n\nAccording to Marshall and Shipman, the tacit and changing nature of much knowledge adds to the knowledge engineering problem, and limits the Semantic Web's applicability to specific domains. A further issue that they point out are domain- or organisation-specific ways to express knowledge, which must be solved through community agreement rather than only technical means. As it turns out, specialized communities and organizations for intra-company projects have tended to adopt semantic web technologies greater than peripheral and less-specialized communities. The practical constraints toward adoption have appeared less challenging where domain and scope is more limited than that of the general public and the World-Wide Web.\n\nFinally, Marshall and Shipman see pragmatic problems in the idea of (Knowledge Navigator-style) intelligent agents working in the largely manually curated Semantic Web:\n\nCory Doctorow's critique (\"metacrap\") is from the perspective of human behavior and personal preferences. For example, people may include spurious metadata into Web pages in an attempt to mislead Semantic Web engines that naively assume the metadata's veracity. This phenomenon was well-known with metatags that fooled the Altavista ranking algorithm into elevating the ranking of certain Web pages: the Google indexing engine specifically looks for such attempts at manipulation. Peter Gärdenfors and Timo Honkela point out that logic-based semantic web technologies cover only a fraction of the relevant phenomena related to semantics.\n\nEnthusiasm about the semantic web could be tempered by concerns regarding censorship and privacy. For instance, text-analyzing techniques can now be easily bypassed by using other words, metaphors for instance, or by using images in place of words. An advanced implementation of the semantic web would make it much easier for governments to control the viewing and creation of online information, as this information would be much easier for an automated content-blocking machine to understand. In addition, the issue has also been raised that, with the use of FOAF files and geolocation meta-data, there would be very little anonymity associated with the authorship of articles on things such as a personal blog. Some of these concerns were addressed in the \"Policy Aware Web\" project and is an active research and development topic.\n\nAnother criticism of the semantic web is that it would be much more time-consuming to create and publish content because there would need to be two formats for one piece of data: one for human viewing and one for machines. However, many web applications in development are addressing this issue by creating a machine-readable format upon the publishing of data or the request of a machine for such data. The development of microformats has been one reaction to this kind of criticism. Another argument in defense of the feasibility of semantic web is the likely falling price of human intelligence tasks in digital labor markets, such as Amazon's Mechanical Turk.\n\nSpecifications such as eRDF and RDFa allow arbitrary RDF data to be embedded in HTML pages. The GRDDL (Gleaning Resource Descriptions from Dialects of Language) mechanism allows existing material (including microformats) to be automatically interpreted as RDF, so publishers only need to use a single format, such as HTML.\n\nThe first research group explicitly focusing on the Corporate Semantic Web was the ACACIA team at INRIA-Sophia-Antipolis, founded in 2002. Results of their work include the RDF(S) based Corese search engine, and the application of semantic web technology in the realm of E-learning.\n\nSince 2008, the Corporate Semantic Web research group, located at the Free University of Berlin, focuses on building blocks: Corporate Semantic Search, Corporate Semantic Collaboration, and Corporate Ontology Engineering.\n\nOntology engineering research includes the question of how to involve non-expert users in creating ontologies and semantically annotated content and for extracting explicit knowledge from the interaction of users within enterprises.\n\nTim O'Reilly, who coined the term Web 2.0 proposed a long-term vision of the Semantic Web as a web of data, where sophisticated applications manipulate the data web. The data web transforms the Web from a distributed file system into a distributed database system.\n\n\n"}
{"id": "6182013", "url": "https://en.wikipedia.org/wiki?curid=6182013", "title": "Shetab Banking System", "text": "Shetab Banking System\n\nThe Shetab (Interbank Information Transfer Network) system is an electronic banking clearance and automated payments system used in Iran. The system was introduced in 2002 with the intention of creating a uniform backbone for the Iranian banking system to handle ATM, POS and other card-based transactions.\n\nPrior to its introduction, some Iranian banks were issuing cards that only worked on the issuing banks ATMs and POS machines. Since the introduction of Shetab, all banks must adhere to its standards and be able to connect to it. Furthermore, all issued credit or debit cards must be Shetab capable. As of the end of 2016, the Shetab system had 44,013 ATMs connected to it.\n\nShetab was introduced in 2002, and now all card issuing banks in Iran are required to connect to the system. In 2002, when the system was introduced there were at approximately 2.8 million domestic debit cards in circulation, of those approximately 530,000 were capable of using the Shetab system.\n\nIn 2005, the government obliged the Central Bank of Iran and the Iranian banks, mostly state owned, to set up all the necessary infrastructures (regulatory, hardware, software) for fully launching e-money in Iran by March 2005. While this plan has not yet fully materialised, local debit/credit cards are now commonplace and have removed the main obstacle to the growth of e-commerce (in the national scale) as well as the full roll out of e-government initiatives. By 2010 it is expected that 12 million cards would be issued, all of which work with the Shetab system.\n\nThe Agricultural Bank (Keshavarzi Bank) was the first Iranian bank to connect to the Shetab system. Saman Bank was the first bank to introduce online banking services in Iran. Since, it has been at the forefront of expansion and enhancement of electronic banking.\n\nIn 2007, before the imposition of new sanctions against Iran, Tetra-Tech IT Company announced that using VISA and MasterCard is now possible for online sales and in Iranian e-card terminals at shopping malls, hotels, restaurants, and travel agencies for Iranians and foreign tourists. Iran's electronic commerce will reach 10,000 billion rials ($US1 billion) by March 2009.\n\nIn 2010 nearly every bank branch in Iran had a Shetab system Connected ATM unit. More than 70% of shops, restaurants and markets are connected to the Shetab system. Many online stores are also linked to the Shetab system., Mobile and SMS Banking are recent additions to the Shetab system.\n\n\nAs of 2018, nearly all Iranian banks and some international banks in Qatar, Kuwait and Bahrain are members of the Shetab System, as follows:\n\nAs of 2006, Iran was still very much a cash based society. It is expected that a unified clearance system, such as Shetab, will provide significantly greater efficiency, reduce crime, reduce money printing costs, and improve tax collection among other benefits. It is also expected to improve the quality of life of citizens whom, once the system is fully operational, would no longer be required to spend significant amounts of time organizing things in person and would consequently be able to conduct activities immediately over the phone or over the internet. The impact of the system is already being felt as corporations establish e-commerce, supply chains, online banking and retailing systems.\n\n\n"}
{"id": "42764263", "url": "https://en.wikipedia.org/wiki?curid=42764263", "title": "Silicon Peach", "text": "Silicon Peach\n\nSilicon Peach is a term used to refer to Atlanta, Ga. and the concentration of high tech development in the area. The term is a continuation of the reference following Silicon Valley (California), Silicon Alley (New York), then Silicon Beach (Los Angeles). Atlanta has long been a high tech center. Some of the traditional engines of technology development in Atlanta have been the ATDC at the Georgia Institute of Technology and the Technology Association of Georgia (TAG).\nNew centers of innovation and technology acceleration are emerging, including the recent launch of the Atlanta Tech Village; the Ponce City Market; Tech Square Labs; and the Flatiron Accelerator.\n\nAtlanta is one of the fastest growing high-tech urban centers in the United States, with a particular focus on:\n\nAccording to a 2012 Jones Lang LeSalle report, Atlanta is the 10th largest technology city in the United States. The report went on to say that Atlanta tech companies generated $113 billion in revenue and nearly 30,000 new jobs. It is estimated that Atlanta companies will invest over $1 billion into new technology over the next five years.\n"}
{"id": "48817345", "url": "https://en.wikipedia.org/wiki?curid=48817345", "title": "Snowskates", "text": "Snowskates\n\nSnowskates are a type of snow sport equipment intended to allow the user to emulate the actions of ice skating or rollerblading on snow. They were first produced commercially in Germany in the 1930s.\n\nSnowskates consist of a pair of flexible ski boots with integrated bases resembling sled runners that are approximately the same length as the boot itself, incorporating steel edges to grip the snow.\nSled Dogs Snowskates is a winter sports company that is now owned and operated out of Oslo, Norway producing the current generation of snowskates. Since 2014, Sled Dogs Snowskates have produced a range of comfortable, lightweight snowskates for different user requirements (casual user to advanced user) Sled Dogs Snowskates combine the freedom and aggressive movements from inline skates and ice skates, with the freedom and challenge of slalom and snowboarding, without moving away from the key concept of 'Skates on Snow'.\n\n\"The similar term snowskating may also apply to a skateboard for use on snow (which is a shortened term of \"snowskateboarding\").\"\n\nSnowskates were invented in The Netherlands, and originally consisted of a simple wood runner that was tied to the users boots around 1865.\n\nThe earliest versions came in many varieties and they were mostly custom-made as opposed to commercially produced. The more well-known, commercially produced models came from Germany in the 1950s. The modern version of snowskates was invented by Swiss inventor Hanes Jacob in the 1970s.\n\nMultiple types of snowskates and accessories are available at the official website and at the official distributors in more than 30 countries around the World.\n\nIn 2017 there are distributors in Europe, Russia, Cyprus, Australia and New Zealand, China, South Korea, Canada, USA and in South America.\n\nThe available models are named after Norwegian dog breeds: The Halden is the entry-level model claiming 'all-round practicality and performance; the Lunde is a special women's fit with a fleece inner lining; the Hygen is the racing model. The K9.02 model is the top model designed for freestyle and professional level users.\n\nThe skates were featured in British Channel 4 TVs show \"The Jump\" where celebrities (Steve-O, Dean Cain, Heather Mills, Brian McFadden, Stacey Solomon etc.) were competing with SledDogs snowskates in series 2 in 2015 and in series 3 in 2016. Both series were watched by over 2,5 million viewers.\n\nThe first ever cross downhill competition using snowskates, called SledDogs Bonefight was held in Wagrain, Austria on the 20th of December, 2015. It was followed by 2 more races during the winter season of 2015–16. The next race was held in the biggest indoor ski slope of the World in Snowworld, Landgraaf, The Netherlands followed by the third race in Red Deer, Canada. The first ever SledDogs Bonefight World Champion was Luca Dallago who is also a top 10 athlete in RedBull Crashed Ice series.\n\nThe 2016/17 Bonefight tournament was held at slopes in Atholville and Nakiska in Canada and Wagrain in Austria. The World Champion in the women's tournament was jointly held by Karine Roy and Makara Martin, both from Canada, with Shaelyn Moltzahn, also from Canada, making up the top three. In the men's tournament, Luca Dallego from Austria held on to his world title for a second year, with Nathan Oostenbrink from Canada and Philipp Auerswald from Germany sharing second place.\n\nThe 2017/18 Bonefight tournament returned to Atholville and Wagrain and debuted at the Iceland Winter Games in Akureyri. Germany's Philipp Auerswald beat former title-holder Luca Dallago to the top spot in the men's event, with Canada's Samuel Nadeau taking third. Canada's Karine Roy took the No.1 spot for the women, followed by Iceland's Eva Maria Karvelsdottir and Finland's Miisa Klemola. The Freestyle Jump title was closely fought between Canada's Vincent Leblanc and Iceland's Isak Andri Bjarnason, with the title ultimately going to Bjarnason following a technical tie and an audience vote.\n\nIngi Freyr Sveinbjörnsson from Iceland is the most famous skater on SledDogs snowskates. He was special advisor/coach on UK TV series \"The Jump\" and he was the Top freestyle snowskater in the first international tournament in 2015. He has also been on television in South Korea. He also developed the following tricks on SledDogs snowskates: 360, 540, 720, frontflip, play dead, the twisted donut. , he held the fastest measured downhill speed record on SledDogs snowskates of . until April 2018, when Jari-Pekka Rahkonen of Finland broke the record with a speed of 115.27 km/h \n"}
{"id": "18912307", "url": "https://en.wikipedia.org/wiki?curid=18912307", "title": "Spoken Web", "text": "Spoken Web\n\nSpoken Web is a web portal, managing a wide range of online data-intensive content like Wikipedia articles, news updates, weather, travel and business articles for computer users who are blind or visually impaired. Spoken Web is based in Tel Aviv, Israel.\n\nSpoken Web was founded by Eyal Shalom in 2006.\n\nThe site provides a simple, easy-to-use interface for navigating between the different sections and articles. Using the keyboard to navigate, everybody can hear the full range of an article content provided in a logical, clear, and understandable manner.\n\nNavigation uses three keys: 'TAB' to pass from item to item, 'Enter' to select an item and 'Backspace' to go back to previous page.\n"}
{"id": "26125745", "url": "https://en.wikipedia.org/wiki?curid=26125745", "title": "Traveline", "text": "Traveline\n\nTraveline is a public transport route planner service provided by a partnership between local authorities and transport operators in the UK to provide impartial and comprehensive information about public transport which has operated since 2000. It prepares comprehensive public transport data for the UK and provides a number of regional public transport journey planners.\n\nFollowing on from a successful prototype one number travel service developed by Lancashire County Council in 1999, the national Traveline organisation was established in 2000.\n\nA prototype national door-to-door journey for Great Britain (i.e. UK without Northern Ireland) using the Lancashire Traveline model for regional journey planners was available for evaluation by 'stakeholders and key opinion formers' by November 2003 and the service was officially launched by Alistair Darling, the then Secretary of State for Transport on 31 December 2004.\n\nIn March 2010 Gordon Brown, the then Prime Minister of the United Kingdom announced that the NaPTAN dataset would be released as Open Data.\n\nThe Traveline regions assemble the public transport information within their areas and make it available through a number of local public transport journey planners. As of May 2014, all regions provide information for use within the Google Maps journey planner, and as of December 2016 all regions were added to the Apple Maps journey planner.\n\nDetails of all bus stops in the country are assembled into the National Public Transport Access Nodes database (NaPTAN) which is updated daily. The organisation also maintains the associated National Public Transport Gazetteer of all place names, both formal and informal, that may be used to indicate the destination for a requested journey. These datasets are provided to both Google Maps and OpenStreetMap. They have also been released as Open Data via data.gov.uk. \n\nPublic transport schedules are provided for use with the Transport Direct Portal on a weekly basis for use within the 'national journey planner for Great Britain'.\n\nOnce a year all the schedules for the county by all modes (including the information collected and maintained by Traveline) is assembled into the National Public Transport Data Repository which is prepared every October is used for the creation of Core Accessibility Indicators for every part of the UK. This data is also used within products such as MySociety's Mapumental.\n\nSome bus operators provide information via the Electronic Bus Service Registration (EBSR) system using the TransXChange when passing information between operators, Vehicle and Operator Services Agency (VOSA), Traveline.\n\nFor operational and data preparation purposes the UK is divided in the following regions/nations: The boundaries are not identical to the Regions of England.\n\nThe advisory body of the national organisation consists of the following organisations:-\n\n"}
{"id": "48788029", "url": "https://en.wikipedia.org/wiki?curid=48788029", "title": "Tuen Mun - Chek Lap Kok TBM", "text": "Tuen Mun - Chek Lap Kok TBM\n\nThe Tuen Mun - Chek Lap Kok TBM is the world's largest tunnel boring machine launched in June 2015 by Herrenknecht in Germany. The TBM is used to drill a 5 km tunnel connecting Tuen Mun to the Hong Kong International Airport, part of the Tuen Mun–Chek Lap Kok Link project. The machine has a diameter of 17.6 m, 0.2 m more than Bertha, the previous largest tunnel boring machine.\n\nThe machine will excavate a 5 km-long underwater tunnel, working at pressures as high as 5 bars. The drilling will take place in depths of up to 50 m below sea level.\n\n"}
{"id": "41790110", "url": "https://en.wikipedia.org/wiki?curid=41790110", "title": "Water-jet printer", "text": "Water-jet printer\n\nA water-jet printer (or waterjet printer) is a printer that makes use of paper coated with special dyes and ink cartridges filled with water to print paper copies of documents. Using paper treated with oxazolidine, the water jet changes the colour of the chemical to produce a print which fades in about a day, depending on temperature, and the paper can be re-used rather than being disposed of. The print fades away within about 22 hours at temperatures below 35 degrees Celsius (95 degrees Fahrenheit) as the water evaporates. While the chemical treatment makes the paper slightly more expensive, use of water instead of ink in the printer makes it much cheaper overall, and the only change needed to the printer is to replace what's in the cartridge.\n\nThe technology for water-jet printing was developed by a team of Chinese scientists led by Sean Xiao-An Zhang, a chemistry professor at Jilin University in China.\n\nTo create rewritable paper the researchers used four oxazolidines. Some of the isomers of these oxazolidines are colourless in the absence of water. But when the paper is wetted the water changes the dyes’ absorption of visible light. The exact wavelengths absorbed vary with the compound used, which allows printing in a variety of colours. The rewriteable paper was made by first coating ordinary paper with a layer of polyethylene glycol (PEG) to prevent it reacting with the dye, before a second layer of PEG containing the chosen dye was laid on top. Finally, another layer of PEG was added to prevent the dye absorbing water from the air or, conversely, losing water too quickly. The team used a commercial inkjet printer and a cartridge filled with water to print trial documents. The printed page is dry to the touch and the print can be rapidly ‘erased’ by heating it to 70 °C. The print remained legible for around 22 hours before evaporation wiped the page clean. Paper prepared in this way can be printed on and then erased more than 50 times.\n"}
{"id": "2620577", "url": "https://en.wikipedia.org/wiki?curid=2620577", "title": "Water Environment Federation", "text": "Water Environment Federation\n\nThe Water Environment Federation (WEF) is a not-for-profit technical and educational organization of more than 34,000 individual members and 75 Member Associations (MAs) representing water quality professionals around the world. WEF, which was formerly known as the Federation of Sewage Works Associations and later as the Water Pollution Control Federation, and its members have protected public health and the environment since 1928. As a global water sector leader, the organization's mission is to connect water professionals; enrich the expertise of water professionals; increase the awareness of the impact and value of water; and provide a platform for water sector innovation. WEF members include experts and specialists in the fields of:\nand related disciplines.\n\nWEF is headquartered in Alexandria, Virginia, United States.\n\n"}
{"id": "1780166", "url": "https://en.wikipedia.org/wiki?curid=1780166", "title": "WebCT", "text": "WebCT\n\nWebCT (Course Tools) or Blackboard Learning System, now owned by Blackboard, is an online proprietary virtual learning environment system that is licensed to colleges and other institutions and used in many campuses for e-learning. To their WebCT courses, instructors can add such tools as discussion boards, mail systems, and live chat, along with content including documents and web pages. The latest versions of this software are now called Webcourses. WebCT is significant in that it was the world's first widely successful course management system for higher education. At its height, it was in use by over 10 million students in 80 countries.\n\nWebCT was originally developed at the University of British Columbia by a faculty member in computer science, Murray Goldberg. Goldberg is also the creator of Silicon Chalk (sold to Wimba (website)) and Brainify (website) an academic social bookmarking and networking site. In 1995, Goldberg began looking at the application of web-based systems to education. His research showed that student satisfaction and academic performance could be improved through the use of a web-based educational resource, or web-based course tools (from which the name WebCT is derived).{Goldberg, M., Salari, S. & Swoboda, P. (1996) ‘World Wide Web – Course Tool: An Environment for Building WWW-Based Courses’ Computer Networks and ISDN Systems, 28:7-11 pp1219-1231} In order to continue his research, he decided to build a system to ease the creation of web-based learning environments. This led to the first version of WebCT in early 1996, first presented at the 5th international World Wide Web conference in Paris during the spring of 1996. \n\nIn 1997, Goldberg created a company, WebCT Educational Technologies Corporation, a spinoff company of UBC. Goldberg grew the company until 1999, at which point it served approximately 2-3 million students in 30 countries. In mid-1999, WebCT was acquired by ULT (Universal Learning Technology), a Boston-based company headed by Carol Vallone. Ms. Vallone continued to grow the company to the point where its product was used by over 10 million students in 80 countries. Goldberg resigned from his position of Canadian president of WebCT in 2002. In February 2006, WebCT was acquired by rival Blackboard Inc. As part of the acquisition terms with Blackboard, the WebCT name will be phased out in favor of the Blackboard brand. The majority of WebCT users moved away from Blackboard LMS. Many selected an open source LMS.\n\nThe software was used in electronic publishing. In order to use a textbook or other learning tool published in the WebCT format, some publishers require the student to purchase a password at the bookstore or to obtain it online. The software permitted integration of material prepared locally with material purchased from publishers.\n\nWebCT's user interface has been criticized as needlessly complex and unintuitive. The \"Vista\" version of the product represented an attempt to derive balance between flexibility and ease of use, however it has also been the target of ease-of-use criticisms.\n\nSome WebCT criticisms which were apparent include problems using it in multiple tabs or browser windows, heavy reliance on Java for its user experience, usage of too many browser framesets, issues with some features requiring pop-up blockers to be turned off, and problems using standard browser navigation tools (i.e. the Back and Forward commands).\n\nWebCT, like most of its competitors, does not meet all guidelines for accessibility; these include, but are not limited to, the following studies:\n\n\n"}
{"id": "42912386", "url": "https://en.wikipedia.org/wiki?curid=42912386", "title": "Zoë Quinn", "text": "Zoë Quinn\n\nZoë Tiberius Quinn (born 1987) is an American video game developer, programmer, writer, and artist. She developed the interactive fiction game \"Depression Quest\", which was released in 2013. In 2014, a blog post by her ex-boyfriend sparked the Gamergate controversy, in which Quinn was subjected to extensive harassment.\n\nZoë Tiberius Quinn was born in 1987 and grew up in a small town near the Adirondack Mountains in New York. Growing up, she often played video games. A favorite of hers was \"Commander Keen\", an MS-DOS game featuring an eight-year-old protagonist who builds a spaceship with items found around his house and then travels the Galaxy defending the Earth. As a teenager, she suffered from depression and was diagnosed with the condition at the age of 14. She has described receiving little sympathy or assistance from school district officials and says they were \"less than understanding about teens with depression and suicide issues\".\n\nAt the age of 24, Quinn moved to Canada and made her first foray into video game programming. Her first game was the result of a six-week course on video game creation that she attended after seeing an advertisement in a newspaper. In a later interview for \"The New Yorker\", she said, \"I felt like I'd found my calling.\"\n\nOne of Quinn's earliest creative works, \"Depression Quest\", was conceived as a \"choose-your-own path\" adventure detailing the troubled life of a person suffering from depression, with many of the \"correct\" paths blocked due to the protagonists' struggle with mental self-care. Quinn thought this sort of game narrative would be a good way to depict depression, imposing a set of rules on players they might not experience in their day-to-day lives. \"Depression Quest\" was released in February 2013.\n\nQuinn attempted to publish the game on Steam Greenlight service twice — in December 2013 and later in August 2014, when it was accepted and released by Steam. \"Depression Quest\" was featured in a \"Playboy\" article as one of several video games dealing with the subjective experience of depression.\n\nQuinn created the Game Developer Help List, designed to bring experienced game developers and novice developers into contact with one another. In 2014, she was intended to be part of the canceled YouTube reality television show codenamed \"Game_Jam\", which was meant to bring together a number of prominent indie game developers.\n\nIn 2015, she served as a narrative design consultant for Loveshack Entertainment's iOS game \"Framed\". As of 2014 she was also working on a full motion video game starring Greg Sestero.\n\nIn 2015, Quinn wrote a chapter for \"Videogames for Humans\", a book about games made using the Twine tool. She also contributed a chapter to the book \"The State of Play: Sixteen Voices on Video Games,\" detailing her experiences making \"Depression Quest\" and the subsequent harassment she faced. In 2015, she appeared in the documentary \"GTFO\". She also wrote a scenario for \"Widow's Walk\", an expansion for \"Betrayal at House on the Hill\", released in 2016.\n\nQuinn is currently working with erotica author Chuck Tingle on a full motion dating sim under the working title \"Project Tingler\". In January 2018, her role as Narrative Designer at Heart Machine on a new, unannounced project was also announced. In June 2018, it was announced that Quinn was working with Robbi Rodriguez on DC Vertigo's \"Goddess Mode\".\n\nShe has additionally worked on \"Fez\", \"Jazzpunk\", and \"They Bleed Pixels\".\n\nIn August 2014, Eron Gjoni, a former boyfriend of Quinn, posted a lengthy blog post detailing his relationship with Quinn. Based on the contents of the post, Quinn was falsely accused of receiving positive coverage from a journalist with whom she was in a relationship. It was later shown that the journalist in question had only once briefly mentioned Quinn's work, and not while they were in a relationship. These accusations sparked the Gamergate controversy. Quinn suffered a long period of harassment including doxing, rape threats, and death threats. Harassment associated with Gamergate resulted in widespread recognition of sexism in gaming.\n\nAccording to \"The New Yorker\", the harassment escalated to the point where Quinn, \"fearing for her safety, chose to leave her home\" and began working with the authorities to identify those responsible for the harassment. She detailed the experience in an interview on MSNBC's \"Ronan Farrow Daily,\" saying that Gamergate represented a rapidly shrinking fringe among an increasingly diverse gaming community and those attacking Quinn and other women in gaming needed \"to just grow up\". Speaking with BBC News, Quinn said the harassment had consumed her life, leading her to feel as if \"surrounded by nothing but hate — it's virulent, it's everywhere\" and that she was \"just trying to survive\". The attacks boiled down to \"the same accusation everybody makes toward every successful woman: she got to where she is because she had sex with someone\" and she also pointed out that Gamergate had targeted \"the people with the least power in the industry\". \"[I] used to go to games events and feel like I was going home... Now it's just like... are any of the people I'm currently in the room with, the ones that said they wanted to beat me to death?\" Quinn says her therapist remarked of the harassment, \"I don't even know what to tell you, this is so far outside anything I'm aware of.\"\n\nIn January 2015, Quinn co-founded Crash Override, a private network of experts to assist victims of online harassment which in March 2015 joined forces with Randi Harper's Online Abuse Prevention Initiative.\n\nOn September 24, 2015, she spoke at the United Nations along with Anita Sarkeesian about online harassment. In her speech, Quinn spoke about the need for technology companies to provide proper moderation and terms of service which protect marginalized groups. She also raised concerns about providing better protections for transgender women and victims of domestic violence on the Internet.\n\nIn September 2017, she published the memoir \"Crash Override: How Gamergate (Nearly) Destroyed My Life, and How We Can Win the Fight Against Online Hate\". The book has received generally positive reviews, with critics praising Quinn's thoughtful, nuanced portrayal of her harassers, but lamenting the book's \"scattered\" narrative flow. The book was nominated for the 2018 Hugo Award for Best Related Work (\"i.e.\", non-fiction work related to science fiction or fantasy).\n\nQuinn is interested in human enhancement, and has implanted an NTAG216 chip in the back of her hand that can be programmed to perform various functions. Her first use of the chip was to load it with the download code for the game \"Deus Ex.\" She also has a magnetic implant in her left ring finger.\n\n\n"}
