{"id": "30594626", "url": "https://en.wikipedia.org/wiki?curid=30594626", "title": "AN/ALE-47", "text": "AN/ALE-47\n\nThe AN/ALE-47 Airborne Countermeasures Dispenser System is used to protect military aircraft from incoming radar and infrared homing missiles. It works by dispensing flares or chaff. It is used on a variety of U.S. Air Force, Navy, and Army aircraft, as well as in other militaries.\n\nThe AN/ALE-47 countermeasure dispenser system was developed by Tracor, now part of BAE Systems, as an improved version of the older ALE-40 system, with more autonomy and software. The AN/ALE-47 countermeasure dispenser system is also manufactured by Symetrics Industries, LLC out of Melbourne, Florida. The AN/ALE-47 system can be integrated on a wide range of aircraft, including helicopters, cargo aircraft and fighters. It reached initial operational capability (IOC) in the U.S. Navy in 1998. It has been integrated on 38 different types of aircraft, including the F-16, F/A-18, C-17, CH-47 and UH-60. As of 2008, over 3000 sets have been delivered and the system is used by 30 different nations.\n\nThe ALE-47 is integrated with the aircraft's radar warning receivers, missile warning receivers and other electronic warfare sensors. When the aircraft's sensors detect a threat, the countermeasure dispenser system automatically launches radiofrequency and infrared countermeasures at the optimum time to defeat incoming missiles. The ALE is compatible with a wide variety of countermeasures such as different types of flares and chaff. It is also designed to work with advanced future countermeasures.\n\nThe ALE-47 consists of a cockpit control unit, sequencer units, countermeasure dispensers and an optional programmer. The cockpit control unit provides an interface with the pilot. A programmer can be added to add extra features, such as advanced threat evaluation. It can also be used to fully integrate the system with an aircraft's glass cockpit eliminating the need for the cockpit control unit. The sequencer units control the dispensers, and are automatically capable of detecting misfires and correcting them. The sequencers are built into the dispenser units on the rotary-wing version. Each dispenser can hold five different types of countermeasures for a total of 30. The whole system can accommodate up to 32 dispensers on fixed-wing aircraft and 16 on rotary-wing aircraft.\n\n\n"}
{"id": "26253855", "url": "https://en.wikipedia.org/wiki?curid=26253855", "title": "Alholmens Kraft Power Station", "text": "Alholmens Kraft Power Station\n\nThe Alholmens Kraft Power Station (also known as Jakobstad Power Station or Pietarsaari Power Station) is a biomass power station in Alholmen, Jakobstad in Ostrobothnia region, Finland. It is the largest biomass cogeneration power station in the world.\n\nThe power station was commissioned in 2001. It was built next to the existing, now decommissioned power station. The power station is located slightly north of UPM-Kymmene Wisaforest pulp and paper mill in Alholmen. The decommissioned power station had two black liquor recovery boilers, one oil fired package boiler, one bark boiler as well as several steam turbines with the electrical output of 75 MW.\n\nThe new power station was designed by Metso and its boiler was manufactured by Kværner. It employs 400 people. The power station has an installed capacity of 265 MW of electrical power. In addition, it provides 60 MW district heating for the city of Jakobstad and 100 MW process steam and heat for the UPM-Kymmene paper mill.\n\nThe new power station uses wood-based biofuels (forest residues) as the main fuel. Peat is also used while coal is a reserve fuel. It burns about 300,000 bales of forest residues per year. The power station is equipped with the circulating fluidized bed boiler with capacity of 500 MWth, which is the largest biomass-fired CFB boiler in the world. The operating temperature of the boiler is and the operational pressure is .\n\n"}
{"id": "35566863", "url": "https://en.wikipedia.org/wiki?curid=35566863", "title": "Angle–sensitive pixel", "text": "Angle–sensitive pixel\n\nAn angle-sensitive pixel (ASP) is a light sensor made entirely in CMOS with a sensitivity to incoming light that is sinusoidal in incident angle.\n\nASPs are typically composed of two gratings (a diffraction grating and an analyzer grating) above a single photodiode. ASPs exploit the moire effect and the Talbot effect to gain their sinusoidal light sensitivity. According to the moire effect, if light acted as a particle, at certain incident angles the gaps in the diffraction and analyzer gratings line up, while at other incident angles light passed by the diffraction grating is blocked by the analyzer grating. The amount of light reaching the photodiode would be proportional to a sinusoidal function of incident angle, as the two gratings come in and out of phase with each other with shifting incident angle. The wave nature of light becomes important at small scales such as those in ASPs, meaning a pure-moire model of ASP function is insufficient. However, at half-integer multiples of the Talbot depth, the periodicity of the diffraction grating is recapitulated, and the moire effect is rescued. By building ASPs where the vertical separation between the gratings is approximately equal to a half-integer multiple of the Talbot depth, the sinusoidal sensitivity with incident angle is observed.\n\nASPs can be used in miniature imaging devices. They do not require any focusing elements to achieve sinusoidal incident angle sensitivity, meaning that they can be deployed without a lens to image the near field, or the far field using a Fourier-complete planar Fourier capture array. They can also be used in conjunction with a lens, in which case they perform a depth-sensitive, physics-based wavelet transform of the far-away scene, allowing single-lens 3D photography similar to that of the Lytro camera.\n\n"}
{"id": "9711622", "url": "https://en.wikipedia.org/wiki?curid=9711622", "title": "BBC Jam", "text": "BBC Jam\n\nBBC Jam (formerly known as BBC Digital Curriculum) was an online educational service operated by the BBC from January 2006 to 20 March 2007. The service was available free across the United Kingdom offering multi-media educational resources. Jam was the BBC's provision for the Digital Curriculum, an initiative launched by the British Government to provide computer-based learning in UK schools, and had a budget of £150 million. The service was shut down due to a legal challenge concerning fair trading by the BBC.\n\nThe content of the service was connected with the National Curriculum for schools in England, Wales, Scotland and Northern Ireland. It covered school subjects such as maths, science, literacy, geography, business studies and languages, and was designed to provide free, independent computer-based learning for school children.\n\nThe service was required to support users with disability by incorporating accessibility features such as audible text, subtitles on videos etc. There were also subjects which were translated into Welsh, Scottish Gaelic and Irish.\n\nBBC Jam was commissioned in 2003 by the Secretary of State for Culture, Media and Sport, and designed:-\n\nIn consultation with BECTA, the Government's educational technology department, the service was allowed to cover no more than 50% of the learning outcomes that are amendable to ICT.\n\nThe service was suspended on 20 March 2007 at the request of the BBC Trust, in response to complaints made to the European Commission by a number of commercial producers of interactive educational products who felt that the BBC was exceeding its public service remit by offering free content to schools which could be provided commercially. This resulted in departmental restructuring and a number of job losses in the BBC. The Trust requested that the BBC management prepare a fresh proposal, including how the BBC should deliver its Charter obligation — promoting formal education and learning, whilst meeting the online needs of school age children.\n\nThe new proposal was subjected to a full Public Value Test by the Trust and a market impact assessment by Ofcom, the United Kingdom's telecoms regulator.\n\nIn February 2008 it was announced that the BBC's digital curriculum project would finally be closed.\n\n"}
{"id": "2868014", "url": "https://en.wikipedia.org/wiki?curid=2868014", "title": "Bag tag", "text": "Bag tag\n\nBag tags, also known as baggage tags, baggage checks or luggage tickets, have traditionally been used by bus, train, and airline carriers to route checked luggage to its final destination. The passenger stub is typically handed to the passenger or attached to the ticket envelope:\n\na) to aid the passenger in identifying their bag among similar bags at the destination baggage carousel;\n\nb) as proof—still requested at a few airports—that the passenger is not removing someone else's bag from the baggage reclaim hall; and\n\nc) as a means for the passenger and carrier to identify and trace a specific bag that has gone astray and was not delivered at the destination. The carriers' liability is restricted to published tariffs and international agreements.\n\nThe first \"separable coupon ticket\" was patented by John Michael Lyons of Moncton, New Brunswick on June 5, 1882. The ticket showed the issuing station, the destination, and a consecutive number for reference. The lower half of the ticket was given to the passenger, while the upper half, with a hole at the top, was inserted into a brass sleeve and then attached to the baggage by a strap. \n\nAt some point, reinforced paper tags were introduced. These are designed not to detach as easily as older tags during transport.\n\nThe Warsaw Convention of 1929, specifically Article Four, established the criteria for issuing a \"baggage check\" or \"luggage ticket\". This agreement also established limit of liability on checked baggage.\n\nPrior to the 1990s, airline bag tags consisted of a paper tag attached with a string.\n\nThe tag contained basic information:\n\n\nThese tags became obsolete because they offered little security and were easy to replicate.\n\nCurrent bag tags include a bar code using the Interleaved 2 of 5 symbology. These bag tags are printed using a thermal or barcode printer on an adhesive thermal paper stock. This printed strip is then attached to the luggage at check-in, allowing automated sorting of the bags by bar code readers.\n\nThere are two ways that bar code baggage tags are read: hand held scanners, and in-line arrays. In-line arrays are built into the baggage conveyor system and use a 360-degree array of lasers to read the bar code tags from multiple angles because baggage and the orientation of the bar code tag can shift as the bag travels through the conveyor belt system.\n\nOne of the limitations of this system is that in order to read bar codes from the bottom of the belt, laser arrays are placed below the gap between two sections of conveyor belt. Due to the frequent build-up of debris and dust on these lower arrays, the rate of successful reads can be low.\n\nFrequently, the \"read rate\", the percentage of bar code tags successfully read by these arrays, can be as low as 85%. This means that more than one out of ten bar code baggage tags are not successfully read, and these bags are shunted off for manual reading, resulting in extra labor and delay.\n\nFor flights departing from an international airport within the European Union, bag tags are issued with green edges. Passengers are eligible to take these bags through a separate \"Blue Channel\" at Customs if arriving at another EU airport.\n\nBar codes cannot be automatically scanned without direct sight and undamaged print. Because of reading problems with poorly printed, obscured, crumpled, scored or otherwise damaged bar codes, some airlines have started using radio-frequency identification (RFID) chips embedded in the tags.\n\nIn the US, McCarran International Airport has installed an RFID system throughout the airport. Hong Kong International Airport has also installed an RFID system. The International Air Transport Association (IATA) is working to standardize RFID bag tags.\n\nBritish Airways is currently conducting a trial to test re-usable electronic luggage tags featuring electronic paper technology. The passenger checks in using the British Airways smartphone app, then holds the smartphone close to the tag. The flight details and barcode are transmitted to the tag using NFC technology. Because the tag utilises electronic paper, the battery need only power the tag during the transmission of data.\n\nFast Travel Global Ltd has developed a re-usable electronic luggage tag product called the eTag. This is also electronic paper-based but is not limited to a single airline. The passenger will check in using a supported airline's smartphone app and send the relevant flight information to the tag via Bluetooth Low Energy.\n\nQantas introduced Q Bag Tags in 2011. Unlike the British Airways tags, they do not feature a screen, which means there is no barcode to scan. This has limited the use of the tags to domestic flights within Australia on the Qantas network. The tags were initially given free of charge to members of the Qantas Frequent Flyer program with Silver, Gold or Platinum status. The tags can also be purchased for A$29.95.\n\nOver the last years, there have been numerous of initiatives to develop electronic bag tags, by both independent technology companies as well as some airlines. The main benefits of electronic bag tags include self-control and ease-of-use by passengers, time-saving by skipping queues at the airport, improved read rates compared to printed bag tags and, as electronic bag tags are adopted, significant operational cost reduction for the airlines.\n\nThe first company to successfully launch has been Rimowa in a partnership with Lufthansa in March, 2016. The concept of electronic bag tags has been gaining ground following that launch. On January 9, 2018, Lufthansa introduced a new electronic bag tag to their passengers, BAGTAG. BAGTAG is the first fully secure operational electronic bag tag that can be attached to any suitcase and has integrated radio-frequency identification technology. \n\nThe first automated baggage sorting systems were developed in the 1980s by Eastern Air Lines at their Miami International Airport hub. Other airlines soon followed with their own systems, including United Air Lines, TWA, Delta, and American Airlines. None of these systems were interchangeable. In some systems, the bar code was used to represent a three-letter destination airport code, and in others it was a two-digit sorting symbol instructing the system at which pier to deliver the bag.\n\nAs a result of the bombing of Air India Flight 182 on June 23, 1985, the airline industry, led by IATA, convened the Baggage Security Working Group (BSWG) to change international standards and require passenger baggage reconciliation. The Chairman of the BSWG, John Vermilye of Eastern Airlines, proposed that the industry adopt the already-proven license plate system.\n\nThis concept used a barcode to represent the baggage tag number. At check-in, this number was associated with the passenger details, including flight number, destination, connection information, and even class of service to indicate priority handling.\n\nWorking with Allen Davidson of Litton Industries, with whom Eastern had developed the license plate concept, the BSWG adopted this system as the common industry standard for passenger baggage reconciliation. Initially the barcode, or license plate, was used to match baggage with passengers, ensuring that only the baggage of passengers who had actually boarded the flight were carried onto the aircraft. This standard was adopted by IATA Resolution in 1987.\n\nBy 1989, the license plate concept was expanded to become the industry standard for automated baggage sorting as well. The barcodes were enlarged to facilitate automated reading. The barcode was shown in two different orientations or in a \"T\" shape, called the \"orthogonal\" representation.\n\nThe term \"license plate\" is the official term used by the IATA, the airlines, and the airports for the ten-digit numeric code on a bag tag issued by a carrier or handling agent at check-in. The license plate is printed on the carrier tag in barcode form and in human-readable form (as defined in Resolution 740 in the \"IATA Passenger Services Conference Resolutions Manual\", published annually by IATA).\n\nThe license plate is the index number linking the Baggage Source Message (BSM), sent by a carrier's departure control system, to the airport's baggage handling system. This message (BSM) contains the flight details and passenger information. Each digit in the license plate has a specific meaning. The automated baggage handling system scans the barcodes on the carrier tags and sorts the bags accordingly. Both the license plate number and the BSM are essential for automated sorting of baggage.\n\nThe human-readable license plate will have either a two-character or a three-digit IATA carrier code. For example, it may be either \"BA728359\" or \"0125728359.\" \"BA\" would be the two-character IATA code for British Airways), and \"125\" would be the three-digit IATA carrier code. Nevertheless, the barcode will always be the full ten digits.\n\nThe first digit in the ten-digit license plate is not part of the carrier code. It can be in the range of zero to nine.\n\nZero is for interline or online tags, one is for fallback tags, and two is for \"rush\" tags. \n\nFallback tags are pre-printed or demand-printed tags for use only by the airport's baggage handling system. These tags are used when there is a problem in communication between the carrier's departure control system and the airport's baggage handling system (as defined in IATA Recommended Practice 1740b). \n\nA \"rush\" bag is a bag that missed its original flight and is now flying unaccompanied.\n\nThe purpose of numbers in the range of three to nine is not defined by the IATA, but they can be used by each carrier for their own specific needs. The first digit is commonly used as a million indicator for the normal six-digit tag number.\n\nBesides the license plate number, the tag also has:\n\n"}
{"id": "18084359", "url": "https://en.wikipedia.org/wiki?curid=18084359", "title": "Bell nipple", "text": "Bell nipple\n\nA Bell nipple is a section of large diameter pipe fitted to the top of the blowout preventers that the flow line attaches to via a side outlet, to allow the drilling fluid to flow back over the shale shakers to the mud tanks.\n\nSee Drilling rig (petroleum) for a diagram.\n"}
{"id": "8420153", "url": "https://en.wikipedia.org/wiki?curid=8420153", "title": "Biohydrometallurgy", "text": "Biohydrometallurgy\n\nBiohydrometallurgy is a subfield within hydrometallurgy which includes aspects of biotechnology.\n\n\nBiohydrometallurgy is used to perform processes involving metals, for example, microbial mining, oil recovery, bioleaching, water-treatment and others. Biohydrometallurgy is mainly used to recover certain metals from sulfide ores. It is usually utilized when conventional mining procedures are too expensive or ineffective in recovering a metal such as copper, cobalt, gold, lead, nickel, uranium and zinc.\n\n\n"}
{"id": "3324309", "url": "https://en.wikipedia.org/wiki?curid=3324309", "title": "Board support package", "text": "Board support package\n\nIn embedded systems, a board support package (BSP) is the layer of software containing hardware-specific drivers and other routines that allow a particular operating system (traditionally a real-time operating system, or RTOS) to function in a particular hardware environment (a computer or CPU card), integrated with the RTOS itself. Third-party hardware developers who wish to support a particular RTOS must create a BSP that allows that RTOS to run on their platform. In most cases the RTOS image and license, the BSP containing it, and the hardware are bundled together by the hardware vendor. \n\nBSPs are typically customizable, allowing the user to specify which drivers and routines should be included in the build based on their selection of hardware and software options. For instance, a particular single-board computer might be paired with any of several graphics cards; in that case the BSP might include a driver for each graphics card supported; when building the BSP image the user would specify which graphics driver to include based in his choice of hardware.\n\nSome suppliers also provide a root file system, a toolchain for building programs to run on the embedded system, and utilities to configure the device (while running) along with the BSP.\nMany RTOS providers provide template BSP's, developer assistance, and test suites to aid BSP developers in bringing up the RTOS on a new hardware platform. \n\nThe term \"BSP\" has been in use since 1981 when Hunter & Ready, the developers of the VRTX, first coined the term to describe the hardware-dependent software needed to run VRTX on a specific hardware platform. Since the 1980s it has been in wide use throughout the industry. Virtually all RTOS providers now use the term BSP. \n\nThe Wind River board support package for the ARM Integrator 920T single-board computer contains, among other things, the following elements:\n\nAdditionally the BSP is supposed to perform the following operations\n"}
{"id": "41716110", "url": "https://en.wikipedia.org/wiki?curid=41716110", "title": "Byzantine beacon system", "text": "Byzantine beacon system\n\nIn the 9th century, during the Arab–Byzantine wars, the Byzantine Empire used a system of beacons to transmit messages from the border with the Abbasid Caliphate across Asia Minor to the Byzantine capital, Constantinople.\n\nAccording to the Byzantine sources (Constantine Porphyrogenitus, Theophanes Continuatus and Symeon Magister), the line of beacons began with the fortress of Loulon, on the northern exit of the Cilician Gates, and continued with Mt. Argaios (identified mostly with Keçikalesı on Hasan Dağı, but also with Erciyes Dağı near Caesarea), Mt. Samos or Isamos (unidentified, probably north of Lake Tatta), the fortress of Aigilon (unidentified, probably south of Dorylaion), Mt. Mamas (unidentified, Constantine Porphyrogenitus has Mysian Olympus instead), Mt. Kyrizos (somewhere between Lake Ascania and the Gulf of Kios, possibly Katerlı Dağı according to W. M. Ramsay), Mt. Mokilos above Pylae on the southern shore of the Gulf of Nicomedia (identified by Ramsay with Samanlı Dağı), Mt. Saint Auxentius south-east of Chalcedon (modern Kayışdağı) and the lighthouse (\"Pharos\") of the Great Palace in Constantinople. This main line was complemented by secondary branches that transmitted the messages to other locations, as well as along the frontier itself.\n\nThe main line of beacons stretched over some . In the open spaces of central Asia Minor, the stations were placed over apart, while in Bithynia, with its more broken terrain, the intervals were reduced to ca. . Based on modern experiments, a message could be transmitted the entire length of the line within an hour. The system was reportedly devised in the reign of Emperor Theophilos (ruled 829–842) by Leo the Mathematician, and functioned through two identical water clocks placed at the two terminal stations, Loulon and the Lighthouse. Different messages were assigned to each of twelve hours, so that the lighting of a bonfire on the first beacon on a particular hour signalled a specific event and was transmitted down the line to Constantinople.\n\nAccording to some of the Byzantine chroniclers, the system was disbanded by Theophilos' son and successor, Michael III (r. 842–867) because the sight of the lit beacons and the news of an Arab invasion threatened to distract the people and spoil his performance as one of the charioteers in the Hippodrome races. This tale is usually dismissed by modern scholars as part of a deliberate propaganda campaign by 10th-century sources keen to blacken Michael's image in favour of the succeeding Macedonian dynasty. If indeed there is some element of truth in this report, it may reflect a cutting-back or modification of the system, perhaps due to the receding of the Arab danger during Michael III's reign. The surviving portions of the system or a new but similar one seem to have been reactivated under Manuel I Komnenos (r. 1143–1180).\n\n\n"}
{"id": "26527371", "url": "https://en.wikipedia.org/wiki?curid=26527371", "title": "Cathy Hudgins", "text": "Cathy Hudgins\n\nCatherine M. \"Cathy\" Hudgins (born 1944) is a Democratic member of the Fairfax County Board of Supervisors; representing the Hunter Mill district, which includes the town of Reston. In 1984 she was Virginia's National Committeewoman on the Democratic National Committee; as well as the leader of Virginia's delegation to the 1988 Democratic National Convention.\n\nShe worked twelve years in the private sector for AT&T as a programmer, consultant, and analyst.\n\nHudgins was elected to the Board of Supervisors in November 1999, defeating incumbent Republican supervisor Robert B. Dix, Jr.\n\nIn April 2013, Hudgins was diagnosed with noninvasive breast cancer.\n\n"}
{"id": "1628008", "url": "https://en.wikipedia.org/wiki?curid=1628008", "title": "Chartered Semiconductor Manufacturing", "text": "Chartered Semiconductor Manufacturing\n\nChartered Semiconductor was created in 1987, as a venture that included Singapore Technologies Engineering Ltd. Yet, it was not until 2000, that ST Engineering (Singapore Technologies Semiconductors), a wholly owned subsidiary of Temasek Holdings wholly acquired Chartered.\n\nPrior to 2010, Chartered Semiconductor Manufacturing (abbreviated CSM) was the world's third largest dedicated independent semiconductor foundry, with its headquarters and main operations located in the Woodlands Industrial Park, Kranji Singapore. The company was listed on the Singapore Exchange under the trading symbol of CHARTERED, as well as on NASDAQ (CHRT), until its buyout.\n\nChartered provides comprehensive wafer fabrication services and technologies to semiconductor suppliers and systems companies. Chartered's customer base is primarily high-growth, technologically advanced companies operating in the communication, computer and consumer sectors. It does not provide design services and works from customers' designs to produce communications chips.\n\nBesides its own fabs, Chartered operates joint venture facilities with other firms, it offers chip assembly and test services through sister firm STATS ChipPAC. Chartered owns 6 fabrication facilities, all of which are located in Singapore, including the newest, Chartered's first 300-mm facility which started commercial shipment in June 2005.\n\nThe other major semiconductor foundries include TSMC and UMC, Taiwanese-based companies, which are primarily Chartered's main competitors.\n\nIn 2006, AMD announced that it will manufacture CPUs with Chartered on a 65 nanometer process. It also has alliances with IBM, Infineon, Samsung and Agere Systems.\n\nIn September 2009, it was announced that Chartered Semiconductor was about to be acquired by the main stockholder of GlobalFoundries, a joint venture between AMD and Advanced Technology Investment Company (ATIC), of Abu Dhabi, United Arab Emirates.\n\nBy acquiring Chartered, ATIC is expanding its investments and expertise in technology in the semiconductor industry which currently consist of a GLOBALFOUNDRIES facility in Dresden, Germany. The transaction was completed at the end of 2009, making ATIC the sole owner of Chartered.\n\n"}
{"id": "3963759", "url": "https://en.wikipedia.org/wiki?curid=3963759", "title": "Chen Chunxian", "text": "Chen Chunxian\n\nChen Chunxian (; 1934 – 11 August 2004) was the founder of the Silicon Valley of China, also known as Zhongguancun.\n\nChen Chunxian was born in 1934 in Sichuan Province, China. In 1958, he graduated from Moscow University, Department of Physics. \n\nFrom 1959 to 1986, he was a member of the Chinese Academy of Science, Institute of Physics.\n\nAfter living for many years in a poorly maintained apartment and with no health care, Chen Chunxian died on 11 August 2004.\n\nChen created the first tokamak in China.\n\nAs a result of Richard Nixon's efforts and Mao Zedong's agreement, China and the United States engaged in exchanging friendship greetings. On one trip, in 1979, Chen was invited to visit the United States. On this trip, Chen saw Route 128 and Silicon Valley. They both impressed him greatly.\n\nUpon return, on October 23, 1980, Chen Chunxian founded the first non-governmental entity in Zhongguancun, called the \"Advanced Technology Service Association\" because only government-run entities could be called \"company\".\n\nAlthough Chen's company was shut down after investigations, he received validation from the central government in 1983, when Hu Yaobang mentioned him in a national statement. Zhongguancun, and the independent enterprises run-by-individuals, emerged in China.\n\nAlthough Chen was unable to achieve success with his company, Huaxia Guigu, founded in 1983, he paved the road for future entrepreneurs, most notably Liu Chuanzhi and his company Lenovo.\n"}
{"id": "11231800", "url": "https://en.wikipedia.org/wiki?curid=11231800", "title": "Cisco TelePresence", "text": "Cisco TelePresence\n\nCisco TelePresence, first introduced in October 2006, is a range of products developed by Cisco Systems designed to link two physically separated rooms so they resemble a single conference room regardless of location.\n\nCisco documented the Telepresence concept and implementation details in the book Cisco TelePresence Fundamentals, where the difference between Telepresence and the at this point in time prevalent Videoconferencing is defined as quality, simplicity, and reliability.\nThese were the initial products:\n\n\nThey were designed for an experience to feel as if local and remote participants are in the same room. These products offer features including up to three 1080p flat panel displays, special tables, microphones, speakers, cameras, collaboration interfaces and lighting.\n\nIn 2008 Cisco reported to have sold about 2,000 rooms, with about another 250 non-revenue (internal and philanthropic) units installed.\n\nLater other products were developed that expanded the use-cases for smaller offices and Webex connectivity.\n\nIn 2010 Cisco acquired the Norwegian company Tandberg and integrated their products into the Cisco portfolio.\n\nCurrently there is a wide range of collaboration endpoints and conferencing infrastructure products offered.\n\n\n"}
{"id": "40406762", "url": "https://en.wikipedia.org/wiki?curid=40406762", "title": "Combustion and Flame", "text": "Combustion and Flame\n\nCombustion and Flame is a monthly peer-reviewed scientific journal published by Elsevier on behalf of the Combustion Institute. It covers fundamental research on combustion science. The editors-in-chief are Fokion Egolfopoulos (University of Southern California) and Thierry Poinsot (Centre National de la Recherche Scientifique).\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 4.494.\n"}
{"id": "31347293", "url": "https://en.wikipedia.org/wiki?curid=31347293", "title": "Commercial Telegraphers Union of America", "text": "Commercial Telegraphers Union of America\n\nThe Commercial Telegraphers Union of America (CTUA) was a United States labor union formed to promote the interests of commercial telegraph operators.\nThe first practical telegraph system in the United States was put into operation by Samuel F. B. Morse and Alfred Vail between Baltimore, Maryland and Washington, DC, in 1844. By 1846, telegraph lines extended along the entire eastern seaboard and were rapidly being built westward into the interior of the country. \n\nEarly uses of the telegraph included sending press reports, commodities prices, and business transactions. As time went on, the telegraph was increasingly used by the general public for sending personal messages. During the American Civil War, the telegraph was used extensively by the Union Army for military intelligence purposes.\n\nDuring the American Civil War, telegraph operators in the North organized the first telegraphers' association, the National Telegraphic Union (NTU), in 1863. The NTU saw itself primarily as a mutual benefit organization that sought to improve professional standards and provide members with benefits in the event of death, retirement, or sickness. The NTU avoided taking a stand on controversial issues, such as the admission of women as members, or the right to strike to obtain higher pay and better working conditions; as telegraphers became dissatisfied with pay rates and working conditions in the late 1860s, they abandoned the NTU for more militant organizations and the NTU itself gradually faded away.\n\nThe Telegraphers' Protective League (TPL), founded in 1868, took a more activist stance in demanding pay increases and better working conditions for its members. Its primary goal was to organize the telegraph operators who worked for Western Union, by then the largest telegraph company, and, after its absorption of its two rivals, the American Telegraph Company and the United States Telegraph Company, in 1866, a near monopoly. The TPL called a strike in January 1870 after Western Union attempted to cut the wages of four operators in San Francisco; however, the strike was unsuccessful and was called off after only two weeks when the telegraph company simply hired non-union operators to take the place of the strikers.\n\nThe next major labor action by telegraphers took place in July 1883, when the Brotherhood of Telegraphers, a union affiliated with the Knights of Labor, called a nationwide strike against the telegraph companies. The strikers demanded an eight-hour workday, wage increases, and equal pay for women. About 8000 telegraphers, about one-third of all the operators in the U.S., joined the strike. Like the earlier strike, the 1883 strike failed to achieve its objectives, and was called off after about a month. The Brotherhood of Telegraphers gradually faded away, ceasing to exist around 1890.\n\nTelegraph operators who worked for the railroads began to see themselves as occupationally distinct from the commercial operators in the late nineteenth century. They organized their own union, the Order of Railroad Telegraphers, in 1886.\n\nAround 1900, several new labor organizations were formed by the commercial operators, including the Brotherhood of Commercial Telegraphers, the International Union of Commercial Telegraphers, and the Order of Commercial Telegraphers. Members of these groups met in Pittsburgh, Pennsylvania, on December 5, 1902, to discuss their common interests. At this meeting, a new organization, the Commercial Telegraphers Union, was founded; in a following meeting, held on March 17, 1903, the Commercial Telegraphers Union merged with the Order of Commercial Telegraphers to form the Commercial Telegraphers Union of America. Percy Thomas and I.J. McDonald were elected associate presidents of the new organization, which had approximately 8000 members in 60 locals, in both the U.S. and Canada, at its inception. Goals of the union included equal wages for equal work for both men and women, modifications to the \"bonus\" system that paid operators on a per-message basis, and better working conditions for operators in branch offices. The CTUA joined the American Federation of Labor in July 1903.\n\nWestern Union refused to recognize the CTUA, and threatened to discharge employees who joined the union. The CTUA filed a request in federal court in St. Louis in 1903 for an injunction against Western Union to prevent the company from discriminating against union members; however, the request was turned down. In an attempt to head off the increasing unionization of its employees, Western Union granted a ten percent wage increase to all telegraphers in March 1907. Western Union's principal competitor, the Postal Telegraph Company, also granted its operators a fifteen percent wage increase at this time.\n\nBy 1907, discontent was widespread among telegraphers. As in earlier strikes, the major issues were low pay, long working hours, and poor working conditions, as well as the telegraph companies' use of the \"sliding scale,\" in which an operator who accepted a promotion or new position would be given a lower salary than the person who previously held that position. Another issue was the cost of providing a typewriter; as the use of the typewriter to copy messages became commonplace during the 1890s, employers required telegraphers to purchase their own machine.\n\nIn May 1907, the CTUA presented Western Union with a \"Bill of Grievances,\" alleging that the company had failed to grant the promised ten percent raise, and that the company was singling out union members for harassment. However, the telegraph company largely ignored the bill of grievances and continued to discharge employees for union activities. \n\nTelegraphers in San Francisco went on strike in June 1907 after Western Union rejected their demand for a 25% increase in salary, in part to compensate them for increased living expenses brought about by the 1906 San Francisco earthquake. U.S. Labor Bureau Commissioner Charles P. Neill negotiated a settlement with Western Union and the CTUA in July in which Western Union agreed to re-employ the striking workers and discuss wage increases. Although the strike appeared to be nearing an end, it suddenly erupted into a nationwide walkout when Western Union fired a union operator who had refused to work with a nonunion woman operator hired during the strike to replace one of the strikers. Although caught by surprise by the spontaneous strike, CTUA president Samuel J. Small authorized the action on August 15, 1907. Between 10,000 and 15,000 operators joined the strike as it spread across the entire U.S.\n\nThe CTUA had admitted women as members since its inception, and by 1907 probably 25% of CTUA members were women. Some of the issues affecting the women strikers included equal pay for equal work, improvements in working conditions for female employees, and an end to sexual harassment on the job. The telegraphers' strike came to the attention of the Women's Trade Union League and its president, Margaret Dreier Robins. Some of the WTUL's leaders, including Rose Pastor Stokes, Rose Schneiderman, and Harriot Eaton Stanton Blatch addressed meetings of the striking telegraphers; they spoke in support of the CTUA's demand for equal pay for equal work, and emphasized the importance of getting the vote for women.\n\nSeveral women strike leaders gained visibility during the strike, including Louise Forcey, who led the strike of the Postal Telegraph workers in Chicago, and Mary Macaulay, who would later become International Vice President of the CTUA. Ola Delight Smith, who was blacklisted by Western Union for her role in the strike, later became a journalist and labor organizer for the CTUA.\n\nAlthough the strike disrupted telegraphic communication across the entire U.S. and Canada, Western Union was able to continue operating on a limited basis; as time went on, operators deserted the strike or were replaced by strikebreakers. Finally, on November 9, 1907, the CTUA called off the strike without having achieved any of its stated goals. The CTUA executive board blamed president Samuel J. Small for the strike failure; he was replaced by Sylvester J. Konenkamp in 1908.\nWorking conditions for telegraphers improved briefly after the American Telephone and Telegraph Company acquired control of Western Union in 1909; salaries were adjusted, a pension plan was established, and many offices were modernized. However, after the two companies were once again separated in 1913 to prevent prosecution under the Sherman Anti-Trust Act, relations between the union and the telegraph companies once again became confrontational.\n\nAfter the entry of the U.S. into World War I, the National War Labor Board was established to govern relations between workers and employers during the war. The NWLB requested that workers not strike for the duration of the law, but also requested that companies not interfere with the right of workers to join unions. As the NWLB had no power of enforcement, it relied upon voluntary cooperation to settle disputes.\n\nIn April 1918, telegraphers in Seattle, Washington, were discharged by Western Union and the Postal Telegraph Company for wearing union ribbons to work. The CTUA protested the action to the NWLB, which requested the companies to take back the discharged workers in June. While the Postal Telegraph agreed to reinstate the telegraphers, Western Union refused to rehire them. The CTUA then ordered a nationwide strike to begin in July. In order to avoid a crippling strike during wartime, President Woodrow Wilson ordered the telegraph industry to be put under government control under the direction of Postmaster General Albert S. Burleson. \n\nTelegraphers generally saw Burleson as being more in sympathy with the telegraph companies than with the union. In June 1919, female telephone operators in Atlanta, who had recently been organized by the CTUA, were discharged by Southern Bell Telephone Company and Western Union for union activities. When Burleson failed to support the union operators, the CTUA called a nationwide strike on June 11, 1919.\n\nThe strike was largely ineffectual, as the CTUA's membership had declined to about 3500. However, reflecting the increased involvement and militancy of its female members, about thirty percent of the pickets were women. In Oklahoma City, Oklahoma, three female pickets were arrested for \"coercing and intimidating\" Western Union employees. The strike was finally called off after less than a month.\n\nS. J. Konenkamp resigned as CTUA president in July 1919; he was replaced by a new slate of officers that included Mary J. Macaulay as international vice president. She was the first female telegrapher to be elected to a national office in a union. One of her first accomplishments was to set up a defense fund to aid the strikers arrested in Oklahoma City; eventually charges against them were dropped. \nAfter the failed strike of 1919, the CTUA entered a long period of decline. Membership remained constant at about 2000 members throughout the 1920s and into the depression years as Morse operators were replaced by Teletype operators. Many Western Union employees left the CTUA to join the Association of Western Union Employees (AWUE), a company union which had been established by Western Union in 1918.\n\nA rival union, the American Communications Association (ACA) was organized by the Congress of Industrial Organizations (CIO) in 1937. By 1939, the ACA had succeeded in negotiating a \"closed shop\" agreement with the Postal Telegraph Company, effectively shutting out the CTUA. In the same year, the National Labor Relations Board found that Western Union had been guilty of unfair labor practices in establishing the AWUE. The NLRB required the company to disenfranchise the AWUE and hold elections in each local district to determine which union should represent telegraph workers. The CTUA went on the offensive, emphasizing its \"American\" roots and alleging that the ACA was infiltrated by \"Communists.\" The CTUA won the majority of the district elections in 1940 and 1941.\n\nIn 1943, the Postal Telegraph Company was acquired by and merged into Western Union. To establish union representation, the NLRB ordered that new elections be held in each of the seven national divisions of Western Union. National membership at the time was divided nearly evenly, with the ACA having 18,353 members and the newly invigorated CTUA 20,000 members. The CTUA was victorious in every division except the Metropolitan, representing New York City, which remained a stronghold of the ACA until about 1950.\n\nLike many unions of the era, the CTUA practiced racial discrimination and opposed admitting large numbers of immigrants to the U.S. The original CTUA constitution contained a \"whites only\" clause that was not removed until after World War II; an attempt by Canadian members to remove the restriction in 1916 was defeated when they were accused of \"fomenting socialism.\" Theophilus Eugene \"Bull\" Connor (1897-1973), Alabama politician and notorious segregationist, was a telegraph operator and CTUA member before entering politics.\n\nThe CTUA continued to represent a shrinking membership in the 1960s as the telegraph business declined. In 1968, the Commercial Telegraphers Union of America officially changed its name to the United Telegraph Workers. The United Telegraph Workers merged with the Communications Workers of America in 1986. \n\n"}
{"id": "11158526", "url": "https://en.wikipedia.org/wiki?curid=11158526", "title": "Commodity cell", "text": "Commodity cell\n\nA commodity cell is a type of battery made in large volumes for use by original equipment manufacturers. For example, commodity cells are used in laptops and cell phones, as the energy storage element in its batteries. \n\nThe auto industry battery consortium, USABC, set about to invent automotive batteries made from specialty cells for cars.\n\nTesla Motors uses commodity cells to make their automotive batteries. \n\nPractically all commodity cells today are made in Asia – mainly Japan, South Korea, and China. There is no significant production anywhere in the US. A modern lithium ion cell plant – such as those in Japan – is a highly automated affair with very low labor content.\n\n\n\n"}
{"id": "2297543", "url": "https://en.wikipedia.org/wiki?curid=2297543", "title": "Coolgardie safe", "text": "Coolgardie safe\n\nThe Coolgardie safe is a low-tech food storage unit for cooling and prolonging the life of whatever edibles were kept in it. It applies the basic principle of heat transfer which occurs during evaporation of water. It was named after the place where it was invented — the small mining town of Coolgardie, Western Australia, near Kalgoorlie-Boulder.\n\nCoolgardie was the site of a gold rush in the early 1890s, before the Kalgoorlie-Boulder gold rush. \n\nFor the prospectors who had rushed here to find their fortune, one challenge was to extend the life of their perishable foods — hence the invention of the Coolgardie safe.\n\nThe safe was invented in the late 1890s by Arthur Patrick McCormick, who used the same principle as explorers and travelers in the Outback used to cool their canvas water bags: when the canvas bag is wet the fibers expand and it holds water. Some water seeps out and evaporates. It is most effective when air continually moves past it, such as when in a moving vehicle or when exposed to a breeze.\n\nThis technology is commonly thought to have been adopted by explorer and scientist Thomas Mitchell, who had observed the way some Indigenous Australians used kangaroo skins to carry water.\n\nThe Coolgardie safe was made of wire mesh, hessian, a wooden frame and had a hot dip galvanised iron tray on top. The galvanised iron tray was filled with water. The hessian bag was hung over the side with one of the ends in the tray to soak up the water. \n\nGradually the hessian bag, acting as a wick, would draw water from the tray by the process of capillary action. When a breeze came it would pass through the wet bag and evaporate the water. This would cool the air inside the safe, and in turn cool the food stored in the safe. This cooling is due to the water in the hessian needing energy to change state and evaporate. This energy is taken from the interior of the safe (metal mesh), thus making the interior cooler. There is a metal tray below the safe to catch excess water from the hessian.\n\nIt was usually placed on a veranda where there was a breeze. The Coolgardie safe was a common household item in Australia until the mid-twentieth century. Safes could be purchased ready-made or easily constructed at home. Some of the metal panel safes are highly decorated.\n\n"}
{"id": "3555307", "url": "https://en.wikipedia.org/wiki?curid=3555307", "title": "Design management", "text": "Design management\n\nDesign management is a field of inquiry that uses project management, design, strategy, and supply chain techniques to control a creative process, support a culture of creativity, and build a structure and organization for design. The objective of design management is to develop and maintain an efficient business environment in which an organization can achieve its strategic and mission goals through design. Design management is a comprehensive activity at all levels of business (operational to strategic), from the discovery phase to the execution phase. \"Simply put, design management is the business side of design. Design management encompasses the ongoing processes, business decisions, and strategies that enable innovation and create effectively-designed products, services, communications, environments, and brands that enhance our quality of life and provide organizational success.\" The discipline of design management overlaps with marketing management, operations management, and strategic management.\n\nTraditionally, design management was seen as limited to the management of design projects, but over time, it evolved to include other aspects of an organization at the functional and strategic level. A more recent debate concerns the integration of design thinking into strategic management as a cross-disciplinary and human-centered approach to management. This paradigm also focuses on a collaborative and iterative style of work and an abductive mode of inference, compared to practices associated with the more traditional management paradigm.\n\nDesign has become a strategic asset in brand equity, differentiation, and product quality for many companies. More and more organizations apply design management to improve design-relevant activities and to better connect design with corporate strategy.\n\nThe multifaceted nature of design management leads to varied opinion, making it difficult to give an overall definition; furthermore, design managers have a broad range of roles and responsibilities. These factors, combined with a multitude of other influences such as the industry involved, company size, the market situation, and the importance of design within the organization's activities. As a result, design management is not restricted to a single design discipline and usually depends on the context of its application within an individual organization.\n\nOn an abstract level, design management plays three key roles in the interface of design, organization, and market. The three key roles are to:\n\n\nUnlike unique sciences such as mathematics, the perspective, activity, or discipline of design is not brought to a generally accepted common denominator. The historical beginnings of design are complex and the nature of design is still the subject of ongoing discussion. In design, there are strong differentiations between theory and practice. The fluid nature of the theory allows the designer to operate without being constrained by a rigid structure. In practice, decisions are often referred to as \"intuition\". In his \"Classification of Design\" (1976), Gorb divided design into three different classes. Design management operates in and across all three classes: product (e.g. industrial design, packaging design, service design), information (e.g. graphic design, branding, media design, web design), and environment (e.g. retail design, exhibition design, interior design).\n\nManagement in all business and organizational activities is the act of getting people together to accomplish desired goals and objectives efficiently and effectively. Management comprises planning, organizing, staffing, leading or directing, and controlling an organization (a group of one or more people or entities), or effort for the purpose of accomplishing a goal. Resourcing encompasses the deployment and manipulation of human resources, financial resources, technological resources, and natural resources. Towards the end of the 20th century, business management came to consist of six separate branches, namely human resource management, operations management (or production management), strategic management, marketing management, financial management, and information technology management, which was responsible for management information systems. Although it is difficult to subdivide management into functional categories in this way, it helps in navigating the discipline of management. Design management overlaps mainly with the branches marketing management, operations management, and strategic management.\n\nDesign managers often operate in the area of design leadership; however, design management and design leadership are interdependent rather than interchangeable. Like management and leadership, they differ in their objectives, achievements of objectives, accomplishments, and outcomes. Design leadership leads from creation of a vision to changes, innovations, and implementation of creative solutions. It stimulates communication and collaboration through motivation, sets ambitions, and points out future directions to achieve long-term objectives. In contrast, design management is reactive and responds to a given business situation by using specific skills, tools, methods, and techniques. Design management requires design leadership to know where to go and design leadership requires design management to know how to get there.\n\nDifficulties arise in tracing the history of design management. Even though design management as an expression is first mentioned in literature in 1964, earlier contributions created the context in which the expression could arise. Throughout its history, design management was influenced by a number of different disciplines: architecture, industrial design, management, software development, engineering; and movements such as system theory, design methodologies. It cannot be attributed directly to either design or to management.\n\nEarly contributions to design management show how different design disciplines were coordinated to achieve business objectives at a corporate level, and demonstrate the early understanding of design as a competitive force. In that context, design was merely understood as an aesthetic function, and the management of design was at the level of project planning.\n\nThe practice of managing design to achieve a business objective was first documented in 1907. The Deutscher Werkbund (German Work Federation) was established in Munich by twelve architects and twelve business firms as a state-sponsored effort to better compete with Great Britain and the United States by integrating traditional craft and industrial mass-production techniques. A German designer and architect, Peter Behrens, created the entire corporate identity (logotype, product design, publicity, etc.) of Allgemeine Elektrizitäts Gesellschaft (AEG), and is regarded as the first industrial designer in history. His work for AEG was the first large-scale demonstration of the viability and vitality of the Werkbund's initiatives and objectives and can be considered as first contribution to design management.\n\nIn the following years, companies applied the principles of corporate identity and corporate design to increase awareness and recognition by consumers and differentiation from competitors. Olivetti became famous for its attention to design through their corporate design activities. In 1936 Olivetti hired Giovanni Pintori in their publicity department and promoted Marcello Nizzoli from the product design department to develop design in a comprehensive corporate philosophy. In 1956, inspired by the compelling brand character of Olivetti, Thomas Watson, Jr., CEO of IBM, retained American architect and industrial designer Eliot Noyes to develop a corporate-wide IBM Design Program consisting of coherent brand-design strategy together with a design management system to guide and oversee the comprehensive brand identity elements of: products, graphics, exhibits, architecture, interiors and fine art. This seminal effort by Noyes, with his inclusion of Paul Rand and Charles Eames as consultants, is considered to be the first comprehensive corporate design program in America. Up to and during the 1960s, debates in the design community were focused on ergonomics, functionalism, and corporate design, while debates in management addressed Just in time, Total quality management, and product specification. The main proponents of design management at that time were AEG, Bauhaus, HfG Ulm, the British Design Council, Deutscher Werkbund, Olivetti, IBM, Peter Behrens, and Walter Paepcke.\n\nThe work of designers in the 1960s was influenced by industry, as the debate on design evolved from an aesthetic function into active cooperation with industry. Designers had to work in a team with engineers and marketers, and design was perceived as one part of the product development process. In the early years, design management was strongly influenced by system science and the emergence of a design science (e.g. the \"blooming period of design methodologies\" in Germany, the US, and Great Britain), as its main contributors had backgrounds in architecture. Early discussions on design management were strongly influenced by Anglo-Saxon literature (e.g. Farr and Horst Rittel), methodological studies in Design Research (e.g. HfG Ulm and Christopher Alexander), and theories in business studies. Design management dealt with two main issues:\n\nInstruments and checklists were developed to structure the processes and decisions of companies for successful corporate development. In this period the main contributors to design management were Michael Farr, Horst Rittel, HfG Ulm, Christopher Alexander, James Pilditch, the London Business School, Peter Gorb, the Design Management Institute, and the Royal Society of Arts. Debates in design disciplines were focusing on design science, design methodology, wicked problems, Ulm methodology, the relationship of design and business, new German design, and semiotic and scenario technique.\n\nIn the 1980s several managers realized the economic effect of design, which increased the demand for design management. As companies were unsure how to manage design, there was a market for consultancy; focusing on helping organizations manage the product development process, including market research, product concepts, projects, communications, and market launch phases—as well as the positioning of products and companies.\n\nThree important works were published in 1990: the \"Publication of Design Management – A Handbook of Issues and Methods\" by Mark Oakley (Editor), the book \"Design Management\" by French researcher Brigitte Borja de Mozota, and the \"Publication of Design Management – Papers from the London Business School\" by Peter Gorb (Editor). This new method-based design management approach helped to improve communication amongst technical and marketing managers. Examples of the new methods included trend research, the product effect triad, style mapping, milieus, product screenings, empiric design methods, and service design, giving design a more communicative and central role within organizations.\n\nIn the management community the topics of management theory, positioning strategy, brand management, strategic management, advertisement, competitive strategy, leadership, business ethics, mass customization, core competencies, strategic intent, reputation management, and system theory were discussed. Main issues and debates in design management included the topics of design leadership, design thinking, and corporate identity; plus the involvement of design management at the operational, tactical, and strategic levels.\n\nIn 1980 Robert Blaich, the senior managing director of design at Philips, introduced a design management system that regards design, production, and marketing as a single unit. This was an important contribution to the definition of design as a core element in business. At Philips Design, Stefano Marzano became CEO and Chief Creative Director in 1991, continuing the work of Robert Blaich to align design processes with business processes and furthering design strategy as an important asset of the overall business strategy.\n\nUpon being appointed corporate head of the IBM Design Program in 1989, Tom Hardy, initiated a strategic design management effort, in collaboration with IBM design consultant Richard Sapper, to return to the roots of the IBM Design Program first established in 1956 by Eliot Noyes, Paul Rand and Charles Eames. The intent was to reprise IBM's brand image with customer experience-driven quality, approachability and contemporary product innovation. The highly successful IBM ThinkPad was the first product to emerge from this strategy in 1992 and, together with other innovative, award-winning products that followed, served to position design as a strategic asset for IBM's brand turnaround efforts initiated in 1993 by newly appointed CEO Louis V. Gerstner, Jr.\n\nAs a consultant following his 22-year tenure at IBM, Hardy served as Corporate Design Advisor to Samsung from 1996-2003 where his integration of a brand-design philosophy and guiding principles, together with a comprehensive design management system, became a strategic corporate asset that significantly helped elevate Samsung's image from follower to global brand-design leader and dramatically increased brand equity value.\n\nDesign management has taken a more strategic role within business since 2000, and more academic programs for design management have been established. Design management has been recognized (and subsidized) throughout the European Union as a function for corporate advantage of both companies and nations. The main issues and debates included the topics of design thinking, strategic design management, design leadership, and product service systems. Design management was influenced by the following design trends: sustainable design, inclusive design, interactive design, design probes, product clinics, and co-design. It was also influenced by the later management trends of open innovation and design thinking.\n\nIn 1965 the term \"design management\" was first published in a series of articles in the \"Design Journal\". This series includes a pre-publication of the first chapter of the book \"Design Management\" by Michael Farr, which is considered as the first comprehensive literature on design management. His thoughts on system theory and project management led to a framework on how to deal with design as a business function at the corporate management level by providing the language and methodology to effectively manage it.\n\nThe term \"architectural management\" was coined by the architects Brunton, Baden Hellard and Boobyer in 1964 where they highlighted the tension and synergy between the management of individual projects (job management) and the management of the business (office management). Although they did not use the term \"design management\", they stressed identical issues; while the design community discussed methodologies for design. Christopher Alexander's work played an important role in the development of the design methodology, where he devoted his attention to the problems of form and context; and focused on disassembling complex design challenges into constituent parts to approach a solution. His intention was to bring more rationalism and structure into the solving of design problems.\n\nDesign policies have a history reaching back to the end of the 19th century, when design programs with roots in the crafts sector were implemented in Sweden (1845) and Finland (1875). In 1907 the Deutscher Werkbund (German Work Federation) was established in Munich to better compete with Great Britain and United States. The success of the Deutscher Werkbund inspired a group of British designers, industrialists and business people after they had seen the Werkbund Exhibition in Cologne in 1914, to found the Design and Industries Association and campaign for a greater involvement of government in the promotion of good design. In 1944 design management by managing design policies was used by the British Government. The British Design Council was founded by Hugh Dalton, president of the Board of Trade in the British wartime government, as the Council of Industrial Design with the objective \"to promote by all practicable means the improvement of design in the products of British industry\".\n\nGermany also realized the national importance of design during World War II. Between 1933 and 1945 Adolf Hitler used design, architecture and propaganda to increase his power; shown through the annual Reichsparteitage in Nürnberg on September 5. Heinrich Himmler coordinated several design activities for Hitler, including: the all-black SS-uniform designed by Professor Karl Diebitsch and Walter Heck in 1933; the Dachau concentration camp, designed by Theodor Eicke, and prototypes for other Nazi concentration camps; and the Wewelsburg redesign commissioned by Heinrich Himmler in 1944.\n\nSince the 1990s the practice of design promotion evolved, and governments have used policy management and design management to promote design as part of their efforts of fostering technology, manufacturing and innovation.\n\nIn America the Chicago industrialist Walter Paepcke, of the Container Corporation of America, founded the Aspen Design Conference after World War II as a way of bringing business and designers together – to the benefit of both. In 1951 the first conference topic, \"Design as a function of management\", was chosen to ensure the participation of the business community. After several years, business leaders stopped attending because the increased participation of designers changed the dialogue, focusing not on the need for collaboration between business and design, but rather on the business community's failure to understand the value of design.\n\nThe Royal Society of Arts (RSA) Presidential Medals for Design Management were instituted in June 1964. These were to recognize outstanding examples of design policy in organizations that maintained a consistently high standard in all aspects of design management, throughout all industries and disciplines. With these awards the RSA introduced the term \"design management\". In 1965 the first medals were given to four companies; Conran & Co Ltd, Jaeger & Co Ltd, S. Hille & Co Ltd and W. & A. Gilbey Ltd. in the category \"current achievements\" and two companies London Transport and Heal and Son Ltd. in the category \"long pioneering in the field of design management\". The medal selection committee included representatives of the RSA council and the faculty of Royal Designers for Industry.\n\nThe Design Management Institute (DMI) was founded in 1975 at the Massachusetts College of Art in Boston. Since the mid-1980s the DMI has been an international non-profit organization that seeks to heighten the awareness of design as an essential part of business strategy, and become the leading resource and international authority on design management. One year later the first conference was organized. The DMI increased its international presence and established the \"European International Conference on Design Management\" in 1997, and a professional development program for design management.\n\nIn 2007 the European Commission funded the Award for Design Management Innovating and Reinforcing Enterprises (ADMIRE) project for two years, as part of the Pro Inno Europe Initiative, which is the EU's \"focal point for innovation policy analysis, learning and development\". The aim was to encourage companies – especially small and medium enterprises (SMEs) – to introduce design management procedures to; improve their competitiveness, stimulate innovation, establish a European knowledge-sharing platform, organize the Design Management Europe Award, and to identify and test new activities to promote Design Management.\n\nTeaching design to managers was pioneered at the London Business School (LBS) in 1976 by Peter Gorb (1926-2013), the first Honorary Fellow of the DMI and a long-standing Fellow of the RSA. Gorb had previously embedded design management in the Burton Retail Group before joining LBS where he later founded the Design Management Unit in 1982 (in collaboration with Charles Handy) which he led for over 20 years. In 1979 his talk at the RSA entitled \"Design and its Use by Managers\" provided a background introduction to the wide scope of design within industry and commerce, an appreciation of the power of design as a management resource, and advocated the teaching of design to managers. Gorb produced two books based on seminars at the Design Management Unit at LBS, \"Design Talks\" (1988) with Eric Schneider and \"Design Management: Papers from the London Business School\" (1990). Gorb is also remembered as introducing the concept of Silent Design, design undertaken by non-designers, in an influential paper with Angela Dumas (1987).\n\nIn 1991 the University of Art and Design Helsinki founded the Institute of Design Leadership and Management and established an international training program. The International Design Management Conference was organized in the same year by them. In 1995 the Helsinki School of Economics (HSE), University of Art and Design Helsinki (TaiK), and University of Technology (TKK) cooperated to create the International Design Business Management Program (IDBM), which aims to bring together experts from different fields within the concept of design business management.\n\nThe first international research project on design management, the TRIAD research project, was initiated by Earl Powell, then president of DMI and the Harvard Business School in 1989. In the same year Earl Powell and Thomas Walton, Ph.D. developed the Design Management Review and DMI published the first issue. The publication is solely focusing on design management and has become the flagship publication of the discipline.\n\nDesign and design management have experienced different generations of theories. In its first generation design focused on the object, in the second on the process, and in the third on the user. Similar shifts can be seen in management and design management in almost parallel steps. For design management this has been illustrated by Brigitte Borja de Mozota, using Findeli's Bremen Model as a framework. Design management research organised itself into:\nIt is difficult to predict where design management research is heading.\n\nDifferent types of design management depend on the type and strategic orientation of the business.\n\nIn product-focused companies, design management focuses mainly on product design management, including strong interactions with product design, product marketing, research and development, and new product development. This perspective of design management is mainly focused on the aesthetic, semiotic, and ergonomic aspects of the product to express the product's qualities and to manage diverse product groups and product design platforms and can be applied together with a user-centered design perspective.\n\nIn market and brand focused companies, design management focuses mainly on brand design management, including corporate brand management and product brand management. Focusing on the brand as the core for design decisions results in a strong focus on the brand experience, customer touch points, reliability, recognition, and trust relations. The design is driven by the brand vision and strategy.\n\n\nMarket and brand focused organizations are concerned with the expression and perception of the corporate brand. Corporate design management implements, develops, and maintains the corporate identity, or brand. This type of brand management is strongly anchored in the organization to control and influence corporate design activities. The design program plays the role of a quality program within many fields of the organization to achieve uniform internal branding. It is strongly linked to strategy, corporate culture, product development, marketing, organizational structure, and technological development. Achieving a consistent corporate brand requires the involvement of designers and a widespread design awareness among employees. A creative culture, knowledge sharing processes, determination, design leadership, and good work relations support the work of corporate brand management.\n\n\nThe main focus of product brand management lies on the single product or product family. Product design management is linked to research and development, marketing, and brand management, and is present in the fast-moving consumer goods (FMCG) industry. It is responsible for the visual expressions of the individual product brand, with its diverse customer–brand touch points and the execution of the brand through design.\n\nService design management deals with the newly emerging field of service design. It is the activity of planning and organizing people, infrastructure, communication, and material components of a service. The aim is to improve the quality of the service, the interaction between the service provider and its customers, and the customer's experience. The increasing importance and size of the service sector in terms of people employed and economic importance requires that services should be well-designed in order to remain competitive and to continue to attract customers. Design management traditionally focuses in the design and development of manufactured products; service design managers can apply many of the same theoretical and methodological approaches. Systematic and strategic management of service design helps the business gain competitive advantages and conquer new markets. Companies that proactively identify the interests of their customers and use this information to develop services that create good experiences for the customer will open up new and profitable business opportunities.\n\nCompanies in the service sector innovate by addressing the intangibility, heterogeneity, inseparability, and perishability of service (the IHIP challenge):\n\nService design management differs in several ways from product design management. For example, the application of international trading strategies of services is difficult because the evolution of service 'from a craftsmanship attitude to industrialization of services' requires the development of new tools, approaches, and policies. Whereas goods can be manufactured centrally and delivered around the globe, services have to be performed at the place of consumption, which makes it difficult to achieve global quality consistency and effective cost control.\n\nBusiness design management deals with the newly emerging field of integrating design thinking into management. In organisation and management theory, design thinking forms part of the Architecture / Design / Anthropology (A/D/A) paradigm which characterizes innovative, human-centered enterprises. This paradigm focuses on a collaborative and iterative style of work and an adductive mode of thinking, compared to practices associated with the more traditional Mathematics / Economics / Psychology (M/E/P) management paradigm. Since 2006, the term \"Business Design\" is trademarked by the Rotman School of Management; they define business design as the application of design thinking principles to business practice. The designerly way of problem solving is an integrative way of thinking that is characterized by a deep understanding of the user, creative resolution of tensions, collaborative prototyping, and continuous modification and enhancement of ideas and solutions. This approach to problem solving can be applied to all components of business, and the management of the problem solving process forms the core of business design management activity. Universities other than the Rotman School of Management are offering similar academic education concepts, including the Aalto University in Finland, which initiated their International Design Business Management (IDBM) program in 1995.\n\nEngineering Design Management is a knowledge area within engineering management. It represents the adaptation and application of customary management practices, with the intention of achieving a productive [engineering design process]. Engineering design management is primarily applied in the context of engineering design teams, whereby the activities, outputs and influences of design teams are planned, guided, monitored and controlled. The output of an engineering design process is ultimately a description of a technical system. That technical system may either be an artifact (technical object), production facility, a process plant or any infrastructure for the benefit of society. Therefore, the domain of engineering design management includes high volume, mass production as well as low-volume, infrastructure.\n\nUrban design management involves mediation among a range of self-interested stakeholders engaged in the production of the built environment. Such mediation can encourage a joint search for mutually beneficial outcomes or integrative development. Integrative development aims to produce sustainable solutions by increasing stakeholder satisfaction with the process and with the resulting urban development.\n\nConventional real estate development and urban planning activities are subject to conflicting interests and positional bargaining. The integrative negotiation approach emphasises mutual gains. The approach has been applied in land use planning and environmental management, but has not been used as a coordinated approach to real estate development, city design, and urban planning. Urban design management involves reordering the chain of events in the production of the built environment according to the principles of integrative negotiation. Such negotiation can be used in urban development and planning activities to reach more efficient agreements. This leads to integrative developments and more sustainable ways to produce the built environment.\n\nUrban design management offers prescriptive advice for practitioners trying to organise city planning activities in a way that will increase sustainability by increasing satisfaction levels. Real estate development and urban planning often occur at very different decision-making levels. The practitioners involved may have diverse educational and professional backgrounds. They certainly have conflicting interests. Providing prescriptive advice for differing, possibly conflicting, groups requires construction of a framework that accommodates all of their daily activities and responsibilities. Urban design management provides a common framework to help bring together the conventional practices of urban and regional planning, real estate development, and urban design.\n\nThe work on \"Integrative Negotiation Consensus Building\" and the \"Mutual Gains Approach\" provide a helpful theoretical framework for developing the theory of urban design management. Negotiation theory provides a useful framework for merging the perspectives of urban planning, city design, and real estate project proposals regarding production of the built environment. \"Interests\", a key construct in negotiation theory, is an important variable that will allow integrated development, as defined above, to occur. The path-breaking work of Roger Fisher and William Ury (1981), \"Getting to yes\", advises negotiators to focus on interests and mutual gains instead of bargaining over positions.\n\nArchitectural management can be defined as an ordered way of thinking which helps to realise a quality building for an acceptable cost or as a process function with the aim of delivering greater architectural value to the client and society. Research by Kiran Gandhi describes architectural management as a set of practical techniques for an architect to successfully operate his practice. The term \"architectural management\" has been in use since the 1960s. The evolution of the field of architectural management has not been a smooth affair. Architectural practice was merely considered a business until after the Second World War, and even then practitioners appeared to be concerned about the conflict between art and commerce, demonstrating indifference to management. There was apparent conflict between the image of an architect and the need for professional management of the architectural business. Reluctance to embrace management and business as an inherent part of architectural practice could also be seen in architectural education programmes and publications. It appears that the management of architectural design, as well as architectural management in general, is still not being given enough importance. Architectural management falls into two distinct parts: office or practice management and project management. Office management provides an overall framework within which many individual projects are commenced, managed, and completed. Architectural management extends between the management of the design process, construction, and project management, through to facilities management of buildings in use. It is a powerful tool that can be applied to the benefit of professional service firms and the total building processes, yet it continues to receive too little attention both in theory and in practice.\n\nDesign plays a vital role in product and brand development, and is of great economic importance for organisations and companies. Creativity and design in particular (as an activity: design skills, methods and processes) play a growing role in creating products and services with high added value to consumers. Design generates 50% of world export revenue in the creative industries' products (goods and services). The creative industry workforce is 3.1% of total employment in the European Union (EU), which creates a revenue that is 2.6% of the EU gross value. Creative industries have attained an unprecedented average annual growth rate of 8.7 per cent across the EU between 2000 and 2005.\n\nThe increasing importance of creative industries (and especially design) in knowledge-intense industries is reflected not only in the policies and studies on EU levels, but has initiated design and creative policies and programmes in the most advanced economies. Furthermore, design and creativity has been recognised on a regional and local level as a driving force for competitiveness, economic growth, job market, and citizen's satisfaction. The investment in creative and cultural industries are considered a significant component of EU growth in the Lisbon Strategy and the Europe 2020 strategy; and designers are increasingly involved in innovation issues.\n\nTo better understand the value of design and its role in innovation, the EU holds a public consultation on the basis of their publication \"Design as a driver of user-centred innovation\" and have published the mini-study \"Design as a tool for innovation\". The report highlights the importance of design in user-centred innovation and recommends the integration of design into the EU innovation policy. In addition to the design share in the export of all creative industry products, design can also have a positive impact on all business performance indicators; from turnover and profit to market share and competitiveness. Design management research results can be classified as follows:\n\nIf and how design management is applied in a company correlates with the importance and integration of design in the company, but depends also on industry type, company size, ownership for design and type of competitive competence. A research from the Danish Design Centre (DDC) led to the \"Danish Design Ladder\", which shows how companies interpreted and applied design in differing depth:\nThe research showed that companies that considered design on a higher level of the ladder were constantly growing. Additionally, the Danish Design Centre published an \"Evaluation of the Importance of Design\" in 2006, with the result that most companies considered design as a promoter for innovation (71%), as a growth potential for the company (79%), and to make products more user friendly (71%). With increasing importance of design for the company, design management also becomes more important.\n\nThe value of design can be leveraged if it is managed well. Research by Chiva and Alegre shows that there is no link between the level of design investment and business success, but instead a strong correlation between design management skills and business success. This means that efficient and effective design management is crucial for maximising the value of design. Effective design management increases the efficiency of operations and process management, has a significant positive impact on process management, improves quality performance (internal and external quality), and increases operating performance. To measure and communicate the value of design management, Borja de Mozota suggests adapting the Balanced Score Card model and structuring the values in the following four categories:\n\nThree different orientations for the choice of design management can be identified in companies. These orientations influence the perception of management and the responsibility of design managers within the organisation. The strategic orientations are; market focus, product focus and brand focus.\n\nDepending on the strategic orientation, design management overlaps with other management branches to differing extents:\n\nMarketing management: The concepts and elements of brand management overlap with those of design management. In practice, design management can be part of the job profile of a marketing manager, though the discipline includes aspects that are not in the domain of marketing management. This intersection is called \"brand design management\" and consists of positioning, personality, purpose, personnel, project and practice, where the objective is to increase brand equity.\n\nOperations management: At the operational level design management deals with the management of design projects. Processes and tools from operations management can be applied to design management in the execution of design projects.\n\nStrategic management: Due to the increasing importance of design as a differentiator and its supporting role in brand equity, design management deals with strategic design issues and supports the strategic direction of the business or enterprise. The debate on design thinking suggests the integration of design thinking into strategic management. Design thinking and strategic thinking have some commonalities in their characteristics, both are synthetic, adductive, hypothesis-driven, opportunistic, dialectical, enquiring and value-driven.\n\nInnovation management: The value of the coordinating role of design in new product development has been well documented. Design management can help to improve innovation management, which can be measured by three variables: it reduces time-to-market, by improving sources and communication skills and developing cross-functional innovation; it stimulates networking innovation, by managing product and customer information flows with internal (e.g. teams) and external (e.g. suppliers, society) actors; it improves the learning process by promoting a continuous learning process.\n\nLike the management of strategy, design can be managed on three levels: strategic (corporate level or enterprise wide), tactical (business level or individual business units), and operational (individual project level). These three levels have been termed differently by various authors over the last 50 years.\n\nOperational level\n\nOperational design management involves the management of individual design projects and design teams. Its goal is to achieve the objectives set by strategic design management. Success of good design management can be measured by evaluating the quality of operational design management outcomes. It includes the selection and management of design suppliers and encompasses the documentation, supervision, and evaluation of design processes and results. It deals with personal leadership, emotional intelligence, and the cooperation with and management of internal communications. Regular management functions, tools, and concepts can often be applied to the management of design on the operational level. It is implemented to achieve specific design objectives and manage the judgment of design proposals. It can help to build brand equity through the consistent creation and implementation of high-quality design solutions that best fit the brand identity and desired consumer experience, in the most efficient way. Depending on the type of company and industry, the following job titles are associated with this role: operational design manager, senior designer, team leader, visual communication manager, corporate design coordinator, and others.\n\nTactical level\n\nTactical design management addresses the organisation of design resources and design processes. Its goal is to create a structure for design in the company, bridging the gap between objectives set through strategic design management and the implementation of design on the operational level. It defines how design is organised within the company. This includes the use of a central body to coordinate different design projects and activities. It deals with defining activities, developing design skills and competencies, managing processes, systems and procedures, assigning of roles and responsibilities, developing innovative products and service concepts, and finding new market opportunities. Outcomes of tactical design management are related to the creation of a structure for design within the company, to build internal resources and competencies for the implementation of design. Depending on the type of company and industry, the following job titles are associated with this function: tactical design manager, design director, design & innovation manager, brand design manager, new product development (NPD) manager, visual identity manager, and others.\n\nStrategic level\n\nStrategic design management involves the creation of strategic long-term vision and planning for design, and deals with defining the role of design within the company. The goal of strategic design management is to support and strengthen the corporate visio by creating a relationship between the design and corporate strategy. It includes the creation of design, brand and product strategies, ensuring that design management becomes a central element in the corporate strategy formulation process. Strategic design management is responsible for the development and implementation of a corporate design programme that influences the design vision, mission, and positioning. It allows design to interact with the needs of corporate management and focuses on the long-term capabilities of design. Where strategic design management is applied, there is often a strong belief in the potential to differentiate the company and gain competitive advantage by design. As a result, design thinking becomes integrated into the corporate culture. Depending on the type of company and industry the following job titles are associated with this function: design strategist, strategic design manager, chief design officer, vice president design and innovation, chief creative officer, innovation design director, and others.\n\nDesign management is not a standard model that can be projected onto every enterprise, nor is there a specific way of applying it that leads to guaranteed success. Design management processes are carried out by humans with different responsibilities and backgrounds, who work in different industries and enterprises with different sizes and traditions, whilst having different target groups and markets to serve. Design management is multifaceted, and so are the different applications of and views on design management. The function of design management in an organisation depends on its tasks, authority, and practice.\n\nTask\n\nSimilar tasks can be grouped into categories to describe the job profile of a design manager. Different categories in management that encompass design were defined by several authors; those tasks occur on all three design management levels (strategic, tactical, and operational):\n\nAuthority and position\n\nThe authority and position of the design management function has a large influence on what the design manager does in his or her daily job. Kootstra (2006) distinguishes design management types by organisational function: design management as line function, design management as staff function, and design management as support function. Design management as a \"line function\" is directly responsible for design execution in the \"primary\" organisational process and can take place on all levels of the design management hierarchy. The main attributes for design managers in the line are authority over and direct responsibility for the result. Design management as a staff function is not directly responsible for design execution in the \"primary\" organisational process, but consults as a specialist on all levels of the design management hierarchy. The main attributes for design managers in this function are their limited authority and the need to consult line managers and staff. When the design process is defined as a \"secondary\" organisational process, design management is seen as \"supportive function\". In this function it has only a supportive character, classifying the design manager as a creative specialist towards product management, brand management, marketing, R&D, and communication. Various authors use different concepts to describe the authority and position of design management; they can be grouped as follows:\n\nToday, most developed countries have some kind of design promotion programme. The Design Management Institute has dedicated three issues to design policy development. Although initiatives promote design in different complexities, scopes and focuses, specific targets tend to address the following objectives:\n\nA very comprehensive analysis on the situation of design on national level in Britain is the Cox review. The chairman of the Design Council, Sir George Cox, published the Cox Review of Creativity in Business in 2005 to communicate the competitive advantage of design for the British industry.\n\nInnovation policies have been excessively focused on the supply of technologies, neglecting the demand side (the user). There have been several initiatives by the European Commission to support and research design and design management in recent years. However, a European-wide policy to support design has never been planned, due to the inconsistencies and differences in design policies in each nation. Nonetheless, there are currently plans to include design in the EU innovation policy.\n\nWhile design management had its origins in business schools, it has increasingly become embedded in the curriculum in design schools, particularly at the postgraduate level. Teaching design to managers was pioneered at the London Business School in 1976, and the first programme of design management at a design school was started in the 1980s at the Royal College of Art (RCA), DeMontfort, Middlesex, Staffordshire. Although, in the UK, some design management courses have not been sustainable, including those at the RCA, Westminster and Middlesex, other postgraduate courses have flourished including ones at Brunel, Lancaster and more recently the University of the Arts with each providing a specific point of view on design management.\n\nThe Design Leadership Fellowship at the University of Oxford was founded in 2005. In the same year the Stanford University Institute of Design founded the D-school, a faculty intended to advance multidisciplinary innovation. The Finnish Aalto University was founded in 2010 and is a merger of the three established Finnish universities – the Helsinki School of Economics (HSE), University of Art and Design Helsinki (TaiK), and University of Technology (TKK) – that had been cooperating on the IDBM design management program since 1995. Since 2006 the Lucerne University of Applied Sciences and Arts in Switzerland offers one of the few undergraduate studies in design management, completely taught in English.\n\n\"BusinessWeek\" annually publishes a lists of the best programmes that combine design thinking and business thinking (\"D-schools 2009\" and \"D-school Programmes to Watch 2009\"). The article \"Finland – World´s Innovation Hot Spot\" in the Harvard Business Review shows the interest of business leaders in the blended education of design and management. Business Schools (such as the Rotman School of Management, Wharton University of Pennsylvania and MIT Sloan Executive Education) have acted on this interest and developed new academic curricula.\n\nIntegrated education models are emerging in the academic world, a model which is referred to as T-shape and π-shaped education. T-shaped professionals are taught general knowledge in a few disciplines (e.g. management and engineering) and specific, deep knowledge in a single domain (e.g. design). This model also applies to companies, when they shift their focus from \"small T\" innovations (innovations involving only one discipline, like chemists) to \"big T\" innovations (innovations involving several disciplines, like design, ethnography, lead user, etc.). Like in education, this shift makes breaking down silos of departments and disciplines of knowledge essential.\n\nBooks\n"}
{"id": "41102", "url": "https://en.wikipedia.org/wiki?curid=41102", "title": "Electronic warfare support measures", "text": "Electronic warfare support measures\n\nIn military telecommunications, the terms electronic support (ES) or electronic support measures (ESM) describe the division of electronic warfare involving actions taken under direct control of an operational commander to detect, intercept, identify, locate, record, and/or analyze sources of radiated electromagnetic energy for the purposes of immediate threat recognition (such as warning that fire control RADAR has locked on a combat vehicle, ship, or aircraft) or longer-term operational planning. Thus, electronic support provides a source of information required for decisions involving electronic protection (EP), electronic attack (EA), avoidance, targeting, and other tactical employment of forces. Electronic support data can be used to produce signals intelligence (SIGINT), communications intelligence (COMINT) and electronics intelligence (ELINT).\n\nElectronic support measures gather intelligence through passive \"listening\" to electromagnetic radiations of military interest. Electronic support measures can provide (1) initial detection or knowledge of foreign systems, (2) a library of technical and operational data on foreign systems, and (3) tactical combat information utilizing that library. ESM collection platforms can remain electronically silent and detect and analyze RADAR transmissions beyond the RADAR detection range because of the greater power of the transmitted electromagnetic pulse with respect to a reflected echo of that pulse. United States airborne ESM receivers are designated in the AN/ALR series.\n\nDesirable characteristics for electromagnetic surveillance and collection equipment include (1) wide-spectrum or bandwidth capability because foreign frequencies are initially unknown, (2) wide dynamic range because signal strength is initially unknown, (3) narrow bandpass to discriminate the signal of interest from other electromagnetic radiation on nearby frequencies, and (4) good angle-of arrival measurement for bearings to locate the transmitter. The frequency spectrum of interest ranges from 30 MHz to 50 GHz. Multiple receivers are typically required for surveillance of the entire spectrum, but tactical receivers may be functional within a specific signal strength threshold of a smaller frequency range.\n\n"}
{"id": "33092714", "url": "https://en.wikipedia.org/wiki?curid=33092714", "title": "Energies (journal)", "text": "Energies (journal)\n\nEnergies is a monthly peer-reviewed open-access scientific journal. It was established in 2008 and is published by MDPI. The editor-in-chief is Enrico Sciubba (Sapienza University of Rome). The journal publishes original papers, review articles, technical notes, and letters to the editor.\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 2.676.\n"}
{"id": "16086815", "url": "https://en.wikipedia.org/wiki?curid=16086815", "title": "European Biofuels Technology Platform", "text": "European Biofuels Technology Platform\n\nThe European Biofuels Technology Platform (BiofuelsTP) is a European Seventh Framework Programme initiative to improve the competitive situation of the European Union in the field of biofuel.\n\nThe programme is a joint initiative (Public-Private Partnership) of the European Commission, representing the European Communities, and the industry. The main objective of the programme is to produce a Strategic Research Agenda. The BiofuelsTP initiative was launched at a Conference in June 2006.\n\n\n\n"}
{"id": "38273081", "url": "https://en.wikipedia.org/wiki?curid=38273081", "title": "Fluorescence image-guided surgery", "text": "Fluorescence image-guided surgery\n\nFluorescence guided surgery (FGS), (also called 'Fluorescence image-guided surgery', or in the specific case of tumor resection, 'fluorescence guided resection') is a medical imaging technique used to detect fluorescently labelled structures during surgery. Similarly to standard image-guided surgery, FGS has the purpose of guiding the surgical procedure and providing the surgeon of real time visualization of the operating field. When compared to other medical imaging modalities, FGS is cheaper and superior in terms of resolution and number of molecules detectable. As a drawback, penetration depth is usually very poor (100 μm) in the visible wavelengths, but it can reach up to 1–2 cm when excitation wavelengths in the near infrared are used.\n\nFGS is performed using imaging devices with the purpose of providing real time simultaneous information from color reflectance images (bright field) and fluorescence emission. One or more light sources are used to excite and illuminate the sample. Light is collected using optical filters that match the emission spectrum of the fluorophore. Imaging lenses and digital cameras (CCD or CMOS) are used to produce the final image. Live video processing can also be performed to enhance contrast during fluorescence detection and improve signal-to-background ratio. In recent years a number of commercial companies have emerged to offer devices specializing in fluorescence in the NIR wavelengths, with the goal of capitalizing upon the growth in off label use of indocyanine green (ICG). However commercial systems with multiple fluorescence channels also exist commercially, for use with fluorescein and protoporphyrin IX (PpIX).\n\nFluorescence excitation is accomplished using various kind of light sources. Halogen lamps have the advantage of delivering high power for a relatively low cost. Using different band-pass filters, the same source can be used to produce several excitation channels from the UV to the near infrared. Light-emitting diodes (LEDs) have become very popular for low cost broad band illumination and narrow band excitation in FGS. Because of their characteristic light emission spectrum, a narrow range of wavelengths that matches the absorption spectrum of a given fluorophore can be selected without using a filter, further reducing the complexity of the optical system. Both halogen lamps and LEDs are suitable for white light illumination of the sample. Excitation can also be performed using laser diodes, particularly when high power over a short wavelength range (typically 5-10 nm) is needed. In this case the system has to account for the limits of exposure to laser radiation.\n\nLive images from the fluorescent dye and the surgical field are obtained using a combination of filters, lenses and cameras. During open surgery, hand held devices are usually preferred for their ease of use and mobility. A stand or arm can be used to maintain the system on top of the operating field, particularly when the weight and complexity of the device is high (e.g. when multiple cameras are used). The main disadvantage of such devices is that operating theater lights can interfere with the fluorescence emission channel, with a consequent decrease of signal-to-background ratio. This issue is usually solved by dimming or switching off the theater lights during fluorescence detection. \nFGS can also be performed using minimally invasive devices such as laparoscopes or endoscopes. In this case, a system of filters, lenses and cameras is attached to the end of the probe. Unlike open surgery, the background from external light sources is reduced. Nevertheless, the excitation power density at the sample is limited by the low light transmission of the fiber optics in endoscopes and laparoscopes, particularly in the near infrared. Moreover, the ability of collecting light is much reduced compared to standard imaging lenses used for open surgery devices.\nFGS devices can also be implemented for robotic surgery (for example in the da Vinci Surgical System).\n\nThe major limitation in FGS is the availability of clinically approved fluorescent dyes which have a novel biological indication. Indocyanine Green (ICG) has been widely used as a non-specific agent to detect sentinel lymph nodes during surgery. ICG has the main advantage of absorbing and emitting light in the near infrared, allowing detection of nodes under several centimeters of tissue. Methylene Blue can also be used for the same purpose, with an excitation peak in the red portion of the spectrum. First clinical applications using tumor-specific agents that detect deposits of ovarian cancer during surgery have been carried out.\n\nThe first uses of FGS dates back to the 1940s when fluorescein was first used in humans to enhance the imaging of brain tumors, cysts, edema and blood flow in vivo. In modern times the use has fallen off, until a multicenter trial in Germany concluded that FGS to help guide glioma resection based upon fluorescence from PpIX provided significant short term benefit.\n\n"}
{"id": "7051798", "url": "https://en.wikipedia.org/wiki?curid=7051798", "title": "Food Products Association", "text": "Food Products Association\n\nThe Food Products Association or FPA (formerly the National Food Processors Association or NFPA) was the principal scientific and technical U.S. trade association representing the food products industry. Since 1907, the food industry has relied on FPA for government and regulatory affairs representation, scientific research, technical assistance, education, communications, and crisis management. FPA is headquartered in Washington, DC, with subsidiaries in Dublin, CA and Seattle, WA.\n\nFPA started in 1907 as the National Canners Association. It became the National Food Processors Association in 1978, and the Food Products Association in 2005. \n\nOn January 1, 2007, FPA has merged with the Grocery Manufacturers Association and formed the world's largest trade association representing the food, beverage, and consumer products industry (GMA/FPA). Effective January 1, 2008 the association will use the single name Grocery Manufacturers Association. Also effective January 1, 2008, FPA's former Seattle, Washington office became independently incorporated under the name Seafood Products Association.\n\n"}
{"id": "13952850", "url": "https://en.wikipedia.org/wiki?curid=13952850", "title": "Free-piston engine", "text": "Free-piston engine\n\nA free-piston engine is a linear, 'crankless' internal combustion engine, in which the piston motion is not controlled by a crankshaft but determined by the interaction of forces from the combustion chamber gases, a rebound device (e.g., a piston in a closed cylinder) and a load device (e.g. a gas compressor or a linear alternator).\n\nThe purpose of all such piston engines is to generate power. In the free-piston engine, this power is not delivered to a crankshaft but is instead extracted through either exhaust gas pressure driving a turbine, through driving a linear load such as an air compressor for pneumatic power, or by incorporating a linear alternator directly into the pistons to produce electrical power.\n\nThe basic configuration of free-piston engines is commonly known as single piston, dual piston or opposed pistons, referring to the number of combustion cylinders. The free-piston engine is usually restricted to the two-stroke operating principle, since a power stroke is required every fore-and-aft cycle. However, a split cycle four-stroke version has been patented, GB2480461 (A) published 2011-11-23.\n\nThe modern free-piston engine was proposed by R.P. Pescara and the original application was a single piston air compressor. Pescara set up the \"Bureau Technique Pescara\" to develop free-piston engines and Robert Huber was technical director of the Bureau from 1924 to 1962.\n\nThe engine concept was a topic of much interest in the period 1930-1960, and a number of commercially available units were developed. These first generation free-piston engines were without exception opposed piston engines, in which the two pistons were mechanically linked to ensure symmetric motion. The free-piston engines provided some advantages over conventional technology, including compactness and a vibration-free design.\n\nThe first successful application of the free-piston engine concept was as air compressors. In these engines, air compressor cylinders were coupled to the moving pistons, often in a multi-stage configuration. Some of these engines utilised the air remaining in the compressor cylinders to return the piston, thereby eliminating the need for a rebound device.\n\nFree-piston air compressors were in use among others by the German Navy, and had the advantages of high efficiency, compactness and low noise and vibration.\n\nAfter the success of the free-piston air compressor, a number of industrial research groups started the development of free-piston gas generators. In these engines there is no load device coupled to the engine itself, but the power is extracted from an exhaust turbine. (The only load for the engine is supercharging the inlet air.)\n\nA number of free-piston gas generators were developed, and such units were in widespread use in large-scale applications such as stationary and marine powerplants. Attempts were made to use free-piston gas generators for vehicle propulsion (e.g. in gas turbine locomotives) but without success.\n\nModern applications of the free-piston engine concept include hydraulic engines, aimed for off-highway vehicles, and free-piston engine generators, aimed for use with hybrid electric vehicles.\n\nThese engines are commonly of the single piston type, with the hydraulic cylinder acting as both load and rebound device using a hydraulic control system. This gives the unit high operational flexibility. Excellent part load performance has been reported.\n\nFree-piston linear generators that eliminate a heavy crankshaft with electrical coils in the piston and cylinder walls are being investigated by multiple research groups for use in hybrid electric vehicles as range extenders. The first free piston generator was patented in 1934. Examples include the Stelzer engine and the Free Piston Power Pack manufactured by Pempek Systems based on a German patent. A single piston Free-piston linear generator was demonstrated in 2013 at the German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR).\n\nThese engines are mainly of the dual piston type, giving a compact unit with high power-to-weight ratio. A challenge with this design is to find an electric motor with sufficiently low weight. Control challenges in the form of high cycle-to-cycle variations were reported for dual piston engines.\nIn June 2014 Toyota announced a prototype Free Piston Engine Linear Generator (FPEG). As the piston is forced downward during its power stroke it passes through windings in the cylinder to generate a burst of three-phase AC electricity. The piston generates electricity on both strokes, reducing piston dead losses. The generator operates on a two-stroke cycle, using hydraulically activated exhaust poppet valves, gasoline direct injection and electronically operated valves. The engine is easily modified to operate under various fuels including hydrogen, natural gas, ethanol, gasoline and diesel. A two-cylinder FPEG is inherently balanced.\n\nToyota claims a thermal-efficiency rating of 42% in continuous use, greatly exceeding today's average of 25-30%. Toyota demonstrated a 24 inch long by 2.5 inch in diameter unit producing 15 hp (greater than 11 kW).\n\nThe operational characteristics of free-piston engines differ from those of conventional, crankshaft engines. The main difference is due to the piston motion not being restricted by a crankshaft in the free-piston engine, leading to the potentially valuable feature of variable compression ratio. This does, however, also present a control challenge, since the position of the dead centres must be accurately controlled in order to ensure fuel ignition and efficient combustion, and to avoid excessive in-cylinder pressures or, worse, the piston hitting the cylinder head.\n\nPotential advantages of the free-piston concept include\n\n\nThe main challenge for the free-piston engine is engine control, which can only be said to be fully solved for single piston hydraulic free-piston engines. Issues such as the influence of cycle-to-cycle variations in the combustion process and engine performance during transient operation in dual piston engines are topics that need further investigation. Crankshaft engines can connect traditional accessories such as alternator, oil pump, fuel pump, cooling system, starter etc.\n\nRotational movement to spin conventional automobile engine accessories such as alternators, air conditioner compressors, power steering pumps, and anti-pollution devices could be captured from a turbine situated in the exhaust stream.\n\nMost free piston engines are of the opposed piston type with a single central combustion chamber. A variation is the Opposing piston engine which has two separate combustion chambers. An example is the Stelzer engine.\n\nIn the 21st century, research continues into free-piston engines and patents have been published in many countries. In the UK, Newcastle University is undertaking research into free-piston engines.\n\nA new kind of the free-piston engine, a Free-piston linear generator is being developed by the German aerospace center.\n\n\n"}
{"id": "1112273", "url": "https://en.wikipedia.org/wiki?curid=1112273", "title": "Heat of combustion", "text": "Heat of combustion\n\nThe heating value (or energy value or calorific value) of a substance, usually a fuel or food (see food energy), is the amount of heat released during the combustion of a specified amount of it.\n\nThe \"calorific value\" is the total energy released as heat when a substance undergoes complete combustion with oxygen under standard conditions. The chemical reaction is typically a hydrocarbon or other organic molecule reacting with oxygen to form carbon dioxide and water and release heat. It may be expressed with the quantities:\n\n\nThere are two kinds of heat of combustion, called higher and lower heating value, depending on how much the products are allowed to cool and whether compounds like are allowed to condense.\nThe values are conventionally measured with a bomb calorimeter. They may also be calculated as the difference between the heat of formation Δ\"H\" of the products and reactants (though this approach is somewhat artificial since most heats of formation are calculated from measured heats of combustion). For a fuel of composition CHON, the (higher) heat of combustion is usually to a good approximation (±3%), though it can be drastically wrong if \"o\" + \"n\" > \"c\" (for instance in the case of nitroglycerine () this formula would predict a heat of combustion of 0). The value corresponds to an exothermic reaction (a negative change in enthalpy) because the double bond in molecular oxygen is much weaker than other double bonds or pairs of single bonds, particularly those in the combustion products carbon dioxide and water; conversion of the weak bonds in oxygen to the stronger bonds in carbon dioxide and water releases energy as heat.\n\nBy convention, the heat of combustion is defined to be the heat released for the complete combustion of a compound in its standard state to form stable products in their standard states: hydrogen is converted to water (in its liquid state), carbon is converted to carbon dioxide gas, and nitrogen is converted to nitrogen gas. That is, the heat of combustion, Δ\"H°\", is the heat of reaction of the following process: \n\nChlorine and sulfur are not quite standardized; they are usually assumed to convert to hydrogen chloride gas and SO or SO gas, respectively, or to dilute aqueous hydrochloric and sulfuric acids, respectively, when the combustion is conducted in a bomb containing some water quantity of water.\n\nThe quantity known as higher heating value (HHV) (or \"gross energy\" or \"upper heating value\" or \"gross calorific value\" (GCV) or \"higher calorific value\" (HCV)) is determined by bringing all the products of combustion back to the original pre-combustion temperature, and in particular condensing any vapor produced. Such measurements often use a standard temperature of . This is the same as the thermodynamic heat of combustion since the enthalpy change for the reaction assumes a common temperature of the compounds before and after combustion, in which case the water produced by combustion is condensed to a liquid. The higher heating value takes into account the latent heat of vaporization of water in the combustion products, and is useful in calculating heating values for fuels where condensation of the reaction products is practical (e.g., in a gas-fired boiler used for space heat). In other words, HHV assumes all the water component is in liquid state at the end of combustion (in product of combustion) and that heat delivered at temperatures below can be put to use\n\nThe quantity known as lower heating value (LHV) (\"net calorific value\" (NCV) or \"lower calorific value\" (LCV)) is determined by subtracting the heat of vaporization of the water from the higher heating value. This treats any HO formed as a vapor. The energy required to vaporize the water therefore is not released as heat.\n\nLHV calculations assume that the water component of a combustion process is in vapor state at the end of combustion, as opposed to the higher heating value (HHV) (a.k.a. \"gross calorific value\" or \"gross CV\") which assumes that all of the water in a combustion process is in a liquid state after a combustion process.\n\nThe LHV assumes that the latent heat of vaporization of water in the fuel and the reaction products is not recovered. It is useful in comparing fuels where condensation of the combustion products is impractical, or heat at a temperature below cannot be put to use.\n\nThe above is but one definition of lower heating value adopted by the American Petroleum Institute (API) and uses a reference temperature of .\n\nAnother definition, used by Gas Processors Suppliers Association (GPSA) and originally used by API (data collected for API research project 44), is the enthalpy of all combustion products minus the enthalpy of the fuel at the reference temperature (API research project 44 used 25 °C. GPSA currently uses 60 °F), minus the enthalpy of the stoichiometric oxygen (O) at the reference temperature, minus the heat of vaporization of the vapor content of the combustion products.\n\nThe distinction between the two is that this second definition assumes that the combustion products are all returned to the reference temperature and the heat content from the condensing vapor is considered not to be useful.\nThis is more easily calculated from the higher heating value than when using the preceding definition and will in fact give a slightly different answer.\n\nGross heating value (see AR) accounts for water in the exhaust leaving as vapor, and includes liquid water in the fuel prior to combustion. This value is important for fuels like wood or coal, which will usually contain some amount of water prior to burning.\n\nThe higher heating value is experimentally determined in a bomb calorimeter. The combustion of a stoichiometric mixture of fuel and oxidizer (e.g. two moles of hydrogen and one mole of oxygen) in a steel container at is initiated by an ignition device and the reactions allowed to complete. When hydrogen and oxygen react during combustion, water vapor is produced. The vessel and its contents are then cooled to the original 25 °C and the higher heating value is determined as the heat released between identical initial and final temperatures.\n\nWhen the lower heating value (LHV) is determined, cooling is stopped at 150 °C and the reaction heat is only partially recovered. The limit of 150 °C is based on acid gas dew-point.\n\nNote: Higher heating value (HHV) is calculated with the product of water being in liquid form while lower heating value (LHV) is calculated with the product of water being in vapor form.\n\nThe difference between the two heating values depends on the chemical composition of the fuel. In the case of pure carbon or carbon monoxide, the two heating values are almost identical, the difference being the sensible heat content of carbon dioxide between 150 °C and 25 °C (sensible heat exchange causes a change of temperature. In contrast, latent heat is added or subtracted for phase transitions at constant temperature. Examples: heat of vaporization or heat of fusion). For hydrogen the difference is much more significant as it includes the sensible heat of water vapor between 150 °C and 100 °C, the latent heat of condensation at 100 °C, and the sensible heat of the condensed water between 100 °C and 25 °C. All in all, the higher heating value of hydrogen is 18.2% above its lower heating value (142 MJ/kg vs. 120 MJ/kg). For hydrocarbons the difference depends on the hydrogen content of the fuel. For gasoline and diesel the higher heating value exceeds the lower heating value by about 10% and 7% respectively, and for natural gas about 11%.\n\nA common method of relating HHV to LHV is:\n\nwhere \"H\" is the heat of vaporization of water, n is the moles of water vaporized and n is the number of moles of fuel combusted.\n\n\nEngine manufacturers typically rate their engines fuel consumption by the lower heating values since the exhaust is never condensed in the engine. American consumers should be aware that the corresponding fuel-consumption figure based on the higher heating value will be somewhat higher.\n\nThe difference between HHV and LHV definitions causes endless confusion when quoters do not bother to state the convention being used. since there is typically a 10% difference between the two methods for a power plant burning natural gas. For simply benchmarking part of a reaction the LHV may be appropriate, but HHV should be used for overall energy efficiency calculations if only to avoid confusion, and in any case, the value or convention should be clearly stated.\n\nBoth HHV and LHV can be expressed in terms of AR (all moisture counted), MF and MAF (only water from combustion of hydrogen). AR, MF, and MAF are commonly used for indicating the heating values of coal:\n\n\n\nThe International Energy Agency reports the following typical higher heating values:\n\n\nThe lower heating value of natural gas is normally about 90 percent of its higher heating value.\n\n"}
{"id": "1052492", "url": "https://en.wikipedia.org/wiki?curid=1052492", "title": "Hurley (stick)", "text": "Hurley (stick)\n\nA hurley (or camán) is a wooden stick used in the Irish sports of hurling and camogie. It measures between 45 and 96 cm (18 to 38 inches) long with a flattened, curved \"bas\" at the end, which provides the striking surface. It is used to strike the leather \"sliotar\" ball.\n\nA hurley is also known as a hurl, hurley stick, hurling stick or as a \"camán\" (the Irish word).\n\nHurleys are made from ash wood; the base of the tree near the root is the only part used and is usually bought from local craftsmen in Ireland (for about 20–50 euro), who still use traditional production methods. However, for some time in the 1970s, hurleys made from plastic were used, mainly produced by Wavin. These proved more likely to cause injury, however, and were phased out. As of 2012, at least one manufacturer was producing synthetic hurleys approved for use by the GAA. Steel bands are used to reinforce the flattened end of the hurley though these are not permitted in camogie due to increased risk of injury. Bands have been put on hurleys since the beginning; the 8th century Brehon Laws permit only a king's son to have a bronze band, while all others must use a copper band.\n\nNo matter how well crafted the hurley is, a hurler may well expect to use several hurleys over the course of the hurling season. The hurleys often break if two collide in the course of a game, or occasionally they break off on the other players (arms, legs, etc.). Two hurleys colliding is colloquially known as \"the clash of the ash\". Some hurleys can be repaired by a method called \"splicing\". This method involves cutting a bas-shaped piece from another broken hurley and fixing it to the broken bas by way of glue and nails; the two-piece bas is then banded (\"hooped\") and sanded into shape. (The face of the hurley is called the bas, and is the area used to strike the ball.) Throwing the hurley (e.g., to block a ball going high over one's head) is illegal, though camogie players may drop it to make a handpass.\n\nThere are names associated with different parts of the hurley. With respect to the picture above, the \"bas\" is the rounded end of the hurley where the sliotar makes contact as it is being struck. At the same end the \"heel\" of the hurley is the area to the left of the band and at the hurley's edge (nearest the bottom of the picture above). It is used to give height to a ball struck on the ground. The rounded area to the right of the band is the \"toe\" of the hurley and is used in the roll lift or jab lift techniques which allow a player to gain legal possession of a ball into the hand from the ground. The handle is at the opposite end of the hurley to the boss, with the timber cut to form a small lip at the peak (to prevent the hurley from slipping from the player's hand). The handle is typically wrapped with a self-adhesive synthetic foam grip.\n\nWhen selecting a hurley, choosing the correct size is very important as a hurley that is the incorrect length can be difficult to swing correctly. A correctly sized hurley should be just touching the ground when gripped at the top and held parallel to the player's leg with their arms relaxed. Traditionally it was recommended that the toe of the hurley should reach the player's hip when the heel of the hurley is placed on the ground and held parallel to the player's leg. This has proved inaccurate and unsuitable since two players of the same height can have a difference of 4 inches or 10 centimetres in hip height. \n\nThe hurley is often given as a gift to or between politicians; for example, Mary and Martin McAleese were given two when she was awarded the freedom of Kilkenny in 2009, and Barack Obama was given one by Enda Kenny on his visit to Ireland in 2011. Prince Philip was also given a hurley and sliotar as a gift during Queen Elizabeth II's visit to Ireland.\n\nJason Statham used a hurley as a weapon in the opening scene of the 2011 film \"Blitz\".\n\n"}
{"id": "18504615", "url": "https://en.wikipedia.org/wiki?curid=18504615", "title": "Hydrogen odorant", "text": "Hydrogen odorant\n\nA hydrogen odorant in any form, is a minute amount of odorant such as ethyl isobutyrate, with a rotting-cabbage-like smell, that is added to the otherwise colorless and almost odorless hydrogen gas, so that leaks can be detected before a fire or explosion occurs. Odorants are considered non-toxic in the extremely low concentrations occurring in hydrogen gas delivered to the end user.\n\nThe approach is not new, for the same safety reasons the odorant \"tert\"-butyl mercaptan is used in natural gas.\n\n\n"}
{"id": "56457803", "url": "https://en.wikipedia.org/wiki?curid=56457803", "title": "ISO 14006", "text": "ISO 14006\n\nISO 14006, \"Environmental management systems - Guidelines for incorporating ecodesign\", is an international standard that specifies guidelines to help organizations establish, document, implement, maintain, and continuously improve their ecodesign management as part of the environmental management system. The standard is intended to be used by organizations that have implemented an environmental management system in compliance with ISO 14001, but can help to integrate ecodesign into other management systems. The guideline is applicable to any organization regardless of its size or activity.\n\nISO 14006 was developed by ISO/TC207/SC1 Environmental management systems, and was published for the first time in July 2011.. \n\nISO/TC 207 was established in the year 1993.\n\nThe \" 'ISO 14006' \" adopts a scheme in 5 chapters in the following subdivision:\n\n\n"}
{"id": "5770329", "url": "https://en.wikipedia.org/wiki?curid=5770329", "title": "Ignaz Schwinn", "text": "Ignaz Schwinn\n\nIgnaz Schwinn (1860-1948) was a designer, a founder, and the eventual sole owner of the Schwinn Bicycle Company.\n\nIgnaz Schwinn was born in the town of Hardheim, Germany in 1860. In his early years, he completed a mechanical apprenticeship, after which he became an itinerant bicycle repairman. Schwinn reportedly had a falling-out with an early partner in Germany over brake designs and decided to seek his fortune abroad. He arrived in Chicago in 1891 and, by 1895, had teamed with Adolph Arnold, another German immigrant, to found Arnold, Schwinn, and Company. In 1908, Schwinn bought Arnold's interest, thereby becoming sole owner. Ignaz Schwinn maintained the original company name and ran operations through World War II, after which his son Frank succeeded him, the name was changed to the Schwinn Bicycle Company, and the corporation grew to have national standing in the market. Ignaz Schwinn died in 1948 of a stroke.\n\n"}
{"id": "1922181", "url": "https://en.wikipedia.org/wiki?curid=1922181", "title": "Internal documentation", "text": "Internal documentation\n\nComputer software is said to have Internal Documentation if the notes on how and why various parts of code operate is included within the source code as comments. It is often combined with meaningful variable names with the intention of providing potential future programmers a means of understanding the workings of the code.\n\nThis contrasts with external documentation, where programmers keep their notes and explanations in a separate document.\n\nInternal documentation has become increasingly popular as it cannot be lost, and any programmer working on the code is immediately made aware of its existence and has it readily available.\n"}
{"id": "6037441", "url": "https://en.wikipedia.org/wiki?curid=6037441", "title": "Joseph Gilbert Hamilton", "text": "Joseph Gilbert Hamilton\n\nJoseph Gilbert Hamilton (November 11, 1907 – February 18, 1957) was an American professor of Medical Physics, Experimental Medicine, General Medicine, and Experimental Radiology as well as director (1948-1957) of the Crocker Laboratory, part of the Lawrence Berkeley National Laboratory. Hamilton studied the medical effects of exposure to radioactive isotopes, which included the use of unsuspecting human subjects.\n\nHe was married to painter Leah Hamilton.\n\nHamilton received his B.S. in Chemistry in 1929 from the University of California. He studied medicine in Berkeley and interned at the University of California Hospital, San Francisco. He was awarded his M.D. degree in 1936. At that time the cyclotron in Berkeley was among the first to produce useful amounts of radioactive isotopes which could be used in studies of their effects on living tissue. In a series of papers published in 1937 Hamilton detailed early medical trials using radio sodium, followed by papers detailing the use of the radioactive isotopes of potassium, chlorine, bromine, and iodine. Radioactive iodine was found to be particularly useful in the diagnosis and treatment of thyroid disorders.\n\nConcern was expressed while working on the Manhattan Project in 1944, for the safety of laboratory personnel working with newly isolated plutonium. Hamilton lead a team who conducted toxicity experiments on rats. Finding the results obtained with rats unsatisfactory Hamilton was included in the decision making to continue the trials with human subjects. These trials were conducted in secret from 1945 to 1947.\n\nTrials were carried out by three teams headed by Louis Hempelmann, Wright Haskell Langham and Joseph Gilbert Hamilton. They consisted of injecting plutonium into unsuspecting human patients then measuring its concentration in excreta. During these trials 18 human subjects were injected with Plutonium, three by Joseph Gilbert Hamilton's team at University of California Hospital, San Francisco.\n\nAlbert Stevens, CAL-1, had been diagnosed with terminal stomach cancer, which was later found to have been an ulcer. Stevens is significant as he is recorded as having survived the highest known accumulated radiation dose of any human. He lived 20 years after the injection until his death at 79 years of age.\n\nSimeon Shaw, CAL-2, was 4 years old at the time of injection and had been diagnosed with bone cancer. Shaw lived for 255 days post injection, with his cause of death being recorded as from bone cancer.\n\nElmer Allen, CAL-3, was 36 at the time of injection and lived for 44 years post injection, with his cause of death being recorded as respiratory failure, pneumonia. He died in 1991 shortly before Eileen Welsome could interview Allen for her work in exposing the trials.\n\nHamilton's studies of isotope retention in humans, especially of radioactive strontium and the transuranic elements, were the principal source for the U.S. Atomic Energy Commission setting of far lower tolerance limits of these substances than had been theorised before trials. This series of human trials were terminated by the Atomic Energy Commission in 1950.\n\nOnce the AEC took over control of the Manhattan Project's various roles, Hamilton returned to his work at Berkeley. In a memo written in 1950, Hamilton gave some recommendations to the AEC's Director of Biology and Medicine, Shields Warren. Hamilton wrote that large primates like \"chimpanzees ... [should] be substituted for humans in the planned studies on radiation's cognitive effects.\" He further warned that by using humans the AEC would be open \"to considerable criticism,\" since the experiments as proposed had \"a little of the Buchenwald touch.\" Dr. Eugene Saenger would be the one who carried out these experiments from 1960 to 1971 at the University of Cincinnati, exposing \"at least 90 cancer patients to large radiation doses.\"\n\nHamilton died at the age of 49. His name was added to the \"Monument to the X-ray and Radium Martyrs of All Nations\" erected in Hamburg, Germany.\n\n\n"}
{"id": "14249287", "url": "https://en.wikipedia.org/wiki?curid=14249287", "title": "Kimchi refrigerator", "text": "Kimchi refrigerator\n\nA kimchi refrigerator is a refrigerator designed specifically to meet the storage requirements of kimchi and facilitate different fermentation processes. Kimchi usually becomes too sour when stored in a conventional refrigerator for about a week. In contrast, kimchi can be stored up to 4 months without losing its qualities in a kimchi refrigerator. The kimchi refrigerator aims to be colder, with more consistent temperature, more humidity, and less moving air than a conventional refrigerator, providing the ideal environment for fermentation of kimchi. Some models may include features such as a UV Sterilizer.\n\nIn a consumer survey aimed at Korean homemakers conducted by a top-ranking Korean media agency in 2004, the kimchi refrigerator was ranked first for most wanted household appliance. \n\nThe prototype of kimchi refrigerators was created by Billtec in 1992. In December 1995, after three years of comprehensive work on development to best suit kimchi fermentation and storage, WiniaMando launched its first commercial brand, DIMCHAE (Hangul: 딤채), into the mass market. As of November 2007, more than a dozen home appliance manufacturers, including Samsung and LG, were involved in commercial production of kimchi refrigerators.\nAlthough the top-loading types were introduced first, this design took up much space and heavy plastic kimchi containers had to be lifted to get to the bottom part. The door-drawer types (the \"stand-type\" in Korea) are gaining popularity because of their space-saving ergonomic design. A single door with a wine-bar type design at the top can be opened in full or partially at the wine-bar section to save energy. The top portion can be used as a freezer.\n\nTwo deep drawers make up the bottom portion to store anything from kimchi to wine, beer, etc.\n"}
{"id": "3862683", "url": "https://en.wikipedia.org/wiki?curid=3862683", "title": "List of observatory codes", "text": "List of observatory codes\n\nThis is a list of observatory codes, or IAU codes, with their corresponding astronomical observatories. The Minor Planet Center (MPC) – a service of the International Astronomical Union – assigns for each registered observatory a 3-digit code in the range 000 to Z99.\n\nThe code serves as a unique identifier for observations taken of hundreds of thousands of minor planets and thousands of comets orbiting in the Solar System. More than 197 millions such astrometric records exist. This list is based on MPC's periodically published and revised \"List Of Observatory Codes\". Over time, the number of astronomical observatories worldwide has been growing constantly. As of October 2018, this list contains 2095 observatory codes published in MPC's official list.\n\nThe registry is limited to observatories which perform minor planet observations. While this includes most optical telescopes of note and a great many amateur facilities, it does not include the U.S. National Solar Observatory or many notable radio observatories (such as the Llano de Chajnantor Observatory, MeerKAT, or the South Pole Telescope).\n\nThe following list contains an observatory's name and code as used by the MPC, its location and, optionally, a link to the corresponding article on Wikipedia and an external link to the observatory webpage.\n\n\n"}
{"id": "49388523", "url": "https://en.wikipedia.org/wiki?curid=49388523", "title": "Lumus", "text": "Lumus\n\nLumus is an Israeli-based Augmented Reality company headquartered in Ness Ziona, Israel. Founded in 2000 by Dr. Yaakov Amitai, Lumus has developed technology for see-through wearable displays, via its patented Light-guide Optical Element (LOE) platform to market producers of smart glasses and augmented reality eyewear. Lumus' technology enables a small natural looking form factor, wide field of view and true see-through performance.\n\n\nThe LOE is a patented optical waveguide that makes use of multiple partial reflectors embedded in a single substrate to reflect a virtual image into the eye of the wearer. Specifically, the image is coupled into the LOE by a \"Pod\" (micro-display projector) that sits at the edge of the waveguide—in an eyeglass configuration, this is embedded in the temple of the glasses. The image travels through total internal reflection to the multiple array of partial reflectors and are reflected to the eye. While each partial reflector shows only a portion of the image, the optics are such that the wearer sees the combined array and perceives it as a single uniform image projected at infinity. The transparent display enables a virtual image to be seamlessly overlaid over the wearer's real world view. This is especially true when the source image comprises a black background with light color wording or symbology being displayed. Black is essentially see-through color, while lighter colored objects, symbols or characters appear to float in the wearer's line of sight. Conversely, full screen images like documents, internet pages, movies which are typically brighter colors can be displayed to look like a large virtual image floating a few meter's away from the wearer.\n\n\nUsing Lumus Optical Engines (OE) or Development Kits allows smart eyewear manufacturers to maintain their own industrial design and branding.\n\n"}
{"id": "30103821", "url": "https://en.wikipedia.org/wiki?curid=30103821", "title": "Manchester Digital", "text": "Manchester Digital\n\nManchester Digital is an association of people and organisations working with digital media and digital technology based in and around Manchester, UK. It acts as a trade association and collaboration network. Its stated aims are to foster cooperation and promote the work done by its members to other businesses and outside the city and its region.\n\nMembers include companies, freelancers, public and third sector organisations, individuals and students working in areas including 'new media', web development, digital marketing, SEO, software development, internet services provision, Internet hosting and games development. Members agree to abide by a code of practice.\n\nManchester Digital organises events including The Big Chip Awards for digital work in Northern England. It recently co-launched the IBZL programme with The Open University.\n\nManchester Digital is structured as a not-for profit association and is funded by member subscription. It works closely with other organisations such as the Manchester Digital Development Agency, part of Manchester City Council, and the One Digital Alliance.\n\nManchester Digital was formed in 2001 by a small group of individuals and companies working in the internet industry, including broadcaster Tony Wilson and Internet Service Provider (ISP) Poptel. It had initial support from the inward investment agency for Manchester and Manchester City Council. It took responsibility for the Big Chip Awards in 2002.\n\nOne Digital, a national alliance of organisations with similar aims and objectives was launched in 2009 by Manchester Digital, Bristol Media and South East Media Network. One Digital now includes Wired Sussex.\n\nThe association was initially governed by an informal voluntary board. In 2003 it adopted a new two-tier structure with a formal constitution.\n\nManchester Digital Association is now governed by an elected 12-person council and is the sole member of Manchester Digital Limited, a not-for-profit company that runs events and other activities.\n\n\n"}
{"id": "2386505", "url": "https://en.wikipedia.org/wiki?curid=2386505", "title": "Messenger bag", "text": "Messenger bag\n\nA messenger bag (also called a courier bag) is a type of sack, usually made out of some kind of cloth (natural or synthetic), that is worn over one shoulder with a strap that goes across the chest resting the bag on the lower back. While messenger bags are sometimes used by couriers, they are now also an urban fashion icon. Some types of messenger bags are called carryalls. A smaller version is often called a sling bag.\n\nThis design of bag has been used in the transportation of mail and goods by numerous types of messengers, including Pony Express riders, postal workers, messengers on foot (especially in ancient times), and bicycle couriers. Some Royal Mail carriers in the United Kingdom currently use large messenger bags to deliver mail in lieu of a Postbag.\n\nPre-dating today's messenger bags described herein as specifically for bicycle messengers, fashion brands had been creating \"messenger style\" bags modeled after military map case bags and document pouches featuring a shoulder strap intended for wear across the chest for over a century.\n\nSimilar in function to backpacks, messenger bags ensure comfort to people carrying heavy and/or bulky items, while allowing easy access to the contents.\n\nMessenger bags typically incorporate features that make them particularly suitable for cycling, such as fittings that make it easy to adjust the shoulder strap, quick release buckles, an adjustable hinged buckle, and the ability to attach a variety of accessories, such as lights, phone holsters, or U-locks. Contemporary stabilizing straps help to prevent the bag from shifting while riding. The top-opening one-strap design allows messenger bags to be easily swung around front so that their contents can be accessed without having to remove the bag.\n\nMessenger bags are often used as a fashion accessory. While they can be found in the possession of either gender, they are often commonly employed by men in a function analogous to a woman's purse (that is, to carry bulky items that do not fit into pockets, or a large number of items, while doubling as a fashion accessory). Messenger bags have also become fashionable in urban environments, among cyclists and commuters. Many college and high-school students make use of them for fashionable and functional purposes, especially those who commute on bicycles. Many companies design messenger bags specifically for the collegiate market. Compared to a backpack, it is much easier to place and remove text-books, notebooks, essays and supplies from a messenger bag because they can be easily shifted to lie on the side of the body (or, if the strap is long enough, it will be there by default), granting the wearer better accessibility. Messenger bags also provide more weather resistance than traditional leather satchel-style school bags.\n\nMaterials used in messenger bags are often more durable and water-resistant than other over-the-shoulder bags. Contemporary bags use thicker gauges of canvas and tarp shielding for the inner waterproof lining. Other materials include ballistic nylon, vinyl waterproof tarp lining used to make the messenger bag waterproof. The liner also provides the support structure for the messenger bag; this keeps the bag from falling over on itself. Some companies eschew the standard PVC waterproof lining for compounds such as thermoplastic polyurethanes, which are newer, comparatively more expensive, more durable, more environmentally friendly, and less volatile.\n\n"}
{"id": "3800090", "url": "https://en.wikipedia.org/wiki?curid=3800090", "title": "Motorboating (electronics)", "text": "Motorboating (electronics)\n\nIn electronics, motorboating is a type of low frequency parasitic oscillation (unwanted cyclic variation of the output voltage) that sometimes occurs in audio and radio equipment and often manifests itself as a sound similar to an idling motorboat engine, a \"put-put-put\", in audio output from speakers or earphones. It is a problem encountered particularly in radio transceivers and older vacuum tube audio systems, guitar amplifiers, PA systems and is caused by some type of unwanted feedback in the circuit. The amplifying devices in audio and radio equipment are vulnerable to a variety of feedback problems, which can cause distinctive noise in the output. The term motorboating is applied to oscillations whose frequency is below the range of hearing, from 1 to 10 hertz, so the individual oscillations are heard as pulses. Sometimes the oscillations can even be seen visually as the woofer cones in speakers slowly moving in and out.\n\nBesides sounding annoying, motorboating can cause clipping of the audio output waveform, and thus distortion in the output.\n\nAlthough low frequency parasitic oscillations in audio equipment may be due to a range of causes, there are a few types of equipment in which it is frequently seen:\n\nAs with all electronic oscillation, motorboating occurs when some of the output energy from an amplifying device like a transistor or vacuum tube gets coupled back into the input circuit of the device (or possibly into an earlier stage of the amplifier circuit) with the proper phase for positive feedback. This indicates there is an unwanted feedback path through the circuit from output to input of an amplifying stage. The technical conditions for oscillation, given by the Barkhausen stability criterion, are that the total gain around the feedback loop (comprising the amplifying device and the feedback path) at the oscillation frequency must be one (0 dB), and that the phase shift must be a multiple of 360° (2π radians). Since most amplifying devices, transistors and tubes, are inverting, with the output signal 180° opposite in phase from the input, the feedback path must contribute the other 180° of shift.\n\nMany types of parasitic oscillation are caused by small interelectrode capacitances (parasitic capacitance) or mutual inductance between adjacent wires or electronic components on the circuit board, which create an inadvertent feedback path. However these usually cause oscillations of high frequency, at the upper end of or above the passband of the equipment. This is because the phase shift of the small reactances in the feedback path, which increases with frequency, only become significant at high frequencies. Low frequency oscillations like motorboating indicate that some device or circuit with a large time constant is involved, such as the interstage coupling capacitors or transformers, or the filter capacitors and supply transformer winding.\n\nIn vacuum tube circuits, a common cause is feedback through the plate power supply circuit. The power supply provides DC current to each tube's plate circuit, so the power supply wiring (power busses) can be an inadvertent feedback path between stages. The increasing impedance of the filter capacitors at low frequencies can mean that low frequency swings in the current drawn by output stages can cause voltage swings in the power supply voltage which feed back to earlier stages, making the system a subaudio oscillator. This is caused by inadequate power supply filtering or decoupling. The electrolytic capacitors used in equipment of 1960s vintage contained liquid electrolyte, which dried out over decades, decreasing the capacitance and increasing the leakage current, and these are often the cause.\n\nOne solution suggested is a \"capacitor job\", replacing all the old electrolytic capacitors. A more radical but comprehensive solution is to add modern IC voltage regulators, or replace the entire power supply with a modern regulated one.\n\nIn equipment that includes radio transmitters, motorboating can be caused by radio frequency interference (RFI), the strong radio signal from the transmitter getting into audio or receiver circuits. Receiver audio circuits with automatic gain control (AGC) have a long time constant feedback loop which adjusts the gain of the audio stage to compensate for differences in audio level from causes like different speaking voices. Squelch circuits used in two-way radios to cut out noise similarly have a feedback loop which turns off the audio when high frequency noise is detected.\n\nIf the inaudible radio frequency (RF) transmitter signal is inadvertently coupled into the receiver's audio signal path, it can trigger the AGC or squelch circuit to reduce the gain. Then, after a delay time set by the circuit's time constant, the circuit increases the gain again until the amplitude of the radio signal triggers another gain reduction. This repetitive cycle is heard as motorboating.\n\nAn example might be a 27 MHz Citizen's band radio in a car, connected to the car's 12 volt DC supply. If the decoupling capacitors which bypass radio noise from the power supply wires are missing or inadequate, or the long power leads pick up excessive RF from the antenna then it is possible for the RF transmitter signal to enter the radio's receiving circuits through the supply wires. This then causes the motorboating to occur.\n"}
{"id": "13052519", "url": "https://en.wikipedia.org/wiki?curid=13052519", "title": "Naphthalocyanine", "text": "Naphthalocyanine\n\nNaphthalocyanine is a derivative of phthalocyanine used as a component in the development of IBM's single-molecule logic switch.\n\nIt was used to show the first images of charge distribution over a single molecule in November 2011.\n\nIts derivatives may also have uses in photodynamic cancer treatment.\n\n"}
{"id": "8682372", "url": "https://en.wikipedia.org/wiki?curid=8682372", "title": "Oxford Science Park", "text": "Oxford Science Park\n\nThe Oxford Science Park (OSP) is a science and technology park located on the southern edge of the city of Oxford, England. It was officially opened in 1991 and is owned by Magdalen College, Oxford. The park maintains strong links with the nearby University of Oxford and currently contains just over 60 companies.\n\nThere are two amenity buildings on the Science Park, the Magdalen Centre and the Sadler Building.\nBoth contain:\n\n\nThere is a nursery on the Science Park operated by The Oxford Nursery. There is also an Oxford Science Park Netball Club.\n\nThe science park is situated in Littlemore, which is about 5 km to the south of Oxford city centre, south of the Oxford Ring Road.\n\n\n"}
{"id": "706020", "url": "https://en.wikipedia.org/wiki?curid=706020", "title": "Pillow", "text": "Pillow\n\nA pillow is a support of the body at rest for comfort, therapy, decoration or play. Pillows are used by many species including humans. Some types of pillows include throw pillows and decorative pillows. Pillows that aid sleeping are a form of bedding that supports the head and neck. Other types of pillows are designed to support the body when lying down or sitting. Decorative pillows used on beds, couches or chairs are also referred to as cushions.\n\nIn contemporary western culture, pillows consist of a plain or patterned fabric envelope (known as a pillowcase) which contains a soft stuffing, typically synthetic and typically standardized in sizes and shape. Pillows have been historically made of a variety of natural materials and many cultures continue to use pillows made from natural materials.\n\nThe word \"pillow\" comes from Middle English \"pilwe\", from Old English \"pyle\" (akin to Old High German \"pfuliwi\") and from Latin \"pulvinus\". The first known use of the word \"pillow\" was before the 12th century.\n\nThough the exact origin is unknown, use of pillows evolved in animals well into prehistory, the earliest examples including reptiles and mammals resting their heads on themselves, and one another, to support the head and neck. Animals, including humans, evolved use of inanimate objects in their nests out of wood and stone as pillows. Since domestication, many animals have also learned to make use of human-made pillows and cushions, as well as to rest on members of own and other species, for this purpose.\n\nSometime between 5 and 23 million years ago tree dwelling great apes began building sleeping platforms, including wooden pillows, to improve their sleep. According to studies on chimpanzees that sleep up to eight to nine hours a night using specifically selected ironwood pillows, sturdy pillows enabled great apes to escape being hunted by night predators and not fall out of the trees while asleep. It is likely that this was necessitated by the evolution of large, energy consuming brains. Though it may also have led to longer periods of REM sleep, that in turn increased their cognitive capacity.\n\nThe earliest recorded use of the modern human device dates back to the civilizations of Mesopotamia around 7,000 BC. During this time, only the wealthy used pillows. The number of pillows symbolized status so the more pillows one owned the more affluence they held. Pillows have long been produced around the world in order to help solve the reoccurring problem of neck, back, and shoulder pain while sleeping. Besides for comfort, the pillow was also used for keeping bugs and insects out of people's hair, mouth, nose, and ears while sleeping.\n\nPillow use has been associated with the mummies and tombs of ancient Egypt during the 11th dynasty, dating to 2055–1985 B.C. Ancient Egyptian pillows were wooden or stone headrests. These pillows were mostly used by placing them under the heads of the deceased because the head of a human was considered to be the essence of life and sacred. The ancient Egyptians used these wooden or stone pillows in order to provide support to a corpse’s head, uphold body vigor, keep blood circulating, and keep demons away.\n\nThe Romans and Greeks of ancient Europe mastered the creation of the softer type pillow. These pillows were stuffed with reeds, feathers, and straw in order to make them softer and more comfortable. Only upper-class people typically owned these softer pillows; however, all classes of people were allowed to use some type of pillow while sleeping, lying down or sitting in order to give them support. People in ancient Europe started to use pillows when going to church in order to kneel on while praying and to place holy books on. This is a tradition that still lives on today. Additionally, the Romans and Greeks used their pillows by placing them under the head of those deceased just like the ancient Egyptians did.\n\nChinese pillows were traditionally solid, though sometimes used with a softer fabric over them. Over many Chinese dynasties, pillows were made from a wide range of materials including bamboo, jade, porcelain, wood, and bronze. Ceramic pillows became the most popular. The use of the ceramic pillow first appeared in the Sui Dynasty between 581 and 618 while mass production appeared in the Tang Dynasty between 618 and 907. The Chinese decorated their pillows by making them different shapes and by painting pictures of animals, humans, and plants on them. One common type of pottery used was Cizhou ware. Chinese ceramic pillows reached their peak in terms of production and use during the Song, Jin, and Yuan dynasties between the 10th and 14th century, but slowly phased out during the Ming and Qing dynasties between 1368 and 1911 with the emergence of better pillow making materials.\n\nPillows consist of a filler material enclosed in a fabric cover or shell. Covers are made of cloth, such as silk, known as the pillow case or pillow slip. Some pillows have a fancier cover called a \"sham\" which is closed on all sides and usually has a slit in the back through which the pillow is placed. Rectangular standard bed pillow cases usually do not have zippers, but instead, have one side open all the time. Often, a zippered pillow protector is often placed around standard pillows with the case in turn covering the protector.\n\nFillers are chosen on the basis of comfort, resilience, cost and to a lesser extent for ethical and health reasons. The most common synthetic fillers are foam, synthetic plastic fibers (typically polyester) and viscoelastic foam and latex. Synthetic fillers are more common as they are comfortable, inexpensive and retain their form longer. Natural fillers have been used since antiquity. The most common are feathers, or down, wool, cotton (particularly in India), buckwheat (in Asia). Historically, materials have included straw, wood or stone. Feathers and down are usually the most comfortable and common; they offer the advantage of softness and their ability to conform to shapes desired by the user, but are the most expensive. Down has been known to be plucked from live geese. There are currently hypoallergenic and cruelty-free varieties of down pillows to allow people sensitive to down to enjoy the comfort of feather or down pillows. In India, traditional pillows are usually made with plant-borne materials, such as the fluffy, glossy fruit-fibres of silk-cotton tree (Ceiba pentandra and Bombax ceiba).\n\nThe normal lifespan of a pillow is two to four years. Replacement is recommended for sanitary reasons. All types of pillow covers should be laundered periodically since they are the part that is in contact with a person's body. Pillows accumulate dust and microbes among the fill, even when washable pillows are washed. Manufacturers recommend tumble-drying for fifteen minutes every week to freshen them up, and for the heat to kill dust mites. Charities in most countries will not accept used pillows due to hygiene regulations. While some animal shelters accept forms of bedding, most reject donation of used pillows due to the mess they can cause.\n\nRecycling of pillows, like most textile and bedding items, is expensive and has poor yield. As such, few are recycled and most end up in landfills. Their light weight means that they make up a low proportion of household waste by mass. Most of the few pillows collected for recycling are sent to India and Pakistan and used as low-cost bedding, or in South East Asia, co-mingled with other textiles to manufacture cheap bedding.\n\nA pillow is designed to provide support and comfort to the body and head. There are three main types of pillows; bed pillows, orthopedic pillows and decorative pillows, with some overlapping of use between these. The appropriate size of a bed pillow depends on the size of the bed. Larger pillows than standard are available for queen- and king-sized beds.\n\nThe choice of bed pillow depends to some extent upon sleeping positions: one manufacturer recommends a thinner and softer pillow for sleeping face down, medium support for sleeping on one's back, and a thicker and firmer pillow for sleeping on the side.\n\nThe classic bed pillow shape is usually a square or rectangle. In the US, they are common in these three sizes (in inches): Standard (20×26 inches), Queen (20×30 inches), and King (20×36 inches). In the US, a less common size is Jumbo (20×28 inches), which is larger than the Standard Size but smaller than the Queen Size.\n\nPillows are generally covered with a removable pillow case, which facilitates laundering. Apart from the color and from the material of which they are made, pillowcases are described by three characteristics:\n\n\n\"Size\" conforms to the pillow the case is to contain. They are typically described as:\n\n\n\"Square\" is also called \"continental\" in the UK. German pillow sizes are 80×80 cm (older) or 40×80 cm (newer). When considered as a subset of bed pillows, Euro pillows finish 26×26 in and older style travel pillows commonly finished 12×16 in.\n\nThe main distinguishing feature is whether the pillow case is plain or with a valance around the edge. In the former case this is described as 'plain style' and in the latter as 'Oxford style'.\n\"Oxford ... has a 5cm-10cm valance round all four sides. With a hemstitched or corded decoration around the inner edge of the valance.\"\n\nThe opening/closure of pillow cases ranges from the straightforward \"bag style\" common in the United States to the \"housewife style\" more common in Europe, with a pocket inside the open end to fully contain the pillow.\n\"Housewife is ... essentially a bag, with a flap in the open end to tuck the pillowcase behind to keep it in...\"\n\nOther methods of closure are ties or buttons/buttonholes.\n\nBody pillows are as long as a full adult body, providing support to the head and neck at the top and to the knees and legs lower down. This type of pillow can be especially useful in providing support for those who sleep on their sides and for pregnant women. Size is 40×140 cm. (See also: Dutch wife)\n\nNeck pillows support the neck by providing a deep area for the head to rest and a supportive area to keep the neck in alignment with the spine while sleeping. These can also be known as cervical pillows. Cervical pillows help patients to maintain comfortable positioning after therapeutic, orthopedic and surgical measures.\n\nTravel pillows provide support for the neck in a sitting position. Their \"U\" shape fits around the back of the neck and keeps the head from slipping into an uncomfortable and possibly harmful position during sleep. However, U-shaped pillows can sometimes force the head forward, creating neck stiffness.\n\nDoughnut pillows are firm pillows shaped like a torus, with a space in the middle to alleviate pressure on the tailbone area while sitting. These pillows are used primarily by individuals who have suffered an injury to the tailbone area, or who suffer pain from hemorrhoids or another ailment of the colon.\n\nLumbar pillows are designed to support the inward curve of the lower back, filling the space created between the lower back and the back of the chair when in a sitting position. These pillows are generally used to support the lower back while driving or sitting, such as in an office chair. Orthopedic pillows are similar to memory foam pillows.\n\nDecorative pillows serve a dual purpose. They often have fancy cover material which serves to decorate the room where they are found. Since decorative textiles are commonly 54 inches in width, many decorative pillows finish about 17x17 inches. (54/3 = 18 less seam allowance) When used to decorate a fully made up bed, decorative pillows are likely thrown aside at bedtime, since they are not covered with a washable pillow case, thus, while found on the bed, they are primarily there for decoration, hence they fall under this category. These pillows may be custom made, as well as made by freelancers.\n\nDecorative pillows are also found on furnishings in more public parts of the home, such as sofas, chairs and window seats. Here, their common use may overlap both orthopedic and bed pillows. For example, unless a person has some particular medical condition, they will likely use a handy decorative pillow for lumbar support, as needed, while seated on a sofa. Likewise, for the occasional nap, decorative pillows are handy for supporting the head or neck, even though they may not be covered with a pillow case, as are bed pillows.\n\nThere are five common synonyms for decorative pillows which are descriptive of their use in the home. \"Accent\" pillows emphasize or accent some other part of the home decor. The terms \"sofa pillow\" and \"couch pillow\" refer to the place these decorative pillows are likely found. The terms \"toss pillow\" and \"throw pillow\" may refer to the way they generally arrive in their places.\n\nNovelty pillows are shaped like humorous objects (a banana, tweety bird, a human leg, a chainsaw, a dill pickle, a former president) and are meant to brighten up and add humor to a room or lounge area.\n\nTent-flap pillows are placed at the front of a stack of pillows decorating a bed. \"Tent flap\" is the term used to describe a separate flap of fabric that is attached at the top of the pillow and folds down over the face. The tent flap can be loose or tacked down; if the flap is loose a decorative tassel or bead is usually used to weigh the flap down so it hangs properly.\n\nFloor pillows are another subset of decorative pillows. These pillows often finish 26×26 inches (one half of the width of the textile, less seam allowance)\n\nIn many parts of the world, pillows have cultural significance, and references have extended to a wide variety of other uses, forms and activities.\n\nIn some cultures, pillows have forms for hugging. An example is the \"Dakimakura\", a kind of \"hugging pillow\" originating in Japan that have been endowed with anthropomorphic and zoomorphic qualities and pop culture references for additional psychological comfort. Other types of hugging pillows are more practical, such as the \"Guling\", long hugging pillow originating from Indonesia and the \"Abrazador\", a long hugging pillow originating from the Philippines. A \"husband pillow\" (also known as a boyfriend pillow) is a large, high-backed pillow with two \"arms.\" It is used to prop the user upright while in bed or on the floor, as for reading or watching television. Because of this common use, a husband pillow is also called a reading pillow.\n\nA pillow fight is a common game mostly played by young children (but also by teens and adults) in which they engage in mock physical conflict, using pillows as weapons.\n\nPillow fights are known to occur during children's sleepovers. Since pillows are usually soft, injuries rarely occur. The heft of a pillow can still knock a young person off balance, especially on a soft surface such as a bed, which is a common venue. In earlier eras, pillows would often break, shedding feathers throughout a room. Modern pillows tend to be stronger and are often filled with a solid block of artificial filling, so breakage occurs far less frequently.\n\nPillow talk refers to the relaxed, intimate conversation that often occurs between two sexual partners after sexual activity, usually accompanied by cuddling, caresses, and other physical intimacy. It is associated with honesty, sexual afterglow, and bonding. In the western world there are many cultural references to Pillow talk.\n\nChinese rock pillows () played an important role in ancient China. Made from jade, they were believed to translate the energy from the stone to the human brain. Originating in the Ming dynasty, this piece of material was trusted to cure headaches or depression, or simply to better the intelligence of those who use it. It was more common among royalty because it was expensive and rare. Families often married off children based on the quality and intricacy of these pillows. Today, rock pillows are still commonly used during the hot summer months in China.\n\nCushioning designed for specific parts of the body are sometimes called pillows. An example is Eye pillows which are designed to comfort the eyes. Mouse pads wrist rests though not called pillows, are another examples.\n\nA sex pillow is a specially-designed and typically firm pillow used to enhance sexual intercourse. An ordinary firm pillow, however, may be used in place of a special one. Some contain a high-density urethane core to balance firm support with softness. In addition to more common pillow shapes, there are wedge-shaped, ramp-shaped, prism-shaped, etc. pillows which lend themselves to various sexual positions, some of which might be difficult or uncomfortable without them.\n\nA pillow menu is a list of available pillows provided by a hotel to guests, usually free of charge. It allows guests to make an alternative pillow choice. Some common pillow alternatives are memory foam, buckwheat hull, and hypoallergenic. Some hotels offer pillows to treat specific conditions such as headaches or stress.\n\n\n"}
{"id": "955745", "url": "https://en.wikipedia.org/wiki?curid=955745", "title": "RV-C", "text": "RV-C\n\nRV-C is a communications protocol based on the Controller Area Network bus. The protocol is used in recreation vehicles to allow house and chassis components to communicate. RV-C is used for control, coordination, and diagnostics, in a multi-vendor environment.\n\nRV-C was initially developed by the Recreational Vehicle Industry Association. The first formal specification was approved in 2005, and the first RV-C products were marketed at that time. The RVIA has continued to refine and expand the protocol, and in 2008 applied to ISO with the intention of opening the RV-C protocol to the world community.\n\nIn 2006 the first RV-C-equipped RVs were sold in America. The leading adopters were Country Coach, Foretravel, Newell Coach, and Western RV. RV-C-compliant components for these RVs were manufactured by Valid Manufacturing Ltd., Automated Engineering Corp, SilverLeaf Electronics, and HWH Corporation.\n\nIn 2007, the RVIA hosted a Network Fest at their main industry show. The Fest was an educational event featuring over two dozen RV-C compliant products from 14 exhibitors.\n\nRV-C is based on Controller Area Network, and operates at a bus speed of 250 kbit/s. Data is contained in packets consisting of a header and eight data bytes. The header contains an 8-bit Source Address and a 17-bit Parameter Group Number, as well as a few additional bits. The total bus capacity is approximately 2500 data packets per second, although in practice bus loads are much lower.\n\nRV-C is peer-to-peer. Each CAN transceiver on the network requires a unique source address, which can be assigned either dynamically or statically. Data packets are prioritized based on their contents, not the device.\n\nThe Application Layer details the Parameter Group Numbers, which uniquely identifies how the contents of the data packet are to be interpreted. The primary work of the RV-C committee is the creation of new Parameter Groups as new components are introduced in the RV marketplace.\n\nTo be considered RV-C-compliant, a device must support certain PGNs. These are\n\n\nA key concept in RV-C is the instance. In an RV, multiple \"instances\" of a device are common. RV-C handles this using a unique method in which an instance number is assigned to each physical unit of a certain type.\n\nAn idea that underlies much of RV-C's design is that \"every data packet stands alone\". That is, with very few exceptions, all the information necessary to interpret a data packet is contained within that packet. This greatly reduces the memory and speed required for a microprocessor to implement the protocol. In general, the committee has been intent on keeping the cost of implementation to a minimum.\n\nRV-C draws heavily from the SAE J1939 protocol. The primary differences between J1939 and RV-C are:\n\n\nThe RVIA web site for RV-C\n"}
{"id": "40277999", "url": "https://en.wikipedia.org/wiki?curid=40277999", "title": "Star Fireworks", "text": "Star Fireworks\n\nStar Fireworks is a British company that stages professional fireworks displays and special effects for events. It specializes in providing choreographed firework sequences for film and television use. It was formed in 1971. It was known as Bracknell Fireworks until the name of the company was changed in 2005. Its main offices are in Bracknell, Berkshire.\n\nStar Fireworks has become well known for its fireworks and special effects for the UK version of the reality television show \"Big Brother\". It has fired finale fireworks for every Big Brother series at Elstree Studios since 2002.\n\nIn 2013, Star Fireworks won the British Firework Championships Champion of Champions competition in Plymouth, Devon, in which previous winners from the past six years are invited to compete. The company was also crowned British Champions in 2010. In 2008, they won the British Firework Championships Northern heat in Salford, Greater Manchester and placed second in the main competition. The company has also won or been runner-up in various other UK firework competitions.\n\nStar Fireworks are members of the British Pyrotechnists Association (BPA). The company directors are Members of the Institute of Explosives Engineers.\n\nCompany staff are noted for their heavy involvement in representing the UK professional fireworks display industry in the UK and in Europe. Andy Hubble, a director of Star Fireworks, used to be a consultant and media representative to the British Government's former Department of Trade and Industry (DTI), on its firework safety campaign, from 1995 to 2005. He chaired the British Pyrotechnists Association from 2008 to 2010 and a second term from 2013 to 2016, sitting on various Government committees concerning professional fireworks and explosives safety and control in the UK. Andy has sat on the Council of the Institute of Explosives Engineers since 2016. He has represented UK interests as part of a multi-national team producing harmonized firework regulations across the EU sitting on the European Committee for Standardization (CEN) Technical Committee for Category 4 Fireworks (professional fireworks). He is also a regular columnist for \"Fireworks (magazine)\" magazine.\n\nIn 2013 Star Fireworks took action when changes in the European Pyrotechnic Directive threatened their business and others with closure, gaining the support of local Member of Parliament (MP) Dr Phillip Lee and the Consumer Affairs Minister Jo Swinson MP who, with the industry, found a solution to the problem.\n\n"}
{"id": "17959575", "url": "https://en.wikipedia.org/wiki?curid=17959575", "title": "Suitport", "text": "Suitport\n\nA suitport or suitlock is an alternative technology to an airlock, designed for use in hazardous environments and in human spaceflight, especially planetary surface exploration. Suitports present advantages over traditional airlocks in terms of mass, volume, and ability to mitigate contamination by—and of—the local environment.\n\nIn a suitport system, a rear-entry space suit is attached and sealed against the outside of a spacecraft, space habitat, or pressurized rover, facing outward. To begin an extra-vehicular activity (EVA), an astronaut in shirt-sleeves first enters the suit feet-first from inside the pressurized environment, and closes and seals the space suit backpack and the vehicle's hatch (which seals to the backpack for dust containment). The astronaut then unseals and separates the suit from the vehicle, and is ready to perform an EVA.\n\nTo re-enter the vehicle, the astronaut backs up to the suitport and seals the suit to the vehicle, before opening the hatch and backpack and transferring back into the vehicle. If the vehicle and suit do not operate at the same pressure, it will be necessary to equalize the two pressures before the hatch can be opened.\n\nSuitports carry three major advantages over traditional airlocks. First, the mass and volume required for a suitport is significantly less than that required for an airlock. Launch mass is at a premium in modern chemical rocket-powered launch vehicles, at an estimated cost of US$60,000 per kilogram delivered to the lunar surface.\n\nSecondly, suitports can eliminate or minimize the problem of dust migration. During the Apollo program, it was discovered that the lunar soil is electrically charged, and adheres readily to any surface with which it comes into contact, a problem magnified by the sharp, barb-like shapes of the dust particles. Lunar dust may be harmful in several ways:\n\n\nDuring the Apollo missions, the astronauts donned their space suits inside the Apollo Lunar Module cabin, which was then depressurized to allow them to exit the vehicle. Upon the end of EVA, the astronauts would re-enter the cabin in their suits, bringing with them a great deal of dust which had adhered to the suits. Several astronauts reported a \"gunpowder\" smell and respiratory and/or eye irritation upon opening their helmets and being exposed to the dust.\n\nWhen the suit is attached to the vehicle, any dust which may have adhered to the backpack of the suit is sealed between the outside of the backpack and the vehicle-side hatch. Any dust on the suit that is not on the backpack remains sealed outside the vehicle. Likewise, the suitport prevents contamination of the external environment by microbes carried by the astronaut.\n\nAdditionally, the suitports significantly reduce the ingress and egress time, and virtually remove the need of pumpdown of the airlock, which normally is either associated with air loss, or requires heavy and complex pumping machinery as the only space that needs to be pressurized is the area between the vehicle hatch and the life-support backpack, and even that only in case of need for repairs, decontamination and refitting of the suit.\n\nDisadvantages of suitports include the additional mass of the interface on the rear of the space suit which may be more than 4.5 kg, and increased mechanical complexity, potentially reducing the overall reliability of the EVA system. According to NASA's Exploration Systems Mission Directorate, disadvantages of suitports also include:\n\nThe first EVA rear entry space suit was developed at NPP Zvezda in 1962. The suitport concept was suggested for use in the Soviet manned Moon program. A patent for a suitport was first filed in 1980 in the Soviet Union, by Isaak Abramov of Zvezda and Yuri Nazarov of CKBM.\n\nA US patent for a suitport was first filed in 1987 by Marc M. Cohen of NASA's Ames Research Center. Further patents were filed in 1996 by Philip Culbertson Jr., and in 2003 by Joerg Boettcher, Stephen Ransom, and Frank Steinsiek.\n\nAs of 1995, suitports have found a practical, terrestrial application as part of a NASA Ames hazardous materials vehicle, where the use of the suitport eliminates the need to decontaminate the hazmat suit before doffing. A suitport prototype built by Brand Griffin has been used in a simulated lunar gravity test on board NASA Johnson's C-135 aircraft.\n\nSuitports may find use as part of future NASA projects aimed at achieving a return to the Moon and manned exploration of Mars. NASA's conceptual Space Exploration Vehicle has two suitports on the back of the craft.\n\nTesting has been taking place in combination with the Z-1 prototype spacesuit inside NASA's human-rated thermal vacuum chamber B at the Johnson Space Center. Early unmanned tests of the suitport were conducted in June 2012. The first manned tests of the suitport occurred on 16 and 18 July 2012; during these manned tests the spacesuit was kept at a pressure of with the chamber pressure at approximately , equivalent to an altitude of . Future manned tests were planned for September and August 2012, where NASA planned to keep the spacesuit at a pressure of and the vacuum chamber at roughly . Suitports may eventually be tested on the International Space Station.\n\n"}
{"id": "223407", "url": "https://en.wikipedia.org/wiki?curid=223407", "title": "Switched-mode power supply", "text": "Switched-mode power supply\n\nA switched-mode power supply (switching-mode power supply, switch-mode power supply, switched power supply, SMPS, or switcher) is an electronic power supply that incorporates a switching regulator to convert electrical power efficiently. Like other power supplies, an SMPS transfers power from a DC or AC source (often mains power) to DC loads, such as a personal computer, while converting voltage and current characteristics. Unlike a linear power supply, the pass transistor of a switching-mode supply continually switches between low-dissipation, full-on and full-off states, and spends very little time in the high dissipation transitions, which minimizes wasted energy. Ideally, a switched-mode power supply dissipates no power. Voltage regulation is achieved by varying the ratio of on-to-off time. In contrast, a linear power supply regulates the output voltage by continually dissipating power in the pass transistor. This higher power conversion efficiency is an important advantage of a switched-mode power supply. Switched-mode power supplies may also be substantially smaller and lighter than a linear supply due to the smaller transformer size and weight.\n\nSwitching regulators are used as replacements for linear regulators when higher efficiency, smaller size or lighter weight are required. They are, however, more complicated; their switching currents can cause electrical noise problems if not carefully suppressed, and simple designs may have a poor power factor.\n\n\nA linear regulator provides the desired output voltage by dissipating excess power in ohmic losses (e.g., in a resistor or in the collector–emitter region of a pass transistor in its active mode). A linear regulator regulates either output voltage or current by dissipating the excess electric power in the form of heat, and hence its maximum power efficiency is voltage-out/voltage-in since the volt difference is wasted.\n\nIn contrast, a switched-mode power supply changes output voltage and current by switching ideally lossless storage elements, such as inductors and capacitors, between different electrical configurations. Ideal switching elements (approximated by transistors operated outside of their active mode) have no resistance when \"on\" and carry no current when \"off\", and so converters with ideal components would operate with 100% efficiency (i.e., all input power is delivered to the load; no power is wasted as dissipated heat).\nFor example, if a DC source, an inductor, a switch, and the corresponding electrical ground are placed in series and the switch is driven by a square wave, the peak-to-peak voltage of the waveform measured across the switch can exceed the input voltage from the DC source. This is because the inductor responds to changes in current by inducing its own voltage to counter the change in current, and this voltage adds to the source voltage while the switch is open. If a diode-and-capacitor combination is placed in parallel to the switch, the peak voltage can be stored in the capacitor, and the capacitor can be used as a DC source with an output voltage greater than the DC voltage driving the circuit. This boost converter acts like a step-up transformer for DC signals. A buck–boost converter works in a similar manner, but yields an output voltage which is opposite in polarity to the input voltage. Other buck circuits exist to boost the average output current with a reduction of voltage.\n\nIn an SMPS, the output current flow depends on the input power signal, the storage elements and circuit topologies used, and also on the pattern used (e.g., pulse-width modulation with an adjustable duty cycle) to drive the switching elements. The spectral density of these switching waveforms has energy concentrated at relatively high frequencies. As such, switching transients and ripple introduced onto the output waveforms can be filtered with a small LC filter.\n\nThe main advantage of the switching power supply is greater efficiency than linear regulators because the switching transistor dissipates little power when acting as a switch.\n\nOther advantages include smaller size and lighter weight from the elimination of heavy line-frequency transformers, and comparable heat generation. Standby power loss is often much less than transformers.\n\nDisadvantages include greater complexity, the generation of high-amplitude, high-frequency energy that the low-pass filter must block to avoid electromagnetic interference (EMI), a ripple voltage at the switching frequency and the harmonic frequencies thereof.\n\nVery low cost SMPSs may couple electrical switching noise back onto the mains power line, causing interference with A/V equipment connected to the same phase. Non-power-factor-corrected SMPSs also cause harmonic distortion.\n\nThere are two main types of regulated power supplies available: SMPS and linear. The following table compares linear regulated and unregulated AC-to-DC supplies with switching regulators in general:\n\nIf the SMPS has an AC input, then the first stage is to convert the input to DC. This is called \"rectification\". An SMPS with a DC input does not require this stage. In some power supplies (mostly computer ATX power supplies), the rectifier circuit can be configured as a voltage doubler by the addition of a switch operated either manually or automatically. This feature permits operation from power sources that are normally at 115 V or at 230 V. The rectifier produces an unregulated DC voltage which is then sent to a large filter capacitor. The current drawn from the mains supply by this rectifier circuit occurs in short pulses around the AC voltage peaks. These pulses have significant high frequency energy which reduces the power factor. To correct for this, many newer SMPS will use a special PFC circuit to make the input current follow the sinusoidal shape of the AC input voltage, correcting the power factor. Power supplies that use active PFC usually are auto-ranging, supporting input voltages from , with no input voltage selector switch.\n\nAn SMPS designed for AC input can usually be run from a DC supply, because the DC would pass through the rectifier unchanged. If the power supply is designed for and has no voltage selector switch, the required DC voltage would be (115 × √). This type of use may be harmful to the rectifier stage, however, as it will only use half of diodes in the rectifier for the full load. This could possibly result in overheating of these components, causing them to fail prematurely. On the other hand, if the power supply has a voltage selector switch, based on the Delon circuit, for 115/230V (computer ATX power supplies typically are in this category), the selector switch would have to be put in the position, and the required voltage would be (230 × √). The diodes in this type of power supply will handle the DC current just fine because they are rated to handle double the nominal input current when operated in the mode, due to the operation of the voltage doubler. This is because the doubler, when in operation, uses only half of the bridge rectifier and runs twice as much current through it.\n\nThe inverter stage converts DC, whether directly from the input or from the rectifier stage described above, to AC by running it through a power oscillator, whose output transformer is very small with few windings at a frequency of tens or hundreds of kilohertz. The frequency is usually chosen to be above 20 kHz, to make it inaudible to humans. The switching is implemented as a multistage (to achieve high gain) MOSFET amplifier. MOSFETs are a type of transistor with a low on-resistance and a high current-handling capacity.\n\nIf the output is required to be isolated from the input, as is usually the case in mains power supplies, the inverted AC is used to drive the primary winding of a high-frequency transformer. This converts the voltage up or down to the required output level on its secondary winding. The output transformer in the block diagram serves this purpose.\n\nIf a DC output is required, the AC output from the transformer is rectified. For output voltages above ten volts or so, ordinary silicon diodes are commonly used. For lower voltages, Schottky diodes are commonly used as the rectifier elements; they have the advantages of faster recovery times than silicon diodes (allowing low-loss operation at higher frequencies) and a lower voltage drop when conducting. For even lower output voltages, MOSFETs may be used as synchronous rectifiers; compared to Schottky diodes, these have even lower conducting state voltage drops.\n\nThe rectified output is then smoothed by a filter consisting of inductors and capacitors. For higher switching frequencies, components with lower capacitance and inductance are needed.\n\nSimpler, non-isolated power supplies contain an inductor instead of a transformer. This type includes \"boost converters\", \"buck converters\", and the \"buck-boost converters\". These belong to the simplest class of single input, single output converters which use one inductor and one active switch. The buck converter reduces the input voltage in direct proportion to the ratio of conductive time to the total switching period, called the duty cycle. For example an ideal buck converter with a 10 V input operating at a 50% duty cycle will produce an average output voltage of 5 V. A feedback control loop is employed to regulate the output voltage by varying the duty cycle to compensate for variations in input voltage. The output voltage of a boost converter is always greater than the input voltage and the buck-boost output voltage is inverted but can be greater than, equal to, or less than the magnitude of its input voltage. There are many variations and extensions to this class of converters but these three form the basis of almost all isolated and non-isolated DC to DC converters. By adding a second inductor the Ćuk and SEPIC converters can be implemented, or, by adding additional active switches, various bridge converters can be realized.\n\nOther types of SMPSs use a capacitor-diode voltage multiplier instead of inductors and transformers. These are mostly used for generating high voltages at low currents (\"Cockcroft-Walton generator\"). The low voltage variant is called charge pump.\n\nA feedback circuit monitors the output voltage and compares it with a reference voltage, as shown in the block diagram above. Depending on design and safety requirements, the controller may contain an isolation mechanism (such as an opto-coupler) to isolate it from the DC output. Switching supplies in computers, TVs and VCRs have these opto-couplers to tightly control the output voltage.\n\n\"Open-loop regulators\" do not have a feedback circuit. Instead, they rely on feeding a constant voltage to the input of the transformer or inductor, and assume that the output will be correct. Regulated designs compensate for the impedance of the transformer or coil. Monopolar designs also compensate for the magnetic hysteresis of the core.\n\nThe feedback circuit needs power to run before it can generate power, so an additional non-switching power-supply for stand-by is added.\n\nAny switched-mode power supply that gets its power from an AC power line (called an \"off-line\" converter) requires a transformer for galvanic isolation. Some DC-to-DC converters may also include a transformer, although isolation may not be critical in these cases. SMPS transformers run at high frequency. Most of the cost savings (and space savings) in off-line power supplies result from the smaller size of the high frequency transformer compared to the 50/60 Hz transformers formerly used. There are additional design tradeoffs.\n\nThe terminal voltage of a transformer is proportional to the product of the core area, magnetic flux, and frequency. By using a much higher frequency, the core area (and so the mass of the core) can be greatly reduced. However, core losses increase at higher frequencies. Cores generally use ferrite material which has a low loss at the high frequencies and high flux densities used. The laminated iron cores of lower-frequency (<400 Hz) transformers would be unacceptably lossy at switching frequencies of a few kilohertz. Also, more energy is lost during transitions of the switching semiconductor at higher frequencies. Furthermore, more attention to the physical layout of the circuit board is required as parasitics become more significant, and the amount of electromagnetic interference will be more pronounced.\n\nAt low frequencies (such as the line frequency of 50 or 60 Hz), designers can usually ignore the skin effect. For these frequencies, the skin effect is only significant when the conductors are large, more than in diameter.\n\nSwitching power supplies must pay more attention to the skin effect because it is a source of power loss. At 500 kHz, the skin depth in copper is about – a dimension smaller than the typical wires used in a power supply. The effective resistance of conductors increases, because current concentrates near the surface of the conductor and the inner portion carries less current than at low frequencies.\n\nThe skin effect is exacerbated by the harmonics present in the high speed PWM switching waveforms. The appropriate skin depth is not just the depth at the fundamental, but also the skin depths at the harmonics.\n\nIn addition to the skin effect, there is also a proximity effect, which is another source of power loss.\n\nSimple off-line switched mode power supplies incorporate a simple full-wave rectifier connected to a large energy storing capacitor. Such SMPSs draw current from the AC line in short pulses when the mains instantaneous voltage exceeds the voltage across this capacitor. During the remaining portion of the AC cycle the capacitor provides energy to the power supply.\n\nAs a result, the input current of such basic switched mode power supplies has high harmonic content and relatively low power factor. This creates extra load on utility lines, increases heating of building wiring, the utility transformers, and standard AC electric motors, and may cause stability problems in some applications such as in emergency generator systems or aircraft generators. Harmonics can be removed by filtering, but the filters are expensive. Unlike displacement power factor created by linear inductive or capacitive loads, this distortion cannot be corrected by addition of a single linear component. Additional circuits are required to counteract the effect of the brief current pulses. Putting a current regulated boost chopper stage after the off-line rectifier (to charge the storage capacitor) can correct the power factor, but increases the complexity and cost.\n\nIn 2001, the European Union put into effect the standard IEC/EN61000-3-2 to set limits on the harmonics of the AC input current up to the 40th harmonic for equipment above 75 W. The standard defines four classes of equipment depending on its type and current waveform. The most rigorous limits (class D) are established for personal computers, computer monitors, and TV receivers. To comply with these requirements, modern switched-mode power supplies normally include an additional power factor correction (PFC) stage.\n\nSwitched-mode power supplies can be classified according to the circuit topology. The most important distinction is between isolated converters and non-isolated ones.\n\nNon-isolated converters are simplest, with the three basic types using a single inductor for energy storage. In the voltage relation column, \"D\" is the duty cycle of the converter, and can vary from 0 to 1. The input voltage (V) is assumed to be greater than zero; if it is negative, for consistency, negate the output voltage (V).\n\nWhen equipment is human-accessible, voltage and power limits of <=42.4 V peak/60 V dc and 250 VA apply for safety certification (UL, CSA, VDE approval).\n\nThe buck, boost, and buck-boost topologies are all strongly related. Input, output and ground come together at one point. One of the three passes through an inductor on the way, while the other two pass through switches. One of the two switches must be active (e.g., a transistor), while the other can be a diode. Sometimes, the topology can be changed simply by re-labeling the connections. A 12 V input, 5 V output buck converter can be converted to a 7 V input, −5 V output buck-boost by grounding the \"output\" and taking the output from the \"ground\" pin.\n\nLikewise, SEPIC and Zeta converters are both minor rearrangements of the Ćuk converter.\n\nThe \"neutral point clamped\" (NPC) topology is used in power supplies and active filters and is mentioned here for completeness.\n\nSwitchers become less efficient as duty cycles become extremely short. For large voltage changes, a transformer (isolated) topology may be better.\n\nAll isolated topologies include a transformer, and thus can produce an output of higher or lower voltage than the input by adjusting the turns ratio. For some topologies, multiple windings can be placed on the transformer to produce multiple output voltages. Some converters use the transformer for energy storage, while others use a separate inductor.\n\n\nIn a quasi-resonant zero-current/zero-voltage switch (ZCS/ZVS) \"each switch cycle delivers a quantized 'packet' of energy to the converter output, and switch turn-on and turn-off occurs at zero current and voltage, resulting in an essentially lossless switch.\" Quasi-resonant switching, also known as \"valley switching\", reduces EMI in the power supply by two methods:\n\nHigher input voltage and synchronous rectification mode makes the conversion process more efficient. The power consumption of the controller also has to be taken into account. Higher switching frequency allows component sizes to be shrunk, but can produce more RFI. A resonant forward converter produces the lowest EMI of any SMPS approach because it uses a soft-switching resonant waveform compared with conventional hard switching.\n\n\"For failure in switching components, circuit board and so on read the failure modes of electronics article.\"\n\nPower supplies which use capacitors suffering from the capacitor plague may experience premature failure when the capacitance drops to 4% of the original value. This usually causes the switching semiconductor to fail in a conductive way. That may expose connected loads to the full input volt and current, and precipitate wild oscillations in output.\n\nFailure of the switching transistor is common. Due to the large switching voltages this transistor must handle (around for a mains supply), these transistors often short out, in turn immediately blowing the main internal power fuse.\n\nThe main filter capacitor will often store up to long after the power cord has been removed from the wall. Not all power supplies contain a small \"bleeder\" resistor to slowly discharge this capacitor. Any contact with this capacitor may result in a severe electrical shock.\n\nThe primary and secondary sides may be connected with a capacitor to reduce EMI and compensate for various capacitive couplings in the converter circuit, where the transformer is one. This may result in electric shock in some cases. The current flowing from line or neutral through a resistor to any accessible part must, according to , be less than for IT equipment.\n\nSwitched-mode power supply units (PSUs) in domestic products such as personal computers often have universal inputs, meaning that they can accept power from mains supplies throughout the world, although a manual voltage range switch may be required. Switch-mode power supplies can tolerate a wide range of power frequencies and voltages.\n\nDue to their high volumes mobile phone chargers have always been particularly cost sensitive. The first chargers were linear power supplies, but they quickly moved to the cost effective ringing choke converter (RCC) SMPS topology, when new levels of efficiency were required. Recently, the demand for even lower no-load power requirements in the application has meant that flyback topology is being used more widely; primary side sensing flyback controllers are also helping to cut the bill of materials (BOM) by removing secondary-side sensing components such as optocouplers.\n\nSwitched-mode power supplies are used for DC to DC conversion as well. In automobiles where heavy vehicles use a nominal cranking supply, 12V for accessories may be furnished through a DC/DC switch-mode supply. This has the advantage over tapping the battery at the 12V position (using half the cells) that all the 12V load is evenly divided over all cells of the 24V battery. In industrial settings such as telecommunications racks, bulk power may be distributed at a low DC voltage (from a battery back up system, for example) and individual equipment items will have DC/DC switched-mode converters to supply whatever voltages are needed.\n\nA common use for switched-mode power supplies is as extra-low-voltage sources for lighting, and for this application they are often called \"electronic transformers\".\n\nThe term \"switchmode\" was widely used until Motorola claimed ownership of the trademark SWITCHMODE for products aimed at the switching-mode power supply market and started to enforce their trademark. \"Switching-mode power supply\", \"switching power supply\", and \"switching regulator\" refer to this type of power supply.\n\n\n\n"}
{"id": "952948", "url": "https://en.wikipedia.org/wiki?curid=952948", "title": "Traffic cone", "text": "Traffic cone\n\nTraffic cones, also called pylons, witches' hats, road cones, highway cones, safety cones, channelizing devices, or construction cones, are usually cone-shaped markers that are placed on roads or footpaths to temporarily redirect traffic in a safe manner. They are often used to create separation or merge lanes during road construction projects or automobile accidents, although heavier, more permanent markers or signs are used if the diversion is to stay in place for a long period of time.\n\nTraffic cones were invented by Charles D. Scanlon, an American who got the idea while working as a painter for the Street Painting Department of the City of Los Angeles. The patent for his invention was granted in 1943.\n\nTraffic cones were first used in the United Kingdom in 1958, when the M6 motorway opened. These traffic cones were a substitute for red lantern paraffin burners being used during construction on the Preston Bypass. In 1961, David Morgan of Burford, Oxfordshire, UK believes that he constructed the first experimental plastic traffic cones, which replaced pyramid-shaped wooden ones previously used.\n\nIn the United States on May 1, 1959 the Pacific Gas and Electric Company in Oakland, California adopted the policy of placing the orange safety cones at left front and the left rear corners of their service trucks while parked on the street to increase visibility and safety for the workers. This policy was implemented as the result of a suggestion by their employee, Russell Storch, a cable splicer. He was awarded $45 for his suggestion. This policy is still in use today.\n\nAlthough originally made of concrete, today's versions are more commonly brightly colored thermoplastic or rubber cones. Recycled PVCs from bottles can be used to create modern traffic cones. Not all traffic cones are conical. Pillar-shaped movable bollards fulfill a similar function.\n\nTraffic cones are typically used outdoors during road work or other situations requiring traffic redirection or advance warning of hazards or dangers, or the prevention of traffic. Traffic cones are also used to mark where children are playing or to block off an area.\nFor night time use or low-light situations traffic cones are usually fitted with a retroreflective sleeve to increase visibility. On occasion, traffic cones may also be fitted with flashing lights for the same reason.\n\nIn the US, cones are required by the US Federal Highway Administration's \"Manual on Uniform Traffic Control Devices\" (MUTCD) to be fitted with reflective white bands to increase night-time visibility. Reflective collars, white strips made from white reflective plastic, slip over cones snugly, and tape or adhesive can be used to permanently attach the collars to the cones.\n\nTraffic cones are designed to be highly visible and easily movable. Various sizes are used, commonly ranging from around to a little over . Traffic cones come in many different colors, with orange, yellow, pink, and red being the most common colors due to their brightness. Others come in green and blue, and may also have a retroreflective strip (commonly known as \"flash tape\") to increase their visibility.\n\nTypical traffic cones are fluorescent \"safety\" orange, as well as lime green. Traffic cones also commonly come with reflective striping around them, to increase visibility.\n\nIn the United States, they come in such sizes as:\n\nCones are easy to move or remove. Where sturdier (and larger) markers are needed, construction sites use traffic barrels (plastic orange barrels with reflective stripes, normally about the same size as a drum). When a lane closure must also be a physical barrier against cars accidentally crossing it, a Jersey barrier is preferred. See also Fitch Barrier.\n\nIn many countries such as Australia or American states such as California, traffic barrels are rarely seen. Devices called bollards are used instead of cones where larger and sturdier warning or delineation devices are needed. Typically, bollards are high fluorescent orange posts with reflective sleeve and heavyweight rubber bases. Larger devices such as barrier boards may be used instead of cones where larger areas need to be excluded or for longer periods. In Canada they are often referred to as pylons.\n\nCones are used to lay out courses for autocross competitions.\n\nCones are also frequently used in indoor public spaces to mark off areas which are closed to pedestrians, such as a restroom being out of order, or to denote a dangerous condition, such as a slippery floor. They can be used on school playgrounds to limit areas of a playing field, and on ice rinks to define class, private party, or private lesson areas. Some of the cones used for this purpose are miniature, as small as tall, and some are disposable full-size cones made of biodegradable paper.\n\nBeing distinctive, easily portable and usually left unguarded, traffic cones are often stolen. Students are frequently blamed, to the extent that the British National Union of Students has attempted to play down this \"outdated stereotype\".\n\nThe term \"road cone\" is also commonly used in the construction industry as a lighthearted insult. It is used to describe an individual who spends most of the day just standing still, making no attempt to get involved in the work they should be doing.\n\nIn 2007 the artist Dennis Oppenheim commemorated the traffic cone with a monumental sculpture of five five-metre-tall cones. They were installed temporarily in Miami, Seattle's Olympic Sculpture Park, and Seoul, Korea. An orange-and-white cone is the logo used by VideoLAN (best known for its VLC media player software).\nGerman group Kraftwerk featured traffic cones on their first two albums, as well as in their concerts at the time.\nTraditionally, but unofficially, the Wellington Statue in Glasgow is decorated with a traffic cone. The presence of the cone is given as the reason the statue is in the Lonely Planet 1000 Ultimate Sights guide (at number 229) as a \"most bizarre monument\".\n\n\"The Traffic Cones\" is a Belgian TV series on Nickelodeon.\n\n"}
{"id": "47962855", "url": "https://en.wikipedia.org/wiki?curid=47962855", "title": "Visible file", "text": "Visible file\n\nA visible file, sometimes just called a kardex, after a prominent purveyor, is a filing system for overlapping cards fixed in shallow drawers.\n\nThe best-known version was commercialized by Kardex.\n\nThe Library Bureau company commercialized the very similar L. B. Speedac.\n\nAnother brand was the Index Visible System.\n"}
{"id": "42713269", "url": "https://en.wikipedia.org/wiki?curid=42713269", "title": "Wedge-based mechanical exfoliation", "text": "Wedge-based mechanical exfoliation\n\nWedge-based mechanical exfoliation is a method that involves the use of an ultra-sharp single crystal diamond wedge to penetrate inside a material and cleave a thin layer of material. It was proposed to produce few layers of graphene from a bulk highly ordered pyrolytic graphite (HOPG).\n\nMolecular dynamics simulations studies have been performed to understand how and under what conditions graphene layers separate, fold and shear during the wedge-based mechanical exfoliation machining technique. Molecular simulations of initial wedge engagement show that the entry location of the wedge tip vis-a-vis the nearest graphene layer plays a key role in determining whether layers separate or fold and which layers and how\nmany of them fold.\n"}
{"id": "49945789", "url": "https://en.wikipedia.org/wiki?curid=49945789", "title": "West Baltimore Innovation District", "text": "West Baltimore Innovation District\n\nThe West Baltimore Innovation Village District is a neighborhood district of Baltimore City that will specialize in attracting startup companies and other employers to West Baltimore. Following the Death of Freddie Gray and the subsequent 2015 Baltimore protests, government leaders decided to launch the innovation district as a way to attract redevelopment and revitalization to the areas hardest hit by the protests. The district includes the neighborhoods of Mondawmin, Coppin Heights, Penn-North, Reservoir Hill and Bolton Hill.\n\nEfforts to form the district originated from the community organization known as the Mount Royal Community Development Corporation (MRCDC). Similar innovation districts have been formed in neighborhoods like University City in Philadelphia and the South Boston Seaport District. The West Baltimore Innovation District neighbors the nearby Station North Arts and Entertainment District and includes access to numerous forms of transportation including the Baltimore Metro Subway and Baltimore Light Rail.\n\n\n"}
{"id": "22434448", "url": "https://en.wikipedia.org/wiki?curid=22434448", "title": "YouView", "text": "YouView\n\nYouView is a hybrid television platform in the United Kingdom, developed by a partnership of three telecommunications operators and four broadcasters. It was formed from a project originally titled \"Project Canvas\", which was rebranded as YouView in September 2010. The service was due to launch by the end of that year, but was delayed until 4 July 2012. At its launch, YouView's chairman Alan Sugar stated his ambition for the service to replace Freeview devices.\n\nYouView provides access to free-to-air digital terrestrial television (DTT) channels (both DVB-T and DVB-T2 channels in common with the Freeview television platform) and to TV on demand (catch-up TV) services via a 'hybrid' set-top box (STB) purchased by users, connected with both a broadband internet connection \"and\" a normal television aerial. No contract is required, and there is no subscription charge. Catch-up and on-demand content is delivered over the internet, which may be chargeable by the Internet service provider (ISP), or subject to limits and fair usage clauses.\n\nBT and TalkTalk have each partnered with YouView, allowing BT subsidiaries BT TV and Plusnet TV, and TalkTalk TV customers to access the service, along with BT's and TalkTalk's own offerings. Both companies provide customers with their own branded YouView set-top box.\n\nThe YouView hardware is a digital terrestrial television DVB-T2 HD set-top box that provides viewing and recording of all free-to-air channels available on digital terrestrial television in the United Kingdom. There is no access to some streamed channels in the 225-256 range on Freeview, as YouView lacks support for the MHEG-5 integration channel used by these channels to provide them over IP.\n\nAdditional content is provided over a broadband internet connection, and can be accessed via the YouView menu and EPG via the search function.\n\nThere are ten free catch up TV players available, providing access to TV programming and movies: BBC iPlayer, ITV Hub, All 4, My5, STV Player, Milkshake!, UKTV Play, S4C, POP FUN and Quest. The content on ITV Hub varies by region. STV Player is only available in STV Group regions. \n\nThere are an additional three pay TV on-demand players: Netflix, Now TV, and Sky Store from Sky. \n\nBT Infinity subscribers can access additional live IPTV channels via the EPG. Customers who subscribe to BT TV will receive either a YouView+ (YouView plus) box (with recording capabilities), or a standard YouView box (no recording capabilities), or a YouView Ultra HD box - depending on the subscription type taken. They can then view on-demand content through the BT TV Player. \n\nSubscribers to BT Infinity who are not BT TV subscribers, and have bought their YouView boxes from retail vendors, will have their product's YouView branding replaced by BT TV branding. They will also get the BT TV player on the starting page, which they cannot use because they are not BT TV subscribers. This has not been popular with these users, and there appears to be no way of restoring the YouView branding or removing the BT TV Player.\n\nA free YouView box is provided to TalkTalk Plus or Essentials customers taking out a fixed term contract and allow access to the content previously available from TalkTalk TV. Content that TalkTalk provides, such as film rentals and box sets, are available from the TalkTalk Player application within the YouView menu bar. In the TalkTalk Player, there are 'boosts' that customers can buy for a minimum of one month, and customers can watch and record them from the main guide the same as Freeview channels. These boosts range from Sky premium channels, sports, TV box sets, films, and foreign language channels. TalkTalk use their own brand of Huawei-manufactured boxes, with the TalkTalk logo on the front, a remote control with a direct button on the TalkTalk Player, and a 320 GB hard drive for Plus TV customers. Essentials customers cannot record, but can still pause live TV. Unlike BT, you do not need a fibre broadband package.\n\nOn 29 July 2013 it was revealed that the YouView partners intended to launch the service on smartphones, tablets, and smart TVs, three days after BSkyB announced the £10 Now TV set-top box which streams on demand content.\n\nProject Canvas was initially announced on 11 December 2008 as a partnership between the BBC, BT and ITV plc. It followed the failure of Project Kangaroo, a proposed video-on-demand service offering content from BBC Worldwide, ITV.com, and Channel 4's 4oD, which was blocked on competition grounds, and ended up as the SeeSaw internet television service. Project Canvas differed from Kangaroo, however, in that it was a proposed TV platform (a device that would deliver internet-connected TV), rather than a video-on-demand service (that would act as a single content portal, much like the music video equivalent VEVO).\n\nOn 30 July 2009, Project Canvas announced that Five had signed up to the project. On 9 July 2010, Five announced that it would not pursue further involvement in Project Canvas, pending a review of its digital investment strategy. Charles Constable, Director of Strategy at Five said: \"We continue to support the objectives of Project Canvas, and despite withdrawing our interest in the venture we believe it will be a critical part of our strategy for reaching consumers in the future.\" On 24 August 2010, Five re-joined Project Canvas, following their acquisition by Northern & Shell.\n\nOn 16 December 2009, Project Canvas announced that Channel 4 and TalkTalk had also signed up to the project.\n\nThe six partners invited any further expressions of interest from companies interested in becoming part of the joint venture. The canvas partners proposed that all prospective venture partners should be granted an equal proportion of shares in the new joint venture company. Interested parties had until 23 April 2010 to express their interest.\n\nOn 22 March 2010 transmission firm Arqiva joined as an equal partner in the project.\n\nOn 17 May 2010 the then Project Canvas director Richard Halton said: \"We have also put out an invitation for an eighth partner, and we would like a company that can add scale and expertise to the platform. It is a question of finding an organisation that shares the aims of the venture.\" It was later reported that, EE, the UK operations of Orange and T-Mobile, had decided against joining, after holding advanced discussions.\n\nThere are three main elements to the project: setting the technical standard, building the technical platform, and creating the user experience.\n\nThe BBC and other broadcasters with investments in YouView have announced intentions to sell their shares. An on-demand service for by Sony's UK Kids channel POP was released via a software update on 10 January 2018 called POP FUN. \n\nOn 23 July 2010 Kip Meek was announced as the non-executive chairman of Project Canvas. Meek led the Board, and oversaw the appointment of Richard Halton as CEO from programme director. Meek stepped down from his full-time role at Ingenious Media and his non-executive positions at the Broadband Stakeholder Group and Phorm to take the job. As chair, Meek was paid £97,000 during his eight-month tenure.\n\nMeek left YouView on 7 March 2011, and was replaced by Alan Sugar with immediate effect, who was brought on board by Channel 5's Richard Desmond. The move was partly based on his experience with set-top boxes, in particular that of Sky, and partly due to his likely influence in retaining confidence in the various partners. Sugar was paid £500,000 for chairing YouView for the year ending March 2012.\n\nAlan Sugar stepped down as Chairman in March 2013, with TalkTalk group chairman Sir Charles Dunstone taking on the chairmanship as an interim position. In October 2013 Bwin.Party Digital Entertainment chairman Simon Duffy was named as non-executive chairman of YouView on a permanent basis.\n\nOn 13 November 2009, BBC Future Media and Technology director Erik Huggers previewed the work-in-progress user interface that could power the Project Canvas at C21 Media's FutureMedia conference in London. The mock-up of how the Beijing Olympic Games would look on Canvas allowed users to watch highlights instantly, send clips to friends, monitor what's being said on Twitter, access archives at the touch of a button, and use commercial third party applications and services.\n\nFollowing the BBC Trust's provisional approval of the BBC's participation, the partners formed a new joint venture to develop the technical specification for devices with standards body the Digital TV Group (DTG), create and market a new consumer brand, build a common user experience, and build the technology platform. All technical specifications must be clearly published to allow manufacturers to adapt to the new Canvas standard.\n\nThe BBC initially began working with three innovation partners; Technicolor SA, Humax, and Cisco Systems; from the consumer device manufacturing sector on the development of the Canvas core technical specifications. The relationships have non-disclosure agreements (NDAs), non-binding collaboration agreements, and agreements ensuring any intellectual property (IP) the trio develop can be shared with the industry. On 5 March 2010, in a 'questions and answers' (Q&A) session at the DTG Summit, Richard Halton, then programme director for Project Canvas, confirmed that Canvas has direct collaboration relationships with various manufacturers and industry players – including Cisco, Technicolor, Humax, Intel, LG Group, and Broadcom – and Halton stressed that collaboration with the DTG was \"absolutely critical\" to the project's future success.\n\nThe system offers access to the broadcasters' own video on demand (VOD), but has also promised to offer a software developers' kit (SDK) to encourage internet content on to the screen. Canvas also promised to offer payment mechanisms to content owners who wish to charge for their content.\n\nSpeaking on 25 February 2010, BT Vision chief executive Marc Watson announced that the project was targeting a commercial launch within the next 12 months, initially aimed at the UK's 10 million Freeview households as a starting point. Open technical standards required for third-party developers to create services for the Canvas platform would be published in the summer. He added that a management group, called The Venture, would run the platform in a neutral, non-discriminatory way, and that should BSkyB wish to take part, it would apply to this body, which is not influenced by individual corporate considerations.\n\nOn 7 May 2010, Project Canvas submitted key documents to the Digital TV Group (DTG), making the next set of technical specifications available to industry. The publication of these documents is in addition to Project Canvas partners' active participation in the DTG Connected TV working groups. This DTG work was to result in publication of the UK Connected TV Specification: D-Book 7, by December 2010. In addition to the previously available Consumer Device Platform Specification and Broadcast Content Delivery Specification, further documents were published in the members' area of the DTG website; including the Consumer Device Software Management Specification, IP Content Delivery Specification, and System Metadata Model.\n\nOn 6 July 2010 Project Canvas published information on its content protection strategy. Providers can choose to make content available with no protection at all, or adopt transport encryption, file encryption, device authentication, or digital rights management (DRM). Conditional access (CA) upgrade will also be possible for those who require it. For providers of premium content, Canvas will support Marlin as the required DRM solution, at launch, which has been developed over the past five years by Intertrust, Panasonic, Philips, Samsung, and Sony. The selection of Marlin follows widespread industry engagement with content owners, content distributors, device manufacturers, and internet service providers (ISPs), from which it was concluded that a common DRM solution present on all devices at launch, and widely supported by content providers would benefit all industry participants. Marlin is referenced in Release 1 of the Open IPTV Forum specifications, and therefore has the potential to be widely adopted as a part of internet-connected TV device deployments worldwide.\n\nOn 11 August 2010, the Project Canvas partners formally invited expressions of interest from consumer equipment manufacturers to develop and bring Canvas devices to market in 2011. By the 25 August deadline, more than forty organisations had expressed their support for the Project Canvas, representing a broad range of consumer device manufacturers; including set-top boxes, internet-enabled TVs, and recorders. Project Canvas evaluated the responses, with those selected to be taken through to the next stage of the evaluation process by the end of September. The initial target of Canvas was twin-tuner digital terrestrial television DVB-T2 IP-connected digital video recorders (DVR). Project Canvas also welcomed a response from device manufacturers proposing other device categories, such as internet-enabled TV (IETV) and set-top boxes. Canvas devices would be made available to consumers though both retail channels and bundled with broadband packages.\n\nThe service offer on-demand TV; from 4oD, BBC iPlayer, Demand Five, and ITV Player; and high-definition PVR capabilities without a monthly subscription fee, but will allow commercial broadcasters to use micro payments, and pre-, mid- and post-roll adverts to generate income. The YouView electronic programme guide (EPG) will not carry any advertising, in line with the not-for-profit aims of the business. While YouView will not have a payment system, commercial content owners will be able to insert their own, or use facilitators such as PayPal.\n\nIn May 2010 the \"Financial Times\" reported that the name YouView was the most likely brand for the service, having been registered as intellectual property (IP) by the group of broadcasters in April. On 16 September 2010 YouView TV Ltd was incorporated, and the product branded under the YouView name. At the same time, Richard Halton was appointed as CEO of YouView TV Ltd, having previously served as programme director for Project Canvas.\n\nOn 16 September 2010 a 'Notice of threatened opposition' was filed against YouView with the Intellectual Property Owners Association (IPO). Robin Fry, an intellectual property partner at law firm Beachcroft LLP, said that the site's similarity to YouTube could cause consumers to confuse the two brands. The use of the world 'You', coupled with a capitalised character in the middle of the word, might suggest a connection between the services. If Google did object, it would be highly likely to win any legal action, according to Fry. \"Google is involved in litigation all over the world, and would not be frightened of taking action against this trademark. It has a clear interest in protecting its trademarks to stop them being diluted\", he said.\n\nOn 4 November 2010, YouView appointed Adam & Eve as its advertising agency to create through-the-line advertising and communications. OMD was also appointed as YouView's planning and buying agency, and Momentum will act as YouView's specialist retail and experiential marketing agency. The teams are tasked with coordinating the brand launch and roll-out of YouView across all media and channels.\n\nOn 14 April 2011, YouView announced additional hardware partnerships with Vestel, Pace plc, Huawei, and Manhattan.\n\nOn 13 May 2011, Technicolor, one of the original set-top-box hardware manufacturing partners decided to withdraw from the project. The six remaining manufacturers at that point were Humax, Huawei, Pace, Manhattan, Vestel, and Cisco.\n\nTalkTalk began running an in-house trial at the beginning of February 2012 to prepare for the launch.\n\nOn-demand players are available to all customers who have a YouView box connected to broadband. \nPlayers vary as rentals, on demand, subscription and catch-up.\n\nOn 14 April 2011, YouView published its final core technical specification. The minimum specifications for a YouView-enabled digital terrestrial television (DTT) high-definition (HD) digital video recorder (DVR or PVR) product require:\n\nAnalogue HD outputs are forbidden as part of the rights management strategy.\n\nThe specifications will evolve over time to reflect different devices, including non-PVR and Freesat variants.\n\nSet-top boxes available for the YouView service:\n\nNote 1: each YouView box comes in a variety of hard disk storage sizes, and is often designated by ModelNo/StorageSize – e.g. DTR-T1000/500GB\n\nNote 2: each YouView box does not have support for Wi-Fi, and must connect directly to the router or through power-line adapters. YouView have not allowed Wi-Fi in their software, so USB Wi-Fi dongles will not work – however YouView suggest the use of a third-party (not supplied by YouView) Wireless Ethernet Bridge product may work, see YouView support for updated information.\n\nNote 3: YouView+ has become the designated way to show the set-top box records TV programmes. \nThis is because of services offered by TalkTalk TV.\n\nOn 26 February 2009, the BBC Trust launched a public consultation on the BBC Executive's proposal (Project Canvas) to develop a joint venture partnership to help enable the delivery of internet protocol television (IPTV), which would allow viewers to watch on-demand services, via television sets.\n\nOn 22 December 2009, the BBC Trust gave provisional approval to the BBC's involvement in Project Canvas after ruling that the likely public value of the proposal justifies any potential negative market impact. A period of consultation on the provisional conclusions closed on 2 February 2010. The BBC Trust has revealed that Project Canvas will cost over £115m in its first four years of operation.\n\nOn 25 June 2010, the BBC Trust gave final approval to the BBC's involvement in Project Canvas, stipulating a number of conditions. The Trust concluded that Project Canvas will deliver significant public value for licence fee payers. The Trust will review the BBC's involvement in Canvas against the conditions of its approval, twelve months after launch of Canvas to consumers.\n\nThe Digital TV Group (DTG) welcomed the decision, and is working with the Canvas partners and Virgin Media, Sky, and DTG members to develop the UK specification for Connected TV devices and services which will form the 7th edition of the DTG D-Book. A spokesman for Virgin Media said: \"We are disappointed the BBC Trust has approved Canvas and ignored the significant concerns raised by the commercial sector about the proposal. Our position on this matter remains unchanged. As it stands, Canvas will severely restrict competition and innovation and ultimately this will harm consumers\". A spokesman for British Sky Broadcasting (BSkyB) said: \"The BBC's involvement in Canvas is an unnecessary use of public funds. The BBC Trust's announcement is a predictable decision from a body that has shown little inclination to think independently or set meaningful boundaries on the BBC's activities\".\n\n\nFollowing the BBC's proposal in February 2009, there have been comments by a number of organisations and companies which have been published by the BBC in a 392-page document. The BBC Trust reached its provisional conclusions following more than 800 written responses.\n\nThe UK's Intellect Technology Association, said in a submission to the BBC Trust that Project Canvas risks isolating the UK as a \"technological island\" in a global market by trying to create a standard IPTV set-top box for just the UK.\n\nBritish Sky Broadcasting has continuously, strongly criticised the project funding, saying that public money will be used to give public service broadcasters a foothold in the valuable IPTV, to the disadvantage of \"the private sector\". BSkyB is likely to raise state aid complaints if it appears that the BBC is shouldering the costs of developing the venture for its partners. \nSky estimates that it will lose four percent of revenue as a result of the project.\n\nOn 4 February 2010, \"The Guardian\" learned that in a submission to the BBC Trust as part of a final consultation on Project Canvas, the Digital TV Group said that there is \"widespread concern\" that the venture's partners are developing critical technology standards that do not involve key players such as set top-box manufacturers and TV makers. The DTG argue that the BBC Trust's provisional approval document does not contain a \"clear and unequivocal condition\" that Project Canvas will have to work with industry and there appears to be a \"parallel process\" taking place where Project Canvas and its preferred technology partners are developing a separate standard.\n\nMorgan Stanley compiled a creative analysis for BT about the potential impact of Canvas, describing it as \"Freeview 2.0\". Morgan Stanley noted that the BBC Trust will reach a final decision on the BBC's Canvas involvement at the end of March. If it gives the full green light, then the platform could enter launch phase by the end of the year or in early 2011. However, Morgan Stanley noted that any launch date would be dependent on the completion of major technical work, such as locking down specifications with manufacturers and also creating a workable electronic programming guide. Morgan Stanley expects that Canvas set-top boxes will cost between £150 and £200. The report predicts that an App Store-style resource could be introduced to Canvas in the future, along with the integration of social networks such as Twitter. As Canvas will be an open platform, the report noted that content providers would no longer need to pay the current sum of around £10m for capacity on digital terrestrial television to reach their target audience. Internet service providers would also benefit from the service due an increased consumer demand for fixed broadband packages, along with greater opportunities to sell more expensive tariffs for high definition streaming. Storing popular programmes on local Canvas drives would also ease the burden of heavy streaming traffic over broadband networks. Considering outstanding issues facing the project, Morgan Stanley said: \"Canvas looks pro-competitive (breaking down platform barriers for content owners), but some media companies could still try to challenge its creation. \"It looks unlikely that this could freeze launch however as content is not being aggregated and the BBC shareholding will be only 16.6%.\"\n\nOn 4 March 2010, \"The Daily Telegraph\" learned that Neil Berkett, Virgin Media's chief executive, would tell the Cable Congress in Brussels \"the BBC Trust's consultation has been a shameless whitewash that contravenes almost every principle of good regulation.\" Mr Berkett objects to proposals to force all broadcasters to use a single 'Project Canvas' brand controlled by the BBC and its partners, which he claims will penalise commercial rivals. \"The BBC Trust has stubbornly ignored all requests to address our concerns by imposing safeguards to prevent the BBC emerging as de facto gatekeeper of the digital world. This is a blatant demonstration that the Trust is incapable of regulating the BBC's activities in an objective way.\" On 28 April 2010, Neil Berkett confirmed that Virgin Media had made a submission to the Office of Fair Trading over Project Canvas. \"Canvas needs to be an open platform but it is closed and will require a [second] dedicated set-top box,\" said Berkett. \"We will oppose it vigorously if it is not an open world at large for customers to take advantage of. [The current proposals] are asking pay-TV customers to buy a second set-top box.\" He added that he considered it a misuse of the licence fee to create a product that was not accessible to all the public. \"It is funded by the BBC licence fee and should be available everywhere, on a Virgin Box, a PS3 and even a Sky box,\" he said. \"I find it extraordinary that it looks like the BBC Trust is endorsing what is a closed product. It is inappropriate for the BBC to be doing this.\"\n\nOn 14 June 2010, Neil Berkett revealed to The Guardian that \"Far from trying to block the development of these open standards, we have offered to work commercially with Canvas to explore mutually beneficial ways in which we could incorporate them as a self-contained service in the next generation of Virgin Media set-top boxes.\" Berkett went on to explain his opposition to the project; \"The Canvas consortium has rejected the opportunity to incorporate Canvas into the Virgin Media customer experience, insisting that if we want to use their standards we must also accept that the entire Virgin Media entertainment service be accessed by our subscribers via a Canvas-imposed interface, including the Canvas channel listing and search facility. This \"shop window\" to services would be entirely controlled by the joint venture partners and would allow the Canvas partners to give preference and prominence to their own channel content above that of any other content provider. At this point, Canvas starts to look less like a set of genuinely \"open\" standards and more like a fully-fledged competing distribution platform from which established pay TV operators are effectively excluded, along with other innovative platforms offering a differentiated user experience, such as the PS3 and the Xbox. Unless we accept the Canvas consortium's conditions, people who want both Canvas and a pay TV service will have to buy two set-top boxes. Far from simplifying the digital world, Canvas will complicate it.\" On the BBC's involvement Berkett said: \"Quite rightly, much attention has been focused on whether the BBC should be using the licence fee to bank-roll such a controversial intervention in a dynamic market. And it's true that many private sector companies are already investing precious capital in \"connected TV\". But it's the closed nature of the Canvas platform which gives the BBC's involvement significance. A set of standards that are genuinely open to all and to which the BBC has contributed is one thing. A proprietary gateway to the digital world, underpinned by the formidable brand and marketing muscle of the BBC, is quite another.\"\n\nOn 5 March 2010, in a Q&A session at the DTG Summit, Richard Halton, then programme director for Project Canvas, faced criticism from Digital TV Group members about the lack of published technical specifications for the platform. In response, he asked members to appreciate the \"tough and pretty challenging\" schedule exerted on the project. However, Halton acknowledged the DTG's desire for more effective engagement with Canvas, and committed to publishing the remaining key technical documents by the end of May.\n\nOn 19 April 2010, 3view managing director John Donovan has said that he is \"concerned\" about the potential market impact of IPTV joint venture Project Canvas. \"The honest answer is that no-one has spoken to me from the BBC about what Canvas will entail. We do not understand what Canvas's remit will be and we do not subscribe to the belief that Canvas will provide something the commercial market can't. We have proved that we can do it,\" he told Digital Spy. Donovan declined to be drawn on whether 3view would consider a legal challenge to Canvas, but did say that his business will defend its ability to compete in the market. \"I think we would have to reserve comment [on possible legal action]. But at this stage, clearly, we will protect our business at all costs. And I think, we will just have to leave it there for the moment.\"\n\nOn 22 March 2010, the Project Canvas partners submitted analysis to the Office of Fair Trading setting out why the proposed Canvas joint venture does not constitute a qualifying merger under the Enterprise Act 2002. On 19 May 2010, the OFT confirmed that it would not investigate Project Canvas over competition issues. As none of the partners were contributing a \"pre-existing business\" to Project Canvas, the OFT ruled that it \"did not have the jurisdiction\" to investigate the venture on competition grounds. \"Unlike in the Project Kangaroo joint venture which was blocked by the Competition Commission in 2009, it is not proposed that the joint venture partners will contribute any video-on-demand content or other business to [Project] Canvas, and Canvas will have no role in aggregating, marketing or directly retailing any such television content,\" said the OFT. The OFT also said that it found that none of the partners would have a \"material influence\" over the policy of the venture – the \"lowest level of control that may give rise to a relevant merger situation\".\n\nOn 3 August 2010, Virgin Media lodged a complaint with Ofcom claiming that Project Canvas is an anti-competitive cartel that will crush the nascent online TV market. The cable TV company, which is also calling on the Office of Fair Trading to step in on the grounds that the venture breaches the Competition Act, argues that the six partners backing Project Canvas have not stuck to their stated pledge of creating a set of open standards for delivering next-generation TV services. \"Collectively the BBC, ITV and Channel 4 account for around two-thirds of all television viewing in the UK while BT and TalkTalk control over half the national broadband market,\" said Virgin Media. \"[Project Canvas] will stifle future innovation as well as eliminate existing consumer choice for home entertainment. These well-resourced companies usually are direct competitors in their respective markets but... they are establishing a single new TV platform of their own with a considerable incentive to favour this over other TV services. This could severely affect consumer options for watching public service programming in the future.\"\n\nVirgin Media has also complained that Project Canvas has wrongly rejected what the company considers fair offers of integrating the technology into its own set-top boxes with a compromise on user interface. \"We have not taken the decision to file this complaint lightly,\" said a Virgin Media spokesman. \"However, the Canvas partners have significantly exceeded their original claims to be creating a common set of open standards which could have been improved upon by others and are now intent on controlling every aspect of how people watch television.\"\n\nOn 18 August 2010, IP Vision formally complained to Ofcom, calling on the regulator to examine the impact Project Canvas will have on innovation, competition and consumer choice. IP Vision is challenging the validity of Project Canvas, under the 1998 Competition Act. The company has already challenged the BBC over support for BBC iPlayer.\n\nOn 30 August 2010, Six TV, the largest holder of analogue television restricted service licences in the UK (none of which are in use), announced that it would formally request a full Ofcom investigation of Canvas, warning that it could be a \"poison pill\" for regional broadcasters. Six TV will also submit its complaint regarding Canvas – which includes broader concerns regarding anti-competitive practices affecting digital television transmission in the UK – to the Office of Fair Trading. Six TV also intends to lobby communications minister Ed Vaizey. On 9 September 2010, United For Local TV an umbrella group which represents five local broadcasters with restricted service licenses (including Six TV), asked Ofcom to investigate the Project Canvas connected-TV venture on competition grounds. United For Local TV complained that qualification for including those services is \"arbitrary\", fees are charged that are \"unrelated to a service provider's ability to pay\", their channels would be hard to find on the EPG, the JV members may exploit viewer data \"for commercial advantage\" and Canvas could \"prevent viewers from obtaining any streamed services on the open internet from TV channels who are unwilling or unable to meet the access terms.\"\n\nOn 13 September 2010, the Open Source Consortium, the UK trade body for organisations working in Open Source Software, submitted a formal objection to Ofcom asking it to investigate the project. In its submission to the media regulator, the OSC said that Canvas will have \"adverse consequences\" for the device and software sector by \"diminishing consumer choice and causing inevitable consumer harm\".\n\nOn 24 September 2010, ISBA – the trade body representing advertisers, joined a growing list of parties asking Ofcom to investigate YouView. ISBA director of media and advertising Bob Wootton believes that the project could represent a \"quasi-monopoly\". Wootton also claimed that the BBC Trust's recent consultation on Canvas/YouView was \"insufficient\" for a platform that would carry advertising.\n\nOn 28 September 2010, Electra Entertainment – a UK-based IPTV service provider, complained to media regulator Ofcom that YouView will \"damage\" the UK's interactive TV sector. Electra has developed an IPTV platform, called Trove, which brings rich media services to the TV screen. It is currently available on Tesco-branded Freeview set top boxes. Electra believes that \"the proposed vision, shareholder structure and aims of YouView are anti-competitive and significantly damage the UK interactive TV market\". The firm also claimed that YouView's estimated initial marketing budget of £48 million is a \"major cause for concern amongst venture capitalists looking to fund competing private British businesses in this emerging broadband TV sector\".\n\nOn 13 October 2010, British Sky Broadcasting submitted a last-minute complaint to Ofcom and the Office of Fair Trading. The media regulator is now likely to delay a decision on whether to launch a full investigation into the service, previously called Project Canvas, while it considers Sky's objection. News of Sky's move prompted an angry reaction from the YouView chief executive, Richard Halton, \"While we welcome justifiable scrutiny, the timing of this submission is clearly designed to extend the regulatory process in pursuit of commercial self-interest rather than the public interest\", he said. Sky is believed to argue in its submission that the seven-partner YouView venture will stifle competition in the on-demand market.\n\nOn 19 October 2010, Ofcom announced that it will not open an investigation into Project Canvas (YouView) under the Competition Act following complaints made by Virgin Media and IPVision. Ofcom also received submissions from 11 other parties, including BSkyB.\n\nThe regulator said that it is \"premature\" to open an investigation into YouView, as \"whether or not YouView and its partners will harm competition in the ways alleged will depend upon how this emerging market develops and how they act, particularly in relation to providing access to content and issuing technical standards\". Ofcom noted that there is \"little evidence\" at this stage that YouView partners the BBC, ITV, Channel 4 and Channel 5 plan to restrict access to their video on-demand content.\n\nOfcom do not propose to open a Competition Act investigation, however Ofcom will continue to monitor developments, particularly in relation to YouView’s approach to sharing standards and its effects on content syndication. If evidence emerges that the operation of YouView could cause harm to viewers and consumers in the future, Ofcom may reconsider whether to investigate.\n\n\n"}
